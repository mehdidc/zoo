{"content": {"hp_model": {"f0": 64, "f1": 16, "f2": 64, "f3": 16, "nonlin": "leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.6298739910125732, 1.2193150520324707, 0.945069432258606, 0.8188000917434692, 0.7430329918861389, 0.6891723871231079, 0.6460698843002319, 0.614921510219574, 0.5870854258537292, 0.5629152059555054, 0.5435085892677307, 0.5243579149246216, 0.5067723989486694, 0.49338144063949585, 0.4793863296508789, 0.46515020728111267, 0.45466795563697815, 0.4443366825580597, 0.4350537359714508, 0.42470547556877136, 0.416493684053421, 0.40776515007019043, 0.40151602029800415, 0.391846239566803, 0.38439786434173584, 0.37741076946258545, 0.3713737726211548, 0.3651174008846283, 0.35931143164634705, 0.3529225289821625, 0.3480556011199951, 0.3434597849845886, 0.3389001190662384, 0.33149459958076477, 0.32784947752952576, 0.32399287819862366, 0.3200650215148926, 0.31569764018058777, 0.311082661151886, 0.3060467839241028, 0.301517516374588, 0.3001270294189453, 0.29349520802497864, 0.2921830415725708, 0.2876449227333069, 0.2845034897327423, 0.28144583106040955, 0.27845340967178345, 0.27504852414131165, 0.27240779995918274, 0.269783079624176, 0.266668438911438, 0.26429250836372375, 0.2623452842235565, 0.2586766183376312, 0.2576759159564972, 0.2547267973423004, 0.2530040144920349, 0.24856328964233398, 0.24640555679798126, 0.24414266645908356, 0.2426379919052124, 0.23995253443717957, 0.23743212223052979, 0.23520900309085846, 0.23582671582698822, 0.23316539824008942, 0.23042632639408112, 0.22851961851119995, 0.2264755815267563, 0.2234956920146942, 0.2231336086988449, 0.22171036899089813, 0.21917366981506348, 0.2177465856075287, 0.21537427604198456, 0.21439024806022644, 0.21289554238319397, 0.2110430896282196, 0.2094612717628479, 0.20743528008460999, 0.20553070306777954, 0.20588791370391846, 0.20502953231334686, 0.20322321355342865, 0.2001509815454483, 0.19884634017944336, 0.1986963450908661, 0.19622761011123657, 0.19603237509727478, 0.19554588198661804, 0.19357721507549286, 0.19184164702892303, 0.19166995584964752, 0.19050714373588562, 0.1898203045129776, 0.18784569203853607, 0.18612490594387054, 0.1852773129940033, 0.18565315008163452, 0.18358798325061798, 0.18180783092975616, 0.18091155588626862, 0.18028970062732697, 0.17952391505241394, 0.178447425365448, 0.17725354433059692, 0.17479541897773743, 0.1740199774503708, 0.17538726329803467, 0.17291809618473053, 0.17196498811244965, 0.17057093977928162, 0.1705576479434967, 0.16933684051036835, 0.16858568787574768, 0.16618016362190247, 0.16611498594284058, 0.1656101942062378, 0.1656336635351181, 0.16331374645233154, 0.16232088208198547, 0.16213370859622955, 0.1615990400314331, 0.16091212630271912, 0.16134953498840332, 0.1609116643667221, 0.15976059436798096, 0.1590982973575592, 0.15824344754219055, 0.15582676231861115, 0.15580134093761444, 0.15402758121490479, 0.153446763753891, 0.15437458455562592, 0.15288442373275757, 0.1510404646396637, 0.15321148931980133, 0.1511869728565216, 0.14953553676605225, 0.14965090155601501, 0.14902563393115997, 0.14717049896717072, 0.14770078659057617, 0.14740034937858582, 0.14698995649814606, 0.14490875601768494, 0.14541223645210266, 0.14383183419704437, 0.1441262662410736, 0.1429920792579651, 0.142229825258255, 0.1424119621515274, 0.14118792116641998, 0.14100193977355957], "moving_avg_accuracy_train": [0.03615644250069213, 0.07721696450200718, 0.1334606885829065, 0.18913179554479856, 0.24074913924613264, 0.29228858731510554, 0.34056432841286777, 0.3868856944018689, 0.42979094057053213, 0.46909855646756715, 0.5053378607391382, 0.5393620224203923, 0.5710393494442261, 0.5997673635584967, 0.625359978641139, 0.64947209311668, 0.6721610762422582, 0.6924741681612218, 0.7117277909930324, 0.7291629723380814, 0.7448431179510341, 0.7596153749716837, 0.773345100971193, 0.7856601098386362, 0.7974549331109556, 0.8078122185870047, 0.8180800029344689, 0.8269931628650438, 0.8357310444894382, 0.8436137391418694, 0.8510337933088471, 0.8577976562721669, 0.8639479840546493, 0.8698761931588743, 0.8755323076931161, 0.8806856258406096, 0.8854027032816963, 0.8894967941107803, 0.8937790391010035, 0.8976818877172044, 0.9012595916872705, 0.9046282626865022, 0.9076112745096294, 0.9108400339230538, 0.9137738191808499, 0.9164839803771521, 0.9188998739657288, 0.9212043865287812, 0.9235294918093195, 0.9256849016284798, 0.9278457317002664, 0.9295718426791415, 0.9312741520839387, 0.9331829407042178, 0.9346636852838975, 0.936112540748448, 0.937672277035591, 0.9389667937487909, 0.9402457550335186, 0.9416176732778596, 0.9429476947501383, 0.9438517453251891, 0.9448467524498776, 0.9459677622478024, 0.947120866340944, 0.9482586414235809, 0.949182729696782, 0.9499747734664449, 0.9508201463412844, 0.9517273942060024, 0.9523905295604576, 0.9533314013056392, 0.9541874864715407, 0.9549161104422808, 0.9556346870826226, 0.9563162111934356, 0.956948256181281, 0.9576101386715417, 0.9581616190365765, 0.9587323561270126, 0.9594737759452822, 0.9602247591388676, 0.9609540863868948, 0.9615175110065571, 0.9621013230749675, 0.962619814538927, 0.9629166849445766, 0.963388517453718, 0.9639014502690697, 0.9643491389100292, 0.9648031759118834, 0.965239783096904, 0.9655559275550707, 0.9659917364353148, 0.9663444368977726, 0.9668221583865668, 0.9671079845491098, 0.9675419754537412, 0.9679185442774147, 0.9683761109056441, 0.9688088111615175, 0.9692889582441937, 0.9695513547555071, 0.9699688371740133, 0.9705654244387548, 0.970937267411546, 0.9711277668608675, 0.9712457739914566, 0.97148912813993, 0.9719127599687942, 0.9721778072231145, 0.9726000004591364, 0.9729776492227465, 0.9734035636159482, 0.9737682853793533, 0.9740035290140371, 0.974152505316214, 0.9745051840250872, 0.9748899520809118, 0.9751014207490203, 0.9752894174015084, 0.9755515842923099, 0.9757108766809545, 0.9759170188485917, 0.9760862347089799, 0.9763733515654629, 0.9766596585220119, 0.976935935973382, 0.9772217880605676, 0.9774651400949963, 0.9777957280200205, 0.9780421399275515, 0.9783685423407579, 0.9785088446912151, 0.9787002209732933, 0.9788306069485923, 0.9790851020573045, 0.9792072268587261, 0.9795054401847583, 0.9796878016722348, 0.9797496204633447, 0.9799564280967813, 0.9802169236799603, 0.9802630687000687, 0.9803232364574612, 0.9804773327891053, 0.9806252840340044, 0.9807514647079849, 0.9808394506776625, 0.9811488638313342, 0.9813459194124865, 0.9815837593533899, 0.9817048453966408, 0.9818486640188908, 0.9818665296848773], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.036801698983433724, 0.07816738634224396, 0.13350320912556474, 0.1880907529532191, 0.2381625723303369, 0.28818547054722793, 0.33466592222179725, 0.3788900244179157, 0.41975784614781086, 0.4574198509720358, 0.4918029070551786, 0.5239165910766939, 0.553416021718648, 0.5802839216175664, 0.6039055671064122, 0.6260582203449426, 0.6464859485269393, 0.6649197320157363, 0.6823534518205633, 0.6979004037958865, 0.7118275073827889, 0.7244341132898413, 0.7363965872206463, 0.7470000043174823, 0.7569103201507943, 0.7661531642652329, 0.7743821567148391, 0.7817425103204636, 0.788683181869366, 0.7948199229821282, 0.8003165169037949, 0.8054883255390931, 0.8102273730209518, 0.814772247964715, 0.8189989717751712, 0.8226147701925336, 0.825895461847979, 0.8288145517701089, 0.8316614592625258, 0.8339876243773123, 0.8363304611489184, 0.8384857833510446, 0.8401946692478679, 0.8419778366886684, 0.8435094451978888, 0.8451696840835969, 0.8463434277422553, 0.8476480556946864, 0.8486726479508955, 0.8496690526776433, 0.850786573002876, 0.8516224723667449, 0.8521265231345886, 0.8530338879992171, 0.8534975419797923, 0.8542932485310601, 0.8552321995156801, 0.8559409190407687, 0.8564414007436196, 0.8569569874670739, 0.8576773631744329, 0.8579218397711462, 0.8583137966543478, 0.8587052379602986, 0.8592274040644946, 0.8596332298760422, 0.8598306631949139, 0.8598668692775309, 0.860299345329823, 0.8606987217908166, 0.8607366597585723, 0.8610180330805314, 0.8615042321727041, 0.8616854636994096, 0.8618953411811252, 0.86186244533485, 0.8623262678665006, 0.8624242663151668, 0.8626955703877163, 0.8629428325789898, 0.8631521320112263, 0.8633394719915797, 0.8635792611440782, 0.8634420969837366, 0.8635749968956793, 0.8637169618616083, 0.8635772051521041, 0.8636121745371196, 0.8641014841918413, 0.8641928594454734, 0.8642150915261521, 0.8643683482338531, 0.8642234585347148, 0.8644979488540596, 0.8642058217491506, 0.8643854490057115, 0.8644128361928662, 0.8645127858661248, 0.8645630309523286, 0.8645716304361619, 0.8646658486990216, 0.8647872662293454, 0.8647846197080675, 0.8649440177711463, 0.8653814742865767, 0.8655198670028739, 0.8656210358937009, 0.8655879585656259, 0.8656945253314278, 0.8659216242384206, 0.8658340740133736, 0.8660004489444911, 0.8657788279016987, 0.8658185091175529, 0.866072919265662, 0.8655500529320929, 0.8654030330963384, 0.8654660277441594, 0.8656193496685386, 0.8657217478153896, 0.8655209373975554, 0.8653869771291854, 0.8656235053198813, 0.865356188438119, 0.865176638400783, 0.8652724205320902, 0.8652253766151763, 0.8654749763312942, 0.8654401798935714, 0.8655106373842595, 0.8655374280321287, 0.8655950721829822, 0.8656622474759791, 0.8655598957987878, 0.8654443947354753, 0.865364857840994, 0.8655150602157802, 0.8653785991396088, 0.8656770002855123, 0.8657206872110274, 0.8659095783449698, 0.8662169462352469, 0.8664528382081078, 0.8663609947110923, 0.8661196441575283, 0.8662208409804802, 0.8662366169163177, 0.8663281754807101, 0.866197911114115, 0.8663665824462277, 0.8661184960671923, 0.8662177486818586, 0.8661738281999679, 0.8661943054138567, 0.8660530139914469], "moving_var_accuracy_train": [0.011765595008752709, 0.025762733711061737, 0.05165666882635056, 0.07438445129697736, 0.09092515770431453, 0.10573947429917228, 0.11614045147609858, 0.12383742685227164, 0.1280214255061868, 0.12912508096314768, 0.1280321574336205, 0.12564773389326755, 0.12211403793031499, 0.11733032329183109, 0.11149212848356264, 0.1055754622155407, 0.0996510255914416, 0.0933995183620718, 0.08739588445521121, 0.08139216594650277, 0.07546575204984307, 0.06988315304221596, 0.0645913861221888, 0.05949718250061678, 0.05479952495478234, 0.05028503272119551, 0.04620537600773023, 0.04229983818648926, 0.03875700954537771, 0.03544054046569115, 0.03239200125368998, 0.02956454970800011, 0.026948533523687878, 0.024569973139969836, 0.022400900510593, 0.020399820650897273, 0.018560095962074426, 0.016854940583318087, 0.015334485124392902, 0.01393812665784244, 0.012659513683335044, 0.011495693813711115, 0.010426209668172252, 0.00947741268750301, 0.008607135282202468, 0.007812526517371703, 0.007083802742116471, 0.0064232194712842226, 0.005829552555246083, 0.00528840942311827, 0.004801591160198677, 0.00434824717618135, 0.003939503174350164, 0.0035783441228873095, 0.0032402431511908366, 0.002935111475486172, 0.0026634953235064305, 0.0024122277528425707, 0.0021857266552688065, 0.0019840934267643276, 0.001801604698138395, 0.0016287999953048021, 0.0014748303483779507, 0.0013386572802435454, 0.0012167583936657682, 0.0011067333435472158, 0.0010037454614225051, 0.0009090169152778109, 0.0008245471214276586, 0.0007495002974772111, 0.0006785080042144451, 0.0006186243605609308, 0.0005633578608063282, 0.0005118001107423275, 0.0004652672711605001, 0.00042292082006702576, 0.0003842240658602682, 0.00034974445515246366, 0.000317507184974388, 0.00028868813391454454, 0.0002647666506453962, 0.00024336576739428733, 0.00022381645476729343, 0.00020429183500893867, 0.00018693018028903904, 0.00017065666284392497, 0.00015438418489928735, 0.00014094939965950313, 0.00012922236035113458, 0.00011810395038921775, 0.00010814890174177094, 9.904964407369835e-05, 9.004420553219442e-05, 8.274914939987184e-05, 7.559381300584631e-05, 7.008839209296313e-05, 6.381482224041346e-05, 5.912847296409623e-05, 5.449186237835216e-05, 5.0926981113939664e-05, 4.751934860544174e-05, 4.484228473392004e-05, 4.097772362287291e-05, 3.844857538844118e-05, 3.78069651296631e-05, 3.5270673384424453e-05, 3.2070216407708474e-05, 2.8988525912766574e-05, 2.6622664495702763e-05, 2.5575573383973414e-05, 2.3650266468780556e-05, 2.288946397878617e-05, 2.18840848788142e-05, 2.132830402395881e-05, 2.0392671303875417e-05, 1.8851460282420705e-05, 1.71660597016715e-05, 1.656889417673684e-05, 1.6244422870110658e-05, 1.5022451561423949e-05, 1.3838291077402321e-05, 1.3073045277355023e-05, 1.1994107335340468e-05, 1.117714794131028e-05, 1.0317139213841358e-05, 1.0027350095947133e-05, 9.762360146667277e-06, 9.47308720322131e-06, 9.261181224634527e-06, 8.868045016116154e-06, 8.964835900050706e-06, 8.614821763603202e-06, 8.71218640536593e-06, 8.01813051072381e-06, 7.545941391730073e-06, 6.944351775548975e-06, 6.832826441220005e-06, 6.283774001198488e-06, 6.455777291487097e-06, 6.109500971370355e-06, 5.532944940641876e-06, 5.364575021806775e-06, 5.43883905932806e-06, 4.914119419322533e-06, 4.455288908657108e-06, 4.223471132626798e-06, 3.998130157168558e-06, 3.7416112038272924e-06, 3.437123861185688e-06, 3.955039972051838e-06, 3.90901409341606e-06, 4.02722322147502e-06, 3.7564573681589376e-06, 3.5669657962958033e-06, 3.213141854856499e-06], "duration": 114490.17585, "accuracy_train": [0.3615644250069214, 0.4467616625138427, 0.6396542053110004, 0.6901717582018273, 0.7053052325581396, 0.7561436199358619, 0.7750459982927279, 0.8037779883028792, 0.8159381560885014, 0.8228670995408823, 0.831491599183278, 0.8455794775516795, 0.8561352926587301, 0.8583194905869325, 0.8556935143849206, 0.8664811233965486, 0.8763619243724622, 0.8752919954318937, 0.8850103964793282, 0.8860796044435216, 0.885964428467608, 0.8925656881575305, 0.8969126349667773, 0.8964951896456257, 0.9036083425618309, 0.901027787871447, 0.9104900620616464, 0.9072116022402179, 0.9143719791089886, 0.9145579910137505, 0.9178142808116464, 0.9186724229420451, 0.9193009340969915, 0.9232300750968992, 0.926437338501292, 0.927065489168051, 0.9278564002514765, 0.926343611572536, 0.9323192440130121, 0.9328075252630121, 0.9334589274178663, 0.9349463016795865, 0.934458380917774, 0.9398988686438722, 0.9401778865010151, 0.9408754311438722, 0.9406429162629198, 0.9419449995962532, 0.9444554393341639, 0.9450835900009228, 0.9472932023463455, 0.945106841489018, 0.9465949367271133, 0.9503620382867294, 0.9479903865010151, 0.9491522399294019, 0.9517099036198781, 0.9506174441675894, 0.9517564065960686, 0.9539649374769288, 0.954917888000646, 0.951988200500646, 0.9538018165720746, 0.9560568504291252, 0.9574988031792175, 0.9584986171673128, 0.9574995241555924, 0.957103167393411, 0.9584285022148394, 0.9598926249884644, 0.9583587477505537, 0.9617992470122739, 0.9618922529646549, 0.9614737261789406, 0.9621018768456996, 0.962449928190753, 0.96263666107189, 0.9635670810838871, 0.96312494232189, 0.9638689899409376, 0.9661465543097084, 0.966983607881137, 0.96751803161914, 0.966588332583518, 0.9673556316906607, 0.9672862377145626, 0.9655885185954227, 0.9676350100359912, 0.9685178456072352, 0.9683783366786637, 0.9688895089285714, 0.9691692477620893, 0.9684012276785714, 0.9699140163575121, 0.969518741059893, 0.9711216517857143, 0.9696804200119971, 0.9714478935954227, 0.9713076636904762, 0.9724942105597084, 0.972703113464378, 0.9736102819882798, 0.9719129233573275, 0.9737261789405685, 0.9759347098214286, 0.9742838541666666, 0.9728422619047619, 0.9723078381667589, 0.9736793154761905, 0.9757254464285714, 0.9745632325119971, 0.9763997395833334, 0.9763764880952381, 0.9772367931547619, 0.97705078125, 0.9761207217261905, 0.9754932920358066, 0.9776792924049464, 0.9783528645833334, 0.9770046387619971, 0.9769813872739018, 0.9779110863095238, 0.977144508178756, 0.9777722983573275, 0.9776091774524732, 0.9789574032738095, 0.9792364211309523, 0.9794224330357143, 0.9797944568452381, 0.9796553084048542, 0.9807710193452381, 0.9802598470953304, 0.9813061640596161, 0.9797715658453304, 0.9804226075119971, 0.9800040807262828, 0.9813755580357143, 0.9803063500715209, 0.9821893601190477, 0.9813290550595238, 0.9803059895833334, 0.9818176967977114, 0.9825613839285714, 0.9806783738810447, 0.9808647462739941, 0.9818641997739018, 0.9819568452380952, 0.9818870907738095, 0.9816313244047619, 0.983933582214378, 0.9831194196428571, 0.9837243188215209, 0.9827946197858989, 0.98314303161914, 0.982027320678756], "end": "2016-01-30 19:19:22.908000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0], "moving_var_accuracy_valid": [0.012189285432605406, 0.026370437705346365, 0.05129187348277548, 0.07298088560472271, 0.08824748090586287, 0.10194334592943352, 0.11119290282733799, 0.11767555348007876, 0.12093960780858931, 0.12161148649414985, 0.12009008875528376, 0.11736267819265891, 0.11345835804718818, 0.10860947864727402, 0.102770370002953, 0.09690999341221687, 0.09097462277909307, 0.08493539986459038, 0.07917727115423029, 0.0734349134803143, 0.06783710006116564, 0.06248372866751064, 0.05752326284366629, 0.05278282864646492, 0.04838847502106238, 0.04431849902487046, 0.0404960959730045, 0.03693405962250252, 0.033674209954199966, 0.030645725282145563, 0.027853065656588336, 0.02530848753197081, 0.022979765918091536, 0.020867692320572406, 0.01894170983604406, 0.01716520483619466, 0.015545550792218178, 0.01406768548675768, 0.012733860878515323, 0.011509174187935041, 0.010407656726387046, 0.009408699777903146, 0.008494112419188083, 0.007673318352366656, 0.00692709893875964, 0.006259196583302231, 0.005645675992558175, 0.005096426880150739, 0.0045962322957590175, 0.004145544467598484, 0.003742229685934409, 0.003374295267059618, 0.00303915234494273, 0.0027426469094265177, 0.0024703169936071954, 0.0022289836344880497, 0.0020140199316029126, 0.001817138488729798, 0.0016376789772708145, 0.0014763035465683542, 0.001333343662349294, 0.0012005472153714294, 0.001081875165618889, 0.00097506668572104, 0.0008800139341122775, 0.0007934947920049135, 0.0007144961320430284, 0.000643058316762492, 0.0005804358049084982, 0.0005238277384360104, 0.00047145791819698627, 0.00042502466489408004, 0.00038464970441973867, 0.00034648033777421287, 0.00031222874101277345, 0.0002810156061418156, 0.00025485022759543446, 0.00022945163809935987, 0.00020716892738746186, 0.00018700228196981628, 0.00016869631004384558, 0.0001521425454536099, 0.00013744578044715315, 0.00012387052846437766, 0.00011164243709728908, 0.00010065957985152132, 9.076940930703217e-05, 8.170347409732421e-05, 7.568794213142726e-05, 6.819429285107158e-05, 6.13793119546661e-05, 5.545276932529783e-05, 5.009642961701561e-05, 4.576489107404009e-05, 4.1956446175438585e-05, 3.805119511959091e-05, 3.425282612981407e-05, 3.09174529514931e-05, 2.7848428774532432e-05, 2.5064251457179e-05, 2.263772004096782e-05, 2.0506627986900284e-05, 1.8456028224884124e-05, 1.6839095085015137e-05, 1.687749940254746e-05, 1.536212235760963e-05, 1.39180264220893e-05, 1.253607076657354e-05, 1.1384671970077427e-05, 1.0710369995085442e-05, 9.708318372728716e-06, 8.986612094794871e-06, 8.529993864791254e-06, 7.691165868337159e-06, 7.504569992651845e-06, 9.21461581840774e-06, 8.487687725514405e-06, 7.674633883849768e-06, 7.118739007922991e-06, 6.501233531437282e-06, 6.214033593490051e-06, 5.75413841565692e-06, 5.682234839036198e-06, 5.757136192608276e-06, 5.471566516513656e-06, 5.006977814962112e-06, 4.526198204533261e-06, 4.6342785486552e-06, 4.181747822493445e-06, 3.808251362190602e-06, 3.433885875290821e-06, 3.120402920910222e-06, 2.848975308722259e-06, 2.658360570264845e-06, 2.5125889738752396e-06, 2.3182651347412206e-06, 2.2894854017896487e-06, 2.228131489399371e-06, 2.80670753534859e-06, 2.543213708962342e-06, 2.6100110824046744e-06, 3.199285153924833e-06, 3.3801618442739808e-06, 3.1180627113430774e-06, 3.3305072475598226e-06, 3.0896236955840272e-06, 2.7829012473895696e-06, 2.5800578590728607e-06, 2.4747713200053907e-06, 2.48334435249457e-06, 2.78893158041126e-06, 2.598698156033023e-06, 2.356189418995305e-06, 2.1243443236935846e-06, 2.0915792857433817e-06], "accuracy_test": 0.7991470025510204, "start": "2016-01-29 11:31:12.732000", "learning_rate_per_epoch": [0.004355000331997871, 0.0021775001659989357, 0.001451666816137731, 0.0010887500829994678, 0.0008710000547580421, 0.0007258334080688655, 0.0006221429212018847, 0.0005443750414997339, 0.0004838889290113002, 0.00043550002737902105, 0.0003959091263823211, 0.00036291670403443277, 0.00033500001882202923, 0.0003110714606009424, 0.00029033335158601403, 0.00027218752074986696, 0.00025617648498155177, 0.0002419444645056501, 0.00022921054915059358, 0.00021775001368951052, 0.00020738097373396158, 0.00019795456319116056, 0.00018934783292934299, 0.00018145835201721638, 0.0001742000167723745, 0.00016750000941101462, 0.0001612963096704334, 0.0001555357303004712, 0.00015017241821624339, 0.00014516667579300702, 0.0001404838840244338, 0.00013609376037493348, 0.00013196970394346863, 0.00012808824249077588, 0.0001244285813299939, 0.00012097223225282505, 0.00011770270793931559, 0.00011460527457529679, 0.00011166667536599562, 0.00010887500684475526, 0.00010621952242217958, 0.00010369048686698079, 0.00010127907444257289, 9.897728159558028e-05, 9.677778143668547e-05, 9.467391646467149e-05, 9.265958215110004e-05, 9.072917600860819e-05, 8.887755393516272e-05, 8.710000838618726e-05, 8.539216651115566e-05, 8.375000470550731e-05, 8.216981950681657e-05, 8.06481548352167e-05, 7.91818238212727e-05, 7.77678651502356e-05, 7.640351395821199e-05, 7.508620910812169e-05, 7.381356408586726e-05, 7.258333789650351e-05, 7.139344961615279e-05, 7.02419420122169e-05, 6.912698881933466e-05, 6.804688018746674e-05, 6.700000812998042e-05, 6.598485197173432e-05, 6.50000074529089e-05, 6.404412124538794e-05, 6.31159491604194e-05, 6.221429066499695e-05, 6.133803253760561e-05, 6.0486116126412526e-05, 5.965753734926693e-05, 5.8851353969657794e-05, 5.806667104479857e-05, 5.7302637287648395e-05, 5.65584450669121e-05, 5.583333768299781e-05, 5.51265875401441e-05, 5.443750342237763e-05, 5.376543413149193e-05, 5.310976121108979e-05, 5.246988439466804e-05, 5.1845243433490396e-05, 5.123529626871459e-05, 5.0639537221286446e-05, 5.0057475164067e-05, 4.948864079779014e-05, 4.893258665106259e-05, 4.8388890718342736e-05, 4.7857145546004176e-05, 4.7336958232335746e-05, 4.682796134147793e-05, 4.632979107555002e-05, 4.5842109102522954e-05, 4.5364588004304096e-05, 4.489691127673723e-05, 4.443877696758136e-05, 4.3989901314489543e-05, 4.355000419309363e-05, 4.311881639296189e-05, 4.269608325557783e-05, 4.2281557398382574e-05, 4.1875002352753654e-05, 4.147619256400503e-05, 4.108490975340828e-05, 4.070093928021379e-05, 4.032407741760835e-05, 3.9954131352715194e-05, 3.959091191063635e-05, 3.9234237192431465e-05, 3.88839325751178e-05, 3.853982707369141e-05, 3.8201756979105994e-05, 3.786956949625164e-05, 3.7543104554060847e-05, 3.7222223909338936e-05, 3.690678204293363e-05, 3.659664071165025e-05, 3.6291668948251754e-05, 3.5991739423479885e-05, 3.5696724808076397e-05, 3.5406505048740655e-05, 3.512097100610845e-05, 3.484000262687914e-05, 3.456349440966733e-05, 3.4291340853087604e-05, 3.402344009373337e-05, 3.3759693906176835e-05, 3.350000406499021e-05, 3.32442759827245e-05, 3.299242598586716e-05, 3.2744363124947995e-05, 3.250000372645445e-05, 3.225926047889516e-05, 3.202206062269397e-05, 3.1788324122317135e-05, 3.15579745802097e-05, 3.133093923679553e-05, 3.1107145332498476e-05, 3.088652738370001e-05, 3.066901626880281e-05, 3.0454548323177733e-05, 3.0243058063206263e-05, 3.003448546223808e-05, 2.9828768674633466e-05, 2.962585313071031e-05, 2.9425676984828897e-05, 2.9228189305285923e-05, 2.9033335522399284e-05, 2.8841061066486873e-05, 2.8651318643824197e-05, 2.846405368472915e-05, 2.827922253345605e-05, 2.8096776077290997e-05], "accuracy_train_first": 0.3615644250069214, "accuracy_train_last": 0.982027320678756, "batch_size_eval": 1024, "accuracy_train_std": [0.01674678821605219, 0.015831973207652273, 0.019451010608230766, 0.018418655539488623, 0.019708571081267622, 0.021479251798813603, 0.018649879529104726, 0.020313321838632936, 0.018172656681096314, 0.017698729892834254, 0.017346995321465605, 0.01773712803631681, 0.016765933494856486, 0.01613765178064486, 0.015651053546334583, 0.015323355630688427, 0.015472096394439108, 0.016267949222086096, 0.01529924600400801, 0.015509356439992242, 0.014208875617271846, 0.014478272190522718, 0.015268998892081552, 0.013586075076210928, 0.013530372488956922, 0.013220652145831008, 0.013513197771131358, 0.013688525412306621, 0.014333992937411695, 0.014027685615203638, 0.012239240523566432, 0.014065324859285569, 0.013169401655262645, 0.01323453507174249, 0.0125114816230238, 0.012969417459915917, 0.011950183798355038, 0.013436888718743155, 0.012387403760292069, 0.012226252808207265, 0.012225312140111305, 0.012189023225833776, 0.011678458419090267, 0.01091821983882332, 0.01131611113740707, 0.01070999002705213, 0.011684140454971431, 0.01106415946047127, 0.011285706089888636, 0.011224784982649227, 0.010328552952837721, 0.01055703631140762, 0.010439627337861968, 0.010094553870814826, 0.00995310461292999, 0.010621353748679906, 0.009576860034752704, 0.009598870043500933, 0.009787575513817306, 0.009469183752219138, 0.010340267019419931, 0.010415370939747865, 0.009463588218726863, 0.010081557144274077, 0.009642659560104412, 0.009159353994180773, 0.00929582632863381, 0.009757174497723859, 0.009515946369584923, 0.009696663305240308, 0.008600184423632065, 0.009148899682505523, 0.009149594107988587, 0.009566313341819063, 0.008754049193985468, 0.009605535019660619, 0.008856628167012225, 0.008909268850109018, 0.009058686110120806, 0.00882540425011708, 0.009236546910488228, 0.008251156181942061, 0.00864197861470719, 0.008772183954030007, 0.008768514754822656, 0.008361505731892791, 0.00881171285717086, 0.008354183479100115, 0.008836158665991174, 0.008479865070170553, 0.008808601556958017, 0.008321949370526764, 0.008728975295625961, 0.007823114407837985, 0.0078066187553304505, 0.008184788026714638, 0.008447616000076283, 0.00756019465780768, 0.008860983176675148, 0.007278866563036744, 0.00774053906374108, 0.007737157870132371, 0.007796724807756182, 0.007522869547905281, 0.007907317993046614, 0.007670736050308955, 0.007873985217180631, 0.007899873510411319, 0.00802172969497753, 0.007590106310678261, 0.0074076820893762855, 0.007369548056060137, 0.007314875287938605, 0.006972384310526572, 0.007352067590958728, 0.007675280649112081, 0.007354901341301339, 0.0064431864541803, 0.006914923758544272, 0.007143055136172377, 0.007435682764626533, 0.006772372611284575, 0.007228766942602405, 0.006562726136175497, 0.006874748484455576, 0.007249283017415771, 0.00677943379617629, 0.0069296066342303874, 0.006401933180097017, 0.0062027691514436765, 0.006185463982698984, 0.006299425510802824, 0.005625434845059818, 0.006366679568720399, 0.006162456502655597, 0.006115453968504411, 0.006066499948217659, 0.005940956267506068, 0.00582754843668919, 0.006228318093412627, 0.006346463753981588, 0.005404640896702192, 0.006055930309138842, 0.005827556445011158, 0.005865353652364557, 0.0055993904800593726, 0.00535834566368505, 0.006125388193210514, 0.0060019472960015485, 0.005080747118596709, 0.006073580662328368, 0.005453508062186695, 0.005333442957765, 0.006243225276248724, 0.005771549496060985], "accuracy_test_std": 0.014861872742918928, "error_valid": [0.6319830101656627, 0.5495414274284638, 0.36847438582454817, 0.3206213525978916, 0.31119105327560237, 0.261608445500753, 0.24701001270707834, 0.2230930558170181, 0.21243175828313254, 0.20362210560993976, 0.1987495881965362, 0.18706025272966864, 0.1810891025037651, 0.17790497929216864, 0.18349962349397586, 0.1745679005082832, 0.1696644978350903, 0.1691762165850903, 0.16074306993599397, 0.16217702842620485, 0.1628285603350903, 0.16210643354668675, 0.1559411474021084, 0.15756924181099397, 0.15389683734939763, 0.1506612387048193, 0.15155691123870485, 0.15201430722891573, 0.14885077419051207, 0.14994940700301207, 0.15021413780120485, 0.14796539674322284, 0.1471211996423193, 0.14432387754141573, 0.14296051393072284, 0.14484304405120485, 0.14457831325301207, 0.14491363893072284, 0.14271637330572284, 0.1450768895896084, 0.1425840079066265, 0.1421163168298193, 0.14442535768072284, 0.1419736563441265, 0.1427060782191265, 0.13988816594503017, 0.1430928793298193, 0.14061029273343373, 0.14210602174322284, 0.1413633047816265, 0.13915574407003017, 0.14085443335843373, 0.1433370199548193, 0.1387998282191265, 0.14232957219503017, 0.13854539250753017, 0.13631724162274095, 0.13768060523343373, 0.13905426393072284, 0.13840273202183728, 0.13583925545933728, 0.13987787085843373, 0.13815859139683728, 0.1377717902861446, 0.13607310099774095, 0.13671433782003017, 0.13839243693524095, 0.13980727597891573, 0.13580837019954817, 0.13570689006024095, 0.1389218985316265, 0.13644960702183728, 0.13411997599774095, 0.13668345256024095, 0.13621576148343373, 0.1384336172816265, 0.1334993293486446, 0.13669374764683728, 0.13486269295933728, 0.13483180769954817, 0.1349641730986446, 0.13497446818524095, 0.13426263648343373, 0.13779238045933728, 0.13522890389683728, 0.13500535344503017, 0.13768060523343373, 0.13607310099774095, 0.13149472891566272, 0.13498476327183728, 0.13558481974774095, 0.13425234139683728, 0.13708054875753017, 0.13303163827183728, 0.13842332219503017, 0.13399790568524095, 0.13534067912274095, 0.13458766707454817, 0.13498476327183728, 0.13535097420933728, 0.13448618693524095, 0.13411997599774095, 0.13523919898343373, 0.1336213996611446, 0.13068141707454817, 0.13323459855045183, 0.1334684440888554, 0.13470973738704817, 0.1333463737763554, 0.1320344855986446, 0.13495387801204817, 0.13250217667545183, 0.13621576148343373, 0.13382435993975905, 0.1316373894013554, 0.13915574407003017, 0.13592014542545183, 0.13396702042545183, 0.13300075301204817, 0.13335666886295183, 0.13628635636295183, 0.1358186652861446, 0.1322477409638554, 0.13704966349774095, 0.13643931193524095, 0.1338655402861446, 0.13519801863704817, 0.1322786262236446, 0.13487298804593373, 0.13385524519954817, 0.13422145613704817, 0.13388613045933728, 0.13373317488704817, 0.13536126929593373, 0.13559511483433728, 0.13535097420933728, 0.1331331184111446, 0.13584955054593373, 0.1316373894013554, 0.13388613045933728, 0.13239040144954817, 0.13101674275225905, 0.1314241340361446, 0.13446559676204817, 0.13605251082454817, 0.13286838761295183, 0.1336213996611446, 0.13284779743975905, 0.13497446818524095, 0.13211537556475905, 0.1361142813441265, 0.1328889777861446, 0.13422145613704817, 0.1336213996611446, 0.13521860881024095], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.06783005898244854, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 64, "valid_ratio": 0.15, "learning_rate": 0.004355000310606565, "optimization": "rmsprop", "nb_data_augmentation": 4, "learning_rate_decay_method": "lin", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 2.8724853657062766e-06, "rotation_range": [0, 0], "momentum": 0.8349347381959233}, "accuracy_valid_max": 0.8693185829254518, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.864781391189759, "accuracy_valid_std": [0.017925598769628636, 0.01951203954046912, 0.00851460034545994, 0.01851830590487067, 0.016490734587940988, 0.018361922767141457, 0.018747960840952847, 0.017025595692555878, 0.013215704558505893, 0.016201655953948547, 0.016108680410936462, 0.0115677930331491, 0.014851195257386512, 0.012783468112686429, 0.012140205636137765, 0.015317781228164464, 0.011390523305282762, 0.011841516471303538, 0.010503847719618379, 0.011431772401959243, 0.011096204360652808, 0.0117453181076035, 0.00897189309981877, 0.011157354569384374, 0.011375072699677056, 0.010050962757540565, 0.008563720801691878, 0.010510999328497632, 0.009604906612486825, 0.009116239121583176, 0.009451409018941564, 0.01070819171562584, 0.0078077684034693765, 0.006983887187784768, 0.009102983960122008, 0.009414104671078933, 0.008452930289309098, 0.007662952773112585, 0.008722229467549992, 0.008526205048455965, 0.008373666139750368, 0.00764068812205764, 0.008036037531796929, 0.0068370713020530875, 0.004354441638720422, 0.007543705411703603, 0.0075903329312505124, 0.008432538227492306, 0.006636613419773258, 0.008868828072969326, 0.007387587362466398, 0.006551479623860734, 0.00805801458214699, 0.008933300614215343, 0.00818184670525799, 0.006753922060454466, 0.006748574580160191, 0.008935491760101407, 0.008472604931927306, 0.006665201938018691, 0.0071509694426605255, 0.007107921965180832, 0.006650856693688922, 0.009297023340145892, 0.0071421041091062855, 0.008065067927242325, 0.007940378442988673, 0.009336984599845473, 0.008439033015953945, 0.007917146583553464, 0.008597468519039182, 0.00888245154235322, 0.010768474477658416, 0.007196033330858729, 0.008180789816902607, 0.009887571383915692, 0.01020976866677764, 0.00889989757280303, 0.010734157117685772, 0.009608713560636164, 0.009414739606753245, 0.008029772648057189, 0.007618817778471611, 0.007435532614186609, 0.009542319250576988, 0.00921942492661729, 0.008483819887555876, 0.010041869055984521, 0.011873830574835148, 0.009336452569861669, 0.008247551718847232, 0.009247886063289027, 0.008840060686304775, 0.009828958455060103, 0.008564195641627328, 0.00796584171014286, 0.011045663513051467, 0.00851535937887603, 0.007683485814088043, 0.0074673361985272005, 0.009033765345567992, 0.008574469983381048, 0.009650007284945283, 0.009147240218136319, 0.009866463031907212, 0.009521850005074934, 0.01081859450869222, 0.009896033024585055, 0.01199045037319091, 0.009712396338257283, 0.009613299461323273, 0.011876607862942602, 0.008195348717866676, 0.012488177596759471, 0.011149744654079164, 0.00785678079184599, 0.010060491800088042, 0.008630788307521809, 0.01030843885910847, 0.011721643474679213, 0.012048887540134293, 0.01110006427647533, 0.01155872739449766, 0.009695046965106643, 0.009751832513560281, 0.007767190279122622, 0.008400470866208728, 0.009108700096888323, 0.009898714481493574, 0.009092935589359703, 0.00861820471708581, 0.008316159602817983, 0.008779515803721502, 0.011086642975547198, 0.00847741541812393, 0.01104723833779597, 0.009151321704886263, 0.01033609021549738, 0.01116043119374285, 0.00776769097691096, 0.008120816039266988, 0.01025843629828212, 0.010295737057226785, 0.011350726469640353, 0.010623022600517002, 0.008997016185794194, 0.009801518745662421, 0.011106762564461367, 0.00981964044041821, 0.011671627072694342, 0.00963479476003453, 0.007220385394118923, 0.0077439226364618575, 0.0092637881463365, 0.009811409660328016], "accuracy_valid": [0.36801698983433734, 0.45045857257153615, 0.6315256141754518, 0.6793786474021084, 0.6888089467243976, 0.738391554499247, 0.7529899872929217, 0.7769069441829819, 0.7875682417168675, 0.7963778943900602, 0.8012504118034638, 0.8129397472703314, 0.8189108974962349, 0.8220950207078314, 0.8165003765060241, 0.8254320994917168, 0.8303355021649097, 0.8308237834149097, 0.839256930064006, 0.8378229715737951, 0.8371714396649097, 0.8378935664533133, 0.8440588525978916, 0.842430758189006, 0.8461031626506024, 0.8493387612951807, 0.8484430887612951, 0.8479856927710843, 0.8511492258094879, 0.8500505929969879, 0.8497858621987951, 0.8520346032567772, 0.8528788003576807, 0.8556761224585843, 0.8570394860692772, 0.8551569559487951, 0.8554216867469879, 0.8550863610692772, 0.8572836266942772, 0.8549231104103916, 0.8574159920933735, 0.8578836831701807, 0.8555746423192772, 0.8580263436558735, 0.8572939217808735, 0.8601118340549698, 0.8569071206701807, 0.8593897072665663, 0.8578939782567772, 0.8586366952183735, 0.8608442559299698, 0.8591455666415663, 0.8566629800451807, 0.8612001717808735, 0.8576704278049698, 0.8614546074924698, 0.863682758377259, 0.8623193947665663, 0.8609457360692772, 0.8615972679781627, 0.8641607445406627, 0.8601221291415663, 0.8618414086031627, 0.8622282097138554, 0.863926899002259, 0.8632856621799698, 0.861607563064759, 0.8601927240210843, 0.8641916298004518, 0.864293109939759, 0.8610781014683735, 0.8635503929781627, 0.865880024002259, 0.863316547439759, 0.8637842385165663, 0.8615663827183735, 0.8665006706513554, 0.8633062523531627, 0.8651373070406627, 0.8651681923004518, 0.8650358269013554, 0.865025531814759, 0.8657373635165663, 0.8622076195406627, 0.8647710961031627, 0.8649946465549698, 0.8623193947665663, 0.863926899002259, 0.8685052710843373, 0.8650152367281627, 0.864415180252259, 0.8657476586031627, 0.8629194512424698, 0.8669683617281627, 0.8615766778049698, 0.866002094314759, 0.864659320877259, 0.8654123329254518, 0.8650152367281627, 0.8646490257906627, 0.865513813064759, 0.865880024002259, 0.8647608010165663, 0.8663786003388554, 0.8693185829254518, 0.8667654014495482, 0.8665315559111446, 0.8652902626129518, 0.8666536262236446, 0.8679655144013554, 0.8650461219879518, 0.8674978233245482, 0.8637842385165663, 0.866175640060241, 0.8683626105986446, 0.8608442559299698, 0.8640798545745482, 0.8660329795745482, 0.8669992469879518, 0.8666433311370482, 0.8637136436370482, 0.8641813347138554, 0.8677522590361446, 0.862950336502259, 0.863560688064759, 0.8661344597138554, 0.8648019813629518, 0.8677213737763554, 0.8651270119540663, 0.8661447548004518, 0.8657785438629518, 0.8661138695406627, 0.8662668251129518, 0.8646387307040663, 0.8644048851656627, 0.8646490257906627, 0.8668668815888554, 0.8641504494540663, 0.8683626105986446, 0.8661138695406627, 0.8676095985504518, 0.868983257247741, 0.8685758659638554, 0.8655344032379518, 0.8639474891754518, 0.8671316123870482, 0.8663786003388554, 0.867152202560241, 0.865025531814759, 0.867884624435241, 0.8638857186558735, 0.8671110222138554, 0.8657785438629518, 0.8663786003388554, 0.864781391189759], "seed": 692234056, "model": "residualv3", "loss_std": [0.26149794459342957, 0.15482118725776672, 0.1326473504304886, 0.12509556114673615, 0.12300523370504379, 0.1204143613576889, 0.12011317908763885, 0.11930874735116959, 0.11526677757501602, 0.11327875405550003, 0.11236110329627991, 0.10872209817171097, 0.1077486127614975, 0.10594567656517029, 0.10640337318181992, 0.10371366143226624, 0.10434533655643463, 0.10121765732765198, 0.10037112981081009, 0.09968762844800949, 0.09844791144132614, 0.09708213061094284, 0.09721115976572037, 0.09512752294540405, 0.0944199413061142, 0.09529447555541992, 0.0922091007232666, 0.09131092578172684, 0.09072341024875641, 0.08986262232065201, 0.08786516636610031, 0.08793458342552185, 0.08517005294561386, 0.08511610329151154, 0.08563412725925446, 0.08465629816055298, 0.08338149636983871, 0.08402527868747711, 0.08222895115613937, 0.08081259578466415, 0.08085549622774124, 0.07955225557088852, 0.07813910394906998, 0.07832534611225128, 0.0762479230761528, 0.07742712646722794, 0.0770372748374939, 0.07656993716955185, 0.07647975534200668, 0.074846550822258, 0.07564545422792435, 0.07384057343006134, 0.0731254518032074, 0.07215770334005356, 0.07182023674249649, 0.07112621515989304, 0.07142972201108932, 0.07227832078933716, 0.0713622197508812, 0.07114550471305847, 0.06933137774467468, 0.06879527121782303, 0.07021599262952805, 0.06920664012432098, 0.06831952929496765, 0.0678674727678299, 0.06710237264633179, 0.06666087359189987, 0.0664716437458992, 0.06574534624814987, 0.0648738294839859, 0.06535482406616211, 0.06494900584220886, 0.06377658247947693, 0.06475532799959183, 0.06405683606863022, 0.06343536078929901, 0.06327136605978012, 0.06301865726709366, 0.061332836747169495, 0.062433239072561264, 0.06223726645112038, 0.060661591589450836, 0.060760170221328735, 0.062310006469488144, 0.059459615498781204, 0.060146234929561615, 0.05963774770498276, 0.058116454631090164, 0.057398103177547455, 0.05842408537864685, 0.057880934327840805, 0.056659799069166183, 0.0579344667494297, 0.05627588555216789, 0.05635308474302292, 0.05685757100582123, 0.05579223856329918, 0.057534437626600266, 0.055898167192935944, 0.05581599101424217, 0.055863212794065475, 0.05493352189660072, 0.05319046229124069, 0.05564742535352707, 0.05366751551628113, 0.05480349436402321, 0.05463666841387749, 0.05287671461701393, 0.05396510288119316, 0.0523226223886013, 0.05265909433364868, 0.05308708921074867, 0.05336170643568039, 0.05260023474693298, 0.0518081858754158, 0.05128520354628563, 0.05177358165383339, 0.05071573704481125, 0.05152028053998947, 0.05053286254405975, 0.04902235418558121, 0.05058375000953674, 0.04879877343773842, 0.0497402586042881, 0.04977923259139061, 0.050924357026815414, 0.05015398561954498, 0.049233578145504, 0.0497683510184288, 0.04899361729621887, 0.04903384670615196, 0.04831460118293762, 0.04856540262699127, 0.04882266744971275, 0.04823929816484451, 0.046603549271821976, 0.048967380076646805, 0.049390919506549835, 0.04706097021698952, 0.0467584989964962, 0.047746531665325165, 0.04670512303709984, 0.0459713451564312, 0.0463627390563488, 0.04553861916065216, 0.04622753709554672, 0.04788888618350029, 0.047247275710105896, 0.045823972672224045, 0.04620938375592232, 0.04528915509581566, 0.045306239277124405, 0.045197561383247375, 0.04467674717307091]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:24 2016", "state": "available"}], "summary": "1415e1d73af1d4188b2d30aa18ac33a0"}