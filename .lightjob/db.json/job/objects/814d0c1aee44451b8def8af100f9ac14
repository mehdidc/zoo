{"content": {"hp_model": {"f0": 32, "f1": 32, "f2": 32, "f3": 32, "nonlin": "very_leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.08572272107633695, 0.08607775517910307, 0.0731989902816592, 0.06558681085866981, 0.06771136243835366, 0.06087634587571339, 0.06164608607238919, 0.06054261981155089, 0.057160615809382205, 0.05919989915672933, 0.05979089136595394, 0.05761698375968124, 0.05609117443229915, 0.055156046272612946, 0.05603964108435428, 0.059625561321213764, 0.056803337431315507, 0.05669208147937114, 0.057424592522977304, 0.056153936607987606, 0.05694304130661738, 0.05704771193293543, 0.05677318787126063, 0.053657486263340666, 0.058364779336036746, 0.05800615507662374, 0.05472951285047242, 0.05457287023327475, 0.05663794602616217, 0.05434296735912827, 0.05493004541738049, 0.053166471342670905, 0.05343299090443628, 0.05388021875596045, 0.056501591349642576, 0.054523990676158145, 0.0561367832868679, 0.05178043083783177, 0.05328525641025641, 0.05465467410978923, 0.054167430539633125, 0.05404497453298994, 0.05530716785819896, 0.05355151783354306, 0.055223423956530654, 0.05503514188423797, 0.05411604265443402, 0.0551865945523601, 0.05429388082603063, 0.05439168169888077, 0.05355101827104508, 0.05697263142164923, 0.057569759444506596, 0.05432130251875737, 0.0551023437402378, 0.05544066980646738, 0.05195871157921963, 0.05697873540579219, 0.05835209660547286, 0.05687534894959899, 0.0566549476113038, 0.0561367832868679, 0.05603820892598033, 0.05529894430743604, 0.05592925758530495, 0.05723918885403329, 0.056136942137956385, 0.05486962130957814, 0.05736306515735719, 0.05687534894959899, 0.05485856884771116, 0.05622820719717177, 0.05445918634235189, 0.0563880005083649, 0.05891224626098498, 0.057326521483747984, 0.05830669122508459, 0.05694679963378922, 0.0569643352153084, 0.05680380839121939, 0.05850044963287479, 0.05697873540579219, 0.05688804740208167, 0.05608672280225362, 0.057574715945544155, 0.05551525211516169, 0.0575338120420353, 0.057178085817023314, 0.05742909572447789, 0.057242927737420055, 0.056965587550291796, 0.05790414106139751, 0.058789207335016455, 0.054227321524643046, 0.05668500274593382, 0.05749163818798749, 0.056531412532343166, 0.05593627253756756, 0.05496558661126249, 0.056820760347988004, 0.05606255064270689, 0.05525683991874252, 0.056788264652132776, 0.05709646132180734, 0.05697701383544828, 0.0578653193335894, 0.05809601868300492, 0.05656736637053301, 0.056615269303957494, 0.05667178674449225, 0.05768674296524461, 0.05884636454358062, 0.05770482636614328, 0.05595731211788107, 0.0576267334679258, 0.05674537955505624, 0.05744772590930182, 0.057111921181293976, 0.057574715945544155, 0.057064591434271256, 0.05663164784749588, 0.056505379038865045, 0.05765891127405776, 0.05705380788448187, 0.056681856358421136, 0.05641013626840677, 0.057863470030204485, 0.056094830864126165, 0.0569756052392025, 0.05774931522842846, 0.05694319790853535, 0.056085291846128524, 0.0562339162564176, 0.05679642957072433, 0.0556812557242025, 0.0571606158093822, 0.05722750326902092, 0.05663794602616218, 0.05543230519118041, 0.0565938440564408, 0.05547395490599672, 0.056751036587652764, 0.057781270440128034, 0.056050460653777806, 0.05724105832625382, 0.05759283452148056, 0.05649543581304443, 0.05727345281573216, 0.05837348756707361, 0.0570388012933665, 0.059115937568662145, 0.05719196442866478, 0.056778842133237986, 0.05788565777149983, 0.05580747171283705, 0.05515362108125124, 0.0568772303807117, 0.05724121411284952, 0.056758578420717264, 0.05784790271724374, 0.05745145122130572, 0.05678512398622403, 0.055943924209556864, 0.058102004648310016, 0.05845409161886785, 0.05764715608114839, 0.053961585378008915, 0.05422732152464305, 0.056254844516669955, 0.057060059465961294, 0.05812594234502326, 0.055229236880554755, 0.05642910286580183, 0.05637645484310321, 0.057125035356765004, 0.05671661424845798, 0.05834781747209288, 0.05634243674110757, 0.056072570631156754, 0.05511723040564152, 0.055905176850772245, 0.055260712925996006, 0.05541605492684851, 0.055830955706421596, 0.056488963884662076, 0.05550946914251889, 0.055507541351084495, 0.05666753808944701, 0.054841010359419406, 0.054663647117982495, 0.056279726329673395, 0.056950557712938533, 0.05517932268639627, 0.054663647117982495, 0.05522536166585626, 0.05710505064934883, 0.056685002745933806, 0.05368739234428439, 0.0538782326691239, 0.05726784738847961, 0.053687392344284386, 0.05517221149071157, 0.054317526700449495], "moving_avg_accuracy_train": [0.04135683358433734, 0.10196371423192768, 0.1684074971762048, 0.23170134836219874, 0.2909295568994728, 0.3465033933782002, 0.3979505691006211, 0.44546961234116145, 0.4891544846913827, 0.5288709074571841, 0.5651686811692971, 0.5983402543174277, 0.6288912062953235, 0.6566717957561525, 0.682057891782947, 0.705378363899833, 0.7264397368471991, 0.746173869337178, 0.7640851909877976, 0.7804595220396202, 0.7952952528175858, 0.8089674406382369, 0.8213359450683891, 0.8329664695374539, 0.8433351087282868, 0.8528927876144942, 0.8619088552385868, 0.8702515728773788, 0.877555293601689, 0.8845380925547732, 0.8907473104077297, 0.8965803353910532, 0.9021524411591768, 0.9071979274649459, 0.9119271181521862, 0.9162892820899796, 0.9203423004171263, 0.9240629649537269, 0.9275315743318482, 0.9307803935552899, 0.9338102231756645, 0.936464121791833, 0.9390338240704811, 0.9413465561212643, 0.9433597732501017, 0.945451695021477, 0.9472591234108956, 0.9491893669433, 0.9509760025381266, 0.952558089784314, 0.9541655149926296, 0.9553957067162582, 0.9566558348398131, 0.9578676045184824, 0.9590264389461522, 0.960194107551537, 0.9611650017662628, 0.9619282079149377, 0.96282923124995, 0.9635530852333888, 0.964409278969086, 0.9650598420360328, 0.9656641740974897, 0.9664480955431625, 0.9670500856876414, 0.9676295274200821, 0.9683016273889173, 0.9688665135958088, 0.9694549187121315, 0.9700244870818822, 0.970393555692971, 0.9709304425935534, 0.9711759713763668, 0.9714181257447542, 0.9717066595558209, 0.972095763931564, 0.9725824413034678, 0.9729239712695066, 0.9734172480883391, 0.9737929555084208, 0.9740769694455306, 0.9743608199407365, 0.9745786347840123, 0.9749629211550086, 0.9752970130756523, 0.9755812236656775, 0.9758299537087483, 0.9761432309282349, 0.9764087082872186, 0.9766382252597016, 0.9767459577036108, 0.9769605750356594, 0.97702430668872, 0.9772134422849083, 0.9773860174841283, 0.9777037033863178, 0.9778507841019028, 0.9779784504206283, 0.9782557183303726, 0.9783758355033595, 0.9785874801156742, 0.9787685476161551, 0.978764433818395, 0.9786924896835435, 0.9787195133055505, 0.978837961071381, 0.9789892741509899, 0.9790878053202282, 0.9793106136436271, 0.9792899438455295, 0.9794666535272416, 0.9795409783853608, 0.9796478745227284, 0.9798687986668412, 0.980053511420639, 0.9803421173568884, 0.9805783310730067, 0.9806073767307663, 0.9807511759552799, 0.9809770749260169, 0.9812109751141381, 0.9812308791087484, 0.9812817369810061, 0.9814828178009778, 0.9814967159907596, 0.9814856927350571, 0.9815746046362502, 0.981553439353348, 0.9817014651469289, 0.9817476213430794, 0.9818879947509401, 0.9819060853360871, 0.982007080718141, 0.9820556196342787, 0.9822310817672363, 0.9824266482893078, 0.9825720670447144, 0.9826323490450622, 0.98278308251405, 0.9829210957987896, 0.9830406014297539, 0.9830987400819593, 0.983104001615932, 0.9831981571772304, 0.9831652390498689, 0.9833497505364481, 0.983501691898466, 0.9835890227086194, 0.9836417356486007, 0.9837221215716925, 0.9837709372759691, 0.9837889866206614, 0.9838922980489566, 0.9839217429428562, 0.9839623623232694, 0.9841518753379304, 0.9843224370511253, 0.9844171135267357, 0.9844129021740621, 0.984562067528945, 0.9846021898423156, 0.9846641847135058, 0.9847129206096251, 0.9847261918016746, 0.9846910726215071, 0.984720647588272, 0.9847707966848666, 0.9847053322272232, 0.9848534925285973, 0.9849727178239304, 0.985032957336718, 0.9850918792235281, 0.9851590278975609, 0.9852265211921422, 0.9852284360910002, 0.9852701632650327, 0.9852724202819029, 0.9853003363862427, 0.9853160482295462, 0.9854125495812903, 0.9855041071231613, 0.9855959215614476, 0.9857256178089173, 0.9857435116003148, 0.9856607831812472, 0.9856192718811947, 0.9856807445424729, 0.9858231369556955, 0.9859630559408488, 0.9860278007985711, 0.9861566660500393, 0.9861785182703365, 0.9862899586119775, 0.9863996675700568, 0.9864725208431716, 0.986549854602228, 0.9866453397745353, 0.9867336295922624, 0.9867895588017108, 0.9867151774697325, 0.9867235354757713, 0.9866745817775918, 0.9867646537203146], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 1234423, "moving_var_accuracy_train": [0.01539348915710316, 0.04691288607787421, 0.08195458409760267, 0.10981413006943516, 0.1304045432413051, 0.1451601506258536, 0.15446544257159117, 0.15934143354889913, 0.1605826028443053, 0.15872089069568165, 0.1547065570142153, 0.14913908069888973, 0.14262541862980202, 0.13530872712394199, 0.12757793925488242, 0.1197147451071843, 0.11173550347031828, 0.10406687698947482, 0.09654752827997491, 0.08930584390852957, 0.08235614968712313, 0.07580289319663924, 0.06959942299352358, 0.06385690258900083, 0.05843879043812783, 0.05341705442554175, 0.0488069542616075, 0.04455266727385224, 0.040577499574235495, 0.03695858494778468, 0.03360971593011547, 0.030554961961208597, 0.02777890102930814, 0.025230123314932653, 0.022908398184445917, 0.020788814633982994, 0.01885777578862637, 0.017096588311108992, 0.015495210739160013, 0.01404068310236345, 0.0127192335998836, 0.011510698840679349, 0.010419059284819414, 0.009425291922185951, 0.008519240118837958, 0.007706701337232151, 0.006965432379954822, 0.006302421702808841, 0.0057009081332662715, 0.005153344320430582, 0.00466126423059048, 0.004208758152623391, 0.003802173643351018, 0.0034351717508031973, 0.003103740650799653, 0.002805637635467699, 0.0025335575921066216, 0.0022854441855243365, 0.0020642063544240325, 0.0018625014002856904, 0.0016828488696745464, 0.001518373073443769, 0.0013698227212639354, 0.0012383712446344124, 0.0011177956493774192, 0.0010090378589313219, 0.0009121995383511636, 0.0008238514523566726, 0.0007445822923492383, 0.0006730437362646989, 0.0006069652673954488, 0.0005488629685520568, 0.0004945192311455593, 0.0004455950566741654, 0.00040178481684790716, 0.0003629689551001191, 0.00032880375336901617, 0.0002969731624914365, 0.00026946574442227023, 0.00024378957456958357, 0.00022013659236087832, 0.00019884807305744866, 0.00017939025550526492, 0.00016278031408914042, 0.00014750683938318106, 0.0001334831363802049, 0.00012069162245111848, 0.00010950574375225004, 9.918947343022191e-05, 8.974462845311939e-05, 8.087462212304362e-05, 7.320170530368e-05, 6.59180902857285e-05, 5.9648231720864965e-05, 5.3951448343250904e-05, 4.94646225009759e-05, 4.471285488295149e-05, 4.038825759508834e-05, 3.704132927954581e-05, 3.3467049568808364e-05, 3.05234855892239e-05, 2.7766205987874674e-05, 2.4989737699075306e-05, 2.253734755602361e-05, 2.0290185285738694e-05, 1.8387435616236932e-05, 1.6754752887159754e-05, 1.5166653120247086e-05, 1.4096779749004706e-05, 1.2690946939084813e-05, 1.1702889049673511e-05, 1.058231780551613e-05, 9.626927082621391e-06, 9.103501671426593e-06, 8.500220717024066e-06, 8.39983912326695e-06, 8.062027488082413e-06, 7.263417591386344e-06, 6.723179784984497e-06, 6.510134911306676e-06, 6.351505102204179e-06, 5.719920112996777e-06, 5.171206810232318e-06, 5.017987594653416e-06, 4.517927272300956e-06, 4.067228154567399e-06, 3.731653274674567e-06, 3.3625196700100564e-06, 3.22347242309622e-06, 2.9202987307743356e-06, 2.8056111004068336e-06, 2.527995413804779e-06, 2.3669964771902925e-06, 2.151501066889677e-06, 2.21343360111922e-06, 2.336306622003779e-06, 2.2929954896193483e-06, 2.096401216750841e-06, 2.09124630313349e-06, 2.0535506737016116e-06, 1.9767299688212827e-06, 1.8094778978614227e-06, 1.6287792617329978e-06, 1.5456887630703828e-06, 1.4008723147442935e-06, 1.5671854813874608e-06, 1.6182425306752254e-06, 1.5250583112261458e-06, 1.3975603664769157e-06, 1.315961399511023e-06, 1.2058120164160477e-06, 1.0881628243688192e-06, 1.075405802879689e-06, 9.75668238582506e-07, 8.929508213106064e-07, 1.1268923837127039e-06, 1.2760248274132908e-06, 1.2290950599779472e-06, 1.1063451734022229e-06, 1.1959633839382117e-06, 1.090855245816239e-06, 1.0163599977195388e-06, 9.361006860825458e-07, 8.440757383200322e-07, 7.707683758287405e-07, 7.015636461781954e-07, 6.540416685635637e-07, 6.272078586379669e-07, 7.620503469034555e-07, 8.137773516383575e-07, 7.650588065826024e-07, 7.197990246317307e-07, 6.883996219877307e-07, 6.605577631098492e-07, 5.94534988337594e-07, 5.507519029784918e-07, 4.957225598070149e-07, 4.5316408375992456e-07, 4.100694335638592e-07, 4.528750882035505e-07, 4.83032630644283e-07, 5.10598387280288e-07, 6.109285980217551e-07, 5.527174281547503e-07, 5.590416072321285e-07, 5.186461387973016e-07, 5.007915176790928e-07, 6.331927600013682e-07, 7.460693856583068e-07, 7.091895165058678e-07, 7.87726842178926e-07, 7.132518337483216e-07, 7.536971980791482e-07, 7.866519776169671e-07, 7.557551744871326e-07, 7.340042496464583e-07, 7.426605878568414e-07, 7.385503562998735e-07, 6.928480088956536e-07, 6.733564509278974e-07, 6.06649512219609e-07, 5.67552742086736e-07, 5.838140616707048e-07], "duration": 224528.247509, "accuracy_train": [0.4135683358433735, 0.647425640060241, 0.7664015436746988, 0.8013460090361446, 0.8239834337349398, 0.846667921686747, 0.8609751506024096, 0.8731410015060241, 0.8823183358433735, 0.8863187123493976, 0.8918486445783133, 0.8968844126506024, 0.9038497740963856, 0.9066971009036144, 0.9105327560240963, 0.9152626129518072, 0.915992093373494, 0.9237810617469879, 0.9252870858433735, 0.9278285015060241, 0.9288168298192772, 0.9320171310240963, 0.932652484939759, 0.9376411897590361, 0.9366528614457831, 0.9389118975903614, 0.9430534638554217, 0.945336031626506, 0.9432887801204819, 0.9473832831325302, 0.9466302710843374, 0.9490775602409639, 0.9523013930722891, 0.9526073042168675, 0.9544898343373494, 0.9555487575301205, 0.9568194653614458, 0.9575489457831325, 0.9587490587349398, 0.9600197665662651, 0.9610786897590361, 0.9603492093373494, 0.9621611445783133, 0.9621611445783133, 0.9614787274096386, 0.9642789909638554, 0.9635259789156626, 0.9665615587349398, 0.9670557228915663, 0.966796875, 0.9686323418674698, 0.9664674322289156, 0.9679969879518072, 0.968773531626506, 0.9694559487951807, 0.970703125, 0.9699030496987951, 0.9687970632530121, 0.9709384412650602, 0.9700677710843374, 0.9721150225903614, 0.9709149096385542, 0.9711031626506024, 0.9735033885542169, 0.9724679969879518, 0.9728445030120482, 0.9743505271084337, 0.9739504894578314, 0.9747505647590361, 0.9751506024096386, 0.9737151731927711, 0.9757624246987951, 0.9733857304216867, 0.973597515060241, 0.9743034638554217, 0.975597703313253, 0.9769625376506024, 0.9759977409638554, 0.9778567394578314, 0.9771743222891566, 0.9766330948795181, 0.9769154743975904, 0.976538968373494, 0.9784214984939759, 0.9783038403614458, 0.9781391189759037, 0.9780685240963856, 0.9789627259036144, 0.9787980045180723, 0.9787038780120482, 0.9777155496987951, 0.9788921310240963, 0.9775978915662651, 0.9789156626506024, 0.9789391942771084, 0.9805628765060241, 0.9791745105421686, 0.9791274472891566, 0.9807511295180723, 0.979456890060241, 0.980492281626506, 0.9803981551204819, 0.9787274096385542, 0.9780449924698795, 0.9789627259036144, 0.9799039909638554, 0.9803510918674698, 0.9799745858433735, 0.9813158885542169, 0.9791039156626506, 0.9810570406626506, 0.9802099021084337, 0.9806099397590361, 0.9818571159638554, 0.9817159262048193, 0.9829395707831325, 0.9827042545180723, 0.9808687876506024, 0.9820453689759037, 0.9830101656626506, 0.9833160768072289, 0.981410015060241, 0.9817394578313253, 0.9832925451807228, 0.9816217996987951, 0.9813864834337349, 0.9823748117469879, 0.9813629518072289, 0.9830336972891566, 0.9821630271084337, 0.9831513554216867, 0.9820689006024096, 0.9829160391566265, 0.9824924698795181, 0.9838102409638554, 0.9841867469879518, 0.9838808358433735, 0.9831748870481928, 0.9841396837349398, 0.9841632153614458, 0.9841161521084337, 0.9836219879518072, 0.9831513554216867, 0.9840455572289156, 0.9828689759036144, 0.9850103539156626, 0.9848691641566265, 0.984375, 0.9841161521084337, 0.9844455948795181, 0.9842102786144579, 0.9839514307228916, 0.9848221009036144, 0.9841867469879518, 0.9843279367469879, 0.9858574924698795, 0.9858574924698795, 0.9852692018072289, 0.984375, 0.9859045557228916, 0.9849632906626506, 0.9852221385542169, 0.9851515436746988, 0.9848456325301205, 0.984375, 0.9849868222891566, 0.9852221385542169, 0.9841161521084337, 0.9861869352409639, 0.9860457454819277, 0.9855751129518072, 0.9856221762048193, 0.9857633659638554, 0.9858339608433735, 0.9852456701807228, 0.9856457078313253, 0.9852927334337349, 0.9855515813253012, 0.9854574548192772, 0.9862810617469879, 0.986328125, 0.9864222515060241, 0.9868928840361446, 0.9859045557228916, 0.9849162274096386, 0.9852456701807228, 0.9862339984939759, 0.9871046686746988, 0.9872223268072289, 0.9866105045180723, 0.987316453313253, 0.9863751882530121, 0.987292921686747, 0.9873870481927711, 0.9871282003012049, 0.9872458584337349, 0.9875047063253012, 0.9875282379518072, 0.987292921686747, 0.9860457454819277, 0.9867987575301205, 0.9862339984939759, 0.9875753012048193], "end": "2016-01-23 07:09:06.418000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0], "accuracy_valid": [0.41920405982905984, 0.6427617521367521, 0.7536057692307693, 0.7877938034188035, 0.8050213675213675, 0.8229166666666666, 0.8338675213675214, 0.8410790598290598, 0.8458867521367521, 0.8484241452991453, 0.8517628205128205, 0.8532318376068376, 0.8577724358974359, 0.8568376068376068, 0.859375, 0.8609775641025641, 0.8631143162393162, 0.8636485042735043, 0.8605769230769231, 0.8667200854700855, 0.8619123931623932, 0.8660523504273504, 0.8644497863247863, 0.8681891025641025, 0.8659188034188035, 0.8659188034188035, 0.8691239316239316, 0.8672542735042735, 0.8657852564102564, 0.8707264957264957, 0.8679220085470085, 0.8683226495726496, 0.8671207264957265, 0.8707264957264957, 0.8688568376068376, 0.8705929487179487, 0.8715277777777778, 0.8693910256410257, 0.8689903846153846, 0.8703258547008547, 0.8704594017094017, 0.8713942307692307, 0.8688568376068376, 0.8696581196581197, 0.8684561965811965, 0.8703258547008547, 0.8691239316239316, 0.8703258547008547, 0.8705929487179487, 0.8708600427350427, 0.8700587606837606, 0.8703258547008547, 0.8685897435897436, 0.8704594017094017, 0.8717948717948718, 0.8697916666666666, 0.8743322649572649, 0.8720619658119658, 0.8713942307692307, 0.8715277777777778, 0.8721955128205128, 0.8715277777777778, 0.8701923076923077, 0.8719284188034188, 0.8717948717948718, 0.8704594017094017, 0.8721955128205128, 0.8711271367521367, 0.8701923076923077, 0.8715277777777778, 0.8721955128205128, 0.8736645299145299, 0.8737980769230769, 0.8713942307692307, 0.8707264957264957, 0.8724626068376068, 0.8725961538461539, 0.8704594017094017, 0.8717948717948718, 0.8731303418803419, 0.8729967948717948, 0.8720619658119658, 0.8724626068376068, 0.8727297008547008, 0.8723290598290598, 0.8735309829059829, 0.8731303418803419, 0.8733974358974359, 0.8732638888888888, 0.8731303418803419, 0.8736645299145299, 0.8711271367521367, 0.8709935897435898, 0.8725961538461539, 0.8720619658119658, 0.875, 0.8727297008547008, 0.8739316239316239, 0.875534188034188, 0.8707264957264957, 0.8721955128205128, 0.8707264957264957, 0.8719284188034188, 0.8700587606837606, 0.8713942307692307, 0.8716613247863247, 0.8719284188034188, 0.8719284188034188, 0.8729967948717948, 0.8736645299145299, 0.8733974358974359, 0.8727297008547008, 0.8713942307692307, 0.8723290598290598, 0.8739316239316239, 0.8736645299145299, 0.8740651709401709, 0.8736645299145299, 0.8723290598290598, 0.8732638888888888, 0.8735309829059829, 0.8704594017094017, 0.8725961538461539, 0.875267094017094, 0.874198717948718, 0.8735309829059829, 0.8732638888888888, 0.874732905982906, 0.874198717948718, 0.8721955128205128, 0.8754006410256411, 0.8760683760683761, 0.8739316239316239, 0.8754006410256411, 0.8728632478632479, 0.8728632478632479, 0.8719284188034188, 0.8762019230769231, 0.8732638888888888, 0.8724626068376068, 0.8733974358974359, 0.8739316239316239, 0.875, 0.8740651709401709, 0.8715277777777778, 0.8740651709401709, 0.8740651709401709, 0.875, 0.8735309829059829, 0.8736645299145299, 0.8733974358974359, 0.8727297008547008, 0.8764690170940171, 0.8743322649572649, 0.8732638888888888, 0.8743322649572649, 0.8731303418803419, 0.8721955128205128, 0.875534188034188, 0.874198717948718, 0.8756677350427351, 0.8737980769230769, 0.874465811965812, 0.8728632478632479, 0.8719284188034188, 0.8723290598290598, 0.8760683760683761, 0.8725961538461539, 0.8723290598290598, 0.8709935897435898, 0.8736645299145299, 0.8743322649572649, 0.8748664529914529, 0.8736645299145299, 0.8720619658119658, 0.8740651709401709, 0.8727297008547008, 0.8737980769230769, 0.8733974358974359, 0.874465811965812, 0.8743322649572649, 0.8739316239316239, 0.874198717948718, 0.8728632478632479, 0.8725961538461539, 0.8716613247863247, 0.8732638888888888, 0.8724626068376068, 0.8737980769230769, 0.875, 0.8745993589743589, 0.8731303418803419, 0.8736645299145299, 0.875, 0.8740651709401709, 0.8733974358974359, 0.874465811965812, 0.8762019230769231, 0.8736645299145299, 0.874198717948718, 0.8737980769230769, 0.8733974358974359, 0.8745993589743589], "accuracy_test": 0.8268229166666666, "start": "2016-01-20 16:46:58.171000", "learning_rate_per_epoch": [0.0060626426711678505, 0.0030313213355839252, 0.0020208810456097126, 0.0015156606677919626, 0.0012125285575166345, 0.0010104405228048563, 0.0008660918101668358, 0.0007578303338959813, 0.0006736269569955766, 0.0006062642787583172, 0.0005511493654921651, 0.0005052202614024282, 0.0004663571307901293, 0.0004330459050834179, 0.00040417618583887815, 0.00037891516694799066, 0.00035662605660036206, 0.0003368134784977883, 0.00031908645178191364, 0.0003031321393791586, 0.00028869727975688875, 0.00027557468274608254, 0.0002635931596159935, 0.0002526101307012141, 0.00024250571732409298, 0.00023317856539506465, 0.00022454232384916395, 0.00021652295254170895, 0.00020905665587633848, 0.00020208809291943908, 0.0001955691259354353, 0.00018945758347399533, 0.0001837164454627782, 0.00017831302830018103, 0.0001732183591229841, 0.00016840673924889416, 0.0001638552057556808, 0.00015954322589095682, 0.0001554523769300431, 0.0001515660696895793, 0.00014786933024879545, 0.00014434863987844437, 0.00014099168765824288, 0.00013778734137304127, 0.00013472540013026446, 0.00013179657980799675, 0.00012899239663966, 0.00012630506535060704, 0.00012372739729471505, 0.00012125285866204649, 0.00011887535220012069, 0.00011658928269753233, 0.00011438948422437534, 0.00011227116192458197, 0.00011022987018804997, 0.00010826147627085447, 0.00010636215301929042, 0.00010452832793816924, 0.00010275666136294603, 0.00010104404645971954, 9.938758739735931e-05, 9.778456296771765e-05, 9.623242658562958e-05, 9.472879173699766e-05, 9.327142470283434e-05, 9.18582227313891e-05, 9.048720676219091e-05, 8.915651415009052e-05, 8.786439138930291e-05, 8.660917956149206e-05, 8.538933616364375e-05, 8.420336962444708e-05, 8.304990478791296e-05, 8.19276028778404e-05, 8.083523425739259e-05, 7.977161294547841e-05, 7.873561844462529e-05, 7.772618846502155e-05, 7.674231164855883e-05, 7.578303484478965e-05, 7.484744128305465e-05, 7.393466512439772e-05, 7.304389146156609e-05, 7.217431993922219e-05, 7.132520840968937e-05, 7.049584382912144e-05, 6.968554953346029e-05, 6.889367068652064e-05, 6.811958155594766e-05, 6.736270006513223e-05, 6.662245141342282e-05, 6.589828990399837e-05, 6.51897062198259e-05, 6.449619831983e-05, 6.381729326676577e-05, 6.315253267530352e-05, 6.250147271202877e-05, 6.186369864735752e-05, 6.123881757957861e-05, 6.0626429331023246e-05, 6.002616646583192e-05, 5.9437676100060344e-05, 5.8860608987743035e-05, 5.829464134876616e-05, 5.773945667897351e-05, 5.719474211218767e-05, 5.666021388606168e-05, 5.613558096229099e-05, 5.562057776842266e-05, 5.5114935094024986e-05, 5.4618405556539074e-05, 5.4130738135427237e-05, 5.3651707276003435e-05, 5.318107650964521e-05, 5.2718634833581746e-05, 5.226416396908462e-05, 5.1817460189340636e-05, 5.137833068147302e-05, 5.094657899462618e-05, 5.052202322985977e-05, 5.010448512621224e-05, 4.9693793698679656e-05, 4.9289777962258086e-05, 4.8892281483858824e-05, 4.850114419241436e-05, 4.811621329281479e-05, 4.773734326590784e-05, 4.736439586849883e-05, 4.69972328573931e-05, 4.663571235141717e-05, 4.627971793524921e-05, 4.592911136569455e-05, 4.558377986541018e-05, 4.5243603381095454e-05, 4.490846549742855e-05, 4.457825707504526e-05, 4.425286897458136e-05, 4.3932195694651455e-05, 4.361613537184894e-05, 4.330458978074603e-05, 4.299746797187254e-05, 4.2694668081821874e-05, 4.2396102799102664e-05, 4.210168481222354e-05, 4.1811330447671935e-05, 4.152495239395648e-05, 4.124246697756462e-05, 4.09638014389202e-05, 4.068887938046828e-05, 4.041761712869629e-05, 4.014995283796452e-05, 3.9885806472739205e-05, 3.962511618738063e-05, 3.9367809222312644e-05, 3.9113823731895536e-05, 3.8863094232510775e-05, 3.861555887851864e-05, 3.837115582427941e-05, 3.8129830500110984e-05, 3.789151742239483e-05, 3.765616565942764e-05, 3.7423720641527325e-05, 3.719412779901177e-05, 3.696733256219886e-05, 3.6743291275342926e-05, 3.6521945730783045e-05, 3.630324863479473e-05, 3.6087159969611093e-05, 3.587362516555004e-05, 3.5662604204844683e-05, 3.545404979377054e-05, 3.524792191456072e-05, 3.504417691146955e-05, 3.4842774766730145e-05, 3.464367182459682e-05, 3.444683534326032e-05, 3.425221802899614e-05, 3.405979077797383e-05, 3.386951357242651e-05, 3.3681350032566115e-05, 3.349526377860457e-05, 3.331122570671141e-05, 3.312919579911977e-05, 3.294914495199919e-05, 3.2771044061519206e-05, 3.259485310991295e-05, 3.242055026930757e-05, 3.2248099159915e-05, 3.207747431588359e-05, 3.190864663338289e-05, 3.174158700858243e-05, 3.157626633765176e-05, 3.1412655516760424e-05, 3.1250736356014386e-05, 3.109047611360438e-05, 3.093184932367876e-05, 3.077483779634349e-05, 3.0619408789789304e-05, 3.0465542295132764e-05, 3.0313214665511623e-05, 3.0162402254063636e-05, 3.001308323291596e-05, 2.9865235774195753e-05], "accuracy_train_last": 0.9875753012048193, "error_valid": [0.5807959401709402, 0.35723824786324787, 0.24639423076923073, 0.21220619658119655, 0.19497863247863245, 0.17708333333333337, 0.1661324786324786, 0.15892094017094016, 0.15411324786324787, 0.15157585470085466, 0.14823717948717952, 0.14676816239316237, 0.1422275641025641, 0.1431623931623932, 0.140625, 0.1390224358974359, 0.13688568376068377, 0.13635149572649574, 0.13942307692307687, 0.1332799145299145, 0.1380876068376068, 0.1339476495726496, 0.1355502136752137, 0.13181089743589747, 0.13408119658119655, 0.13408119658119655, 0.13087606837606836, 0.13274572649572647, 0.1342147435897436, 0.12927350427350426, 0.13207799145299148, 0.1316773504273504, 0.13287927350427353, 0.12927350427350426, 0.13114316239316237, 0.12940705128205132, 0.1284722222222222, 0.13060897435897434, 0.13100961538461542, 0.12967414529914534, 0.12954059829059827, 0.12860576923076927, 0.13114316239316237, 0.13034188034188032, 0.13154380341880345, 0.12967414529914534, 0.13087606837606836, 0.12967414529914534, 0.12940705128205132, 0.1291399572649573, 0.12994123931623935, 0.12967414529914534, 0.1314102564102564, 0.12954059829059827, 0.1282051282051282, 0.13020833333333337, 0.1256677350427351, 0.12793803418803418, 0.12860576923076927, 0.1284722222222222, 0.12780448717948723, 0.1284722222222222, 0.1298076923076923, 0.12807158119658124, 0.1282051282051282, 0.12954059829059827, 0.12780448717948723, 0.1288728632478633, 0.1298076923076923, 0.1284722222222222, 0.12780448717948723, 0.12633547008547008, 0.12620192307692313, 0.12860576923076927, 0.12927350427350426, 0.1275373931623932, 0.12740384615384615, 0.12954059829059827, 0.1282051282051282, 0.1268696581196581, 0.12700320512820518, 0.12793803418803418, 0.1275373931623932, 0.1272702991452992, 0.12767094017094016, 0.12646901709401714, 0.1268696581196581, 0.1266025641025641, 0.12673611111111116, 0.1268696581196581, 0.12633547008547008, 0.1288728632478633, 0.12900641025641024, 0.12740384615384615, 0.12793803418803418, 0.125, 0.1272702991452992, 0.12606837606837606, 0.12446581196581197, 0.12927350427350426, 0.12780448717948723, 0.12927350427350426, 0.12807158119658124, 0.12994123931623935, 0.12860576923076927, 0.12833867521367526, 0.12807158119658124, 0.12807158119658124, 0.12700320512820518, 0.12633547008547008, 0.1266025641025641, 0.1272702991452992, 0.12860576923076927, 0.12767094017094016, 0.12606837606837606, 0.12633547008547008, 0.1259348290598291, 0.12633547008547008, 0.12767094017094016, 0.12673611111111116, 0.12646901709401714, 0.12954059829059827, 0.12740384615384615, 0.12473290598290598, 0.12580128205128205, 0.12646901709401714, 0.12673611111111116, 0.12526709401709402, 0.12580128205128205, 0.12780448717948723, 0.12459935897435892, 0.12393162393162394, 0.12606837606837606, 0.12459935897435892, 0.12713675213675213, 0.12713675213675213, 0.12807158119658124, 0.12379807692307687, 0.12673611111111116, 0.1275373931623932, 0.1266025641025641, 0.12606837606837606, 0.125, 0.1259348290598291, 0.1284722222222222, 0.1259348290598291, 0.1259348290598291, 0.125, 0.12646901709401714, 0.12633547008547008, 0.1266025641025641, 0.1272702991452992, 0.12353098290598286, 0.1256677350427351, 0.12673611111111116, 0.1256677350427351, 0.1268696581196581, 0.12780448717948723, 0.12446581196581197, 0.12580128205128205, 0.1243322649572649, 0.12620192307692313, 0.12553418803418803, 0.12713675213675213, 0.12807158119658124, 0.12767094017094016, 0.12393162393162394, 0.12740384615384615, 0.12767094017094016, 0.12900641025641024, 0.12633547008547008, 0.1256677350427351, 0.12513354700854706, 0.12633547008547008, 0.12793803418803418, 0.1259348290598291, 0.1272702991452992, 0.12620192307692313, 0.1266025641025641, 0.12553418803418803, 0.1256677350427351, 0.12606837606837606, 0.12580128205128205, 0.12713675213675213, 0.12740384615384615, 0.12833867521367526, 0.12673611111111116, 0.1275373931623932, 0.12620192307692313, 0.125, 0.12540064102564108, 0.1268696581196581, 0.12633547008547008, 0.125, 0.1259348290598291, 0.1266025641025641, 0.12553418803418803, 0.12379807692307687, 0.12633547008547008, 0.12580128205128205, 0.12620192307692313, 0.1266025641025641, 0.12540064102564108], "accuracy_train_std": [0.08667522855752187, 0.08654336880041857, 0.07294166178252305, 0.0724136781434625, 0.06856339069306554, 0.0665040235583207, 0.06501730063996382, 0.06309895403831123, 0.060150736144719524, 0.058190962328875696, 0.059001554359504216, 0.057119183269686055, 0.05602367809666809, 0.05508760801424203, 0.052910965545286084, 0.05261469096169598, 0.05144439818536059, 0.05088475237484407, 0.04966419320408933, 0.049304811357315015, 0.04899562104967471, 0.04779253646747451, 0.0468753543906258, 0.04505089179579603, 0.045026818850079905, 0.0457097347500204, 0.04316775385564397, 0.04314593505635906, 0.043136828330924526, 0.04245585418860033, 0.04222546953185423, 0.04169166742590653, 0.04024999934448492, 0.03996174596001171, 0.03947883242843327, 0.03912843921765707, 0.03795918319849219, 0.03784225037967834, 0.03663370934856453, 0.03729387167052298, 0.037082468760827214, 0.03754614622217439, 0.03651283520673954, 0.03588306854881696, 0.0360888238793949, 0.033639100022700784, 0.035052962335149866, 0.03220590879092315, 0.032924100964594694, 0.03318789516057314, 0.03234823291547173, 0.03349778898188094, 0.03235104870592836, 0.032051552472792455, 0.031147732335951472, 0.030725699876978523, 0.0314983544481821, 0.031343949841663575, 0.03066190799203324, 0.031033210696224653, 0.029700893368162506, 0.03024098752766618, 0.030251239852078837, 0.029348117791183306, 0.03011397325903332, 0.0305021804458599, 0.029323956929844266, 0.029659027179076966, 0.028338009460405565, 0.028028212033910146, 0.02999497889406736, 0.027828633231881342, 0.028746894110540513, 0.029632027238324164, 0.028416073505740255, 0.02721549345422856, 0.027080710660368025, 0.027953036780072413, 0.026545082803989744, 0.027299891728652145, 0.02696111396809554, 0.027122065169379254, 0.027205562596826466, 0.0268696864138832, 0.026373526416525916, 0.026668040474186187, 0.026346175497285306, 0.0259276667734765, 0.025778883412182447, 0.026042260857083277, 0.02625917598369015, 0.026915068691022715, 0.026563360533139315, 0.0258609898805723, 0.02600770676787368, 0.02473857858062117, 0.0258858908411503, 0.02567669252827355, 0.024855769922152566, 0.02513476245115249, 0.024787002785094384, 0.025387811517134595, 0.02590583071389093, 0.026451966145293444, 0.026461060273396852, 0.02547922108235991, 0.024555734274924808, 0.025142956551203424, 0.02403043431060401, 0.025700500874686366, 0.02439119776406678, 0.025095252309337974, 0.02486436775249495, 0.023693052767390692, 0.023986185244341787, 0.02328322155362463, 0.023487710809757745, 0.024783785640693057, 0.023400129106121055, 0.023413436245239724, 0.02285732642788407, 0.02401162371944787, 0.02368025352345217, 0.023396780429920494, 0.024128432966288436, 0.02370043694753841, 0.0236803353670739, 0.024249589445489297, 0.023508825078809806, 0.024275105462264926, 0.022849100282177428, 0.023745591626297528, 0.0233448441743046, 0.02378292026770338, 0.022420293570722, 0.02216274495897807, 0.022585348162817927, 0.023422457140129808, 0.022195451250769323, 0.022426405504073488, 0.022524069221727595, 0.022578200165307115, 0.023136938509980942, 0.021994556996918595, 0.023247150959123716, 0.021954376984449957, 0.02182363944289067, 0.022361732116564944, 0.02222829625489857, 0.022063670132286682, 0.02212972759709824, 0.022781288349359136, 0.02158750682530499, 0.022393801613685278, 0.022590704612729846, 0.021507018379373893, 0.021266329090238893, 0.022145498559328066, 0.02242740548004305, 0.021124206246902012, 0.022353992425274957, 0.021473018568675485, 0.02171405620095872, 0.021958525642562763, 0.022295865310405852, 0.021753152804139128, 0.022147348816127232, 0.02323122211057695, 0.020821208122984255, 0.020868281031491714, 0.02128413178404266, 0.021658187404180768, 0.021376135217056374, 0.02112919931110943, 0.021472077300930224, 0.021486372389139075, 0.02224393509763364, 0.021525921115690457, 0.021290439837208956, 0.020848104764974874, 0.021910143147969057, 0.021010448310993114, 0.02064107305904228, 0.022440820599302684, 0.0221900992080012, 0.021574574934097833, 0.021062882713765777, 0.01999850476440452, 0.020454912868905658, 0.021235060233112012, 0.020620674482440485, 0.020839284791643195, 0.02055258240274389, 0.020431306764524662, 0.020251080430025787, 0.02059494619807602, 0.01997666010271833, 0.020409993161218092, 0.020008691700663, 0.020973729596311583, 0.020545239266396915, 0.020992941073146473, 0.01951848771962323], "accuracy_test_std": 0.06397151756650991, "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-3, 3], "momentum": 0.5904523320578263, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.006062642815116278, "patience_threshold": 1, "do_flip": true, "batch_size": 32, "optimization": "rmsprop", "nb_data_augmentation": 4, "learning_rate_decay_method": "lin", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 1.0996984734312926e-05, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.028404573068003303}, "accuracy_valid_max": 0.8764690170940171, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = 1234423\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-10, -4], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128, 256],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-3, 3)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_optimizer.learning_rate = learning_rate\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8745993589743589, "loss_train": [1.807718276977539, 1.2678104639053345, 0.9890264272689819, 0.864220142364502, 0.7853013873100281, 0.7297452688217163, 0.6874750852584839, 0.6521227359771729, 0.6220596432685852, 0.5961416363716125, 0.5753175020217896, 0.5555723309516907, 0.5395469069480896, 0.5242375135421753, 0.5099280476570129, 0.49763840436935425, 0.4864114224910736, 0.47656163573265076, 0.46535342931747437, 0.45692873001098633, 0.4480896294116974, 0.43889930844306946, 0.4331042766571045, 0.42578768730163574, 0.4167557954788208, 0.411361426115036, 0.4056999087333679, 0.39937824010849, 0.3927723467350006, 0.38942715525627136, 0.383395791053772, 0.37826794385910034, 0.373439222574234, 0.3680964708328247, 0.36500677466392517, 0.3597337007522583, 0.35681387782096863, 0.3521651327610016, 0.3471769690513611, 0.3442752957344055, 0.34000810980796814, 0.33681175112724304, 0.33166709542274475, 0.33053481578826904, 0.32841789722442627, 0.3258996605873108, 0.32000845670700073, 0.31943926215171814, 0.3153838813304901, 0.3114294707775116, 0.30944427847862244, 0.30868053436279297, 0.30504927039146423, 0.30273252725601196, 0.29986318945884705, 0.29889219999313354, 0.2948594391345978, 0.29334506392478943, 0.29274696111679077, 0.29003483057022095, 0.2891092598438263, 0.2852605879306793, 0.2822791635990143, 0.2805745601654053, 0.2786971926689148, 0.27846914529800415, 0.2768736779689789, 0.274000346660614, 0.27240413427352905, 0.2710214853286743, 0.2706546187400818, 0.2668187916278839, 0.26596373319625854, 0.2649681568145752, 0.26292309165000916, 0.2630000412464142, 0.25999367237091064, 0.26020753383636475, 0.2585785984992981, 0.25785934925079346, 0.2564433217048645, 0.2533648610115051, 0.2534250020980835, 0.25212618708610535, 0.2514459192752838, 0.25030195713043213, 0.2480894923210144, 0.24702496826648712, 0.24627043306827545, 0.24561963975429535, 0.24413184821605682, 0.24369074404239655, 0.24114978313446045, 0.24142035841941833, 0.23970191180706024, 0.23922912776470184, 0.2377544641494751, 0.23676155507564545, 0.2364063858985901, 0.23478364944458008, 0.2338598221540451, 0.23277857899665833, 0.23256176710128784, 0.2316901832818985, 0.22975076735019684, 0.22923991084098816, 0.22939665615558624, 0.22765755653381348, 0.22806860506534576, 0.22684790194034576, 0.22661691904067993, 0.22436153888702393, 0.2245757132768631, 0.2239772379398346, 0.2239268273115158, 0.22262729704380035, 0.2221222072839737, 0.22080574929714203, 0.2214459329843521, 0.2195940613746643, 0.2197554111480713, 0.21915866434574127, 0.21796678006649017, 0.21776194870471954, 0.21745260059833527, 0.2161921262741089, 0.21539485454559326, 0.21474888920783997, 0.21389125287532806, 0.21437039971351624, 0.21279436349868774, 0.21281030774116516, 0.21264873445034027, 0.21122272312641144, 0.21234486997127533, 0.21049271523952484, 0.20970432460308075, 0.20959573984146118, 0.2097286432981491, 0.20905815064907074, 0.20764727890491486, 0.20748591423034668, 0.20705008506774902, 0.20710960030555725, 0.2064717411994934, 0.20607990026474, 0.20503991842269897, 0.20552340149879456, 0.20484335720539093, 0.20439019799232483, 0.204362690448761, 0.20374049246311188, 0.2026006281375885, 0.20224392414093018, 0.20142598450183868, 0.20181427896022797, 0.20183683931827545, 0.2008473426103592, 0.2007521390914917, 0.1998542696237564, 0.19912150502204895, 0.19891685247421265, 0.1987757682800293, 0.19865435361862183, 0.19847305119037628, 0.1982284039258957, 0.19750764966011047, 0.196965292096138, 0.19688226282596588, 0.19551032781600952, 0.1974180042743683, 0.19579340517520905, 0.19605256617069244, 0.19508635997772217, 0.19444485008716583, 0.1935657411813736, 0.1932564079761505, 0.19355185329914093, 0.1935112327337265, 0.19334539771080017, 0.1925750970840454, 0.1932429075241089, 0.192305788397789, 0.19211909174919128, 0.19233080744743347, 0.1914251297712326, 0.19068728387355804, 0.1913267821073532, 0.18969108164310455, 0.19004954397678375, 0.1901133805513382, 0.18898415565490723, 0.1885724812746048, 0.18870316445827484, 0.18885056674480438, 0.18882407248020172, 0.18742917478084564, 0.1876499056816101, 0.18808959424495697, 0.1870885193347931, 0.18732289969921112, 0.18690453469753265, 0.187038853764534], "accuracy_train_first": 0.4135683358433735, "model": "residualv4", "loss_std": [0.3024808466434479, 0.20222827792167664, 0.17852328717708588, 0.168847918510437, 0.16442842781543732, 0.1592417061328888, 0.1528548151254654, 0.1504095047712326, 0.14652660489082336, 0.14143812656402588, 0.13955272734165192, 0.13642996549606323, 0.13271038234233856, 0.13175956904888153, 0.1278093308210373, 0.12541945278644562, 0.1227901428937912, 0.1205902025103569, 0.11809387803077698, 0.1145588606595993, 0.11326952278614044, 0.11322660744190216, 0.11072976887226105, 0.10790402442216873, 0.10641225427389145, 0.104817695915699, 0.10124640166759491, 0.1014714315533638, 0.0990929901599884, 0.0994221642613411, 0.09582321345806122, 0.09540914744138718, 0.09370587021112442, 0.09173635393381119, 0.09124922007322311, 0.09089966118335724, 0.08893699944019318, 0.0864507183432579, 0.08495951443910599, 0.08457671105861664, 0.08226700872182846, 0.08140532672405243, 0.07913223654031754, 0.07986008375883102, 0.08131319284439087, 0.08024830371141434, 0.0767195075750351, 0.07480370253324509, 0.0748727098107338, 0.07331787794828415, 0.07205374538898468, 0.07234282046556473, 0.0709775984287262, 0.07117336988449097, 0.06842650473117828, 0.06951075047254562, 0.0676235482096672, 0.06691814959049225, 0.06700816750526428, 0.06473255157470703, 0.06449009478092194, 0.06325454264879227, 0.060709256678819656, 0.060635797679424286, 0.060614220798015594, 0.059023208916187286, 0.05877912417054176, 0.05814729258418083, 0.058351293206214905, 0.05859915167093277, 0.05681317299604416, 0.053687550127506256, 0.05341049283742905, 0.05359984189271927, 0.053708337247371674, 0.05388091132044792, 0.051187582314014435, 0.052838876843452454, 0.0511292964220047, 0.05101718008518219, 0.049554601311683655, 0.047610584646463394, 0.04722053185105324, 0.04790782183408737, 0.04739172011613846, 0.04771711677312851, 0.04716521129012108, 0.047898393124341965, 0.044270385056734085, 0.04551626369357109, 0.04519466683268547, 0.045471157878637314, 0.04443826898932457, 0.04416147619485855, 0.04261741042137146, 0.042693763971328735, 0.04074154794216156, 0.0407470241189003, 0.041921500116586685, 0.04112307354807854, 0.04092562571167946, 0.037786971777677536, 0.03895551338791847, 0.0384393185377121, 0.03580212593078613, 0.03857984021306038, 0.03825360909104347, 0.03731635957956314, 0.037251196801662445, 0.03558190539479256, 0.037176378071308136, 0.03625005483627319, 0.03617532551288605, 0.034621432423591614, 0.03526763617992401, 0.03522628918290138, 0.03476293385028839, 0.03370824456214905, 0.03514355421066284, 0.03412685915827751, 0.034289825707674026, 0.033089715987443924, 0.03341484069824219, 0.031541381031274796, 0.033293455839157104, 0.0323195606470108, 0.03195993974804878, 0.03217502683401108, 0.030871324241161346, 0.03181979060173035, 0.029820134863257408, 0.03063449077308178, 0.030285539105534554, 0.030787818133831024, 0.03022436425089836, 0.029181361198425293, 0.02902127057313919, 0.02950618788599968, 0.02977975644171238, 0.028951428830623627, 0.02854282781481743, 0.02817705273628235, 0.02761121653020382, 0.028394615277647972, 0.029439635574817657, 0.027370553463697433, 0.02853095531463623, 0.028245070949196815, 0.027782678604125977, 0.02693924866616726, 0.026960188522934914, 0.026659483090043068, 0.02673419937491417, 0.026078831404447556, 0.024630453437566757, 0.027017829939723015, 0.024772798642516136, 0.025182919576764107, 0.02608506567776203, 0.0249232929199934, 0.023272817954421043, 0.024810239672660828, 0.02428274415433407, 0.024329157546162605, 0.02450503595173359, 0.024433160200715065, 0.02396932989358902, 0.023760974407196045, 0.023534193634986877, 0.023028060793876648, 0.02411404438316822, 0.024147341027855873, 0.024107202887535095, 0.02311193384230137, 0.022430280223488808, 0.02260885201394558, 0.02235528454184532, 0.023200232535600662, 0.021810149773955345, 0.021583782508969307, 0.022302672266960144, 0.02274377830326557, 0.022258611395955086, 0.02277793176472187, 0.02216554619371891, 0.021217938512563705, 0.021930638700723648, 0.021826671436429024, 0.02032121643424034, 0.022013148292899132, 0.020819297060370445, 0.02039700746536255, 0.020970070734620094, 0.021397190168499947, 0.02020632103085518, 0.020807011052966118, 0.020204981788992882, 0.020484132692217827, 0.02111155353486538, 0.019977886229753494, 0.02067733369767666, 0.01960914582014084, 0.020200561732053757]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:08 2016", "state": "available"}], "summary": "65e8262588d20840e1b18a8daccfb372"}