{"content": {"hp_model": {"f0": 32, "f1": 32, "f2": 32, "f3": 64, "nonlin": "rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.4805617332458496, 1.02200186252594, 0.8318202495574951, 0.7249174118041992, 0.6482929587364197, 0.5940366983413696, 0.5493744611740112, 0.5159731507301331, 0.4828050136566162, 0.45544368028640747, 0.4295361638069153, 0.40764886140823364, 0.3894663453102112, 0.3720630705356598, 0.3527557849884033, 0.34058472514152527, 0.3268915116786957, 0.31170618534088135, 0.3015446662902832, 0.2879735827445984, 0.28000202775001526, 0.26846373081207275, 0.2599314749240875, 0.2497585266828537, 0.24639064073562622, 0.23642821609973907, 0.228656604886055, 0.2197902649641037, 0.21752746403217316, 0.2119087427854538, 0.20560449361801147, 0.20074327290058136, 0.19815756380558014, 0.19203369319438934, 0.18900249898433685, 0.18168851733207703, 0.1806906759738922, 0.1777247190475464, 0.1749180555343628, 0.1705932766199112, 0.16879910230636597, 0.16609179973602295, 0.16351565718650818, 0.15894275903701782, 0.15736247599124908, 0.154927596449852, 0.15281051397323608, 0.15147671103477478, 0.1495298147201538, 0.14674806594848633, 0.14831984043121338, 0.14596500992774963, 0.14486609399318695, 0.14484858512878418, 0.1401028335094452, 0.1388232409954071, 0.13673874735832214, 0.13907690346240997, 0.1385849416255951, 0.13339734077453613, 0.13625311851501465, 0.13445137441158295, 0.13305138051509857, 0.13159465789794922, 0.13127659261226654, 0.13070730865001678, 0.12814593315124512, 0.1307774782180786, 0.12953844666481018, 0.12499532848596573, 0.13085415959358215, 0.12592297792434692, 0.12990282475948334, 0.12353517860174179, 0.1286095827817917, 0.12361009418964386, 0.12276705354452133, 0.127569779753685, 0.12030506879091263, 0.12265215069055557, 0.12403421849012375, 0.12392140179872513, 0.11835609376430511, 0.12240868806838989, 0.11904589086771011, 0.12181150168180466, 0.12091969698667526, 0.1177721843123436, 0.12322097271680832, 0.11976490169763565, 0.09793215990066528, 0.08411813527345657, 0.08094694465398788, 0.0781978964805603, 0.07764852046966553, 0.07557346671819687, 0.07464531064033508, 0.0741235688328743, 0.07325274497270584, 0.07244931906461716, 0.0716388002038002, 0.07109395414590836, 0.07090573757886887, 0.06960053741931915, 0.06932415068149567, 0.06963935494422913, 0.06872769445180893, 0.0679759532213211, 0.06708019226789474, 0.06712567061185837, 0.06663475930690765, 0.06672590225934982, 0.06664270162582397, 0.06658351421356201, 0.06629937887191772, 0.06660585105419159, 0.06632732599973679, 0.06677339971065521, 0.06671935319900513, 0.0667705237865448, 0.06676903367042542, 0.06647839397192001, 0.0668673887848854, 0.06676160544157028, 0.06639795750379562, 0.0664924904704094, 0.0667606070637703, 0.0666155144572258, 0.06663402169942856, 0.06656976789236069, 0.06625792384147644, 0.06693745404481888, 0.06659019738435745, 0.06618113070726395, 0.06631278991699219, 0.06643855571746826, 0.06658293306827545, 0.06658382713794708, 0.06644858419895172, 0.06656615436077118, 0.06648626178503036, 0.0667094960808754, 0.06630980968475342, 0.06647294759750366, 0.06639082729816437, 0.06651125848293304, 0.06646957248449326, 0.06667473167181015, 0.06683551520109177, 0.0664636567234993, 0.06674911081790924, 0.06665302067995071, 0.06664610654115677, 0.06650085747241974, 0.06640013307332993, 0.06652666628360748, 0.06665147095918655, 0.06684216856956482, 0.0663825124502182, 0.06665007770061493, 0.06666219979524612, 0.06673095375299454, 0.0666520744562149, 0.06685119867324829, 0.06667719781398773, 0.06641222536563873, 0.06643692404031754, 0.06685231626033783, 0.06665202230215073, 0.06651034206151962, 0.06634453684091568, 0.06669638305902481, 0.06644672900438309, 0.0667397603392601, 0.06649549305438995, 0.06664922833442688], "moving_avg_accuracy_train": [0.05582222669227573, 0.11927599732661956, 0.18111338308791292, 0.24024465981811202, 0.2942486370752727, 0.3456626005410566, 0.3945157583994648, 0.43875070419457735, 0.4805312680612953, 0.518380097119876, 0.5546433096070393, 0.5873543893144786, 0.6174917975475749, 0.6450640744823246, 0.6694212496522169, 0.6935001930586269, 0.7138018376220424, 0.7332822509147938, 0.7514005964270889, 0.7680744448512404, 0.7837131686650736, 0.7974790636964068, 0.8099078607055498, 0.8222653285744097, 0.8332637446718414, 0.8431926542405099, 0.8530074709558554, 0.8617941948770196, 0.868907225757304, 0.8753575653816473, 0.8822068628590326, 0.8875761378422784, 0.8926525538545621, 0.8976514447465607, 0.9015181863172627, 0.9063652970356194, 0.9114228801273694, 0.9159955231027738, 0.9195484320616455, 0.9224228183912875, 0.9250444991248331, 0.9271182347743653, 0.9301309312708582, 0.9323445239307325, 0.9345133965365057, 0.936949229126995, 0.9393970285345705, 0.9412257350918739, 0.9432297160077511, 0.9457841056046042, 0.9466045499894649, 0.9469405910453357, 0.9493424571265257, 0.9513530019269776, 0.9522233123724304, 0.9535576880900138, 0.9535077682739527, 0.9548437264906235, 0.9558903039153892, 0.957601811804812, 0.9590282726624445, 0.9600098901867131, 0.9605165997537746, 0.9619864392939009, 0.9634207938764157, 0.9639049584614117, 0.9650987050998128, 0.9654803269244107, 0.9664933212772353, 0.9674469409709865, 0.9683586771179816, 0.9684584795681436, 0.9688876653530235, 0.9694551860201022, 0.9703938540990629, 0.970234335279688, 0.9712159240731477, 0.9720761024991662, 0.9720597485361635, 0.9720753290016224, 0.9727402849407458, 0.9734480272800046, 0.9741664476913083, 0.9749501737924249, 0.9756299145977062, 0.976178938353421, 0.9764475563478593, 0.9764661343059582, 0.9770896462098955, 0.9777066104948675, 0.9797801644751427, 0.981706816926438, 0.983468705918318, 0.985049755713391, 0.9864633999337186, 0.98775195577368, 0.9889116560296454, 0.9899670120040618, 0.990905206636989, 0.9917495818066235, 0.9925304457985802, 0.9932262479449127, 0.9938478195789928, 0.9944235100913317, 0.99492535551077, 0.9953723660906454, 0.9957793259101523, 0.9961711663846132, 0.9965493994485328, 0.9968874840572509, 0.9971917602050973, 0.9974656087381589, 0.997709747269105, 0.997931797095766, 0.9981316419397608, 0.9983115022993562, 0.9984710514741825, 0.9986192960291451, 0.9987480658309925, 0.9988662838014647, 0.9989703548260801, 0.9990663438970435, 0.999150408912101, 0.9992260674256529, 0.999296485236659, 0.9993552109689454, 0.9994127144256223, 0.9994621423878219, 0.9995089527026112, 0.999548756837112, 0.9995845805581627, 0.9996168219071083, 0.9996458391211595, 0.9996742797626149, 0.9997022014887343, 0.9997273310422418, 0.999747622491589, 0.9997658847960015, 0.9997823208699728, 0.9997971133365469, 0.9998104265564636, 0.9998224084543886, 0.9998331921625212, 0.999840572351031, 0.9998495396694993, 0.9998576102561207, 0.9998648737840801, 0.9998714109592435, 0.9998796195657, 0.9998846821627014, 0.9998892385000027, 0.9998933392035738, 0.9998970298367879, 0.9998980262578709, 0.9999012481856553, 0.9999041479206612, 0.999909082830976, 0.9999111991014499, 0.9999107785960668, 0.999910400141222, 0.9999077343830522, 0.9999053352006995, 0.9999078262342009, 0.9999100681643522, 0.9999097607526789, 0.999909484082173, 0.9999092350787175, 0.9999113361244172, 0.9999155522143564, 0.9999170215464921, 0.9999183439454143, 0.9999172089556349, 0.9999208377624522, 0.9999194533909689, 0.999918207456634, 0.999919411264542], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.05547007365399096, 0.11756810935146836, 0.17825591085043296, 0.23541105324618786, 0.2874844175187076, 0.33706158288280075, 0.38375019704369234, 0.4256977371096544, 0.4650903833911588, 0.5006058297094225, 0.534707138445935, 0.5653188970693385, 0.5932946669068324, 0.6192013106736041, 0.641423657721831, 0.6629446485008527, 0.6811741682610837, 0.6988421782986801, 0.7145429272892188, 0.7292371838355227, 0.7426360016906752, 0.7544599156405836, 0.7650129004507722, 0.775730407468948, 0.7847586056621887, 0.7927089675639668, 0.8009142450357328, 0.8083701779305028, 0.8139014359751784, 0.8187930894879768, 0.8240552173802935, 0.8278083929314208, 0.8316176150558239, 0.8353196171985848, 0.8379179677434101, 0.8414486542616142, 0.8452051501400462, 0.8485065771911771, 0.8507963268045141, 0.8525966357656289, 0.8541467601691112, 0.8558236633596549, 0.8579636708941712, 0.8596525965381879, 0.861132919998074, 0.8631365978307214, 0.8652736157584926, 0.8661266841054596, 0.8675883344543263, 0.8693035632735774, 0.8696509801484034, 0.8691528437299486, 0.8707258876702068, 0.8721070651400084, 0.8724660710695316, 0.8735215982811025, 0.8734726552263357, 0.8741763239092745, 0.8747821231354403, 0.8758919544024686, 0.8768806545288633, 0.8778528453353897, 0.8775477059919862, 0.8786718887060406, 0.8795402572996684, 0.8796504023151835, 0.8806905037440568, 0.8803477982021058, 0.8809223874989887, 0.8817498411907314, 0.8823470356296402, 0.8824857966280466, 0.8828120240059046, 0.8831126881339286, 0.8838278865174485, 0.8836648714914566, 0.8842079287700217, 0.8852673222635016, 0.8853398111403141, 0.8853094538967646, 0.8856218702352509, 0.8865042779971174, 0.8874815504515472, 0.8879917961970551, 0.8888416423680122, 0.8892311744791025, 0.8896733796497917, 0.8893662979442252, 0.8900685459265345, 0.8901858147807937, 0.8923737586227445, 0.8945280725665694, 0.8966897702044908, 0.8985742629223701, 0.9002713358771209, 0.9018363521388064, 0.9033048724219137, 0.9044800563017102, 0.9053627053213886, 0.9062690117376684, 0.9071701367310703, 0.9079465871487011, 0.9086687770784093, 0.9094540548675563, 0.9099919654576079, 0.9104262273549947, 0.9110601741789831, 0.9118270683292324, 0.9126159588231164, 0.9132761026339523, 0.9139445037598644, 0.9145826858669353, 0.9150583640046394, 0.9155363319622326, 0.9159909171865666, 0.9163390087322171, 0.9166400840920527, 0.9170087081659046, 0.9173038487386215, 0.917507410589157, 0.9177526809195485, 0.9179357736144913, 0.9181504146735994, 0.9183425621181369, 0.9184300455994708, 0.9186318805538309, 0.9187647038877551, 0.9189330730132869, 0.9189737124363557, 0.9190469090108678, 0.9191005788966786, 0.9191244677314083, 0.9191703817451651, 0.9192493549599559, 0.9193194013446079, 0.9193712655682043, 0.919417943369441, 0.9194589238818945, 0.9194469782181027, 0.9194494636605997, 0.9194384640189373, 0.9195272501001008, 0.919521708354398, 0.9194546561183556, 0.9194431372309176, 0.9194205632009734, 0.9194979028240237, 0.9195441239309285, 0.9195735158958929, 0.9195989391557011, 0.9195862285044382, 0.9196236170433016, 0.9196816807907787, 0.9196596664673484, 0.9196032324825112, 0.9196389206235673, 0.9196832469817678, 0.9196865196103983, 0.919664021405006, 0.919643773020153, 0.919675407107445, 0.919641813121098, 0.9196604066583858, 0.9196038986544448, 0.9195540709595575, 0.9195936457442494, 0.9195814644341317, 0.9195816787776161, 0.9195452505930021, 0.919574529891759, 0.9195764671981403, 0.9196015953277239, 0.9196740682780087, 0.9196528152058554, 0.9196957521058271, 0.919647916588392], "moving_var_accuracy_train": [0.028045088935954397, 0.06147800911180222, 0.08974496870074108, 0.11223904282035746, 0.12726300457464912, 0.13832726487053287, 0.14597421767812652, 0.1489873697757828, 0.14979917245139263, 0.14771205995620434, 0.14477603917958637, 0.13992856788226576, 0.13411008146911388, 0.12754114742050127, 0.1201264805187625, 0.11333199210700806, 0.10570820384412062, 0.09855276197821623, 0.09165195577732074, 0.08498891519103291, 0.07869115081465779, 0.07252753452731517, 0.06666505603043399, 0.06137291353655974, 0.056324308592773965, 0.051579126940501756, 0.047288189890854, 0.043254229557169434, 0.0393841634761874, 0.03582020905999304, 0.03266040403739719, 0.029653825658268858, 0.026920373088209913, 0.024453235970739878, 0.022142477587037246, 0.020139680169177606, 0.018355924472829445, 0.016708513599571144, 0.015151270698244311, 0.013710502499368176, 0.01240131113824916, 0.011199883440321511, 0.01016158215790919, 0.009189523874292916, 0.008312907562284285, 0.0075350163297358665, 0.0068354401942198224, 0.00618199368385236, 0.005599937771067925, 0.005098668149873738, 0.0045948594957842094, 0.004136389858526865, 0.0037746715187219155, 0.003433584980401342, 0.003097043444804387, 0.0028033641273250364, 0.002523050142484853, 0.00228680818744658, 0.0020679852874561835, 0.0018875500920105702, 0.0017171081980147322, 0.0015540695348888209, 0.001400973372668103, 0.0012803198898647644, 0.0011708042584937155, 0.0010558335707526226, 0.0009630754930076057, 0.0008680786606599301, 0.00079050621262363, 0.000719640106044058, 0.0006551574606552896, 0.0005897313593512856, 0.0005324160273576447, 0.00048207314198993225, 0.00044179570765307566, 0.0003978451531713804, 0.00036673228688925463, 0.0003367182205216192, 0.0003030488055384103, 0.00027274610974270454, 0.00024945099637721426, 0.00022901398970850794, 0.0002107577417240574, 0.00019521000696579287, 0.00017984743433049414, 0.00016457553465649735, 0.00014876738183327193, 0.00013389374991468886, 0.00012400327877238273, 0.0001150287552555243, 0.00014222251471200623, 0.00016140817025354353, 0.00017320562860556087, 0.00017838253183550707, 0.000178529788486946, 0.00017562019501254098, 0.00017016231766446143, 0.0001631700719926431, 0.00015477494731665868, 0.00014571417742884938, 0.00013663049685137536, 0.0001273247128078055, 0.00011806940319366274, 0.00010924523896826875, 0.00010058735449654234, 9.232698517357284e-05, 8.458483330845329e-05, 7.750820059444007e-05, 7.104492279077437e-05, 6.496914133556587e-05, 5.930548296934315e-05, 5.404987184394927e-05, 4.918131726018664e-05, 4.4706940663849306e-05, 4.0595688252506256e-05, 3.682726716783955e-05, 3.337364390374534e-05, 3.0234067546055635e-05, 2.7359895748260427e-05, 2.474968557031743e-05, 2.2372194016766258e-05, 2.0217899730789417e-05, 1.825971209852012e-05, 1.6485258784723917e-05, 1.4881360919213595e-05, 1.3424263231985492e-05, 1.2111596736555062e-05, 1.0922425173924472e-05, 9.849903506668031e-06, 8.87917247811144e-06, 8.002805281209568e-06, 7.211880294325121e-06, 6.498270253294181e-06, 5.855723058742358e-06, 5.277167357973536e-06, 4.75513407231157e-06, 4.283326351329923e-06, 3.857995322059036e-06, 3.4746270906014374e-06, 3.1291337351474218e-06, 2.8178155380536445e-06, 2.537326077149262e-06, 2.284640064684131e-06, 2.056666262857678e-06, 1.8517233517765228e-06, 1.6671372259146073e-06, 1.5008983328688938e-06, 1.35119311151406e-06, 1.2166802313422893e-06, 1.0952428772036503e-06, 9.859054313697134e-07, 8.874662301607491e-07, 7.988421941063601e-07, 7.189669103904981e-07, 6.471636467192756e-07, 5.82522958215288e-07, 5.244898424520985e-07, 4.7208116561335415e-07, 4.2487464047501355e-07, 3.82388465480138e-07, 3.442135753317039e-07, 3.098440224821933e-07, 2.7891546746512366e-07, 2.5106915697584267e-07, 2.2596309179569042e-07, 2.0336747153524132e-07, 1.8303128240620435e-07, 1.6476788370287314e-07, 1.484510740619676e-07, 1.3362539708809843e-07, 1.2027859602947397e-07, 1.0826233024272331e-07, 9.755461136871788e-08, 8.78163985914804e-08, 7.904872990363621e-08, 7.115689929458763e-08], "duration": 158337.914656, "accuracy_train": [0.5582222669227574, 0.6903599330357143, 0.7376498549395534, 0.772426150389904, 0.7802844323897195, 0.8083882717331118, 0.8341941791251385, 0.8368652163505905, 0.8565563428617571, 0.8590195586471022, 0.8810122219915099, 0.8817541066814323, 0.8887284716454411, 0.8932145668950721, 0.8886358261812477, 0.9102106837163161, 0.8965166386927832, 0.908605970549557, 0.9144657060377446, 0.9181390806686047, 0.9244616829895718, 0.9213721189784054, 0.9217670337878369, 0.9334825393941492, 0.9322494895487264, 0.9325528403585271, 0.9413408213939645, 0.9408747101674971, 0.9329245036798633, 0.9334106220007383, 0.9438505401555003, 0.9358996126914912, 0.9383402979651162, 0.9426414627745479, 0.9363188604535806, 0.9499892935008305, 0.9569411279531194, 0.9571493098814139, 0.9515246126914912, 0.9482922953580657, 0.9486396257267442, 0.945781855620155, 0.9572451997392949, 0.9522668578696014, 0.9540332499884644, 0.958871722441399, 0.9614272232027501, 0.9576840941076044, 0.961265544250646, 0.9687736119762828, 0.9539885494532114, 0.9499649605481728, 0.9709592518572352, 0.9694479051310447, 0.9600561063815062, 0.9655670695482651, 0.9530584899294019, 0.9668673504406607, 0.9653095007382798, 0.9730053828096161, 0.971866420381137, 0.9688444479051311, 0.9650769858573275, 0.9752149951550388, 0.9763299851190477, 0.968262439726375, 0.9758424248454227, 0.9689149233457919, 0.9756102704526578, 0.9760295182147471, 0.9765643024409376, 0.9693567016196014, 0.9727503374169435, 0.9745628720238095, 0.9788418668097084, 0.9687986659053157, 0.9800502232142857, 0.9798177083333334, 0.97191256286914, 0.972215553190753, 0.9787248883928571, 0.9798177083333334, 0.9806322313930418, 0.9820037087024732, 0.9817475818452381, 0.9811201521548542, 0.9788651182978036, 0.9766333359288483, 0.9827012533453304, 0.9832592890596161, 0.9984421502976191, 0.9990466889880952, 0.9993257068452381, 0.9992792038690477, 0.9991861979166666, 0.9993489583333334, 0.9993489583333334, 0.9994652157738095, 0.9993489583333334, 0.9993489583333334, 0.9995582217261905, 0.9994884672619048, 0.9994419642857143, 0.9996047247023809, 0.9994419642857143, 0.9993954613095238, 0.9994419642857143, 0.9996977306547619, 0.9999534970238095, 0.9999302455357143, 0.9999302455357143, 0.9999302455357143, 0.9999069940476191, 0.9999302455357143, 0.9999302455357143, 0.9999302455357143, 0.9999069940476191, 0.9999534970238095, 0.9999069940476191, 0.9999302455357143, 0.9999069940476191, 0.9999302455357143, 0.9999069940476191, 0.9999069940476191, 0.9999302455357143, 0.9998837425595238, 0.9999302455357143, 0.9999069940476191, 0.9999302455357143, 0.9999069940476191, 0.9999069940476191, 0.9999069940476191, 0.9999069940476191, 0.9999302455357143, 0.9999534970238095, 0.9999534970238095, 0.9999302455357143, 0.9999302455357143, 0.9999302455357143, 0.9999302455357143, 0.9999302455357143, 0.9999302455357143, 0.9999302455357143, 0.9999069940476191, 0.9999302455357143, 0.9999302455357143, 0.9999302455357143, 0.9999302455357143, 0.9999534970238095, 0.9999302455357143, 0.9999302455357143, 0.9999302455357143, 0.9999302455357143, 0.9999069940476191, 0.9999302455357143, 0.9999302455357143, 0.9999534970238095, 0.9999302455357143, 0.9999069940476191, 0.9999069940476191, 0.9998837425595238, 0.9998837425595238, 0.9999302455357143, 0.9999302455357143, 0.9999069940476191, 0.9999069940476191, 0.9999069940476191, 0.9999302455357143, 0.9999534970238095, 0.9999302455357143, 0.9999302455357143, 0.9999069940476191, 0.9999534970238095, 0.9999069940476191, 0.9999069940476191, 0.9999302455357143], "end": "2016-02-05 07:37:06.856000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0], "moving_var_accuracy_valid": [0.02769236164061264, 0.05962861981391799, 0.08681284108952576, 0.10753194970108435, 0.12118347213090275, 0.13118618284766015, 0.1376860047932756, 0.1397537693722175, 0.13974441766453327, 0.13712209824274912, 0.13387598173636076, 0.12892210145688232, 0.12307368459319798, 0.11680670385520382, 0.10957052784467, 0.10278185245719965, 0.09549450572767755, 0.08875448236310728, 0.08209765579657163, 0.07583118079595186, 0.06986381759559668, 0.06413568030589055, 0.05872440167093814, 0.05388574611400616, 0.04923074676615346, 0.044876546378861314, 0.04099483094647321, 0.037395666269807726, 0.033931452982838055, 0.03075366215135771, 0.027927505845817824, 0.025261532201694258, 0.022865970540262213, 0.020702716865021054, 0.018693208008503093, 0.016936078933261243, 0.015369472391497205, 0.013930619937512937, 0.01258474452338764, 0.01135544008224811, 0.010241522045019743, 0.00924267787931187, 0.008359626781610768, 0.007549336331928843, 0.0068141249166489585, 0.006168844948697447, 0.005593062064440242, 0.005040305388437591, 0.004555502645274912, 0.004126430469868927, 0.0037148737092462594, 0.003345619597344152, 0.003033327842751583, 0.002747163919304213, 0.0024736074946906873, 0.0022362739844709193, 0.002012668144827317, 0.0018158576768827236, 0.0016375748435162605, 0.0014849028881360963, 0.001345210350781883, 0.0012191957103823434, 0.0010981141295141432, 0.0009996767975339375, 0.0009064956939101369, 0.0008159553118391086, 0.0007440960794962948, 0.0006707434953430199, 0.0006066405215495482, 0.0005521385859024011, 0.0005001344980929333, 0.0004502943398157488, 0.0004062227245527508, 0.0003664140423583996, 0.0003343762166726634, 0.00030117776009368913, 0.0002737141849545438, 0.0002564435976253345, 0.00023084652959815487, 0.0002077701706984627, 0.00018787158934559445, 0.00017609222153485498, 0.00016707855243305533, 0.00015271385367703028, 0.00014394261493794186, 0.00013091397083428227, 0.000119582482467712, 0.00010847292678598357, 0.00010206400416530402, 9.198137160638702e-05, 0.00012586711874552, 0.00015505002398799546, 0.00018160145168934964, 0.0001954031217540735, 0.0002017833191023902, 0.00020364847028621152, 0.0002026925896546686, 0.00019485284505120552, 0.00018237918417353768, 0.00017153378763789355, 0.00016168864515770528, 0.00015094565790128796, 0.0001405451167623067, 0.0001320405559412248, 0.00012144063057310932, 0.00011099381807549623, 0.00010351143344875244, 9.845342984305617e-05, 9.420922076081525e-05, 8.871040734359879e-05, 8.386020719532275e-05, 7.913967409185829e-05, 7.326213389887887e-05, 6.799200082536416e-05, 6.305263027847265e-05, 5.783787676800606e-05, 5.286990644190624e-05, 4.880586916812458e-05, 4.4709253870284334e-05, 4.0611265326196596e-05, 3.709155660831028e-05, 3.368410736195178e-05, 3.0730333684052034e-05, 2.798958607962799e-05, 2.5259507707221872e-05, 2.3100193075713758e-05, 2.0948952110454883e-05, 1.9109190361300233e-05, 1.721313538953657e-05, 1.554004149726554e-05, 1.401196145732549e-05, 1.2615901399415637e-05, 1.1373284129407372e-05, 1.0292086634356065e-05, 9.3070364349458e-06, 8.40054187065458e-06, 7.580097037743846e-06, 6.837201955578011e-06, 6.154766049971066e-06, 5.5393450417936104e-06, 4.986499466664557e-06, 4.558796233873448e-06, 4.10319300899502e-06, 3.7333377293198518e-06, 3.3611981192981456e-06, 3.0296645888196165e-06, 2.780530885579639e-06, 2.5217053135332725e-06, 2.277309770620151e-06, 2.0553958728116593e-06, 1.8513103314302316e-06, 1.6787604238322753e-06, 1.5412269703887718e-06, 1.3914659472747336e-06, 1.2809825043487666e-06, 1.164347044622257e-06, 1.0655957744419e-06, 9.591325878810854e-07, 8.677748523058199e-07, 7.846873408776403e-07, 7.152250460990159e-07, 6.538595447572028e-07, 5.915850669413324e-07, 5.611649508317564e-07, 5.273936483485721e-07, 4.887497557643574e-07, 4.4121023903357595e-07, 3.9708962861838217e-07, 3.693237794650044e-07, 3.4010689753983317e-07, 3.061299861899861e-07, 2.811997936382973e-07, 3.003507709813728e-07, 2.743809315668091e-07, 2.635350348227384e-07, 2.5777566189503807e-07], "accuracy_test": 0.9074338329081633, "start": "2016-02-03 11:38:08.941000", "learning_rate_per_epoch": [0.0013140520313754678, 0.0013140520313754678, 0.0013140520313754678, 0.0013140520313754678, 0.0013140520313754678, 0.0013140520313754678, 0.0013140520313754678, 0.0013140520313754678, 0.0013140520313754678, 0.0013140520313754678, 0.0013140520313754678, 0.0013140520313754678, 0.0013140520313754678, 0.0013140520313754678, 0.0013140520313754678, 0.0013140520313754678, 0.0013140520313754678, 0.0013140520313754678, 0.0013140520313754678, 0.0013140520313754678, 0.0013140520313754678, 0.0013140520313754678, 0.0013140520313754678, 0.0013140520313754678, 0.0013140520313754678, 0.0013140520313754678, 0.0013140520313754678, 0.0013140520313754678, 0.0013140520313754678, 0.0013140520313754678, 0.0013140520313754678, 0.0013140520313754678, 0.0013140520313754678, 0.0013140520313754678, 0.0013140520313754678, 0.0013140520313754678, 0.0013140520313754678, 0.0013140520313754678, 0.0013140520313754678, 0.0013140520313754678, 0.0013140520313754678, 0.0013140520313754678, 0.0013140520313754678, 0.0013140520313754678, 0.0013140520313754678, 0.0013140520313754678, 0.0013140520313754678, 0.0013140520313754678, 0.0013140520313754678, 0.0013140520313754678, 0.0013140520313754678, 0.0013140520313754678, 0.0013140520313754678, 0.0013140520313754678, 0.0013140520313754678, 0.0013140520313754678, 0.0013140520313754678, 0.0013140520313754678, 0.0013140520313754678, 0.0013140520313754678, 0.0013140520313754678, 0.0013140520313754678, 0.0013140520313754678, 0.0013140520313754678, 0.0013140520313754678, 0.0013140520313754678, 0.0013140520313754678, 0.0013140520313754678, 0.0013140520313754678, 0.0013140520313754678, 0.0013140520313754678, 0.0013140520313754678, 0.0013140520313754678, 0.0013140520313754678, 0.0013140520313754678, 0.0013140520313754678, 0.0013140520313754678, 0.0013140520313754678, 0.0013140520313754678, 0.0013140520313754678, 0.0013140520313754678, 0.0013140520313754678, 0.0013140520313754678, 0.0013140520313754678, 0.0013140520313754678, 0.0013140520313754678, 0.0013140520313754678, 0.0013140520313754678, 0.0013140520313754678, 0.00013140520604792982, 0.00013140520604792982, 0.00013140520604792982, 0.00013140520604792982, 0.00013140520604792982, 0.00013140520604792982, 0.00013140520604792982, 0.00013140520604792982, 0.00013140520604792982, 0.00013140520604792982, 0.00013140520604792982, 0.00013140520604792982, 0.00013140520604792982, 0.00013140520604792982, 0.00013140520604792982, 0.00013140520604792982, 0.00013140520604792982, 0.00013140520604792982, 1.3140520422894042e-05, 1.3140520422894042e-06, 1.3140520138676948e-07, 1.314051978340558e-08, 1.314051978340558e-09, 1.3140519228294067e-10, 1.314051905482172e-11, 1.3140519488502589e-12, 1.314051948850259e-13, 1.3140519319096e-14, 1.3140519742612473e-15, 1.3140519742612473e-16, 1.3140520073484718e-17, 1.3140519866689565e-18, 1.3140520125183506e-19, 1.3140520125183506e-20, 1.3140520327131898e-21, 1.3140520579567388e-22, 1.3140520737339569e-23, 1.3140520540124343e-24, 1.3140520786643375e-25, 1.3140521402940958e-26, 1.3140521788126946e-27, 1.314052130664446e-28, 1.3140520704791353e-29, 1.314052032863316e-30, 1.314051985843542e-31, 1.3140519270688245e-32, 1.314051890334626e-33, 1.314051844416878e-34, 1.3140518731154705e-35, 1.3140518731154705e-36, 1.3140518282739196e-37, 1.3140518002479503e-38, 1.3140522206374896e-39, 1.314053621935954e-40, 1.313997569997381e-41, 1.3144179595366784e-42, 1.317220556465328e-43, 1.2611686178923354e-44, 1.401298464324817e-45, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "accuracy_train_first": 0.5582222669227574, "accuracy_train_last": 0.9999302455357143, "batch_size_eval": 1024, "accuracy_train_std": [0.017456893681442025, 0.018841755540888707, 0.018240920141385196, 0.01849002789413666, 0.016717595529233617, 0.017491668347158193, 0.01710735706008546, 0.017051884699790767, 0.016917864889984535, 0.015829377403220362, 0.016695921035888964, 0.016897534368695614, 0.017946548786389162, 0.019221548920572578, 0.01696181416170825, 0.016795151349600086, 0.01690819117870337, 0.01619573245857693, 0.015755226591526682, 0.014735894308689695, 0.014769008453807964, 0.014444587856881214, 0.01319079273106917, 0.015127240255938998, 0.015660436128697847, 0.013745592135212927, 0.014180780077979478, 0.014035975977295527, 0.014096018300431045, 0.01438364158757657, 0.014237165510464427, 0.01428632057089328, 0.013999904501031415, 0.014088001652654595, 0.013116612938243712, 0.012841456088512285, 0.010674974285688577, 0.012837303591021606, 0.012190837306174623, 0.011971923046260281, 0.012511257458750188, 0.012199346324698278, 0.010464094024456796, 0.012399683990472433, 0.011885779836949547, 0.009901040808222924, 0.010242729181386292, 0.011504137536983842, 0.0097317538191237, 0.00948478470693632, 0.011951425482400692, 0.011875547244104542, 0.00895158538253894, 0.009167756863019445, 0.010414855572561563, 0.009297203167964872, 0.010421602893115741, 0.009430449085886552, 0.010280370043253167, 0.008232025265038965, 0.00980740655609471, 0.00955714468106587, 0.010365229867460368, 0.0072888059955302275, 0.00815328606545503, 0.008907452750451957, 0.007829057959143045, 0.00889120400948612, 0.007604453416986778, 0.007731669593293633, 0.0075134840487580306, 0.00821418824828567, 0.007835222622807413, 0.008526672905272637, 0.007296154503062047, 0.006519058067264507, 0.00593309456877631, 0.007266378342494275, 0.008187788359925056, 0.006991116205159766, 0.0075660285097806715, 0.006891075814559469, 0.005822085319956108, 0.006949255035427923, 0.006965867989583572, 0.006891961639272907, 0.007001246345279434, 0.007604023533591764, 0.006794388586793772, 0.006543372407236929, 0.0014304862737418178, 0.0011958138830565283, 0.0008111404177135607, 0.0008269819216786988, 0.000974622949922558, 0.0009687810014880792, 0.0007584405892066635, 0.0007132562151310438, 0.0008434875905513884, 0.0007278867114257128, 0.0006464582763419756, 0.0008041116288035056, 0.0007725654634410412, 0.0006414208623574089, 0.0008823319364309092, 0.0009029244716772508, 0.0008014177799299707, 0.0005831443547239794, 0.0002977643339580008, 0.0003296467373223081, 0.0003296467373223081, 0.0003296467373223081, 0.00035719613782871133, 0.0003296467373223081, 0.0003296467373223081, 0.0003296467373223081, 0.00035719613782871133, 0.0002977643339580008, 0.00035719613782871133, 0.0003296467373223081, 0.00035719613782871133, 0.0003296467373223081, 0.00035719613782871133, 0.00035719613782871133, 0.0003296467373223081, 0.0003813527591810064, 0.0003296467373223081, 0.00035719613782871133, 0.0003296467373223081, 0.00035719613782871133, 0.00035719613782871133, 0.00035719613782871133, 0.00035719613782871133, 0.0003296467373223081, 0.0002977643339580008, 0.0002977643339580008, 0.0003296467373223081, 0.0003296467373223081, 0.0003296467373223081, 0.0003296467373223081, 0.0003296467373223081, 0.0003296467373223081, 0.0003296467373223081, 0.00035719613782871133, 0.0003296467373223081, 0.0003296467373223081, 0.0003296467373223081, 0.0003296467373223081, 0.0002977643339580008, 0.0003296467373223081, 0.0003296467373223081, 0.0003296467373223081, 0.0003296467373223081, 0.00035719613782871133, 0.0003296467373223081, 0.0003296467373223081, 0.0002977643339580008, 0.0003296467373223081, 0.00035719613782871133, 0.00035719613782871133, 0.0003813527591810064, 0.0003813527591810064, 0.0003296467373223081, 0.0003296467373223081, 0.00035719613782871133, 0.00035719613782871133, 0.00035719613782871133, 0.0003296467373223081, 0.0002977643339580008, 0.0003296467373223081, 0.0003296467373223081, 0.00035719613782871133, 0.0002977643339580008, 0.00035719613782871133, 0.00035719613782871133, 0.0003296467373223081], "accuracy_test_std": 0.007027520638250138, "error_valid": [0.4452992634600903, 0.3235495693712349, 0.27555387565888556, 0.2501926651920181, 0.24385530402861444, 0.21674392884036142, 0.1960522755082832, 0.19677440229668675, 0.18037580007530118, 0.17975515342620485, 0.15838108292545183, 0.15917527532003017, 0.15492340455572284, 0.14763889542545183, 0.1585752188441265, 0.14336643448795183, 0.15476015389683728, 0.14214573136295183, 0.14415033179593373, 0.13851450724774095, 0.13677463761295183, 0.13912485881024095, 0.14001023625753017, 0.12781202936746983, 0.1339876105986446, 0.13573777532003017, 0.1252382577183735, 0.12452642601656627, 0.13631724162274095, 0.13718202889683728, 0.1285856315888554, 0.13841302710843373, 0.13409938582454817, 0.13136236351656627, 0.13869687735316272, 0.12677516707454817, 0.12098638695406627, 0.12178057934864461, 0.12859592667545183, 0.13120058358433728, 0.13190212019954817, 0.12908420792545183, 0.12277626129518071, 0.12514707266566272, 0.12554416886295183, 0.11883030167545183, 0.11549322289156627, 0.12619570077183728, 0.11925681240587349, 0.11525937735316272, 0.12722226797816272, 0.1353303840361446, 0.11511671686746983, 0.11546233763177716, 0.12430287556475905, 0.11697865681475905, 0.12696783226656627, 0.11949065794427716, 0.11976568382906627, 0.11411956419427716, 0.11422104433358427, 0.11339743740587349, 0.1251985480986446, 0.11121046686746983, 0.11264442535768071, 0.11935829254518071, 0.10994858339608427, 0.12273655167545183, 0.11390630882906627, 0.11080307558358427, 0.11227821442018071, 0.11626535438629515, 0.11425192959337349, 0.11418133471385539, 0.10973532803087349, 0.11780226374246983, 0.1109045557228916, 0.10519813629518071, 0.11400778896837349, 0.11496376129518071, 0.11156638271837349, 0.10555405214608427, 0.10372299745858427, 0.10741599209337349, 0.10350974209337349, 0.10726303652108427, 0.10634677381400603, 0.11339743740587349, 0.10361122223268071, 0.10875876553087349, 0.08793474679969882, 0.08608310193900603, 0.08385495105421681, 0.08446530261671681, 0.08445500753012047, 0.08407850150602414, 0.08347844503012047, 0.08494328878012047, 0.08669345350150603, 0.08557423051581325, 0.08471973832831325, 0.08506535909262047, 0.08483151355421681, 0.08347844503012047, 0.0851668392319277, 0.08566541556852414, 0.08323430440512047, 0.08127088431852414, 0.0802840267319277, 0.08078260306852414, 0.0800398861069277, 0.0796736751694277, 0.08066053275602414, 0.0801619564194277, 0.0799178157944277, 0.0805281673569277, 0.0806502376694277, 0.0796736751694277, 0.0800398861069277, 0.08066053275602414, 0.0800398861069277, 0.08041639213102414, 0.0799178157944277, 0.07992811088102414, 0.08078260306852414, 0.0795516048569277, 0.0800398861069277, 0.0795516048569277, 0.08066053275602414, 0.08029432181852414, 0.08041639213102414, 0.08066053275602414, 0.08041639213102414, 0.0800398861069277, 0.08005018119352414, 0.0801619564194277, 0.0801619564194277, 0.08017225150602414, 0.08066053275602414, 0.0805281673569277, 0.08066053275602414, 0.0796736751694277, 0.0805281673569277, 0.08114881400602414, 0.08066053275602414, 0.08078260306852414, 0.07980604056852414, 0.0800398861069277, 0.0801619564194277, 0.08017225150602414, 0.0805281673569277, 0.0800398861069277, 0.0797957454819277, 0.08053846244352414, 0.08090467338102414, 0.0800398861069277, 0.0799178157944277, 0.0802840267319277, 0.08053846244352414, 0.08053846244352414, 0.0800398861069277, 0.08066053275602414, 0.08017225150602414, 0.08090467338102414, 0.0808943782944277, 0.08005018119352414, 0.0805281673569277, 0.08041639213102414, 0.08078260306852414, 0.0801619564194277, 0.0804060970444277, 0.08017225150602414, 0.0796736751694277, 0.08053846244352414, 0.0799178157944277, 0.08078260306852414], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.017926996622472324, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 64, "valid_ratio": 0.15, "learning_rate": 0.0013140519732732865, "optimization": "adam", "nb_data_augmentation": 4, "learning_rate_decay_method": "discrete", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 1.760565417047977e-06, "rotation_range": [0, 0], "momentum": 0.7769502844730376}, "accuracy_valid_max": 0.9204483951430723, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.9192173969314759, "accuracy_valid_std": [0.019496320404856923, 0.011833911489812695, 0.011312141763468897, 0.011536862783057395, 0.008933539474124522, 0.011818923394972963, 0.012693460581716602, 0.010083570958214483, 0.008938861258249985, 0.009545348665467385, 0.015632551638412203, 0.008494566667115082, 0.0057760729355161335, 0.01288198648276785, 0.006573610347278939, 0.009659354691202543, 0.00794882777704103, 0.009557898035178865, 0.005428966480873335, 0.0060431166851749795, 0.007432180627951067, 0.008095993174879305, 0.007530926465941735, 0.008986493705968596, 0.006719828313539411, 0.0062489261485441485, 0.008048897198062595, 0.009191884411306523, 0.007720748346564372, 0.006085433810823931, 0.0077014994227042515, 0.005730313439469619, 0.00620339585085589, 0.008714643587214535, 0.010368337854663026, 0.005929591790039258, 0.008940402908898453, 0.00690067188482361, 0.005014196815516272, 0.005167680598315575, 0.007039985989200241, 0.00579236148007147, 0.012590852339614055, 0.008129219119939024, 0.009777179759045143, 0.008329915213408914, 0.010303131269560646, 0.0064551328917299665, 0.007844770372812728, 0.005605960715526924, 0.006854301820437821, 0.006081002108628969, 0.007112660373486317, 0.006319681981576699, 0.007432311011223366, 0.006868277110026657, 0.007994681257126818, 0.007922322205258105, 0.005024902653997056, 0.0074535933618527644, 0.009524130474750316, 0.005996723599288511, 0.004476155310949904, 0.004841971677632894, 0.009945351512208116, 0.010743986693988707, 0.006292600549910358, 0.008217076974921552, 0.008002862442693444, 0.010082826470302562, 0.007577644464881154, 0.012349881420668, 0.00874057195157598, 0.005668820840799866, 0.006993669908730361, 0.007219138461095103, 0.011174339174923362, 0.006533078393729496, 0.006406962710666332, 0.007519947211817574, 0.007127103744791476, 0.008614057453167663, 0.006328347790220351, 0.005417444747630992, 0.009276118218941956, 0.0064362950782465905, 0.01094978179999527, 0.007545961097646956, 0.005221474640832813, 0.007211058886779108, 0.00903797083328173, 0.008828139755669512, 0.010046440672951771, 0.010460339741976252, 0.011012705355763213, 0.011485473885570495, 0.009128891226194596, 0.00940186951700817, 0.008803692126794667, 0.008451772426835964, 0.009048933531066743, 0.008723875354418183, 0.008942287056408349, 0.009102736833584146, 0.01066089814367273, 0.010675181169857731, 0.009293431786172097, 0.01053998523611695, 0.01115737405294718, 0.011196835028263363, 0.011127114330784138, 0.011092993709097707, 0.010852538937230267, 0.011375849174689713, 0.011364543744985061, 0.01181463677901529, 0.011235151814569905, 0.011638396605347427, 0.011212494011733262, 0.011123761318081134, 0.01093257238286185, 0.011058657692065205, 0.01146895986228458, 0.010910910606310099, 0.010583840239745585, 0.011232075340396424, 0.011307775606858686, 0.011546084783396852, 0.010483786924282059, 0.010571280280450648, 0.01080788057997205, 0.010808511787831515, 0.010818904806404323, 0.011454416147603954, 0.009993848008689455, 0.011407243320916464, 0.011185634091166245, 0.011138599770921784, 0.010819535371114342, 0.01174379462381516, 0.010426777804203927, 0.011504470175329337, 0.011476838811698508, 0.010957639652729757, 0.010994409620912774, 0.010628798075834545, 0.010648643479243918, 0.011254940971838125, 0.011228182592827155, 0.010362388929183725, 0.011214159113913541, 0.01168113256592547, 0.011335906180499627, 0.010973070887269077, 0.011011281270489257, 0.011475211825526068, 0.01077217864134703, 0.011628234251895225, 0.010642167416648984, 0.010918616753120532, 0.01118055303756704, 0.010907322965769689, 0.011031057023955737, 0.011172492734584835, 0.011930656382626099, 0.01033397971813045, 0.011288325592659042, 0.010596241297177376, 0.010402065994543243, 0.011046219647152602, 0.01143415576091454, 0.011031057023955737, 0.010687984936522107, 0.010346827739328835, 0.010958692829248372, 0.011186183266469571], "accuracy_valid": [0.5547007365399097, 0.6764504306287651, 0.7244461243411144, 0.7498073348079819, 0.7561446959713856, 0.7832560711596386, 0.8039477244917168, 0.8032255977033133, 0.8196241999246988, 0.8202448465737951, 0.8416189170745482, 0.8408247246799698, 0.8450765954442772, 0.8523611045745482, 0.8414247811558735, 0.8566335655120482, 0.8452398461031627, 0.8578542686370482, 0.8558496682040663, 0.861485492752259, 0.8632253623870482, 0.860875141189759, 0.8599897637424698, 0.8721879706325302, 0.8660123894013554, 0.8642622246799698, 0.8747617422816265, 0.8754735739834337, 0.863682758377259, 0.8628179711031627, 0.8714143684111446, 0.8615869728915663, 0.8659006141754518, 0.8686376364834337, 0.8613031226468373, 0.8732248329254518, 0.8790136130459337, 0.8782194206513554, 0.8714040733245482, 0.8687994164156627, 0.8680978798004518, 0.8709157920745482, 0.8772237387048193, 0.8748529273343373, 0.8744558311370482, 0.8811696983245482, 0.8845067771084337, 0.8738042992281627, 0.8807431875941265, 0.8847406226468373, 0.8727777320218373, 0.8646696159638554, 0.8848832831325302, 0.8845376623682228, 0.875697124435241, 0.883021343185241, 0.8730321677334337, 0.8805093420557228, 0.8802343161709337, 0.8858804358057228, 0.8857789556664157, 0.8866025625941265, 0.8748014519013554, 0.8887895331325302, 0.8873555746423193, 0.8806417074548193, 0.8900514166039157, 0.8772634483245482, 0.8860936911709337, 0.8891969244164157, 0.8877217855798193, 0.8837346456137049, 0.8857480704066265, 0.8858186652861446, 0.8902646719691265, 0.8821977362575302, 0.8890954442771084, 0.8948018637048193, 0.8859922110316265, 0.8850362387048193, 0.8884336172816265, 0.8944459478539157, 0.8962770025414157, 0.8925840079066265, 0.8964902579066265, 0.8927369634789157, 0.893653226185994, 0.8866025625941265, 0.8963887777673193, 0.8912412344691265, 0.9120652532003012, 0.913916898060994, 0.9161450489457832, 0.9155346973832832, 0.9155449924698795, 0.9159214984939759, 0.9165215549698795, 0.9150567112198795, 0.913306546498494, 0.9144257694841867, 0.9152802616716867, 0.9149346409073795, 0.9151684864457832, 0.9165215549698795, 0.9148331607680723, 0.9143345844314759, 0.9167656955948795, 0.9187291156814759, 0.9197159732680723, 0.9192173969314759, 0.9199601138930723, 0.9203263248305723, 0.9193394672439759, 0.9198380435805723, 0.9200821842055723, 0.9194718326430723, 0.9193497623305723, 0.9203263248305723, 0.9199601138930723, 0.9193394672439759, 0.9199601138930723, 0.9195836078689759, 0.9200821842055723, 0.9200718891189759, 0.9192173969314759, 0.9204483951430723, 0.9199601138930723, 0.9204483951430723, 0.9193394672439759, 0.9197056781814759, 0.9195836078689759, 0.9193394672439759, 0.9195836078689759, 0.9199601138930723, 0.9199498188064759, 0.9198380435805723, 0.9198380435805723, 0.9198277484939759, 0.9193394672439759, 0.9194718326430723, 0.9193394672439759, 0.9203263248305723, 0.9194718326430723, 0.9188511859939759, 0.9193394672439759, 0.9192173969314759, 0.9201939594314759, 0.9199601138930723, 0.9198380435805723, 0.9198277484939759, 0.9194718326430723, 0.9199601138930723, 0.9202042545180723, 0.9194615375564759, 0.9190953266189759, 0.9199601138930723, 0.9200821842055723, 0.9197159732680723, 0.9194615375564759, 0.9194615375564759, 0.9199601138930723, 0.9193394672439759, 0.9198277484939759, 0.9190953266189759, 0.9191056217055723, 0.9199498188064759, 0.9194718326430723, 0.9195836078689759, 0.9192173969314759, 0.9198380435805723, 0.9195939029555723, 0.9198277484939759, 0.9203263248305723, 0.9194615375564759, 0.9200821842055723, 0.9192173969314759], "seed": 629355188, "model": "residualv3", "loss_std": [0.28823891282081604, 0.1556372195482254, 0.1329490840435028, 0.12757965922355652, 0.12032733112573624, 0.11634359508752823, 0.11417801678180695, 0.10812518745660782, 0.10587617754936218, 0.10309121012687683, 0.09927698969841003, 0.09834904223680496, 0.09397657960653305, 0.09183985739946365, 0.08885964751243591, 0.08718516677618027, 0.0879049226641655, 0.08205577731132507, 0.08140768855810165, 0.07759173214435577, 0.0756060853600502, 0.07315189391374588, 0.07329300791025162, 0.06916183978319168, 0.0690513625741005, 0.06675179302692413, 0.06448908150196075, 0.06185494735836983, 0.06335868686437607, 0.059686943888664246, 0.05736513435840607, 0.058418288826942444, 0.055374134331941605, 0.054907891899347305, 0.05102643370628357, 0.05185607448220253, 0.050997957587242126, 0.04831545054912567, 0.050815388560295105, 0.047099750488996506, 0.046554964035749435, 0.04577745124697685, 0.04626253619790077, 0.04307473450899124, 0.042898304760456085, 0.04282631725072861, 0.04159924015402794, 0.04218913987278938, 0.03870313987135887, 0.040298886597156525, 0.04051334410905838, 0.04097704961895943, 0.03885407745838165, 0.03858061879873276, 0.03613738715648651, 0.0375676192343235, 0.037005119025707245, 0.03682394325733185, 0.037879154086112976, 0.03413816913962364, 0.034752070903778076, 0.03325434401631355, 0.03492463380098343, 0.03268716484308243, 0.034120794385671616, 0.03664125129580498, 0.03154870122671127, 0.03356194496154785, 0.03371935710310936, 0.030379360541701317, 0.03423400968313217, 0.03185093775391579, 0.033602505922317505, 0.031358491629362106, 0.03231065347790718, 0.028806107118725777, 0.02842722274363041, 0.03244004398584366, 0.02855946496129036, 0.029670272022485733, 0.029273593798279762, 0.02928791381418705, 0.026870103552937508, 0.030169324949383736, 0.027180176228284836, 0.029094839468598366, 0.028801042586565018, 0.02803300879895687, 0.029764285311102867, 0.028046254068613052, 0.022525480017066002, 0.01255360059440136, 0.01121481228619814, 0.009130868129432201, 0.009153001941740513, 0.008364371955394745, 0.007702786475419998, 0.008052767254412174, 0.007490925490856171, 0.007228412665426731, 0.0069479672238230705, 0.00692443922162056, 0.0072007072158157825, 0.006032883655279875, 0.00609664898365736, 0.006422388833016157, 0.006006027106195688, 0.005943576339632273, 0.00559961935505271, 0.005838945508003235, 0.005172456614673138, 0.005149670876562595, 0.005177690647542477, 0.005076449830085039, 0.004541163798421621, 0.005112893879413605, 0.005197508726269007, 0.0050229523330926895, 0.004811672493815422, 0.0054018739610910416, 0.005479956977069378, 0.0047333440743386745, 0.00521820317953825, 0.005211688112467527, 0.004887377843260765, 0.0049233571626245975, 0.005006071645766497, 0.004759142175316811, 0.004894048906862736, 0.0051050372421741486, 0.005025128833949566, 0.005627900827676058, 0.004787666257470846, 0.004804459400475025, 0.004785202909260988, 0.004843472968786955, 0.004897439852356911, 0.0049128481186926365, 0.004776600748300552, 0.004775110632181168, 0.00498961890116334, 0.004986263345927, 0.0045003038831055164, 0.004983924329280853, 0.0050561754032969475, 0.004740988835692406, 0.004829504061490297, 0.00514169130474329, 0.005206153262406588, 0.0050921449437737465, 0.005306248087435961, 0.005249828100204468, 0.004961800761520863, 0.0047560497187078, 0.005000504665076733, 0.0048590837977826595, 0.0052132499404251575, 0.0051450724713504314, 0.00466958899050951, 0.005033770110458136, 0.005168503616005182, 0.0055139451287686825, 0.005195703823119402, 0.005220917519181967, 0.004652782343327999, 0.00510025629773736, 0.004829886835068464, 0.005539058241993189, 0.005162710323929787, 0.00481093255802989, 0.004766303114593029, 0.004921145737171173, 0.004727891180664301, 0.005228480324149132, 0.00510863633826375, 0.005110413301736116]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:38 2016", "state": "available"}], "summary": "bdae651a515c786d889129ec06e73470"}