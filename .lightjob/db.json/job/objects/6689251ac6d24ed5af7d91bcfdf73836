{"content": {"hp_model": {"f0": 16, "f1": 64, "f2": 32, "f3": 32, "nonlin": "very_leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.6823716163635254, 1.2412837743759155, 1.05665922164917, 0.949331521987915, 0.8722115755081177, 0.8204580545425415, 0.7783859372138977, 0.7442337274551392, 0.7118651866912842, 0.6897104978561401, 0.6670072674751282, 0.6464559435844421, 0.6323297023773193, 0.6130224466323853, 0.6022487282752991, 0.5914117097854614, 0.5783424377441406, 0.5707868933677673, 0.5579925179481506, 0.5512415170669556, 0.5435173511505127, 0.5331019759178162, 0.5268769264221191, 0.5211123824119568, 0.5157246589660645, 0.5135822892189026, 0.5069610476493835, 0.49998971819877625, 0.4970472753047943, 0.493691623210907, 0.48800167441368103, 0.4860996901988983, 0.4817541539669037, 0.47822123765945435, 0.47569799423217773, 0.4698846936225891, 0.4680909514427185, 0.4670949876308441, 0.46236076951026917, 0.45899099111557007, 0.4596026539802551, 0.45444345474243164, 0.45433109998703003, 0.45239588618278503, 0.45061713457107544, 0.44551900029182434, 0.4447495937347412, 0.446071594953537, 0.44079843163490295, 0.4401873052120209, 0.43873369693756104, 0.43974292278289795, 0.43399447202682495, 0.4344639480113983, 0.4318234622478485, 0.434263676404953, 0.4281305968761444, 0.4261353313922882, 0.4269140362739563, 0.4262385070323944, 0.4255285859107971, 0.42177674174308777, 0.4203170835971832, 0.42061084508895874, 0.4173741042613983, 0.41995537281036377, 0.41717299818992615, 0.41643843054771423, 0.41491326689720154, 0.4125840365886688, 0.4150635302066803, 0.41213470697402954, 0.40886175632476807, 0.4133785367012024, 0.40831226110458374, 0.40623149275779724, 0.40657931566238403, 0.40720346570014954, 0.4037361443042755, 0.4022446572780609, 0.4066474735736847, 0.401964008808136, 0.40064868330955505, 0.4016536474227905, 0.3992619812488556, 0.40279966592788696, 0.40155795216560364, 0.3979867696762085, 0.39531210064888, 0.39713791012763977, 0.3988414406776428, 0.39562350511550903, 0.3965785503387451, 0.3938649892807007, 0.3946041762828827, 0.39270034432411194, 0.3941800892353058, 0.3916992247104645, 0.3924199044704437, 0.3905434012413025, 0.3897055387496948, 0.3913240134716034, 0.39157554507255554, 0.38890784978866577, 0.38884368538856506, 0.3890579044818878, 0.38596951961517334, 0.38497066497802734, 0.38559240102767944, 0.38651883602142334, 0.3844110071659088, 0.3852035105228424, 0.3796031177043915, 0.3860500156879425, 0.3814185857772827, 0.3801575303077698, 0.3807486295700073, 0.3826930820941925, 0.3828391134738922, 0.38026243448257446, 0.37927401065826416, 0.381725937128067, 0.37854239344596863, 0.37949076294898987, 0.37779709696769714, 0.3785068690776825, 0.3777221739292145, 0.3769315779209137, 0.3771546185016632, 0.3756377100944519, 0.3734055161476135, 0.3766328990459442, 0.37440556287765503, 0.3743857741355896, 0.3775267004966736, 0.3715175688266754, 0.3731045126914978, 0.37549206614494324, 0.37087488174438477, 0.37327706813812256, 0.3733445107936859, 0.3731718063354492, 0.37156999111175537, 0.36950236558914185, 0.37176358699798584, 0.3666840195655823, 0.3692467510700226, 0.37080028653144836, 0.3698584735393524, 0.3726710379123688, 0.3700062334537506, 0.3692049980163574, 0.36861902475357056, 0.3704327344894409, 0.36776456236839294, 0.3693722188472748, 0.3657413721084595, 0.36604419350624084, 0.3649834394454956, 0.3685106039047241, 0.36548957228660583, 0.3681085705757141, 0.3649285137653351, 0.3660392761230469, 0.3641044497489929, 0.3656189441680908, 0.36369776725769043, 0.36485713720321655, 0.36495062708854675, 0.36403709650039673, 0.3616079092025757, 0.36328309774398804, 0.3644670844078064, 0.3649137020111084, 0.36530381441116333, 0.3643726408481598, 0.3613939583301544, 0.360010027885437, 0.3610765337944031, 0.36451032757759094, 0.36302417516708374, 0.36113932728767395, 0.3622303009033203, 0.3609095513820648, 0.35945242643356323, 0.3611631393432617, 0.3611920475959778, 0.36318203806877136, 0.3590080440044403, 0.3606341481208801, 0.35724711418151855, 0.35840097069740295, 0.36035045981407166, 0.3623170852661133, 0.35842272639274597, 0.3586581349372864, 0.3583574891090393, 0.35754939913749695, 0.35627877712249756, 0.3579530417919159, 0.35750070214271545, 0.35525673627853394, 0.3581266403198242, 0.3584805428981781, 0.35467326641082764, 0.355779767036438, 0.3596497178077698, 0.35598593950271606, 0.35684457421302795, 0.3556482791900635, 0.3563615679740906], "moving_avg_accuracy_train": [0.045559362310815796, 0.09793456851005905, 0.14693927648578808, 0.1909641702138704, 0.24316779255779342, 0.28441853346051355, 0.3219061304132348, 0.3687589331272638, 0.40955926806988996, 0.4437991764707728, 0.47616800504577933, 0.5046352826431302, 0.5280845760784757, 0.554384674441466, 0.5719271847566402, 0.5931876186780785, 0.6125658253930041, 0.63200576731499, 0.6467117527662614, 0.6559393041011616, 0.6697352005701982, 0.6838187111863973, 0.6956543117766446, 0.6955108846537512, 0.7090811264186272, 0.7199694778202307, 0.7312469596016997, 0.7373986989612973, 0.7421563395337445, 0.7484674581297426, 0.7584340299039776, 0.7689569276126386, 0.7748460313341635, 0.7752707120693334, 0.7837824921163221, 0.7865890486159763, 0.7867388276264256, 0.7779935069196615, 0.7891645514957832, 0.797044765867938, 0.8040555064969065, 0.8078098345213114, 0.8115939545148705, 0.802167076107265, 0.8085493643206433, 0.8120733752342323, 0.8179972763193823, 0.8206624904902311, 0.8236352426582162, 0.8275060623904105, 0.8258798800912457, 0.8296888523015287, 0.8339818826479262, 0.8354691276322568, 0.8383659664551791, 0.8360069279780351, 0.8362409535091814, 0.8375419991812423, 0.828393479259196, 0.8298007254408807, 0.8274845171282895, 0.8272859137220312, 0.8294523445847395, 0.8364770670407173, 0.8410348897928563, 0.8460018495781055, 0.8447996154612418, 0.8399883544561384, 0.8430672791964788, 0.8458475399603858, 0.8465414217040113, 0.8464196146030546, 0.8374436370447886, 0.8403913111656144, 0.8461874946254648, 0.8436505536312184, 0.8487883886623842, 0.8465866324085027, 0.848768744419129, 0.8482544331347466, 0.8485963068086289, 0.8532883585982347, 0.8550235122756021, 0.85565316244962, 0.8535558759492593, 0.845272807042041, 0.8467771858524179, 0.8501537539043116, 0.8449166854883174, 0.8488866372041128, 0.850115951894785, 0.8517872741794668, 0.8487394717453702, 0.8514432998972913, 0.8577382045767206, 0.8606463527882992, 0.8630988169061562, 0.8649269993074564, 0.8632060887247175, 0.8647375506405901, 0.8667834003661213, 0.8632658459918846, 0.8648986495144662, 0.8669442328084515, 0.8687716313195498, 0.8733266913615059, 0.8747716103863464, 0.8683118082958937, 0.8724642050664335, 0.8745721538208182, 0.871152197055846, 0.8671163828043035, 0.870021747453921, 0.8681241827756663, 0.8737439098065622, 0.8764929716106051, 0.877620793975892, 0.8778761232738547, 0.8637136630724844, 0.8630556992834623, 0.865278926593488, 0.8652964071403499, 0.8649497769064275, 0.8611878295948453, 0.8624236797737421, 0.8661211138739334, 0.8664196403487586, 0.8683248584498184, 0.8699188632955897, 0.8701724362568207, 0.8707376543040345, 0.8690754185836255, 0.8652356924840761, 0.868284732045423, 0.8605929689391643, 0.8633111881086014, 0.8653693936563607, 0.8702975179385191, 0.8689486165795601, 0.8707917434551958, 0.8723369137421513, 0.8699934140611347, 0.874489291139702, 0.8774660120580315, 0.8763207678739615, 0.8763830482928684, 0.8778871636947554, 0.8769324634232404, 0.8658674160530925, 0.8667236271921077, 0.8696586423683381, 0.8691894674079901, 0.8699227007555539, 0.8720474905671801, 0.866694828881263, 0.868079812874366, 0.868840342366968, 0.8721961985995403, 0.8772179536992116, 0.8767436183295211, 0.8817433975253157, 0.8798338449982364, 0.8713497856645257, 0.8694909374448339, 0.8714031372184845, 0.86946952381848, 0.8727786658499302, 0.8719212272653525, 0.8652932757620546, 0.868276103126556, 0.8692172547821082, 0.8727373111825888, 0.8697101381231154, 0.8746552668276458, 0.8749792842570998, 0.8723348137782798, 0.8748901440006733, 0.8776826925043528, 0.8756886040805603, 0.8753609852515002, 0.8750794663682845, 0.8770348465471629, 0.8743706875488235, 0.8737260345550615, 0.8755309088069658, 0.8781176548990913, 0.8812290852379013, 0.8852475342261082, 0.8786261295945642, 0.878818271906887, 0.8775661361094707, 0.8798771356629754, 0.8761570506900241, 0.8764122699415736, 0.8758957026465672, 0.87836687824639, 0.8758045724008023, 0.8758205277264751, 0.8789546243920427, 0.8765543609711571, 0.8813497434706177, 0.8816666922559387, 0.876965078747399, 0.8777394917325243, 0.8794592044559588, 0.8725263531491134, 0.8730935609785819, 0.8690567058324126, 0.8559412729406626, 0.8595967390159208, 0.8579651836255413], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.0454538368317018, 0.09765526461314003, 0.14616242852268446, 0.1894882566888648, 0.24096061771990301, 0.2814525465823253, 0.3175901067772554, 0.36300306522603587, 0.401517594217363, 0.4340647358686689, 0.46492789942412127, 0.4917230365807302, 0.5142129187754433, 0.5394985522555797, 0.5559779727265579, 0.5757840369938118, 0.5933225560636324, 0.6109169523774498, 0.6238100145286356, 0.6318796143088293, 0.6443020044404464, 0.6573407128348807, 0.6676645352355041, 0.6671849087074356, 0.6798519776540867, 0.6892675940829551, 0.6994781358324156, 0.705107141243903, 0.7095831905796031, 0.7149291119207392, 0.7235328568845387, 0.7326312078207083, 0.7378604745574627, 0.7381618412112797, 0.7465450111470041, 0.7487581858135537, 0.748549835935135, 0.7404334865660192, 0.7499415309817064, 0.7563572458711111, 0.7618568046047831, 0.7645297226006301, 0.7677685154478714, 0.7580282673481145, 0.7642050681076554, 0.7670184891959863, 0.7719137317673515, 0.773457857225782, 0.7750652376835501, 0.7786584056508878, 0.7773997751761605, 0.7810178960678065, 0.7845334939798512, 0.7851950906944414, 0.7877518888068497, 0.7848535471588455, 0.7853275356752651, 0.785918846725585, 0.7777246171264451, 0.7787497187138759, 0.7771026565281057, 0.7763111008715301, 0.7777897893536543, 0.7834176310602315, 0.7872549259831993, 0.7917613846273492, 0.7914388441503071, 0.7860141038900204, 0.7883872263982321, 0.7909879333517824, 0.7912767588513181, 0.7911490182542134, 0.7831968435089878, 0.7856930006490529, 0.7906639161338465, 0.7872759707271637, 0.792494509761375, 0.7907019076651322, 0.7927496456448538, 0.7920239857208052, 0.7929172137959385, 0.7970506971414952, 0.7980292505918789, 0.7985253536765163, 0.7968294389319822, 0.7894516825858473, 0.7911057177817054, 0.7933307422949505, 0.7896212561678652, 0.7929093717502503, 0.7932957865612946, 0.7947801374514754, 0.7924293827424724, 0.7944996857144602, 0.8003692178018996, 0.8024960584934717, 0.8042291686644558, 0.8061907703409319, 0.8048645973448658, 0.805709762939822, 0.8078173034474212, 0.8047324445729351, 0.8067525524311536, 0.8078504346598003, 0.8093104848496938, 0.8133234605081733, 0.8149656885349162, 0.8092616942766806, 0.8125175655116631, 0.8133644182413702, 0.8108855453648989, 0.8071164738385747, 0.8092685077423677, 0.8076976551797272, 0.8126684593624323, 0.8155054114307071, 0.8154940151483292, 0.8164441430009661, 0.8042422938834749, 0.803655284468772, 0.8057353097210664, 0.806320593696249, 0.8065593809945457, 0.8036247284278472, 0.8042775276106648, 0.807470292074749, 0.8077273573702861, 0.8090786744852907, 0.8102826528575447, 0.8111781274531908, 0.8117285893690164, 0.81017116482594, 0.8062476277390538, 0.808877171252799, 0.8012670861399438, 0.8034731264905578, 0.8045112677665772, 0.809151679017028, 0.8082924281428101, 0.8096117993157279, 0.8105367086631461, 0.8091952460046629, 0.8130356105702358, 0.8156800975647935, 0.8147489418651666, 0.8142332850186348, 0.8152869837663947, 0.8150919697365173, 0.8042419230358325, 0.805616311775171, 0.8086649027362081, 0.8086702886391686, 0.8093790257296944, 0.8109263864756256, 0.8058959066194787, 0.8071037204191273, 0.8077767432849706, 0.8105636985686724, 0.815546014705781, 0.8151765541896456, 0.8201227718410877, 0.8184126113267832, 0.8116182663744513, 0.8092214999779701, 0.8105526797053236, 0.8083223305506799, 0.8108838286703107, 0.8101606566466532, 0.804638019775286, 0.8077967940402273, 0.808789222599307, 0.811893057540054, 0.809153341159166, 0.8131230550477675, 0.8129694174176142, 0.8103489981720877, 0.8124483933473488, 0.8154914869945417, 0.8136930795413224, 0.814447971511889, 0.8142269954262121, 0.8158399051174012, 0.8132783520115345, 0.8128569550633932, 0.8146271648187254, 0.8168714442894132, 0.8187021603649899, 0.8223192548988975, 0.8168970585599866, 0.8162774828338826, 0.81474021365441, 0.8169843657641195, 0.8134537273334154, 0.8140157694155858, 0.8132856086073104, 0.8151075207322721, 0.8134411951933069, 0.8127837873644883, 0.8159418850022865, 0.8140822597550699, 0.8186566850879212, 0.8184664978008912, 0.8149963422790851, 0.8160653615715079, 0.8177497527957577, 0.8112555392349319, 0.8113782204394507, 0.8078375545984424, 0.7960852962206766, 0.7990485086449192, 0.7973563130891621], "moving_var_accuracy_train": [0.01868089944751365, 0.04150126952248153, 0.05896429520431172, 0.07051158709380102, 0.08798739205686339, 0.09450326547638715, 0.09770081825635553, 0.10768740253015753, 0.1119006682600161, 0.11126194337972216, 0.10956541861161337, 0.10590234979469372, 0.1002609390787768, 0.09646010173602584, 0.0895837485758452, 0.08469342817301125, 0.07960371941508762, 0.07504454955095058, 0.06948648866869259, 0.06330416913456731, 0.058686693055569984, 0.05460313119150232, 0.05040355104433885, 0.0453633810819612, 0.04248440612777972, 0.039302971281204954, 0.03651730851106674, 0.03320617273429586, 0.030089271755215637, 0.027438816541088838, 0.025588927863358764, 0.02402661746270536, 0.02193608960022074, 0.01974410382374009, 0.01842174703748093, 0.0166504631682046, 0.014985618755151881, 0.01417538258801409, 0.013880974461507963, 0.0130517570223172, 0.012188935677585505, 0.011096896920060446, 0.010116083305185288, 0.009904269303272777, 0.009280444798493152, 0.00846416819491569, 0.007933584812023898, 0.007204156630009946, 0.006563276266079288, 0.006041797848063658, 0.005461418283088342, 0.005045850878467879, 0.0047071367766169025, 0.004256330177745964, 0.0039062222364652783, 0.003565685575648562, 0.0032096099296267605, 0.0029038834152311783, 0.00336675382458477, 0.003047901518469088, 0.002791394755148029, 0.002512610269450023, 0.002303590046651075, 0.0025173515722376467, 0.002452580149173129, 0.0024293583398303656, 0.0021994308076930887, 0.0021878218190568475, 0.0020543576351612835, 0.0019184905208830389, 0.0017309747156619658, 0.0015580107768243606, 0.002127323257280371, 0.001992789976055603, 0.0020958726627522475, 0.0019442100229516127, 0.0019873651599237128, 0.0018322582193448969, 0.0016918869128526826, 0.0015250788664426026, 0.0013736228782783863, 0.0014343987404176442, 0.0013180556909326135, 0.0011898182559141185, 0.0011104239263040627, 0.0016168646083691927, 0.0014755465479782733, 0.0014306027994620677, 0.0015343844898600867, 0.0015227906905058018, 0.001384112552933543, 0.001270841161253652, 0.0012273589422238497, 0.0011704192280775532, 0.001410009729577697, 0.0013451246908044974, 0.0012647434439684347, 0.0011683493576034058, 0.001078168220947113, 0.000991459779250317, 0.0009299833112203917, 0.0009483436790797595, 0.0008775037372619767, 0.0008274130626494643, 0.000774726224249799, 0.0008839907496972408, 0.0008143817936226322, 0.0011085050016907175, 0.001152836091981552, 0.001077543514343405, 0.0010750541013775756, 0.0011141388612964007, 0.0010786952688919913, 0.0010032325073762339, 0.001187141243754649, 0.001136443186601214, 0.0010342467175298638, 0.0009314087832304605, 0.002643445415506002, 0.002382997121084381, 0.002189182066024347, 0.00197026660954758, 0.0017743213212644438, 0.0017242594173140847, 0.0015655794065647893, 0.0015320606362356282, 0.0013796566351176095, 0.001274359675719306, 0.001169791371182454, 0.0010533909272842157, 0.00095092707752386, 0.0008807016180833063, 0.0009253229249510177, 0.0009164604126758428, 0.0013572833485534946, 0.0012880534527760085, 0.0011973739981898511, 0.0012962142788344516, 0.0011829686648368184, 0.0010952458484703525, 0.0010072092245645287, 0.0009559162189023968, 0.0010422407933624324, 0.0010177645208567755, 0.0009277923269414141, 0.0008350480039024858, 0.0007719044717919814, 0.0007029170980886611, 0.0017345428480123604, 0.0015676864408422882, 0.001488446623520382, 0.001341583087459102, 0.001212263458991009, 0.0011316696987842235, 0.001276361613020753, 0.00116598907766904, 0.0010545958158841952, 0.0010504921737790253, 0.001172405174930794, 0.001057189603824169, 0.0011764507715020622, 0.0010916232120349331, 0.001630274255832748, 0.0014983446805841342, 0.0013814187842948657, 0.0012769266528914753, 0.0012477877764611254, 0.001129625807151917, 0.0014120308966073442, 0.0013509031387243773, 0.0012237847228006781, 0.0012129234240836888, 0.0011741050722633311, 0.0012767832461763396, 0.0011500498072100152, 0.001097983843509167, 0.0010469528720675495, 0.0010124425291694148, 0.0009469857740296066, 0.0008532532035010383, 0.0007686411590853978, 0.0007261886479724146, 0.0007174494716910686, 0.0006494447218632587, 0.0006138183892636178, 0.000612657848443395, 0.0006385210523784584, 0.0007200003375780022, 0.0010425872974718945, 0.0009386608357383681, 0.0008589053486610728, 0.0008210812842216586, 0.0008635244456532945, 0.0007777582328852182, 0.0007023839855291282, 0.0006871059665826514, 0.0006774840711413816, 0.0006097379551789994, 0.0006371672168432028, 0.0006253018755656603, 0.0007697329278542833, 0.0006936637438615032, 0.0008232438957284921, 0.00074631694539942, 0.0006983019575197615, 0.0010610516069532385, 0.000957841968754208, 0.0010087235671191745, 0.0024559824298492202, 0.0023306460769105717, 0.0021215392261464006], "duration": 222964.782911, "accuracy_train": [0.45559362310815804, 0.5693114243032484, 0.5879816482673496, 0.5871882137666113, 0.7130003936531008, 0.6556752015849945, 0.6592945029877261, 0.7904341575535253, 0.7767622825535253, 0.7519583520787191, 0.767487462220838, 0.7608407810192875, 0.7391282169965855, 0.7910855597083795, 0.7298097775932079, 0.7845315239710224, 0.7869696858273348, 0.8069652446128645, 0.779065621827704, 0.7389872661152639, 0.7938982687915282, 0.810570306732189, 0.8021747170888703, 0.6942200405477114, 0.8312133023025102, 0.8179646404346622, 0.8327442956349206, 0.7927643531976744, 0.7849751046857696, 0.8052675254937246, 0.848133175872093, 0.8636630069905868, 0.8278479648278886, 0.7790928386858619, 0.8603885125392212, 0.8118480571128645, 0.7880868387204688, 0.6992856205587855, 0.8897039526808784, 0.8679666952173312, 0.8671521721576227, 0.841598786740956, 0.8456510344569029, 0.717325170438815, 0.8659899582410484, 0.8437894734565338, 0.8713123860857327, 0.84464941802787, 0.8503900121700813, 0.8623434399801587, 0.8112442393987633, 0.8639696021940754, 0.872619155765504, 0.848854332491233, 0.8644375158614802, 0.8147755816837394, 0.8383471832894979, 0.8492514102297897, 0.7460567999607789, 0.8424659410760429, 0.8066386423149685, 0.825498483065707, 0.848950222349114, 0.8996995691445183, 0.8820552945621077, 0.8907044876453488, 0.8339795084094684, 0.7966870054102067, 0.8707776018595422, 0.8708698868355482, 0.8527863573966408, 0.8453233506944444, 0.756659839020395, 0.8669203782530455, 0.8983531457641197, 0.8208180846830011, 0.8950289039428755, 0.8267708261235696, 0.8684077525147655, 0.8436256315753045, 0.8516731698735696, 0.895516824704688, 0.8706398953719084, 0.8613200140157806, 0.8346802974460132, 0.7707251868770765, 0.8603165951458103, 0.8805428663713547, 0.7977830697443706, 0.8846162026462717, 0.8611797841108343, 0.8668291747416021, 0.8213092498385014, 0.875777753264581, 0.9143923466915835, 0.8868196866925065, 0.8851709939668696, 0.8813806409191584, 0.8477178934800664, 0.878520707883444, 0.8851960478959026, 0.8316078566237541, 0.8795938812177003, 0.885354482454319, 0.8852182179194352, 0.9143222317391103, 0.8877758816099114, 0.8101735894818198, 0.909835776001292, 0.8935436926102805, 0.8403725861710963, 0.8307940545404209, 0.8961700293004798, 0.8510461006713732, 0.9243214530846253, 0.9012345278469915, 0.8877711952634736, 0.8801740869555187, 0.7362515212601514, 0.8571340251822629, 0.8852879723837209, 0.8654537320621077, 0.861830104801126, 0.8273303037906055, 0.8735463313838132, 0.8993980207756552, 0.8691063786221853, 0.8854718213593578, 0.8842649069075305, 0.8724545929078996, 0.875824616728959, 0.8541152970999446, 0.8306781575881322, 0.8957260880975452, 0.791367100982835, 0.8877751606335363, 0.8838932435861941, 0.9146506364779439, 0.8568085043489295, 0.8873798853359173, 0.8862434463247508, 0.848901916931986, 0.914952184846807, 0.9042565003229974, 0.8660135702173312, 0.8769435720630308, 0.8914242023117387, 0.8683401609796051, 0.7662819897217608, 0.8744295274432448, 0.8960737789544113, 0.8649668927648578, 0.8765218008836286, 0.8911705988718162, 0.8185208737080103, 0.8805446688122923, 0.8756851078003876, 0.9023989046926911, 0.9224137495962532, 0.872474600002307, 0.9267414102874677, 0.862647872254522, 0.7949932516611297, 0.852761303467608, 0.88861293518134, 0.8520670032184385, 0.9025609441329827, 0.8642042800041528, 0.8056417122323736, 0.895121549407069, 0.8776876196820783, 0.904417818786914, 0.8424655805878553, 0.9191614251684201, 0.8778954411221853, 0.8485345794689, 0.897888116002215, 0.9028156290374677, 0.8577418082664268, 0.8724124157899593, 0.8725457964193429, 0.894633268157069, 0.8503932565637689, 0.8679241576112033, 0.8917747770741048, 0.9013983697282208, 0.9092319582871908, 0.9214135751199704, 0.8190334879106681, 0.8805475527177926, 0.8662969139327242, 0.9006761316445183, 0.8426762859334626, 0.8787092432055187, 0.8712465969915099, 0.9006074586447952, 0.8527438197905132, 0.8759641256575305, 0.9071614943821521, 0.8549519901831857, 0.9245081859657622, 0.8845192313238279, 0.8346505571705426, 0.8847092085986527, 0.8949366189668696, 0.8101306913875047, 0.8781984314437985, 0.8327250095168882, 0.7379023769149132, 0.8924959336932448, 0.8432811851121264], "end": "2016-02-03 05:41:14.293000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0], "moving_var_accuracy_valid": [0.018594461544506745, 0.04125991695184248, 0.058310429811584914, 0.06937353330699489, 0.08628081552728127, 0.0924091007015484, 0.09492149994297334, 0.10399038110431197, 0.10694166348469478, 0.10578134500325681, 0.10377602428478658, 0.0998602362334813, 0.09442636582032184, 0.0907379985827158, 0.08410834041597788, 0.0792280280102074, 0.0740736220696488, 0.069452324897513, 0.06400317187247075, 0.058188920650736185, 0.05375887057490144, 0.04991305476676712, 0.045880981070726926, 0.04129495333811208, 0.03860954972559377, 0.0355464792476546, 0.03293012778824642, 0.029922286326724768, 0.027110372852952883, 0.024656545442528124, 0.02285711074489427, 0.02131641957822413, 0.01943088469583886, 0.017488613622995273, 0.016372250104236853, 0.01477910837275508, 0.013301588222526105, 0.01256430554400743, 0.01212150116710282, 0.011279803628271695, 0.0104240295818305, 0.009445927039160154, 0.008595742347210207, 0.00859002000989257, 0.008074393817510905, 0.007338192479742198, 0.006820043830260537, 0.006159498358116843, 0.005566801569729295, 0.0051263191171298855, 0.004627944561464108, 0.004282967294396784, 0.0039659054230696605, 0.0035732542726775035, 0.003274763794698284, 0.0030228908740054595, 0.002722623772628193, 0.0024535082341894474, 0.002812465999281275, 0.0025406768987341257, 0.0023110245334548576, 0.002085561123326484, 0.001896683687638334, 0.0019920687393431232, 0.0019253853563413223, 0.0019156203463100941, 0.00172499460291306, 0.0018173454046459358, 0.0016862962581321768, 0.001578539722243162, 0.0014214365315414839, 0.0012794397373286734, 0.0017206295122032347, 0.0016046437651939922, 0.0016665693954872312, 0.0016032160226464782, 0.0016879927672461177, 0.0015481142910005966, 0.0014310419394028902, 0.0012926769863909338, 0.001170589995299698, 0.0012073021568816838, 0.0010951900428908343, 0.0009878861030370317, 0.0009149826341198842, 0.001313365969034309, 0.0012066518639931125, 0.0011305432843546769, 0.001141331541862562, 0.001124503724424423, 0.00101339719963773, 0.0009318871577605812, 0.0008884328713016194, 0.000838164973733847, 0.001064411138689802, 0.0009986810867667632, 0.0009258460158730055, 0.0008678923445200896, 0.0007969317234075346, 0.00072366729501286, 0.0006912761084321144, 0.0007077956860683637, 0.0006737436392910538, 0.0006172173838537524, 0.0005746813644814484, 0.0006621489907532378, 0.0006202063077042931, 0.0008510056314157259, 0.0008613113457632341, 0.0007816346470992234, 0.0007587744790286492, 0.0008107501326607161, 0.0007713563687023209, 0.0007164289317940736, 0.0008671660866198431, 0.000852884151297059, 0.0007675969050446214, 0.0006989619009673681, 0.0019690318078448158, 0.0017752298475368828, 0.0016366454082348393, 0.0014760638833958045, 0.0013289706694206752, 0.0012735832736636811, 0.0011500602672551, 0.0011267979448376553, 0.001014712893449416, 0.0009296761256122118, 0.0008497545883386906, 0.0007719960022678499, 0.0006975234769280334, 0.0006496012701016188, 0.0007231884325409972, 0.0007131000811030138, 0.0011630106318168158, 0.0010905090948919705, 0.0009911578211835527, 0.0010858427882249823, 0.0009839033179860816, 0.0009011796488148114, 0.000818760799641804, 0.000753080418254569, 0.0008105079763976815, 0.000792396981937379, 0.0007209607421761707, 0.0006512577858089289, 0.0005961245366873153, 0.0005368543572652251, 0.0015426805422020864, 0.0014054129876432603, 0.0013485168505083898, 0.001213665426529107, 0.0010968196582475808, 0.0010086866199252604, 0.0011355695061806316, 0.0010351418831341607, 0.0009357043328222759, 0.0009120379773202252, 0.0010442454463990437, 0.0009410494114159861, 0.0010671300917733208, 0.0009867389234581663, 0.001303533141093857, 0.001224880229418188, 0.001118340561675024, 0.001051276621672105, 0.0010052004130567435, 0.0009093871717332781, 0.0010929441166768034, 0.001073450398720821, 0.0009749695888526317, 0.0009641767520219869, 0.0009353134894491392, 0.0009836097957204239, 0.0008854612568409735, 0.0008587145043578065, 0.0008125101948392126, 0.0008146029458655599, 0.0007622510755891542, 0.0006911547250152715, 0.0006224787263877139, 0.0005836441527963256, 0.0005843337263442679, 0.0005274985322009672, 0.0005029514621817354, 0.0004979874290465139, 0.000478352378286239, 0.000548267496262636, 0.0007580426648756537, 0.000685693265111485, 0.0006383927073717434, 0.0006198794048581961, 0.0006700801339276597, 0.0006059151422540688, 0.0005501218412821351, 0.0005249839312736649, 0.0004974753053625662, 0.000451617440306838, 0.0004962179224849505, 0.00047771998477722405, 0.000618276290431999, 0.0005567742022261299, 0.0006094745961132017, 0.0005588123567300312, 0.0005284656852239962, 0.0008551924046641161, 0.0007698086202991843, 0.0008056545896484148, 0.0019681293234834977, 0.001850342041975818, 0.0016910795699685492], "accuracy_test": 0.6441625478316326, "start": "2016-01-31 15:45:09.510000", "learning_rate_per_epoch": [0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573, 0.0014023453695699573], "accuracy_train_first": 0.45559362310815804, "accuracy_train_last": 0.8432811851121264, "batch_size_eval": 1024, "accuracy_train_std": [0.01659233591043956, 0.018520409298073286, 0.01897176106256768, 0.020952035098978253, 0.02103107427171636, 0.017594919839853694, 0.02539504961862429, 0.020122627748386315, 0.025537998067116048, 0.022074294594758768, 0.02355694011846122, 0.022244037042208488, 0.02289204287633822, 0.022440761560846782, 0.019194296992208763, 0.022678046147992646, 0.02203623709012935, 0.02250413699500601, 0.022694024092285874, 0.023607370116005437, 0.022803012233579327, 0.023316076036766803, 0.020293347246860223, 0.020712793011442313, 0.019685078129499007, 0.019291860064479985, 0.022476459872732026, 0.018375807165239264, 0.017115937238120996, 0.021594859354196066, 0.022174194296417236, 0.02243451477967459, 0.01716966724844364, 0.02020791225029116, 0.020905994335600118, 0.019651747832663113, 0.02136276006341341, 0.01692928553080832, 0.018765719476862393, 0.02037125407958015, 0.019298103791451703, 0.02417274098193487, 0.018630784355181666, 0.02114546243803853, 0.019195004685189296, 0.020604516930449356, 0.020711389373262726, 0.02017351690441875, 0.019529084100576806, 0.019360014597836095, 0.019025697463959646, 0.020435425120659913, 0.01789065777439402, 0.022066668967405768, 0.017524882532587726, 0.022140519320162727, 0.019887820521531763, 0.020083020177925768, 0.023682790419284207, 0.01997440107125442, 0.019481307720750844, 0.018816270458791914, 0.018500427375808776, 0.019006795221739533, 0.01715681198506671, 0.017034585349268827, 0.018827513699951136, 0.02332919399907234, 0.021062906748684537, 0.021523677936025106, 0.018648333182676585, 0.019321495394170878, 0.01981580699012224, 0.01834964342003007, 0.01647903972303651, 0.020700269468231967, 0.016825322362843908, 0.021239541325064743, 0.020208780728145324, 0.022345759397794786, 0.016528971474612766, 0.017842452855845185, 0.017893356994508777, 0.01995275958196032, 0.016234759473628124, 0.019597576491106303, 0.020950666292262266, 0.018611842395214847, 0.021692948467542255, 0.016040067480817682, 0.018963398505872142, 0.018137209771925046, 0.019787671448342835, 0.016455604531247288, 0.017510213819896984, 0.018606492875447952, 0.018689898515505034, 0.018108234978251064, 0.01991007713553362, 0.02040971882081218, 0.017924335318661437, 0.01897364144368908, 0.014917139336474622, 0.02050079339192944, 0.01571639342973974, 0.016492172966412505, 0.0155522374297664, 0.02085581546171216, 0.016004978924276884, 0.015318635317698587, 0.019305243777589572, 0.020285112211045227, 0.015822963102784415, 0.017672781970167376, 0.015011342288772376, 0.016723138419291735, 0.020099342279035384, 0.016106252101489846, 0.01982560043642884, 0.01826085829809522, 0.015935913346425995, 0.022194626963263124, 0.016721501601406775, 0.02043066106378411, 0.01898631133922001, 0.01767612461227199, 0.016204134040208772, 0.019351542608490858, 0.016913973444609706, 0.019483638603318563, 0.017353557061439286, 0.017892880350319714, 0.019508369914688208, 0.01676707992131924, 0.01791848974624773, 0.01604806799548411, 0.01793777841721382, 0.017148741538744294, 0.01802751309697507, 0.01694341734181426, 0.014741518595768322, 0.019136086882002024, 0.015675544764522317, 0.01634499955198007, 0.017927429450333283, 0.017866568541783444, 0.016350487865060964, 0.01635903402377445, 0.019915465411634994, 0.019351467340558922, 0.01627765957540539, 0.019637060832982157, 0.018810016418707824, 0.01435760284405985, 0.017301319664011428, 0.016034901509530772, 0.017610260754303348, 0.016075331107061785, 0.016778445733614714, 0.01881399141979669, 0.013814684966490439, 0.016992724410026704, 0.01843003642772224, 0.020592517885128465, 0.01718312386951475, 0.0185661806471933, 0.01646645525628229, 0.016306054430914943, 0.01987473255455632, 0.014402042894443993, 0.018395220832721156, 0.01764994144700543, 0.01860364934262593, 0.014599948825845864, 0.01699111792474572, 0.015394009126475492, 0.014591376134620696, 0.016747818075760514, 0.016396503296364277, 0.014731120737044159, 0.019028290054650863, 0.016176677568606544, 0.01579043148098787, 0.015576280976418687, 0.01791687720172799, 0.016246222618326936, 0.016467120178440306, 0.01637797227818406, 0.018355412064521928, 0.017152022355762978, 0.0148751236406928, 0.016779322577933447, 0.019016209023809825, 0.015397076934316712, 0.01648340939205104, 0.01591449144571292, 0.018460577989854888, 0.01731229786946157, 0.016961925678227803, 0.014959995121817352, 0.015694248899146057, 0.0178700252419098, 0.02049279264678397, 0.017385679195182153, 0.013715897977180109, 0.017577520565498755, 0.015707104420398077, 0.016476399150218826, 0.017825885270843018, 0.014775329654237133, 0.01476046202679403], "accuracy_test_std": 0.013692403164423517, "error_valid": [0.5454616316829819, 0.43253188535391573, 0.41727309629141573, 0.42057928981551207, 0.295788133000753, 0.3541200936558735, 0.3571718514683735, 0.22828030873493976, 0.2518516448606928, 0.27301098926957834, 0.2573036285768072, 0.2671207290097892, 0.2833781414721386, 0.23293074642319278, 0.2957072430346386, 0.24596138460090367, 0.2488307723079819, 0.23073348079819278, 0.2601524261106928, 0.2954939876694277, 0.243896484375, 0.22531091161521077, 0.23942106315888556, 0.3371317300451807, 0.2061444018260542, 0.22599185805722888, 0.20862698842243976, 0.24423181005271077, 0.25013236539909633, 0.2369575960090362, 0.1990334384412651, 0.1854836337537651, 0.21507612481174698, 0.25912585890436746, 0.17800645943147586, 0.2313232421875, 0.25332531297063254, 0.33261365775602414, 0.1644860692771084, 0.18590132012424698, 0.18864716679216864, 0.21141401543674698, 0.20308234892695776, 0.3296339655496988, 0.18020372505647586, 0.2076607210090362, 0.18402908509036142, 0.21264501364834332, 0.2104683381965362, 0.1890030826430723, 0.23392789909638556, 0.18641901590737953, 0.18382612481174698, 0.20885053887424698, 0.18923692818147586, 0.24123152767319278, 0.21040656767695776, 0.2087593538215362, 0.29602344926581325, 0.21202436699924698, 0.23772090314382532, 0.23081290003765065, 0.20890201430722888, 0.1659317935805723, 0.1782094197100903, 0.16768048757530118, 0.2114640201430723, 0.26280855845256024, 0.19025467102786142, 0.1856057040662651, 0.20612381165286142, 0.21000064711972888, 0.28837272919804224, 0.19184158509036142, 0.16459784450301207, 0.2432155379329819, 0.16053863893072284, 0.2254315112010542, 0.18882071253765065, 0.21450695359563254, 0.19904373352786142, 0.16574795274849397, 0.19316376835466864, 0.19700971856174698, 0.21843379376882532, 0.27694812452936746, 0.1940079654555723, 0.18664403708584332, 0.24376411897590367, 0.1774975880082832, 0.20322648013930722, 0.19186070453689763, 0.2287274096385542, 0.18686758753765065, 0.1468049934111446, 0.17836237528237953, 0.18017283979668675, 0.1761548145707832, 0.20707095961972888, 0.1866837467055723, 0.17321483198418675, 0.22303128529743976, 0.17506647684487953, 0.18226862528237953, 0.1775490634412651, 0.15055975856551207, 0.17025425922439763, 0.24207425404743976, 0.15817959337349397, 0.1790139071912651, 0.21142431052334332, 0.22680516989834332, 0.17136318712349397, 0.2064400178840362, 0.14259430299322284, 0.1589620199548193, 0.1846085513930723, 0.17500470632530118, 0.3055743481739458, 0.2016278002635542, 0.1755444630082832, 0.1884118505271084, 0.1912915333207832, 0.22278714467243976, 0.18984727974397586, 0.16379482774849397, 0.18995905496987953, 0.17875947147966864, 0.17888154179216864, 0.18076260118599397, 0.1833172533885542, 0.20384565606174698, 0.22906420604292166, 0.16745693712349397, 0.267223679875753, 0.17667251035391573, 0.18614546074924698, 0.14908461972891573, 0.19944082972515065, 0.17851386012801207, 0.1811391072100903, 0.20287791792168675, 0.1524011083396084, 0.16051951948418675, 0.19363145943147586, 0.19040762660015065, 0.1752297275037651, 0.18666315653237953, 0.29340849727033136, 0.1820141895707832, 0.16389777861445776, 0.19128123823418675, 0.1842423404555723, 0.17514736681099397, 0.23937841208584332, 0.1820259553840362, 0.18616605092243976, 0.16435370387801207, 0.13961314006024095, 0.1881485904555723, 0.13536126929593373, 0.19697883330195776, 0.2495308381965362, 0.21234939759036142, 0.17746670274849397, 0.21175081184111444, 0.16606268825301207, 0.1963478915662651, 0.2450657120670181, 0.16377423757530118, 0.18227892036897586, 0.16017242799322284, 0.21550410626882532, 0.1511495199548193, 0.1884133212537651, 0.21323477503765065, 0.16865705007530118, 0.15712067018072284, 0.20249258753765065, 0.17875800075301207, 0.18776178934487953, 0.16964390766189763, 0.2097756259412651, 0.19093561746987953, 0.1694409473832832, 0.16293004047439763, 0.1648213949548193, 0.14512689429593373, 0.23190270849021077, 0.1892986987010542, 0.19909520896084332, 0.16281826524849397, 0.21832201854292166, 0.18092585184487953, 0.19328583866716864, 0.1684952701430723, 0.20155573465737953, 0.19313288309487953, 0.15563523625753017, 0.20265436746987953, 0.14017348691641573, 0.18324518778237953, 0.21623505741716864, 0.17431346479668675, 0.16709072618599397, 0.2471923828125, 0.18751764871987953, 0.22402843797063254, 0.3096850291792168, 0.17428257953689763, 0.21787344691265065], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.06223527749877464, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 64, "valid_ratio": 0.15, "learning_rate": 0.0014023454039472749, "optimization": "rmsprop", "nb_data_augmentation": 3, "learning_rate_decay_method": "none", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 4.2961217411307555e-05, "rotation_range": [0, 0], "momentum": 0.7829265547198823}, "accuracy_valid_max": 0.8646387307040663, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.7821265530873494, "accuracy_valid_std": [0.014856872087822616, 0.020649613795722832, 0.013977962479011636, 0.010246509341332739, 0.010193026510418843, 0.005047804497135288, 0.010861765135632388, 0.012351709900982869, 0.01853634045185436, 0.011212716245413878, 0.010361341422873581, 0.008595378432848364, 0.01235527965315132, 0.01174898750733124, 0.012938157857836367, 0.011084583765186366, 0.013970990351315252, 0.016623849218550285, 0.013725932433661544, 0.012159714601734994, 0.012897928654172159, 0.01906590766354311, 0.016965255665474462, 0.015417672927218691, 0.009427324255584436, 0.010873024779532918, 0.00982095176953066, 0.006295916108139129, 0.013590395122981972, 0.011862196701171265, 0.012496989673038502, 0.008434959726274551, 0.010428705667920124, 0.010999611663561865, 0.011696103790665262, 0.014414121219339661, 0.014586479171659637, 0.015351401726041743, 0.008422087145432055, 0.018115640294925826, 0.017353211455109284, 0.0064603181373140495, 0.01146580454046806, 0.013695254688480695, 0.012123846233656817, 0.015478488559268182, 0.009100381190808093, 0.010366465117388668, 0.0170933219430459, 0.011892425974831469, 0.013660223196089573, 0.013828198986508024, 0.015371783700548104, 0.01156759557228049, 0.009803921587925132, 0.010248586881290805, 0.01259181907502874, 0.011945666619236333, 0.014559342859806326, 0.015537397846379546, 0.01730590645653147, 0.013524387466627366, 0.013750598025450408, 0.01404212034142843, 0.007523092040413327, 0.011794187595441258, 0.015097248327416903, 0.010840841712543533, 0.006775717835189462, 0.010763483068301093, 0.020118195838145152, 0.014755881316208823, 0.00855830475336192, 0.008124115133899228, 0.01672167545838614, 0.01302306513773327, 0.014068708905519879, 0.014143320490068188, 0.018627818350960102, 0.01140929482049951, 0.008148089952917131, 0.01027204309464861, 0.0107146415999638, 0.018049113942921212, 0.015217314056224947, 0.010539101923595813, 0.008741893222392725, 0.01857460459685931, 0.012645032392918448, 0.013688844195065553, 0.018563594531326184, 0.015758294081940273, 0.01962385546177926, 0.012506518157512138, 0.01496634194467922, 0.011997456374866338, 0.013102511212861613, 0.007800553948554881, 0.012762993974717642, 0.010955153612560007, 0.006584130166928837, 0.010247846782890384, 0.012639075084059434, 0.009888695216061654, 0.011623214830590246, 0.012017747227301697, 0.006928642449103512, 0.013305725024281003, 0.012180806426026345, 0.016260014165741923, 0.007750762271696183, 0.01202907098692154, 0.015547904529778177, 0.018980782055570224, 0.008077629132489732, 0.008241299201422442, 0.013884792238839002, 0.009368773997572747, 0.010902848063313968, 0.008612359812943572, 0.011049275535208814, 0.015101209322441049, 0.008907405941473245, 0.013973693307248446, 0.012431412742474578, 0.007503913753992456, 0.007628281340313894, 0.014767060582260873, 0.01342572095551078, 0.01380388274119289, 0.010960616038888262, 0.009135626957485323, 0.009345729848967416, 0.010032353372264904, 0.01618045148943219, 0.015115328942837805, 0.01470931183714962, 0.010210741585361333, 0.015913646752739978, 0.01055234354023628, 0.011897541168649918, 0.015635454387990213, 0.011020300673238599, 0.010352403774788214, 0.013751106976141621, 0.015917978313688286, 0.01090714597219378, 0.0065508864644631135, 0.01727235204857335, 0.015896207808500623, 0.01803219898640807, 0.016549580349294896, 0.010738459998914374, 0.0140723669422486, 0.017798855039542493, 0.018308905730392074, 0.01473936507790975, 0.01100133857729543, 0.010301006460097604, 0.006317747175945284, 0.007911621322747614, 0.013862106656748543, 0.015981301529629788, 0.011778996814323277, 0.013308460782780954, 0.011234042051152691, 0.007146436225996683, 0.0058516323264401525, 0.014120488143273692, 0.008170419119460254, 0.01016002798461453, 0.00713099272471331, 0.009808132946851664, 0.009845957743750978, 0.008761524325211103, 0.01547106773372517, 0.009465620527644882, 0.004934696256439003, 0.006675989080918037, 0.011659575137173434, 0.011710046075735824, 0.0051209373781860275, 0.013208374943717365, 0.01315575771964198, 0.009273415579256122, 0.014735854122702549, 0.014514128585077767, 0.012359806130193397, 0.01701731534989474, 0.009520784993118214, 0.009254898508786179, 0.008485186364484913, 0.014556805115441376, 0.011566771005835433, 0.015090645983115374, 0.009000519823827302, 0.010899614589200532, 0.01487735763634552, 0.010680505280362698, 0.00992532826687146, 0.007874902708881174, 0.012872494628629343, 0.011732123250134294, 0.006135966791962898, 0.010409831015323669, 0.013968919279365711, 0.01221123786660112, 0.013953499215629208, 0.017502306433035804, 0.012988260063072079, 0.010588856068672579], "accuracy_valid": [0.4545383683170181, 0.5674681146460843, 0.5827269037085843, 0.5794207101844879, 0.704211866999247, 0.6458799063441265, 0.6428281485316265, 0.7717196912650602, 0.7481483551393072, 0.7269890107304217, 0.7426963714231928, 0.7328792709902108, 0.7166218585278614, 0.7670692535768072, 0.7042927569653614, 0.7540386153990963, 0.7511692276920181, 0.7692665192018072, 0.7398475738893072, 0.7045060123305723, 0.756103515625, 0.7746890883847892, 0.7605789368411144, 0.6628682699548193, 0.7938555981739458, 0.7740081419427711, 0.7913730115775602, 0.7557681899472892, 0.7498676346009037, 0.7630424039909638, 0.8009665615587349, 0.8145163662462349, 0.784923875188253, 0.7408741410956325, 0.8219935405685241, 0.7686767578125, 0.7466746870293675, 0.6673863422439759, 0.8355139307228916, 0.814098679875753, 0.8113528332078314, 0.788585984563253, 0.7969176510730422, 0.6703660344503012, 0.8197962749435241, 0.7923392789909638, 0.8159709149096386, 0.7873549863516567, 0.7895316618034638, 0.8109969173569277, 0.7660721009036144, 0.8135809840926205, 0.816173875188253, 0.791149461125753, 0.8107630718185241, 0.7587684723268072, 0.7895934323230422, 0.7912406461784638, 0.7039765507341867, 0.787975633000753, 0.7622790968561747, 0.7691870999623494, 0.7910979856927711, 0.8340682064194277, 0.8217905802899097, 0.8323195124246988, 0.7885359798569277, 0.7371914415474398, 0.8097453289721386, 0.8143942959337349, 0.7938761883471386, 0.7899993528802711, 0.7116272708019578, 0.8081584149096386, 0.8354021554969879, 0.7567844620670181, 0.8394613610692772, 0.7745684887989458, 0.8111792874623494, 0.7854930464043675, 0.8009562664721386, 0.834252047251506, 0.8068362316453314, 0.802990281438253, 0.7815662062311747, 0.7230518754706325, 0.8059920345444277, 0.8133559629141567, 0.7562358810240963, 0.8225024119917168, 0.7967735198606928, 0.8081392954631024, 0.7712725903614458, 0.8131324124623494, 0.8531950065888554, 0.8216376247176205, 0.8198271602033133, 0.8238451854292168, 0.7929290403802711, 0.8133162532944277, 0.8267851680158133, 0.7769687147025602, 0.8249335231551205, 0.8177313747176205, 0.8224509365587349, 0.8494402414344879, 0.8297457407756024, 0.7579257459525602, 0.841820406626506, 0.8209860928087349, 0.7885756894766567, 0.7731948301016567, 0.828636812876506, 0.7935599821159638, 0.8574056970067772, 0.8410379800451807, 0.8153914486069277, 0.8249952936746988, 0.6944256518260542, 0.7983721997364458, 0.8244555369917168, 0.8115881494728916, 0.8087084666792168, 0.7772128553275602, 0.8101527202560241, 0.836205172251506, 0.8100409450301205, 0.8212405285203314, 0.8211184582078314, 0.819237398814006, 0.8166827466114458, 0.796154343938253, 0.7709357939570783, 0.832543062876506, 0.732776320124247, 0.8233274896460843, 0.813854539250753, 0.8509153802710843, 0.8005591702748494, 0.8214861398719879, 0.8188608927899097, 0.7971220820783133, 0.8475988916603916, 0.8394804805158133, 0.8063685405685241, 0.8095923733998494, 0.8247702724962349, 0.8133368434676205, 0.7065915027296686, 0.8179858104292168, 0.8361022213855422, 0.8087187617658133, 0.8157576595444277, 0.824852633189006, 0.7606215879141567, 0.8179740446159638, 0.8138339490775602, 0.8356462961219879, 0.860386859939759, 0.8118514095444277, 0.8646387307040663, 0.8030211666980422, 0.7504691618034638, 0.7876506024096386, 0.822533297251506, 0.7882491881588856, 0.8339373117469879, 0.8036521084337349, 0.7549342879329819, 0.8362257624246988, 0.8177210796310241, 0.8398275720067772, 0.7844958937311747, 0.8488504800451807, 0.8115866787462349, 0.7867652249623494, 0.8313429499246988, 0.8428793298192772, 0.7975074124623494, 0.8212419992469879, 0.8122382106551205, 0.8303560923381024, 0.7902243740587349, 0.8090643825301205, 0.8305590526167168, 0.8370699595256024, 0.8351786050451807, 0.8548731057040663, 0.7680972915097892, 0.8107013012989458, 0.8009047910391567, 0.837181734751506, 0.7816779814570783, 0.8190741481551205, 0.8067141613328314, 0.8315047298569277, 0.7984442653426205, 0.8068671169051205, 0.8443647637424698, 0.7973456325301205, 0.8598265130835843, 0.8167548122176205, 0.7837649425828314, 0.8256865352033133, 0.832909273814006, 0.7528076171875, 0.8124823512801205, 0.7759715620293675, 0.6903149708207832, 0.8257174204631024, 0.7821265530873494], "seed": 814242457, "model": "residualv3", "loss_std": [0.3219590187072754, 0.1932268887758255, 0.17547179758548737, 0.163535475730896, 0.1471402943134308, 0.14873908460140228, 0.13813073933124542, 0.13898980617523193, 0.12956643104553223, 0.1284656971693039, 0.1269882172346115, 0.11169052124023438, 0.11517011374235153, 0.10985962301492691, 0.11060641705989838, 0.10961806029081345, 0.10286008566617966, 0.10260026156902313, 0.10179836302995682, 0.09734059870243073, 0.09683117270469666, 0.09652471542358398, 0.09454426914453506, 0.09396219998598099, 0.09412416815757751, 0.0968330129981041, 0.09507912397384644, 0.088013656437397, 0.0949743390083313, 0.09080714732408524, 0.09076826274394989, 0.09457652270793915, 0.08762580156326294, 0.08997102826833725, 0.08243270963430405, 0.08389788120985031, 0.08193690329790115, 0.08104642480611801, 0.07899889349937439, 0.07739454507827759, 0.07743702083826065, 0.07709592580795288, 0.07983101904392242, 0.08565600216388702, 0.07816430181264877, 0.07948776334524155, 0.07633863389492035, 0.07793989777565002, 0.0753520280122757, 0.07687053084373474, 0.0745500847697258, 0.08515811711549759, 0.07262933999300003, 0.07226922363042831, 0.07337279617786407, 0.0803743228316307, 0.07155298441648483, 0.07201385498046875, 0.0719364583492279, 0.07564644515514374, 0.07601536065340042, 0.06955704838037491, 0.06836634874343872, 0.06726720184087753, 0.06589755415916443, 0.0704280212521553, 0.0672125592827797, 0.07021009176969528, 0.06907039135694504, 0.06587106734514236, 0.07150053977966309, 0.06610266864299774, 0.06819258630275726, 0.07332106679677963, 0.06655808538198471, 0.06443020701408386, 0.06657590717077255, 0.06637535989284515, 0.06826629489660263, 0.06346208602190018, 0.08679252862930298, 0.06775183975696564, 0.06810294091701508, 0.06492944061756134, 0.0687747523188591, 0.07646649330854416, 0.06706739962100983, 0.06372056901454926, 0.06390202790498734, 0.0662108063697815, 0.0666353702545166, 0.07056756317615509, 0.0703953430056572, 0.06163551285862923, 0.07223355025053024, 0.061721790581941605, 0.06192133575677872, 0.06806784123182297, 0.06642946600914001, 0.06221367046236992, 0.06607465445995331, 0.06525643169879913, 0.0673268660902977, 0.06358933448791504, 0.062472470104694366, 0.061502955853939056, 0.062965989112854, 0.07020257413387299, 0.06307847052812576, 0.0633016973733902, 0.06172436475753784, 0.06455004215240479, 0.062194883823394775, 0.06431830674409866, 0.058573223650455475, 0.05934756621718407, 0.060566436499357224, 0.06019456684589386, 0.0701838880777359, 0.06609649956226349, 0.06042807549238205, 0.07124373316764832, 0.06445761770009995, 0.06151840463280678, 0.060654159635305405, 0.062362026423215866, 0.05937673896551132, 0.060851044952869415, 0.06069035083055496, 0.05881589278578758, 0.06022209674119949, 0.06179741397500038, 0.05798182263970375, 0.06435339897871017, 0.06789316982030869, 0.055524468421936035, 0.06097634881734848, 0.058298349380493164, 0.057614587247371674, 0.05882037431001663, 0.06142115592956543, 0.0605618879199028, 0.05608973279595375, 0.061729881912469864, 0.06260179728269577, 0.056695789098739624, 0.05565059930086136, 0.06082870066165924, 0.058561258018016815, 0.06951994448900223, 0.06668936461210251, 0.0606771819293499, 0.057835791260004044, 0.05780372396111488, 0.0635647103190422, 0.059911470860242844, 0.05782395228743553, 0.06083322688937187, 0.057374484837055206, 0.05826674774289131, 0.05502115935087204, 0.06284043192863464, 0.05605808645486832, 0.059851597994565964, 0.056177400052547455, 0.06080931797623634, 0.060279730707407, 0.06082402914762497, 0.060713451355695724, 0.05522013083100319, 0.06178487837314606, 0.05575162172317505, 0.053461991250514984, 0.06307805329561234, 0.06858991831541061, 0.058145612478256226, 0.05472148582339287, 0.057183887809515, 0.06137201562523842, 0.06101522967219353, 0.054637134075164795, 0.0590817891061306, 0.05694008618593216, 0.057246215641498566, 0.054581742733716965, 0.05454126000404358, 0.05449574068188667, 0.05633334815502167, 0.05360954999923706, 0.05434325337409973, 0.05743654817342758, 0.05398561805486679, 0.05658965930342674, 0.06463402509689331, 0.06449221074581146, 0.05472303554415703, 0.056511204689741135, 0.05797986686229706, 0.05544431880116463, 0.06736423075199127, 0.05411762371659279, 0.05289354547858238, 0.055299632251262665, 0.05768369138240814, 0.05687856301665306, 0.05576585978269577, 0.06057975813746452, 0.05630077049136162, 0.05514419451355934, 0.05915967747569084, 0.058827999979257584]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:33 2016", "state": "available"}], "summary": "67060158b44faf774482bdb8a51c6c41"}