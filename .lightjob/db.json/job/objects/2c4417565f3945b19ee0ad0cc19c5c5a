{"content": {"hp_model": {"f0": 64, "f1": 32, "f2": 16, "f3": 64, "nonlin": "rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.5748255252838135, 1.26960027217865, 1.0895859003067017, 0.9776327610015869, 0.8987197875976562, 0.8370727300643921, 0.7859779596328735, 0.7440276741981506, 0.7095775604248047, 0.6805537343025208, 0.6546760201454163, 0.630307674407959, 0.6097837090492249, 0.5921449661254883, 0.5743846893310547, 0.5571743249893188, 0.5434378981590271, 0.5285888314247131, 0.5153309106826782, 0.5026624202728271, 0.4923209249973297, 0.4813379943370819, 0.4692932665348053, 0.46132925152778625, 0.4518965780735016, 0.44230449199676514, 0.4322884976863861, 0.42535075545310974, 0.4161326289176941, 0.408901184797287, 0.4006185531616211, 0.394295334815979, 0.3873090147972107, 0.380888968706131, 0.3737906515598297, 0.3662913739681244, 0.36173877120018005, 0.3555857241153717, 0.3480421304702759, 0.34182146191596985, 0.3345143795013428, 0.3314564824104309, 0.3245890140533447, 0.32007506489753723, 0.3160669505596161, 0.3093970715999603, 0.3034084141254425, 0.29917067289352417, 0.2962327301502228, 0.2908779978752136, 0.2854936420917511, 0.2801676392555237, 0.2789992690086365, 0.2729809284210205, 0.26889798045158386, 0.2652474343776703, 0.263419508934021, 0.2585005760192871, 0.25406739115715027, 0.25169679522514343, 0.24737821519374847, 0.2443777620792389, 0.24196261167526245, 0.23613376915454865, 0.2340724915266037, 0.22957634925842285, 0.2281089425086975, 0.2226121872663498, 0.22185571491718292, 0.2202901691198349, 0.21471409499645233, 0.21039853990077972, 0.2091108113527298, 0.20644763112068176, 0.20365218818187714, 0.20300139486789703, 0.1985115110874176, 0.1958446055650711, 0.19470016658306122, 0.1911792904138565, 0.18753936886787415, 0.18738186359405518, 0.18403008580207825, 0.18167558312416077, 0.18006852269172668, 0.17907209694385529, 0.1755315065383911, 0.17273369431495667, 0.17139333486557007, 0.1675252467393875, 0.16701023280620575, 0.16449326276779175, 0.16180899739265442, 0.16204427182674408, 0.15966273844242096, 0.15718920528888702, 0.1570800542831421, 0.15656256675720215, 0.1513948291540146, 0.15118256211280823, 0.14883455634117126, 0.14843301475048065, 0.1465708613395691, 0.14350095391273499, 0.14128221571445465, 0.1399199664592743, 0.13937890529632568, 0.13790209591388702, 0.13742835819721222, 0.13573358952999115, 0.13211239874362946, 0.13265275955200195, 0.13029706478118896, 0.12861818075180054, 0.12785184383392334, 0.1265983283519745, 0.12347662448883057, 0.12409340590238571, 0.12246078252792358, 0.12082912027835846, 0.12066519260406494, 0.11847221106290817, 0.11798598617315292, 0.11565897613763809, 0.11405061185359955, 0.11508788913488388, 0.11358243972063065, 0.109805166721344, 0.11108947545289993, 0.10991857200860977, 0.10977061092853546, 0.10667483508586884, 0.10813198983669281, 0.10551485419273376, 0.10519470274448395, 0.10257899016141891, 0.10188460350036621, 0.10240842401981354, 0.10061507672071457, 0.09977337718009949, 0.10032548755407333, 0.0985293984413147, 0.09770108759403229, 0.09784238785505295, 0.0966605544090271, 0.09415145218372345, 0.09380516409873962, 0.09331946074962616, 0.0925212875008583, 0.09189973771572113, 0.08986859023571014, 0.09046459943056107, 0.08893199265003204, 0.08893617242574692, 0.08707895874977112, 0.08542520552873611, 0.0855521485209465, 0.08467857539653778, 0.08512677997350693, 0.08381210267543793, 0.08266156166791916, 0.08196357637643814], "moving_avg_accuracy_train": [0.0538470588235294, 0.11251176470588234, 0.1700747058823529, 0.22530958823529407, 0.2782068647058823, 0.3276073547058823, 0.3730607368823529, 0.41610525142941174, 0.4557911968747058, 0.49268031248135286, 0.5259863988802764, 0.5574065825216605, 0.586028277210671, 0.6123925083131333, 0.6364779633641728, 0.6588207552630496, 0.6796210326779211, 0.6981106941160113, 0.7155372717632338, 0.7314353092927928, 0.7462917783635135, 0.7597190711153974, 0.7722483404744459, 0.7840964476034719, 0.7949550381372423, 0.8050501225588123, 0.8141945220676369, 0.8226150698608732, 0.8303723864041976, 0.8380433830578955, 0.8448272800462236, 0.850932787335719, 0.856799508602147, 0.862413675388991, 0.8672005431442096, 0.8719604888297886, 0.8763832634762215, 0.8805943488933052, 0.8847560904745629, 0.8887698931918124, 0.8923399626961607, 0.8960047899559563, 0.8989525462544783, 0.9020525857466776, 0.9046190918778921, 0.9072277709253971, 0.9098838173622691, 0.9121236709201598, 0.9145089508869673, 0.9166533499159175, 0.9184327208066787, 0.9205565075495402, 0.9225926215004685, 0.9244863005268922, 0.9261553175330265, 0.9275515504856062, 0.9289375719076338, 0.9305708735403999, 0.9321631979510658, 0.9336574663912534, 0.9349881903403633, 0.9362305477769152, 0.9370898459404001, 0.9380114495816543, 0.939480892858783, 0.9407775094552576, 0.9419138761567907, 0.942889547364641, 0.9436335338046475, 0.9447478274830063, 0.9459012800288233, 0.9467652696729998, 0.9476722721174644, 0.948410927258659, 0.9489980698269107, 0.9499217922559843, 0.9504049071480328, 0.9512491223155825, 0.9520347983193183, 0.9529183773109159, 0.9533841866386478, 0.9538081209159595, 0.9542696617655401, 0.9548121073536919, 0.9552132495594992, 0.9558378069564905, 0.956122261554959, 0.9564135648112279, 0.9567204436242227, 0.9571142816147415, 0.9577652063944438, 0.9580733916373524, 0.9587766407089113, 0.9591648589909614, 0.9596436672095123, 0.9595357710767964, 0.959892782204411, 0.9603717392780875, 0.9608639771149846, 0.9614128735211332, 0.9619045273454905, 0.9621940746109414, 0.9625370200910237, 0.9627444945525097, 0.9632088686266704, 0.9634597464698857, 0.963972595352309, 0.964255335817078, 0.9643027434118407, 0.9644418808353626, 0.9646047515753557, 0.9650289823001731, 0.9655190252466264, 0.9656212403690225, 0.965722645743885, 0.9661503811694965, 0.9666882842290174, 0.9672500440414098, 0.9673532749313865, 0.9675167709676595, 0.9677745056355995, 0.9679852903661572, 0.9682479378001297, 0.9683666734318814, 0.9683017707945757, 0.968525711362177, 0.9687437284612535, 0.9692317085563046, 0.9695085377006741, 0.9693553309894303, 0.9694550920081343, 0.9693284063367326, 0.9694567421736475, 0.9696781267798121, 0.9699620788077133, 0.9702952826916479, 0.9704986955989536, 0.9709170613331759, 0.9709594728469172, 0.9712894079151666, 0.9714875259471794, 0.9715128909995203, 0.9716180724878036, 0.9718327358272586, 0.9718518151857092, 0.972080751314197, 0.972451499712189, 0.9723898791527348, 0.9723979500609907, 0.9726263903490093, 0.9729990454317554, 0.9730850232415211, 0.9732824032703101, 0.9733000452962203, 0.9733253348842452, 0.9736163308075854, 0.9738782271385915, 0.9740598161894383, 0.9740350110410827, 0.9743020981722685, 0.9742248295315122, 0.974437640696008], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.052119999999999986, 0.1095213333333333, 0.1658625333333333, 0.2194896133333333, 0.27063398533333327, 0.31837058679999997, 0.36208019478666664, 0.403072175308, 0.44065829111053334, 0.47569912866614666, 0.507049215799532, 0.5365576275529121, 0.5635018647976209, 0.5879783449845255, 0.610660510486073, 0.6312744594374656, 0.6504403468270524, 0.6672763121443471, 0.6831220142632457, 0.6976631461702545, 0.710656831553229, 0.7226178150645728, 0.7331293668914489, 0.7431764302023041, 0.7525654538487403, 0.7611889084638662, 0.769043350950813, 0.776072349189065, 0.7826784476034918, 0.7886372695098093, 0.7939735425588283, 0.7987495216362789, 0.8033012361393177, 0.8076377791920526, 0.8113540012728474, 0.814911934478896, 0.8184340743643397, 0.8212173335945724, 0.8242289335684484, 0.8268460402116036, 0.8292947695237766, 0.8320319592380656, 0.8340554299809257, 0.8359832203161666, 0.8378115649512166, 0.8391370751227616, 0.8409033676104855, 0.8422130308494369, 0.8436717277644932, 0.8450645549880438, 0.8458247661559061, 0.8468956228736488, 0.8478593939196173, 0.8486201211943223, 0.84951810907489, 0.8502996315007344, 0.8510163350173277, 0.8519813681822616, 0.852689898030702, 0.8531942415609652, 0.8539148174048687, 0.8547366689977152, 0.8548230020979436, 0.8548740352214826, 0.8554799650326678, 0.855971968529401, 0.8565481050097943, 0.8570532945088148, 0.8572012983912667, 0.8573878352188067, 0.8573157183635927, 0.8576241465272334, 0.8576750652078434, 0.8576808920203924, 0.8579661361516865, 0.8581695225365178, 0.8584192369495327, 0.8589373132545794, 0.8590435819291214, 0.8594725570695426, 0.8596453013625883, 0.8598674378929961, 0.8598673607703632, 0.8601206246933268, 0.8602418955573274, 0.8604177060015946, 0.8606159354014351, 0.8607810085279582, 0.8607162410084958, 0.8605912835743128, 0.8608121552168816, 0.8609176063618601, 0.8613725123923408, 0.8616752611531067, 0.861467735037796, 0.8611209615340164, 0.861342198713948, 0.8617146455092198, 0.8616898476249646, 0.8621475295291348, 0.862279443242888, 0.8621981655852657, 0.8620450156934059, 0.8617871807907319, 0.862221796044992, 0.8623329497738261, 0.8622596547964435, 0.8623803559834659, 0.8622223203851193, 0.8624667550132741, 0.8624467461786134, 0.8629754048940854, 0.8631445310713436, 0.8632700779642092, 0.863169736834455, 0.8634394298176762, 0.8635621535025753, 0.8635926048189844, 0.8636600110037526, 0.8635740099033773, 0.8638566089130395, 0.8638176146884022, 0.8638625198862286, 0.8637562678976058, 0.863567307774512, 0.8635572436637274, 0.8636281859640214, 0.8637853673676191, 0.8635534972975238, 0.8633314809011048, 0.8632783328109943, 0.8630971661965616, 0.8633474495769055, 0.8634927046192149, 0.8635034341572935, 0.8639130907415641, 0.8641884483340744, 0.8642762701673335, 0.8642619764839335, 0.8642357788355401, 0.8640388676186528, 0.8639683141901208, 0.8639181494377753, 0.8642330011606644, 0.8641430343779312, 0.8642220642734714, 0.8645465245127909, 0.8643185387281785, 0.8640466848553607, 0.8643486830364913, 0.8644338147328421, 0.8645104332595578, 0.8648327232669353, 0.8647094509402418, 0.8645185058462176, 0.8647866552615958, 0.8649079897354363, 0.8651238574285592, 0.8651714716857033, 0.8653609911837996, 0.865198225398753, 0.8653850695255443], "moving_var_accuracy_train": [0.026095551695501727, 0.05445992597231833, 0.07883536314705882, 0.0984098568892422, 0.1137519679223706, 0.12434044684029445, 0.13050049171778766, 0.1341259146393365, 0.1348880915683846, 0.1336465440637113, 0.1302655481782532, 0.1261240448209526, 0.12088445300069568, 0.11505166183524249, 0.10876747795685908, 0.10238353330970182, 0.09603904384355216, 0.08951194768005376, 0.08329392338850042, 0.07723925942527177, 0.07150176554198813, 0.06597421870359248, 0.060789640149277555, 0.05597407491721769, 0.051437848320916725, 0.04721126005413266, 0.04324271443011233, 0.03955659361334469, 0.03614251789159023, 0.033057863809380604, 0.030166268753576767, 0.02748513685157781, 0.025046388932181668, 0.022825419857358035, 0.02074910479777579, 0.018878108064365174, 0.017166345678086822, 0.015609310273787917, 0.01420426008331165, 0.01292882958525749, 0.011750655193124631, 0.010696468303409444, 0.009705024877827786, 0.008821014593723761, 0.007998195717845444, 0.007259623003416919, 0.006597151947148615, 0.005982589248080915, 0.005435536367953305, 0.004933368755916243, 0.004468527327226614, 0.0040622688256663425, 0.003693353783290193, 0.003356292587257228, 0.003045733888432395, 0.0027587056977099803, 0.0025001246263798575, 0.0022741212317542374, 0.002069528581838036, 0.0018826712671962966, 0.0017103415765352802, 0.0015531984868831547, 0.0014045241781987568, 0.0012717159398230368, 0.0011639777177430216, 0.001062710877353001, 0.0009680617531408791, 0.0008798229865792447, 0.0007968223303275419, 0.0007283149509094607, 0.0006674575307975804, 0.0006074300806650203, 0.0005540909535069031, 0.0005035923609147336, 0.0004563357523823396, 0.000418381545277869, 0.00037864399074035457, 0.00034719388490840716, 0.00031803007746318383, 0.00029325347622639897, 0.0002658809335719778, 0.00024091032265809804, 0.00021873646999477178, 0.0001995110479402435, 0.00018100817876973854, 0.0001664180083719932, 0.00015050443730210326, 0.0001362177118559082, 0.0001234435121231036, 0.00011249513617577701, 0.00010505895017767384, 9.54078584554259e-05, 9.031810591971905e-05, 8.264271623840815e-05, 7.644176040593446e-05, 6.89023585444365e-05, 6.315923519715782e-05, 5.890791058326525e-05, 5.5197802317597274e-05, 5.238960746798319e-05, 4.932615806823099e-05, 4.514808083177901e-05, 4.169177716938118e-05, 3.791001032196271e-05, 3.605979881654075e-05, 3.302027616483398e-05, 3.208537433417501e-05, 2.9596316434518165e-05, 2.665691211143708e-05, 2.416545390391194e-05, 2.1987650415033997e-05, 2.140863074444221e-05, 2.142904647431554e-05, 1.9380173208102196e-05, 1.7534703337750923e-05, 1.742785135288328e-05, 1.8289123530572826e-05, 1.9300377958887544e-05, 1.7466249712807166e-05, 1.5960203326419628e-05, 1.4962027425300575e-05, 1.3865696506497016e-05, 1.3099979926998447e-05, 1.1916865286525954e-05, 1.0763089928836579e-05, 1.0138125336311054e-05, 9.552095902087213e-06, 1.07400074703734e-05, 1.0355716099887397e-05, 9.531395157230155e-06, 8.667825989182945e-06, 7.945486724311048e-06, 7.29916883520997e-06, 7.010352246309094e-06, 7.034975809020666e-06, 7.330701682540236e-06, 6.970022812013605e-06, 7.848289518954385e-06, 7.079649195539352e-06, 7.351398619332378e-06, 6.9695155488765885e-06, 6.278354466911231e-06, 5.75008732951743e-06, 5.589801740319181e-06, 5.034097763557246e-06, 5.002393745544837e-06, 5.739243742513485e-06, 5.1994932083891725e-06, 4.680130143590937e-06, 4.6817818159419095e-06, 5.463449930616393e-06, 4.983634591503611e-06, 4.835901014236204e-06, 4.355112082516516e-06, 3.925356943627151e-06, 4.294928895869921e-06, 4.482743200033389e-06, 4.331240130516796e-06, 3.903653775929602e-06, 4.155308219142433e-06, 3.793511382827077e-06, 3.821757570151019e-06], "duration": 198449.692812, "accuracy_train": [0.5384705882352941, 0.6404941176470588, 0.6881411764705883, 0.7224235294117647, 0.7542823529411765, 0.7722117647058824, 0.7821411764705882, 0.8035058823529412, 0.8129647058823529, 0.8246823529411764, 0.8257411764705882, 0.8401882352941177, 0.8436235294117647, 0.8496705882352941, 0.8532470588235294, 0.8599058823529412, 0.8668235294117647, 0.8645176470588235, 0.8723764705882353, 0.8745176470588235, 0.88, 0.8805647058823529, 0.8850117647058824, 0.8907294117647059, 0.8926823529411765, 0.8959058823529412, 0.8964941176470588, 0.8984, 0.9001882352941176, 0.9070823529411765, 0.9058823529411765, 0.9058823529411765, 0.9096, 0.9129411764705883, 0.9102823529411764, 0.9148, 0.9161882352941176, 0.9184941176470588, 0.9222117647058824, 0.9248941176470589, 0.9244705882352942, 0.9289882352941177, 0.9254823529411764, 0.9299529411764705, 0.9277176470588235, 0.9307058823529412, 0.9337882352941177, 0.9322823529411765, 0.9359764705882353, 0.9359529411764705, 0.9344470588235294, 0.9396705882352941, 0.9409176470588235, 0.9415294117647058, 0.9411764705882353, 0.9401176470588235, 0.9414117647058824, 0.9452705882352941, 0.9464941176470588, 0.9471058823529411, 0.9469647058823529, 0.9474117647058824, 0.9448235294117647, 0.9463058823529412, 0.9527058823529412, 0.9524470588235294, 0.9521411764705883, 0.9516705882352942, 0.9503294117647059, 0.9547764705882353, 0.9562823529411765, 0.9545411764705882, 0.955835294117647, 0.9550588235294117, 0.9542823529411765, 0.9582352941176471, 0.9547529411764706, 0.9588470588235294, 0.9591058823529411, 0.9608705882352941, 0.9575764705882353, 0.9576235294117647, 0.9584235294117647, 0.9596941176470588, 0.9588235294117647, 0.9614588235294118, 0.9586823529411764, 0.9590352941176471, 0.9594823529411765, 0.9606588235294118, 0.9636235294117647, 0.9608470588235294, 0.9651058823529411, 0.9626588235294118, 0.9639529411764706, 0.958564705882353, 0.9631058823529411, 0.9646823529411764, 0.9652941176470589, 0.9663529411764706, 0.9663294117647059, 0.9648, 0.9656235294117647, 0.9646117647058824, 0.9673882352941177, 0.9657176470588236, 0.9685882352941176, 0.9668, 0.9647294117647058, 0.9656941176470588, 0.9660705882352941, 0.9688470588235294, 0.9699294117647059, 0.9665411764705882, 0.966635294117647, 0.97, 0.9715294117647059, 0.9723058823529411, 0.9682823529411765, 0.9689882352941176, 0.9700941176470588, 0.9698823529411764, 0.9706117647058824, 0.9694352941176471, 0.9677176470588236, 0.9705411764705882, 0.9707058823529412, 0.9736235294117647, 0.972, 0.9679764705882353, 0.9703529411764706, 0.9681882352941177, 0.9706117647058824, 0.9716705882352941, 0.9725176470588235, 0.9732941176470589, 0.9723294117647059, 0.9746823529411764, 0.9713411764705883, 0.9742588235294117, 0.9732705882352941, 0.9717411764705882, 0.9725647058823529, 0.973764705882353, 0.9720235294117647, 0.9741411764705883, 0.9757882352941176, 0.971835294117647, 0.9724705882352941, 0.9746823529411764, 0.9763529411764705, 0.9738588235294118, 0.9750588235294118, 0.9734588235294117, 0.9735529411764706, 0.9762352941176471, 0.9762352941176471, 0.9756941176470588, 0.9738117647058824, 0.9767058823529412, 0.9735294117647059, 0.9763529411764705], "end": "2016-02-06 15:43:16.897000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0], "moving_var_accuracy_valid": [0.024448449599999995, 0.051657822256, 0.07506101738736, 0.0934376890325616, 0.1076356412147749, 0.11738112516958367, 0.12283778112575797, 0.1256770852167345, 0.12582382160515407, 0.1242921821140286, 0.12070841557206341, 0.11647429129272027, 0.11136078944974062, 0.10561659324582595, 0.09968525960779966, 0.09354114766935531, 0.08749301405729268, 0.08129476020504978, 0.07542506066531263, 0.06978555525301462, 0.06432652246619869, 0.059181456358606555, 0.05425774521902781, 0.04974046202767601, 0.045559799710208466, 0.04167309546467986, 0.03806101631923866, 0.03469957603341495, 0.03162238325642329, 0.028779712957381846, 0.02615802395212685, 0.023747511342248367, 0.021559223152278092, 0.01957255128788429, 0.01773958891807994, 0.016079560024560274, 0.014583253246457949, 0.013194646709296234, 0.011956809647990466, 0.010822771907826241, 0.009794461194242274, 0.008882444942606132, 0.008031050352970416, 0.007261392697863204, 0.006565339025017529, 0.005924617917449599, 0.005360234228074347, 0.004839647765462059, 0.004374833159125807, 0.003954809552285202, 0.003564529886234365, 0.003218397504600341, 0.0029049174458017314, 0.0026196340550998782, 0.00236492808969271, 0.0021339322764423172, 0.0019251620241743596, 0.0017410274228417248, 0.0015714428114727325, 0.0014165877918941238, 0.0012796020786260663, 0.0011577208311294382, 0.0010420158286542501, 0.0009378376852061084, 0.0008473582751102425, 0.0007648010545663977, 0.0006913083483061171, 0.0006244744613447913, 0.0005622241615532995, 0.0005063149092902279, 0.00045573022592845867, 0.00041101335472475397, 0.00036993535366059414, 0.0003329421238602351, 0.0003003801894041511, 0.0002707144646575489, 0.00024420423378440035, 0.0002221994379266182, 0.00020008113141465673, 0.00018172919531308544, 0.00016382484109879572, 0.00014788645873219084, 0.00013309781291250285, 0.00012036531515332717, 0.00010846114324009363, 9.789321272690532e-05, 8.845754550886507e-05, 7.985703319187972e-05, 7.190908335688767e-05, 6.485870426441707e-05, 5.881189238039439e-05, 5.303078263815045e-05, 4.959015984344444e-05, 4.5456055168407684e-05, 4.1298053448390316e-05, 3.82505148698635e-05, 3.48659763909343e-05, 3.262782828961543e-05, 2.9370579876225747e-05, 2.8318776417246964e-05, 2.5643509826407706e-05, 2.3138613362423783e-05, 2.1035846030571715e-05, 1.953057096084634e-05, 1.927752763788216e-05, 1.7460971236997516e-05, 1.5763223496683418e-05, 1.4318020135952448e-05, 1.3110995375460068e-05, 1.2337630424884373e-05, 1.1107470563576266e-05, 1.2512043844219137e-05, 1.1518272434302743e-05, 1.0508303391646412e-05, 9.548088133365019e-06, 9.247888066817115e-06, 8.458649185652215e-06, 7.621129811126441e-06, 6.899909173718874e-06, 6.276483959738778e-06, 6.367595366123754e-06, 5.744520775506942e-06, 5.188216989082734e-06, 4.771000655951285e-06, 4.6152539434331786e-06, 4.154640126022809e-06, 3.7844714031594616e-06, 3.6283782055764736e-06, 3.7494139496725655e-06, 3.818094077215439e-06, 3.461707144835418e-06, 3.410928510017143e-06, 3.633611593302489e-06, 3.4601416798191374e-06, 3.1151636187236277e-06, 4.3140139101779984e-06, 4.565008752937614e-06, 4.177921947216961e-06, 3.7619685369615263e-06, 3.3919485342974498e-06, 3.401719926892223e-06, 3.1063480107015124e-06, 2.8183617310322425e-06, 3.4287100245853377e-06, 3.158685220084912e-06, 2.8990282175783e-06, 3.5565954179140796e-06, 3.6687335379907856e-06, 3.967000937686297e-06, 4.3911269565733584e-06, 4.017240912428219e-06, 3.6683504089102855e-06, 4.23635300771811e-06, 3.9494823057022154e-06, 3.882674335519104e-06, 4.141543882676415e-06, 3.85988798528795e-06, 3.893288935167532e-06, 3.5243640990012227e-06, 3.495186450529287e-06, 3.384102112512963e-06, 3.359888450709529e-06], "accuracy_test": 0.8247, "start": "2016-02-04 08:35:47.204000", "learning_rate_per_epoch": [0.003242222359403968, 0.002292597433552146, 0.0018718979554250836, 0.001621111179701984, 0.0014499658718705177, 0.0013236317317932844, 0.0012254448374733329, 0.001146298716776073, 0.0010807408252730966, 0.0010252806823700666, 0.000977566814981401, 0.0009359489777125418, 0.000899230653885752, 0.0008665203349664807, 0.0008371382136829197, 0.000810555589850992, 0.0007863544160500169, 0.0007641991251148283, 0.000743816839531064, 0.0007249829359352589, 0.0007075109169818461, 0.0006912441458553076, 0.0006760500837117434, 0.0006618158658966422, 0.0006484444602392614, 0.0006358521059155464, 0.0006239659851416945, 0.0006127224187366664, 0.0006020655855536461, 0.0005919461254961789, 0.0005823203246109188, 0.0005731493583880365, 0.0005643984768539667, 0.0005560365389101207, 0.0005480356048792601, 0.0005403704126365483, 0.0005330180865712464, 0.0005259579047560692, 0.0005191710661165416, 0.0005126403411850333, 0.0005063500721007586, 0.0005002857651561499, 0.0004944342654198408, 0.0004887834074907005, 0.0004833219572901726, 0.0004780396120622754, 0.0004729267384391278, 0.0004679744888562709, 0.0004631746269296855, 0.0004585194692481309, 0.00045400194358080626, 0.000449615326942876, 0.0004453534784261137, 0.00044121057726442814, 0.0004371811810415238, 0.00043326016748324037, 0.0004294428217690438, 0.00042572463280521333, 0.0004221013805363327, 0.00041856910684145987, 0.0004151240282226354, 0.00041176265222020447, 0.00040848160278983414, 0.000405277794925496, 0.00040214817272499204, 0.0003990899713244289, 0.00039610048406757414, 0.00039317720802500844, 0.00039031769847497344, 0.0003875196853186935, 0.0003847809857688844, 0.0003820995625574142, 0.00037947340751998127, 0.0003769006871152669, 0.0003743795969057828, 0.000371908419765532, 0.00036948552588000894, 0.00036710937274619937, 0.0003647785051725805, 0.00036249146796762943, 0.0003602469223551452, 0.0003580435586627573, 0.000355880125425756, 0.00035375545849092305, 0.0003516683937050402, 0.00034961782512255013, 0.00034760270500555634, 0.0003456220729276538, 0.000343674881150946, 0.0003417602274566889, 0.00033987723872996867, 0.0003380250418558717, 0.00033620279282331467, 0.0003344097058288753, 0.00033264499506913126, 0.0003309079329483211, 0.00032919779187068343, 0.00032751390244811773, 0.0003258555952925235, 0.0003242222301196307, 0.00032261316664516926, 0.0003210278518963605, 0.0003194656455889344, 0.0003179260529577732, 0.00031640849192626774, 0.00031491246772930026, 0.00031343745649792254, 0.00031198299257084727, 0.0003105485811829567, 0.0003091337566729635, 0.0003077381115872413, 0.0003063612093683332, 0.00030500261345878243, 0.000303661945508793, 0.0003023387980647385, 0.00030103279277682304, 0.00029974355129525065, 0.0002984707534778863, 0.0002972140209749341, 0.00029597306274808943, 0.0002947474713437259, 0.0002935370139311999, 0.0002923413412645459, 0.0002911601623054594, 0.00028999318601563573, 0.0002888401213567704, 0.00028770070639438927, 0.00028657467919401824, 0.00028546174871735275, 0.0002843617112375796, 0.0002832742757163942, 0.00028219923842698336, 0.000281136337434873, 0.0002800853399094194, 0.0002790460712276399, 0.00027801826945506036, 0.000277001759968698, 0.0002759962808340788, 0.00027500171563588083, 0.00027401780243963003, 0.00027304436662234366, 0.00027208126266486943, 0.0002711282577365637, 0.00027018520631827414, 0.0002692519046831876, 0.00026832823641598225, 0.00026741399778984487, 0.0002665090432856232, 0.00026561319828033447, 0.0002647263463586569, 0.0002638483128976077, 0.0002629789523780346, 0.0002621181483846158, 0.0002612657262943685, 0.00026042156969197094, 0.0002595855330582708, 0.0002587574999779463, 0.0002579373540356755, 0.00025712494971230626, 0.00025632017059251666, 0.0002555229002609849, 0.0002547330514062196], "accuracy_train_first": 0.5384705882352941, "accuracy_train_last": 0.9763529411764705, "batch_size_eval": 1024, "accuracy_train_std": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "accuracy_test_std": 0, "error_valid": [0.4788, 0.3738666666666667, 0.3270666666666666, 0.2978666666666666, 0.2690666666666667, 0.252, 0.24453333333333338, 0.22799999999999998, 0.22106666666666663, 0.2089333333333333, 0.2108, 0.19786666666666664, 0.19399999999999995, 0.1917333333333333, 0.18520000000000003, 0.18320000000000003, 0.1770666666666667, 0.18120000000000003, 0.17426666666666668, 0.17146666666666666, 0.1724, 0.1697333333333333, 0.17226666666666668, 0.1664, 0.16293333333333337, 0.1612, 0.16026666666666667, 0.16066666666666662, 0.1578666666666667, 0.15773333333333328, 0.15800000000000003, 0.15826666666666667, 0.15573333333333328, 0.15333333333333332, 0.1552, 0.15306666666666668, 0.1498666666666667, 0.1537333333333334, 0.1486666666666666, 0.14959999999999996, 0.1486666666666666, 0.1433333333333333, 0.14773333333333338, 0.1466666666666666, 0.14573333333333338, 0.14893333333333336, 0.1432, 0.14600000000000002, 0.1432, 0.14239999999999997, 0.14733333333333332, 0.14346666666666663, 0.14346666666666663, 0.1445333333333333, 0.14239999999999997, 0.14266666666666672, 0.1425333333333333, 0.1393333333333333, 0.14093333333333335, 0.14226666666666665, 0.13959999999999995, 0.1378666666666667, 0.14439999999999997, 0.14466666666666672, 0.13906666666666667, 0.13959999999999995, 0.13826666666666665, 0.13839999999999997, 0.14146666666666663, 0.14093333333333335, 0.1433333333333333, 0.13959999999999995, 0.1418666666666667, 0.14226666666666665, 0.13946666666666663, 0.14, 0.1393333333333333, 0.13639999999999997, 0.14, 0.13666666666666671, 0.13880000000000003, 0.13813333333333333, 0.14013333333333333, 0.13759999999999994, 0.13866666666666672, 0.138, 0.13759999999999994, 0.13773333333333337, 0.1398666666666667, 0.1405333333333333, 0.1372, 0.13813333333333333, 0.13453333333333328, 0.13560000000000005, 0.14039999999999997, 0.14200000000000002, 0.13666666666666671, 0.13493333333333335, 0.1385333333333333, 0.13373333333333337, 0.13653333333333328, 0.1385333333333333, 0.1393333333333333, 0.1405333333333333, 0.1338666666666667, 0.13666666666666671, 0.13839999999999997, 0.13653333333333328, 0.1392, 0.1353333333333333, 0.13773333333333337, 0.13226666666666664, 0.1353333333333333, 0.13560000000000005, 0.13773333333333337, 0.13413333333333333, 0.1353333333333333, 0.13613333333333333, 0.13573333333333337, 0.1372, 0.13360000000000005, 0.13653333333333328, 0.13573333333333337, 0.1372, 0.13813333333333333, 0.13653333333333328, 0.13573333333333337, 0.13480000000000003, 0.1385333333333333, 0.13866666666666672, 0.1372, 0.1385333333333333, 0.13439999999999996, 0.1352, 0.13639999999999997, 0.13239999999999996, 0.1333333333333333, 0.13493333333333335, 0.1358666666666667, 0.136, 0.13773333333333337, 0.13666666666666671, 0.13653333333333328, 0.13293333333333335, 0.13666666666666671, 0.13506666666666667, 0.13253333333333328, 0.13773333333333337, 0.13839999999999997, 0.13293333333333335, 0.13480000000000003, 0.13480000000000003, 0.13226666666666664, 0.13639999999999997, 0.1372, 0.13280000000000003, 0.134, 0.13293333333333335, 0.13439999999999996, 0.13293333333333335, 0.13626666666666665, 0.13293333333333335], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.0890717600231229, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 16, "valid_ratio": 0.15, "learning_rate": 0.003242222310415774, "optimization": "nesterov_momentum", "nb_data_augmentation": 3, "learning_rate_decay_method": "sqrt", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 9.25903850865559e-07, "rotation_range": [0, 0], "momentum": 0.8427924153143072}, "accuracy_valid_max": 0.8677333333333334, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        y_pred = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            y_pred.extend((nnet.predict(X[mini_batch]) == y[mini_batch]).tolist())\n        return np.mean(y_pred)\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            acc = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = acc\n            status[\"accuracy_train_std\"] = 0\n            acc = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = acc\n            status[\"accuracy_valid_std\"] = 0\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n\n    # rescaling to [-1, 1]\n    X_min = X_train.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X_train.max(axis=(0, 2, 3))[None, :, None, None]\n    def preprocess(a):\n        return (a / 255.) * 2 - 1\n        # return 2 * ((a - X_min) / (X_max - X_min)) - 1\n    X_train = preprocess(X_train)\n    X_valid = preprocess(X_valid)\n\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = preprocess(X_test)\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    acc = evaluate(X_test, y_test, batch_size_eval)\n    light.set(\"accuracy_test\", acc)\n    light.set(\"accuracy_test_std\", 0)\n    print(\"Test accuracy : {}+-{}\".format(acc, 0))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8670666666666667, "accuracy_valid_std": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "accuracy_valid": [0.5212, 0.6261333333333333, 0.6729333333333334, 0.7021333333333334, 0.7309333333333333, 0.748, 0.7554666666666666, 0.772, 0.7789333333333334, 0.7910666666666667, 0.7892, 0.8021333333333334, 0.806, 0.8082666666666667, 0.8148, 0.8168, 0.8229333333333333, 0.8188, 0.8257333333333333, 0.8285333333333333, 0.8276, 0.8302666666666667, 0.8277333333333333, 0.8336, 0.8370666666666666, 0.8388, 0.8397333333333333, 0.8393333333333334, 0.8421333333333333, 0.8422666666666667, 0.842, 0.8417333333333333, 0.8442666666666667, 0.8466666666666667, 0.8448, 0.8469333333333333, 0.8501333333333333, 0.8462666666666666, 0.8513333333333334, 0.8504, 0.8513333333333334, 0.8566666666666667, 0.8522666666666666, 0.8533333333333334, 0.8542666666666666, 0.8510666666666666, 0.8568, 0.854, 0.8568, 0.8576, 0.8526666666666667, 0.8565333333333334, 0.8565333333333334, 0.8554666666666667, 0.8576, 0.8573333333333333, 0.8574666666666667, 0.8606666666666667, 0.8590666666666666, 0.8577333333333333, 0.8604, 0.8621333333333333, 0.8556, 0.8553333333333333, 0.8609333333333333, 0.8604, 0.8617333333333334, 0.8616, 0.8585333333333334, 0.8590666666666666, 0.8566666666666667, 0.8604, 0.8581333333333333, 0.8577333333333333, 0.8605333333333334, 0.86, 0.8606666666666667, 0.8636, 0.86, 0.8633333333333333, 0.8612, 0.8618666666666667, 0.8598666666666667, 0.8624, 0.8613333333333333, 0.862, 0.8624, 0.8622666666666666, 0.8601333333333333, 0.8594666666666667, 0.8628, 0.8618666666666667, 0.8654666666666667, 0.8644, 0.8596, 0.858, 0.8633333333333333, 0.8650666666666667, 0.8614666666666667, 0.8662666666666666, 0.8634666666666667, 0.8614666666666667, 0.8606666666666667, 0.8594666666666667, 0.8661333333333333, 0.8633333333333333, 0.8616, 0.8634666666666667, 0.8608, 0.8646666666666667, 0.8622666666666666, 0.8677333333333334, 0.8646666666666667, 0.8644, 0.8622666666666666, 0.8658666666666667, 0.8646666666666667, 0.8638666666666667, 0.8642666666666666, 0.8628, 0.8664, 0.8634666666666667, 0.8642666666666666, 0.8628, 0.8618666666666667, 0.8634666666666667, 0.8642666666666666, 0.8652, 0.8614666666666667, 0.8613333333333333, 0.8628, 0.8614666666666667, 0.8656, 0.8648, 0.8636, 0.8676, 0.8666666666666667, 0.8650666666666667, 0.8641333333333333, 0.864, 0.8622666666666666, 0.8633333333333333, 0.8634666666666667, 0.8670666666666667, 0.8633333333333333, 0.8649333333333333, 0.8674666666666667, 0.8622666666666666, 0.8616, 0.8670666666666667, 0.8652, 0.8652, 0.8677333333333334, 0.8636, 0.8628, 0.8672, 0.866, 0.8670666666666667, 0.8656, 0.8670666666666667, 0.8637333333333334, 0.8670666666666667], "seed": 180940963, "model": "residualv3", "loss_std": [0.304075688123703, 0.26693299412727356, 0.25817784667015076, 0.25467032194137573, 0.25210994482040405, 0.24805645644664764, 0.24611152708530426, 0.24134692549705505, 0.23703333735466003, 0.23328372836112976, 0.22888483107089996, 0.22758150100708008, 0.22562497854232788, 0.21937720477581024, 0.21871015429496765, 0.21448297798633575, 0.21109257638454437, 0.21019986271858215, 0.2064676135778427, 0.2041233330965042, 0.20078153908252716, 0.20087051391601562, 0.1952272206544876, 0.19262264668941498, 0.1941855102777481, 0.1920720934867859, 0.18669995665550232, 0.1870320588350296, 0.18171297013759613, 0.18232156336307526, 0.17593985795974731, 0.1774568259716034, 0.17382171750068665, 0.17449522018432617, 0.17061932384967804, 0.1695481389760971, 0.16767577826976776, 0.1647796630859375, 0.16188184916973114, 0.1595710664987564, 0.15859878063201904, 0.1585468351840973, 0.15501601994037628, 0.15199324488639832, 0.15350307524204254, 0.1473938524723053, 0.14833851158618927, 0.1439659595489502, 0.14423371851444244, 0.14190511405467987, 0.13799826800823212, 0.1391231119632721, 0.13935156166553497, 0.13589079678058624, 0.13296674191951752, 0.13337111473083496, 0.13162294030189514, 0.1299562305212021, 0.12778259813785553, 0.1259792298078537, 0.12619651854038239, 0.12404046207666397, 0.12212368100881577, 0.11879169195890427, 0.11863323301076889, 0.1186661347746849, 0.11819800734519958, 0.11370859295129776, 0.11586283892393112, 0.11365623772144318, 0.11222375929355621, 0.10743571072816849, 0.10893936455249786, 0.10866463929414749, 0.106282077729702, 0.10684754699468613, 0.10466243326663971, 0.10417327284812927, 0.10285021364688873, 0.101023830473423, 0.10009553283452988, 0.09839148074388504, 0.096634142100811, 0.09808352589607239, 0.09611357748508453, 0.09566326439380646, 0.09505627304315567, 0.09445951879024506, 0.09109038859605789, 0.09106601029634476, 0.0900346115231514, 0.0907234326004982, 0.08851294964551926, 0.08852516859769821, 0.08988412469625473, 0.08673711121082306, 0.0861796960234642, 0.08625856041908264, 0.08445650339126587, 0.08374828100204468, 0.08238480240106583, 0.08377087116241455, 0.083033986389637, 0.08032239228487015, 0.07987295091152191, 0.07738076150417328, 0.07807362079620361, 0.07846470177173615, 0.07908125221729279, 0.07468581944704056, 0.07718051970005035, 0.07677952200174332, 0.07439327239990234, 0.07496695965528488, 0.07251222431659698, 0.07389494776725769, 0.07051292061805725, 0.07242075353860855, 0.07004784047603607, 0.0702354684472084, 0.07098408043384552, 0.06800615787506104, 0.06935816258192062, 0.06661149859428406, 0.0659179836511612, 0.06766146421432495, 0.06698612868785858, 0.06452605873346329, 0.06505091488361359, 0.06407560408115387, 0.06543539464473724, 0.06230172887444496, 0.0657576471567154, 0.06360480189323425, 0.06270609050989151, 0.061766888946294785, 0.0600484237074852, 0.06211771070957184, 0.061175111681222916, 0.062317050993442535, 0.060561709105968475, 0.05948936939239502, 0.05929217487573624, 0.06003868952393532, 0.0590706430375576, 0.05671059340238571, 0.05764063820242882, 0.05582534521818161, 0.05618509277701378, 0.05734578147530556, 0.05398629605770111, 0.05474843829870224, 0.05517563223838806, 0.055999524891376495, 0.052601154893636703, 0.05294321849942207, 0.05370217189192772, 0.05206252634525299, 0.05308083817362785, 0.05230635777115822, 0.05106718838214874, 0.051358893513679504]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:42 2016", "state": "available"}], "summary": "62924bab4f27ed3b582d1f0af252c700"}