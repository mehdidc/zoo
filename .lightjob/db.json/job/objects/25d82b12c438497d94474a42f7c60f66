{"content": {"hp_model": {"f0": 64, "f1": 64, "f2": 32, "f3": 16, "nonlin": "rectify", "nbg1": 4, "nbg3": 3, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.011263520995871856, 0.016521398781678024, 0.018521097809104405, 0.01895802418451087, 0.016189515805949156, 0.01845479060673784, 0.018520785924855808, 0.01665669746203462, 0.01529202141961973, 0.013264871653530191, 0.01537791201402306, 0.018545677169765882, 0.015477918390349017, 0.013520809144189239, 0.009767999026857656, 0.012341721107341169, 0.015254245789911875, 0.013028418618993984, 0.01703622664698622, 0.015396319689148449, 0.017769542227562354, 0.014134499468770847, 0.013382446852444427, 0.01472161202375083, 0.015368834247227792, 0.011406224733766748, 0.017112962819813785, 0.013235405685670013, 0.01292067235131898, 0.013434017241187615, 0.016749197656213098, 0.013177195750689458, 0.01229020357896327, 0.00986572211212107, 0.011500367663213476, 0.01011904648434142, 0.00918456962877406, 0.010325984319952401, 0.010489068677408597, 0.009492821240876477, 0.012516579528928139, 0.010197084489822579, 0.008976860596864157, 0.013177244339004726, 0.0116274811419835, 0.012680258869334734, 0.011395061721475398, 0.013880348862445074, 0.010555259104065875, 0.011141439285836751, 0.013800619909125428, 0.013200050563833119, 0.012211882532201875, 0.009739252751718873, 0.010634044824427898, 0.009668905476241156, 0.011033355599812897, 0.010304371938839301, 0.010868309369883386, 0.011699077931741702, 0.010659890116466536, 0.012487410960066123, 0.0097659829293257, 0.011539847318443189, 0.01158006638910896, 0.014781341643525457, 0.013659488373344306, 0.01343932403144897, 0.013124564278393352, 0.014159023372434108, 0.014925662717849836, 0.01391102798547437, 0.011426906612458097, 0.01357029308736541, 0.012814461530390366, 0.013406899535580179, 0.01376593456894301, 0.012417373555616758, 0.011934292087066022, 0.013974662900095467, 0.014847641620023952, 0.013431310277592477, 0.015003027807520917, 0.015992002889247525, 0.013952567916775366, 0.015321972428233098, 0.013480778973162392, 0.014881235804645001, 0.014945791377906798, 0.015185470595071542, 0.013543503186657339, 0.01615633062598884, 0.013059649828211171, 0.017407561216139216, 0.015256243890649876, 0.014761816428392598, 0.01486015692804473, 0.014621548063400443, 0.01443416540205298, 0.014174664758016703, 0.01310378297117655, 0.014301471846057425, 0.014744376113360007, 0.01450063810413241, 0.014014011864895799, 0.01578254338600458, 0.015121024696924905, 0.01334711983303579, 0.014478179520673342, 0.01423938439396632, 0.013433583146039978, 0.014489886759359161, 0.014883355486371486, 0.014509588727061212, 0.014607898546350622, 0.015808529348012978, 0.014255444403687985, 0.014317060946265324, 0.013658152828188774, 0.014576089275918757, 0.015157359940454062, 0.01425024383123891, 0.014981682824112524, 0.013615308322686273, 0.014444961140142508, 0.014881564081477422, 0.01524886959668104, 0.015886276515647738, 0.013707581459782452, 0.014156972398201786, 0.014978583196655966, 0.015467313892345926, 0.01470843302685569, 0.014287300281624149, 0.01451924290650665, 0.01441585885267983, 0.013794524715513788, 0.013610703549162047, 0.013709257809399866, 0.014499282323801842, 0.013784760523317174, 0.01483158925474775, 0.013259682316390235, 0.015827534647713026, 0.014378332582982165, 0.0143841312964315, 0.01504998252680861, 0.015318433396589195, 0.015294318091514401, 0.014041692182961392, 0.01386210049318491, 0.015171638381096097, 0.01466480296505442, 0.014729221658589534, 0.01430689985795505, 0.014394729319124229, 0.015346153583327629, 0.014643945483741599, 0.01409023983811924, 0.014627679935685057, 0.01311515727618118, 0.014300549141567063, 0.014107991353932302, 0.013828512218356685, 0.013460392152715182, 0.013192633470440952, 0.013136173430922176, 0.01349798528469983, 0.013566893323976759, 0.015104631702217501, 0.013864193762683, 0.014603186174095301, 0.014178619567326701, 0.013272047183794917, 0.014258757891147672, 0.013897305451275533, 0.014224988578063627, 0.014681108786109945, 0.015246053634029772, 0.015269768357399075, 0.013918272980907942, 0.013232683756301563, 0.01387390329862241, 0.013460727682668747, 0.014542308312673226, 0.014159135808779078, 0.014335926398812229, 0.014012696370988172, 0.012891171758826703, 0.013602802208779083, 0.01499777991659145, 0.013784864714541264, 0.012642958997146868, 0.01331825357090262, 0.013860352661227412, 0.014449283060051417, 0.01379529907042132, 0.013660908419696692, 0.013894557979313275, 0.013219876791869334, 0.014583058597537274, 0.012811667305248225, 0.013973224662067794, 0.013467320483318498, 0.013012156662133536, 0.01459191011730412, 0.012563336470698955], "moving_avg_accuracy_train": [0.026285446803479134, 0.05517249179528884, 0.08421895818192827, 0.11283902607266862, 0.14159169859690784, 0.16979825409712845, 0.1972810417365112, 0.22408886237367587, 0.24995940206607942, 0.2749703652082625, 0.2991331244492487, 0.32234899366729913, 0.34446816075151976, 0.36585427786781294, 0.385975858980764, 0.4051569232418792, 0.423728615417276, 0.4412587969726322, 0.4575869485426724, 0.4728774149044904, 0.48741510989313885, 0.5011475716566858, 0.5145436594176507, 0.5269533807774729, 0.538912824791826, 0.5501129837462906, 0.5608511078695851, 0.5709757629960172, 0.5804505676776355, 0.5889847591910643, 0.5970516504508061, 0.604876575354738, 0.6128631984290372, 0.6200531959541661, 0.6266334757208296, 0.6326837188418069, 0.638854203835164, 0.6448655864982054, 0.6503105278829917, 0.6557134596138878, 0.6610825840751642, 0.6661078555391409, 0.6705960110151271, 0.6751281022470401, 0.6792650049295436, 0.683169506853302, 0.6869462643513421, 0.69038487362934, 0.6937910477247393, 0.6972171106225312, 0.7003819474388773, 0.7030140617343029, 0.7062386193620908, 0.7096173767330523, 0.7121817110074216, 0.7149426553840235, 0.7177716273467747, 0.7203689995823352, 0.7227206575848345, 0.7251114092001514, 0.7275491150063267, 0.7297266299949429, 0.7316027241763733, 0.7334215254194503, 0.7352837696798309, 0.7373108509355927, 0.7390655777479488, 0.7407052136504795, 0.7423598863722718, 0.7440396819266283, 0.7456258305898164, 0.747146298241429, 0.7485102130255363, 0.7498518128181746, 0.7512496624921988, 0.7528704143642876, 0.7541525239349187, 0.7552505108306018, 0.7564174831533186, 0.7574284830557331, 0.7586358578202501, 0.759771251135678, 0.7607816596196093, 0.7618187301955774, 0.7626499313616049, 0.7636747411681816, 0.7644342014309777, 0.7652875596770456, 0.7660231381616297, 0.7668617980096417, 0.7676651676561213, 0.7684696165951049, 0.7691356000664085, 0.7698697717238965, 0.7703724881942255, 0.7708342696615785, 0.7713869486155018, 0.7718146052097472, 0.7721948818957677, 0.7727624901036161, 0.7730899030764586, 0.7734496068210461, 0.7740800795899382, 0.7745966401986815, 0.7749938991381617, 0.7753444927860843, 0.7757948496513482, 0.7761444754051133, 0.7763870950192255, 0.7767449255516792, 0.7771855916689919, 0.7775147618590972, 0.7778433868694314, 0.7781461969227984, 0.7783233588208194, 0.7785781716790475, 0.7788051781026433, 0.7789954248445661, 0.7792853376480385, 0.7793299842830594, 0.7795841520426918, 0.7796710329001613, 0.7798492431195121, 0.7801885966776236, 0.7802778120894571, 0.7804347998220027, 0.7807272234539128, 0.7808463175940789, 0.7809813320083052, 0.7811539982549185, 0.7812605697518703, 0.7813913973800886, 0.7814765901621518, 0.7815276870291038, 0.781652657171247, 0.7817558297039378, 0.7818672501250169, 0.7818978100885211, 0.7820066942640083, 0.7821117375660127, 0.7822549965163605, 0.782360714132397, 0.7824907372189727, 0.7826170585921289, 0.7827331450744166, 0.7828561520013142, 0.7829343061521886, 0.7829767431022614, 0.7830614032846985, 0.7830795047774727, 0.7832120535614455, 0.7832686405468016, 0.7834055632907556, 0.7834845798841146, 0.7835882829502897, 0.7836792905610378, 0.7837378738249783, 0.7837813702649242, 0.7838646588394377, 0.7839071025219853, 0.7839290257946114, 0.7839743694256985, 0.7840314547353435, 0.7839851392152053, 0.784006234264938, 0.7840763370346882, 0.7840721083584434, 0.7840310280712333, 0.7840381975913063, 0.7839796541391617, 0.7838990632465171, 0.7839102007514612, 0.7839178272594639, 0.7839991679762085, 0.7840212213474692, 0.7840898975066037, 0.7840540137510061, 0.7840775579912156, 0.7840801826657466, 0.783998803466863, 0.7839813657592962, 0.7839773336153525, 0.7839992452738892, 0.784039892105858, 0.7840253209808203, 0.7841004905254109, 0.7840798956072367, 0.7841845209701471, 0.7841926532908142, 0.7841395906080045, 0.7841708532041809, 0.7842175546823971, 0.7842665614592201, 0.7842804766726558, 0.7842930724623854, 0.7842834462850377, 0.7842514591396919, 0.7842041055672232, 0.7842382172627157, 0.784259581144602, 0.7842462926037852, 0.7842110814289548, 0.7842979379120745, 0.7843226663730819, 0.784321706548712, 0.7843045666651125, 0.7843355716484259], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 527961540, "moving_var_accuracy_train": [0.006218322422926785, 0.01310664249586365, 0.01938925313222886, 0.024822302393641256, 0.02978051774985241, 0.03396295393355041, 0.037364391088078056, 0.04009588520509978, 0.04210986010037585, 0.043528808586035056, 0.04443047813467234, 0.044838219573151684, 0.04475771558833353, 0.04439823807730646, 0.04360231650794145, 0.042553303892848574, 0.04140214325588297, 0.04002769431856843, 0.03842440168995945, 0.0366861467750205, 0.03491963327776521, 0.033124894504774204, 0.031427501559992035, 0.029670762062048685, 0.027990940566035835, 0.026320838554879693, 0.024726520486577246, 0.0231764462107822, 0.02166674890349732, 0.020155565836238506, 0.018725681863983153, 0.017404178725354408, 0.01623783618599732, 0.015079317147099808, 0.013961086168657877, 0.012894426528198477, 0.011947657840857842, 0.011078122550465686, 0.010237136775572947, 0.009476148139614168, 0.00878798080297884, 0.00813646290226089, 0.007504108468224429, 0.006938556279811422, 0.006398726326070817, 0.005896059910917436, 0.005434828994616708, 0.0049977623990556394, 0.004602404356863601, 0.004247805083993874, 0.003913170304261378, 0.003584205504812898, 0.003319364901385933, 0.003090172423593778, 0.002840337473670741, 0.002624909050959873, 0.002434445887158178, 0.0022517183812129113, 0.0020763192013380878, 0.0019201285207795398, 0.0017815973550787346, 0.001646111763301692, 0.0015131781513698962, 0.0013916326778892713, 0.0012836809932682267, 0.0011922944196985528, 0.0011007765734027135, 0.0010148945690982513, 0.0009380465885346154, 0.0008696373476210744, 0.0008053164210945657, 0.0007455911759015144, 0.000687774430156123, 0.0006351959971729753, 0.0005892622508562036, 0.000553977555448495, 0.0005133740444635791, 0.00047288681702504755, 0.0004378545549404274, 0.00040326818667052466, 0.0003760611524014075, 0.0003500570989877325, 0.000324239716828562, 0.0003014953835615568, 0.00027756390361104795, 0.00025925962950684484, 0.00023852468557305714, 0.0002212261996809413, 0.0002039732610756947, 0.00018990608803413287, 0.00017672410433068145, 0.00016487593675650072, 0.00015238014893729533, 0.00014199320624749362, 0.00013006840026860532, 0.00011898073935406105, 0.00010983175165364334, 0.00010049458795169244, 9.174662237789964e-05, 8.54715718386624e-05, 7.788920794786642e-05, 7.126476820791177e-05, 6.771575459795031e-05, 6.334569290070307e-05, 5.8431455595606004e-05, 5.369455318971764e-05, 5.015048962555837e-05, 4.6235584172265154e-05, 4.214180424940598e-05, 3.908000803406958e-05, 3.691968687318969e-05, 3.420289531235602e-05, 3.1754555357875296e-05, 2.940434517786851e-05, 2.674638770307531e-05, 2.465611526723331e-05, 2.2654290987693793e-05, 2.071460629423464e-05, 1.9399590567366824e-05, 1.7477571408798267e-05, 1.631122551824746e-05, 1.4748037516974377e-05, 1.3559063705806457e-05, 1.3239604871852494e-05, 1.1987278892044701e-05, 1.1010357336368545e-05, 1.0678925827227139e-05, 9.738683972501597e-06, 8.928875603691261e-06, 8.304310737797485e-06, 7.576097019680779e-06, 6.9725301324597095e-06, 6.34059741025463e-06, 5.730035677539996e-06, 5.2975899376314895e-06, 4.8636320873847185e-06, 4.488999470747215e-06, 4.0485047259969065e-06, 3.7503561264408074e-06, 3.474627371460795e-06, 3.3118727760074365e-06, 3.081271427470692e-06, 2.925298312107745e-06, 2.776382284741711e-06, 2.620028698596844e-06, 2.494202165320221e-06, 2.2997545904784137e-06, 2.085987184013869e-06, 1.94189458402519e-06, 1.7506541019885436e-06, 1.7337113129839954e-06, 1.5891589638907145e-06, 1.5989736078087394e-06, 1.4952688452622965e-06, 1.4425308941431145e-06, 1.3728192716554366e-06, 1.2664253338152111e-06, 1.1568102630253758e-06, 1.1035621165232359e-06, 1.00941910056469e-06, 9.128028594519783e-07, 8.400269774281827e-07, 7.853528728807425e-07, 7.261237322437798e-07, 6.575163691284278e-07, 6.359943171554361e-07, 5.725558207649384e-07, 5.304885486638673e-07, 4.779023119601808e-07, 4.6095810286529844e-07, 4.7331632037387383e-07, 4.2710108448389893e-07, 3.849144486543369e-07, 4.059698135936894e-07, 3.697499928899619e-07, 3.7522272710227805e-07, 3.492892496341593e-07, 3.193493058941108e-07, 2.874763755522468e-07, 3.183319040955036e-07, 2.892353764925884e-07, 2.604581625063704e-07, 2.387334332742073e-07, 2.2972957448866304e-07, 2.086674762035571e-07, 2.386548724887401e-07, 2.1860674113127098e-07, 2.9526426609556424e-07, 2.663330512408987e-07, 2.650405808795098e-07, 2.473326720686895e-07, 2.422286574699312e-07, 2.396207692941917e-07, 2.174013908494174e-07, 1.9708913703469e-07, 1.7821419294420464e-07, 1.6960137085613186e-07, 1.7282248120039683e-07, 1.660127030047291e-07, 1.5351917174756843e-07, 1.3975652242616683e-07, 1.369393116799506e-07, 1.911418184511913e-07, 1.7753110766021964e-07, 1.597862882595861e-07, 1.4645163992187165e-07, 1.4045825684206709e-07], "duration": 192733.920809, "accuracy_train": [0.2628544680347914, 0.31515589672157623, 0.34563715566168324, 0.37041963708933184, 0.40036575131506097, 0.4236572535991141, 0.44462613049095606, 0.46535924810815804, 0.48279425929771136, 0.5000690334879107, 0.5165979576181248, 0.5312918166297527, 0.5435406645095053, 0.5583293319144518, 0.5670700889973238, 0.5777865015919158, 0.5908738449958472, 0.599030430970838, 0.6045403126730343, 0.6104916121608527, 0.6182543647909745, 0.6247397275286084, 0.6351084492663345, 0.638640873015873, 0.646547820921004, 0.650914414336471, 0.6574942249792359, 0.6620976591339055, 0.6657238098122, 0.6657924828119233, 0.6696536717884828, 0.6753008994901255, 0.6847428060977299, 0.6847631736803249, 0.6858559936208011, 0.6871359069306017, 0.6943885687753784, 0.6989680304655776, 0.6993150003460686, 0.7043398451919527, 0.7094047042266519, 0.7113352987149317, 0.7109894102990033, 0.7159169233342562, 0.7164971290720745, 0.7183100241671281, 0.7209370818337025, 0.7213323571313216, 0.7244466145833334, 0.7280516767026578, 0.7288654787859912, 0.7267030903931341, 0.7352596380121816, 0.7400261930717055, 0.7352607194767442, 0.7397911547734404, 0.7432323750115356, 0.7437453497023809, 0.7438855796073275, 0.7466281737380029, 0.7494884672619048, 0.7493242648924879, 0.748487571809247, 0.7497907366071429, 0.7520439680232558, 0.7555545822374492, 0.7548581190591547, 0.7554619367732558, 0.7572519408684016, 0.7591578419158361, 0.7599011685585086, 0.7608305071059431, 0.7607854460825028, 0.7619262109519195, 0.7638303095584165, 0.767457181213086, 0.7656915100705981, 0.7651323928917497, 0.7669202340577703, 0.766527482177464, 0.7695022307009044, 0.7699897909745294, 0.7698753359749908, 0.7711523653792912, 0.7701307418558508, 0.7728980294273717, 0.7712693437961425, 0.7729677838916574, 0.7726433445228867, 0.7744097366417497, 0.7748954944744371, 0.7757096570459578, 0.7751294513081396, 0.7764773166412883, 0.7748969364271872, 0.7749903028677556, 0.7763610592008121, 0.7756635145579549, 0.775617372069952, 0.7778709639742525, 0.7760366198320414, 0.776686940522333, 0.7797543345099667, 0.7792456856773717, 0.7785692295934846, 0.7784998356173864, 0.7798480614387228, 0.7792911071889996, 0.7785706715462348, 0.7799654003437615, 0.7811515867248062, 0.7804772935700444, 0.78080101196244, 0.7808714874031008, 0.7799178159030085, 0.7808714874031008, 0.7808482359150055, 0.7807076455218714, 0.7818945528792912, 0.7797318039982466, 0.7818716618793835, 0.7804529606173864, 0.7814531350936692, 0.7832427787006275, 0.7810807507959578, 0.7818476894149132, 0.7833590361411037, 0.781918164855574, 0.7821964617363418, 0.7827079944744371, 0.7822197132244371, 0.7825688460340532, 0.7822433252007198, 0.7819875588316721, 0.7827773884505352, 0.7826843824981543, 0.7828700339147286, 0.7821728497600591, 0.7829866518433923, 0.7830571272840532, 0.7835443270694905, 0.7833121726767257, 0.7836609449981543, 0.7837539509505352, 0.7837779234150055, 0.7839632143433923, 0.7836376935100591, 0.7833586756529162, 0.7838233449266334, 0.78324241821244, 0.7844049926172019, 0.7837779234150055, 0.7846378679863418, 0.7841957292243448, 0.7845216105458656, 0.7844983590577703, 0.784265123200443, 0.7841728382244371, 0.7846142560100591, 0.7842890956649132, 0.7841263352482466, 0.7843824621054817, 0.7845452225221484, 0.7835682995339608, 0.7841960897125323, 0.78470726196244, 0.7840340502722407, 0.7836613054863418, 0.7841027232719637, 0.7834527630698597, 0.7831737452127169, 0.7840104382959578, 0.7839864658314876, 0.7847312344269103, 0.784219701688815, 0.784707982938815, 0.7837310599506275, 0.7842894561531008, 0.7841038047365264, 0.7832663906769103, 0.783824426391196, 0.7839410443198597, 0.7841964502007198, 0.7844057135935769, 0.7838941808554817, 0.7847770164267257, 0.7838945413436692, 0.7851261492363418, 0.784265844176818, 0.7836620264627169, 0.7844522165697674, 0.7846378679863418, 0.7847076224506275, 0.7844057135935769, 0.784406434569952, 0.7841968106889073, 0.7839635748315799, 0.7837779234150055, 0.7845452225221484, 0.7844518560815799, 0.7841266957364341, 0.7838941808554817, 0.7850796462601514, 0.7845452225221484, 0.7843130681293835, 0.7841503077127169, 0.7846146164982466], "end": "2016-01-25 21:06:02.608000", "learning_rate_per_epoch": [0.00019144652469549328, 0.00018349224410485476, 0.00017586845206096768, 0.00016856141155585647, 0.00016155796765815467, 0.0001548455038573593, 0.00014841192751191556, 0.0001422456552973017, 0.00013633558410219848, 0.00013067106192465872, 0.00012524190242402256, 0.00012003831216134131, 0.00011505091970320791, 0.00011027074651792645, 0.00010568917787168175, 0.00010129797010449693, 9.708920697448775e-05, 9.305531420977786e-05, 8.918902312871069e-05, 8.548337063984945e-05, 8.193167741410434e-05, 7.85275551606901e-05, 7.526486297138035e-05, 7.213773642433807e-05, 6.914053665241227e-05, 6.626786489505321e-05, 6.351454794639722e-05, 6.0875623603351414e-05, 5.834634430357255e-05, 5.592215165961534e-05, 5.3598680096911266e-05, 5.137174593983218e-05, 4.923733649775386e-05, 4.7191606427077204e-05, 4.523087409324944e-05, 4.335160701884888e-05, 4.155042188358493e-05, 3.982406997238286e-05, 3.816944808932021e-05, 3.658357309177518e-05, 3.506358552840538e-05, 3.36067532771267e-05, 3.221044971724041e-05, 3.087216100539081e-05, 2.9589475161628798e-05, 2.8360082069411874e-05, 2.718176801863592e-05, 2.6052410248667002e-05, 2.4969976948341355e-05, 2.3932516342028975e-05, 2.2938160327612422e-05, 2.1985117200529203e-05, 2.1071671653771773e-05, 2.0196179320919327e-05, 1.935706131916959e-05, 1.8552807887317613e-05, 1.7781969290808775e-05, 1.7043157640728168e-05, 1.6335043255821802e-05, 1.5656349205528386e-05, 1.500585403846344e-05, 1.4382385415956378e-05, 1.3784821021545213e-05, 1.3212084922997747e-05, 1.2663144843827467e-05, 1.2137012163293548e-05, 1.163273918791674e-05, 1.1149418241984677e-05, 1.0686178939067759e-05, 1.024218636302976e-05, 9.81664106802782e-06, 9.408776350028347e-06, 9.017857337312307e-06, 8.643180080980528e-06, 8.284070645458996e-06, 7.93988147052005e-06, 7.609992735524429e-06, 7.293810085684527e-06, 6.9907646320643835e-06, 6.700310223095585e-06, 6.42192344457726e-06, 6.1551031649287324e-06, 5.899368716200115e-06, 5.654259894072311e-06, 5.419334684120258e-06, 5.194170171307633e-06, 4.978361175744794e-06, 4.771518433699384e-06, 4.573269961838378e-06, 4.3832583287439775e-06, 4.201141109660966e-06, 4.026590886496706e-06, 3.859292974084383e-06, 3.6989458749303594e-06, 3.5452608244668227e-06, 3.397961108930758e-06, 3.256781610616599e-06, 3.1214676710078493e-06, 2.991776000271784e-06, 2.8674726308963727e-06, 2.748333827184979e-06, 2.6341451757616596e-06, 2.524700903450139e-06, 2.4198038772738073e-06, 2.3192651497083716e-06, 2.2229035039345035e-06, 2.1305456812115153e-06, 2.042025016635307e-06, 1.9571823486330686e-06, 1.8758646547212265e-06, 1.7979256199396332e-06, 1.7232248410437023e-06, 1.6516277128175716e-06, 1.5830053143872647e-06, 1.5172340681601781e-06, 1.454195512451406e-06, 1.3937760741100647e-06, 1.3358669548324542e-06, 1.280363903788384e-06, 1.2271668765606591e-06, 1.176180148831918e-06, 1.1273118616372813e-06, 1.080473907677515e-06, 1.0355820450058673e-06, 9.925553285938804e-07, 9.51316337705066e-07, 9.11790721147554e-07, 8.739073678043496e-07, 8.375979518859822e-07, 8.027971603041806e-07, 7.694422947679413e-07, 7.374732717835286e-07, 7.068325089676364e-07, 6.774648113605508e-07, 6.493172577393125e-07, 6.223392006177164e-07, 5.964820388726366e-07, 5.716992177440261e-07, 5.479460583046603e-07, 5.251798143035558e-07, 5.033594447922951e-07, 4.824457278118643e-07, 4.624009193321399e-07, 4.4318892378214514e-07, 4.247751519415033e-07, 4.0712646409701847e-07, 3.9021102793412865e-07, 3.7399840380203386e-07, 3.584594026051491e-07, 3.435660005379759e-07, 3.292913959285215e-07, 3.156098955514608e-07, 3.0249682936300815e-07, 2.899285789226269e-07, 2.778825205496105e-07, 2.663369684796635e-07, 2.5527111802148283e-07, 2.446650171350484e-07, 2.3449959485333238e-07, 2.2475653338460688e-07, 2.154182681124439e-07, 2.064680018065701e-07, 1.978896051468837e-07, 1.8966761672345456e-07, 1.8178724303652416e-07, 1.7423428744223202e-07, 1.6699515015261568e-07, 1.600567856030466e-07, 1.5340668824137538e-07, 1.4703289252793184e-07, 1.4092391609210608e-07, 1.3506875973234855e-07, 1.2945687899446057e-07, 1.240781557498849e-07, 1.189229124065605e-07, 1.1398186217093098e-07, 1.0924610904794463e-07, 1.047071123139176e-07, 1.0035670783281603e-07, 9.61870512128371e-08, 9.219063912269121e-08, 8.836027376446509e-08, 8.468904866276716e-08, 8.117036287558221e-08, 7.77978712562799e-08, 7.456549866446949e-08, 7.146742575514509e-08, 6.849807476783099e-08, 6.565209531572691e-08, 6.292435728028067e-08, 6.030995791661553e-08, 5.7804179220966034e-08, 5.5402512799673787e-08, 5.310063144747801e-08, 5.089438914751554e-08, 4.877981396589348e-08, 4.675309384083448e-08, 4.481058013539041e-08, 4.294877697930133e-08, 4.116432705814077e-08, 3.94540187187431e-08, 3.78147717583488e-08, 3.62436303191771e-08, 3.4737766441139684e-08, 3.329446940369962e-08, 3.1911138620444035e-08, 3.058528363908408e-08], "accuracy_valid": [0.25021325536521083, 0.30492428699171686, 0.3308752588478916, 0.3558805534638554, 0.3918824712914157, 0.41667451054216864, 0.43233039580195787, 0.45681652390813254, 0.47310335090361444, 0.49261548145707834, 0.5043857068900602, 0.5203475032944277, 0.5297263271837349, 0.5347106198230422, 0.5414347820971386, 0.5523505153426205, 0.5664194865399097, 0.5676093044051205, 0.5792883447853916, 0.5814959054969879, 0.5882200677710843, 0.5935205666415663, 0.6016080925263554, 0.6039377235504518, 0.6115472632718373, 0.614710796310241, 0.6188817771084337, 0.6200921851468373, 0.6240896201995482, 0.6258088996611446, 0.6258691994540663, 0.634486186935241, 0.6431443547628012, 0.6396543204066265, 0.6446797933923193, 0.6474359351468373, 0.6522584478539157, 0.6533673757530121, 0.6563176534262049, 0.6589929052146084, 0.6610372152673193, 0.6642419286521084, 0.6637742375753012, 0.6669068853539157, 0.6657170674887049, 0.6690026708396084, 0.6735707478350903, 0.6761342243975903, 0.6744561252823795, 0.6766225056475903, 0.6763474797628012, 0.6768254659262049, 0.6833981433546686, 0.6866940417921686, 0.6863278308546686, 0.6859513248305723, 0.6881691806287651, 0.6903561511671686, 0.6866734516189759, 0.6876603092055723, 0.6913533038403614, 0.6924622317394578, 0.6914650790662651, 0.6924416415662651, 0.6956257647778614, 0.6944050616528614, 0.6936726397778614, 0.696887648249247, 0.6973656344126506, 0.6986775225903614, 0.6991966891001506, 0.6989319583019578, 0.6989319583019578, 0.6995629000376506, 0.7000305911144578, 0.7029705737010542, 0.7047104433358433, 0.7007527179028614, 0.7020954913403614, 0.7031029391001506, 0.7053207948983433, 0.7060326266001506, 0.7078945665474398, 0.7072739198983433, 0.7056664156626506, 0.707507765436747, 0.708850538874247, 0.7070091891001506, 0.7065414980233433, 0.7093285250376506, 0.7113125352974398, 0.7100506518260542, 0.710071241999247, 0.7107933687876506, 0.708850538874247, 0.7079548663403614, 0.7107021837349398, 0.7097153261483433, 0.7083416674510542, 0.7115154955760542, 0.7093285250376506, 0.7083725527108433, 0.7123802828501506, 0.7132553652108433, 0.710681593561747, 0.7113934252635542, 0.7133568453501506, 0.712878859186747, 0.7093285250376506, 0.7111492846385542, 0.7115463808358433, 0.7126244234751506, 0.712512648249247, 0.713611281061747, 0.7107830737010542, 0.712878859186747, 0.7120037768260542, 0.7131332948983433, 0.7130112245858433, 0.7116787462349398, 0.7126244234751506, 0.7126450136483433, 0.7120346620858433, 0.711780226374247, 0.713245070124247, 0.712146437311747, 0.713855421686747, 0.7121361422251506, 0.711902296686747, 0.7129803393260542, 0.7107830737010542, 0.7137436464608433, 0.715442335749247, 0.713245070124247, 0.713489210749247, 0.7139980821724398, 0.7142422227974398, 0.7138657167733433, 0.713855421686747, 0.7150864198983433, 0.7138657167733433, 0.713000929499247, 0.7139877870858433, 0.713733351374247, 0.714587843561747, 0.714709913874247, 0.7153305605233433, 0.715564406061747, 0.714587843561747, 0.7136318712349398, 0.7134789156626506, 0.7139877870858433, 0.7148422792733433, 0.7152084902108433, 0.7157070665474398, 0.7157173616340362, 0.7160629823983433, 0.7147202089608433, 0.7153511506965362, 0.7149643495858433, 0.7149643495858433, 0.7145981386483433, 0.714709913874247, 0.714099562311747, 0.7134995058358433, 0.7138657167733433, 0.7150864198983433, 0.7135098009224398, 0.7143539980233433, 0.7132553652108433, 0.7149849397590362, 0.7147202089608433, 0.713489210749247, 0.714709913874247, 0.7141098573983433, 0.714099562311747, 0.713733351374247, 0.713611281061747, 0.7134995058358433, 0.7134789156626506, 0.7137230562876506, 0.7139877870858433, 0.714343702936747, 0.713122999811747, 0.7143539980233433, 0.7150967149849398, 0.7133568453501506, 0.7149746446724398, 0.713611281061747, 0.712512648249247, 0.7139877870858433, 0.7148422792733433, 0.7137436464608433, 0.7134789156626506, 0.7150864198983433, 0.7146996187876506, 0.7138760118599398, 0.713977491999247, 0.7136215761483433, 0.7153305605233433, 0.7145775484751506, 0.7147202089608433, 0.7142319277108433, 0.7153511506965362, 0.715076124811747, 0.714099562311747, 0.7153305605233433], "accuracy_test": 0.7098772321428571, "start": "2016-01-23 15:33:48.687000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0], "accuracy_train_last": 0.7846146164982466, "batch_size_eval": 1024, "accuracy_train_std": [0.01676335531192094, 0.017450512845319302, 0.015975483651729763, 0.015523697273397304, 0.01364972638681837, 0.012543560798527587, 0.012720074511366706, 0.014010719445572212, 0.01480293441506502, 0.01341074413854273, 0.012338938756374753, 0.013402139446733439, 0.0132675552829613, 0.015600995857683977, 0.016280310082537133, 0.016905290424114473, 0.017594617158010483, 0.017357706985930532, 0.017168767910435933, 0.01731240264993438, 0.016035615974212374, 0.016527790036305185, 0.01624214431175729, 0.015720428751099726, 0.014137160353587872, 0.015515655236881407, 0.014988443828735546, 0.015061933137562674, 0.016152781610581184, 0.017003849647850434, 0.015297139127102378, 0.016402584790559824, 0.016460599307862537, 0.01480227837532135, 0.015795099469559967, 0.015667029839728862, 0.014782735919377227, 0.015124861350796935, 0.01484789480068338, 0.014246885570399914, 0.015816513134982335, 0.01570883366375767, 0.014094327057425264, 0.014060194154580888, 0.0145889447557433, 0.014765691753592346, 0.016478228431309732, 0.014657964070681007, 0.015128716122513815, 0.015164827679846452, 0.013847704883294577, 0.013332855322649084, 0.014349684154988792, 0.012144525206194415, 0.013833844859757655, 0.014171072103030194, 0.01336188777948681, 0.012621874942377735, 0.013449391743344646, 0.014802816780016928, 0.013483297169686595, 0.014273078688495098, 0.013455674537959855, 0.014425913546591963, 0.012625171777574313, 0.01284748831240651, 0.013358205659606113, 0.013326151972780875, 0.012933525581916405, 0.013134431501795727, 0.012029613747053637, 0.012561944863784582, 0.012139399298345952, 0.013215350271915081, 0.012060794644626163, 0.012633610163768726, 0.01259679692023857, 0.01377888720642922, 0.01203538391237704, 0.013298223232973272, 0.012740902786310057, 0.011732196917792483, 0.012732289213859521, 0.012337389298987676, 0.012076352818548367, 0.011851133939551634, 0.012549659492523848, 0.012157654687092942, 0.011745351432380535, 0.011492188758798358, 0.013010849123754905, 0.01194331394624652, 0.01180003196983433, 0.011439153370815266, 0.011500142537229897, 0.01203338274885504, 0.012085404013983779, 0.01155540611645072, 0.011860138523591595, 0.012030200382930106, 0.011928843291116913, 0.011031141199033548, 0.012340762810026025, 0.011058339788726945, 0.011822011920721995, 0.01201036852983474, 0.012058710055062372, 0.012525213173509029, 0.011666149993827713, 0.012208785749932687, 0.012320029250068167, 0.012652034793798584, 0.012091864501765831, 0.012540131230501888, 0.012211581298568473, 0.012183790753173761, 0.011701042822425631, 0.011906779913315837, 0.011457701391192511, 0.011440341722717011, 0.012331485440182904, 0.0122732280930557, 0.01163142330735216, 0.013012978589304466, 0.011609328967897342, 0.011963075018320416, 0.01167960238971062, 0.011734582976544313, 0.012227105085502414, 0.012361773617543334, 0.011781688404161786, 0.012083399040911663, 0.012228302789549509, 0.012128081528277667, 0.012104191867357762, 0.01217539079847457, 0.012238158283021348, 0.011820640725110588, 0.01186443678081643, 0.012041778458498666, 0.011974410785995357, 0.012287080833110392, 0.011879577931210372, 0.01232413419792986, 0.012313441542409988, 0.012126250217873804, 0.011220919688390901, 0.012139704231873627, 0.01180461670549512, 0.012096605470681764, 0.01215507898163611, 0.011921808273788486, 0.012637116572867754, 0.011870820153291793, 0.012293841743870547, 0.012306465355184456, 0.011810646166306091, 0.012102244881288679, 0.012267501258865887, 0.011850534887793356, 0.012000384420152627, 0.011734926633248866, 0.01209034189310865, 0.011737972242372025, 0.01187782223803612, 0.01172624374256159, 0.012092245380215355, 0.011821463668503817, 0.012337688501706398, 0.012193840453944279, 0.011563713817852836, 0.012416138853429003, 0.011992015526123353, 0.011658648249938593, 0.011864568604111786, 0.011985270171464084, 0.012299644500065933, 0.012064353204923684, 0.011597352009804761, 0.012152300415883445, 0.012091587117520574, 0.01219920553174715, 0.012451976410337316, 0.012136861027113876, 0.012456919452647807, 0.01201653986798959, 0.011894736430468712, 0.01172249732389628, 0.012219573562752188, 0.012025524674929889, 0.011658771188297765, 0.012023899758472992, 0.011319627659489068, 0.012294856888951034, 0.011719995452220595, 0.012157716315020599, 0.011835647474857615, 0.012125762791967394, 0.01204141301768284, 0.011770040038070155, 0.011902732561210307, 0.011887971152830147, 0.012105370999339832, 0.011967644631157265, 0.011675862729563228, 0.0115327723290012, 0.011930925568596845], "accuracy_test_std": 0.012003963049605125, "error_valid": [0.7497867446347892, 0.6950757130082832, 0.6691247411521084, 0.6441194465361446, 0.6081175287085843, 0.5833254894578314, 0.5676696041980421, 0.5431834760918675, 0.5268966490963856, 0.5073845185429217, 0.49561429310993976, 0.4796524967055723, 0.4702736728162651, 0.46528938017695776, 0.4585652179028614, 0.4476494846573795, 0.4335805134600903, 0.4323906955948795, 0.4207116552146084, 0.41850409450301207, 0.41177993222891573, 0.40647943335843373, 0.3983919074736446, 0.39606227644954817, 0.3884527367281627, 0.38528920368975905, 0.38111822289156627, 0.3799078148531627, 0.37591037980045183, 0.3741911003388554, 0.37413080054593373, 0.36551381306475905, 0.3568556452371988, 0.3603456795933735, 0.3553202066076807, 0.3525640648531627, 0.34774155214608427, 0.34663262424698793, 0.34368234657379515, 0.3410070947853916, 0.3389627847326807, 0.3357580713478916, 0.3362257624246988, 0.33309311464608427, 0.33428293251129515, 0.3309973291603916, 0.3264292521649097, 0.3238657756024097, 0.3255438747176205, 0.3233774943524097, 0.3236525202371988, 0.32317453407379515, 0.31660185664533136, 0.31330595820783136, 0.31367216914533136, 0.3140486751694277, 0.3118308193712349, 0.30964384883283136, 0.31332654838102414, 0.3123396907944277, 0.3086466961596386, 0.30753776826054224, 0.3085349209337349, 0.3075583584337349, 0.3043742352221386, 0.3055949383471386, 0.3063273602221386, 0.303112351750753, 0.30263436558734935, 0.3013224774096386, 0.30080331089984935, 0.30106804169804224, 0.30106804169804224, 0.30043709996234935, 0.29996940888554224, 0.2970294262989458, 0.2952895566641567, 0.2992472820971386, 0.2979045086596386, 0.29689706089984935, 0.2946792051016567, 0.29396737339984935, 0.29210543345256024, 0.2927260801016567, 0.29433358433734935, 0.292492234563253, 0.291149461125753, 0.29299081089984935, 0.2934585019766567, 0.29067147496234935, 0.28868746470256024, 0.2899493481739458, 0.289928758000753, 0.28920663121234935, 0.291149461125753, 0.2920451336596386, 0.28929781626506024, 0.2902846738516567, 0.2916583325489458, 0.2884845044239458, 0.29067147496234935, 0.2916274472891567, 0.28761971714984935, 0.2867446347891567, 0.289318406438253, 0.2886065747364458, 0.28664315464984935, 0.287121140813253, 0.29067147496234935, 0.2888507153614458, 0.2884536191641567, 0.28737557652484935, 0.287487351750753, 0.286388718938253, 0.2892169262989458, 0.287121140813253, 0.2879962231739458, 0.2868667051016567, 0.2869887754141567, 0.28832125376506024, 0.28737557652484935, 0.2873549863516567, 0.2879653379141567, 0.288219773625753, 0.286754929875753, 0.287853562688253, 0.286144578313253, 0.28786385777484935, 0.288097703313253, 0.2870196606739458, 0.2892169262989458, 0.2862563535391567, 0.284557664250753, 0.286754929875753, 0.286510789250753, 0.28600191782756024, 0.28575777720256024, 0.2861342832266567, 0.286144578313253, 0.2849135801016567, 0.2861342832266567, 0.286999070500753, 0.2860122129141567, 0.286266648625753, 0.285412156438253, 0.285290086125753, 0.2846694394766567, 0.284435593938253, 0.285412156438253, 0.28636812876506024, 0.28652108433734935, 0.2860122129141567, 0.2851577207266567, 0.2847915097891567, 0.28429293345256024, 0.2842826383659638, 0.2839370176016567, 0.2852797910391567, 0.2846488493034638, 0.2850356504141567, 0.2850356504141567, 0.2854018613516567, 0.285290086125753, 0.285900437688253, 0.2865004941641567, 0.2861342832266567, 0.2849135801016567, 0.28649019907756024, 0.2856460019766567, 0.2867446347891567, 0.2850150602409638, 0.2852797910391567, 0.286510789250753, 0.285290086125753, 0.2858901426016567, 0.285900437688253, 0.286266648625753, 0.286388718938253, 0.2865004941641567, 0.28652108433734935, 0.28627694371234935, 0.2860122129141567, 0.285656297063253, 0.286877000188253, 0.2856460019766567, 0.28490328501506024, 0.28664315464984935, 0.28502535532756024, 0.286388718938253, 0.287487351750753, 0.2860122129141567, 0.2851577207266567, 0.2862563535391567, 0.28652108433734935, 0.2849135801016567, 0.28530038121234935, 0.28612398814006024, 0.286022508000753, 0.2863784238516567, 0.2846694394766567, 0.28542245152484935, 0.2852797910391567, 0.2857680722891567, 0.2846488493034638, 0.284923875188253, 0.285900437688253, 0.2846694394766567], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-3, 3], "momentum": 0.9172452439813913, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.00019974563145667588, "patience_threshold": 1, "do_flip": true, "batch_size": 32, "optimization": "nesterov_momentum", "nb_data_augmentation": 2, "learning_rate_decay_method": "exp", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.000905100862972813, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.041548329615713864}, "accuracy_valid_max": 0.7160629823983433, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-3, 3)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.7153305605233433, "loss_train": [23.250158309936523, 22.020002365112305, 20.948223114013672, 19.985326766967773, 19.103408813476562, 18.291358947753906, 17.541318893432617, 16.84805679321289, 16.206157684326172, 15.613663673400879, 15.060709953308105, 14.549718856811523, 14.073227882385254, 13.6298189163208, 13.217750549316406, 12.83275032043457, 12.476112365722656, 12.142932891845703, 11.830992698669434, 11.539027214050293, 11.268854141235352, 11.012808799743652, 10.772432327270508, 10.55001163482666, 10.339428901672363, 10.140233993530273, 9.955718040466309, 9.779600143432617, 9.614706993103027, 9.458701133728027, 9.310824394226074, 9.171297073364258, 9.039539337158203, 8.914617538452148, 8.796483039855957, 8.684965133666992, 8.580662727355957, 8.4811372756958, 8.386091232299805, 8.296152114868164, 8.208601951599121, 8.130255699157715, 8.054508209228516, 7.979184150695801, 7.910877227783203, 7.846850395202637, 7.7833638191223145, 7.723840236663818, 7.667718887329102, 7.614448070526123, 7.5630202293396, 7.513330936431885, 7.46806526184082, 7.422958850860596, 7.380568981170654, 7.339828968048096, 7.300990104675293, 7.264590263366699, 7.22914457321167, 7.194418907165527, 7.164050102233887, 7.133612155914307, 7.103951930999756, 7.077885627746582, 7.049478530883789, 7.024359703063965, 7.000443935394287, 6.97667932510376, 6.953855991363525, 6.933721542358398, 6.912156581878662, 6.892788410186768, 6.8717169761657715, 6.85709285736084, 6.838909149169922, 6.821821212768555, 6.806830406188965, 6.792169094085693, 6.777313709259033, 6.764364242553711, 6.751339435577393, 6.738674163818359, 6.724989891052246, 6.7146172523498535, 6.702490329742432, 6.695415496826172, 6.6815032958984375, 6.672895431518555, 6.663527488708496, 6.655287265777588, 6.646886825561523, 6.637587070465088, 6.628971099853516, 6.620887279510498, 6.615854740142822, 6.605843544006348, 6.6008687019348145, 6.594455242156982, 6.588099956512451, 6.583634376525879, 6.577592849731445, 6.571442604064941, 6.565656661987305, 6.562298774719238, 6.557620048522949, 6.552121639251709, 6.548459053039551, 6.543054103851318, 6.539096355438232, 6.535698413848877, 6.534122943878174, 6.528009414672852, 6.526234149932861, 6.523687362670898, 6.518614292144775, 6.514715194702148, 6.512134552001953, 6.509172439575195, 6.508298873901367, 6.504895210266113, 6.504100322723389, 6.501203536987305, 6.500154972076416, 6.496712684631348, 6.494089126586914, 6.491114616394043, 6.490372657775879, 6.48712158203125, 6.4875288009643555, 6.48257303237915, 6.483835697174072, 6.480957984924316, 6.480312824249268, 6.4792327880859375, 6.477912425994873, 6.4772467613220215, 6.475666046142578, 6.4735541343688965, 6.474298000335693, 6.471719264984131, 6.471462726593018, 6.470494270324707, 6.469569206237793, 6.467024803161621, 6.467784404754639, 6.465031147003174, 6.465099334716797, 6.464884281158447, 6.46457052230835, 6.463587284088135, 6.4621806144714355, 6.461052894592285, 6.461843013763428, 6.460738182067871, 6.459769248962402, 6.459747314453125, 6.459010601043701, 6.4576897621154785, 6.457951068878174, 6.456024646759033, 6.456543445587158, 6.45863151550293, 6.454806327819824, 6.45662260055542, 6.454731464385986, 6.454279899597168, 6.455294132232666, 6.456270217895508, 6.454349994659424, 6.452895641326904, 6.4527106285095215, 6.453701019287109, 6.451980113983154, 6.451961040496826, 6.452547550201416, 6.452389240264893, 6.453104496002197, 6.451488018035889, 6.452071189880371, 6.449774265289307, 6.449862480163574, 6.451837539672852, 6.450820446014404, 6.449832439422607, 6.450713157653809, 6.4496002197265625, 6.449499607086182, 6.451159477233887, 6.449653625488281, 6.45081901550293, 6.447737216949463, 6.449225902557373, 6.45095157623291, 6.447710990905762, 6.450610637664795, 6.449321746826172, 6.450442314147949, 6.4469099044799805, 6.449213027954102, 6.448847770690918, 6.447847366333008, 6.449570655822754, 6.4484477043151855, 6.447568416595459, 6.449400901794434, 6.447868824005127, 6.448110580444336], "accuracy_train_first": 0.2628544680347914, "model": "residualv5", "loss_std": [0.4047870934009552, 0.338270366191864, 0.3050820231437683, 0.2831512689590454, 0.26518040895462036, 0.2514808773994446, 0.23862959444522858, 0.22718438506126404, 0.21805822849273682, 0.20998017489910126, 0.2035396248102188, 0.1980109065771103, 0.1943599432706833, 0.18956491351127625, 0.18615637719631195, 0.18353450298309326, 0.1818370223045349, 0.1797226220369339, 0.17694924771785736, 0.1745128184556961, 0.17339354753494263, 0.1706947386264801, 0.16958948969841003, 0.1690540313720703, 0.1669720560312271, 0.16703350841999054, 0.16541333496570587, 0.16676673293113708, 0.16542358696460724, 0.16510751843452454, 0.16520282626152039, 0.1648545116186142, 0.1648060530424118, 0.162968248128891, 0.16251078248023987, 0.16298972070217133, 0.1609031707048416, 0.16299517452716827, 0.16271784901618958, 0.15970300137996674, 0.1602601557970047, 0.1614857316017151, 0.16096088290214539, 0.16086773574352264, 0.16240017116069794, 0.16044631600379944, 0.16134940087795258, 0.15887027978897095, 0.15788982808589935, 0.16084302961826324, 0.15789271891117096, 0.15824103355407715, 0.1589404046535492, 0.15808378159999847, 0.15873983502388, 0.15670321881771088, 0.16008810698986053, 0.15849032998085022, 0.15623946487903595, 0.15780439972877502, 0.1564796417951584, 0.15505191683769226, 0.15674251317977905, 0.15704086422920227, 0.15586644411087036, 0.15641459822654724, 0.1555526852607727, 0.15583005547523499, 0.15534767508506775, 0.15553849935531616, 0.15613780915737152, 0.15455500781536102, 0.15413682162761688, 0.15390180051326752, 0.1544026881456375, 0.1550968587398529, 0.1543467491865158, 0.15371641516685486, 0.15432476997375488, 0.15379613637924194, 0.15359477698802948, 0.15168872475624084, 0.1526038497686386, 0.1522117406129837, 0.1525198519229889, 0.15211988985538483, 0.15125325322151184, 0.15321855247020721, 0.15203671157360077, 0.15248002111911774, 0.15235690772533417, 0.15116465091705322, 0.15239793062210083, 0.15200583636760712, 0.15215015411376953, 0.1509552299976349, 0.15107104182243347, 0.1508111208677292, 0.15095733106136322, 0.15198899805545807, 0.15174347162246704, 0.1495901495218277, 0.14885474741458893, 0.15061728656291962, 0.1506279706954956, 0.14998579025268555, 0.14997254312038422, 0.15139439702033997, 0.15076152980327606, 0.15077270567417145, 0.15052682161331177, 0.14978361129760742, 0.15228186547756195, 0.1495286524295807, 0.1506618857383728, 0.1492113471031189, 0.149063378572464, 0.14965873956680298, 0.14965178072452545, 0.15059338510036469, 0.15094491839408875, 0.1494973748922348, 0.14932064712047577, 0.15112102031707764, 0.14875726401805878, 0.1484457552433014, 0.15077528357505798, 0.1482284665107727, 0.150090292096138, 0.14844948053359985, 0.14887192845344543, 0.1504274159669876, 0.15004846453666687, 0.14896447956562042, 0.1508444845676422, 0.14988122880458832, 0.14938300848007202, 0.14915788173675537, 0.1484224796295166, 0.14893430471420288, 0.1496584713459015, 0.14898507297039032, 0.15007400512695312, 0.1492692232131958, 0.14918683469295502, 0.14904074370861053, 0.149198517203331, 0.14836010336875916, 0.14802691340446472, 0.15052096545696259, 0.14962181448936462, 0.1494884490966797, 0.1485060304403305, 0.14732928574085236, 0.14850953221321106, 0.1484534740447998, 0.14829121530056, 0.1453569531440735, 0.14942239224910736, 0.148351788520813, 0.15024077892303467, 0.15098492801189423, 0.14772891998291016, 0.14855670928955078, 0.1484314352273941, 0.14839054644107819, 0.14816904067993164, 0.14919576048851013, 0.14790529012680054, 0.1487542688846588, 0.149190291762352, 0.15075601637363434, 0.14810311794281006, 0.1482335478067398, 0.1463666558265686, 0.14739990234375, 0.14941413700580597, 0.1492706537246704, 0.14811532199382782, 0.14746367931365967, 0.1481073945760727, 0.14889037609100342, 0.14852681756019592, 0.14728176593780518, 0.15006037056446075, 0.14974066615104675, 0.14878451824188232, 0.14913520216941833, 0.14865773916244507, 0.14929887652397156, 0.14787091314792633, 0.14807996153831482, 0.14871853590011597, 0.14838668704032898, 0.14814533293247223, 0.14800752699375153, 0.14923126995563507, 0.1489018350839615, 0.14974641799926758, 0.14688177406787872, 0.14779646694660187, 0.14929524064064026, 0.14944931864738464, 0.14850759506225586, 0.14917916059494019, 0.14878205955028534, 0.14962513744831085]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:14 2016", "state": "available"}], "summary": "5091d32c4129b4907a0ac6154a3565c3"}