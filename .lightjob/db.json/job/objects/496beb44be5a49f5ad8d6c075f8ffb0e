{"content": {"hp_model": {"f0": 32, "f1": 32, "f2": 32, "f3": 32, "nonlin": "rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.9928468465805054, 1.6967854499816895, 1.6133487224578857, 1.5423905849456787, 1.489216685295105, 1.4473997354507446, 1.4122487306594849, 1.3800970315933228, 1.3522241115570068, 1.3255393505096436, 1.3037827014923096, 1.2818931341171265, 1.2627662420272827, 1.2440006732940674, 1.2275292873382568, 1.2132625579833984, 1.198529839515686, 1.185897946357727, 1.172282099723816, 1.1627557277679443, 1.1509137153625488, 1.1400964260101318, 1.1312156915664673, 1.1224396228790283, 1.1147407293319702, 1.105951189994812, 1.0979928970336914, 1.0906431674957275, 1.0844653844833374, 1.0783331394195557, 1.0719058513641357, 1.0670719146728516, 1.061706304550171, 1.0559312105178833, 1.051282525062561, 1.0451668500900269, 1.0422654151916504, 1.0358834266662598, 1.0317997932434082, 1.029598593711853, 1.0252050161361694, 1.0206326246261597, 1.0167810916900635, 1.0157783031463623, 1.0124183893203735, 1.008851408958435, 1.0046898126602173, 1.0033820867538452, 1.0003337860107422, 0.997314989566803, 0.9952963590621948, 0.992732584476471, 0.9900787472724915, 0.9880669713020325, 0.985546886920929, 0.9842528700828552, 0.9820162653923035, 0.9810541272163391, 0.9797524213790894, 0.978320837020874, 0.9759258031845093, 0.9743046760559082, 0.9725047945976257, 0.9698987603187561, 0.9697659611701965, 0.9682427048683167, 0.9671816825866699, 0.9663830399513245, 0.9659954309463501, 0.9632594585418701, 0.9615650773048401, 0.962790310382843, 0.9612796902656555, 0.9595929980278015, 0.961211621761322, 0.9591894745826721, 0.9577039480209351, 0.9585902690887451, 0.9565390348434448, 0.9548949003219604, 0.9558644890785217, 0.9543339610099792, 0.9527686834335327, 0.9534091353416443, 0.9518749713897705, 0.9531497359275818, 0.95197594165802, 0.9523503184318542, 0.9521548748016357, 0.9500374794006348, 0.9494485855102539, 0.9497160315513611, 0.9500582218170166, 0.9491193294525146, 0.9483612775802612, 0.9475521445274353, 0.9471156001091003, 0.9481987953186035, 0.9475308060646057, 0.9477096796035767, 0.9463335275650024, 0.9464195370674133, 0.9457578659057617, 0.945123016834259, 0.9445579051971436, 0.9438527822494507, 0.9454907774925232, 0.9444441795349121, 0.9457294940948486, 0.9444937109947205, 0.9450705647468567, 0.9441516399383545, 0.9445669651031494, 0.9430941939353943, 0.9440863728523254, 0.9417217969894409, 0.9445710778236389, 0.9428777098655701, 0.9422330260276794, 0.9430694580078125, 0.9427738785743713, 0.9419108629226685, 0.9421342611312866, 0.9414663314819336, 0.9407356381416321, 0.9430416226387024, 0.9421402215957642, 0.9426522254943848, 0.9409098029136658, 0.9421523809432983, 0.9416725635528564, 0.9406467080116272, 0.9404618740081787, 0.9417902231216431, 0.9416637420654297, 0.9406076669692993, 0.940941572189331, 0.9395130276679993, 0.9411416053771973, 0.9409326910972595, 0.9407243728637695, 0.9417656064033508, 0.9421418905258179, 0.9401821494102478, 0.9413814544677734, 0.9408172369003296, 0.9401768445968628, 0.9405518770217896, 0.9403861165046692, 0.9410221576690674, 0.9399068355560303, 0.9397759437561035, 0.9394844770431519, 0.9414127469062805, 0.9395233392715454, 0.9397282600402832, 0.9400739073753357, 0.940422534942627, 0.9402854442596436, 0.9410613179206848, 0.9400426745414734, 0.9395259022712708, 0.9400862455368042, 0.9402595162391663, 0.9396908283233643, 0.9399923086166382, 0.9404550194740295, 0.9412277340888977, 0.940200686454773, 0.9401424527168274, 0.9387539029121399, 0.9399561285972595, 0.9405033588409424, 0.9410008788108826, 0.9399544596672058, 0.9410638809204102, 0.9408159852027893, 0.9385939836502075, 0.9393411874771118, 0.9399417042732239, 0.9410929083824158, 0.9399557113647461, 0.9394426345825195, 0.9404800534248352, 0.9397797584533691, 0.9404354691505432, 0.940133273601532, 0.9398048520088196, 0.9394645094871521, 0.9395825266838074, 0.9400889873504639, 0.9390212893486023, 0.940417468547821, 0.9396411180496216, 0.940261721611023, 0.9403154850006104, 0.9395023584365845, 0.9391068816184998, 0.9396070241928101, 0.9401329159736633, 0.9403392672538757, 0.9393170475959778, 0.9374095797538757, 0.9408162236213684, 0.9399711489677429, 0.9394032955169678, 0.939972460269928, 0.9401882886886597, 0.9397478699684143, 0.9387551546096802, 0.9378033876419067, 0.9394410252571106, 0.9402498602867126, 0.9379151463508606, 0.9388707280158997, 0.9399532675743103, 0.9403545260429382, 0.9400211572647095, 0.9367039203643799, 0.939327597618103, 0.939176619052887, 0.9398661255836487, 0.9401952028274536, 0.941249668598175, 0.9394119381904602, 0.9398781657218933, 0.9401487708091736, 0.9394378066062927, 0.9399705529212952, 0.9391534924507141, 0.94140625, 0.9391958713531494, 0.9394686818122864, 0.9384329915046692, 0.9395695328712463, 0.9393518567085266, 0.9389377236366272, 0.9395409822463989], "moving_avg_accuracy_train": [0.040458823529411755, 0.08148588235294116, 0.12158199999999997, 0.1598285058823529, 0.19464094941176466, 0.22756508976470585, 0.2584062278470588, 0.28702678153294114, 0.314648809262, 0.34029216362991765, 0.36443235903163174, 0.38708324077552736, 0.40839609316856285, 0.4287306014987654, 0.4474646001724183, 0.46517461074341176, 0.4816406790808353, 0.4970671994080459, 0.5113628324084178, 0.5247418432852231, 0.5372041295449361, 0.5488131283551484, 0.5596871096372806, 0.5700431045559056, 0.5796693823356092, 0.5883871499844012, 0.596877846750667, 0.6046418267814827, 0.6118647029268639, 0.6187535267518245, 0.6252758211354656, 0.6312823566689779, 0.6371023562961978, 0.6424650618430486, 0.6476891438940379, 0.6524072883281635, 0.6569595006718179, 0.6611506094281655, 0.6650590778971137, 0.66878140540152, 0.6722703236848975, 0.6755468207281725, 0.6785686092435905, 0.6814341012604079, 0.684069514663779, 0.6866060926091657, 0.6889031304070727, 0.6911210526604832, 0.6932207121003172, 0.6952774644196972, 0.6970767768012569, 0.6988185108858371, 0.7004401892090182, 0.7020314644057634, 0.7034871414945988, 0.704893721462786, 0.706187878728272, 0.7074302673260331, 0.7086142994169593, 0.7097434577105576, 0.7108561707630312, 0.7117917301573163, 0.7126502042004083, 0.713467536721544, 0.714292547755272, 0.7149927047444506, 0.7158134342700055, 0.7163967967253579, 0.7170018229351751, 0.7176098759357753, 0.7181712412833743, 0.7187141171550369, 0.719216823086592, 0.7196621996014623, 0.7200889208177866, 0.7204964993242433, 0.7207950846859366, 0.7212026350408722, 0.7215741362426674, 0.7219155461478124, 0.722182815062443, 0.7224445335561986, 0.7227483154946965, 0.7229511310040504, 0.7231877826095277, 0.7234290043485748, 0.7236672803843056, 0.7238958464635221, 0.724181555934817, 0.7243798709295707, 0.7245018838366136, 0.7246540483941287, 0.7248309964958923, 0.7249690733168913, 0.7250227542204963, 0.7252710670337409, 0.7254616073891904, 0.725588387826742, 0.7257142549264206, 0.7258463588455433, 0.7259676053139301, 0.7262320212531254, 0.7263758779513423, 0.7264041725091492, 0.7264908140817637, 0.726589967967705, 0.7267286182297581, 0.7269286975832528, 0.7269605337072805, 0.7270738921012584, 0.727128855832309, 0.7272253820137841, 0.7273404908712291, 0.7273546770782239, 0.7274780328998133, 0.7274996413745378, 0.7275496772370841, 0.7275500036310227, 0.7275855915032146, 0.7277093852940695, 0.7278184467646625, 0.7278907197352551, 0.7278169418793767, 0.7278399535737919, 0.7279406640987657, 0.7279842447477127, 0.728063467331765, 0.7280759441280003, 0.7281036438328472, 0.7281779853319155, 0.7282307750340181, 0.7282641681188516, 0.7283036336599076, 0.7283579761762699, 0.7284092373821723, 0.7285118430557198, 0.7285571293383832, 0.7285155340516037, 0.7286004512346786, 0.7287051119935637, 0.728693424323619, 0.7286923171853747, 0.7287477913491902, 0.7288047769201536, 0.7287972404046088, 0.7288916340112068, 0.7288801176689097, 0.7289662235490775, 0.7289213659000522, 0.7288268763688706, 0.7288547769672776, 0.7288728286823146, 0.7288890752258479, 0.7288966382914983, 0.7289528568152897, 0.728994041721996, 0.7289769904909729, 0.7290204679124638, 0.7290125387682762, 0.7290383437149781, 0.7291015681670097, 0.7290431760561911, 0.7290165055093955, 0.7289901490761029, 0.7289970165214338, 0.7290243736928199, 0.7291289951470674, 0.7291996250441254, 0.7291902507750069, 0.7292571080504473, 0.7292608090101085, 0.7292829634032153, 0.7292276082393644, 0.729180141533075, 0.729224480320944, 0.7291608558182614, 0.7291882996482, 0.7291377049774976, 0.729174522715042, 0.7292758939729496, 0.7293130104580076, 0.7293370035298539, 0.7293444796474566, 0.7293347375650638, 0.7293071461614986, 0.7292893727218193, 0.7293510236849315, 0.729373568375262, 0.7293750350671475, 0.7293951786192563, 0.7293286019338012, 0.7293510358580682, 0.7293218146252025, 0.7292955155156234, 0.729210669846414, 0.7292590146264785, 0.7291637013991247, 0.7291108606709769, 0.7291150687215263, 0.7291659147905502, 0.7292916762526716, 0.7292636850979927, 0.7292620224705463, 0.7292840555176093, 0.7292897676129072, 0.72931843791044, 0.729306594119396, 0.7293241700015741, 0.7292764588837697, 0.7292735188777456, 0.7293391081664417, 0.7293393149968563, 0.7292995011442295, 0.729256609853336, 0.7292674194562376, 0.7292536186870844, 0.7291353156419054, 0.7291088429012442, 0.7291579586111199, 0.7292256921617726, 0.7292090052985365, 0.7292151635922122, 0.7292465884094615, 0.7292725178038095, 0.7292723248469579, 0.729269798244615, 0.7291592890083888, 0.7292174777546088, 0.7292345535085596, 0.7292946275694683, 0.7292522236360509, 0.7292705306842104, 0.7292634776157894, 0.729245365148328, 0.7292737698099658, 0.7291934516524986, 0.7292646947225428, 0.7292488134855827], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.04035999999999999, 0.08140399999999998, 0.12150359999999998, 0.15897990666666664, 0.19332191599999995, 0.2257363910666666, 0.2563894186266666, 0.2852704767639999, 0.31289009575426663, 0.3384544195121733, 0.3623556442276227, 0.3847200798048604, 0.405994738491041, 0.42591526464193685, 0.44451040484440985, 0.46180603102663553, 0.4777587612573053, 0.4929162184649081, 0.506811263285084, 0.5198501369565756, 0.5319317899275846, 0.5433652776014928, 0.5540020831746768, 0.5639752081905425, 0.5731510207048216, 0.5815025853010061, 0.5894456601042388, 0.5967144274271483, 0.6035363180177667, 0.6098226862159901, 0.6160404175943912, 0.6215297091682854, 0.6268834049181236, 0.6319150644263112, 0.6366968913170135, 0.6408805355186454, 0.6447258153001142, 0.6484799004367694, 0.6523252437264258, 0.6556793860204498, 0.6588181140850715, 0.6616429693432311, 0.664318672408908, 0.6669134718346839, 0.6695954579845488, 0.6719292455194272, 0.6739096543008178, 0.6758920222040694, 0.6779028199836625, 0.6796458713186296, 0.6812946175201, 0.6828584891014233, 0.684172640191281, 0.6854887095054862, 0.6867665052216043, 0.6879831880327771, 0.6890648692294994, 0.6898783823065495, 0.6907705440758946, 0.691693489668305, 0.6926974740348079, 0.6936543932979937, 0.6945689539681944, 0.6953787252380417, 0.6961475193809041, 0.6968661007761471, 0.6976994906985324, 0.6983295416286792, 0.6990965874658113, 0.6997602620525636, 0.7003309025139739, 0.7008444789292432, 0.7014666977029855, 0.701920027932687, 0.7024213584727517, 0.7027925559588099, 0.7031666336962622, 0.7034633036599693, 0.703863639960639, 0.7041706092979084, 0.7044868817014509, 0.7048115268646392, 0.7052637075115086, 0.705470670093691, 0.7056169364176552, 0.7058819094425564, 0.7063203851649674, 0.7065816799818041, 0.7068968453169571, 0.7070471607852614, 0.7071824447067353, 0.7073842002360617, 0.7074991135457889, 0.7076158688578766, 0.7077609486387557, 0.7079715204415468, 0.7081477017307254, 0.7084129315576528, 0.7085316384018876, 0.7087184745616989, 0.7088732937721957, 0.7090126310616428, 0.7090580346221451, 0.7091922311599306, 0.7093396747106041, 0.709459040572877, 0.7095531365155893, 0.7098911561973636, 0.709888707244294, 0.7099531698531979, 0.7100378528678781, 0.7101274009144237, 0.710181327489648, 0.7102965280740166, 0.7103735419332816, 0.7104428544066201, 0.7105052356326248, 0.710481378736029, 0.7106332408624261, 0.7105965834428501, 0.7105769250985652, 0.7106658992553754, 0.7106393093298379, 0.7108287117301874, 0.710799173890502, 0.7108392565014517, 0.7109286641846398, 0.7110624644328425, 0.7110762179895582, 0.7112085961906024, 0.7111277365715422, 0.7110816295810546, 0.7111067999562826, 0.7110894532939877, 0.7111005079645889, 0.7111237905014633, 0.711211411451317, 0.7112502703061854, 0.7113385766089002, 0.7114980522813436, 0.7115882470532092, 0.711616089014555, 0.7116011467797662, 0.711654365435123, 0.7115689288916107, 0.711505369335783, 0.7115148324022047, 0.7115900158286509, 0.7115510142457858, 0.7114759128212071, 0.7115416548724198, 0.7115074893851778, 0.7114367404466599, 0.7115463997353273, 0.7116584264284613, 0.7117992504522819, 0.711912658740387, 0.7119213928663484, 0.7118092535797136, 0.7118283282217421, 0.7117921620662346, 0.7116929458596111, 0.7116969846069833, 0.7116739528129516, 0.7116132241983232, 0.7116252351118242, 0.7115827116006418, 0.7116244404405776, 0.7115819963965199, 0.7116237967568679, 0.7115547504145145, 0.7116659420397298, 0.7117126811690901, 0.7117014130521812, 0.7117179384136296, 0.7117461445722666, 0.7117981967817066, 0.7116183771035359, 0.7117098727265156, 0.7117522187871974, 0.7117369969084777, 0.7117899638842966, 0.711810967495867, 0.711736537412947, 0.7117362170049856, 0.7117092619711537, 0.7117516691073716, 0.7118965021966345, 0.7119601853103044, 0.7120175001126072, 0.7118824167680131, 0.7118808417578785, 0.7118394242487573, 0.7118554818238816, 0.7117366003081601, 0.7117762736106774, 0.7117453129162764, 0.7118374482913155, 0.7117603701288506, 0.711717666449299, 0.7118525664710358, 0.711813976490599, 0.7118192455082057, 0.7117706542907185, 0.71172692219498, 0.711794229975482, 0.7118948069779338, 0.7118786596134737, 0.7118907936521264, 0.7118083809535805, 0.711694209524889, 0.7116714552390668, 0.7115443097151601, 0.7115232120769774, 0.7116375575359464, 0.7116871351156852, 0.7117717549374499, 0.7117279127770382, 0.711701788166001, 0.711651609349401, 0.7116197817477943, 0.7115378035730149, 0.7114906898823801, 0.7115149542274753, 0.7115367921380611, 0.7115297795909217, 0.7115634682984961, 0.7116204548019799, 0.7116050759884486, 0.7116312350562704, 0.7114814448839766, 0.7115199670622455, 0.7114746370226877, 0.7114205066537522, 0.7114784559883771, 0.7113839437228727, 0.7115655493505854, 0.7116889944155268], "moving_var_accuracy_train": [0.014732247612456744, 0.02840799885259515, 0.04003648682063667, 0.04919799504845273, 0.05518535156400381, 0.05942280756942496, 0.06204110899641531, 0.06320922293635205, 0.0637550883855009, 0.06329781415609823, 0.06221277404678485, 0.060609058636089876, 0.05863629186662676, 0.05649409274124377, 0.05400334782385924, 0.05142581331129562, 0.048723414638600374, 0.04599287093939291, 0.04323286995138551, 0.04052056434462203, 0.03786628511953111, 0.03529257628795758, 0.03282750987947928, 0.03050997856832261, 0.02829296772651848, 0.026147666208871555, 0.024181726972174484, 0.022306068748227176, 0.0205449913317081, 0.018917595241759403, 0.017408698633825347, 0.01599253499248092, 0.014698133054180381, 0.013487147245802363, 0.01238405182070134, 0.011345994620742643, 0.01039789889366384, 0.00951619753776526, 0.00870206291594359, 0.007956558122799768, 0.0072704552676125574, 0.006640028636722608, 0.006058206625537558, 0.005526285363469802, 0.005036165461382827, 0.00459045696430175, 0.0041788987116766945, 0.0038052814526085815, 0.0034644304352172807, 0.0031560594626250296, 0.002869591241780433, 0.0026099348561948853, 0.002372609935830273, 0.002158138353013236, 0.0019613954797945555, 0.0017830621366772491, 0.0016198295102598187, 0.0014717383240844565, 0.0013371818796070964, 0.0012149386777144007, 0.001104587982977268, 0.0010020066271016574, 0.0009084387635354546, 0.0008236071792328627, 0.0007473722501615324, 0.0006770470034308411, 0.0006154046756748155, 0.0005569270138961669, 0.0005045288229376415, 0.00045740349670772687, 0.00041449932651831806, 0.0003757018217747869, 0.0003404060588798947, 0.0003081506951518866, 0.0002789744446048501, 0.0002525720822946938, 0.00022811725302918217, 0.00020680040335253837, 0.00018736248130370156, 0.00016967527968331158, 0.0001533506457695304, 0.00013863205032234115, 0.00012559939648552424, 0.00011340966401448206, 0.00010257273345440877, 9.283915145546867e-05, 8.406621553275388e-05, 7.612977605259411e-05, 6.925146756522293e-05, 6.268028034299767e-05, 5.654623665406351e-05, 5.109999946173109e-05, 4.6271795192017645e-05, 4.1816202549290593e-05, 3.7660517049068174e-05, 3.444939862315387e-05, 3.1331209404331666e-05, 2.8342747978010347e-05, 2.5651055921243177e-05, 2.324301333814693e-05, 2.1051018359198787e-05, 1.9575158623383358e-05, 1.7803895507641753e-05, 1.6030711194890998e-05, 1.44952009343479e-05, 1.3134164278788403e-05, 1.1993762907415925e-05, 1.1154672345928322e-05, 1.0048326960473425e-05, 9.159145393793164e-06, 8.2704199599929e-06, 7.527233697384865e-06, 6.893760769207284e-06, 6.206195928506633e-06, 5.72252626413588e-06, 5.1544759733415876e-06, 4.661560663874121e-06, 4.195405556283737e-06, 3.787263470479652e-06, 3.5464612473200073e-06, 3.2988647618992884e-06, 3.0159887262138377e-06, 2.763378401754658e-06, 2.4918064042979476e-06, 2.3339092524325415e-06, 2.117611783853028e-06, 1.9623365658829715e-06, 1.767503943293343e-06, 1.5976590118015234e-06, 1.4876330369747593e-06, 1.3639505071099854e-06, 1.2375913394312478e-06, 1.1278499658657233e-06, 1.0416429510403128e-06, 9.611280570114214e-07, 9.597665695074554e-07, 8.822475391338598e-07, 8.09594296160898e-07, 7.935332183772205e-07, 8.127647665931056e-07, 7.327177045924007e-07, 6.594569659289877e-07, 6.212077149953103e-07, 5.883131411779296e-07, 5.29993018659146e-07, 5.571850934923279e-07, 5.026602194022432e-07, 5.191222008573511e-07, 4.853198588563933e-07, 5.17142316497171e-07, 4.724340753707021e-07, 4.28123447575604e-07, 3.8768665440903966e-07, 3.4943278862644124e-07, 3.4293421151929466e-07, 3.2390655923104354e-07, 2.941326036225746e-07, 2.817319188758302e-07, 2.541245689361708e-07, 2.3470516951109117e-07, 2.472106345722452e-07, 2.5317631856770475e-07, 2.3426054930831132e-07, 2.1708644856059015e-07, 1.958022599528853e-07, 1.829577673937972e-07, 2.631728288540184e-07, 2.817527871943794e-07, 2.5436840076848217e-07, 2.69160618205569e-07, 2.423678303067336e-07, 2.2254840148143504e-07, 2.278713088179784e-07, 2.2536197178983814e-07, 2.2051912759805706e-07, 2.348999109127676e-07, 2.1818839403676347e-07, 2.1940794096440355e-07, 2.096670590489301e-07, 2.811855405119027e-07, 2.6546568762824956e-07, 2.441001263350229e-07, 2.2019314471122533e-07, 1.9902800376422236e-07, 1.859767733440831e-07, 1.7022215243196708e-07, 1.874075084627126e-07, 1.7324112517527072e-07, 1.5593637332352842e-07, 1.439946002152062e-07, 1.6948723560930322e-07, 1.5706804067050644e-07, 1.4904616065514708e-07, 1.4036633307150233e-07, 1.9111878801666805e-07, 1.9304186905034975e-07, 2.554991839225685e-07, 2.5507854849098064e-07, 2.297300628467167e-07, 2.300249611786348e-07, 3.4936597325511194e-07, 3.214809185919343e-07, 2.893577057029684e-07, 2.647910315986018e-07, 2.3860558073297175e-07, 2.2214289630525064e-07, 2.0119108515137263e-07, 1.8385218134526298e-07, 1.8595412007010813e-07, 1.6743650078188932e-07, 1.8941044382856368e-07, 1.7046978445509116e-07, 1.6768909175848672e-07, 1.674771480932607e-07, 1.51781060917966e-07, 1.3831710588914053e-07, 2.504458897879007e-07, 2.3170855479210873e-07, 2.3024887592213402e-07, 2.485144932861371e-07, 2.2616910659946063e-07, 2.0389351716848749e-07, 1.923918377040453e-07, 1.7920365535491224e-07, 1.6128362491054e-07, 1.4521271589407622e-07, 2.406020659263642e-07, 2.470152310135675e-07, 2.2493794026913402e-07, 2.3492418138883083e-07, 2.276146053733644e-07, 2.0786947694689607e-07, 1.875302412195741e-07, 1.7172977039544101e-07, 1.6181821658070886e-07, 2.0369545269292946e-07, 2.2900608268761665e-07, 2.0837539760532037e-07], "duration": 272416.356052, "accuracy_train": [0.40458823529411764, 0.4507294117647059, 0.4824470588235294, 0.5040470588235294, 0.5079529411764706, 0.5238823529411765, 0.5359764705882353, 0.5446117647058824, 0.5632470588235294, 0.5710823529411765, 0.5816941176470588, 0.5909411764705882, 0.6002117647058823, 0.6117411764705882, 0.6160705882352941, 0.6245647058823529, 0.629835294117647, 0.6359058823529412, 0.6400235294117647, 0.6451529411764706, 0.649364705882353, 0.6532941176470588, 0.6575529411764706, 0.6632470588235294, 0.6663058823529412, 0.6668470588235295, 0.6732941176470588, 0.6745176470588236, 0.6768705882352941, 0.6807529411764706, 0.6839764705882353, 0.6853411764705882, 0.6894823529411764, 0.6907294117647059, 0.6947058823529412, 0.6948705882352941, 0.6979294117647059, 0.6988705882352941, 0.7002352941176471, 0.7022823529411765, 0.7036705882352942, 0.7050352941176471, 0.705764705882353, 0.7072235294117647, 0.7077882352941176, 0.709435294117647, 0.7095764705882353, 0.7110823529411765, 0.7121176470588235, 0.7137882352941176, 0.7132705882352941, 0.7144941176470588, 0.7150352941176471, 0.7163529411764706, 0.7165882352941176, 0.7175529411764706, 0.717835294117647, 0.7186117647058824, 0.7192705882352941, 0.7199058823529412, 0.7208705882352942, 0.7202117647058823, 0.7203764705882353, 0.7208235294117648, 0.7217176470588236, 0.7212941176470589, 0.7232, 0.7216470588235294, 0.7224470588235294, 0.7230823529411765, 0.7232235294117647, 0.7236, 0.7237411764705882, 0.7236705882352941, 0.7239294117647059, 0.7241647058823529, 0.7234823529411765, 0.7248705882352942, 0.7249176470588236, 0.7249882352941176, 0.7245882352941176, 0.7248, 0.7254823529411765, 0.7247764705882352, 0.7253176470588235, 0.7256, 0.7258117647058824, 0.7259529411764706, 0.7267529411764706, 0.7261647058823529, 0.7256, 0.7260235294117647, 0.7264235294117647, 0.7262117647058823, 0.7255058823529412, 0.7275058823529412, 0.7271764705882353, 0.7267294117647058, 0.7268470588235294, 0.7270352941176471, 0.7270588235294118, 0.7286117647058824, 0.7276705882352941, 0.7266588235294118, 0.7272705882352941, 0.7274823529411765, 0.7279764705882353, 0.7287294117647058, 0.7272470588235294, 0.7280941176470588, 0.7276235294117647, 0.7280941176470588, 0.7283764705882353, 0.7274823529411765, 0.7285882352941176, 0.7276941176470588, 0.728, 0.7275529411764706, 0.7279058823529412, 0.7288235294117648, 0.7288, 0.7285411764705882, 0.7271529411764706, 0.7280470588235294, 0.7288470588235294, 0.7283764705882353, 0.7287764705882352, 0.7281882352941177, 0.7283529411764705, 0.7288470588235294, 0.7287058823529412, 0.7285647058823529, 0.7286588235294118, 0.7288470588235294, 0.7288705882352942, 0.7294352941176471, 0.728964705882353, 0.7281411764705882, 0.7293647058823529, 0.7296470588235294, 0.7285882352941176, 0.7286823529411764, 0.7292470588235294, 0.7293176470588235, 0.7287294117647058, 0.7297411764705882, 0.7287764705882352, 0.7297411764705882, 0.7285176470588235, 0.7279764705882353, 0.7291058823529412, 0.729035294117647, 0.729035294117647, 0.728964705882353, 0.7294588235294117, 0.7293647058823529, 0.7288235294117648, 0.7294117647058823, 0.7289411764705882, 0.7292705882352941, 0.7296705882352941, 0.7285176470588235, 0.7287764705882352, 0.7287529411764706, 0.7290588235294118, 0.7292705882352941, 0.7300705882352941, 0.729835294117647, 0.7291058823529412, 0.7298588235294118, 0.7292941176470589, 0.7294823529411765, 0.7287294117647058, 0.7287529411764706, 0.7296235294117647, 0.7285882352941176, 0.7294352941176471, 0.7286823529411764, 0.7295058823529412, 0.7301882352941177, 0.7296470588235294, 0.7295529411764706, 0.7294117647058823, 0.7292470588235294, 0.7290588235294118, 0.7291294117647059, 0.7299058823529412, 0.7295764705882353, 0.7293882352941177, 0.7295764705882353, 0.7287294117647058, 0.7295529411764706, 0.7290588235294118, 0.7290588235294118, 0.7284470588235294, 0.7296941176470588, 0.7283058823529411, 0.728635294117647, 0.7291529411764706, 0.7296235294117647, 0.7304235294117647, 0.7290117647058824, 0.7292470588235294, 0.7294823529411765, 0.7293411764705883, 0.7295764705882353, 0.7292, 0.7294823529411765, 0.7288470588235294, 0.7292470588235294, 0.7299294117647059, 0.7293411764705883, 0.7289411764705882, 0.7288705882352942, 0.7293647058823529, 0.7291294117647059, 0.7280705882352941, 0.7288705882352942, 0.7296, 0.729835294117647, 0.7290588235294118, 0.7292705882352941, 0.7295294117647059, 0.7295058823529412, 0.7292705882352941, 0.7292470588235294, 0.7281647058823529, 0.7297411764705882, 0.7293882352941177, 0.729835294117647, 0.7288705882352942, 0.7294352941176471, 0.7292, 0.7290823529411765, 0.7295294117647059, 0.7284705882352941, 0.7299058823529412, 0.7291058823529412], "end": "2016-02-08 19:05:48.055000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0, 211.0, 212.0, 213.0, 214.0, 215.0, 216.0, 217.0, 218.0, 219.0, 220.0, 221.0, 222.0, 223.0, 224.0, 225.0, 226.0, 227.0, 228.0, 229.0, 230.0, 231.0, 232.0, 233.0, 234.0, 235.0, 236.0, 237.0], "moving_var_accuracy_valid": [0.014660366399999998, 0.028355819183999993, 0.03999203854703999, 0.0486330967447024, 0.05438414951568894, 0.05840201830874804, 0.061018289365220316, 0.06242350010088654, 0.06304674026930539, 0.06262387808516649, 0.061502907162735494, 0.05985412825465769, 0.05794221534911325, 0.05571944007335867, 0.053259509218369444, 0.050625806461850116, 0.04785363223197783, 0.04513600558978284, 0.042360055465796825, 0.0396541599588072, 0.037002441009533524, 0.03447871867208487, 0.032049121500091864, 0.029739378353321445, 0.027523200335664105, 0.025398617982935877, 0.023426588120610044, 0.021559444114100407, 0.01982234342376369, 0.01819577490750002, 0.016724139068195587, 0.015322916059225063, 0.01404858297693907, 0.012871583055902181, 0.011790217566025748, 0.010768721718675814, 0.009824925136188187, 0.008969271019488672, 0.008205423902677554, 0.007486133947166849, 0.006826185077222958, 0.006215384834566626, 0.005658280833171018, 0.005153049606393978, 0.004702482093127187, 0.004281252962136055, 0.0038884258363951307, 0.0035349512952901952, 0.0032178459351549227, 0.002923405393246405, 0.0026555301302535316, 0.0024119883661340154, 0.0021863324673033805, 0.001983287566531176, 0.001799653666907225, 0.001633011153783535, 0.0014802403463072646, 0.0013381725434153213, 0.0012115188626779171, 0.0010980334335090759, 0.0009973019516318071, 0.0009058130067549324, 0.0008227594970547395, 0.0007463851129344955, 0.0006770660015479431, 0.0006140066343874522, 0.0005588568198133073, 0.0005065438154031857, 0.0004611846677092219, 0.000419030376552206, 0.00038005801372277255, 0.0003444260589593829, 0.0003134678588850215, 0.0002839706476709698, 0.00025783557369748626, 0.00023329210449064074, 0.0002112223014244939, 0.0001908921888883383, 0.00017324539238220983, 0.00015676892471020153, 0.0001419922863383643, 0.00012874160804236156, 0.00011770765327475462, 0.00010632238954109193, 9.588269512471682e-05, 8.692632194757252e-05, 7.996403838511018e-05, 7.258210937835013e-05, 6.62178611368536e-05, 5.979942768327209e-05, 5.398420056962899e-05, 4.895212815519035e-05, 4.4175760958443376e-05, 3.988087108870547e-05, 3.6082217265213996e-05, 3.2873059895868825e-05, 2.9865112526191828e-05, 2.7511723023400382e-05, 2.488737255487383e-05, 2.2712805054903548e-05, 2.0657245440862816e-05, 1.8766254818850726e-05, 1.69081826867223e-05, 1.5379442814832592e-05, 1.4037154939066515e-05, 1.2761673326845296e-05, 1.1565192412075022e-05, 1.1436988918269353e-05, 1.0293344002782656e-05, 9.301408454024709e-06, 8.435808525400194e-06, 7.664397346621231e-06, 6.924130291597902e-06, 6.351157834187781e-06, 5.769422261439012e-06, 5.2357180059378125e-06, 4.747168961564618e-06, 4.277574429044805e-06, 4.057375935044956e-06, 3.6637322392301574e-06, 3.300837069807395e-06, 3.0420009680473474e-06, 2.7441640885034384e-06, 2.79260710297664e-06, 2.5211987484385e-06, 2.2835384148996354e-06, 2.1271281777273438e-06, 2.0755379177265206e-06, 1.8696865688548708e-06, 1.8404338049746275e-06, 1.7152349264282632e-06, 1.5628441249318062e-06, 1.412261642540633e-06, 1.2737436385215375e-06, 1.147469126348301e-06, 1.0376009024232608e-06, 1.002937689860252e-06, 9.162340162893032e-07, 8.947926425527761e-07, 1.034205789208706e-06, 1.0040010821349003e-06, 9.105775472256104e-07, 8.215292259274097e-07, 7.648663308364536e-07, 7.540743244587701e-07, 7.150252462460732e-07, 6.443286682563795e-07, 6.307687299404649e-07, 5.813819681402568e-07, 5.740057870898178e-07, 5.555033640596092e-07, 5.10458552319983e-07, 5.044614078005163e-07, 5.622417033397082e-07, 6.189673527764548e-07, 7.355532686638531e-07, 7.77750900095922e-07, 7.006623746930993e-07, 7.43773113686529e-07, 6.726703800345639e-07, 6.17175259268872e-07, 6.440524342527253e-07, 5.797939941504829e-07, 5.265887665622859e-07, 5.071215716183337e-07, 4.5770777284466025e-07, 4.282112365897223e-07, 4.0106177767225215e-07, 3.771690717888173e-07, 3.5517759573694465e-07, 3.6256641269477963e-07, 4.3758196908730255e-07, 4.134846880988442e-07, 3.732789534170368e-07, 3.3840884621438125e-07, 3.117282480584628e-07, 3.049403158208968e-07, 5.654623341554205e-07, 5.842591419599412e-07, 5.419719274613197e-07, 4.898600850410144e-07, 4.661235812835139e-07, 4.2348158844613863e-07, 4.309919647928793e-07, 3.8789369226494673e-07, 3.556434876783564e-07, 3.36264425730398e-07, 4.914275968660927e-07, 4.787846878797101e-07, 4.6047109815892467e-07, 5.786515782234725e-07, 5.208087463134434e-07, 4.841665622383269e-07, 4.3807051748433643e-07, 5.214587987580577e-07, 4.834786572759281e-07, 4.437578729285065e-07, 4.757824316378922e-07, 4.816735766347735e-07, 4.4991865719660905e-07, 5.687089342581546e-07, 5.252407201434321e-07, 4.7296651104795273e-07, 4.469198176951853e-07, 4.194403017048103e-07, 4.1826930737927925e-07, 4.674839774410474e-07, 4.2308221610799336e-07, 3.8209910854340394e-07, 4.0501587362373587e-07, 4.818303224262335e-07, 4.383071078931304e-07, 5.399702553493528e-07, 4.899792228464014e-07, 5.586552564430829e-07, 5.249111585134866e-07, 5.368646707817304e-07, 5.004774189696051e-07, 4.565721347892258e-07, 4.3357614402878195e-07, 3.993354956422534e-07, 4.198857363395303e-07, 3.978744613126844e-07, 3.633858411675628e-07, 3.313393060995875e-07, 2.9864795784607473e-07, 2.78997523223828e-07, 2.8032492511513765e-07, 2.5442100375430387e-07, 2.351375748426275e-07, 4.1355767880035984e-07, 3.855575348876289e-07, 3.6549509377574525e-07, 3.553164559679475e-07, 3.5000793882223254e-07, 3.954002599169355e-07, 6.526856700776441e-07, 7.245652595955009e-07], "accuracy_test": 0.7056, "start": "2016-02-05 15:25:31.699000", "learning_rate_per_epoch": [0.00032786716474220157, 0.0003142640634905547, 0.0003012253437191248, 0.00028872760594822466, 0.0002767483820207417, 0.00026526619330979884, 0.0002542603760957718, 0.0002437111979816109, 0.0002335996978217736, 0.00022390771482605487, 0.00021461785945575684, 0.00020571342611219734, 0.00019717843679245561, 0.0001889975683297962, 0.00018115612328983843, 0.00017364001541864127, 0.00016643574053887278, 0.00015953037654981017, 0.00015291151066776365, 0.00014656725397799164, 0.00014048622688278556, 0.0001346575008938089, 0.00012907059863209724, 0.00012371549382805824, 0.00011858257494168356, 0.00011366262333467603, 0.00010894679871853441, 0.0001044266318785958, 0.00010009400284616277, 9.594113362254575e-05, 9.196056635119021e-05, 8.81451487657614e-05, 8.448803419014439e-05, 8.098265243461356e-05, 7.762270979583263e-05, 7.440216722898185e-05, 7.131524762371555e-05, 6.83563994243741e-05, 6.552031118189916e-05, 6.28018970019184e-05, 6.019626744091511e-05, 5.769874405814335e-05, 5.530484122573398e-05, 5.301026249071583e-05, 5.081088238512166e-05, 4.870275370194577e-05, 4.668209294322878e-05, 4.4745269406121224e-05, 4.288880154490471e-05, 4.110935697099194e-05, 3.940374153899029e-05, 3.776889207074419e-05, 3.620187271735631e-05, 3.469986768322997e-05, 3.326018122606911e-05, 3.1880226742941886e-05, 3.0557526770280674e-05, 2.9289703888935037e-05, 2.807448254316114e-05, 2.690968176466413e-05, 2.5793207896640524e-05, 2.472305641276762e-05, 2.3697304641245864e-05, 2.2714111764798872e-05, 2.1771709725726396e-05, 2.086840868287254e-05, 2.0002584278699942e-05, 1.917268309625797e-05, 1.837721538322512e-05, 1.7614751413930207e-05, 1.688392148935236e-05, 1.618341229914222e-05, 1.5511966921621934e-05, 1.4868380276311655e-05, 1.4251495485950727e-05, 1.3660205695487093e-05, 1.309344770561438e-05, 1.2550204701256007e-05, 1.202950079459697e-05, 1.1530401025083847e-05, 1.105200863094069e-05, 1.0593464139674325e-05, 1.0153944458579645e-05, 9.732660146255512e-06, 9.328855412604753e-06, 8.941804480855353e-06, 8.570811587560456e-06, 8.215210982598364e-06, 7.874364200688433e-06, 7.547659151896369e-06, 7.234509212139528e-06, 6.934351858944865e-06, 6.6466477619542275e-06, 6.370880328177009e-06, 6.1065543377480935e-06, 5.853195489180507e-06, 5.610348125628661e-06, 5.377576599130407e-06, 5.1544625421229284e-06, 4.940605322190095e-06, 4.735621132567758e-06, 4.539141627901699e-06, 4.350813924247632e-06, 4.170300144323846e-06, 3.99727559852181e-06, 3.8314296944008674e-06, 3.6724647998198634e-06, 3.520095333442441e-06, 3.3740477647370426e-06, 3.2340594771085307e-06, 3.099879450019216e-06, 2.9712664399994537e-06, 2.847989435394993e-06, 2.729827201619628e-06, 2.6165675990341697e-06, 2.5080071281990968e-06, 2.403950702500879e-06, 2.304211648151977e-06, 2.2086105673224665e-06, 2.1169760202610632e-06, 2.0291433884267462e-06, 1.944954874488758e-06, 1.8642592749529285e-06, 1.7869117527880007e-06, 1.7127733826782787e-06, 1.6417109236499527e-06, 1.5735968190710992e-06, 1.5083087419043295e-06, 1.4457294810199528e-06, 1.3857466001354624e-06, 1.3282524378155358e-06, 1.2731436527246842e-06, 1.2203213373140898e-06, 1.169690563074255e-06, 1.12116049422184e-06, 1.0746439329523128e-06, 1.0300573194399476e-06, 9.873206181509886e-07, 9.463569767831359e-07, 9.070928967958025e-07, 8.69457892349601e-07, 8.333843766195059e-07, 7.988075481080159e-07, 7.656652769583161e-07, 7.338980481108592e-07, 7.034488476165279e-07, 6.742629921063781e-07, 6.462880151048012e-07, 6.194737238729431e-07, 5.937719720350287e-07, 5.691365458915243e-07, 5.455232781059749e-07, 5.228897066444915e-07, 5.011951884625887e-07, 4.804007858183468e-07, 4.6046909574215533e-07, 4.4136439214526035e-07, 4.230523131809605e-07, 4.0550000335315417e-07, 3.886759429860831e-07, 3.725498913809133e-07, 3.570929152374447e-07, 3.422772465455637e-07, 3.2807628258524346e-07, 3.144645006614155e-07, 3.014174581039697e-07, 2.8891173542433535e-07, 2.7692487947206246e-07, 2.654353465914028e-07, 2.5442253104301926e-07, 2.438666228954389e-07, 2.337486790793264e-07, 2.2405052391150093e-07, 2.147547348840817e-07, 2.0584462845363305e-07, 1.9730420319774566e-07, 1.8911811139332713e-07, 1.8127165901660192e-07, 1.7375074889969255e-07, 1.665418807306196e-07, 1.596321084207375e-07, 1.5300902589387988e-07, 1.466607244537954e-07, 1.405758212058572e-07, 1.3474337379193457e-07, 1.291529088121024e-07, 1.2379439340293175e-07, 1.1865819971035307e-07, 1.1373511199508357e-07, 1.0901627689463567e-07, 1.044932247395991e-07, 1.0015783402650413e-07, 9.600231720696684e-08, 9.201921358226173e-08, 8.82013679870397e-08, 8.454192368390068e-08, 8.103430815253887e-08, 7.767221887888809e-08, 7.444962335512173e-08, 7.136073065794335e-08, 6.839999855401402e-08, 6.55621050782429e-08, 6.284195563921458e-08, 6.023466170290703e-08, 5.773554434540529e-08, 5.534011648933301e-08, 5.3044072245711504e-08, 5.084329046667335e-08, 4.873381698189405e-08, 4.6711864598592e-08, 4.477380244338747e-08, 4.2916152409588904e-08, 4.113557494633824e-08, 3.942887261132455e-08, 3.7792979412643035e-08, 3.6224960808795004e-08, 3.472199949783317e-08, 3.3281395417361637e-08, 3.1900562191822246e-08, 3.0577016474353513e-08, 2.9308385052218e-08, 2.8092388859590756e-08, 2.6926844753916157e-08, 2.580965841048055e-08, 2.4738824322412256e-08, 2.37124186952542e-08, 2.2728597670607087e-08, 2.1785595549772552e-08, 2.0881717688325807e-08, 2.001534227247248e-08, 1.9184911437264418e-08, 1.8388934819313363e-08, 1.7625984227720437e-08, 1.6894688315005624e-08, 1.6193732577107767e-08, 1.552185935338457e-08, 1.4877862497542083e-08, 1.4260584713099433e-08], "accuracy_train_first": 0.40458823529411764, "accuracy_train_last": 0.7291058823529412, "batch_size_eval": 1024, "accuracy_train_std": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "accuracy_test_std": 0, "error_valid": [0.5964, 0.5492, 0.5176000000000001, 0.5037333333333334, 0.49760000000000004, 0.48253333333333337, 0.46773333333333333, 0.4548, 0.43853333333333333, 0.43146666666666667, 0.4225333333333333, 0.41400000000000003, 0.4025333333333333, 0.39480000000000004, 0.38813333333333333, 0.3825333333333333, 0.3786666666666667, 0.3706666666666667, 0.3681333333333333, 0.3628, 0.3593333333333333, 0.35373333333333334, 0.3502666666666666, 0.3462666666666666, 0.3442666666666667, 0.3433333333333334, 0.3390666666666666, 0.33786666666666665, 0.3350666666666666, 0.3336, 0.32799999999999996, 0.3290666666666666, 0.3249333333333333, 0.3228, 0.3202666666666667, 0.3214666666666667, 0.32066666666666666, 0.3177333333333333, 0.3130666666666667, 0.3141333333333334, 0.3129333333333333, 0.3129333333333333, 0.3116, 0.3097333333333333, 0.3062666666666667, 0.3070666666666667, 0.3082666666666667, 0.3062666666666667, 0.30400000000000005, 0.30466666666666664, 0.3038666666666666, 0.3030666666666667, 0.30400000000000005, 0.30266666666666664, 0.3017333333333333, 0.3010666666666667, 0.3012, 0.30279999999999996, 0.3012, 0.30000000000000004, 0.2982666666666667, 0.2977333333333333, 0.2972, 0.29733333333333334, 0.2969333333333334, 0.29666666666666663, 0.29479999999999995, 0.29600000000000004, 0.29400000000000004, 0.2942666666666667, 0.2945333333333333, 0.2945333333333333, 0.2929333333333334, 0.29400000000000004, 0.2930666666666667, 0.2938666666666667, 0.29346666666666665, 0.2938666666666667, 0.2925333333333333, 0.2930666666666667, 0.29266666666666663, 0.2922666666666667, 0.29066666666666663, 0.29266666666666663, 0.2930666666666667, 0.2917333333333333, 0.2897333333333333, 0.2910666666666667, 0.2902666666666667, 0.29159999999999997, 0.29159999999999997, 0.29079999999999995, 0.29146666666666665, 0.29133333333333333, 0.2909333333333334, 0.29013333333333335, 0.2902666666666667, 0.2892, 0.2904, 0.28959999999999997, 0.2897333333333333, 0.2897333333333333, 0.2905333333333333, 0.28959999999999997, 0.28933333333333333, 0.28946666666666665, 0.28959999999999997, 0.2870666666666667, 0.29013333333333335, 0.28946666666666665, 0.2892, 0.2890666666666667, 0.28933333333333333, 0.2886666666666666, 0.2889333333333334, 0.2889333333333334, 0.2889333333333334, 0.2897333333333333, 0.28800000000000003, 0.2897333333333333, 0.28959999999999997, 0.2885333333333333, 0.28959999999999997, 0.28746666666666665, 0.28946666666666665, 0.28879999999999995, 0.28826666666666667, 0.2877333333333333, 0.28879999999999995, 0.28759999999999997, 0.28959999999999997, 0.28933333333333333, 0.2886666666666666, 0.2890666666666667, 0.28879999999999995, 0.2886666666666666, 0.28800000000000003, 0.2884, 0.2878666666666667, 0.2870666666666667, 0.28759999999999997, 0.28813333333333335, 0.2885333333333333, 0.2878666666666667, 0.2892, 0.2890666666666667, 0.2884, 0.2877333333333333, 0.28879999999999995, 0.2892, 0.2878666666666667, 0.28879999999999995, 0.2892, 0.28746666666666665, 0.28733333333333333, 0.2869333333333334, 0.2870666666666667, 0.28800000000000003, 0.2892, 0.28800000000000003, 0.2885333333333333, 0.2892, 0.28826666666666667, 0.2885333333333333, 0.2889333333333334, 0.28826666666666667, 0.28879999999999995, 0.28800000000000003, 0.28879999999999995, 0.28800000000000003, 0.2890666666666667, 0.28733333333333333, 0.2878666666666667, 0.2884, 0.28813333333333335, 0.28800000000000003, 0.2877333333333333, 0.29000000000000004, 0.28746666666666665, 0.2878666666666667, 0.2884, 0.2877333333333333, 0.28800000000000003, 0.2889333333333334, 0.28826666666666667, 0.2885333333333333, 0.2878666666666667, 0.28680000000000005, 0.28746666666666665, 0.28746666666666665, 0.28933333333333333, 0.28813333333333335, 0.2885333333333333, 0.28800000000000003, 0.28933333333333333, 0.2878666666666667, 0.2885333333333333, 0.28733333333333333, 0.2889333333333334, 0.2886666666666666, 0.2869333333333334, 0.2885333333333333, 0.28813333333333335, 0.2886666666666666, 0.2886666666666666, 0.28759999999999997, 0.2872, 0.28826666666666667, 0.28800000000000003, 0.2889333333333334, 0.28933333333333333, 0.2885333333333333, 0.28959999999999997, 0.2886666666666666, 0.28733333333333333, 0.2878666666666667, 0.28746666666666665, 0.2886666666666666, 0.2885333333333333, 0.28879999999999995, 0.2886666666666666, 0.2892, 0.2889333333333334, 0.28826666666666667, 0.28826666666666667, 0.2885333333333333, 0.28813333333333335, 0.2878666666666667, 0.2885333333333333, 0.28813333333333335, 0.2898666666666667, 0.28813333333333335, 0.2889333333333334, 0.2890666666666667, 0.28800000000000003, 0.28946666666666665, 0.28680000000000005, 0.2872], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.04148968369120981, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 16, "valid_ratio": 0.15, "learning_rate": 0.000342059107159884, "optimization": "nesterov_momentum", "nb_data_augmentation": 4, "learning_rate_decay_method": "exp", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 8.321621296151378e-06, "rotation_range": [0, 0], "momentum": 0.6900394032616732}, "accuracy_valid_max": 0.7132, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        y_pred = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            y_pred.extend((nnet.predict(X[mini_batch]) == y[mini_batch]).tolist())\n        return np.mean(y_pred)\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            acc = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = acc\n            status[\"accuracy_train_std\"] = 0\n            acc = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = acc\n            status[\"accuracy_valid_std\"] = 0\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n\n    # rescaling to [-1, 1]\n    X_min = X_train.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X_train.max(axis=(0, 2, 3))[None, :, None, None]\n    def preprocess(a):\n        return (a / 255.) * 2 - 1\n        # return 2 * ((a - X_min) / (X_max - X_min)) - 1\n    X_train = preprocess(X_train)\n    X_valid = preprocess(X_valid)\n\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = preprocess(X_test)\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    acc = evaluate(X_test, y_test, batch_size_eval)\n    light.set(\"accuracy_test\", acc)\n    light.set(\"accuracy_test_std\", 0)\n    print(\"Test accuracy : {}+-{}\".format(acc, 0))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.7128, "accuracy_valid_std": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "accuracy_valid": [0.4036, 0.4508, 0.4824, 0.4962666666666667, 0.5024, 0.5174666666666666, 0.5322666666666667, 0.5452, 0.5614666666666667, 0.5685333333333333, 0.5774666666666667, 0.586, 0.5974666666666667, 0.6052, 0.6118666666666667, 0.6174666666666667, 0.6213333333333333, 0.6293333333333333, 0.6318666666666667, 0.6372, 0.6406666666666667, 0.6462666666666667, 0.6497333333333334, 0.6537333333333334, 0.6557333333333333, 0.6566666666666666, 0.6609333333333334, 0.6621333333333334, 0.6649333333333334, 0.6664, 0.672, 0.6709333333333334, 0.6750666666666667, 0.6772, 0.6797333333333333, 0.6785333333333333, 0.6793333333333333, 0.6822666666666667, 0.6869333333333333, 0.6858666666666666, 0.6870666666666667, 0.6870666666666667, 0.6884, 0.6902666666666667, 0.6937333333333333, 0.6929333333333333, 0.6917333333333333, 0.6937333333333333, 0.696, 0.6953333333333334, 0.6961333333333334, 0.6969333333333333, 0.696, 0.6973333333333334, 0.6982666666666667, 0.6989333333333333, 0.6988, 0.6972, 0.6988, 0.7, 0.7017333333333333, 0.7022666666666667, 0.7028, 0.7026666666666667, 0.7030666666666666, 0.7033333333333334, 0.7052, 0.704, 0.706, 0.7057333333333333, 0.7054666666666667, 0.7054666666666667, 0.7070666666666666, 0.706, 0.7069333333333333, 0.7061333333333333, 0.7065333333333333, 0.7061333333333333, 0.7074666666666667, 0.7069333333333333, 0.7073333333333334, 0.7077333333333333, 0.7093333333333334, 0.7073333333333334, 0.7069333333333333, 0.7082666666666667, 0.7102666666666667, 0.7089333333333333, 0.7097333333333333, 0.7084, 0.7084, 0.7092, 0.7085333333333333, 0.7086666666666667, 0.7090666666666666, 0.7098666666666666, 0.7097333333333333, 0.7108, 0.7096, 0.7104, 0.7102666666666667, 0.7102666666666667, 0.7094666666666667, 0.7104, 0.7106666666666667, 0.7105333333333334, 0.7104, 0.7129333333333333, 0.7098666666666666, 0.7105333333333334, 0.7108, 0.7109333333333333, 0.7106666666666667, 0.7113333333333334, 0.7110666666666666, 0.7110666666666666, 0.7110666666666666, 0.7102666666666667, 0.712, 0.7102666666666667, 0.7104, 0.7114666666666667, 0.7104, 0.7125333333333334, 0.7105333333333334, 0.7112, 0.7117333333333333, 0.7122666666666667, 0.7112, 0.7124, 0.7104, 0.7106666666666667, 0.7113333333333334, 0.7109333333333333, 0.7112, 0.7113333333333334, 0.712, 0.7116, 0.7121333333333333, 0.7129333333333333, 0.7124, 0.7118666666666666, 0.7114666666666667, 0.7121333333333333, 0.7108, 0.7109333333333333, 0.7116, 0.7122666666666667, 0.7112, 0.7108, 0.7121333333333333, 0.7112, 0.7108, 0.7125333333333334, 0.7126666666666667, 0.7130666666666666, 0.7129333333333333, 0.712, 0.7108, 0.712, 0.7114666666666667, 0.7108, 0.7117333333333333, 0.7114666666666667, 0.7110666666666666, 0.7117333333333333, 0.7112, 0.712, 0.7112, 0.712, 0.7109333333333333, 0.7126666666666667, 0.7121333333333333, 0.7116, 0.7118666666666666, 0.712, 0.7122666666666667, 0.71, 0.7125333333333334, 0.7121333333333333, 0.7116, 0.7122666666666667, 0.712, 0.7110666666666666, 0.7117333333333333, 0.7114666666666667, 0.7121333333333333, 0.7132, 0.7125333333333334, 0.7125333333333334, 0.7106666666666667, 0.7118666666666666, 0.7114666666666667, 0.712, 0.7106666666666667, 0.7121333333333333, 0.7114666666666667, 0.7126666666666667, 0.7110666666666666, 0.7113333333333334, 0.7130666666666666, 0.7114666666666667, 0.7118666666666666, 0.7113333333333334, 0.7113333333333334, 0.7124, 0.7128, 0.7117333333333333, 0.712, 0.7110666666666666, 0.7106666666666667, 0.7114666666666667, 0.7104, 0.7113333333333334, 0.7126666666666667, 0.7121333333333333, 0.7125333333333334, 0.7113333333333334, 0.7114666666666667, 0.7112, 0.7113333333333334, 0.7108, 0.7110666666666666, 0.7117333333333333, 0.7117333333333333, 0.7114666666666667, 0.7118666666666666, 0.7121333333333333, 0.7114666666666667, 0.7118666666666666, 0.7101333333333333, 0.7118666666666666, 0.7110666666666666, 0.7109333333333333, 0.712, 0.7105333333333334, 0.7132, 0.7128], "seed": 106911195, "model": "residualv3", "loss_std": [0.4868575930595398, 0.20814412832260132, 0.21800510585308075, 0.22413361072540283, 0.22944629192352295, 0.2325938642024994, 0.23514965176582336, 0.2366415560245514, 0.23898787796497345, 0.23970705270767212, 0.23996947705745697, 0.2411275953054428, 0.24378587305545807, 0.24313439428806305, 0.2420557737350464, 0.2455863505601883, 0.24531209468841553, 0.24632109701633453, 0.244729682803154, 0.2456788271665573, 0.24590088427066803, 0.244472473859787, 0.24672332406044006, 0.24504493176937103, 0.2449372559785843, 0.24460382759571075, 0.24540570378303528, 0.2443777620792389, 0.24373115599155426, 0.24429048597812653, 0.2442168891429901, 0.24500255286693573, 0.24603961408138275, 0.2445167750120163, 0.2445446103811264, 0.24266192317008972, 0.24444933235645294, 0.243002250790596, 0.24177397787570953, 0.24349549412727356, 0.2412673979997635, 0.24267268180847168, 0.24077658355236053, 0.2426600605249405, 0.24244877696037292, 0.2409176081418991, 0.24103720486164093, 0.24267324805259705, 0.24227222800254822, 0.24048839509487152, 0.24256636202335358, 0.24068838357925415, 0.23981663584709167, 0.24016691744327545, 0.2399158775806427, 0.24084055423736572, 0.24110105633735657, 0.2403753399848938, 0.23992317914962769, 0.23772168159484863, 0.2395162135362625, 0.24153022468090057, 0.23928876221179962, 0.23792503774166107, 0.23898489773273468, 0.23948487639427185, 0.23990342020988464, 0.2381371557712555, 0.23871542513370514, 0.23955468833446503, 0.23879507184028625, 0.23861023783683777, 0.23910631239414215, 0.23806604743003845, 0.23826490342617035, 0.2368839681148529, 0.23895156383514404, 0.2392963469028473, 0.2382834106683731, 0.23867915570735931, 0.2393464297056198, 0.23968185484409332, 0.23990926146507263, 0.23850756883621216, 0.2369154989719391, 0.2385753095149994, 0.23899762332439423, 0.2377772033214569, 0.23913133144378662, 0.23939666152000427, 0.23788504302501678, 0.23836034536361694, 0.2371751219034195, 0.23743145167827606, 0.23846781253814697, 0.23809674382209778, 0.23668108880519867, 0.23829536139965057, 0.23681239783763885, 0.23863151669502258, 0.23661009967327118, 0.23695294559001923, 0.23754923045635223, 0.23768998682498932, 0.23788782954216003, 0.2372491955757141, 0.2382197380065918, 0.23705603182315826, 0.2367188036441803, 0.23778361082077026, 0.23744229972362518, 0.23775993287563324, 0.23717054724693298, 0.23810124397277832, 0.23837415874004364, 0.23737113177776337, 0.23836959898471832, 0.237269788980484, 0.23562301695346832, 0.23732489347457886, 0.23766137659549713, 0.23663456737995148, 0.23846547305583954, 0.23773051798343658, 0.23503202199935913, 0.23637755215168, 0.23606687784194946, 0.23720712959766388, 0.23731935024261475, 0.23687127232551575, 0.2353912591934204, 0.23585696518421173, 0.23617210984230042, 0.2375139743089676, 0.23739489912986755, 0.23615165054798126, 0.2369370013475418, 0.2363603264093399, 0.23666028678417206, 0.23734240233898163, 0.23787063360214233, 0.2369617074728012, 0.23649586737155914, 0.2377498894929886, 0.23643968999385834, 0.23613891005516052, 0.23831811547279358, 0.2365940362215042, 0.23592014610767365, 0.23706303536891937, 0.2375379353761673, 0.23650987446308136, 0.23772788047790527, 0.23685215413570404, 0.23741720616817474, 0.2372036725282669, 0.23704347014427185, 0.23653249442577362, 0.23654118180274963, 0.23882097005844116, 0.2359173595905304, 0.2373446226119995, 0.2365988940000534, 0.23553971946239471, 0.23430822789669037, 0.23703140020370483, 0.23807479441165924, 0.2365843653678894, 0.23753899335861206, 0.23744256794452667, 0.23756735026836395, 0.23630359768867493, 0.23584222793579102, 0.23655655980110168, 0.236403688788414, 0.23645322024822235, 0.23755744099617004, 0.23644720017910004, 0.23664893209934235, 0.23660996556282043, 0.23856444656848907, 0.23619724810123444, 0.23668360710144043, 0.23838306963443756, 0.23760508000850677, 0.23699335753917694, 0.23526878654956818, 0.23645004630088806, 0.23801667988300323, 0.2347760945558548, 0.23840893805027008, 0.23677170276641846, 0.23780320584774017, 0.23474425077438354, 0.2358129471540451, 0.23742355406284332, 0.23613697290420532, 0.23604728281497955, 0.23536749184131622, 0.23615211248397827, 0.23593389987945557, 0.23651546239852905, 0.23601728677749634, 0.2372441291809082, 0.23655614256858826, 0.23830433189868927, 0.23808611929416656, 0.23624533414840698, 0.23635359108448029, 0.23593562841415405, 0.23647412657737732, 0.23744423687458038, 0.2363002598285675, 0.23495802283287048, 0.23681886494159698, 0.2371472716331482, 0.2369566410779953, 0.236068993806839, 0.2368307113647461, 0.2350284606218338, 0.23660051822662354, 0.23752889037132263, 0.23699544370174408, 0.23711802065372467, 0.23531530797481537, 0.23581726849079132, 0.2376512885093689, 0.23623669147491455, 0.23578356206417084, 0.23614908754825592, 0.2371407151222229, 0.2367590069770813, 0.2362293303012848, 0.2377810925245285, 0.23623178899288177, 0.23676520586013794, 0.23655857145786285, 0.2383626103401184]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:46 2016", "state": "available"}], "summary": "d48f90569a5644c96f35f74e7853360b"}