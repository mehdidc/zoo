{"content": {"hp_model": {"f0": 32, "f1": 16, "f2": 32, "f3": 64, "nonlin": "very_leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.418927550315857, 1.0333235263824463, 0.8616881966590881, 0.7604902386665344, 0.6844702959060669, 0.6290344595909119, 0.5814202427864075, 0.5418283343315125, 0.5098181366920471, 0.47852006554603577, 0.450054794549942, 0.4274345338344574, 0.40174993872642517, 0.3858526051044464, 0.3671385645866394, 0.3509407937526703, 0.33774444460868835, 0.3212970197200775, 0.30997729301452637, 0.2962266206741333, 0.287621408700943, 0.2822897434234619, 0.2686367630958557, 0.26294586062431335, 0.25205644965171814, 0.24881231784820557, 0.24151304364204407, 0.23713894188404083, 0.22933581471443176, 0.22514306008815765, 0.22028419375419617, 0.21430397033691406, 0.21196210384368896, 0.20647373795509338, 0.20544655621051788, 0.19901646673679352, 0.19346082210540771, 0.19118145108222961, 0.18943826854228973, 0.1866617500782013, 0.18193569779396057, 0.17810016870498657, 0.17501473426818848, 0.1764521598815918, 0.16966398060321808, 0.16766604781150818, 0.16847388446331024, 0.16550280153751373, 0.15925332903862, 0.15897022187709808, 0.1565345674753189, 0.15435360372066498, 0.1520012766122818, 0.1528494656085968, 0.1510223150253296, 0.14773157238960266, 0.14520363509655, 0.14663735032081604, 0.14226789772510529, 0.139961838722229, 0.14081057906150818, 0.14033453166484833, 0.1360282301902771, 0.1360422670841217, 0.13504625856876373, 0.1347057968378067, 0.13080817461013794, 0.13175496459007263, 0.13024155795574188, 0.12889398634433746, 0.12586882710456848, 0.12689268589019775, 0.12534765899181366, 0.12538141012191772, 0.12675686180591583, 0.12214306741952896, 0.12134572118520737, 0.12146129459142685, 0.12072395533323288, 0.11859376728534698, 0.11891352385282516, 0.11857305467128754, 0.11604585498571396, 0.11442965269088745, 0.11529767513275146, 0.11414115875959396, 0.11266546696424484, 0.11196720600128174, 0.1104443296790123, 0.11320198327302933, 0.08305113017559052, 0.0717894583940506, 0.06578695774078369, 0.06475155800580978, 0.0622284859418869, 0.060381509363651276, 0.061058614403009415, 0.05951112508773804, 0.058504946529865265, 0.05711774528026581, 0.056459736078977585, 0.05584995821118355, 0.05559004843235016, 0.05454770103096962, 0.05277932435274124, 0.05207660421729088, 0.052684396505355835, 0.05216517671942711, 0.050842754542827606, 0.05187708139419556, 0.05359126627445221, 0.05218951776623726, 0.050561632961034775, 0.0520164929330349, 0.051791734993457794, 0.051797814667224884, 0.05073600634932518, 0.05248042941093445, 0.051526427268981934, 0.0519944429397583, 0.05083979293704033, 0.05270029231905937, 0.05140487477183342, 0.05209886282682419, 0.051901742815971375, 0.05164090171456337, 0.05202317237854004, 0.05258140712976456, 0.05292162671685219, 0.05241641029715538, 0.051845088601112366, 0.051923856139183044, 0.05192533880472183, 0.05067982152104378, 0.051554448902606964, 0.05165207013487816, 0.05158175155520439, 0.0515834204852581, 0.05241003632545471, 0.05134286731481552, 0.05174950137734413, 0.05151047930121422, 0.052567970007658005, 0.05247185379266739, 0.0512772761285305, 0.05010845139622688, 0.05263949930667877], "moving_avg_accuracy_train": [0.06226351974898485, 0.1270945264915559, 0.18902047608953945, 0.2475274809157553, 0.3029590396198147, 0.35455417377729614, 0.40151992844760087, 0.44469344634250485, 0.48586041382757883, 0.5244729322224142, 0.5581548466182975, 0.5905843468448029, 0.6208609592544995, 0.6488816795838945, 0.6744980725220535, 0.697578042315114, 0.7188290318324491, 0.7391965158135176, 0.7572366798929263, 0.7734146627953372, 0.7885677243051168, 0.8034727578627464, 0.8166408943884411, 0.828387405321044, 0.8394498715591961, 0.8488388630104655, 0.8580863732118185, 0.8659187062383202, 0.8741185383299737, 0.8819610557767382, 0.8890169963300169, 0.895578967418453, 0.9005314664349594, 0.9047074085926816, 0.9092329835441371, 0.9132502695266558, 0.9182305811156569, 0.9219106852064722, 0.9254762561572628, 0.9288760403618115, 0.9322542833863539, 0.9352295618929566, 0.9373005208072508, 0.9391761177206195, 0.9410011945271382, 0.9429391656982707, 0.9454134004296618, 0.9470891514200566, 0.9484879732197268, 0.9501491996322963, 0.9521139113654952, 0.9536263855563266, 0.9546714281387985, 0.9563304734939847, 0.9573911366350808, 0.958994377882287, 0.959716534922639, 0.9608175190791846, 0.9620991205189036, 0.9631292928789272, 0.9638518349077104, 0.9651856804347965, 0.9653212593032309, 0.9667011497419553, 0.9673408736439595, 0.9680444722914683, 0.9687498627849589, 0.9690568321981389, 0.9696493249080962, 0.970245311316096, 0.9711537228928198, 0.9712993253059188, 0.9717140356324697, 0.9723616424858894, 0.9727189852682621, 0.9727080974926355, 0.9724262921326762, 0.972939894318218, 0.9732463513149676, 0.9735989646203941, 0.9739930104571735, 0.9746010568816942, 0.9752064273840009, 0.9757884992658481, 0.9757031749714153, 0.9762192960528545, 0.9769465468416259, 0.9773755331169962, 0.9777988231457819, 0.9781844344693083, 0.9800985989092822, 0.9818748253278777, 0.9835292326760423, 0.9850158741405809, 0.9863561766074751, 0.9875601236788705, 0.9886576269359835, 0.9896500301650041, 0.9905571439639799, 0.9913758715318677, 0.9921057508965381, 0.9927719429199795, 0.9933576008970384, 0.9938939936716296, 0.9944092632032762, 0.9948730057817581, 0.9952880489535822, 0.995656937510605, 0.9959819617654969, 0.9962884344877567, 0.996552634193743, 0.9967950642267497, 0.9970155764052652, 0.9972186876635483, 0.9973991626471934, 0.997561590132474, 0.9977124251668457, 0.997843526400161, 0.997956867212526, 0.9980635242412733, 0.998159515567146, 0.9982459077604313, 0.9983213355855787, 0.9983892206282113, 0.9984503171665807, 0.9985076291999226, 0.9985568848811208, 0.9986105155894373, 0.9986564580781125, 0.9986978063179204, 0.9987303694361284, 0.9987527007960869, 0.9987820996152876, 0.9988062334037588, 0.9988256286645735, 0.9988477346969256, 0.998865304977233, 0.9988741427830811, 0.998889072254773, 0.9988955333328671, 0.9989083237495804, 0.9989175099758129, 0.9989281027282315, 0.9989353110565988, 0.9989441237009389, 0.9989497299320355, 0.9989524503912129], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.061701836643448776, 0.12502804675734183, 0.18441833201948415, 0.24046174448771646, 0.2934216411985834, 0.34243243971050213, 0.38664702118184646, 0.42703623001998714, 0.4651728625714523, 0.5005284290533883, 0.5311012626823115, 0.5603288858493213, 0.5870040756717686, 0.6113158927845616, 0.6338639439428523, 0.6539720254992448, 0.6716694083220613, 0.6892124990222348, 0.7040776643120293, 0.7174898456406156, 0.7299799659334817, 0.7421763111605402, 0.7527063621792753, 0.7620400122471158, 0.7708166562596029, 0.7786251861814589, 0.7857219872639908, 0.7914193374363115, 0.7981706348203159, 0.8042813645423507, 0.8094768750195914, 0.81409885878081, 0.817665647177955, 0.8209682654420871, 0.8243312468798061, 0.8272938064915243, 0.8315949698935767, 0.8342339892351679, 0.8370842984253559, 0.8395000037955462, 0.8418907766652386, 0.8436528767566214, 0.8449274140056581, 0.8466330795139777, 0.8480786112180769, 0.8494294473854258, 0.8514641005384495, 0.8528884853829479, 0.8538009852068519, 0.8549782979719347, 0.8564367405297864, 0.8576974221808741, 0.8585624514706933, 0.8595830594392113, 0.8600590649598987, 0.8614060857982462, 0.8620243780561474, 0.8627049704180778, 0.8640387478962248, 0.864928824302009, 0.8652385232912357, 0.8661662840551092, 0.8660969189214357, 0.8675472797401356, 0.8680997395014383, 0.8689641937327703, 0.86964248727365, 0.8697321671513302, 0.8699552453816037, 0.8704347184902806, 0.8711703905606802, 0.8709515301567206, 0.8713843209475244, 0.8720291508068382, 0.8723430020100399, 0.8725044272890811, 0.871872431002191, 0.8720106126478303, 0.8720261423563154, 0.8723922110555483, 0.8729972870603098, 0.8736670143030739, 0.8745414120350407, 0.8753151334539011, 0.8753349484688272, 0.8758859203952879, 0.8770928914675814, 0.8774823351427359, 0.8780229994070768, 0.8780335230262336, 0.8801864313128421, 0.8821484628332898, 0.8837433927641928, 0.8853731126933458, 0.8866089565444931, 0.8876937134220467, 0.888929430794074, 0.8899408316529196, 0.890691371510971, 0.8912458165793768, 0.8918668874534421, 0.8926486663285799, 0.8932068124498634, 0.8937426765267897, 0.894213776673433, 0.8945767316491621, 0.8949644262835682, 0.895361150070874, 0.8956948169256089, 0.8959829100636203, 0.8962676374589902, 0.8965106555749135, 0.8967049578167444, 0.8969652790531423, 0.8971141189471503, 0.8971504186017576, 0.897231916415904, 0.8973662996048859, 0.8974506233812196, 0.8975509288424199, 0.8976167896950003, 0.8976394433685725, 0.8977829314959472, 0.8977513203870151, 0.8977727280226359, 0.8979150947158542, 0.897871296793591, 0.8979173278823042, 0.8979099277371461, 0.8979276816690037, 0.8979436602076756, 0.8980312830799804, 0.8980369014775547, 0.8981284367627811, 0.8981121327608252, 0.898207322440315, 0.8981698933306961, 0.8982104788281987, 0.8981981776509511, 0.8981748995601783, 0.8981295352159827, 0.8981863635562067, 0.8982130949999083, 0.8982615673617398, 0.898268571393638, 0.8983359101785965, 0.8983598939913091], "moving_var_accuracy_train": [0.03489071302379005, 0.06922917663870867, 0.09681966807734581, 0.11794532779322485, 0.1338047143171566, 0.1443827637039975, 0.149796526339359, 0.15159244753221746, 0.15168567568624997, 0.14993544730874842, 0.14515214479421812, 0.14010198267926455, 0.134341843742401, 0.1279741062781644, 0.12108249193480748, 0.11376840779216398, 0.10645600801214053, 0.09954391684439821, 0.09251855284006628, 0.08562224173317597, 0.07912655501793088, 0.07321333974432441, 0.06745260414592599, 0.061949168403141235, 0.056855654996259414, 0.05196346794088156, 0.04753676915111053, 0.04333520120174176, 0.03960681629854936, 0.036199680387819684, 0.03302778902286033, 0.030112545301663535, 0.027322035990073663, 0.024746778827208073, 0.02245642840225843, 0.02035603284202066, 0.01854366108953044, 0.016811183475650516, 0.015244484793931559, 0.013824063108275894, 0.012544369530844129, 0.011369603117486384, 0.010271242643159999, 0.009275779152876942, 0.00837817938573648, 0.007574163037504103, 0.00687184327130789, 0.006209932216613385, 0.005606549316797141, 0.0050707314438617965, 0.004598399129226743, 0.00415914741990545, 0.0037530617039075193, 0.003402527416931852, 0.003072399731928587, 0.002788293201206418, 0.0025141574782041455, 0.0022736512254004112, 0.0020610686231129793, 0.0018645130566238915, 0.0016827603538117253, 0.0015304966134417012, 0.0013776123867636244, 0.0012569880266932124, 0.0011349724440610502, 0.0010259306591659315, 0.0009278157749841015, 0.0008358822694713441, 0.0007554534710263821, 0.0006831049221104296, 0.000622221334233918, 0.0005601900013748284, 0.0005057188631318779, 0.00045892152854805547, 0.0004141786204702745, 0.00037276182531616994, 0.00033620037113266945, 0.00030495441886434335, 0.0002753042199956203, 0.00024889282328453225, 0.0002254009900494272, 0.0002061883751338367, 0.0001888677986260214, 0.0001730302878441539, 0.00015579278117672249, 0.00014261093179540322, 0.0001331098820037805, 0.00012145515682350845, 0.00011092221117738296, 0.00010116825489512986, 0.00012402765893496366, 0.0001400197156525198, 0.0001506513171502174, 0.00015547711103196577, 0.00015609709625363568, 0.00015353278358476503, 0.00014902012582065035, 0.0001429818907593218, 0.00013608940068202002, 0.00012851329408759208, 0.00012045647966157748, 0.00011240513800429256, 0.00010425158159869777, 9.64158783165298e-05, 8.916381469706625e-05, 8.218294783923305e-05, 7.551500056561073e-05, 6.918820941657044e-05, 6.322015537132561e-05, 5.7743469599597224e-05, 5.2597336001426414e-05, 4.786655328941624e-05, 4.351752854833752e-05, 3.9537063342675504e-05, 3.5876497985903484e-05, 3.252629237908453e-05, 2.9478424009521133e-05, 2.6685269408960475e-05, 2.4132357725792133e-05, 2.1821503449243943e-05, 1.9722282116104683e-05, 1.781722640404018e-05, 1.608670797489429e-05, 1.4519512588523938e-05, 1.3101156412677991e-05, 1.1820602793902269e-05, 1.0660377613684743e-05, 9.620226128187025e-06, 8.677199925759462e-06, 7.82486702560034e-06, 7.0519235330471364e-06, 6.351219386480827e-06, 5.723876062966377e-06, 5.156730414383509e-06, 4.644442958223752e-06, 4.184396752398578e-06, 3.7687355099094597e-06, 3.392564920228397e-06, 3.0553144303305485e-06, 2.7501586970687405e-06, 2.476615180199163e-06, 2.229713142950788e-06, 2.0077516862899474e-06, 1.8074441576416133e-06, 1.6273987061798412e-06, 1.4649417040058323e-06, 1.3185141416884723e-06], "duration": 73510.694954, "accuracy_train": [0.6226351974898486, 0.7105735871746954, 0.7463540224713916, 0.7740905243516981, 0.8018430679563492, 0.818910381194629, 0.8242117204803433, 0.8332551073966408, 0.8563631211932448, 0.8719855977759321, 0.8612920761812477, 0.8824498488833518, 0.8933504709417681, 0.9010681625484496, 0.9050456089654854, 0.9052977704526578, 0.9100879374884644, 0.9225038716431341, 0.9195981566076044, 0.9190165089170359, 0.9249452778931341, 0.9376180598814139, 0.9351541231196937, 0.9341060037144703, 0.9390120677025655, 0.93333978607189, 0.9413139650239941, 0.9364097034768365, 0.9479170271548542, 0.9525437127976191, 0.9525204613095238, 0.954636707214378, 0.945103957583518, 0.9422908880121816, 0.9499631581072352, 0.9494058433693245, 0.9630533854166666, 0.9550316220238095, 0.957566394714378, 0.9594740982027501, 0.9626584706072352, 0.9620070684523809, 0.9559391510358989, 0.9560564899409376, 0.9574268857858066, 0.9603809062384644, 0.9676815130121816, 0.9621709103336102, 0.9610773694167589, 0.9651002373454227, 0.9697963169642857, 0.9672386532738095, 0.9640768113810447, 0.9712618816906607, 0.9669371049049464, 0.9734235491071429, 0.9662159482858066, 0.9707263764880952, 0.973633533476375, 0.97240084411914, 0.9703547131667589, 0.9771902901785714, 0.96654146911914, 0.9791201636904762, 0.9730983887619971, 0.9743768601190477, 0.975098377226375, 0.9718195569167589, 0.9749817592977114, 0.9756091889880952, 0.9793294270833334, 0.9726097470238095, 0.9754464285714286, 0.9781901041666666, 0.9759350703096161, 0.9726101075119971, 0.9698900438930418, 0.9775623139880952, 0.9760044642857143, 0.9767724843692323, 0.9775394229881875, 0.9800734747023809, 0.9806547619047619, 0.9810271462024732, 0.9749352563215209, 0.9808643857858066, 0.9834918039405685, 0.9812364095953304, 0.9816084334048542, 0.9816549363810447, 0.9973260788690477, 0.9978608630952381, 0.9984188988095238, 0.9983956473214286, 0.9984188988095238, 0.9983956473214286, 0.99853515625, 0.9985816592261905, 0.9987211681547619, 0.9987444196428571, 0.9986746651785714, 0.9987676711309523, 0.9986285226905685, 0.9987215286429494, 0.9990466889880952, 0.9990466889880952, 0.9990234375, 0.9989769345238095, 0.9989071800595238, 0.9990466889880952, 0.9989304315476191, 0.9989769345238095, 0.9990001860119048, 0.9990466889880952, 0.9990234375, 0.9990234375, 0.9990699404761905, 0.9990234375, 0.9989769345238095, 0.9990234375, 0.9990234375, 0.9990234375, 0.9990001860119048, 0.9990001860119048, 0.9990001860119048, 0.9990234375, 0.9990001860119048, 0.9990931919642857, 0.9990699404761905, 0.9990699404761905, 0.9990234375, 0.9989536830357143, 0.9990466889880952, 0.9990234375, 0.9990001860119048, 0.9990466889880952, 0.9990234375, 0.9989536830357143, 0.9990234375, 0.9989536830357143, 0.9990234375, 0.9990001860119048, 0.9990234375, 0.9990001860119048, 0.9990234375, 0.9990001860119048, 0.9989769345238095], "end": "2016-02-02 06:12:45.792000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0], "moving_var_accuracy_valid": [0.034264049806573556, 0.0669295248124166, 0.09198142618284269, 0.11105106029431816, 0.12518871020151756, 0.1342883645183489, 0.13845389099868935, 0.13929009561395886, 0.1384507107338522, 0.1358557843917944, 0.13068248935753132, 0.12530252602571276, 0.11917636519171389, 0.11257830873447557, 0.10589620936036011, 0.09894560291923088, 0.09186981885630367, 0.08545267725250388, 0.07889616777908973, 0.07262553047309864, 0.06676700537016118, 0.061429062365223386, 0.05628409389881553, 0.05143973772123407, 0.04698902927020802, 0.0428388845990519, 0.03900827740959193, 0.035399587859507135, 0.032269849220861804, 0.029378933458397444, 0.026683980074629778, 0.024207846672167514, 0.021901559820180834, 0.019809569424749967, 0.017930399279628955, 0.016216350186742914, 0.014761215227569017, 0.013347773512579745, 0.012086114523638803, 0.010930023763195018, 0.009888463541105631, 0.008927562157583528, 0.008049425948617813, 0.007270667007192477, 0.006562406363641233, 0.0059225885524362715, 0.0053675880182706285, 0.004849089066110697, 0.004371674062857251, 0.00394698124469297, 0.003571426612474651, 0.003228587815255687, 0.0029124635147803244, 0.0026305919289309147, 0.0023695719673393466, 0.002148944956855893, 0.0019374910290159298, 0.0017479107797823995, 0.0015891303630550702, 0.001437347450822766, 0.0012944759269158423, 0.0011727749945391048, 0.0010555407987811205, 0.0009689186374427844, 0.0008747736797892338, 0.0007940218418729219, 0.0007187603968340204, 0.000646956739474765, 0.0005827089405986865, 0.0005265070966963172, 0.0004787273075831806, 0.0004312856757126548, 0.0003898428789588309, 0.00035460084099011213, 0.00032002728009086144, 0.00028825907516819704, 0.0002630279414111625, 0.0002368969947747706, 0.0002132094658439042, 0.00019309457589253653, 0.00017708017104712547, 0.00016340896515971789, 0.0001539492111867627, 0.00014394209357411648, 0.00012955141793005354, 0.00011932840671077802, 0.00012050657856387998, 0.00010982091809255267, 0.00010146968690391468, 9.132371493256461e-05, 0.00012390647025423878, 0.00014616193241388827, 0.00015443995253291008, 0.00016289984070692529, 0.00016035564685600113, 0.0001549103595210017, 0.00015316230038067097, 0.0001470524556180667, 0.00013741700076297457, 0.00012644198469159395, 0.00011726934749794507, 0.00011104301663465489, 0.00010274245880552463, 9.505256570543292e-05, 8.754472726839576e-05, 7.997588137121399e-05, 7.333105740001792e-05, 6.741445953074468e-05, 6.167501570720822e-05, 5.625449304201066e-05, 5.135867094487674e-05, 4.67543240923911e-05, 4.241867193377674e-05, 3.878670905547638e-05, 3.510741797636371e-05, 3.16085351630488e-05, 2.850745869013988e-05, 2.5819242394454353e-05, 2.3301312648305492e-05, 2.106173205339443e-05, 1.8994597715178413e-05, 1.7099756643997437e-05, 1.5575080563875116e-05, 1.4026565867358841e-05, 1.2628033862388803e-05, 1.154764495419126e-05, 1.041014478072324e-05, 9.388200052804006e-06, 8.44987290685886e-06, 7.6077224350406374e-06, 6.849248014819383e-06, 6.233423123095813e-06, 5.610364908307952e-06, 5.12473679345057e-06, 4.614655498423492e-06, 4.234739624313414e-06, 3.823874106103808e-06, 3.4563113389612868e-06, 3.112042075720247e-06, 2.8057146937384737e-06, 2.5436645378832503e-06, 2.3183632263684707e-06, 2.092958034472976e-06, 1.9048083597793513e-06, 1.714769031966909e-06, 1.5841027364073813e-06, 1.4308694722167567e-06], "accuracy_test": 0.8896683673469388, "start": "2016-02-01 09:47:35.097000", "learning_rate_per_epoch": [0.0009446371695958078, 0.0009446371695958078, 0.0009446371695958078, 0.0009446371695958078, 0.0009446371695958078, 0.0009446371695958078, 0.0009446371695958078, 0.0009446371695958078, 0.0009446371695958078, 0.0009446371695958078, 0.0009446371695958078, 0.0009446371695958078, 0.0009446371695958078, 0.0009446371695958078, 0.0009446371695958078, 0.0009446371695958078, 0.0009446371695958078, 0.0009446371695958078, 0.0009446371695958078, 0.0009446371695958078, 0.0009446371695958078, 0.0009446371695958078, 0.0009446371695958078, 0.0009446371695958078, 0.0009446371695958078, 0.0009446371695958078, 0.0009446371695958078, 0.0009446371695958078, 0.0009446371695958078, 0.0009446371695958078, 0.0009446371695958078, 0.0009446371695958078, 0.0009446371695958078, 0.0009446371695958078, 0.0009446371695958078, 0.0009446371695958078, 0.0009446371695958078, 0.0009446371695958078, 0.0009446371695958078, 0.0009446371695958078, 0.0009446371695958078, 0.0009446371695958078, 0.0009446371695958078, 0.0009446371695958078, 0.0009446371695958078, 0.0009446371695958078, 0.0009446371695958078, 0.0009446371695958078, 0.0009446371695958078, 0.0009446371695958078, 0.0009446371695958078, 0.0009446371695958078, 0.0009446371695958078, 0.0009446371695958078, 0.0009446371695958078, 0.0009446371695958078, 0.0009446371695958078, 0.0009446371695958078, 0.0009446371695958078, 0.0009446371695958078, 0.0009446371695958078, 0.0009446371695958078, 0.0009446371695958078, 0.0009446371695958078, 0.0009446371695958078, 0.0009446371695958078, 0.0009446371695958078, 0.0009446371695958078, 0.0009446371695958078, 0.0009446371695958078, 0.0009446371695958078, 0.0009446371695958078, 0.0009446371695958078, 0.0009446371695958078, 0.0009446371695958078, 0.0009446371695958078, 0.0009446371695958078, 0.0009446371695958078, 0.0009446371695958078, 0.0009446371695958078, 0.0009446371695958078, 0.0009446371695958078, 0.0009446371695958078, 0.0009446371695958078, 0.0009446371695958078, 0.0009446371695958078, 0.0009446371695958078, 0.0009446371695958078, 0.0009446371695958078, 9.446371404919773e-05, 9.446371404919773e-05, 9.446371404919773e-05, 9.446371404919773e-05, 9.446371404919773e-05, 9.446371404919773e-05, 9.446371404919773e-05, 9.446371404919773e-05, 9.446371404919773e-05, 9.446371404919773e-05, 9.446371404919773e-05, 9.446371404919773e-05, 9.446371404919773e-05, 9.446371404919773e-05, 9.446371223020833e-06, 9.446371223020833e-07, 9.446371507237927e-08, 9.446371684873611e-09, 9.446371462829006e-10, 9.446371462829006e-11, 9.446371636301354e-12, 9.446371636301354e-13, 9.446371636301354e-14, 9.446371466894764e-15, 9.446371890411238e-16, 9.446372155109034e-17, 9.446371989672912e-18, 9.446371576082605e-19, 9.446371576082605e-20, 9.446371899200032e-21, 9.446372303096816e-22, 9.446372050661326e-23, 9.446371892889145e-24, 9.446371695673919e-25, 9.446371449154886e-26, 9.44637129508049e-27, 9.446371487673485e-28, 9.446371608044106e-29, 9.44637190897066e-30, 9.446372285128852e-31, 9.446372520227722e-32, 9.446372520227722e-33, 9.446372703898715e-34, 9.446372474309974e-35, 9.4463727612959e-36, 9.446373120028307e-37, 9.446372671612798e-38, 9.446372951872491e-39, 9.446377155767884e-40, 9.446433207706457e-41, 9.446152948013592e-42, 9.444751649549267e-43, 9.388699710976274e-44, 9.80908925027372e-45, 1.401298464324817e-45, 0.0, 0.0, 0.0], "accuracy_train_first": 0.6226351974898486, "accuracy_train_last": 0.9989769345238095, "batch_size_eval": 1024, "accuracy_train_std": [0.019500315243265468, 0.02455414762297632, 0.021837395279070585, 0.02094335326529443, 0.023712542228654584, 0.02068095253451793, 0.021412951345633355, 0.020470608526789506, 0.021478481554426998, 0.021180180014942943, 0.021961895825464508, 0.02131523751066702, 0.023052811854229173, 0.02040471777351883, 0.021199438189007488, 0.020351900560741508, 0.021958554571207455, 0.019083448234718442, 0.01752025863437219, 0.019779671842653213, 0.01854350893580218, 0.016143032784433502, 0.015733256815317547, 0.016649646449607994, 0.016364642578235063, 0.016532455961614043, 0.01600341243391079, 0.014760785580609613, 0.014370602157517258, 0.013555218728817967, 0.013539954635693411, 0.012453139766430647, 0.014082822667135328, 0.013753476922301203, 0.01362458109089527, 0.011740759780102113, 0.010155258782426105, 0.012922138714998813, 0.012538460501713323, 0.010945986400145226, 0.011744630307578104, 0.010880404602552919, 0.011182301312202464, 0.010741587407529424, 0.010804793653839318, 0.012130366901919076, 0.009379142750177401, 0.010603009891628967, 0.010717127997734287, 0.009777346295512279, 0.00961659172861027, 0.009714810085658374, 0.010240223352398473, 0.009281459107573496, 0.008971875455819612, 0.008315227587658023, 0.00990086094079642, 0.00879806937439167, 0.008085901689227563, 0.00807958971783813, 0.007882698222142269, 0.007491490906650763, 0.0096805253845536, 0.008004727910626834, 0.008926460415373896, 0.00847018234377044, 0.007623972529350364, 0.008449188521148225, 0.008601880700181351, 0.008317827863717816, 0.006183365940952511, 0.008582413713514208, 0.007439022830526693, 0.007857902236946842, 0.007974090917568783, 0.008165938916781818, 0.008555376462965048, 0.006867381230696656, 0.007205110563377589, 0.006749525904953068, 0.006631337395759393, 0.00655465181832418, 0.0066481367786367365, 0.00622544026249141, 0.006912023783025005, 0.005918493748084968, 0.005820652378117258, 0.0072040385583504035, 0.006702357189118801, 0.006459515752795086, 0.001870697459589907, 0.001872863705878187, 0.0014756911879299135, 0.0013945078773599192, 0.0013300176382138292, 0.001661952871885085, 0.001368681422317842, 0.0013678911904838847, 0.0013047793756136717, 0.0013356962998832842, 0.0010617048043927524, 0.0010233296635354069, 0.0013486333451367444, 0.0011372118683162452, 0.0010758682385843819, 0.0010327954015305436, 0.0010439892262204078, 0.001106340890380424, 0.0011711458574649395, 0.001117281787270644, 0.0011034049960662827, 0.0011662887094479588, 0.0011374232522912025, 0.0011374232522912025, 0.00108661843801599, 0.0009995435853889824, 0.001020949516373705, 0.0010655170421679315, 0.001106340890380424, 0.0011276372445109878, 0.0011073177920192204, 0.0011073177920192204, 0.001117281787270644, 0.0010967704999135747, 0.0010967704999135747, 0.0010655170421679315, 0.0010967704999135747, 0.0010738563280092532, 0.0010856229101496235, 0.0010856229101496235, 0.0011276372445109878, 0.0011153445828879702, 0.0010327954015305436, 0.0010655170421679315, 0.0010758682385843819, 0.001117281787270644, 0.0011073177920192204, 0.0010947970021385374, 0.0011672154388031187, 0.0011553439591522364, 0.0010655170421679315, 0.001054551755064041, 0.0011276372445109878, 0.001117281787270644, 0.0010655170421679315, 0.0010967704999135747, 0.001185597915176894], "accuracy_test_std": 0.007998843510235998, "error_valid": [0.38298163356551207, 0.3050360622176205, 0.2810691006212349, 0.2551475432981928, 0.22993928840361444, 0.21647037368222888, 0.2154217455760542, 0.20946089043674698, 0.19159744446536142, 0.18127147260918675, 0.19374323465737953, 0.1766225056475903, 0.17291921592620485, 0.16987775320030118, 0.16320359563253017, 0.16505524049322284, 0.1690541462725903, 0.15289968467620485, 0.1621358480798193, 0.1618005224021084, 0.15760895143072284, 0.14805658179593373, 0.1525231786521084, 0.1539571371423193, 0.15019354762801207, 0.15109804452183728, 0.15040680299322284, 0.15730451101280118, 0.1410676887236446, 0.14072206795933728, 0.14376353068524095, 0.14430328736822284, 0.15023325724774095, 0.14930817018072284, 0.14540192018072284, 0.14604315700301207, 0.12969455948795183, 0.14201483669051207, 0.13726291886295183, 0.13875864787274095, 0.13659226750753017, 0.14048822242093373, 0.14360175075301207, 0.1380159309111446, 0.13891160344503017, 0.13841302710843373, 0.13022402108433728, 0.13429205101656627, 0.13798651637801207, 0.1344258871423193, 0.13043727644954817, 0.13095644295933728, 0.13365228492093373, 0.1312314688441265, 0.13565688535391573, 0.1264707266566265, 0.13241099162274095, 0.13116969832454817, 0.12395725480045183, 0.12706048804593373, 0.13197418580572284, 0.12548386907003017, 0.1345273672816265, 0.11939947289156627, 0.12692812264683728, 0.12325571818524095, 0.12425287085843373, 0.12946071394954817, 0.12803705054593373, 0.1252500235316265, 0.12220856080572284, 0.13101821347891573, 0.12472056193524095, 0.12216738045933728, 0.12483233716114461, 0.12604274519954817, 0.1338155355798193, 0.12674575254141573, 0.1278340902673193, 0.12431317065135539, 0.12155702889683728, 0.12030544051204817, 0.11758900837725905, 0.11772137377635539, 0.12448671639683728, 0.11915533226656627, 0.11204436888177716, 0.11901267178087349, 0.11711102221385539, 0.12187176440135539, 0.10043739410768071, 0.10019325348268071, 0.10190223785768071, 0.09995940794427716, 0.10226844879518071, 0.10254347467996983, 0.09994911285768071, 0.10095656061746983, 0.10255376976656627, 0.10376417780496983, 0.10254347467996983, 0.10031532379518071, 0.10176987245858427, 0.10143454678087349, 0.10154632200677716, 0.10215667356927716, 0.10154632200677716, 0.10106833584337349, 0.10130218138177716, 0.10142425169427716, 0.10116981598268071, 0.10130218138177716, 0.10154632200677716, 0.10069182981927716, 0.10154632200677716, 0.10252288450677716, 0.10203460325677716, 0.10142425169427716, 0.10179046263177716, 0.10154632200677716, 0.10179046263177716, 0.10215667356927716, 0.10092567535768071, 0.10253317959337349, 0.10203460325677716, 0.10080360504518071, 0.10252288450677716, 0.10166839231927716, 0.10215667356927716, 0.10191253294427716, 0.10191253294427716, 0.10118011106927716, 0.10191253294427716, 0.10104774567018071, 0.10203460325677716, 0.10093597044427716, 0.10216696865587349, 0.10142425169427716, 0.10191253294427716, 0.10203460325677716, 0.10227874388177716, 0.10130218138177716, 0.10154632200677716, 0.10130218138177716, 0.10166839231927716, 0.10105804075677716, 0.10142425169427716], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.0898906961586543, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 32, "valid_ratio": 0.15, "learning_rate": 0.000944637181136046, "optimization": "adam", "nb_data_augmentation": 1, "learning_rate_decay_method": "discrete", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 4.994128635911035e-07, "rotation_range": [0, 0], "momentum": 0.5708512294078989}, "accuracy_valid_max": 0.9000508871423193, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8985757483057228, "accuracy_valid_std": [0.018202528721854454, 0.009433638257820533, 0.011122877596164924, 0.008255981955613098, 0.013373757987779063, 0.012882054067070083, 0.011249394956506745, 0.009516593872973123, 0.011682794565562067, 0.008829997762701252, 0.015124480349021887, 0.0073671585895261375, 0.011283819390559443, 0.012104086817346666, 0.012287214877356274, 0.011836386111813558, 0.009775453987670607, 0.009295308484570771, 0.010879122847504814, 0.008020317332293516, 0.00932310599188739, 0.009569608567590497, 0.013476602446078642, 0.011882120624719677, 0.007952011626479726, 0.012458295460967378, 0.011065902078972858, 0.010057726480204601, 0.006221634602915825, 0.008996912324990298, 0.00844373108550908, 0.00960796075655891, 0.01008561150215566, 0.010394734771650866, 0.006435813809968701, 0.009188425964355724, 0.0073850233648204144, 0.011374344664612825, 0.01126706365691781, 0.012716377171858655, 0.010186872013600023, 0.01126935517032763, 0.009851636354221847, 0.006697228001923954, 0.007574675633831833, 0.008339841323203144, 0.012796357803241704, 0.011566959504719566, 0.013220109539341248, 0.010375365114110567, 0.00942634310474505, 0.009722283699848846, 0.010028521642980707, 0.008971563643798783, 0.010471743588370414, 0.010718775339098905, 0.008270072538298463, 0.010346865368826839, 0.009071111885434744, 0.008652546662820294, 0.009373844075651158, 0.010408573407003234, 0.008130212608507867, 0.010838990046799765, 0.007692173042115546, 0.010206915473462124, 0.013999111507272895, 0.007360441213989925, 0.01115734526379012, 0.010761067651804436, 0.011949175025875745, 0.01164084673309203, 0.006684301696890556, 0.009374669209871822, 0.007478587636878537, 0.009791574431462212, 0.009907505993916819, 0.012890972000773097, 0.01198502353591493, 0.009804897605414472, 0.008964296863858056, 0.010098706221103492, 0.010640337995562552, 0.012118770426667555, 0.009247264816559034, 0.007093409043278289, 0.011729424795332411, 0.009149343011692393, 0.010626885851956081, 0.009644727922696184, 0.008932315567406578, 0.008884754924046747, 0.009022945196050295, 0.009538337608563966, 0.011181181778581803, 0.010677823856755343, 0.009556343959127292, 0.009215636603360076, 0.009393428340303437, 0.00849837010282502, 0.010233161668366849, 0.009015809199515026, 0.009794525317379744, 0.011370137369311605, 0.01097139995148881, 0.010785330241420131, 0.011758104027782108, 0.011002962110709294, 0.010826110627167986, 0.010822851528644706, 0.010093500824200935, 0.009662443448813729, 0.010029101396939756, 0.010044797339912872, 0.01043683501348247, 0.01120930074887709, 0.010982979299631632, 0.00948442919703178, 0.010793758121983354, 0.010005300478840995, 0.010203305752676452, 0.010685391017860674, 0.010314833279436423, 0.01143929945542452, 0.009958321252952042, 0.010019189311875277, 0.010929297334327391, 0.010667903004546795, 0.010123984928307678, 0.011239618735882632, 0.010584135041745744, 0.01001587270155095, 0.010493644073871463, 0.010199655097453056, 0.010652383626357533, 0.010926266191661503, 0.010332380442614482, 0.01045305228632936, 0.010402365948008422, 0.010906736644955997, 0.010838574054449903, 0.010001926495249157, 0.0104254067795723, 0.010781975560873496, 0.0109654665267904, 0.010527063926373747, 0.009769238686450693], "accuracy_valid": [0.6170183664344879, 0.6949639377823795, 0.7189308993787651, 0.7448524567018072, 0.7700607115963856, 0.7835296263177711, 0.7845782544239458, 0.790539109563253, 0.8084025555346386, 0.8187285273908133, 0.8062567653426205, 0.8233774943524097, 0.8270807840737951, 0.8301222467996988, 0.8367964043674698, 0.8349447595067772, 0.8309458537274097, 0.8471003153237951, 0.8378641519201807, 0.8381994775978916, 0.8423910485692772, 0.8519434182040663, 0.8474768213478916, 0.8460428628576807, 0.8498064523719879, 0.8489019554781627, 0.8495931970067772, 0.8426954889871988, 0.8589323112763554, 0.8592779320406627, 0.856236469314759, 0.8556967126317772, 0.849766742752259, 0.8506918298192772, 0.8545980798192772, 0.8539568429969879, 0.8703054405120482, 0.8579851633094879, 0.8627370811370482, 0.861241352127259, 0.8634077324924698, 0.8595117775790663, 0.8563982492469879, 0.8619840690888554, 0.8610883965549698, 0.8615869728915663, 0.8697759789156627, 0.8657079489834337, 0.8620134836219879, 0.8655741128576807, 0.8695627235504518, 0.8690435570406627, 0.8663477150790663, 0.8687685311558735, 0.8643431146460843, 0.8735292733433735, 0.867589008377259, 0.8688303016754518, 0.8760427451995482, 0.8729395119540663, 0.8680258141942772, 0.8745161309299698, 0.8654726327183735, 0.8806005271084337, 0.8730718773531627, 0.876744281814759, 0.8757471291415663, 0.8705392860504518, 0.8719629494540663, 0.8747499764683735, 0.8777914391942772, 0.8689817865210843, 0.875279438064759, 0.8778326195406627, 0.8751676628388554, 0.8739572548004518, 0.8661844644201807, 0.8732542474585843, 0.8721659097326807, 0.8756868293486446, 0.8784429711031627, 0.8796945594879518, 0.882410991622741, 0.8822786262236446, 0.8755132836031627, 0.8808446677334337, 0.8879556311182228, 0.8809873282191265, 0.8828889777861446, 0.8781282355986446, 0.8995626058923193, 0.8998067465173193, 0.8980977621423193, 0.9000405920557228, 0.8977315512048193, 0.8974565253200302, 0.9000508871423193, 0.8990434393825302, 0.8974462302334337, 0.8962358221950302, 0.8974565253200302, 0.8996846762048193, 0.8982301275414157, 0.8985654532191265, 0.8984536779932228, 0.8978433264307228, 0.8984536779932228, 0.8989316641566265, 0.8986978186182228, 0.8985757483057228, 0.8988301840173193, 0.8986978186182228, 0.8984536779932228, 0.8993081701807228, 0.8984536779932228, 0.8974771154932228, 0.8979653967432228, 0.8985757483057228, 0.8982095373682228, 0.8984536779932228, 0.8982095373682228, 0.8978433264307228, 0.8990743246423193, 0.8974668204066265, 0.8979653967432228, 0.8991963949548193, 0.8974771154932228, 0.8983316076807228, 0.8978433264307228, 0.8980874670557228, 0.8980874670557228, 0.8988198889307228, 0.8980874670557228, 0.8989522543298193, 0.8979653967432228, 0.8990640295557228, 0.8978330313441265, 0.8985757483057228, 0.8980874670557228, 0.8979653967432228, 0.8977212561182228, 0.8986978186182228, 0.8984536779932228, 0.8986978186182228, 0.8983316076807228, 0.8989419592432228, 0.8985757483057228], "seed": 985768232, "model": "residualv3", "loss_std": [0.29046526551246643, 0.19529196619987488, 0.18211452662944794, 0.17534799873828888, 0.16867400705814362, 0.16186819970607758, 0.1610921174287796, 0.15385523438453674, 0.14907722175121307, 0.14835324883460999, 0.14115339517593384, 0.1377650499343872, 0.13205988705158234, 0.12927430868148804, 0.12274996936321259, 0.11910638958215714, 0.11867589503526688, 0.11248265951871872, 0.11349323391914368, 0.10887155681848526, 0.1071464940905571, 0.10453103482723236, 0.09805267304182053, 0.09769905358552933, 0.09576199948787689, 0.09833668172359467, 0.09621013700962067, 0.09292258322238922, 0.09022156149148941, 0.08949292451143265, 0.08642194420099258, 0.08689794689416885, 0.08746571838855743, 0.08453376591205597, 0.08434508740901947, 0.08201782405376434, 0.08026257157325745, 0.0792028158903122, 0.08010585606098175, 0.0791909471154213, 0.07680010050535202, 0.0778958797454834, 0.07474908232688904, 0.07753784209489822, 0.07335210591554642, 0.0729527473449707, 0.07512158900499344, 0.0729040578007698, 0.07085880637168884, 0.06860321760177612, 0.07075399160385132, 0.0656164288520813, 0.06837272644042969, 0.0691150352358818, 0.06623630225658417, 0.06818189471960068, 0.06629388779401779, 0.06748487055301666, 0.06481694430112839, 0.06573689728975296, 0.06457982957363129, 0.06474538892507553, 0.0649447962641716, 0.06529215723276138, 0.061324384063482285, 0.06325237452983856, 0.0628906711935997, 0.06418967992067337, 0.06121267378330231, 0.06035279110074043, 0.06083495914936066, 0.06257926672697067, 0.058980464935302734, 0.05940350517630577, 0.06121667101979256, 0.05887320637702942, 0.05820871517062187, 0.05968412756919861, 0.05932261794805527, 0.05607721954584122, 0.05994867905974388, 0.05778783932328224, 0.05633163079619408, 0.05571840703487396, 0.058414194732904434, 0.05635278299450874, 0.05471838265657425, 0.0545770637691021, 0.05555466562509537, 0.05467083305120468, 0.04328465834259987, 0.035441651940345764, 0.03283294290304184, 0.03407048434019089, 0.030437882989645004, 0.031607192009687424, 0.03137444332242012, 0.03223814442753792, 0.031195325776934624, 0.030323490500450134, 0.028706019744277, 0.028504449874162674, 0.028431899845600128, 0.028433067724108696, 0.027438323944807053, 0.026719698682427406, 0.026728542521595955, 0.026963742449879646, 0.0267792996019125, 0.027404693886637688, 0.02846968173980713, 0.027034146711230278, 0.02515466883778572, 0.02736896462738514, 0.027137987315654755, 0.02734668366611004, 0.02593255788087845, 0.026496602222323418, 0.025783533230423927, 0.025664540007710457, 0.02534415014088154, 0.029302731156349182, 0.02477736957371235, 0.027595357969403267, 0.026759717613458633, 0.026382893323898315, 0.02712296135723591, 0.02685806341469288, 0.027181629091501236, 0.0271751806139946, 0.027353523299098015, 0.026699496433138847, 0.0268548671156168, 0.02605065517127514, 0.02611585520207882, 0.027119074016809464, 0.026840418577194214, 0.02632908523082733, 0.026963496580719948, 0.025576023384928703, 0.026781540364027023, 0.02624371461570263, 0.02637055143713951, 0.027215871959924698, 0.02585533633828163, 0.02457101084291935, 0.026952087879180908]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:32 2016", "state": "available"}], "summary": "6d7da4aa66e201ab6192aa222514a6eb"}