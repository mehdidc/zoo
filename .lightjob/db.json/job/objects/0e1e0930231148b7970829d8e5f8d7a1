{"content": {"hp_model": {"f0": 16, "f1": 32, "f2": 32, "f3": 32, "nonlin": "leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.00967768850491286, 0.010429789131023926, 0.011830209190973067, 0.009995335252002274, 0.013439783128748314, 0.010134848888664378, 0.010931418446118014, 0.00925313973014819, 0.012486062822853906, 0.014215371627901064, 0.01394152548521302, 0.01248480853044867, 0.012643645975464083, 0.015473059852856607, 0.01364785618659373, 0.01178757128754253, 0.01461458755007876, 0.011920151543930484, 0.009911725279990066, 0.009626617260703576, 0.01144368357139557, 0.010633707163799406, 0.011282621912427635, 0.012119387349546656, 0.009940092572830905, 0.008739545877144546, 0.010320095446984706, 0.008494834541749296, 0.007270315692387332, 0.008081563620092642, 0.008850187323073222, 0.00989477252488038, 0.008689150172689898, 0.008590917009952797, 0.0078065382644649855, 0.009058332873381645, 0.0070463372114379415, 0.007117644675839173, 0.00779907195542802, 0.00882914144740878, 0.009367934952027406, 0.006983841349342225, 0.007542584771957648, 0.0063708205237543, 0.0071344049662377665, 0.009893053273625274, 0.008804623144691676, 0.00706603945024208, 0.008644019479112948, 0.008926749028614967, 0.008187946097256411, 0.008045944176739534, 0.0065724664750255145, 0.010869968691896718, 0.007697995047434947, 0.009567901422338449, 0.007510430520380389, 0.005979803131237082, 0.006177945534728988, 0.00810275252093354, 0.0065834246264070825, 0.007671539051715972, 0.0073467516216412275, 0.0065086612299624464, 0.007388548979837629, 0.005294972194550569, 0.0081491423229931, 0.006490697653716723, 0.0061508306758913936, 0.0067687896371756455, 0.007159105804500853, 0.00868892650137232, 0.0066930389986090775, 0.007208221446768995, 0.007732026033829414, 0.008425550415586465, 0.00965668326301561, 0.007381896780390843, 0.009547162590483224, 0.00691473786176724, 0.008311658126433353, 0.0063448462197869294, 0.00835244094261809, 0.006570353440614472, 0.007362320587464752, 0.0065979827063331695, 0.0059666421597305295, 0.006311386760522718, 0.0055881141379826435, 0.006870851507599374, 0.006070953704827206, 0.005837081943795479, 0.008445239284171237, 0.005900253275152708, 0.005421192620317846, 0.008796660800173658, 0.006508702937504356, 0.008829854824698342, 0.00805711259683863, 0.010332787506835745, 0.011161397600536313, 0.009715439512391635, 0.00885329745110474, 0.00989862630951151, 0.009088055815784528, 0.01044853529764767, 0.009350209580463474, 0.009278507458261913, 0.01128236242450614, 0.009774990521913439, 0.009449982549634152, 0.008077376343119247, 0.008739021161564074, 0.011302553398097808, 0.011582554907421883, 0.007174683400192463, 0.009398796732846307, 0.008873067245540685, 0.008424698050410063, 0.010392343025990592, 0.0067433455533957645, 0.009680913964304814, 0.009491516992971067, 0.007914639082691884, 0.008798405606472824, 0.008590981465862233, 0.0064681667154808985, 0.009967268958614299, 0.009250257457383396, 0.008279410483840786, 0.0100836245858221, 0.009473691780611559, 0.009265689874710804, 0.010161520281211258, 0.010543731205974424, 0.009699135773426407, 0.008401204680540947, 0.008434665331307146, 0.009819557065375429, 0.009134089129193885, 0.009273397852134975, 0.007743576271206873, 0.008573566449418734, 0.006970552859389752, 0.007316079603048991, 0.007625556315896065, 0.008382177400222115, 0.00914394454997218, 0.009812707651066281, 0.00855764760080391], "moving_avg_accuracy_train": [0.05233816243309338, 0.11216795793535435, 0.17107768628587575, 0.22819292141790093, 0.28212178069268523, 0.332533536299358, 0.3790478011690992, 0.4218873462471413, 0.4612564505101625, 0.49734190301590153, 0.5305626415972017, 0.5608633768203257, 0.5888688576425845, 0.6143619646397233, 0.6375800203502157, 0.658841174657479, 0.6784434963470929, 0.6964343942379927, 0.7129816617171096, 0.7281137648733333, 0.7420162177222404, 0.7548050098481337, 0.7665846760721612, 0.7773420164487491, 0.7873443130792988, 0.7963906299718118, 0.8049090253310351, 0.8127872057448216, 0.8198542445314966, 0.8264562507204196, 0.8324678468035548, 0.8381711078331014, 0.8432949945061866, 0.8480994077655163, 0.8525790565226948, 0.8567757899208129, 0.8606459280291376, 0.8642220222301921, 0.8675497529563699, 0.8707725030956258, 0.8737705263244999, 0.8765107801531518, 0.879028017677473, 0.8815981980410472, 0.8838671464920643, 0.8861486183277232, 0.8882670831953015, 0.8903154996046843, 0.8921613274243008, 0.8938109827667268, 0.8954607941892051, 0.8970758688515877, 0.8985107267107996, 0.8998113993793283, 0.9011982075714712, 0.9024602497884382, 0.9038169408717944, 0.9050541667908442, 0.906223545787055, 0.907366703736035, 0.9083793058972691, 0.9094974418911523, 0.9104295037190175, 0.9113961704509825, 0.9123219380323608, 0.9131411779627441, 0.9140086301357849, 0.9149009802831976, 0.9157251659504296, 0.9166178874794638, 0.9174259871532136, 0.9182415604167128, 0.9190291268741186, 0.91979381235485, 0.920609768276757, 0.9212906862326731, 0.9219779171549022, 0.9226731548956226, 0.9233918387658333, 0.9240061382145083, 0.9246821685075831, 0.9252487430927789, 0.925805271342102, 0.9264245491116847, 0.9270238599293369, 0.9276398974783006, 0.9282083903116812, 0.9288710603878867, 0.9294209604802811, 0.9299390860027126, 0.9304310837562619, 0.9308436547999326, 0.9313637422142269, 0.9318945999049489, 0.9324768953765711, 0.9329684452665165, 0.9334549819460294, 0.9339068158504482, 0.9343622944894252, 0.934749009825228, 0.9350993427274413, 0.9355308637310907, 0.9359193047320127, 0.936373533329271, 0.9366893331144225, 0.9370734982710497, 0.93748206197869, 0.9378822853500809, 0.9382890254093419, 0.9386272257257813, 0.9389989632284155, 0.9392938552557493, 0.9396198200958535, 0.9400085195531378, 0.9403002924420931, 0.9405954401254862, 0.9408610369917213, 0.9410884484272852, 0.9414256522014356, 0.9417290995493522, 0.9420556445362774, 0.9424588530673765, 0.9427286987441661, 0.9430366279711245, 0.9433020303848832, 0.9435455789037039, 0.9438392854790035, 0.9441547025729452, 0.944492020331293, 0.944809593255482, 0.9450930476896237, 0.9454435238303605, 0.9457846010915658, 0.9459938422790131, 0.9463146567810398, 0.9466662048995397, 0.9469617079157227, 0.9472252994326591, 0.9475369005109878, 0.947722046429112, 0.9479374337827863, 0.9481545699380071, 0.9483848336610299, 0.9485664583260269, 0.9487090662828761, 0.9489468035845442, 0.9490955908917413, 0.9492807248396657, 0.9495263283546841, 0.9498356911241437], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 259984302, "moving_var_accuracy_train": [0.024653549221855812, 0.054404634168251545, 0.08019737560041643, 0.1015369887980535, 0.11755818668236355, 0.12867447394424936, 0.13527921807717638, 0.13826833587190115, 0.13839083961896473, 0.13627119459996379, 0.1325766323869512, 0.1275821801438129, 0.1218827247342047, 0.1155435388000923, 0.10884088791886284, 0.10202512926927196, 0.0952808754829527, 0.08866583959694448, 0.082263564186479, 0.07609803268120645, 0.07022773317003053, 0.06467693868938129, 0.05945808964758862, 0.05455376403062967, 0.04999880106853624, 0.04573544360556053, 0.04181496678046883, 0.03819206164211147, 0.03482234281281166, 0.0317323869029977, 0.028884401799898828, 0.02628870629724926, 0.02389612359927191, 0.02171425272024252, 0.01972343272510749, 0.017909602593530662, 0.016253444054975166, 0.014743195697090988, 0.013368540253455425, 0.012125161294250552, 0.010993538454353316, 0.009961765528326956, 0.009022617338278917, 0.008179808048362748, 0.00740816038718683, 0.006714190372100287, 0.006083162375446733, 0.005512610225978116, 0.004992012926437335, 0.004517303898532753, 0.004090070408247136, 0.0037045395629080537, 0.003352614960302526, 0.0030325792087881934, 0.0027466304205655246, 0.002486302133525634, 0.002254237416433996, 0.0020425902265635136, 0.001850638229038175, 0.001677335697001203, 0.001518830395533506, 0.0013781994088875114, 0.001248198121257431, 0.0011317883102678786, 0.00102632288977367, 0.0009297309873681134, 0.0008435301480839215, 0.0007663437323458158, 0.0006958228972378718, 0.0006334131730696947, 0.0005759490815071562, 0.0005243406110896537, 0.00047748889830416573, 0.00043500270343372135, 0.0003974944896888055, 0.00036191788408412466, 0.0003299766727399239, 0.00030132920511103154, 0.00027584484314763783, 0.0002516566331466564, 0.00023060412244638428, 0.00021043277104705518, 0.000192177007173001, 0.00017641085105879497, 0.0001620023270583091, 0.00014921761470807678, 0.00013720451015171626, 0.00012743624380562774, 0.00011741413042960348, 0.00010808880389959717, 9.945847961511577e-05, 9.104456544828309e-05, 8.437452717002065e-05, 7.847336344320692e-05, 7.367763924533096e-05, 6.848446696954535e-05, 6.376648173719462e-05, 5.9227218458116635e-05, 5.517164372738377e-05, 5.1000418113150886e-05, 4.7004974583194324e-05, 4.398037051418992e-05, 4.0940311163546126e-05, 3.870319261429689e-05, 3.5730438891583035e-05, 3.348564081052204e-05, 3.163939545827717e-05, 2.991706463551682e-05, 2.8414295454234096e-05, 2.660228099516792e-05, 2.518575183343311e-05, 2.344982842015511e-05, 2.206112327099759e-05, 2.121479635673618e-05, 1.9859499489626635e-05, 1.8657558935775e-05, 1.742667830038247e-05, 1.6149454119571497e-05, 1.5557866175325875e-05, 1.4830802194411943e-05, 1.430740663134486e-05, 1.43398600441702e-05, 1.3561224243290921e-05, 1.3058485498298811e-05, 1.2386582919529521e-05, 1.1681767556754136e-05, 1.128996277244671e-05, 1.105635798355746e-05, 1.097477161607271e-05, 1.0784967514066424e-05, 1.042958850877126e-05, 1.0492131384925909e-05, 1.0489921529435594e-05, 9.834966247211407e-06, 9.777767124886093e-06, 9.912265128985155e-06, 9.706936909245818e-06, 9.361567608528778e-06, 9.299267935816793e-06, 8.677852241217552e-06, 8.227592426200977e-06, 7.829166172717609e-06, 7.52344199470895e-06, 7.067985465655667e-06, 6.544220183300269e-06, 6.398469386410083e-06, 5.957861412815797e-06, 5.670546479600998e-06, 5.6463816109449345e-06, 5.9430913579999205e-06], "duration": 68175.525381, "accuracy_train": [0.5233816243309339, 0.6506361174557033, 0.7012652414405685, 0.7422300376061277, 0.7674815141657438, 0.7862393367594132, 0.79767618499677, 0.8074432519495202, 0.8155783888773532, 0.8221109755675526, 0.8295492888289037, 0.8335699938284422, 0.8409181850429125, 0.8437999276139718, 0.8465425217446475, 0.8501915634228497, 0.8548643915536176, 0.8583524752560908, 0.861907069029162, 0.8643026932793466, 0.867138293362403, 0.8699041389811739, 0.8726016720884091, 0.87415807983804, 0.8773649827542451, 0.8778074820044297, 0.8815745835640458, 0.8836908294689, 0.8834575936115725, 0.8858743064207272, 0.8865722115517718, 0.8895004570990217, 0.8894099745639534, 0.8913391270994832, 0.8928958953373015, 0.8945463905038759, 0.8954771710040605, 0.8964068700396824, 0.8974993294919711, 0.8997772543489295, 0.900752735384367, 0.9011730646110188, 0.901683155396364, 0.9047298213132153, 0.9042876825512183, 0.9066818648486527, 0.9073332670035069, 0.9087512472891289, 0.908773777800849, 0.9086578808485604, 0.9103090969915099, 0.9116115408130308, 0.9114244474437062, 0.9115174533960871, 0.9136794813007567, 0.9138186297411407, 0.9160271606220007, 0.9161892000622923, 0.9167479567529531, 0.9176551252768549, 0.9174927253483758, 0.9195606658361019, 0.9188180601698044, 0.9200961710386674, 0.9206538462647655, 0.9205143373361941, 0.9218156996931525, 0.9229321316099114, 0.9231428369555187, 0.9246523812407714, 0.9246988842169619, 0.9255817197882059, 0.9261172249907714, 0.9266759816814323, 0.9279533715739202, 0.9274189478359173, 0.9281629954549648, 0.9289302945621077, 0.9298599935977298, 0.9295348332525839, 0.9307664411452565, 0.9303479143595422, 0.9308140255860096, 0.9319980490379292, 0.9324176572882059, 0.9331842354189737, 0.9333248258121077, 0.9348350910737356, 0.9343700613118309, 0.9346022157045959, 0.9348590635382059, 0.9345567941929678, 0.9360445289428755, 0.936672319121447, 0.9377175546211702, 0.9373923942760245, 0.9378338120616464, 0.9379733209902179, 0.9384616022402179, 0.938229447847453, 0.9382523388473607, 0.939414552763935, 0.9394152737403102, 0.9404615907045959, 0.9395315311807864, 0.9405309846806941, 0.941159135347453, 0.9414842956925988, 0.9419496859426911, 0.9416710285737356, 0.9423446007521227, 0.9419478835017534, 0.9425535036567922, 0.943506814668697, 0.9429262484426911, 0.9432517692760245, 0.9432514087878369, 0.9431351513473607, 0.9444604861687893, 0.9444601256806018, 0.9449945494186047, 0.9460877298472684, 0.9451573098352714, 0.9458079910137505, 0.9456906521087117, 0.9457375155730897, 0.9464826446567, 0.9469934564184201, 0.9475278801564231, 0.947667749573182, 0.9476441375968992, 0.9485978090969915, 0.9488542964424143, 0.9478770129660392, 0.9492019872992802, 0.9498301379660392, 0.9496212350613695, 0.9495976230850868, 0.9503413102159468, 0.9493883596922297, 0.9498759199658545, 0.9501087953349945, 0.9504572071682356, 0.9502010803110004, 0.9499925378945183, 0.951086439299557, 0.9504346766565154, 0.9509469303709857, 0.9517367599898486, 0.9526199560492802], "end": "2016-01-27 19:14:28.717000", "learning_rate_per_epoch": [0.001117965904995799, 0.0005589829524978995, 0.0003726552822627127, 0.00027949147624894977, 0.00022359317517839372, 0.00018632764113135636, 0.00015970940876286477, 0.00013974573812447488, 0.00012421843712218106, 0.00011179658758919686, 0.00010163326078327373, 9.316382056567818e-05, 8.599737338954583e-05, 7.985470438143238e-05, 7.453106081811711e-05, 6.987286906223744e-05, 6.576269515790045e-05, 6.210921856109053e-05, 5.884030906599946e-05, 5.589829379459843e-05, 5.3236472012940794e-05, 5.081663039163686e-05, 4.8607213102513924e-05, 4.658191028283909e-05, 4.4718635763274506e-05, 4.2998686694772914e-05, 4.140614328207448e-05, 3.992735219071619e-05, 3.855054819723591e-05, 3.7265530409058556e-05, 3.6063414881937206e-05, 3.493643453111872e-05, 3.3877753594424576e-05, 3.2881347578950226e-05, 3.1941883207764477e-05, 3.1054609280545264e-05, 3.0215294827939942e-05, 2.942015453299973e-05, 2.8665792342508212e-05, 2.7949146897299215e-05, 2.7267460609436966e-05, 2.6618236006470397e-05, 2.599920662760269e-05, 2.540831519581843e-05, 2.484368633304257e-05, 2.4303606551256962e-05, 2.3786507881595753e-05, 2.3290955141419545e-05, 2.2815629563410766e-05, 2.2359317881637253e-05, 2.192089959862642e-05, 2.1499343347386457e-05, 2.1093695977469906e-05, 2.070307164103724e-05, 2.0326651792856865e-05, 1.9963676095358096e-05, 1.9613436961662956e-05, 1.9275274098617956e-05, 1.8948574506794102e-05, 1.8632765204529278e-05, 1.8327309589949436e-05, 1.8031707440968603e-05, 1.7745489458320662e-05, 1.746821726555936e-05, 1.7199474314111285e-05, 1.6938876797212288e-05, 1.6686057279002853e-05, 1.6440673789475113e-05, 1.620240436750464e-05, 1.5970941603882238e-05, 1.5745998098282143e-05, 1.5527304640272632e-05, 1.5314601114368998e-05, 1.5107647413969971e-05, 1.4906211617926601e-05, 1.4710077266499866e-05, 1.451903699489776e-05, 1.4332896171254106e-05, 1.4151466530165635e-05, 1.3974573448649608e-05, 1.3802047760691494e-05, 1.3633730304718483e-05, 1.3469468285620678e-05, 1.3309118003235199e-05, 1.3152539395377971e-05, 1.2999603313801344e-05, 1.285018242924707e-05, 1.2704157597909216e-05, 1.2561414223455358e-05, 1.2421843166521285e-05, 1.228533892572159e-05, 1.2151803275628481e-05, 1.202113890030887e-05, 1.1893253940797877e-05, 1.1768061995098833e-05, 1.1645477570709772e-05, 1.152542154159164e-05, 1.1407814781705383e-05, 1.1292584531474859e-05, 1.1179658940818626e-05, 1.1068968888139352e-05, 1.096044979931321e-05, 1.0854038009711076e-05, 1.0749671673693229e-05, 1.0647294402588159e-05, 1.0546847988734953e-05, 1.0448278771946207e-05, 1.035153582051862e-05, 1.0256567293254193e-05, 1.0163325896428432e-05, 1.0071764336316846e-05, 9.981838047679048e-06, 9.893503374769352e-06, 9.806718480831478e-06, 9.721442438603844e-06, 9.637637049308978e-06, 9.555264114169404e-06, 9.474287253397051e-06, 9.394670996698551e-06, 9.316382602264639e-06, 9.239387509296648e-06, 9.163654794974718e-06, 9.089153536478989e-06, 9.015853720484301e-06, 8.943727152654901e-06, 8.872744729160331e-06, 8.802880984148942e-06, 8.73410863277968e-06, 8.666402209200896e-06, 8.599737157055642e-06, 8.534090738976374e-06, 8.469438398606144e-06, 8.405758308072109e-06, 8.343028639501426e-06, 8.281228474515956e-06, 8.220336894737557e-06, 8.160334800777491e-06, 8.10120218375232e-06, 8.042919944273308e-06, 7.985470801941119e-06, 7.928835657367017e-06, 7.872999049141072e-06, 7.817942787369248e-06, 7.763652320136316e-06, 7.710109457548242e-06, 7.657300557184499e-06, 7.605210157635156e-06, 7.553823706984986e-06, 7.503126653318759e-06, 7.453105808963301e-06], "accuracy_valid": [0.5179046263177711, 0.6442223974021084, 0.6903664462537651, 0.7154629259224398, 0.7374164627259037, 0.7521163756588856, 0.7630218138177711, 0.7665412627070783, 0.774303758000753, 0.7792380459337349, 0.7845988445971386, 0.7888816006212349, 0.7914553722703314, 0.7921171992658133, 0.7924422298569277, 0.7959719738328314, 0.803368258189006, 0.8014857280685241, 0.8050463573042168, 0.8031844173569277, 0.8063788356551205, 0.8056155285203314, 0.8074362881212349, 0.8098173945783133, 0.8119028849774097, 0.8108851421310241, 0.8148016872176205, 0.8139574901167168, 0.8144457713667168, 0.8147002070783133, 0.815453219126506, 0.8171519084149097, 0.8150561229292168, 0.8161547557417168, 0.8153105586408133, 0.8159209102033133, 0.8162768260542168, 0.8186167521649097, 0.8173857539533133, 0.8196947948042168, 0.8188505977033133, 0.8172430934676205, 0.819725680064006, 0.8173445736069277, 0.8201933711408133, 0.8194609492658133, 0.8214552546121988, 0.8209360881024097, 0.8223097467996988, 0.821678805064006, 0.8226759577371988, 0.8222994517131024, 0.821922945689006, 0.8222185617469879, 0.8228083231362951, 0.8235407450112951, 0.822411226939006, 0.823265719126506, 0.8236422251506024, 0.8241510965737951, 0.824242281626506, 0.823631930064006, 0.8243437617658133, 0.823509859751506, 0.8254835749246988, 0.825218844126506, 0.8243540568524097, 0.8253615046121988, 0.823509859751506, 0.8251070689006024, 0.8254835749246988, 0.8247614481362951, 0.8260836314006024, 0.8251276590737951, 0.8275793604103916, 0.8275793604103916, 0.8261248117469879, 0.8257174204631024, 0.8267351633094879, 0.8272028543862951, 0.8287897684487951, 0.8258086055158133, 0.8264601374246988, 0.8279249811746988, 0.8290236139871988, 0.8280573465737951, 0.8263071818524097, 0.827538180064006, 0.8275484751506024, 0.826927828501506, 0.8271925592996988, 0.8277926157756024, 0.8290339090737951, 0.828148531626506, 0.8282808970256024, 0.8297766260353916, 0.8279146860881024, 0.8272131494728916, 0.8290544992469879, 0.8331137048192772, 0.8309164391942772, 0.8312517648719879, 0.8275793604103916, 0.8304075677710843, 0.8262057017131024, 0.8310282144201807, 0.8295324854103916, 0.8288000635353916, 0.8310282144201807, 0.8291765695594879, 0.8297766260353916, 0.8291559793862951, 0.8297869211219879, 0.8317606362951807, 0.8277117258094879, 0.8277926157756024, 0.8302752023719879, 0.8285559229103916, 0.8295324854103916, 0.8280676416603916, 0.8286471079631024, 0.8288000635353916, 0.8305193429969879, 0.8284235575112951, 0.8296545557228916, 0.8297766260353916, 0.828026461314006, 0.8296545557228916, 0.8307737787085843, 0.8302752023719879, 0.8295427804969879, 0.8308855539344879, 0.8299295816076807, 0.8285662179969879, 0.8290544992469879, 0.8294104150978916, 0.8292780496987951, 0.8309973291603916, 0.8296442606362951, 0.8287794733621988, 0.8295324854103916, 0.8301222467996988, 0.8298986963478916, 0.8302649072853916, 0.8317297510353916, 0.8315767954631024, 0.8311399896460843, 0.8295324854103916, 0.8307428934487951, 0.8305090479103916], "accuracy_test": 0.8143036511479591, "start": "2016-01-27 00:18:13.192000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0], "accuracy_train_last": 0.9526199560492802, "batch_size_eval": 1024, "accuracy_train_std": [0.013965552129563385, 0.016459493757077386, 0.01734084692214926, 0.017017422575901067, 0.01516107034119793, 0.0160920893864851, 0.01608270768053008, 0.014692560424915162, 0.015249249775867007, 0.014685861383841562, 0.013012750919250816, 0.013709395233536158, 0.01343478895291875, 0.014032290161989026, 0.014185714927948918, 0.014445285141633738, 0.013531822280459288, 0.012999795831302133, 0.013510493134172342, 0.014112470909763308, 0.012882235924744988, 0.01343444385157044, 0.013327149336061642, 0.013139622033947121, 0.013501857903919253, 0.013321144089148468, 0.01305727090523947, 0.012091279053417813, 0.012027556752960437, 0.011807796708304622, 0.011850561671964112, 0.012312002637228467, 0.011444617128235366, 0.011458719335309129, 0.012012491274278373, 0.01229891916196207, 0.011594418186715335, 0.012162629963488034, 0.011748462550682504, 0.010976483122312346, 0.012048994082999523, 0.011646743634839085, 0.012038369487408714, 0.011264742893913448, 0.010869890713531683, 0.01113182892049176, 0.01131078662292289, 0.010438851977401964, 0.01097016887162792, 0.011222643435224616, 0.010487579951341842, 0.010599541975457805, 0.01121513200014855, 0.010342180292998228, 0.009765511411960016, 0.010046593114418276, 0.009763380666542188, 0.010112958619734382, 0.01037603144682425, 0.009674928250188028, 0.010224707977768605, 0.009517083979705708, 0.009090526160713206, 0.00896779136968027, 0.009285123449839696, 0.010041894329535075, 0.009329002075090133, 0.00948083253500719, 0.009141289682280462, 0.009771744982502306, 0.009515722727790162, 0.010239879876723625, 0.009080577261100663, 0.009255206659533449, 0.00949423789514722, 0.009366173127146767, 0.008611230935898051, 0.008523908627031193, 0.008733255653536035, 0.009151114782611925, 0.00906005164511276, 0.00914494055798596, 0.009062410873605467, 0.009475178013013747, 0.008537418351702561, 0.009287395286875163, 0.008321165636515608, 0.00885500560771387, 0.007908638285273028, 0.008898481281889735, 0.008536629698589626, 0.008058649729588781, 0.008459349559829197, 0.008462200477704582, 0.00830813632457749, 0.00808184033737707, 0.00820663542403965, 0.00846412014307249, 0.008353601055189374, 0.008621622308625924, 0.008781356435010924, 0.008264909970932752, 0.008719672188247036, 0.00705943124136849, 0.0078578343100636, 0.008184909002347902, 0.0077853930994353894, 0.008099111874037597, 0.007834358692294055, 0.008427444484949894, 0.008074530283707169, 0.009142518808343204, 0.008242538145870459, 0.008409770848817066, 0.007600831946802819, 0.008067125680690502, 0.00800936046270094, 0.0077694695962109905, 0.008441543483863227, 0.007533296421790484, 0.0078657535381679, 0.007493297447710127, 0.007941014251592125, 0.007910997653537893, 0.008348336105327592, 0.008681558404807199, 0.007215557982085441, 0.007748662086810426, 0.007838752460822891, 0.007814522105402415, 0.007470505066197301, 0.0075589469665368145, 0.007199565374261934, 0.007577993034462914, 0.00711266863073656, 0.0077890284349906936, 0.007773753771900346, 0.007198374945382152, 0.007820504934470587, 0.007668862902552191, 0.0072463590020714706, 0.007348391908364034, 0.008057183201495306, 0.007732874115022343, 0.006884986838040696, 0.007568912657397681, 0.0070986507809239425, 0.007535980849315002, 0.007602004548787262, 0.007421297131138494], "accuracy_test_std": 0.012648414985541057, "error_valid": [0.4820953736822289, 0.3557776025978916, 0.3096335537462349, 0.28453707407756024, 0.26258353727409633, 0.24788362434111444, 0.23697818618222888, 0.23345873729292166, 0.22569624199924698, 0.2207619540662651, 0.21540115540286142, 0.2111183993787651, 0.20854462772966864, 0.20788280073418675, 0.2075577701430723, 0.20402802616716864, 0.19663174181099397, 0.19851427193147586, 0.1949536426957832, 0.1968155826430723, 0.19362116434487953, 0.19438447147966864, 0.1925637118787651, 0.19018260542168675, 0.1880971150225903, 0.18911485786897586, 0.18519831278237953, 0.1860425098832832, 0.1855542286332832, 0.18529979292168675, 0.18454678087349397, 0.1828480915850903, 0.1849438770707832, 0.1838452442582832, 0.18468944135918675, 0.18407908979668675, 0.1837231739457832, 0.1813832478350903, 0.18261424604668675, 0.1803052051957832, 0.18114940229668675, 0.18275690653237953, 0.18027431993599397, 0.1826554263930723, 0.17980662885918675, 0.18053905073418675, 0.17854474538780118, 0.1790639118975903, 0.17769025320030118, 0.17832119493599397, 0.17732404226280118, 0.17770054828689763, 0.17807705431099397, 0.17778143825301207, 0.17719167686370485, 0.17645925498870485, 0.17758877306099397, 0.17673428087349397, 0.17635777484939763, 0.17584890342620485, 0.17575771837349397, 0.17636806993599397, 0.17565623823418675, 0.17649014024849397, 0.17451642507530118, 0.17478115587349397, 0.1756459431475903, 0.17463849538780118, 0.17649014024849397, 0.17489293109939763, 0.17451642507530118, 0.17523855186370485, 0.17391636859939763, 0.17487234092620485, 0.1724206395896084, 0.1724206395896084, 0.17387518825301207, 0.17428257953689763, 0.17326483669051207, 0.17279714561370485, 0.17121023155120485, 0.17419139448418675, 0.17353986257530118, 0.17207501882530118, 0.17097638601280118, 0.17194265342620485, 0.1736928181475903, 0.17246181993599397, 0.17245152484939763, 0.17307217149849397, 0.17280744070030118, 0.17220738422439763, 0.17096609092620485, 0.17185146837349397, 0.17171910297439763, 0.1702233739646084, 0.17208531391189763, 0.1727868505271084, 0.17094550075301207, 0.16688629518072284, 0.16908356080572284, 0.16874823512801207, 0.1724206395896084, 0.16959243222891573, 0.17379429828689763, 0.1689717855798193, 0.1704675145896084, 0.1711999364646084, 0.1689717855798193, 0.17082343044051207, 0.1702233739646084, 0.17084402061370485, 0.17021307887801207, 0.1682393637048193, 0.17228827419051207, 0.17220738422439763, 0.16972479762801207, 0.1714440770896084, 0.1704675145896084, 0.1719323583396084, 0.17135289203689763, 0.1711999364646084, 0.16948065700301207, 0.17157644248870485, 0.1703454442771084, 0.1702233739646084, 0.17197353868599397, 0.1703454442771084, 0.16922622129141573, 0.16972479762801207, 0.17045721950301207, 0.16911444606551207, 0.1700704183923193, 0.17143378200301207, 0.17094550075301207, 0.1705895849021084, 0.17072195030120485, 0.1690026708396084, 0.17035573936370485, 0.17122052663780118, 0.1704675145896084, 0.16987775320030118, 0.1701013036521084, 0.1697350927146084, 0.1682702489646084, 0.16842320453689763, 0.16886001035391573, 0.1704675145896084, 0.16925710655120485, 0.1694909520896084], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "momentum": 0.9814712884517109, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.0011179658761536237, "patience_threshold": 1, "do_flip": true, "batch_size": 32, "l2_decay": 2.0041129734620808e-08, "optimization": "nesterov_momentum", "nb_data_augmentation": 1, "learning_rate_decay_method": "lin", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.04905700476489906}, "accuracy_valid_max": 0.8331137048192772, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        #nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        nb_data_augmentation=make_constant_param(1),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8305090479103916, "loss_train": [1.5850615501403809, 1.1949769258499146, 0.9887768626213074, 0.8846662640571594, 0.8162288069725037, 0.7710015773773193, 0.7363873720169067, 0.7076597213745117, 0.6791552901268005, 0.660577118396759, 0.6407855749130249, 0.6246240139007568, 0.6110177636146545, 0.5989201068878174, 0.5852391719818115, 0.5758053064346313, 0.5659884810447693, 0.5572159886360168, 0.546911895275116, 0.5386353731155396, 0.5323532223701477, 0.5244485139846802, 0.5188949704170227, 0.5131350755691528, 0.507009744644165, 0.5006394386291504, 0.49648842215538025, 0.49235713481903076, 0.4862375557422638, 0.4813864231109619, 0.47690045833587646, 0.4722141623497009, 0.46805280447006226, 0.4648447632789612, 0.4600435495376587, 0.4583892822265625, 0.45102500915527344, 0.44889551401138306, 0.44627881050109863, 0.43997716903686523, 0.43904581665992737, 0.4359697699546814, 0.434726744890213, 0.4304302930831909, 0.4280520975589752, 0.42575258016586304, 0.422692209482193, 0.42169713973999023, 0.4174667000770569, 0.41518354415893555, 0.4112136960029602, 0.408864825963974, 0.40661484003067017, 0.404778391122818, 0.40204358100891113, 0.4025542736053467, 0.3998885750770569, 0.3965435326099396, 0.3958035707473755, 0.39373764395713806, 0.3926389217376709, 0.38809970021247864, 0.3879358768463135, 0.38651567697525024, 0.3818327784538269, 0.3821440637111664, 0.38172003626823425, 0.38133037090301514, 0.3791820704936981, 0.3747820258140564, 0.374399870634079, 0.3729705810546875, 0.37010589241981506, 0.3682665228843689, 0.3681595027446747, 0.3654271364212036, 0.3636813461780548, 0.3644125759601593, 0.363110214471817, 0.36059167981147766, 0.3615046441555023, 0.35966718196868896, 0.35858407616615295, 0.35597577691078186, 0.354943186044693, 0.3530074656009674, 0.3541140854358673, 0.352583646774292, 0.3504493534564972, 0.3510906994342804, 0.3504023849964142, 0.3454555869102478, 0.34720245003700256, 0.34754884243011475, 0.34494125843048096, 0.34302231669425964, 0.34249642491340637, 0.34128278493881226, 0.3399091362953186, 0.33728528022766113, 0.336377888917923, 0.33706799149513245, 0.3375750780105591, 0.333541601896286, 0.33433976769447327, 0.3344326913356781, 0.3329947292804718, 0.3322988450527191, 0.33243653178215027, 0.33201220631599426, 0.3281201124191284, 0.3293987810611725, 0.3256189227104187, 0.32707905769348145, 0.3250919282436371, 0.32257264852523804, 0.32421568036079407, 0.32403257489204407, 0.32450345158576965, 0.323615700006485, 0.3244912624359131, 0.32341256737709045, 0.3185269832611084, 0.31986895203590393, 0.32050034403800964, 0.3203539252281189, 0.3202117681503296, 0.31835541129112244, 0.31294241547584534, 0.31556034088134766, 0.3132621645927429, 0.31506526470184326, 0.3138045072555542, 0.31447654962539673, 0.3101534843444824, 0.31023865938186646, 0.3115808665752411, 0.3117039203643799, 0.30875253677368164, 0.31149300932884216, 0.3074660897254944, 0.3080698847770691, 0.31057897210121155, 0.3086477816104889, 0.30615225434303284, 0.3041413724422455, 0.30720609426498413, 0.3040772080421448, 0.3029215335845947, 0.30176427960395813], "accuracy_train_first": 0.5233816243309339, "model": "residualv3", "loss_std": [0.28458696603775024, 0.20016755163669586, 0.18627746403217316, 0.17915983498096466, 0.17796435952186584, 0.17363278567790985, 0.17198264598846436, 0.1692834198474884, 0.16633862257003784, 0.16473740339279175, 0.1641554981470108, 0.16174963116645813, 0.16020262241363525, 0.15900641679763794, 0.15614204108715057, 0.15561670064926147, 0.15693704783916473, 0.1554889976978302, 0.15377254784107208, 0.1511317938566208, 0.15232868492603302, 0.1495368331670761, 0.14465467631816864, 0.14916706085205078, 0.146977961063385, 0.14806601405143738, 0.14669451117515564, 0.14442488551139832, 0.14429470896720886, 0.14183753728866577, 0.14171595871448517, 0.14043375849723816, 0.14040054380893707, 0.1381593644618988, 0.1372145414352417, 0.13938374817371368, 0.1366889774799347, 0.1358080953359604, 0.1343332678079605, 0.13528668880462646, 0.1368670016527176, 0.1339847445487976, 0.13218237459659576, 0.1305055320262909, 0.13272510468959808, 0.1342877745628357, 0.13169312477111816, 0.13092248141765594, 0.13039076328277588, 0.13017180562019348, 0.13117758929729462, 0.1293269693851471, 0.12976400554180145, 0.12690292298793793, 0.12796366214752197, 0.12736015021800995, 0.12402892112731934, 0.12709425389766693, 0.1265229433774948, 0.12513336539268494, 0.1262870728969574, 0.12487013638019562, 0.1254998743534088, 0.12327747046947479, 0.1235101968050003, 0.12127894908189774, 0.12256056815385818, 0.12147128582000732, 0.12348927557468414, 0.123040109872818, 0.12098921090364456, 0.12097501009702682, 0.11933894455432892, 0.11966657638549805, 0.11840729415416718, 0.12369199097156525, 0.11920200288295746, 0.11820891499519348, 0.1182837188243866, 0.11807655543088913, 0.11851831525564194, 0.11789672821760178, 0.11595126241445541, 0.11712755262851715, 0.11577059328556061, 0.11837808787822723, 0.11636019498109818, 0.11874996870756149, 0.11606234312057495, 0.11559358239173889, 0.11815979331731796, 0.11347290128469467, 0.11764659732580185, 0.11437560617923737, 0.11552023142576218, 0.11553767323493958, 0.11609327048063278, 0.11527860164642334, 0.11375150084495544, 0.1118183583021164, 0.11273092776536942, 0.11324821412563324, 0.11078865081071854, 0.11410868167877197, 0.10961630195379257, 0.11440636217594147, 0.1105157732963562, 0.11542634665966034, 0.11222615092992783, 0.11194731295108795, 0.11125583201646805, 0.11281979829072952, 0.11278702318668365, 0.10973817110061646, 0.10880786925554276, 0.11143900454044342, 0.10922021418809891, 0.10944966971874237, 0.10905396193265915, 0.11094902455806732, 0.1109839379787445, 0.11034455895423889, 0.1078772097826004, 0.11192867159843445, 0.11127931624650955, 0.10957834869623184, 0.1105964407324791, 0.11044555902481079, 0.10620436072349548, 0.1092025488615036, 0.10637359321117401, 0.10612829029560089, 0.10804124176502228, 0.10622759908437729, 0.10675712674856186, 0.10585784912109375, 0.10622663795948029, 0.10998225957155228, 0.10814671963453293, 0.10819249600172043, 0.10594669729471207, 0.10550764203071594, 0.10786174237728119, 0.1065293699502945, 0.10518699139356613, 0.10527698695659637, 0.10617802292108536, 0.1041284129023552, 0.10595263540744781, 0.10372365266084671]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:18 2016", "state": "available"}], "summary": "6014ffc0901fe8f83de43e90be606f2b"}