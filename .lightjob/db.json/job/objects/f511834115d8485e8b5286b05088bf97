{"content": {"hp_model": {"f0": 16, "f1": 32, "f2": 64, "f3": 32, "nonlin": "very_leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.5254852771759033, 1.1142290830612183, 0.8851484060287476, 0.7714264988899231, 0.7001654505729675, 0.6463518142700195, 0.6080108880996704, 0.5759032964706421, 0.5518350601196289, 0.5283458828926086, 0.5083890557289124, 0.49160659313201904, 0.47646597027778625, 0.46250030398368835, 0.4516047537326813, 0.4385688304901123, 0.4290311932563782, 0.4206031262874603, 0.4104219973087311, 0.40258586406707764, 0.39514899253845215, 0.38859543204307556, 0.3827391266822815, 0.37503281235694885, 0.37025564908981323, 0.36423295736312866, 0.36007437109947205, 0.35269367694854736, 0.34710681438446045, 0.3448224365711212, 0.33969736099243164, 0.33464449644088745, 0.33266791701316833, 0.3267965614795685, 0.32360073924064636, 0.3204105496406555, 0.31541764736175537, 0.31149452924728394, 0.3090397119522095, 0.30822357535362244, 0.30387255549430847, 0.3004053831100464, 0.29972851276397705, 0.29605308175086975, 0.29328277707099915, 0.2895028293132782, 0.28825506567955017, 0.28721997141838074, 0.2848672866821289, 0.2830348014831543, 0.28086817264556885, 0.27755236625671387, 0.27675530314445496, 0.2715015709400177, 0.27396705746650696, 0.26951101422309875, 0.26903295516967773, 0.266480952501297, 0.26516351103782654, 0.26380252838134766, 0.2615278959274292, 0.25888335704803467, 0.25808820128440857, 0.25820931792259216, 0.2538253962993622, 0.2543201148509979, 0.25121375918388367, 0.25140631198883057, 0.2511271834373474, 0.24871566891670227, 0.2486291080713272, 0.24516648054122925, 0.24578703939914703, 0.2442227303981781, 0.24137812852859497, 0.24207356572151184, 0.2402784824371338, 0.2375861406326294, 0.2378564178943634, 0.23712314665317535, 0.23659780621528625, 0.23549090325832367, 0.2340099811553955, 0.23264481127262115, 0.23174895346164703, 0.2320176512002945, 0.23048129677772522, 0.2304811030626297, 0.22812378406524658, 0.2262723743915558, 0.22622692584991455, 0.22526858747005463, 0.22502079606056213, 0.22400343418121338, 0.22252103686332703, 0.22089241445064545, 0.22050979733467102, 0.21985037624835968, 0.21903371810913086, 0.2179591804742813, 0.21630366146564484, 0.2178242951631546, 0.21526522934436798, 0.21416312456130981, 0.21443456411361694, 0.2152731567621231, 0.21294648945331573, 0.21162347495555878, 0.21249717473983765, 0.21276283264160156, 0.21037451922893524, 0.20981290936470032, 0.20901231467723846, 0.2090539038181305, 0.20934610068798065, 0.20654729008674622, 0.20523564517498016, 0.2047470659017563, 0.20459161698818207, 0.2041901797056198, 0.20482558012008667, 0.2045225352048874, 0.2041705846786499, 0.20123960077762604, 0.20181623101234436, 0.2028169482946396, 0.20093156397342682, 0.2005164474248886, 0.20022043585777283, 0.1992889940738678, 0.19831055402755737, 0.19871214032173157, 0.19662189483642578, 0.1969299614429474, 0.19740180671215057, 0.19658710062503815, 0.19544640183448792, 0.19416393339633942, 0.19452247023582458, 0.19257014989852905, 0.1946331411600113, 0.19222870469093323, 0.19348162412643433, 0.19302918016910553, 0.1911754608154297, 0.19294124841690063, 0.19195540249347687, 0.18810778856277466, 0.18897895514965057, 0.189274862408638, 0.19094647467136383, 0.18820448219776154, 0.18787148594856262, 0.18908433616161346, 0.18674109876155853, 0.1871580183506012, 0.1865890920162201, 0.18507806956768036, 0.18314284086227417, 0.18555136024951935, 0.18446199595928192, 0.18503360450267792, 0.18555878102779388, 0.18281003832817078, 0.18430665135383606], "moving_avg_accuracy_train": [0.06185813275770578, 0.12849297011985508, 0.19459015358959714, 0.25772080245752, 0.3164144570885675, 0.3707082042553106, 0.4207698841470091, 0.46656719061741336, 0.5085773537942748, 0.5471491134141552, 0.582275284460152, 0.6143512546240943, 0.6435845679882815, 0.6704106249053079, 0.6950352377389281, 0.71747175684871, 0.7379413167558471, 0.7565614501746237, 0.7737357358396088, 0.7894110127309154, 0.8037488795675969, 0.8168065637373138, 0.8287931753245459, 0.8397856667506554, 0.8498275743626884, 0.8589817649469068, 0.8673157233786188, 0.8749813354838171, 0.882038532546362, 0.8884341877300332, 0.8945156900822145, 0.899924082227786, 0.9048682208742396, 0.9095458102393811, 0.9138626696108841, 0.9177526014893121, 0.9214696708727268, 0.924840573905886, 0.9280463395011778, 0.930922263990521, 0.9336500689106826, 0.9361469460173996, 0.9385848336646444, 0.9408160628304797, 0.9428730332535502, 0.9447915917545149, 0.9466531890851542, 0.9483704433124822, 0.9499462351004201, 0.9514829942500311, 0.9529427713465766, 0.9542891228168008, 0.9554660340054972, 0.9565624204074576, 0.9576817016513649, 0.9585705082304145, 0.959514521280112, 0.9604338874891256, 0.9612427158867616, 0.9620984364827015, 0.9627825545130951, 0.9636261253237827, 0.9643295715307917, 0.9649906109516327, 0.9656133761184665, 0.9662504504840562, 0.966837912501219, 0.9673503522749989, 0.9680067884737633, 0.9685394523324132, 0.9689747440754547, 0.9693990226787065, 0.9697366955942522, 0.9700754784503862, 0.9704710638244782, 0.9708503421492563, 0.9711661160046517, 0.9715061160459363, 0.9718912432402537, 0.9722679765032068, 0.9726093615886742, 0.9728678160894135, 0.9732166104829177, 0.9734329052358903, 0.973720504368309, 0.9740374723077241, 0.9743273577019979, 0.9746557559699579, 0.9748653199539883, 0.9750956720717309, 0.9753332880098605, 0.975586669883939, 0.9757519706015714, 0.9759704236140886, 0.9761670313253541, 0.9763184737262259, 0.9764989136655727, 0.9766125175348036, 0.9767774679373309, 0.9768631442817484, 0.9769611793310099, 0.9771377665301072, 0.9773431619366664, 0.9774931766192456, 0.9776653561657006, 0.9777971023182336, 0.9779808501198174, 0.978108948662653, 0.9782892694202341, 0.9784748456389711, 0.978609312152501, 0.9788489867016013, 0.9789623151505349, 0.9790573713569654, 0.9792220490999141, 0.9793655727221301, 0.9794947439821245, 0.9795853493815773, 0.9797040605732183, 0.9798156590897706, 0.9798858708201439, 0.9800024677024614, 0.9801539078727376, 0.9802042095688526, 0.9802401444512991, 0.9803539021026532, 0.980500425767434, 0.980709026976451, 0.9807968587633944, 0.9808503667835574, 0.9809588697242954, 0.9810797738590548, 0.9811700224386808, 0.9812187301258296, 0.9812555195001976, 0.9813374580621288, 0.9814042994190758, 0.9815363920581484, 0.9816482999868851, 0.9817073086394517, 0.9817882100660196, 0.9818401310594638, 0.9819217732345251, 0.9819811921527669, 0.9820324161280126, 0.9821459470212098, 0.9822457636274591, 0.9823309482754644, 0.9823194029991823, 0.9823832728171581, 0.9824291659581075, 0.9824472182968667, 0.9824867889874827, 0.9825129938673427, 0.9824785576854349, 0.9825661477110037, 0.9826519181316253, 0.9827012457732893, 0.9827595194460066, 0.9827863891145474, 0.9827919345769391, 0.9827551809609765, 0.9827616302363721, 0.982716209212781, 0.9827916237808441], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.0610701595444277, 0.12714111328125, 0.19215725891848642, 0.2533605724656438, 0.309912516442724, 0.36149388928075576, 0.40881029713355366, 0.4517745116784814, 0.49094999961078384, 0.5270584599753831, 0.5594400339703147, 0.5884816762811146, 0.6149549212565423, 0.638803196779608, 0.660352093969117, 0.6798050775786059, 0.6975955835632152, 0.7134371700205231, 0.7280210991498713, 0.7410245650537848, 0.7527388618898972, 0.7634190949121274, 0.7732652972432038, 0.781805378494034, 0.7896511725346909, 0.7968700490688724, 0.8034189546006147, 0.8094350398916828, 0.814800688528644, 0.8199003860067283, 0.8241981744956639, 0.828221787015977, 0.8315277144890781, 0.8348316095499595, 0.8381052904153701, 0.8408501136422819, 0.8432716264215024, 0.8454886385252106, 0.8474340917848884, 0.8492582419060983, 0.8508643854300969, 0.8522875595565149, 0.8539590412702911, 0.8552446777588493, 0.8564342536576631, 0.8572607313415956, 0.8583606241807041, 0.8594868640969711, 0.8603895872317017, 0.8613495519366189, 0.8621636625373847, 0.8626532509617336, 0.8632169803648072, 0.8638240520948928, 0.8642473168308101, 0.8647706214334973, 0.8652843937216235, 0.865731493223708, 0.8660738771279938, 0.8663067214370317, 0.8665417248863255, 0.8670116346642592, 0.8674223464331496, 0.8679049388323798, 0.8679811500507985, 0.8680863612411253, 0.868401807383579, 0.8686368807867874, 0.8688505058669942, 0.8690885080382014, 0.8694634604158571, 0.8696676697206569, 0.8698402805723864, 0.8699915123043044, 0.8701764489880306, 0.8705026129182938, 0.8707870419502596, 0.8710064069852788, 0.8712882552268865, 0.8714930905193333, 0.8715075733536952, 0.8714697207623017, 0.8717286221800474, 0.8716788127199493, 0.8716228066832706, 0.8717321221651694, 0.8718671271926283, 0.8719275965610913, 0.8718589191715485, 0.8717706364411405, 0.8717654536799331, 0.8718340313823465, 0.8720575312467475, 0.871963653357389, 0.8721141853766953, 0.8722629007339805, 0.8722227575920584, 0.872248693429238, 0.8720248065317209, 0.8720817149975247, 0.8716995094710404, 0.8717045279328218, 0.8717934642585156, 0.8717127565280706, 0.8718232250394201, 0.8718758775919541, 0.8719100283493249, 0.872040479298278, 0.8721792106888567, 0.8720934608831488, 0.8719583394277405, 0.8719333568592134, 0.8719749962297679, 0.8720480632483575, 0.8723834077612477, 0.8726485967290988, 0.8727519599477551, 0.8728694009070459, 0.8727777262530882, 0.8724999065645265, 0.8725560741347305, 0.872560885348893, 0.8724278495719102, 0.8725257849178066, 0.8725570096075018, 0.8724090658474293, 0.8723553357028219, 0.8723527181716964, 0.872388012996093, 0.8723455066418904, 0.8722085651644482, 0.8722073881472503, 0.8721686782293626, 0.8723342993465619, 0.8723704065448123, 0.8724649676881474, 0.8725256586546489, 0.8725568959706599, 0.8726104531262294, 0.8726576250575824, 0.8726624291933904, 0.8728142667992773, 0.8729275360907351, 0.8731027206405472, 0.873101695329128, 0.8730244418353718, 0.8731655217482202, 0.873157186817374, 0.872962461876224, 0.8726549911027582, 0.8726315265369101, 0.8725015746550564, 0.8724090320238882, 0.8723867788120868, 0.8724064605411944, 0.8725472739185509, 0.8726353258471025, 0.8727634007077989, 0.8728064554035853, 0.8728950622634526, 0.8728639156474236, 0.8727382274429974, 0.8728336570989236, 0.8726845216695281, 0.8726988431753916], "moving_var_accuracy_train": [0.03443785729442959, 0.07095588551750744, 0.10317983592945137, 0.128731161774871, 0.14686255144292212, 0.15870659513128593, 0.1653914817603674, 0.16772887310382786, 0.16683967008476383, 0.16354572883785196, 0.15829578698524038, 0.15172601904433944, 0.14424469663214431, 0.13629696293636975, 0.12812461065758401, 0.11984272609969812, 0.11162947943485518, 0.10358691580816706, 0.09588282902027295, 0.08850597486881788, 0.08150554721077377, 0.07488952053258108, 0.06869367819541147, 0.06291182418564814, 0.05752820094348079, 0.052529573696402446, 0.04790171009503574, 0.043640393566058436, 0.039724590482868916, 0.03612027108163782, 0.03284110601121031, 0.029820251760491794, 0.02705822714704082, 0.02454932301275671, 0.02226210818498104, 0.02017208149665223, 0.018279222790197076, 0.01655356739650803, 0.014990703054324836, 0.013566071223907992, 0.012276432378659316, 0.011104898698367808, 0.010047898494156332, 0.009087914097054966, 0.008217202833241953, 0.0074286103504123705, 0.0067169392169641245, 0.006071785953999198, 0.005486955436429669, 0.0049595145509419205, 0.004482741638392115, 0.004050781435085278, 0.0036581693710914607, 0.0033031710022639483, 0.0029841290165642164, 0.0026928259091224508, 0.0024315637639521995, 0.002196014495593463, 0.0019823008764255183, 0.0017906611084278086, 0.001615807154900614, 0.0014606309448243486, 0.0013190213794373113, 0.0011910519995367336, 0.0010754373276602536, 0.0009715463686198515, 0.0008774977363523478, 0.0007921113134128774, 0.0007167783584190242, 0.0006476540996539278, 0.0005845939998025755, 0.0005277547108209141, 0.0004760054467198612, 0.00042943786646036793, 0.0003879024699080911, 0.0003504068913461002, 0.00031626362036125234, 0.00028567765857778846, 0.0002584447993222348, 0.00023387767095274983, 0.00021153879784669138, 0.00019098610662259457, 0.0001729824137207948, 0.00015610522313018636, 0.00014123912016587978, 0.0001280194262208444, 0.00011597378547507911, 0.00010534701572916373, 9.520756772687171e-05, 8.616436983752089e-05, 7.805608486024778e-05, 7.082829774122704e-05, 6.399138691235215e-05, 5.802174368921784e-05, 5.256746064945781e-05, 4.751712779154824e-05, 4.305844215779663e-05, 3.886875049395495e-05, 3.5226753162205025e-05, 3.1770141769919286e-05, 2.867962543088068e-05, 2.609231023775769e-05, 2.3862764671302653e-05, 2.1679027849076466e-05, 1.9777937230125605e-05, 1.7956356945478138e-05, 1.646459054221228e-05, 1.4965814618080348e-05, 1.3761873336804053e-05, 1.269563279977024e-05, 1.1588800709141416e-05, 1.0946915643604823e-05, 9.967814115283676e-06, 9.05235384518385e-06, 8.391187291869652e-06, 7.737459833888849e-06, 7.113880780176957e-06, 6.476376747849173e-06, 5.9555701962519e-06, 5.472101236696759e-06, 4.969258296765149e-06, 4.594685963784035e-06, 4.341624493965206e-06, 3.930234390257053e-06, 3.5488327932194134e-06, 3.3104167430717377e-06, 3.172597727831803e-06, 3.246968134678945e-06, 2.9917011263899483e-06, 2.7182989877468015e-06, 2.5524250823113143e-06, 2.428742862297615e-06, 2.259171831188609e-06, 2.0546065971563156e-06, 1.861327060038149e-06, 1.735619705416436e-06, 1.6022676378613966e-06, 1.5990770617494649e-06, 1.5518798162016872e-06, 1.42803002428102e-06, 1.3441323892394817e-06, 1.2339812563576156e-06, 1.1705721334606346e-06, 1.085290390719846e-06, 1.0003764124075624e-06, 1.016342144558335e-06, 1.0043781240506e-06, 9.692481299476168e-07, 8.735229575927323e-07, 8.228848446678472e-07, 7.595519836768614e-07, 6.865297677212717e-07, 6.31969346951621e-07, 5.749526738127914e-07, 5.281300620509701e-07, 5.44365169058061e-07, 5.561377376348349e-07, 5.22422909960518e-07, 5.007430073520385e-07, 4.571665184042305e-07, 4.1172663594204693e-07, 3.82711426924802e-07, 3.4481462261046725e-07, 3.2890078480596003e-07, 3.4719692001051023e-07], "duration": 185541.656767, "accuracy_train": [0.618581327577058, 0.7282065063791989, 0.7894648048172758, 0.8258966422688261, 0.8446573487679956, 0.8593519287559985, 0.871325003172296, 0.8787429488510521, 0.8866688223860282, 0.8942949499930787, 0.8984108238741234, 0.9030349860995754, 0.9066843882659652, 0.9118451371585455, 0.9166567532415099, 0.9194004288367479, 0.9221673559200813, 0.9241426509436139, 0.928304306824474, 0.9304885047526762, 0.9327896810977298, 0.9343257212647655, 0.9366726796096345, 0.9387180895856404, 0.9402047428709857, 0.9413694802048725, 0.9423213492640274, 0.9439718444306018, 0.9455533061092655, 0.945995084383075, 0.9492492112518457, 0.9485996115379292, 0.949365468692322, 0.9516441145256552, 0.9527144039544113, 0.9527619883951642, 0.9549232953234589, 0.955178701204319, 0.956898229858804, 0.9568055843946106, 0.9582003131921374, 0.9586188399778516, 0.9605258224898486, 0.9608971253229974, 0.961385767061185, 0.9620586182631967, 0.9634075650609081, 0.9638257313584349, 0.9641283611918604, 0.9653138265965301, 0.9660807652154854, 0.9664062860488187, 0.9660582347037652, 0.9664298980251015, 0.9677552328465301, 0.9665697674418604, 0.9680106387273901, 0.9687081833702473, 0.9685221714654854, 0.9697999218461609, 0.9689396167866371, 0.9712182626199704, 0.9706605873938722, 0.9709399657392026, 0.9712182626199704, 0.9719841197743633, 0.9721250706556847, 0.971962310239018, 0.9739147142626431, 0.9733334270602622, 0.9728923697628276, 0.9732175301079733, 0.9727757518341639, 0.9731245241555924, 0.9740313321913067, 0.9742638470722591, 0.9740080807032114, 0.9745661164174971, 0.9753573879891103, 0.975658575869786, 0.9756818273578812, 0.9751939065960686, 0.9763557600244556, 0.9753795580126431, 0.9763088965600776, 0.9768901837624585, 0.9769363262504615, 0.9776113403815985, 0.9767513958102622, 0.9771688411314139, 0.9774718314530271, 0.977867106750646, 0.9772396770602622, 0.9779365007267442, 0.9779365007267442, 0.9776814553340717, 0.9781228731196937, 0.9776349523578812, 0.9782620215600776, 0.9776342313815062, 0.9778434947743633, 0.9787270513219823, 0.9791917205956996, 0.9788433087624585, 0.9792149720837948, 0.97898281769103, 0.9796345803340717, 0.9792618355481728, 0.9799121562384644, 0.9801450316076044, 0.979819510774271, 0.9810060576435032, 0.9799822711909376, 0.9799128772148394, 0.9807041487864526, 0.9806572853220746, 0.9806572853220746, 0.9804007979766519, 0.9807724612979882, 0.9808200457387413, 0.9805177763935032, 0.9810518396433187, 0.9815168694052234, 0.9806569248338871, 0.9805635583933187, 0.9813777209648394, 0.9818191387504615, 0.9825864378576044, 0.9815873448458842, 0.9813319389650241, 0.9819353961909376, 0.98216791107189, 0.9819822596553157, 0.9816570993101699, 0.9815866238695091, 0.9820749051195091, 0.9820058716315985, 0.9827252258098007, 0.982655471345515, 0.9822383865125508, 0.9825163229051311, 0.9823074200004615, 0.9826565528100776, 0.9825159624169435, 0.9824934319052234, 0.9831677250599853, 0.9831441130837025, 0.9830976101075121, 0.9822154955126431, 0.9829581011789406, 0.9828422042266519, 0.9826096893456996, 0.9828429252030271, 0.9827488377860835, 0.9821686320482651, 0.9833544579411223, 0.9834238519172205, 0.9831451945482651, 0.9832839825004615, 0.9830282161314139, 0.9828418437384644, 0.9824243984173128, 0.9828196737149317, 0.9823074200004615, 0.983470354893411], "end": "2016-02-05 15:19:44.208000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0], "moving_var_accuracy_valid": [0.033566079481036695, 0.06949780988217283, 0.1005919216356566, 0.12424533977445586, 0.14060390710529164, 0.15048925860946688, 0.15558991481735102, 0.15664423691877916, 0.15479228291950703, 0.1510474428166729, 0.14537979554450867, 0.13843256888303376, 0.13089680629049147, 0.1229257878702588, 0.11481240381398923, 0.10673693057438971, 0.09891175644564651, 0.09127918355444113, 0.0840654840986457, 0.0771807468184093, 0.07069769488984937, 0.06465453179754664, 0.05906160792089243, 0.05381184401874022, 0.04898466797402185, 0.04455521078236148, 0.0404856831770985, 0.036762854399453294, 0.03334568062716588, 0.030245174793761112, 0.02738689618744565, 0.02479391168812367, 0.022412882927427945, 0.020269836137845, 0.018339305401737505, 0.016573181352486702, 0.01496863673449739, 0.01351600934505955, 0.012198471506023911, 0.011008572068403912, 0.009930932134740665, 0.008956067742613549, 0.008085605628427589, 0.007291920816211241, 0.006575464551961467, 0.0059240656850236655, 0.005342546994838999, 0.004819708042496035, 0.004345071419768232, 0.00391885806790359, 0.003532937245745745, 0.003181800792598478, 0.002866480830897638, 0.0025831495725770954, 0.0023264469926494264, 0.002096266922749225, 0.0018890158881507203, 0.0017019133810185273, 0.0015327770835579008, 0.001379987323452373, 0.0012424856306977557, 0.0011202244044225596, 0.0010097201213942492, 0.0009108441680689773, 0.0008198120246103953, 0.0007379304467004856, 0.0006650329584495376, 0.0005990269981486475, 0.0005395350194078231, 0.00048609132276853485, 0.0004387474940612688, 0.0003952480576166434, 0.0003559914024101921, 0.0003205981014998231, 0.00028884610554272955, 0.0002609189411730993, 0.00023555514592381443, 0.0002124327204987339, 0.0001919043943305365, 0.00017309157237076908, 0.00015578430290611256, 0.0001402187679835782, 0.00012680016068221717, 0.00011414247345483285, 0.00010275645619464963, 9.258835944642967e-05, 8.349356071873949e-05, 7.517711354756647e-05, 6.770185144731961e-05, 6.100181086698205e-05, 5.490187152940745e-05, 4.9454010487881306e-05, 4.4958179143578104e-05, 4.0541678752213916e-05, 3.6691449876520366e-05, 3.322135120630076e-05, 2.991371933226112e-05, 2.6928401407886917e-05, 2.4686689353017032e-05, 2.2247167579036624e-05, 2.1337180401409945e-05, 1.9203689025896823e-05, 1.735450715355824e-05, 1.567768007798455e-05, 1.4219741698184211e-05, 1.2822718149960798e-05, 1.1550942803025755e-05, 1.0549005573467936e-05, 9.667322604708573e-06, 8.766767606848287e-06, 8.054411115568244e-06, 7.254587162583305e-06, 6.544732980946601e-06, 5.938308785701932e-06, 6.356581388061556e-06, 6.353849947284974e-06, 5.814620547295568e-06, 5.357289902838215e-06, 4.897199092158613e-06, 5.102133197116161e-06, 4.620313040888116e-06, 4.1584900668347605e-06, 3.901927721767718e-06, 3.598056937373437e-06, 3.2470260748551724e-06, 3.1193096726690598e-06, 2.833361061357931e-06, 2.5500866184448777e-06, 2.306289478263093e-06, 2.0919216417652145e-06, 2.0515061917847945e-06, 1.8463680409316711e-06, 1.6752173565243428e-06, 1.7545688110329334e-06, 1.5908454978191061e-06, 1.5122372364966578e-06, 1.3941640535810024e-06, 1.2635295774270508e-06, 1.1629919398986554e-06, 1.066719465876908e-06, 9.602552367769755e-07, 1.0717216401523609e-06, 1.080018867623305e-06, 1.2482236192965488e-06, 1.1234107187384506e-06, 1.0647825675424123e-06, 1.1374361870718817e-06, 1.024317808014578e-06, 1.263146251566133e-06, 1.9876761152302743e-06, 1.7938637763612083e-06, 1.7664648231008549e-06, 1.6668955880427304e-06, 1.5046628781577893e-06, 1.3576829244879912e-06, 1.4003702972219914e-06, 1.3301115465945942e-06, 1.3447289214167044e-06, 1.226939390738356e-06, 1.1749060322045413e-06, 1.0661464341946017e-06, 1.1017095133618216e-06, 1.0734999350973108e-06, 1.1663223282960576e-06, 1.0515360452382253e-06], "accuracy_test": 0.8485610650510204, "start": "2016-02-03 11:47:22.551000", "learning_rate_per_epoch": [0.0010239607654511929, 0.0005119803827255964, 0.000341320235747844, 0.0002559901913627982, 0.00020479214435908943, 0.000170660117873922, 0.00014628010103479028, 0.0001279950956813991, 0.0001137734143412672, 0.00010239607217954472, 9.308733569923788e-05, 8.5330058936961e-05, 7.876620657043532e-05, 7.314005051739514e-05, 6.826404569437727e-05, 6.399754784069955e-05, 6.023298192303628e-05, 5.68867071706336e-05, 5.3892668802291155e-05, 5.119803608977236e-05, 4.8760033678263426e-05, 4.654366784961894e-05, 4.4520031224237755e-05, 4.26650294684805e-05, 4.0958428144222125e-05, 3.938310328521766e-05, 3.7924470234429464e-05, 3.657002525869757e-05, 3.530898902681656e-05, 3.413202284718864e-05, 3.303099219920114e-05, 3.199877392034978e-05, 3.1029114325065166e-05, 3.011649096151814e-05, 2.9256019843160175e-05, 2.84433535853168e-05, 2.7674614102579653e-05, 2.6946334401145577e-05, 2.6255402190145105e-05, 2.559901804488618e-05, 2.4974651751108468e-05, 2.4380016839131713e-05, 2.3813039661035873e-05, 2.327183392480947e-05, 2.275468250445556e-05, 2.2260015612118877e-05, 2.178639806516003e-05, 2.133251473424025e-05, 2.0897157810395584e-05, 2.0479214072111063e-05, 2.0077661247341894e-05, 1.969155164260883e-05, 1.932001396198757e-05, 1.8962235117214732e-05, 1.8617467503645457e-05, 1.8285012629348785e-05, 1.796422293409705e-05, 1.765449451340828e-05, 1.735526711854618e-05, 1.706601142359432e-05, 1.6786241758381948e-05, 1.651549609960057e-05, 1.6253345165750943e-05, 1.599938696017489e-05, 1.5753241314087063e-05, 1.5514557162532583e-05, 1.5282996173482388e-05, 1.505824548075907e-05, 1.4840010408079252e-05, 1.4628009921580087e-05, 1.4421982086787466e-05, 1.42216767926584e-05, 1.4026859389559831e-05, 1.3837307051289827e-05, 1.3652809684572276e-05, 1.3473167200572789e-05, 1.3298191333888099e-05, 1.3127701095072553e-05, 1.2961528227606323e-05, 1.279950902244309e-05, 1.2641490684472956e-05, 1.2487325875554234e-05, 1.2336876352492254e-05, 1.2190008419565856e-05, 1.2046596566506196e-05, 1.1906519830517936e-05, 1.1769663615268655e-05, 1.1635916962404735e-05, 1.1505176189530175e-05, 1.137734125222778e-05, 1.125231574405916e-05, 1.1130007806059439e-05, 1.1010330126737244e-05, 1.0893199032580014e-05, 1.077853357855929e-05, 1.0666257367120124e-05, 1.055629581969697e-05, 1.0448578905197792e-05, 1.0343037502025254e-05, 1.0239607036055531e-05, 1.0138224752154201e-05, 1.0038830623670947e-05, 9.941366442944854e-06, 9.845775821304414e-06, 9.752006917551626e-06, 9.660006980993785e-06, 9.569725989422295e-06, 9.481117558607366e-06, 9.394135304319207e-06, 9.308733751822729e-06, 9.22487106436165e-06, 9.142506314674392e-06, 9.061599484994076e-06, 8.982111467048526e-06, 8.90400588104967e-06, 8.82724725670414e-06, 8.751801033213269e-06, 8.67763355927309e-06, 8.60471209307434e-06, 8.53300571179716e-06, 8.46248531161109e-06, 8.393120879190974e-06, 8.324884220201056e-06, 8.257748049800284e-06, 8.191685992642306e-06, 8.126672582875472e-06, 8.062683264142834e-06, 7.999693480087444e-06, 7.937679583847057e-06, 7.876620657043532e-06, 7.816493962309323e-06, 7.757278581266291e-06, 7.698952686041594e-06, 7.641498086741194e-06, 7.584894319734303e-06, 7.529122740379535e-06, 7.474165613530204e-06, 7.420005204039626e-06, 7.3666237767611165e-06, 7.314004960790044e-06, 7.2621328399691265e-06, 7.210991043393733e-06, 7.160564564401284e-06, 7.1108383963292e-06, 7.061797987262253e-06, 7.013429694779916e-06, 6.965718966966961e-06, 6.918653525644913e-06, 6.872219728393247e-06, 6.826404842286138e-06, 6.781196589145111e-06, 6.736583600286394e-06, 6.692553597531514e-06, 6.649095666944049e-06, 6.6061979850928765e-06, 6.563850547536276e-06, 6.522042895085178e-06, 6.480764113803161e-06, 6.440004653995857e-06, 6.399754511221545e-06, 6.360004590533208e-06, 6.320745342236478e-06, 6.2819676713843364e-06, 6.243662937777117e-06, 6.2058225012151524e-06], "accuracy_train_first": 0.618581327577058, "accuracy_train_last": 0.983470354893411, "batch_size_eval": 1024, "accuracy_train_std": [0.01740078765776522, 0.017835577516983118, 0.01693172725218704, 0.016357773893280938, 0.01551569308163769, 0.013579561506448918, 0.01491910614474181, 0.013869637385390342, 0.012721978897615427, 0.011221105431955848, 0.011621917509238505, 0.011381829340622669, 0.010814940242666627, 0.011029516473791353, 0.009346223279739038, 0.010311356479095661, 0.01027486496385338, 0.0097020153969719, 0.008231617513290497, 0.009665754593169627, 0.009675015605205497, 0.008929264857101075, 0.009537899626024809, 0.008156125673003323, 0.00887422783018378, 0.008074255779263892, 0.00939859974407501, 0.008865135629434293, 0.007781492412990108, 0.008515794143683994, 0.00809582224827545, 0.00806876196770473, 0.0074868628823309994, 0.007948609000283944, 0.0067816126459504375, 0.007511398404422633, 0.006700780652195356, 0.006719122355903717, 0.007188511705441811, 0.007162587387476387, 0.006749615882996321, 0.007328933021120065, 0.006629881658566903, 0.006156366173576123, 0.006562167920905007, 0.006267381796226967, 0.006285658853599481, 0.0069771084350844365, 0.0064025177596214195, 0.006105679176009663, 0.006064432224718413, 0.0068385057572784335, 0.005585355943981544, 0.006686016756203686, 0.0061453699976796936, 0.006223281881897966, 0.0064221328261513675, 0.005766371537862615, 0.0064535039444982975, 0.006350692317283099, 0.006495567939993793, 0.005850528898731619, 0.005849054027266738, 0.005669906034971308, 0.005973433384649585, 0.005577632827169255, 0.005551085382755562, 0.005760293922392996, 0.005244345107011141, 0.005394919488652201, 0.005297772387913443, 0.005575283585052103, 0.006082191625287057, 0.005763950862857244, 0.005012545344563871, 0.005399541835819686, 0.005259660400672294, 0.005408314316800824, 0.005138293744936283, 0.005265456137504969, 0.006042229508521895, 0.005890689548765649, 0.005431629196855372, 0.005784044531041899, 0.005172772553191832, 0.005427891325010112, 0.005966937501715535, 0.005247571795585859, 0.005898824859584335, 0.005403726780840375, 0.00502663512640602, 0.005313129906996427, 0.005125135694865688, 0.005038664825284753, 0.0055939110978584, 0.005098605087960178, 0.00514977699148607, 0.005440081492911557, 0.0053912144403785575, 0.005210490089711877, 0.005779334505461463, 0.00523926869902312, 0.005040099407875686, 0.0049546891450559915, 0.005425411510897488, 0.005582041332639218, 0.004684308487712809, 0.0050414749635404, 0.00527586696974119, 0.005195837962944055, 0.005295508100560418, 0.004811608156133686, 0.004580815700859466, 0.00475055451840971, 0.004851478822725732, 0.004840189409063694, 0.004479605660864298, 0.004679435070574271, 0.004860509719502125, 0.0048334539326001, 0.004815402072507146, 0.00415301042255868, 0.004620403934919973, 0.004683277886676855, 0.0046862416732156385, 0.004398688138630228, 0.00444780792889775, 0.004508166390768833, 0.004546172073593262, 0.0042599192690329435, 0.004037237451979858, 0.0045641104518199, 0.00426636443800994, 0.004525931322681957, 0.004358269298818854, 0.004000855185231472, 0.00434975966738691, 0.004530805465007309, 0.004461744413787246, 0.004768464954163755, 0.004317254424017222, 0.004092665786727063, 0.004005832476403808, 0.004694877943901107, 0.004076966226254819, 0.004492255988009999, 0.004190264315691142, 0.004639919001488879, 0.0043974644181507235, 0.004618941790002608, 0.004549309695225412, 0.0045047379925104445, 0.004167989990601459, 0.004188271018678737, 0.003749213998937063, 0.00419921268924987, 0.004240464507748361, 0.0044823670529784195, 0.004287337531159217, 0.004518231918442602, 0.0041087437536529514, 0.004390343249639656, 0.004017750713241967, 0.004245171256504805, 0.0037632852346023665], "accuracy_test_std": 0.011710426671993701, "error_valid": [0.38929840455572284, 0.27822030308734935, 0.22269743034638556, 0.19580960560993976, 0.1811199877635542, 0.17427375517695776, 0.1653420321912651, 0.16154755741716864, 0.15647060899849397, 0.14796539674322284, 0.14912580007530118, 0.15014354292168675, 0.1467858739646084, 0.14656232351280118, 0.14570783132530118, 0.14511806993599397, 0.14228986257530118, 0.14398855186370485, 0.14072353868599397, 0.14194424181099397, 0.1418324665850903, 0.14045880788780118, 0.1381188817771084, 0.14133389024849397, 0.13973668109939763, 0.13816006212349397, 0.13764089561370485, 0.13642019248870485, 0.13690847373870485, 0.13420233669051207, 0.13712172910391573, 0.13556570030120485, 0.13871893825301207, 0.1354333349021084, 0.13243158179593373, 0.13444647731551207, 0.13493475856551207, 0.13455825254141573, 0.13505682887801207, 0.13432440700301207, 0.13468032285391573, 0.13490387330572284, 0.13099762330572284, 0.1331845938441265, 0.13285956325301207, 0.13530096950301207, 0.1317403402673193, 0.1303769766566265, 0.13148590455572284, 0.1300107657191265, 0.13050934205572284, 0.1329404532191265, 0.13170945500753017, 0.13071230233433728, 0.13194330054593373, 0.1305196371423193, 0.13009165568524095, 0.13024461125753017, 0.13084466773343373, 0.1315976797816265, 0.13134324407003017, 0.12875917733433728, 0.12888124764683728, 0.12775172957454817, 0.13133294898343373, 0.13096673804593373, 0.12875917733433728, 0.12924745858433728, 0.1292268684111446, 0.12876947242093373, 0.12716196818524095, 0.1284944465361446, 0.12860622176204817, 0.12864740210843373, 0.12815912085843373, 0.12656191170933728, 0.12665309676204817, 0.12701930769954817, 0.1261751105986446, 0.1266633918486446, 0.12836208113704817, 0.12887095256024095, 0.12594126506024095, 0.12876947242093373, 0.12888124764683728, 0.12728403849774095, 0.12691782756024095, 0.12752817912274095, 0.12875917733433728, 0.12902390813253017, 0.12828119117093373, 0.12754876929593373, 0.1259309699736446, 0.12888124764683728, 0.12653102644954817, 0.12639866105045183, 0.12813853068524095, 0.1275178840361446, 0.12999017554593373, 0.12740610881024095, 0.1317403402673193, 0.1282503059111446, 0.12740610881024095, 0.12901361304593373, 0.12718255835843373, 0.12765024943524095, 0.12778261483433728, 0.1267854621611446, 0.12657220679593373, 0.12867828736822284, 0.12925775367093373, 0.12829148625753017, 0.12765024943524095, 0.12729433358433728, 0.12459849162274095, 0.12496470256024095, 0.12631777108433728, 0.12607363045933728, 0.12804734563253017, 0.13000047063253017, 0.12693841773343373, 0.1273958137236446, 0.12876947242093373, 0.1265927969691265, 0.12716196818524095, 0.12892242799322284, 0.1281282355986446, 0.12767083960843373, 0.12729433358433728, 0.12803705054593373, 0.12902390813253017, 0.12780320500753017, 0.1281797110316265, 0.1261751105986446, 0.12730462867093373, 0.12668398202183728, 0.12692812264683728, 0.12716196818524095, 0.1269075324736446, 0.12691782756024095, 0.12729433358433728, 0.12581919474774095, 0.1260530402861446, 0.1253206184111446, 0.1269075324736446, 0.12767083960843373, 0.1255647590361446, 0.12691782756024095, 0.1287900625941265, 0.13011224585843373, 0.12757965455572284, 0.1286679922816265, 0.1284238516566265, 0.1278135000941265, 0.12741640389683728, 0.12618540568524095, 0.12657220679593373, 0.12608392554593373, 0.12680605233433728, 0.12630747599774095, 0.12741640389683728, 0.12839296639683728, 0.12630747599774095, 0.12865769719503017, 0.12717226327183728], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.038471583875255705, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 16, "valid_ratio": 0.15, "learning_rate": 0.001023960718063729, "optimization": "adam", "nb_data_augmentation": 2, "learning_rate_decay_method": "lin", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 9.701703513876036e-06, "rotation_range": [0, 0], "momentum": 0.5127891881701852}, "accuracy_valid_max": 0.875401508377259, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8728277367281627, "accuracy_valid_std": [0.014211040746941045, 0.015617403090373252, 0.020446941408058777, 0.013935759902926958, 0.012643908490380273, 0.014127248079570675, 0.013171570113673123, 0.014365599953486732, 0.007944594227539707, 0.008096366007269928, 0.01006110851757028, 0.014477102905797954, 0.00829844805880495, 0.012249057747126162, 0.010441768165720297, 0.014149011048381184, 0.012554071740966817, 0.013464475391983055, 0.015076027992089085, 0.015890161433210582, 0.016530289227349317, 0.01538707312830721, 0.013356034056636148, 0.016353720440416535, 0.017319828622148786, 0.016488916253500078, 0.013918127749633174, 0.013016971429436617, 0.014582470624097777, 0.011603492270891943, 0.01013680250565416, 0.014377265341480299, 0.011616628506368213, 0.012643443162065563, 0.008764284985702583, 0.012397239172685355, 0.012732220484065228, 0.011876259633277231, 0.012412341540553958, 0.013081379384161942, 0.012140813897908753, 0.011050178062646998, 0.012016894509177655, 0.010489512758909064, 0.012930864293298776, 0.011914004344717493, 0.011916520328759455, 0.00929915983565017, 0.01213961231102406, 0.009669391363512669, 0.01209171996903139, 0.009056410292598481, 0.008844121537076581, 0.009061612772945558, 0.00928524973385839, 0.012461954879889495, 0.011240987339413222, 0.009063082588680967, 0.008023911017842509, 0.009371815081071391, 0.009797568039555312, 0.008802568238976556, 0.008632843136208993, 0.008428048478490251, 0.008416341973981493, 0.00868667378405191, 0.009480663147434364, 0.008164654224964344, 0.009118776165422037, 0.008586311188997067, 0.008487164306922867, 0.0072174010242321614, 0.008136044976394178, 0.010001084876161477, 0.009623739407477285, 0.01122022793113892, 0.009676439016786971, 0.007102544384409899, 0.008671776406437374, 0.007301350609128364, 0.007497597168887315, 0.009275801084080175, 0.010863708988832762, 0.008202892063080804, 0.00895812628359459, 0.009320259164829463, 0.008615108516518387, 0.010668915850079216, 0.00770489426209861, 0.008456894661486448, 0.009576584985863074, 0.009258994095818932, 0.007744563231697435, 0.009500610866417687, 0.008398686142461295, 0.009232054642430923, 0.0075076808712166085, 0.007660127274164134, 0.009087471247468648, 0.007266147038717404, 0.011310913339723947, 0.008277650483490818, 0.009372535618294089, 0.009177272939019961, 0.009396600482578309, 0.008980506498877246, 0.01071765327857565, 0.00919125561398628, 0.010758298541274752, 0.011159217858268518, 0.008510452409492915, 0.010023994289806088, 0.008361857503976384, 0.009266920636242284, 0.00916815361299044, 0.009131294470790537, 0.008976735298378688, 0.00864455396471758, 0.011080182502256461, 0.008569424338150968, 0.010891633648693921, 0.007995406729487984, 0.007906899937176567, 0.010493209031099548, 0.009162580800614183, 0.010681265419502144, 0.009045170174466701, 0.008448830929660391, 0.009997125299074995, 0.009619681062062842, 0.009404615166591822, 0.009002501794267296, 0.011268175543026945, 0.008982390180936243, 0.008422579087168603, 0.009080899406819332, 0.008571734996833657, 0.008402466691281564, 0.008989422892190227, 0.007616274113533558, 0.0091894127453175, 0.010212129356579282, 0.009405610542600753, 0.010175377316087753, 0.011261638855035056, 0.010578936738881262, 0.008073114285401955, 0.008233030615493025, 0.010406413590189291, 0.009991314619012988, 0.011725314380081887, 0.010659014711162595, 0.009664962107888702, 0.010406965641438492, 0.00864240418587103, 0.009478629177960985, 0.010420577245534317, 0.01035677388862395, 0.00906398994603527, 0.00896442909235251, 0.00880637116303758, 0.00822115718496703, 0.009680518032703806, 0.010257063965757509, 0.009568022596291328], "accuracy_valid": [0.6107015954442772, 0.7217796969126506, 0.7773025696536144, 0.8041903943900602, 0.8188800122364458, 0.8257262448230422, 0.8346579678087349, 0.8384524425828314, 0.843529391001506, 0.8520346032567772, 0.8508741999246988, 0.8498564570783133, 0.8532141260353916, 0.8534376764871988, 0.8542921686746988, 0.854881930064006, 0.8577101374246988, 0.8560114481362951, 0.859276461314006, 0.858055758189006, 0.8581675334149097, 0.8595411921121988, 0.8618811182228916, 0.858666109751506, 0.8602633189006024, 0.861839937876506, 0.8623591043862951, 0.8635798075112951, 0.8630915262612951, 0.8657976633094879, 0.8628782708960843, 0.8644342996987951, 0.8612810617469879, 0.8645666650978916, 0.8675684182040663, 0.8655535226844879, 0.8650652414344879, 0.8654417474585843, 0.8649431711219879, 0.8656755929969879, 0.8653196771460843, 0.8650961266942772, 0.8690023766942772, 0.8668154061558735, 0.8671404367469879, 0.8646990304969879, 0.8682596597326807, 0.8696230233433735, 0.8685140954442772, 0.8699892342808735, 0.8694906579442772, 0.8670595467808735, 0.8682905449924698, 0.8692876976656627, 0.8680566994540663, 0.8694803628576807, 0.869908344314759, 0.8697553887424698, 0.8691553322665663, 0.8684023202183735, 0.8686567559299698, 0.8712408226656627, 0.8711187523531627, 0.8722482704254518, 0.8686670510165663, 0.8690332619540663, 0.8712408226656627, 0.8707525414156627, 0.8707731315888554, 0.8712305275790663, 0.872838031814759, 0.8715055534638554, 0.8713937782379518, 0.8713525978915663, 0.8718408791415663, 0.8734380882906627, 0.8733469032379518, 0.8729806923004518, 0.8738248894013554, 0.8733366081513554, 0.8716379188629518, 0.871129047439759, 0.874058734939759, 0.8712305275790663, 0.8711187523531627, 0.872715961502259, 0.873082172439759, 0.872471820877259, 0.8712408226656627, 0.8709760918674698, 0.8717188088290663, 0.8724512307040663, 0.8740690300263554, 0.8711187523531627, 0.8734689735504518, 0.8736013389495482, 0.871861469314759, 0.8724821159638554, 0.8700098244540663, 0.872593891189759, 0.8682596597326807, 0.8717496940888554, 0.872593891189759, 0.8709863869540663, 0.8728174416415663, 0.872349750564759, 0.8722173851656627, 0.8732145378388554, 0.8734277932040663, 0.8713217126317772, 0.8707422463290663, 0.8717085137424698, 0.872349750564759, 0.8727056664156627, 0.875401508377259, 0.875035297439759, 0.8736822289156627, 0.8739263695406627, 0.8719526543674698, 0.8699995293674698, 0.8730615822665663, 0.8726041862763554, 0.8712305275790663, 0.8734072030308735, 0.872838031814759, 0.8710775720067772, 0.8718717644013554, 0.8723291603915663, 0.8727056664156627, 0.8719629494540663, 0.8709760918674698, 0.8721967949924698, 0.8718202889683735, 0.8738248894013554, 0.8726953713290663, 0.8733160179781627, 0.8730718773531627, 0.872838031814759, 0.8730924675263554, 0.873082172439759, 0.8727056664156627, 0.874180805252259, 0.8739469597138554, 0.8746793815888554, 0.8730924675263554, 0.8723291603915663, 0.8744352409638554, 0.873082172439759, 0.8712099374058735, 0.8698877541415663, 0.8724203454442772, 0.8713320077183735, 0.8715761483433735, 0.8721864999058735, 0.8725835961031627, 0.873814594314759, 0.8734277932040663, 0.8739160744540663, 0.8731939476656627, 0.873692524002259, 0.8725835961031627, 0.8716070336031627, 0.873692524002259, 0.8713423028049698, 0.8728277367281627], "seed": 77494558, "model": "residualv3", "loss_std": [0.34621357917785645, 0.268285870552063, 0.25448885560035706, 0.24489249289035797, 0.23688636720180511, 0.22928932309150696, 0.2221670150756836, 0.215917706489563, 0.21155914664268494, 0.20558613538742065, 0.1997334063053131, 0.19572222232818604, 0.1940300166606903, 0.18912677466869354, 0.18712151050567627, 0.1843314915895462, 0.18033967912197113, 0.1779681295156479, 0.17450450360774994, 0.17300045490264893, 0.17056462168693542, 0.16499128937721252, 0.16613484919071198, 0.16459867358207703, 0.16043975949287415, 0.1592588871717453, 0.15573018789291382, 0.1539054811000824, 0.15287095308303833, 0.15547314286231995, 0.1506434977054596, 0.14830456674098969, 0.14728960394859314, 0.14483439922332764, 0.14369098842144012, 0.145588219165802, 0.13907800614833832, 0.13909627497196198, 0.13852311670780182, 0.13764092326164246, 0.13667583465576172, 0.1366463005542755, 0.1359817385673523, 0.1332397609949112, 0.12998269498348236, 0.13236643373966217, 0.13156746327877045, 0.12839742004871368, 0.12835006415843964, 0.12587061524391174, 0.12648241221904755, 0.12746436893939972, 0.1271912306547165, 0.12482879310846329, 0.1254531741142273, 0.1211060956120491, 0.12169140577316284, 0.12048693001270294, 0.12027201801538467, 0.1191108450293541, 0.11790445446968079, 0.11726357042789459, 0.1180351972579956, 0.11848025768995285, 0.11615777760744095, 0.11556193232536316, 0.11350864917039871, 0.11389920115470886, 0.11463148891925812, 0.11437773704528809, 0.11276887357234955, 0.11150805652141571, 0.11314481496810913, 0.10835830867290497, 0.10987933725118637, 0.10996748507022858, 0.10844103246927261, 0.10994300991296768, 0.10821572691202164, 0.10757303237915039, 0.1091102585196495, 0.10877688974142075, 0.10498878359794617, 0.10664112865924835, 0.10706642270088196, 0.10530444234609604, 0.10565539449453354, 0.10552610456943512, 0.10389381647109985, 0.10479001700878143, 0.10385455936193466, 0.10288770496845245, 0.10462380200624466, 0.1023583859205246, 0.10025791078805923, 0.10055116564035416, 0.10071269422769547, 0.0994400754570961, 0.09911864995956421, 0.09935496747493744, 0.10148360580205917, 0.09939803928136826, 0.0995413139462471, 0.0981292575597763, 0.09923809766769409, 0.0987614244222641, 0.09586776793003082, 0.09748566895723343, 0.09729321300983429, 0.09812413156032562, 0.09583337604999542, 0.09530749917030334, 0.09750773757696152, 0.09402871131896973, 0.09726204723119736, 0.09320563077926636, 0.09280624240636826, 0.09468981623649597, 0.0938045009970665, 0.09266085177659988, 0.09406895935535431, 0.09385758638381958, 0.09436264634132385, 0.09065254777669907, 0.09329403936862946, 0.09357873350381851, 0.0923597440123558, 0.09367683529853821, 0.09123462438583374, 0.09078817069530487, 0.09176921099424362, 0.0924510657787323, 0.08946606516838074, 0.09033868461847305, 0.09022389352321625, 0.09094826877117157, 0.0900859460234642, 0.09057991951704025, 0.08900368213653564, 0.08829280734062195, 0.09008540213108063, 0.08926214277744293, 0.08911845833063126, 0.0889299064874649, 0.0870542898774147, 0.09051334857940674, 0.09109357744455338, 0.08620806783437729, 0.08514958620071411, 0.08726843446493149, 0.08657518774271011, 0.08614469319581985, 0.08613105118274689, 0.08500538021326065, 0.08558245003223419, 0.08459442108869553, 0.08469485491514206, 0.08512561768293381, 0.08158879727125168, 0.08541053533554077, 0.0837990865111351, 0.08311659097671509, 0.08558575809001923, 0.08280031383037567, 0.08554784208536148]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:40 2016", "state": "available"}], "summary": "901a654eee8b3ffdf3d69882788f024e"}