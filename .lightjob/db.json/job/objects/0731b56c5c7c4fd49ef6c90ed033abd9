{"content": {"hp_model": {"f0": 64, "f1": 32, "f2": 64, "f3": 64, "nonlin": "leaky_rectify", "nbg1": 3, "nbg3": 5, "nbg2": 7, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.019001046138872022, 0.017538399963842332, 0.013095725424983133, 0.01048062724419523, 0.00751821527632909, 0.016091663269229253, 0.0147979981306667, 0.013583832676557505, 0.012714504674569402, 0.010371352384667718, 0.010840265564013409, 0.00783000161820763, 0.014573873848428629, 0.012922794923153624, 0.014629553660587168, 0.016886841650026203, 0.015525875492061931, 0.013748795114073445, 0.01402362283873963, 0.01612726587816035, 0.019716270614689658, 0.012205036480436282, 0.010571507502376213, 0.013345308201181185, 0.01361089806800051, 0.015104677455682803, 0.014010060303666937, 0.013467839014840626, 0.013542157720294464, 0.013448949881996245, 0.009094718214013893, 0.017123292892850566, 0.014498987312718722, 0.012912623246654716, 0.013693051239693941, 0.010903236904532114, 0.015119348149412681, 0.009977789453421372, 0.008827659633449128, 0.009591162317930883, 0.011967667724700416, 0.01015583337217876, 0.00953488856220938, 0.013439157287933937, 0.019103470711568243, 0.006284040341071909, 0.011623535283945729, 0.008886974590055444, 0.010225726168440741, 0.01387894932037169, 0.013598346938611776, 0.012934570538058639, 0.012240908059240082, 0.013501999643086965, 0.013329169562202108, 0.0078286777169263, 0.013437462454488295, 0.01081394097479031, 0.012393638834102922, 0.012871756679296404, 0.017476443737696135, 0.011620633295627722, 0.013045581139773886, 0.013734289045850471, 0.01382116821250355, 0.015873209272863644, 0.014218389513448242, 0.007493723812722658, 0.012301194242792232, 0.01324377227478818, 0.014761074385960306, 0.011469323192929232, 0.013098804432595512, 0.010057540342138209, 0.014426819421798357, 0.011368558375956801, 0.015103539807275914, 0.012082935632491144, 0.010465129430413042, 0.012212362975913858, 0.0103392012830369, 0.016019124227583954, 0.012856265353207821, 0.014731272470988788, 0.014819805963554518, 0.016392330318381214, 0.01800601215638006, 0.012375207539048862, 0.010764553331682985, 0.01330736592920334, 0.012872392210545489, 0.022648190204828823, 0.014660972886778013, 0.010960922217378908, 0.010865094583326742, 0.013422280383172242, 0.013015061076054752, 0.018205110038038843, 0.010005274211830948, 0.016674077153697807, 0.010468612093012215, 0.0116905632592979, 0.016470184932552392, 0.010280138098360697, 0.013382148798994044, 0.011786513170458132, 0.012960857412957405, 0.011934278221776187, 0.013229009935135764, 0.010651153844545444, 0.014044131333145948, 0.011565509311939686, 0.012943271302259394, 0.014552591601658008, 0.016040689075395975, 0.014598148009853816, 0.013494598480669742, 0.013330088026552068, 0.011427789536960541, 0.014625665398898705, 0.013660029933858644, 0.014485324072447089, 0.01355065568040193, 0.009747246214957303, 0.013241036872979632, 0.012458753207537502, 0.012876974097461885, 0.021000924411857153, 0.016800555428665393, 0.017994964625002847, 0.012152824211430489, 0.013884622977832092, 0.012925093620026515, 0.010322354630141617, 0.013815298776907611, 0.008792231767839819, 0.014073534230062574, 0.013365811651823956, 0.016569398871369694, 0.01346763600534201, 0.012601599929442934, 0.015307806800157626, 0.008949975323642289, 0.011237497378981364, 0.011862818761914765, 0.011590002326115097, 0.013990506150432881, 0.011769896384764012, 0.016346201785652364, 0.013897306696430183, 0.016247854053715878, 0.013489227807600068, 0.011599101517176493, 0.017056584037810373, 0.013452083511577532, 0.01573684223316704, 0.012846207862843718, 0.014907134487514885, 0.011761386911979175, 0.015125178821170128, 0.014096477614993693, 0.016030224308205793, 0.016388914195901147, 0.013575070589771697, 0.014151290008985585, 0.01244554445714612, 0.014483734558266374, 0.012012556724511307, 0.006864495000953021, 0.009283877066899659, 0.014262341174076114, 0.011411457122639634, 0.015476634594911292, 0.01724193924437687, 0.01363450828181281, 0.013995144301740623, 0.010732316370155235, 0.013980787240634755, 0.01405990184230688, 0.009844911864380456, 0.01563336909215721, 0.013329857361642099, 0.009285660306364024, 0.01098360251268534, 0.015300986640633323, 0.016181426270017004, 0.01526220749887345, 0.01575599097707866, 0.015411389475261758, 0.012736168721958287, 0.014478530531211767, 0.01138054017357277, 0.020279501888899952, 0.011193153902817755, 0.01319447557315492, 0.013020185518238045, 0.010812863997337887, 0.01282127496415415, 0.012890708224791473], "moving_avg_accuracy_train": [0.06070220936000369, 0.12590630695713362, 0.1891258914872877, 0.24890852255358292, 0.30541191964646197, 0.3585547078751528, 0.4079080462653784, 0.45360713571299155, 0.49535469765272044, 0.5332806376269741, 0.5685929061478684, 0.600901360059429, 0.6306837410108503, 0.6577619990849222, 0.6831648695206529, 0.7056762472961162, 0.726682607720159, 0.7448399225756809, 0.7625973052777381, 0.7797415241143515, 0.794731795844666, 0.8085554285352549, 0.8211362789829937, 0.8336169324442827, 0.8441822028511096, 0.8535400638863659, 0.8623966172061088, 0.8717393169426778, 0.8801058579781996, 0.8879147627673121, 0.8945848123073029, 0.9007295828242193, 0.906392409771587, 0.9115099164123223, 0.9171363766675463, 0.9217351250865244, 0.9255113114981377, 0.9299119823566757, 0.9336494078924643, 0.9377059491710935, 0.941042941232574, 0.9437299778009925, 0.9464971551316352, 0.9489131378696715, 0.9513153869172374, 0.9535216970338839, 0.9558537151650286, 0.9574759480747439, 0.9592080361530206, 0.9611017008032224, 0.9625222587360047, 0.9640449735981462, 0.9654665702478831, 0.9669180321957322, 0.9682104331047582, 0.9695641479788062, 0.9709662141702204, 0.9721559941293981, 0.9728082693069438, 0.9736348793917532, 0.9743741781704627, 0.9753510088653211, 0.9764161683954558, 0.977237628192815, 0.9781491111687992, 0.9786833804459761, 0.9796897064263878, 0.9804745281194818, 0.9809181258277901, 0.9812847756331156, 0.9816404091924507, 0.9820255114648815, 0.9822977347969832, 0.9824916185708841, 0.9830938692507096, 0.9835451780101625, 0.9836863249782031, 0.9841087592947054, 0.984298215879539, 0.9847244210772994, 0.9853684224219504, 0.9857085333047554, 0.9861704541183367, 0.9867628941600837, 0.9872518763214563, 0.9876385178928913, 0.9874749625690875, 0.9876648733062264, 0.9881171359756038, 0.9884102400863768, 0.9887902912265486, 0.9890696663813023, 0.9890257740729524, 0.9891420920144944, 0.9894117555809021, 0.9890848273811544, 0.9890091920382955, 0.9893153970904184, 0.9896026073813766, 0.9899960273718288, 0.9903640202072741, 0.9905650054258416, 0.9904691633654003, 0.9908107689407741, 0.9909531283931345, 0.9909929322943157, 0.9911240148089318, 0.9912536148161338, 0.9913656405738154, 0.9915989972378717, 0.9917067116879033, 0.9905272200941406, 0.9909815665371075, 0.9912021412822063, 0.9914471615289857, 0.9914886432927538, 0.9918840497968117, 0.9920167374135682, 0.992064076655554, 0.9922903326804748, 0.9919080977005411, 0.991917544886467, 0.9918306441549724, 0.9920639673882847, 0.9922228050244563, 0.9922565129517817, 0.9924239978173179, 0.9924377306606322, 0.9925499274231496, 0.9927764625451295, 0.9928780015584737, 0.9930694401669306, 0.9926929637466753, 0.9929284106755791, 0.9929706130973162, 0.9932294483649655, 0.9933415284165734, 0.9935516464082493, 0.9934571204948146, 0.9935347715405712, 0.9936465822579612, 0.9934774585928886, 0.9935461003824093, 0.993566097412044, 0.993688690386325, 0.9938222395024544, 0.99409356837959, 0.9942029057380596, 0.9944966218606821, 0.9943122126508044, 0.9943880598381049, 0.9942354692185893, 0.9943213158979208, 0.9944544175295665, 0.9947415836635146, 0.9948977266364489, 0.9947686101478225, 0.9950615594008974, 0.9951252509310458, 0.9952453883748551, 0.994955911627855, 0.9951394499293552, 0.995021038343581, 0.995084131681842, 0.9951944301577146, 0.9952192940240953, 0.9952393103062095, 0.9952363986208266, 0.9954011888182677, 0.9955146227638219, 0.9956353505541156, 0.9957439695165612, 0.9956952422077622, 0.9956444121834145, 0.9955405364412635, 0.9957051758400035, 0.9955627797953165, 0.9955275211610229, 0.9954818735461204, 0.9955825887212703, 0.9956476557420004, 0.9958317740963718, 0.9959719039784013, 0.9959817634317516, 0.9957907101909667, 0.9957489345587747, 0.9959299365267159, 0.99606490046333, 0.9961212638396161, 0.9960348431473304, 0.9961151385945022, 0.9960409201219567, 0.9960415528121419, 0.9959932941083087, 0.9959382355308112, 0.9960560935253491, 0.9962109938454332, 0.9962294963954137, 0.9963507803868248], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 402973569, "moving_var_accuracy_train": [0.03316282399067148, 0.06811071068270876, 0.09727008242792559, 0.11970874098001193, 0.1364715718293309, 0.14824181811287312, 0.15533940439392685, 0.1586011249416027, 0.15842674279864594, 0.15552946082515753, 0.15119912151546747, 0.1454737351113196, 0.138909273536408, 0.13161743472570153, 0.12426344369050209, 0.11639795848559849, 0.1087295672414218, 0.10082380326214263, 0.09357934469977426, 0.08686672838545605, 0.08020242976584846, 0.07390202217614245, 0.06793632014042353, 0.06254458852376825, 0.057294754120315754, 0.0523534047766807, 0.04782401112936167, 0.04382718436173466, 0.040074457005652826, 0.03661582225113636, 0.033354646073816115, 0.030359005308784593, 0.02761171325922863, 0.025086241801267506, 0.022862531116173295, 0.02076661438774545, 0.018818289203308185, 0.01711075341902404, 0.015525393223841725, 0.014120953645764553, 0.012809077925353556, 0.011593151622498365, 0.010502751893661535, 0.009505009457609782, 0.008606445716227585, 0.007789611383582177, 0.007059595022299846, 0.006377320276590132, 0.0057665894109292975, 0.0052222041621031836, 0.00471814560945638, 0.004267198993473222, 0.003858667527436788, 0.0034917614507675968, 0.0031576180066776958, 0.0028583491016518953, 0.0025902062979326697, 0.0023439258553007497, 0.002113362435935854, 0.0019081757504330478, 0.0017222772395475553, 0.0015586372994505595, 0.0014129846529272315, 0.001277759353422605, 0.001157460629019927, 0.0010442835590627505, 0.0009489694309661381, 0.0008596159936790823, 0.000775425404652522, 0.0006990927529049767, 0.0006303217546712077, 0.0005686243130461694, 0.0005124288316244177, 0.00046152426672201404, 0.00041863619298196614, 0.00037860569005099937, 0.0003409244232451829, 0.00030843803768649375, 0.0002779172780956758, 0.00025176040812148996, 0.00023031700689655178, 0.0002083263849203179, 0.00018941408397046264, 0.00017363154240100296, 0.0001584203201481681, 0.00014392371347620708, 0.0001297720952240872, 0.0001171194804944041, 0.00010724840614397468, 9.729675570734525e-05, 8.8867029958924e-05, 8.06827812568749e-05, 7.263184194377794e-05, 6.549042652112118e-05, 5.9595849820438535e-05, 5.45982032685068e-05, 4.9189869287460556e-05, 4.511473616422441e-05, 4.1345670308892395e-05, 3.8604116877990046e-05, 3.5962473732643356e-05, 3.2729781882122765e-05, 2.9539474998857185e-05, 2.7635776821110735e-05, 2.50545950620868e-05, 2.25633947108213e-05, 2.0461698870481762e-05, 1.8566694440234546e-05, 1.6822972929668233e-05, 1.5630773630636594e-05, 1.4172117892283395e-05, 2.5275709880865622e-05, 2.4606015104909085e-05, 2.2583292557996682e-05, 2.0865277594183416e-05, 1.8794236465292883e-05, 1.8321929549825658e-05, 1.66481906276081e-05, 1.5003540599333355e-05, 1.3963912638716615e-05, 1.3882453593809784e-05, 1.2495011478326083e-05, 1.1313475964702157e-05, 1.0672085949061625e-05, 9.831941906136488e-06, 8.858973734804077e-06, 8.225536982976515e-06, 7.404680603548333e-06, 6.7775055648680506e-06, 6.561618461795607e-06, 5.9982481566942445e-06, 5.728262008295722e-06, 6.431046262540047e-06, 6.286858943258571e-06, 5.674202448536941e-06, 5.70974346569518e-06, 5.251826560841573e-06, 5.123990038591016e-06, 4.692007369527982e-06, 4.277073796739071e-06, 3.9618811457743616e-06, 3.823118357985231e-06, 3.4832117796040954e-06, 3.138489532391572e-06, 2.959901915240001e-06, 2.8244300214865115e-06, 3.2045612554464143e-06, 2.9916970515157047e-06, 3.4689497925603086e-06, 3.428115623493851e-06, 3.137079223537064e-06, 3.0329263756606013e-06, 2.7959606092648233e-06, 2.675808947459108e-06, 3.1504075490935596e-06, 3.0547924461549006e-06, 2.899352810256391e-06, 3.3817909131249902e-06, 3.080121320926241e-06, 2.902006237479181e-06, 3.365976697215078e-06, 3.3325558005518033e-06, 3.125491953306677e-06, 2.8487696819722e-06, 2.6733844977934485e-06, 2.4116099546766474e-06, 2.1740548231560927e-06, 1.956725642046402e-06, 2.005455360396029e-06, 1.9207151643923377e-06, 1.8598204420959146e-06, 1.7800211089111142e-06, 1.623388153625121e-06, 1.4843025606392489e-06, 1.4329838328420593e-06, 1.5336406341151285e-06, 1.562766272585978e-06, 1.4176781869576009e-06, 1.2946637109784977e-06, 1.2564892584298654e-06, 1.1689437872671132e-06, 1.3571455242882545e-06, 1.3981584263977982e-06, 1.2592174631413277e-06, 1.4618077841571843e-06, 1.3313338367467572e-06, 1.4930558646596529e-06, 1.5076876558711151e-06, 1.385510361961266e-06, 1.3141761502614218e-06, 1.2407847647638459e-06, 1.166281723290275e-06, 1.0496571536330828e-06, 9.656515607307541e-07, 8.963694272621471e-07, 9.317470464244584e-07, 1.0545193242415315e-06, 9.521484910194041e-07, 9.89321901070737e-07], "duration": 268440.397784, "accuracy_train": [0.607022093600037, 0.7127431853313031, 0.7581021522586747, 0.78695220215024, 0.8139424934823736, 0.8368398019333703, 0.8520880917774087, 0.8648989407415099, 0.8710827551102805, 0.8746140973952565, 0.8864033228359173, 0.8916774452634736, 0.8987251695736435, 0.9014663217515688, 0.9117907034422297, 0.9082786472752861, 0.9157398515365448, 0.9082557562753784, 0.9224137495962532, 0.9340394936438722, 0.9296442414174971, 0.9329681227505537, 0.9343639330126431, 0.9459428135958842, 0.9392696365125508, 0.9377608132036729, 0.9421055970837948, 0.9558236145717978, 0.9554047272978959, 0.9581949058693245, 0.9546152581672205, 0.9560325174764673, 0.9573578522978959, 0.9575674761789406, 0.9677745189645626, 0.9631238608573275, 0.9594969892026578, 0.969518020083518, 0.9672862377145626, 0.974214820678756, 0.9710758697858989, 0.9679133069167589, 0.9714017511074198, 0.9706569825119971, 0.9729356283453304, 0.9733784880837025, 0.9768418783453304, 0.9720760442621816, 0.9747968288575121, 0.9781446826550388, 0.9753072801310447, 0.9777494073574198, 0.978260940095515, 0.979981189726375, 0.9798420412859912, 0.9817475818452381, 0.9835848098929494, 0.9828640137619971, 0.9786787459048542, 0.9810743701550388, 0.9810278671788483, 0.9841424851190477, 0.9860026041666666, 0.9846307663690477, 0.9863524579526578, 0.9834918039405685, 0.9887466402500923, 0.9875379233573275, 0.9849105052025655, 0.9845846238810447, 0.9848411112264673, 0.9854914319167589, 0.9847477447858989, 0.9842365725359912, 0.98851412536914, 0.9876069568452381, 0.9849566476905685, 0.9879106681432264, 0.9860033251430418, 0.9885602678571429, 0.9911644345238095, 0.98876953125, 0.9903277414405685, 0.9920948545358066, 0.9916527157738095, 0.9911182920358066, 0.9860029646548542, 0.9893740699404762, 0.9921875, 0.9910481770833334, 0.9922107514880952, 0.9915840427740864, 0.9886307432978036, 0.9901889534883721, 0.9918387276785714, 0.9861424735834257, 0.9883284739525655, 0.9920712425595238, 0.9921875, 0.9935368072858989, 0.9936759557262828, 0.9923738723929494, 0.9896065848214286, 0.99388521911914, 0.992234363464378, 0.9913511674049464, 0.9923037574404762, 0.9924200148809523, 0.9923738723929494, 0.993699207214378, 0.9926761417381875, 0.9799117957502769, 0.9950706845238095, 0.9931873139880952, 0.99365234375, 0.9918619791666666, 0.9954427083333334, 0.993210925964378, 0.9924901298334257, 0.9943266369047619, 0.988467982881137, 0.9920025695598007, 0.9910485375715209, 0.9941638764880952, 0.99365234375, 0.9925598842977114, 0.9939313616071429, 0.9925613262504615, 0.9935596982858066, 0.9948152786429494, 0.9937918526785714, 0.9947923876430418, 0.989304675964378, 0.9950474330357143, 0.9933504348929494, 0.9955589657738095, 0.9943502488810447, 0.9954427083333334, 0.9926063872739018, 0.9942336309523809, 0.9946528787144703, 0.9919553456072352, 0.9941638764880952, 0.993746070678756, 0.9947920271548542, 0.9950241815476191, 0.9965355282738095, 0.9951869419642857, 0.9971400669642857, 0.9926525297619048, 0.9950706845238095, 0.9928621536429494, 0.9950939360119048, 0.995652332214378, 0.9973260788690477, 0.9963030133928571, 0.9936065617501846, 0.9976981026785714, 0.9956984747023809, 0.99632662536914, 0.9923506209048542, 0.9967912946428571, 0.9939553340716132, 0.9956519717261905, 0.9961871164405685, 0.9954430688215209, 0.9954194568452381, 0.9952101934523809, 0.9968843005952381, 0.9965355282738095, 0.9967219006667589, 0.9967215401785714, 0.9952566964285714, 0.9951869419642857, 0.9946056547619048, 0.9971869304286637, 0.9942812153931341, 0.9952101934523809, 0.9950710450119971, 0.9964890252976191, 0.9962332589285714, 0.9974888392857143, 0.9972330729166666, 0.9960704985119048, 0.9940712310239018, 0.9953729538690477, 0.9975589542381875, 0.9972795758928571, 0.9966285342261905, 0.9952570569167589, 0.9968377976190477, 0.9953729538690477, 0.9960472470238095, 0.9955589657738095, 0.9954427083333334, 0.9971168154761905, 0.9976050967261905, 0.9963960193452381, 0.9974423363095238], "end": "2016-01-28 05:29:13.372000", "learning_rate_per_epoch": [0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817, 0.0001620203984202817], "accuracy_valid": [0.6011698159826807, 0.6978142060429217, 0.7378341490963856, 0.7510589231927711, 0.7697562711784638, 0.7922289744917168, 0.800194430064006, 0.8072642131024097, 0.8103056758283133, 0.8130221079631024, 0.8219538309487951, 0.8214552546121988, 0.8229715737951807, 0.8269998941076807, 0.8315576760165663, 0.829747211502259, 0.8300825371799698, 0.8243143472326807, 0.8336122811558735, 0.8472650367093373, 0.8404290992093373, 0.8369184746799698, 0.8362875329442772, 0.8523713996611446, 0.843419086502259, 0.8457693076995482, 0.842808734939759, 0.8531038215361446, 0.8546907355986446, 0.8579866340361446, 0.8548834007906627, 0.8568174063441265, 0.8539891989834337, 0.857630718185241, 0.8643754706325302, 0.8610383918486446, 0.854945171310241, 0.8643342902861446, 0.8600103539156627, 0.8683523155120482, 0.8640092596950302, 0.8621267295745482, 0.8639474891754518, 0.8650975974209337, 0.8673669286521084, 0.8657991340361446, 0.8718114646084337, 0.8668668815888554, 0.8694921286709337, 0.8682714255459337, 0.8672845679593373, 0.8722188558923193, 0.8714761389307228, 0.8699907050075302, 0.8740396154932228, 0.8733983786709337, 0.8778443853539157, 0.8791768637048193, 0.8735513342432228, 0.8772237387048193, 0.8676919592432228, 0.8786988775414157, 0.8813844244164157, 0.8751588384789157, 0.8787900625941265, 0.8751691335655121, 0.8833787297628012, 0.8824521719691265, 0.8795224844691265, 0.8762677663780121, 0.8783532567771084, 0.8784032614834337, 0.8793195241905121, 0.8753206184111446, 0.8848435735128012, 0.8809064382530121, 0.8751691335655121, 0.8821271413780121, 0.8730218726468373, 0.8783120764307228, 0.8810182134789157, 0.8842332219503012, 0.8813741293298193, 0.8880276967243976, 0.8836022802146084, 0.8808152532003012, 0.8841523319841867, 0.8851788991905121, 0.8864098974021084, 0.8859216161521084, 0.8877629659262049, 0.8852921451430723, 0.8845891378012049, 0.8816079748682228, 0.8862878270896084, 0.8771016683923193, 0.8811711690512049, 0.8896352009600903, 0.8898278661521084, 0.8920869022966867, 0.8914250753012049, 0.8889527837914157, 0.8867055134600903, 0.8901940770896084, 0.8883939076618976, 0.8913133000753012, 0.8876511907003012, 0.8851994893637049, 0.892798733998494, 0.8884850927146084, 0.8915677357868976, 0.869471538497741, 0.8936223409262049, 0.890479398060994, 0.8906117634600903, 0.8902249623493976, 0.8981801228350903, 0.893164944935994, 0.8869893637048193, 0.8928090290850903, 0.8811402837914157, 0.889502835560994, 0.8847112081137049, 0.8921574971762049, 0.8942738728350903, 0.8922898625753012, 0.8890042592243976, 0.8946915592055723, 0.8898793415850903, 0.8951798404555723, 0.8902146672628012, 0.8940297322100903, 0.8839684911521084, 0.8941209172628012, 0.8935414509600903, 0.8906720632530121, 0.891333890248494, 0.893164944935994, 0.8907441288591867, 0.890113187123494, 0.8910588643637049, 0.8874173451618976, 0.8916280355798193, 0.8931546498493976, 0.8898381612387049, 0.8920560170368976, 0.8974477009600903, 0.8909573842243976, 0.9001641330948795, 0.887915921498494, 0.8929516895707832, 0.8897366810993976, 0.8896043157003012, 0.8903882130082832, 0.897681546498494, 0.8983124882341867, 0.8891057393637049, 0.8994317112198795, 0.8882409520896084, 0.8935414509600903, 0.889869046498494, 0.8988316547439759, 0.8960137424698795, 0.8947724491716867, 0.8978139118975903, 0.8931546498493976, 0.8962475880082832, 0.8949857045368976, 0.8944665380271084, 0.8964299581137049, 0.8970917851091867, 0.898658108998494, 0.890845608998494, 0.8907544239457832, 0.8896146107868976, 0.8939076618975903, 0.8934899755271084, 0.8935208607868976, 0.8951386601091867, 0.8959622670368976, 0.8988213596573795, 0.8979462772966867, 0.8959519719503012, 0.8943547628012049, 0.8890248493975903, 0.8945386036332832, 0.8982110080948795, 0.8969697147966867, 0.8950165897966867, 0.8878850362387049, 0.892920804310994, 0.8923001576618976, 0.8959225574171686, 0.8885968679405121, 0.8928090290850903, 0.891578030873494, 0.898413968373494, 0.8930531697100903, 0.8974477009600903], "accuracy_test": 0.6939971301020409, "start": "2016-01-25 02:55:12.974000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0], "accuracy_train_last": 0.9974423363095238, "batch_size_eval": 1024, "accuracy_train_std": [0.015264313293242503, 0.021893313774090206, 0.017792738995703893, 0.01570351367663621, 0.016847935030408295, 0.017618116304700563, 0.018089760796001754, 0.01727045045009967, 0.017345848668219664, 0.017700335819269587, 0.018152545440388863, 0.0198526184966174, 0.017388616813116965, 0.0179405699002554, 0.01715598256495195, 0.017634593567544297, 0.016883417222467376, 0.01644460843730544, 0.017122715857776735, 0.015477506151978864, 0.015953609773063766, 0.015299125079818763, 0.01501720740670022, 0.013624151983435806, 0.014251622148717107, 0.012352352306852029, 0.01480893787136928, 0.013148878766993177, 0.011850758303482097, 0.01086608245074481, 0.010810560541009435, 0.010742830515577164, 0.01116592652423238, 0.010585476868984357, 0.010115695501490747, 0.009113396285516674, 0.01035886266227304, 0.008533135305851355, 0.008589230957005815, 0.007811787060577699, 0.008668745013480524, 0.008781437745512146, 0.008872295790655087, 0.008844144077967147, 0.00782029844313182, 0.008014093433042756, 0.007340855631050309, 0.007227038577477879, 0.006419134668041472, 0.007044398505708857, 0.006211687828585295, 0.006524297543330048, 0.007153475285988017, 0.00723536159305483, 0.0067000726542914574, 0.006181791942356228, 0.006496920829475992, 0.005853340249491109, 0.006242963472113119, 0.0061747638611732245, 0.006317243772075384, 0.005086923984477112, 0.005291948789407107, 0.005606116529043386, 0.004529939592941022, 0.005867277976976566, 0.004348489111037926, 0.004630191673146391, 0.005269982931881179, 0.005296593212305861, 0.004806330508712245, 0.004677466326898824, 0.005643472084738165, 0.00492469133936649, 0.004882631081514922, 0.004753152433342768, 0.005080036033012619, 0.004625993232163735, 0.004840720496578, 0.004694497903235575, 0.003888494226258719, 0.004139091433913727, 0.003494715246799891, 0.0029674536067091305, 0.003753297239966563, 0.0038413013678088113, 0.004778736423600756, 0.004586440751007305, 0.003416307525722042, 0.004224162545844108, 0.0039032732289013988, 0.004841131873504957, 0.004127053753770496, 0.0038485183363953968, 0.0034613493140143335, 0.004776689071151249, 0.003872385269112477, 0.003424289840517009, 0.003992491077358868, 0.003073672697958009, 0.0033376893069163725, 0.003418488764506578, 0.004366391742920618, 0.003125622766662906, 0.0035342079711414972, 0.0025636482364072865, 0.003205080795536303, 0.0031305864359573725, 0.0038496190406834456, 0.003023302249602219, 0.0037359123745077496, 0.006116556581238594, 0.0028745460306037187, 0.003459161948516648, 0.0027723926236389237, 0.0033333260672353845, 0.002988517301536389, 0.003322153503201871, 0.0032148006157895473, 0.0032892433101313276, 0.003311552368432133, 0.0036931566705841225, 0.0033629034197849824, 0.0030098779666559126, 0.0028847779355110994, 0.0037764402199960937, 0.0033392407497082103, 0.0031591870121684884, 0.002738448326700963, 0.0028314661263970846, 0.0029820883305124767, 0.002709922014062313, 0.0047368976078220725, 0.0025607268251105846, 0.0034250218126681276, 0.002307058532664554, 0.002526821713240623, 0.0025895937113701367, 0.003169653369855468, 0.003493918109423705, 0.003023398460982622, 0.003341760127580263, 0.0026487313101351364, 0.0031832724878378864, 0.002500843301655042, 0.002651485359030586, 0.0019438299673109065, 0.0026650100609703293, 0.001762364635568936, 0.0031409309506637427, 0.0022947228034738334, 0.003235879164456124, 0.002114863126274022, 0.0019089179527836935, 0.0018089896826367121, 0.002250595117871328, 0.003030880756659075, 0.0018433312849217443, 0.002467398135368269, 0.0019278140160145776, 0.0030213165392840393, 0.0017218664975421892, 0.0032619894464768827, 0.002143807377113003, 0.0023905527020269567, 0.002427023554126321, 0.002507821474968664, 0.002920446143621202, 0.0016676358888617101, 0.0019084642395805808, 0.0019278803199082857, 0.0022534758808014097, 0.0026322491997925793, 0.002208401809784282, 0.0022068099921325655, 0.0019086281307269365, 0.0024720531801976347, 0.0025995953979489742, 0.0020104106902079795, 0.00206676826252445, 0.0020613988336403927, 0.0020120263813973157, 0.0017432414571918304, 0.0019471646316075461, 0.003269982035206755, 0.002627829653300804, 0.001495643909676625, 0.0020164550591307497, 0.0019670539055114743, 0.0030032904811586984, 0.0018797789184717018, 0.002627829653300804, 0.002098309927953586, 0.002336398566426191, 0.0024637801672153284, 0.0019291730387547936, 0.001680714319987091, 0.002146831439804151, 0.0018572103373991683], "accuracy_test_std": 0.01151357106964059, "error_valid": [0.3988301840173193, 0.30218579395707834, 0.26216585090361444, 0.24894107680722888, 0.2302437288215362, 0.2077710255082832, 0.19980556993599397, 0.1927357868975903, 0.18969432417168675, 0.18697789203689763, 0.17804616905120485, 0.17854474538780118, 0.1770284262048193, 0.1730001058923193, 0.16844232398343373, 0.17025278849774095, 0.16991746282003017, 0.1756856527673193, 0.1663877188441265, 0.15273496329066272, 0.15957090079066272, 0.16308152532003017, 0.16371246705572284, 0.1476286003388554, 0.15658091349774095, 0.15423069230045183, 0.15719126506024095, 0.1468961784638554, 0.1453092644013554, 0.1420133659638554, 0.14511659920933728, 0.1431825936558735, 0.14601080101656627, 0.14236928181475905, 0.13562452936746983, 0.1389616081513554, 0.14505482868975905, 0.1356657097138554, 0.13998964608433728, 0.13164768448795183, 0.13599074030496983, 0.13787327042545183, 0.13605251082454817, 0.13490240257906627, 0.1326330713478916, 0.1342008659638554, 0.12818853539156627, 0.1331331184111446, 0.13050787132906627, 0.13172857445406627, 0.13271543204066272, 0.1277811441076807, 0.12852386106927716, 0.13000929499246983, 0.12596038450677716, 0.12660162132906627, 0.12215561464608427, 0.12082313629518071, 0.12644866575677716, 0.12277626129518071, 0.13230804075677716, 0.12130112245858427, 0.11861557558358427, 0.12484116152108427, 0.12120993740587349, 0.12483086643448793, 0.11662127023719882, 0.11754782803087349, 0.12047751553087349, 0.12373223362198793, 0.1216467432228916, 0.12159673851656627, 0.12068047580948793, 0.12467938158885539, 0.11515642648719882, 0.11909356174698793, 0.12483086643448793, 0.11787285862198793, 0.12697812735316272, 0.12168792356927716, 0.11898178652108427, 0.11576677804969882, 0.11862587067018071, 0.11197230327560237, 0.1163977197853916, 0.11918474679969882, 0.11584766801581325, 0.11482110080948793, 0.1135901025978916, 0.1140783838478916, 0.11223703407379515, 0.1147078548569277, 0.11541086219879515, 0.11839202513177716, 0.1137121729103916, 0.12289833160768071, 0.11882883094879515, 0.1103647990399097, 0.1101721338478916, 0.10791309770331325, 0.10857492469879515, 0.11104721620858427, 0.1132944865399097, 0.1098059229103916, 0.11160609233810237, 0.10868669992469882, 0.11234880929969882, 0.11480051063629515, 0.10720126600150603, 0.1115149072853916, 0.10843226421310237, 0.13052846150225905, 0.10637765907379515, 0.10952060193900603, 0.1093882365399097, 0.10977503765060237, 0.1018198771649097, 0.10683505506400603, 0.11301063629518071, 0.1071909709149097, 0.11885971620858427, 0.11049716443900603, 0.11528879188629515, 0.10784250282379515, 0.1057261271649097, 0.10771013742469882, 0.11099574077560237, 0.1053084407944277, 0.1101206584149097, 0.1048201595444277, 0.10978533273719882, 0.1059702677899097, 0.1160315088478916, 0.10587908273719882, 0.1064585490399097, 0.10932793674698793, 0.10866610975150603, 0.10683505506400603, 0.10925587114081325, 0.10988681287650603, 0.10894113563629515, 0.11258265483810237, 0.10837196442018071, 0.10684535015060237, 0.11016183876129515, 0.10794398296310237, 0.1025522990399097, 0.10904261577560237, 0.09983586690512047, 0.11208407850150603, 0.10704831042921681, 0.11026331890060237, 0.11039568429969882, 0.10961178699171681, 0.10231845350150603, 0.10168751176581325, 0.11089426063629515, 0.10056828878012047, 0.1117590479103916, 0.1064585490399097, 0.11013095350150603, 0.10116834525602414, 0.10398625753012047, 0.10522755082831325, 0.1021860881024097, 0.10684535015060237, 0.10375241199171681, 0.10501429546310237, 0.1055334619728916, 0.10357004188629515, 0.10290821489081325, 0.10134189100150603, 0.10915439100150603, 0.10924557605421681, 0.11038538921310237, 0.1060923381024097, 0.1065100244728916, 0.10647913921310237, 0.10486133989081325, 0.10403773296310237, 0.10117864034262047, 0.10205372270331325, 0.10404802804969882, 0.10564523719879515, 0.1109751506024097, 0.10546139636671681, 0.10178899190512047, 0.10303028520331325, 0.10498341020331325, 0.11211496376129515, 0.10707919568900603, 0.10769984233810237, 0.10407744258283136, 0.11140313205948793, 0.1071909709149097, 0.10842196912650603, 0.10158603162650603, 0.1069468302899097, 0.1025522990399097], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "momentum": 0.5881189757498207, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.00016202040361768, "patience_threshold": 1, "do_flip": true, "batch_size": 32, "optimization": "adam", "nb_data_augmentation": 4, "learning_rate_decay_method": "none", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 1.0028300323700362e-10, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.03816016975820963}, "accuracy_valid_max": 0.9001641330948795, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8974477009600903, "loss_train": [1.5185770988464355, 1.0902209281921387, 0.883545458316803, 0.7538953423500061, 0.6622924208641052, 0.5932294130325317, 0.5393146276473999, 0.49371883273124695, 0.4523846209049225, 0.41684743762016296, 0.38459980487823486, 0.35810062289237976, 0.33063000440597534, 0.30853697657585144, 0.2867094874382019, 0.26772141456604004, 0.24708427488803864, 0.2339150309562683, 0.21910592913627625, 0.20544186234474182, 0.19224880635738373, 0.18137750029563904, 0.1693601906299591, 0.1602974534034729, 0.15458714962005615, 0.14162710309028625, 0.13433608412742615, 0.12911154329776764, 0.12362151592969894, 0.11807479709386826, 0.11085830628871918, 0.1075468584895134, 0.10134974867105484, 0.0980101078748703, 0.09251663088798523, 0.08754538744688034, 0.08450940251350403, 0.08466805517673492, 0.08173346519470215, 0.07788285613059998, 0.07394233345985413, 0.07261306047439575, 0.07195523381233215, 0.06831510365009308, 0.06286448985338211, 0.06475178152322769, 0.059914421290159225, 0.06224428117275238, 0.05748477205634117, 0.05744212865829468, 0.05720704793930054, 0.05592450127005577, 0.053489018231630325, 0.052568469196558, 0.04828304424881935, 0.05066986382007599, 0.04940544441342354, 0.045396436005830765, 0.046128060668706894, 0.04733851179480553, 0.043509047478437424, 0.04599953070282936, 0.04453868791460991, 0.041109349578619, 0.0431269109249115, 0.03950159624218941, 0.038906995207071304, 0.03894691541790962, 0.03859947621822357, 0.038341816514730453, 0.03738659992814064, 0.03625408932566643, 0.03547270968556404, 0.033963464200496674, 0.038998693227767944, 0.03132271021604538, 0.036331113427877426, 0.03340693563222885, 0.031075408682227135, 0.03351140394806862, 0.030094195157289505, 0.031528323888778687, 0.03198012337088585, 0.030317537486553192, 0.028737174347043037, 0.030306227505207062, 0.029777074232697487, 0.028148995712399483, 0.026559041813015938, 0.02818909101188183, 0.026115862652659416, 0.02746165171265602, 0.029119182378053665, 0.025133199989795685, 0.027077214792370796, 0.025402158498764038, 0.02804715186357498, 0.025170588865876198, 0.028026876971125603, 0.023606043308973312, 0.022942611947655678, 0.024748653173446655, 0.022569619119167328, 0.024959228932857513, 0.02072933129966259, 0.023461611941456795, 0.023574242368340492, 0.023600984364748, 0.02117697149515152, 0.023338915780186653, 0.02509348653256893, 0.018647652119398117, 0.024496594443917274, 0.02193346805870533, 0.018111499026417732, 0.021703118458390236, 0.020201729610562325, 0.02093069814145565, 0.019282735884189606, 0.021280381828546524, 0.01844174414873123, 0.020610099658370018, 0.01870111934840679, 0.018560973927378654, 0.0210737232118845, 0.018911033868789673, 0.01864820159971714, 0.017624011263251305, 0.017443707212805748, 0.01849621906876564, 0.018012454733252525, 0.01982739567756653, 0.017576102167367935, 0.01752506010234356, 0.01838594488799572, 0.015830840915441513, 0.017681242898106575, 0.01634029485285282, 0.016947263851761818, 0.01675700955092907, 0.017699114978313446, 0.01553830411285162, 0.017191356047987938, 0.015890490263700485, 0.01630523055791855, 0.017356490716338158, 0.015236187726259232, 0.0157626885920763, 0.01538796629756689, 0.017399799078702927, 0.01343908254057169, 0.019968654960393906, 0.012672803364694118, 0.014430577866733074, 0.01471956167370081, 0.014215230010449886, 0.016417622566223145, 0.014659643173217773, 0.01662079431116581, 0.012809530831873417, 0.012769385240972042, 0.016343142837285995, 0.015024378895759583, 0.011511716060340405, 0.015418422408401966, 0.014807933010160923, 0.011835573241114616, 0.014865188859403133, 0.014181138947606087, 0.011359365656971931, 0.013903479091823101, 0.012999511323869228, 0.012851224280893803, 0.014263136312365532, 0.010629452764987946, 0.014840962365269661, 0.011875038035213947, 0.013651453889906406, 0.013828620314598083, 0.011713488958775997, 0.014606817625463009, 0.00976341962814331, 0.01460818201303482, 0.011201019398868084, 0.011448521167039871, 0.012831774540245533, 0.012509451247751713, 0.01020741555839777, 0.013089107349514961, 0.010281517170369625, 0.011764134280383587, 0.013125614263117313, 0.012512861751019955, 0.011032693088054657, 0.011873327195644379, 0.011878160759806633, 0.008532597683370113, 0.013056894764304161, 0.011499637737870216], "accuracy_train_first": 0.607022093600037, "model": "residualv5", "loss_std": [0.2638384997844696, 0.18118296563625336, 0.16726751625537872, 0.16059613227844238, 0.1543479859828949, 0.14901338517665863, 0.14360879361629486, 0.13878941535949707, 0.1309402734041214, 0.12415938079357147, 0.12130521237850189, 0.11583304405212402, 0.11092253774404526, 0.10668418556451797, 0.10139621794223785, 0.09595076739788055, 0.09181378036737442, 0.08596277236938477, 0.08293820917606354, 0.07961322367191315, 0.07656755298376083, 0.072064109146595, 0.06971205025911331, 0.06900621205568314, 0.06430266797542572, 0.0613412968814373, 0.059373367577791214, 0.05968570336699486, 0.054970044642686844, 0.05331879481673241, 0.05270683020353317, 0.050853438675403595, 0.048739608377218246, 0.047013670206069946, 0.04544098302721977, 0.04380344599485397, 0.04293723776936531, 0.043830808252096176, 0.0420769639313221, 0.04200310632586479, 0.04100542888045311, 0.039516497403383255, 0.03935405984520912, 0.03918265551328659, 0.035899724811315536, 0.03827124834060669, 0.03528212383389473, 0.03675796464085579, 0.03532809391617775, 0.03514659032225609, 0.0344117172062397, 0.03451818600296974, 0.03372807800769806, 0.03420703113079071, 0.031179068610072136, 0.03325020521879196, 0.03378084674477577, 0.02926497533917427, 0.03307231143116951, 0.0317581444978714, 0.02997550182044506, 0.031106650829315186, 0.031689032912254333, 0.028187977150082588, 0.03090135008096695, 0.02827467955648899, 0.027652829885482788, 0.027664944529533386, 0.030104096978902817, 0.02944801189005375, 0.029675785452127457, 0.027504008263349533, 0.027599215507507324, 0.025252997875213623, 0.033250920474529266, 0.02364177070558071, 0.02912442944943905, 0.027852054685354233, 0.024725666269659996, 0.02656661719083786, 0.024891581386327744, 0.026669342070817947, 0.026315070688724518, 0.025177249684929848, 0.02437247708439827, 0.02618684247136116, 0.025392567738890648, 0.02428646758198738, 0.022562803700566292, 0.02338867075741291, 0.023593192920088768, 0.023845000192523003, 0.025283196941018105, 0.022970059886574745, 0.023468155413866043, 0.023649483919143677, 0.027087407186627388, 0.02323940582573414, 0.026941625401377678, 0.02295011468231678, 0.021697083488106728, 0.02393905632197857, 0.020785348489880562, 0.023164236918091774, 0.020191343501210213, 0.02233703061938286, 0.022597238421440125, 0.02303612418472767, 0.021534504368901253, 0.02200034074485302, 0.026918543502688408, 0.018716607242822647, 0.02487434819340706, 0.025986146181821823, 0.01739874668419361, 0.020836113020777702, 0.020464345812797546, 0.020752590149641037, 0.019925951957702637, 0.021828481927514076, 0.01954154670238495, 0.021456491202116013, 0.018715964630246162, 0.019551677629351616, 0.021638229489326477, 0.01961161196231842, 0.020148437470197678, 0.018035685643553734, 0.017521822825074196, 0.020306043326854706, 0.01903596706688404, 0.021686168387532234, 0.018315652385354042, 0.01895095221698284, 0.01918177679181099, 0.017086010426282883, 0.020321138203144073, 0.018909180536866188, 0.017955778166651726, 0.01888972893357277, 0.02022172510623932, 0.018824342638254166, 0.02047768607735634, 0.017748726531863213, 0.01826818846166134, 0.02137843891978264, 0.018503062427043915, 0.01840641163289547, 0.018158208578824997, 0.021301144734025, 0.01588454842567444, 0.023109734058380127, 0.015351149253547192, 0.016462253406643867, 0.0175000187009573, 0.01647990755736828, 0.020094627514481544, 0.018188564106822014, 0.021505333483219147, 0.014824694022536278, 0.015808451920747757, 0.02284478023648262, 0.020695747807621956, 0.014322170056402683, 0.019774671643972397, 0.019491801038384438, 0.014822250232100487, 0.018543589860200882, 0.01750214956700802, 0.014439528807997704, 0.016213448718190193, 0.016691170632839203, 0.016303613781929016, 0.017758432775735855, 0.013320562429726124, 0.020407719537615776, 0.017611391842365265, 0.01789550483226776, 0.016915034502744675, 0.014799829572439194, 0.018558749929070473, 0.01299688033759594, 0.018443522974848747, 0.01443813368678093, 0.014806379564106464, 0.017482353374361992, 0.015403724275529385, 0.014893556945025921, 0.016584165394306183, 0.013860879465937614, 0.016201183199882507, 0.016858534887433052, 0.01788349635899067, 0.016018923372030258, 0.01635470986366272, 0.01551531720906496, 0.011290709488093853, 0.0160101018846035, 0.016244331374764442]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:19 2016", "state": "available"}], "summary": "9ba1128957e189c1bf9f98517571380d"}