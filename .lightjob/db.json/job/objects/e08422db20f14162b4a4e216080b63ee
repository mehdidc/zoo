{"content": {"hp_model": {"f0": 64, "f1": 16, "f2": 32, "f3": 16, "nonlin": "very_leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.009145451748011168, 0.011124333672623258, 0.012795371190351162, 0.014823142138035586, 0.011969437852240226, 0.01386122664376764, 0.013477551307190075, 0.014162775758658083, 0.013414725754668117, 0.015564433033662005, 0.014989343335545557, 0.011558773429497822, 0.013722534424164436, 0.015177991942847384, 0.015058195595705327, 0.016183418960162968, 0.015226899445383931, 0.016938284774065105, 0.017464155125294683, 0.014667119688987077, 0.015928389854539177, 0.015156259142487903, 0.016603991926718596, 0.01805589858542731, 0.01717949858461098, 0.01726901256175732, 0.01592506626925696, 0.0159622835675156, 0.01680190986676932, 0.018233041757701067, 0.01680984311966477, 0.016757249383596982, 0.017196514686442756, 0.01748972210825797, 0.017813968737154705, 0.018959542001410735, 0.01862415793184442, 0.01743045712164315, 0.016475485955646317, 0.017079438418704868, 0.017911767594690257, 0.01955307632334827, 0.015118191860626764, 0.018442123119708092, 0.01728808199860592, 0.018623586507093698, 0.020439205111006845, 0.021095818069015573, 0.01979468348954491, 0.019313881648836205, 0.019736998083134984, 0.019376244053054895, 0.01786082496502745, 0.018941990950996607, 0.01893177525884793, 0.02035748536310469, 0.018530382150068223, 0.019232519176783938, 0.01958659825810206, 0.017763780187686862, 0.017756279480104855, 0.016867464657698984, 0.018582882148941562, 0.019126312691685188, 0.017638639581719726, 0.019527606289218492, 0.018062467041195247, 0.019018420598118625, 0.019663929342280437, 0.020344152663896387, 0.01957337632744581, 0.018085785885429875, 0.019490575500664676, 0.017217188353664255, 0.017148782742008904, 0.017376980377633144, 0.01839117333158382, 0.018138427196496526, 0.018173629506737208, 0.015856921051570574, 0.01634859850736409, 0.017928618098459474, 0.016852130774430398, 0.01746239083416679, 0.017478175918719946, 0.016827738199774622, 0.016354234681519655, 0.016986860477084535, 0.01760588488789728, 0.017650874791474393, 0.01684450656020998, 0.01611902889214749, 0.014776273537743323, 0.016271142870704223, 0.015965235769811483, 0.01654635336836802, 0.0168575887824176, 0.016480627646491244, 0.01748639883767588, 0.016987216504485748, 0.01697082284657337, 0.01618604859340884, 0.016471220834612943, 0.017494397316260107, 0.01783303758454807, 0.01735286287447586, 0.017402411369824476, 0.016574393779279283, 0.01717401561087247, 0.01692315099284375, 0.018502792505948547, 0.017098415291347546, 0.01676489353966548, 0.017017933909728767, 0.016390356362569653, 0.016554358915864625, 0.017078201488991553, 0.016995482493711717, 0.016795518569220014, 0.016090581960657574, 0.01598364375782679, 0.017876239533343587, 0.017478011074359253, 0.01651978048979771, 0.018481215480624966, 0.0167672911740328, 0.017765740766901218, 0.017122151920708206, 0.017077066117254357, 0.016926528557412363, 0.015342634016216125, 0.01513804873319106, 0.016071826283978494, 0.016779108812881582, 0.015694740423607287, 0.015193590137605439, 0.015349708559173772, 0.016706321117259244, 0.017252297470013285, 0.01677254227206436, 0.01684932286286993, 0.015081917433373686, 0.017970872990625324, 0.01696995969111369, 0.015132089154215668, 0.018026517003219247, 0.01756584145722973, 0.01754221104762352, 0.01788313433418071, 0.016628178931472323, 0.017064393632459547, 0.01782369454408773, 0.01824981157159549, 0.01737089119585613, 0.01642143436192975, 0.015920171437179043, 0.017011143391526203, 0.017009116942834985, 0.016901595857083156, 0.017610325241527877, 0.018017768427198197, 0.01729532670745232, 0.017659660104434163, 0.017217713991651708, 0.016440725395201375, 0.016974715422882144, 0.01681002635443586, 0.018007258567517292, 0.016108932313972004, 0.016865128205953647, 0.01638317720072499, 0.016618787436954784, 0.015887607059837622, 0.017069590651964867, 0.016803288846935705, 0.01635271968722479, 0.016333847342311497, 0.015887861310781203, 0.015668758669297095, 0.01637264491828449, 0.016218251738342584, 0.0154566595765054, 0.01660551708242028, 0.015574375881840474, 0.015755783403125445, 0.016912758978013212, 0.015760614005655618, 0.015693512130646008, 0.01525436716937855, 0.0162281250211487, 0.01532290688926337, 0.016498630250818762, 0.015676107229569417, 0.017006414635630785, 0.01624105806886974, 0.015792481154171856, 0.016226283923645238, 0.015666996597745363, 0.01618356384435017, 0.015553155731396828, 0.01604440239612978, 0.016994666219722124], "moving_avg_accuracy_train": [0.05519018275309153, 0.11426971041262918, 0.1757804405237749, 0.23391088193074586, 0.28799041755527, 0.3379685890530191, 0.38378367182361234, 0.42585179449567157, 0.46419226579938055, 0.4992424864035972, 0.5311618897104504, 0.5602566901497044, 0.5868325994962141, 0.6109902279913602, 0.6330111114941344, 0.6531182250990122, 0.671330812686241, 0.6881221031588036, 0.7033505220245863, 0.7171654170466569, 0.7298754071283976, 0.7413864778150595, 0.7520091471997127, 0.7616577971542061, 0.7704973670834884, 0.7785528893210144, 0.7858609880550259, 0.7926498294084842, 0.7987620757265874, 0.8044119069366898, 0.8095222235162305, 0.8142053219414163, 0.818499201632426, 0.822463710801963, 0.8260503341962038, 0.8293456885176779, 0.8324393545427097, 0.8352491585045054, 0.8378733852689497, 0.8402050345200633, 0.8424429196281807, 0.8445174340457152, 0.8464287469465147, 0.8482488739072249, 0.8498543279420746, 0.8514156021603717, 0.8528695049842017, 0.8542431937899527, 0.8555143548984527, 0.8566421238544362, 0.8577012576933835, 0.8587427977543792, 0.8597545885711801, 0.8606466351646436, 0.8614377432082567, 0.8621824367261169, 0.8629851222766964, 0.8637796188853133, 0.8645436021045246, 0.8652195252089484, 0.865913922557701, 0.8664807154025216, 0.8670721731223746, 0.8676371092512132, 0.8682524725635873, 0.868838887676876, 0.8694316572990463, 0.8699605357101993, 0.8704527662730849, 0.8709213864654054, 0.8713222903968456, 0.8717923498803707, 0.8721782731322284, 0.8726696551386346, 0.8731257777396197, 0.8735688401638397, 0.8738561694468744, 0.8742565638301678, 0.8746934684417523, 0.8750588889529204, 0.8753250244439333, 0.8755784612298833, 0.8759251729753427, 0.8763139434569704, 0.8764685243904354, 0.8767680104007736, 0.8770909541350594, 0.8774188779745067, 0.8776838545931228, 0.8779687644284303, 0.878299551993293, 0.8785253253838493, 0.8788074683484175, 0.8791356575831586, 0.8794312441873381, 0.879671695494195, 0.8799392188953569, 0.8801751954635084, 0.880417909455825, 0.8806966977715012, 0.8809523296508663, 0.8812194925767911, 0.8814228449756273, 0.8816826280941129, 0.8819117105054934, 0.8822085294304886, 0.882485003107041, 0.882701277332605, 0.8829145253260886, 0.8831342782083007, 0.8833576684880151, 0.8835911997254539, 0.8837387430165666, 0.883931949798797, 0.8842057452039763, 0.8843754672067422, 0.8845561908925833, 0.8847279986098033, 0.8848734691553383, 0.8850648465153673, 0.8852254964441646, 0.8853583474895783, 0.8854988037209177, 0.8856346591196362, 0.8858033959058546, 0.8859924253455848, 0.8861114706651699, 0.8863301464980162, 0.8864711511761493, 0.886632896569793, 0.8868180310026531, 0.8869289205184361, 0.8870844886052506, 0.8871755996607461, 0.887341304967835, 0.887439358568043, 0.8875042832224974, 0.887669744354382, 0.8877394961670981, 0.8878627987652277, 0.8879877580452202, 0.8881234007876713, 0.8882826816368296, 0.888481765874863, 0.8885354557510164, 0.8887022871312027, 0.8888222444876653, 0.8889883348287198, 0.8891285515892494, 0.8892592888248889, 0.889451393147688, 0.8895732419108539, 0.8897315897274282, 0.8898578627694971, 0.8899761588049782, 0.8900804442833766, 0.8902392251365081, 0.8903683212067444, 0.8904750628794439, 0.8906129470146261, 0.8906905397600996, 0.8907766132238737, 0.8909098829126989, 0.8909997068445741, 0.8911224010618332, 0.8912490658502142, 0.891409531087129, 0.8915190725682094, 0.8916433446845428, 0.8917527202451583, 0.8918140640152161, 0.8919366666749255, 0.8920726217543876, 0.8921833195330371, 0.892341040205241, 0.8924574121733196, 0.8925783148398009, 0.8926686702444329, 0.8927802170431254, 0.8928248055905201, 0.8929299673522045, 0.8930061198937006, 0.8931071011179241, 0.8932677747328298, 0.8933773956076457, 0.8934528389557036, 0.8935510009522982, 0.8936114449635191, 0.89369843270577, 0.893825369554702, 0.8938443897640067, 0.8939358406166481, 0.8939556197078995, 0.8940337305637984, 0.8941738568960307, 0.8942279270307631, 0.8943393691698796, 0.8944373419462747, 0.8945556722819167, 0.8946854571209084, 0.8947720004926585, 0.8948754661641383], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 252963515, "moving_var_accuracy_train": [0.027413606450876784, 0.05608576110205577, 0.08452931426110606, 0.10648871679851903, 0.12216121067894473, 0.13242544824737607, 0.13807409970612525, 0.14019423224187552, 0.13940473467580272, 0.13652092288786072, 0.13203846536626465, 0.1264531855430383, 0.12016437760707965, 0.11340025897695712, 0.10642450687144621, 0.09942072034197676, 0.09246393342738138, 0.085755067006249, 0.07926670297598962, 0.07305769459862817, 0.06720581976966689, 0.061677780527880144, 0.05652557241879295, 0.051710883190412775, 0.047243036840183536, 0.04310275610283867, 0.039273155256509346, 0.03576063503316024, 0.03252080752532271, 0.029556012107114265, 0.026835448916290996, 0.02434928672240169, 0.022080294675369313, 0.02001372120443047, 0.01812812389033646, 0.016413045742239337, 0.014857878093285325, 0.013443145268690306, 0.012160809836822306, 0.01099365814721204, 0.009939365500305036, 0.008984161440891557, 0.008118623349845264, 0.007336576774238678, 0.0066261164407369465, 0.005985442991325727, 0.00540592319298342, 0.004882314062100492, 0.004408625310964313, 0.003979209545230602, 0.003591384471106771, 0.003242009275284025, 0.0029270218336682865, 0.0026414813744256448, 0.0023829659044131055, 0.0021496604298916816, 0.0019404931237404959, 0.0017521248351163787, 0.0015821653848378703, 0.0014280606947419294, 0.0012895943143693286, 0.0011635261700928534, 0.001050321953192932, 0.0009481621333406429, 0.0008567539680625224, 0.0007741735154221112, 0.0006999185463046111, 0.0006324441030382031, 0.0005713803110777307, 0.0005162187239318123, 0.00046604336719882947, 0.00042142763374141356, 0.00038062530117419213, 0.0003447358775427507, 0.00031213472023264104, 0.00028268798701517836, 0.0002551622113656633, 0.00023108883118865283, 0.0002096979188264021, 0.00018992991629360287, 0.00017157437756043254, 0.00015499501164464315, 0.00014057739179013462, 0.00012787993499758707, 0.00011530699888274659, 0.00010458352582796626, 9.50638071448002e-05, 8.652523283062014e-05, 7.850462302327733e-05, 7.138472324924412e-05, 6.523103464193029e-05, 5.916669379268668e-05, 5.396646628551595e-05, 4.9539193221164e-05, 4.5371616864181036e-05, 4.1354806656485265e-05, 3.78634449223598e-05, 3.4578264896573474e-05, 3.16506291455123e-05, 2.9185072555579545e-05, 2.6854694219751672e-05, 2.4811609058674825e-05, 2.270261793581932e-05, 2.1039741560088427e-05, 1.9408076164914763e-05, 1.82601818165407e-05, 1.712210287932458e-05, 1.5830863457181436e-05, 1.4657049471986677e-05, 1.362596648795273e-05, 1.2712498792795392e-05, 1.193208046325309e-05, 1.0934793621699308e-05, 1.017727400582771e-05, 9.834221920320886e-06, 9.110049752294592e-06, 8.492994232681067e-06, 7.909355834680287e-06, 7.3088753677764875e-06, 6.907615476384254e-06, 6.449129525349155e-06, 5.963061175221926e-06, 5.544306633998108e-06, 5.155986174846926e-06, 4.896636484571944e-06, 4.728561997877023e-06, 4.383251891125425e-06, 4.375298780851833e-06, 4.1167097760652645e-06, 3.940492949743727e-06, 3.854916478843283e-06, 3.580093193354339e-06, 3.4398967407352016e-06, 3.1706180865634018e-06, 3.1006805170839433e-06, 2.8771430419993097e-06, 2.627365634603658e-06, 2.61102554662418e-06, 2.393710830356395e-06, 2.2911715236703604e-06, 2.20258776620964e-06, 2.1479195718053913e-06, 2.161461114802173e-06, 2.3020258078225294e-06, 2.0977666522525905e-06, 2.138484371761043e-06, 2.0541438409103835e-06, 2.097003469343526e-06, 2.0642497818100165e-06, 2.0116548266728825e-06, 2.1426259815482397e-06, 2.0619874731588734e-06, 2.0814550049675515e-06, 2.016813434850836e-06, 1.941077659460596e-06, 1.8448490425576707e-06, 1.8872663721924814e-06, 1.8485318931271516e-06, 1.766222766030631e-06, 1.7607088020421343e-06, 1.6388236291890073e-06, 1.5416190367647399e-06, 1.5473044227245225e-06, 1.4651890290897182e-06, 1.4541549647201335e-06, 1.4531351857887903e-06, 1.539563497532787e-06, 1.4936011724750682e-06, 1.483233085309303e-06, 1.4425768961179625e-06, 1.3321867296303123e-06, 1.334250766177837e-06, 1.3671797422438045e-06, 1.34074775180077e-06, 1.4305552705846421e-06, 1.4093816581168172e-06, 1.4000005851656464e-06, 1.3334774189649247e-06, 1.3121138717554705e-06, 1.198795731608881e-06, 1.1784471235331411e-06, 1.1127952973665856e-06, 1.0932906364410489e-06, 1.216305667538543e-06, 1.2028257265434242e-06, 1.1337684427847243e-06, 1.107113596685283e-06, 1.029283543449015e-06, 9.944569948213107e-07, 1.040027967890353e-06, 9.392810863592412e-07, 9.206223037628987e-07, 8.320809854431871e-07, 8.037846391821531e-07, 9.001246761275811e-07, 8.364244237447162e-07, 8.645561347077299e-07, 8.644885054682299e-07, 9.040582699197722e-07, 9.652493828169454e-07, 9.361322412800875e-07, 9.388653237250102e-07], "duration": 144303.506368, "accuracy_train": [0.5519018275309154, 0.6459854593484681, 0.7293770115240864, 0.7570848545934846, 0.7747062381759875, 0.7877721325327611, 0.7961194167589516, 0.8044648985442044, 0.8092565075327611, 0.8146944718415466, 0.81843651947213, 0.8221098941029901, 0.8260157836148025, 0.8284088844476744, 0.831199063019103, 0.8340822475429125, 0.8352441009712993, 0.8392437174118678, 0.8404062918166297, 0.8414994722452934, 0.8442653178640642, 0.8449861139950166, 0.8476131716615909, 0.8484956467446475, 0.8500534964470285, 0.8510525894587486, 0.8516338766611297, 0.8537494015896088, 0.8537722925895165, 0.8552603878276117, 0.8555150727320967, 0.8563532077680879, 0.8571441188515135, 0.8581442933277963, 0.8583299447443706, 0.859003877410945, 0.8602823487679956, 0.8605373941606681, 0.8614914261489479, 0.8611898777800849, 0.8625838856012367, 0.8631880638035253, 0.8636305630537099, 0.8646300165536176, 0.8643034142557217, 0.8654670701250462, 0.865954630398671, 0.8666063930417128, 0.8669548048749538, 0.8667920444582872, 0.8672334622439092, 0.8681166583033407, 0.8688607059223883, 0.868675054505814, 0.8685577156007751, 0.8688846783868586, 0.8702092922319121, 0.8709300883628645, 0.8714194510774271, 0.8713028331487633, 0.8721634986964747, 0.8715818510059062, 0.8723952926010521, 0.8727215344107604, 0.8737907423749538, 0.8741166236964747, 0.8747665838985788, 0.8747204414105758, 0.874882841339055, 0.8751389681962901, 0.8749304257798081, 0.8760228852320967, 0.8756515823989479, 0.8770920931962901, 0.8772308811484865, 0.8775564019818198, 0.876442132994186, 0.8778601132798081, 0.8786256099460132, 0.878347673553433, 0.8777202438630491, 0.877859392303433, 0.8790455786844776, 0.8798128777916205, 0.8778597527916205, 0.8794633844938169, 0.8799974477436323, 0.8803701925295312, 0.8800686441606681, 0.8805329529461978, 0.881276640077058, 0.8805572858988556, 0.8813467550295312, 0.8820893606958287, 0.8820915236249538, 0.8818357572559062, 0.882346929505814, 0.8822989845768733, 0.882602335386674, 0.8832057926125876, 0.8832530165651532, 0.8836239589101144, 0.8832530165651532, 0.8840206761604835, 0.883973452207918, 0.8848798997554448, 0.8849732661960132, 0.8846477453626799, 0.8848337572674418, 0.8851120541482096, 0.8853681810054448, 0.885692980862403, 0.8850666326365817, 0.8856708108388703, 0.8866699038505905, 0.8859029652316353, 0.8861827040651532, 0.8862742680647839, 0.8861827040651532, 0.8867872427556294, 0.8866713458033407, 0.8865540068983019, 0.8867629098029715, 0.8868573577081026, 0.8873220269818198, 0.8876936903031561, 0.887182878541436, 0.8882982289936323, 0.8877401932793466, 0.8880886051125876, 0.8884842408983942, 0.8879269261604835, 0.8884846013865817, 0.8879955991602067, 0.8888326527316353, 0.8883218409699151, 0.8880886051125876, 0.8891588945413437, 0.888367262481543, 0.8889725221483942, 0.8891123915651532, 0.8893441854697305, 0.8897162092792543, 0.890273524017165, 0.8890186646363971, 0.8902037695528792, 0.8899018606958287, 0.8904831478982096, 0.8903905024340162, 0.8904359239456442, 0.8911803320528792, 0.8906698807793466, 0.8911567200765966, 0.8909943201481173, 0.8910408231243078, 0.8910190135889626, 0.8916682528146919, 0.8915301858388703, 0.8914357379337394, 0.8918539042312662, 0.8913888744693614, 0.8915512743978405, 0.8921093101121264, 0.8918081222314507, 0.892226649017165, 0.8923890489456442, 0.8928537182193614, 0.8925049458979328, 0.892761793731543, 0.8927371002906977, 0.8923661579457364, 0.8930400906123109, 0.8932962174695459, 0.8931795995408823, 0.8937605262550757, 0.8935047598860282, 0.8936664388381322, 0.8934818688861205, 0.8937841382313585, 0.8932261025170728, 0.8938764232073644, 0.893691492767165, 0.8940159321359358, 0.8947138372669805, 0.8943639834809893, 0.8941318290882245, 0.8944344589216501, 0.8941554410645073, 0.8944813223860282, 0.8949678011950905, 0.8940155716477483, 0.8947588982904209, 0.894133631529162, 0.8947367282668882, 0.8954349938861205, 0.8947145582433554, 0.8953423484219268, 0.8953190969338316, 0.8956206453026948, 0.8958535206718347, 0.8955508908384091, 0.8958066572074567], "end": "2016-01-31 02:52:08.679000", "learning_rate_per_epoch": [0.0005062675336375833, 0.00025313376681879163, 0.00016875583969522268, 0.00012656688340939581, 0.00010125350672751665, 8.437791984761134e-05, 7.232393545564264e-05, 6.328344170469791e-05, 5.625194899039343e-05, 5.0626753363758326e-05, 4.602432090905495e-05, 4.218895992380567e-05, 3.894365727319382e-05, 3.616196772782132e-05, 3.375117012183182e-05, 3.1641720852348953e-05, 2.9780443583149463e-05, 2.8125974495196715e-05, 2.6645659090718254e-05, 2.5313376681879163e-05, 2.4107977878884412e-05, 2.3012160454527475e-05, 2.2011630790075287e-05, 2.1094479961902834e-05, 2.025070170930121e-05, 1.947182863659691e-05, 1.8750648450804874e-05, 1.808098386391066e-05, 1.7457501598983072e-05, 1.687558506091591e-05, 1.633121064514853e-05, 1.5820860426174477e-05, 1.5341440303018317e-05, 1.4890221791574731e-05, 1.4464786545431707e-05, 1.4062987247598357e-05, 1.3682905773748644e-05, 1.3322829545359127e-05, 1.2981218787899707e-05, 1.2656688340939581e-05, 1.2347988558758516e-05, 1.2053988939442206e-05, 1.1773663572967052e-05, 1.1506080227263737e-05, 1.1250389434280805e-05, 1.1005815395037644e-05, 1.0771649613161571e-05, 1.0547239980951417e-05, 1.033199077937752e-05, 1.0125350854650605e-05, 9.926814527716488e-06, 9.735914318298455e-06, 9.552217306918465e-06, 9.375324225402437e-06, 9.20486399991205e-06, 9.04049193195533e-06, 8.881886060407851e-06, 8.728750799491536e-06, 8.580805115343537e-06, 8.437792530457955e-06, 8.29946748126531e-06, 8.165605322574265e-06, 8.035992323129904e-06, 7.910430213087238e-06, 7.788731636537705e-06, 7.670720151509158e-06, 7.5562315942079294e-06, 7.445110895787366e-06, 7.33721071810578e-06, 7.2323932727158535e-06, 7.130528501875233e-06, 7.031493623799179e-06, 6.935171768418513e-06, 6.841452886874322e-06, 6.750233751517953e-06, 6.661414772679564e-06, 6.574903181899572e-06, 6.490609393949853e-06, 6.408449735317845e-06, 6.328344170469791e-06, 6.250216301850742e-06, 6.173994279379258e-06, 6.0996089814580046e-06, 6.026994469721103e-06, 5.956088443781482e-06, 5.886831786483526e-06, 5.819167199661024e-06, 5.753040113631869e-06, 5.6883991419454105e-06, 5.625194717140403e-06, 5.563379545492353e-06, 5.502907697518822e-06, 5.443736881716177e-06, 5.385824806580786e-06, 5.329131909093121e-06, 5.273619990475709e-06, 5.219252670940477e-06, 5.16599538968876e-06, 5.113813585921889e-06, 5.062675427325303e-06, 5.0125499910791405e-06, 4.963407263858244e-06, 4.915218596579507e-06, 4.867957159149228e-06, 4.821595666726353e-06, 4.776108653459232e-06, 4.73147201773827e-06, 4.6876621127012186e-06, 4.644656200980535e-06, 4.602431999956025e-06, 4.560968591249548e-06, 4.520245965977665e-06, 4.480243660509586e-06, 4.4409430302039254e-06, 4.402326339913998e-06, 4.364375399745768e-06, 4.327072929299902e-06, 4.290402557671769e-06, 4.254348823451437e-06, 4.2188962652289774e-06, 4.184028966847109e-06, 4.149733740632655e-06, 4.115996034670388e-06, 4.082802661287133e-06, 4.050140432809712e-06, 4.017996161564952e-06, 3.986358478869079e-06, 3.955215106543619e-06, 3.924554675904801e-06, 3.894365818268852e-06, 3.8646376196993515e-06, 3.835360075754579e-06, 3.806522727245465e-06, 3.7781157971039647e-06, 3.75012973563571e-06, 3.722555447893683e-06, 3.695383384183515e-06, 3.66860535905289e-06, 3.642212504928466e-06, 3.6161966363579268e-06, 3.590549795262632e-06, 3.5652642509376165e-06, 3.540332272677915e-06, 3.5157468118995894e-06, 3.491500137897674e-06, 3.4675858842092566e-06, 3.443996774876723e-06, 3.420726443437161e-06, 3.3977687508013332e-06, 3.3751168757589767e-06, 3.352765133968205e-06, 3.330707386339782e-06, 3.308938175905496e-06, 3.287451590949786e-06, 3.266242174504441e-06, 3.2453046969749266e-06, 3.224633928766707e-06, 3.2042248676589224e-06, 3.1840725114307133e-06, 3.1641720852348953e-06, 3.144518814224284e-06, 3.125108150925371e-06, 3.105935775238322e-06, 3.086997139689629e-06, 3.0682881515531335e-06, 3.0498044907290023e-06, 3.0315420644910773e-06, 3.0134972348605515e-06, 2.995665909111267e-06, 2.978044221890741e-06, 2.960628762593842e-06, 2.943415893241763e-06, 2.926401975855697e-06, 2.909583599830512e-06, 2.8929573545610765e-06, 2.8765200568159344e-06, 2.8602685233636294e-06, 2.8441995709727053e-06, 2.8283102437853813e-06, 2.8125973585702013e-06, 2.79705818684306e-06, 2.7816897727461765e-06, 2.76648916042177e-06, 2.751453848759411e-06, 2.736581336648669e-06, 2.7218684408580884e-06, 2.7073128876509145e-06, 2.692912403290393e-06, 2.678664259292418e-06, 2.6645659545465605e-06, 2.650615442689741e-06, 2.6368099952378543e-06, 2.623147793201497e-06, 2.6096263354702387e-06, 2.5962438030546764e-06, 2.58299769484438e-06, 2.5698859644762706e-06, 2.5569067929609446e-06, 2.5440579065616475e-06, 2.5313377136626514e-06, 2.5187439405272016e-06, 2.5062749955395702e-06], "accuracy_valid": [0.5492266919239458, 0.6418207007718373, 0.7176807817206325, 0.7455245787838856, 0.7603156767695783, 0.7712314100150602, 0.7736522260918675, 0.7793998258659638, 0.7802646131400602, 0.7850768307605422, 0.7870299557605422, 0.7896949124623494, 0.7959822689194277, 0.7947821559676205, 0.7989222515060241, 0.7994414180158133, 0.7974265224962349, 0.8033579631024097, 0.803856539439006, 0.803612398814006, 0.8053110881024097, 0.8038256541792168, 0.8060332148908133, 0.8075083537274097, 0.8067347515060241, 0.8067656367658133, 0.8075995387801205, 0.8088099468185241, 0.8093291133283133, 0.8115675592996988, 0.810814547251506, 0.8097968044051205, 0.8121676157756024, 0.8122588008283133, 0.8123705760542168, 0.8137545298381024, 0.8143854715737951, 0.8155958796121988, 0.8150973032756024, 0.8150973032756024, 0.8158503153237951, 0.8148531626506024, 0.8143442912274097, 0.8157282450112951, 0.8157179499246988, 0.8159620905496988, 0.8165930322853916, 0.8192991693335843, 0.8170813135353916, 0.8168371729103916, 0.8185564523719879, 0.8182005365210843, 0.8179152155496988, 0.8194109445594879, 0.8183020166603916, 0.8188005929969879, 0.8189226633094879, 0.8181902414344879, 0.8194212396460843, 0.8189123682228916, 0.8186682275978916, 0.8187800028237951, 0.8190344385353916, 0.8206213525978916, 0.8185358621987951, 0.8186682275978916, 0.8186682275978916, 0.8194212396460843, 0.8202654367469879, 0.8206419427710843, 0.8221067865210843, 0.8185358621987951, 0.8206213525978916, 0.8209772684487951, 0.8207434229103916, 0.8204786921121988, 0.8180475809487951, 0.8208449030496988, 0.8198683405496988, 0.8210787485881024, 0.8196139048381024, 0.8212419992469879, 0.8202448465737951, 0.8208757883094879, 0.8206110575112951, 0.8212214090737951, 0.8215979150978916, 0.8197462702371988, 0.8204992822853916, 0.8196344950112951, 0.8210993387612951, 0.8206007624246988, 0.8206007624246988, 0.8208551981362951, 0.8210993387612951, 0.8209772684487951, 0.8218420557228916, 0.8213434793862951, 0.8204786921121988, 0.8209875635353916, 0.8218317606362951, 0.8215876200112951, 0.8207228327371988, 0.8213640695594879, 0.8229715737951807, 0.8218317606362951, 0.8213434793862951, 0.8213434793862951, 0.8204992822853916, 0.8213434793862951, 0.8217199854103916, 0.8202551416603916, 0.8215876200112951, 0.8212214090737951, 0.8223200418862951, 0.8207331278237951, 0.8209875635353916, 0.8204786921121988, 0.8222082666603916, 0.8217199854103916, 0.8220861963478916, 0.8209875635353916, 0.8216082101844879, 0.8231848291603916, 0.8211199289344879, 0.8213537744728916, 0.8215979150978916, 0.8234495599585843, 0.8222082666603916, 0.8212317041603916, 0.8209772684487951, 0.8222082666603916, 0.8226965479103916, 0.8215876200112951, 0.8236834054969879, 0.8220656061746988, 0.8220656061746988, 0.8220759012612951, 0.8228186182228916, 0.8217199854103916, 0.8228289133094879, 0.8225538874246988, 0.8225950677710843, 0.8236731104103916, 0.8225641825112951, 0.8236937005835843, 0.8230524637612951, 0.8234186746987951, 0.821312594126506, 0.8234186746987951, 0.8241716867469879, 0.8255247552710843, 0.8238157708960843, 0.8229509836219879, 0.8235613351844879, 0.8225538874246988, 0.8240496164344879, 0.8233171945594879, 0.8236834054969879, 0.8235510400978916, 0.8231848291603916, 0.8220759012612951, 0.8235613351844879, 0.8228186182228916, 0.8234186746987951, 0.8228083231362951, 0.8227980280496988, 0.8235613351844879, 0.8225641825112951, 0.8218317606362951, 0.8224318171121988, 0.8228186182228916, 0.8234186746987951, 0.8236628153237951, 0.8243952371987951, 0.8242834619728916, 0.8230627588478916, 0.8240290262612951, 0.8240290262612951, 0.8247717432228916, 0.8220656061746988, 0.8225538874246988, 0.8228083231362951, 0.8241510965737951, 0.8227980280496988, 0.8238966608621988, 0.8237745905496988, 0.8221773814006024, 0.8218317606362951, 0.8251379541603916, 0.8224318171121988, 0.8234186746987951, 0.8234186746987951, 0.8223406320594879, 0.8247717432228916, 0.8230318735881024, 0.8232966043862951, 0.8240187311746988, 0.8245173075112951, 0.8242834619728916, 0.8234186746987951, 0.8249041086219879], "accuracy_test": 0.8180584343112244, "start": "2016-01-29 10:47:05.172000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0], "accuracy_train_last": 0.8958066572074567, "batch_size_eval": 1024, "accuracy_train_std": [0.014744618511508538, 0.014220882298631346, 0.017593498682324335, 0.01453178884465229, 0.014568010045918877, 0.01404552193158595, 0.014576356756886864, 0.014303744331567095, 0.014352425646373311, 0.012798146026179527, 0.01385470887195726, 0.012873891651805316, 0.013316377075514219, 0.01258869970149046, 0.011823553157062745, 0.012554938987125955, 0.01198023996162957, 0.01262396688190004, 0.011937942298716142, 0.011911011548013196, 0.012234356426476092, 0.0114970379642826, 0.012825737231670412, 0.012522435054322573, 0.012036988703987818, 0.011974294604493384, 0.012231535600681347, 0.012059868128234437, 0.011376028058297726, 0.011876813608664813, 0.012315441206717114, 0.012421125855664848, 0.01172049286898508, 0.011609028584373267, 0.011318461099269796, 0.011702063385622066, 0.011787669847787426, 0.011380830513352199, 0.011350970340982112, 0.010612379561186918, 0.01180634531785553, 0.010917693471510724, 0.01085996105807321, 0.010729554464026842, 0.011522047853851922, 0.011358386630597211, 0.010712076107785348, 0.011013794475416984, 0.011368099025656761, 0.010674304095556584, 0.01054527728300736, 0.010921390949686065, 0.010511095323716196, 0.010264459171370302, 0.011095657972048985, 0.011388584418015888, 0.010958626723860498, 0.010888233117422081, 0.010928044531386951, 0.01034484512631491, 0.010823942142569026, 0.01093744840152523, 0.010458439541533545, 0.01033205901090959, 0.010506651165674182, 0.01050020161229853, 0.010475950344935939, 0.010289186856538913, 0.010463799866524234, 0.010604984113184683, 0.01012495914889557, 0.010452831405486694, 0.010404121457972119, 0.01063928612639727, 0.010288793527669487, 0.009985699401130919, 0.010524664228803108, 0.009826730404218878, 0.010306186960556662, 0.009984419101085175, 0.011029104237112557, 0.01056183030321765, 0.009725073499715804, 0.010089098010661572, 0.010116034368391409, 0.00996159166396813, 0.010049659388111492, 0.010142625579589054, 0.010647219995958605, 0.010017431687454028, 0.01056123225047648, 0.00999188289089263, 0.010914783107581873, 0.010254429335503775, 0.01037686275110456, 0.009995541579959799, 0.010719638561682327, 0.010013459467417489, 0.010248134760062855, 0.010389401544531178, 0.010570192490224015, 0.009960166357218341, 0.01023181042455617, 0.010321719243110128, 0.010158248000756242, 0.010164676979336993, 0.010123248145953447, 0.010812276039456392, 0.010386403000168748, 0.010877796740631997, 0.010631539703130936, 0.01056633732699993, 0.010471748663322292, 0.01103112206757705, 0.010250809489902594, 0.010500473244462238, 0.010342533080561798, 0.010163954813307051, 0.010062104383481499, 0.010549521652106424, 0.010810084117118363, 0.011105921034852237, 0.010369755852547125, 0.010697385725397187, 0.010218433379613511, 0.010225147999343024, 0.010758089478171294, 0.01016500357848747, 0.010672709631061594, 0.011337157668868319, 0.010250198670725784, 0.010602038200506448, 0.010686257438048168, 0.010705532574435798, 0.011272727657785291, 0.010945683394991813, 0.010630140007081923, 0.01029238372672263, 0.010804495457119175, 0.010750345469430003, 0.010667115015297017, 0.010506681708145806, 0.010480579846675534, 0.010665613293117254, 0.010908128906505306, 0.01035519414412357, 0.010518531294848494, 0.010942946788418619, 0.010933303810194053, 0.01098069917678633, 0.010307096534528201, 0.010690160916940163, 0.010273598494259647, 0.01062982264098064, 0.010593866568766324, 0.01079189682450169, 0.010933701763876105, 0.01035241441063511, 0.011177072001750773, 0.010644113384260884, 0.011036658579943813, 0.011036835993083327, 0.010641704728203952, 0.010915804335234307, 0.010464083342711067, 0.01062391185434091, 0.01080581614247295, 0.010490329169046545, 0.010500109292241053, 0.010707186368953205, 0.010748244356614126, 0.010719845180215662, 0.010440981402603989, 0.01011550997802157, 0.010913262250386286, 0.010541525705896629, 0.01082867517331383, 0.010371494593074715, 0.010899802800237186, 0.010452705416771178, 0.01033586304346907, 0.010502906404843686, 0.010206747797126504, 0.010302892010379639, 0.009920035093122476, 0.01053832012673638, 0.010654412096381698, 0.010316803511040461, 0.010122082689877519, 0.010662212207618284, 0.010452120078942556, 0.010368779779696156, 0.01059689588519081, 0.010538218324769507, 0.0105973028619491, 0.010446085279988701, 0.010336287638740853, 0.010562967007318236, 0.010098325162790852, 0.010683696834608666, 0.010782831483721216, 0.01052834115731383], "accuracy_test_std": 0.008593658716009377, "error_valid": [0.4507733080760542, 0.3581792992281627, 0.28231921827936746, 0.25447542121611444, 0.23968432323042166, 0.22876858998493976, 0.22634777390813254, 0.2206001741340362, 0.21973538685993976, 0.21492316923945776, 0.21297004423945776, 0.21030508753765065, 0.2040177310805723, 0.20521784403237953, 0.20107774849397586, 0.20055858198418675, 0.2025734775037651, 0.1966420368975903, 0.19614346056099397, 0.19638760118599397, 0.1946889118975903, 0.1961743458207832, 0.19396678510918675, 0.1924916462725903, 0.19326524849397586, 0.19323436323418675, 0.19240046121987953, 0.19119005318147586, 0.19067088667168675, 0.18843244070030118, 0.18918545274849397, 0.19020319559487953, 0.18783238422439763, 0.18774119917168675, 0.1876294239457832, 0.18624547016189763, 0.18561452842620485, 0.18440412038780118, 0.18490269672439763, 0.18490269672439763, 0.18414968467620485, 0.18514683734939763, 0.1856557087725903, 0.18427175498870485, 0.18428205007530118, 0.18403790945030118, 0.1834069677146084, 0.18070083066641573, 0.1829186864646084, 0.1831628270896084, 0.18144354762801207, 0.18179946347891573, 0.18208478445030118, 0.18058905544051207, 0.1816979833396084, 0.18119940700301207, 0.18107733669051207, 0.18180975856551207, 0.18057876035391573, 0.1810876317771084, 0.1813317724021084, 0.18121999717620485, 0.1809655614646084, 0.1793786474021084, 0.18146413780120485, 0.1813317724021084, 0.1813317724021084, 0.18057876035391573, 0.17973456325301207, 0.17935805722891573, 0.17789321347891573, 0.18146413780120485, 0.1793786474021084, 0.17902273155120485, 0.1792565770896084, 0.17952130788780118, 0.18195241905120485, 0.17915509695030118, 0.18013165945030118, 0.17892125141189763, 0.18038609516189763, 0.17875800075301207, 0.17975515342620485, 0.17912421169051207, 0.17938894248870485, 0.17877859092620485, 0.1784020849021084, 0.18025372976280118, 0.1795007177146084, 0.18036550498870485, 0.17890066123870485, 0.17939923757530118, 0.17939923757530118, 0.17914480186370485, 0.17890066123870485, 0.17902273155120485, 0.1781579442771084, 0.17865652061370485, 0.17952130788780118, 0.1790124364646084, 0.17816823936370485, 0.17841237998870485, 0.17927716726280118, 0.17863593044051207, 0.1770284262048193, 0.17816823936370485, 0.17865652061370485, 0.17865652061370485, 0.1795007177146084, 0.17865652061370485, 0.1782800145896084, 0.1797448583396084, 0.17841237998870485, 0.17877859092620485, 0.17767995811370485, 0.17926687217620485, 0.1790124364646084, 0.17952130788780118, 0.1777917333396084, 0.1782800145896084, 0.1779138036521084, 0.1790124364646084, 0.17839178981551207, 0.1768151708396084, 0.17888007106551207, 0.1786462255271084, 0.1784020849021084, 0.17655044004141573, 0.1777917333396084, 0.1787682958396084, 0.17902273155120485, 0.1777917333396084, 0.1773034520896084, 0.17841237998870485, 0.17631659450301207, 0.17793439382530118, 0.17793439382530118, 0.17792409873870485, 0.1771813817771084, 0.1782800145896084, 0.17717108669051207, 0.17744611257530118, 0.17740493222891573, 0.1763268895896084, 0.17743581748870485, 0.17630629941641573, 0.17694753623870485, 0.17658132530120485, 0.17868740587349397, 0.17658132530120485, 0.17582831325301207, 0.17447524472891573, 0.17618422910391573, 0.17704901637801207, 0.17643866481551207, 0.17744611257530118, 0.17595038356551207, 0.17668280544051207, 0.17631659450301207, 0.1764489599021084, 0.1768151708396084, 0.17792409873870485, 0.17643866481551207, 0.1771813817771084, 0.17658132530120485, 0.17719167686370485, 0.17720197195030118, 0.17643866481551207, 0.17743581748870485, 0.17816823936370485, 0.17756818288780118, 0.1771813817771084, 0.17658132530120485, 0.17633718467620485, 0.17560476280120485, 0.1757165380271084, 0.1769372411521084, 0.17597097373870485, 0.17597097373870485, 0.1752282567771084, 0.17793439382530118, 0.17744611257530118, 0.17719167686370485, 0.17584890342620485, 0.17720197195030118, 0.17610333913780118, 0.17622540945030118, 0.17782261859939763, 0.17816823936370485, 0.1748620458396084, 0.17756818288780118, 0.17658132530120485, 0.17658132530120485, 0.17765936794051207, 0.1752282567771084, 0.17696812641189763, 0.17670339561370485, 0.17598126882530118, 0.17548269248870485, 0.1757165380271084, 0.17658132530120485, 0.17509589137801207], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "momentum": 0.676955523062997, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.0005062675281285321, "patience_threshold": 1, "do_flip": true, "batch_size": 16, "l2_decay": 4.47158893723544e-06, "optimization": "adam", "nb_data_augmentation": 2, "learning_rate_decay_method": "lin", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.06188533294923974}, "accuracy_valid_max": 0.8255247552710843, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop', 'santa_sss'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8249041086219879, "loss_train": [1.5821139812469482, 1.1938847303390503, 0.9849720597267151, 0.8890063166618347, 0.8356019258499146, 0.7948943972587585, 0.7669127583503723, 0.7423760294914246, 0.724243700504303, 0.7079466581344604, 0.6979925632476807, 0.6830803155899048, 0.6756488084793091, 0.6671333909034729, 0.6594408750534058, 0.6513416171073914, 0.644708514213562, 0.6394609808921814, 0.6330509185791016, 0.6282192468643188, 0.6231233477592468, 0.6190653443336487, 0.6152143478393555, 0.6105673909187317, 0.6088424921035767, 0.6025474071502686, 0.59962397813797, 0.5959857106208801, 0.5921810269355774, 0.5910648107528687, 0.588829517364502, 0.5858955979347229, 0.5845476984977722, 0.5803778767585754, 0.578879714012146, 0.5764383673667908, 0.5743271112442017, 0.5723627805709839, 0.5710667967796326, 0.5699459910392761, 0.566510796546936, 0.5666602253913879, 0.5612746477127075, 0.5609840750694275, 0.5602059960365295, 0.5586079955101013, 0.5573292970657349, 0.5578464269638062, 0.5543595552444458, 0.5530678033828735, 0.5521858930587769, 0.5506086945533752, 0.5485578179359436, 0.5462595224380493, 0.5460609793663025, 0.5440610647201538, 0.5448911190032959, 0.5399495363235474, 0.5413831472396851, 0.5394464731216431, 0.5382710695266724, 0.5374179482460022, 0.5380777716636658, 0.5356320142745972, 0.534157931804657, 0.5339823961257935, 0.5337590575218201, 0.5331339240074158, 0.5304806232452393, 0.5289661884307861, 0.5274807810783386, 0.528559148311615, 0.5269772410392761, 0.5259327292442322, 0.5261489748954773, 0.5264759659767151, 0.5223463177680969, 0.5230271220207214, 0.5226913690567017, 0.5217372179031372, 0.5193754434585571, 0.5193395018577576, 0.5194148421287537, 0.5195648074150085, 0.5191423892974854, 0.5182473063468933, 0.5157381296157837, 0.5158133506774902, 0.5141606330871582, 0.5133911371231079, 0.5140695571899414, 0.5135278701782227, 0.5123149156570435, 0.5120543241500854, 0.5110004544258118, 0.5110505223274231, 0.5104283690452576, 0.510597825050354, 0.5097783207893372, 0.507014274597168, 0.5088308453559875, 0.5076692700386047, 0.5063992142677307, 0.5065808892250061, 0.5043940544128418, 0.5065782070159912, 0.5070525407791138, 0.5036423206329346, 0.504753828048706, 0.5019741058349609, 0.502701461315155, 0.5016016364097595, 0.5019103288650513, 0.500505805015564, 0.501248836517334, 0.49969375133514404, 0.5005328059196472, 0.49893689155578613, 0.49775591492652893, 0.4989188611507416, 0.49629655480384827, 0.49900946021080017, 0.4965757727622986, 0.49582600593566895, 0.4960194230079651, 0.4967222809791565, 0.4945119321346283, 0.49488380551338196, 0.4962967336177826, 0.4942009150981903, 0.49366042017936707, 0.4929529130458832, 0.494184285402298, 0.4935839772224426, 0.49030375480651855, 0.4916238784790039, 0.49108022451400757, 0.4913705885410309, 0.4909556806087494, 0.48907163739204407, 0.4883825480937958, 0.49021461606025696, 0.4897371828556061, 0.48925745487213135, 0.4899764657020569, 0.4891088604927063, 0.4885209798812866, 0.4877553880214691, 0.4878585636615753, 0.48436233401298523, 0.48648369312286377, 0.48701661825180054, 0.4870971143245697, 0.48673269152641296, 0.485904335975647, 0.4843102693557739, 0.4837065637111664, 0.48585250973701477, 0.48437753319740295, 0.4821265935897827, 0.4822898805141449, 0.4828891456127167, 0.48492521047592163, 0.4829998314380646, 0.48319464921951294, 0.4793490171432495, 0.4815351665019989, 0.4813128709793091, 0.48084309697151184, 0.4805144965648651, 0.479350209236145, 0.4790502190589905, 0.47988077998161316, 0.4793795347213745, 0.4788801372051239, 0.4788338243961334, 0.4786655008792877, 0.48016151785850525, 0.47807440161705017, 0.47651323676109314, 0.47684210538864136, 0.47503334283828735, 0.4776410162448883, 0.4761319160461426, 0.4763266146183014, 0.4764931797981262, 0.47550612688064575, 0.4756437838077545, 0.4768156111240387, 0.47559550404548645, 0.47257760167121887, 0.4734393060207367, 0.47344812750816345, 0.4758763015270233, 0.47286731004714966, 0.4740663170814514, 0.4730934500694275, 0.47336000204086304, 0.4714081585407257, 0.47033464908599854, 0.47129398584365845, 0.47051292657852173], "accuracy_train_first": 0.5519018275309154, "model": "residualv3", "loss_std": [0.35581502318382263, 0.2626293897628784, 0.24775826930999756, 0.2419530600309372, 0.23755274713039398, 0.23340493440628052, 0.2321825623512268, 0.2303822785615921, 0.2284705489873886, 0.22854232788085938, 0.227472722530365, 0.22344614565372467, 0.22517843544483185, 0.22271610796451569, 0.22222687304019928, 0.22120709717273712, 0.21936291456222534, 0.21761082112789154, 0.21895413100719452, 0.21752044558525085, 0.21650809049606323, 0.21678321063518524, 0.21633242070674896, 0.21551468968391418, 0.21342314779758453, 0.21092207729816437, 0.21297170221805573, 0.210316002368927, 0.21258531510829926, 0.21261265873908997, 0.21287600696086884, 0.2119130641222, 0.2097712755203247, 0.21074776351451874, 0.20816780626773834, 0.20841313898563385, 0.21020537614822388, 0.20843824744224548, 0.20850594341754913, 0.20821592211723328, 0.2081921100616455, 0.20824399590492249, 0.20746396481990814, 0.20675189793109894, 0.20585104823112488, 0.20747295022010803, 0.20471276342868805, 0.20480062067508698, 0.2077869176864624, 0.20432093739509583, 0.20454061031341553, 0.20370928943157196, 0.20237812399864197, 0.20387320220470428, 0.20312657952308655, 0.20279282331466675, 0.20398980379104614, 0.20193637907505035, 0.2032366544008255, 0.20260865986347198, 0.2024659663438797, 0.20134279131889343, 0.2003766894340515, 0.199984610080719, 0.20043079555034637, 0.19933047890663147, 0.20054678618907928, 0.20019322633743286, 0.19993621110916138, 0.19941827654838562, 0.1990189105272293, 0.20072369277477264, 0.1982116848230362, 0.19787979125976562, 0.20132605731487274, 0.19962316751480103, 0.19837430119514465, 0.19966691732406616, 0.19920748472213745, 0.20035549998283386, 0.19783344864845276, 0.19638286530971527, 0.19782327115535736, 0.19872841238975525, 0.19945497810840607, 0.19899630546569824, 0.19780395925045013, 0.19474054872989655, 0.19587522745132446, 0.19614875316619873, 0.1958136260509491, 0.19511760771274567, 0.19743947684764862, 0.19518980383872986, 0.19416770339012146, 0.19431768357753754, 0.19524604082107544, 0.19643913209438324, 0.196495920419693, 0.1953153759241104, 0.19700568914413452, 0.19566196203231812, 0.19676662981510162, 0.1952553540468216, 0.19539344310760498, 0.19391711056232452, 0.196111261844635, 0.1936737298965454, 0.19580060243606567, 0.19448959827423096, 0.1938987374305725, 0.19211581349372864, 0.19373488426208496, 0.19318313896656036, 0.19564376771450043, 0.19235478341579437, 0.19460709393024445, 0.19578224420547485, 0.19138121604919434, 0.19443926215171814, 0.1928512305021286, 0.19261661171913147, 0.19171003997325897, 0.19316992163658142, 0.19310522079467773, 0.1933611035346985, 0.19330435991287231, 0.1931144744157791, 0.19258230924606323, 0.19219139218330383, 0.1927398145198822, 0.19229336082935333, 0.19416674971580505, 0.19451327621936798, 0.19164763391017914, 0.19460773468017578, 0.19194066524505615, 0.19261139631271362, 0.1909748911857605, 0.1903403401374817, 0.1903848648071289, 0.19146834313869476, 0.191335991024971, 0.19132113456726074, 0.1926441639661789, 0.19173644483089447, 0.19065025448799133, 0.18956436216831207, 0.19030176103115082, 0.189430370926857, 0.1912926733493805, 0.1909407526254654, 0.19297128915786743, 0.18964733183383942, 0.19056230783462524, 0.18973079323768616, 0.19118492305278778, 0.19021442532539368, 0.19001871347427368, 0.19138175249099731, 0.19126702845096588, 0.1891465187072754, 0.19009067118167877, 0.1899302899837494, 0.18938562273979187, 0.1870531290769577, 0.1902739554643631, 0.1906997412443161, 0.19005854427814484, 0.18925151228904724, 0.1884770393371582, 0.1885879784822464, 0.1888074427843094, 0.1892198920249939, 0.19134432077407837, 0.18877165019512177, 0.18811377882957458, 0.19021888077259064, 0.18881410360336304, 0.19011159241199493, 0.18743965029716492, 0.18772952258586884, 0.18921628594398499, 0.18717312812805176, 0.18866075575351715, 0.1881890743970871, 0.18805328011512756, 0.19008319079875946, 0.1890760213136673, 0.18981635570526123, 0.1854262799024582, 0.18715685606002808, 0.18933142721652985, 0.18766890466213226, 0.18701356649398804, 0.18712572753429413, 0.18796931207180023, 0.18869011104106903, 0.18854260444641113, 0.18529966473579407, 0.18591715395450592, 0.18998420238494873]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:25 2016", "state": "available"}], "summary": "7e9386f2a4019ccfc46606c803344200"}