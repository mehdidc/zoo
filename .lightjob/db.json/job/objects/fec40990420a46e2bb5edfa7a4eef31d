{"content": {"hp_model": {"f0": 32, "f1": 16, "f2": 64, "f3": 64, "nonlin": "very_leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.013122131400248888, 0.009619895684114772, 0.013722797973364124, 0.012198135516873465, 0.009429012463347692, 0.01125101614707471, 0.004901276628479883, 0.005400407149701708, 0.0036496190888325353, 0.007822732458109598, 0.006654172692649756, 0.007202877306805178, 0.009586179199176335, 0.009687717905023544, 0.007746750380626586, 0.011260413373869111, 0.010211230392969983, 0.010177851395247954, 0.011262524748768548, 0.011141501314480235, 0.010806585527835954, 0.011305683779459548, 0.010826757254576443, 0.011287340924933336, 0.01098561882596116, 0.01051329196769138, 0.010804422792038932, 0.011010814032182235, 0.011107158191105377, 0.010561613249020772, 0.011013781645613993, 0.011239450920380981, 0.011315109776155172, 0.01150738942818914, 0.01168598199552625, 0.011936096340390896, 0.011922664597957705, 0.011498625872065139, 0.011185619491243597, 0.010606806315679, 0.010538092200958221, 0.010300601604794346, 0.010448078191478341, 0.010375320082732988, 0.0105678888654679, 0.010106608328653977, 0.009995174353859528, 0.010099083187598619, 0.01014729506096405, 0.01014729506096405, 0.009992282723088004, 0.010016114574840194, 0.009838905875946314, 0.009795133937429203, 0.01013728431648585, 0.01013728431648585, 0.010263008396053224], "moving_avg_accuracy_train": [0.05823104481012365, 0.11987656704215113, 0.18501004085773112, 0.24735662380822598, 0.3061047264813901, 0.361270074881147, 0.4124089843884642, 0.4597289666366794, 0.5034444315397796, 0.5442669643513331, 0.5823278923567221, 0.6181079891318011, 0.6518307595996196, 0.6831439006766177, 0.7121162782411542, 0.7388074743373045, 0.7632340906678783, 0.7855272901570614, 0.8058074085366118, 0.8241827479651119, 0.8408554120817144, 0.8559445151437995, 0.8695642354294381, 0.8818684506138845, 0.8929771215120291, 0.902986551064407, 0.9120113137032136, 0.9201452258221872, 0.9274913233661682, 0.9341144368997987, 0.9400915151217327, 0.9454894867119497, 0.9503546365895735, 0.954733271479435, 0.9586786931779293, 0.9622411984506218, 0.9654590789400926, 0.9683644719758544, 0.9709886263032782, 0.9733526903467691, 0.9754803479859109, 0.9774022153075671, 0.9791365461946768, 0.9807020942906945, 0.982108762428301, 0.9833770889009563, 0.9845255581727747, 0.9855615056662207, 0.9864985087079412, 0.9873441365942991, 0.9881075268408308, 0.9887992283603284, 0.9894217597278762, 0.9899866882562882, 0.9904951239318591, 0.9909527160398729, 0.9913645489370854], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 213755005, "moving_var_accuracy_train": [0.030517691217107668, 0.061667455796731475, 0.09368203491862195, 0.1192976990771862, 0.13842998527873726, 0.15197592772746257, 0.1603150275450945, 0.16443615127032768, 0.1651919129885426, 0.1636710343560417, 0.1603416390861202, 0.15582941310461434, 0.15048149902637942, 0.1442579643607134, 0.13738675588032057, 0.13005985983367688, 0.12242381011855864, 0.11465430979788337, 0.10689042763149224, 0.09924026276035634, 0.09181803604302612, 0.08468536171968755, 0.07788629657365008, 0.07146021031803153, 0.06542481240833795, 0.05978402928718027, 0.05453864342464344, 0.04968022381941176, 0.045197887779601736, 0.0410728896975558, 0.03728712990444029, 0.033820659789595364, 0.030651620960621514, 0.027759010856047768, 0.025123206941853538, 0.02272510924202984, 0.020545791111427514, 0.018567183778515045, 0.016772441074070773, 0.015145496155879236, 0.013671688883555902, 0.012337762161218763, 0.01113105707773073, 0.01004000983752616, 0.009053817291017759, 0.008162913430287128, 0.007358492922273215, 0.0066323023149284874, 0.00597697385573738, 0.005385712248863318, 0.004852385905993484, 0.004371453374322813, 0.003937795944622761, 0.0035468886483404093, 0.0031945263450321075, 0.0028769582253647453, 0.0025907888598453084], "duration": 19726.440824, "accuracy_train": [0.5823104481012367, 0.6746862671303987, 0.7712113051979512, 0.8084758703626799, 0.834837650539867, 0.857758210478959, 0.872659169954319, 0.8856088068706165, 0.8968836156676817, 0.9116697596553157, 0.9248762444052234, 0.9401288601075121, 0.9553356938099853, 0.9649621703696014, 0.9728676763219823, 0.9790282392026578, 0.9830736376430418, 0.9861660855597084, 0.9883284739525655, 0.9895608028216132, 0.990909389131137, 0.9917464427025655, 0.9921417180001846, 0.9926063872739018, 0.9929551595953304, 0.9930714170358066, 0.9932341774524732, 0.9933504348929494, 0.9936062012619971, 0.9937224587024732, 0.99388521911914, 0.9940712310239018, 0.9941409854881875, 0.9941409854881875, 0.994187488464378, 0.9943037459048542, 0.9944200033453304, 0.9945130092977114, 0.9946060152500923, 0.9946292667381875, 0.9946292667381875, 0.9946990212024732, 0.9947455241786637, 0.9947920271548542, 0.9947687756667589, 0.9947920271548542, 0.99486178161914, 0.9948850331072352, 0.9949315360834257, 0.9949547875715209, 0.9949780390596161, 0.9950245420358066, 0.9950245420358066, 0.9950710450119971, 0.9950710450119971, 0.9950710450119971, 0.9950710450119971], "end": "2016-01-24 14:46:35.778000", "learning_rate_per_epoch": [0.007112120743840933, 0.0035560603719204664, 0.002370706992223859, 0.0017780301859602332, 0.0014224241022020578, 0.0011853534961119294, 0.0010160172823816538, 0.0008890150929801166, 0.000790235644672066, 0.0007112120511010289, 0.0006465564365498722, 0.0005926767480559647, 0.0005470862379297614, 0.0005080086411908269, 0.0004741413868032396, 0.0004445075464900583, 0.00041836005402728915, 0.000395117822336033, 0.0003743221459444612, 0.00035560602555051446, 0.00033867242746055126, 0.0003232782182749361, 0.00030922263977117836, 0.00029633837402798235, 0.00028448482044041157, 0.0002735431189648807, 0.0002634118718560785, 0.00025400432059541345, 0.0002452455519232899, 0.0002370706934016198, 0.00022942325449548662, 0.00022225377324502915, 0.00021551881218329072, 0.00020918002701364458, 0.0002032034535659477, 0.0001975589111680165, 0.00019221947877667844, 0.0001871610729722306, 0.00018236206960864365, 0.00017780301277525723, 0.00017346635286230594, 0.00016933621373027563, 0.00016539815987925977, 0.00016163910913746804, 0.0001580471289344132, 0.00015461131988558918, 0.00015132171392906457, 0.00014816918701399118, 0.000145145328133367, 0.00014224241022020578, 0.0001394533464917913, 0.00013677155948244035, 0.00013419095193967223, 0.00013170593592803925, 0.00012931128730997443, 0.00012700216029770672, 0.00012477404379751533], "accuracy_valid": [0.5638957195971386, 0.6479345114834337, 0.7246784991528614, 0.7464893754706325, 0.7581684158509037, 0.7642822265625, 0.7724418180534638, 0.7686473432793675, 0.7651881941829819, 0.7618511153990963, 0.7620040709713856, 0.7670501341302711, 0.7675281202936747, 0.7701930769954819, 0.7721564970820783, 0.7693385848079819, 0.7710475691829819, 0.7705284026731928, 0.7701724868222892, 0.7691856292356928, 0.7682090667356928, 0.7694297698606928, 0.7696842055722892, 0.7690738540097892, 0.7688297133847892, 0.7679752211972892, 0.7680972915097892, 0.7673442794615963, 0.7674766448606928, 0.7668662932981928, 0.7672427993222892, 0.7669986586972892, 0.7666324477597892, 0.7667545180722892, 0.7672427993222892, 0.7673648696347892, 0.7674869399472892, 0.7673648696347892, 0.7672427993222892, 0.7671207290097892, 0.7672427993222892, 0.7676193053463856, 0.7673751647213856, 0.7674972350338856, 0.7677413756588856, 0.7677413756588856, 0.7676193053463856, 0.7672427993222892, 0.7671207290097892, 0.7671207290097892, 0.7672427993222892, 0.7672427993222892, 0.7676193053463856, 0.7677413756588856, 0.7676193053463856, 0.7676193053463856, 0.7672427993222892], "accuracy_test": 0.5287527901785715, "start": "2016-01-24 09:17:49.337000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0], "accuracy_train_last": 0.9950710450119971, "batch_size_eval": 1024, "accuracy_train_std": [0.017079698920939666, 0.02277607563396071, 0.022458981422738077, 0.02350118490523157, 0.024748476917889492, 0.025324914630873527, 0.025705160139520226, 0.025836992391110265, 0.02592456197049403, 0.025056198623980456, 0.02236833107728227, 0.019177881268472315, 0.015221361802278816, 0.01193460537240882, 0.009848993973931186, 0.008465220008985344, 0.006478065218321807, 0.005647169481999779, 0.005066304116775079, 0.004638101670185358, 0.004190358702015309, 0.003587112828226827, 0.0033224889316617805, 0.0031839485879027866, 0.0030557622406710795, 0.0030204251800502947, 0.0028148160122071623, 0.002829647200093943, 0.0027643792212549007, 0.002680461690583268, 0.0024863398253831956, 0.00248935519857246, 0.0024396583477720095, 0.002476607579855496, 0.002392210585647053, 0.0024107658321558117, 0.002442274933200164, 0.002448451586077917, 0.002366242952628996, 0.002347084032031961, 0.002413854778610654, 0.0024034071342423215, 0.002439125759149258, 0.0024271202520569097, 0.0024931604706883268, 0.0025189369473658757, 0.00244939367967896, 0.0024470715904129432, 0.0024417566462536892, 0.002429433810006169, 0.0024168244643500343, 0.0023812113192999986, 0.0023812113192999986, 0.0022653177807466242, 0.0022653177807466242, 0.0022653177807466242, 0.0022653177807466242], "accuracy_test_std": 0.01452588107532651, "error_valid": [0.4361042804028614, 0.35206548851656627, 0.2753215008471386, 0.25351062452936746, 0.24183158414909633, 0.2357177734375, 0.2275581819465362, 0.23135265672063254, 0.2348118058170181, 0.23814888460090367, 0.23799592902861444, 0.23294986586972888, 0.23247187970632532, 0.2298069230045181, 0.22784350291792166, 0.2306614151920181, 0.2289524308170181, 0.22947159732680722, 0.22982751317771077, 0.23081437076430722, 0.23179093326430722, 0.23057023013930722, 0.23031579442771077, 0.23092614599021077, 0.23117028661521077, 0.23202477880271077, 0.23190270849021077, 0.23265572053840367, 0.23252335513930722, 0.23313370670180722, 0.23275720067771077, 0.23300134130271077, 0.23336755224021077, 0.23324548192771077, 0.23275720067771077, 0.23263513036521077, 0.23251306005271077, 0.23263513036521077, 0.23275720067771077, 0.23287927099021077, 0.23275720067771077, 0.23238069465361444, 0.23262483527861444, 0.23250276496611444, 0.23225862434111444, 0.23225862434111444, 0.23238069465361444, 0.23275720067771077, 0.23287927099021077, 0.23287927099021077, 0.23275720067771077, 0.23275720067771077, 0.23238069465361444, 0.23225862434111444, 0.23238069465361444, 0.23238069465361444, 0.23275720067771077], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-3, 3], "momentum": 0.9385938599382826, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.00711212073577835, "patience_threshold": 1, "do_flip": true, "batch_size": 32, "optimization": "nesterov_momentum", "nb_data_augmentation": 0, "learning_rate_decay_method": "lin", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 1.125300440770964e-09, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.04595078790253829}, "accuracy_valid_max": 0.7724418180534638, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-3, 3)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.7672427993222892, "loss_train": [1.3449543714523315, 0.9590161442756653, 0.7291138768196106, 0.6091649532318115, 0.5233039259910583, 0.45010924339294434, 0.38193804025650024, 0.31718358397483826, 0.2542671263217926, 0.19537851214408875, 0.14292392134666443, 0.10076285153627396, 0.06886748224496841, 0.04692193493247032, 0.03205467015504837, 0.022765133529901505, 0.01644255593419075, 0.012563434429466724, 0.010110043920576572, 0.008454660885035992, 0.007273590192198753, 0.006400804501026869, 0.00573354959487915, 0.005210338160395622, 0.004789280239492655, 0.004441515542566776, 0.004149132873862982, 0.00390032771974802, 0.0036857868544757366, 0.0034990087151527405, 0.003334649605676532, 0.003189472248777747, 0.0030599769670516253, 0.002943804021924734, 0.002838971558958292, 0.0027437806129455566, 0.002657022327184677, 0.0025775462854653597, 0.0025046365335583687, 0.0024374721106141806, 0.0023752690758556128, 0.00231737969443202, 0.002263497095555067, 0.0022132499143481255, 0.0021662283688783646, 0.0021221500355750322, 0.0020806482061743736, 0.0020416362676769495, 0.00200478988699615, 0.001969982637092471, 0.0019369934452697635, 0.0019057311583310366, 0.0018760564271360636, 0.001847800682298839, 0.0018208468100056052, 0.001795185380615294, 0.0017706721555441618], "accuracy_train_first": 0.5823104481012367, "model": "residualv3", "loss_std": [0.3105595111846924, 0.2132296860218048, 0.1877339482307434, 0.173958882689476, 0.16311103105545044, 0.15272390842437744, 0.14162199199199677, 0.129028782248497, 0.11354038119316101, 0.09511201828718185, 0.07552281767129898, 0.05718201398849487, 0.040489327162504196, 0.02843296155333519, 0.018786605447530746, 0.012019535526633263, 0.0077439481392502785, 0.005429706070572138, 0.004118343349546194, 0.0032948844600468874, 0.0027364289853721857, 0.002347733359783888, 0.002062014536932111, 0.0018484927713871002, 0.001680899178609252, 0.0015456628752872348, 0.001433099969290197, 0.00133804720826447, 0.0012565396027639508, 0.0011860933154821396, 0.0011244224151596427, 0.0010699850972741842, 0.00102174689527601, 0.0009786164155229926, 0.0009398959809914231, 0.000904786866158247, 0.0008728537941351533, 0.000843729532789439, 0.0008171097724698484, 0.0007927214028313756, 0.0007701790891587734, 0.0007492319564335048, 0.0007297879783436656, 0.0007116966880857944, 0.000694812391884625, 0.0006790243205614388, 0.0006642059306614101, 0.0006502812611870468, 0.0006371773197315633, 0.0006248235004022717, 0.0006131374393589795, 0.0006020592409186065, 0.0005915838992223144, 0.0005816256743855774, 0.0005721495253965259, 0.0005631591775454581, 0.0005545537569560111]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:11 2016", "state": "available"}], "summary": "9db285e3090ec453ff39fb8682fbf651"}