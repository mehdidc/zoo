{"content": {"hp_model": {"f0": 32, "f1": 64, "f2": 16, "f3": 16, "nonlin": "rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.6734585762023926, 1.3312147855758667, 1.1661040782928467, 1.0543948411941528, 0.982499361038208, 0.9270297884941101, 0.8782433867454529, 0.8398540616035461, 0.8063063621520996, 0.7735962867736816, 0.7474043369293213, 0.7216177582740784, 0.6984627842903137, 0.6790080666542053, 0.6601079702377319, 0.6451254487037659, 0.6296446919441223, 0.6143536567687988, 0.597404956817627, 0.585243284702301, 0.5709192156791687, 0.5573106408119202, 0.5449498891830444, 0.5352035164833069, 0.5219146609306335, 0.510545015335083, 0.4968682527542114, 0.4884883165359497, 0.4808078110218048, 0.47013434767723083, 0.4600962996482849, 0.4520735442638397, 0.4434812664985657, 0.4344622492790222, 0.4257853031158447, 0.41977325081825256, 0.40958237648010254, 0.40401700139045715, 0.3965625762939453, 0.3914947807788849, 0.38559865951538086, 0.37801817059516907, 0.37230607867240906, 0.3662503957748413, 0.362336665391922, 0.35476022958755493, 0.34931936860084534, 0.34435126185417175, 0.34283334016799927, 0.33447518944740295, 0.3301801383495331, 0.32904472947120667, 0.32446253299713135, 0.32000210881233215, 0.3170892894268036, 0.3121272623538971, 0.31221815943717957, 0.30414265394210815, 0.304219514131546, 0.2974410951137543, 0.29827845096588135, 0.29606255888938904, 0.291782408952713, 0.2903922200202942, 0.2861039638519287, 0.2842710018157959, 0.2819293439388275, 0.2816944420337677, 0.27928319573402405, 0.2806210517883301, 0.2756789028644562, 0.27462828159332275, 0.27242863178253174, 0.27176433801651, 0.2711876630783081, 0.26733458042144775, 0.2675975263118744, 0.26690778136253357, 0.2629324197769165, 0.2629900872707367, 0.262690931558609, 0.2609473764896393, 0.26405230164527893, 0.26100480556488037, 0.260602742433548, 0.25850239396095276, 0.257129043340683, 0.2567274570465088, 0.2587706744670868, 0.2538490891456604, 0.2567317485809326, 0.2561377286911011, 0.25575533509254456, 0.25465595722198486, 0.25268760323524475, 0.25334152579307556, 0.2517309784889221, 0.2522522211074829, 0.2513079345226288, 0.25044143199920654, 0.2513018250465393, 0.2504500150680542, 0.25120702385902405, 0.24954119324684143, 0.25002822279930115, 0.24914763867855072, 0.25116777420043945, 0.25126707553863525, 0.24838221073150635, 0.25010794401168823, 0.25063154101371765, 0.2492848038673401, 0.24880313873291016, 0.24875175952911377, 0.24901948869228363, 0.24728456139564514, 0.24566888809204102, 0.24976404011249542, 0.2458122968673706, 0.24797497689723969, 0.2484026998281479, 0.24867229163646698, 0.24798040091991425, 0.2468579113483429, 0.24690492451190948], "moving_avg_accuracy_train": [0.043082790438122916, 0.09354595106877536, 0.14742192132273207, 0.19730091378348213, 0.2478617111382919, 0.2962282183075007, 0.3410835177276735, 0.38389636972590097, 0.4250406107121979, 0.46334650171046865, 0.49832178270059746, 0.5307483044524554, 0.5604505019695576, 0.5878405689456824, 0.6135886668764648, 0.6374456569106253, 0.6595864105544188, 0.6799198096314059, 0.698587242312599, 0.7159808807209201, 0.7324859074109635, 0.7469964896522868, 0.7609299812313088, 0.773825979566742, 0.7858903242376518, 0.7970782974259668, 0.8076776793216591, 0.8170543626111156, 0.8261281071478076, 0.8340849975450608, 0.8413603835359866, 0.8485659956991839, 0.85519756706988, 0.861440420960668, 0.867293685296864, 0.8727105408697062, 0.8777414598066836, 0.8824482151618402, 0.8870587160374521, 0.8916707993409605, 0.8957240901617556, 0.8995719426004526, 0.9033861793631556, 0.9069956677102933, 0.910002355697708, 0.9130757483982861, 0.9159511559204915, 0.9185227105999909, 0.9210393256603314, 0.9235903085670281, 0.925911733771141, 0.9280406881798796, 0.9302960746786598, 0.9323352231228001, 0.9340960159118029, 0.9359667587742956, 0.9377969477743576, 0.9394882957017945, 0.941164004706735, 0.9426674204159249, 0.9440624553792235, 0.9453853801128497, 0.946829345446895, 0.9481266251475451, 0.9492965380757584, 0.9503239191230645, 0.9514112143358503, 0.952406092117843, 0.9533107827168745, 0.9542272387059845, 0.9550661802331342, 0.9558536715444461, 0.9565462097805977, 0.9572531635014582, 0.9580149798859469, 0.9588215584189006, 0.9594498228485591, 0.9600268865792992, 0.9605089694583756, 0.9610126706114674, 0.9614868198420796, 0.9619811637092006, 0.9623957741574481, 0.9628246189858432, 0.9631919420921038, 0.9634505974699182, 0.9637832605623043, 0.9641362078657084, 0.9644352952971147, 0.9647393151687044, 0.9649339861400675, 0.9651555487952099, 0.9654038193586567, 0.9655296066157589, 0.9656451763447791, 0.96593749010565, 0.9661541055630621, 0.9663304222354381, 0.9664449654620143, 0.9666131585325994, 0.9667017172294503, 0.9668279230328065, 0.9668718258891789, 0.9669275784527619, 0.9671149395397485, 0.9672370254930273, 0.9673934418759873, 0.9674179591801753, 0.9675143934670304, 0.9675500310513905, 0.9675798157773238, 0.9676437883627973, 0.9677106642849616, 0.9677220605387282, 0.9678904272861658, 0.9679303141671838, 0.9679964392946238, 0.9680187855771861, 0.9680458726779209, 0.968114428895963, 0.968136601962439, 0.9682263121865531, 0.9682395860239609, 0.9682538936752562, 0.9681970160971363], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.042204413356551194, 0.09163813947195029, 0.14395349415239078, 0.19288263410085651, 0.2413439824189335, 0.2880549284447359, 0.3311334070328677, 0.37206071133146346, 0.4114669978319767, 0.44823380755555614, 0.4813179063274855, 0.5118421950904448, 0.5398858735012045, 0.565391679741069, 0.5893296449089048, 0.6116988912143245, 0.6316917880021542, 0.6501910309357489, 0.6667803439283939, 0.6825540402866841, 0.6969731820976242, 0.7097234766043526, 0.7217329095820951, 0.7331151297308134, 0.7430610115713615, 0.7525077930384724, 0.7613933147982546, 0.7693598403402665, 0.7770749116996886, 0.7833317936471895, 0.7890637321759193, 0.7949356322075743, 0.7999801725730368, 0.8048407302404318, 0.8092183206670663, 0.8134939189467452, 0.816967510391679, 0.8201669848796196, 0.8230231273649258, 0.8260697298204512, 0.828805642051132, 0.8312342834183081, 0.833237984688676, 0.8349579256305765, 0.8367438360513291, 0.8383451254507142, 0.8400132190332783, 0.8411400562508089, 0.842155239255246, 0.8428531483587124, 0.8442128059908381, 0.845260451878953, 0.8464902719489643, 0.8473122302585859, 0.8477549059526068, 0.8481909646796353, 0.8487797595426205, 0.8494735138688554, 0.8501537803754187, 0.8508047003423949, 0.8513071381112427, 0.8517827166570462, 0.8524589960079079, 0.8529414590765448, 0.853363468807068, 0.8538419633231986, 0.8543479095925354, 0.8548713558791402, 0.8553801081394943, 0.8557901865574725, 0.8560992514860626, 0.8561800384044744, 0.8565233603358644, 0.8568567641366153, 0.8573043414409507, 0.8575952387162833, 0.8578448392328326, 0.8577256238054078, 0.857952037799114, 0.85786181213479, 0.8576920712921693, 0.8578099182386301, 0.8578793593966948, 0.8579917140726127, 0.8579809109823695, 0.8580210458348103, 0.857910682827007, 0.857957840494984, 0.8580145484447326, 0.858039112519687, 0.8580378356333056, 0.8579878583105625, 0.8580018548590242, 0.8580032742300495, 0.858017788203882, 0.8580664423654215, 0.8579993383208974, 0.858124109166895, 0.8581143326157928, 0.8581411253048913, 0.8581662682337395, 0.8582011039009528, 0.8583667333451949, 0.8583438718988531, 0.8584850765293743, 0.858449351255955, 0.8584680856521969, 0.8583364032164953, 0.8583786394479331, 0.8584034155163175, 0.8583270282192037, 0.8583956455215304, 0.8585051997099646, 0.8585661478771459, 0.858596587165109, 0.858650455604095, 0.8586348135169536, 0.8586350016870956, 0.8587226792762926, 0.8589358664503199, 0.8589558069607849, 0.8590469956077034, 0.8588818362389511, 0.8587576068695741, 0.8587333086732041], "moving_var_accuracy_train": [0.016705141487416943, 0.037953402566190536, 0.06028164384681854, 0.07664470446223273, 0.0919877820783967, 0.10384287501229919, 0.11156656848572946, 0.11690637430315569, 0.12045137396988641, 0.1216123081394404, 0.12046050984854259, 0.11787777268000178, 0.11402998024810616, 0.10937892414390493, 0.10440771265299292, 0.09908934514910392, 0.09359232738142993, 0.08795411870550311, 0.08229496422111468, 0.07678831571271803, 0.07156122729579766, 0.06630011753905778, 0.061417385473396084, 0.05677240788366393, 0.05240510280634412, 0.04829112922227179, 0.044473138369181195, 0.04081712423786003, 0.03747640737332837, 0.034298575579140636, 0.03134509919307921, 0.028677876893589027, 0.026205888853831866, 0.023936058990764213, 0.02185079942219225, 0.019929800398646308, 0.01816461166693561, 0.016547532414001715, 0.015084089637517702, 0.013767122485352438, 0.012538272735118669, 0.011417699177116686, 0.010406864878142612, 0.009483434045481461, 0.008616452194816293, 0.007839818659562361, 0.007130248509374921, 0.006476739699664326, 0.005886065891955288, 0.005356026926872085, 0.004868925368989496, 0.004422824853960934, 0.004026323282894761, 0.0036611140920004425, 0.0033229062040126345, 0.003022112693329476, 0.0027500477499800644, 0.002500788895286863, 0.0022759820117813257, 0.002068726139754944, 0.0018793686287188797, 0.001707182934504551, 0.0015552299640274197, 0.001414853379220147, 0.0012856863076345394, 0.0011666172832183583, 0.0010605954528142452, 0.0009634439437427445, 0.0008744657350882538, 0.0007945781857992066, 0.000721454773193074, 0.0006548905789622923, 0.00059371800394285, 0.0005388442556195115, 0.0004901831078906397, 0.0004470199174699737, 0.0004058703714651431, 0.0003682803572626515, 0.0003335439566570733, 0.00030247299465600125, 0.0002742490526264124, 0.0002490235300944119, 0.0002256682934991348, 0.00020475663513079223, 0.0001854953079972497, 0.00016754790063777528, 0.00015178909317132113, 0.00013773133004501116, 0.0001247632766651368, 0.00011311880173951615, 0.00010214799264938809, 9.237500347583291e-05, 8.369224758231775e-05, 7.546542473052965e-05, 6.803908951786903e-05, 6.200420657923265e-05, 5.6226086228818085e-05, 5.088326572655584e-05, 4.591302051069054e-05, 4.15763186405574e-05, 3.7489270561592844e-05, 3.388369464864058e-05, 3.051267233095539e-05, 2.7489380232974576e-05, 2.505637980192846e-05, 2.268488664162744e-05, 2.0636592741189594e-05, 1.857834335091241e-05, 1.6804205160952467e-05, 1.5135214981628418e-05, 1.3629677652555873e-05, 1.2303542312529889e-05, 1.111343958196489e-05, 1.000326449516763e-05, 9.258064300435385e-06, 8.346576539887921e-06, 7.551271678209707e-06, 6.800638717487946e-06, 6.1271782449750525e-06, 5.55676001576776e-06, 5.005508818083547e-06, 4.577389255070713e-06, 4.121236082399403e-06, 3.7109548541297566e-06, 3.3689748987518807e-06], "duration": 132986.204239, "accuracy_train": [0.43082790438122925, 0.5477143967446475, 0.6323056536083426, 0.6462118459302326, 0.7029088873315799, 0.7315267828303802, 0.7447812125092286, 0.7692120377099483, 0.7953387795888703, 0.8080995206949059, 0.8130993116117571, 0.8225870002191769, 0.8277702796234773, 0.8343511717308048, 0.8453215482535069, 0.8521585672180694, 0.8588531933485604, 0.8629204013242894, 0.866594136443337, 0.8725236263958103, 0.8810311476213547, 0.8775917298241971, 0.8863314054425065, 0.8898899645856404, 0.8944694262758398, 0.8977700561208011, 0.9030721163828904, 0.9014445122162238, 0.9077918079780363, 0.9056970111203396, 0.906838857454319, 0.9134165051679586, 0.9148817094061462, 0.9176261059777593, 0.9199730643226283, 0.9214622410252861, 0.9230197302394795, 0.9248090133582503, 0.9285532239179586, 0.933179549072536, 0.932203707548911, 0.9342026145487264, 0.9377143102274824, 0.939481062834533, 0.9370625475844407, 0.9407362827034883, 0.9418298236203396, 0.9416667027154854, 0.943688861203396, 0.9465491547272978, 0.946804560608158, 0.9472012778585271, 0.9505945531676817, 0.9506875591200628, 0.9499431510128276, 0.9528034445367294, 0.9542686487749169, 0.9547104270487264, 0.9562453857511997, 0.9561981617986341, 0.956617770048911, 0.9572917027154854, 0.9598250334533037, 0.959802142453396, 0.9598257544296788, 0.9595703485488187, 0.9611968712509228, 0.9613599921557769, 0.961452998108158, 0.9624753426079733, 0.9626166539774824, 0.9629410933462532, 0.9627790539059615, 0.9636157469892026, 0.9648713273463455, 0.9660807652154854, 0.9651042027154854, 0.9652204601559615, 0.9648477153700628, 0.9655459809892949, 0.9657541629175894, 0.966430258513289, 0.9661272681916758, 0.966684222441399, 0.9664978500484496, 0.9657784958702473, 0.9667772283937799, 0.9673127335963455, 0.9671270821797711, 0.9674754940130121, 0.9666860248823367, 0.9671496126914912, 0.9676382544296788, 0.9666616919296788, 0.9666853039059615, 0.9685683139534883, 0.9681036446797711, 0.9679172722868217, 0.9674758545011997, 0.9681268961678663, 0.9674987455011074, 0.9679637752630121, 0.9672669515965301, 0.9674293515250092, 0.9688011893226283, 0.968335799072536, 0.9688011893226283, 0.9676386149178663, 0.9683823020487264, 0.9678707693106312, 0.9678478783107235, 0.9682195416320598, 0.9683125475844407, 0.9678246268226283, 0.9694057280131044, 0.9682892960963455, 0.9685915654415835, 0.9682199021202473, 0.968289656584533, 0.9687314348583426, 0.9683361595607235, 0.9690337042035806, 0.9683590505606312, 0.968382662536914, 0.9676851178940569], "end": "2016-02-02 23:55:40.934000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0], "moving_var_accuracy_valid": [0.016030912560935737, 0.03642106080371278, 0.0574110217414041, 0.0732164661921326, 0.0870313401001732, 0.09796541839778472, 0.10487067441521944, 0.10945900510804618, 0.11248880333908638, 0.1134061076804268, 0.11191651523634111, 0.1091104535530677, 0.10527743928701594, 0.10060461072582394, 0.09570138524063027, 0.09063469533902006, 0.08516866910283731, 0.079731800094599, 0.07423546783525056, 0.06905120652295732, 0.06401729072573757, 0.059078691743238626, 0.05446886089293675, 0.050187969223268056, 0.046059457391216564, 0.042256686772881046, 0.03874159056808502, 0.03543862127418027, 0.032430460081491023, 0.0295397512186866, 0.02688147217049218, 0.024503637843278716, 0.022282300546439867, 0.020266695679338545, 0.018412495792894952, 0.016735772879448198, 0.015170788129240241, 0.013745839049307058, 0.012444673093443687, 0.011283741862797445, 0.010222734618123598, 0.009253545846324462, 0.008364324630719884, 0.007554515939240524, 0.006827769629595046, 0.006168069816300792, 0.0055763056604724364, 0.005030102953458501, 0.004536368026905134, 0.004087114918264932, 0.003695041446327819, 0.003335415358856995, 0.0030154859396127197, 0.0027200178848162523, 0.0024497797522653195, 0.002206513101959547, 0.001988981906279693, 0.001794415371238251, 0.0016191386967939919, 0.0014610380983452675, 0.0013172062819148242, 0.0011875212283023984, 0.0010728852893157753, 0.0009676916958975848, 0.000872525356221733, 0.0007873334336172627, 0.0007109039249026386, 0.0006422794965470188, 0.0005803810066540551, 0.0005238563847686737, 0.00047233043646256593, 0.00042515613155198767, 0.00038370134793394854, 0.0003463316359897501, 0.00031350140138098085, 0.0002829128522660464, 0.0002551822708001972, 0.00022979195458340228, 0.000207274128793976, 0.00018661998194910285, 0.0001682172913370744, 0.00015152055332847816, 0.00013641189666553066, 0.0001228843191577831, 0.00011059693760283402, 9.95517410999746e-05, 8.970618693139969e-05, 8.0755582849101e-05, 7.270896668827309e-05, 6.544350056345108e-05, 5.889916518105546e-05, 5.303172825804713e-05, 4.7730318562562016e-05, 4.295730483783278e-05, 3.866347025297717e-05, 3.481842827459547e-05, 3.137711202225933e-05, 2.8379510696132038e-05, 2.5542419855081908e-05, 2.2994638503275846e-05, 2.0700864154787863e-05, 1.8641699452700904e-05, 1.702442752263024e-05, 1.5326688581926757e-05, 1.3973468452860051e-05, 1.2587608264022041e-05, 1.1332006236042755e-05, 1.035486798728928e-05, 9.335436281774974e-06, 8.407417335678774e-06, 7.619190774553889e-06, 6.899646704705733e-06, 6.3177011160666716e-06, 5.7193631162047874e-06, 5.155765756849628e-06, 4.666305459633843e-06, 4.201876987681721e-06, 3.7816896075855702e-06, 3.4727068836537014e-06, 3.534475135816303e-06, 3.1846062378531126e-06, 2.940983938009234e-06, 2.8923840979881307e-06, 2.7420421141316775e-06, 2.4731515238400537e-06], "accuracy_test": 0.8408462213010204, "start": "2016-02-01 10:59:14.730000", "learning_rate_per_epoch": [0.004632617346942425, 0.004352298099547625, 0.004088941030204296, 0.0038415193557739258, 0.0036090691573917866, 0.0033906844910234213, 0.003185514360666275, 0.0029927589930593967, 0.002811667276546359, 0.002641533501446247, 0.0024816945660859346, 0.002331527415663004, 0.0021904469467699528, 0.0020579032134264708, 0.00193337956443429, 0.0018163908971473575, 0.0017064812127500772, 0.0016032221028581262, 0.0015062112361192703, 0.0014150704955682158, 0.0013294446980580688, 0.0012490000808611512, 0.0011734231375157833, 0.0011024193372577429, 0.0010357119608670473, 0.000973041111137718, 0.0009141624323092401, 0.0008588465279899538, 0.0008068777970038354, 0.0007580536766909063, 0.0007121838862076402, 0.0006690896698273718, 0.0006286030984483659, 0.000590566371101886, 0.0005548312328755856, 0.0005212584510445595, 0.0004897171165794134, 0.00046008435310795903, 0.000432244676630944, 0.00040608958806842566, 0.000381517136702314, 0.00035843157093040645, 0.00033674290170893073, 0.0003163666115142405, 0.00029722327599301934, 0.00027923830202780664, 0.0002623416075948626, 0.0002464673307258636, 0.00023155361122917384, 0.00021754231420345604, 0.00020437884086277336, 0.0001920118957059458, 0.00018039326823782176, 0.0001694776874501258, 0.0001592226035427302, 0.00014958805695641786, 0.00014053648919798434, 0.00013203264097683132, 0.00012404334847815335, 0.00011653749243123457, 0.00010948581621050835, 0.00010286083852406591, 9.663673699833453e-05, 9.07892535906285e-05, 8.529560000170022e-05, 8.013437036424875e-05, 7.528544665547088e-05, 7.072992593748495e-05, 6.645006214966998e-05, 6.242917152121663e-05, 5.8651585277402773e-05, 5.51025805179961e-05, 5.1768325647572055e-05, 4.86358258058317e-05, 4.5692875573877245e-05, 4.292800076655112e-05, 4.033042932860553e-05, 3.789003676502034e-05, 3.559731339919381e-05, 3.344332071719691e-05, 3.141966590192169e-05, 2.951846363430377e-05, 2.773230335151311e-05, 2.6054223781102337e-05, 2.44776838371763e-05, 2.2996540792519227e-05, 2.160502117476426e-05, 2.0297702576499432e-05, 1.906949000840541e-05, 1.791559589037206e-05, 1.683152368059382e-05, 1.581304968567565e-05, 1.4856203051749617e-05, 1.3957254850538447e-05, 1.3112702617945615e-05, 1.2319253983150702e-05, 1.157381666416768e-05, 1.0873485734919086e-05, 1.02155318018049e-05, 9.597390999260824e-06, 9.01665316632716e-06, 8.471056389680598e-06, 7.958473361213692e-06, 7.476906830561347e-06, 7.024479600659106e-06, 6.5994286160275806e-06, 6.200097686814843e-06, 5.824930212838808e-06, 5.472464181366377e-06, 5.14132580065052e-06, 4.830224497709423e-06, 4.5379479161056224e-06, 4.263356913725147e-06, 4.005381470051361e-06, 3.763016138691455e-06, 3.535316182023962e-06, 3.3213943879673025e-06, 3.120416977253626e-06, 2.931600647571031e-06, 2.754209617705783e-06, 2.5875526716845343e-06, 2.4309799755428685e-06, 2.2838814857095713e-06, 2.1456839931488503e-06, 2.0158488496235805e-06], "accuracy_train_first": 0.43082790438122925, "accuracy_train_last": 0.9676851178940569, "batch_size_eval": 1024, "accuracy_train_std": [0.017288890832211322, 0.018303637640814788, 0.02290768391983203, 0.020290688168993615, 0.019953560286762425, 0.019389832403201905, 0.018062685482417958, 0.02082926309460897, 0.019460001061435535, 0.02116634714336698, 0.02040592742002366, 0.017484888483676502, 0.018022100038216665, 0.01744339780769843, 0.018239681794470067, 0.017087351673505648, 0.01753913050219353, 0.017921903646686308, 0.01893976074986235, 0.017267082093057374, 0.018321728816761498, 0.018027497530164062, 0.017805057594573095, 0.017158740760185972, 0.016711035384146394, 0.017148106607458035, 0.016303113656958683, 0.01513428936457867, 0.014653190670692363, 0.01672427290996459, 0.01616989971092696, 0.01608593066336081, 0.016442468003435864, 0.01511251449082781, 0.01541321051608874, 0.014527317624101282, 0.013984357550996598, 0.014575209699135581, 0.013988324726596389, 0.013602652025073006, 0.012025072616226859, 0.013279335290689616, 0.012400945197686968, 0.011925718635418026, 0.013186168902048311, 0.011938225500718986, 0.011615638267740057, 0.011882075976847336, 0.011536678413693198, 0.01151309803565102, 0.011364209294382876, 0.011501697070680314, 0.010270944132272607, 0.010548348914902675, 0.011107815730936919, 0.009724083470135045, 0.010280294010355585, 0.010468417649533513, 0.008419739641256098, 0.009647452184603372, 0.00892679912895482, 0.009474488557219536, 0.008627033263228492, 0.00961849639950271, 0.008560758581163423, 0.008441650304301225, 0.00887936063456224, 0.008551140995297035, 0.00834166535408196, 0.00870600548154752, 0.00856278364817707, 0.007427157137871985, 0.00820839587875371, 0.007365316705886065, 0.007285310588027495, 0.007223044126873457, 0.0077219985599359855, 0.0069900222154508735, 0.0077582571955173195, 0.0074682851299034585, 0.007782626469909769, 0.00706461938977085, 0.006641375473777141, 0.0067436475546817694, 0.007454884014739835, 0.007204016620961941, 0.0067948718083477464, 0.007298653304346385, 0.007370518693692983, 0.006235731255776634, 0.006654788846559151, 0.007140780827352873, 0.006593439154983142, 0.00673040944815398, 0.006687708546517384, 0.006367402891263393, 0.006571848479356358, 0.0064926148113970145, 0.006582036632588042, 0.006439744275712784, 0.006544149207023974, 0.006748665554944498, 0.006366266136126869, 0.006478918729064768, 0.006297095428463227, 0.00672788709018349, 0.006080628268254669, 0.006722157445444893, 0.006334329970643589, 0.006211089284619585, 0.0062305297614502686, 0.006515563198729196, 0.0063531691502701874, 0.00656266284041849, 0.00591246345124167, 0.006205050019314709, 0.006203662912961367, 0.006247753952394431, 0.006490260843774657, 0.0066224160859836454, 0.006362692405879395, 0.006348397601739391, 0.0063775982767650665, 0.0059411850282867, 0.00639047130338227], "accuracy_test_std": 0.007117841250122175, "error_valid": [0.5779558664344879, 0.46345832548945776, 0.3852083137236446, 0.36675510636295183, 0.3225038827183735, 0.29154655732304224, 0.2811602856739458, 0.2595935499811747, 0.23387642366340367, 0.22086490493222888, 0.22092520472515065, 0.21343920604292166, 0.20772102080195776, 0.20505606410015065, 0.1952286685805723, 0.18697789203689763, 0.18837214090737953, 0.18331578266189763, 0.18391583913780118, 0.17548269248870485, 0.17325454160391573, 0.1755238728350903, 0.17018219361822284, 0.16444488893072284, 0.16742605186370485, 0.16247117375753017, 0.15863698936370485, 0.1589414297816265, 0.15348944606551207, 0.16035626882530118, 0.15934882106551207, 0.15221726750753017, 0.15461896413780118, 0.15141425075301207, 0.15138336549322284, 0.1480256965361446, 0.15177016660391573, 0.15103774472891573, 0.1512715902673193, 0.1465108480798193, 0.14657114787274095, 0.1469079442771084, 0.14872870387801207, 0.1495626058923193, 0.14718297016189763, 0.1472432699548193, 0.1449739387236446, 0.14871840879141573, 0.1487081137048193, 0.1508656697100903, 0.14355027532003017, 0.14531073512801207, 0.14244134742093373, 0.1452901449548193, 0.14826101280120485, 0.1478845067771084, 0.14592108669051207, 0.14428269719503017, 0.14372382106551207, 0.1433370199548193, 0.1441709219691265, 0.14393707643072284, 0.14145448983433728, 0.14271637330572284, 0.14283844361822284, 0.1418515860316265, 0.14109857398343373, 0.14041762754141573, 0.1400411215173193, 0.14051910768072284, 0.1411191641566265, 0.1430928793298193, 0.1403867422816265, 0.1401426016566265, 0.13866746282003017, 0.13978668580572284, 0.13990875611822284, 0.14334731504141573, 0.14001023625753017, 0.1429502188441265, 0.14383559629141573, 0.14112945924322284, 0.14149567018072284, 0.1409970938441265, 0.1421163168298193, 0.14161774049322284, 0.14308258424322284, 0.14161774049322284, 0.14147508000753017, 0.14173981080572284, 0.1419736563441265, 0.1424619375941265, 0.1418721762048193, 0.14198395143072284, 0.1418515860316265, 0.14149567018072284, 0.1426045980798193, 0.1407529532191265, 0.1419736563441265, 0.14161774049322284, 0.1416074454066265, 0.1414853750941265, 0.1401426016566265, 0.14186188111822284, 0.14024408179593373, 0.1418721762048193, 0.1413633047816265, 0.1428487387048193, 0.1412412344691265, 0.14137359986822284, 0.1423604574548193, 0.14098679875753017, 0.1405088125941265, 0.14088531861822284, 0.14112945924322284, 0.14086472844503017, 0.1415059652673193, 0.1413633047816265, 0.14048822242093373, 0.13914544898343373, 0.14086472844503017, 0.14013230657003017, 0.1426045980798193, 0.1423604574548193, 0.1414853750941265], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.060509939886134956, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 16, "valid_ratio": 0.15, "learning_rate": 0.004930991057587885, "optimization": "rmsprop", "nb_data_augmentation": 2, "learning_rate_decay_method": "exp", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 3.196339994256222e-06, "rotation_range": [0, 0], "momentum": 0.6701835037819999}, "accuracy_valid_max": 0.8613325371799698, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8585146249058735, "accuracy_valid_std": [0.009720178232267255, 0.01665928510892283, 0.006679202897780098, 0.009481557006750354, 0.015042902548570134, 0.01572632925603014, 0.012809472199039587, 0.011161729665455474, 0.012637569229095794, 0.010067000525946003, 0.0140091529915165, 0.013212906048899409, 0.009665162520413696, 0.007955720315538735, 0.010604138587035856, 0.014706591119537704, 0.009131535968528295, 0.01265648118381917, 0.014165511367860893, 0.014774205180941954, 0.010651250205595203, 0.009027744198449506, 0.01226233914870265, 0.009451017204776934, 0.008584492156294503, 0.012007258504074862, 0.010196557880695773, 0.010529682673737755, 0.009760874881657887, 0.010969810098605457, 0.008793099549347257, 0.009275173778043677, 0.009860293689943197, 0.011145149136839541, 0.011128930885445006, 0.011899994651877847, 0.009611412387272566, 0.008253145620224455, 0.01176208942727353, 0.009491866572232407, 0.011054566910745096, 0.008949894601902174, 0.011948462510823214, 0.010914454954210572, 0.01357226840556142, 0.012453329657143812, 0.014993098474447119, 0.011200925122009845, 0.010843253422267659, 0.013968159891966234, 0.011503898589204408, 0.011189552466299426, 0.010528216982761532, 0.01068554324365743, 0.009726263095381256, 0.010534846969050528, 0.013435268809040895, 0.010606497970336243, 0.012093873830460587, 0.009434610511057219, 0.008442559151099139, 0.012830054246458236, 0.006599192299469884, 0.007737236167113626, 0.00924370962169144, 0.009314322200318146, 0.008447513367981538, 0.010297765386582667, 0.011690867435095146, 0.008753583194302821, 0.008331882377992652, 0.008838390793009798, 0.009958493823999092, 0.008303909733041794, 0.00993375937912533, 0.009115164211772768, 0.010087290214824667, 0.010345077398333765, 0.009185391042911724, 0.009251825891731192, 0.010548756058835986, 0.010413897045964581, 0.010198615479232807, 0.009368299186449564, 0.008814307680457907, 0.010594722748994808, 0.009882166783572954, 0.008853965885163902, 0.008942795491697745, 0.0092930556561864, 0.010518636945373513, 0.009698756977830488, 0.010074085539624212, 0.010365955251143666, 0.010873263102341713, 0.009997931878569712, 0.01012233666182521, 0.011066065489554548, 0.00909652403029277, 0.009132332782853407, 0.009507647127214993, 0.010295547868962517, 0.009997466285900111, 0.01008415730698756, 0.009681518108634568, 0.010400140368019528, 0.009604476581088522, 0.010431992627383901, 0.009754646743269525, 0.010202461134676915, 0.010131803380354479, 0.009237624178684303, 0.009724489918614165, 0.009712407028277574, 0.0096535207370912, 0.009235145899803416, 0.009709764453549208, 0.009629268277230672, 0.009564203974657655, 0.008528396292200055, 0.010039179619483725, 0.00946744927131942, 0.01029747435936862, 0.008971093126354064, 0.008904797867594953], "accuracy_valid": [0.42204413356551207, 0.5365416745105422, 0.6147916862763554, 0.6332448936370482, 0.6774961172816265, 0.7084534426769578, 0.7188397143260542, 0.7404064500188253, 0.7661235763365963, 0.7791350950677711, 0.7790747952748494, 0.7865607939570783, 0.7922789791980422, 0.7949439358998494, 0.8047713314194277, 0.8130221079631024, 0.8116278590926205, 0.8166842173381024, 0.8160841608621988, 0.8245173075112951, 0.8267454583960843, 0.8244761271649097, 0.8298178063817772, 0.8355551110692772, 0.8325739481362951, 0.8375288262424698, 0.8413630106362951, 0.8410585702183735, 0.8465105539344879, 0.8396437311746988, 0.8406511789344879, 0.8477827324924698, 0.8453810358621988, 0.8485857492469879, 0.8486166345067772, 0.8519743034638554, 0.8482298333960843, 0.8489622552710843, 0.8487284097326807, 0.8534891519201807, 0.853428852127259, 0.8530920557228916, 0.8512712961219879, 0.8504373941076807, 0.8528170298381024, 0.8527567300451807, 0.8550260612763554, 0.8512815912085843, 0.8512918862951807, 0.8491343302899097, 0.8564497246799698, 0.8546892648719879, 0.8575586525790663, 0.8547098550451807, 0.8517389871987951, 0.8521154932228916, 0.8540789133094879, 0.8557173028049698, 0.8562761789344879, 0.8566629800451807, 0.8558290780308735, 0.8560629235692772, 0.8585455101656627, 0.8572836266942772, 0.8571615563817772, 0.8581484139683735, 0.8589014260165663, 0.8595823724585843, 0.8599588784826807, 0.8594808923192772, 0.8588808358433735, 0.8569071206701807, 0.8596132577183735, 0.8598573983433735, 0.8613325371799698, 0.8602133141942772, 0.8600912438817772, 0.8566526849585843, 0.8599897637424698, 0.8570497811558735, 0.8561644037085843, 0.8588705407567772, 0.8585043298192772, 0.8590029061558735, 0.8578836831701807, 0.8583822595067772, 0.8569174157567772, 0.8583822595067772, 0.8585249199924698, 0.8582601891942772, 0.8580263436558735, 0.8575380624058735, 0.8581278237951807, 0.8580160485692772, 0.8581484139683735, 0.8585043298192772, 0.8573954019201807, 0.8592470467808735, 0.8580263436558735, 0.8583822595067772, 0.8583925545933735, 0.8585146249058735, 0.8598573983433735, 0.8581381188817772, 0.8597559182040663, 0.8581278237951807, 0.8586366952183735, 0.8571512612951807, 0.8587587655308735, 0.8586264001317772, 0.8576395425451807, 0.8590132012424698, 0.8594911874058735, 0.8591146813817772, 0.8588705407567772, 0.8591352715549698, 0.8584940347326807, 0.8586366952183735, 0.8595117775790663, 0.8608545510165663, 0.8591352715549698, 0.8598676934299698, 0.8573954019201807, 0.8576395425451807, 0.8585146249058735], "seed": 584702067, "model": "residualv3", "loss_std": [0.2988406717777252, 0.2544669806957245, 0.2565145492553711, 0.2569728493690491, 0.25554776191711426, 0.25466325879096985, 0.25278687477111816, 0.2500646114349365, 0.24752888083457947, 0.24555423855781555, 0.2434803992509842, 0.2400335967540741, 0.23808152973651886, 0.23479844629764557, 0.2328484058380127, 0.22882667183876038, 0.22693324089050293, 0.22327668964862823, 0.22216372191905975, 0.21937134861946106, 0.21485702693462372, 0.21114981174468994, 0.21009768545627594, 0.2057117372751236, 0.2056470364332199, 0.20191852748394012, 0.1956637054681778, 0.19761984050273895, 0.19435355067253113, 0.1921202540397644, 0.189240962266922, 0.18454356491565704, 0.18529556691646576, 0.18194669485092163, 0.17960728704929352, 0.1756996363401413, 0.17307044565677643, 0.17187678813934326, 0.16832217574119568, 0.16784606873989105, 0.16433322429656982, 0.1647176891565323, 0.1617661863565445, 0.15711520612239838, 0.15800051391124725, 0.15585102140903473, 0.15365591645240784, 0.14843766391277313, 0.15254241228103638, 0.14654767513275146, 0.14629904925823212, 0.14836999773979187, 0.14321886003017426, 0.1441287100315094, 0.14306135475635529, 0.13942545652389526, 0.1396847665309906, 0.13464796543121338, 0.1341533064842224, 0.13080962002277374, 0.1332269310951233, 0.1348203867673874, 0.13046526908874512, 0.1303432732820511, 0.12747836112976074, 0.12927059829235077, 0.12831445038318634, 0.12522512674331665, 0.12481050193309784, 0.12770064175128937, 0.12408553063869476, 0.12606953084468842, 0.12328344583511353, 0.12321794033050537, 0.12311790883541107, 0.12020585685968399, 0.1216217428445816, 0.1209113597869873, 0.11749806255102158, 0.11876785755157471, 0.1202450841665268, 0.1193491593003273, 0.11983180791139603, 0.11801143735647202, 0.11719907820224762, 0.11801315099000931, 0.11733955144882202, 0.11670149117708206, 0.12012796849012375, 0.11471009254455566, 0.11662817746400833, 0.11843673884868622, 0.11662834137678146, 0.1150893121957779, 0.11492621898651123, 0.11364183574914932, 0.11422894895076752, 0.11475575715303421, 0.11515628546476364, 0.11346153169870377, 0.11325867474079132, 0.1131657212972641, 0.11608622223138809, 0.11337481439113617, 0.11308296024799347, 0.11433085054159164, 0.11366721987724304, 0.1128806546330452, 0.11244489252567291, 0.1125861257314682, 0.11432687938213348, 0.1135837510228157, 0.11506419628858566, 0.11408863216638565, 0.1124999076128006, 0.11053035408258438, 0.11218924820423126, 0.11360210925340652, 0.11236013472080231, 0.11245694756507874, 0.11291239410638809, 0.11378271877765656, 0.11173033714294434, 0.11382319778203964, 0.11167560517787933]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:30 2016", "state": "available"}], "summary": "56542185b5edf9da74aef195304b9962"}