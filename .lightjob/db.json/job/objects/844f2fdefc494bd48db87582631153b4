{"content": {"hp_model": {"f0": 32, "f1": 16, "f2": 32, "f3": 32, "nonlin": "leaky_rectify", "nbg1": 2, "nbg3": 7, "nbg2": 4, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.6544605493545532, 1.1524732112884521, 0.9132453799247742, 0.7804507613182068, 0.69389408826828, 0.6273621916770935, 0.5734068751335144, 0.5302582383155823, 0.49755340814590454, 0.463377445936203, 0.4356255829334259, 0.4086015820503235, 0.3904772698879242, 0.3694171607494354, 0.35176193714141846, 0.33745792508125305, 0.32168322801589966, 0.30869296193122864, 0.2945140600204468, 0.281682550907135, 0.2717183530330658, 0.2623094320297241, 0.25464344024658203, 0.24267472326755524, 0.23875337839126587, 0.22984199225902557, 0.22377541661262512, 0.21722641587257385, 0.2118951827287674, 0.2070469856262207, 0.20180600881576538, 0.19465823471546173, 0.19344639778137207, 0.18728671967983246, 0.18627361953258514, 0.18113091588020325, 0.17942242324352264, 0.17409877479076385, 0.17312555015087128, 0.17049561440944672, 0.16925862431526184, 0.1663598269224167, 0.1635482758283615, 0.16126243770122528, 0.15753814578056335, 0.15825925767421722, 0.15482060611248016, 0.15318183600902557, 0.1543409377336502, 0.15260060131549835, 0.15327367186546326, 0.14967213571071625, 0.14967350661754608, 0.14797483384609222, 0.147591233253479, 0.14795459806919098, 0.14748719334602356, 0.1448226422071457, 0.14400850236415863, 0.14421214163303375, 0.14686955511569977, 0.1419491320848465, 0.142278790473938, 0.1426742523908615, 0.1386006623506546, 0.14028418064117432, 0.14159049093723297, 0.14045514166355133, 0.13952213525772095, 0.1400100588798523, 0.13727740943431854, 0.13843271136283875, 0.13951048254966736, 0.13895796239376068, 0.13628850877285004, 0.13679471611976624, 0.13765543699264526, 0.13690906763076782, 0.13564243912696838, 0.13660842180252075, 0.13865508139133453, 0.1378234475851059, 0.13644516468048096, 0.1378890872001648, 0.13684377074241638, 0.1346697360277176, 0.13626039028167725, 0.13537006080150604, 0.1379946917295456, 0.13714705407619476, 0.1378147304058075, 0.13579805195331573, 0.1349528729915619, 0.1361287385225296, 0.13634854555130005, 0.1354297250509262, 0.1359725445508957, 0.13512511551380157, 0.1356765627861023, 0.1341094970703125, 0.13594113290309906, 0.13461756706237793, 0.13459090888500214, 0.1373050957918167, 0.13424266874790192, 0.13488589227199554, 0.13443689048290253, 0.133874773979187, 0.13670185208320618, 0.1341080367565155, 0.13600456714630127, 0.13414102792739868, 0.13532036542892456, 0.13591216504573822, 0.13401009142398834, 0.13504154980182648, 0.13372978568077087, 0.1360897421836853, 0.13466113805770874, 0.133970707654953, 0.1339135318994522, 0.13478176295757294, 0.1353650540113449, 0.13555273413658142, 0.13399390876293182, 0.13528108596801758, 0.1340882033109665, 0.13411161303520203, 0.1352727860212326, 0.13561390340328217, 0.13638651371002197, 0.13407070934772491, 0.1350763738155365, 0.1363070011138916, 0.13245569169521332, 0.13465867936611176, 0.13505561649799347, 0.13508126139640808, 0.1350785344839096, 0.13385124504566193, 0.13586430251598358, 0.13545407354831696, 0.13552924990653992, 0.13484372198581696, 0.13470624387264252, 0.13537836074829102, 0.13454653322696686, 0.13534456491470337, 0.13648326694965363, 0.13574351370334625, 0.13376130163669586, 0.13476119935512543, 0.1341170370578766, 0.13686367869377136, 0.13671061396598816], "moving_avg_accuracy_train": [0.030082352941176463, 0.05799647058823529, 0.10239682352941175, 0.14406772941176468, 0.1929009564705882, 0.24538968435294112, 0.28680601003529405, 0.3332901149141176, 0.37829522106976465, 0.4237221695510235, 0.4669358349488623, 0.5084116632186819, 0.545742261602696, 0.5800221530894852, 0.6099093495452426, 0.6391513557671888, 0.6647538672492934, 0.6889820099361288, 0.7120226324719277, 0.7337874280482644, 0.7540298617140262, 0.7720786402485059, 0.7886684232824789, 0.8045309927189368, 0.8192590699176313, 0.8327213982199858, 0.8451598466332814, 0.8565097443228944, 0.867298769890605, 0.8767877164309563, 0.8856971800819784, 0.8940451091326042, 0.9019017746899319, 0.9090645383974093, 0.9156404374988448, 0.9217658055136662, 0.9273115779034761, 0.9323310083484226, 0.9371167310429921, 0.9414638814681047, 0.945536316850706, 0.9492885675185766, 0.952870299002013, 0.956159739690047, 0.9591767068975129, 0.9619155067959968, 0.9644745443516913, 0.9669047369753457, 0.9690166162189876, 0.9710302487147359, 0.9728636944314976, 0.9745537955765831, 0.976119592489513, 0.977599397946444, 0.9788982816812114, 0.9801284535130903, 0.9811861963970754, 0.9821875767573679, 0.9830841131992782, 0.9839168783499386, 0.9847063669855329, 0.9854333773458032, 0.9860900396112228, 0.986718682708924, 0.9872726967909728, 0.9878254271118756, 0.9883017079300997, 0.9887444783135604, 0.9891241481292632, 0.9894799686104545, 0.9898072658670561, 0.9901135981038799, 0.9903963559405506, 0.9906390732876721, 0.9908292836059637, 0.9910216493630143, 0.9912347785443599, 0.991386594807571, 0.991553817679755, 0.9917066712058972, 0.9918654158500133, 0.991998874265012, 0.9921236927208636, 0.9922195587428949, 0.9922917205156642, 0.9924060778758624, 0.9924619406765115, 0.992523981902978, 0.9925774660656214, 0.9926326606355298, 0.9926729239837415, 0.9927256315853673, 0.9927636566621247, 0.9928143498194417, 0.992850561896321, 0.9928996233537477, 0.9929437786654317, 0.9929929302106533, 0.9930277548366467, 0.9930426264118056, 0.9930607167118015, 0.9930769979817978, 0.9931128275953828, 0.9931238977770209, 0.9931597432934365, 0.9931449454346811, 0.9931833920676836, 0.9932062293315034, 0.9932244299277647, 0.9932172810526353, 0.9932273176532541, 0.9932245858879287, 0.9932644802403123, 0.993262738098634, 0.9932541113475941, 0.9932722296245995, 0.9933003007797867, 0.9932949765841609, 0.9932784201022155, 0.9932917545625821, 0.9933155202827945, 0.993294556489809, 0.993289806723181, 0.9932996495802746, 0.9932943905046, 0.9933084808659047, 0.9933046916028437, 0.9933106930307947, 0.9933231531394799, 0.9933131907667084, 0.9933324599253317, 0.9933474492269162, 0.9933397631277541, 0.9933422574032139, 0.9933656787217161, 0.9933655814377798, 0.9933466703528253, 0.993357885670484, 0.9933656265152003, 0.9933937697460332, 0.9934002751243711, 0.9933943652589928, 0.9933913993213288, 0.9933722593891959, 0.9933597393326292, 0.9933814124581899, 0.9933679770947238, 0.9933747087970162, 0.9933431202702557, 0.9933452788314654, 0.99335663330126, 0.9933480287946634, 0.9933355788563736, 0.9933243739119126, 0.9933260541677802], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.02994666666666666, 0.058218666666666655, 0.10241013333333332, 0.14352911999999998, 0.19052287466666665, 0.2404439205333333, 0.2792528618133333, 0.3222209089653333, 0.3635721514021333, 0.4051616029285866, 0.44484544263572795, 0.4825475650388218, 0.515786141868273, 0.5463941943481123, 0.5730081082466344, 0.598547297421971, 0.6206259010131072, 0.6414966442451299, 0.6605603131539501, 0.6789309485052218, 0.6957845203213663, 0.7104460682892296, 0.72393479479364, 0.7365146486476093, 0.748089850449515, 0.7586541987378969, 0.7681487788641072, 0.7769739009776964, 0.7850765108799267, 0.7920355264586008, 0.798565307146074, 0.8047354430981333, 0.8101952321216532, 0.8150557089094879, 0.8197901380185391, 0.8240244575500184, 0.82791534512835, 0.8314171439488484, 0.8348487628872968, 0.8376838865985671, 0.8403554979387103, 0.8425866148115059, 0.8449146199970219, 0.8471031579973197, 0.8490728421975877, 0.8508055579778289, 0.8523383355133793, 0.8537845019620414, 0.8548460517658372, 0.8559747799225867, 0.8570706352636613, 0.8581502384039619, 0.8590818812302323, 0.8597470264405424, 0.8603456571298214, 0.8610444247501726, 0.8616599822751554, 0.8621339840476399, 0.8627072523095426, 0.8631565270785884, 0.8635742077040629, 0.8639234536003232, 0.8642511082402909, 0.8645193307495952, 0.8647473976746357, 0.8650326579071721, 0.8651693921164548, 0.8655057862381427, 0.8657018742809951, 0.8658116868528956, 0.8660038515009394, 0.8661234663508455, 0.8662844530490943, 0.8665226744108515, 0.8666304069697663, 0.8667673662727897, 0.8668906296455108, 0.8670549000142931, 0.8671360766795305, 0.8671558023449109, 0.8671868887770864, 0.8672015332327111, 0.8672680465761067, 0.867341241918496, 0.8673537843933131, 0.8674584059539818, 0.8675125653585837, 0.867507975489392, 0.8676105112737861, 0.8676361268130741, 0.8676591807984334, 0.8677065960519234, 0.867615936446731, 0.8675743428020579, 0.8676569085218522, 0.8676512176696669, 0.8676327625693668, 0.8676161529790968, 0.8675745376811871, 0.8674704172464017, 0.8675100421884282, 0.867545704636252, 0.8675644675059602, 0.8675146874220308, 0.8676432186798276, 0.8675722301451781, 0.867601673797327, 0.8675481730842609, 0.8675266891091682, 0.867560686864918, 0.8674979515117596, 0.8675481563605836, 0.8676066740578585, 0.8675660066520727, 0.8675694059868654, 0.8676524653881788, 0.8677138855160275, 0.8677158302977581, 0.8676775806013156, 0.867629822541184, 0.8675468402870656, 0.8674988229250257, 0.8674689406325231, 0.8675087132359375, 0.867451175245677, 0.8675060577211093, 0.8675954519489983, 0.8675959067540985, 0.8675829827453553, 0.8676246844708198, 0.8675688826904044, 0.8675453277546974, 0.8674841283125609, 0.8675757154813049, 0.867564810599841, 0.8675949962065236, 0.8675821632525379, 0.8676772802606174, 0.8676162189012223, 0.8676412636777667, 0.8676238039766567, 0.8676747569123243, 0.8676806145544251, 0.8676725530989826, 0.8676252977890844, 0.8676094346768426, 0.8676218245424916, 0.8677129754215758, 0.8677283445460849, 0.8677688434248098, 0.867765292415662, 0.8677487631740958, 0.8677338868566862, 0.8676938315043509, 0.8676577816872492], "moving_var_accuracy_train": [0.008144531626297576, 0.014342860139792388, 0.03065109619752249, 0.04321416615127349, 0.06035490612095367, 0.07911501450122756, 0.08664132134834535, 0.09742413727098001, 0.10591085976461156, 0.11389224262302092, 0.11930980625476618, 0.1228610246052977, 0.12311708432614497, 0.12138137453664488, 0.1172824376908457, 0.11325004827272027, 0.10782444079316994, 0.10232502279633583, 0.09687035309823672, 0.09144667472673056, 0.0859898123404724, 0.08032265676570544, 0.07476737919916356, 0.06955523126138527, 0.06455195445698311, 0.059727867561168185, 0.0551475157954233, 0.05079214581396312, 0.04676055888687322, 0.0428948639561967, 0.03931978444351697, 0.0360149972740738, 0.03296904228978373, 0.030133884716167757, 0.02750967828548133, 0.02509639165678617, 0.02286355281370375, 0.020803949670258278, 0.01892968297861631, 0.017206794132121686, 0.015635377288418684, 0.014198554025247632, 0.012894157826497828, 0.011702125824208888, 0.010613832061966321, 0.009619958079725113, 0.008716900330655693, 0.007898362823282703, 0.007148666846411962, 0.006470292604222168, 0.005853517052566758, 0.005293873324235658, 0.004786551471564961, 0.004327604742121731, 0.003910028158517547, 0.0035326452472893254, 0.0031894501026379827, 0.0028795299560079994, 0.0025988109587322577, 0.0023451713430244223, 0.002116263839473574, 0.0019093943521016792, 0.0017223357648689464, 0.0015536589176806386, 0.0014010554103405497, 0.0012636994665753017, 0.0011393711106780463, 0.0010271984101224703, 0.0009257759116308254, 0.0008343377944012599, 0.0007518681264087444, 0.0006775258687217273, 0.0006104928497973435, 0.000549973770212952, 0.0004953020128783183, 0.00044610485285085765, 0.0004019031839972414, 0.0003619202991974956, 0.00032597994067857947, 0.00029359222441480826, 0.00026445980073164753, 0.0002381741209952884, 0.0002144969257180505, 0.00019312994579386638, 0.00017386381710752266, 0.0001565951338492541, 0.0001409637063367959, 0.0001269019777271495, 0.00011423752495531752, 0.00010284119042471219, 9.257166161712393e-05, 8.333949827683387e-05, 7.501856160731212e-05, 6.753983361236974e-05, 6.0797652081740026e-05, 5.473955011300954e-05, 4.928314232565784e-05, 4.437657096267104e-05, 3.994982865758422e-05, 3.595683626555515e-05, 3.236409796958511e-05, 2.913007389040084e-05, 2.622862035224756e-05, 2.360686125731633e-05, 2.1257739241008576e-05, 1.9133936106521425e-05, 1.7233845788172327e-05, 1.5515155074924103e-05, 1.3966620922770138e-05, 1.2570418788233668e-05, 1.1314283509578138e-05, 1.0182922321496462e-05, 9.178954123515775e-06, 8.261086026682842e-06, 7.435647211516093e-06, 6.695036938019248e-06, 6.032625151999173e-06, 5.429617760330805e-06, 4.88912303814743e-06, 4.401811004832124e-06, 3.966713189463829e-06, 3.5739971960644296e-06, 3.2168005190051675e-06, 2.8959924036265485e-06, 2.6066420841564465e-06, 2.3477647202760765e-06, 2.1131174748793826e-06, 1.902129881628495e-06, 1.7133141822416775e-06, 1.5428760038586586e-06, 1.3919301077392313e-06, 1.2547592094232217e-06, 1.1298149735638819e-06, 1.0168894688981218e-06, 9.201375454517153e-07, 8.281238760840221e-07, 7.485301506830072e-07, 6.748091857663641e-07, 6.078675532820255e-07, 5.542091709292543e-07, 4.991691333621957e-07, 4.4956655860508103e-07, 4.046890738206129e-07, 3.6751719945701425e-07, 3.321762458592025e-07, 3.03186140617373e-07, 2.744921074788195e-07, 2.474507390727101e-07, 2.3168618037149435e-07, 2.0855949681281e-07, 1.888638629903844e-07, 1.7064381449528265e-07, 1.5497444171654896e-07, 1.4060695456823714e-07, 1.2657166844943782e-07], "duration": 47193.929936, "accuracy_train": [0.3008235294117647, 0.30922352941176473, 0.502, 0.5191058823529412, 0.6324, 0.7177882352941176, 0.6595529411764706, 0.7516470588235294, 0.7833411764705882, 0.832564705882353, 0.8558588235294118, 0.8816941176470589, 0.8817176470588235, 0.8885411764705883, 0.8788941176470588, 0.9023294117647059, 0.8951764705882352, 0.907035294117647, 0.9193882352941176, 0.9296705882352941, 0.9362117647058823, 0.9345176470588236, 0.9379764705882353, 0.9472941176470588, 0.9518117647058824, 0.9538823529411765, 0.9571058823529411, 0.9586588235294118, 0.9644, 0.9621882352941177, 0.9658823529411765, 0.9691764705882353, 0.9726117647058824, 0.9735294117647059, 0.9748235294117648, 0.9768941176470588, 0.9772235294117647, 0.9775058823529412, 0.9801882352941177, 0.9805882352941176, 0.9821882352941177, 0.9830588235294118, 0.9851058823529412, 0.985764705882353, 0.9863294117647059, 0.9865647058823529, 0.9875058823529411, 0.9887764705882353, 0.9880235294117647, 0.9891529411764706, 0.9893647058823529, 0.989764705882353, 0.9902117647058823, 0.9909176470588236, 0.9905882352941177, 0.9912, 0.9907058823529412, 0.9912, 0.9911529411764706, 0.9914117647058823, 0.9918117647058824, 0.9919764705882353, 0.992, 0.9923764705882353, 0.9922588235294117, 0.9928, 0.9925882352941177, 0.9927294117647059, 0.9925411764705883, 0.9926823529411765, 0.9927529411764706, 0.9928705882352942, 0.9929411764705882, 0.9928235294117647, 0.9925411764705883, 0.9927529411764706, 0.9931529411764706, 0.9927529411764706, 0.9930588235294118, 0.9930823529411764, 0.9932941176470588, 0.9932, 0.9932470588235294, 0.9930823529411764, 0.9929411764705882, 0.9934352941176471, 0.992964705882353, 0.9930823529411764, 0.9930588235294118, 0.9931294117647059, 0.993035294117647, 0.9932, 0.9931058823529412, 0.9932705882352941, 0.9931764705882353, 0.9933411764705883, 0.9933411764705883, 0.9934352941176471, 0.9933411764705883, 0.9931764705882353, 0.9932235294117647, 0.9932235294117647, 0.9934352941176471, 0.9932235294117647, 0.9934823529411765, 0.9930117647058824, 0.9935294117647059, 0.9934117647058823, 0.9933882352941177, 0.9931529411764706, 0.9933176470588235, 0.9932, 0.9936235294117647, 0.9932470588235294, 0.9931764705882353, 0.9934352941176471, 0.9935529411764706, 0.9932470588235294, 0.9931294117647059, 0.9934117647058823, 0.9935294117647059, 0.9931058823529412, 0.9932470588235294, 0.9933882352941177, 0.9932470588235294, 0.9934352941176471, 0.9932705882352941, 0.9933647058823529, 0.9934352941176471, 0.9932235294117647, 0.9935058823529411, 0.9934823529411765, 0.9932705882352941, 0.9933647058823529, 0.9935764705882353, 0.9933647058823529, 0.9931764705882353, 0.9934588235294117, 0.9934352941176471, 0.9936470588235294, 0.9934588235294117, 0.9933411764705883, 0.9933647058823529, 0.9932, 0.9932470588235294, 0.9935764705882353, 0.9932470588235294, 0.9934352941176471, 0.9930588235294118, 0.9933647058823529, 0.9934588235294117, 0.9932705882352941, 0.9932235294117647, 0.9932235294117647, 0.9933411764705883], "end": "2016-02-07 18:54:08.918000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0], "moving_var_accuracy_valid": [0.008071225599999997, 0.014457856895999995, 0.03058804274175999, 0.042746178048025586, 0.058347277042260726, 0.07494154672183119, 0.08100259735911847, 0.08951861530771507, 0.09595608103654665, 0.1019276152373329, 0.1059081179187184, 0.10811035643012742, 0.10724254769394062, 0.10494996881402396, 0.1008296756496031, 0.09661695973824544, 0.09134244639323169, 0.08612849306142169, 0.08078645500566563, 0.0757451316939836, 0.0707270044712427, 0.06558895292344405, 0.060667569315496696, 0.05602508689083207, 0.051628445872542436, 0.04747005037811232, 0.04353436880625834, 0.03988187694851038, 0.03648455983870884, 0.033271954935256004, 0.030328501764168892, 0.027638286786754052, 0.025142741773710793, 0.02284108570778543, 0.020758710507904572, 0.01884420461416613, 0.01709603520807445, 0.015496795042080196, 0.014053099614720642, 0.012720130991372442, 0.011512355456610238, 0.010405920853449874, 0.009414105241398994, 0.008515802004467823, 0.007699138706660111, 0.006956245571769972, 0.006281765677354368, 0.0056724116861940534, 0.0051153125094480995, 0.004615247503769844, 0.004164530843749917, 0.003758567645839846, 0.0033905225064575326, 0.0030554520191689656, 0.00275313204557139, 0.0024822133266995125, 0.002237402193628627, 0.00201568407338863, 0.0018170733945507113, 0.00163718268545855, 0.0014750345308567657, 0.0013286288320355816, 0.0011967321668998547, 0.0010777064400403463, 0.0009704039267369884, 0.0008740958946656901, 0.0007868545713950149, 0.0007091875633014686, 0.0006386148616562689, 0.0005748619046991686, 0.0005177080594968718, 0.0004660660229580471, 0.00041969267111535984, 0.00037823414875860117, 0.00034051519062099414, 0.00030663249221505657, 0.000276105987725042, 0.00024873825173907655, 0.0002239237334239804, 0.00020153486199845464, 0.00018139007309499785, 0.00016325299592622296, 0.00014696751255724759, 0.00013231897932485025, 0.00011908849721543606, 0.00010727815853250322, 9.65767418492143e-05, 8.691925726638566e-05, 7.832195382347899e-05, 7.049566384380825e-05, 6.345088083559593e-05, 5.7126026608408024e-05, 5.1487396423689885e-05, 4.635422706281563e-05, 4.1780158239300274e-05, 3.760243388755759e-05, 3.384525581534559e-05, 3.046321314021147e-05, 2.7432478323371326e-05, 2.4786800075493246e-05, 2.232225129221935e-05, 2.0101472454660514e-05, 1.8094493616711623e-05, 1.6307346765844576e-05, 1.4825294647337864e-05, 1.3388119531069134e-05, 1.2057109935828985e-05, 1.0877159878933251e-05, 9.793597941712005e-06, 8.824640774105041e-06, 7.9775982175178e-06, 7.202523137375027e-06, 6.513089711686815e-06, 5.876665281558299e-06, 5.289102752695767e-06, 4.822282254745146e-06, 4.374005918215289e-06, 3.936639365977577e-06, 3.5561427828812972e-06, 3.2210559953609378e-06, 2.9609248863119836e-06, 2.685583401196218e-06, 2.425061623723479e-06, 2.1967922011923314e-06, 2.0069085639819653e-06, 1.8333264825699568e-06, 1.7219157861318992e-06, 1.5497260691478214e-06, 1.396256732250982e-06, 1.272282364186311e-06, 1.173078676045346e-06, 1.06076432340631e-06, 9.88396236525942e-07, 9.650504981800607e-07, 8.696156963197129e-07, 7.908546643448923e-07, 7.132513602823919e-07, 7.233514312881994e-07, 6.845726946599499e-07, 6.217605926834022e-07, 5.62328103880717e-07, 5.294611083710217e-07, 4.7682380527276064e-07, 4.2972630832014857e-07, 4.0685125631036364e-07, 3.684308756492807e-07, 3.32969367021573e-07, 3.7444877513978264e-07, 3.391297875193865e-07, 3.199782413691704e-07, 2.8809390422595266e-07, 2.617434562441403e-07, 2.3756085399675935e-07, 2.282446498534196e-07, 2.171164886857112e-07], "accuracy_test": 0.8662, "start": "2016-02-07 05:47:34.988000", "learning_rate_per_epoch": [0.0031079489272087812, 0.002888802671805024, 0.0026851086877286434, 0.0024957775603979826, 0.0023197962436825037, 0.0021562238689512014, 0.002004185225814581, 0.0018628669204190373, 0.0017315131844952703, 0.001609421451576054, 0.0014959386317059398, 0.0013904576189815998, 0.0012924142647534609, 0.0012012841179966927, 0.001116579631343484, 0.001037847832776606, 0.0009646675316616893, 0.0008966472814790905, 0.0008334232843481004, 0.000774657295551151, 0.0007200349937193096, 0.0006692641763947904, 0.0006220733048394322, 0.0005782099324278533, 0.0005374394240789115, 0.000499543733894825, 0.00046432012459263206, 0.00043158017797395587, 0.00040114877629093826, 0.0003728631418198347, 0.00034657196374610066, 0.00032213464146479964, 0.00029942041146568954, 0.00027830779436044395, 0.00025868386728689075, 0.00024044366728048772, 0.00022348960919771343, 0.00020773100550286472, 0.00019308357150293887, 0.00017946894513443112, 0.00016681430861353874, 0.00015505196643061936, 0.00014411901065614074, 0.0001339569571428001, 0.00012451143993530422, 0.00011573194205993786, 0.00010757149721030146, 9.998646419262514e-05, 9.293625771533698e-05, 8.638317376608029e-05, 8.029215678106993e-05, 7.463063229806721e-05, 6.936831050552428e-05, 6.447704072343186e-05, 5.993066224618815e-05, 5.570485518546775e-05, 5.17770167789422e-05, 4.812613769900054e-05, 4.47326892754063e-05, 4.15785179939121e-05, 3.8646750908810645e-05, 3.592170833144337e-05, 3.3388812880730256e-05, 3.10345167235937e-05, 2.8846223358414136e-05, 2.6812231226358563e-05, 2.4921659132814966e-05, 2.316439349669963e-05, 2.1531035599764436e-05, 2.001284883590415e-05, 1.8601711417431943e-05, 1.72900763573125e-05, 1.6070925994426943e-05, 1.4937740161258262e-05, 1.3884456166124437e-05, 1.290544150833739e-05, 1.199545840790961e-05, 1.114964015869191e-05, 1.0363461115048267e-05, 9.632717592467088e-06, 8.953499673225451e-06, 8.322173925989773e-06, 7.735364306427073e-06, 7.189931693574181e-06, 6.68295842842781e-06, 6.211732397787273e-06, 5.773733391833957e-06, 5.366618552216096e-06, 4.9882096391229425e-06, 4.636483026843052e-06, 4.309557425585808e-06, 4.005683877039701e-06, 3.7232368867989862e-06, 3.4607055567903444e-06, 3.2166858545679133e-06, 2.989872200487298e-06, 2.7790515559900086e-06, 2.583096147645847e-06, 2.400958010184695e-06, 2.231662620033603e-06, 2.0743045752169564e-06, 1.928042138388264e-06, 1.792092803043488e-06, 1.6657295418553986e-06, 1.5482763728869031e-06, 1.4391049489859142e-06, 1.3376313745538937e-06, 1.2433129086275585e-06, 1.1556450090211001e-06, 1.074158717528917e-06, 9.984180451283464e-07, 9.280180393034243e-07, 8.6258199871736e-07, 8.017599952836463e-07, 7.45226657272724e-07, 6.926795776962535e-07, 6.438376658479683e-07, 5.98439669374784e-07, 5.562427531913272e-07, 5.170211920813017e-07, 4.805652338291111e-07, 4.466798202429345e-07, 4.151837345034437e-07, 3.859084642954258e-07, 3.5869746284333814e-07, 3.334051541514782e-07, 3.098962224612478e-07, 2.880449585518363e-07, 2.6773446393235645e-07, 2.488560824076558e-07, 2.313088458549828e-07, 2.1499889157894359e-07, 1.9983897914244153e-07, 1.8574802140847169e-07, 1.726506297927699e-07, 1.6047675899244496e-07, 1.4916129487119179e-07, 1.3864369918792363e-07, 1.28867711168823e-07, 1.197810490793927e-07, 1.1133509758565197e-07, 1.0348468748588857e-07, 9.618781859899173e-08, 8.940546081248613e-08, 8.310134091971122e-08, 7.724173656242783e-08, 7.179529859513423e-08, 6.67329018710916e-08, 6.202746050121277e-08, 5.7653807061797124e-08, 5.3588546933269754e-08, 4.980993395520272e-08, 4.6297756739477336e-08, 4.3033228536160095e-08, 3.9998887757519697e-08], "accuracy_train_first": 0.3008235294117647, "accuracy_train_last": 0.9933411764705883, "batch_size_eval": 1024, "accuracy_train_std": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "accuracy_test_std": 0, "error_valid": [0.7005333333333333, 0.6873333333333334, 0.4998666666666667, 0.48640000000000005, 0.3865333333333333, 0.3102666666666667, 0.3714666666666666, 0.2910666666666667, 0.26426666666666665, 0.22053333333333336, 0.19799999999999995, 0.17813333333333337, 0.1850666666666667, 0.17813333333333337, 0.18746666666666667, 0.17159999999999997, 0.18066666666666664, 0.17066666666666663, 0.16786666666666672, 0.15573333333333328, 0.1525333333333333, 0.15759999999999996, 0.15466666666666662, 0.15026666666666666, 0.14773333333333338, 0.14626666666666666, 0.14639999999999997, 0.14359999999999995, 0.14200000000000002, 0.14533333333333331, 0.14266666666666672, 0.13973333333333338, 0.14066666666666672, 0.1412, 0.13759999999999994, 0.1378666666666667, 0.13706666666666667, 0.13706666666666667, 0.13426666666666665, 0.13680000000000003, 0.13560000000000005, 0.1373333333333333, 0.13413333333333333, 0.13319999999999999, 0.13319999999999999, 0.13360000000000005, 0.1338666666666667, 0.13319999999999999, 0.13560000000000005, 0.1338666666666667, 0.13306666666666667, 0.13213333333333332, 0.13253333333333328, 0.13426666666666665, 0.13426666666666665, 0.1326666666666667, 0.13280000000000003, 0.13360000000000005, 0.13213333333333332, 0.13280000000000003, 0.1326666666666667, 0.13293333333333335, 0.13280000000000003, 0.13306666666666667, 0.13319999999999999, 0.13239999999999996, 0.13360000000000005, 0.13146666666666662, 0.13253333333333328, 0.13319999999999999, 0.13226666666666664, 0.13280000000000003, 0.13226666666666664, 0.1313333333333333, 0.13239999999999996, 0.132, 0.132, 0.13146666666666662, 0.13213333333333332, 0.1326666666666667, 0.13253333333333328, 0.1326666666666667, 0.13213333333333332, 0.132, 0.13253333333333328, 0.13160000000000005, 0.132, 0.13253333333333328, 0.13146666666666662, 0.13213333333333332, 0.13213333333333332, 0.1318666666666667, 0.13319999999999999, 0.13280000000000003, 0.13160000000000005, 0.13239999999999996, 0.13253333333333328, 0.13253333333333328, 0.13280000000000003, 0.13346666666666662, 0.13213333333333332, 0.13213333333333332, 0.13226666666666664, 0.13293333333333335, 0.13119999999999998, 0.13306666666666667, 0.13213333333333332, 0.13293333333333335, 0.1326666666666667, 0.13213333333333332, 0.13306666666666667, 0.132, 0.1318666666666667, 0.13280000000000003, 0.13239999999999996, 0.13160000000000005, 0.13173333333333337, 0.13226666666666664, 0.1326666666666667, 0.13280000000000003, 0.13319999999999999, 0.13293333333333335, 0.13280000000000003, 0.13213333333333332, 0.13306666666666667, 0.132, 0.13160000000000005, 0.13239999999999996, 0.13253333333333328, 0.132, 0.13293333333333335, 0.1326666666666667, 0.13306666666666667, 0.13160000000000005, 0.13253333333333328, 0.13213333333333332, 0.13253333333333328, 0.13146666666666662, 0.13293333333333335, 0.13213333333333332, 0.13253333333333328, 0.1318666666666667, 0.13226666666666664, 0.13239999999999996, 0.13280000000000003, 0.13253333333333328, 0.13226666666666664, 0.13146666666666662, 0.13213333333333332, 0.1318666666666667, 0.13226666666666664, 0.13239999999999996, 0.13239999999999996, 0.1326666666666667, 0.1326666666666667], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.07051157353004094, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 128, "valid_ratio": 0.15, "learning_rate": 0.003343719771063004, "optimization": "rmsprop", "nb_data_augmentation": 2, "learning_rate_decay_method": "exp", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 2.4413392945812927e-07, "rotation_range": [0, 0], "momentum": 0.8836592656973088}, "accuracy_valid_max": 0.8688, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        y_pred = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            y_pred.extend((nnet.predict(X[mini_batch]) == y[mini_batch]).tolist())\n        return np.mean(y_pred)\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            acc = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = acc\n            status[\"accuracy_train_std\"] = 0\n            acc = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = acc\n            status[\"accuracy_valid_std\"] = 0\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n\n    # rescaling to [-1, 1]\n    X_min = X_train.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X_train.max(axis=(0, 2, 3))[None, :, None, None]\n    def preprocess(a):\n        return (a / 255.) * 2 - 1\n        # return 2 * ((a - X_min) / (X_max - X_min)) - 1\n    X_train = preprocess(X_train)\n    X_valid = preprocess(X_valid)\n\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = preprocess(X_test)\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    acc = evaluate(X_test, y_test, batch_size_eval)\n    light.set(\"accuracy_test\", acc)\n    light.set(\"accuracy_test_std\", 0)\n    print(\"Test accuracy : {}+-{}\".format(acc, 0))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8673333333333333, "accuracy_valid_std": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "accuracy_valid": [0.29946666666666666, 0.31266666666666665, 0.5001333333333333, 0.5136, 0.6134666666666667, 0.6897333333333333, 0.6285333333333334, 0.7089333333333333, 0.7357333333333334, 0.7794666666666666, 0.802, 0.8218666666666666, 0.8149333333333333, 0.8218666666666666, 0.8125333333333333, 0.8284, 0.8193333333333334, 0.8293333333333334, 0.8321333333333333, 0.8442666666666667, 0.8474666666666667, 0.8424, 0.8453333333333334, 0.8497333333333333, 0.8522666666666666, 0.8537333333333333, 0.8536, 0.8564, 0.858, 0.8546666666666667, 0.8573333333333333, 0.8602666666666666, 0.8593333333333333, 0.8588, 0.8624, 0.8621333333333333, 0.8629333333333333, 0.8629333333333333, 0.8657333333333334, 0.8632, 0.8644, 0.8626666666666667, 0.8658666666666667, 0.8668, 0.8668, 0.8664, 0.8661333333333333, 0.8668, 0.8644, 0.8661333333333333, 0.8669333333333333, 0.8678666666666667, 0.8674666666666667, 0.8657333333333334, 0.8657333333333334, 0.8673333333333333, 0.8672, 0.8664, 0.8678666666666667, 0.8672, 0.8673333333333333, 0.8670666666666667, 0.8672, 0.8669333333333333, 0.8668, 0.8676, 0.8664, 0.8685333333333334, 0.8674666666666667, 0.8668, 0.8677333333333334, 0.8672, 0.8677333333333334, 0.8686666666666667, 0.8676, 0.868, 0.868, 0.8685333333333334, 0.8678666666666667, 0.8673333333333333, 0.8674666666666667, 0.8673333333333333, 0.8678666666666667, 0.868, 0.8674666666666667, 0.8684, 0.868, 0.8674666666666667, 0.8685333333333334, 0.8678666666666667, 0.8678666666666667, 0.8681333333333333, 0.8668, 0.8672, 0.8684, 0.8676, 0.8674666666666667, 0.8674666666666667, 0.8672, 0.8665333333333334, 0.8678666666666667, 0.8678666666666667, 0.8677333333333334, 0.8670666666666667, 0.8688, 0.8669333333333333, 0.8678666666666667, 0.8670666666666667, 0.8673333333333333, 0.8678666666666667, 0.8669333333333333, 0.868, 0.8681333333333333, 0.8672, 0.8676, 0.8684, 0.8682666666666666, 0.8677333333333334, 0.8673333333333333, 0.8672, 0.8668, 0.8670666666666667, 0.8672, 0.8678666666666667, 0.8669333333333333, 0.868, 0.8684, 0.8676, 0.8674666666666667, 0.868, 0.8670666666666667, 0.8673333333333333, 0.8669333333333333, 0.8684, 0.8674666666666667, 0.8678666666666667, 0.8674666666666667, 0.8685333333333334, 0.8670666666666667, 0.8678666666666667, 0.8674666666666667, 0.8681333333333333, 0.8677333333333334, 0.8676, 0.8672, 0.8674666666666667, 0.8677333333333334, 0.8685333333333334, 0.8678666666666667, 0.8681333333333333, 0.8677333333333334, 0.8676, 0.8676, 0.8673333333333333, 0.8673333333333333], "seed": 558563872, "model": "residualv5", "loss_std": [0.24290548264980316, 0.14155049622058868, 0.11781204491853714, 0.10554149001836777, 0.1023174449801445, 0.09485221654176712, 0.09173694252967834, 0.08614116162061691, 0.08558215200901031, 0.08183450996875763, 0.08016033470630646, 0.07723994553089142, 0.07638930529356003, 0.07095924764871597, 0.06677999347448349, 0.06854165345430374, 0.06355907022953033, 0.0630677118897438, 0.06102495640516281, 0.06091785058379173, 0.0593860037624836, 0.05570582300424576, 0.054394371807575226, 0.05436621606349945, 0.05501016229391098, 0.05177430436015129, 0.051798347383737564, 0.04948325827717781, 0.05042366683483124, 0.04759210720658302, 0.04842231050133705, 0.0453658290207386, 0.04569825530052185, 0.04606102779507637, 0.044652026146650314, 0.04436440020799637, 0.044817812740802765, 0.04222104325890541, 0.04525967314839363, 0.043181661516427994, 0.041720274835824966, 0.04103883355855942, 0.03978564962744713, 0.041560541838407516, 0.03982045501470566, 0.04115802422165871, 0.038255780935287476, 0.040474601089954376, 0.03887645900249481, 0.03955777734518051, 0.039708204567432404, 0.03844558075070381, 0.03857554495334625, 0.03827871009707451, 0.038966257125139236, 0.03680950403213501, 0.037698231637477875, 0.03755004331469536, 0.03738440200686455, 0.03787241131067276, 0.03804304823279381, 0.03647419437766075, 0.036601364612579346, 0.038182348012924194, 0.035961348563432693, 0.037549059838056564, 0.03824831172823906, 0.0382852777838707, 0.03569779172539711, 0.03617722913622856, 0.03564058616757393, 0.03522707521915436, 0.03483671322464943, 0.03662034496665001, 0.03632378205657005, 0.03760584816336632, 0.03560022637248039, 0.036675576120615005, 0.0344872884452343, 0.0357043519616127, 0.03608649969100952, 0.03645205870270729, 0.03492891043424606, 0.03545442968606949, 0.03539358451962471, 0.03444678336381912, 0.036103032529354095, 0.03486185893416405, 0.03662842884659767, 0.03642268478870392, 0.034992095082998276, 0.03507401794195175, 0.035109348595142365, 0.03527619689702988, 0.034987762570381165, 0.034182317554950714, 0.03763601928949356, 0.034221816807985306, 0.03500866889953613, 0.035295095294713974, 0.03663032129406929, 0.03592396900057793, 0.03455513343214989, 0.035782452672719955, 0.03481082618236542, 0.036740049719810486, 0.035208553075790405, 0.03597287833690643, 0.035426001995801926, 0.03548773378133774, 0.035036079585552216, 0.03710552304983139, 0.03659617528319359, 0.03657198324799538, 0.03436143323779106, 0.03566134721040726, 0.035550158470869064, 0.03611185401678085, 0.03450828418135643, 0.03569311648607254, 0.03513920679688454, 0.035938311368227005, 0.03323978930711746, 0.03622611612081528, 0.03546036407351494, 0.03654838725924492, 0.03535601869225502, 0.03609691560268402, 0.03520014509558678, 0.03580629080533981, 0.034953974187374115, 0.03603808954358101, 0.03571029007434845, 0.036984898149967194, 0.03531697764992714, 0.033387377858161926, 0.03396277502179146, 0.03507857024669647, 0.03548414260149002, 0.03760365769267082, 0.03673265501856804, 0.035619623959064484, 0.03406856581568718, 0.03521040081977844, 0.03372497111558914, 0.03452499955892563, 0.03620072454214096, 0.03521457687020302, 0.0362243689596653, 0.03744840249419212, 0.03506628796458244, 0.03516590595245361, 0.035865120589733124, 0.0358818918466568, 0.03569599986076355]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:44 2016", "state": "available"}], "summary": "0146cf3d71ec687347600a0c22d4447e"}