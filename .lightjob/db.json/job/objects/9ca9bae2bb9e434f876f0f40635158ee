{"content": {"hp_model": {"f0": 16, "f1": 32, "f2": 16, "f3": 16, "nonlin": "leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.08569899983976079, 0.085721472753853, 0.08909107015194363, 0.08503117472322662, 0.08381146818241192, 0.08067842163299187, 0.07960387565860075, 0.08169235452736151, 0.08181670039915344, 0.08083025777511853, 0.08357183572136168, 0.07768258562576288, 0.08088275427094599, 0.07923840441669672, 0.0772110781507734, 0.0793562575261929, 0.0792978026854949, 0.08085066487443442, 0.08104796199591953, 0.08391514244935776, 0.07846656740722174, 0.0789872698118129, 0.08227200286872938, 0.08048897150301247, 0.08151237247726897, 0.08147954605804747, 0.07920632434035052, 0.07823438631157459, 0.08040073401426905, 0.07608336111764706, 0.07747199977738346, 0.07909783119495849, 0.08076370586771339, 0.07851564707611186, 0.0765769539986501, 0.07851439774297542, 0.07848293070709642, 0.07774558133180953, 0.07658848170095206, 0.07744494541728542, 0.07694730168916522, 0.07595667351317514, 0.07771299976470683, 0.07742007009252883, 0.07488260743501841, 0.07584106278627101, 0.07439623250710842, 0.07238999334158161, 0.0736841913371375, 0.07547332123112453, 0.07269975818937573, 0.07149044668383209, 0.07587738634685623, 0.07260549367445936, 0.07281447807175492, 0.07135410542947569, 0.07142155945879601, 0.07193660571630144, 0.07383313978769869, 0.0701089997243947, 0.06819494009181015, 0.07456229800438649, 0.07101436738133222, 0.07157833124288102, 0.0699930306271567, 0.07085911650774641, 0.0715343401303386, 0.06923559159367931, 0.06860794787477448, 0.06955132820806692, 0.07032705169385882, 0.06875699866920036, 0.07115737523652442, 0.07048663740277604, 0.07247125020515965, 0.0713777215908486, 0.07164308481166455, 0.06988886393354274, 0.07318132365397689, 0.07107009939011769, 0.07028024727803821, 0.07245845214145621, 0.07105014633036305, 0.06879485902751777, 0.07037154407838675, 0.06956402016166072, 0.06806300296200414, 0.07055024421294961, 0.07188861643564484, 0.06977700204266722, 0.06945998038741479, 0.07167444434253613, 0.07173264709911029, 0.06985070290038023, 0.07082084870994458, 0.07153097425723133, 0.07034328009694776, 0.0725273381332249, 0.0713075999788365, 0.07245845214145621, 0.07088516203396306, 0.07154132069565473, 0.06980728373522319, 0.06876218625186444, 0.07051800538597505, 0.0715084063502793, 0.07030561939248219, 0.06854592117932443, 0.06968761357961384, 0.07018679891612233, 0.07130309784165101, 0.07095557544362978, 0.0681508585811567, 0.06817963905827772, 0.06940488271528192, 0.07112139946562761, 0.06730994455090826, 0.06966956852980524, 0.06951272530735073, 0.0704911916857428, 0.06963269602509711, 0.07142455593708749, 0.07299978043906871, 0.07042234000781687, 0.07071197452892895, 0.06990417354021342, 0.06826551580418314, 0.06976434883604013, 0.06870146701564608, 0.06618450159086309, 0.06958004204862497, 0.07164644541793849, 0.07171611137660128, 0.06884034169469007, 0.07082122645370761, 0.06599506183593129, 0.06974133712432073, 0.0684580515203665, 0.07203521184505624, 0.06930459290695237, 0.06744970117008482, 0.0711890739919756, 0.06900918100657732, 0.0679438042490387], "moving_avg_accuracy_train": [0.0434605609939759, 0.08998517507530118, 0.14207240681475902, 0.19189001553087348, 0.24038738445971386, 0.2855316279414533, 0.3281004530991152, 0.3674242556807699, 0.40492411173919896, 0.4396176004146766, 0.4718818381141728, 0.501371459272635, 0.5290628148513956, 0.55464862673975, 0.577941764818787, 0.5997456681561855, 0.620435163840567, 0.6389780555890403, 0.656426729698811, 0.6725705778132672, 0.6875824394596514, 0.7012131262365778, 0.713880781986414, 0.7256722971612666, 0.7365058581077906, 0.7463596021162887, 0.7556397751877924, 0.764504920409977, 0.7724811979472925, 0.7800622385441296, 0.787127550834295, 0.7935592799376124, 0.7998231749860198, 0.8054818589934419, 0.8102970014073507, 0.8151247937364952, 0.8197757179773035, 0.8240980332277659, 0.8281928421037844, 0.8320452446403939, 0.8357618421643063, 0.8392785608093214, 0.8425094961440519, 0.8455867656561528, 0.848608096620658, 0.8512449337959416, 0.854072247645263, 0.856517997278327, 0.8589286034239882, 0.8613005209430351, 0.8635646706559605, 0.8656447623253042, 0.8677003915144605, 0.8695551641100024, 0.8713609428797251, 0.8730261475375357, 0.8746754341392038, 0.8761668515686569, 0.8775985474358876, 0.879272992391094, 0.8807635207122256, 0.8819379216530513, 0.8831501912347341, 0.8842529996715017, 0.8856926281682069, 0.8869718216766874, 0.8882572261054041, 0.8894799786454661, 0.8906651697869437, 0.8918471467841529, 0.8927650299973039, 0.8938523259433566, 0.8950214984695029, 0.896064341092432, 0.8968546502060804, 0.8979071369927012, 0.8990120369982504, 0.8999876217020398, 0.9008279973330406, 0.9016313986539535, 0.9024227015596424, 0.9033066550482565, 0.903777476742226, 0.9046624361463166, 0.9055342008148175, 0.9063611459441792, 0.9070724522834962, 0.9080656023864718, 0.908674704798427, 0.9095382207643675, 0.9103059724831115, 0.9109451794516679, 0.9118993249101155, 0.9128545354913932, 0.9135212656771936, 0.9139542482962213, 0.9147039665388883, 0.9154210698849995, 0.9160288122940898, 0.9165804867875724, 0.9171828861509839, 0.9177744619937168, 0.9182056942582005, 0.9187255804046696, 0.9192828981172146, 0.9201045141789872, 0.9207710205924138, 0.9213614637138953, 0.921695196860578, 0.9220332072950022, 0.9224739001197187, 0.9231576095053372, 0.9236717619584179, 0.9240874359131785, 0.9247745131049933, 0.9256211393547349, 0.9261313145758879, 0.9269810972749256, 0.9275858866438186, 0.9280054794553404, 0.9283007522929388, 0.9285523788708738, 0.9290306311946298, 0.9294234076836005, 0.929967512698373, 0.9306101627839574, 0.9311414846079713, 0.9315820236471741, 0.9319973340836616, 0.932491124771681, 0.9327143391017418, 0.9333552734144592, 0.9337579802597603, 0.9339862861494469, 0.9344035460887191, 0.9350308684376785, 0.9354213245155973, 0.935916277907411, 0.9361711297853447, 0.9366569912044005, 0.9372801663309485, 0.9374574584327934, 0.9379864678605984, 0.9385119927612855], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 1234423, "moving_var_accuracy_train": [0.016999383257199897, 0.034780302370226175, 0.05571998952572341, 0.07248413781687801, 0.08640367717237067, 0.09610533393098078, 0.102803744415615, 0.1064406230193821, 0.10845271355706997, 0.10844018560964246, 0.10696499635764453, 0.1040952365265067, 0.10058701343796021, 0.09642001602404052, 0.09166114695575824, 0.08677372406690194, 0.0819488487452781, 0.07684851338031072, 0.07190376809598048, 0.06705900577386613, 0.06238130910729112, 0.057815338794658104, 0.053478030434959516, 0.04938158586253257, 0.04549972166131775, 0.041823615934051074, 0.038416348850779526, 0.03528203116399538, 0.03232641707776641, 0.029611024958767784, 0.02709919020270906, 0.024761575435764304, 0.02263854532278504, 0.02066287713276923, 0.01880525978768832, 0.0171345020178796, 0.015615731682735286, 0.014222300196581177, 0.012950977314503146, 0.011789448630789512, 0.010734821642103327, 0.00977264526814677, 0.008889331229566984, 0.008085624395461233, 0.0073592179230888115, 0.006685872323380549, 0.006089228423465581, 0.00553414080252772, 0.0050330259201804475, 0.00458035726261686, 0.0041684589016580354, 0.00379055404366809, 0.003449529141571085, 0.0031355378598445357, 0.002851331606546711, 0.002591154604863588, 0.0023565204610272073, 0.002140887348464375, 0.0019452463911241468, 0.0017759556451838784, 0.0016183551527503479, 0.001468932595603622, 0.001335265713891321, 0.00121268482053604, 0.0011100691103591684, 0.0010137892236124998, 0.0009272806821595327, 0.0008480087279116322, 0.0007758499574969991, 0.0007108385883446852, 0.0006473373158470759, 0.000593243496531093, 0.000546221826441041, 0.0005013873304227181, 0.0004568698938364872, 0.00042115246037694235, 0.0003900244505396108, 0.00035958789511406075, 0.00032998518641327515, 0.0003027957509139475, 0.00027815161841951945, 0.0002573688205078643, 0.000233626996064689, 0.00021731267478021656, 0.00020242117003741283, 0.0001883335972564457, 0.00017405384790597432, 0.00016552558725874242, 0.00015231208026711532, 0.0001437918106513101, 0.00013471761390088796, 0.00012492312244865775, 0.00012062435220667893, 0.00011677376227727337, 0.00010909714831546351, 9.98746990193373e-05, 9.494592610789325e-05, 9.007946837813843e-05, 8.439567906258784e-05, 7.869521387716239e-05, 7.409165742679264e-05, 6.983214948346005e-05, 6.452258592850002e-05, 6.050286178326453e-05, 5.7248002899386526e-05, 5.759867918611099e-05, 5.5836888459748783e-05, 5.339080733111738e-05, 4.905412691675863e-05, 4.517697370909906e-05, 4.2407167829999825e-05, 4.2373577762845205e-05, 4.051539469164118e-05, 3.801891875247408e-05, 3.8465702484834136e-05, 4.107011629711496e-05, 3.9305613473910246e-05, 4.187422784677367e-05, 4.097873668863e-05, 3.8465386167093527e-05, 3.540352198799538e-05, 3.243301320170548e-05, 3.1248239448136974e-05, 2.9511875835917055e-05, 2.9225140656230406e-05, 3.001961878312204e-05, 2.9558382830870907e-05, 2.8349216353339923e-05, 2.7066639545904128e-05, 2.65544387834856e-05, 2.4347416639437515e-05, 2.560984611446024e-05, 2.450841673228519e-05, 2.252668727244707e-05, 2.1840971257495113e-05, 2.319867409728125e-05, 2.225091022660742e-05, 2.2230628944558183e-05, 2.0592111367278872e-05, 2.0657452097294133e-05, 2.2086832032696638e-05, 2.016104123381627e-05, 2.066359588279386e-05, 2.1082824085694598e-05], "duration": 52812.586502, "accuracy_train": [0.43460560993975905, 0.5087067018072289, 0.6108574924698795, 0.6402484939759037, 0.6768637048192772, 0.6918298192771084, 0.7112198795180723, 0.7213384789156626, 0.7424228162650602, 0.7518589984939759, 0.7622599774096386, 0.7667780496987951, 0.778285015060241, 0.7849209337349398, 0.7875800075301205, 0.7959807981927711, 0.806640625, 0.8058640813253012, 0.813464796686747, 0.8178652108433735, 0.8226891942771084, 0.8238893072289156, 0.8278896837349398, 0.8317959337349398, 0.834007906626506, 0.8350432981927711, 0.8391613328313253, 0.8442912274096386, 0.8442676957831325, 0.8482916039156626, 0.8507153614457831, 0.8514448418674698, 0.8561982304216867, 0.856410015060241, 0.8536332831325302, 0.8585749246987951, 0.8616340361445783, 0.8629988704819277, 0.8650461219879518, 0.8667168674698795, 0.8692112198795181, 0.8709290286144579, 0.8715879141566265, 0.8732821912650602, 0.8758000753012049, 0.874976468373494, 0.8795180722891566, 0.8785297439759037, 0.8806240587349398, 0.8826477786144579, 0.8839420180722891, 0.8843655873493976, 0.8862010542168675, 0.8862481174698795, 0.8876129518072289, 0.8880129894578314, 0.8895190135542169, 0.8895896084337349, 0.8904838102409639, 0.8943429969879518, 0.8941782756024096, 0.8925075301204819, 0.8940606174698795, 0.8941782756024096, 0.8986492846385542, 0.8984845632530121, 0.8998258659638554, 0.9004847515060241, 0.901331890060241, 0.9024849397590361, 0.9010259789156626, 0.9036379894578314, 0.9055440512048193, 0.9054499246987951, 0.9039674322289156, 0.9073795180722891, 0.9089561370481928, 0.9087678840361446, 0.9083913780120482, 0.9088620105421686, 0.9095444277108434, 0.9112622364457831, 0.9080148719879518, 0.9126270707831325, 0.9133800828313253, 0.9138036521084337, 0.9134742093373494, 0.917003953313253, 0.9141566265060241, 0.9173098644578314, 0.9172157379518072, 0.9166980421686747, 0.9204866340361446, 0.9214514307228916, 0.9195218373493976, 0.9178510918674698, 0.9214514307228916, 0.921875, 0.9214984939759037, 0.9215455572289156, 0.9226044804216867, 0.9230986445783133, 0.9220867846385542, 0.9234045557228916, 0.9242987575301205, 0.9274990587349398, 0.926769578313253, 0.9266754518072289, 0.9246987951807228, 0.9250753012048193, 0.9264401355421686, 0.9293109939759037, 0.9282991340361446, 0.9278285015060241, 0.9309582078313253, 0.9332407756024096, 0.9307228915662651, 0.9346291415662651, 0.9330289909638554, 0.9317818147590361, 0.9309582078313253, 0.9308170180722891, 0.9333349021084337, 0.9329583960843374, 0.9348644578313253, 0.9363940135542169, 0.9359233810240963, 0.935546875, 0.9357351280120482, 0.9369352409638554, 0.9347232680722891, 0.9391236822289156, 0.9373823418674698, 0.9360410391566265, 0.9381588855421686, 0.9406767695783133, 0.9389354292168675, 0.9403708584337349, 0.938464796686747, 0.9410297439759037, 0.9428887424698795, 0.9390530873493976, 0.9427475527108434, 0.9432417168674698], "end": "2016-01-20 22:46:36.349000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0], "accuracy_valid": [0.4285523504273504, 0.4969284188034188, 0.5973557692307693, 0.6164529914529915, 0.6513087606837606, 0.6665331196581197, 0.6760149572649573, 0.6840277777777778, 0.7016559829059829, 0.7115384615384616, 0.7155448717948718, 0.719017094017094, 0.7270299145299145, 0.734107905982906, 0.734107905982906, 0.7391826923076923, 0.7469284188034188, 0.7466613247863247, 0.750267094017094, 0.7566773504273504, 0.7601495726495726, 0.7600160256410257, 0.7640224358974359, 0.764823717948718, 0.7642895299145299, 0.7682959401709402, 0.7702991452991453, 0.7737713675213675, 0.7725694444444444, 0.7767094017094017, 0.7772435897435898, 0.7763087606837606, 0.7816506410256411, 0.7831196581196581, 0.7784455128205128, 0.7840544871794872, 0.781784188034188, 0.7845886752136753, 0.7884615384615384, 0.7861912393162394, 0.7877938034188035, 0.7881944444444444, 0.7888621794871795, 0.7909989316239316, 0.7889957264957265, 0.7903311965811965, 0.7909989316239316, 0.7915331196581197, 0.7920673076923077, 0.7930021367521367, 0.7939369658119658, 0.7919337606837606, 0.7923344017094017, 0.7931356837606838, 0.7939369658119658, 0.7954059829059829, 0.7959401709401709, 0.7983440170940171, 0.7958066239316239, 0.7986111111111112, 0.797676282051282, 0.7956730769230769, 0.7975427350427351, 0.7950053418803419, 0.7984775641025641, 0.7992788461538461, 0.7980769230769231, 0.7996794871794872, 0.7952724358974359, 0.7992788461538461, 0.7992788461538461, 0.7995459401709402, 0.7991452991452992, 0.796875, 0.797142094017094, 0.7995459401709402, 0.7984775641025641, 0.8010149572649573, 0.8015491452991453, 0.7978098290598291, 0.7988782051282052, 0.8000801282051282, 0.8003472222222222, 0.8000801282051282, 0.7996794871794872, 0.7988782051282052, 0.8000801282051282, 0.8018162393162394, 0.8019497863247863, 0.8018162393162394, 0.7982104700854701, 0.8024839743589743, 0.8016826923076923, 0.8048878205128205, 0.8035523504273504, 0.8000801282051282, 0.8022168803418803, 0.7998130341880342, 0.8010149572649573, 0.8000801282051282, 0.8020833333333334, 0.8010149572649573, 0.7987446581196581, 0.8024839743589743, 0.8014155982905983, 0.8015491452991453, 0.8010149572649573, 0.8028846153846154, 0.8022168803418803, 0.8010149572649573, 0.7999465811965812, 0.8010149572649573, 0.8007478632478633, 0.8012820512820513, 0.7999465811965812, 0.8000801282051282, 0.8003472222222222, 0.8031517094017094, 0.8002136752136753, 0.8008814102564102, 0.8002136752136753, 0.8010149572649573, 0.8007478632478633, 0.8019497863247863, 0.8011485042735043, 0.8007478632478633, 0.8008814102564102, 0.7979433760683761, 0.8024839743589743, 0.8003472222222222, 0.7982104700854701, 0.8020833333333334, 0.7975427350427351, 0.7988782051282052, 0.8002136752136753, 0.8003472222222222, 0.8030181623931624, 0.8016826923076923, 0.8004807692307693, 0.8007478632478633, 0.8011485042735043, 0.797676282051282, 0.7992788461538461, 0.7980769230769231], "accuracy_test": 0.8004807692307693, "start": "2016-01-20 08:06:23.762000", "learning_rate_per_epoch": [0.0003778719110414386, 0.00026719580637291074, 0.0002181644522352144, 0.0001889359555207193, 0.00016898945614229888, 0.00015426556637976319, 0.00014282215852290392, 0.00013359790318645537, 0.00012595730368047953, 0.00011949359031859785, 0.00011393267050152645, 0.0001090822261176072, 0.00010480281343916431, 0.00010099051723955199, 9.75661096163094e-05, 9.446797776035964e-05, 9.164740185951814e-05, 8.906526636565104e-05, 8.668976079206914e-05, 8.449472807114944e-05, 8.245841308962554e-05, 8.056256046984345e-05, 7.879173790570349e-05, 7.713278318988159e-05, 7.557438220828772e-05, 7.410677790176123e-05, 7.272148650372401e-05, 7.141107926145196e-05, 7.0169051468838e-05, 6.898965511936694e-05, 6.786779704270884e-05, 6.679895159322768e-05, 6.577905878657475e-05, 6.48045024718158e-05, 6.387200846802443e-05, 6.297865184023976e-05, 6.212176231201738e-05, 6.129891698947176e-05, 6.0507933085318655e-05, 5.9746795159298927e-05, 5.901367694605142e-05, 5.830690133734606e-05, 5.762492583016865e-05, 5.696633525076322e-05, 5.632981992675923e-05, 5.571417568717152e-05, 5.511828203452751e-05, 5.45411130588036e-05, 5.398170105763711e-05, 5.34391583641991e-05, 5.2912651881342754e-05, 5.2401406719582155e-05, 5.1904698921134695e-05, 5.142185545992106e-05, 5.0952239689650014e-05, 5.0495258619775996e-05, 5.005035927752033e-05, 4.9617014155955985e-05, 4.919473212794401e-05, 4.87830548081547e-05, 4.838154200115241e-05, 4.798978261533193e-05, 4.760738738696091e-05, 4.723398888017982e-05, 4.6869241487002e-05, 4.6512817789334804e-05, 4.6164401283022016e-05, 4.582370092975907e-05, 4.549043296719901e-05, 4.5164331822888926e-05, 4.484514647629112e-05, 4.453263318282552e-05, 4.422656274982728e-05, 4.3926716898567975e-05, 4.3632891902234405e-05, 4.334488039603457e-05, 4.306250048102811e-05, 4.2785570258274674e-05, 4.2513911466812715e-05, 4.224736403557472e-05, 4.1985767893493176e-05, 4.172897388343699e-05, 4.147683284827508e-05, 4.122920654481277e-05, 4.0985964005813e-05, 4.0746977902017534e-05, 4.0512120904168114e-05, 4.0281280234921724e-05, 4.005434311693534e-05, 3.983119677286595e-05, 3.961173933930695e-05, 3.939586895285174e-05, 3.918349102605134e-05, 3.8974510971456766e-05, 3.876884147757664e-05, 3.8566391594940796e-05, 3.836708128801547e-05, 3.81708268832881e-05, 3.7977555621182546e-05, 3.778719110414386e-05, 3.7599660572595894e-05, 3.741489490494132e-05, 3.72328249795828e-05, 3.705338895088062e-05, 3.687652133521624e-05, 3.6702163924928755e-05, 3.653025851235725e-05, 3.6360743251862004e-05, 3.619356357376091e-05, 3.602867218432948e-05, 3.58660145138856e-05, 3.570553963072598e-05, 3.554720024112612e-05, 3.539094905136153e-05, 3.523673876770772e-05, 3.5084525734419e-05, 3.49342699337285e-05, 3.478592770989053e-05, 3.463946268311702e-05, 3.449482755968347e-05, 3.435199323575944e-05, 3.421091605559923e-05, 3.407156327739358e-05, 3.393389852135442e-05, 3.37978926836513e-05, 3.366350574651733e-05, 3.353071224410087e-05, 3.339947579661384e-05, 3.3269767300225794e-05, 3.314155765110627e-05, 3.3014821383403614e-05, 3.2889529393287376e-05, 3.276564893894829e-05, 3.264316183049232e-05, 3.25220353261102e-05, 3.24022512359079e-05, 3.228377681807615e-05, 3.216659388272092e-05, 3.2050676963990554e-05, 3.1936004234012216e-05, 3.182255750289187e-05, 3.1710307666799054e-05, 3.1599236535839736e-05, 3.148932592011988e-05], "accuracy_train_last": 0.9432417168674698, "error_valid": [0.5714476495726496, 0.5030715811965811, 0.4026442307692307, 0.3835470085470085, 0.34869123931623935, 0.3334668803418803, 0.3239850427350427, 0.3159722222222222, 0.29834401709401714, 0.28846153846153844, 0.2844551282051282, 0.280982905982906, 0.2729700854700855, 0.265892094017094, 0.265892094017094, 0.2608173076923077, 0.25307158119658124, 0.25333867521367526, 0.24973290598290598, 0.2433226495726496, 0.2398504273504274, 0.23998397435897434, 0.2359775641025641, 0.23517628205128205, 0.23571047008547008, 0.23170405982905984, 0.22970085470085466, 0.22622863247863245, 0.22743055555555558, 0.22329059829059827, 0.22275641025641024, 0.22369123931623935, 0.21834935897435892, 0.2168803418803419, 0.22155448717948723, 0.21594551282051277, 0.21821581196581197, 0.21541132478632474, 0.21153846153846156, 0.21380876068376065, 0.21220619658119655, 0.21180555555555558, 0.21113782051282048, 0.20900106837606836, 0.21100427350427353, 0.20966880341880345, 0.20900106837606836, 0.20846688034188032, 0.2079326923076923, 0.2069978632478633, 0.20606303418803418, 0.20806623931623935, 0.20766559829059827, 0.20686431623931623, 0.20606303418803418, 0.20459401709401714, 0.2040598290598291, 0.20165598290598286, 0.20419337606837606, 0.20138888888888884, 0.20232371794871795, 0.20432692307692313, 0.2024572649572649, 0.2049946581196581, 0.2015224358974359, 0.20072115384615385, 0.20192307692307687, 0.20032051282051277, 0.2047275641025641, 0.20072115384615385, 0.20072115384615385, 0.20045405982905984, 0.2008547008547008, 0.203125, 0.20285790598290598, 0.20045405982905984, 0.2015224358974359, 0.1989850427350427, 0.19845085470085466, 0.2021901709401709, 0.20112179487179482, 0.1999198717948718, 0.1996527777777778, 0.1999198717948718, 0.20032051282051277, 0.20112179487179482, 0.1999198717948718, 0.19818376068376065, 0.1980502136752137, 0.19818376068376065, 0.20178952991452992, 0.19751602564102566, 0.1983173076923077, 0.19511217948717952, 0.1964476495726496, 0.1999198717948718, 0.19778311965811968, 0.20018696581196582, 0.1989850427350427, 0.1999198717948718, 0.19791666666666663, 0.1989850427350427, 0.2012553418803419, 0.19751602564102566, 0.19858440170940173, 0.19845085470085466, 0.1989850427350427, 0.19711538461538458, 0.19778311965811968, 0.1989850427350427, 0.20005341880341876, 0.1989850427350427, 0.1992521367521367, 0.19871794871794868, 0.20005341880341876, 0.1999198717948718, 0.1996527777777778, 0.19684829059829057, 0.19978632478632474, 0.19911858974358976, 0.19978632478632474, 0.1989850427350427, 0.1992521367521367, 0.1980502136752137, 0.19885149572649574, 0.1992521367521367, 0.19911858974358976, 0.20205662393162394, 0.19751602564102566, 0.1996527777777778, 0.20178952991452992, 0.19791666666666663, 0.2024572649572649, 0.20112179487179482, 0.19978632478632474, 0.1996527777777778, 0.19698183760683763, 0.1983173076923077, 0.19951923076923073, 0.1992521367521367, 0.19885149572649574, 0.20232371794871795, 0.20072115384615385, 0.20192307692307687], "accuracy_train_std": [0.08776888816773437, 0.09089412940369504, 0.08715260250967426, 0.08402866726110604, 0.08312393764867072, 0.080244709571875, 0.0798974925444793, 0.07939891599063244, 0.07689125948958017, 0.0771794266665561, 0.07475114049125581, 0.07398774100492993, 0.07417561355787035, 0.0740844968124468, 0.07290803882134013, 0.07382217412529775, 0.07125432070909662, 0.07162388541090765, 0.06953814620529876, 0.06920487728270733, 0.06861240833214467, 0.06821353801258784, 0.06810738947614567, 0.06817079301550687, 0.06728733534982072, 0.06618530889267127, 0.0648238823900431, 0.06555841461537962, 0.0653282538309899, 0.06355560376151506, 0.06395395649744107, 0.06442627152977275, 0.06403584262206119, 0.06342295034228741, 0.06425876691679373, 0.064521213910016, 0.06223506205686297, 0.06165981276341479, 0.06341262524971207, 0.06207912458074253, 0.061482389717294195, 0.060591041280613926, 0.06130714032198451, 0.06153353852939353, 0.059997489405258356, 0.06054573178931872, 0.06045604136643081, 0.059689843438591864, 0.05942394541785036, 0.05917267405338971, 0.05909683038658365, 0.0587940219416114, 0.0585223934364204, 0.05739677740689842, 0.05805736320753928, 0.058329424744187564, 0.05736081459944467, 0.05723379207486049, 0.05779273572561468, 0.0567736371006167, 0.05666751330187889, 0.057685803778785245, 0.05577229800374673, 0.05726133503528748, 0.05517904500938646, 0.055534778148435944, 0.05527517957829497, 0.05497450808278575, 0.05492303302982952, 0.0548601706833474, 0.055394848279503334, 0.055347260967778984, 0.05437952687155789, 0.05278623773124611, 0.05466675400948606, 0.05341726901888958, 0.052966419107530686, 0.05298958121588948, 0.05322763928797106, 0.05356475873147131, 0.05292613828277959, 0.05209708133646674, 0.05353071614144671, 0.051706469740556975, 0.05199132219276186, 0.051973937548811584, 0.05148078312342731, 0.05134995059406465, 0.051900163890033234, 0.050990140611593356, 0.05128363714737734, 0.05059849105080807, 0.04989472313056471, 0.049304480044627834, 0.04910057558091513, 0.050572520329101096, 0.05014741440890862, 0.04941058844013093, 0.04915548941930258, 0.04879547032808367, 0.04894161401056318, 0.04975144604637137, 0.04942501516361426, 0.04908820811720509, 0.04906717683909765, 0.04818228527112799, 0.04806335721942131, 0.0483017573837861, 0.0497454799671575, 0.048872418081738735, 0.049393169912628335, 0.048302857927370824, 0.04878059633392029, 0.049602207169771685, 0.047697695605398376, 0.045702023403219695, 0.04763396434066263, 0.04702217239455075, 0.047612918722264135, 0.047626192497849246, 0.04707696995134861, 0.047678118287084945, 0.04715192489097505, 0.047921849728368146, 0.04658035389823016, 0.04609460029260866, 0.04579275679558022, 0.04715507209405175, 0.045962218423650655, 0.04596863335801253, 0.04732399065090488, 0.04587133736481828, 0.04549762549590535, 0.04575649592080095, 0.04533918543386258, 0.04473395620759525, 0.0462129931769193, 0.04456527255878724, 0.04478700358720272, 0.04438558820830363, 0.04435620465012532, 0.04551167438722196, 0.04392339931276366, 0.04274929131630537], "accuracy_test_std": 0.0765842900099711, "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-3, 3], "momentum": 0.9703622609144249, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.0003778719160179196, "patience_threshold": 1, "do_flip": true, "batch_size": 32, "optimization": "rmsprop", "nb_data_augmentation": 3, "learning_rate_decay_method": "sqrt", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 7.261865486135597e-09, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.07868076915599907}, "accuracy_valid_max": 0.8048878205128205, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = 1234423\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-10, -4], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128, 256],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-3, 3)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_optimizer.learning_rate = learning_rate\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.7980769230769231, "loss_train": [2.049072504043579, 1.4042394161224365, 1.1894136667251587, 1.073683500289917, 0.9986442923545837, 0.9405171275138855, 0.8939895033836365, 0.8535770773887634, 0.8186051845550537, 0.7886183857917786, 0.7660761475563049, 0.7396590113639832, 0.7202213406562805, 0.7043452262878418, 0.6872567534446716, 0.6719450950622559, 0.6612784266471863, 0.6455928683280945, 0.6358344554901123, 0.6256400346755981, 0.617515504360199, 0.6060066223144531, 0.5955113172531128, 0.589352548122406, 0.5794284343719482, 0.5738270878791809, 0.5678853988647461, 0.5589683651924133, 0.5543020367622375, 0.5460947751998901, 0.5407902002334595, 0.5356113910675049, 0.5301152467727661, 0.5236027240753174, 0.5181720852851868, 0.5151316523551941, 0.5087352395057678, 0.5042799115180969, 0.5036768913269043, 0.49582093954086304, 0.49159711599349976, 0.48645681142807007, 0.4838705062866211, 0.4790797829627991, 0.475027859210968, 0.4727211892604828, 0.46706223487854004, 0.46470561623573303, 0.4622330665588379, 0.45722243189811707, 0.4562568664550781, 0.45072051882743835, 0.44912001490592957, 0.4450520873069763, 0.4411436915397644, 0.44145864248275757, 0.4361424446105957, 0.43679752945899963, 0.4310510754585266, 0.42667442560195923, 0.42749321460723877, 0.4237865209579468, 0.4228029251098633, 0.4188513159751892, 0.4157213270664215, 0.41485297679901123, 0.4117717444896698, 0.41015809774398804, 0.4075048863887787, 0.40620866417884827, 0.405992716550827, 0.40078502893447876, 0.39831608533859253, 0.39573028683662415, 0.39305004477500916, 0.3936186134815216, 0.3924269378185272, 0.3886476755142212, 0.3879896402359009, 0.3855191767215729, 0.3848112225532532, 0.3828115463256836, 0.37983280420303345, 0.3788296580314636, 0.37479934096336365, 0.375597208738327, 0.3735203146934509, 0.3700631856918335, 0.36908721923828125, 0.3668600618839264, 0.3668558597564697, 0.3658008873462677, 0.36442315578460693, 0.3607829511165619, 0.35773393511772156, 0.3601089119911194, 0.3576560914516449, 0.3561430871486664, 0.35549989342689514, 0.35217010974884033, 0.3527785837650299, 0.3513302505016327, 0.3475683629512787, 0.3471854031085968, 0.3461320102214813, 0.34780052304267883, 0.344461053609848, 0.3450789451599121, 0.3404883146286011, 0.3407706618309021, 0.3398141860961914, 0.33782872557640076, 0.33639487624168396, 0.3351607918739319, 0.3354032337665558, 0.3334919512271881, 0.32889023423194885, 0.3313187062740326, 0.32828643918037415, 0.3261512815952301, 0.3275454044342041, 0.323944628238678, 0.32474225759506226, 0.32152751088142395, 0.3206257224082947, 0.32091087102890015, 0.31852424144744873, 0.3183172941207886, 0.3173690438270569, 0.31635892391204834, 0.31531578302383423, 0.31575047969818115, 0.313468873500824, 0.31071770191192627, 0.31199783086776733, 0.311987966299057, 0.3106750249862671, 0.31002941727638245, 0.30732905864715576, 0.3046741187572479, 0.3064209818840027, 0.3046466112136841, 0.30332937836647034, 0.30280327796936035], "accuracy_train_first": 0.43460560993975905, "model": "residualv2", "loss_std": [3.8199703693389893, 0.19242101907730103, 0.18849676847457886, 0.18414461612701416, 0.18222056329250336, 0.17993874847888947, 0.17629790306091309, 0.17562271654605865, 0.1728530079126358, 0.1711103916168213, 0.16884693503379822, 0.16594816744327545, 0.16549602150917053, 0.1654825210571289, 0.16383731365203857, 0.16329534351825714, 0.1622147411108017, 0.161683589220047, 0.15992295742034912, 0.16105569899082184, 0.15767140686511993, 0.1587052196264267, 0.15906725823879242, 0.15710671246051788, 0.1575329452753067, 0.15533249080181122, 0.15383324027061462, 0.153756245970726, 0.15475042164325714, 0.15317782759666443, 0.15297725796699524, 0.1531694084405899, 0.15165570378303528, 0.15211790800094604, 0.14895471930503845, 0.1488211303949356, 0.1483924388885498, 0.1483808159828186, 0.14845038950443268, 0.14866110682487488, 0.1469094455242157, 0.1489974558353424, 0.14840370416641235, 0.1455899029970169, 0.14489662647247314, 0.14749927818775177, 0.14482846856117249, 0.14397744834423065, 0.14331579208374023, 0.14384451508522034, 0.14483751356601715, 0.14121684432029724, 0.14257100224494934, 0.1407533884048462, 0.1432255655527115, 0.14190982282161713, 0.13950273394584656, 0.1389482468366623, 0.14009244740009308, 0.14050491154193878, 0.13733066618442535, 0.1393299102783203, 0.1400819718837738, 0.13634279370307922, 0.13766475021839142, 0.1369129866361618, 0.13776463270187378, 0.13891346752643585, 0.13523149490356445, 0.1372060626745224, 0.13744677603244781, 0.137491375207901, 0.13470356166362762, 0.13439112901687622, 0.1345389038324356, 0.13708709180355072, 0.1356123685836792, 0.13423286378383636, 0.13111962378025055, 0.13347776234149933, 0.13203929364681244, 0.13212238252162933, 0.133416548371315, 0.1316189467906952, 0.13199205696582794, 0.1332414299249649, 0.1311781108379364, 0.13153234124183655, 0.12991058826446533, 0.13165752589702606, 0.13056258857250214, 0.12892363965511322, 0.12929578125476837, 0.12967059016227722, 0.12880228459835052, 0.12999393045902252, 0.1279396116733551, 0.13029833137989044, 0.12822775542736053, 0.1283789575099945, 0.1287367343902588, 0.126039057970047, 0.12809601426124573, 0.12598435580730438, 0.12650366127490997, 0.12663516402244568, 0.1275821179151535, 0.12752588093280792, 0.12430135160684586, 0.1253959685564041, 0.12542076408863068, 0.12419308722019196, 0.12382514029741287, 0.12430379539728165, 0.12323109060525894, 0.12551036477088928, 0.12279774248600006, 0.12325127422809601, 0.1223670020699501, 0.12260332703590393, 0.1252979040145874, 0.12357697635889053, 0.12002823501825333, 0.12206069380044937, 0.1206631064414978, 0.12396985292434692, 0.12176279723644257, 0.1207297071814537, 0.11813531816005707, 0.12120642513036728, 0.11977040767669678, 0.11915451288223267, 0.12100595980882645, 0.11845646798610687, 0.11911313980817795, 0.12014226615428925, 0.11922656744718552, 0.11816684156656265, 0.11856427043676376, 0.1145438477396965, 0.11597510427236557, 0.11820971965789795, 0.11897112429141998, 0.11766120791435242]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:04 2016", "state": "available"}], "summary": "2dd2569f53d516fd7a1f091049b65464"}