{"content": {"hp_model": {"f0": 16, "f1": 64, "f2": 16, "f3": 16, "nonlin": "rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.6510130167007446, 1.2479536533355713, 1.0804786682128906, 0.9675082564353943, 0.8865535855293274, 0.8230351805686951, 0.7727580666542053, 0.7319194674491882, 0.6949281692504883, 0.6643768548965454, 0.6375093460083008, 0.6143316626548767, 0.5901015996932983, 0.5693092942237854, 0.5519859790802002, 0.5329705476760864, 0.5162773728370667, 0.502233624458313, 0.4864637851715088, 0.4719700217247009, 0.4594842195510864, 0.44721537828445435, 0.4358691871166229, 0.423727810382843, 0.4123990833759308, 0.4022631049156189, 0.3947546184062958, 0.3821849822998047, 0.3720710575580597, 0.3646697402000427, 0.35822975635528564, 0.34848278760910034, 0.3421783745288849, 0.33316031098365784, 0.32770028710365295, 0.3220185339450836, 0.31556621193885803, 0.3075243830680847, 0.3014623522758484, 0.2984083294868469, 0.2881283462047577, 0.28495898842811584, 0.2787111699581146, 0.2753143310546875, 0.2699578106403351, 0.26505348086357117, 0.2622290253639221, 0.25460827350616455, 0.25218990445137024, 0.2494894415140152, 0.24276292324066162, 0.23891694843769073, 0.23537924885749817, 0.23129385709762573, 0.23059646785259247, 0.22576816380023956, 0.2230512499809265, 0.21943581104278564, 0.21672706305980682, 0.2113182693719864, 0.2097151279449463, 0.20775540173053741, 0.2047787755727768, 0.20133647322654724, 0.19869111478328705, 0.19799046218395233, 0.19820836186408997, 0.1891087293624878, 0.18748241662979126, 0.1866963505744934, 0.18525628745555878, 0.18276560306549072, 0.17939455807209015, 0.17745624482631683, 0.175524041056633, 0.17359277606010437, 0.17272275686264038, 0.17120976746082306, 0.17215244472026825, 0.1649969518184662, 0.16540510952472687, 0.16130688786506653, 0.16296285390853882, 0.16054926812648773, 0.15734083950519562, 0.1559453159570694, 0.1555090993642807, 0.15625305473804474, 0.15210740268230438, 0.15197551250457764, 0.1472444087266922, 0.15105769038200378, 0.14579029381275177, 0.14532865583896637, 0.14363668859004974, 0.14293546974658966, 0.14274969696998596, 0.1405256986618042, 0.13975952565670013, 0.13818466663360596, 0.13759002089500427, 0.13646471500396729, 0.13545042276382446, 0.13297432661056519, 0.13336093723773956, 0.13049079477787018, 0.13113033771514893, 0.13014432787895203, 0.13111717998981476, 0.12873327732086182, 0.12757672369480133, 0.12746775150299072, 0.12433483451604843, 0.12520281970500946, 0.1236552745103836, 0.1214948520064354, 0.12261517345905304, 0.12080521136522293, 0.12217303365468979, 0.1202540248632431, 0.11894537508487701, 0.11758635193109512, 0.11821848154067993, 0.11680369824171066, 0.09118817746639252, 0.07476790994405746, 0.06865445524454117, 0.06581907719373703, 0.06462118774652481, 0.06344868242740631, 0.06171592324972153, 0.06003951281309128, 0.059258539229631424, 0.05804349109530449, 0.05705544725060463, 0.05660798400640488, 0.0560680590569973, 0.05586284026503563, 0.055966079235076904, 0.05456763505935669, 0.05366510525345802, 0.05550605431199074, 0.05407824367284775, 0.05330750346183777, 0.0526592880487442, 0.05191650986671448, 0.05150001868605614, 0.051419731229543686, 0.05112968757748604, 0.04976115748286247, 0.05058515444397926, 0.05129290744662285, 0.049507033079862595, 0.04967408627271652, 0.0495661199092865, 0.05057079717516899, 0.05000179633498192, 0.049482278525829315, 0.05030277371406555, 0.05087222158908844, 0.04996820539236069, 0.04994179680943489, 0.04968995973467827, 0.05066203698515892, 0.05072388797998428, 0.05034029483795166, 0.05060455575585365, 0.05048152804374695, 0.050573959946632385, 0.04919317737221718, 0.0499122180044651, 0.05032070726156235, 0.04957933723926544, 0.05006227269768715, 0.050299931317567825], "moving_avg_accuracy_train": [0.0482470588235294, 0.09988352941176468, 0.15214694117647054, 0.20323107058823525, 0.2536538458823529, 0.29432610835294115, 0.3386723210470588, 0.38072744188352936, 0.41879822710694115, 0.4570878161609529, 0.49106374042721057, 0.5221126605021366, 0.5499484532754523, 0.5756430197126129, 0.600605776564881, 0.6226816694966282, 0.6412746790175536, 0.6618977993510924, 0.6807856664748066, 0.6982647468861495, 0.713075919256358, 0.7279753861542517, 0.7397684357741207, 0.7498104157261203, 0.7605870212123318, 0.7695730249734516, 0.7800651342408124, 0.788023326699084, 0.7972515822644697, 0.8039711299203756, 0.8111857816342203, 0.8191213211178571, 0.827192718417836, 0.8305252112819348, 0.8372491607419765, 0.8430983623148377, 0.8476520554951186, 0.8517786146514891, 0.8564289884804578, 0.8619272661030002, 0.8652004218456414, 0.8702780267199008, 0.8734172828714402, 0.8728543781137079, 0.8777148226552783, 0.8817292227426917, 0.8814880651743049, 0.8844286704215802, 0.8865905092617752, 0.8907691053944211, 0.8935839595608613, 0.8961973283106576, 0.8975870072442977, 0.9003318359316327, 0.9042398288090576, 0.9066252576928577, 0.9072803789823954, 0.9109193999076852, 0.9121451069757402, 0.9139753021605191, 0.9157683601797614, 0.9179679947500204, 0.9189641364514889, 0.9144959581004576, 0.9118910681727648, 0.9118196084143119, 0.9101011769846454, 0.9110581181097103, 0.9138534827693274, 0.9169951933159242, 0.9197897916313906, 0.9216908124682516, 0.9157523194567205, 0.9182312051581073, 0.9208010258187671, 0.9228079820604198, 0.9252965956190837, 0.9273692889983518, 0.9295076542161637, 0.9291921829121943, 0.931743552856269, 0.9340303740412303, 0.9351520425194603, 0.9367474265028083, 0.9379832720878215, 0.9370767095849217, 0.9375149209793707, 0.9372975465284924, 0.9381936742285842, 0.937910777393961, 0.9402467584780942, 0.9419562002773436, 0.9433205802496093, 0.9441649928128837, 0.9454731994139483, 0.9462364677078476, 0.9467892915252982, 0.9484491859021801, 0.948284267311962, 0.9494299582278246, 0.9462046094638656, 0.947233560282185, 0.9469807924892606, 0.9407791838285698, 0.9413577360339481, 0.9427325506658475, 0.9444334132463216, 0.9451806601569835, 0.946032005905991, 0.9459417464918625, 0.9473334541956174, 0.9471154028937028, 0.9483544508396267, 0.9497331234027228, 0.950717458121274, 0.9506433593679701, 0.9513860822547024, 0.9514357093233499, 0.9489321383910149, 0.9506742186695605, 0.9511009144496633, 0.9508943524164617, 0.9516402112924626, 0.9526385431043928, 0.9565276299704241, 0.9601736905027934, 0.9634974979231024, 0.9665665716602039, 0.9693052086118306, 0.9717793936330005, 0.9740696895638181, 0.9761121323721422, 0.9779644485466927, 0.9796362389861412, 0.9811408503816447, 0.9825067653434802, 0.9837007946914851, 0.9848083622811601, 0.9857557613471617, 0.9867401852124456, 0.9875791078676716, 0.9883035500220809, 0.9890567244316375, 0.9896357578708267, 0.9902133585543322, 0.9907308462283109, 0.9912459968995975, 0.991707279562579, 0.9921271398416152, 0.9925144258574536, 0.9928606303305317, 0.9931745672974785, 0.9934406399794955, 0.9936636348050754, 0.9938808007363324, 0.994076250074464, 0.9942545074199588, 0.9944102331485511, 0.9945480333631078, 0.9946744064973853, 0.9948046129064702, 0.9949288574981761, 0.9950242070424761, 0.9950935510441108, 0.9951512547632291, 0.9952055410516121, 0.9952732222405686, 0.9953153117812177, 0.9953720158972136, 0.9954277554839628, 0.9954896858179194, 0.9955195407655392, 0.9955746455125146, 0.9956077691965572, 0.9956352275710192], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.047399999999999984, 0.09787333333333331, 0.14833933333333332, 0.19747873333333332, 0.24570419333333332, 0.285173774, 0.32790972993333334, 0.36823875693999997, 0.404614881246, 0.4409133931214, 0.4728353871425933, 0.5020451817616673, 0.5277873302521673, 0.5515419305602839, 0.5741877375042554, 0.59456896375383, 0.611472067378447, 0.6301248606406022, 0.6467257079098754, 0.6620264704522212, 0.6743971567403324, 0.6870774410662992, 0.6975563636263359, 0.7061073939303689, 0.714856654537332, 0.7223843224169322, 0.7302925568419056, 0.7365033011577151, 0.7437196377086103, 0.7490276739377493, 0.7543382398773077, 0.7603177492229103, 0.766152640967286, 0.7684440435372241, 0.773572972516835, 0.7772156752651515, 0.7804941077386363, 0.7832313636314393, 0.786534893934962, 0.7903614045414659, 0.7923785974206526, 0.7961807376785874, 0.798135997244062, 0.7973757308529892, 0.8006914911010236, 0.8033956753242546, 0.8030561077918291, 0.8049371636793129, 0.8058701139780483, 0.8083231025802435, 0.810077458988886, 0.8122297130899974, 0.8128334084476643, 0.8145100676028978, 0.8172457275092747, 0.8182944880916805, 0.8183983726158458, 0.8213985353542612, 0.8222853484855018, 0.8230701469702849, 0.8242031322732564, 0.8256094857125974, 0.825701870474671, 0.8225983500938706, 0.8203118484178169, 0.8202539969093686, 0.8184685972184318, 0.8189817374965886, 0.8209368970802632, 0.8231498740389035, 0.8246615533016798, 0.8260487313048451, 0.821217191507694, 0.8228421390235913, 0.8246112584545655, 0.8261767992757757, 0.8277057860148649, 0.8292152074133784, 0.8308936866720406, 0.8302843180048365, 0.8315758862043529, 0.8335916309172509, 0.8341924678255257, 0.8356798877096399, 0.8367518989386759, 0.8361700423781416, 0.8368597048069941, 0.8367070676596281, 0.8370763608936653, 0.8366753914709655, 0.8378211856572022, 0.838799067091482, 0.8394924937156671, 0.8392499110107671, 0.8401249199096904, 0.840179094585388, 0.8404145184601824, 0.8414130666141643, 0.8412317599527479, 0.8419619172908065, 0.8390057255617258, 0.8397718196722199, 0.8398746377049979, 0.835087173934498, 0.8350051232077149, 0.8361579442202768, 0.837182149798249, 0.8377439348184241, 0.8387828746699151, 0.8386512538695903, 0.8403861284826313, 0.8396141823010348, 0.8404927640709313, 0.8413768209971715, 0.8420791388974543, 0.8417778916743756, 0.842400102506938, 0.8420534255895774, 0.840141416363953, 0.8414472747275576, 0.8417025472548019, 0.8414122925293217, 0.8417110632763896, 0.8423532902820839, 0.8458646279205422, 0.8489048317951546, 0.8516410152823058, 0.8541835804207418, 0.8562585557120009, 0.8582593668074675, 0.8599534301267207, 0.8615314204473818, 0.8627649450693103, 0.8638884505623794, 0.8648729388394748, 0.8657856449555272, 0.8666737471266412, 0.8674330390806438, 0.8681564018392461, 0.8686740949886548, 0.8694333521564559, 0.8698633502741436, 0.8702103485800625, 0.8705359803887229, 0.871015715683184, 0.8712608107815323, 0.8715880630367123, 0.8718825900663744, 0.8720943310597369, 0.8723382312870965, 0.8725977414917202, 0.8727646340092149, 0.8730215039416268, 0.8732260202141308, 0.8733700848593844, 0.873513076373446, 0.8736017687361014, 0.8736415918624914, 0.8737974326762422, 0.873857689408618, 0.8739785871344228, 0.8741407284209805, 0.8742066555788824, 0.8743459900209942, 0.8743380576855615, 0.8744775852503387, 0.8744831600586381, 0.874461510719441, 0.8744686929808302, 0.8745418236827471, 0.8746343079811391, 0.8745575438496918, 0.8746484561313893, 0.8746636105182504, 0.8746505827997586], "moving_var_accuracy_train": [0.02095000816608996, 0.04285193320276816, 0.06314991776607612, 0.08032122048928983, 0.09517120485561058, 0.1005421807803373, 0.10818724192511087, 0.11328621642973108, 0.1150020569745023, 0.11669668494637796, 0.11541628731945844, 0.11255097752788489, 0.1082693620089672, 0.10338432250761345, 0.09865414332384125, 0.09317483443006282, 0.08696865101446354, 0.08209960374364177, 0.07710040708962537, 0.07214003064889855, 0.06690036502682897, 0.06220827554871892, 0.057239132167877245, 0.0524227912032968, 0.048225729115216116, 0.04412989057604823, 0.04070766073034754, 0.03720689010213882, 0.03425264739894559, 0.031233753545350955, 0.028578838984984617, 0.026287710168553382, 0.024245266241065017, 0.021920689195161935, 0.020135523742716512, 0.018429889799804516, 0.0167735259140453, 0.015249429736879995, 0.01391912055393441, 0.01279928800987193, 0.011615781145525, 0.010686241672304425, 0.00970631186773878, 0.008738532436861401, 0.008077294483450393, 0.007414603707661772, 0.006673666749650707, 0.006084124507668369, 0.005517773981440309, 0.0051231425740541564, 0.004682138952453675, 0.00427539232321001, 0.003865233958736438, 0.0035465173235681453, 0.0033293172661813695, 0.0030475979782002434, 0.0027467008355162694, 0.0025912130116169207, 0.0023456129308053486, 0.002141198167454305, 0.0019560138642521936, 0.001803958008011087, 0.0016324928918146203, 0.001648925162622776, 0.001545101710179058, 0.0013906374976348555, 0.0012781508070775602, 0.001158577353221368, 0.0011130461901213626, 0.0010905746775365967, 0.0010518052274862061, 0.0009791496267372005, 0.0011986259572955077, 0.001134067230450815, 0.0010800963114573222, 0.0010083375405147686, 0.0009632425634625839, 0.0009055828277164818, 0.0008561779971875624, 0.000771455896761459, 0.0007528957044090623, 0.000724672094156048, 0.0006635281463159346, 0.0006200825821732537, 0.0005718201527458999, 0.0005220348376162855, 0.0004715596168906814, 0.00042482892006866484, 0.0003895734317556471, 0.00035133636415144045, 0.00036531399636515426, 0.00035508231811382814, 0.0003363278806809215, 0.0003091123858059696, 0.00029360378782499304, 0.00026948661543874103, 0.0002452884814531321, 0.0002455568773894577, 0.0002212459729231074, 0.00021093484470300784, 0.0002834672320752498, 0.0002646491669464048, 0.00023875927426602307, 0.0005610228966626126, 0.0005079331108854844, 0.00047415083724569563, 0.0004527721551800383, 0.000412520341171479, 0.00037779141331350987, 0.0003400855928387083, 0.0003235086865490548, 0.00029158573522654953, 0.0002762443200145774, 0.00026572653033922705, 0.0002478741108486127, 0.00022313611539092213, 0.00020578723943011498, 0.0001852306811005863, 0.00022311841970962167, 0.00022812017101073634, 0.0002069467775084803, 0.00018663611061967578, 0.00017297924872389207, 0.00016465132151190955, 0.00028431115922455377, 0.00037552385995341187, 0.0004374007358637745, 0.0004784335847112938, 0.0004980914174154997, 0.0005033765993447824, 0.0005002480384667809, 0.00048776738824757526, 0.00046987032631732976, 0.0004480372431464756, 0.0004236082178951398, 0.00039803890925232136, 0.00037106637308216285, 0.0003450000894652334, 0.0003185781654310571, 0.0002954421620068147, 0.00027223206679919683, 0.0002497322080350439, 0.0002298644324524381, 0.00020989550672048765, 0.00019190855899471374, 0.00017512784452971983, 0.00016000348200389075, 0.0001459181690600068, 0.0001329128960392175, 0.00012097152055787216, 0.00010995308633669891, 9.984478547597097e-05, 9.049745897741461e-05, 8.189525330979168e-05, 7.413017735410167e-05, 6.706096361267572e-05, 6.0640848382413754e-05, 5.479501806708297e-05, 4.94864163525614e-05, 4.468150623890924e-05, 4.036593899571962e-05, 3.646827556326121e-05, 3.2903271827319124e-05, 2.9656221959651686e-05, 2.6720567236487302e-05, 2.407503352279617e-05, 2.1708756860563564e-05, 1.955382493939562e-05, 1.762738065639392e-05, 1.58926047045333e-05, 1.4337862530455825e-05, 1.2912098138486686e-05, 1.1648217122891124e-05, 1.0493270016603033e-05, 9.450728675895548e-06], "duration": 163659.415797, "accuracy_train": [0.4824705882352941, 0.5646117647058824, 0.6225176470588235, 0.6629882352941177, 0.7074588235294118, 0.6603764705882353, 0.7377882352941176, 0.7592235294117647, 0.7614352941176471, 0.8016941176470588, 0.7968470588235295, 0.8015529411764706, 0.8004705882352942, 0.8068941176470589, 0.8252705882352941, 0.8213647058823529, 0.8086117647058824, 0.8475058823529412, 0.8507764705882352, 0.8555764705882353, 0.8463764705882353, 0.8620705882352941, 0.8459058823529412, 0.8401882352941177, 0.8575764705882353, 0.8504470588235294, 0.8744941176470589, 0.8596470588235294, 0.8803058823529412, 0.8644470588235295, 0.8761176470588236, 0.8905411764705883, 0.8998352941176471, 0.8605176470588235, 0.8977647058823529, 0.8957411764705883, 0.8886352941176471, 0.8889176470588235, 0.8982823529411764, 0.9114117647058824, 0.8946588235294117, 0.9159764705882353, 0.9016705882352941, 0.8677882352941176, 0.9214588235294118, 0.9178588235294117, 0.8793176470588235, 0.9108941176470589, 0.9060470588235294, 0.9283764705882352, 0.9189176470588235, 0.9197176470588235, 0.9100941176470588, 0.9250352941176471, 0.9394117647058824, 0.9280941176470588, 0.9131764705882353, 0.9436705882352941, 0.9231764705882353, 0.9304470588235294, 0.9319058823529411, 0.937764705882353, 0.9279294117647059, 0.8742823529411765, 0.8884470588235294, 0.9111764705882353, 0.8946352941176471, 0.9196705882352941, 0.9390117647058823, 0.9452705882352941, 0.9449411764705883, 0.9388, 0.8623058823529411, 0.9405411764705882, 0.9439294117647059, 0.9408705882352941, 0.9476941176470588, 0.9460235294117647, 0.9487529411764706, 0.9263529411764706, 0.9547058823529412, 0.9546117647058824, 0.9452470588235294, 0.9511058823529411, 0.9491058823529411, 0.9289176470588235, 0.9414588235294118, 0.9353411764705882, 0.9462588235294118, 0.935364705882353, 0.9612705882352941, 0.9573411764705883, 0.9556, 0.951764705882353, 0.9572470588235295, 0.9531058823529411, 0.951764705882353, 0.9633882352941177, 0.9468, 0.9597411764705882, 0.9171764705882353, 0.9564941176470588, 0.9447058823529412, 0.884964705882353, 0.946564705882353, 0.9551058823529411, 0.9597411764705882, 0.9519058823529412, 0.9536941176470588, 0.9451294117647059, 0.9598588235294118, 0.9451529411764706, 0.9595058823529412, 0.9621411764705883, 0.9595764705882353, 0.9499764705882353, 0.9580705882352941, 0.9518823529411765, 0.9264, 0.9663529411764706, 0.9549411764705882, 0.9490352941176471, 0.9583529411764706, 0.9616235294117647, 0.9915294117647059, 0.9929882352941176, 0.9934117647058823, 0.9941882352941176, 0.9939529411764706, 0.9940470588235294, 0.9946823529411765, 0.9944941176470589, 0.9946352941176471, 0.9946823529411765, 0.9946823529411765, 0.9948, 0.9944470588235295, 0.9947764705882353, 0.9942823529411765, 0.9956, 0.9951294117647059, 0.9948235294117647, 0.995835294117647, 0.9948470588235294, 0.9954117647058823, 0.9953882352941177, 0.9958823529411764, 0.9958588235294118, 0.9959058823529412, 0.996, 0.9959764705882352, 0.996, 0.995835294117647, 0.9956705882352941, 0.995835294117647, 0.995835294117647, 0.9958588235294118, 0.9958117647058824, 0.9957882352941176, 0.9958117647058824, 0.9959764705882352, 0.9960470588235294, 0.9958823529411764, 0.9957176470588235, 0.9956705882352941, 0.9956941176470588, 0.9958823529411764, 0.9956941176470588, 0.9958823529411764, 0.9959294117647058, 0.9960470588235294, 0.9957882352941176, 0.9960705882352942, 0.9959058823529412, 0.9958823529411764], "end": "2016-02-07 22:14:54.738000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0], "moving_var_accuracy_valid": [0.020220839999999993, 0.0411267724, 0.059935449564, 0.07567403029884001, 0.08903788219886041, 0.09415472416099693, 0.10117650911071951, 0.10569673197338761, 0.10703606055177892, 0.10819069217591801, 0.10654274627892814, 0.10356738056623169, 0.09917456638977104, 0.0943356390729793, 0.08951756831497408, 0.08430436093440376, 0.07844535905026424, 0.07373216341356417, 0.06883924024272735, 0.06406233622784988, 0.059033407518214655, 0.054577173261679414, 0.0501077262976847, 0.04575503474126066, 0.0418684773176516, 0.03819162163923652, 0.034935321020795884, 0.03178894902332353, 0.029078733739933242, 0.026424437603428583, 0.024035812838471303, 0.02195402234275151, 0.020065033763493608, 0.018105785118781918, 0.01653195981920476, 0.014998187387097617, 0.013595101723736656, 0.012303024679767155, 0.011170942023987078, 0.010185627472383545, 0.009203686329151767, 0.008413424131105664, 0.007606489077710517, 0.006851042214808019, 0.006264886387529225, 0.005704211259594844, 0.005134827888617057, 0.004653190441021887, 0.0041957049632588925, 0.0038302888446754982, 0.0034749598578848475, 0.0031691536515381204, 0.002855518319148126, 0.0025952671605387707, 0.0024030949605951153, 0.0021726845533684785, 0.00195551322598088, 0.0018409706914955793, 0.0016639515601136873, 0.00150309958205778, 0.001364342525122747, 0.0012457087425775896, 0.0011212146828182012, 0.0010957797633227744, 0.0010332545962218657, 0.0009299592577729468, 0.0008656522005032292, 0.000781456796958508, 0.0007377149582413643, 0.0007080188655924855, 0.0006577835467748087, 0.0006093235574095204, 0.0007584851929716692, 0.0007064007635392886, 0.0006639287392348142, 0.00061959412787721, 0.0005786749191242817, 0.0005413126038364679, 0.0005125369770486527, 0.00046462525089691786, 0.00043317606153324314, 0.0004264274961081068, 0.0003870337914104044, 0.00036824217347428573, 0.00034176082880347104, 0.00031063175943645476, 0.0002838492918847459, 0.00025567404558507576, 0.00023133403846091902, 0.00020964762291628955, 0.00020049845947958672, 0.00018905488242720978, 0.00017447695853264794, 0.00015755887999783277, 0.000148693757156804, 0.0001338507955005061, 0.00012096453555786497, 0.00011784196774446185, 0.00010635361991928141, 0.00010051642557223991, 0.00016911640886677933, 0.0001574868696553049, 0.00014183332662055347, 0.00033392827814313016, 0.0003005960412247079, 0.00028249740368527416, 0.00026368863691029354, 0.0002401601948993024, 0.00022585873954451585, 0.00020342878190576764, 0.00021017401302195736, 0.00019451971988529383, 0.00018201490123431505, 0.00017084742095038312, 0.0001582019327528643, 0.00014319848848229192, 0.00013236295651548516, 0.00012020832482921162, 0.00014108950585614623, 0.00014232794986269865, 0.00012868163144491972, 0.00011657169855139996, 0.0001057179043299912, 9.885821363858052e-05, 0.0001999378203760057, 0.0002631295947312819, 0.0003041969359363838, 0.0003319589796914601, 0.00033751278385633817, 0.0003397907108283836, 0.0003316402945122994, 0.0003208867461299755, 0.0003024923184531122, 0.00028360346794440615, 0.0002639660756596099, 0.0002450667601821661, 0.00022765861336098518, 0.00021008147046760358, 0.0001937826065456376, 0.0001768164016635763, 0.00016432300451893603, 0.00014955478949797748, 0.00013568298096697546, 0.00012306900754358095, 0.00011283342036398739, 0.00010209072279269773, 9.28454968601122e-05, 8.434166271491525e-05, 7.631100467785523e-05, 6.92152900982245e-05, 6.289987100513647e-05, 5.6860561916184116e-05, 5.176834518416106e-05, 4.6967952817215284e-05, 4.245794913360215e-05, 3.8396173378084496e-05, 3.462735305701661e-05, 3.117889068387411e-05, 2.8279578848561482e-05, 2.5484298827874787e-05, 2.306741528603042e-05, 2.0997281928686738e-05, 1.893667124715933e-05, 1.7217730903270753e-05, 1.5496524110452438e-05, 1.412208317140106e-05, 1.2710154560649137e-05, 1.1443357349573283e-05, 1.0299485878523925e-05, 9.317670186737367e-06, 8.46288327710509e-06, 7.669629536286216e-06, 6.977051969328576e-06, 6.281413671365934e-06, 5.654799797271231e-06], "accuracy_test": 0.7987, "start": "2016-02-06 00:47:15.323000", "learning_rate_per_epoch": [0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 0.00034257734660059214, 3.425773320486769e-05, 3.425773320486769e-05, 3.425773320486769e-05, 3.425773320486769e-05, 3.425773320486769e-05, 3.425773320486769e-05, 3.425773320486769e-05, 3.425773320486769e-05, 3.425773320486769e-05, 3.425773320486769e-05, 3.425773320486769e-05, 3.425773320486769e-05, 3.425773320486769e-05, 3.425773320486769e-05, 3.425773320486769e-05, 3.425773320486769e-05, 3.425773320486769e-05, 3.425773320486769e-05, 3.425773320486769e-05, 3.425773320486769e-05, 3.425773320486769e-05, 3.425773320486769e-05, 3.425773320486769e-05, 3.425773229537299e-06, 3.425773229537299e-07, 3.425773087428752e-08, 3.4257729986109098e-09, 3.42577299861091e-10, 3.425772859833032e-11, 3.4257729032011186e-12, 3.42577284899101e-13, 3.425772916753646e-14, 3.4257728744019984e-15, 3.4257728744019984e-16, 3.4257728082275494e-17, 3.42577284958658e-18, 3.4257729012853683e-19, 3.425773030532339e-20, 3.425772949752982e-21, 3.4257730002400803e-22, 3.42577300024008e-23, 3.425772842467899e-24, 3.4257729410755124e-25, 3.4257730027052706e-26, 3.425772925668073e-27, 3.425772925668073e-28, 3.425772865482762e-29, 3.425772715019485e-30, 3.425772809059033e-31, 3.425772867833751e-32, 3.425772941302148e-33, 3.425773033137644e-34], "accuracy_train_first": 0.4824705882352941, "accuracy_train_last": 0.9958823529411764, "batch_size_eval": 1024, "accuracy_train_std": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "accuracy_test_std": 0, "error_valid": [0.526, 0.44786666666666664, 0.39746666666666663, 0.3602666666666666, 0.3202666666666667, 0.35960000000000003, 0.28746666666666665, 0.26880000000000004, 0.268, 0.23240000000000005, 0.23986666666666667, 0.23506666666666665, 0.24053333333333338, 0.2346666666666667, 0.22199999999999998, 0.22199999999999998, 0.23640000000000005, 0.20199999999999996, 0.20386666666666664, 0.2002666666666667, 0.21426666666666672, 0.19879999999999998, 0.20813333333333328, 0.2169333333333333, 0.20640000000000003, 0.20986666666666665, 0.19853333333333334, 0.2076, 0.19133333333333336, 0.20320000000000005, 0.19786666666666664, 0.18586666666666662, 0.18133333333333335, 0.2109333333333333, 0.1802666666666667, 0.18999999999999995, 0.18999999999999995, 0.19213333333333338, 0.1837333333333333, 0.17520000000000002, 0.18946666666666667, 0.16959999999999997, 0.1842666666666667, 0.2094666666666667, 0.16946666666666665, 0.17226666666666668, 0.19999999999999996, 0.17813333333333337, 0.1857333333333333, 0.16959999999999997, 0.17413333333333336, 0.1684, 0.1817333333333333, 0.1704, 0.15813333333333335, 0.17226666666666668, 0.18066666666666664, 0.15159999999999996, 0.1697333333333333, 0.16986666666666672, 0.16559999999999997, 0.16173333333333328, 0.17346666666666666, 0.20533333333333337, 0.2002666666666667, 0.1802666666666667, 0.1976, 0.1764, 0.16146666666666665, 0.15693333333333337, 0.16173333333333328, 0.16146666666666665, 0.2222666666666666, 0.1625333333333333, 0.15946666666666665, 0.15973333333333328, 0.1585333333333333, 0.1572, 0.15400000000000003, 0.17520000000000002, 0.15680000000000005, 0.14826666666666666, 0.1604, 0.15093333333333336, 0.15359999999999996, 0.1690666666666667, 0.15693333333333337, 0.16466666666666663, 0.15959999999999996, 0.16693333333333338, 0.1518666666666667, 0.15239999999999998, 0.15426666666666666, 0.16293333333333337, 0.15200000000000002, 0.15933333333333333, 0.15746666666666664, 0.14959999999999996, 0.1604, 0.15146666666666664, 0.1876, 0.15333333333333332, 0.1592, 0.20799999999999996, 0.1657333333333333, 0.15346666666666664, 0.15359999999999996, 0.1572, 0.1518666666666667, 0.1625333333333333, 0.14400000000000002, 0.16733333333333333, 0.15159999999999996, 0.15066666666666662, 0.15159999999999996, 0.16093333333333337, 0.15200000000000002, 0.1610666666666667, 0.1770666666666667, 0.14680000000000004, 0.15600000000000003, 0.1612, 0.15559999999999996, 0.1518666666666667, 0.12253333333333338, 0.12373333333333336, 0.12373333333333336, 0.12293333333333334, 0.12506666666666666, 0.12373333333333336, 0.12480000000000002, 0.12426666666666664, 0.12613333333333332, 0.126, 0.12626666666666664, 0.126, 0.1253333333333333, 0.12573333333333336, 0.1253333333333333, 0.1266666666666667, 0.12373333333333336, 0.12626666666666664, 0.1266666666666667, 0.1265333333333334, 0.1246666666666667, 0.1265333333333334, 0.12546666666666662, 0.12546666666666662, 0.126, 0.12546666666666662, 0.12506666666666666, 0.12573333333333336, 0.1246666666666667, 0.12493333333333334, 0.1253333333333333, 0.12519999999999998, 0.12560000000000004, 0.126, 0.12480000000000002, 0.12560000000000004, 0.12493333333333334, 0.12439999999999996, 0.12519999999999998, 0.12439999999999996, 0.12573333333333336, 0.12426666666666664, 0.12546666666666662, 0.12573333333333336, 0.12546666666666662, 0.12480000000000002, 0.12453333333333338, 0.12613333333333332, 0.12453333333333338, 0.12519999999999998, 0.12546666666666662], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.096084001029436, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 32, "valid_ratio": 0.15, "learning_rate": 0.0003425773456929362, "optimization": "rmsprop", "nb_data_augmentation": 2, "learning_rate_decay_method": "discrete", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 2.8595787762010875e-06, "rotation_range": [0, 0], "momentum": 0.8971017024849821}, "accuracy_valid_max": 0.8774666666666666, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        y_pred = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            y_pred.extend((nnet.predict(X[mini_batch]) == y[mini_batch]).tolist())\n        return np.mean(y_pred)\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            acc = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = acc\n            status[\"accuracy_train_std\"] = 0\n            acc = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = acc\n            status[\"accuracy_valid_std\"] = 0\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n\n    # rescaling to [-1, 1]\n    X_min = X_train.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X_train.max(axis=(0, 2, 3))[None, :, None, None]\n    def preprocess(a):\n        return (a / 255.) * 2 - 1\n        # return 2 * ((a - X_min) / (X_max - X_min)) - 1\n    X_train = preprocess(X_train)\n    X_valid = preprocess(X_valid)\n\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = preprocess(X_test)\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    acc = evaluate(X_test, y_test, batch_size_eval)\n    light.set(\"accuracy_test\", acc)\n    light.set(\"accuracy_test_std\", 0)\n    print(\"Test accuracy : {}+-{}\".format(acc, 0))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8745333333333334, "accuracy_valid_std": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "accuracy_valid": [0.474, 0.5521333333333334, 0.6025333333333334, 0.6397333333333334, 0.6797333333333333, 0.6404, 0.7125333333333334, 0.7312, 0.732, 0.7676, 0.7601333333333333, 0.7649333333333334, 0.7594666666666666, 0.7653333333333333, 0.778, 0.778, 0.7636, 0.798, 0.7961333333333334, 0.7997333333333333, 0.7857333333333333, 0.8012, 0.7918666666666667, 0.7830666666666667, 0.7936, 0.7901333333333334, 0.8014666666666667, 0.7924, 0.8086666666666666, 0.7968, 0.8021333333333334, 0.8141333333333334, 0.8186666666666667, 0.7890666666666667, 0.8197333333333333, 0.81, 0.81, 0.8078666666666666, 0.8162666666666667, 0.8248, 0.8105333333333333, 0.8304, 0.8157333333333333, 0.7905333333333333, 0.8305333333333333, 0.8277333333333333, 0.8, 0.8218666666666666, 0.8142666666666667, 0.8304, 0.8258666666666666, 0.8316, 0.8182666666666667, 0.8296, 0.8418666666666667, 0.8277333333333333, 0.8193333333333334, 0.8484, 0.8302666666666667, 0.8301333333333333, 0.8344, 0.8382666666666667, 0.8265333333333333, 0.7946666666666666, 0.7997333333333333, 0.8197333333333333, 0.8024, 0.8236, 0.8385333333333334, 0.8430666666666666, 0.8382666666666667, 0.8385333333333334, 0.7777333333333334, 0.8374666666666667, 0.8405333333333334, 0.8402666666666667, 0.8414666666666667, 0.8428, 0.846, 0.8248, 0.8432, 0.8517333333333333, 0.8396, 0.8490666666666666, 0.8464, 0.8309333333333333, 0.8430666666666666, 0.8353333333333334, 0.8404, 0.8330666666666666, 0.8481333333333333, 0.8476, 0.8457333333333333, 0.8370666666666666, 0.848, 0.8406666666666667, 0.8425333333333334, 0.8504, 0.8396, 0.8485333333333334, 0.8124, 0.8466666666666667, 0.8408, 0.792, 0.8342666666666667, 0.8465333333333334, 0.8464, 0.8428, 0.8481333333333333, 0.8374666666666667, 0.856, 0.8326666666666667, 0.8484, 0.8493333333333334, 0.8484, 0.8390666666666666, 0.848, 0.8389333333333333, 0.8229333333333333, 0.8532, 0.844, 0.8388, 0.8444, 0.8481333333333333, 0.8774666666666666, 0.8762666666666666, 0.8762666666666666, 0.8770666666666667, 0.8749333333333333, 0.8762666666666666, 0.8752, 0.8757333333333334, 0.8738666666666667, 0.874, 0.8737333333333334, 0.874, 0.8746666666666667, 0.8742666666666666, 0.8746666666666667, 0.8733333333333333, 0.8762666666666666, 0.8737333333333334, 0.8733333333333333, 0.8734666666666666, 0.8753333333333333, 0.8734666666666666, 0.8745333333333334, 0.8745333333333334, 0.874, 0.8745333333333334, 0.8749333333333333, 0.8742666666666666, 0.8753333333333333, 0.8750666666666667, 0.8746666666666667, 0.8748, 0.8744, 0.874, 0.8752, 0.8744, 0.8750666666666667, 0.8756, 0.8748, 0.8756, 0.8742666666666666, 0.8757333333333334, 0.8745333333333334, 0.8742666666666666, 0.8745333333333334, 0.8752, 0.8754666666666666, 0.8738666666666667, 0.8754666666666666, 0.8748, 0.8745333333333334], "seed": 356587298, "model": "residualv3", "loss_std": [0.3569626212120056, 0.18584439158439636, 0.1806836575269699, 0.17837810516357422, 0.1753021478652954, 0.17325951159000397, 0.17321404814720154, 0.17266054451465607, 0.1684887707233429, 0.16876715421676636, 0.16573992371559143, 0.16388799250125885, 0.16303855180740356, 0.15949253737926483, 0.15992967784404755, 0.1549127995967865, 0.1542206108570099, 0.1516290307044983, 0.15123054385185242, 0.14911045134067535, 0.1461210697889328, 0.14684666693210602, 0.14354263246059418, 0.1399761438369751, 0.13979634642601013, 0.13771772384643555, 0.13740675151348114, 0.13356463611125946, 0.13077327609062195, 0.12959502637386322, 0.1282094120979309, 0.1251847892999649, 0.12515795230865479, 0.12154773622751236, 0.12038733065128326, 0.11951835453510284, 0.118372842669487, 0.11938277631998062, 0.11412548273801804, 0.11347945779561996, 0.1093386858701706, 0.11140383780002594, 0.10889679938554764, 0.11068332940340042, 0.10472692549228668, 0.10665231198072433, 0.10275910794734955, 0.10306841880083084, 0.10218365490436554, 0.0987766832113266, 0.10045154392719269, 0.09854202717542648, 0.09600083529949188, 0.09812484681606293, 0.09594094753265381, 0.09382253885269165, 0.0905783474445343, 0.09189162403345108, 0.0898430123925209, 0.09134870767593384, 0.08870705217123032, 0.08891155570745468, 0.08723723143339157, 0.08522368222475052, 0.08194640278816223, 0.08406688272953033, 0.08516348153352737, 0.0812276229262352, 0.08206018060445786, 0.08148802816867828, 0.07846829295158386, 0.08034541457891464, 0.07988273352384567, 0.07762438803911209, 0.07693445682525635, 0.07860251516103745, 0.0781392902135849, 0.0749729573726654, 0.07691621035337448, 0.07467067241668701, 0.07327959686517715, 0.07255695015192032, 0.07293066382408142, 0.07012436538934708, 0.07259044051170349, 0.07061776518821716, 0.07095908373594284, 0.06847715377807617, 0.06765931844711304, 0.06992215663194656, 0.0667852982878685, 0.06771736592054367, 0.06526874750852585, 0.06621066480875015, 0.06633275002241135, 0.06499393284320831, 0.06629733741283417, 0.06355573236942291, 0.06517914682626724, 0.06430982053279877, 0.060676511377096176, 0.06150355190038681, 0.06330500543117523, 0.062067750841379166, 0.06283795088529587, 0.058758847415447235, 0.06194340065121651, 0.061036255210638046, 0.0634092390537262, 0.06095043569803238, 0.059986308217048645, 0.060689326375722885, 0.05929534137248993, 0.05854344740509987, 0.057574279606342316, 0.056090351194143295, 0.05734157934784889, 0.05628810077905655, 0.05736827850341797, 0.05563636124134064, 0.05596144124865532, 0.054828379303216934, 0.05657379701733589, 0.054975476115942, 0.047881659120321274, 0.03537381440401077, 0.030102092772722244, 0.029069049283862114, 0.029059940949082375, 0.029307840391993523, 0.027134671807289124, 0.02638719417154789, 0.026960451155900955, 0.02532433159649372, 0.02472156286239624, 0.022749459370970726, 0.023456037044525146, 0.024435177445411682, 0.02465207502245903, 0.02395143173635006, 0.022997381165623665, 0.02423890493810177, 0.022896716371178627, 0.022104397416114807, 0.02241942286491394, 0.022008150815963745, 0.021080536767840385, 0.021939592435956, 0.021735047921538353, 0.019930604845285416, 0.02203776128590107, 0.02242327108979225, 0.020132509991526604, 0.019862337037920952, 0.0202473234385252, 0.021138764917850494, 0.020891383290290833, 0.0191787239164114, 0.021366862580180168, 0.021332532167434692, 0.021576646715402603, 0.020757997408509254, 0.02048402465879917, 0.02084910310804844, 0.02156008407473564, 0.02155960351228714, 0.021366924047470093, 0.021355580538511276, 0.02058693952858448, 0.0199111420661211, 0.021169334650039673, 0.022041458636522293, 0.02022215537726879, 0.02058664709329605, 0.02165350876748562]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:45 2016", "state": "available"}], "summary": "05bed334e94d19af7e95c0311d5cbcda"}