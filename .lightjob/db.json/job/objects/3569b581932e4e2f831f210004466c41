{"content": {"hp_model": {"f0": 32, "f1": 32, "f2": 32, "f3": 16, "nonlin": "rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.01337388996484952, 0.013406711817976738, 0.014231434189354344, 0.015465063611798145, 0.014629768047204462, 0.014837779739929937, 0.014283021196224332, 0.014301658103991099, 0.013872117114489303, 0.014557628667917048, 0.013900682756489292, 0.014007985048447013, 0.012390643571817706, 0.013954506250634135, 0.014536945509721419, 0.013877699969011042, 0.013971253548697729, 0.014886925952994253, 0.016776980139078097, 0.016095380889295154, 0.01532821774391615, 0.015247640496488923, 0.014785770047420505, 0.014190954857218213, 0.01436270509273738, 0.014980169874805362, 0.014066146306730672, 0.014301475173463633, 0.014260196530281653, 0.014536063421268527, 0.014632671493659022, 0.013998054602275592, 0.01447110124552856, 0.013979702105170915, 0.013712807699270479, 0.013930957348762568, 0.01417613245363408, 0.014140430909226537, 0.014967312265885869, 0.014268654322477635, 0.013087480124904972, 0.013830460744335431, 0.013721125169702105, 0.013499365678012337, 0.014090262634801433, 0.014155017856239975, 0.014699571402594242, 0.01454422634960455, 0.013605277912017147, 0.013822754423135974, 0.013257424420687494, 0.013102650131720087, 0.013717873322331424, 0.013311873495363308, 0.012626451396324016, 0.01450256165216005, 0.014543781963438045, 0.013510419357025467, 0.013969006612463771, 0.013503879874062609, 0.014017907071031576, 0.014387204064622705, 0.013010322993912369, 0.014012757575643901, 0.014263274236120648, 0.013722381210221764, 0.013795257284432951, 0.013644393314425459, 0.013572261791629946, 0.014714726553060505, 0.013972039395614108, 0.01330584345190259, 0.014212181653792684, 0.013077853514089683, 0.013646491843939235, 0.013754338857128281, 0.014470538095568803, 0.014785401021752657, 0.013905120335201498, 0.014624143202561451, 0.013312129576024276, 0.013623377062057487, 0.013987212700765684, 0.014270549192650559, 0.014923616877596855, 0.014943615488946632, 0.014478953614962108, 0.01409894970737309, 0.014134843404988323, 0.015252565662756441, 0.01396933433898151, 0.013999992509492685, 0.013633606279860405, 0.01307082961827207, 0.014395531115614863, 0.014416442893538002, 0.013907740910290445, 0.014404337945366257, 0.015307318449679865, 0.014625561059852694, 0.014025283699408834, 0.014469868116626004, 0.01464857741032654, 0.01402804681483485, 0.01342001478770008, 0.013926600768986672, 0.014398997740378606, 0.013508306493889708, 0.013302166339303858, 0.014395487465737229, 0.013554888939058155, 0.013802426002358639, 0.01387093443612915, 0.012555306623546954, 0.013003230266302367, 0.014750918547095572, 0.014060805187493378, 0.014558340889940251, 0.014044214271129287, 0.01459270019032874, 0.012919017574751142, 0.014165059452667104, 0.013984756044251617, 0.015254937257095329, 0.014251217072059112, 0.013823501221815021, 0.01371878941365793, 0.013421894255717886, 0.012650685288206471, 0.013461795923485367, 0.01401357752245212, 0.013853735415879372, 0.014418914658582497, 0.014113336302369581, 0.013553193496079866, 0.013827896775572404, 0.013077853514089683, 0.013985569667641046, 0.013567505540354338, 0.014289613421815018, 0.014658127801049003, 0.013676953253148329, 0.013829552050295868, 0.014199821923038392, 0.01458288535252921, 0.013888112157150684, 0.01316329172104922, 0.014203563835426904, 0.01429298012768705], "moving_avg_accuracy_train": [0.009982170254245108, 0.01910101816283684, 0.027382386042474158, 0.03494024883057631, 0.04196074513268848, 0.048562787861713835, 0.05472315425711314, 0.060504541045087686, 0.06596351947449365, 0.07108110500974085, 0.07585663180573982, 0.0803219445387871, 0.08462664720446358, 0.08868685545951557, 0.09252000724975819, 0.0960349480276432, 0.0993866957324924, 0.10254742789304715, 0.10557341239587052, 0.10828749785317346, 0.11084872130521307, 0.11323051627394425, 0.11547411314461183, 0.11759562082701294, 0.11955151676618318, 0.12153968769476974, 0.12322204863644079, 0.12484778062680188, 0.1263783326847843, 0.12776055193222502, 0.1290510161822934, 0.13025661183473589, 0.13137420000526745, 0.13239863054922207, 0.1334066485447336, 0.13437435465856035, 0.13524754321217644, 0.13602643746400236, 0.13681114764778854, 0.1374778592834342, 0.1381011872924293, 0.13863424466599183, 0.1391023705581505, 0.13955158564680759, 0.14014189113136089, 0.14055698072462014, 0.1410606975942493, 0.141462889503106, 0.14181559767465773, 0.14210509719452125, 0.14234475647193143, 0.14253022288707679, 0.1427482598856984, 0.14297010587018136, 0.14317670665382584, 0.14334873251506747, 0.1435035197413662, 0.1437009930140919, 0.14382989083454503, 0.14390637134319093, 0.14397284260334395, 0.14402104099343405, 0.14419927817546754, 0.14429229837264024, 0.14427599910246738, 0.14431015788431184, 0.14437345287130518, 0.1443444599512843, 0.14442296197087534, 0.14451682922778375, 0.14450597865781084, 0.1445753042531778, 0.14463769728900805, 0.14469381497243652, 0.14461876285180786, 0.14462097040752778, 0.14477409188029475, 0.14480963070698472, 0.1447671748402822, 0.14481735626383058, 0.14486716984264317, 0.14482822460879413, 0.14482579807930085, 0.1448538411372807, 0.14483486601326284, 0.1447852002694947, 0.14486380608464564, 0.1449926800385196, 0.145022600042235, 0.1450960670705881, 0.1450714705437157, 0.14499821644453978, 0.1449717792362246, 0.14496662298803586, 0.14502243623371364, 0.14508658299886204, 0.14502573249820985, 0.1449268252690607, 0.14503773551162677, 0.14508411235613594, 0.1449560796042802, 0.1449082794430862, 0.1449698909944402, 0.14503696713470637, 0.1450671447752409, 0.1450314895850461, 0.14499943596268952, 0.1449915140418543, 0.1450657284726172, 0.14505350244959875, 0.1451889473550634, 0.14519691547831493, 0.14516920955709844, 0.1451954275018131, 0.14509582681397032, 0.14508055490799782, 0.14506684624144128, 0.14505915873915948, 0.14508940631923947, 0.14507012616512097, 0.14503649798474766, 0.1451062140212212, 0.14514109271715186, 0.14512826966728976, 0.14507022594622337, 0.14505755017584432, 0.14499963900631269, 0.145052114601344, 0.14504586421425317, 0.1449518832111095, 0.14504168646899449, 0.1450899933665764, 0.14511719353273345, 0.14510447130132242, 0.14504880741685278, 0.14501266081368724, 0.14503596849108558, 0.14504760875668726, 0.1449511281504907, 0.14498055304538995, 0.14506051387341834, 0.1450952762376915, 0.14504518215720402, 0.14507915254428908, 0.14505628351886535, 0.14510076951383188, 0.14501059857596846, 0.14503872672593898, 0.14499425154780798], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 718743222, "moving_var_accuracy_train": [0.0008967935068626227, 0.0015554946407966104, 0.0020171746623379476, 0.002329548805418167, 0.0025401802398280866, 0.002678444929608167, 0.0027521514637774357, 0.002777756216129192, 0.0027681846039507487, 0.0027270732789508164, 0.0026596168566694724, 0.002573106331237753, 0.002482569883472917, 0.002382680514795155, 0.002276649937139001, 0.002160178221473451, 0.0020452683134187626, 0.001930653532193772, 0.00181999741887634, 0.001704294015814596, 0.0015929034043746357, 0.0014846695893948308, 0.0013815061727179723, 0.0012838627090645584, 0.0011899061984818665, 0.0011064909914051703, 0.0010213149373071962, 0.0009429704841168275, 0.0008697567421248926, 0.000799975838344364, 0.0007349659363362686, 0.0006745504905973359, 0.0006183364714078114, 0.0005659479457215145, 0.0005184980536628388, 0.00047507634440119393, 0.00043443083421257003, 0.0003964478370910601, 0.00036234498403479377, 0.0003301110252772622, 0.00030059676301071595, 0.0002730944381812288, 0.00024775727102129, 0.00022479769168205573, 0.00020545406759969322, 0.00018645935517361316, 0.0001700969958189929, 0.00015454312122104193, 0.0001402084365874519, 0.0001269418826767175, 0.0001147646235322847, 0.00010359774129937807, 9.366582836435148e-05, 8.474218629539721e-05, 7.665212262008006e-05, 6.925324643049538e-05, 6.254355355627309e-05, 5.664015944161474e-05, 5.112567533051135e-05, 4.606575121128484e-05, 4.1498941945993354e-05, 3.736995551465953e-05, 3.391887640072672e-05, 3.060486357439248e-05, 2.7546768212826736e-05, 2.4802592792937927e-05, 2.2358389812050503e-05, 2.013011613554749e-05, 1.8172567625711565e-05, 1.643461042041599e-05, 1.4792208992193024e-05, 1.3356242436530551e-05, 1.2055654211158527e-05, 1.0878431539583073e-05, 9.841283772922505e-06, 8.857199255350563e-06, 8.182494998616442e-06, 7.375612572577302e-06, 6.65427382087674e-06, 6.0115100162131405e-06, 5.432691548298888e-06, 4.903072974625002e-06, 4.4128186695709385e-06, 3.978614520521594e-06, 3.583993566452868e-06, 3.2477943847439673e-06, 2.9786248138494604e-06, 2.830238796348434e-06, 2.5552717763145614e-06, 2.3483212369784183e-06, 2.1189340154882366e-06, 1.955336081354098e-06, 1.7660928070701902e-06, 1.5897228084216244e-06, 1.4587865931172474e-06, 1.3499412011165395e-06, 1.248272131871463e-06, 1.2114886784859796e-06, 1.201049547791939e-06, 1.1003018983723838e-06, 1.1378031784649506e-06, 1.0445865593099954e-06, 9.742917527211989e-07, 9.173554547861862e-07, 8.338161192016502e-07, 7.618761405719445e-07, 6.949354388703567e-07, 6.260067064507955e-07, 6.129760714068536e-07, 5.530237450157984e-07, 6.628292722611305e-07, 5.971177639283778e-07, 5.443145501696378e-07, 4.960695207782254e-07, 5.35745241869248e-07, 4.842697976906255e-07, 4.3753416577038265e-07, 3.9431262841534096e-07, 3.631156104800667e-07, 3.3014956851755206e-07, 3.073123023027688e-07, 3.203240037467311e-07, 2.9924031424047414e-07, 2.707961582863207e-07, 2.7403820445476286e-07, 2.480804604016136e-07, 2.534557463701578e-07, 2.5289336439817866e-07, 2.2795563400742906e-07, 2.846519311736905e-07, 3.2876836419714453e-07, 3.168935349633313e-07, 2.918628228177394e-07, 2.641332370846485e-07, 2.6560612568442457e-07, 2.508047053996384e-07, 2.3061346529102442e-07, 2.08771580811418e-07, 2.7167098907884425e-07, 2.5229631012944766e-07, 2.846102852873477e-07, 2.6702505448734685e-07, 2.629073011376043e-07, 2.4700245581222406e-07, 2.2700914114548627e-07, 2.221192607644115e-07, 2.730845170045232e-07, 2.528968006909482e-07, 2.4540949384990723e-07], "duration": 64885.378701, "accuracy_train": [0.0998217025424511, 0.10117064934016241, 0.10191469695921003, 0.10296101392349574, 0.10514521185169803, 0.10798117242294204, 0.1101664518157069, 0.11253702213685861, 0.11509432533914729, 0.11713937482696567, 0.11883637296973053, 0.12050975913621263, 0.12336897119555187, 0.1252287297549834, 0.12701837336194166, 0.12766941502860835, 0.12955242507613512, 0.13099401733803986, 0.13280727292128092, 0.13271426696889996, 0.13389973237356959, 0.1346666709925249, 0.13566648498062014, 0.13668918996862311, 0.13715458021871538, 0.13943322605204872, 0.13836329711148027, 0.1394793685400517, 0.14015330120662606, 0.14020052515919157, 0.14066519443290884, 0.14110697270671835, 0.1414324935400517, 0.14161850544481358, 0.1424788105043374, 0.1430837096830011, 0.1431062401947213, 0.1430364857304356, 0.14387353930186414, 0.1434782640042451, 0.14371113937338503, 0.14343176102805463, 0.14331550358757844, 0.1435945214447213, 0.14545464049234033, 0.1442927870639535, 0.14559414942091178, 0.14508261668281652, 0.14498997121862311, 0.14471059287329271, 0.14450168996862311, 0.14419942062338503, 0.14471059287329271, 0.14496671973052788, 0.14503611370662606, 0.14489696526624216, 0.14489660477805463, 0.14547825246862311, 0.14498997121862311, 0.14459469592100407, 0.1445710839447213, 0.1444548265042451, 0.1458034128137689, 0.14512948014719454, 0.14412930567091178, 0.14461758692091178, 0.1449431077542451, 0.14408352367109634, 0.14512948014719454, 0.1453616345399594, 0.14440832352805463, 0.14519923461148027, 0.14519923461148027, 0.14519887412329271, 0.14394329376614987, 0.14464083840900702, 0.14615218513519748, 0.14512948014719454, 0.1443850720399594, 0.14526898907576596, 0.14531549205195646, 0.14447771750415284, 0.1448039593138612, 0.1451062286590993, 0.14466408989710225, 0.1443382085755814, 0.14557125842100407, 0.14615254562338503, 0.14529188007567367, 0.14575727032576596, 0.14485010180186414, 0.14433892955195646, 0.14473384436138798, 0.1449202167543374, 0.14552475544481358, 0.14566390388519748, 0.14447807799234033, 0.14403666020671835, 0.1460359276947213, 0.14550150395671835, 0.14380378483757844, 0.14447807799234033, 0.14552439495662606, 0.14564065239710225, 0.1453387435400517, 0.14471059287329271, 0.14471095336148027, 0.1449202167543374, 0.1457336583494832, 0.14494346824243265, 0.1464079515042451, 0.14526862858757844, 0.14491985626614987, 0.1454313890042451, 0.14419942062338503, 0.1449431077542451, 0.14494346824243265, 0.14498997121862311, 0.1453616345399594, 0.14489660477805463, 0.14473384436138798, 0.1457336583494832, 0.14545500098052788, 0.14501286221853082, 0.14454783245662606, 0.14494346824243265, 0.14447843848052788, 0.14552439495662606, 0.1449896107304356, 0.14410605418281652, 0.1458499157899594, 0.14552475544481358, 0.14536199502814692, 0.14498997121862311, 0.14454783245662606, 0.14468734138519748, 0.14524573758767073, 0.14515237114710225, 0.1440828026947213, 0.1452453770994832, 0.14578016132567367, 0.14540813751614987, 0.14459433543281652, 0.14538488602805463, 0.1448504622900517, 0.14550114346853082, 0.14419906013519748, 0.14529188007567367, 0.14459397494462903], "end": "2016-01-27 22:01:12.102000", "learning_rate_per_epoch": [0.00010724334424594417, 9.770026372279972e-05, 8.900637476472184e-05, 8.108611655188724e-05, 7.387064397335052e-05, 6.729724555043504e-05, 6.13087831879966e-05, 5.5853204685263336e-05, 5.0883088988484815e-05, 4.635524237528443e-05, 4.223030919092707e-05, 3.847243351629004e-05, 3.504895357764326e-05, 3.193011070834473e-05, 2.9088800147292204e-05, 2.6500325475353748e-05, 2.4142185793607496e-05, 2.1993886548443697e-05, 2.003675399464555e-05, 1.8253776943311095e-05, 1.6629459423711523e-05, 1.5149682440096512e-05, 1.3801583008898888e-05, 1.257344501937041e-05, 1.1454593732196372e-05, 1.0435303011036012e-05, 9.506714377494063e-06, 8.660756975586992e-06, 7.890076631156262e-06, 7.187975825218018e-06, 6.548351848323364e-06, 5.965644959360361e-06, 5.4347901823348366e-06, 4.951173650624696e-06, 4.510592134465696e-06, 4.109215751668671e-06, 3.7435559079312952e-06, 3.410434374018223e-06, 3.1069557735463604e-06, 2.8304823445068905e-06, 2.5786109745240537e-06, 2.3491525098506827e-06, 2.1401124286057893e-06, 1.9496737877489068e-06, 1.7761815342964837e-06, 1.6181274986593053e-06, 1.4741378890903434e-06, 1.3429612408799585e-06, 1.2234573887326405e-06, 1.1145876896989648e-06, 1.0154058145417366e-06, 9.25049619127094e-07, 8.42733811623475e-07, 7.677429039176786e-07, 6.994250725256279e-07, 6.371865310939029e-07, 5.804863008052052e-07, 5.288315492180118e-07, 4.817733270101598e-07, 4.3890258893952705e-07, 3.9984669797377137e-07, 3.642662136371655e-07, 3.318518793093972e-07, 3.0232195058488287e-07, 2.7541975100575655e-07, 2.509114551685343e-07, 2.2858402815018053e-07, 2.0824340651870443e-07, 1.897128072414489e-07, 1.7283115028021712e-07, 1.5745172277092934e-07, 1.4344082899242494e-07, 1.3067671034150408e-07, 1.1904840135912309e-07, 1.0845484155197482e-07, 9.880395879235948e-08, 9.001185929946587e-08, 8.200212420206299e-08, 7.470514162832842e-08, 6.805748142824086e-08, 6.200136937195566e-08, 5.648415779546667e-08, 5.14578992749648e-08, 4.6878902537628164e-08, 4.2707370084826834e-08, 3.8907042920754975e-08, 3.54448879136271e-08, 3.2290813578583766e-08, 2.941740540052251e-08, 2.6799689578638208e-08, 2.441491098181814e-08, 2.224234307846018e-08, 2.026310141900467e-08, 1.8459983763818855e-08, 1.6817317316508706e-08, 1.5320823720799126e-08, 1.3957495603733605e-08, 1.2715483777014924e-08, 1.1583992431951629e-08, 1.0553187657080798e-08, 9.614109508504498e-09, 8.758594738367265e-09, 7.979208405117788e-09, 7.2691763719490154e-09, 6.622326687022451e-09, 6.033037180941392e-09, 5.4961857287594285e-09, 5.007105841059456e-09, 4.5615471400139995e-09, 4.155636723623957e-09, 3.785846303117069e-09, 3.4489615607924407e-09, 3.1420546164895313e-09, 2.8624580483693762e-09, 2.607741356186466e-09, 2.375690755229698e-09, 2.1642891923079333e-09, 1.9716992483154172e-09, 1.7962470399979225e-09, 1.6364074539865214e-09, 1.4907912682105007e-09, 1.358132717399485e-09, 1.2372788349424013e-09, 1.1271792388356516e-09, 1.0268768058097066e-09, 9.354998997679331e-10, 8.522541561362118e-10, 7.764160425693944e-10, 7.073264196577611e-10, 6.44384767767292e-10, 5.870440245026032e-10, 5.348057552367891e-10, 4.872158787527781e-10, 4.438608369738972e-10, 4.0436373693886196e-10, 3.683812976884582e-10, 3.3560076939664896e-10, 3.057372410797399e-10, 2.7853111483899795e-10, 2.5374594092575364e-10, 2.3116628056207844e-10, 2.1059587407279423e-10, 1.9185593391757294e-10, 1.7478357650091425e-10, 1.592304066377892e-10, 1.4506124079716187e-10, 1.3215292749002572e-10, 1.2039326480195456e-10, 1.0968003588684994e-10], "accuracy_valid": [0.1041921592620482, 0.1056570030120482, 0.10627764966114459, 0.10800722420933735, 0.11069277108433735, 0.11412103492093373, 0.11607415992093373, 0.11840379094503012, 0.12158791415662651, 0.12404991057981928, 0.1256368246423193, 0.12747817441641568, 0.13016372129141568, 0.13177122552710843, 0.13349050498870482, 0.13286985833960843, 0.13457884271460843, 0.13678640342620482, 0.13913662462349397, 0.13912632953689757, 0.14071324359939757, 0.14107945453689757, 0.14242222797439757, 0.14315464984939757, 0.14339879047439757, 0.14536221056099397, 0.14388707172439757, 0.14388707172439757, 0.14474156391189757, 0.14498570453689757, 0.14522984516189757, 0.14559605609939757, 0.14620640766189757, 0.14681675922439757, 0.14681675922439757, 0.14768154649849397, 0.14719326524849397, 0.14755947618599397, 0.14927875564759036, 0.14742711078689757, 0.14754918109939757, 0.14792568712349397, 0.14755947618599397, 0.14730504047439757, 0.15025531814759036, 0.14841396837349397, 0.15037738846009036, 0.14988910721009036, 0.14963467149849397, 0.14878017931099397, 0.14865810899849397, 0.14791539203689757, 0.14914639024849397, 0.14876988422439757, 0.14852574359939757, 0.14976703689759036, 0.14940082596009036, 0.14975674181099397, 0.14902431993599397, 0.14853603868599397, 0.14865810899849397, 0.14940082596009036, 0.14950230609939757, 0.14939053087349397, 0.14792568712349397, 0.14829189806099397, 0.14840367328689757, 0.14853603868599397, 0.14828160297439757, 0.14976703689759036, 0.14829189806099397, 0.14939053087349397, 0.15001117752259036, 0.14889195453689757, 0.14816982774849397, 0.14890224962349397, 0.15074359939759036, 0.14915668533509036, 0.14779332172439757, 0.15013324783509036, 0.14914639024849397, 0.14902431993599397, 0.14939053087349397, 0.14976703689759036, 0.14891254471009036, 0.14952289627259036, 0.15037738846009036, 0.15049945877259036, 0.14926846056099397, 0.15050975385918675, 0.14890224962349397, 0.14903461502259036, 0.14828160297439757, 0.14939053087349397, 0.15001117752259036, 0.15025531814759036, 0.14914639024849397, 0.14804775743599397, 0.15124217573418675, 0.15001117752259036, 0.14879047439759036, 0.15001117752259036, 0.15013324783509036, 0.14963467149849397, 0.14963467149849397, 0.14890224962349397, 0.14927875564759036, 0.14889195453689757, 0.14913609516189757, 0.14940082596009036, 0.15000088243599397, 0.14951260118599397, 0.14926846056099397, 0.14913609516189757, 0.1474168157003012, 0.14952289627259036, 0.14988910721009036, 0.14964496658509036, 0.14914639024849397, 0.14976703689759036, 0.14864781391189757, 0.14939053087349397, 0.15037738846009036, 0.15014354292168675, 0.14853603868599397, 0.14890224962349397, 0.14841396837349397, 0.14975674181099397, 0.14876988422439757, 0.14816982774849397, 0.14987881212349397, 0.14926846056099397, 0.15013324783509036, 0.14902431993599397, 0.14902431993599397, 0.14926846056099397, 0.14889195453689757, 0.14841396837349397, 0.14791539203689757, 0.14902431993599397, 0.15001117752259036, 0.14939053087349397, 0.14865810899849397, 0.15025531814759036, 0.14964496658509036, 0.14926846056099397, 0.14767125141189757, 0.14976703689759036, 0.14853603868599397], "accuracy_test": 0.1491031568877551, "start": "2016-01-27 03:59:46.723000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0], "accuracy_train_last": 0.14459397494462903, "batch_size_eval": 1024, "accuracy_train_std": [0.0085446282321466, 0.008381403820526738, 0.00821074974013091, 0.008052036839496322, 0.008045792720952157, 0.008293830756664537, 0.008105311333676897, 0.008681715876083249, 0.00869364240204094, 0.009249192162033447, 0.009279217479879502, 0.008975139430531492, 0.009314918736642286, 0.00916206528521515, 0.009318170825383044, 0.009380819622722445, 0.009325590278170419, 0.009164009936918719, 0.009198822175328325, 0.008958183576451636, 0.009523057845055334, 0.009349712310908656, 0.009462795888685175, 0.009401404736018061, 0.009207887210790027, 0.009414994894408118, 0.009385608234225337, 0.009287693506312976, 0.009497142724505794, 0.009256433806591545, 0.009368909504252974, 0.009240598612844928, 0.009362066294092482, 0.00944326286902255, 0.009389151440409933, 0.009457487070593398, 0.009468004790117905, 0.009537336369723518, 0.009312496150395743, 0.009518949585236079, 0.009498429695329933, 0.009660533867267652, 0.009455210188806748, 0.009546565101370577, 0.009297244597206817, 0.00933396572042417, 0.009267641207262246, 0.009404287314234297, 0.00934096551368237, 0.009478807298839747, 0.009404231437842896, 0.00954584911634228, 0.009493169448853938, 0.009401438615117353, 0.009595695373317923, 0.009212926182446662, 0.009410936612848633, 0.009167705450633026, 0.00933367010022024, 0.009466633877831757, 0.0093938008641279, 0.00934145920183723, 0.009359650850381917, 0.009264697340981932, 0.009540055802646216, 0.009520741550766317, 0.009287568686199302, 0.009474820585716524, 0.009038917419446346, 0.009222679897194086, 0.009442506991556873, 0.009327119164656415, 0.009231686870065866, 0.009397957739260636, 0.009587410752274619, 0.009362980749477076, 0.009348195555842789, 0.009056484944743887, 0.009500910548095431, 0.009178258671547219, 0.009250808642110874, 0.009492646803194518, 0.009345816093400607, 0.009210868675654602, 0.009232018893097176, 0.009459681509007108, 0.009204611212887789, 0.009089016575746598, 0.009210222955398367, 0.00930451569891138, 0.009296113089050803, 0.009374497605543609, 0.0093176680843896, 0.009301507305336777, 0.00887066336666504, 0.00952649816714784, 0.00925983453998574, 0.009328187607381886, 0.009207474153744392, 0.009265067360974804, 0.009520767742086395, 0.009391317324686541, 0.00915835527499684, 0.009362162710450511, 0.009357421204654218, 0.009341265612080357, 0.009311560534256056, 0.009393813630964896, 0.00925548318630556, 0.009213427466609022, 0.009391334951831193, 0.009537876847077437, 0.009146928062254837, 0.009479685123714839, 0.009389961995523834, 0.009265539085055749, 0.009340702593602803, 0.009189016641690808, 0.009188146799464173, 0.009304168591167637, 0.009433915705568984, 0.009399113772609609, 0.009291838764963378, 0.009341510346549146, 0.009500825634106853, 0.009220818015258926, 0.009400507118307484, 0.009453594656429367, 0.009593296429028227, 0.009863221667404228, 0.00920393317969744, 0.009170213902866636, 0.009160398552094164, 0.009243219905442197, 0.009387825828351277, 0.009398630195630866, 0.009444655418502068, 0.009330839262448973, 0.00961858425689574, 0.009201615193677687, 0.009063332480387385, 0.009279749879252256, 0.009290373861263765, 0.009132716177379277, 0.009187508711479277, 0.009142238070488562, 0.00959612774849562, 0.009259398908237364, 0.009503348537316679], "accuracy_test_std": 0.006637896945581177, "error_valid": [0.8958078407379518, 0.8943429969879518, 0.8937223503388554, 0.8919927757906626, 0.8893072289156626, 0.8858789650790663, 0.8839258400790663, 0.8815962090549698, 0.8784120858433735, 0.8759500894201807, 0.8743631753576807, 0.8725218255835843, 0.8698362787085843, 0.8682287744728916, 0.8665094950112951, 0.8671301416603916, 0.8654211572853916, 0.8632135965737951, 0.860863375376506, 0.8608736704631024, 0.8592867564006024, 0.8589205454631024, 0.8575777720256024, 0.8568453501506024, 0.8566012095256024, 0.854637789439006, 0.8561129282756024, 0.8561129282756024, 0.8552584360881024, 0.8550142954631024, 0.8547701548381024, 0.8544039439006024, 0.8537935923381024, 0.8531832407756024, 0.8531832407756024, 0.852318453501506, 0.852806734751506, 0.852440523814006, 0.8507212443524097, 0.8525728892131024, 0.8524508189006024, 0.852074312876506, 0.852440523814006, 0.8526949595256024, 0.8497446818524097, 0.851586031626506, 0.8496226115399097, 0.8501108927899097, 0.850365328501506, 0.851219820689006, 0.851341891001506, 0.8520846079631024, 0.850853609751506, 0.8512301157756024, 0.8514742564006024, 0.8502329631024097, 0.8505991740399097, 0.850243258189006, 0.850975680064006, 0.851463961314006, 0.851341891001506, 0.8505991740399097, 0.8504976939006024, 0.850609469126506, 0.852074312876506, 0.851708101939006, 0.8515963267131024, 0.851463961314006, 0.8517183970256024, 0.8502329631024097, 0.851708101939006, 0.850609469126506, 0.8499888224774097, 0.8511080454631024, 0.851830172251506, 0.851097750376506, 0.8492564006024097, 0.8508433146649097, 0.8522066782756024, 0.8498667521649097, 0.850853609751506, 0.850975680064006, 0.850609469126506, 0.8502329631024097, 0.8510874552899097, 0.8504771037274097, 0.8496226115399097, 0.8495005412274097, 0.850731539439006, 0.8494902461408133, 0.851097750376506, 0.8509653849774097, 0.8517183970256024, 0.850609469126506, 0.8499888224774097, 0.8497446818524097, 0.850853609751506, 0.851952242564006, 0.8487578242658133, 0.8499888224774097, 0.8512095256024097, 0.8499888224774097, 0.8498667521649097, 0.850365328501506, 0.850365328501506, 0.851097750376506, 0.8507212443524097, 0.8511080454631024, 0.8508639048381024, 0.8505991740399097, 0.849999117564006, 0.850487398814006, 0.850731539439006, 0.8508639048381024, 0.8525831842996988, 0.8504771037274097, 0.8501108927899097, 0.8503550334149097, 0.850853609751506, 0.8502329631024097, 0.8513521860881024, 0.850609469126506, 0.8496226115399097, 0.8498564570783133, 0.851463961314006, 0.851097750376506, 0.851586031626506, 0.850243258189006, 0.8512301157756024, 0.851830172251506, 0.850121187876506, 0.850731539439006, 0.8498667521649097, 0.850975680064006, 0.850975680064006, 0.850731539439006, 0.8511080454631024, 0.851586031626506, 0.8520846079631024, 0.850975680064006, 0.8499888224774097, 0.850609469126506, 0.851341891001506, 0.8497446818524097, 0.8503550334149097, 0.850731539439006, 0.8523287485881024, 0.8502329631024097, 0.851463961314006], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "momentum": 0.8614131721919465, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.00011771856656738365, "patience_threshold": 1, "do_flip": true, "batch_size": 32, "l2_decay": 6.602506454104955e-08, "optimization": "adadelta", "nb_data_augmentation": 1, "learning_rate_decay_method": "exp", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.08898529666420489}, "accuracy_valid_max": 0.15124217573418675, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        #nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        nb_data_augmentation=make_constant_param(1),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.14853603868599397, "loss_train": [3.2127685546875, 3.0967578887939453, 3.00639271736145, 2.9344654083251953, 2.873150110244751, 2.820247173309326, 2.7753448486328125, 2.7367756366729736, 2.7032852172851562, 2.6748054027557373, 2.648667573928833, 2.6279256343841553, 2.608327627182007, 2.5924665927886963, 2.577162027359009, 2.5637474060058594, 2.552410125732422, 2.5420806407928467, 2.532590389251709, 2.5242955684661865, 2.516637086868286, 2.5099494457244873, 2.5041747093200684, 2.498410940170288, 2.4940574169158936, 2.489522933959961, 2.4852094650268555, 2.4817774295806885, 2.479567766189575, 2.4751346111297607, 2.4731857776641846, 2.471376657485962, 2.469575881958008, 2.467741012573242, 2.464606523513794, 2.4628546237945557, 2.462632656097412, 2.4608309268951416, 2.460543155670166, 2.4594385623931885, 2.4567418098449707, 2.456592082977295, 2.455777406692505, 2.4546520709991455, 2.4554944038391113, 2.4555771350860596, 2.453158140182495, 2.4523322582244873, 2.4530842304229736, 2.4529316425323486, 2.4533944129943848, 2.4516491889953613, 2.451451063156128, 2.451234817504883, 2.4514596462249756, 2.452191114425659, 2.451566457748413, 2.4508793354034424, 2.4518654346466064, 2.4508092403411865, 2.449538469314575, 2.451496124267578, 2.4514386653900146, 2.4509382247924805, 2.4500954151153564, 2.450125217437744, 2.4509804248809814, 2.4498963356018066, 2.4498000144958496, 2.4503121376037598, 2.451164960861206, 2.4504106044769287, 2.4506094455718994, 2.450080633163452, 2.450612783432007, 2.44987416267395, 2.449172258377075, 2.4507055282592773, 2.4502463340759277, 2.450382947921753, 2.4500088691711426, 2.451413154602051, 2.450390338897705, 2.4506049156188965, 2.4512786865234375, 2.4507739543914795, 2.450631856918335, 2.449500799179077, 2.450580358505249, 2.450388193130493, 2.450178384780884, 2.4496352672576904, 2.4491703510284424, 2.450960636138916, 2.4501829147338867, 2.4515085220336914, 2.4505743980407715, 2.450904130935669, 2.4508543014526367, 2.4508304595947266, 2.4507102966308594, 2.450854778289795, 2.449497938156128, 2.4497973918914795, 2.4504148960113525, 2.4508817195892334, 2.450923442840576, 2.450636863708496, 2.450732707977295, 2.4497323036193848, 2.4508473873138428, 2.450641632080078, 2.450892210006714, 2.449582099914551, 2.4508049488067627, 2.450737953186035, 2.4510083198547363, 2.4506304264068604, 2.4506607055664062, 2.4503750801086426, 2.4504685401916504, 2.449622869491577, 2.4508724212646484, 2.4502487182617188, 2.450695276260376, 2.4501330852508545, 2.4495465755462646, 2.449525833129883, 2.449932336807251, 2.450427770614624, 2.45036244392395, 2.4502243995666504, 2.450192451477051, 2.450035572052002, 2.449643850326538, 2.450479745864868, 2.450697183609009, 2.4502100944519043, 2.4494855403900146, 2.4498291015625, 2.4501147270202637, 2.449979066848755, 2.4506378173828125, 2.4504566192626953, 2.4507997035980225, 2.450467109680176, 2.4491748809814453, 2.4504318237304688, 2.4510090351104736], "accuracy_train_first": 0.0998217025424511, "model": "residualv3", "loss_std": [0.2594289481639862, 0.25052982568740845, 0.24463069438934326, 0.24104969203472137, 0.23873312771320343, 0.23556891083717346, 0.23408204317092896, 0.2314319759607315, 0.23139990866184235, 0.22851760685443878, 0.2289757877588272, 0.2271437793970108, 0.2252093404531479, 0.22569121420383453, 0.22443793714046478, 0.22380399703979492, 0.2226739078760147, 0.2225184589624405, 0.22220376133918762, 0.2215113490819931, 0.22039438784122467, 0.22114861011505127, 0.2205195128917694, 0.2203969955444336, 0.21972087025642395, 0.21871943771839142, 0.21892540156841278, 0.21932627260684967, 0.21956416964530945, 0.21820174157619476, 0.21854785084724426, 0.21850281953811646, 0.21751022338867188, 0.21831582486629486, 0.21832312643527985, 0.2178967446088791, 0.21850213408470154, 0.21802091598510742, 0.21723952889442444, 0.2176101803779602, 0.2181021273136139, 0.2163686603307724, 0.2167811393737793, 0.2176266312599182, 0.21793143451213837, 0.21701675653457642, 0.217370867729187, 0.21762260794639587, 0.2176816165447235, 0.21821947395801544, 0.2157842367887497, 0.21691346168518066, 0.2173137068748474, 0.21739955246448517, 0.21761196851730347, 0.21753312647342682, 0.21747463941574097, 0.21772673726081848, 0.21706528961658478, 0.21709153056144714, 0.21591927111148834, 0.21681642532348633, 0.21754087507724762, 0.21700799465179443, 0.21672074496746063, 0.21616552770137787, 0.21650373935699463, 0.21653805673122406, 0.217060849070549, 0.2172798365354538, 0.21771909296512604, 0.21690401434898376, 0.2167138308286667, 0.2160135805606842, 0.21654535830020905, 0.2175041288137436, 0.21669623255729675, 0.21655236184597015, 0.2168491631746292, 0.21748434007167816, 0.21677331626415253, 0.21713300049304962, 0.21611465513706207, 0.21608737111091614, 0.21732042729854584, 0.2170892059803009, 0.2168862223625183, 0.2168707549571991, 0.21774037182331085, 0.21702657639980316, 0.21707075834274292, 0.21672025322914124, 0.21720460057258606, 0.21712486445903778, 0.21681077778339386, 0.21599867939949036, 0.21760676801204681, 0.2168818563222885, 0.21610134840011597, 0.21747440099716187, 0.21726559102535248, 0.217087522149086, 0.21685846149921417, 0.21758006513118744, 0.21730469167232513, 0.21801640093326569, 0.2177729457616806, 0.2169577181339264, 0.2168920338153839, 0.21732883155345917, 0.2169252187013626, 0.21775461733341217, 0.21675312519073486, 0.21685664355754852, 0.2172563374042511, 0.2171068787574768, 0.21578232944011688, 0.2173137217760086, 0.2177816778421402, 0.21706745028495789, 0.21698801219463348, 0.2159520834684372, 0.21681560575962067, 0.2177359014749527, 0.21723634004592896, 0.21763254702091217, 0.21755197644233704, 0.2163570672273636, 0.21728821098804474, 0.21804103255271912, 0.21593378484249115, 0.2172601968050003, 0.21682916581630707, 0.21836502850055695, 0.21686796844005585, 0.21613165736198425, 0.21665404736995697, 0.21712444722652435, 0.2172437608242035, 0.2167511135339737, 0.21713724732398987, 0.21687942743301392, 0.2179531306028366, 0.21747197210788727, 0.21666453778743744, 0.2162025272846222, 0.21752507984638214, 0.2175372838973999, 0.21659575402736664]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:18 2016", "state": "available"}], "summary": "5550fc7f8463ffec2efd2aab8d54eeca"}