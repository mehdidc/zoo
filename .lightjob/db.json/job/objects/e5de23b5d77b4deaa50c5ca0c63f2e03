{"content": {"hp_model": {"f0": 64, "f1": 16, "f2": 32, "f3": 16, "nonlin": "very_leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.5873578786849976, 1.1720480918884277, 0.9856065511703491, 0.8656662702560425, 0.7886366844177246, 0.728431224822998, 0.6793022751808167, 0.6395540237426758, 0.6054196953773499, 0.5751093626022339, 0.5504973530769348, 0.5277752876281738, 0.5067721605300903, 0.4875171482563019, 0.4715866446495056, 0.45402947068214417, 0.43828609585762024, 0.4246182441711426, 0.4145642817020416, 0.40086475014686584, 0.3906816840171814, 0.3802884817123413, 0.36943474411964417, 0.3597414493560791, 0.35093992948532104, 0.3450755178928375, 0.33493688702583313, 0.3284217119216919, 0.32177481055259705, 0.3136589229106903, 0.30966120958328247, 0.30336499214172363, 0.2962070107460022, 0.2937891185283661, 0.28645503520965576, 0.28128981590270996, 0.2766939699649811, 0.27310711145401, 0.26754534244537354, 0.2649713158607483, 0.2610020339488983, 0.2569049298763275, 0.25313258171081543, 0.24980030953884125, 0.247663676738739, 0.24458619952201843, 0.24096904695034027, 0.2383805364370346, 0.23557735979557037, 0.23358264565467834, 0.23155677318572998, 0.22971829771995544, 0.22580596804618835, 0.2231534868478775, 0.22243447601795197, 0.22212713956832886, 0.21882669627666473, 0.21567821502685547, 0.2157520055770874, 0.21439261734485626, 0.2128998041152954, 0.21147294342517853, 0.21143606305122375, 0.2084806114435196, 0.20875580608844757, 0.20595893263816833, 0.20519141852855682, 0.20371191203594208, 0.20431610941886902, 0.2032526433467865, 0.20227555930614471, 0.200065016746521, 0.20070497691631317, 0.19882355630397797, 0.19742658734321594, 0.19943372905254364, 0.19735361635684967, 0.1960373967885971, 0.19550451636314392, 0.19478729367256165, 0.1949780434370041, 0.19569683074951172, 0.19410796463489532, 0.1934814453125, 0.1926862895488739, 0.191880002617836, 0.19142882525920868, 0.19234813749790192, 0.1901697963476181, 0.19072650372982025, 0.18927454948425293, 0.18947896361351013, 0.19003763794898987, 0.18975132703781128, 0.18967698514461517, 0.18797573447227478, 0.18886178731918335, 0.1878698319196701, 0.18798226118087769, 0.18679985404014587, 0.188258558511734, 0.1876033991575241, 0.1877090185880661, 0.18640632927417755, 0.18797755241394043, 0.18553631007671356, 0.18584074079990387, 0.18485230207443237, 0.18679635226726532, 0.18543285131454468, 0.18649479746818542, 0.1861688196659088, 0.18598294258117676, 0.18497170507907867, 0.1871117502450943, 0.18437913060188293, 0.18423736095428467, 0.1845821589231491, 0.185176819562912, 0.18399108946323395, 0.18525609374046326, 0.18466852605342865, 0.18362189829349518, 0.18612143397331238, 0.1843806356191635, 0.18492799997329712, 0.18417209386825562, 0.18448148667812347, 0.1844574213027954, 0.18482230603694916, 0.18520502746105194, 0.18391546607017517, 0.1833214908838272, 0.18356870114803314, 0.18519294261932373, 0.1828571856021881, 0.18351934850215912, 0.18478375673294067, 0.18229226768016815, 0.1848311722278595, 0.18470078706741333, 0.1838543564081192, 0.18400223553180695, 0.18427447974681854, 0.18334414064884186, 0.18347379565238953, 0.1849975287914276, 0.184432253241539, 0.18420842289924622, 0.1827174723148346, 0.18340566754341125, 0.18167823553085327, 0.18242302536964417, 0.18263918161392212, 0.18254885077476501, 0.182223379611969, 0.18314296007156372, 0.18462684750556946, 0.18248333036899567, 0.18262095749378204, 0.18185321986675262, 0.1840805560350418, 0.18317130208015442, 0.1828441023826599, 0.1816563606262207, 0.18435123562812805, 0.1841139942407608], "moving_avg_accuracy_train": [0.051429209492663344, 0.1066842638975406, 0.16431773266628596, 0.22022784567572787, 0.27418730129710855, 0.32467807132889637, 0.3712172345956007, 0.4150272361152765, 0.45546499157813053, 0.492063476443481, 0.5268596183305558, 0.5593128914550491, 0.5883278138670839, 0.6155293415831349, 0.640747608456106, 0.6637811591703422, 0.6847439057429259, 0.704517113596328, 0.7223060973155988, 0.738918143862878, 0.7546779212482311, 0.7683550547474851, 0.7809829121372622, 0.7928709980749294, 0.803668075864105, 0.8141525647374122, 0.8235722565840844, 0.8324941187175272, 0.8405027241030649, 0.8481594029143807, 0.8551667433826787, 0.8618242309814725, 0.867950864500158, 0.8738275578812608, 0.8792376338576234, 0.8842368745208644, 0.8887873443915908, 0.8931290888537791, 0.8972599452531003, 0.9009637290708135, 0.9045203848412885, 0.9078886055049078, 0.911038730886726, 0.9140295926517817, 0.9166935746010738, 0.9193514668756471, 0.921810891091783, 0.9241986869493821, 0.9264803088010016, 0.9286639407519739, 0.9305109153091131, 0.9324030937521313, 0.934096825853247, 0.9357304306894801, 0.9373215827801852, 0.9388234101749241, 0.9401401775980464, 0.9415669395597718, 0.942790715651552, 0.943947917705583, 0.945166038766097, 0.9463180791943507, 0.9473806003631401, 0.9483926008888416, 0.9493917930655537, 0.9502909578781381, 0.9511491424809204, 0.9519912630877102, 0.952770061924288, 0.953443043042675, 0.9541789704313755, 0.9548506056764439, 0.955538818802967, 0.9561627888168194, 0.9567988386888289, 0.957380512071238, 0.9579085963153877, 0.9584420008553606, 0.9588941631556218, 0.9593173852675236, 0.9597540887396638, 0.9601146058300755, 0.9604716232947792, 0.960876572272518, 0.9611992097227302, 0.9615174491648167, 0.961724917749627, 0.962013837877119, 0.9622971895775946, 0.9626102987794419, 0.9628687734753719, 0.9631246882386229, 0.9633132309446147, 0.9635270251097509, 0.963721765007183, 0.9638924166660715, 0.9640761940447763, 0.9642299679415629, 0.9643730507951087, 0.9645389556466148, 0.9646464894320365, 0.9648153494520111, 0.9648975329568839, 0.9650552395172309, 0.965120445510829, 0.9652046354443345, 0.9653129584678228, 0.9653454171199332, 0.965425783180642, 0.9654748250983658, 0.965539889163603, 0.9656054583175638, 0.9657063592835188, 0.9657947729064311, 0.9659115835968235, 0.9659841611348432, 0.966028590628594, 0.9661034544051126, 0.9661522306135031, 0.9662100440450929, 0.9662202234549524, 0.9662154340309688, 0.9662158098958213, 0.9661951857860842, 0.9662301025099398, 0.9662336257756955, 0.9663088763279709, 0.9663231594512185, 0.9663801560407036, 0.9663941784926502, 0.9663720296137155, 0.96642185008696, 0.9664387867271658, 0.9664307061176183, 0.966439745659511, 0.9664502424448426, 0.9664573644028315, 0.9664660632650123, 0.9664762173897845, 0.9664458285723176, 0.9664138283389785, 0.9664292059563542, 0.9664709475977066, 0.9664643732963616, 0.9664654318715795, 0.9664454942988088, 0.9664694031618866, 0.9664699947993709, 0.9664495648850023, 0.9664544294501658, 0.966414629731432, 0.9664090369190954, 0.9663923776439448, 0.9664726793486811, 0.9664822079139053, 0.9664838081761785, 0.9665108250491291, 0.9664839148633376, 0.9665341725556675, 0.96650961396566, 0.9664666209441862, 0.9665232583260503, 0.966541643837576, 0.9665419508051011, 0.9665189755877784, 0.96648899729695, 0.9664736065304332], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.05182178910956324, 0.10686630800545932, 0.1636911782873682, 0.2183463921604386, 0.2707731540174369, 0.31960636548881066, 0.3644300145197489, 0.40651293315437037, 0.44510674526062005, 0.4799083883644526, 0.5118696332535344, 0.5421057744556659, 0.5689531201087439, 0.5937943244758815, 0.6166366011303265, 0.6374418792703059, 0.6564250362698566, 0.6744752625468619, 0.6903552847673262, 0.7051630886042232, 0.7191360552050208, 0.7315987733385096, 0.7425365169572189, 0.7528199393390572, 0.7620495759115521, 0.7706492175767975, 0.7786462722404279, 0.7860817320834033, 0.7922721281521713, 0.7984853097270144, 0.8041158532554424, 0.8089127287262083, 0.8135748020508767, 0.8174582857012107, 0.8216452508057884, 0.8251540832176795, 0.8281146608710621, 0.8311952493302661, 0.8340420506397095, 0.8367038870855277, 0.8394779578555142, 0.8416053220850229, 0.8433999385964002, 0.8452846776527994, 0.8469544697237393, 0.8485803824087449, 0.8501678331550692, 0.8515944798094418, 0.8527574209945368, 0.8540868887971915, 0.8550270621633308, 0.8557226157832176, 0.8566212867632542, 0.857428031627968, 0.8579587895062103, 0.8587528249004688, 0.8591256598803014, 0.8598905164732201, 0.860578887406847, 0.8612360718495207, 0.861705467535427, 0.8621716042344446, 0.8628017353207893, 0.8633342912220687, 0.863726083297151, 0.8640919327046347, 0.86454326748387, 0.8650257994986608, 0.8652738843172434, 0.8654361254977178, 0.8657561295236237, 0.8662140020757794, 0.8663941537789694, 0.8664433385046117, 0.8668091056048284, 0.8671403550123425, 0.8675473132516957, 0.8677294406897038, 0.8680154256964111, 0.8684457696572669, 0.8687832215883776, 0.868913970871558, 0.8690692958288299, 0.8692335023528747, 0.8695064470629938, 0.8696381159862125, 0.869708819400769, 0.8697714229652103, 0.869987487088117, 0.8702053293525734, 0.8703424112516535, 0.8704525484209158, 0.8703665073871826, 0.8704487913717324, 0.8704384272477369, 0.8705532288659601, 0.8706066926887014, 0.8706680466690783, 0.8705991359215981, 0.8707222807349353, 0.8707842829419388, 0.8708746470046725, 0.8708715549510426, 0.8709664283527757, 0.8709409216244259, 0.8709423796314111, 0.8710057565026074, 0.8710139675616839, 0.870948115327353, 0.8708267836515454, 0.8708406849644783, 0.8707778949412985, 0.8707335909516868, 0.8705828245711266, 0.8706333288233512, 0.8706767236330342, 0.8708510858141585, 0.8709093260185107, 0.8709871857735872, 0.8709718103344062, 0.8709325288679837, 0.8709846837842726, 0.8709573515127731, 0.8708818653261041, 0.8708648149004214, 0.870861676548557, 0.870895473125629, 0.8708892689512437, 0.8709070697481374, 0.8709750071163206, 0.870861134275547, 0.87082174289242, 0.8708717398663557, 0.8707570162279882, 0.8707646577433671, 0.8706860858884581, 0.8706753768666303, 0.8706402951758258, 0.8706209286853517, 0.8705811437987442, 0.8705941655257975, 0.8706669202363955, 0.8706957783821837, 0.870770578838393, 0.8707890711239815, 0.8708433647834206, 0.8709176726480755, 0.8709835202176054, 0.8709817478739322, 0.870904851559807, 0.870909916573254, 0.8709256526079467, 0.8709296670252394, 0.8709068069209835, 0.8708760848132225, 0.8709705052287376, 0.870906940210382, 0.8708741457563618, 0.8708578672876534, 0.8708686602369754, 0.870766451592796, 0.8707477060005344, 0.8706464152574087, 0.8706407028073455, 0.8705968814912194, 0.8706337730201849, 0.8706527093476844], "moving_var_accuracy_train": [0.023804672301362285, 0.048902294406799565, 0.07390661546698116, 0.09464942055084014, 0.1113890841543579, 0.1231940364645481, 0.13036767627617793, 0.1346047547469461, 0.13586118787411375, 0.13433011093665598, 0.13179404325501945, 0.12809357335795415, 0.12286100752534632, 0.11723421476359547, 0.11123444214392345, 0.10488589805607872, 0.09835223894526819, 0.09203583279006597, 0.08568028098693972, 0.07959589370264647, 0.07387163958150479, 0.06816805145016207, 0.0627864113454548, 0.057779709496261586, 0.053050930545705005, 0.04873515805354501, 0.04466021759856688, 0.040910592454063636, 0.03739677305064866, 0.03418471831936068, 0.031208171871372432, 0.028486253954388027, 0.02597544930339975, 0.023688724098919242, 0.021583271987657467, 0.019649876453772742, 0.017871249792794967, 0.016253781518289954, 0.014781979137787279, 0.013427243355123741, 0.012198367222038242, 0.011080634693783911, 0.010061880833696097, 0.009136200037205536, 0.00828645123192037, 0.007521385630817463, 0.0068236859750099585, 0.006192631499027069, 0.005620220533588453, 0.0051011127167053675, 0.004621703280167309, 0.0041917560054925826, 0.0037983989608164747, 0.0034425770475835036, 0.003121105227606947, 0.002829294074558548, 0.0025619695551220567, 0.0023240934468686907, 0.002105162753487139, 0.0019066985274831035, 0.0017293830449974042, 0.0015683895148326428, 0.0014217111244565082, 0.0012887573175870382, 0.0011688670508823564, 0.0010592568220358324, 0.0009599594671443234, 0.0008703460244773118, 0.0007887701706802752, 0.0007139692858835964, 0.000647446659388191, 0.000586761838571135, 0.0005323483904816911, 0.00048261759863720504, 0.00043799687373063375, 0.0003972422816717995, 0.00036002791022489227, 0.0003265858028317759, 0.00029576727926059637, 0.0002678026039385605, 0.0002427387328479183, 0.00021963461271543613, 0.00019881830467482415, 0.00018041232727848683, 0.00016330794886915302, 0.00014788864106473365, 0.00013348716588140904, 0.00012088972285389855, 0.00010952334424397049, 9.945334617010714e-05, 9.010929406902106e-05, 8.168779595656698e-05, 7.383895152875502e-05, 6.6866427881296e-05, 6.0521097742033114e-05, 5.473108586596258e-05, 4.956194440367874e-05, 4.4818567665307525e-05, 4.052096522558607e-05, 3.6716588480806855e-05, 3.314900126778999e-05, 3.0090724498123507e-05, 2.714243920456964e-05, 2.4652037516701145e-05, 2.222510015944101e-05, 2.006638164762988e-05, 1.8165348379625715e-05, 1.6358295618534495e-05, 1.478059439010567e-05, 1.332418093834147e-05, 1.2029862837774102e-05, 1.0865570379556959e-05, 9.870642385977027e-06, 8.953930865827846e-06, 8.181340415754433e-06, 7.410613865404064e-06, 6.687318298098244e-06, 6.0690277336000444e-06, 5.483537026784663e-06, 4.96526485995598e-06, 4.469670957426165e-06, 4.0229103089224025e-06, 3.6206205494996487e-06, 3.262386679671745e-06, 2.9471206101478336e-06, 2.6525202697473204e-06, 2.4382320533323723e-06, 2.196244916486483e-06, 2.005857925754139e-06, 1.807041795606085e-06, 1.6307527715880302e-06, 1.4900162104180206e-06, 1.343596237409366e-06, 1.209824279924362e-06, 1.0895772717905866e-06, 9.816111871322192e-07, 8.839065689893684e-07, 7.961969439196057e-07, 7.175052057766686e-07, 6.540660072422921e-07, 5.97875540921928e-07, 5.402162268751099e-07, 5.018758857927214e-07, 4.520772901570284e-07, 4.068796463747557e-07, 3.6976924300917476e-07, 3.37937022311301e-07, 3.041464703943861e-07, 2.7748825596493274e-07, 2.4995240631650686e-07, 2.392133241864805e-07, 2.1557350771632756e-07, 1.9651393998158508e-07, 2.348978200352569e-07, 2.1222518002881257e-07, 1.9102570958001794e-07, 1.7849234143828317e-07, 1.671605301884277e-07, 1.7317699791457107e-07, 1.612874172115305e-07, 1.6179427454933532e-07, 1.7448498431421504e-07, 1.6007872918932508e-07, 1.440717043319461e-07, 1.3441527939793516e-07, 1.2906203274708508e-07, 1.182877107181253e-07], "duration": 124909.51978, "accuracy_train": [0.5142920949266335, 0.603979753541436, 0.6830189515849945, 0.7234188627607051, 0.7598224018895349, 0.7790950016149871, 0.7900697039959395, 0.8093172497923589, 0.8194047907438169, 0.8214498402316353, 0.8400248953142304, 0.851392349575489, 0.8494621155753967, 0.8603430910275931, 0.8677120103128461, 0.8710831155984681, 0.8734086248961794, 0.8824759842769472, 0.8824069507890366, 0.8884265627883905, 0.8965159177164084, 0.8914492562407714, 0.8946336286452565, 0.899863771513935, 0.900841775966685, 0.9085129645971761, 0.9083494832041344, 0.9127908779185124, 0.9125801725729051, 0.9170695122162238, 0.9182328075973607, 0.9217416193706165, 0.9230905661683279, 0.926717798311185, 0.9279283176448875, 0.9292300404900333, 0.9297415732281286, 0.9322047890134736, 0.9344376528469915, 0.9342977834302326, 0.9365302867755629, 0.9382025914774824, 0.9393898593230897, 0.9409473485372831, 0.9406694121447029, 0.943272497346807, 0.9439457090370063, 0.945688849667774, 0.9470149054655776, 0.9483166283107235, 0.9471336863233666, 0.9494326997392949, 0.949340414763289, 0.9504328742155776, 0.9516419515965301, 0.9523398567275747, 0.9519910844061462, 0.9544077972153008, 0.9538047004775747, 0.9543627361918604, 0.9561291283107235, 0.9566864430486341, 0.9569432908822444, 0.957500605620155, 0.9583845226559615, 0.958383441191399, 0.9588728039059615, 0.9595703485488187, 0.9597792514534883, 0.959499873108158, 0.9608023169296788, 0.9608953228820598, 0.9617327369416758, 0.9617785189414912, 0.962523287536914, 0.9626155725129198, 0.9626613545127353, 0.9632426417151162, 0.9629636238579733, 0.96312638427464, 0.9636844199889257, 0.9633592596437799, 0.9636847804771133, 0.9645211130721669, 0.96410294677464, 0.9643816041435955, 0.9635921350129198, 0.9646141190245479, 0.9648473548818751, 0.9654282815960686, 0.9651950457387413, 0.9654279211078812, 0.9650101152985419, 0.9654511725959765, 0.9654744240840717, 0.9654282815960686, 0.9657301904531194, 0.9656139330126431, 0.965660796477021, 0.9660320993101699, 0.9656142935008305, 0.9663350896317828, 0.9656371845007383, 0.9664745985603543, 0.9657072994532114, 0.9659623448458842, 0.9662878656792175, 0.9656375449889257, 0.966149077727021, 0.9659162023578812, 0.9661254657507383, 0.9661955807032114, 0.9666144679771133, 0.9665904955126431, 0.9669628798103543, 0.966637358977021, 0.9664284560723514, 0.9667772283937799, 0.966591216489018, 0.9667303649294019, 0.9663118381436876, 0.9661723292151162, 0.9662191926794942, 0.9660095687984496, 0.96654435302464, 0.9662653351674971, 0.9669861312984496, 0.9664517075604466, 0.9668931253460686, 0.9665203805601699, 0.9661726897033037, 0.9668702343461609, 0.966591216489018, 0.9663579806316908, 0.9665211015365448, 0.9665447135128276, 0.9665214620247323, 0.96654435302464, 0.9665676045127353, 0.9661723292151162, 0.9661258262389257, 0.9665676045127353, 0.9668466223698781, 0.9664052045842562, 0.9664749590485419, 0.9662660561438722, 0.9666845829295865, 0.9664753195367294, 0.9662656956556847, 0.9664982105366371, 0.9660564322628276, 0.9663587016080657, 0.9662424441675894, 0.9671953946913067, 0.9665679650009228, 0.9664982105366371, 0.9667539769056847, 0.9662417231912146, 0.9669864917866371, 0.9662885866555924, 0.9660796837509228, 0.9670329947628276, 0.9667071134413067, 0.9665447135128276, 0.9663121986318751, 0.9662191926794942, 0.9663350896317828], "end": "2016-02-02 20:28:47.571000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0], "moving_var_accuracy_valid": [0.024169480438644433, 0.04902162393910594, 0.07318105448819728, 0.09274768067097716, 0.10821000083317314, 0.11885114363332704, 0.1250484648820329, 0.12848236676110317, 0.1290394710810254, 0.1270359132374617, 0.12352601248745426, 0.11940142935186594, 0.11394830613422195, 0.10810724443048884, 0.10199244641226378, 0.09568893815737455, 0.08936328658866341, 0.0833592539476569, 0.07729290450439322, 0.07153705354420206, 0.06614054235042491, 0.06092436220485556, 0.05590863410358755, 0.05126950967617844, 0.04690923442990361, 0.042883895517848866, 0.03917108191570183, 0.03575154829182013, 0.0325212824940321, 0.029616586872166617, 0.026940255368779666, 0.02445331996044003, 0.022203602313557284, 0.02011897508956326, 0.018264853671689497, 0.01654917544857318, 0.014973143084091242, 0.013561239002976948, 0.012278053601938285, 0.011114016601123032, 0.010071874158742765, 0.009105417849953433, 0.008223861900764259, 0.00743344588248428, 0.0067151951442774175, 0.006067467958383034, 0.005483401161392782, 0.004953378931341396, 0.004470212928007167, 0.004039098996951112, 0.0036431444308815796, 0.0032831841413366604, 0.002962134212976235, 0.0026717783271692896, 0.002407135829780208, 0.002172096676668204, 0.0019561380623010654, 0.001765789306540539, 0.0015934750667668449, 0.0014380145826153917, 0.00129619611514338, 0.0011685320544285807, 0.001055252435659524, 0.0009522797341854603, 0.0008584332700377898, 0.0007737945551346164, 0.0006982484273676813, 0.0006305191189385952, 0.0005680211217396366, 0.0005114559093714487, 0.00046123194162366786, 0.0004169955729274593, 0.00037558810736017413, 0.0003380510688592852, 0.000305450032117764, 0.00027589256443579386, 0.00024979384306941083, 0.00022511299239554846, 0.00020333777997254618, 0.00018467076529709767, 0.00016722855301968067, 0.0001506595560931821, 0.0001358107330650276, 0.0001224723338013745, 0.00011089558975427534, 9.996206112692191e-05, 9.001084576969922e-05, 8.104503404925595e-05, 7.336068399119727e-05, 6.645171286172884e-05, 5.997566459905438e-05, 5.40872699036271e-05, 4.8745170448637214e-05, 4.393158929079404e-05, 3.953939709731039e-05, 3.570407209149955e-05, 3.215939030542865e-05, 2.8977330073058523e-05, 2.6122335285817077e-05, 2.364658356270199e-05, 2.1316523669491482e-05, 1.9258362277046226e-05, 1.7332612096502457e-05, 1.568035954805988e-05, 1.4118178931973914e-05, 1.270638017083584e-05, 1.1471891803975879e-05, 1.0325309416998728e-05, 9.331807126196374e-06, 8.531118793565224e-06, 7.679746132720001e-06, 6.947254802546233e-06, 6.2701949137513e-06, 5.847749935941324e-06, 5.285931057782173e-06, 4.774285937570731e-06, 4.5704768756714606e-06, 4.143956480731214e-06, 3.784120105803459e-06, 3.4078357323931925e-06, 3.080939461592626e-06, 2.7973267330713614e-06, 2.5243175373521502e-06, 2.323169263017213e-06, 2.0934687898591427e-06, 1.884210554145053e-06, 1.7060693763266125e-06, 1.5358088647121668e-06, 1.3850797935714073e-06, 1.2881111881752618e-06, 1.276003284150455e-06, 1.1623680853173204e-06, 1.0686285534101483e-06, 1.0802193168716196e-06, 9.727229200000309e-07, 9.310124554545857e-07, 8.389433582456894e-07, 7.661255476885114e-07, 6.928885414992294e-07, 6.378452221706152e-07, 5.755867883326087e-07, 5.656673407270918e-07, 5.165957398593615e-07, 5.152921401155616e-07, 4.668406077405546e-07, 4.4668676006420053e-07, 4.517130128039488e-07, 4.4556483324038275e-07, 4.010366207352076e-07, 4.1415034679598844e-07, 3.729662013673556e-07, 3.378981863212594e-07, 3.042534076049309e-07, 2.785313261437417e-07, 2.5917282467687693e-07, 3.134924760035772e-07, 3.1850783243025635e-07, 2.96336335117532e-07, 2.6908759849721104e-07, 2.4322722844309764e-07, 3.1292396810384e-07, 2.8479414635657403e-07, 3.4865306350757505e-07, 3.140814459283337e-07, 2.9995607105873214e-07, 2.8220932813750826e-07, 2.572156558163032e-07], "accuracy_test": 0.8693000637755102, "start": "2016-02-01 09:46:58.052000", "learning_rate_per_epoch": [0.0011058017844334245, 0.001051010680384934, 0.0009989344980567694, 0.000949438544921577, 0.0009023950551636517, 0.0008576825493946671, 0.0008151854854077101, 0.0007747940835542977, 0.000736404035706073, 0.0006999161560088396, 0.0006652362062595785, 0.0006322746048681438, 0.0006009461940266192, 0.0005711700650863349, 0.0005428693257272243, 0.0005159708671271801, 0.0004904051893390715, 0.0004661062266677618, 0.0004430112603586167, 0.00042106062755919993, 0.0004001976049039513, 0.00038036832120269537, 0.0003615215537138283, 0.00034360861172899604, 0.00032658324926160276, 0.0003104014613199979, 0.0002950214548036456, 0.0002804035320878029, 0.00026650988729670644, 0.00025330466451123357, 0.00024075373949017376, 0.00022882470511831343, 0.00021748672588728368, 0.00020671053789556026, 0.00019646828877739608, 0.0001867335377028212, 0.00017748112441040576, 0.00016868715465534478, 0.00016032892744988203, 0.00015238483319990337, 0.0001448343537049368, 0.00013765798939857632, 0.0001308372156927362, 0.0001243543956661597, 0.00011819279461633414, 0.00011233649274799973, 0.00010677036334527656, 0.00010148002911591902, 9.645182581152767e-05, 9.167275857180357e-05, 8.713048737263307e-05, 8.28132833703421e-05, 7.870999252190813e-05, 7.481001375708729e-05, 7.110326987458393e-05, 6.75801929901354e-05, 6.423168088076636e-05, 6.104908243287355e-05, 5.802417945233174e-05, 5.514915392268449e-05, 5.241658436716534e-05, 4.9819409468909726e-05, 4.7350920794997364e-05, 4.5004744606558233e-05, 4.277481639292091e-05, 4.065537723363377e-05, 3.864095560857095e-05, 3.672634557005949e-05, 3.490660310490057e-05, 3.3177024306496605e-05, 3.15331453748513e-05, 2.9970718969707377e-05, 2.8485708753578365e-05, 2.7074278477812186e-05, 2.5732782887644134e-05, 2.4457756808260456e-05, 2.3245906049851328e-05, 2.2094101950642653e-05, 2.0999366824980825e-05, 1.995887578232214e-05, 1.896993853733875e-05, 1.803000304789748e-05, 1.7136639144155197e-05, 1.62875403475482e-05, 1.548051295685582e-05, 1.4713473319716286e-05, 1.398443964717444e-05, 1.329152837570291e-05, 1.2632950529223308e-05, 1.2007003533653915e-05, 1.1412072126404382e-05, 1.0846618351934012e-05, 1.0309182471246459e-05, 9.798375685932115e-06, 9.312878319178708e-06, 8.851437087287195e-06, 8.412859642703552e-06, 7.996012755029369e-06, 7.59982049203245e-06, 7.223259217425948e-06, 6.865355771878967e-06, 6.5251861087745056e-06, 6.201871656230651e-06, 5.8945770433638245e-06, 5.602508281299379e-06, 5.324910944182193e-06, 5.06106835018727e-06, 4.810298833035631e-06, 4.571954832499614e-06, 4.3454201659187675e-06, 4.1301100281998515e-06, 3.925468263332732e-06, 3.7309662275220035e-06, 3.5461016523186117e-06, 3.3703968256304506e-06, 3.2033979096013354e-06, 3.0446735763689503e-06, 2.893813871196471e-06, 2.7504290756041883e-06, 2.6141487978748046e-06, 2.484621063558734e-06, 2.361511178605724e-06, 2.2445012746175053e-06, 2.1332891719794134e-06, 2.027587470365688e-06, 1.9271230939921224e-06, 1.8316366094950354e-06, 1.740881316436571e-06, 1.6546229062441853e-06, 1.572638439029106e-06, 1.4947162298994954e-06, 1.4206549394657486e-06, 1.3502633464668179e-06, 1.2833595519623486e-06, 1.219770751959004e-06, 1.1593326689762762e-06, 1.1018892109859735e-06, 1.0472920166648692e-06, 9.954000006473507e-07, 9.460791829951631e-07, 8.992021776066395e-07, 8.546478511561872e-07, 8.123011525640322e-07, 7.720526582488674e-07, 7.337984584410151e-07, 6.974397024350765e-07, 6.628824280596746e-07, 6.300374479906168e-07, 5.988198950035439e-07, 5.691491082870925e-07, 5.409485197560571e-07, 5.141451993040391e-07, 4.886699684902851e-07, 4.644570026357542e-07, 4.4144374555799004e-07, 4.195707674625737e-07, 3.9878156599115755e-07, 3.790224525346275e-07, 3.602423817028466e-07, 3.4239283763781714e-07, 3.2542772032684297e-07, 3.0930320349398244e-07, 2.9397762091321056e-07, 2.794114095650002e-07, 2.6556693910606555e-07, 2.524084266042337e-07, 2.399019081167353e-07], "accuracy_train_first": 0.5142920949266335, "accuracy_train_last": 0.9663350896317828, "batch_size_eval": 1024, "accuracy_train_std": [0.015107763491419048, 0.014522666249733557, 0.014423801334837134, 0.014291964315955814, 0.015442250145364073, 0.016066031752639234, 0.016159576784825515, 0.01635788077510214, 0.017410683136018696, 0.017882038318156865, 0.018125210086563863, 0.017179960221160293, 0.01841289666574918, 0.016847944749055505, 0.01773734663686853, 0.01737712690439711, 0.016461664136207546, 0.015497404066651286, 0.01603427885584075, 0.01574148336674533, 0.01519165311495591, 0.016632881064615617, 0.01565900330635904, 0.016073745938404025, 0.015388153261738003, 0.015401597225696152, 0.015324295326877234, 0.013851434346523923, 0.014520049970663395, 0.014269053732010414, 0.01331611324522087, 0.013396336936823208, 0.01372951863329646, 0.013794042892262819, 0.012382871305854734, 0.011658942427083206, 0.012549160261536437, 0.01271410462167249, 0.012460990161913835, 0.011576546489996876, 0.012118814129554673, 0.012535059896932686, 0.010410939305969737, 0.010240611559444111, 0.010420940255993901, 0.010117799097140754, 0.010732597155013294, 0.010724950424682715, 0.009813829080613196, 0.010656960657076463, 0.00930746465400629, 0.009443517499837848, 0.009745415687682706, 0.008972572393672848, 0.009361846234337187, 0.008847356492938621, 0.008720071767750855, 0.009390374719811181, 0.008307043013299919, 0.007867466437561046, 0.00791135532082624, 0.007938347875347748, 0.008396182503908922, 0.00821730535909426, 0.00744131282883704, 0.008358628915531125, 0.007466138082943627, 0.007364208128023984, 0.00747520227948549, 0.00739310306857689, 0.006573788180716521, 0.007440193165188463, 0.006339303357315571, 0.006723772920282952, 0.006472409032921336, 0.007233212253910474, 0.006757908419374167, 0.006857731904053041, 0.006630849071898859, 0.006483306541458537, 0.006466996224147828, 0.006187220657616419, 0.005859045663249957, 0.006530832500058757, 0.006126257731942281, 0.006374554126024888, 0.0065852290239271465, 0.006308462675980056, 0.006133871138143797, 0.006126482453088118, 0.0063438132777858805, 0.006334133893603659, 0.005868603144300188, 0.00621796062874876, 0.006118071643032098, 0.005903762914406532, 0.005910250953873894, 0.0059322660708837575, 0.0056875919141393585, 0.005897219662635607, 0.005710007153363972, 0.005923150112389313, 0.005961663805901152, 0.005642031924571267, 0.0058306299360637226, 0.006015561372555678, 0.005957264653288357, 0.005708810298568279, 0.0057577439410978915, 0.006121529626382074, 0.006041824774246978, 0.005856587340016571, 0.005380385890655154, 0.0059965131679577555, 0.005458673953394879, 0.005726822478671941, 0.005500354269624465, 0.005253221892638851, 0.00536898468749028, 0.005283266669800893, 0.005456287238158856, 0.005467060646817133, 0.005225746113398278, 0.005502002803039458, 0.0052517696607490534, 0.00556326785947448, 0.005384619985015137, 0.005315036006159362, 0.005412316008560937, 0.005884483241621739, 0.0056667212502392985, 0.005506008391741482, 0.005385874935908664, 0.005720143891294171, 0.005563458823840232, 0.005379678965337006, 0.005414304194593039, 0.005400976001018209, 0.005520508712232513, 0.005590272264975734, 0.005613268783252395, 0.0054667766212748185, 0.0055985303183696405, 0.005434364858143376, 0.005382287701333726, 0.005147038415007786, 0.005155532110632697, 0.005032960345402108, 0.005239352434668518, 0.005360380441095815, 0.004989123201306935, 0.005338037450739846, 0.005164906228407979, 0.005290139663826542, 0.005284916034109687, 0.005014583384598129, 0.005289697253035497, 0.0055425480396092495, 0.005111776622863337, 0.005312874601967805, 0.00535460737577403, 0.004983863608709672, 0.0054998223502791925, 0.005048717956665092, 0.0053084131516834355, 0.00525607399876555, 0.005261309536235024], "accuracy_test_std": 0.01098685302214843, "error_valid": [0.48178210890436746, 0.39773302193147586, 0.32488498917545183, 0.2897566829819277, 0.25738598926957834, 0.24089473126882532, 0.23215714420180722, 0.2147407991340362, 0.20754894578313254, 0.2068768237010542, 0.20047916274472888, 0.18576895472515065, 0.1894207690135542, 0.18263483621987953, 0.17778290897966864, 0.17531061746987953, 0.17272655073418675, 0.1630727009600903, 0.16672451524849397, 0.16156667686370485, 0.15510724538780118, 0.1562367634600903, 0.15902379047439763, 0.15462925922439763, 0.15488369493599397, 0.15195400743599397, 0.14938023578689763, 0.1469991293298193, 0.15201430722891573, 0.14559605609939763, 0.14520925498870485, 0.14791539203689763, 0.1444665380271084, 0.1475903614457832, 0.14067206325301207, 0.14326642507530118, 0.14524014024849397, 0.14107945453689763, 0.14033673757530118, 0.1393395849021084, 0.1355554052146084, 0.13924839984939763, 0.14044851280120485, 0.1377526708396084, 0.13801740163780118, 0.13678640342620485, 0.13554511012801207, 0.13556570030120485, 0.1367761083396084, 0.13394790097891573, 0.13651137754141573, 0.13801740163780118, 0.13529067441641573, 0.1353112645896084, 0.1372643895896084, 0.13410085655120485, 0.13751882530120485, 0.13322577419051207, 0.13322577419051207, 0.13284926816641573, 0.13406997129141573, 0.13363316547439763, 0.1315270849021084, 0.13187270566641573, 0.1327477880271084, 0.13261542262801207, 0.13139471950301207, 0.13063141236822284, 0.13249335231551207, 0.13310370387801207, 0.13136383424322284, 0.1296651449548193, 0.1319844808923193, 0.1331139989646084, 0.12989899049322284, 0.12987840032003017, 0.1287900625941265, 0.13063141236822284, 0.12941070924322284, 0.12768113469503017, 0.1281797110316265, 0.1299092855798193, 0.12953277955572284, 0.12928863893072284, 0.12803705054593373, 0.1291768637048193, 0.12965484986822284, 0.1296651449548193, 0.12806793580572284, 0.1278340902673193, 0.1284238516566265, 0.12855621705572284, 0.13040786191641573, 0.1288106527673193, 0.12965484986822284, 0.12841355657003017, 0.1289121329066265, 0.12877976750753017, 0.13002106080572284, 0.12816941594503017, 0.12865769719503017, 0.12831207643072284, 0.1291562735316265, 0.1281797110316265, 0.12928863893072284, 0.12904449830572284, 0.1284238516566265, 0.1289121329066265, 0.1296445547816265, 0.13026520143072284, 0.1290342032191265, 0.1297872152673193, 0.1296651449548193, 0.13077407285391573, 0.1289121329066265, 0.1289327230798193, 0.12757965455572284, 0.1285665121423193, 0.12831207643072284, 0.12916656861822284, 0.1294210043298193, 0.1285459219691265, 0.12928863893072284, 0.12979751035391573, 0.12928863893072284, 0.12916656861822284, 0.12880035768072284, 0.12916656861822284, 0.1289327230798193, 0.12841355657003017, 0.13016372129141573, 0.12953277955572284, 0.12867828736822284, 0.1302754965173193, 0.12916656861822284, 0.13002106080572284, 0.1294210043298193, 0.12967544004141573, 0.12955336972891573, 0.12977692018072284, 0.12928863893072284, 0.12867828736822284, 0.12904449830572284, 0.12855621705572284, 0.12904449830572284, 0.1286679922816265, 0.12841355657003017, 0.1284238516566265, 0.1290342032191265, 0.1297872152673193, 0.12904449830572284, 0.1289327230798193, 0.1290342032191265, 0.1292989340173193, 0.1294004141566265, 0.1281797110316265, 0.1296651449548193, 0.1294210043298193, 0.12928863893072284, 0.1290342032191265, 0.1301534262048193, 0.1294210043298193, 0.13026520143072284, 0.12941070924322284, 0.12979751035391573, 0.1290342032191265, 0.1291768637048193], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.04954872330046761, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 32, "valid_ratio": 0.15, "learning_rate": 0.0011634492969883436, "optimization": "adam", "nb_data_augmentation": 4, "learning_rate_decay_method": "exp", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 4.897736744524783e-08, "rotation_range": [0, 0], "momentum": 0.6563166260411704}, "accuracy_valid_max": 0.8724203454442772, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8708231362951807, "accuracy_valid_std": [0.02520556792941318, 0.020801424966839725, 0.028263997356784514, 0.02268346955753319, 0.017169355208836057, 0.016849872300201046, 0.015142200544043981, 0.018716215490873364, 0.015653662847196343, 0.012226207841201119, 0.015656803590778434, 0.014374496146791834, 0.012321627854982595, 0.012975367715132835, 0.014452077587123668, 0.010888103227913181, 0.00716483914278356, 0.007959193153700087, 0.009920094363124318, 0.006964100376053556, 0.01119297379570709, 0.012039325561287272, 0.010826918281071674, 0.011687360144414832, 0.013261168255506226, 0.015121164883610462, 0.012558337708702313, 0.012214642279872116, 0.011834061462248344, 0.014142087840662546, 0.013135595123026996, 0.012449503602037917, 0.010388360279130072, 0.01619062691262481, 0.010475126071662537, 0.013096532757003624, 0.01539839066105208, 0.012947613185237808, 0.013095765066015059, 0.012511895140823903, 0.012517927403512118, 0.014685450687285289, 0.013489433057760113, 0.012279413756775204, 0.012869228947301668, 0.012784616968417662, 0.013273778292851932, 0.013522717951254266, 0.011889461175892933, 0.012633786813802382, 0.011198424424236984, 0.014047303111934866, 0.010957666400382664, 0.014315958616336265, 0.014580481068912255, 0.01625190515234414, 0.013281594276908972, 0.013880513968090948, 0.01427010590910382, 0.012792269790820394, 0.01317218190154304, 0.017601577732068385, 0.015772065808487265, 0.01325273210252304, 0.013959034181168875, 0.013418093385515991, 0.01430169326812779, 0.012789276558700588, 0.015241886118891476, 0.014058529252660963, 0.012883543266448841, 0.01396842012306869, 0.01295143764144354, 0.0147329582375982, 0.012083291063218175, 0.012859207584441777, 0.011822211986404348, 0.013220108475829606, 0.013385137438034475, 0.01312228502914704, 0.012377310845641813, 0.014587885862054714, 0.013023298745112127, 0.01357047320252274, 0.012208335962893583, 0.01329602883423246, 0.013094464694988237, 0.013692602608828618, 0.013785445048388639, 0.01442268371399223, 0.013325661370820436, 0.013562559588571263, 0.014714353687646557, 0.014674913159265479, 0.01326630915707994, 0.011723963478123624, 0.012224324668863681, 0.011738708528665385, 0.013485884035044304, 0.01225822841516402, 0.011903167805260085, 0.01332342374799201, 0.012549018612683052, 0.011289314293909692, 0.013473497636852525, 0.013174066023104697, 0.012057642505525906, 0.012777417235015838, 0.012941626769372112, 0.011973160329295148, 0.013391762687164853, 0.013443611011489233, 0.01376207523726392, 0.014297910170019554, 0.012427425468331, 0.013253860600538776, 0.012989065831462237, 0.013774469328691783, 0.012444430696255615, 0.012716353358043966, 0.01348472659189439, 0.01221413536190735, 0.013771033739691574, 0.01497634101242386, 0.013384728341630805, 0.012939385171833843, 0.013291476837439671, 0.013085961404906348, 0.014149980016451437, 0.011487719195603653, 0.013645196717553824, 0.01315082394041761, 0.01321347966357186, 0.013666912532767354, 0.013838640291384532, 0.013091166897041458, 0.013807944766868873, 0.014518416357189853, 0.014429673913062314, 0.013486176908872886, 0.013375819010443936, 0.013339186816558534, 0.012438687544596422, 0.01326034656048224, 0.013074150496724612, 0.012587187433413168, 0.012662523572737252, 0.012814905795209284, 0.012975825724425745, 0.01377645030601475, 0.013879103065733268, 0.01417523157494366, 0.013673649929722158, 0.014190832841575251, 0.013262459457584032, 0.013901515926642854, 0.01350853715404327, 0.013782020278159397, 0.013692903382694615, 0.012697223942957848, 0.013976026856608455, 0.013902587638818131, 0.013761226472869658, 0.0145951136503795, 0.014856463624762303, 0.01258405629788755, 0.013684838334493021], "accuracy_valid": [0.5182178910956325, 0.6022669780685241, 0.6751150108245482, 0.7102433170180723, 0.7426140107304217, 0.7591052687311747, 0.7678428557981928, 0.7852592008659638, 0.7924510542168675, 0.7931231762989458, 0.7995208372552711, 0.8142310452748494, 0.8105792309864458, 0.8173651637801205, 0.8222170910203314, 0.8246893825301205, 0.8272734492658133, 0.8369272990399097, 0.833275484751506, 0.8384333231362951, 0.8448927546121988, 0.8437632365399097, 0.8409762095256024, 0.8453707407756024, 0.845116305064006, 0.848045992564006, 0.8506197642131024, 0.8530008706701807, 0.8479856927710843, 0.8544039439006024, 0.8547907450112951, 0.8520846079631024, 0.8555334619728916, 0.8524096385542168, 0.8593279367469879, 0.8567335749246988, 0.854759859751506, 0.8589205454631024, 0.8596632624246988, 0.8606604150978916, 0.8644445947853916, 0.8607516001506024, 0.8595514871987951, 0.8622473291603916, 0.8619825983621988, 0.8632135965737951, 0.8644548898719879, 0.8644342996987951, 0.8632238916603916, 0.8660520990210843, 0.8634886224585843, 0.8619825983621988, 0.8647093255835843, 0.8646887354103916, 0.8627356104103916, 0.8658991434487951, 0.8624811746987951, 0.8667742258094879, 0.8667742258094879, 0.8671507318335843, 0.8659300287085843, 0.8663668345256024, 0.8684729150978916, 0.8681272943335843, 0.8672522119728916, 0.8673845773719879, 0.8686052804969879, 0.8693685876317772, 0.8675066476844879, 0.8668962961219879, 0.8686361657567772, 0.8703348550451807, 0.8680155191076807, 0.8668860010353916, 0.8701010095067772, 0.8701215996799698, 0.8712099374058735, 0.8693685876317772, 0.8705892907567772, 0.8723188653049698, 0.8718202889683735, 0.8700907144201807, 0.8704672204442772, 0.8707113610692772, 0.8719629494540663, 0.8708231362951807, 0.8703451501317772, 0.8703348550451807, 0.8719320641942772, 0.8721659097326807, 0.8715761483433735, 0.8714437829442772, 0.8695921380835843, 0.8711893472326807, 0.8703451501317772, 0.8715864434299698, 0.8710878670933735, 0.8712202324924698, 0.8699789391942772, 0.8718305840549698, 0.8713423028049698, 0.8716879235692772, 0.8708437264683735, 0.8718202889683735, 0.8707113610692772, 0.8709555016942772, 0.8715761483433735, 0.8710878670933735, 0.8703554452183735, 0.8697347985692772, 0.8709657967808735, 0.8702127847326807, 0.8703348550451807, 0.8692259271460843, 0.8710878670933735, 0.8710672769201807, 0.8724203454442772, 0.8714334878576807, 0.8716879235692772, 0.8708334313817772, 0.8705789956701807, 0.8714540780308735, 0.8707113610692772, 0.8702024896460843, 0.8707113610692772, 0.8708334313817772, 0.8711996423192772, 0.8708334313817772, 0.8710672769201807, 0.8715864434299698, 0.8698362787085843, 0.8704672204442772, 0.8713217126317772, 0.8697245034826807, 0.8708334313817772, 0.8699789391942772, 0.8705789956701807, 0.8703245599585843, 0.8704466302710843, 0.8702230798192772, 0.8707113610692772, 0.8713217126317772, 0.8709555016942772, 0.8714437829442772, 0.8709555016942772, 0.8713320077183735, 0.8715864434299698, 0.8715761483433735, 0.8709657967808735, 0.8702127847326807, 0.8709555016942772, 0.8710672769201807, 0.8709657967808735, 0.8707010659826807, 0.8705995858433735, 0.8718202889683735, 0.8703348550451807, 0.8705789956701807, 0.8707113610692772, 0.8709657967808735, 0.8698465737951807, 0.8705789956701807, 0.8697347985692772, 0.8705892907567772, 0.8702024896460843, 0.8709657967808735, 0.8708231362951807], "seed": 930100289, "model": "residualv3", "loss_std": [0.30724644660949707, 0.1935654580593109, 0.18618330359458923, 0.17896392941474915, 0.17591583728790283, 0.17290857434272766, 0.1677764654159546, 0.1654123067855835, 0.1610470563173294, 0.16012433171272278, 0.15693742036819458, 0.15379752218723297, 0.15255458652973175, 0.1494821459054947, 0.14878828823566437, 0.14522486925125122, 0.14243027567863464, 0.13931475579738617, 0.13917285203933716, 0.1372619867324829, 0.13563141226768494, 0.13337518274784088, 0.12950101494789124, 0.1281173676252365, 0.1275814324617386, 0.12662455439567566, 0.12319114804267883, 0.1213761493563652, 0.12102683633565903, 0.11660629510879517, 0.11651124805212021, 0.11686228960752487, 0.11177738755941391, 0.1129540205001831, 0.11017711460590363, 0.10903777927160263, 0.10865942388772964, 0.1087256520986557, 0.10636831820011139, 0.10564713180065155, 0.1034994050860405, 0.10247617214918137, 0.1025192067027092, 0.10093501210212708, 0.10026755928993225, 0.10033120214939117, 0.09827464073896408, 0.0979304164648056, 0.09677524119615555, 0.09788879752159119, 0.09562484174966812, 0.09471592307090759, 0.09344415366649628, 0.09467288106679916, 0.09501628577709198, 0.09356826543807983, 0.0910254567861557, 0.09224653989076614, 0.09100453555583954, 0.09236421436071396, 0.08980190753936768, 0.09115767478942871, 0.08992748707532883, 0.08994287252426147, 0.09021863341331482, 0.08898753672838211, 0.08703286945819855, 0.08762292563915253, 0.08771808445453644, 0.08856284618377686, 0.08752716332674026, 0.08617757260799408, 0.08778305351734161, 0.08729557693004608, 0.08542991429567337, 0.08622367680072784, 0.08568506687879562, 0.08334176242351532, 0.08477350324392319, 0.08517902344465256, 0.08371711522340775, 0.08537670224905014, 0.08432082831859589, 0.08438578993082047, 0.08362671732902527, 0.0850314125418663, 0.08239010721445084, 0.08434564620256424, 0.08406388759613037, 0.0828460231423378, 0.08141949772834778, 0.08433807641267776, 0.08380917459726334, 0.0835355594754219, 0.08273768424987793, 0.08051490783691406, 0.08250650018453598, 0.08313246816396713, 0.08179260790348053, 0.08231193572282791, 0.0829382985830307, 0.08271638303995132, 0.08310502767562866, 0.08096624910831451, 0.08342607319355011, 0.08241789042949677, 0.081182099878788, 0.08200192451477051, 0.0821300521492958, 0.0808449313044548, 0.08306954801082611, 0.08234237879514694, 0.08011455088853836, 0.08116530627012253, 0.08311065286397934, 0.07988910377025604, 0.07918060570955276, 0.08087553083896637, 0.08169520646333694, 0.08037685602903366, 0.08165812492370605, 0.08087503910064697, 0.08159377425909042, 0.08141661435365677, 0.08109153807163239, 0.08072731643915176, 0.08158408105373383, 0.08112597465515137, 0.08223148435354233, 0.08028758317232132, 0.08256813138723373, 0.08092770725488663, 0.08084528148174286, 0.08280926197767258, 0.08120211213827133, 0.0801357552409172, 0.07986704260110855, 0.08147366344928741, 0.08060488104820251, 0.08092677593231201, 0.08134237676858902, 0.08075427263975143, 0.07979552447795868, 0.08056184649467468, 0.08154239505529404, 0.08010325580835342, 0.08055712282657623, 0.08157419413328171, 0.08025309443473816, 0.07960954308509827, 0.07999163866043091, 0.07816319167613983, 0.07966601103544235, 0.07969015836715698, 0.07936237007379532, 0.08162657171487808, 0.08073864132165909, 0.08105060458183289, 0.08057790249586105, 0.07952620089054108, 0.07925575971603394, 0.07964601367712021, 0.07922190427780151, 0.07895302027463913, 0.08017583936452866, 0.08174973726272583, 0.08081360906362534]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:30 2016", "state": "available"}], "summary": "28d153401bcb43b72ce1c444aa3121d3"}