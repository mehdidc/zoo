{"content": {"hp_model": {"f0": 16, "f1": 16, "f2": 16, "f3": 16, "nonlin": "very_leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [2.0985639095306396, 1.6517256498336792, 1.5473721027374268, 1.4764806032180786, 1.424810767173767, 1.3828375339508057, 1.3464688062667847, 1.3158761262893677, 1.2920780181884766, 1.2706857919692993, 1.2520030736923218, 1.2366447448730469, 1.2215235233306885, 1.2066419124603271, 1.1962511539459229, 1.1860946416854858, 1.175965666770935, 1.166645884513855, 1.1608214378356934, 1.1523634195327759, 1.1462949514389038, 1.139378547668457, 1.1345882415771484, 1.130629301071167, 1.1248655319213867, 1.1216527223587036, 1.1178231239318848, 1.1157402992248535, 1.1103636026382446, 1.1078041791915894, 1.1065599918365479, 1.102945327758789, 1.099721074104309, 1.0982986688613892, 1.097281575202942, 1.0936863422393799, 1.0938999652862549, 1.0893287658691406, 1.0905283689498901, 1.0893746614456177, 1.0865422487258911, 1.083568811416626, 1.0847598314285278, 1.0836671590805054, 1.081817865371704, 1.082110047340393, 1.0828847885131836, 1.0802685022354126, 1.079857349395752, 1.0791065692901611, 1.078510046005249, 1.0789241790771484, 1.0784953832626343, 1.079039216041565, 1.0790883302688599, 1.0761997699737549, 1.0762040615081787, 1.0759037733078003, 1.0760269165039062, 1.076088786125183, 1.0755677223205566, 1.074951171875, 1.0755043029785156, 1.0757436752319336, 1.0729303359985352, 1.074203372001648, 1.0737688541412354, 1.0729899406433105, 1.0746954679489136, 1.0715594291687012, 1.0725982189178467, 1.0734995603561401, 1.072014570236206, 1.072818636894226, 1.072760820388794, 1.0743110179901123, 1.0738556385040283, 1.0729256868362427, 1.0725563764572144, 1.0727535486221313, 1.072243571281433, 1.0722583532333374, 1.0718382596969604, 1.0718239545822144, 1.071106195449829, 1.072460412979126, 1.0710088014602661, 1.073012351989746, 1.071002721786499, 1.072405457496643, 1.0705831050872803, 1.0711365938186646, 1.0723623037338257, 1.0729581117630005, 1.0712815523147583, 1.0732090473175049, 1.0723676681518555, 1.071906328201294, 1.072703242301941, 1.0725094079971313, 1.070681095123291, 1.0736804008483887, 1.0734049081802368, 1.070868730545044, 1.0724793672561646, 1.0730888843536377, 1.0732859373092651, 1.0720984935760498, 1.0720231533050537, 1.0707168579101562, 1.0702859163284302, 1.0717390775680542, 1.0722194910049438, 1.0723730325698853, 1.0710489749908447, 1.0711233615875244, 1.0691713094711304, 1.0714818239212036, 1.0706994533538818, 1.0720914602279663, 1.0734564065933228, 1.0723917484283447, 1.0733551979064941, 1.0727635622024536, 1.071587085723877, 1.071846604347229, 1.0713021755218506, 1.0710225105285645, 1.0723074674606323, 1.0717779397964478, 1.0712147951126099, 1.0708662271499634, 1.0731333494186401, 1.0730324983596802, 1.071489930152893, 1.0720328092575073, 1.070504903793335, 1.0714694261550903, 1.0710028409957886, 1.0704402923583984, 1.0724241733551025, 1.0714176893234253], "moving_avg_accuracy_train": [0.03786324592215762, 0.07894665171361663, 0.11896922999002167, 0.15724415171001174, 0.19346033457449413, 0.22771018881199673, 0.2594647206125523, 0.28974312252462675, 0.31803517066806664, 0.3444304347186003, 0.36833727098788094, 0.39097868750758674, 0.41183716003243714, 0.43102577064879477, 0.44852807113328774, 0.46451948770143736, 0.47946518407825744, 0.4932673722388149, 0.5060963147226207, 0.5178421815603901, 0.5287321152477434, 0.538653927255638, 0.5479623770746017, 0.5564725153938119, 0.5644757619049106, 0.5717204643458333, 0.5784057821081401, 0.5844388441358828, 0.5900801885025178, 0.595299124363414, 0.6001635773525064, 0.6046531200879092, 0.6087867865997901, 0.6126512817354922, 0.616173505185005, 0.6194735704276249, 0.6224807954781164, 0.625212946758101, 0.6276510286684389, 0.6299894616139335, 0.6320545237351167, 0.6339409453810773, 0.6357247193195753, 0.6373116228202036, 0.6387351135755125, 0.6402091344600247, 0.6414614566406373, 0.6425327069829412, 0.6435246980279102, 0.6443873351314962, 0.6452613287259047, 0.6460478869120536, 0.6467116114522067, 0.6474088728395164, 0.648064381971447, 0.6485962114699464, 0.649053967728129, 0.6494566117164363, 0.6498515073404276, 0.6501976128067817, 0.6505067465288721, 0.6507779914323248, 0.6510825296656613, 0.6512845705113876, 0.6516361431356363, 0.6518711422403082, 0.651992032728579, 0.6521496622930226, 0.6524193760367271, 0.6525481500655755, 0.6527291508582059, 0.6528060571144396, 0.6530031559295738, 0.6531108264477276, 0.6532959774223716, 0.6534115681721979, 0.6535225392446515, 0.6535967644753174, 0.6535798978745927, 0.6537206110506347, 0.6538402053650065, 0.6539431539015034, 0.6539591497712737, 0.6540084232862098, 0.6539992549782145, 0.6540282058819711, 0.6540635262417713, 0.6541534432858297, 0.6541854684028447, 0.6541958340129571, 0.6542563523846865, 0.6543409377073105, 0.6544123781512343, 0.6544581094091083, 0.6545132184340521, 0.6544768581481866, 0.6545137802087371, 0.6545562746096519, 0.6545435104919407, 0.6545389261347917, 0.6546510576538338, 0.6546914861031053, 0.6545977354717538, 0.6546435682368708, 0.6546335202563915, 0.6546431503620739, 0.6546402638107779, 0.6545841874919925, 0.6546150269157816, 0.6545847618234097, 0.6545527647961998, 0.6545054744276909, 0.6544814061400528, 0.6545132231037976, 0.6545279795759482, 0.6545945946282279, 0.6545779985086592, 0.6546257689212669, 0.6546198260211575, 0.6546402342920576, 0.6546817450775068, 0.6547098041891728, 0.6547211064968151, 0.654701123736807, 0.6547203776825707, 0.6546724939206352, 0.6546875633039501, 0.654682560607276, 0.6547035987683554, 0.6546551037978506, 0.65469051338392, 0.6546201115125823, 0.6546311185414644, 0.6547200438781633, 0.6547442731097637, 0.6547498033765374, 0.6548012475440055, 0.6548591009411369, 0.6548368723831066, 0.6548494187642127, 0.6548700111024462, 0.6548931584556568], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.03806196465549698, 0.07878981257059486, 0.11792290303793296, 0.15585691050334147, 0.1915986973332784, 0.2252261487595891, 0.25630975564567837, 0.28583838333788764, 0.3136654425003338, 0.3397037128210835, 0.3636295258857372, 0.38589723853624486, 0.40639189909527096, 0.42532846337437336, 0.44266433997556554, 0.45820456425172884, 0.4728571523483933, 0.4865226148714606, 0.4991622985085615, 0.5104382985146331, 0.5212886763533053, 0.5311903527691796, 0.5401496601598068, 0.548384964757531, 0.5559198387166423, 0.5629575729360925, 0.569388160474938, 0.5750037613137394, 0.5806356505720492, 0.5856911143646184, 0.5905462075591806, 0.5948781408318771, 0.5986537809561442, 0.6022848201703942, 0.6057124763781289, 0.6087586868540208, 0.6115491044073236, 0.6140503321913654, 0.6164499805893223, 0.6187683555537334, 0.6206463439817937, 0.6223355040583884, 0.6239747299138448, 0.6254053230933941, 0.626742714588648, 0.6281518274483073, 0.62933663882057, 0.6300591131632871, 0.6310287819015518, 0.6317905909760804, 0.6325362247907464, 0.6333303950451056, 0.6339230779615288, 0.6345622378329211, 0.6351161561806531, 0.635550559011383, 0.6361033014912688, 0.6364898769332564, 0.6367279315497952, 0.6370011568436109, 0.6373244198301835, 0.6373772458723911, 0.6375377411176067, 0.6374858448296413, 0.6375601789743127, 0.6376291387218362, 0.6376779659546978, 0.6379782581205233, 0.6380654156010163, 0.6382293065522099, 0.6383168027606938, 0.6383711352858292, 0.6386153470584511, 0.6388483741937205, 0.6388230764957341, 0.6389987095935252, 0.6390936852079679, 0.6389451706498971, 0.6389956425250429, 0.6391529895112434, 0.6392325371339143, 0.6391942667130681, 0.6394091115026047, 0.639512904559799, 0.6396063183112739, 0.6395794978976918, 0.6396652228067177, 0.6396447189748411, 0.6396140584949022, 0.6396729427903668, 0.6398124173836945, 0.6400498668162589, 0.6400285491858377, 0.6399472986535492, 0.639899616745649, 0.6398302299487197, 0.6399651533488027, 0.6398892128915579, 0.6396743821050376, 0.639591927187079, 0.6395766938998471, 0.6395548949447268, 0.6395698379615493, 0.6395323995343704, 0.6394742908874091, 0.6394850872787134, 0.6393950887635681, 0.6394646925095757, 0.639303491283844, 0.6392581254480049, 0.6391409654822706, 0.6392105379852484, 0.6393209518542687, 0.6393959102738871, 0.6396373598150225, 0.6396806774385655, 0.6396199480324348, 0.6395398479957576, 0.639397604301227, 0.6394282763823995, 0.6393948460992048, 0.6393535813217391, 0.6394140992720201, 0.6394309148248633, 0.6393839841575124, 0.6395035264891257, 0.6394015360390083, 0.6395050571339026, 0.6396979413866268, 0.6396863727280093, 0.6397878832338229, 0.6399657214164647, 0.6398531030587038, 0.6397018889030592, 0.6396299198452081, 0.6395620591671631, 0.639588492792992, 0.6395990465163284, 0.6394498534610811, 0.6394376500238586, 0.6396738960813372, 0.6397013530469986], "moving_var_accuracy_train": [0.012902628525856072, 0.026802981756101657, 0.03853894452751021, 0.047869776768801504, 0.054887306203384015, 0.059956048220657185, 0.06303559600744393, 0.06498307100784154, 0.06568872380041385, 0.06539024109904903, 0.06399504837279985, 0.06220924721368913, 0.059904005376949676, 0.05722742983573044, 0.054261661552402744, 0.05113702403186728, 0.0480336861903734, 0.04494482115351094, 0.04193157492543504, 0.03898010592282702, 0.0361494112319789, 0.03342045129046101, 0.0308582313037044, 0.028424210261242765, 0.026158256827575208, 0.024014802565935297, 0.022015563571588886, 0.02014158775130531, 0.018413851872541465, 0.016817602308968657, 0.015348808204019597, 0.013995331329374704, 0.012749582985920245, 0.011609033591213005, 0.010559784754346385, 0.009601820154361675, 0.00872302876146424, 0.00791790774086831, 0.007179615157395132, 0.00651086805942079, 0.005898161587557823, 0.005340372708439175, 0.004834972082768241, 0.004374139238974174, 0.003954962248450806, 0.00357902066171753, 0.003235233393142264, 0.0029220382494910143, 0.0026386908406416027, 0.0023815190415297895, 0.0021502419206044145, 0.0019407857925657541, 0.0017506719856959915, 0.001579980348106491, 0.0014258495432942417, 0.001285810172504085, 0.001159115022380824, 0.001044662619774623, 0.0009415998407817878, 0.0008485179576481705, 0.0007645262348065546, 0.0006887357755047412, 0.0006206968897743397, 0.0005589945853269818, 0.0005042075565853747, 0.0004542838221396065, 0.000408986970517035, 0.00036831189718161184, 0.00033213541699533795, 0.00029907112005035706, 0.00026945885962771665, 0.0002425662048151759, 0.00021865921582000372, 0.0001968976307023188, 0.00017751639558279153, 0.00015988500701752113, 0.0001440073375260624, 0.0001296561882372628, 0.00011669312975351661, 0.00010520201855937146, 9.481054190370473e-05, 8.542487332383568e-05, 7.688468880209948e-05, 6.921807083535698e-05, 6.229702027266475e-05, 5.60748616388532e-05, 5.0478603225315687e-05, 4.5503508576093885e-05, 4.096238819156294e-05, 3.6867116385263654e-05, 3.321336700658828e-05, 2.9956422397160125e-05, 2.700671379069639e-05, 2.4324864543147378e-05, 2.191971113050488e-05, 1.9739638650948395e-05, 1.7777943932851197e-05, 1.6016401506548024e-05, 1.4416227660201735e-05, 1.2974794041155791e-05, 1.1790475935104323e-05, 1.0626138477188358e-05, 9.642627257378778e-06, 8.697270312865317e-06, 7.82845193878419e-06, 7.0464413953248536e-06, 6.341872245397828e-06, 5.735986002614771e-06, 5.1709470328899905e-06, 4.662096111947457e-06, 4.205100788505168e-06, 3.8047181202379894e-06, 3.429459850442638e-06, 3.095624738035824e-06, 2.7880220454652524e-06, 2.549157927630739e-06, 2.2967210155303473e-06, 2.087587024863696e-06, 1.879146184932713e-06, 1.6949800441296355e-06, 1.540990347494078e-06, 1.3939771364720769e-06, 1.255729102247247e-06, 1.1337499883004302e-06, 1.0237114193176453e-06, 9.419759692997869e-07, 8.498221491912162e-07, 7.650651770382097e-07, 6.925420973287887e-07, 6.444537470742531e-07, 5.912929214373521e-07, 5.767714406843062e-07, 5.20184688779167e-07, 5.39335659464341e-07, 4.906855944934313e-07, 4.41892289699385e-07, 4.2152158202779613e-07, 4.0949256386178697e-07, 3.7299028660454916e-07, 3.3710796305382895e-07, 3.072135662937866e-07, 2.8131440931028487e-07], "duration": 51115.177688, "accuracy_train": [0.37863245922157623, 0.44869730383674783, 0.47917243447766705, 0.5017184471899225, 0.5194059803548358, 0.5359588769495202, 0.5452555068175526, 0.5622487397332964, 0.5726636039590255, 0.5819878111734035, 0.5834987974114064, 0.5947514361849391, 0.5995634127560908, 0.6037232661960134, 0.6060487754937246, 0.608442236814784, 0.6139764514696383, 0.6174870656838317, 0.6215567970768734, 0.6235549831003138, 0.626741518433924, 0.6279502353266888, 0.631738425445275, 0.6330637602667036, 0.6365049805047989, 0.6369227863141381, 0.6385736419689, 0.6387364023855666, 0.6408522878022334, 0.6422695471114802, 0.6439436542543374, 0.6450590047065338, 0.6459897852067183, 0.6474317379568106, 0.6478735162306202, 0.6491741576112035, 0.6495458209325398, 0.6498023082779623, 0.6495937658614802, 0.651035358123385, 0.6506400828257659, 0.6509187401947213, 0.6517786847660576, 0.6515937543258582, 0.6515465303732927, 0.653475322420635, 0.6527323562661499, 0.6521739600636766, 0.652452617432632, 0.6521510690637689, 0.6531272710755813, 0.6531269105873939, 0.6526851323135844, 0.6536842253253046, 0.6539639641588225, 0.6533826769564415, 0.6531737740517718, 0.6530804076112035, 0.6534055679563492, 0.6533125620039683, 0.6532889500276855, 0.6532191955633998, 0.6538233737656884, 0.6531029381229236, 0.654800296753876, 0.6539861341823551, 0.6530800471230159, 0.6535683283730159, 0.6548467997300664, 0.6537071163252123, 0.6543581579918789, 0.6534982134205427, 0.6547770452657807, 0.6540798611111112, 0.6549623361941675, 0.654451884920635, 0.6545212788967332, 0.6542647915513105, 0.6534280984680694, 0.654987029635013, 0.6549165541943521, 0.6548696907299741, 0.6541031125992064, 0.654451884920635, 0.6539167402062569, 0.6542887640157807, 0.6543814094799741, 0.6549626966823551, 0.65447369445598, 0.6542891245039683, 0.654801017730251, 0.6551022056109266, 0.6550553421465486, 0.6548696907299741, 0.6550091996585456, 0.6541496155753969, 0.6548460787536914, 0.6549387242178848, 0.6544286334325398, 0.6544976669204503, 0.6556602413252123, 0.6550553421465486, 0.6537539797895903, 0.6550560631229236, 0.6545430884320782, 0.6547298213132152, 0.6546142848491141, 0.6540795006229236, 0.6548925817298819, 0.6543123759920635, 0.6542647915513105, 0.6540798611111112, 0.6542647915513105, 0.6547995757775009, 0.6546607878253046, 0.6551941300987449, 0.6544286334325398, 0.6550557026347361, 0.6545663399201734, 0.6548239087301587, 0.6550553421465486, 0.6549623361941675, 0.6548228272655962, 0.6545212788967332, 0.6548936631944444, 0.6542415400632152, 0.6548231877537837, 0.6546375363372093, 0.6548929422180694, 0.6542186490633075, 0.6550091996585456, 0.6539864946705427, 0.6547301818014027, 0.6555203719084532, 0.6549623361941675, 0.6547995757775009, 0.6552642450512182, 0.6553797815153193, 0.6546368153608343, 0.6549623361941675, 0.6550553421465486, 0.6551014846345515], "end": "2016-01-30 05:55:17.413000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0], "moving_var_accuracy_valid": [0.013038418380926715, 0.026663394904992137, 0.03777964434021674, 0.04695258020766608, 0.05375460011906935, 0.0585563895120224, 0.06139646611426059, 0.0631042781833005, 0.06376295735964296, 0.06348858531534661, 0.06229172756105473, 0.06052521404511944, 0.0582529726434748, 0.05565501657939652, 0.052794308479242734, 0.049688364766299564, 0.04665181333104423, 0.043667335791664014, 0.04073845663451159, 0.037808944556292766, 0.03508762639384103, 0.032461252517059065, 0.02993754996563089, 0.027554177145423468, 0.025309728361098374, 0.023224522851481123, 0.021274242671185814, 0.019430633159093956, 0.017773033432745347, 0.016225749516892613, 0.014815321934554315, 0.013502680554010731, 0.01228071162374144, 0.01117130047334609, 0.010159909869717279, 0.009227433467116455, 0.00837476799150083, 0.007593596456199707, 0.006886061622484087, 0.006245829222516153, 0.005652987865087896, 0.00511336843445836, 0.004626215143659293, 0.00418201300090172, 0.0037799092449157467, 0.0034197887118854863, 0.0030904438425875236, 0.0027860971809117317, 0.0025159497799782686, 0.002269577979574748, 0.0020476239096874345, 0.00184853787625487, 0.0016668455459841619, 0.0015038377194565298, 0.0013562153773344638, 0.0012222921919751327, 0.0011028126910192521, 0.0009938763870684579, 0.0008949987783657111, 0.000806170769079766, 0.0007264941827981802, 0.00065386987983498, 0.0005887147203651136, 0.0005298674873509436, 0.0004769304687014255, 0.00042928022085228956, 0.00038637365565508087, 0.00034854786855327774, 0.0003137614495356027, 0.00028262704677699057, 0.0002544332423777829, 0.00022901648634959297, 0.0002066515922236178, 0.00018647514781320256, 0.00016783339279359308, 0.00015132767637959088, 0.00013627609204768063, 0.0001228469920085435, 0.00011058521949931578, 9.97495202159817e-05, 8.983151861283677e-05, 8.086154837755884e-05, 7.319081809212202e-05, 6.596869327140556e-05, 5.9450359104946566e-05, 5.3511797205714396e-05, 4.8226756325390624e-05, 4.340786435694617e-05, 3.907553850652232e-05, 3.519919089814135e-05, 3.1854350267982774e-05, 2.9176355338410348e-05, 2.6262809776870237e-05, 2.3695943640157827e-05, 2.134681135521098e-05, 1.925546096798294e-05, 1.7493753786193867e-05, 1.5796280984993322e-05, 1.4632023288026157e-05, 1.323001028068354e-05, 1.1909097729974218e-05, 1.0722464706975806e-05, 9.652227880044045e-06, 8.699619814506397e-06, 7.860047366720634e-06, 7.075091688635324e-06, 6.4404801143272555e-06, 5.840034236019212e-06, 5.489903329013809e-06, 4.959435527664949e-06, 4.587030093036032e-06, 4.171890082267734e-06, 3.8644220762893344e-06, 3.5285487507054685e-06, 3.7003748038653216e-06, 3.3472250720634544e-06, 3.045695111777934e-06, 2.7988697434813528e-06, 2.7010821868365914e-06, 2.4394409572239925e-06, 2.205555116011938e-06, 2.000324641144362e-06, 1.8332539777857396e-06, 1.6524734453639603e-06, 1.507048488669473e-06, 1.484956961230247e-06, 1.4300797323435418e-06, 1.3835213129023255e-06, 1.5800081961528488e-06, 1.4232118812974382e-06, 1.373630138282361e-06, 1.5209048973021955e-06, 1.4829604581148995e-06, 1.5404559001088387e-06, 1.4330262176897374e-06, 1.3311692405431885e-06, 1.2043409456590158e-06, 1.0849092807794535e-06, 1.1767454623077197e-06, 1.060411230997355e-06, 1.4566799049652787e-06, 1.3177968791387661e-06], "accuracy_test": 0.639453125, "start": "2016-01-29 15:43:22.235000", "learning_rate_per_epoch": [0.00018531596288084984, 0.00017143921286333352, 0.0001586015714565292, 0.0001467252295697108, 0.0001357382134301588, 0.00012557391892187297, 0.00011617074778769165, 0.00010747170017566532, 9.942404722096398e-05, 9.197901817969978e-05, 8.509148028679192e-05, 7.871969137340784e-05, 7.282503793248907e-05, 6.737178046023473e-05, 6.232687155716121e-05, 5.765973764937371e-05, 5.3342086175689474e-05, 4.9347745516570285e-05, 4.5652508561033756e-05, 4.223397627356462e-05, 3.9071430364856496e-05, 3.6145698686596006e-05, 3.3439049730077386e-05, 3.093507984885946e-05, 2.861861139535904e-05, 2.6475603590370156e-05, 2.449306884955149e-05, 2.265898910991382e-05, 2.096224852721207e-05, 1.9392562535358593e-05, 1.7940416000783443e-05, 1.6597008652752265e-05, 1.5354198694694787e-05, 1.4204451872501522e-05, 1.3140799637767486e-05, 1.2156795492046513e-05, 1.1246474969084375e-05, 1.0404321074020118e-05, 9.62522881309269e-06, 8.904476089810487e-06, 8.237694601120893e-06, 7.62084300731658e-06, 7.05018237567856e-06, 6.522253443108639e-06, 6.03385660724598e-06, 5.582031917583663e-06, 5.164040430827299e-06, 4.7773487494851e-06, 4.419613105710596e-06, 4.088665264134761e-06, 3.7824995615665102e-06, 3.4992599466932006e-06, 3.2372297482652357e-06, 2.9948207611596445e-06, 2.7705639240593882e-06, 2.563099769758992e-06, 2.371170694459579e-06, 2.1936136818112573e-06, 2.0293523448344786e-06, 1.8773912415781524e-06, 1.7368091675962205e-06, 1.606754153726797e-06, 1.4864378954371205e-06, 1.3751310916632065e-06, 1.272159124710015e-06, 1.1768978538384545e-06, 1.0887698635997367e-06, 1.0072410532302456e-06, 9.318172260464053e-07, 8.620412472737371e-07, 7.974902018759167e-07, 7.377728934443439e-07, 6.825272862442944e-07, 6.314185725386778e-07, 5.841369556947029e-07, 5.403958311944734e-07, 4.999301381758414e-07, 4.624945688647131e-07, 4.278622327547055e-07, 3.958232355216751e-07, 3.6618337162508396e-07, 3.387629874396225e-07, 3.1339587280854175e-07, 2.899282947055326e-07, 2.682180024748959e-07, 2.481334036019689e-07, 2.2955278211611585e-07, 2.123635027828641e-07, 1.9646138582629646e-07, 1.8175003901887976e-07, 1.681403034581308e-07, 1.5554968513242784e-07, 1.4390187175195024e-07, 1.3312626379047288e-07, 1.231575623705794e-07, 1.1393532872716605e-07, 1.0540367156863795e-07, 9.751087759468646e-08, 9.020910596291287e-08, 8.345410407173404e-08, 7.720493044871546e-08, 7.142370606061377e-08, 6.607538693970127e-08, 6.11275581263726e-08, 5.6550231164465004e-08, 5.231565936014704e-08, 4.839817790980305e-08, 4.4774044027917625e-08, 4.142129128581473e-08, 3.8319598161251633e-08, 3.5450167246153796e-08, 3.2795600901636135e-08, 3.033981244016104e-08, 2.8067919544128017e-08, 2.5966148342604356e-08, 2.4021762357051557e-08, 2.22229754598402e-08, 2.0558884372690045e-08, 1.901940294146698e-08, 1.759519996369363e-08, 1.6277644121487356e-08, 1.505874891449821e-08, 1.3931125586452708e-08, 1.2887940492589678e-08, 1.1922870690739273e-08, 1.103006752600777e-08, 1.0204118439105514e-08, 9.440017656459077e-09, 8.733134215788141e-09, 8.079183544396074e-09, 7.474201701995753e-09, 6.914521399892237e-09, 6.3967511287899015e-09, 5.9177520661535254e-09, 5.474621200818319e-09, 5.06467268124311e-09, 4.685421828298786e-09, 4.3345700362351636e-09, 4.009990561826271e-09, 3.7097160898724724e-09, 3.431926742791802e-09, 3.1749385343005088e-09], "accuracy_train_first": 0.37863245922157623, "accuracy_train_last": 0.6551014846345515, "batch_size_eval": 1024, "accuracy_train_std": [0.014779541050866794, 0.013464454458832032, 0.013822266726726372, 0.016076051877698967, 0.01515673428644779, 0.015562289590336434, 0.01703563729919645, 0.015871106866345595, 0.016474635619585017, 0.015554396136171579, 0.015358908735934523, 0.013784387192712843, 0.01387217852614197, 0.013639267839200277, 0.012027848129230667, 0.013281392905727446, 0.011910637699432972, 0.011599671434701898, 0.011286442541524194, 0.012212578372582607, 0.012325799730234318, 0.01287487149961303, 0.013642481800791292, 0.013108603754526656, 0.013754654031478549, 0.013604924243209561, 0.014128355300398571, 0.013833778896660893, 0.013231242492806768, 0.014040309037452662, 0.013908755494406913, 0.013997939312409842, 0.014264600222671555, 0.014287396886733908, 0.014719147129645673, 0.01487104217159184, 0.014646380808653787, 0.01407611159557759, 0.01485716330174892, 0.014200494830094211, 0.013924843745034747, 0.01428467700380703, 0.013783836067811691, 0.014319799818827177, 0.014155689118708995, 0.01382441705367987, 0.013999506223074666, 0.013586216083713142, 0.013860173830842783, 0.013608020495311745, 0.013188676864454239, 0.01358592794673235, 0.013678271750661686, 0.013737425352736607, 0.013459490325258797, 0.01357629434863271, 0.013681088777993967, 0.012990517399364899, 0.013104090283513111, 0.013809672363378657, 0.014309632515659133, 0.013720375119385257, 0.013729774152858785, 0.01355669180957574, 0.013223144035226559, 0.013254014956034899, 0.013988932531141286, 0.0138820709309886, 0.013238454783574431, 0.014315591072741904, 0.013848917930062838, 0.014310941170809761, 0.01378146720907815, 0.014412763737473869, 0.01364461657784926, 0.013782195214544036, 0.013985039108412662, 0.013822729504443437, 0.013942814544238945, 0.01399533968274564, 0.013877716292735485, 0.013508381811541063, 0.013959323280186856, 0.014036856582255812, 0.013646351480279999, 0.013808750354462215, 0.01435677396345857, 0.013082207460162833, 0.013749856269095122, 0.013416583234099784, 0.014001652520346079, 0.013342539148511308, 0.013302650683524164, 0.013633868081649621, 0.013648935494172293, 0.014042600584207336, 0.01364895852668071, 0.013793741309604824, 0.013451434116092495, 0.014149303717956925, 0.013338030367718978, 0.013021450994034648, 0.014157892156510839, 0.013931221357761968, 0.013955914122168815, 0.013907716178473766, 0.013514200574120493, 0.014324429730742612, 0.013693516453902575, 0.0138880930562279, 0.013682387972305863, 0.013825772697098972, 0.01397628988127737, 0.01349180286689839, 0.013696618944867869, 0.01369743959609004, 0.013713895530493186, 0.01388560122045426, 0.014056808496744854, 0.013603787563050135, 0.013509289724317832, 0.01313243376626523, 0.013612394129248454, 0.013898720320283916, 0.013941393112202086, 0.013999754842385428, 0.013537137840587282, 0.014059614016878877, 0.014063289428684572, 0.013476558837472997, 0.013928878661999256, 0.013520243190873464, 0.013496763005101529, 0.012891245414326733, 0.0138740036496274, 0.014337109578663397, 0.013905988276051618, 0.013437533298038505, 0.013589934444326697, 0.013494012965562829, 0.014152838211061272, 0.013545098209782613], "accuracy_test_std": 0.011651827139277073, "error_valid": [0.6193803534450302, 0.5546595561935241, 0.5298792827560241, 0.5027370223079819, 0.4867252211972892, 0.47212678840361444, 0.4639377823795181, 0.4484039674322289, 0.43589102503765065, 0.42595185429216864, 0.4210381565323795, 0.41369334760918675, 0.40915615587349397, 0.40424245811370485, 0.40131277061370485, 0.4019334172628012, 0.3952695547816265, 0.39048822242093373, 0.3870805487575302, 0.38807770143072284, 0.3810579230986446, 0.37969455948795183, 0.37921657332454817, 0.37749729386295183, 0.3762662956513554, 0.3737028190888554, 0.37273655167545183, 0.37445583113704817, 0.3686773461031627, 0.36880971150225905, 0.36575795368975905, 0.3661344597138554, 0.36736545792545183, 0.3650358269013554, 0.36343861775225905, 0.36382541886295183, 0.36333713761295183, 0.36343861775225905, 0.36195318382906627, 0.36036626976656627, 0.3624517601656627, 0.36246205525225905, 0.36127223738704817, 0.3617193382906627, 0.36122076195406627, 0.35916615681475905, 0.36000005882906627, 0.36343861775225905, 0.36024419945406627, 0.3613531273531627, 0.36075307087725905, 0.3595220726656627, 0.3607427757906627, 0.35968532332454817, 0.35989857868975905, 0.36053981551204817, 0.35892201618975905, 0.3600309440888554, 0.3611295769013554, 0.36053981551204817, 0.3597662132906627, 0.36214731974774095, 0.36101780167545183, 0.36298122176204817, 0.3617708137236446, 0.36175022355045183, 0.36188258894954817, 0.35931911238704817, 0.36115016707454817, 0.36029567488704817, 0.36089573136295183, 0.36113987198795183, 0.35918674698795183, 0.3590543815888554, 0.3614046027861446, 0.3594205925263554, 0.36005153426204817, 0.36239146037274095, 0.3605501105986446, 0.35943088761295183, 0.36005153426204817, 0.36115016707454817, 0.35865728539156627, 0.35955295792545183, 0.35955295792545183, 0.36066188582454817, 0.35956325301204817, 0.36053981551204817, 0.36066188582454817, 0.35979709855045183, 0.3589323112763554, 0.3578130882906627, 0.36016330948795183, 0.36078395613704817, 0.36052952042545183, 0.3607942512236446, 0.35882053605045183, 0.3607942512236446, 0.3622590949736446, 0.36115016707454817, 0.36056040568524095, 0.3606412956513554, 0.36029567488704817, 0.36080454631024095, 0.36104868693524095, 0.36041774519954817, 0.36141489787274095, 0.3599088737763554, 0.36214731974774095, 0.36115016707454817, 0.3619134742093373, 0.36016330948795183, 0.35968532332454817, 0.35992946394954817, 0.35818959431475905, 0.35992946394954817, 0.36092661662274095, 0.3611810523343373, 0.36188258894954817, 0.36029567488704817, 0.36090602644954817, 0.36101780167545183, 0.36004123917545183, 0.36041774519954817, 0.3610383918486446, 0.3594205925263554, 0.36151637801204817, 0.35956325301204817, 0.3585661003388554, 0.36041774519954817, 0.3592985222138554, 0.35843373493975905, 0.3611604621611446, 0.36165903849774095, 0.36101780167545183, 0.36104868693524095, 0.36017360457454817, 0.3603059699736446, 0.3618928840361446, 0.3606721809111446, 0.3581998894013554, 0.36005153426204817], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.07488158953665423, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 128, "valid_ratio": 0.15, "learning_rate": 0.0002003159524305289, "optimization": "adam", "nb_data_augmentation": 3, "learning_rate_decay_method": "exp", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 1.5725562973590562e-05, "rotation_range": [0, 0], "momentum": 0.5146936137767739}, "accuracy_valid_max": 0.6421869117093373, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.6399484657379518, "accuracy_valid_std": [0.016289345589475233, 0.015261915188188637, 0.017270884339488576, 0.012323598311292507, 0.013825258328084216, 0.010000633272853157, 0.014804594888393603, 0.016490260281968365, 0.017081910363271724, 0.016495062786705674, 0.018427737549972362, 0.017336331875634027, 0.017830693063859163, 0.017814403853448953, 0.016735568587589098, 0.016635985632715327, 0.01465425398465788, 0.012541033669554058, 0.012321445283729263, 0.012593688687930203, 0.012622593506212663, 0.010694934943861771, 0.012129800156667843, 0.011739396923720875, 0.011277887883326879, 0.011612931812858277, 0.011296098262027454, 0.010562541779348697, 0.011060369715215457, 0.01088006553335088, 0.01167258071292767, 0.011647725779232215, 0.00974385957298069, 0.010157797752867674, 0.01022590300509585, 0.01004769753794204, 0.009489127932656292, 0.008944941510270898, 0.009945469827289503, 0.008709038259770024, 0.00816817169709676, 0.010205939222994355, 0.009338388837326761, 0.008060242804702376, 0.007571649217958557, 0.008143795314610564, 0.00814287879122834, 0.008591454799131914, 0.008120447392692161, 0.007236334901726859, 0.009211445422556043, 0.008491804035453793, 0.007622480436041271, 0.009703905636965797, 0.00833239119733025, 0.009840690857605621, 0.008163128103132846, 0.008469583199615268, 0.008318940522067119, 0.009292404375321497, 0.007687959094275864, 0.010823617658388891, 0.00850920082186731, 0.008629155246919054, 0.00996083447824404, 0.008670966206574166, 0.008555239969323017, 0.009921254300031377, 0.00940682000806957, 0.00949714652468745, 0.009351759979181213, 0.00869977073341314, 0.00826286435025983, 0.008683269324670667, 0.009758894549228738, 0.008403112684017153, 0.009862122285093748, 0.010879339464714895, 0.009672832142659484, 0.008322314423191713, 0.009467421055118058, 0.009071349381849282, 0.008127905634277508, 0.008427366795374275, 0.008595436815717722, 0.00966029325583736, 0.009078817451343684, 0.009595354343271103, 0.009061765422422022, 0.008545161212793386, 0.008712177424274789, 0.006045875905330188, 0.008678613319522605, 0.010072539058061753, 0.007969967356117711, 0.009884263458919329, 0.00809317867808595, 0.010469937587225916, 0.009243076861446278, 0.009058198553160325, 0.011136755174100517, 0.008015142244531691, 0.009073476676612516, 0.01105426645679508, 0.010315949355747813, 0.009427790472952094, 0.01087617059798501, 0.008170255905609443, 0.010812598237051364, 0.009201824025626278, 0.011314682804242318, 0.008719723899705762, 0.009874393384839315, 0.009239674564501196, 0.008421418849649507, 0.009594128739637284, 0.010636287757943338, 0.01155803871894959, 0.008271865530258023, 0.009319760297559215, 0.00958738980054942, 0.007731126304393461, 0.008898953552848646, 0.009465647835155867, 0.010319050774323664, 0.0071956400015055315, 0.009143134671398089, 0.010015304276328293, 0.008692692299042704, 0.00897432910770112, 0.008757180275297847, 0.00803276810148643, 0.0096841752035265, 0.01026228577819999, 0.008757741321293717, 0.011454882947324961, 0.009955297244670388, 0.009711656916633076, 0.008831489103748136, 0.009870877032825034, 0.007969407170001112, 0.00912111614875891], "accuracy_valid": [0.3806196465549699, 0.4453404438064759, 0.4701207172439759, 0.4972629776920181, 0.5132747788027108, 0.5278732115963856, 0.5360622176204819, 0.5515960325677711, 0.5641089749623494, 0.5740481457078314, 0.5789618434676205, 0.5863066523908133, 0.590843844126506, 0.5957575418862951, 0.5986872293862951, 0.5980665827371988, 0.6047304452183735, 0.6095117775790663, 0.6129194512424698, 0.6119222985692772, 0.6189420769013554, 0.6203054405120482, 0.6207834266754518, 0.6225027061370482, 0.6237337043486446, 0.6262971809111446, 0.6272634483245482, 0.6255441688629518, 0.6313226538968373, 0.631190288497741, 0.634242046310241, 0.6338655402861446, 0.6326345420745482, 0.6349641730986446, 0.636561382247741, 0.6361745811370482, 0.6366628623870482, 0.636561382247741, 0.6380468161709337, 0.6396337302334337, 0.6375482398343373, 0.637537944747741, 0.6387277626129518, 0.6382806617093373, 0.6387792380459337, 0.640833843185241, 0.6399999411709337, 0.636561382247741, 0.6397558005459337, 0.6386468726468373, 0.639246929122741, 0.6404779273343373, 0.6392572242093373, 0.6403146766754518, 0.640101421310241, 0.6394601844879518, 0.641077983810241, 0.6399690559111446, 0.6388704230986446, 0.6394601844879518, 0.6402337867093373, 0.637852680252259, 0.6389821983245482, 0.6370187782379518, 0.6382291862763554, 0.6382497764495482, 0.6381174110504518, 0.6406808876129518, 0.6388498329254518, 0.6397043251129518, 0.6391042686370482, 0.6388601280120482, 0.6408132530120482, 0.6409456184111446, 0.6385953972138554, 0.6405794074736446, 0.6399484657379518, 0.637608539627259, 0.6394498894013554, 0.6405691123870482, 0.6399484657379518, 0.6388498329254518, 0.6413427146084337, 0.6404470420745482, 0.6404470420745482, 0.6393381141754518, 0.6404367469879518, 0.6394601844879518, 0.6393381141754518, 0.6402029014495482, 0.6410676887236446, 0.6421869117093373, 0.6398366905120482, 0.6392160438629518, 0.6394704795745482, 0.6392057487763554, 0.6411794639495482, 0.6392057487763554, 0.6377409050263554, 0.6388498329254518, 0.639439594314759, 0.6393587043486446, 0.6397043251129518, 0.639195453689759, 0.638951313064759, 0.6395822548004518, 0.638585102127259, 0.6400911262236446, 0.637852680252259, 0.6388498329254518, 0.6380865257906627, 0.6398366905120482, 0.6403146766754518, 0.6400705360504518, 0.641810405685241, 0.6400705360504518, 0.639073383377259, 0.6388189476656627, 0.6381174110504518, 0.6397043251129518, 0.6390939735504518, 0.6389821983245482, 0.6399587608245482, 0.6395822548004518, 0.6389616081513554, 0.6405794074736446, 0.6384836219879518, 0.6404367469879518, 0.6414338996611446, 0.6395822548004518, 0.6407014777861446, 0.641566265060241, 0.6388395378388554, 0.638340961502259, 0.6389821983245482, 0.638951313064759, 0.6398263954254518, 0.6396940300263554, 0.6381071159638554, 0.6393278190888554, 0.6418001105986446, 0.6399484657379518], "seed": 511562789, "model": "residualv3", "loss_std": [0.3774268627166748, 0.08670037984848022, 0.085612952709198, 0.0866597443819046, 0.08693909645080566, 0.08675356209278107, 0.08631864190101624, 0.08711304515600204, 0.08586501330137253, 0.08735896646976471, 0.08733760565519333, 0.08814390748739243, 0.08663211017847061, 0.08980541676282883, 0.08729448914527893, 0.08756174892187119, 0.08815397322177887, 0.08857696503400803, 0.08967097103595734, 0.08748391270637512, 0.08926501125097275, 0.08807974308729172, 0.08747607469558716, 0.08846073597669601, 0.08912477642297745, 0.09064267575740814, 0.08865408599376678, 0.08873456716537476, 0.08741427958011627, 0.08841440081596375, 0.08728395402431488, 0.08724286407232285, 0.08789228647947311, 0.08862681686878204, 0.08525220304727554, 0.08773212134838104, 0.09114003926515579, 0.08931463956832886, 0.0887375995516777, 0.08895935863256454, 0.08708356320858002, 0.08668503165245056, 0.08815734833478928, 0.08804281800985336, 0.08998581022024155, 0.08842844516038895, 0.08905075490474701, 0.08798141777515411, 0.08636870235204697, 0.08872678130865097, 0.08680190145969391, 0.08637481927871704, 0.0860486552119255, 0.08749799430370331, 0.087705597281456, 0.08951564878225327, 0.08937107026576996, 0.08651041984558105, 0.08650736510753632, 0.0884658694267273, 0.08838127553462982, 0.08690623193979263, 0.0867205336689949, 0.08709713071584702, 0.08828870952129364, 0.08723834156990051, 0.08736015111207962, 0.08717796206474304, 0.08682917058467865, 0.08726058900356293, 0.08874912559986115, 0.08784712851047516, 0.08805927634239197, 0.08750608563423157, 0.08849479258060455, 0.08736342936754227, 0.08836232870817184, 0.08891092985868454, 0.08732432872056961, 0.08897322416305542, 0.08836383372545242, 0.08688553422689438, 0.08660031855106354, 0.0886555165052414, 0.08871497958898544, 0.08982128649950027, 0.08640822023153305, 0.08646973222494125, 0.08648371696472168, 0.08703262358903885, 0.08847710490226746, 0.08600497245788574, 0.08825717121362686, 0.08840876072645187, 0.0875762552022934, 0.08866038918495178, 0.08630843460559845, 0.08946011960506439, 0.08667942136526108, 0.0897255539894104, 0.08672143518924713, 0.08816573023796082, 0.086887426674366, 0.08854459226131439, 0.0868029072880745, 0.08723726123571396, 0.08934768289327621, 0.08775725960731506, 0.08844499289989471, 0.08820775151252747, 0.08834303915500641, 0.08718769252300262, 0.08639711141586304, 0.08672154694795609, 0.08813270181417465, 0.0866590067744255, 0.08906394243240356, 0.0887497216463089, 0.0881456658244133, 0.08736768364906311, 0.08850947767496109, 0.087797150015831, 0.08868881314992905, 0.0866796225309372, 0.08871378749608994, 0.08858241140842438, 0.08786195516586304, 0.08843810111284256, 0.08707022666931152, 0.08698675036430359, 0.0881638303399086, 0.08842600136995316, 0.0882512554526329, 0.08599653840065002, 0.08851876109838486, 0.08854471892118454, 0.08921326696872711, 0.08885172754526138, 0.08661910146474838, 0.08816942572593689, 0.08614290505647659, 0.08698450028896332]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:23 2016", "state": "available"}], "summary": "c0882aedaee85ccdbb3af779f3dfa5a9"}