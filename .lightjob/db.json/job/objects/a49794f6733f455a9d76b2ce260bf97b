{"content": {"hp_model": {"f1": 195, "f2": 60, "f3": 228, "nonlin": "leaky_rectify", "ds2": 1707, "ds1": 2870, "do2": 0.4283480807763398, "do3": 0.7039040895262265, "do1": 0.9189819747001806, "do4": 0.3864547152685135, "do5": 0.4500320090331267}, "accuracy_valid_std": [0.05623772197393481, 0.06117473462067372, 0.058876512684523534, 0.06571734173539431, 0.06974146498813781, 0.06389718479069595, 0.0641028423248948, 0.06442477115389256, 0.06525899981008763, 0.06169293642205545, 0.06258839811151065, 0.06288688786621457, 0.05861451078925696, 0.060079532364271146, 0.06475143460413178, 0.05857615986546039, 0.06034788209238783, 0.06059621012998279, 0.06209806657050501, 0.0618081756530823, 0.054193106209283755, 0.058909218832412825, 0.05907640270113858, 0.05417631964848241, 0.05644632533495742, 0.057931085288205134, 0.0559582682744821, 0.0559611366462634, 0.051553642800127926, 0.05544501247437883, 0.05579548627547947, 0.05955807305894807, 0.05521292684920582, 0.05592096605265163, 0.054798554118978104, 0.05307783811335337, 0.05656121799220669, 0.05090436394788332, 0.057377054443056526, 0.057088495517876085, 0.055103638392535015, 0.05385340040354436, 0.05609483086412616, 0.05444886945062329, 0.05509651742843296, 0.055891936001773085, 0.05450223417796157, 0.052904171684128594, 0.05595635594494166, 0.052461031881217104, 0.05513437744963401, 0.05398702859248591, 0.05180866650980455, 0.05441348240001533, 0.05213296178161902, 0.054389878242132074, 0.053995286798205665, 0.05070954187391762, 0.04831670971692735, 0.0516721662131422, 0.0507917741873307, 0.052040512201919645, 0.05209874032474661, 0.053119151541099974, 0.04784749089592295, 0.05006249689634332, 0.04709875644129999, 0.04812030628083608, 0.04842216378340773, 0.051462578436437346, 0.05382656868925226, 0.05208264847237651, 0.053156406827602025, 0.053443170190460965, 0.05514311068617068, 0.050883338063845034, 0.050050026565946544, 0.05222832109566804, 0.052796860196065956, 0.05043623391620581, 0.04765439327307455, 0.05411175812783884, 0.04660886261355089, 0.047265831470563346, 0.047405990321916454, 0.049352441107564496, 0.04921021527824305, 0.04895732060959181, 0.04861844830510805, 0.045823946725745884, 0.04850055228298636, 0.04586168384829995, 0.04942682856824082, 0.05134287647685008, 0.0467395451381086, 0.04667348538205723, 0.04834143462432338, 0.04632830227219395, 0.05007033380006593, 0.04993979626203924, 0.04589200665416742, 0.04659546799726807, 0.047950257523426645, 0.047451866248374774, 0.0488117513318654, 0.04698312067314031, 0.046141602720794775, 0.04839305783390268, 0.04360713723587619, 0.048669044796053024, 0.04745524878152576, 0.05067787853130862, 0.050139388015859, 0.052317029945209696, 0.04828347722815009, 0.04971752633385037, 0.04877958735648854, 0.04828015272115898, 0.049515873972285324, 0.049019575111016386, 0.04482526777387217, 0.04517482954432672, 0.0466742496128548, 0.044739643108744676, 0.045472315173505945, 0.047042301037218875, 0.044117835897515174, 0.04677044278048874, 0.046492008388526636, 0.0438420815145809, 0.04696375707107233, 0.04599565261859861, 0.05007460795786454, 0.04545819333615499, 0.04670213547829357, 0.04774786487342423, 0.045627366915089534, 0.04444768669345726, 0.04539262624062377, 0.04307217116456653, 0.04369049115921239, 0.04573317258945407, 0.04587995768230014, 0.04757723467359371, 0.04686986329514648, 0.04448378487750644, 0.04711503636226121, 0.0473638355965758, 0.04787990853001928, 0.04557026277063351, 0.0485545775800933, 0.04897990156077646, 0.04653686912562904, 0.04718879342364053, 0.04615706106245288, 0.046934126697808776, 0.04758623046379926, 0.04626319733701426, 0.04745449712832848, 0.0455150462418735, 0.04961949840587419, 0.049466143668932705, 0.04540441173752257, 0.047333325303149584, 0.04598285509600653, 0.04740937612819315, 0.0448288484958313, 0.045110827392407835, 0.048654751121425456, 0.04740373298338153, 0.04655717643055754, 0.04571874122981073, 0.0478292230157801, 0.048577712885862295, 0.04389208895906131, 0.0438579437069674, 0.04548447213121368, 0.04736760093660326, 0.045524449529538435, 0.04336351774660603, 0.045320274815318175, 0.04801827500692785, 0.048570002341719265, 0.045572610926152654], "moving_avg_accuracy_train": [0.029823983433734935, 0.06489693147590361, 0.09769093561746987, 0.13224366057981926, 0.16510598503388552, 0.19783060866905117, 0.23042188891660387, 0.2610976970128953, 0.2912402804742564, 0.31944870724610785, 0.34886019947330427, 0.3768318602488654, 0.40614556804928004, 0.43186666636483395, 0.4576206059030493, 0.48301112437900945, 0.5058555315194218, 0.5276697336385638, 0.5513993717204906, 0.5734337568375981, 0.5937094511839588, 0.6130282651017074, 0.63099642880238, 0.6460853213137083, 0.660521875778723, 0.6742536678695255, 0.6875700179500428, 0.7007948497393759, 0.7137937721449563, 0.7260387360449185, 0.7373721741874146, 0.7463203859855407, 0.7556774287122878, 0.7639928748470831, 0.7721380150732182, 0.7806711073911976, 0.7878731984593067, 0.7952328100892796, 0.8032001164297492, 0.8105048224072562, 0.8172790766123137, 0.8241548022342148, 0.8303900185469378, 0.8350275038910392, 0.8407825660019352, 0.8471928259680067, 0.8530820712326519, 0.8593354228443265, 0.8645563421562794, 0.8692834074888443, 0.8731683197520079, 0.8776083590117468, 0.8824138822973191, 0.8865670723808402, 0.8909049999319127, 0.8939643493363117, 0.8968189497942468, 0.899449272435304, 0.9034402450411712, 0.9072580240009095, 0.9109740514200956, 0.9148032276033872, 0.9178000321020846, 0.9210830936509123, 0.9243555260026886, 0.9261806096976004, 0.9283761782459127, 0.9312275664454178, 0.9336432134153339, 0.9360502587906679, 0.9379389264356974, 0.9391704479487542, 0.9411989039068908, 0.942709190474033, 0.9443014114868706, 0.9459085444345691, 0.947891485171835, 0.9496502470462178, 0.9506636673717165, 0.9522652223212918, 0.9540666556614519, 0.9554432167519332, 0.9565385788116796, 0.9575455831293067, 0.9584095300874603, 0.9599118564461842, 0.9612804223075898, 0.9615591007093609, 0.9628900129275815, 0.9637960417553053, 0.9647291258327868, 0.966150132677219, 0.9672643174516657, 0.9675634881161377, 0.9686751739430781, 0.9693674268800956, 0.9701410569330499, 0.9709996922036004, 0.9710806341278186, 0.9714993967692536, 0.9726034104055813, 0.9724063223770714, 0.9732808068562318, 0.9744419957489219, 0.9750470243366803, 0.9753891780777111, 0.9758453656916267, 0.9761876928272833, 0.9766463896590128, 0.9769745029521477, 0.9772909833798246, 0.9780840988972638, 0.9787061295195856, 0.9790753509049764, 0.97961002213978, 0.980074754112549, 0.9804177116832218, 0.9807499051233334, 0.9810253475929277, 0.9812803053035144, 0.9819592213093075, 0.9817042818590995, 0.9819948852996956, 0.9823646738781597, 0.9827351342011871, 0.9828049942750443, 0.9835997019258531, 0.9838843100465209, 0.9840133865719892, 0.9842754515292481, 0.984370120231745, 0.9842364779374861, 0.9845350628244605, 0.9850696966022554, 0.984946104201066, 0.9847548635098751, 0.9853498779118997, 0.9857630264158904, 0.9859983766357472, 0.9862431361107267, 0.9863222298791721, 0.9866146115599296, 0.9865436059762258, 0.9865126452280009, 0.9869130561570081, 0.9870404628907048, 0.987065708770309, 0.9871896160559287, 0.987350549028649, 0.987728351806507, 0.9878259985535671, 0.9879115274632707, 0.9880520388735701, 0.9883361610404299, 0.988170654876146, 0.9883746737258807, 0.9885276995761842, 0.9885712963354333, 0.9888646749850225, 0.9887992729985684, 0.9890580881685911, 0.9892063079661898, 0.9888926048804142, 0.9892926892718908, 0.9893845046820511, 0.9895542055692677, 0.9897116426930638, 0.9896391983032754, 0.9899410917259599, 0.9901398477642073, 0.9904057952167021, 0.9907463339179234, 0.9907210228152877, 0.9908653173711084, 0.9907575130436361, 0.990613425895899, 0.9907402421918512, 0.9909155590871239, 0.9908380280278092, 0.9908129601647873, 0.9908798192687904, 0.9906787914081764, 0.9904107993155515, 0.9905602314321892], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 1234, "moving_var_accuracy_train": [0.008005229890701264, 0.018275712060949106, 0.026127161223587783, 0.03425946232214287, 0.04055290740664748, 0.04613572559455192, 0.0510818769686674, 0.054442736093045146, 0.057175640523266694, 0.05861951453942619, 0.060542885960757506, 0.06153032162356934, 0.06311093064628523, 0.06275401166868222, 0.062447999117456486, 0.062005305062013495, 0.06050157699418444, 0.05873415402161908, 0.05792860013095023, 0.05650536726525631, 0.05455476456977406, 0.052458237253494064, 0.050118107689112186, 0.04715536901516673, 0.044315559057042084, 0.041581062177563084, 0.03901888257500882, 0.03669105990021333, 0.03454270176354872, 0.03243788385539625, 0.03035011685102469, 0.028035739615379314, 0.026020153891152926, 0.024040458301823775, 0.022233502255372028, 0.020665475010399253, 0.01906575855113937, 0.017646657646121732, 0.016453293614415547, 0.015288192817734431, 0.014172388216273644, 0.013180629820095293, 0.012212468140283799, 0.011184777759106217, 0.010364386642298041, 0.009697770873561813, 0.009040142674289936, 0.008488068064274039, 0.00788458324400394, 0.007297231239528572, 0.006703341005208037, 0.006210432442339445, 0.005797226684539098, 0.005372744906913911, 0.005004828955167713, 0.004588582628654717, 0.004203063059759234, 0.003845024128547836, 0.0036038724767600934, 0.0033746641547528684, 0.003161477477298872, 0.002977293041753177, 0.0027603912724085775, 0.002581358583368237, 0.0024196020465039794, 0.0022076202162944797, 0.002030242885918073, 0.0019003923293047593, 0.001762871248923665, 0.0016387289309815532, 0.001506959627143833, 0.0013699134715635445, 0.00126995382657409, 0.0011634871335506939, 0.0010699549299791205, 0.0009862053237854104, 0.0009229732771144501, 0.0008585151393800464, 0.0007819068122472469, 0.0007268009353311054, 0.0006833273005093541, 0.0006320488543808628, 0.0005796423313201643, 0.0005308046174496269, 0.00048444179482319105, 0.0004563104757339201, 0.00042753618081357376, 0.0003854815175967398, 0.00036287531183054416, 0.0003339757747774885, 0.00030841401036058546, 0.0002957459533918342, 0.00027734402745713184, 0.0002504151524897442, 0.00023649624564115213, 0.000217159548236321, 0.00020083012454219558, 0.00018738240283847491, 0.00016870312691049286, 0.00015341107356819816, 0.0001490395811941551, 0.00013448521629357699, 0.00012791920260285007, 0.00012726251914312633, 0.00011783080355685817, 0.00010710134584368445, 9.826417551112639e-05, 8.949244877027546e-05, 8.243682894419604e-05, 7.516207104796265e-05, 6.854730269308873e-05, 6.735386243980603e-05, 6.410077505177998e-05, 5.891761742947135e-05, 5.5598715650461524e-05, 5.198262634403914e-05, 4.7842942767171446e-05, 4.405182082533267e-05, 4.032945572930567e-05, 3.688154106406383e-05, 3.734172944395649e-05, 3.4192503609011387e-05, 3.1533306485286726e-05, 2.96106681716211e-05, 2.78847690128971e-05, 2.5140215980881386e-05, 2.8310236635079975e-05, 2.6208229012721995e-05, 2.3737352856292748e-05, 2.1981719947071518e-05, 1.9864207421456355e-05, 1.8038529044643616e-05, 1.70370525527442e-05, 1.7905846784703106e-05, 1.6252737840918646e-05, 1.4956621074531487e-05, 1.6647338214627906e-05, 1.6518829570313312e-05, 1.536545414716154e-05, 1.4368073537775531e-05, 1.2987568601860036e-05, 1.245819516685721e-05, 1.1257751786425545e-05, 1.0140603719158856e-05, 1.0569503555858219e-05, 9.65864548239394e-06, 8.698517124087463e-06, 7.966842550545506e-06, 7.4032530908685414e-06, 7.947542232396584e-06, 7.238601994059773e-06, 6.580578544209454e-06, 6.100211797607551e-06, 6.216719269157076e-06, 5.841577955985237e-06, 5.632033379810719e-06, 5.2795822395795966e-06, 4.768730112374808e-06, 5.066496389450387e-06, 4.59834352899458e-06, 4.741376806199914e-06, 4.464961101181788e-06, 4.904151625289853e-06, 5.854344145490051e-06, 5.34478035682721e-06, 5.069487841243352e-06, 4.785617088661777e-06, 4.354289086301795e-06, 4.7391169256131176e-06, 4.620740897709965e-06, 4.795219235335875e-06, 5.359396775068011e-06, 4.8292229648109326e-06, 4.5336889378850735e-06, 4.184916001292329e-06, 3.953274356450213e-06, 3.7026882770766796e-06, 3.609043573281586e-06, 3.3022388023797162e-06, 2.977670501950096e-06, 2.720134709848089e-06, 2.8118310455505636e-06, 3.1770257963806213e-06, 3.060292834087736e-06], "duration": 259634.740493, "accuracy_train": [0.2982398343373494, 0.3805534638554217, 0.39283697289156627, 0.44321818524096385, 0.46086690512048195, 0.4923522213855422, 0.5237434111445783, 0.5371799698795181, 0.562523531626506, 0.5733245481927711, 0.6135636295180723, 0.6285768072289156, 0.6699689382530121, 0.6633565512048193, 0.6894060617469879, 0.7115257906626506, 0.7114551957831325, 0.7239975527108434, 0.7649661144578314, 0.7717432228915663, 0.7761907003012049, 0.7868975903614458, 0.7927099021084337, 0.7818853539156626, 0.7904508659638554, 0.797839796686747, 0.8074171686746988, 0.8198183358433735, 0.8307840737951807, 0.8362434111445783, 0.8393731174698795, 0.8268542921686747, 0.8398908132530121, 0.838831890060241, 0.8454442771084337, 0.8574689382530121, 0.8526920180722891, 0.8614693147590361, 0.8749058734939759, 0.8762471762048193, 0.8782473644578314, 0.8860363328313253, 0.8865069653614458, 0.8767648719879518, 0.892578125, 0.9048851656626506, 0.9060852786144579, 0.9156155873493976, 0.9115446159638554, 0.9118269954819277, 0.9081325301204819, 0.9175687123493976, 0.9256635918674698, 0.9239457831325302, 0.9299463478915663, 0.9214984939759037, 0.9225103539156626, 0.9231221762048193, 0.9393589984939759, 0.9416180346385542, 0.9444182981927711, 0.9492658132530121, 0.9447712725903614, 0.9506306475903614, 0.9538074171686747, 0.9426063629518072, 0.9481362951807228, 0.9568900602409639, 0.9553840361445783, 0.9577136671686747, 0.9549369352409639, 0.9502541415662651, 0.9594550075301205, 0.9563017695783133, 0.9586314006024096, 0.9603727409638554, 0.9657379518072289, 0.9654791039156626, 0.9597844503012049, 0.9666792168674698, 0.9702795557228916, 0.9678322665662651, 0.9663968373493976, 0.9666086219879518, 0.9661850527108434, 0.9734327936746988, 0.973597515060241, 0.9640672063253012, 0.9748682228915663, 0.9719503012048193, 0.9731268825301205, 0.9789391942771084, 0.9772919804216867, 0.9702560240963856, 0.9786803463855421, 0.975597703313253, 0.9771037274096386, 0.9787274096385542, 0.9718091114457831, 0.9752682605421686, 0.9825395331325302, 0.9706325301204819, 0.9811511671686747, 0.9848926957831325, 0.980492281626506, 0.9784685617469879, 0.9799510542168675, 0.9792686370481928, 0.9807746611445783, 0.9799275225903614, 0.9801393072289156, 0.9852221385542169, 0.9843044051204819, 0.982398343373494, 0.9844220632530121, 0.9842573418674698, 0.9835043298192772, 0.9837396460843374, 0.9835043298192772, 0.9835749246987951, 0.9880694653614458, 0.9794098268072289, 0.9846103162650602, 0.9856927710843374, 0.9860692771084337, 0.983433734939759, 0.9907520707831325, 0.9864457831325302, 0.9851750753012049, 0.9866340361445783, 0.9852221385542169, 0.9830336972891566, 0.9872223268072289, 0.9898814006024096, 0.9838337725903614, 0.9830336972891566, 0.9907050075301205, 0.9894813629518072, 0.9881165286144579, 0.9884459713855421, 0.9870340737951807, 0.989246046686747, 0.9859045557228916, 0.9862339984939759, 0.9905167545180723, 0.9881871234939759, 0.987292921686747, 0.988304781626506, 0.9887989457831325, 0.9911285768072289, 0.9887048192771084, 0.9886812876506024, 0.9893166415662651, 0.9908932605421686, 0.9866810993975904, 0.990210843373494, 0.9899049322289156, 0.9889636671686747, 0.9915050828313253, 0.9882106551204819, 0.9913874246987951, 0.9905402861445783, 0.9860692771084337, 0.9928934487951807, 0.990210843373494, 0.9910815135542169, 0.9911285768072289, 0.9889871987951807, 0.9926581325301205, 0.9919286521084337, 0.9927993222891566, 0.9938111822289156, 0.9904932228915663, 0.992163968373494, 0.9897872740963856, 0.9893166415662651, 0.9918815888554217, 0.9924934111445783, 0.9901402484939759, 0.9905873493975904, 0.9914815512048193, 0.9888695406626506, 0.9879988704819277, 0.9919051204819277], "end": "2016-01-20 05:25:04.953000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0], "accuracy_valid": [0.2918002136752137, 0.37606837606837606, 0.38782051282051283, 0.42788461538461536, 0.44751602564102566, 0.4754273504273504, 0.5078792735042735, 0.5188301282051282, 0.5412660256410257, 0.546607905982906, 0.5818643162393162, 0.6016292735042735, 0.6375534188034188, 0.6271367521367521, 0.6525106837606838, 0.6665331196581197, 0.6669337606837606, 0.672676282051282, 0.7083333333333334, 0.7146100427350427, 0.7198183760683761, 0.7259615384615384, 0.7297008547008547, 0.7196848290598291, 0.7235576923076923, 0.7331730769230769, 0.7398504273504274, 0.7458600427350427, 0.750801282051282, 0.7576121794871795, 0.7594818376068376, 0.7474626068376068, 0.7548076923076923, 0.7554754273504274, 0.7572115384615384, 0.7701655982905983, 0.7642895299145299, 0.7677617521367521, 0.782051282051282, 0.7783119658119658, 0.7729700854700855, 0.781784188034188, 0.781517094017094, 0.7796474358974359, 0.7932692307692307, 0.7948717948717948, 0.7986111111111112, 0.8075587606837606, 0.8015491452991453, 0.8024839743589743, 0.7932692307692307, 0.8004807692307693, 0.8071581196581197, 0.8068910256410257, 0.8094284188034188, 0.8028846153846154, 0.8026175213675214, 0.8022168803418803, 0.8108974358974359, 0.8107638888888888, 0.8143696581196581, 0.8154380341880342, 0.8108974358974359, 0.8159722222222222, 0.8207799145299145, 0.8052884615384616, 0.8157051282051282, 0.8169070512820513, 0.8175747863247863, 0.8199786324786325, 0.8190438034188035, 0.8139690170940171, 0.8218482905982906, 0.8141025641025641, 0.8153044871794872, 0.8166399572649573, 0.8215811965811965, 0.8238514957264957, 0.811965811965812, 0.8145032051282052, 0.8217147435897436, 0.8201121794871795, 0.8209134615384616, 0.8191773504273504, 0.8195779914529915, 0.8238514957264957, 0.8258547008547008, 0.8179754273504274, 0.8287927350427351, 0.8217147435897436, 0.8243856837606838, 0.8295940170940171, 0.8265224358974359, 0.8162393162393162, 0.8285256410256411, 0.828392094017094, 0.8255876068376068, 0.8307959401709402, 0.8222489316239316, 0.8223824786324786, 0.8318643162393162, 0.8190438034188035, 0.8310630341880342, 0.8353365384615384, 0.8259882478632479, 0.8251869658119658, 0.8269230769230769, 0.8241185897435898, 0.8318643162393162, 0.8294604700854701, 0.8257211538461539, 0.8322649572649573, 0.8297275641025641, 0.828926282051282, 0.8305288461538461, 0.8307959401709402, 0.8269230769230769, 0.8313301282051282, 0.8282585470085471, 0.8263888888888888, 0.8337339743589743, 0.8229166666666666, 0.828125, 0.8358707264957265, 0.8337339743589743, 0.8303952991452992, 0.8401442307692307, 0.8345352564102564, 0.8310630341880342, 0.8364049145299145, 0.8352029914529915, 0.828659188034188, 0.8314636752136753, 0.8425480769230769, 0.8323985042735043, 0.8271901709401709, 0.8377403846153846, 0.8401442307692307, 0.8340010683760684, 0.8353365384615384, 0.8322649572649573, 0.8384081196581197, 0.8317307692307693, 0.8322649572649573, 0.8349358974358975, 0.8321314102564102, 0.8319978632478633, 0.8301282051282052, 0.8336004273504274, 0.8338675213675214, 0.8302617521367521, 0.8341346153846154, 0.8310630341880342, 0.8364049145299145, 0.8279914529914529, 0.8331997863247863, 0.8356036324786325, 0.8341346153846154, 0.8356036324786325, 0.8287927350427351, 0.8342681623931624, 0.8348023504273504, 0.8295940170940171, 0.8393429487179487, 0.8330662393162394, 0.8354700854700855, 0.8353365384615384, 0.8305288461538461, 0.8346688034188035, 0.8350694444444444, 0.8369391025641025, 0.8341346153846154, 0.8321314102564102, 0.8350694444444444, 0.8360042735042735, 0.8302617521367521, 0.8340010683760684, 0.8358707264957265, 0.8338675213675214, 0.8354700854700855, 0.8373397435897436, 0.8282585470085471, 0.8234508547008547, 0.8318643162393162], "accuracy_test": 0.8245192307692307, "start": "2016-01-17 05:17:50.212000", "learning_rate_per_epoch": [0.0007239046972244978, 0.0005118779372423887, 0.00041794657590799034, 0.0003619523486122489, 0.00032374003785662353, 0.00029553286731243134, 0.0002736102615017444, 0.00025593896862119436, 0.00024130157544277608, 0.0002289187686983496, 0.00021826548618264496, 0.00020897328795399517, 0.00020077504450455308, 0.0001934716710820794, 0.0001869113912107423, 0.00018097617430612445, 0.00017557268438395113, 0.00017062597908079624, 0.00016607512952759862, 0.00016187001892831177, 0.0001579689560458064, 0.00015433700173161924, 0.000150944571942091, 0.00014776643365621567, 0.0001447809481760487, 0.0001419694017386064, 0.00013931552530266345, 0.0001368051307508722, 0.0001344257325399667, 0.0001321663148701191, 0.00013001712795812637, 0.00012796948431059718, 0.00012601564230863005, 0.00012414863158483058, 0.00012236222391948104, 0.00012065078772138804, 0.0001190092007163912, 0.00011743284994736314, 0.00011591752263484523, 0.0001144593843491748, 0.0001130549208028242, 0.00011170092329848558, 0.00011039443779736757, 0.00010913274309132248, 0.00010791334352688864, 0.00010673392534954473, 0.00010559235670370981, 0.00010448664397699758, 0.00010341496090404689, 0.00010237559035886079, 0.00010136693890672177, 0.00010038752225227654, 9.94359579635784e-05, 9.851095092017204e-05, 9.761129331309348e-05, 9.67358355410397e-05, 9.588352259015664e-05, 9.505334310233593e-05, 9.424436575500295e-05, 9.345569560537115e-05, 9.268650319427252e-05, 9.19359881663695e-05, 9.120342292590067e-05, 9.048808715306222e-05, 8.978932601166889e-05, 8.910651376936585e-05, 8.84390392457135e-05, 8.778634219197556e-05, 8.714788418728858e-05, 8.652316319057718e-05, 8.591168443672359e-05, 8.531298954039812e-05, 8.472664194414392e-05, 8.415221236646175e-05, 8.358931518159807e-05, 8.303756476379931e-05, 8.249659731518477e-05, 8.196607086574659e-05, 8.144564344547689e-05, 8.093500946415588e-05, 8.043385605560616e-05, 7.994189945748076e-05, 7.945886318339035e-05, 7.89844780229032e-05, 7.851848931750283e-05, 7.806064968463033e-05, 7.761073356959969e-05, 7.716850086580962e-05, 7.673374784644693e-05, 7.63062562327832e-05, 7.588583684992045e-05, 7.54722859710455e-05, 7.506542169721797e-05, 7.466506940545514e-05, 7.427105447277427e-05, 7.388321682810783e-05, 7.350138912443072e-05, 7.312541856663302e-05, 7.275515963556245e-05, 7.239047408802435e-05, 7.203120912890881e-05, 7.167724834289402e-05, 7.132845348678529e-05, 7.09847008693032e-05, 7.064586679916829e-05, 7.031184213701636e-05, 6.998251046752557e-05, 6.965776265133172e-05, 6.933749682502821e-05, 6.902160384925082e-05, 6.870999641250819e-05, 6.84025653754361e-05, 6.809923070250079e-05, 6.779989053029567e-05, 6.750446482328698e-05, 6.721286626998335e-05, 6.692501483485103e-05, 6.664083048235625e-05, 6.636023317696527e-05, 6.608315743505955e-05, 6.580952322110534e-05, 6.553925049956888e-05, 6.527228833874688e-05, 6.500856397906318e-05, 6.474800466094166e-05, 6.449055945267901e-05, 6.423615559469908e-05, 6.398474215529859e-05, 6.3736253650859e-05, 6.349064642563462e-05, 6.324784772004932e-05, 6.300782115431502e-05, 6.27705012448132e-05, 6.253584433579817e-05, 6.230379949556664e-05, 6.207431579241529e-05, 6.184735684655607e-05, 6.162286445032805e-05, 6.140079494798556e-05, 6.118111195974052e-05, 6.0963775467826054e-05, 6.074873454053886e-05, 6.0535952798090875e-05, 6.032539386069402e-05, 6.0117014072602615e-05, 5.991078069200739e-05, 5.970665370114148e-05, 5.95046003581956e-05, 5.93045842833817e-05, 5.9106572734890506e-05, 5.891052933293395e-05, 5.871642497368157e-05, 5.852422691532411e-05, 5.8333906054031104e-05, 5.814542601001449e-05, 5.7958761317422614e-05, 5.777388651040383e-05, 5.7590768847148865e-05, 5.740937922382727e-05, 5.72296921745874e-05, 5.70516858715564e-05, 5.6875327572925016e-05, 5.6700591812841594e-05, 5.65274604014121e-05, 5.6355904234806076e-05, 5.618590148515068e-05, 5.601743032457307e-05, 5.585046164924279e-05, 5.56849772692658e-05, 5.552095899474807e-05, 5.535837772185914e-05, 5.5197218898683786e-05, 5.5037457059370354e-05, 5.487907401402481e-05, 5.4722051572753116e-05, 5.456637154566124e-05, 5.441201210487634e-05, 5.425895142252557e-05, 5.410717858467251e-05, 5.395667176344432e-05, 5.380741276894696e-05, 5.3659387049265206e-05, 5.351257641450502e-05, 5.336696267477237e-05], "accuracy_train_last": 0.9919051204819277, "error_valid": [0.7081997863247863, 0.6239316239316239, 0.6121794871794872, 0.5721153846153846, 0.5524839743589743, 0.5245726495726496, 0.49212072649572647, 0.4811698717948718, 0.45873397435897434, 0.453392094017094, 0.4181356837606838, 0.39837072649572647, 0.36244658119658124, 0.37286324786324787, 0.3474893162393162, 0.3334668803418803, 0.33306623931623935, 0.32732371794871795, 0.29166666666666663, 0.2853899572649573, 0.28018162393162394, 0.27403846153846156, 0.27029914529914534, 0.2803151709401709, 0.2764423076923077, 0.26682692307692313, 0.2601495726495726, 0.2541399572649573, 0.24919871794871795, 0.24238782051282048, 0.24051816239316237, 0.2525373931623932, 0.2451923076923077, 0.2445245726495726, 0.24278846153846156, 0.22983440170940173, 0.23571047008547008, 0.23223824786324787, 0.21794871794871795, 0.22168803418803418, 0.2270299145299145, 0.21821581196581197, 0.21848290598290598, 0.2203525641025641, 0.20673076923076927, 0.20512820512820518, 0.20138888888888884, 0.19244123931623935, 0.19845085470085466, 0.19751602564102566, 0.20673076923076927, 0.19951923076923073, 0.19284188034188032, 0.19310897435897434, 0.19057158119658124, 0.19711538461538458, 0.1973824786324786, 0.19778311965811968, 0.1891025641025641, 0.18923611111111116, 0.1856303418803419, 0.18456196581196582, 0.1891025641025641, 0.1840277777777778, 0.1792200854700855, 0.19471153846153844, 0.1842948717948718, 0.18309294871794868, 0.1824252136752137, 0.18002136752136755, 0.18095619658119655, 0.18603098290598286, 0.17815170940170943, 0.1858974358974359, 0.18469551282051277, 0.1833600427350427, 0.17841880341880345, 0.17614850427350426, 0.18803418803418803, 0.18549679487179482, 0.1782852564102564, 0.17988782051282048, 0.17908653846153844, 0.1808226495726496, 0.18042200854700852, 0.17614850427350426, 0.1741452991452992, 0.1820245726495726, 0.1712072649572649, 0.1782852564102564, 0.17561431623931623, 0.17040598290598286, 0.1734775641025641, 0.18376068376068377, 0.17147435897435892, 0.17160790598290598, 0.1744123931623932, 0.16920405982905984, 0.17775106837606836, 0.1776175213675214, 0.16813568376068377, 0.18095619658119655, 0.16893696581196582, 0.16466346153846156, 0.17401175213675213, 0.17481303418803418, 0.17307692307692313, 0.17588141025641024, 0.16813568376068377, 0.17053952991452992, 0.17427884615384615, 0.1677350427350427, 0.1702724358974359, 0.17107371794871795, 0.16947115384615385, 0.16920405982905984, 0.17307692307692313, 0.1686698717948718, 0.17174145299145294, 0.17361111111111116, 0.16626602564102566, 0.17708333333333337, 0.171875, 0.16412927350427353, 0.16626602564102566, 0.1696047008547008, 0.15985576923076927, 0.1654647435897436, 0.16893696581196582, 0.1635950854700855, 0.16479700854700852, 0.17134081196581197, 0.16853632478632474, 0.15745192307692313, 0.16760149572649574, 0.1728098290598291, 0.16225961538461542, 0.15985576923076927, 0.16599893162393164, 0.16466346153846156, 0.1677350427350427, 0.16159188034188032, 0.16826923076923073, 0.1677350427350427, 0.16506410256410253, 0.16786858974358976, 0.1680021367521367, 0.16987179487179482, 0.1663995726495726, 0.1661324786324786, 0.16973824786324787, 0.16586538461538458, 0.16893696581196582, 0.1635950854700855, 0.17200854700854706, 0.1668002136752137, 0.16439636752136755, 0.16586538461538458, 0.16439636752136755, 0.1712072649572649, 0.16573183760683763, 0.1651976495726496, 0.17040598290598286, 0.16065705128205132, 0.16693376068376065, 0.1645299145299145, 0.16466346153846156, 0.16947115384615385, 0.16533119658119655, 0.16493055555555558, 0.16306089743589747, 0.16586538461538458, 0.16786858974358976, 0.16493055555555558, 0.16399572649572647, 0.16973824786324787, 0.16599893162393164, 0.16412927350427353, 0.1661324786324786, 0.1645299145299145, 0.1626602564102564, 0.17174145299145294, 0.17654914529914534, 0.16813568376068377], "accuracy_train_std": [0.058867438539727375, 0.06345201747392074, 0.06085675345112251, 0.060727861127747015, 0.06280512645823806, 0.06300368332962832, 0.062422725790400646, 0.06069240321908607, 0.06063372334076691, 0.06035513113305312, 0.06001594525389203, 0.05983470290881146, 0.060287453051580224, 0.058722701283981646, 0.055425842792751916, 0.0547918094572469, 0.05625598418067511, 0.055594706440984736, 0.052347540481533755, 0.050725803731051546, 0.05284584602708441, 0.0499333795425166, 0.048750664353190774, 0.05080442194775497, 0.05113213433926812, 0.050070208693050115, 0.04850840332160923, 0.047965692490346414, 0.045229371624401646, 0.045633927774790056, 0.04518109057173434, 0.04669270067915144, 0.04551997147851572, 0.045960965449331016, 0.045892825699473415, 0.04503935486227954, 0.04618965177415549, 0.04346040421386522, 0.04077557633580584, 0.04068889052146849, 0.041682289470212845, 0.03977019622372841, 0.04083370536967193, 0.04163466976784961, 0.039440122216121844, 0.03656820816077848, 0.03605173435021982, 0.034650286071050314, 0.036020264311633333, 0.03709467414767203, 0.03713406976853458, 0.033325759965757255, 0.0318672702465155, 0.03275697075733568, 0.03230191264686254, 0.03412731684826705, 0.03397770493305597, 0.03391741743288901, 0.03019443304129486, 0.030067995600438966, 0.028927136638055687, 0.02701906233075452, 0.029439776442296253, 0.027475065984647738, 0.02684321236772981, 0.029320132782061612, 0.02777609239596842, 0.025963905651560166, 0.025869382052162392, 0.02465861839169248, 0.02677695409798106, 0.0279296913660939, 0.026081015792170543, 0.025978819745517665, 0.025622429647680977, 0.0246093565491098, 0.022585262351223222, 0.023120837933555603, 0.02615394995751235, 0.022820194425852106, 0.020940648858484897, 0.022612791148259117, 0.02269581155159676, 0.023300053608065655, 0.023757749679611685, 0.021173953146649924, 0.02133595848497706, 0.02385237677255455, 0.0200761047593589, 0.020706761678288672, 0.02040044091873928, 0.018949710687937524, 0.0183575495644206, 0.022277976299868253, 0.01852947870231148, 0.019903903251432837, 0.018857967117233778, 0.018563694330754592, 0.02128815094533376, 0.019455276532697865, 0.016791427359339126, 0.02171406895162304, 0.01799526422684709, 0.015545627233765355, 0.017214844395834514, 0.018232607319191237, 0.017872638251257525, 0.017616860676855108, 0.01742445043509253, 0.017753251553939114, 0.017794640236079906, 0.015244426238637535, 0.0159854237651631, 0.01686283645415815, 0.01574214809648868, 0.01607688920363507, 0.01612228995284008, 0.015694289472637762, 0.015706422043667668, 0.015977055979291492, 0.013772395796363525, 0.017760985178493532, 0.015670225841130196, 0.01451838425072895, 0.014978541101476893, 0.015190571697768463, 0.011950151406617161, 0.013671368716389538, 0.015745243233506477, 0.014196261865488143, 0.01543617303517185, 0.01651844757121214, 0.014488890809779578, 0.012884935431170051, 0.016250236595805934, 0.01647386971862585, 0.011600853207545366, 0.01266252980773669, 0.014050548762139275, 0.013116702546388947, 0.014346323922819738, 0.012522164400631178, 0.014560867745223253, 0.014573031992668345, 0.012103196486144002, 0.013068254405299, 0.0143600774683663, 0.013840577780141855, 0.013016031618139606, 0.011488230677371964, 0.013215643017801508, 0.01326497218274975, 0.012655290357679982, 0.01196626591312024, 0.014162755293238115, 0.012057013901811683, 0.012453878510034383, 0.013391439078026243, 0.011323554621350897, 0.013680094422872553, 0.011917229762204516, 0.011984322627868634, 0.014978541101476893, 0.010337537532329343, 0.012710215476197552, 0.011737140326031244, 0.011710123286913041, 0.012605222636291692, 0.010135549861031113, 0.011144927108508673, 0.011196999715462059, 0.009679682434281592, 0.01219074528945493, 0.01088085208616438, 0.01204111281883178, 0.013055706056493975, 0.011307503592664863, 0.011274940096063285, 0.012346706046626563, 0.012051869026859358, 0.011892298291879527, 0.01294930247603385, 0.01369750894724041, 0.010944609829629996], "accuracy_test_std": 0.046969927712784285, "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1.0, 1.2], "translation_range": [-3, 3], "momentum": 0.7376612323937914, "shear_range": [1, 1.1], "patience_check_each": 1, "learning_rate": 0.0007239047174747005, "patience_threshold": 1, "do_flip": true, "batch_size": 64, "optimization": "adam", "nb_data_augmentation": 2, "learning_rate_decay_method": "sqrt", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 9.161177788362866e-09, "valid_ratio": 0.15, "rotation_range": [-90, 90], "learning_rate_decay": 0.9700705158863846}, "accuracy_valid_max": 0.8425480769230769, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = 1234\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.98, interval=[0.8, 1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-10, -6], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128, 256, 512],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1.0, 1.2)),\n        rotation_range=make_constant_param((-90, 90)),\n        shear_range=make_constant_param((1, 1.1)),\n        translation_range=make_constant_param((-3, 3)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    instantiate = instantiate_random\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n    )\n    batch_optimizer.learning_rate = learning_rate\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8318643162393162, "loss_train": [1.984126091003418, 1.7658952474594116, 1.6404281854629517, 1.553511619567871, 1.478883981704712, 1.4181995391845703, 1.3629037141799927, 1.3077770471572876, 1.2555336952209473, 1.2081319093704224, 1.166216492652893, 1.1285682916641235, 1.0917136669158936, 1.0552923679351807, 1.0278230905532837, 1.0003582239151, 0.9719260334968567, 0.9513630867004395, 0.927783191204071, 0.9051752090454102, 0.8833577036857605, 0.8694527745246887, 0.8508443832397461, 0.8309590220451355, 0.8158913254737854, 0.8028687238693237, 0.7891907095909119, 0.7721130847930908, 0.7592370510101318, 0.7456643581390381, 0.7336142063140869, 0.7234260439872742, 0.7141532301902771, 0.7005746364593506, 0.6943047046661377, 0.6829575300216675, 0.6691811680793762, 0.6606162786483765, 0.6520190238952637, 0.6480396389961243, 0.631606936454773, 0.6290649175643921, 0.6163280606269836, 0.610662043094635, 0.6047871708869934, 0.5943320989608765, 0.5894578695297241, 0.581638514995575, 0.5756132006645203, 0.5664518475532532, 0.5614302158355713, 0.5555197596549988, 0.5507298111915588, 0.5473325252532959, 0.5372244119644165, 0.535986602306366, 0.5268646478652954, 0.5226579308509827, 0.5172356963157654, 0.5140268206596375, 0.5080004930496216, 0.5010824203491211, 0.49879810214042664, 0.49153515696525574, 0.4878522455692291, 0.48198309540748596, 0.47669559717178345, 0.47512534260749817, 0.46804317831993103, 0.47067201137542725, 0.46046343445777893, 0.45746538043022156, 0.4548262357711792, 0.45051419734954834, 0.44680964946746826, 0.4399929940700531, 0.4410448968410492, 0.4408229887485504, 0.4338740408420563, 0.42929044365882874, 0.43100860714912415, 0.4220762252807617, 0.4210653305053711, 0.41749006509780884, 0.4158218801021576, 0.41018030047416687, 0.4085168242454529, 0.40566134452819824, 0.4011000096797943, 0.4004465341567993, 0.3986850678920746, 0.3937879204750061, 0.39435791969299316, 0.3873673677444458, 0.3880234360694885, 0.38417738676071167, 0.3817179203033447, 0.3782452344894409, 0.3747214376926422, 0.3765500485897064, 0.3701080083847046, 0.3672625422477722, 0.3662140965461731, 0.3647439777851105, 0.3661038279533386, 0.3561881482601166, 0.3561311364173889, 0.3591986298561096, 0.3512779176235199, 0.34893038868904114, 0.3492807447910309, 0.3476812243461609, 0.3479229211807251, 0.3423316478729248, 0.3450264036655426, 0.3421958088874817, 0.33886128664016724, 0.3356647789478302, 0.3326113224029541, 0.33189186453819275, 0.33120524883270264, 0.32736220955848694, 0.32578566670417786, 0.32266825437545776, 0.32570815086364746, 0.3209453821182251, 0.32080501317977905, 0.31874212622642517, 0.3153116703033447, 0.31483930349349976, 0.3144984543323517, 0.31118857860565186, 0.30833062529563904, 0.310049831867218, 0.30804336071014404, 0.30737069249153137, 0.3024822175502777, 0.2992345690727234, 0.2996750473976135, 0.2984977662563324, 0.29709306359291077, 0.29594627022743225, 0.29575416445732117, 0.29294803738594055, 0.28931018710136414, 0.28968873620033264, 0.2859208285808563, 0.28702446818351746, 0.28477585315704346, 0.2845180034637451, 0.28223514556884766, 0.28277507424354553, 0.28047609329223633, 0.28057608008384705, 0.27923229336738586, 0.2772146463394165, 0.2749806344509125, 0.2731155753135681, 0.2757166922092438, 0.2710503041744232, 0.26815882325172424, 0.26705726981163025, 0.2655560374259949, 0.26652708649635315, 0.26407286524772644, 0.26597559452056885, 0.26069778203964233, 0.25727322697639465, 0.2588471472263336, 0.2575281262397766, 0.259617418050766, 0.2560575008392334, 0.2556648552417755, 0.25524479150772095, 0.25593259930610657, 0.24905602633953094, 0.25014275312423706, 0.2508750557899475, 0.2483970671892166, 0.25050094723701477, 0.24985264241695404, 0.24982258677482605, 0.246337428689003, 0.2440890222787857], "accuracy_train_first": 0.2982398343373494, "model": "vgg", "loss_std": [0.1605674773454666, 0.11052735149860382, 0.11744368076324463, 0.11641917377710342, 0.1189865916967392, 0.11750715225934982, 0.11686458438634872, 0.11952955275774002, 0.12318520247936249, 0.12004170566797256, 0.11953873187303543, 0.11977960169315338, 0.1207575798034668, 0.12256547808647156, 0.1226787269115448, 0.11823533475399017, 0.1185448169708252, 0.12072999775409698, 0.11516442894935608, 0.11812298744916916, 0.11635656654834747, 0.11512919515371323, 0.11638985574245453, 0.11461316049098969, 0.11287065595388412, 0.11254171282052994, 0.11278180778026581, 0.10939545184373856, 0.11271499842405319, 0.10857997089624405, 0.10948839783668518, 0.10460095852613449, 0.10648903995752335, 0.10897938162088394, 0.10227610170841217, 0.10340774059295654, 0.09930457174777985, 0.10084515810012817, 0.09827206283807755, 0.10346145182847977, 0.09784524142742157, 0.10245352983474731, 0.09599100798368454, 0.09789212048053741, 0.09806496649980545, 0.09662383794784546, 0.09641306847333908, 0.09311076253652573, 0.09482640773057938, 0.09241829067468643, 0.09219793975353241, 0.09208714216947556, 0.09127634018659592, 0.09350751340389252, 0.09129113703966141, 0.088220976293087, 0.08768600225448608, 0.09248936176300049, 0.08647055923938751, 0.09108169376850128, 0.08537040650844574, 0.0851336345076561, 0.0847405195236206, 0.08860283344984055, 0.08711183816194534, 0.08144252002239227, 0.08381154388189316, 0.08152488619089127, 0.08218900114297867, 0.08242671936750412, 0.08388262987136841, 0.08235442638397217, 0.08271978795528412, 0.07983124256134033, 0.08023341000080109, 0.07768063992261887, 0.08274854719638824, 0.07989846169948578, 0.08275644481182098, 0.07911750674247742, 0.08055726438760757, 0.07762519270181656, 0.0807603970170021, 0.07657603174448013, 0.0763372927904129, 0.07640223950147629, 0.07788300514221191, 0.07970967143774033, 0.0787900760769844, 0.0736725926399231, 0.07593797892332077, 0.07443248480558395, 0.07529549300670624, 0.07647643238306046, 0.0766446590423584, 0.07401254028081894, 0.07370251417160034, 0.07456996291875839, 0.07398547977209091, 0.07160409539937973, 0.07214385271072388, 0.0721331387758255, 0.07199025899171829, 0.073830246925354, 0.0722183808684349, 0.07211123406887054, 0.07156450301408768, 0.07192811369895935, 0.06923547387123108, 0.06888645142316818, 0.0690266564488411, 0.07212009280920029, 0.07084251940250397, 0.06878256052732468, 0.07059863209724426, 0.06974437087774277, 0.07009274512529373, 0.06858568638563156, 0.06945989280939102, 0.06683847308158875, 0.06607206910848618, 0.06758949905633926, 0.06626933068037033, 0.06759832799434662, 0.06826838850975037, 0.06611988693475723, 0.06625775992870331, 0.06595239043235779, 0.06882936507463455, 0.06805597990751266, 0.06362133473157883, 0.06734238564968109, 0.06768827885389328, 0.06639362871646881, 0.06701210886240005, 0.06358624249696732, 0.06768767535686493, 0.06250590085983276, 0.06573814898729324, 0.0641140565276146, 0.06534462422132492, 0.06444650888442993, 0.06424068659543991, 0.06607544422149658, 0.06381284445524216, 0.06571145355701447, 0.06310544162988663, 0.06470385938882828, 0.06229962781071663, 0.061559706926345825, 0.06163667142391205, 0.06323524564504623, 0.06431256234645844, 0.06177012249827385, 0.05994931980967522, 0.06115414947271347, 0.06273648887872696, 0.05959005281329155, 0.06332620233297348, 0.06013135239481926, 0.06026024371385574, 0.05993688479065895, 0.061793018132448196, 0.061335209757089615, 0.06200327351689339, 0.05934737250208855, 0.061741553246974945, 0.06020299345254898, 0.058879923075437546, 0.05765873193740845, 0.05824032053351402, 0.060002122074365616, 0.05908886715769768, 0.06142275407910347, 0.05799833685159683, 0.05863501876592636, 0.06016600877046585, 0.05886932089924812, 0.05870256572961807, 0.05839919298887253, 0.054451752454042435, 0.059538040310144424, 0.055318672209978104, 0.056165169924497604]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:03 2016", "state": "available"}], "summary": "d0a63066d2f8e79e9ae47d099b476038"}