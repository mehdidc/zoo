{"content": {"hp_model": {"f0": 64, "f1": 32, "f2": 16, "f3": 64, "nonlin": "rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.6343305110931396, 1.2249504327774048, 0.949623703956604, 0.8132076263427734, 0.7315713763237, 0.6741244792938232, 0.6307334899902344, 0.5966381430625916, 0.5667084455490112, 0.5424700975418091, 0.5236218571662903, 0.5055493116378784, 0.490996390581131, 0.47518929839134216, 0.461932897567749, 0.4497421383857727, 0.43818822503089905, 0.4293818771839142, 0.4223041534423828, 0.4145936071872711, 0.4058000445365906, 0.39712756872177124, 0.3902967870235443, 0.38595908880233765, 0.3780428469181061, 0.3718050420284271, 0.36702609062194824, 0.3614732623100281, 0.35757628083229065, 0.3505707085132599, 0.3471432626247406, 0.34315893054008484, 0.3403991758823395, 0.3338604271411896, 0.3295125961303711, 0.3284561038017273, 0.32346925139427185, 0.3208959698677063, 0.31730109453201294, 0.31462952494621277, 0.3110823929309845, 0.30781304836273193, 0.30460408329963684, 0.3020063042640686, 0.2986244559288025, 0.29560667276382446, 0.29346147179603577, 0.29244476556777954, 0.2892076075077057, 0.28481975197792053, 0.28376883268356323, 0.2799798250198364, 0.27935484051704407, 0.2792817950248718, 0.2759975790977478, 0.2740774154663086, 0.27289921045303345, 0.26711010932922363, 0.26633596420288086, 0.26793837547302246, 0.26400458812713623, 0.2638916075229645, 0.2612169682979584, 0.2590271830558777, 0.25961801409721375, 0.2543904781341553, 0.25608721375465393, 0.2546391785144806, 0.2535687983036041, 0.25099802017211914, 0.2481093853712082, 0.24888186156749725, 0.2479621320962906, 0.24420949816703796, 0.24261707067489624, 0.24180687963962555, 0.24206364154815674, 0.24051207304000854, 0.23832030594348907, 0.23807035386562347, 0.23709584772586823, 0.23397290706634521, 0.23476064205169678, 0.2302832156419754, 0.22955898940563202, 0.23010611534118652, 0.2287222295999527, 0.2283821403980255, 0.2290235459804535, 0.2251998335123062, 0.22406624257564545, 0.22408299148082733, 0.22410717606544495, 0.22403302788734436, 0.2220107465982437, 0.22232173383235931, 0.2197396606206894, 0.2187114804983139, 0.21766170859336853, 0.21681272983551025, 0.21482238173484802, 0.2162265181541443, 0.21217000484466553, 0.213605135679245, 0.21363085508346558, 0.21027816832065582, 0.21027188003063202, 0.21095408499240875, 0.20878663659095764, 0.20809468626976013, 0.20822925865650177, 0.207047700881958, 0.20678168535232544, 0.20678643882274628, 0.20493140816688538, 0.2045890837907791, 0.20210470259189606, 0.20075823366641998, 0.2034989297389984, 0.20273567736148834, 0.2006249874830246, 0.19997598230838776, 0.20147250592708588, 0.19868819415569305, 0.19822412729263306, 0.19775661826133728, 0.19619740545749664, 0.19690051674842834, 0.19522222876548767, 0.19433745741844177, 0.1936040222644806, 0.19196689128875732, 0.1938316524028778, 0.1932162642478943, 0.1930025815963745, 0.1925002634525299, 0.19090542197227478, 0.1905202865600586, 0.18894241750240326, 0.1867055743932724, 0.18975010514259338, 0.18791590631008148, 0.18925701081752777, 0.18808208405971527, 0.18592031300067902, 0.18408749997615814, 0.1842932254076004], "moving_avg_accuracy_train": [0.053765263069859716, 0.11523708947605202, 0.17742906926275837, 0.23748423559921783, 0.29430281309480544, 0.3467833967551015, 0.3955828560540745, 0.44037862608497197, 0.48154822082351245, 0.519105197086953, 0.553645728850203, 0.585215982584784, 0.6140754592732418, 0.6404904781761133, 0.6646732213791737, 0.6871002495261862, 0.7075914584525358, 0.7262962883017267, 0.7432467484111996, 0.7588603435728483, 0.7732146683194763, 0.7862567213807088, 0.7980619984512943, 0.808840171587431, 0.8188008719789833, 0.8279515142361422, 0.8363707790235376, 0.8441200701976419, 0.851078192261488, 0.857591618190378, 0.8636374243311501, 0.8691669334149694, 0.874234208442797, 0.8790039833118801, 0.8833246103821317, 0.8874619296191585, 0.891176288434882, 0.8946610093975953, 0.8978298103473706, 0.9009421478688351, 0.9038269930441147, 0.906309241166106, 0.9088270047282976, 0.9110812940925849, 0.9131845953311669, 0.9151007097875298, 0.9168927502601888, 0.9186637328534483, 0.9203784167790207, 0.922049479447741, 0.9235000295246077, 0.9248147891402071, 0.9261561468644754, 0.9273006979449161, 0.9285655978982558, 0.9296805761240725, 0.9307562442868591, 0.9317522834679001, 0.9327302070856265, 0.9336288313856003, 0.9345630431448346, 0.935378365237697, 0.9362005107760352, 0.9369753189926823, 0.9377539905471792, 0.9384547588974078, 0.9392714262685566, 0.9399157460990192, 0.9406373959261789, 0.9412635932337087, 0.941883118577189, 0.9424639789232353, 0.9430004878346216, 0.9435415106239262, 0.9441215091843187, 0.9445760425243771, 0.9450479375971055, 0.945542361578028, 0.9460756627668013, 0.9465812104736022, 0.9470268307168475, 0.9474162631917205, 0.947922573438163, 0.9482875718563899, 0.9486694767577756, 0.9490667777380981, 0.9494382634644267, 0.9498120920990657, 0.9502090638369258, 0.9505314251200384, 0.9510400421652974, 0.9513583246262778, 0.951607684606664, 0.9519900745127843, 0.9523295751306734, 0.9525677324201162, 0.9529448704461001, 0.9532539956373244, 0.9534624898939592, 0.9538059557439874, 0.9540894623232893, 0.9543423291446702, 0.9545722344327227, 0.9548139182776564, 0.9550663830678771, 0.9553424655528944, 0.9555839282941627, 0.9558594455791798, 0.9559841061511529, 0.9562102690064141, 0.9564509819082828, 0.9567047898520984, 0.9569517821431898, 0.9571508597658955, 0.957309103287045, 0.9574654373001179, 0.9577062274571493, 0.9578996150127448, 0.9581294673842092, 0.9583247448232984, 0.958577224429193, 0.9586882346828407, 0.958901968056334, 0.9591362528686869, 0.9593261107628813, 0.9594691531795795, 0.9596490806772361, 0.9597505255072607, 0.9598534155495118, 0.960027396795871, 0.960195605661642, 0.9603563302848926, 0.9604985851993713, 0.9606080494807446, 0.9607762857494475, 0.9609231201912987, 0.9609947452222796, 0.9611732481882853, 0.9612781333350806, 0.9614329117386065, 0.9615745374505893, 0.9617321914770789, 0.9618671407033098, 0.9620280864878606, 0.9621567337499273, 0.9623956770750545, 0.9625107086200407], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.053275602409638544, 0.11309637965926203, 0.1748833272543298, 0.23388528185005644, 0.28945697933334896, 0.340396299989999, 0.38741768161562257, 0.430056366908503, 0.4689560860158455, 0.5045600067817309, 0.537017545024868, 0.5664347899576222, 0.5933782956192395, 0.6177220184387162, 0.6397422617661549, 0.6599552237954881, 0.6780451153372495, 0.6944470585286752, 0.7090226134062293, 0.7226441896032569, 0.7347958039166511, 0.7459062437621847, 0.7558140133524572, 0.7648460168082507, 0.7729148142708744, 0.7803943995324165, 0.7872735401514641, 0.7933538739186972, 0.7990113387952762, 0.804311606224107, 0.8087481390316662, 0.8130116322632887, 0.8168233326005893, 0.8202426853815695, 0.8234798237993614, 0.8265306142451029, 0.8294960522087703, 0.8318221199924114, 0.8339776456625979, 0.8358636430974676, 0.8377726783547389, 0.8395030171175331, 0.8409585477194093, 0.8423245599467153, 0.8435010247916521, 0.8447063275270953, 0.8457697744524731, 0.846590540324244, 0.8475886657910666, 0.8486843502285262, 0.8493204332779627, 0.8498715824859345, 0.8503696757904283, 0.8506694163721535, 0.8513095118678449, 0.851800148595217, 0.8521277403339634, 0.8524347799300851, 0.8526765534901639, 0.8529551848504848, 0.8535020103507526, 0.8539198816048339, 0.8540863871849379, 0.8543725785681007, 0.8546678014153569, 0.8548449642331586, 0.8551041260364994, 0.8551755917272772, 0.8553152120537965, 0.8554652844101638, 0.8558608152217829, 0.8561414917474209, 0.8561356939469258, 0.8561782745428206, 0.8560467281502855, 0.8561358559282539, 0.8563493187635158, 0.8565943814748902, 0.8566510989655789, 0.8567631798634487, 0.8569271468451009, 0.8570269185122473, 0.8572845229242003, 0.8575560765146869, 0.8576275172913055, 0.8578749194590123, 0.8580884629046773, 0.858179907229797, 0.8582011719661546, 0.8582335467687862, 0.8585943329522238, 0.858744024045179, 0.8589703722995466, 0.8590011282736583, 0.8590999918205394, 0.8590801352401422, 0.8594183272413538, 0.859391051181375, 0.8594529814548038, 0.8597172677407993, 0.8597445173409662, 0.859877875753707, 0.8599612772314236, 0.8600495751012782, 0.8600293279168281, 0.8602461275705519, 0.8603883010992647, 0.8604949317385853, 0.8605532487115641, 0.8604724861521547, 0.860544225206367, 0.8606098198638177, 0.8606709140728425, 0.8606017695311456, 0.8605904265859378, 0.860603602489091, 0.8605788397081788, 0.8605331686515176, 0.8604055859731129, 0.8605104881250486, 0.8605448944142005, 0.8603917250970274, 0.8604400667063006, 0.8602973801599175, 0.8602676480268323, 0.8603761959594654, 0.8604341794791062, 0.8604110634419636, 0.8604045250571045, 0.8603854039708217, 0.8603681949931673, 0.8603527069132783, 0.8604130393375378, 0.8603798302833021, 0.8603631786743996, 0.8602658315336162, 0.8603613245756612, 0.8603963811711823, 0.8605500024196514, 0.8605051560745236, 0.8605258295201585, 0.8605820862236397, 0.8606540427932938, 0.8606476205358018, 0.8606540475353089, 0.8605499685536154, 0.8605549832287508], "moving_var_accuracy_train": [0.026016331616740992, 0.057423767430484274, 0.08649197183554666, 0.11030238168529033, 0.12832730025436015, 0.14028247518085218, 0.1476867127136161, 0.15097799055620395, 0.1511346112790045, 0.14871588834557778, 0.1445817345214128, 0.13909368935706395, 0.1326801449739423, 0.12569190948929956, 0.11838596415979613, 0.11107411206737854, 0.10374570765001048, 0.09651997282219407, 0.08945383842128017, 0.08270251376399886, 0.0762866821379839, 0.07018887025665355, 0.06442426433140781, 0.059027359043639885, 0.05401756310988831, 0.049369415082365996, 0.04507042975017188, 0.041103850398464155, 0.03742920452251616, 0.03406810652624477, 0.030990261820646473, 0.028166414874954186, 0.02558086887332757, 0.02322753875671045, 0.02107279524556113, 0.01911957241522666, 0.01733178332641148, 0.015707894515462097, 0.014227476759049562, 0.012891908886772248, 0.011677618983263032, 0.010565311086588891, 0.009565832178125898, 0.008654985345154756, 0.007829301695541267, 0.0070794149774760845, 0.006400376161229307, 0.005788565959217028, 0.005236170631976875, 0.004737685622764304, 0.004282853920217356, 0.0038701258638169206, 0.0034993064423353175, 0.003161165772683426, 0.0028594489424427125, 0.0025846926361948503, 0.0023366369305432583, 0.002111902083940451, 0.0019093188869653702, 0.0017256547289613623, 0.001560944020565052, 0.0014108323695445329, 0.0012758324421659627, 0.0011536521479026216, 0.0010437438976204046, 0.0009437891943845026, 0.000855412785301945, 0.0007736078391670959, 0.0007009340615077433, 0.0006343697629685867, 0.0005743870915326584, 0.0005199849710538742, 0.0004705770502564592, 0.000426153696157735, 0.00038656591151247825, 0.00034976872537625177, 0.0003167960174756147, 0.00028731651138425485, 0.00026114455166735377, 0.00023733030285528282, 0.0002153844691804635, 0.0001952109411347887, 0.00017799699761218455, 0.00016139631245873929, 0.00014656934339618747, 0.00013333304167725555, 0.00012124175231332307, 0.00011037530771467465, 0.00010075605598914441, 9.161570156187948e-05, 8.478235309424338e-05, 7.721585130952877e-05, 7.005388977694e-05, 6.436449916196972e-05, 5.896539527169682e-05, 5.357932579515986e-05, 4.95014910314315e-05, 4.541136738293318e-05, 4.126145934008733e-05, 3.81970325172991e-05, 3.51007130901363e-05, 3.216611644532031e-05, 2.942521277405819e-05, 2.7008391224769865e-05, 2.488119833500351e-05, 2.3079072348303588e-05, 2.1295903412260563e-05, 1.984950104012327e-05, 1.8004413259952936e-05, 1.666431866785683e-05, 1.5519371111205763e-05, 1.4547200251179828e-05, 1.3641526952788944e-05, 1.2634061356269628e-05, 1.1596024328514679e-05, 1.065638480845451e-05, 1.0112565425117925e-05, 9.437897602539008e-06, 8.969596856295958e-06, 8.415836674621581e-06, 8.147966569693638e-06, 7.444079400458599e-06, 7.110809054916302e-06, 6.89373250911794e-06, 6.5287734380976464e-06, 6.060046291061649e-06, 5.745406801672425e-06, 5.263485603353724e-06, 4.83241429016813e-06, 4.621598127913835e-06, 4.4140863178380206e-06, 4.205169326725994e-06, 3.9667805402932504e-06, 3.6779443463331102e-06, 3.56488089066359e-06, 3.4024359814207596e-06, 3.108363688845778e-06, 3.0842970998167198e-06, 2.8748754359995316e-06, 2.8029950801818284e-06, 2.703216152815463e-06, 2.6565876661493887e-06, 2.554830542477134e-06, 2.5324793983117495e-06, 2.428182520815786e-06, 2.69920948233997e-06, 2.5483788411832707e-06], "duration": 151443.908735, "accuracy_train": [0.5376526306985973, 0.6684835271317829, 0.7371568873431156, 0.7779807326273532, 0.8056700105550941, 0.8191086496977666, 0.834777989744832, 0.8435405563630491, 0.8520745734703765, 0.857117983457918, 0.8645105147194537, 0.8693482661960132, 0.8738107494693614, 0.8782256483019564, 0.8823179102067183, 0.8889435028492986, 0.8920123387896824, 0.8946397569444444, 0.8958008893964563, 0.8993827000276854, 0.9024035910391289, 0.9036351989318014, 0.9043094920865633, 0.9058437298126615, 0.9084471755029531, 0.9103072945505721, 0.912144162110096, 0.913863690764581, 0.9137012908361019, 0.9162124515503876, 0.9180496795980989, 0.9189325151693429, 0.9198396836932448, 0.9219319571336286, 0.9222102540143964, 0.9246978027523993, 0.9246055177763934, 0.9260234980620154, 0.9263490188953488, 0.9289531855620154, 0.9297905996216316, 0.9286494742640274, 0.9314868767880213, 0.9313698983711702, 0.9321143064784054, 0.9323457398947952, 0.9330211145141197, 0.9346025761927832, 0.9358105721091732, 0.9370890434662238, 0.9365549802164084, 0.9366476256806018, 0.9382283663828904, 0.9376016576688816, 0.9399496974783131, 0.9397153801564231, 0.940437257751938, 0.9407166360972684, 0.9415315196451642, 0.9417164500853636, 0.9429709489779439, 0.9427162640734589, 0.943599820621078, 0.9439485929425065, 0.9447620345376523, 0.9447616740494648, 0.9466214326088963, 0.945714624573182, 0.9471322443706165, 0.9468993690014765, 0.9474588466685124, 0.9476917220376523, 0.9478290680370985, 0.948410715727667, 0.9493414962278516, 0.9486668425849022, 0.9492949932516611, 0.9499921774063308, 0.9508753734657622, 0.9511311398348099, 0.9510374129060539, 0.9509211554655776, 0.9524793656561462, 0.9515725576204319, 0.9521066208702473, 0.9526424865610004, 0.9527816350013842, 0.9531765498108158, 0.953781809477667, 0.953432676668051, 0.9556175955726283, 0.9542228667751015, 0.9538519244301403, 0.9554315836678663, 0.9553850806916758, 0.9547111480251015, 0.9563391126799556, 0.9560361223583426, 0.9553389382036729, 0.9568971483942414, 0.9566410215370063, 0.9566181305370985, 0.9566413820251938, 0.9569890728820598, 0.9573385661798633, 0.957827207918051, 0.9577570929655776, 0.9583391011443337, 0.957106051298911, 0.9582457347037652, 0.9586173980251015, 0.9589890613464378, 0.9591747127630121, 0.9589425583702473, 0.9587332949773901, 0.958872443417774, 0.9598733388704319, 0.9596401030131044, 0.9601981387273901, 0.9600822417751015, 0.9608495408822444, 0.9596873269656699, 0.960825568417774, 0.9612448161798633, 0.9610348318106312, 0.9607565349298633, 0.9612684281561462, 0.9606635289774824, 0.9607794259297711, 0.9615932280131044, 0.9617094854535806, 0.9618028518941492, 0.9617788794296788, 0.9615932280131044, 0.962290412167774, 0.9622446301679586, 0.9616393705011074, 0.9627797748823367, 0.9622220996562385, 0.9628259173703396, 0.9628491688584349, 0.9631510777154854, 0.9630816837393872, 0.9634765985488187, 0.9633145591085271, 0.9645461670011997, 0.9635459925249169], "end": "2016-02-03 03:51:39.006000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0], "moving_var_accuracy_valid": [0.025544608308988966, 0.05519687599483171, 0.08403583043338939, 0.10696332320509616, 0.1240609129351578, 0.13500815114229114, 0.14140642899790482, 0.1436283034496621, 0.1428841664243672, 0.14000450234706144, 0.1354854782115977, 0.12972529908534078, 0.12328634165284513, 0.11629125905396419, 0.10902615319436422, 0.10180061238092122, 0.09456574872676332, 0.08753038751817978, 0.08068936996625914, 0.0742903590124561, 0.06819027868500388, 0.06248222767855449, 0.0571174799949844, 0.052139925773315145, 0.04751188262841922, 0.0432641921267394, 0.03936367609497512, 0.0357600426139662, 0.03247210053203712, 0.02947772599218753, 0.02670709880314172, 0.024199985293652367, 0.021910748299439533, 0.019824901230462755, 0.017936722693639974, 0.01622681632537043, 0.014683279093680628, 0.0132636465063194, 0.01197909847392096, 0.01081320150344788, 0.009764681093624638, 0.008815159634368429, 0.007952710794928571, 0.007174233620082061, 0.0064692668838562065, 0.005835414987627189, 0.00526205176313233, 0.0047419094963654694, 0.004276684836756601, 0.0038598210725593616, 0.00347748038011545, 0.0031324662311489368, 0.002821452480493878, 0.0025401158321914875, 0.0022897917491647773, 0.0020629790938325177, 0.0018576470315749198, 0.001672730788239707, 0.0015059837995049152, 0.0013560841384690125, 0.0012231668877717996, 0.0011024217464595074, 0.0009924290887874086, 0.0008939233294788377, 0.0008053154052968325, 0.0007250663447432521, 0.0006531641938317245, 0.0005878937405531771, 0.0005292798110180555, 0.00047655452532556074, 0.0004303070743994657, 0.00038798538076791696, 0.0003491871452215405, 0.0003142847486637072, 0.00028301201387783743, 0.000254782306337304, 0.00022971417314191625, 0.0002072832574202798, 0.00018658388354200205, 0.0001680385543368076, 0.00015147666544277578, 0.000136418588368585, 0.00012337396982924529, 0.00011170024501887622, 0.00010057615457806349, 9.106940961353136e-05, 8.237287588085671e-05, 7.421084687414028e-05, 6.679383188783749e-05, 6.012388184966261e-05, 5.528299369613196e-05, 4.995636113631025e-05, 4.542182681297672e-05, 4.0888157501171026e-05, 3.688730775917123e-05, 3.320212553731978e-05, 3.091127745073992e-05, 2.7826845556697614e-05, 2.5078679229930516e-05, 2.319943647562527e-05, 2.0886175694446126e-05, 1.895761832124004e-05, 1.7124458747483927e-05, 1.5482181497123312e-05, 1.393765288371439e-05, 1.2966906404035874e-05, 1.185213557403236e-05, 1.0769252855806232e-05, 9.722935394262321e-06, 8.809345173857341e-06, 7.974729083565016e-06, 7.215980106983125e-06, 6.5279746176721754e-06, 5.9182058647232285e-06, 5.327543239904819e-06, 4.7963513557294616e-06, 4.322234978023027e-06, 3.908784088969728e-06, 3.6644017385332e-06, 3.3970017180065314e-06, 3.0679556808046303e-06, 2.972307670233536e-06, 2.6961091038942546e-06, 2.6097332481733975e-06, 2.3567159209961793e-06, 2.2270882120067257e-06, 2.034638187755483e-06, 1.8359835295385145e-06, 1.6527699308737436e-06, 1.490783481252055e-06, 1.344370473334055e-06, 1.2120923515684854e-06, 1.1236431291649182e-06, 1.0212043877974985e-06, 9.215794337291235e-07, 9.147096827244188e-07, 9.053090041630625e-07, 8.258387877526122e-07, 9.556503008082458e-07, 8.781860227693094e-07, 7.94213942682185e-07, 7.432758985930346e-07, 7.15548039981196e-07, 6.443644445047223e-07, 5.802997569582367e-07, 6.19761691135704e-07, 5.5801184472256e-07], "accuracy_test": 0.8404735331632652, "start": "2016-02-01 09:47:35.097000", "learning_rate_per_epoch": [0.002564279828220606, 0.001282139914110303, 0.000854759884532541, 0.0006410699570551515, 0.0005128559423610568, 0.0004273799422662705, 0.00036632566479966044, 0.00032053497852757573, 0.00028491998091340065, 0.0002564279711805284, 0.00023311634140554816, 0.00021368997113313526, 0.00019725228776223958, 0.00018316283239983022, 0.00017095198563765734, 0.00016026748926378787, 0.00015083998732734472, 0.00014245999045670033, 0.00013496208703145385, 0.0001282139855902642, 0.00012210855493322015, 0.00011655817070277408, 0.00011149042256874964, 0.00010684498556656763, 0.00010257118992740288, 9.862614388111979e-05, 9.497332212049514e-05, 9.158141619991511e-05, 8.842343959258869e-05, 8.547599281882867e-05, 8.271870319731534e-05, 8.013374463189393e-05, 7.770544470986351e-05, 7.541999366367236e-05, 7.326513150474057e-05, 7.122999522835016e-05, 6.930485687917098e-05, 6.748104351572692e-05, 6.575076258741319e-05, 6.41069927951321e-05, 6.254341133171692e-05, 6.105427746661007e-05, 5.963441071799025e-05, 5.827908535138704e-05, 5.6983993999892846e-05, 5.574521128437482e-05, 5.455914288177155e-05, 5.3422492783283815e-05, 5.233223782852292e-05, 5.128559496370144e-05, 5.027999577578157e-05, 4.9313071940559894e-05, 4.8382637032773346e-05, 4.748666106024757e-05, 4.662326682591811e-05, 4.5790708099957556e-05, 4.498736234381795e-05, 4.4211719796294346e-05, 4.3462368921609595e-05, 4.2737996409414336e-05, 4.203737262287177e-05, 4.135935159865767e-05, 4.070285285706632e-05, 4.0066872315946966e-05, 3.9450456824852154e-05, 3.885272235493176e-05, 3.8272832171060145e-05, 3.770999683183618e-05, 3.716347418958321e-05, 3.663256575237028e-05, 3.6116616684012115e-05, 3.561499761417508e-05, 3.512711919029243e-05, 3.465242843958549e-05, 3.419039785512723e-05, 3.374052175786346e-05, 3.330233448650688e-05, 3.2875381293706596e-05, 3.245923653594218e-05, 3.205349639756605e-05, 3.165777525282465e-05, 3.127170566585846e-05, 3.08949347527232e-05, 3.052713873330504e-05, 3.0167997465468943e-05, 2.9817205358995125e-05, 2.9474480470526032e-05, 2.913954267569352e-05, 2.881213185901288e-05, 2.8491996999946423e-05, 2.817889799189288e-05, 2.787260564218741e-05, 2.7572899853112176e-05, 2.7279571440885775e-05, 2.6992418497684412e-05, 2.6711246391641907e-05, 2.6435873223817907e-05, 2.616611891426146e-05, 2.590181611594744e-05, 2.564279748185072e-05, 2.5388908397872e-05, 2.5139997887890786e-05, 2.4895920432754792e-05, 2.4656535970279947e-05, 2.442171171423979e-05, 2.4191318516386673e-05, 2.3965230866451748e-05, 2.3743330530123785e-05, 2.3525502911070362e-05, 2.3311633412959054e-05, 2.310161835339386e-05, 2.2895354049978778e-05, 2.2692740458296612e-05, 2.2493681171908975e-05, 2.229808524134569e-05, 2.2105859898147173e-05, 2.1916921468800865e-05, 2.1731184460804798e-05, 2.1548568838625215e-05, 2.1368998204707168e-05, 2.1192394342506304e-05, 2.1018686311435886e-05, 2.0847803170909174e-05, 2.0679675799328834e-05, 2.0514238713076338e-05, 2.035142642853316e-05, 2.019117891904898e-05, 2.0033436157973483e-05, 1.9878138118656352e-05, 1.9725228412426077e-05, 1.9574654288589954e-05, 1.942636117746588e-05, 1.928029814735055e-05, 1.9136416085530072e-05, 1.8994664060301147e-05, 1.885499841591809e-05, 1.8717370039667003e-05, 1.8581737094791606e-05, 1.8448055925546214e-05, 1.831628287618514e-05, 1.8186381566920318e-05, 1.8058308342006058e-05, 1.793202682165429e-05, 1.780749880708754e-05, 1.768468791851774e-05, 1.7563559595146216e-05, 1.7444079276174307e-05], "accuracy_train_first": 0.5376526306985973, "accuracy_train_last": 0.9635459925249169, "batch_size_eval": 1024, "accuracy_train_std": [0.02067952479979876, 0.02037939862458451, 0.018995564700425696, 0.017945047745893443, 0.016963824904252883, 0.01588897396999873, 0.015714024060313282, 0.015132260210861736, 0.014421472321826733, 0.0148922314914789, 0.014552657777723825, 0.014414845292431241, 0.012824450874734007, 0.013836838663336407, 0.01372131905223451, 0.013314861318567878, 0.01284586441094366, 0.013778795137361214, 0.01316217859729774, 0.011929152004162563, 0.012073771150936895, 0.011324998229554352, 0.012318294128689802, 0.011713447629114528, 0.012270936258369735, 0.01153421517176033, 0.010867220059697862, 0.011686189693124644, 0.012577179046058413, 0.010540216333083566, 0.011069168126454055, 0.010752408598416962, 0.01039890475951085, 0.009906262966178867, 0.00986163827252489, 0.010016783297922695, 0.009155605458421603, 0.010139792245508296, 0.009571163005599995, 0.009429070596036749, 0.008637409752761132, 0.009904399708592833, 0.009261559771964606, 0.008773568002883167, 0.009049532359605851, 0.009437428683526195, 0.008097341912980126, 0.008812647498427096, 0.008736020429860727, 0.009209166889737533, 0.008450072441126381, 0.008753808032018839, 0.009784286819459078, 0.008687186394878877, 0.008470261887257767, 0.009362688635710078, 0.008841192197990733, 0.008638034340025769, 0.00879708198605526, 0.009237231850636492, 0.008773839181129497, 0.008673926636291745, 0.008082662227996036, 0.008801166834540866, 0.008621972138644969, 0.009014370435542705, 0.009062831180293853, 0.008714561504563305, 0.00840047663823334, 0.008615637394707137, 0.008172083792636285, 0.008123020136813696, 0.008068765283431192, 0.007979619098320415, 0.007658902960637224, 0.008708792361093221, 0.0077185691301695164, 0.008089812716186601, 0.008231586454476926, 0.007932083715028352, 0.008076592824306322, 0.008035763502813074, 0.007434329030441293, 0.008775211664135963, 0.008017950154854275, 0.008028578807078067, 0.008033509332604194, 0.007868411945717862, 0.008144064866056667, 0.008227636848608775, 0.00835230215262997, 0.008243338885006354, 0.007851895695032684, 0.008077716339474295, 0.008026042879477622, 0.007896236283671356, 0.007682766649317372, 0.007911683751400967, 0.007847104839415127, 0.007357121231843671, 0.007791088715400092, 0.0072000093349829375, 0.007224915435272238, 0.008015348858700394, 0.008150799422505179, 0.007293406310812658, 0.007414024023223628, 0.007468788647970707, 0.007449094239791635, 0.007207193209707652, 0.00744977581200642, 0.007690183170176833, 0.007451212982803576, 0.006836623758033553, 0.007401646002824782, 0.007469541253972864, 0.007839819842562532, 0.007554739550033978, 0.007512626975583409, 0.0071063556321823455, 0.007066098492758121, 0.006882074512039618, 0.007113371880817635, 0.007449628701200991, 0.007124749850482254, 0.0068984541864216785, 0.006526467587640142, 0.0070548988985812975, 0.007341379299744307, 0.008396311701165842, 0.007601010198459279, 0.007234974683558005, 0.007234242717999, 0.007120249634109275, 0.007272695444831958, 0.007000065535795326, 0.007141814945513435, 0.006856225638078004, 0.006575812447968724, 0.0064517981602392315, 0.0069680456388966005, 0.007120477122446342, 0.006857130896146345, 0.007123451165493611, 0.006495041084945898, 0.007072933303003335, 0.00703294257375727], "accuracy_test_std": 0.00904509554788876, "error_valid": [0.46724397590361444, 0.3485166250941265, 0.26903414439006024, 0.23509712678840367, 0.2103977433170181, 0.20114981410015065, 0.1893898837537651, 0.1861954654555723, 0.1809464420180723, 0.17500470632530118, 0.17086461078689763, 0.1688100056475903, 0.16413015342620485, 0.16318447618599397, 0.16207554828689763, 0.15812811794051207, 0.15914586078689763, 0.15793545274849397, 0.1597973926957832, 0.15476162462349397, 0.15583966726280118, 0.15409979762801207, 0.1550160603350903, 0.1538659520896084, 0.15446600856551207, 0.15228933311370485, 0.1508141942771084, 0.15192312217620485, 0.15007147731551207, 0.14798598691641573, 0.15132306570030118, 0.1486169286521084, 0.14887136436370485, 0.1489831395896084, 0.14738593044051207, 0.14601227174322284, 0.14381500611822284, 0.1472432699548193, 0.14662262330572284, 0.14716237998870485, 0.1450460043298193, 0.1449239340173193, 0.14594167686370485, 0.14538133000753017, 0.14591079160391573, 0.14444594785391573, 0.1446592032191265, 0.1460225668298193, 0.14342820500753017, 0.14145448983433728, 0.1449548192771084, 0.1451680746423193, 0.1451474844691265, 0.1466329183923193, 0.14292962867093373, 0.14378412085843373, 0.1449239340173193, 0.1448018637048193, 0.1451474844691265, 0.1445371329066265, 0.14157656014683728, 0.14231927710843373, 0.1444150625941265, 0.14305169898343373, 0.14267519295933728, 0.1435605704066265, 0.14256341773343373, 0.14418121705572284, 0.14342820500753017, 0.14318406438253017, 0.1405794074736446, 0.14133241952183728, 0.14391648625753017, 0.1434385000941265, 0.14513718938253017, 0.14306199407003017, 0.1417295157191265, 0.14120005412274095, 0.14283844361822284, 0.14222809205572284, 0.14159715032003017, 0.14207513648343373, 0.14039703736822284, 0.13999994117093373, 0.1417295157191265, 0.1398984610316265, 0.13998964608433728, 0.1409970938441265, 0.1416074454066265, 0.14147508000753017, 0.13815859139683728, 0.13990875611822284, 0.1389924934111446, 0.14072206795933728, 0.14001023625753017, 0.14109857398343373, 0.13753794474774095, 0.14085443335843373, 0.13998964608433728, 0.13790415568524095, 0.14001023625753017, 0.1389218985316265, 0.1392881094691265, 0.13915574407003017, 0.14015289674322284, 0.13780267554593373, 0.1383321371423193, 0.13854539250753017, 0.1389218985316265, 0.14025437688253017, 0.13881012330572284, 0.1387998282191265, 0.13877923804593373, 0.1400205313441265, 0.13951165992093373, 0.13927781438253017, 0.13964402532003017, 0.13987787085843373, 0.14074265813253017, 0.13854539250753017, 0.13914544898343373, 0.14098679875753017, 0.13912485881024095, 0.14098679875753017, 0.13999994117093373, 0.13864687264683728, 0.1390439688441265, 0.1397969808923193, 0.1396543204066265, 0.13978668580572284, 0.13978668580572284, 0.13978668580572284, 0.1390439688441265, 0.1399190512048193, 0.13978668580572284, 0.14061029273343373, 0.13877923804593373, 0.1392881094691265, 0.1380674063441265, 0.1398984610316265, 0.1392881094691265, 0.13891160344503017, 0.1386983480798193, 0.1394101797816265, 0.1392881094691265, 0.1403867422816265, 0.13939988469503017], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.035798166554318576, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 16, "valid_ratio": 0.15, "learning_rate": 0.0025642797288211497, "optimization": "adam", "nb_data_augmentation": 2, "learning_rate_decay_method": "lin", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 2.6820027651855175e-08, "rotation_range": [0, 0], "momentum": 0.5360025596820893}, "accuracy_valid_max": 0.862462055252259, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8606001153049698, "accuracy_valid_std": [0.013860624903998106, 0.018005000164677612, 0.010961387635718432, 0.009688661312696556, 0.01100881795387333, 0.005021213088672289, 0.004514711874887275, 0.00488373183910849, 0.005535780750752659, 0.007195326314578046, 0.005493713342584263, 0.008607866483146204, 0.004932026090146838, 0.009786877685055385, 0.009481886764348663, 0.007824387313130251, 0.010566100628528467, 0.010697920931616691, 0.010811066662357299, 0.009906729170060522, 0.007425161438548187, 0.007302743599459703, 0.01037594049784318, 0.007575143940466925, 0.008100546539014944, 0.00809992702063037, 0.008344993770800621, 0.007158836896756233, 0.008000371127676506, 0.0105597107765993, 0.012115235610749802, 0.009387758270845498, 0.010156051466147066, 0.008386936977664291, 0.007978889200691635, 0.009019421490894304, 0.008683374321361013, 0.008562000170384354, 0.006488101269256032, 0.009772860664578971, 0.008544410522720891, 0.009215174557702124, 0.009962442158094005, 0.008549498381098093, 0.01021049542506509, 0.009620822022950726, 0.0062230960097400135, 0.010113393652788053, 0.008221499873566601, 0.008121618400351588, 0.010264434724080873, 0.011442084669631517, 0.008719729729173615, 0.007553356439626689, 0.0072665529234902035, 0.006755047373780314, 0.0066624532780091535, 0.007978843792151384, 0.007515541435153294, 0.008652379793907757, 0.008130245997619119, 0.005843242723875412, 0.006808758403019842, 0.007911791512387915, 0.006364820372825796, 0.007904264674006489, 0.00788068811794276, 0.006558118420287242, 0.0077282111164682895, 0.0077048987538643745, 0.007921141942292056, 0.00806693329837088, 0.006905752499588802, 0.007895867643426079, 0.0075180899792493434, 0.007813335960384023, 0.007034929293321085, 0.00778908931316287, 0.008982081940625995, 0.008892317917442766, 0.006605356632832972, 0.00912743864330608, 0.008151789702240822, 0.007644518421606738, 0.008493526663093226, 0.007452968824102553, 0.010064422256442758, 0.009519771667222397, 0.007178657639286389, 0.008147621260069743, 0.008548832834444466, 0.009132096872420769, 0.009661308522680013, 0.007250697929887605, 0.009146373755423416, 0.00898102481274758, 0.008841581889084344, 0.007903936281890848, 0.009428405445392082, 0.009522168599517959, 0.00957926847746483, 0.009430632821781804, 0.0089795645698883, 0.009567200572728442, 0.009530890539369742, 0.008928185324316865, 0.010422738277944156, 0.007725422924713292, 0.008977315030158714, 0.009275909284251586, 0.010626388786332439, 0.009676370055544116, 0.010466305016721737, 0.008668352496665883, 0.008837071466790204, 0.009665653183453747, 0.008882536286215466, 0.008733370552430996, 0.008794573536105067, 0.009383707704248446, 0.009213733817025069, 0.008212935348217204, 0.010707909823238809, 0.009492211125372428, 0.008113546564261106, 0.006950642198043552, 0.008173565610546224, 0.010436165159106175, 0.01046383392107187, 0.007603541886701055, 0.009783782668271299, 0.008279050989237096, 0.008275028993521097, 0.009031248489726014, 0.008603854520023203, 0.008446663219119173, 0.008044926838198149, 0.008445960885525832, 0.008900361282284558, 0.007933329707329264, 0.008100136380419457, 0.008452361038476637, 0.010061710041380116, 0.008038183583677746, 0.008558130865594708, 0.008869475826670576, 0.009273092753739444], "accuracy_valid": [0.5327560240963856, 0.6514833749058735, 0.7309658556099398, 0.7649028732115963, 0.7896022566829819, 0.7988501858998494, 0.8106101162462349, 0.8138045345444277, 0.8190535579819277, 0.8249952936746988, 0.8291353892131024, 0.8311899943524097, 0.8358698465737951, 0.836815523814006, 0.8379244517131024, 0.8418718820594879, 0.8408541392131024, 0.842064547251506, 0.8402026073042168, 0.845238375376506, 0.8441603327371988, 0.8459002023719879, 0.8449839396649097, 0.8461340479103916, 0.8455339914344879, 0.8477106668862951, 0.8491858057228916, 0.8480768778237951, 0.8499285226844879, 0.8520140130835843, 0.8486769342996988, 0.8513830713478916, 0.8511286356362951, 0.8510168604103916, 0.8526140695594879, 0.8539877282567772, 0.8561849938817772, 0.8527567300451807, 0.8533773766942772, 0.8528376200112951, 0.8549539956701807, 0.8550760659826807, 0.8540583231362951, 0.8546186699924698, 0.8540892083960843, 0.8555540521460843, 0.8553407967808735, 0.8539774331701807, 0.8565717949924698, 0.8585455101656627, 0.8550451807228916, 0.8548319253576807, 0.8548525155308735, 0.8533670816076807, 0.8570703713290663, 0.8562158791415663, 0.8550760659826807, 0.8551981362951807, 0.8548525155308735, 0.8554628670933735, 0.8584234398531627, 0.8576807228915663, 0.8555849374058735, 0.8569483010165663, 0.8573248070406627, 0.8564394295933735, 0.8574365822665663, 0.8558187829442772, 0.8565717949924698, 0.8568159356174698, 0.8594205925263554, 0.8586675804781627, 0.8560835137424698, 0.8565614999058735, 0.8548628106174698, 0.8569380059299698, 0.8582704842808735, 0.858799945877259, 0.8571615563817772, 0.8577719079442772, 0.8584028496799698, 0.8579248635165663, 0.8596029626317772, 0.8600000588290663, 0.8582704842808735, 0.8601015389683735, 0.8600103539156627, 0.8590029061558735, 0.8583925545933735, 0.8585249199924698, 0.8618414086031627, 0.8600912438817772, 0.8610075065888554, 0.8592779320406627, 0.8599897637424698, 0.8589014260165663, 0.862462055252259, 0.8591455666415663, 0.8600103539156627, 0.862095844314759, 0.8599897637424698, 0.8610781014683735, 0.8607118905308735, 0.8608442559299698, 0.8598471032567772, 0.8621973244540663, 0.8616678628576807, 0.8614546074924698, 0.8610781014683735, 0.8597456231174698, 0.8611898766942772, 0.8612001717808735, 0.8612207619540663, 0.8599794686558735, 0.8604883400790663, 0.8607221856174698, 0.8603559746799698, 0.8601221291415663, 0.8592573418674698, 0.8614546074924698, 0.8608545510165663, 0.8590132012424698, 0.860875141189759, 0.8590132012424698, 0.8600000588290663, 0.8613531273531627, 0.8609560311558735, 0.8602030191076807, 0.8603456795933735, 0.8602133141942772, 0.8602133141942772, 0.8602133141942772, 0.8609560311558735, 0.8600809487951807, 0.8602133141942772, 0.8593897072665663, 0.8612207619540663, 0.8607118905308735, 0.8619325936558735, 0.8601015389683735, 0.8607118905308735, 0.8610883965549698, 0.8613016519201807, 0.8605898202183735, 0.8607118905308735, 0.8596132577183735, 0.8606001153049698], "seed": 334275602, "model": "residualv3", "loss_std": [0.32695725560188293, 0.2732086181640625, 0.2626659572124481, 0.25041884183883667, 0.24203719198703766, 0.23557700216770172, 0.23080198466777802, 0.2271944284439087, 0.22251492738723755, 0.21662957966327667, 0.2102740854024887, 0.21014416217803955, 0.20664924383163452, 0.2028982788324356, 0.20219863951206207, 0.19969208538532257, 0.19415585696697235, 0.1947077065706253, 0.18957318365573883, 0.18997035920619965, 0.1886569857597351, 0.18425007164478302, 0.182386577129364, 0.18077106773853302, 0.18014536798000336, 0.1768035590648651, 0.17586956918239594, 0.17721675336360931, 0.17392998933792114, 0.1715390980243683, 0.17013366520404816, 0.17031648755073547, 0.1681952178478241, 0.16674436628818512, 0.16503937542438507, 0.16536353528499603, 0.16329503059387207, 0.16326473653316498, 0.16043584048748016, 0.1607074737548828, 0.15822798013687134, 0.1589788794517517, 0.15759404003620148, 0.15565206110477448, 0.15578384697437286, 0.15307387709617615, 0.15507936477661133, 0.15604586899280548, 0.15094849467277527, 0.14964282512664795, 0.15157458186149597, 0.14818882942199707, 0.1494053155183792, 0.1472935676574707, 0.15019868314266205, 0.14913183450698853, 0.1459115445613861, 0.1451718807220459, 0.14354605972766876, 0.14710399508476257, 0.14159555733203888, 0.1449553370475769, 0.1427624225616455, 0.14089292287826538, 0.14008083939552307, 0.1408691257238388, 0.1408950388431549, 0.13885554671287537, 0.14045332372188568, 0.13865108788013458, 0.13625502586364746, 0.13734771311283112, 0.13763336837291718, 0.13597418367862701, 0.1357761025428772, 0.13634949922561646, 0.13472940027713776, 0.13512717187404633, 0.1330561339855194, 0.13296127319335938, 0.1329376995563507, 0.13112714886665344, 0.13448306918144226, 0.13033433258533478, 0.13159213960170746, 0.13011962175369263, 0.12956607341766357, 0.12894946336746216, 0.12974144518375397, 0.12651509046554565, 0.1273869127035141, 0.1282602697610855, 0.12861810624599457, 0.12792924046516418, 0.12620095908641815, 0.1255481094121933, 0.12623928487300873, 0.12602977454662323, 0.12693579494953156, 0.1262938529253006, 0.12153670936822891, 0.12447542697191238, 0.12421336770057678, 0.12356295436620712, 0.12475855648517609, 0.12304510176181793, 0.12204468995332718, 0.12252674996852875, 0.12076447159051895, 0.12194120138883591, 0.12150675803422928, 0.12027213722467422, 0.12033426016569138, 0.12015116214752197, 0.1197536364197731, 0.11906176805496216, 0.12277423590421677, 0.11873719096183777, 0.12031394988298416, 0.11871100217103958, 0.11734682321548462, 0.11825943738222122, 0.11931502819061279, 0.11657854169607162, 0.11620491743087769, 0.11564839631319046, 0.11804832518100739, 0.11739207804203033, 0.11661819368600845, 0.1149207055568695, 0.1147516667842865, 0.11348842084407806, 0.11691582202911377, 0.11656903475522995, 0.1152103915810585, 0.11479426920413971, 0.11104141920804977, 0.11296430230140686, 0.11282426118850708, 0.11183007806539536, 0.11349979043006897, 0.11331086605787277, 0.11445443332195282, 0.11086863279342651, 0.109576016664505, 0.1099260002374649, 0.11185302585363388]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:34 2016", "state": "available"}], "summary": "2effe5fe61e04bec01b6c6980a044b40"}