{"content": {"hp_model": {"f0": 32, "f1": 16, "f2": 32, "f3": 16, "nonlin": "rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.600706696510315, 1.1774890422821045, 0.9973027110099792, 0.8818007707595825, 0.8071146011352539, 0.754530131816864, 0.7156392335891724, 0.6842347979545593, 0.6570687294006348, 0.6343169212341309, 0.6147162914276123, 0.5976412892341614, 0.5808234810829163, 0.5662167072296143, 0.554646909236908, 0.5424413084983826, 0.5304737687110901, 0.5218678116798401, 0.5143969655036926, 0.5061542391777039, 0.4978795647621155, 0.49233195185661316, 0.4883492887020111, 0.4791313111782074, 0.47056299448013306, 0.46962839365005493, 0.46346426010131836, 0.45734935998916626, 0.45182564854621887, 0.449930876493454, 0.4460639953613281, 0.4412952661514282, 0.43814149498939514, 0.43227022886276245, 0.4312511086463928, 0.4287056028842926, 0.42604997754096985, 0.42127737402915955, 0.41980260610580444, 0.41752704977989197, 0.41494837403297424, 0.41295742988586426, 0.4100015163421631, 0.41063782572746277, 0.4076031446456909, 0.40379029512405396, 0.401368647813797, 0.40149059891700745, 0.4000053107738495, 0.3972853422164917, 0.4002949297428131, 0.395084410905838, 0.39568111300468445, 0.39538848400115967, 0.39048928022384644, 0.39473721385002136, 0.3877725899219513, 0.3897027373313904, 0.386309951543808, 0.38576099276542664, 0.3857100307941437, 0.38576728105545044, 0.3840937316417694, 0.38026049733161926, 0.38153570890426636, 0.37963300943374634, 0.37664300203323364, 0.3784782886505127, 0.3755680024623871, 0.3778355121612549, 0.3756447434425354, 0.37426993250846863, 0.37505415081977844, 0.37473106384277344, 0.3740309774875641, 0.3718336522579193, 0.37252718210220337, 0.37384849786758423, 0.3698734641075134, 0.36749476194381714, 0.37055084109306335, 0.3675037622451782, 0.3666693866252899, 0.3651109039783478, 0.36608490347862244, 0.3651939034461975, 0.3652445375919342, 0.36356812715530396, 0.3107755184173584, 0.2822781503200531, 0.27223533391952515, 0.26740947365760803, 0.2625541388988495, 0.25807875394821167, 0.2536429464817047, 0.25211232900619507, 0.25119253993034363, 0.24768184125423431, 0.2450164407491684, 0.2433348298072815, 0.24474574625492096, 0.24032975733280182, 0.241030752658844, 0.23807264864444733, 0.2349885255098343, 0.23458440601825714, 0.23338061571121216, 0.2277158796787262, 0.22681011259555817, 0.22505223751068115, 0.2232145518064499, 0.2250368297100067, 0.22486534714698792, 0.2277790755033493, 0.22746215760707855, 0.22536788880825043, 0.22542624175548553, 0.22725678980350494, 0.22474806010723114, 0.22561146318912506, 0.22363437712192535, 0.22476626932621002, 0.2250043898820877, 0.2275492548942566, 0.22554908692836761, 0.22749905288219452, 0.22577017545700073, 0.22589752078056335, 0.22662276029586792, 0.22395610809326172, 0.22641800343990326, 0.22575537860393524, 0.2236306369304657, 0.22634965181350708, 0.22518256306648254, 0.2244185507297516, 0.22673575580120087, 0.22628985345363617, 0.2248537838459015, 0.22479134798049927, 0.22605741024017334, 0.22532741725444794, 0.2261045128107071, 0.2245871126651764, 0.22749063372612, 0.2251328080892563, 0.22710973024368286, 0.22631923854351044, 0.22674985229969025, 0.22702978551387787, 0.22462676465511322, 0.2267305850982666, 0.22600677609443665, 0.2273537814617157, 0.22576119005680084, 0.22619904577732086, 0.22482018172740936, 0.22447074949741364, 0.2282869666814804, 0.22656002640724182, 0.2258046716451645, 0.2256801277399063], "moving_avg_accuracy_train": [0.058295896635059054, 0.11995175045854095, 0.1808444390731704, 0.23941166191903718, 0.29356343030285326, 0.34430186682683145, 0.39140740308157906, 0.43477397347386415, 0.4752222996983677, 0.5125881886146513, 0.5470543619666413, 0.5789621248286704, 0.6080928797461356, 0.6345173892694457, 0.6592457473570821, 0.6818406332156891, 0.7024479287038747, 0.7212781988468224, 0.7386972308908962, 0.7544487644924674, 0.769008794287453, 0.7820941478148262, 0.7946081823573562, 0.8058311056717775, 0.8155574597940608, 0.8249573896290696, 0.8335800508484255, 0.8414427164446461, 0.8485005863884059, 0.8550199719056192, 0.8617686863187394, 0.8672428211417842, 0.8731017469110499, 0.87740512095218, 0.8811806455344722, 0.8854992684406503, 0.8885515890241416, 0.8925655952599242, 0.8962129699578151, 0.8989563168573824, 0.9023344622515168, 0.9049120163955051, 0.9076573894548748, 0.9101865341726395, 0.9125532289292866, 0.9150877940543074, 0.9166364807918261, 0.9181000172710599, 0.9194102246559418, 0.9213217610796979, 0.9226191109730201, 0.9241239085032098, 0.9254687454410487, 0.9265885260279886, 0.9279588715264263, 0.9294804648785825, 0.930480308381265, 0.9307664003456025, 0.9322375026456214, 0.9331755921108951, 0.9340895549962894, 0.9352468709240783, 0.9357910176091253, 0.936345889841153, 0.9366546486964157, 0.9375415043613349, 0.9380280324216484, 0.9387286134425881, 0.9393407875126888, 0.9395777409400374, 0.9398631146865652, 0.9399083625167736, 0.9408791090389519, 0.9416714007005791, 0.9415846480543861, 0.9415972514763838, 0.9422944413573537, 0.9416387545467384, 0.9426132092242627, 0.9431323257614618, 0.9435437991711498, 0.9440137821993099, 0.9440345161806063, 0.9442997506840388, 0.9445710859180989, 0.9453057498322875, 0.9456787370491326, 0.9459841625609506, 0.9493351092501306, 0.952460279313259, 0.9554450654796168, 0.9583778388031483, 0.9610428753824126, 0.9635529433489701, 0.9658259554117291, 0.9679228195420218, 0.9698099972592851, 0.9714805193702891, 0.972997904114231, 0.9744403163433117, 0.9756664798340268, 0.9768722614256518, 0.9779598621045612, 0.9789154151786658, 0.9798498177072646, 0.9807070199758516, 0.9813669309235414, 0.9822421577335959, 0.9830693893924068, 0.9838511002662891, 0.984545339457545, 0.985186430771342, 0.9857448117632831, 0.9862264283167443, 0.9866831347029548, 0.9870918813505534, 0.9874480915405257, 0.9877826316043579, 0.9880953434058545, 0.9883256307533919, 0.9885817535399943, 0.9887610747253083, 0.9889433901313766, 0.9891446763777905, 0.9893188946019531, 0.9894454640691757, 0.9895616656896667, 0.9896639580481179, 0.9897815617588099, 0.98985252786629, 0.989939684899936, 0.9900227404790177, 0.9900766002097243, 0.9901296882161605, 0.9901798286195814, 0.9902528207195557, 0.9902696854845325, 0.9903011398146784, 0.9903155338677713, 0.9903494148548405, 0.9903496447598603, 0.9903568271208067, 0.9903749169897061, 0.9903958481693347, 0.9904263119750479, 0.9904514042513803, 0.990462361556032, 0.9904791985766469, 0.9905013273416289, 0.9905003168908271, 0.990511033229153, 0.9905346648753222, 0.9905140446294842, 0.990504823052287, 0.9905011378816098, 0.9905327345089618, 0.9905286193902454, 0.9905644072643438, 0.9905594500188987, 0.9905410015563222, 0.9905616003209557, 0.9905685134650782], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.05836931711219878, 0.11877126670745479, 0.17886459019672435, 0.23600357474821154, 0.2888345284556193, 0.33806092712549407, 0.3838743868414085, 0.42525592641404475, 0.4631555502636041, 0.4978877703219575, 0.5300674432615238, 0.5597209787264106, 0.5865118173654412, 0.6106754887915476, 0.6327910630298628, 0.6532474847765752, 0.6716369388120955, 0.6876065104146961, 0.7027024282267657, 0.7163742034763783, 0.7289016162895086, 0.7397683082467776, 0.750002050181889, 0.7595573033244681, 0.7676809569340394, 0.7752638883961324, 0.7820508761096064, 0.7885090509233446, 0.79408638613598, 0.7990898098341289, 0.804288691943713, 0.8086542739918116, 0.8127614028332177, 0.8160427797279833, 0.8188189434438145, 0.8216104595380626, 0.8238817719238647, 0.826550570682156, 0.829687999965597, 0.8314821776610553, 0.8338111224514256, 0.8353750638583914, 0.8373391340915282, 0.8388076514993934, 0.8402412394650414, 0.8417644317365341, 0.8424068538678506, 0.8432465289855836, 0.8442850573276125, 0.8453306256253482, 0.84600411191447, 0.8470627921669236, 0.8477705813331378, 0.8486486436817517, 0.8498224653075525, 0.8511555484548845, 0.8512291878865045, 0.8512884038870107, 0.851812624962918, 0.8522722168999847, 0.8534020935251218, 0.8544748701006969, 0.8547455977461242, 0.8550330802813763, 0.8552001882923952, 0.855845926240264, 0.8563763503236924, 0.8569218266429798, 0.8574626129639981, 0.8577895997380048, 0.8573951465413128, 0.8570899962979496, 0.8578713428183805, 0.8582073142406087, 0.8582512818470448, 0.8579856769115873, 0.8583712500807449, 0.8576541951969173, 0.858313968636563, 0.8587103932149247, 0.8584903563407215, 0.8594266946241945, 0.8595297706437027, 0.8598718272295583, 0.8599477445630784, 0.8606375992483368, 0.8610662444910484, 0.8613502509248503, 0.863762530284549, 0.8658003338731874, 0.8676720077053718, 0.8696372758730877, 0.8714416088091224, 0.8730065323126227, 0.874500412684523, 0.8758174024307546, 0.8770434323307514, 0.8780746465619083, 0.87906171550888, 0.8801118574933836, 0.8807721055260483, 0.8814151568804465, 0.8822034816479741, 0.8828163471974086, 0.8834747009471708, 0.8840347162628452, 0.8841836966320427, 0.8847104629816396, 0.8852700019150268, 0.8857980010175753, 0.886212165053619, 0.8867069829985582, 0.886968184171594, 0.8872276792898262, 0.8875477036236448, 0.8877502763053315, 0.8879814198438496, 0.8881772419972658, 0.8883423044127501, 0.8886119013905263, 0.8887202613267748, 0.8888544063631486, 0.8888774806458849, 0.8890203178128476, 0.8891743148342737, 0.8891887828237379, 0.8893126968041654, 0.8893631842303, 0.8893964158825712, 0.8894019103071154, 0.889455683414205, 0.8895651143668356, 0.889539472894384, 0.8896283178677468, 0.8896350361562733, 0.8896654966784471, 0.8896684970859036, 0.8896223693276144, 0.8896042388989945, 0.8895757144819867, 0.8896243142028393, 0.8897168820766065, 0.8896425312654066, 0.8897088633704171, 0.8897197341399267, 0.8897406953550756, 0.8897361758948692, 0.8898663857244336, 0.8898737112897913, 0.8898558902361133, 0.8898886794128033, 0.8899924613679837, 0.8900238004627365, 0.8900520056480141, 0.8901018043772638, 0.8900489669835887, 0.8900624484855311, 0.8900745818372792, 0.8900855018538525, 0.8901441579937684, 0.8902213625821928, 0.8901687763992746], "moving_var_accuracy_train": [0.03058570408036941, 0.06174013246865558, 0.08893739496225407, 0.11091473179292545, 0.1262149847854831, 0.1367629867750147, 0.1430570720101393, 0.1456772996574265, 0.14583417354095862, 0.14381664307739916, 0.1401262327194241, 0.13527655742521705, 0.12938630962124814, 0.12273197099105085, 0.11596219913533914, 0.10896073902447656, 0.10188661076806546, 0.0948891613541664, 0.08813104931492198, 0.08155094168064261, 0.07530379772125613, 0.06931445624155756, 0.0637924201621865, 0.058546764215460276, 0.05354350547452277, 0.0489843830551983, 0.04475509732821252, 0.04083598118789319, 0.03720070482239112, 0.03386315582785047, 0.030886746561134135, 0.028067767273568574, 0.025569933646739606, 0.0231796115353065, 0.02098994165461928, 0.019058802023409246, 0.017236771769567963, 0.01565810480715928, 0.014212024406124686, 0.012858555535414509, 0.01167540677860826, 0.01056766016903415, 0.009578727811246754, 0.00867842418715265, 0.007860992965077652, 0.007132709851926631, 0.006441024742232664, 0.00581619971924383, 0.0052500295378420355, 0.0047579123275519495, 0.004297269145508082, 0.0038879219714190593, 0.0035154070517815356, 0.003175151523669367, 0.0028745369923682284, 0.002607920510095339, 0.0023561256423545135, 0.0021212497156275884, 0.0019286020218589187, 0.0017436619262767425, 0.001576813687051975, 0.001431186739757203, 0.001290732926315111, 0.0011644305824284782, 0.0010488455124619586, 0.0009510395779493557, 0.0008580660061356723, 0.0007766767294242123, 0.0007023818703107246, 0.0006326490056202429, 0.0005701170486350845, 0.0005131237700668232, 0.0004702925323530325, 0.00042891281381148444, 0.00038608926662492943, 0.000347481769578651, 0.0003171082561919281, 0.0002892667573152704, 0.00026888613885068597, 0.00024442286277835917, 0.00022150436980244626, 0.00020134188924302783, 0.00018121156940054863, 0.00016372355653679376, 0.00014801380616629654, 0.00013807000515096563, 0.0001255150798112376, 0.00011380313451953751, 0.00020348241449111786, 0.00027103436435326725, 0.0003241114640478655, 0.0003691107519480404, 0.00039612145647259007, 0.00041321328159597685, 0.00041839120797340835, 0.0004161236398042361, 0.0004065642334526289, 0.0003910236072175436, 0.0003726433546461187, 0.000354103996528923, 0.00033222488902969155, 0.00031208758334703816, 0.00029152470214321486, 0.00027058996702577034, 0.0002513889430922617, 0.0002328632103464709, 0.00021349623144175205, 0.0001990408060189204, 0.00018529553537308054, 0.0001722656288488842, 0.00015937677845607686, 0.00014713808326410286, 0.00013523037892714275, 0.0001237949315755398, 0.00011329266492683402, 0.0001034670628314583, 9.426232784327371e-05, 8.584334854772639e-05, 7.813911173011139e-05, 7.080249091902293e-05, 6.43126317634739e-05, 5.817077337464819e-05, 5.2652846202791984e-05, 4.7752206959471146e-05, 4.3250154170197477e-05, 3.906931722347475e-05, 3.528391085057004e-05, 3.184969330489061e-05, 2.8789199669318502e-05, 2.5955605398084458e-05, 2.342841199490176e-05, 2.1147654858360954e-05, 1.9058997207850918e-05, 1.7178462514912163e-05, 1.5483242803917807e-05, 1.398286914345397e-05, 1.2587142011788105e-05, 1.133733218457362e-05, 1.0205463664996227e-05, 9.195248590059725e-06, 8.275724206760615e-06, 7.448616062863436e-06, 6.706699646788268e-06, 6.039972710635206e-06, 5.444327830698507e-06, 4.905561648612538e-06, 4.4160860464783275e-06, 3.977028809199195e-06, 3.5837330684359515e-06, 3.2253689506897634e-06, 2.9038656147848296e-06, 2.618505145612358e-06, 2.360481381896847e-06, 2.1251985810812006e-06, 1.9128009473193618e-06, 1.730505974327696e-06, 1.5576077847133773e-06, 1.41337395363438e-06, 1.2722577268125668e-06, 1.1480950660742248e-06, 1.0371043414066622e-06, 9.338240313209338e-07], "duration": 147913.419731, "accuracy_train": [0.5829589663505906, 0.6748544348698782, 0.7288786366048358, 0.7665166675318383, 0.7809293457571982, 0.8009477955426356, 0.8153572293743078, 0.8250731070044297, 0.8392572357189, 0.8488811888612033, 0.8572499221345515, 0.8661319905869325, 0.8702696740033223, 0.8723379749792359, 0.8818009701458103, 0.8851946059431525, 0.8879135880975452, 0.8907506301333518, 0.89546851928756, 0.8962125669066077, 0.900049062442322, 0.899862329561185, 0.9072344932401256, 0.9068374155015688, 0.9030946468946106, 0.9095567581441492, 0.9111840018226283, 0.9122067068106312, 0.9120214158822444, 0.9136944415605389, 0.9225071160368217, 0.9165100345491879, 0.9258320788344407, 0.9161354873223514, 0.9151603667751015, 0.9243668745962532, 0.9160224742755629, 0.9286916513819674, 0.9290393422388336, 0.9236464389534883, 0.9327377707987264, 0.928110003691399, 0.9323657469892026, 0.9329488366325213, 0.9338534817391103, 0.9378988801794942, 0.9305746614294942, 0.9312718455841639, 0.9312020911198781, 0.9385255888935032, 0.9342952600129198, 0.9376670862749169, 0.9375722778815985, 0.9366665513104466, 0.9402919810123662, 0.9431748050479882, 0.939478899905408, 0.93334122802464, 0.9454774233457919, 0.9416183972983574, 0.9423152209648394, 0.9456627142741787, 0.9406883377745479, 0.9413397399294019, 0.9394334783937799, 0.9455232053456073, 0.9424067849644703, 0.9450338426310447, 0.9448503541435955, 0.9417103217861758, 0.9424314784053157, 0.940315592988649, 0.9496158277385567, 0.9488020256552234, 0.940803874238649, 0.9417106822743633, 0.9485691502860835, 0.9357375732511997, 0.9513833013219823, 0.9478043745962532, 0.9472470598583426, 0.9482436294527501, 0.9442211220122739, 0.9466868612149317, 0.94701310302464, 0.9519177250599853, 0.9490356220007383, 0.9487329921673128, 0.9794936294527501, 0.9805868098814139, 0.9823081409768365, 0.9847727987149317, 0.9850282045957919, 0.9861435550479882, 0.9862830639765596, 0.9867945967146549, 0.9867945967146549, 0.9865152183693245, 0.9866543668097084, 0.9874220264050388, 0.9867019512504615, 0.9877242957502769, 0.9877482682147471, 0.9875153928456073, 0.9882594404646549, 0.9884218403931341, 0.9873061294527501, 0.9901191990240864, 0.9905144743217055, 0.9908864981312293, 0.9907934921788483, 0.990956252595515, 0.990770240690753, 0.9905609772978959, 0.9907934921788483, 0.9907706011789406, 0.9906539832502769, 0.9907934921788483, 0.9909097496193245, 0.9903982168812293, 0.9908868586194168, 0.9903749653931341, 0.9905842287859912, 0.990956252595515, 0.9908868586194168, 0.9905845892741787, 0.9906074802740864, 0.9905845892741787, 0.9908399951550388, 0.9904912228336102, 0.9907240982027501, 0.990770240690753, 0.9905613377860835, 0.9906074802740864, 0.9906310922503692, 0.9909097496193245, 0.9904214683693245, 0.9905842287859912, 0.9904450803456073, 0.9906543437384644, 0.9903517139050388, 0.9904214683693245, 0.9905377258098007, 0.9905842287859912, 0.9907004862264673, 0.9906772347383721, 0.9905609772978959, 0.9906307317621816, 0.9907004862264673, 0.9904912228336102, 0.9906074802740864, 0.9907473496908453, 0.9903284624169435, 0.9904218288575121, 0.990467971345515, 0.9908171041551311, 0.9904915833217978, 0.9908864981312293, 0.990514834809893, 0.9903749653931341, 0.9907469892026578, 0.9906307317621816], "end": "2016-02-03 02:53:35.569000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0], "moving_var_accuracy_valid": [0.030662794621299796, 0.06043207479334046, 0.08688973506588041, 0.1075845335594682, 0.12194606723022974, 0.13156060544125586, 0.13729440271740573, 0.13897684880228012, 0.13800659731349493, 0.13506288157378246, 0.13087637557088133, 0.12570272750389888, 0.11959219606835218, 0.11288792361261649, 0.10600101886636867, 0.09916710369584616, 0.09229394150378215, 0.08535979230793925, 0.07887479368844415, 0.07266957126588301, 0.06681503878540992, 0.06119629985371652, 0.05601923513429498, 0.0512390373844344, 0.04670907737770567, 0.0425556772859643, 0.03871467837737351, 0.03521858273695938, 0.03197668447593037, 0.029004324266666125, 0.026347147216703695, 0.023883957254601452, 0.021647378095020504, 0.019579547194447967, 0.017690956239797048, 0.015991993674757357, 0.014439224046866707, 0.013059404023490349, 0.011842054783718654, 0.01068682096777271, 0.00966695472552478, 0.008722272467492102, 0.007884763367669138, 0.007115695921297053, 0.006422622899264606, 0.005801241641601565, 0.005224831833194655, 0.004708694138605249, 0.004247531594799502, 0.003832617352906622, 0.003453437871650675, 0.0031181813193780243, 0.0028108718769745147, 0.0025367236306695438, 0.0022954519824853665, 0.0020819007803361375, 0.0018737595071955278, 0.0016864151152884185, 0.0015202468733874059, 0.001370123208786215, 0.0012446004785998737, 0.0011304980769698116, 0.0010181079103948178, 0.0009170409352280104, 0.00082558816749133, 0.0007467821482180586, 0.0006746360807707806, 0.0006098503724278324, 0.0005514973837900536, 0.0004973099285644268, 0.0004489792756274093, 0.00040491939810388997, 0.00036992197975840694, 0.0003339456729515527, 0.00030056850401013894, 0.00027114656744477953, 0.00024536991071926926, 0.0002254604290051302, 0.00020683209502957343, 0.0001875632575435799, 0.0001692426778233044, 0.00016020897447084905, 0.00014428369901594315, 0.00013090835348569376, 0.00011786938911088358, 0.00011036554558075229, 0.00010098262171956985, 9.161029643758012e-05, 0.00013482109217687743, 0.00015871277415200035, 0.00017436996314355044, 0.0001916934775685298, 0.00020182468590820997, 0.0002036830874636612, 0.0002033998858072381, 0.00019867005515162255, 0.00019233139347763634, 0.00018266887924473519, 0.00017317073727494633, 0.00016577884723600558, 0.00015312430969414292, 0.0001415335141242687, 0.00013297326616371936, 0.00012305637718250048, 0.0001146516064026834, 0.00010600900014652508, 9.560785648552833e-05, 8.854441592058469e-05, 8.250772869031149e-05, 7.67660032919088e-05, 7.06331896014858e-05, 6.57734738290429e-05, 5.981016092129589e-05, 5.4435184276643285e-05, 4.9913406017103254e-05, 4.52913866376847e-05, 4.124309399250405e-05, 3.746390143517085e-05, 3.396272170070325e-05, 3.122059230446748e-05, 2.8204209956074898e-05, 2.554574297752077e-05, 2.2995960482482867e-05, 2.0879986540627996e-05, 1.9005423630038138e-05, 1.710676517150655e-05, 1.5534280725264185e-05, 1.4003793474517102e-05, 1.261335321147946e-05, 1.1352289588641149e-05, 1.024308455319173e-05, 9.326552298415545e-06, 8.399814434559381e-06, 7.630873854729993e-06, 6.8681926878635255e-06, 6.189724009777124e-06, 5.570832630803557e-06, 5.032899298486241e-06, 4.532567780615078e-06, 4.086633783844327e-06, 3.6992278012623935e-06, 3.4064243224203156e-06, 3.115534278312904e-06, 2.8435803838777886e-06, 2.5602859081575668e-06, 2.3082116702064782e-06, 2.0775743328708412e-06, 2.022408297020195e-06, 1.8206504424884858e-06, 1.6414437078273595e-06, 1.4869755080166196e-06, 1.4352142052047776e-06, 1.3005320344236448e-06, 1.1776386232701493e-06, 1.082193981857188e-06, 9.99100695204942e-07, 9.008263837360384e-07, 8.120687093842227e-07, 7.319350593034485e-07, 6.897064381216677e-07, 6.743807305734817e-07, 6.318304172211625e-07], "accuracy_test": 0.8766302614795919, "start": "2016-02-01 09:48:22.149000", "learning_rate_per_epoch": [0.003041443880647421, 0.003041443880647421, 0.003041443880647421, 0.003041443880647421, 0.003041443880647421, 0.003041443880647421, 0.003041443880647421, 0.003041443880647421, 0.003041443880647421, 0.003041443880647421, 0.003041443880647421, 0.003041443880647421, 0.003041443880647421, 0.003041443880647421, 0.003041443880647421, 0.003041443880647421, 0.003041443880647421, 0.003041443880647421, 0.003041443880647421, 0.003041443880647421, 0.003041443880647421, 0.003041443880647421, 0.003041443880647421, 0.003041443880647421, 0.003041443880647421, 0.003041443880647421, 0.003041443880647421, 0.003041443880647421, 0.003041443880647421, 0.003041443880647421, 0.003041443880647421, 0.003041443880647421, 0.003041443880647421, 0.003041443880647421, 0.003041443880647421, 0.003041443880647421, 0.003041443880647421, 0.003041443880647421, 0.003041443880647421, 0.003041443880647421, 0.003041443880647421, 0.003041443880647421, 0.003041443880647421, 0.003041443880647421, 0.003041443880647421, 0.003041443880647421, 0.003041443880647421, 0.003041443880647421, 0.003041443880647421, 0.003041443880647421, 0.003041443880647421, 0.003041443880647421, 0.003041443880647421, 0.003041443880647421, 0.003041443880647421, 0.003041443880647421, 0.003041443880647421, 0.003041443880647421, 0.003041443880647421, 0.003041443880647421, 0.003041443880647421, 0.003041443880647421, 0.003041443880647421, 0.003041443880647421, 0.003041443880647421, 0.003041443880647421, 0.003041443880647421, 0.003041443880647421, 0.003041443880647421, 0.003041443880647421, 0.003041443880647421, 0.003041443880647421, 0.003041443880647421, 0.003041443880647421, 0.003041443880647421, 0.003041443880647421, 0.003041443880647421, 0.003041443880647421, 0.003041443880647421, 0.003041443880647421, 0.003041443880647421, 0.003041443880647421, 0.003041443880647421, 0.003041443880647421, 0.003041443880647421, 0.003041443880647421, 0.003041443880647421, 0.00030414439970627427, 0.00030414439970627427, 0.00030414439970627427, 0.00030414439970627427, 0.00030414439970627427, 0.00030414439970627427, 0.00030414439970627427, 0.00030414439970627427, 0.00030414439970627427, 0.00030414439970627427, 0.00030414439970627427, 0.00030414439970627427, 0.00030414439970627427, 0.00030414439970627427, 0.00030414439970627427, 0.00030414439970627427, 0.00030414439970627427, 0.00030414439970627427, 0.00030414439970627427, 3.0414439606829546e-05, 3.0414439606829546e-06, 3.0414440743697924e-07, 3.0414440743697924e-08, 3.0414439855519504e-09, 3.0414440410631016e-10, 3.04144417984098e-11, 3.0414442665771535e-12, 3.041444320787262e-13, 3.0414442530246263e-14, 3.0414441683213316e-15, 3.0414441153817724e-16, 3.0414441815562214e-17, 3.0414442642742827e-18, 3.0414443676718592e-19, 3.0414442384248885e-20, 3.041444157645532e-21, 3.0414442586197277e-22, 3.0414443848374725e-23, 3.0414443453944273e-24, 3.0414443453944273e-25, 3.041444283764669e-26, 3.041444360801867e-27, 3.041444457098364e-28, 3.041444517283675e-29, 3.041444554899494e-30, 3.041444460859946e-31, 3.0414444020852284e-32, 3.0414443286168315e-33, 3.0414442367813354e-34, 3.041444121986965e-35, 3.0414441937334465e-36, 3.041444104050345e-37, 3.041444047998406e-38, 3.041444047998406e-39, 3.0414482518937992e-40, 3.041378186970583e-41, 3.040817667584853e-42, 3.040817667584853e-43, 3.0828566215145976e-44, 2.802596928649634e-45, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "accuracy_train_first": 0.5829589663505906, "accuracy_train_last": 0.9906307317621816, "batch_size_eval": 1024, "accuracy_train_std": [0.019821262415775306, 0.018136574690584217, 0.02068669589753401, 0.020970853847952574, 0.019995641320601287, 0.01816618245969271, 0.01894170930528675, 0.020056320427274513, 0.017945455580056585, 0.01856762998613693, 0.01940838399655795, 0.017433786504104834, 0.019343980204892315, 0.01878009271112079, 0.017896087777022306, 0.018376416837123852, 0.019404957879403165, 0.01818687432393675, 0.017831788890122848, 0.018554548423218676, 0.01647553142321801, 0.01782799708368691, 0.016438620858986147, 0.01795676343749879, 0.01794362462581775, 0.017854964443685967, 0.018900318242003618, 0.01849718897984174, 0.018004792271073763, 0.01827762399770051, 0.018383967170485037, 0.017020089828767664, 0.015186716633677294, 0.01930372051881191, 0.01671742349632354, 0.01613640844054372, 0.016341268883283716, 0.017232680649421767, 0.016610727347005666, 0.017748247919637715, 0.015756067106769557, 0.015846179651458132, 0.0160981215296014, 0.015443510828094338, 0.014956571312931937, 0.015505129573021477, 0.015428661749322241, 0.016831487912741354, 0.015839377023308225, 0.015239904856617147, 0.015224628697295715, 0.014521311612396266, 0.015266025569226538, 0.01698226631623895, 0.016493899468864454, 0.015244495813164213, 0.015176345263819604, 0.0162127895517917, 0.013908974614317724, 0.013977332197387208, 0.01509845667303855, 0.01507136534571888, 0.015237439034630443, 0.013852168717555667, 0.013678541813123998, 0.014829457292391355, 0.016256303022604827, 0.015746780975664156, 0.014571140782106761, 0.014144260954848057, 0.013821073317866339, 0.01662858857266512, 0.012751148699207146, 0.013956947111598055, 0.01425003754572396, 0.014590860060261607, 0.0151174881587028, 0.012876951417850153, 0.014503647782481905, 0.012370245458331496, 0.011342940405751688, 0.012675251317895642, 0.013893051784056112, 0.012931832214678984, 0.012363907936434947, 0.012865612728909522, 0.01437419114323437, 0.011729274407109897, 0.006944059103992492, 0.0059247778999070985, 0.005018823514509254, 0.00451677889511584, 0.004015068236600189, 0.004538771816650261, 0.004497084345528417, 0.00432858365875593, 0.004447599007625029, 0.004124541378620734, 0.004283149907934038, 0.003961221971747228, 0.0042633245496999025, 0.0041181899323607775, 0.003920192309843019, 0.004335145515213346, 0.00429782350841176, 0.0035006404844227365, 0.0037600038980713695, 0.0028622177368341087, 0.0026655780495082704, 0.0026430215101633193, 0.0026727566900997487, 0.0025856065509811646, 0.002789653141860319, 0.0028657319264368693, 0.00260390549424991, 0.0026038265472547325, 0.002698178878513399, 0.002853544410652343, 0.002808142480378405, 0.002771131181825313, 0.002784660906711947, 0.002768309953553741, 0.0027372958174053702, 0.00266347119388418, 0.0026595376337024495, 0.002834557226410415, 0.002679700283715908, 0.0027033508220020786, 0.0029918890591219363, 0.00299282812690495, 0.002764969713433786, 0.0027320822119930917, 0.0026682376691530515, 0.002619714139318455, 0.0028042362494689176, 0.0030633797118124573, 0.0028143880515766073, 0.002794759351140455, 0.002606911802702995, 0.002638029009679644, 0.0028541854174446548, 0.002630911006812072, 0.002742838739290108, 0.0029373653091052925, 0.0027818203317726106, 0.0027896891972358433, 0.0026942058491869767, 0.002954598642089735, 0.0027407041147912005, 0.002884655494090971, 0.0027713421477856626, 0.0025690366652959215, 0.0028109639857154412, 0.0026985336942952263, 0.0025482173843292977, 0.0027385945301545036, 0.002771730690860475, 0.0027852441129725407, 0.0026220718604739817, 0.002857111073880546, 0.0026905166950003745, 0.002977564831182781], "accuracy_test_std": 0.005591512401579017, "error_valid": [0.41630682887801207, 0.33761118693524095, 0.28029549839984935, 0.24974556428840367, 0.23568688817771077, 0.21890148484563254, 0.20380447571536142, 0.20231021743222888, 0.19574783509036142, 0.18952224915286142, 0.18031550028237953, 0.1733972020896084, 0.1723706348832832, 0.17185146837349397, 0.16816876882530118, 0.16264471950301207, 0.16285797486822284, 0.16866734516189763, 0.1614343114646084, 0.1605798192771084, 0.1583516683923193, 0.16243146413780118, 0.1578942724021084, 0.1544454183923193, 0.1592061605798193, 0.15648972844503017, 0.1568662344691265, 0.15336737575301207, 0.15571759695030118, 0.15587937688253017, 0.14892136907003017, 0.15205548757530118, 0.1502744375941265, 0.1544248282191265, 0.15619558311370485, 0.15326589561370485, 0.15567641660391573, 0.14943024049322284, 0.14207513648343373, 0.1523702230798193, 0.14522837443524095, 0.15054946347891573, 0.14498423381024095, 0.1479756918298193, 0.1468564688441265, 0.14452683782003017, 0.15181134695030118, 0.1491963949548193, 0.1463681875941265, 0.14525925969503017, 0.14793451148343373, 0.14340908556099397, 0.14585931617093373, 0.14344879518072284, 0.13961314006024095, 0.1368467032191265, 0.14810805722891573, 0.14817865210843373, 0.14346938535391573, 0.14359145566641573, 0.1364290168486446, 0.1358701407191265, 0.14281785344503017, 0.1423795769013554, 0.14329583960843373, 0.13834243222891573, 0.13884983292545183, 0.13816888648343373, 0.13767031014683728, 0.13926751929593373, 0.14615493222891573, 0.1456563558923193, 0.13509653849774095, 0.13876894295933728, 0.14135300969503017, 0.14440476750753017, 0.13815859139683728, 0.14879929875753017, 0.1357480704066265, 0.1377217855798193, 0.1434899755271084, 0.13214626082454817, 0.13954254518072284, 0.13704966349774095, 0.13936899943524095, 0.13315370858433728, 0.13507594832454817, 0.13609369117093373, 0.11452695547816272, 0.11585943382906627, 0.11548292780496983, 0.11267531061746983, 0.11231939476656627, 0.11290915615587349, 0.11205466396837349, 0.11232968985316272, 0.11192229856927716, 0.11264442535768071, 0.11205466396837349, 0.11043686464608427, 0.11328566217996983, 0.11279738092996983, 0.11070159544427716, 0.11166786285768071, 0.11060011530496983, 0.11092514589608427, 0.11447548004518071, 0.11054863987198793, 0.10969414768448793, 0.10945000705948793, 0.11006035862198793, 0.10883965549698793, 0.11068100527108427, 0.11043686464608427, 0.10957207737198793, 0.11042656955948793, 0.10993828830948793, 0.11006035862198793, 0.1101721338478916, 0.10896172580948793, 0.11030449924698793, 0.10993828830948793, 0.11091485080948793, 0.10969414768448793, 0.1094397119728916, 0.11068100527108427, 0.10957207737198793, 0.11018242893448793, 0.11030449924698793, 0.11054863987198793, 0.11006035862198793, 0.10945000705948793, 0.11069130035768071, 0.10957207737198793, 0.11030449924698793, 0.11006035862198793, 0.11030449924698793, 0.11079278049698793, 0.11055893495858427, 0.11068100527108427, 0.10993828830948793, 0.10945000705948793, 0.1110266260353916, 0.10969414768448793, 0.11018242893448793, 0.11007065370858427, 0.11030449924698793, 0.10896172580948793, 0.11006035862198793, 0.11030449924698793, 0.10981621799698793, 0.1090735010353916, 0.10969414768448793, 0.10969414768448793, 0.10945000705948793, 0.11042656955948793, 0.10981621799698793, 0.10981621799698793, 0.10981621799698793, 0.10932793674698793, 0.10908379612198793, 0.11030449924698793], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.02863177041345615, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 16, "valid_ratio": 0.15, "learning_rate": 0.003041443886528384, "optimization": "adam", "nb_data_augmentation": 1, "learning_rate_decay_method": "discrete", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 1.5867958177100755e-06, "rotation_range": [0, 0], "momentum": 0.7290820028340178}, "accuracy_valid_max": 0.8911603445030121, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8896955007530121, "accuracy_valid_std": [0.015790922195867873, 0.020724148917578664, 0.015702497659803926, 0.008829456864290425, 0.011519115750530254, 0.005639524521683884, 0.009068009196125302, 0.014069367854333343, 0.011570969988154357, 0.011104901284526547, 0.008822022032598804, 0.01078961632343523, 0.011021683950136868, 0.009911222465062819, 0.012588004354889036, 0.00553632150498314, 0.011353671345523626, 0.010003383995116803, 0.009402290870793507, 0.009256607297467927, 0.0133068603254553, 0.009400960142887795, 0.011479077613920802, 0.01314341658984964, 0.010720734627190615, 0.01095429114020808, 0.010656732211321735, 0.010722434539061432, 0.011072922356437096, 0.013710766096396319, 0.01249915745661985, 0.011405904622809314, 0.009240719541860372, 0.011849757664305217, 0.011208862316895041, 0.00826666227026969, 0.008953829785238048, 0.006473044625838844, 0.009022349336758394, 0.009637673015269358, 0.01092346737587706, 0.0101292548453142, 0.008424104553066832, 0.0059914068229402736, 0.008109351395608554, 0.008831130406383237, 0.009106362742598151, 0.007143634130690929, 0.010117538088663204, 0.006585212889064208, 0.013284040530538893, 0.013852387603015397, 0.008182644007098986, 0.008052820340387824, 0.010664901716482088, 0.008283789395571122, 0.009662494600742914, 0.010928461421877018, 0.008520849052635133, 0.00926515106601046, 0.01167116921986551, 0.010629135078409348, 0.01205763811043676, 0.012575626881653958, 0.007908477829090824, 0.008660741528966611, 0.012478874226902787, 0.008342983678425788, 0.01166193085872052, 0.008707564201611702, 0.006082945364901517, 0.0099771441714626, 0.007447863035543364, 0.006497919369080199, 0.010215953939220883, 0.00632021538669989, 0.008322730655599206, 0.008446060893200455, 0.01065790300108424, 0.008961522149160487, 0.012050865348011275, 0.009648838390587878, 0.008520780512245791, 0.013881614726092582, 0.008621149769472865, 0.01235668232066657, 0.012399493912354538, 0.011916739597846561, 0.008825839244220689, 0.010556214011560192, 0.010988915964428192, 0.01030932682847365, 0.010310551647468412, 0.012651158813429882, 0.012620689612942474, 0.009423043706333348, 0.011649584163168707, 0.012571309570548637, 0.011070923408848668, 0.01279568671528069, 0.00975120821425887, 0.00970240671642481, 0.010370145697481783, 0.011247882762686474, 0.010570688518844162, 0.010327838846210847, 0.013029680676238926, 0.011994891366912048, 0.012210114690853762, 0.01228218465921494, 0.012199928231638677, 0.011739493104184226, 0.011963458723529045, 0.011603556862670174, 0.011719564704042547, 0.012526150442040564, 0.012201289369927716, 0.01208210353820541, 0.01270350687342007, 0.011005588884602707, 0.012319977944588147, 0.011641306707540334, 0.012646368543713333, 0.01173212721406173, 0.012299653879499195, 0.010756682017839123, 0.012168690681902065, 0.012294690679077116, 0.012559550988497854, 0.011621356696929295, 0.012575236516348128, 0.012398108039224112, 0.010813251973622689, 0.012020846651766537, 0.012569038929519591, 0.012594181616933214, 0.011996417942235374, 0.012370121650924285, 0.011891246128525046, 0.011360354240189477, 0.012670989411434595, 0.011797014437012367, 0.01319802265066816, 0.012801127007732551, 0.012197345330159759, 0.011626131988757308, 0.01265411027388702, 0.011336399898073423, 0.011983031340896177, 0.011714876150858065, 0.011287922806201299, 0.012785668867641833, 0.011752431498423251, 0.012594587882414627, 0.011654685303330001, 0.012854924938138989, 0.011763692274002719, 0.011954681464612973, 0.012269629311907143, 0.012267756243241525, 0.010917070331938946, 0.011986476749474185], "accuracy_valid": [0.5836931711219879, 0.662388813064759, 0.7197045016001506, 0.7502544357115963, 0.7643131118222892, 0.7810985151543675, 0.7961955242846386, 0.7976897825677711, 0.8042521649096386, 0.8104777508471386, 0.8196844997176205, 0.8266027979103916, 0.8276293651167168, 0.828148531626506, 0.8318312311746988, 0.8373552804969879, 0.8371420251317772, 0.8313326548381024, 0.8385656885353916, 0.8394201807228916, 0.8416483316076807, 0.8375685358621988, 0.8421057275978916, 0.8455545816076807, 0.8407938394201807, 0.8435102715549698, 0.8431337655308735, 0.8466326242469879, 0.8442824030496988, 0.8441206231174698, 0.8510786309299698, 0.8479445124246988, 0.8497255624058735, 0.8455751717808735, 0.8438044168862951, 0.8467341043862951, 0.8443235833960843, 0.8505697595067772, 0.8579248635165663, 0.8476297769201807, 0.854771625564759, 0.8494505365210843, 0.855015766189759, 0.8520243081701807, 0.8531435311558735, 0.8554731621799698, 0.8481886530496988, 0.8508036050451807, 0.8536318124058735, 0.8547407403049698, 0.8520654885165663, 0.856590914439006, 0.8541406838290663, 0.8565512048192772, 0.860386859939759, 0.8631532967808735, 0.8518919427710843, 0.8518213478915663, 0.8565306146460843, 0.8564085443335843, 0.8635709831513554, 0.8641298592808735, 0.8571821465549698, 0.8576204230986446, 0.8567041603915663, 0.8616575677710843, 0.8611501670745482, 0.8618311135165663, 0.8623296898531627, 0.8607324807040663, 0.8538450677710843, 0.8543436441076807, 0.864903461502259, 0.8612310570406627, 0.8586469903049698, 0.8555952324924698, 0.8618414086031627, 0.8512007012424698, 0.8642519295933735, 0.8622782144201807, 0.8565100244728916, 0.8678537391754518, 0.8604574548192772, 0.862950336502259, 0.860631000564759, 0.8668462914156627, 0.8649240516754518, 0.8639063088290663, 0.8854730445218373, 0.8841405661709337, 0.8845170721950302, 0.8873246893825302, 0.8876806052334337, 0.8870908438441265, 0.8879453360316265, 0.8876703101468373, 0.8880777014307228, 0.8873555746423193, 0.8879453360316265, 0.8895631353539157, 0.8867143378200302, 0.8872026190700302, 0.8892984045557228, 0.8883321371423193, 0.8893998846950302, 0.8890748541039157, 0.8855245199548193, 0.8894513601280121, 0.8903058523155121, 0.8905499929405121, 0.8899396413780121, 0.8911603445030121, 0.8893189947289157, 0.8895631353539157, 0.8904279226280121, 0.8895734304405121, 0.8900617116905121, 0.8899396413780121, 0.8898278661521084, 0.8910382741905121, 0.8896955007530121, 0.8900617116905121, 0.8890851491905121, 0.8903058523155121, 0.8905602880271084, 0.8893189947289157, 0.8904279226280121, 0.8898175710655121, 0.8896955007530121, 0.8894513601280121, 0.8899396413780121, 0.8905499929405121, 0.8893086996423193, 0.8904279226280121, 0.8896955007530121, 0.8899396413780121, 0.8896955007530121, 0.8892072195030121, 0.8894410650414157, 0.8893189947289157, 0.8900617116905121, 0.8905499929405121, 0.8889733739646084, 0.8903058523155121, 0.8898175710655121, 0.8899293462914157, 0.8896955007530121, 0.8910382741905121, 0.8899396413780121, 0.8896955007530121, 0.8901837820030121, 0.8909264989646084, 0.8903058523155121, 0.8903058523155121, 0.8905499929405121, 0.8895734304405121, 0.8901837820030121, 0.8901837820030121, 0.8901837820030121, 0.8906720632530121, 0.8909162038780121, 0.8896955007530121], "seed": 139662169, "model": "residualv4", "loss_std": [0.31716716289520264, 0.26304513216018677, 0.26107069849967957, 0.2529051601886749, 0.2483474612236023, 0.2444196343421936, 0.2405347228050232, 0.2362687736749649, 0.23194585740566254, 0.2292412370443344, 0.22673451900482178, 0.22238770127296448, 0.21600975096225739, 0.2134339064359665, 0.21188990771770477, 0.2081984132528305, 0.20809406042099, 0.204599991440773, 0.202913299202919, 0.1993517130613327, 0.19948507845401764, 0.19919757544994354, 0.1925155073404312, 0.19181542098522186, 0.18737533688545227, 0.1866590529680252, 0.18382613360881805, 0.18217229843139648, 0.17951476573944092, 0.17316046357154846, 0.17732399702072144, 0.17403608560562134, 0.17883452773094177, 0.16899672150611877, 0.16890187561511993, 0.16668780148029327, 0.16490299999713898, 0.1629413515329361, 0.16607366502285004, 0.16734066605567932, 0.1621689349412918, 0.1633904129266739, 0.15857750177383423, 0.15916980803012848, 0.15727223455905914, 0.1588974893093109, 0.15572308003902435, 0.15372717380523682, 0.15414300560951233, 0.15246734023094177, 0.15416336059570312, 0.1537051796913147, 0.15068171918392181, 0.1524246484041214, 0.15060840547084808, 0.1474747657775879, 0.14696791768074036, 0.14828254282474518, 0.1479177325963974, 0.14955846965312958, 0.14969100058078766, 0.14579066634178162, 0.14605046808719635, 0.14014095067977905, 0.1443021148443222, 0.1430666446685791, 0.14312158524990082, 0.14101874828338623, 0.144265815615654, 0.14074589312076569, 0.1416754424571991, 0.14274507761001587, 0.1414179503917694, 0.13996101915836334, 0.14215825498104095, 0.1362271010875702, 0.13862395286560059, 0.1414860635995865, 0.1410866528749466, 0.1363200843334198, 0.13697734475135803, 0.1345224231481552, 0.13740120828151703, 0.13521158695220947, 0.13394373655319214, 0.1334870159626007, 0.1352468729019165, 0.1336589753627777, 0.11989752948284149, 0.10047096759080887, 0.09665022790431976, 0.09220453351736069, 0.09174854308366776, 0.09121207892894745, 0.0884559229016304, 0.0889468714594841, 0.09038535505533218, 0.08733390271663666, 0.08642619848251343, 0.08689024299383163, 0.08770044147968292, 0.08488579839468002, 0.08788391947746277, 0.08465342223644257, 0.08173646777868271, 0.084949791431427, 0.08544677495956421, 0.08217213302850723, 0.08141004294157028, 0.0815165564417839, 0.07882353663444519, 0.08006320893764496, 0.0786265954375267, 0.08390571177005768, 0.08129824697971344, 0.08008532971143723, 0.08008455485105515, 0.08097858726978302, 0.08035705983638763, 0.0819338783621788, 0.08009658753871918, 0.08081353455781937, 0.08121655136346817, 0.08093313872814178, 0.0795445665717125, 0.08284540474414825, 0.08058810979127884, 0.08030320703983307, 0.08164067566394806, 0.07897964119911194, 0.08015479892492294, 0.08130483329296112, 0.07966580241918564, 0.08181826770305634, 0.08115269988775253, 0.07881739735603333, 0.08224646002054214, 0.08140657097101212, 0.07912728935480118, 0.07975611835718155, 0.07942397892475128, 0.08205990493297577, 0.07735943049192429, 0.07720138132572174, 0.08153150230646133, 0.08010691404342651, 0.08123462647199631, 0.08052856475114822, 0.08246144652366638, 0.08163752406835556, 0.0795140489935875, 0.08016248792409897, 0.08015211671590805, 0.0793917328119278, 0.07911019027233124, 0.08113599568605423, 0.08053694665431976, 0.07922685146331787, 0.08341650664806366, 0.08166403323411942, 0.0794813260436058, 0.0797911211848259]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:33 2016", "state": "available"}], "summary": "dd1cc227d77b47e5f3ea3dec699bad8c"}