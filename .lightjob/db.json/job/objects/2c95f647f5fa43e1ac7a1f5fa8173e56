{"content": {"hp_model": {"f0": 16, "f1": 16, "f2": 16, "f3": 64, "nonlin": "very_leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.015899963170301536, 0.011374708260105245, 0.009298434891957938, 0.010537998499867427, 0.009453762194133172, 0.011981843808046686, 0.010604966715302729, 0.010573885622353452, 0.010977286227772323, 0.012134689623206097, 0.01091379707276631, 0.01547065856925765, 0.013674575389946857, 0.01104630002971283, 0.010293456061712818, 0.01118209242307044, 0.010291344282544577, 0.011432355446127562, 0.01184225122060842, 0.010526997044406272, 0.010583466335206354, 0.009555348552283486, 0.01158669151161705, 0.010920573165790372, 0.010183951023027665, 0.01264629807497111, 0.010757156574442173, 0.01319805018432449, 0.009936204475258042, 0.009199931078675498, 0.008305099018522501, 0.01170517187472883, 0.011287615820381946, 0.008953806956208194, 0.009892397325896125, 0.008343503616555758, 0.010186687173670694, 0.008397176795037773, 0.010282854229109913, 0.008943681677068004, 0.0068270160308559775, 0.0075784703687931535, 0.011142695711074624, 0.0095421067372854, 0.005136616387644479, 0.006533532467878904, 0.005849061238066743, 0.009264202238297888, 0.009898871812447346, 0.008056259376137829, 0.008224414845922918, 0.007442690678871984, 0.0074410164867642304, 0.009817356677593004, 0.009385267088956258, 0.010038355345221985, 0.01104364954458737, 0.010909924390424841, 0.0075061439358966656, 0.00987459151825065, 0.008808587007120575, 0.009401556624010933, 0.009021514995702822, 0.008185993095163763, 0.007728260936448749, 0.009044217401580663, 0.006271668054615054, 0.00949065085146392, 0.008668825722403782, 0.010497537012084628, 0.00818705961569538, 0.011732925964210869, 0.008347169615340523, 0.006950466679267094, 0.007718238975121676, 0.009703959245168156, 0.009574823962879765, 0.009085615422317181, 0.006664736063463613, 0.008469710382295003, 0.008287973650056572, 0.006246986914311961, 0.009861881898971475, 0.008044024595395943, 0.006187766363859255, 0.004816581075368024, 0.006235149606872671, 0.007179491630634774, 0.007604283767935017, 0.005204693637405588, 0.005926245203387345, 0.007109085564916197, 0.00922444448841366, 0.0050215493010078455, 0.006642934816969754, 0.005599462391147884, 0.008149216643399565, 0.00910338208112158, 0.006583495430277641, 0.0069513741009094125, 0.007259846293687977, 0.009929520374369148, 0.005159991394619978, 0.008312542769729292, 0.00552930322280616, 0.005525925394060128, 0.006437662732282203, 0.008283811329356528, 0.009335981443758894, 0.00801813790317975, 0.005640352927794091, 0.006128584861095355, 0.0061581991120520105, 0.007749661525337304, 0.00707730156108574, 0.00864035776433092, 0.005318163859436747, 0.008602118028313998, 0.010139040235674145, 0.005452746599713657, 0.007760363171703863, 0.007613356144263927, 0.005904636191465374, 0.0053671277269871556, 0.0059046361914653735, 0.008470466288132859, 0.009330230445169818, 0.00604166777363228, 0.009368241925811373, 0.005954367420568396, 0.004959009173986864, 0.004892024866993034, 0.006580658736894861, 0.007482002093147779, 0.006783473486864918, 0.005722763352478798, 0.006727421402011336, 0.006634690347796862, 0.011415977648154366, 0.007646694583800998, 0.009246305731750528, 0.008615548262858447, 0.007556855042003232, 0.007260279046301542, 0.008732215320173875, 0.008970567366549498, 0.008403486562283524, 0.007237555560777228, 0.008242537802925219, 0.010654328731086973, 0.008208321925318368, 0.007402954744922383], "moving_avg_accuracy_train": [0.05633858997208378, 0.11688612646790789, 0.17784130895423816, 0.23714840608345444, 0.29313067248530644, 0.3449167048814787, 0.39364688675467263, 0.43829202354524444, 0.4795281200210079, 0.5175077431598725, 0.5525938147741177, 0.5847661928828078, 0.6144560441556197, 0.64147220419996, 0.6664517047505712, 0.6895353966901505, 0.7105618355072004, 0.7300785794377926, 0.7478597075705176, 0.7643579795863986, 0.7793482584780725, 0.793153368521046, 0.8059196923370846, 0.817504823019166, 0.8283986152996599, 0.8382913840068663, 0.847573803001667, 0.8561442910339108, 0.8640923900485984, 0.8712922542356452, 0.8778255383289688, 0.8840380623903594, 0.8897316045444114, 0.8950371540902009, 0.8999190334290121, 0.9045032788898669, 0.9086687354808544, 0.9126710515841624, 0.9161939728711597, 0.9194274891937705, 0.9225911311531771, 0.9255335958225587, 0.9283401043881433, 0.9308657818530757, 0.9333715145989235, 0.9355426803224938, 0.9375458459892569, 0.9395462606405158, 0.9414907209552018, 0.9432291094943716, 0.9448633054974538, 0.9463875963716656, 0.9479315912679985, 0.9494303605222895, 0.950732858021417, 0.952044362357472, 0.9531806826278156, 0.9542893653282585, 0.9554592407705619, 0.9565398862590742, 0.9575171174963543, 0.9584246355420769, 0.9594622548713133, 0.9602960948199978, 0.9612093472392992, 0.9620033005333186, 0.9627805654181558, 0.9635360155323942, 0.9642065839911519, 0.9648286607456912, 0.965435176996242, 0.9660761564300158, 0.9666460624739835, 0.9671334733742872, 0.9676047313167127, 0.9681403624612964, 0.9685341108854787, 0.9690605094303287, 0.9695994083361792, 0.9700681052609592, 0.9704574885563841, 0.9708962531282097, 0.9713377523654995, 0.9717001523492799, 0.9720983198501401, 0.9724590678473612, 0.9728651212531936, 0.9731794520934519, 0.9737016959817903, 0.9739670302884194, 0.974266429228708, 0.9745544534166254, 0.9749183068821795, 0.9751690090416452, 0.9754644314982688, 0.9756489315008967, 0.9759940179615951, 0.9761999640797953, 0.9765526542028236, 0.9768701834600052, 0.9771350695010017, 0.9773687084938233, 0.9774882667349725, 0.9777284386829683, 0.9779330037409358, 0.9782473206264398, 0.9785185440305271, 0.9786859151834914, 0.9788946779413973, 0.9791406931437506, 0.9792458854342113, 0.9794429371408825, 0.9795737086030586, 0.9796146730083027, 0.979895609500385, 0.9800670721349256, 0.9801098534607926, 0.9803203455683401, 0.9805819041270469, 0.9807591420608262, 0.9809373655381599, 0.9811675211320461, 0.9812954258629261, 0.9814385140040698, 0.981534705198947, 0.9816911038362598, 0.9817667584431746, 0.9819301786905884, 0.9820516081787185, 0.9820423481775687, 0.982287455396772, 0.9823731932631026, 0.982496932416628, 0.9827570711297917, 0.9829493072442489, 0.9830479870829931, 0.9831669898235679, 0.9831973263305522, 0.9833571987177998, 0.9834382687996467, 0.9834600785994992, 0.983577291571729, 0.9835944996896114, 0.9835681703659529, 0.9835862906044129, 0.9837235426059411, 0.9838540088049261, 0.9839296478030788, 0.9841697478645022, 0.9842555214399936, 0.9843489936996026, 0.9844586953701555], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 912001097, "moving_var_accuracy_train": [0.028566330480383224, 0.058703735013763275, 0.08627316995986338, 0.10930183889292568, 0.1265777823670245, 0.13805614249235862, 0.1456222038716738, 0.1489986776359463, 0.14940255074537792, 0.1474443616347717, 0.14377921726317408, 0.13871685275537338, 0.13277855289725124, 0.12606955373939874, 0.11907837739528075, 0.11196625115780526, 0.10474862620596903, 0.09770189322824292, 0.09077722056447009, 0.08414923532361311, 0.07775668794250327, 0.0716962487179404, 0.06599343506012996, 0.06060202883040512, 0.055609898339619546, 0.050929710359906136, 0.04661220904547084, 0.04261206752692126, 0.03891941127575463, 0.035494012546986434, 0.03232876550168447, 0.029443248048436246, 0.026790671043932315, 0.024364943643384537, 0.022142943991954493, 0.02011778735076735, 0.018262167873193225, 0.016580117893591095, 0.015033804873781389, 0.01362452503668057, 0.012352150207038377, 0.011194858071309561, 0.010146260677136906, 0.009189046029335025, 0.008326649695744051, 0.007536410371562501, 0.006818883388602741, 0.006173009978735207, 0.005589737314100184, 0.005057961535108216, 0.004576200750785804, 0.004139491839730072, 0.0037469979379161833, 0.003392514927623036, 0.003068531932477836, 0.002777159131841473, 0.0025110642324684697, 0.0022710204051939746, 0.002056235841629117, 0.0018611224095127828, 0.0016836049965815484, 0.0015226567979532036, 0.001380081003009528, 0.0012483305042487744, 0.0011310037236561365, 0.0010235766077882826, 0.0009266562133202635, 0.0008391269358641612, 0.0007592612007986701, 0.0006868178961156464, 0.000621446864163722, 0.0005629998694580379, 0.0005096230186027934, 0.00046079884121412825, 0.0004167177135274072, 0.0003776280486821, 0.00034126058420780433, 0.0003096283846392064, 0.0002812792544518274, 0.0002551284202723289, 0.00023098015240189982, 0.00020961476630711347, 0.00019040758386514915, 0.0001725488292128307, 0.0001567207825202185, 0.00014221995632568814, 0.0001294818750086117, 0.00011742292240198815, 0.00010813527827195121, 9.79553710932248e-05, 8.896659151291648e-05, 8.081655375705371e-05, 7.392640248091026e-05, 6.709942638766613e-05, 6.117495359979725e-05, 5.5363820498544804e-05, 5.089920043690679e-05, 4.619100462563108e-05, 4.269141706900298e-05, 3.9329698824599644e-05, 3.6028210474572946e-05, 3.2916674037815805e-05, 2.9753654191274538e-05, 2.7297431853584522e-05, 2.4944310434697195e-05, 2.3339035331844263e-05, 2.1667191012982152e-05, 1.975258983728527e-05, 1.816956785535321e-05, 1.689732238791897e-05, 1.530717891087834e-05, 1.412592539570861e-05, 1.2867243434014727e-05, 1.1595621833086333e-05, 1.1146387463029019e-05, 1.0296343632118831e-05, 9.283181445493388e-06, 8.753625647002314e-06, 8.493978998997024e-06, 7.927300665630372e-06, 7.420443069923811e-06, 7.155143139505008e-06, 6.586865407187693e-06, 6.1124468116926135e-06, 5.584476844270589e-06, 5.246173963623124e-06, 4.773069143187736e-06, 4.5361178242521e-06, 4.215212127114883e-06, 3.794462642995044e-06, 3.9557143188455715e-06, 3.6263017224672636e-06, 3.401473953257362e-06, 3.6703759087098504e-06, 3.635930831153124e-06, 3.3599771432090153e-06, 3.15143429926697e-06, 2.844573602244352e-06, 2.7901488638577874e-06, 2.5702852010078602e-06, 2.3175376872335747e-06, 2.209433846240831e-06, 1.9911555355062625e-06, 1.7982790815144298e-06, 1.6214062607396405e-06, 1.6288086419768351e-06, 1.6191206394776856e-06, 1.5086998979037448e-06, 1.8766622635730497e-06, 1.755209993489058e-06, 1.6583225639878366e-06, 1.6008004162878273e-06], "duration": 75064.261513, "accuracy_train": [0.563385899720838, 0.6618139549303249, 0.7264379513312108, 0.7709122802464008, 0.7969710701019748, 0.8109909964470285, 0.8322185236134183, 0.8400982546603912, 0.8506529883028792, 0.859324351409653, 0.8683684593023256, 0.8743175958610188, 0.8816647056109265, 0.8846176445990217, 0.8912672097060724, 0.897288624146364, 0.8997997848606497, 0.905729274813123, 0.9078898607650425, 0.9128424277293282, 0.9142607685031378, 0.9173993589078073, 0.9208166066814323, 0.9217709991578996, 0.9264427458241048, 0.9273263023717239, 0.9311155739548725, 0.9332786833241048, 0.9356252811807864, 0.9360910319190661, 0.9366250951688816, 0.9399507789428755, 0.9409734839308784, 0.942787100002307, 0.9438559474783131, 0.94576148803756, 0.9461578447997416, 0.948691896513935, 0.9479002644541344, 0.9485291360972684, 0.9510639087878369, 0.9520157778469915, 0.9535986814784054, 0.9535968790374677, 0.9559231093115541, 0.9550831718346253, 0.9555743369901256, 0.9575499925018457, 0.9589908637873754, 0.9588746063468992, 0.9595710695251938, 0.9601062142395718, 0.9618275453349945, 0.9629192838109081, 0.9624553355135659, 0.9638479013819674, 0.9634075650609081, 0.9642675096322444, 0.965988119751292, 0.9662656956556847, 0.9663121986318751, 0.9665922979535806, 0.9688008288344407, 0.967800654358158, 0.9694286190130121, 0.9691488801794942, 0.9697759493816908, 0.9703350665605389, 0.9702417001199704, 0.9704273515365448, 0.9708938232511997, 0.9718449713339794, 0.9717752168696937, 0.971520171477021, 0.9718460527985419, 0.9729610427625508, 0.9720778467031194, 0.9737980963339794, 0.9744494984888336, 0.9742863775839794, 0.9739619382152085, 0.97484513427464, 0.9753112455011074, 0.9749617522033037, 0.9756818273578812, 0.9757057998223514, 0.9765196019056847, 0.9760084296557769, 0.9784018909768365, 0.9763550390480805, 0.9769610196913067, 0.9771466711078812, 0.9781929880721669, 0.9774253284768365, 0.9781232336078812, 0.9773094315245479, 0.9790997961078812, 0.9780534791435955, 0.9797268653100776, 0.97972794677464, 0.9795190438699704, 0.9794714594292175, 0.9785642909053157, 0.9798899862149317, 0.9797740892626431, 0.9810761725959765, 0.9809595546673128, 0.9801922555601699, 0.9807735427625508, 0.9813548299649317, 0.9801926160483574, 0.9812164025009228, 0.9807506517626431, 0.9799833526555003, 0.9824240379291252, 0.9816102358457919, 0.9804948853935955, 0.982214774536268, 0.982935931155408, 0.9823542834648394, 0.9825413768341639, 0.983238921477021, 0.9824465684408453, 0.9827263072743633, 0.9824004259528424, 0.9830986915720746, 0.982447649905408, 0.9834009609173128, 0.98314447357189, 0.9819590081672205, 0.9844934203696014, 0.9831448340600776, 0.9836105847983574, 0.9850983195482651, 0.9846794322743633, 0.9839361056316908, 0.9842380144887413, 0.983470354893411, 0.9847960502030271, 0.984167899536268, 0.9836563667981728, 0.9846322083217978, 0.9837493727505537, 0.9833312064530271, 0.9837493727505537, 0.9849588106196937, 0.9850282045957919, 0.9846103987864526, 0.9863306484173128, 0.9850274836194168, 0.9851902440360835, 0.9854460104051311], "end": "2016-01-24 22:13:03.972000", "learning_rate_per_epoch": [0.007754005026072264, 0.003877002513036132, 0.002584668342024088, 0.001938501256518066, 0.0015508009819313884, 0.001292334171012044, 0.0011077149538323283, 0.000969250628259033, 0.0008615560946054757, 0.0007754004909656942, 0.0007049095584079623, 0.000646167085506022, 0.0005964619340375066, 0.0005538574769161642, 0.0005169336800463498, 0.0004846253141295165, 0.0004561179375741631, 0.00043077804730273783, 0.00040810552309267223, 0.0003877002454828471, 0.00036923831794410944, 0.00035245477920398116, 0.0003371306520421058, 0.000323083542753011, 0.0003101601905655116, 0.0002982309670187533, 0.0002871853648684919, 0.0002769287384580821, 0.0002673794806469232, 0.0002584668400231749, 0.00025012920377776027, 0.00024231265706475824, 0.0002349698479520157, 0.00022805896878708154, 0.0002215429994976148, 0.00021538902365136892, 0.0002095677045872435, 0.00020405276154633611, 0.00019882063497789204, 0.00019385012274142355, 0.0001891220745164901, 0.00018461915897205472, 0.00018032568914350122, 0.00017622738960199058, 0.00017231122183147818, 0.0001685653260210529, 0.00016497883189003915, 0.0001615417713765055, 0.0001582449913257733, 0.0001550800952827558, 0.00015203931252472103, 0.00014911548350937665, 0.0001463019725633785, 0.00014359268243424594, 0.00014098190877120942, 0.00013846436922904104, 0.00013603517436422408, 0.0001336897403234616, 0.0001314238179475069, 0.00012923342001158744, 0.00012711483577731997, 0.00012506460188888013, 0.00012307944416534156, 0.00012115632853237912, 0.00011929238098673522, 0.00011748492397600785, 0.00011573141819098964, 0.00011402948439354077, 0.00011237688158871606, 0.0001107714997488074, 0.00010921133798547089, 0.00010769451182568446, 0.00010621924593579024, 0.00010478385229362175, 0.00010338673018850386, 0.00010202638077316806, 0.00010070136340800673, 9.941031748894602e-05, 9.815196244744584e-05, 9.692506137071177e-05, 9.572845738148317e-05, 9.456103725824505e-05, 9.342174598714337e-05, 9.230957948602736e-05, 9.122358460444957e-05, 9.016284457175061e-05, 8.91264935489744e-05, 8.811369480099529e-05, 8.712364797247574e-05, 8.615561091573909e-05, 8.520884875906631e-05, 8.428266301052645e-05, 8.337639883393422e-05, 8.248941594501957e-05, 8.162110316334292e-05, 8.077088568825275e-05, 7.993819599505514e-05, 7.912249566288665e-05, 7.83232826506719e-05, 7.75400476413779e-05, 7.677232497371733e-05, 7.601965626236051e-05, 7.528159767389297e-05, 7.455774175468832e-05, 7.384766649920493e-05, 7.315098628168926e-05, 7.246733730426058e-05, 7.179634121712297e-05, 7.113766332622617e-05, 7.049095438560471e-05, 6.985590152908117e-05, 6.923218461452052e-05, 6.86195126036182e-05, 6.801758718211204e-05, 6.742613186361268e-05, 6.68448701617308e-05, 6.627354741794989e-05, 6.571190897375345e-05, 6.515970744658262e-05, 6.461671000579372e-05, 6.408268382074311e-05, 6.355741788865998e-05, 6.30406939308159e-05, 6.253230094444007e-05, 6.203204247867689e-05, 6.153972208267078e-05, 6.105515785748139e-05, 6.057816426618956e-05, 6.010856668581255e-05, 5.964619049336761e-05, 5.919087561778724e-05, 5.8742461988003924e-05, 5.830078953295015e-05, 5.786570909549482e-05, 5.743707515648566e-05, 5.7014742196770385e-05, 5.659857561113313e-05, 5.618844079435803e-05, 5.5784206779208034e-05, 5.53857498744037e-05, 5.499294275068678e-05, 5.4605668992735445e-05, 5.422381218522787e-05, 5.384725591284223e-05, 5.3475894674193114e-05, 5.310962296789512e-05, 5.2748331654584035e-05, 5.2391926146810874e-05, 5.204030094319023e-05, 5.169336509425193e-05, 5.1351027650525793e-05, 5.101319038658403e-05], "accuracy_valid": [0.5627279626317772, 0.6526952536709337, 0.7204781038215362, 0.7597965102597892, 0.7746390836784638, 0.7936526378953314, 0.8097659191453314, 0.8118205242846386, 0.8209360881024097, 0.8270601939006024, 0.8288603633283133, 0.8330622293862951, 0.8373758706701807, 0.8374464655496988, 0.8405085184487951, 0.8461958184299698, 0.8462870034826807, 0.8471312005835843, 0.8512007012424698, 0.8512712961219879, 0.8512712961219879, 0.8542009836219879, 0.8572836266942772, 0.8576807228915663, 0.8565409097326807, 0.8585352150790663, 0.859410297439759, 0.8592264566076807, 0.8612310570406627, 0.8584028496799698, 0.8602544945406627, 0.8620752541415663, 0.8595014824924698, 0.8587793557040663, 0.8606412956513554, 0.8588911309299698, 0.8619943641754518, 0.859532367752259, 0.8617193382906627, 0.8610075065888554, 0.8638151237763554, 0.8621061394013554, 0.8626047157379518, 0.8628488563629518, 0.8620752541415663, 0.8594000023531627, 0.8617193382906627, 0.8646799110504518, 0.8618825889495482, 0.8622487998870482, 0.8641916298004518, 0.862828266189759, 0.8649343467620482, 0.8645681358245482, 0.8628591514495482, 0.865443218185241, 0.8634695030120482, 0.865077007247741, 0.8662874152861446, 0.865687358810241, 0.8638254188629518, 0.8655755835843373, 0.864466655685241, 0.8631032920745482, 0.8628694465361446, 0.8642122199736446, 0.8650564170745482, 0.8635915733245482, 0.8633577277861446, 0.8627679663968373, 0.8618928840361446, 0.8633989081325302, 0.866785991622741, 0.8622385048004518, 0.8620046592620482, 0.862269390060241, 0.866053569747741, 0.865077007247741, 0.8635812782379518, 0.8638460090361446, 0.862757671310241, 0.8632253623870482, 0.8641107398343373, 0.863734233810241, 0.8607633659638554, 0.8635812782379518, 0.8631032920745482, 0.8633577277861446, 0.8655549934111446, 0.8650461219879518, 0.8635812782379518, 0.865687358810241, 0.8654638083584337, 0.8626047157379518, 0.8624929405120482, 0.8624826454254518, 0.864222515060241, 0.864588725997741, 0.8635915733245482, 0.8664094855986446, 0.8631032920745482, 0.8673051581325302, 0.8633268425263554, 0.8656976538968373, 0.8649343467620482, 0.8640695594879518, 0.8656667686370482, 0.8656976538968373, 0.8648431617093373, 0.865443218185241, 0.8637033485504518, 0.8655549934111446, 0.8631032920745482, 0.8637239387236446, 0.8642122199736446, 0.8650873023343373, 0.861851703689759, 0.8648122764495482, 0.8657079489834337, 0.8641916298004518, 0.866297710372741, 0.8649446418486446, 0.8649343467620482, 0.8657888389495482, 0.8649343467620482, 0.8665521460843373, 0.8658300192959337, 0.8646696159638554, 0.8635003882718373, 0.8641916298004518, 0.8640592644013554, 0.864171039627259, 0.8643239951995482, 0.865931499435241, 0.8667756965361446, 0.8648122764495482, 0.8656667686370482, 0.8657991340361446, 0.8659417945218373, 0.8645681358245482, 0.864588725997741, 0.8626150108245482, 0.8640798545745482, 0.8624929405120482, 0.8623811652861446, 0.8640798545745482, 0.8633577277861446, 0.8655446983245482, 0.8645475456513554, 0.8649446418486446, 0.8651784873870482, 0.8649240516754518], "accuracy_test": 0.8110172193877551, "start": "2016-01-24 01:21:59.711000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0], "accuracy_train_last": 0.9854460104051311, "batch_size_eval": 1024, "accuracy_train_std": [0.020653296803797773, 0.01917449884663655, 0.019690179221499456, 0.021581510795643927, 0.01831091303609535, 0.018307682482261448, 0.014796862362172565, 0.01694372032481179, 0.01555911255871115, 0.015318275479677733, 0.014394319978438472, 0.015007699098747522, 0.014008768909570653, 0.013902614768004222, 0.01339919846700976, 0.012748153325318182, 0.012306245501702737, 0.011383783159316936, 0.012489175068680188, 0.011686546960168435, 0.012028202670476114, 0.01188746728964885, 0.01153340340504477, 0.010558249914286567, 0.01046500179950292, 0.010371434450332218, 0.00982638931192547, 0.01028742513170159, 0.009782192818756856, 0.009855207507941438, 0.009742735559008666, 0.009594763582759878, 0.009039521596230072, 0.008372182676272091, 0.009117272235400837, 0.008912386782166827, 0.008178546055313122, 0.008543226693313238, 0.00862315402575342, 0.008540941125758855, 0.009150386537453764, 0.008354349903473005, 0.008532465906668245, 0.008436004431593836, 0.0074586741688005736, 0.008537258660262713, 0.007539821485836406, 0.007977479666018101, 0.007664949037417586, 0.007561262198250992, 0.007542590936413941, 0.0077506344915070574, 0.008307511397640492, 0.006968740241442914, 0.00790761476741956, 0.007695554723167252, 0.007157239509533311, 0.00782976883842349, 0.008293044106750235, 0.007184049226141568, 0.007616799352331364, 0.007173742975914365, 0.006982019558282401, 0.007277954702568826, 0.0061758268602271635, 0.007476465045671453, 0.006891562747978373, 0.006968826475368691, 0.00731684910472817, 0.006775916022595355, 0.006582190399737256, 0.006916944803415863, 0.0065663272267009595, 0.00686075264939711, 0.0064295735443232905, 0.006592372564302125, 0.006730928530075935, 0.006319208447840446, 0.005990386304850742, 0.00693163156029396, 0.006717700866206998, 0.005677924604876053, 0.006265899096044321, 0.0051441768975533046, 0.005781042221866567, 0.005528260793582947, 0.00563251155749109, 0.0054813394411790525, 0.005762031126285616, 0.005873466013110368, 0.0055605280986480545, 0.00525024460461461, 0.005468333554512357, 0.005643008007183998, 0.005140438620701981, 0.004987324500452818, 0.00505525750029019, 0.004803337837429893, 0.0056728303778086206, 0.00541720604530893, 0.005023327745594968, 0.005221060114007933, 0.005511442524458721, 0.004978512664683597, 0.004816693050357922, 0.0039026398902766856, 0.005118265865286796, 0.005029301993402026, 0.004883330586124793, 0.004582309249673825, 0.004983270834970155, 0.004801703253208183, 0.0049692585344369685, 0.004764258532555191, 0.004927079673475351, 0.005134671456178702, 0.005065359873635312, 0.0051238764109300975, 0.004623023442083254, 0.0043567107281171, 0.004571528931805724, 0.004559665477787265, 0.004717767998322627, 0.004649473275071353, 0.004855359155617991, 0.003951634930720806, 0.004687913512572555, 0.004785731951864694, 0.004853078155677702, 0.004282502444098712, 0.003949994686011556, 0.004573306524453885, 0.004130253450246419, 0.004225697247453286, 0.004218347487830118, 0.004295321091179734, 0.004025397233585305, 0.004589673330995355, 0.004128485439489194, 0.004520264207851274, 0.003850658300676846, 0.003694387651332165, 0.004616321300357234, 0.004448361706860705, 0.00453693876365504, 0.004083684815921672, 0.004335913242101421, 0.004201908039110391, 0.003616520859417752, 0.004435026118626029, 0.004062194747286852, 0.0042686671521245485], "accuracy_test_std": 0.014741875394124911, "error_valid": [0.43727203736822284, 0.34730474632906627, 0.2795218961784638, 0.24020348974021077, 0.2253609163215362, 0.20634736210466864, 0.19023408085466864, 0.18817947571536142, 0.1790639118975903, 0.17293980609939763, 0.17113963667168675, 0.16693777061370485, 0.1626241293298193, 0.16255353445030118, 0.15949148155120485, 0.15380418157003017, 0.1537129965173193, 0.15286879941641573, 0.14879929875753017, 0.14872870387801207, 0.14872870387801207, 0.14579901637801207, 0.14271637330572284, 0.14231927710843373, 0.1434590902673193, 0.14146478492093373, 0.14058970256024095, 0.1407735433923193, 0.13876894295933728, 0.14159715032003017, 0.13974550545933728, 0.13792474585843373, 0.14049851750753017, 0.14122064429593373, 0.1393587043486446, 0.14110886907003017, 0.13800563582454817, 0.14046763224774095, 0.13828066170933728, 0.1389924934111446, 0.1361848762236446, 0.1378938605986446, 0.13739528426204817, 0.13715114363704817, 0.13792474585843373, 0.14059999764683728, 0.13828066170933728, 0.13532008894954817, 0.13811741105045183, 0.13775120011295183, 0.13580837019954817, 0.13717173381024095, 0.13506565323795183, 0.13543186417545183, 0.13714084855045183, 0.13455678181475905, 0.13653049698795183, 0.13492299275225905, 0.1337125847138554, 0.13431264118975905, 0.13617458113704817, 0.13442441641566272, 0.13553334431475905, 0.13689670792545183, 0.1371305534638554, 0.1357877800263554, 0.13494358292545183, 0.13640842667545183, 0.1366422722138554, 0.13723203360316272, 0.1381071159638554, 0.13660109186746983, 0.13321400837725905, 0.13776149519954817, 0.13799534073795183, 0.13773060993975905, 0.13394643025225905, 0.13492299275225905, 0.13641872176204817, 0.1361539909638554, 0.13724232868975905, 0.13677463761295183, 0.13588926016566272, 0.13626576618975905, 0.1392366340361446, 0.13641872176204817, 0.13689670792545183, 0.1366422722138554, 0.1344450065888554, 0.13495387801204817, 0.13641872176204817, 0.13431264118975905, 0.13453619164156627, 0.13739528426204817, 0.13750705948795183, 0.13751735457454817, 0.13577748493975905, 0.13541127400225905, 0.13640842667545183, 0.1335905144013554, 0.13689670792545183, 0.13269484186746983, 0.1366731574736446, 0.13430234610316272, 0.13506565323795183, 0.13593044051204817, 0.13433323136295183, 0.13430234610316272, 0.13515683829066272, 0.13455678181475905, 0.13629665144954817, 0.1344450065888554, 0.13689670792545183, 0.1362760612763554, 0.1357877800263554, 0.13491269766566272, 0.13814829631024095, 0.13518772355045183, 0.13429205101656627, 0.13580837019954817, 0.13370228962725905, 0.1350553581513554, 0.13506565323795183, 0.13421116105045183, 0.13506565323795183, 0.13344785391566272, 0.13416998070406627, 0.1353303840361446, 0.13649961172816272, 0.13580837019954817, 0.1359407355986446, 0.13582896037274095, 0.13567600480045183, 0.13406850056475905, 0.1332243034638554, 0.13518772355045183, 0.13433323136295183, 0.1342008659638554, 0.13405820547816272, 0.13543186417545183, 0.13541127400225905, 0.13738498917545183, 0.13592014542545183, 0.13750705948795183, 0.1376188347138554, 0.13592014542545183, 0.1366422722138554, 0.13445530167545183, 0.1354524543486446, 0.1350553581513554, 0.13482151261295183, 0.13507594832454817], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-3, 3], "momentum": 0.5676193594221187, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.007754004921650055, "patience_threshold": 1, "do_flip": true, "batch_size": 32, "optimization": "adam", "nb_data_augmentation": 2, "learning_rate_decay_method": "lin", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 3.1384022474067805e-06, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.05553032206339064}, "accuracy_valid_max": 0.8673051581325302, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-3, 3)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8649240516754518, "loss_train": [1.6684621572494507, 1.2726389169692993, 1.036196231842041, 0.9137874245643616, 0.8286281824111938, 0.769191324710846, 0.723642110824585, 0.6868463754653931, 0.6575380563735962, 0.6321749687194824, 0.610834002494812, 0.5927123427391052, 0.5727787017822266, 0.5567848086357117, 0.5444052815437317, 0.5304763317108154, 0.519040048122406, 0.5091046690940857, 0.49844154715538025, 0.4899378716945648, 0.4802122414112091, 0.46971940994262695, 0.46187832951545715, 0.45542633533477783, 0.44912001490592957, 0.4415578544139862, 0.4351929724216461, 0.43035218119621277, 0.4234970808029175, 0.4179837107658386, 0.41352835297584534, 0.40779760479927063, 0.40266257524490356, 0.3966542184352875, 0.3964749276638031, 0.39103439450263977, 0.3848418593406677, 0.3806731700897217, 0.37706562876701355, 0.3723839521408081, 0.3692076504230499, 0.36523041129112244, 0.36295005679130554, 0.35992175340652466, 0.35799697041511536, 0.3527873456478119, 0.3504253923892975, 0.3459025025367737, 0.34334227442741394, 0.34149596095085144, 0.33837440609931946, 0.33590975403785706, 0.3321985602378845, 0.3311198949813843, 0.3291688859462738, 0.3256985545158386, 0.321704238653183, 0.3209722638130188, 0.3172617256641388, 0.31615975499153137, 0.3135429620742798, 0.3137069344520569, 0.3103633522987366, 0.30840468406677246, 0.30757030844688416, 0.30433547496795654, 0.30357280373573303, 0.300139844417572, 0.2996048927307129, 0.2993360757827759, 0.29604923725128174, 0.2938864231109619, 0.2934035360813141, 0.2913016974925995, 0.28953659534454346, 0.2886058986186981, 0.28449341654777527, 0.2827524244785309, 0.28434887528419495, 0.2815023958683014, 0.28019964694976807, 0.2784159779548645, 0.2785811126232147, 0.2792756259441376, 0.2741940915584564, 0.2733161747455597, 0.27257466316223145, 0.27235090732574463, 0.2696046829223633, 0.27044039964675903, 0.26703906059265137, 0.26740792393684387, 0.2648800015449524, 0.26612845063209534, 0.26387882232666016, 0.2604256570339203, 0.2593368589878082, 0.2596395015716553, 0.2611874043941498, 0.259893000125885, 0.2575671672821045, 0.2571634352207184, 0.2543463110923767, 0.2541842460632324, 0.2546888291835785, 0.2527674734592438, 0.25284600257873535, 0.2503277063369751, 0.25023409724235535, 0.2493995726108551, 0.2493283897638321, 0.24818579852581024, 0.2463407665491104, 0.2455272078514099, 0.2447083592414856, 0.24576923251152039, 0.24391071498394012, 0.24296295642852783, 0.24063518643379211, 0.2423066347837448, 0.2398967742919922, 0.24170249700546265, 0.23994621634483337, 0.23880378901958466, 0.23749734461307526, 0.2381557822227478, 0.23651941120624542, 0.23673826456069946, 0.2373666912317276, 0.23402556777000427, 0.23461218178272247, 0.2316179722547531, 0.2314634919166565, 0.23268970847129822, 0.23070912063121796, 0.23068521916866302, 0.22881431877613068, 0.23079544305801392, 0.22882667183876038, 0.22776703536510468, 0.2292603850364685, 0.22644591331481934, 0.2270372360944748, 0.2265685647726059, 0.22420822083950043, 0.2241126447916031, 0.22550411522388458, 0.22415432333946228, 0.22271090745925903, 0.22252526879310608, 0.2242259830236435, 0.22049002349376678], "accuracy_train_first": 0.563385899720838, "model": "residualv3", "loss_std": [0.32242196798324585, 0.19587191939353943, 0.18347060680389404, 0.17673709988594055, 0.1714840829372406, 0.16631977260112762, 0.16365572810173035, 0.15667615830898285, 0.15455122292041779, 0.15186941623687744, 0.14802683889865875, 0.14679959416389465, 0.1428569257259369, 0.14236238598823547, 0.13824383914470673, 0.13507549464702606, 0.13614821434020996, 0.1328178495168686, 0.13247162103652954, 0.13007019460201263, 0.1260487288236618, 0.1255159080028534, 0.12333403527736664, 0.12353133410215378, 0.12054049223661423, 0.12239781022071838, 0.11991492658853531, 0.11960798501968384, 0.11602060496807098, 0.11436456441879272, 0.1147400513291359, 0.1116725355386734, 0.11058225482702255, 0.10805744677782059, 0.11091402918100357, 0.1071171760559082, 0.10767649859189987, 0.10580412298440933, 0.10472644120454788, 0.10408841073513031, 0.10177095979452133, 0.10063299536705017, 0.10133972764015198, 0.09776221960783005, 0.09972535073757172, 0.09770672023296356, 0.09729151427745819, 0.09437276422977448, 0.09503025561571121, 0.09180204570293427, 0.09139153361320496, 0.09263016283512115, 0.09090705960988998, 0.09038282930850983, 0.09142565727233887, 0.09087692201137543, 0.08786551654338837, 0.08767355233430862, 0.08742782473564148, 0.08579602837562561, 0.08532334119081497, 0.08421551436185837, 0.08363372087478638, 0.08238090574741364, 0.08276721835136414, 0.0831412747502327, 0.0829896554350853, 0.0798954963684082, 0.08066094666719437, 0.08260390162467957, 0.08203025907278061, 0.07911863923072815, 0.07841893285512924, 0.0788392424583435, 0.07675863802433014, 0.07667139172554016, 0.076441191136837, 0.07608897984027863, 0.07664162665605545, 0.07381825149059296, 0.07373005151748657, 0.07258548587560654, 0.07347577065229416, 0.07377447932958603, 0.0738070085644722, 0.0716153234243393, 0.07289617508649826, 0.07301857322454453, 0.07192505896091461, 0.06974869966506958, 0.07083950936794281, 0.06922736763954163, 0.06863260269165039, 0.06894347816705704, 0.06923745572566986, 0.06759278476238251, 0.06688651442527771, 0.06641160696744919, 0.07135059684515, 0.06835160404443741, 0.06822921335697174, 0.06582210212945938, 0.06668068468570709, 0.06516993790864944, 0.06612922996282578, 0.06603845208883286, 0.06558746099472046, 0.06450393795967102, 0.06430371105670929, 0.06495457887649536, 0.0623396672308445, 0.06392735242843628, 0.0630246102809906, 0.06256787478923798, 0.06150658056139946, 0.06452053040266037, 0.06413902342319489, 0.06334593147039413, 0.05995376780629158, 0.06327774375677109, 0.06107529252767563, 0.06089138239622116, 0.06087038293480873, 0.0608491450548172, 0.06029624119400978, 0.06003766879439354, 0.05896418169140816, 0.060631316155195236, 0.060576289892196655, 0.05891032889485359, 0.05902736634016037, 0.058097824454307556, 0.05878356099128723, 0.05961697921156883, 0.0586494579911232, 0.05871516093611717, 0.05608227103948593, 0.05822250619530678, 0.0549282431602478, 0.05703740194439888, 0.05835207551717758, 0.05670161545276642, 0.055997081100940704, 0.05765232443809509, 0.055456649512052536, 0.056124597787857056, 0.056164197623729706, 0.055486712604761124, 0.05628160759806633, 0.05456848815083504, 0.056826554238796234, 0.05383387207984924]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:11 2016", "state": "available"}], "summary": "d0321c355dc56dd33340d519c5e40a14"}