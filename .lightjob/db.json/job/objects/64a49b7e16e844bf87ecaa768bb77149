{"content": {"hp_model": {"f0": 64, "f1": 32, "f2": 16, "f3": 16, "nonlin": "very_leaky_rectify", "nbg1": 3, "nbg3": 6, "nbg2": 7, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.006925223150154922, 0.014474471540802381, 0.018640194054384857, 0.015362596995331325, 0.013638562072000688, 0.01820888558156079, 0.023968653738712844, 0.022036672306137567, 0.025389954372501248, 0.018126990878373803, 0.018878791316951693, 0.022316543234027892, 0.01874718879648596, 0.016529883578266687, 0.021034981513304823, 0.022647257808809003, 0.021728036742602142, 0.015493113200970686, 0.01885907349533684, 0.018924503632160975, 0.019609724616205602, 0.01803150203939362, 0.018579759646479736, 0.018299847904417278, 0.01638191707774266, 0.01975978263276298, 0.019952892107213304, 0.017145626148524364, 0.017871307833902353, 0.015540543575286605, 0.014470705510719257, 0.017086224518523964, 0.0163714936463708, 0.01786889086240445, 0.01513127013608942, 0.014799452752100104, 0.013620433075596293, 0.014002783168460492, 0.01627291196657798, 0.018115885783004573, 0.019190867159966603, 0.017066191463658873, 0.01763688875911074, 0.01852475588176914, 0.01711562682041832, 0.020786391044924368, 0.017652942259618075, 0.017596246684169235, 0.020645804961817175, 0.01674819089390283, 0.013676249063439264, 0.01390146395708224, 0.016129947242980554, 0.016603345829667594, 0.018344629103413, 0.014750798523864569, 0.017659080497662835, 0.018510835397774213, 0.012134345858686753, 0.01660674076617198, 0.012808184052668994, 0.011852688596214354, 0.01264503102445251, 0.013748617649695802, 0.016079094216263908, 0.015355015989946295, 0.014781628824403837, 0.015432456118276249, 0.01168551841160413, 0.014623596891854452, 0.009620026433703958, 0.011895517139082731, 0.013866104203738575, 0.009765598420566414, 0.00925910634698994, 0.011957832870669682, 0.009555562581993879, 0.011058125167240631, 0.011738841475341858, 0.010923453118613046, 0.012945902120352683, 0.015349063426988942, 0.012414069693547065, 0.012639486751882402, 0.011385785980712161, 0.012874096243990284, 0.00796772976877582, 0.012245170781181727, 0.013188366677046752, 0.011499031153611219, 0.011908661006490491, 0.013497897468033032, 0.013716721577859579, 0.01366380062674598, 0.015077897645626075, 0.012779141719007856, 0.012141801857463153, 0.010644964603216277, 0.013226728078671611, 0.012232637917722482, 0.013484217372056686, 0.01117865267055986, 0.011845116718288473, 0.009509315843423094, 0.011720070035724886, 0.01095202574087911, 0.012185197835801439, 0.012148134433970613, 0.014478124541448301, 0.012079597868644616, 0.011150478042360195, 0.011584087646300362, 0.012686023947320017, 0.012731416554085526, 0.010373124148969407, 0.011774920053011078, 0.013408131533502019, 0.012228287601411831, 0.013559297071754945, 0.014417636858997348, 0.01278334865283625, 0.011903512340317854, 0.012415004285786242, 0.010935047138749671, 0.012999926878146977, 0.01313367040276967, 0.01066651399984592, 0.011543327848263773, 0.012112349637301423, 0.013440811514166501, 0.010866357182961257, 0.011932643273353645, 0.01247760326335866, 0.01258278641788963, 0.011115111747224604, 0.009735078141444635, 0.013045447830990365, 0.01447867156050221, 0.01304482504205824, 0.011359143122181576, 0.012224293526449726, 0.011169631194571364, 0.011473405415940514, 0.012139614092824576, 0.010770282133730477, 0.013248886947586153, 0.010811077766579472, 0.011903223502135848, 0.011621648538868202, 0.011761678221842731, 0.0109703685006685, 0.013462113021075325, 0.011609827078004124, 0.01129842925945114, 0.012148725562413065, 0.011286099548234308, 0.010056085099004828, 0.01057915745803586, 0.012435706263621296, 0.011694898774602105, 0.010727847606354668, 0.011931733260455126, 0.01083670992527393, 0.009952363274336967, 0.011595867728429665, 0.009381044937874245, 0.012369370343012503, 0.009989991225722802, 0.009609062590480704, 0.007941545769378678, 0.008529749541844269, 0.009866383231414108, 0.009867021288459647, 0.00935948061879271, 0.01007823540880411, 0.012899841262698056, 0.01083285740677096, 0.01276985909161975, 0.01110637266903911, 0.011674166759494096, 0.00999509374150948, 0.011050754326174467, 0.009354209559437584, 0.011578261206331177], "moving_avg_accuracy_train": [0.03761809593023255, 0.08035515296234771, 0.12493452885635842, 0.17096505348202748, 0.21641896240684982, 0.26040275109372113, 0.30260370098088835, 0.34277891957442924, 0.38065225169067585, 0.4157006819583469, 0.44872771413972706, 0.4794121853660275, 0.5077555484612538, 0.5341551072410051, 0.558844589422497, 0.5815163824989812, 0.6028252268129904, 0.6221986433908737, 0.6398667285084586, 0.6567933236416474, 0.6729035519721579, 0.6872471166946541, 0.7007074933592142, 0.713158870788243, 0.7251325177279679, 0.7362339242700475, 0.746362662328231, 0.7557039578686637, 0.7648270894442928, 0.7730657735992544, 0.7808826598386738, 0.7882434143363033, 0.7948912367258089, 0.8014693347275081, 0.8076663877350083, 0.8133807750262454, 0.8188585089681114, 0.8242488489800766, 0.8290350508241786, 0.8337425941279273, 0.8380467403191397, 0.8423040132983461, 0.8461193910844215, 0.849748291250158, 0.8530773327589094, 0.856389510110787, 0.8596912681655536, 0.8624304436803475, 0.8653236011222518, 0.8682598309532713, 0.8708022040606481, 0.8732204760929831, 0.8756853835697406, 0.8781827100095088, 0.8802768800326904, 0.8824405788130593, 0.8844438915309135, 0.8865930678078701, 0.8884649079274614, 0.8901889473695805, 0.8915661606579548, 0.8931613643365299, 0.8948248401329434, 0.8964289251949537, 0.8977470797638674, 0.8992660233021077, 0.9006773224115425, 0.9020823141921674, 0.9032304772566162, 0.9044102723408014, 0.9054161401498645, 0.9066144620256588, 0.9075397442341763, 0.908511790857501, 0.9095214193518081, 0.9104696485752651, 0.9114252532775393, 0.9123086571441376, 0.9130782521336275, 0.9139197331967966, 0.914628093833374, 0.9155004584360553, 0.9162320721070308, 0.9168767537621454, 0.917477785444578, 0.9180792399254525, 0.9187181331106019, 0.9193232918141228, 0.9199609766484912, 0.9206000332149082, 0.921240324340169, 0.9218142612040942, 0.9224678439661137, 0.922904969828131, 0.9234309886837269, 0.9238160499990012, 0.9243090174601107, 0.9249061479965378, 0.9254017488495695, 0.925861776558974, 0.9263315690200478, 0.9267615379255365, 0.9271739784309249, 0.927577690920289, 0.9280527114012113, 0.928391802081642, 0.9287061400939927, 0.929047245122984, 0.929358925995514, 0.9297231080891151, 0.930069509212651, 0.9303114797107288, 0.9305687806887608, 0.9307772442761693, 0.9310043169369614, 0.9312807258959508, 0.931399249576889, 0.9316502603600177, 0.9318318840909964, 0.9320952547500492, 0.9323230958944149, 0.9325119489803148, 0.9327445876290256, 0.9329167960807317, 0.9331577060467634, 0.9333420089816773, 0.9335869006338049, 0.9337374765587965, 0.9339450745043842, 0.9341321289483256, 0.9343747745633215, 0.9345325936013138, 0.934711869165278, 0.9349384294859687, 0.9350748684102954, 0.9353277636290666, 0.9355229253890834, 0.9356799337338038, 0.9358701054178707, 0.9360272008942176, 0.9360942541586627, 0.9362685343883298, 0.9363625715283543, 0.9364726734448249, 0.9365718733161047, 0.936679718341914, 0.9367000850032468, 0.9367857722162849, 0.9368559513104096, 0.9369330994367976, 0.9369956294017557, 0.9370030061475805, 0.9371096626664511, 0.9372103759286912, 0.9373009457670698, 0.9374150107049438, 0.9374757443728214, 0.9375583064596257, 0.9376186614448923, 0.9376404648971177, 0.9376810143434063, 0.9377500609283993, 0.9377773977203877, 0.9378182047772065, 0.9378502808307242, 0.9379233271062711, 0.9379681063661589, 0.9380572358250581, 0.9380909493618766, 0.9380516091783653, 0.9380627420382142, 0.9381355045811166, 0.9381894011744999, 0.938249461754955, 0.9382454236059453, 0.9382975928432651, 0.9383886148377777, 0.9384613061352384, 0.9384663465315428, 0.9384894480298743, 0.9385310215223833, 0.938526729182345, 0.9385553460620064, 0.9385183582846632], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 389599580, "moving_var_accuracy_train": [0.012736090272745613, 0.027900585639367426, 0.0429964138713262, 0.05776605526002252, 0.07058397026293467, 0.08093673624190334, 0.08887134416012576, 0.09451064344546234, 0.09796908267120377, 0.0992277065821336, 0.09912199961630952, 0.09768363062461752, 0.09514538364608631, 0.0919032756153676, 0.0881990828273427, 0.08400526635633467, 0.07969134133468934, 0.07510017063032225, 0.07039960465279002, 0.06593823079273706, 0.06168026282521397, 0.05736387718322854, 0.05325812512447218, 0.0493276438109461, 0.04568519341918616, 0.042225845122180285, 0.03892658262182391, 0.0358192625810049, 0.03298642009062056, 0.030298661331005453, 0.02781872859222512, 0.025524482093971963, 0.023369775767276463, 0.021422240550428462, 0.01962564768918552, 0.017956970919295236, 0.016431323949606558, 0.015049693443647236, 0.01375089365211488, 0.012575252962513408, 0.011484458736180024, 0.010499132221537347, 0.009580232968237906, 0.00874072991913006, 0.007966399583519954, 0.007268494294460577, 0.006639759321284466, 0.006043311131663635, 0.0055143132583500894, 0.005040474943100203, 0.004594600397944198, 0.00418777271475114, 0.0038236773630967984, 0.0034974393809080106, 0.0031871653755911433, 0.002910583169741555, 0.00265564420937705, 0.0024316504164642438, 0.0022200194437176253, 0.0020247683073277047, 0.001839361924570004, 0.0016783278050982605, 0.0015353993901157147, 0.0014050172510796217, 0.0012801533091795908, 0.0011729026835128934, 0.0010735383017482288, 0.0009839504887060205, 0.0008974199456384989, 0.0008202051990406569, 0.0007472906095803756, 0.0006854853264844023, 0.0006246421183245543, 0.0005706817782333518, 0.0005227877476786683, 0.0004786012208527633, 0.0004389597218905631, 0.00040208737122519314, 0.0003672091221333056, 0.00033686102333702625, 0.00030769089412639283, 0.00028377098471385736, 0.0002602112133144958, 0.00023793062191101752, 0.00021738871146950516, 0.00019890556775563, 0.00018268867149834095, 0.00016771575785653003, 0.00015460395960272878, 0.0001428191032981824, 0.00013222694749415325, 0.00012196888445868806, 0.00011361652985410085, 0.00010397458804188986, 9.606739176568277e-05, 8.779510253780227e-05, 8.120274454343652e-05, 7.629155398689625e-05, 7.087298043793845e-05, 6.569031183492369e-05, 6.110762525976745e-05, 5.666072207097557e-05, 5.2525614398243385e-05, 4.873990692503637e-05, 4.589671634819436e-05, 4.23418871193698e-05, 3.899697388151045e-05, 3.61444502605877e-05, 3.340430993123821e-05, 3.125753631381125e-05, 2.9211726327912456e-05, 2.6817501192581538e-05, 2.47315852129892e-05, 2.264954029716738e-05, 2.0848644206963396e-05, 1.945139699975323e-05, 1.7632688066266462e-05, 1.6436476978861982e-05, 1.5089713897867282e-05, 1.4205019444529808e-05, 1.325172178366969e-05, 1.2247538997788232e-05, 1.1509871765875599e-05, 1.0625786346839323e-05, 1.00855462177558e-05, 9.382699742341172e-06, 8.984177059643017e-06, 8.28981733636241e-06, 7.848707765836171e-06, 7.378741274236682e-06, 7.170759197103135e-06, 6.6778449161682315e-06, 6.2993179750636084e-06, 6.131352387760624e-06, 5.68575736962746e-06, 5.692785557760345e-06, 5.466300015140499e-06, 5.141534596432722e-06, 4.952868561577434e-06, 4.679692603617699e-06, 4.252188605710472e-06, 4.100332131214922e-06, 3.7698857714293625e-06, 3.5019990823808903e-06, 3.240364704300135e-06, 3.021003180196274e-06, 2.7226360702212456e-06, 2.516453149503391e-06, 2.309133781822335e-06, 2.131786904286682e-06, 1.953798182516978e-06, 1.7589081116759531e-06, 1.6853978176666958e-06, 1.6081464866193862e-06, 1.521157898572731e-06, 1.4861393991853536e-06, 1.370722664991744e-06, 1.2949988820897879e-06, 1.1982835120996675e-06, 1.0827336756502207e-06, 9.892586264340032e-07, 9.33239641883459e-07, 8.466413794610764e-07, 7.769641844908369e-07, 7.085276249252556e-07, 6.856966877743221e-07, 6.351736580418168e-07, 6.431528362303826e-07, 5.890669756907612e-07, 5.440891284700856e-07, 4.907956807388274e-07, 4.893656015116521e-07, 4.665726263654633e-07, 4.5238082365039764e-07, 4.0728950111218033e-07, 3.9105521490376267e-07, 4.265147247787501e-07, 4.3141947483954505e-07, 3.885061777097497e-07, 3.544586729652177e-07, 3.3456800318329896e-07, 3.012770205120053e-07, 2.7851965067479624e-07, 2.629805466623821e-07], "duration": 128946.752386, "accuracy_train": [0.3761809593023256, 0.46498866625138424, 0.5261489119024547, 0.5852397751130491, 0.625504142730251, 0.6562568492755629, 0.6824122499653931, 0.7043558869162975, 0.7215122407368956, 0.7311365543673864, 0.7459710037721484, 0.7555724264027317, 0.7628458163182908, 0.771751136258767, 0.7810499290559246, 0.7855625201873385, 0.7946048256390735, 0.7965593925918235, 0.798879494566722, 0.809132679840347, 0.8178956069467516, 0.8163391991971208, 0.8218508833402547, 0.8252212676495018, 0.8328953401854927, 0.8361465831487633, 0.8375213048518827, 0.8397756177325582, 0.8469352736249538, 0.8472139309939092, 0.8512346359934477, 0.8544902048149685, 0.8547216382313585, 0.8606722167428018, 0.8634398648025102, 0.8648102606473791, 0.8681581144449059, 0.872761909087763, 0.8721108674210963, 0.8761104838616648, 0.8767840560400517, 0.8806194701112033, 0.8804577911590993, 0.8824083927417867, 0.8830387063376707, 0.8861991062776854, 0.8894070906584534, 0.887083023313492, 0.8913620180993909, 0.8946858994324474, 0.8936835620270396, 0.8949849243839978, 0.8978695508605574, 0.9006586479674235, 0.8991244102413253, 0.9019138678363787, 0.9024737059916021, 0.9059356543004798, 0.9053114690037836, 0.9057053023486527, 0.9039610802533223, 0.9075181974437062, 0.9097961223006644, 0.9108656907530455, 0.9096104708840901, 0.9129365151462717, 0.9133790143964563, 0.9147272402177926, 0.9135639448366556, 0.9150284280984681, 0.9144689504314323, 0.9173993589078073, 0.9158672841108343, 0.9172602104674235, 0.9186080758005721, 0.9190037115863787, 0.9200256955980066, 0.9202592919435216, 0.9200046070390366, 0.9214930627653194, 0.9210033395625692, 0.9233517398601883, 0.9228165951458103, 0.9226788886581765, 0.922887070586471, 0.9234923302533223, 0.9244681717769472, 0.9247697201458103, 0.9257001401578073, 0.9263515423126615, 0.9270029444675157, 0.9269796929794205, 0.9283500888242894, 0.9268391025862864, 0.9281651583840901, 0.927281601836471, 0.928745724610096, 0.9302803228243817, 0.9298621565268549, 0.9300020259436139, 0.9305597011697121, 0.9306312580749354, 0.9308859429794205, 0.9312111033245662, 0.9323278957295128, 0.9314436182055187, 0.9315351822051495, 0.9321171903839055, 0.9321640538482835, 0.9330007469315246, 0.933187119324474, 0.9324892141934293, 0.9328844894910484, 0.9326534165628461, 0.9330479708840901, 0.9337684065268549, 0.9324659627053341, 0.9339093574081765, 0.9334664976698044, 0.9344655906815246, 0.9343736661937062, 0.9342116267534146, 0.9348383354674235, 0.9344666721460871, 0.9353258957410484, 0.9350007353959026, 0.9357909255029531, 0.9350926598837209, 0.9358134560146733, 0.9358156189437985, 0.9365585850982835, 0.9359529649432448, 0.936325349240956, 0.9369774723721853, 0.9363028187292359, 0.9376038205980066, 0.9372793812292359, 0.9370930088362864, 0.937581650574474, 0.93744106018134, 0.9366977335386674, 0.9378370564553341, 0.9372089057885751, 0.9374635906930602, 0.9374646721576227, 0.9376503235741971, 0.9368833849552418, 0.9375569571336286, 0.9374875631575305, 0.9376274325742894, 0.9375583990863787, 0.9370693968600037, 0.9380695713362864, 0.938116795288852, 0.9381160743124769, 0.9384415951458103, 0.9380223473837209, 0.9383013652408637, 0.9381618563122923, 0.9378366959671466, 0.9380459593600037, 0.938371480193337, 0.9380234288482835, 0.9381854682885751, 0.9381389653123846, 0.9385807435861941, 0.9383711197051495, 0.9388594009551495, 0.9383943711932448, 0.9376975475267626, 0.9381629377768549, 0.9387903674672389, 0.9386744705149501, 0.9387900069790514, 0.9382090802648578, 0.9387671159791436, 0.9392078127883905, 0.9391155278123846, 0.9385117100982835, 0.9386973615148578, 0.9389051829549648, 0.9384880981220007, 0.938812897978959, 0.9381854682885751], "end": "2016-01-27 08:09:08.471000", "learning_rate_per_epoch": [0.001148482202552259, 0.0011130545753985643, 0.0010787197388708591, 0.0010454440489411354, 0.0010131947929039598, 0.000981940422207117, 0.0009516501449979842, 0.0009222942171618342, 0.000893843884114176, 0.0008662711479701102, 0.0008395489421673119, 0.0008136510732583702, 0.0007885521044954658, 0.0007642273558303714, 0.0007406529621221125, 0.0007178057567216456, 0.0006956633296795189, 0.0006742039695382118, 0.0006534065469168127, 0.0006332506891340017, 0.0006137165473774076, 0.0005947850295342505, 0.0005764374509453773, 0.0005586558836512268, 0.0005414228071458638, 0.0005247213412076235, 0.0005085350712761283, 0.0004928481066599488, 0.00047764505143277347, 0.00046291094622574747, 0.00044863135553896427, 0.00043479225132614374, 0.0004213800420984626, 0.00040838157292455435, 0.00039578406722284853, 0.00038357515586540103, 0.0003717428771778941, 0.0003602755896281451, 0.0003491620300337672, 0.00033839131356216967, 0.0003279528464190662, 0.00031783635495230556, 0.0003080319438595325, 0.00029852997977286577, 0.00028932112036272883, 0.0002803963143378496, 0.0002717468305490911, 0.00026336414157412946, 0.0002552400401327759, 0.0002473665517754853, 0.00023973594943527132, 0.0002323407243238762, 0.0002251736295875162, 0.00021822762209922075, 0.0002114958770107478, 0.00020497178775258362, 0.00019864895148202777, 0.00019252115453127772, 0.00018658238695934415, 0.0001808268134482205, 0.00017524878785479814, 0.000169842824107036, 0.00016460362530779094, 0.00015952604007907212, 0.0001546050771139562, 0.00014983591972850263, 0.00014521388220600784, 0.00014073440979700536, 0.00013639312237501144, 0.0001321857562288642, 0.00012810817861463875, 0.00012415637320373207, 0.00012032647646265104, 0.00011661471944535151, 0.00011301746417302638, 0.00010953117453027517, 0.00010615242354106158, 0.00010287790064467117, 9.970438986783847e-05, 9.6628769824747e-05, 9.364802826894447e-05, 9.075923298951238e-05, 8.79595463629812e-05, 8.524622535333037e-05, 8.261660696007311e-05, 8.006810094229877e-05, 7.759821164654568e-05, 7.52045089029707e-05, 7.288464985322207e-05, 7.063634984660894e-05, 6.84574042679742e-05, 6.634567398577929e-05, 6.429908535210416e-05, 6.23156302026473e-05, 6.039335858076811e-05, 5.8530382375465706e-05, 5.672487532137893e-05, 5.4975062084849924e-05, 5.3279225539881736e-05, 5.163570313015953e-05, 5.004287595511414e-05, 4.8499183321837336e-05, 4.700311183114536e-05, 4.5553188101621345e-05, 4.414799332153052e-05, 4.2786145058926195e-05, 4.146630453760736e-05, 4.018717663711868e-05, 3.8947506254771724e-05, 3.7746078305644915e-05, 3.658171044662595e-05, 3.54532603523694e-05, 3.435962207731791e-05, 3.329971877974458e-05, 3.2272509997710586e-05, 3.127698801108636e-05, 3.03121760225622e-05, 2.937712451966945e-05, 2.847091673174873e-05, 2.7592663172981702e-05, 2.6741501642391086e-05, 2.5916597223840654e-05, 2.5117138648056425e-05, 2.4342340111616068e-05, 2.3591443095938303e-05, 2.286370909132529e-05, 2.2158423234941438e-05, 2.1474894310813397e-05, 2.081245111185126e-05, 2.0170442439848557e-05, 1.9548237105482258e-05, 1.8945225747302175e-05, 1.836081537476275e-05, 1.779443300620187e-05, 1.7245522030862048e-05, 1.671354220889043e-05, 1.6197973309317604e-05, 1.5698307834099978e-05, 1.5214056475088e-05, 1.4744742657057941e-05, 1.4289906175690703e-05, 1.3849100469087716e-05, 1.3421891708276235e-05, 1.3007861525693443e-05, 1.2606603377207648e-05, 1.2217722542118281e-05, 1.18408379421453e-05, 1.147557941294508e-05, 1.1121587704110425e-05, 1.0778516298159957e-05, 1.0446027772559319e-05, 1.0123795618710574e-05, 9.811503332457505e-06, 9.508844414085615e-06, 9.215521458827425e-06, 8.931247066357173e-06, 8.655742021801416e-06, 8.388735295739025e-06, 8.12996495369589e-06, 7.879177246650215e-06, 7.636125701537821e-06, 7.400571121252142e-06, 7.172282948886277e-06, 6.95103699399624e-06, 6.736615887348307e-06, 6.528809080919018e-06, 6.327412393147824e-06, 6.1322284636844415e-06, 5.9430653891467955e-06, 5.759737632615725e-06, 5.582064659392927e-06, 5.409872755990364e-06, 5.242992301646154e-06, 5.08125958731398e-06, 4.924515906168381e-06, 4.772607553604757e-06, 4.625385372492019e-06, 4.482704298425233e-06, 4.344424723967677e-06, 4.210410679661436e-06, 4.080530743522104e-06, 3.9546571315440815e-06, 3.832666152447928e-06, 3.714438435054035e-06], "accuracy_valid": [0.3839067206325301, 0.468505859375, 0.5280982327748494, 0.5807737787085843, 0.6159815041415663, 0.6403470326618976, 0.6688423616340362, 0.6901031861822289, 0.6944065323795181, 0.7056267060429217, 0.7234901520143072, 0.7279567488704819, 0.7345985504518072, 0.7380474044615963, 0.7438052993222892, 0.7450568877070783, 0.7526870176016567, 0.7460025649472892, 0.7554034497364458, 0.7575904202748494, 0.7642219267695783, 0.7665221432605422, 0.7701842526355422, 0.7668265836784638, 0.7739684323230422, 0.7710387448230422, 0.7721579678087349, 0.7739787274096386, 0.7770304852221386, 0.7785056240587349, 0.7820147778614458, 0.7795321912650602, 0.7820353680346386, 0.7845282497176205, 0.783459031438253, 0.781872117375753, 0.7904788097703314, 0.7945483104292168, 0.7939276637801205, 0.7893698818712349, 0.7939070736069277, 0.7934393825301205, 0.7917201030685241, 0.7924731151167168, 0.7939482539533133, 0.7983839655496988, 0.7967146907944277, 0.7948939311935241, 0.8016695689006024, 0.800316500376506, 0.7941203289721386, 0.7930216961596386, 0.7982001247176205, 0.7999399943524097, 0.802635836314006, 0.8010180369917168, 0.8009062617658133, 0.8026667215737951, 0.8003973903426205, 0.8001841349774097, 0.7998885189194277, 0.7976603680346386, 0.8016989834337349, 0.7986884059676205, 0.7987192912274097, 0.8048433970256024, 0.804466891001506, 0.8048433970256024, 0.8000002941453314, 0.8026255412274097, 0.8012415874435241, 0.805077242564006, 0.8046904414533133, 0.8040697948042168, 0.8004988704819277, 0.8029711619917168, 0.8030829372176205, 0.7997561535203314, 0.8030829372176205, 0.8013430675828314, 0.8026255412274097, 0.805931734751506, 0.8043242305158133, 0.8041918651167168, 0.8045786662274097, 0.8056772990399097, 0.8046698512801205, 0.805565523814006, 0.8045580760542168, 0.8057787791792168, 0.8067656367658133, 0.8056772990399097, 0.805931734751506, 0.807274508189006, 0.8081392954631024, 0.8076304240399097, 0.8060332148908133, 0.8076098338667168, 0.8084746211408133, 0.8055552287274097, 0.8083628459149097, 0.8068877070783133, 0.8067759318524097, 0.8076098338667168, 0.8068877070783133, 0.8083525508283133, 0.807884859751506, 0.8068774119917168, 0.8072436229292168, 0.8067450465926205, 0.8061346950301205, 0.8055243434676205, 0.8071421427899097, 0.8055449336408133, 0.8070097773908133, 0.8071009624435241, 0.8062876506024097, 0.8069994823042168, 0.8063994258283133, 0.807396578501506, 0.8075083537274097, 0.8061243999435241, 0.807762789439006, 0.8071009624435241, 0.8064906108810241, 0.8067759318524097, 0.8058905544051205, 0.8053919780685241, 0.8073451030685241, 0.8077319041792168, 0.8082201854292168, 0.8066332713667168, 0.8073656932417168, 0.8093291133283133, 0.8064803157944277, 0.8074568782944277, 0.8074980586408133, 0.8092070430158133, 0.8073862834149097, 0.8056361186935241, 0.8084643260542168, 0.8082304805158133, 0.8093188182417168, 0.8077113140060241, 0.8071009624435241, 0.8079760448042168, 0.8068568218185241, 0.8063788356551205, 0.8078436794051205, 0.8068568218185241, 0.8080878200301205, 0.8074774684676205, 0.8073451030685241, 0.8073553981551205, 0.8078539744917168, 0.8082201854292168, 0.8053816829819277, 0.8066126811935241, 0.8071112575301205, 0.8057581890060241, 0.8074877635542168, 0.8074774684676205, 0.8069891872176205, 0.8069788921310241, 0.8068774119917168, 0.8077113140060241, 0.8073862834149097, 0.8072333278426205, 0.8063788356551205, 0.8043948253953314, 0.8046492611069277, 0.8052596126694277, 0.8068671169051205, 0.8066126811935241, 0.8067450465926205, 0.8076304240399097, 0.8058905544051205, 0.8070200724774097, 0.8071318477033133, 0.8083628459149097, 0.8067450465926205, 0.8058905544051205, 0.8041609798569277, 0.8062464702560241], "accuracy_test": 0.8011200573979591, "start": "2016-01-25 20:20:01.719000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0], "accuracy_train_last": 0.9381854682885751, "batch_size_eval": 1024, "accuracy_train_std": [0.014905300153963368, 0.015995479877092867, 0.01497286333472667, 0.017056212093946598, 0.015532333792055628, 0.017266804926823472, 0.015656886770658393, 0.016521585509219994, 0.01604771624160098, 0.01654688739985136, 0.014827226710241666, 0.016702854024481337, 0.017781238961552728, 0.016854477619252836, 0.01588512233135236, 0.01655449519365519, 0.014821958440525083, 0.013629552281820324, 0.016662503878382685, 0.01578856607019148, 0.015484910757610357, 0.016337736939833713, 0.015095180579490558, 0.017126247330944606, 0.014763954383054253, 0.0153482092681426, 0.0160262798468949, 0.01454465332870414, 0.01520792437427748, 0.014281857823655113, 0.015166180989167236, 0.01291215844336585, 0.014143456043546667, 0.014050234526626732, 0.014570292534234232, 0.013940311134394465, 0.01335565047487472, 0.013281474097225573, 0.013524575877678888, 0.014038039868319042, 0.014614223885314019, 0.014334525748916867, 0.013815803805019173, 0.013278245015702997, 0.01215359298845136, 0.012771284134911754, 0.012560226634732437, 0.012581767201984702, 0.012759771880107658, 0.01295786708321032, 0.012199950129017865, 0.012523496556275512, 0.01099863603827297, 0.012191637563444074, 0.012296836237554204, 0.012239923961359454, 0.011143578840493362, 0.01167659033830689, 0.010514024310955142, 0.012338395522622, 0.011808860449418309, 0.011558459128541048, 0.01153631198265734, 0.010611074081781127, 0.011527753712897128, 0.011099348472858052, 0.011164956289292602, 0.010772821167104004, 0.010499777590202009, 0.010398200699561398, 0.011439254914030516, 0.010781644956118375, 0.01034798350738919, 0.011533242957235427, 0.010649937685963378, 0.011589678303289247, 0.011038475520781332, 0.011066634478017487, 0.011052830586773567, 0.010779945371881462, 0.010351569039537925, 0.009882974969970641, 0.01016111824410062, 0.010269674564539212, 0.009736306482902845, 0.009228931811422902, 0.00935688873530811, 0.009290296207762037, 0.009653613320734366, 0.01002762910259382, 0.00926089948977186, 0.008872056517309395, 0.008293351246140935, 0.008866062044654031, 0.008807683647540924, 0.009570188001890741, 0.009138786403242467, 0.009405210576513081, 0.009023687445306362, 0.009243798730278224, 0.009274159447991443, 0.009188697023541234, 0.009059040160066251, 0.008175038765340495, 0.008487131816263841, 0.008552959285107877, 0.008706035859148418, 0.008566912681666544, 0.009119093944183376, 0.008371910921195921, 0.00822731833568749, 0.009076912136988413, 0.008221844191771873, 0.008454466320975604, 0.008647324857617617, 0.008278134637240355, 0.009621802052853201, 0.009126274558357754, 0.009219727198200137, 0.008592772205375267, 0.00854320855407708, 0.00906958698805489, 0.00862352890123902, 0.008421718349702696, 0.008209433791726619, 0.008480238495229653, 0.008589954377315731, 0.008910566459693285, 0.00835146248020354, 0.008416145545531835, 0.0077162624048957625, 0.008558078867702852, 0.008229195306144929, 0.008594484893244232, 0.007897910970213652, 0.008759901408799573, 0.008925028850742176, 0.008177385137154805, 0.007819648950243605, 0.00768766561790508, 0.007865941664954293, 0.00848574837332879, 0.007721024541890192, 0.00833705066055288, 0.008092457800471697, 0.00862337375809649, 0.008476599967222284, 0.008512747180337819, 0.0077876547182844065, 0.008171117092198811, 0.007686603392647133, 0.00872291835526479, 0.007528251531927813, 0.008082923118861624, 0.008883538082061139, 0.008728399333422848, 0.007999075720389565, 0.00797026014601589, 0.008341910114164993, 0.008549618575772343, 0.007674720874668599, 0.008172878191536753, 0.00869052998834831, 0.007639780157224382, 0.007932447551723608, 0.00844749307937534, 0.008086028180174471, 0.008101506999543442, 0.0077844162523134095, 0.00791145861284934, 0.008284845771894945, 0.007893341469674743, 0.007950024010257102, 0.008247770098718639, 0.007948075743720955, 0.008568686341373356, 0.008311236189221628, 0.007642382123507156, 0.008039024344054705, 0.007905112276081008, 0.007910952937829792, 0.008080031529909869, 0.008187200453913806, 0.0069519748271344274], "accuracy_test_std": 0.007568898275144469, "error_valid": [0.6160932793674698, 0.531494140625, 0.47190176722515065, 0.41922622129141573, 0.38401849585843373, 0.35965296733810237, 0.3311576383659638, 0.3098968138177711, 0.3055934676204819, 0.29437329395707834, 0.2765098479856928, 0.2720432511295181, 0.2654014495481928, 0.26195259553840367, 0.2561947006777108, 0.25494311229292166, 0.24731298239834332, 0.2539974350527108, 0.2445965502635542, 0.24240957972515065, 0.23577807323042166, 0.23347785673945776, 0.22981574736445776, 0.2331734163215362, 0.22603156767695776, 0.22896125517695776, 0.2278420321912651, 0.22602127259036142, 0.22296951477786142, 0.2214943759412651, 0.2179852221385542, 0.22046780873493976, 0.21796463196536142, 0.21547175028237953, 0.21654096856174698, 0.21812788262424698, 0.20952119022966864, 0.2054516895707832, 0.20607233621987953, 0.2106301181287651, 0.2060929263930723, 0.20656061746987953, 0.20827989693147586, 0.2075268848832832, 0.20605174604668675, 0.20161603445030118, 0.2032853092055723, 0.20510606880647586, 0.19833043109939763, 0.19968349962349397, 0.20587967102786142, 0.20697830384036142, 0.20179987528237953, 0.2000600056475903, 0.19736416368599397, 0.1989819630082832, 0.19909373823418675, 0.19733327842620485, 0.19960260965737953, 0.1998158650225903, 0.2001114810805723, 0.20233963196536142, 0.1983010165662651, 0.20131159403237953, 0.2012807087725903, 0.19515660297439763, 0.19553310899849397, 0.19515660297439763, 0.19999970585466864, 0.1973744587725903, 0.19875841255647586, 0.19492275743599397, 0.19530955854668675, 0.1959302051957832, 0.1995011295180723, 0.1970288380082832, 0.19691706278237953, 0.20024384647966864, 0.19691706278237953, 0.19865693241716864, 0.1973744587725903, 0.19406826524849397, 0.19567576948418675, 0.1958081348832832, 0.1954213337725903, 0.1943227009600903, 0.19533014871987953, 0.19443447618599397, 0.1954419239457832, 0.1942212208207832, 0.19323436323418675, 0.1943227009600903, 0.19406826524849397, 0.19272549181099397, 0.19186070453689763, 0.1923695759600903, 0.19396678510918675, 0.1923901661332832, 0.19152537885918675, 0.1944447712725903, 0.1916371540850903, 0.19311229292168675, 0.1932240681475903, 0.1923901661332832, 0.19311229292168675, 0.19164744917168675, 0.19211514024849397, 0.1931225880082832, 0.1927563770707832, 0.19325495340737953, 0.19386530496987953, 0.19447565653237953, 0.1928578572100903, 0.19445506635918675, 0.19299022260918675, 0.19289903755647586, 0.1937123493975903, 0.1930005176957832, 0.19360057417168675, 0.19260342149849397, 0.1924916462725903, 0.19387560005647586, 0.19223721056099397, 0.19289903755647586, 0.19350938911897586, 0.1932240681475903, 0.19410944559487953, 0.19460802193147586, 0.19265489693147586, 0.1922680958207832, 0.1917798145707832, 0.1933667286332832, 0.1926343067582832, 0.19067088667168675, 0.1935196842055723, 0.1925431217055723, 0.19250194135918675, 0.19079295698418675, 0.1926137165850903, 0.19436388130647586, 0.1915356739457832, 0.19176951948418675, 0.1906811817582832, 0.19228868599397586, 0.19289903755647586, 0.1920239551957832, 0.19314317818147586, 0.19362116434487953, 0.19215632059487953, 0.19314317818147586, 0.19191217996987953, 0.19252253153237953, 0.19265489693147586, 0.19264460184487953, 0.1921460255082832, 0.1917798145707832, 0.1946183170180723, 0.19338731880647586, 0.19288874246987953, 0.19424181099397586, 0.1925122364457832, 0.19252253153237953, 0.19301081278237953, 0.19302110786897586, 0.1931225880082832, 0.19228868599397586, 0.1926137165850903, 0.19276667215737953, 0.19362116434487953, 0.19560517460466864, 0.1953507388930723, 0.1947403873305723, 0.19313288309487953, 0.19338731880647586, 0.19325495340737953, 0.1923695759600903, 0.19410944559487953, 0.1929799275225903, 0.19286815229668675, 0.1916371540850903, 0.19325495340737953, 0.19410944559487953, 0.1958390201430723, 0.19375352974397586], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "momentum": 0.8853148882987936, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.0011850375097687594, "patience_threshold": 1, "do_flip": true, "batch_size": 16, "l2_decay": 2.4628196254051695e-07, "optimization": "nesterov_momentum", "nb_data_augmentation": 1, "learning_rate_decay_method": "exp", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.030847368905729013}, "accuracy_valid_max": 0.8093291133283133, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        #nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        nb_data_augmentation=make_constant_param(1),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8062464702560241, "loss_train": [1.9437956809997559, 1.6259324550628662, 1.4741829633712769, 1.3600200414657593, 1.2595056295394897, 1.1735323667526245, 1.1081821918487549, 1.0530582666397095, 1.0086151361465454, 0.9697644114494324, 0.9327481389045715, 0.9034187197685242, 0.8754727244377136, 0.8495370745658875, 0.8274818062782288, 0.8040762543678284, 0.7853023409843445, 0.765150785446167, 0.7442463636398315, 0.7275302410125732, 0.7152194976806641, 0.7002466917037964, 0.6877062916755676, 0.6722540855407715, 0.660047709941864, 0.6448275446891785, 0.6344550251960754, 0.6250997185707092, 0.616718590259552, 0.608197033405304, 0.5944966077804565, 0.5875492691993713, 0.5806748270988464, 0.5746263265609741, 0.5626436471939087, 0.5536792874336243, 0.5507673621177673, 0.541002094745636, 0.534915030002594, 0.5294708013534546, 0.5247989892959595, 0.5135443210601807, 0.5087841749191284, 0.5053262710571289, 0.5015174150466919, 0.4938080310821533, 0.49366071820259094, 0.4865935742855072, 0.478451669216156, 0.4780846834182739, 0.4726034104824066, 0.4684775471687317, 0.46338000893592834, 0.45579543709754944, 0.45429202914237976, 0.44897764921188354, 0.4468158483505249, 0.44074031710624695, 0.4426814317703247, 0.43769779801368713, 0.4366741180419922, 0.4307849407196045, 0.43053165078163147, 0.427869588136673, 0.4237065613269806, 0.4207031726837158, 0.41691726446151733, 0.41815418004989624, 0.4125380218029022, 0.41079697012901306, 0.4064520001411438, 0.4058281481266022, 0.40389284491539, 0.4031457304954529, 0.3992205858230591, 0.3969757854938507, 0.39668455719947815, 0.39123594760894775, 0.39558544754981995, 0.3925146758556366, 0.3907976448535919, 0.38442331552505493, 0.3877154290676117, 0.38374730944633484, 0.3823755085468292, 0.38182365894317627, 0.38216739892959595, 0.3809521198272705, 0.3789580166339874, 0.37451857328414917, 0.3757810890674591, 0.3721012771129608, 0.3710881471633911, 0.37247174978256226, 0.37032976746559143, 0.3698573112487793, 0.36977797746658325, 0.36877933144569397, 0.3666413724422455, 0.3658456802368164, 0.36477991938591003, 0.36754703521728516, 0.3635025918483734, 0.36138585209846497, 0.3635377585887909, 0.362605482339859, 0.35882502794265747, 0.3613552749156952, 0.3596131205558777, 0.35954129695892334, 0.3557252883911133, 0.3560847342014313, 0.3545767068862915, 0.35562607645988464, 0.3563156723976135, 0.35731643438339233, 0.3553784489631653, 0.35253000259399414, 0.3559234142303467, 0.35465022921562195, 0.3568587601184845, 0.3534403443336487, 0.3553840219974518, 0.35235872864723206, 0.35315266251564026, 0.3531651794910431, 0.3508426547050476, 0.35260289907455444, 0.35017240047454834, 0.35294729471206665, 0.34963124990463257, 0.34912222623825073, 0.349727988243103, 0.3477858603000641, 0.3510718047618866, 0.3492806553840637, 0.34460505843162537, 0.34940987825393677, 0.34651264548301697, 0.3484545350074768, 0.3445136249065399, 0.34457436203956604, 0.34774693846702576, 0.3463345468044281, 0.3439737856388092, 0.346116304397583, 0.3430747985839844, 0.3428329825401306, 0.3447692394256592, 0.3479272723197937, 0.3446313738822937, 0.34349602460861206, 0.34300869703292847, 0.3445112705230713, 0.34346216917037964, 0.3436032235622406, 0.34413766860961914, 0.34342730045318604, 0.34591060876846313, 0.3409772217273712, 0.3406631052494049, 0.34541961550712585, 0.3442046642303467, 0.34340983629226685, 0.34483081102371216, 0.3389270007610321, 0.34232544898986816, 0.33924758434295654, 0.3418894410133362, 0.34135594964027405, 0.3420700132846832, 0.3404844403266907, 0.3417031168937683, 0.34227490425109863, 0.3425601124763489, 0.34110528230667114, 0.34196415543556213, 0.33946946263313293, 0.33967798948287964, 0.34344640374183655, 0.3385390043258667, 0.33933815360069275, 0.3434053361415863, 0.34106189012527466], "accuracy_train_first": 0.3761809593023256, "model": "residualv5", "loss_std": [0.2113427072763443, 0.21301668882369995, 0.22764523327350616, 0.2364896833896637, 0.2406366765499115, 0.24392490088939667, 0.24577639997005463, 0.24725383520126343, 0.2482684850692749, 0.24638885259628296, 0.244298055768013, 0.24462592601776123, 0.24608266353607178, 0.24346061050891876, 0.2427283674478531, 0.24207261204719543, 0.23960986733436584, 0.2358369529247284, 0.23249199986457825, 0.23283112049102783, 0.23089809715747833, 0.23121917247772217, 0.2276080995798111, 0.22338278591632843, 0.22406908869743347, 0.22135232388973236, 0.22403204441070557, 0.21754281222820282, 0.21560640633106232, 0.2170235812664032, 0.21187250316143036, 0.21279017627239227, 0.2085873931646347, 0.21152646839618683, 0.20515212416648865, 0.20541734993457794, 0.2049219161272049, 0.202309712767601, 0.20432503521442413, 0.20133134722709656, 0.198277086019516, 0.19908641278743744, 0.19697454571723938, 0.19686982035636902, 0.19423064589500427, 0.1930665522813797, 0.19296613335609436, 0.19170819222927094, 0.18767108023166656, 0.188487708568573, 0.18706409633159637, 0.1850954294204712, 0.1874152570962906, 0.1850980818271637, 0.18542355298995972, 0.1832314133644104, 0.17980338633060455, 0.1786462813615799, 0.17844629287719727, 0.1786690652370453, 0.17645616829395294, 0.17556774616241455, 0.1763707995414734, 0.17958611249923706, 0.17278504371643066, 0.1725759208202362, 0.17328320443630219, 0.1738860160112381, 0.168885737657547, 0.1721017211675644, 0.16962628066539764, 0.168547585606575, 0.16793645918369293, 0.16818980872631073, 0.16796573996543884, 0.16868311166763306, 0.16786149144172668, 0.16659878194332123, 0.16757933795452118, 0.16472841799259186, 0.1672484129667282, 0.16170169413089752, 0.16572661697864532, 0.16486209630966187, 0.16409167647361755, 0.16421687602996826, 0.16184188425540924, 0.16167862713336945, 0.16272567212581635, 0.16227951645851135, 0.16026805341243744, 0.1608838438987732, 0.1628548949956894, 0.15997307002544403, 0.16259537637233734, 0.158678337931633, 0.16131165623664856, 0.1624370813369751, 0.15637537837028503, 0.1577857881784439, 0.15896324813365936, 0.15767981112003326, 0.1606893390417099, 0.15772901475429535, 0.15868675708770752, 0.15841275453567505, 0.15645724534988403, 0.15493299067020416, 0.15681639313697815, 0.1570318341255188, 0.15691721439361572, 0.15906070172786713, 0.1554000973701477, 0.1541900485754013, 0.1581239253282547, 0.16039049625396729, 0.1551906168460846, 0.15884904563426971, 0.15828000009059906, 0.15572482347488403, 0.1571047455072403, 0.15768103301525116, 0.1597031205892563, 0.1525033563375473, 0.15661565959453583, 0.15628717839717865, 0.153773233294487, 0.15493644773960114, 0.15497684478759766, 0.15451578795909882, 0.1548541784286499, 0.15628397464752197, 0.1549316793680191, 0.15656563639640808, 0.15267710387706757, 0.1534745693206787, 0.1525978147983551, 0.15404599905014038, 0.15154841542243958, 0.15446440875530243, 0.15334045886993408, 0.15106040239334106, 0.15212582051753998, 0.1532079428434372, 0.15421216189861298, 0.15109044313430786, 0.1513027548789978, 0.15385545790195465, 0.15483194589614868, 0.15595108270645142, 0.15151122212409973, 0.15379486978054047, 0.15202100574970245, 0.1510656177997589, 0.15183499455451965, 0.15129096806049347, 0.15188820660114288, 0.15493842959403992, 0.15577656030654907, 0.15122924745082855, 0.15065719187259674, 0.15223629772663116, 0.15016065537929535, 0.1528492569923401, 0.15453502535820007, 0.15153580904006958, 0.1496099829673767, 0.1501154601573944, 0.15118218958377838, 0.151909738779068, 0.1539480984210968, 0.1521865874528885, 0.15100806951522827, 0.15529870986938477, 0.1535143107175827, 0.1483251452445984, 0.1512065827846527, 0.15134954452514648, 0.15083467960357666, 0.15436811745166779, 0.14814186096191406, 0.14968502521514893, 0.15216362476348877, 0.15263411402702332]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:17 2016", "state": "available"}], "summary": "a5e35bf79f0f7b55c4e9c4bc940694c3"}