{"content": {"hp_model": {"f0": 64, "f1": 32, "f2": 16, "f3": 16, "nonlin": "leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.01912343648407426, 0.009180018845794977, 0.00823766788063034, 0.015551672577382486, 0.011576340549733326, 0.011328811827469018, 0.01043529306117661, 0.009148832342228309, 0.010115220330550944, 0.010343420971029514, 0.013408784471509148, 0.014804140200220802, 0.010638760375492396, 0.010773660934004732, 0.009707797758861925, 0.010537941129346935, 0.010206966333838477, 0.012623648111551021, 0.010046540465769523, 0.013061517972576037, 0.010532563539836363, 0.00978321844587127, 0.009150804054546623, 0.011921897792066946, 0.009407067423996886, 0.007654491232733571, 0.009366956239976158, 0.009851817161194062, 0.010213454357521879, 0.0126224749229307, 0.009927518558830704, 0.00925832780279143, 0.01122581128567433, 0.009859496582624905, 0.006699434687721501, 0.007455273288626215, 0.010494526575032314, 0.010796112630295843, 0.011896505560179084, 0.007523979417522686, 0.011714747548263417, 0.013170751204559976, 0.01658055663324343, 0.011773237712456416, 0.01020449539216661, 0.013399297734471195, 0.014678727958824694, 0.012049811323442483, 0.012112974655076488, 0.014054178281719282, 0.012043598746935493, 0.012998140659263316, 0.010875510301198179, 0.012233848313767788, 0.011618014882089767, 0.015104171080971828, 0.010928733666988232, 0.011560953987256345, 0.012118190688653958, 0.014926321874918753, 0.014701957387799041, 0.013626740113552829, 0.013679128539381415, 0.016014391052560497, 0.014098897161507352, 0.01207544042070831, 0.012544013455309689, 0.011834934298832797, 0.010479467197047362, 0.01201316325699386, 0.01520260138249492, 0.014843805020999303, 0.01343858694831149, 0.012868091091591585, 0.01267420829588141, 0.011196334770006137, 0.013123721669000436, 0.014160993323521527, 0.014621772109053495, 0.011857192041638202, 0.014738077196137199, 0.012441433753456571, 0.010930798985897968, 0.014692771515631208, 0.013252116198621692, 0.010903327466835068, 0.009240379773321209, 0.011765374140990572, 0.01562723223306196, 0.01113427231266513, 0.013397483874775072, 0.010354918382271637, 0.005537551094465402, 0.010638834484145626, 0.010668675800899387, 0.010258296289974819, 0.009316047370887202, 0.011323828160140666, 0.009593432511715957, 0.008864235859606493, 0.007768350356126809, 0.009868162366117933, 0.009483893236434391, 0.011488071012399351, 0.007258572316885454, 0.007789068346718777, 0.008819982829573013, 0.011972109490927961, 0.008899252641068523, 0.00938861017455153, 0.010340085669283827, 0.008425850391670936, 0.007998003443259643, 0.009116388958216218, 0.006568875772565148, 0.008078231082108416, 0.008809072588697819, 0.008499982612227407, 0.009611861124119922, 0.007555868754721857, 0.011434601444506962, 0.007885515581852083, 0.008177375643543459, 0.010646073194486511, 0.01071517948703181, 0.00883178778258502, 0.01045218811620606, 0.008045042048030759, 0.007595492601262789, 0.009397010678545452, 0.01108261502602022, 0.011145358642908672, 0.009235093551944242, 0.008782639752129732, 0.01067567393754417, 0.010369939928234855, 0.009312876707442587, 0.010469380282344277, 0.008955149663196193, 0.010130441116965275, 0.008430695765837404, 0.009575573950690518, 0.010678095300884979, 0.01059456891201631, 0.009989887295390071, 0.008896831451395868, 0.009916365402730606, 0.010369997185267548, 0.008679935050573698, 0.00758480857940069, 0.007924283824245213, 0.012878327585884773, 0.013669581939149484, 0.009131175081614434, 0.009486995344416672, 0.007841728621370168, 0.011761344980419948, 0.01002292220799113, 0.010860597302399801, 0.010649084663966715, 0.010302853725460673, 0.011687284633615586, 0.007039998432821171, 0.00988479926538827, 0.009952025958458838, 0.008648426997057911, 0.009091067555616898, 0.011315019259837755, 0.010303874849355818, 0.011119252133290753, 0.008848176612750879, 0.011065119883540208, 0.011064608782834329, 0.009042096380135497, 0.011082323432417785, 0.00937912266553989, 0.010000904821252416, 0.010009806682521554, 0.00829212944691737, 0.009362641275515646, 0.009398588684193484, 0.009488775972925877, 0.01080387102892966, 0.010815748131592293, 0.011559897955213446, 0.008688150764132646, 0.00810831173456193, 0.005763068254717332, 0.012073425701312705, 0.009428578768375053, 0.009936508695828208, 0.010148315744922531, 0.008402368352905365, 0.008770089804267004, 0.00890654188984833, 0.011042237285604243, 0.009900089294671125, 0.010045086643731207, 0.009508668910846509, 0.00880090646983727, 0.010856496942469788, 0.007362473360969255, 0.009774228394371618, 0.010417166668044164, 0.008781914657660438, 0.00822459592075176], "moving_avg_accuracy_train": [0.05307379265296234, 0.10939084165340068, 0.16911470607956114, 0.22718852755355406, 0.28171228983718904, 0.33261564081263406, 0.37946077042023185, 0.423220837106291, 0.46323494035348767, 0.4999985481949846, 0.5336182543297128, 0.5646663962509313, 0.5928236016216853, 0.6187928405851167, 0.6424858098950067, 0.664037491052516, 0.6836731699822866, 0.701649983659584, 0.7181708767953329, 0.7332512331103547, 0.7470536714283798, 0.7596897796050786, 0.7712692512569738, 0.7818230929329097, 0.7915796419591093, 0.8004653119743925, 0.8086693892810138, 0.8162297701664968, 0.8232664836491088, 0.8296483178596409, 0.8354292070788909, 0.8407179657845308, 0.8457801540136636, 0.8503360873710643, 0.854441221885619, 0.8583775502784524, 0.8619830608986785, 0.8654208636127974, 0.8684918148115028, 0.8713650970796604, 0.8741228958399944, 0.8765792659897528, 0.878957409838821, 0.8812861484541914, 0.8833052832973105, 0.8853968361668227, 0.8871606151112792, 0.8889944458862808, 0.8906775538135719, 0.8922458654195717, 0.8935875553518671, 0.8949369103683138, 0.896190821364059, 0.8973844093780776, 0.8985561506454194, 0.8996061756348642, 0.90062789198726, 0.9016356842127219, 0.9025615146990263, 0.903394653990244, 0.9041189387642539, 0.9050264172346354, 0.9057527193961386, 0.9064179810367203, 0.907163200888244, 0.9078291042617211, 0.9085887804680702, 0.9092446233168888, 0.909888324254626, 0.9104607877986169, 0.9110295555084654, 0.9116484753902047, 0.9122076481884858, 0.9125830565712337, 0.9133882069287836, 0.9140267396469887, 0.9145107022409831, 0.9149465209173093, 0.9153339632331088, 0.9158198450970902, 0.9163338326365691, 0.9168591643911385, 0.9173552144583461, 0.9177971534164889, 0.9182111024228466, 0.9186277983071308, 0.9190701818208252, 0.9193311792522071, 0.9197334876547364, 0.9201189609003831, 0.9204564059821333, 0.920741685609326, 0.9209402725047426, 0.9211189646617989, 0.9215331206769313, 0.921875598107208, 0.9221675157039715, 0.9224000146065349, 0.9226163832605454, 0.9228738580181933, 0.9232310712381533, 0.9235549243337455, 0.9237743846043208, 0.9240322806192486, 0.9242969030671982, 0.9246397670644191, 0.9248389905702326, 0.9250439044111882, 0.9252935031323525, 0.9255900773992205, 0.9258266952072406, 0.9260817202059425, 0.9263761666273471, 0.9265877260328109, 0.926852570308452, 0.9271001947029482, 0.9272301588520702, 0.927412014460034, 0.9275084354358191, 0.9277555053866078, 0.927864080245926, 0.9280128787954844, 0.9281305935460578, 0.9283691063525354, 0.928513941316442, 0.9287698508196723, 0.9289351012547318, 0.9291349799200946, 0.9291962881296356, 0.9292887760456311, 0.9294949596663817, 0.9295413764846734, 0.9298944692198808, 0.9300426610137473, 0.9302641729900766, 0.9304124525926009, 0.9306366210872629, 0.9308616602693727, 0.9310433412916234, 0.9311998427164017, 0.9314894314248741, 0.9316685729077097, 0.9318625325696889, 0.9320044360356805, 0.9321855915288733, 0.932371882960842, 0.9325233052567659, 0.9327108106945444, 0.9329097564742501, 0.9329818147819285, 0.933202488277896, 0.9333033660766291, 0.933331341028813, 0.9334379707917494, 0.9335129030926501, 0.9337663180194039, 0.9339479605749295, 0.9340487319546827, 0.9342021331166803, 0.9343960337827254, 0.9345822782726699, 0.9346591093635923, 0.9347491836847082, 0.9349047995308922, 0.9349378248495822, 0.9350675290352127, 0.9352446085248716, 0.935448085795308, 0.9355824232625196, 0.9356453424580469, 0.9358019871816496, 0.9358941032590735, 0.9360583158394509, 0.9362224553010945, 0.9363747229677366, 0.936463007840352, 0.93655881236501, 0.9366426391907552, 0.9367947771958215, 0.9369131002099048, 0.9370707081475707, 0.937270684011708, 0.9374065565596882, 0.937540323401643, 0.9376073432832394, 0.9376746726719236, 0.9377561954610251, 0.9379457513140551, 0.938083907644905, 0.9381406748319188, 0.9383103118406981, 0.938430469114085, 0.9385316352137046, 0.9386739100748093, 0.9387531293248035, 0.9387919827129212, 0.9388501301526848, 0.9390093836448915, 0.939010949808134, 0.9390495256871859, 0.9391190130640192, 0.9393071097388834, 0.9393764874450893, 0.9395202354913703, 0.9396567283747268, 0.9396771212268535], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 869795054, "moving_var_accuracy_train": [0.025351447199126756, 0.05136079255227405, 0.07832717313499622, 0.10084757448683543, 0.11751838292021294, 0.12908690489295566, 0.1359284099152338, 0.1395700598510255, 0.14002320999401877, 0.13818495474832726, 0.13453902103876386, 0.1297610029857286, 0.12392035661577312, 0.11759793330525399, 0.11089035112720307, 0.10398159066091724, 0.09705347057812283, 0.09025661599020493, 0.08368741358120996, 0.07736542654238115, 0.07134344961984904, 0.06564614572654325, 0.060288288627522335, 0.05526191193185663, 0.0505924329787767, 0.046243785865483555, 0.042225169239012365, 0.03851708654731331, 0.03511101592230935, 0.031966464601094875, 0.02907058626247256, 0.026415266354043638, 0.024004371465643805, 0.021790743077893113, 0.0197633379345471, 0.01792645627203843, 0.01625080800632765, 0.014732093593205712, 0.013343760905268617, 0.012083686573674333, 0.010943767002329402, 0.00990369409091008, 0.008964224795320825, 0.008116609527637198, 0.007341640724505755, 0.006646847992708862, 0.0060101614389221495, 0.0054394117128320225, 0.004920966212203011, 0.004451006002624332, 0.004022106589231703, 0.0036362827609522225, 0.003286805119924257, 0.0029709464790567118, 0.0026862086295293647, 0.0024275107388825544, 0.002194154803737076, 0.001983880129890681, 0.0017932065757059487, 0.0016201330078424905, 0.0014628410029630052, 0.0013239685572345589, 0.0011963193349793415, 0.0010806705589352734, 0.00097760167668569, 0.0008838323547423957, 0.0008006430907145928, 0.0007244499502242529, 0.0006557341132770202, 0.0005931101325321076, 0.0005367105896487939, 0.000486487087064024, 0.00044065244632265937, 0.00039785558477493027, 0.00036390443018180236, 0.00033118350345358734, 0.0003001731312397005, 0.000271865259383443, 0.0002460297373777475, 0.00022355149431168777, 0.00020357399359717475, 0.00018570035530868776, 0.00016934491080040977, 0.0001541682101048884, 0.00014029357311318069, 0.00012782693494167689, 0.00011680557000620709, 0.00010573808993827773, 9.662094940116177e-05, 8.829616106903015e-05, 8.04913676109036e-05, 7.317469104103422e-05, 6.621215273221163e-05, 5.987831544193098e-05, 5.543421074157115e-05, 5.094640677965384e-05, 4.661870905139013e-05, 4.244333980348962e-05, 3.862034437308586e-05, 3.5354949193210096e-05, 3.296786583451634e-05, 3.06150066987866e-05, 2.7986971322156515e-05, 2.5786867380581822e-05, 2.3838406002153353e-05, 2.2512566887249735e-05, 2.0618520245942512e-05, 1.8934575361285273e-05, 1.760181351961845e-05, 1.6633238829571908e-05, 1.5473806830264406e-05, 1.4511765896904224e-05, 1.3840877562915839e-05, 1.2859606244986387e-05, 1.2204928033546122e-05, 1.1536295796938275e-05, 1.0534682337757363e-05, 9.778857263312919e-06, 8.884644578123775e-06, 8.545572165556037e-06, 7.797111449684107e-06, 7.2166693798717975e-06, 6.61971330440761e-06, 6.4697372036514594e-06, 6.011557984214942e-06, 5.999809250385609e-06, 5.645597681932773e-06, 5.440601241544902e-06, 4.930369386404445e-06, 4.5143185792106375e-06, 4.445491890482269e-06, 4.020333390616952e-06, 4.740370368461761e-06, 4.463980601539843e-06, 4.459190542301366e-06, 4.21115305279406e-06, 4.24230137350605e-06, 4.273854937517361e-06, 4.1435413883798596e-06, 3.949621513160447e-06, 4.309413942516996e-06, 4.167297586118129e-06, 4.089150981781852e-06, 3.861465226547583e-06, 3.7706745183180107e-06, 3.705947545110787e-06, 3.541711195925582e-06, 3.503964679101815e-06, 3.5097830205562894e-06, 3.2055363158498753e-06, 3.3232538106669812e-06, 3.0825154020954253e-06, 2.781307243433105e-06, 2.6055056761845483e-06, 2.395488756030595e-06, 2.733912006342311e-06, 2.757466967508452e-06, 2.57311410955401e-06, 2.527589947118478e-06, 2.613208167041437e-06, 2.6640704406495425e-06, 2.450790545375496e-06, 2.27873194075828e-06, 2.2688053709346297e-06, 2.051740878912376e-06, 1.9979753729519335e-06, 2.0803921465773623e-06, 2.244979928177985e-06, 2.1829009312314915e-06, 2.000240264600569e-06, 2.021054363034044e-06, 1.8953172722101964e-06, 1.948477488976566e-06, 1.9961056058970678e-06, 2.0051640260490987e-06, 1.8747955920386797e-06, 1.7699225953394032e-06, 1.6561727662361356e-06, 1.6988692428823237e-06, 1.6549853395501478e-06, 1.713049163732674e-06, 1.90165736349676e-06, 1.8776437707987364e-06, 1.8509215057778464e-06, 1.7062543359629586e-06, 1.5764281215919385e-06, 1.4785989957186966e-06, 1.6541218889082736e-06, 1.6604942458027054e-06, 1.5234474429155226e-06, 1.6300931313523604e-06, 1.5970237513468934e-06, 1.529432593622298e-06, 1.5586685591811136e-06, 1.4592829093897267e-06, 1.3269408903647498e-06, 1.224676924087852e-06, 1.330464304699035e-06, 1.1974399500348508e-06, 1.0910888410329795e-06, 1.025436416782207e-06, 1.241316006958792e-06, 1.1605038013282963e-06, 1.2304249284819943e-06, 1.2750552004965608e-06, 1.1512924962076978e-06], "duration": 240831.100797, "accuracy_train": [0.5307379265296235, 0.6162442826573459, 0.7066294859150055, 0.7498529208194906, 0.772426150389904, 0.7907457995916389, 0.8010669368886121, 0.8170614372808231, 0.8233618695782576, 0.830871018768457, 0.8361956095422666, 0.8440996735418974, 0.8462384499584718, 0.8525159912559985, 0.8557225336840162, 0.8580026214700996, 0.8603942803502216, 0.8634413067552602, 0.8668589150170728, 0.8689744399455519, 0.8712756162906055, 0.8734147531953673, 0.8754844961240311, 0.8768076680163345, 0.8793885831949059, 0.8804363421119417, 0.8825060850406055, 0.8842731981358435, 0.8865969049926172, 0.8870848257544297, 0.8874572100521411, 0.8883167941352897, 0.8913398480758582, 0.8913394875876707, 0.8913874325166113, 0.8938045058139534, 0.8944326564807125, 0.896361088039867, 0.8961303755998523, 0.8972246374930787, 0.8989430846830011, 0.8986865973375784, 0.9003607044804356, 0.9022447959925249, 0.901477496885382, 0.9042208119924326, 0.9030346256113879, 0.9054989228612956, 0.9058255251591916, 0.9063606698735696, 0.9056627647425249, 0.9070811055163345, 0.9074760203257659, 0.9081267015042451, 0.9091018220514949, 0.909056400539867, 0.9098233391588224, 0.9107058142418788, 0.9108939890757659, 0.9108929076112033, 0.9106375017303433, 0.9131937234680694, 0.9122894388496677, 0.9124053358019564, 0.9138701795519564, 0.9138222346230158, 0.9154258663252122, 0.9151472089562569, 0.9156816326942598, 0.9156129596945367, 0.9161484648971022, 0.9172187543258582, 0.9172402033730158, 0.9159617320159652, 0.9206345601467331, 0.9197735341108343, 0.9188663655869325, 0.9188688890042451, 0.9188209440753045, 0.9201927818729235, 0.9209597204918788, 0.9215871501822629, 0.9218196650632153, 0.9217746040397747, 0.9219366434800664, 0.9223780612656883, 0.9230516334440754, 0.9216801561346438, 0.923354263277501, 0.9235882201112033, 0.9234934117178849, 0.9233092022540605, 0.922727554563492, 0.9227271940753045, 0.925260524813123, 0.9249578949796974, 0.9247947740748431, 0.9244925047296051, 0.9245637011466408, 0.9251911308370249, 0.9264459902177926, 0.9264696021940754, 0.9257495270394979, 0.9263533447535992, 0.9266785050987449, 0.9277255430394058, 0.9266320021225545, 0.9268881289797897, 0.9275398916228312, 0.9282592458010337, 0.9279562554794205, 0.9283769451942598, 0.929026184419989, 0.928491760681986, 0.9292361687892212, 0.9293288142534146, 0.9283998361941677, 0.9290487149317092, 0.9283762242178849, 0.9299791349437062, 0.9288412539797897, 0.9293520657415099, 0.9291900263012183, 0.9305157216108343, 0.9298174559916021, 0.9310730363487449, 0.9304223551702658, 0.9309338879083611, 0.929748062015504, 0.9301211672895902, 0.9313506122531378, 0.9299591278492986, 0.9330723038367479, 0.9313763871585455, 0.9322577807770396, 0.9317469690153194, 0.9326541375392212, 0.9328870129083611, 0.9326784704918788, 0.9326083555394058, 0.934095729801126, 0.93328084625323, 0.933608169527501, 0.9332815672296051, 0.933815990967608, 0.9340485058485604, 0.9338861059200813, 0.9343983596345515, 0.9347002684916021, 0.9336303395510337, 0.9351885497416021, 0.9342112662652271, 0.9335831155984681, 0.9343976386581765, 0.9341872938007567, 0.9360470523601883, 0.9355827435746585, 0.9349556743724622, 0.9355827435746585, 0.9361411397771319, 0.9362584786821706, 0.9353505891818937, 0.9355598525747508, 0.9363053421465486, 0.9352350527177926, 0.9362348667058878, 0.9368383239318014, 0.9372793812292359, 0.9367914604674235, 0.9362116152177926, 0.9372117896940754, 0.9367231479558878, 0.9375362290628461, 0.9376997104558878, 0.9377451319675157, 0.9372575716938908, 0.9374210530869325, 0.9373970806224622, 0.9381640192414176, 0.9379780073366556, 0.9384891795865633, 0.9390704667889442, 0.9386294094915099, 0.9387442249792359, 0.938210522217608, 0.9382806371700813, 0.9384899005629384, 0.9396517539913253, 0.9393273146225545, 0.9386515795150425, 0.9398370449197121, 0.9395118845745662, 0.9394421301102805, 0.9399543838247508, 0.9394661025747508, 0.9391416632059801, 0.9393734571105574, 0.9404426650747508, 0.9390250452773165, 0.9393967085986527, 0.9397443994555187, 0.9409999798126615, 0.9400008868009413, 0.9408139679078996, 0.9408851643249354, 0.9398606568959949], "end": "2016-01-24 02:56:23.596000", "learning_rate_per_epoch": [0.0013354418333619833, 0.0006677209166809916, 0.00044514727778732777, 0.0003338604583404958, 0.0002670883550308645, 0.00022257363889366388, 0.00019077739852946252, 0.0001669302291702479, 0.00014838241622783244, 0.00013354417751543224, 0.00012140379840275273, 0.00011128681944683194, 0.00010272629151586443, 9.538869926473126e-05, 8.902945410227403e-05, 8.346511458512396e-05, 7.855540025047958e-05, 7.419120811391622e-05, 7.028641266515478e-05, 6.677208875771612e-05, 6.35924661764875e-05, 6.0701899201376364e-05, 5.80626874580048e-05, 5.564340972341597e-05, 5.341767246136442e-05, 5.1363145757932216e-05, 4.946080662193708e-05, 4.769434963236563e-05, 4.604971763910726e-05, 4.4514727051137015e-05, 4.3078769522253424e-05, 4.173255729256198e-05, 4.046793401357718e-05, 3.927770012523979e-05, 3.815547825070098e-05, 3.709560405695811e-05, 3.609302075346932e-05, 3.514320633257739e-05, 3.424209717195481e-05, 3.338604437885806e-05, 3.2571751944487914e-05, 3.179623308824375e-05, 3.1056784791871905e-05, 3.0350949600688182e-05, 2.967648470075801e-05, 2.90313437290024e-05, 2.8413654945325106e-05, 2.7821704861707985e-05, 2.725391459534876e-05, 2.670883623068221e-05, 2.6185132810496725e-05, 2.5681572878966108e-05, 2.5197015929734334e-05, 2.473040331096854e-05, 2.4280760044348426e-05, 2.3847174816182815e-05, 2.342880361538846e-05, 2.302485881955363e-05, 2.263460737594869e-05, 2.2257363525568508e-05, 2.1892488803132437e-05, 2.1539384761126712e-05, 2.1197489331825636e-05, 2.086627864628099e-05, 2.0545257939375006e-05, 2.023396700678859e-05, 1.9931967472075485e-05, 1.9638850062619895e-05, 1.9354229152668267e-05, 1.907773912535049e-05, 1.88090398296481e-05, 1.8547802028479055e-05, 1.8293723769602366e-05, 1.804651037673466e-05, 1.7805890820454806e-05, 1.7571603166288696e-05, 1.7343400031677447e-05, 1.7121048585977405e-05, 1.6904326912481338e-05, 1.669302218942903e-05, 1.6486936146975495e-05, 1.6285875972243957e-05, 1.6089659766294062e-05, 1.5898116544121876e-05, 1.5711080777691677e-05, 1.5528392395935953e-05, 1.534990587970242e-05, 1.5175474800344091e-05, 1.5004963643150404e-05, 1.4838242350379005e-05, 1.4675184502266347e-05, 1.45156718645012e-05, 1.435958893125644e-05, 1.4206827472662553e-05, 1.4057281987334136e-05, 1.3910852430853993e-05, 1.3767441487289034e-05, 1.362695729767438e-05, 1.3489310731529258e-05, 1.3354418115341105e-05, 1.3222195775597356e-05, 1.3092566405248363e-05, 1.2965454516233876e-05, 1.2840786439483054e-05, 1.271849305339856e-05, 1.2598507964867167e-05, 1.2480764780775644e-05, 1.236520165548427e-05, 1.2251759471837431e-05, 1.2140380022174213e-05, 1.2031006917823106e-05, 1.1923587408091407e-05, 1.1818068742286414e-05, 1.171440180769423e-05, 1.161253749160096e-05, 1.1512429409776814e-05, 1.1414032087486703e-05, 1.1317303687974345e-05, 1.1222199645999353e-05, 1.1128681762784254e-05, 1.1036709111067466e-05, 1.0946244401566219e-05, 1.0857250344997738e-05, 1.0769692380563356e-05, 1.0683534128475003e-05, 1.0598744665912818e-05, 1.0515289432078134e-05, 1.0433139323140495e-05, 1.0352261597290635e-05, 1.0272628969687503e-05, 1.0194212336500641e-05, 1.0116983503394295e-05, 1.0040916095022112e-05, 9.965983736037742e-06, 9.892161870084237e-06, 9.819425031309947e-06, 9.747750482347328e-06, 9.677114576334134e-06, 9.60749457590282e-06, 9.538869562675245e-06, 9.47121861827327e-06, 9.40451991482405e-06, 9.338753443444148e-06, 9.273901014239527e-06, 9.209943527821451e-06, 9.146861884801183e-06, 9.084637895284686e-06, 9.02325518836733e-06, 8.962696483649779e-06, 8.902945410227403e-06, 8.84398559719557e-06, 8.785801583144348e-06, 8.728377906663809e-06, 8.671700015838724e-06, 8.615753358753864e-06, 8.560524292988703e-06, 8.505999176122714e-06, 8.452163456240669e-06, 8.399005309911445e-06, 8.346511094714515e-06, 8.294669896713458e-06, 8.243468073487747e-06, 8.192894711100962e-06, 8.142937986121979e-06, 8.093586984614376e-06, 8.044829883147031e-06, 7.996657586772926e-06, 7.949058272060938e-06, 7.90202284406405e-06, 7.855540388845839e-06, 7.809600901964586e-06, 7.764196197967976e-06, 7.719317181908991e-06, 7.67495293985121e-06, 7.631096195837017e-06, 7.5877374001720455e-06, 7.544868822151329e-06, 7.502481821575202e-06, 7.460568667738698e-06, 7.4191211751895025e-06, 7.378131613222649e-06, 7.337592251133174e-06, 7.297496267710812e-06, 7.2578359322506e-06, 7.218604423542274e-06, 7.17979446562822e-06, 7.141400146792876e-06, 7.103413736331277e-06, 7.065829777275212e-06, 7.028640993667068e-06, 6.9918419285386335e-06, 6.955426215426996e-06, 6.919387487869244e-06, 6.883720743644517e-06, 6.8484196162899025e-06, 6.81347864883719e-06, 6.778892384318169e-06, 6.744655365764629e-06, 6.71076259095571e-06, 6.677209057670552e-06, 6.643988854193594e-06, 6.611097887798678e-06, 6.578531156264944e-06, 6.546283202624181e-06, 6.514350388897583e-06, 6.482727258116938e-06], "accuracy_valid": [0.5170177781438253, 0.6132650720067772, 0.6984024967055723, 0.7373752823795181, 0.7572948042168675, 0.775890672063253, 0.7836325771837349, 0.7950968914721386, 0.8008856715926205, 0.8045786662274097, 0.8123308664344879, 0.8156473550451807, 0.814110445689006, 0.8184034967996988, 0.8212008189006024, 0.8200007059487951, 0.8263689523719879, 0.8278749764683735, 0.8246496729103916, 0.8271322595067772, 0.8249246987951807, 0.8286985833960843, 0.8286779932228916, 0.8292177499058735, 0.8275999505835843, 0.8277014307228916, 0.8322592126317772, 0.8310385095067772, 0.8318827066076807, 0.8344976586031627, 0.8338564217808735, 0.8319944818335843, 0.8360742775790663, 0.8347109139683735, 0.8351683099585843, 0.8339476068335843, 0.8396246117281627, 0.8376406014683735, 0.839756977127259, 0.8377523766942772, 0.839756977127259, 0.8409982704254518, 0.840907085372741, 0.8447824501129518, 0.8385156838290663, 0.8431955360504518, 0.8438264777861446, 0.8450265907379518, 0.8456369423004518, 0.8449251105986446, 0.8417203972138554, 0.8440603233245482, 0.844883930252259, 0.8449045204254518, 0.8452707313629518, 0.843592632247741, 0.841954242752259, 0.8441720985504518, 0.8457693076995482, 0.843714702560241, 0.844325054122741, 0.8459016730986446, 0.8459016730986446, 0.8438470679593373, 0.846522319747741, 0.8442941688629518, 0.8446706748870482, 0.8444162391754518, 0.8445280144013554, 0.8458913780120482, 0.847010600997741, 0.846278179122741, 0.8457796027861446, 0.8436941123870482, 0.8441823936370482, 0.8458707878388554, 0.8467458701995482, 0.8448030402861446, 0.845423686935241, 0.8422086784638554, 0.845667827560241, 0.8445383094879518, 0.8437955925263554, 0.847254741622741, 0.8442941688629518, 0.8436735222138554, 0.8443853539156627, 0.8441720985504518, 0.843958843185241, 0.843785297439759, 0.8467561652861446, 0.8438970726656627, 0.8422689782567772, 0.8438970726656627, 0.8457487175263554, 0.843907367752259, 0.8440191429781627, 0.844273578689759, 0.8440191429781627, 0.8414350762424698, 0.8442529885165663, 0.845372211502259, 0.846226703689759, 0.8460031532379518, 0.8436323418674698, 0.8441206231174698, 0.8439985528049698, 0.8430631706513554, 0.8437647072665663, 0.8454839867281627, 0.845372211502259, 0.8449854103915663, 0.8474371117281627, 0.846592914627259, 0.8436323418674698, 0.8454839867281627, 0.8448633400790663, 0.8443750588290663, 0.8431440606174698, 0.8443647637424698, 0.8456369423004518, 0.8462164086031627, 0.8447206795933735, 0.843907367752259, 0.8451383659638554, 0.8433984963290663, 0.845494281814759, 0.8441206231174698, 0.8438764824924698, 0.8456060570406627, 0.8448942253388554, 0.8452398461031627, 0.8463384789156627, 0.8448736351656627, 0.8467252800263554, 0.8443750588290663, 0.8460943382906627, 0.8473356315888554, 0.8480474632906627, 0.845372211502259, 0.8468267601656627, 0.846592914627259, 0.8455045769013554, 0.8458707878388554, 0.846592914627259, 0.8452295510165663, 0.845372211502259, 0.8478239128388554, 0.848546039627259, 0.8486578148531627, 0.8441103280308735, 0.8479562782379518, 0.8474782920745482, 0.8454736916415663, 0.8476812523531627, 0.849888813064759, 0.8484548545745482, 0.8483121940888554, 0.8473356315888554, 0.8483224891754518, 0.845494281814759, 0.8457590126129518, 0.8465620293674698, 0.8470914909638554, 0.8468473503388554, 0.8454736916415663, 0.848057758377259, 0.8477121376129518, 0.8491769813629518, 0.8495431923004518, 0.8456060570406627, 0.8486887001129518, 0.8475900673004518, 0.8453619164156627, 0.8475797722138554, 0.847203266189759, 0.8464605492281627, 0.846226703689759, 0.8481695336031627, 0.8488004753388554, 0.847935688064759, 0.847447406814759, 0.8495431923004518, 0.8477018425263554, 0.8475900673004518, 0.8471929711031627, 0.8471826760165663, 0.8464296639683735, 0.8467458701995482, 0.847325336502259, 0.8484342644013554, 0.847203266189759, 0.8468267601656627, 0.8471929711031627, 0.8463384789156627, 0.8468473503388554, 0.8491666862763554, 0.847813617752259, 0.846104633377259, 0.8462061135165663, 0.8483121940888554, 0.8458399025790663, 0.8467046898531627, 0.846348774002259, 0.8474268166415663, 0.8463384789156627], "accuracy_test": 0.777, "start": "2016-01-21 08:02:32.495000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0], "accuracy_train_last": 0.9398606568959949, "batch_size_eval": 1024, "accuracy_train_std": [0.022707189513981936, 0.019456390826527094, 0.021514642967820772, 0.018672896283977516, 0.016922953906562687, 0.014780088231020389, 0.013806453206864001, 0.014034376683674677, 0.012704615387285994, 0.012452401961359022, 0.012840712085302646, 0.012232311677235215, 0.012440234890341642, 0.01191790952709826, 0.013378824776126834, 0.011512684104762227, 0.012670449488011352, 0.011481514693126508, 0.012568290256283992, 0.012797246551344395, 0.011512463914888426, 0.011698539535278841, 0.010573721544743396, 0.012606713130441658, 0.012236276803974204, 0.01162419215836401, 0.011524883742949093, 0.01212792174595692, 0.01109905905046476, 0.012009109162221622, 0.010466946363577357, 0.011370744950884139, 0.010767378309215337, 0.011545290269657844, 0.00961901486017381, 0.010132113704789746, 0.010524744381333056, 0.010558992112824947, 0.00893305535178328, 0.009775175018924298, 0.010295567357659213, 0.009321772813878753, 0.009736205643156744, 0.00930248988734713, 0.009045555403411095, 0.009269515265159694, 0.009113626680944313, 0.009324357872429264, 0.00916545186924114, 0.008994628833297907, 0.007992609120914847, 0.0082577689694849, 0.008916344088659664, 0.009302524124940743, 0.009123561367853432, 0.009665130476949398, 0.009511244353964514, 0.008976709080396258, 0.00976870182723366, 0.008949104626978885, 0.009211735427127169, 0.00966840515914718, 0.009194610327650354, 0.009775901304435538, 0.009090250394971253, 0.008356763666056496, 0.008909878075424768, 0.008796571990810373, 0.009417117274356743, 0.008895116013428385, 0.00964180870319207, 0.008846898908820264, 0.008838529203924637, 0.00969867391405354, 0.008687003631216255, 0.008931163729418278, 0.009148737596499496, 0.009184996944533435, 0.008643754406709019, 0.008464776447857265, 0.009004681115190014, 0.008768027712111576, 0.008712841389634074, 0.008223288505326491, 0.009224014763529631, 0.009869375570314117, 0.00866392087193223, 0.00906843601070903, 0.009134924335033837, 0.008222157019489309, 0.009328516996742782, 0.009242354974729071, 0.008333452127291863, 0.00871021072145668, 0.008728146013252206, 0.009004081333378081, 0.009127155294691313, 0.009214694450604641, 0.008743071978527328, 0.008804661731335938, 0.009114857323905231, 0.009301273533713812, 0.009324013053882776, 0.009718465204733137, 0.00858371932238931, 0.00893757231574365, 0.009156594610445911, 0.008570612406837918, 0.00844864011012035, 0.00871668026583516, 0.008456564325702742, 0.00841562800918827, 0.008643242878132644, 0.008631432761335983, 0.008112865140692474, 0.00822088828650147, 0.008456527588970737, 0.00825968094547778, 0.009183724872964533, 0.008838936669832206, 0.008578002748697982, 0.008708709113277518, 0.008574878027209675, 0.008694583411366696, 0.008184239301296256, 0.008955529040843097, 0.008441595401012482, 0.0077302422172325315, 0.009263196376876299, 0.009086947215746781, 0.008195041525494223, 0.009740465334040096, 0.00849808532581373, 0.008876560590799111, 0.008700154295333919, 0.008586827075580306, 0.009017370576764584, 0.008293632446158229, 0.008324288772723273, 0.009539487551594533, 0.009129713298766759, 0.009434263819726013, 0.00915537941950439, 0.008903481410144031, 0.008670286018758315, 0.008939857861456674, 0.00871340350724907, 0.008783585205272447, 0.008338307305803637, 0.00865680812400164, 0.009084942870480714, 0.008458692411490292, 0.009187103868817192, 0.00819961569029952, 0.00889915695263829, 0.008632981759117378, 0.008426207813020534, 0.009070996245652217, 0.008003298040011285, 0.008045974552641506, 0.00815876204263151, 0.008366740873092688, 0.008440625492438483, 0.008801564386387595, 0.008229050936572049, 0.007871450434114136, 0.00787257951146074, 0.008190037950007003, 0.008202337999422323, 0.007892171429612651, 0.0086336830244057, 0.008058055802548592, 0.00810118637889829, 0.00878706141563996, 0.00799796207963963, 0.007983941154468504, 0.008541647285110174, 0.00834836472565302, 0.008064159349756687, 0.00832102434041106, 0.008070812334451508, 0.008866438191983982, 0.008761479167022612, 0.008135746727367749, 0.008992292126573206, 0.007977895968058658, 0.0082557991486421, 0.008416741273589358, 0.007992306738671226, 0.007991601652573982, 0.008448787529736023, 0.00814239125972053, 0.00820233186808031, 0.008568086390086821, 0.009172290189972235, 0.008546409169955, 0.008422684221576313, 0.007800902025287691, 0.00873045484365539, 0.008720879726887315, 0.007672042176005349, 0.008031164181950249, 0.007863103959923467, 0.008162715338839424, 0.008050876612758068, 0.008032667187327628], "accuracy_test_std": 0.10366291525902596, "error_valid": [0.4829822218561747, 0.38673492799322284, 0.3015975032944277, 0.2626247176204819, 0.24270519578313254, 0.22410932793674698, 0.2163674228162651, 0.20490310852786142, 0.19911432840737953, 0.1954213337725903, 0.18766913356551207, 0.1843526449548193, 0.18588955431099397, 0.18159650320030118, 0.17879918109939763, 0.17999929405120485, 0.17363104762801207, 0.1721250235316265, 0.1753503270896084, 0.17286774049322284, 0.1750753012048193, 0.17130141660391573, 0.1713220067771084, 0.1707822500941265, 0.17240004941641573, 0.1722985692771084, 0.16774078736822284, 0.16896149049322284, 0.1681172933923193, 0.16550234139683728, 0.1661435782191265, 0.16800551816641573, 0.16392572242093373, 0.1652890860316265, 0.16483169004141573, 0.16605239316641573, 0.16037538827183728, 0.1623593985316265, 0.16024302287274095, 0.16224762330572284, 0.16024302287274095, 0.15900172957454817, 0.15909291462725905, 0.15521754988704817, 0.16148431617093373, 0.15680446394954817, 0.1561735222138554, 0.15497340926204817, 0.15436305769954817, 0.1550748894013554, 0.1582796027861446, 0.15593967667545183, 0.15511606974774095, 0.15509547957454817, 0.15472926863704817, 0.15640736775225905, 0.15804575724774095, 0.15582790144954817, 0.15423069230045183, 0.15628529743975905, 0.15567494587725905, 0.1540983269013554, 0.1540983269013554, 0.15615293204066272, 0.15347768025225905, 0.15570583113704817, 0.15532932511295183, 0.15558376082454817, 0.1554719855986446, 0.15410862198795183, 0.15298939900225905, 0.15372182087725905, 0.1542203972138554, 0.15630588761295183, 0.15581760636295183, 0.1541292121611446, 0.15325412980045183, 0.1551969597138554, 0.15457631306475905, 0.1577913215361446, 0.15433217243975905, 0.15546169051204817, 0.1562044074736446, 0.15274525837725905, 0.15570583113704817, 0.1563264777861446, 0.15561464608433728, 0.15582790144954817, 0.15604115681475905, 0.15621470256024095, 0.1532438347138554, 0.15610292733433728, 0.15773102174322284, 0.15610292733433728, 0.1542512824736446, 0.15609263224774095, 0.15598085702183728, 0.15572642131024095, 0.15598085702183728, 0.15856492375753017, 0.15574701148343373, 0.15462778849774095, 0.15377329631024095, 0.15399684676204817, 0.15636765813253017, 0.15587937688253017, 0.15600144719503017, 0.1569368293486446, 0.15623529273343373, 0.15451601327183728, 0.15462778849774095, 0.15501458960843373, 0.15256288827183728, 0.15340708537274095, 0.15636765813253017, 0.15451601327183728, 0.15513665992093373, 0.15562494117093373, 0.15685593938253017, 0.15563523625753017, 0.15436305769954817, 0.15378359139683728, 0.1552793204066265, 0.15609263224774095, 0.1548616340361446, 0.15660150367093373, 0.15450571818524095, 0.15587937688253017, 0.15612351750753017, 0.15439394295933728, 0.1551057746611446, 0.15476015389683728, 0.15366152108433728, 0.15512636483433728, 0.1532747199736446, 0.15562494117093373, 0.15390566170933728, 0.1526643684111446, 0.15195253670933728, 0.15462778849774095, 0.15317323983433728, 0.15340708537274095, 0.1544954230986446, 0.1541292121611446, 0.15340708537274095, 0.15477044898343373, 0.15462778849774095, 0.1521760871611446, 0.15145396037274095, 0.15134218514683728, 0.1558896719691265, 0.15204372176204817, 0.15252170792545183, 0.15452630835843373, 0.15231874764683728, 0.15011118693524095, 0.15154514542545183, 0.1516878059111446, 0.1526643684111446, 0.15167751082454817, 0.15450571818524095, 0.15424098738704817, 0.15343797063253017, 0.1529085090361446, 0.1531526496611446, 0.15452630835843373, 0.15194224162274095, 0.15228786238704817, 0.15082301863704817, 0.15045680769954817, 0.15439394295933728, 0.15131129988704817, 0.15240993269954817, 0.15463808358433728, 0.1524202277861446, 0.15279673381024095, 0.15353945077183728, 0.15377329631024095, 0.15183046639683728, 0.1511995246611446, 0.15206431193524095, 0.15255259318524095, 0.15045680769954817, 0.1522981574736446, 0.15240993269954817, 0.15280702889683728, 0.15281732398343373, 0.1535703360316265, 0.15325412980045183, 0.15267466349774095, 0.1515657355986446, 0.15279673381024095, 0.15317323983433728, 0.15280702889683728, 0.15366152108433728, 0.1531526496611446, 0.1508333137236446, 0.15218638224774095, 0.15389536662274095, 0.15379388648343373, 0.1516878059111446, 0.15416009742093373, 0.15329531014683728, 0.15365122599774095, 0.15257318335843373, 0.15366152108433728], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-3, 3], "momentum": 0.5489263049539876, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.0013354418003506889, "patience_threshold": 1, "do_flip": true, "batch_size": 16, "optimization": "rmsprop", "nb_data_augmentation": 2, "learning_rate_decay_method": "lin", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 9.548027880254739e-05, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.0040727907747453605}, "accuracy_valid_max": 0.849888813064759, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import os\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-6, -3], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128, 256],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-3, 3)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8463384789156627, "loss_train": [2.3865621089935303, 1.7660990953445435, 1.4365265369415283, 1.286525011062622, 1.1929240226745605, 1.1296745538711548, 1.0834602117538452, 1.0481600761413574, 1.0200574398040771, 0.9938486218452454, 0.9730103611946106, 0.9545895457267761, 0.9402045607566833, 0.9260249137878418, 0.9120683670043945, 0.9006956815719604, 0.8898112773895264, 0.8795821666717529, 0.8704534769058228, 0.8623307943344116, 0.857066810131073, 0.8469213247299194, 0.8409056067466736, 0.8337300419807434, 0.828063428401947, 0.8228315711021423, 0.817496657371521, 0.8118860125541687, 0.807883083820343, 0.8011460304260254, 0.7973982095718384, 0.7925276756286621, 0.788538932800293, 0.7848218679428101, 0.7828239798545837, 0.7767964601516724, 0.7741584181785583, 0.7718746066093445, 0.7658202648162842, 0.7642294764518738, 0.7602925896644592, 0.7578765749931335, 0.7568790912628174, 0.7518041133880615, 0.7487019300460815, 0.7458571195602417, 0.744953989982605, 0.7428686022758484, 0.7389788627624512, 0.7344167828559875, 0.7364488840103149, 0.733469545841217, 0.7316277027130127, 0.7293991446495056, 0.726902961730957, 0.7248833775520325, 0.7234961986541748, 0.7199844121932983, 0.7180238962173462, 0.7182193398475647, 0.7138785123825073, 0.7129169702529907, 0.7102470993995667, 0.7099897861480713, 0.71068274974823, 0.7053897976875305, 0.7023772597312927, 0.701994001865387, 0.7021421790122986, 0.7014564275741577, 0.6991653442382812, 0.6967493295669556, 0.6951353549957275, 0.6940634250640869, 0.6931310892105103, 0.6932506561279297, 0.6898268461227417, 0.6912537217140198, 0.6886458396911621, 0.6862305402755737, 0.6867519617080688, 0.6869741082191467, 0.6825841069221497, 0.6817390322685242, 0.6795470714569092, 0.6790661811828613, 0.6777190566062927, 0.6799490451812744, 0.676055371761322, 0.67691570520401, 0.6737447381019592, 0.6727409362792969, 0.6723294854164124, 0.673705518245697, 0.671669065952301, 0.6698485016822815, 0.6683759689331055, 0.6678205728530884, 0.6674304604530334, 0.6652683019638062, 0.6652855277061462, 0.6649230122566223, 0.6638763546943665, 0.6626502871513367, 0.6622370481491089, 0.6596598029136658, 0.6574594974517822, 0.6602497100830078, 0.6592570543289185, 0.6555471420288086, 0.6560983061790466, 0.6542930006980896, 0.6537839770317078, 0.6512904763221741, 0.6523051261901855, 0.6520476937294006, 0.6508524417877197, 0.6518871784210205, 0.6480259299278259, 0.6486316323280334, 0.6482566595077515, 0.6480337381362915, 0.6463302969932556, 0.6458458304405212, 0.6447262167930603, 0.6449559926986694, 0.6453173160552979, 0.6453810930252075, 0.6412328481674194, 0.6388805508613586, 0.6432323455810547, 0.6408670544624329, 0.6428945660591125, 0.6387739777565002, 0.6374695897102356, 0.6395699381828308, 0.6386036276817322, 0.6369401216506958, 0.6375747323036194, 0.6364428400993347, 0.636883020401001, 0.6346600651741028, 0.6341874003410339, 0.6331667900085449, 0.6342267394065857, 0.6324584484100342, 0.6324694156646729, 0.6313030123710632, 0.6311483383178711, 0.6302201151847839, 0.6290134787559509, 0.6298108100891113, 0.6300860643386841, 0.6291990876197815, 0.6272826194763184, 0.6275327205657959, 0.6290642023086548, 0.6252350807189941, 0.6258219480514526, 0.626119077205658, 0.6253162026405334, 0.6246110200881958, 0.6228272914886475, 0.6230201125144958, 0.6232625842094421, 0.6220278739929199, 0.6246343851089478, 0.6204596757888794, 0.6219201683998108, 0.6220641732215881, 0.6208552122116089, 0.6202381253242493, 0.6195154786109924, 0.6193781495094299, 0.618216335773468, 0.6162362098693848, 0.6177322864532471, 0.6154037117958069, 0.6170811653137207, 0.6166642308235168, 0.617889404296875, 0.6163566708564758, 0.6147317886352539, 0.6130613684654236, 0.6129119396209717, 0.6145317554473877, 0.613932728767395, 0.6129040718078613, 0.6134134531021118, 0.6104075908660889, 0.6129975318908691, 0.6102674603462219, 0.6100332140922546, 0.6121476292610168, 0.6106801629066467, 0.6104877591133118, 0.608544647693634, 0.6108413934707642, 0.609600305557251, 0.6092389225959778, 0.609220027923584, 0.6075209975242615, 0.6080527901649475, 0.6072749495506287, 0.6052523851394653, 0.6053557395935059], "accuracy_train_first": 0.5307379265296235, "model": "residualv4", "loss_std": [0.43538954854011536, 0.2747341990470886, 0.25796303153038025, 0.2537881135940552, 0.24825961887836456, 0.2459583282470703, 0.24095679819583893, 0.23850785195827484, 0.23817801475524902, 0.233381986618042, 0.2358364462852478, 0.22926636040210724, 0.2280118614435196, 0.22743497788906097, 0.22482439875602722, 0.22498254477977753, 0.22168517112731934, 0.2222595512866974, 0.2183987945318222, 0.221523255109787, 0.2195047289133072, 0.2165888398885727, 0.21491064131259918, 0.21447809040546417, 0.21347378194332123, 0.21291105449199677, 0.2106165587902069, 0.2124008983373642, 0.21195858716964722, 0.20858259499073029, 0.20822305977344513, 0.2074154019355774, 0.20634859800338745, 0.20694299042224884, 0.20692235231399536, 0.2047002911567688, 0.20383280515670776, 0.20552439987659454, 0.20424887537956238, 0.2031542807817459, 0.2020881474018097, 0.2021700143814087, 0.2025953084230423, 0.19993431866168976, 0.20047707855701447, 0.1959535926580429, 0.1967781037092209, 0.19720062613487244, 0.19743259251117706, 0.19524450600147247, 0.19896996021270752, 0.19375324249267578, 0.19468680024147034, 0.19459691643714905, 0.19380080699920654, 0.19532623887062073, 0.19749711453914642, 0.19360369443893433, 0.1902291476726532, 0.1945212483406067, 0.1915794312953949, 0.19178509712219238, 0.19057685136795044, 0.19011394679546356, 0.1891707181930542, 0.18696026504039764, 0.18775631487369537, 0.1905175745487213, 0.19095255434513092, 0.18867304921150208, 0.18885311484336853, 0.188630148768425, 0.18699458241462708, 0.18750058114528656, 0.1871456354856491, 0.18822462856769562, 0.18557587265968323, 0.18553446233272552, 0.18658261001110077, 0.1848144382238388, 0.18393805623054504, 0.18607498705387115, 0.18422263860702515, 0.18319664895534515, 0.18349303305149078, 0.18379975855350494, 0.18231947720050812, 0.18527290225028992, 0.18152302503585815, 0.1803087741136551, 0.18295368552207947, 0.18162935972213745, 0.18183687329292297, 0.1817965805530548, 0.18129239976406097, 0.17861443758010864, 0.17932575941085815, 0.17939317226409912, 0.17923037707805634, 0.17612400650978088, 0.17941124737262726, 0.1794745922088623, 0.17810769379138947, 0.18100710213184357, 0.17842121422290802, 0.17762067914009094, 0.17605705559253693, 0.17649713158607483, 0.1799210011959076, 0.17584723234176636, 0.17627955973148346, 0.17539988458156586, 0.17646504938602448, 0.1766429841518402, 0.17462894320487976, 0.17679363489151, 0.17462556064128876, 0.17779456079006195, 0.1736992448568344, 0.17612102627754211, 0.17528627812862396, 0.17382119596004486, 0.17399907112121582, 0.17468303442001343, 0.17154668271541595, 0.1726219803094864, 0.17313215136528015, 0.17354215681552887, 0.17227016389369965, 0.1715121865272522, 0.17369580268859863, 0.1731414943933487, 0.17331641912460327, 0.1726142168045044, 0.16908392310142517, 0.17179548740386963, 0.17252713441848755, 0.1715966910123825, 0.1711859554052353, 0.16961398720741272, 0.17275205254554749, 0.1716519296169281, 0.1676270216703415, 0.1689922958612442, 0.1711730808019638, 0.16915176808834076, 0.16774298250675201, 0.16838473081588745, 0.1718638390302658, 0.16960754990577698, 0.16653457283973694, 0.16802509129047394, 0.17055480182170868, 0.16832563281059265, 0.16495518386363983, 0.16861434280872345, 0.1702331006526947, 0.16710494458675385, 0.16783279180526733, 0.16929134726524353, 0.16921238601207733, 0.16732099652290344, 0.16716481745243073, 0.16708321869373322, 0.165354922413826, 0.1674678921699524, 0.16897302865982056, 0.1662132441997528, 0.16865064203739166, 0.1656854897737503, 0.16736507415771484, 0.1637706607580185, 0.16658300161361694, 0.16688485443592072, 0.16488032042980194, 0.16551823914051056, 0.16698892414569855, 0.1643742173910141, 0.16655008494853973, 0.16376623511314392, 0.1658850610256195, 0.1651528775691986, 0.162759467959404, 0.16315911710262299, 0.16440525650978088, 0.16522251069545746, 0.1640392541885376, 0.1642179638147354, 0.16457197070121765, 0.16215968132019043, 0.16572485864162445, 0.16162364184856415, 0.1623469442129135, 0.16799576580524445, 0.16201747953891754, 0.164511039853096, 0.16551686823368073, 0.16445960104465485, 0.16197852790355682, 0.1626266986131668, 0.1625034064054489, 0.1639910191297531, 0.15944431722164154, 0.16096557676792145, 0.16295091807842255, 0.16037020087242126]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:10 2016", "state": "available"}], "summary": "5125c9367cba0befca736294f7949c83"}