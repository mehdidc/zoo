{"content": {"hp_model": {"f0": 32, "f1": 32, "f2": 32, "f3": 32, "nonlin": "rectify", "nbg1": 6, "nbg3": 1, "nbg2": 4, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.5789352655410767, 1.189444661140442, 1.010299563407898, 0.906859278678894, 0.8333683609962463, 0.7772476077079773, 0.7339410185813904, 0.6992542147636414, 0.667594313621521, 0.6432875990867615, 0.6207849383354187, 0.599972128868103, 0.5835066437721252, 0.5676497220993042, 0.553320586681366, 0.540050745010376, 0.5280969738960266, 0.5152828097343445, 0.5037554502487183, 0.4954194128513336, 0.4856260120868683, 0.4745061993598938, 0.4661621153354645, 0.4612898826599121, 0.44880545139312744, 0.44362300634384155, 0.43482092022895813, 0.42848747968673706, 0.423766553401947, 0.4176175594329834, 0.41115477681159973, 0.40579545497894287, 0.39997684955596924, 0.3936503827571869, 0.38989484310150146, 0.38426700234413147, 0.3787390887737274, 0.3741869628429413, 0.3712065815925598, 0.3670925498008728, 0.36294353008270264, 0.3565745949745178, 0.3549077808856964, 0.3520890474319458, 0.3478304147720337, 0.343537837266922, 0.3385852873325348, 0.3349209129810333, 0.3326666057109833, 0.32973355054855347, 0.325611412525177, 0.32278141379356384, 0.3205835521221161, 0.31581827998161316, 0.31264668703079224, 0.3102613389492035, 0.30901941657066345, 0.3069525361061096, 0.30414658784866333, 0.3004961907863617, 0.2980653941631317, 0.2956252992153168, 0.2906319797039032, 0.2891139090061188, 0.2876890301704407, 0.2856731414794922, 0.28289371728897095, 0.27829861640930176, 0.27737435698509216, 0.2746732831001282, 0.27412015199661255, 0.27106648683547974, 0.27027976512908936, 0.2678656280040741, 0.2648562490940094, 0.26257240772247314, 0.26108264923095703, 0.261516273021698, 0.2591473162174225, 0.2557069957256317, 0.25365617871284485, 0.2528814673423767, 0.25057685375213623, 0.24887683987617493, 0.24762319028377533, 0.24709711968898773, 0.245599627494812, 0.2434738129377365, 0.23977501690387726, 0.2395162582397461, 0.23655550181865692, 0.23830389976501465, 0.23385435342788696, 0.23453888297080994, 0.23161590099334717, 0.2306510955095291, 0.2293461412191391, 0.22947826981544495, 0.22684766352176666, 0.22428706288337708, 0.22214846312999725, 0.22231562435626984, 0.22011572122573853, 0.22122067213058472, 0.21899542212486267, 0.21718399226665497, 0.2140192985534668, 0.2141718864440918, 0.21390970051288605, 0.21250201761722565, 0.21146883070468903, 0.2103581577539444, 0.20699113607406616, 0.2068173587322235, 0.20743632316589355, 0.20324940979480743, 0.20494817197322845, 0.20358142256736755, 0.20175562798976898, 0.19913692772388458, 0.19971130788326263, 0.1991775780916214, 0.19602656364440918, 0.1970943659543991, 0.19483767449855804, 0.19353283941745758, 0.1922370195388794, 0.19096194207668304, 0.1900373101234436, 0.19010496139526367, 0.18850506842136383, 0.18700888752937317, 0.18801862001419067, 0.18599846959114075, 0.18501776456832886, 0.18443381786346436, 0.18390581011772156, 0.18193916976451874, 0.18266631662845612, 0.18255554139614105, 0.18068349361419678, 0.17813222110271454, 0.17806027829647064, 0.1794179528951645, 0.17724081873893738, 0.17586840689182281, 0.17500829696655273, 0.1760333776473999, 0.17459045350551605, 0.1711551547050476, 0.17049367725849152, 0.1708301156759262, 0.1710253804922104, 0.17013217508792877, 0.17035213112831116, 0.16885654628276825, 0.16739748418331146, 0.1673576384782791, 0.1652539223432541, 0.16491898894309998, 0.16525647044181824, 0.16429990530014038, 0.16070222854614258, 0.16225889325141907, 0.16214890778064728, 0.1603863388299942, 0.16103529930114746, 0.16084089875221252, 0.15977638959884644, 0.15733253955841064, 0.15707175433635712, 0.15522557497024536, 0.15583796799182892, 0.1556764543056488, 0.15413519740104675, 0.1526368260383606, 0.1528831124305725, 0.1531486064195633, 0.15100179612636566, 0.1502615511417389, 0.15098857879638672, 0.1502998024225235, 0.15075506269931793, 0.14965464174747467, 0.1495685875415802, 0.14810790121555328, 0.1474812924861908, 0.1488124132156372, 0.14345048367977142, 0.1449761539697647, 0.14678050577640533, 0.1451268047094345, 0.14466431736946106, 0.14349165558815002, 0.14341780543327332, 0.14215752482414246, 0.1416630744934082, 0.14193184673786163], "moving_avg_accuracy_train": [0.0543905882352941, 0.11103858823529408, 0.1695535529411764, 0.2258123152941176, 0.2796193190588235, 0.33022915185882346, 0.3771662366729411, 0.42078608359388225, 0.4608745340580235, 0.49797531594633876, 0.5321824902340578, 0.5635430647400638, 0.5923793465013515, 0.6191131765570986, 0.6432206824308004, 0.665726849481838, 0.686128282180713, 0.7049248657273476, 0.7222747320957893, 0.7384284353567986, 0.7530750035858246, 0.7664098561684185, 0.7788512234927532, 0.7904978658493602, 0.8008080792644242, 0.810553153690923, 0.8196107794983012, 0.8275320544896475, 0.8351953196289181, 0.8424593170777911, 0.8486322088994238, 0.8546654585977167, 0.8605518539144156, 0.8652190214641505, 0.8698830016706766, 0.8742617603271383, 0.8783202901767774, 0.8820106141002761, 0.8858189644549544, 0.8895029503624002, 0.8929408906202779, 0.8959126839111914, 0.8985708272847781, 0.9009796269092415, 0.9034793112771409, 0.9058819683847209, 0.9082020068403664, 0.9100665120386827, 0.9116316255406968, 0.913428462986627, 0.9152103225703172, 0.9168092903132855, 0.9185001259878394, 0.9201136428008202, 0.9214081608736793, 0.9226273447863114, 0.9240163750135626, 0.9252170904533829, 0.9263494990551033, 0.9275874903260636, 0.9288475648228689, 0.9301110436346997, 0.9312622922124062, 0.9322301806382244, 0.9331553978685196, 0.9338398580816677, 0.9347429310970303, 0.9356404026932097, 0.9364434212474181, 0.9372390791226762, 0.9380304653280557, 0.9389215364423089, 0.9395070298569016, 0.9402669151065055, 0.9403814000664431, 0.9409150247656811, 0.9415411693479365, 0.9422105818249076, 0.9428036412894757, 0.9434244536311163, 0.943853772973887, 0.9443248662647336, 0.9449347325794367, 0.9455353769685518, 0.9461488980952261, 0.9466563612268799, 0.9470777839277214, 0.9474217702408316, 0.9479290049814544, 0.9483619868362502, 0.9488481410938016, 0.9494597975726567, 0.9499349942859793, 0.9504709066220872, 0.9506002865481138, 0.9509637873050671, 0.9514838791627958, 0.9517590206582809, 0.9519831185924528, 0.9522883361449722, 0.9526571495892986, 0.9530243758068393, 0.9534419382261554, 0.9538765679329516, 0.9541265581984799, 0.9543044906139261, 0.9544881591995923, 0.9547852256325742, 0.954892585422258, 0.955052738644738, 0.9552768765449701, 0.9556315418316494, 0.9559577994131904, 0.9563314312365772, 0.95661122928939, 0.9568277534192745, 0.9570061545479353, 0.9571196567402006, 0.95729474988971, 0.9575040984301507, 0.9576760415283121, 0.9580849079637163, 0.9583022995202858, 0.9585897166270807, 0.9588719214349608, 0.9592270822326412, 0.9592949622446711, 0.9596431130790275, 0.9599517429475953, 0.960173039241071, 0.9602898529640227, 0.9603996911970322, 0.960435016194976, 0.9604032792813607, 0.9605841278238129, 0.9606151268061375, 0.9608006729490531, 0.9610429585953243, 0.9611316039122625, 0.961312561168095, 0.9616424815218738, 0.9618005863108628, 0.9618840570915412, 0.9621427102059166, 0.9621496156559132, 0.9624028893844395, 0.9625649533871721, 0.9627366933425725, 0.962921847537727, 0.9631402510192485, 0.9632309317996766, 0.963249015090297, 0.9632864665224438, 0.9632354669290228, 0.9633236849420029, 0.9633442576242732, 0.963435714214787, 0.9637227310286025, 0.9638798696904481, 0.9639012944861092, 0.9639888120963218, 0.9640134602984544, 0.9640615260333149, 0.9641730204888068, 0.9641557184399261, 0.9642460289488746, 0.9643249554657518, 0.9643301069780001, 0.9644688609860825, 0.964410210181592, 0.9644397773987269, 0.9646310937765012, 0.9647138667517923, 0.9648801271354366, 0.964982702657187, 0.9652891382738212, 0.9653978715052626, 0.9653098490606187, 0.9653223935663215, 0.9651878012685129, 0.9652290211416616, 0.9653814131451426, 0.9656079777129813, 0.9656001211181537, 0.9657295207710443, 0.9658624510468812, 0.9658173824127814, 0.9657956441715032, 0.9658043150484705, 0.9658756482495058, 0.9661257304833787, 0.9661908044938644, 0.9663599593385956, 0.9665004339929714, 0.9666927435348507, 0.9667811162401891, 0.9669618281455821, 0.9670727041545533], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.05358666666666666, 0.10888133333333332, 0.16484653333333332, 0.21849521333333333, 0.2696590253333333, 0.31785312279999994, 0.3624011438533333, 0.4034276961346666, 0.4405915931878666, 0.47501243386907993, 0.5066845238155053, 0.5353227381006215, 0.5614837976238927, 0.58560208452817, 0.6073885427420197, 0.6279163551344843, 0.6460047196210358, 0.6627909143255988, 0.6778318228930389, 0.691728640603735, 0.7042491098766948, 0.7156508655556919, 0.7260724456667894, 0.7355052011001105, 0.7440480143234328, 0.7519632128910895, 0.7595402249353138, 0.7658795357751158, 0.7717849155309375, 0.7772197573111771, 0.7815911149133927, 0.7860320034220535, 0.7901488030798481, 0.7933472561051967, 0.7965991971613438, 0.799645944111876, 0.8023480163673551, 0.8047532147306196, 0.8074378932575577, 0.8099607705984685, 0.8120446935386216, 0.8138535575180927, 0.8154148684329501, 0.816753381589655, 0.818171376764023, 0.8193009057542873, 0.8205841485121919, 0.8215124003276394, 0.8225344936282087, 0.8236277109320546, 0.8247049398388491, 0.8254077791882976, 0.8262803346028011, 0.8269589678091877, 0.827423071028269, 0.8281074305921087, 0.8285500208662311, 0.8290150187796079, 0.8294468502349805, 0.8299288318781491, 0.8302559486903341, 0.8307503538213008, 0.8308619851058374, 0.830989119928587, 0.8311435412690616, 0.8312825204754888, 0.8316076017612732, 0.8317535082518126, 0.831924824093298, 0.8319990083506349, 0.832185774182238, 0.8324071967640142, 0.8324464770876128, 0.8327218293788514, 0.8326496464409663, 0.8327446817968697, 0.8327768802838493, 0.8328591922554645, 0.832653273029918, 0.8327479457269262, 0.8327264844875669, 0.8326138360388102, 0.8327657857682625, 0.8328758738581029, 0.8330282864722925, 0.8329654578250633, 0.8330822453758903, 0.8331606875049679, 0.8332979520878043, 0.8333548235456906, 0.8335793411911215, 0.8338080737386759, 0.8336539330314751, 0.8337285397283275, 0.8340223524221614, 0.8340067838466119, 0.834179438795284, 0.8342414949157556, 0.8341506787575135, 0.8341356108817621, 0.8343753831269193, 0.8344178448142273, 0.834389393666138, 0.8345371209661909, 0.8347100755362384, 0.834732401315948, 0.8346458278510198, 0.8346879117325845, 0.834699120559326, 0.8346958751700602, 0.8347596209863876, 0.8349236588877488, 0.8350312929989739, 0.8347948303657432, 0.8348220139958356, 0.8348864792629187, 0.8350111646699602, 0.8348567148696309, 0.8351977100493344, 0.8354512723777343, 0.8354128118066275, 0.8353781972926314, 0.8353737108967016, 0.8353030064736982, 0.8353460391596617, 0.8356514352436956, 0.8354596250526594, 0.8355936625473934, 0.8355409629593207, 0.8355601999967219, 0.8354841799970497, 0.8353757619973448, 0.8350781857976103, 0.8349037005511826, 0.834959997162731, 0.8349039974464578, 0.835053597701812, 0.8351882379316308, 0.8351894141384677, 0.8353771393912875, 0.8352127587854921, 0.8355048162402762, 0.8354743346162485, 0.8355535678212903, 0.8355048777058279, 0.8355277232685784, 0.8356416176083872, 0.8359441225142151, 0.835763043596127, 0.8357334059031809, 0.8354933986461961, 0.8353440587815766, 0.8352896529034189, 0.835400687613077, 0.8355406188517692, 0.8353865569665924, 0.8354212346032664, 0.8354924444762731, 0.8354365333619791, 0.8354395466924479, 0.8352289253565364, 0.8353060328208828, 0.8352020962054612, 0.8353485532515817, 0.8355470312597569, 0.8355523281337812, 0.8355837619870699, 0.8354787191216961, 0.8355708472095265, 0.8355604291552405, 0.8354043862397165, 0.8357306142824115, 0.8356508861875036, 0.8357524642354199, 0.8358038844785446, 0.8357834960306901, 0.8355518130942878, 0.8352232984515258, 0.8349809686063733, 0.8349628717457359, 0.8349865845711624, 0.8350745927807128, 0.8352871335026416, 0.8351317534857108, 0.834991911470473, 0.8347327203234257, 0.8345127816244164, 0.8345548367953082, 0.8344726864491107, 0.8345187511375329, 0.8345868760237796, 0.8345548550880683, 0.8345660362459282, 0.8346694326213353, 0.8345491560258684, 0.8346942404232817, 0.8347714830476202, 0.8348943347428581], "moving_var_accuracy_train": [0.026625024797231826, 0.05284348545350863, 0.07837514675893369, 0.09902306715640272, 0.1151775033279781, 0.1267119495795759, 0.13386856399924738, 0.13760592700797972, 0.13830908905272488, 0.13686639229797148, 0.13371093002292783, 0.12919120772075582, 0.12375586726102762, 0.11781255955997101, 0.11126185015902905, 0.10469441314108904, 0.09797093793248068, 0.09135364811646314, 0.08492744407184188, 0.07878317882606038, 0.07283555859144197, 0.0671523673728934, 0.06183021922369525, 0.056867995804970144, 0.05213790073045062, 0.04777880893760756, 0.04373929331124517, 0.03993008335751742, 0.03646560571511851, 0.03329393607404175, 0.030307483807611786, 0.02760433634414817, 0.02515574955815345, 0.022836216678773794, 0.02074836941319822, 0.01884609421822225, 0.01710972977726373, 0.01552132321548048, 0.014099722685748235, 0.01281189618666974, 0.011637081466953388, 0.010552857318533308, 0.009561163122430865, 0.008657267650865113, 0.007847776683230686, 0.007114953865497061, 0.006451901684868424, 0.005837998933092518, 0.005276245262250947, 0.0047776783592897295, 0.0043284857355446555, 0.003918647442577668, 0.0035525130258249956, 0.0032206926517944403, 0.0029137053799836274, 0.002635712526700652, 0.002389505918780545, 0.002163530785009294, 0.0019587188496796204, 0.0017766405661944226, 0.0016132665992124751, 0.001466307347662735, 0.0013316049724855028, 0.0012068757472804483, 0.0010938924148615197, 0.0009887195454258116, 0.0008971874587229161, 0.0008147178102441624, 0.0007390495784053735, 0.0006708422636549796, 0.0006093946664240654, 0.0005556012693575678, 0.0005031263652685932, 0.0004580105590748248, 0.00041232746442180975, 0.0003736575158563609, 0.0003398202776117151, 0.0003098712674294643, 0.0002820496164431417, 0.0002573133264706279, 0.00023324082970625892, 0.00021191410673375918, 0.00019407012835666922, 0.00017791007866058202, 0.0001635067443504044, 0.00014947373938525548, 0.00013612473928179024, 0.0001235772046060757, 0.00011353506788431948, 0.00010386882067512869, 9.560905226683448e-05, 8.94152598732802e-05, 8.250604113312524e-05, 7.684025530774691e-05, 6.930688226429999e-05, 6.356538924062078e-05, 5.964331018083875e-05, 5.4360304745595025e-05, 4.9376253227936575e-05, 4.527704769443649e-05, 4.197355313543534e-05, 3.898989367553525e-05, 3.6660129674207554e-05, 3.469424354505472e-05, 3.178727538627975e-05, 2.88934873478502e-05, 2.6307745957310845e-05, 2.4471207552021366e-05, 2.2127821916787695e-05, 2.014588121714573e-05, 1.8583433280315185e-05, 1.785717714246256e-05, 1.702945551383276e-05, 1.6582916617475906e-05, 1.56292075089492e-05, 1.4488231047454575e-05, 1.3325850607076112e-05, 1.2109210275209761e-05, 1.1174207746734725e-05, 1.0451228274523376e-05, 9.672185308119103e-06, 1.0209512635307562e-05, 9.613893171586383e-06, 9.39598119393318e-06, 9.173139056855979e-06, 9.391077881051103e-06, 8.493439357244728e-06, 8.73497645268792e-06, 8.718750369368912e-06, 8.287623777987024e-06, 7.5816704130169075e-06, 6.932083308591055e-06, 6.250105677049543e-06, 5.634160194516982e-06, 5.365099932828926e-06, 4.837238371692477e-06, 4.6633608748811365e-06, 4.725345796894311e-06, 4.323533147140483e-06, 4.185889588372525e-06, 4.746927588072628e-06, 4.4972089479769325e-06, 4.110194394222824e-06, 4.301287856984834e-06, 3.8715882384432475e-06, 4.0617576486535625e-06, 3.8919645526233785e-06, 3.7682196078895505e-06, 3.699936330950434e-06, 3.759243424521442e-06, 3.4573261175207225e-06, 3.1145365543656652e-06, 2.815706386857692e-06, 2.5575443749337394e-06, 2.3718316977677265e-06, 2.1384576452931034e-06, 1.99989065229956e-06, 2.541309449784332e-06, 2.5094115362256305e-06, 2.2626015794251327e-06, 2.1052754103585663e-06, 1.9002156741380023e-06, 1.7309869405332734e-06, 1.669767368929163e-06, 1.5054848800954727e-06, 1.428340284324874e-06, 1.341570811489766e-06, 1.2076525730467934e-06, 1.2601613885723405e-06, 1.1651045015216632e-06, 1.0564620343314288e-06, 1.2802334385406556e-06, 1.2138723836333977e-06, 1.3412677817960076e-06, 1.3018366425778208e-06, 2.0167780625980867e-06, 1.921506496915496e-06, 1.7990874040737583e-06, 1.620594945276339e-06, 1.6215712304133443e-06, 1.4747058088535716e-06, 1.5362451324924965e-06, 1.8446041498425917e-06, 1.6606992695988907e-06, 1.6453277741526374e-06, 1.6398291208436481e-06, 1.4941268447759527e-06, 1.3489671205031548e-06, 1.214747065419283e-06, 1.1390681890068344e-06, 1.5880314833961269e-06, 1.467339976622708e-06, 1.5781262324247615e-06, 1.5979117658801214e-06, 1.770967228372658e-06, 1.6641581209749328e-06, 1.7916534436338535e-06, 1.7231295035589154e-06], "duration": 222152.888352, "accuracy_train": [0.5439058823529411, 0.6208705882352941, 0.6961882352941177, 0.7321411764705882, 0.7638823529411765, 0.7857176470588235, 0.7996, 0.813364705882353, 0.8216705882352942, 0.8318823529411765, 0.8400470588235294, 0.8457882352941176, 0.8519058823529412, 0.8597176470588235, 0.8601882352941177, 0.8682823529411765, 0.8697411764705882, 0.8740941176470588, 0.8784235294117647, 0.8838117647058824, 0.8848941176470588, 0.8864235294117647, 0.8908235294117647, 0.8953176470588236, 0.8936, 0.8982588235294118, 0.9011294117647058, 0.8988235294117647, 0.904164705882353, 0.9078352941176471, 0.9041882352941176, 0.9089647058823529, 0.9135294117647059, 0.9072235294117647, 0.9118588235294117, 0.9136705882352941, 0.9148470588235295, 0.9152235294117647, 0.9200941176470588, 0.9226588235294118, 0.9238823529411765, 0.9226588235294118, 0.9224941176470588, 0.9226588235294118, 0.9259764705882353, 0.9275058823529412, 0.9290823529411765, 0.9268470588235294, 0.9257176470588235, 0.9296, 0.9312470588235294, 0.9312, 0.9337176470588235, 0.934635294117647, 0.9330588235294117, 0.9336, 0.9365176470588236, 0.9360235294117647, 0.9365411764705882, 0.9387294117647059, 0.9401882352941177, 0.9414823529411764, 0.9416235294117647, 0.9409411764705883, 0.9414823529411764, 0.94, 0.9428705882352941, 0.9437176470588235, 0.9436705882352941, 0.9444, 0.9451529411764706, 0.9469411764705883, 0.9447764705882353, 0.9471058823529411, 0.9414117647058824, 0.9457176470588236, 0.9471764705882353, 0.9482352941176471, 0.9481411764705883, 0.9490117647058823, 0.9477176470588236, 0.948564705882353, 0.9504235294117647, 0.9509411764705883, 0.9516705882352942, 0.9512235294117647, 0.9508705882352941, 0.9505176470588236, 0.9524941176470588, 0.9522588235294117, 0.9532235294117647, 0.9549647058823529, 0.9542117647058823, 0.9552941176470588, 0.951764705882353, 0.9542352941176471, 0.9561647058823529, 0.9542352941176471, 0.954, 0.9550352941176471, 0.9559764705882353, 0.9563294117647059, 0.9572, 0.9577882352941176, 0.9563764705882353, 0.9559058823529412, 0.9561411764705883, 0.9574588235294118, 0.9558588235294118, 0.9564941176470588, 0.9572941176470589, 0.9588235294117647, 0.9588941176470588, 0.9596941176470588, 0.9591294117647059, 0.9587764705882353, 0.9586117647058824, 0.9581411764705883, 0.9588705882352941, 0.9593882352941177, 0.9592235294117647, 0.961764705882353, 0.9602588235294117, 0.9611764705882353, 0.9614117647058823, 0.9624235294117647, 0.9599058823529412, 0.9627764705882353, 0.9627294117647058, 0.9621647058823529, 0.9613411764705883, 0.9613882352941177, 0.9607529411764706, 0.9601176470588235, 0.9622117647058823, 0.9608941176470588, 0.9624705882352941, 0.9632235294117647, 0.9619294117647059, 0.9629411764705882, 0.9646117647058824, 0.9632235294117647, 0.962635294117647, 0.9644705882352941, 0.9622117647058823, 0.9646823529411764, 0.9640235294117647, 0.9642823529411765, 0.9645882352941176, 0.9651058823529411, 0.9640470588235294, 0.9634117647058823, 0.9636235294117647, 0.9627764705882353, 0.9641176470588235, 0.9635294117647059, 0.9642588235294117, 0.9663058823529411, 0.9652941176470589, 0.9640941176470588, 0.9647764705882353, 0.9642352941176471, 0.9644941176470588, 0.9651764705882353, 0.964, 0.9650588235294117, 0.9650352941176471, 0.9643764705882353, 0.9657176470588236, 0.9638823529411765, 0.9647058823529412, 0.9663529411764706, 0.9654588235294118, 0.9663764705882353, 0.9659058823529412, 0.9680470588235294, 0.9663764705882353, 0.9645176470588235, 0.9654352941176471, 0.9639764705882353, 0.9656, 0.9667529411764706, 0.9676470588235294, 0.9655294117647059, 0.9668941176470588, 0.9670588235294117, 0.9654117647058823, 0.9656, 0.9658823529411765, 0.9665176470588235, 0.9683764705882353, 0.9667764705882353, 0.9678823529411764, 0.967764705882353, 0.9684235294117647, 0.9675764705882353, 0.9685882352941176, 0.9680705882352941], "end": "2016-02-09 19:30:36.185000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0], "moving_var_accuracy_valid": [0.0258437776, 0.050776901296, 0.07388814366575999, 0.09240295709086559, 0.10672228230712111, 0.1169540933520481, 0.12311941963475735, 0.1259560796001184, 0.1257908688377342, 0.123874930412774, 0.12051552890566672, 0.11584530187206211, 0.11042038100327715, 0.10461356877172279, 0.09842405974808488, 0.09237417350785844, 0.08608145652525766, 0.08000929786666741, 0.07404442845480752, 0.06837807949168562, 0.06295113089985324, 0.05782601810293982, 0.05302090028075404, 0.048519602128262264, 0.044324458835353166, 0.04045586626710677, 0.036926979644060984, 0.03359596343696756, 0.030550228683815028, 0.027761043362019663, 0.025156917931395732, 0.022818719554973353, 0.020689379954277783, 0.018712512874648254, 0.01693643767287732, 0.015326337908414787, 0.013859414867837776, 0.01252553819355385, 0.011337851863335486, 0.010261350867697474, 0.009274300394312195, 0.008376318254947031, 0.0075606256554080035, 0.006820687647103254, 0.0061567152752237024, 0.005552526269359961, 0.005012094050205396, 0.004518639508080791, 0.004076177629708331, 0.0036793159833983487, 0.0033218281841172178, 0.002994091214065694, 0.0027015342692215403, 0.002435525729558681, 0.002193911682784467, 0.0019787356466195913, 0.0017826250573143626, 0.0016063085591179302, 0.0014473560088587797, 0.0013047111647120654, 0.0011752030969201862, 0.0010598827151299024, 0.0009540065975100977, 0.0008587514071274881, 0.0007730908799682852, 0.0006959556289498289, 0.0006273111666361515, 0.0005647716483083699, 0.0005085586255354276, 0.00045775229271821443, 0.00041229099672908284, 0.00037150314869365854, 0.00033436672031869077, 0.00030161241824543545, 0.0002714980698095875, 0.00024442954829847384, 0.0002199959241517005, 0.0001980573090825709, 0.00017863320272136062, 0.00016085054872525383, 0.00014476963911588199, 0.00013040688226135939, 0.00011757399251774911, 0.00010592566775369659, 9.554216742300421e-05, 8.602347763091771e-05, 7.754388385607947e-05, 6.984487397899962e-05, 6.30299606724113e-05, 5.675607386966912e-05, 5.15341400406909e-05, 4.685159324141883e-05, 4.238026813582456e-05, 3.8192336755179274e-05, 3.5150036171182844e-05, 3.1637213978966316e-05, 2.874178016277852e-05, 2.5902260805292537e-05, 2.338626289614418e-05, 2.1049679974446688e-05, 1.9462128542931197e-05, 1.7532142642639516e-05, 1.5786213588824013e-05, 1.440400242656979e-05, 1.3232821733615907e-05, 1.1914025524211045e-05, 1.0790077655256901e-05, 9.727009367519163e-06, 8.755439170939546e-06, 7.879990046808979e-06, 7.128562804021205e-06, 6.657882421366109e-06, 6.096360096322484e-06, 5.9899552789199185e-06, 5.397610298732902e-06, 4.895251204800483e-06, 4.545644140882281e-06, 4.305772394190039e-06, 4.921694568000771e-06, 5.0081698006528645e-06, 4.520665760356306e-06, 4.079382665533344e-06, 3.67162554871596e-06, 3.3494550327346823e-06, 3.031175838012327e-06, 3.5674591674998993e-06, 3.541833595218087e-06, 3.3493446856475334e-06, 3.039405436330048e-06, 2.7387954651688428e-06, 2.5169272818034008e-06, 2.3710247175633006e-06, 2.930886597642668e-06, 2.9118038488667872e-06, 2.649147240226606e-06, 2.4124562302079445e-06, 2.3726327348055926e-06, 2.2985213846959813e-06, 2.0686816973890924e-06, 2.1789804625668246e-06, 2.204271268365161e-06, 2.7515221535834664e-06, 2.4847321028553655e-06, 2.292759999600621e-06, 2.0848205457342075e-06, 1.8810357687972886e-06, 1.8096794776819642e-06, 2.452294492363557e-06, 2.5021712143110923e-06, 2.2598596284684493e-06, 2.5523050162696996e-06, 2.497796071124184e-06, 2.2746564602147216e-06, 2.1581491749330055e-06, 2.1185610214973713e-06, 2.1203204995260593e-06, 1.9191112959411595e-06, 1.7728377804696955e-06, 1.6236884767370761e-06, 1.461401350507994e-06, 1.7145133397273055e-06, 1.596572055275897e-06, 1.5341402299759964e-06, 1.5737732042036069e-06, 1.7709375613458605e-06, 1.594096317081139e-06, 1.443579469566113e-06, 1.3985275547026558e-06, 1.3350630603379528e-06, 1.202533577000108e-06, 1.3014247426673282e-06, 2.1291048909662188e-06, 1.973403523928261e-06, 1.86892606990182e-06, 1.7058298355386481e-06, 1.5389880512380088e-06, 1.8681820932943324e-06, 2.652660718546977e-06, 2.9159084313573523e-06, 2.627265055505959e-06, 2.369599232762694e-06, 2.2023483140208694e-06, 2.3886755089206128e-06, 2.367094504981372e-06, 2.306387157515018e-06, 2.680368898132767e-06, 2.8476892902163633e-06, 2.5788380977832855e-06, 2.3816924024282537e-06, 2.162620761860391e-06, 1.9881276868095046e-06, 1.79854298104299e-06, 1.6198138475584703e-06, 1.5540497568287173e-06, 1.5288429158997358e-06, 1.565403965664391e-06, 1.4625613762302435e-06, 1.4521380898128313e-06], "accuracy_test": 0.7543, "start": "2016-02-07 05:48:03.297000", "learning_rate_per_epoch": [0.0002835823397617787, 0.00020052299078088254, 0.00016372633399441838, 0.00014179116988088936, 0.00012682187661994249, 0.00011577200348256156, 0.00010718405246734619, 0.00010026149539044127, 9.452744416194037e-05, 8.967660687630996e-05, 8.550329221179709e-05, 8.186316699720919e-05, 7.865158841013908e-05, 7.579056546092033e-05, 7.322064629988745e-05, 7.089558494044468e-05, 6.877881969558075e-05, 6.684099935228005e-05, 6.505825149361044e-05, 6.341093830997124e-05, 6.188273982843384e-05, 6.0459959058789536e-05, 5.9131005400558934e-05, 5.788600174128078e-05, 5.671646795235574e-05, 5.5615073506487533e-05, 5.4575448302784935e-05, 5.3592026233673096e-05, 5.2659917855635285e-05, 5.177481580176391e-05, 5.09328929183539e-05, 5.0130747695220634e-05, 4.93653496960178e-05, 4.8633970436640084e-05, 4.7934165195329115e-05, 4.726372208097018e-05, 4.662065111915581e-05, 4.600312968250364e-05, 4.540951704257168e-05, 4.483830343815498e-05, 4.428812098922208e-05, 4.375770367914811e-05, 4.3245901906630024e-05, 4.2751646105898544e-05, 4.227395766065456e-05, 4.181193435215391e-05, 4.136473580729216e-05, 4.0931583498604596e-05, 4.051176438224502e-05, 4.010459815617651e-05, 3.970947000198066e-05, 3.932579420506954e-05, 3.895302870660089e-05, 3.859066782752052e-05, 3.82382349926047e-05, 3.789528273046017e-05, 3.756139994948171e-05, 3.7236186472000554e-05, 3.691927486215718e-05, 3.6610323149943724e-05, 3.630899664130993e-05, 3.6014993384014815e-05, 3.572801506379619e-05, 3.544779247022234e-05, 3.517406003084034e-05, 3.490657400107011e-05, 3.46450979122892e-05, 3.4389409847790375e-05, 3.413930244278163e-05, 3.389457197044976e-05, 3.3655032893875614e-05, 3.3420499676140025e-05, 3.319080133223906e-05, 3.296577779110521e-05, 3.274526898167096e-05, 3.252912574680522e-05, 3.2317206205334514e-05, 3.210937575204298e-05, 3.1905507057672366e-05, 3.170546915498562e-05, 3.1509149266639724e-05, 3.131643097731285e-05, 3.112720514764078e-05, 3.094136991421692e-05, 3.075882341363467e-05, 3.057947105844505e-05, 3.0403218261199072e-05, 3.0229979529394768e-05, 3.005966755154077e-05, 2.9892204111092724e-05, 2.972750735352747e-05, 2.9565502700279467e-05, 2.9406119210761972e-05, 2.9249285944388248e-05, 2.909493377956096e-05, 2.894300087064039e-05, 2.879342355299741e-05, 2.8646141799981706e-05, 2.8501097403932363e-05, 2.835823397617787e-05, 2.8217496947036125e-05, 2.8078835384803824e-05, 2.794219835777767e-05, 2.7807536753243767e-05, 2.7674803277477622e-05, 2.7543952455744147e-05, 2.7414938813308254e-05, 2.7287724151392467e-05, 2.7162261176272295e-05, 2.703851532714907e-05, 2.6916444767266512e-05, 2.6796013116836548e-05, 2.6677182177081704e-05, 2.6559919206192717e-05, 2.644418964337092e-05, 2.6329958927817643e-05, 2.6217196136713028e-05, 2.6105870347237214e-05, 2.5995950636570342e-05, 2.5887407900881954e-05, 2.5780213036341593e-05, 2.5674338758108206e-05, 2.556975778134074e-05, 2.546644645917695e-05, 2.5364375687786378e-05, 2.526352363929618e-05, 2.5163863028865308e-05, 2.5065373847610317e-05, 2.4968032448668964e-05, 2.4871817004168406e-05, 2.4776703867246397e-05, 2.46826748480089e-05, 2.4589708118583076e-05, 2.4497783670085482e-05, 2.440688149363268e-05, 2.4316985218320042e-05, 2.4228074835264124e-05, 2.4140132154570892e-05, 2.4053140805335715e-05, 2.3967082597664557e-05, 2.3881941160652786e-05, 2.3797701942385174e-05, 2.3714346752967685e-05, 2.363186104048509e-05, 2.3550232072011568e-05, 2.346944165765308e-05, 2.33894770644838e-05, 2.3310325559577905e-05, 2.3231970772030763e-05, 2.3154401787905954e-05, 2.3077604055288248e-05, 2.300156484125182e-05, 2.292627323186025e-05, 2.2851716494187713e-05, 2.2777881895308383e-05, 2.270475852128584e-05, 2.263233545818366e-05, 2.2560599973076023e-05, 2.2489542971015908e-05, 2.241915171907749e-05, 2.2349418941303156e-05, 2.2280333723756485e-05, 2.221188333351165e-05, 2.214406049461104e-05, 2.2076856112107635e-05, 2.201025745307561e-05, 2.194426087953616e-05, 2.1878851839574054e-05, 2.1814026695210487e-05, 2.1749772713519633e-05, 2.1686084437533282e-05, 2.1622950953315012e-05, 2.1560366803896613e-05, 2.1498321075341664e-05, 2.1436810129671358e-05, 2.1375823052949272e-05, 2.1315354388207197e-05, 2.1255395040498115e-05, 2.1195939552853815e-05, 2.113697883032728e-05, 2.1078509234939702e-05, 2.1020521671744063e-05, 2.0963010683772154e-05, 2.0905967176076956e-05, 2.0849387510679662e-05, 2.0793266230612062e-05, 2.0737594240927137e-05, 2.068236790364608e-05, 2.062757994281128e-05, 2.0573224901454523e-05, 2.05192973226076e-05, 2.0465791749302298e-05, 2.041270272457041e-05, 2.036002479144372e-05, 2.0307752492954023e-05, 2.025588219112251e-05, 2.0204404791002162e-05, 2.015332029259298e-05], "accuracy_train_first": 0.5439058823529411, "accuracy_train_last": 0.9680705882352941, "batch_size_eval": 1024, "accuracy_train_std": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "accuracy_test_std": 0, "error_valid": [0.4641333333333333, 0.39346666666666663, 0.3314666666666667, 0.29866666666666664, 0.2698666666666667, 0.24839999999999995, 0.2366666666666667, 0.2273333333333334, 0.22493333333333332, 0.21519999999999995, 0.2082666666666667, 0.2069333333333333, 0.20306666666666662, 0.19733333333333336, 0.19653333333333334, 0.18733333333333335, 0.19120000000000004, 0.18613333333333337, 0.18679999999999997, 0.18320000000000003, 0.1830666666666667, 0.1817333333333333, 0.18013333333333337, 0.17959999999999998, 0.1790666666666667, 0.17679999999999996, 0.17226666666666668, 0.1770666666666667, 0.1750666666666667, 0.1738666666666666, 0.1790666666666667, 0.17400000000000004, 0.17279999999999995, 0.17786666666666662, 0.17413333333333336, 0.17293333333333338, 0.17333333333333334, 0.17359999999999998, 0.1684, 0.16733333333333333, 0.16920000000000002, 0.16986666666666672, 0.17053333333333331, 0.17120000000000002, 0.1690666666666667, 0.17053333333333331, 0.16786666666666672, 0.17013333333333336, 0.16826666666666668, 0.1665333333333333, 0.16559999999999997, 0.16826666666666668, 0.16586666666666672, 0.16693333333333338, 0.1684, 0.1657333333333333, 0.16746666666666665, 0.16679999999999995, 0.16666666666666663, 0.1657333333333333, 0.16679999999999995, 0.16479999999999995, 0.16813333333333336, 0.16786666666666672, 0.16746666666666665, 0.16746666666666665, 0.16546666666666665, 0.16693333333333338, 0.1665333333333333, 0.16733333333333333, 0.16613333333333336, 0.16559999999999997, 0.16720000000000002, 0.16479999999999995, 0.16800000000000004, 0.1664, 0.16693333333333338, 0.1664, 0.16920000000000002, 0.1664, 0.16746666666666665, 0.1684, 0.16586666666666672, 0.16613333333333336, 0.16559999999999997, 0.16759999999999997, 0.16586666666666672, 0.16613333333333336, 0.16546666666666665, 0.16613333333333336, 0.1644, 0.16413333333333335, 0.1677333333333333, 0.16559999999999997, 0.16333333333333333, 0.16613333333333336, 0.16426666666666667, 0.1652, 0.16666666666666663, 0.16600000000000004, 0.16346666666666665, 0.1652, 0.16586666666666672, 0.16413333333333335, 0.1637333333333333, 0.1650666666666667, 0.16613333333333336, 0.16493333333333338, 0.1652, 0.16533333333333333, 0.16466666666666663, 0.16359999999999997, 0.16400000000000003, 0.16733333333333333, 0.16493333333333338, 0.1645333333333333, 0.16386666666666672, 0.1665333333333333, 0.16173333333333328, 0.16226666666666667, 0.16493333333333338, 0.16493333333333338, 0.16466666666666663, 0.16533333333333333, 0.16426666666666667, 0.16159999999999997, 0.16626666666666667, 0.1632, 0.16493333333333338, 0.16426666666666667, 0.1652, 0.16559999999999997, 0.16759999999999997, 0.16666666666666663, 0.1645333333333333, 0.16559999999999997, 0.16359999999999997, 0.16359999999999997, 0.16479999999999995, 0.16293333333333337, 0.16626666666666667, 0.16186666666666671, 0.16479999999999995, 0.1637333333333333, 0.16493333333333338, 0.16426666666666667, 0.16333333333333333, 0.16133333333333333, 0.16586666666666672, 0.1645333333333333, 0.16666666666666663, 0.16600000000000004, 0.1652, 0.16359999999999997, 0.1632, 0.16600000000000004, 0.16426666666666667, 0.16386666666666672, 0.1650666666666667, 0.1645333333333333, 0.16666666666666663, 0.16400000000000003, 0.1657333333333333, 0.16333333333333333, 0.16266666666666663, 0.1644, 0.16413333333333335, 0.16546666666666665, 0.16359999999999997, 0.1645333333333333, 0.16600000000000004, 0.16133333333333333, 0.1650666666666667, 0.16333333333333333, 0.1637333333333333, 0.1644, 0.1665333333333333, 0.1677333333333333, 0.16720000000000002, 0.1652, 0.16479999999999995, 0.16413333333333335, 0.16279999999999994, 0.16626666666666667, 0.16626666666666667, 0.16759999999999997, 0.16746666666666665, 0.1650666666666667, 0.16626666666666667, 0.1650666666666667, 0.16479999999999995, 0.1657333333333333, 0.16533333333333333, 0.1644, 0.1665333333333333, 0.16400000000000003, 0.1645333333333333, 0.16400000000000003], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.04424167593261145, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 16, "valid_ratio": 0.15, "learning_rate": 0.0002835823409912535, "optimization": "rmsprop", "nb_data_augmentation": 3, "learning_rate_decay_method": "sqrt", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 6.12179998879519e-07, "rotation_range": [0, 0], "momentum": 0.9420172485942455}, "accuracy_valid_max": 0.8386666666666667, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        y_pred = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            y_pred.extend((nnet.predict(X[mini_batch]) == y[mini_batch]).tolist())\n        return np.mean(y_pred)\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            acc = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = acc\n            status[\"accuracy_train_std\"] = 0\n            acc = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = acc\n            status[\"accuracy_valid_std\"] = 0\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n\n    # rescaling to [-1, 1]\n    X_min = X_train.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X_train.max(axis=(0, 2, 3))[None, :, None, None]\n    def preprocess(a):\n        return (a / 255.) * 2 - 1\n        # return 2 * ((a - X_min) / (X_max - X_min)) - 1\n    X_train = preprocess(X_train)\n    X_valid = preprocess(X_valid)\n\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = preprocess(X_test)\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    acc = evaluate(X_test, y_test, batch_size_eval)\n    light.set(\"accuracy_test\", acc)\n    light.set(\"accuracy_test_std\", 0)\n    print(\"Test accuracy : {}+-{}\".format(acc, 0))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.836, "accuracy_valid_std": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "accuracy_valid": [0.5358666666666667, 0.6065333333333334, 0.6685333333333333, 0.7013333333333334, 0.7301333333333333, 0.7516, 0.7633333333333333, 0.7726666666666666, 0.7750666666666667, 0.7848, 0.7917333333333333, 0.7930666666666667, 0.7969333333333334, 0.8026666666666666, 0.8034666666666667, 0.8126666666666666, 0.8088, 0.8138666666666666, 0.8132, 0.8168, 0.8169333333333333, 0.8182666666666667, 0.8198666666666666, 0.8204, 0.8209333333333333, 0.8232, 0.8277333333333333, 0.8229333333333333, 0.8249333333333333, 0.8261333333333334, 0.8209333333333333, 0.826, 0.8272, 0.8221333333333334, 0.8258666666666666, 0.8270666666666666, 0.8266666666666667, 0.8264, 0.8316, 0.8326666666666667, 0.8308, 0.8301333333333333, 0.8294666666666667, 0.8288, 0.8309333333333333, 0.8294666666666667, 0.8321333333333333, 0.8298666666666666, 0.8317333333333333, 0.8334666666666667, 0.8344, 0.8317333333333333, 0.8341333333333333, 0.8330666666666666, 0.8316, 0.8342666666666667, 0.8325333333333333, 0.8332, 0.8333333333333334, 0.8342666666666667, 0.8332, 0.8352, 0.8318666666666666, 0.8321333333333333, 0.8325333333333333, 0.8325333333333333, 0.8345333333333333, 0.8330666666666666, 0.8334666666666667, 0.8326666666666667, 0.8338666666666666, 0.8344, 0.8328, 0.8352, 0.832, 0.8336, 0.8330666666666666, 0.8336, 0.8308, 0.8336, 0.8325333333333333, 0.8316, 0.8341333333333333, 0.8338666666666666, 0.8344, 0.8324, 0.8341333333333333, 0.8338666666666666, 0.8345333333333333, 0.8338666666666666, 0.8356, 0.8358666666666666, 0.8322666666666667, 0.8344, 0.8366666666666667, 0.8338666666666666, 0.8357333333333333, 0.8348, 0.8333333333333334, 0.834, 0.8365333333333334, 0.8348, 0.8341333333333333, 0.8358666666666666, 0.8362666666666667, 0.8349333333333333, 0.8338666666666666, 0.8350666666666666, 0.8348, 0.8346666666666667, 0.8353333333333334, 0.8364, 0.836, 0.8326666666666667, 0.8350666666666666, 0.8354666666666667, 0.8361333333333333, 0.8334666666666667, 0.8382666666666667, 0.8377333333333333, 0.8350666666666666, 0.8350666666666666, 0.8353333333333334, 0.8346666666666667, 0.8357333333333333, 0.8384, 0.8337333333333333, 0.8368, 0.8350666666666666, 0.8357333333333333, 0.8348, 0.8344, 0.8324, 0.8333333333333334, 0.8354666666666667, 0.8344, 0.8364, 0.8364, 0.8352, 0.8370666666666666, 0.8337333333333333, 0.8381333333333333, 0.8352, 0.8362666666666667, 0.8350666666666666, 0.8357333333333333, 0.8366666666666667, 0.8386666666666667, 0.8341333333333333, 0.8354666666666667, 0.8333333333333334, 0.834, 0.8348, 0.8364, 0.8368, 0.834, 0.8357333333333333, 0.8361333333333333, 0.8349333333333333, 0.8354666666666667, 0.8333333333333334, 0.836, 0.8342666666666667, 0.8366666666666667, 0.8373333333333334, 0.8356, 0.8358666666666666, 0.8345333333333333, 0.8364, 0.8354666666666667, 0.834, 0.8386666666666667, 0.8349333333333333, 0.8366666666666667, 0.8362666666666667, 0.8356, 0.8334666666666667, 0.8322666666666667, 0.8328, 0.8348, 0.8352, 0.8358666666666666, 0.8372, 0.8337333333333333, 0.8337333333333333, 0.8324, 0.8325333333333333, 0.8349333333333333, 0.8337333333333333, 0.8349333333333333, 0.8352, 0.8342666666666667, 0.8346666666666667, 0.8356, 0.8334666666666667, 0.836, 0.8354666666666667, 0.836], "seed": 48339372, "model": "residualv5", "loss_std": [0.28326982259750366, 0.23544156551361084, 0.235380157828331, 0.2334584891796112, 0.23068773746490479, 0.22989988327026367, 0.22643724083900452, 0.22666729986667633, 0.2246614694595337, 0.2211768478155136, 0.21879468858242035, 0.21648745238780975, 0.21427445113658905, 0.21172814071178436, 0.2093639373779297, 0.20750364661216736, 0.20446732640266418, 0.20453093945980072, 0.20286083221435547, 0.20051805675029755, 0.20002511143684387, 0.19705213606357574, 0.19332371652126312, 0.19753405451774597, 0.19253380596637726, 0.19063661992549896, 0.18796604871749878, 0.18835370242595673, 0.18604373931884766, 0.18588979542255402, 0.18483972549438477, 0.1824944019317627, 0.1818459928035736, 0.17923806607723236, 0.17764215171337128, 0.1781492680311203, 0.1752753108739853, 0.17487670481204987, 0.17435915768146515, 0.17341287434101105, 0.17329376935958862, 0.16922274231910706, 0.16938750445842743, 0.16858674585819244, 0.1671837419271469, 0.16566739976406097, 0.16384494304656982, 0.16453608870506287, 0.1648181974887848, 0.1632266491651535, 0.16273891925811768, 0.16295981407165527, 0.16024774312973022, 0.15877051651477814, 0.15844453871250153, 0.15882350504398346, 0.15720537304878235, 0.15743979811668396, 0.1587173044681549, 0.1540662944316864, 0.15329492092132568, 0.15277142822742462, 0.15204718708992004, 0.1508597433567047, 0.1528082937002182, 0.14968933165073395, 0.1480434685945511, 0.1475992500782013, 0.14797890186309814, 0.14517837762832642, 0.14731279015541077, 0.1450728178024292, 0.14587867259979248, 0.14282120764255524, 0.14275243878364563, 0.14155223965644836, 0.14174045622348785, 0.14010852575302124, 0.1406206637620926, 0.13728158175945282, 0.1363620012998581, 0.1362360417842865, 0.1380244344472885, 0.13511651754379272, 0.1363840401172638, 0.13877931237220764, 0.13716383278369904, 0.13671310245990753, 0.1316503882408142, 0.13274532556533813, 0.13115732371807098, 0.13160531222820282, 0.13337640464305878, 0.1333501935005188, 0.131120502948761, 0.13050325214862823, 0.12950249016284943, 0.12723393738269806, 0.13011974096298218, 0.12703904509544373, 0.12599840760231018, 0.12620285153388977, 0.12371793389320374, 0.12690949440002441, 0.12416638433933258, 0.1227860227227211, 0.12055657804012299, 0.12320354580879211, 0.12225151062011719, 0.12279008328914642, 0.12143494188785553, 0.11994070559740067, 0.12019338458776474, 0.11944567412137985, 0.12142440676689148, 0.11820577830076218, 0.11905526369810104, 0.11635129898786545, 0.11945844441652298, 0.1147250384092331, 0.1151505634188652, 0.11624543368816376, 0.11695660650730133, 0.11658929288387299, 0.1148177832365036, 0.1130201667547226, 0.112360879778862, 0.1120404526591301, 0.112403005361557, 0.11224581301212311, 0.11154891550540924, 0.10989618301391602, 0.11021162569522858, 0.1099209263920784, 0.11042238026857376, 0.11054294556379318, 0.11021433770656586, 0.10801830887794495, 0.1098831444978714, 0.10925837606191635, 0.10907364636659622, 0.1079159528017044, 0.10670294612646103, 0.10810704529285431, 0.10433171689510345, 0.1052582785487175, 0.105572409927845, 0.10696355998516083, 0.10455671697854996, 0.10539232939481735, 0.10359395295381546, 0.10362328588962555, 0.10470004379749298, 0.10511733591556549, 0.10458025336265564, 0.10306155681610107, 0.10109983384609222, 0.10105733573436737, 0.10022086650133133, 0.10259004682302475, 0.10129981487989426, 0.10128293931484222, 0.09723305702209473, 0.0993812084197998, 0.10125836730003357, 0.09725496172904968, 0.09879540652036667, 0.09869266301393509, 0.0972026139497757, 0.09761963784694672, 0.0985165610909462, 0.09726708382368088, 0.09703706204891205, 0.09548027813434601, 0.0958418995141983, 0.09418264776468277, 0.09394824504852295, 0.09609729796648026, 0.09376958757638931, 0.09281859546899796, 0.09308656305074692, 0.09578526765108109, 0.09373705089092255, 0.09312399476766586, 0.09399658441543579, 0.08982231467962265, 0.09226054698228836, 0.09167668968439102, 0.08899486064910889, 0.09016977995634079, 0.09193876385688782, 0.0913965106010437, 0.09062837809324265, 0.08924847841262817, 0.09200126677751541, 0.08854580670595169, 0.08936423063278198, 0.08991175889968872]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:47 2016", "state": "available"}], "summary": "002a00f3857043ad2a817dbffd448840"}