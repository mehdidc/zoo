{"content": {"hp_model": {"f0": 32, "f1": 16, "f2": 64, "f3": 16, "nonlin": "very_leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.020919791118560595, 0.01751113910742212, 0.021172893920302013, 0.015147890549384063, 0.012228320060310171, 0.013328159259459354, 0.007006319512573799, 0.0053551247889633505, 0.0036693294909892374, 0.0087444093823007, 0.005883352975113603, 0.004132657115027066, 0.005305451485408538, 0.00756501520519529, 0.004651984205770267, 0.0068543278552127595, 0.0060587533828095775, 0.007947662608849049, 0.00730294308413752, 0.00604386973138147, 0.00870578342415044, 0.007059234859724893, 0.005494983563905754, 0.007966402690017391, 0.010456429955702324, 0.00916679137344811, 0.007645575459701547, 0.010055825258091945, 0.010908986664157393, 0.00815356519783888, 0.009072780905249213, 0.010276257102244916, 0.011323796737899128, 0.010497738219785682, 0.010003158247909259, 0.010469421913372707, 0.012662869738676004, 0.013209346019823867, 0.011304779263977653, 0.011326834741184628, 0.013898528995372405, 0.011890815104831951, 0.009072779117177994, 0.011818333340802232, 0.013144770617608237, 0.010790012552902774, 0.010997977113396916, 0.00897681433301164, 0.011483437407988569, 0.010843345582688659, 0.013265770437611157, 0.011273906293790098, 0.013913763110779484, 0.012371117612205202, 0.011406762435478508, 0.012073642210422725, 0.011392966649903333, 0.010495091819105162, 0.013033402385848098, 0.013447872176332224, 0.012491649217118622, 0.013186348449559555, 0.013193684726814781, 0.011409595119984368, 0.012794030312788356, 0.012011353562256296, 0.012308593457303271, 0.012091864060677995, 0.012345527915749766, 0.012532005745910468, 0.011006572031339283, 0.013070609851261576, 0.012212439490918757, 0.011813268259946713, 0.012380206163445828, 0.011304082771221984, 0.011525065650877514, 0.012395046845961441, 0.011905851135387786, 0.011368673200205643, 0.012387164947214334, 0.011394818081940363, 0.013312776905171507, 0.012463916080522525, 0.013668024242139054, 0.013624009602646413, 0.013147993524605684, 0.013180472106003059, 0.012287454905062618, 0.012649443471959258, 0.012562361344788844, 0.012385831483190031, 0.01207665081045898, 0.011848835443955565, 0.012892531054934, 0.013472141004557747, 0.01154206153028212, 0.012393714229966116, 0.013848310966930736, 0.013909264063728707, 0.012535837334179612, 0.01277686391198851, 0.013718501505832908, 0.011814142083480321, 0.013632335316448842, 0.01222910612307122, 0.012211763237534331, 0.013117008610327805, 0.012424667806820002, 0.012004184401679115, 0.011442018599190626, 0.013553405438050772, 0.012800630895211055, 0.013676743225739176, 0.013376338339999965, 0.012271138190089651, 0.0124799807542311, 0.011224303622950717, 0.012280848955156898, 0.013806867274994928, 0.012142417521109132, 0.01327432084253258, 0.013506219565042629, 0.012170477603394562, 0.01129353216312488, 0.013438490373663644, 0.012086517835045009, 0.013252717739646864, 0.01140248059929952, 0.012259662564938595, 0.013337466548644485, 0.012624154863099568, 0.01225074837248734, 0.01135646746556311, 0.013211743027654934, 0.012084192974347714, 0.013351209605876706, 0.012214063816158053, 0.012925637501736259, 0.012558273377230296, 0.01358052653223303, 0.01270325487006426, 0.013046481850304558, 0.01350833955995711, 0.012376203091048556], "moving_avg_accuracy_train": [0.0549951946924603, 0.11390399154150514, 0.17431168223254656, 0.23274188250896888, 0.28737705022429566, 0.3390247683573201, 0.3869182869304105, 0.43132689817715353, 0.4728428369181775, 0.510930158869586, 0.546524674705588, 0.5795289655233735, 0.6097211085093805, 0.6374683489527393, 0.6634848571672384, 0.6873786591662306, 0.7093270401926678, 0.7297524429759573, 0.7484701269094893, 0.7659367850841734, 0.7820288372997316, 0.7967836906556296, 0.8102653105735477, 0.8227732256044634, 0.8343697487608407, 0.8451065637980087, 0.8547115325624032, 0.8638372742051108, 0.8721388694359471, 0.8797543922722527, 0.886803639276109, 0.8932457259760358, 0.8992388804571514, 0.9047257254425363, 0.9099336031912875, 0.9146137177187349, 0.9189630045731995, 0.9230121853243514, 0.9266727600908734, 0.9300648975819245, 0.933224850266746, 0.9360687716342666, 0.9387167646662534, 0.9411439920271475, 0.9433379053936465, 0.9453519189044387, 0.9472272740331901, 0.9489522960300187, 0.9505350427616883, 0.9519571536225627, 0.9533580332330824, 0.9546443654706361, 0.9558114011284913, 0.9568036405491416, 0.9577385087062984, 0.9586310072727302, 0.9593948005503944, 0.9601518968669405, 0.9607263627554126, 0.961310775321695, 0.961876274161111, 0.9623829340165946, 0.9627621979758155, 0.9631895299962481, 0.963546227028923, 0.9639114321857114, 0.9642981734494216, 0.9645927621641417, 0.9648556389562177, 0.9650829635226668, 0.9652758938396045, 0.9655216467867626, 0.9656986105630051, 0.9658810934008999, 0.9660197873669192, 0.9661376725387267, 0.9662251319540586, 0.9662968699814286, 0.9664312247191661, 0.9665358318926444, 0.9666090159606704, 0.9667307212421412, 0.9668170405561884, 0.9668760546507172, 0.9669617194191265, 0.9669807250392755, 0.9670628982152575, 0.9671275534784032, 0.9671602026271483, 0.9672384149860189, 0.9672529664887551, 0.9672544731459889, 0.967302296064871, 0.9673244464013979, 0.9674094137733014, 0.9674324420342141, 0.9674206153857022, 0.9674262834925269, 0.9675104037993743, 0.9675233691064985, 0.967511786394815, 0.9674850859126333, 0.9675377853893841, 0.9675549879839358, 0.9675495439797468, 0.967581846756929, 0.9676109192563931, 0.967576630636863, 0.9675969241530955, 0.9675128817700857, 0.9675581153146533, 0.9676081261000024, 0.9676438352115783, 0.9676690700632056, 0.9676986847784612, 0.9677299883198104, 0.9677535472582244, 0.9677514627658829, 0.9677310215811181, 0.9677382011517347, 0.9677539273117088, 0.9677378539211619, 0.9677373748113455, 0.9677345824148824, 0.9677042395699889, 0.9676931349536139, 0.9677017419893525, 0.9677722673393745, 0.9677845868805848, 0.9678119505093407, 0.9677900747990305, 0.967805299940713, 0.9678027265265604, 0.967798049256195, 0.9678031403081041, 0.9677752062203077, 0.9677570049389008, 0.9677383346856439, 0.9677284708553223, 0.9677637712354139, 0.9677792655358297, 0.9677676337692991, 0.9678060293532403, 0.9677870709073497, 0.9678048855381909], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 297381999, "moving_var_accuracy_train": [0.027220242953354533, 0.055730435773837406, 0.08299919404807449, 0.10542606938235247, 0.12174847640565423, 0.1335810098602239, 0.14086701096600013, 0.14452943264517926, 0.14558864790659745, 0.14408557995680993, 0.14107974797952344, 0.1367753220930358, 0.13130187936651969, 0.12510087559986177, 0.11868251633695147, 0.11195248866895975, 0.10509282266919875, 0.09833831411201538, 0.09165764792733426, 0.08523764046472218, 0.07904446371882426, 0.07309936862492833, 0.06742521844293634, 0.062090728044428195, 0.05709196938383291, 0.05242028521973084, 0.04800855552244268, 0.04395721241496345, 0.040181739523856896, 0.036685531264103836, 0.03346420508758585, 0.03049128890827165, 0.02776542112315512, 0.0252598282218824, 0.022977943315507596, 0.020877280231867062, 0.018959798873962125, 0.0172113817693654, 0.015610841861020542, 0.014153317045742237, 0.012827853049900813, 0.011617858743612498, 0.010519179673128303, 0.009520284599768726, 0.00861157544252918, 0.007786924152071145, 0.007039884348594439, 0.006362677221740879, 0.005748955284516288, 0.005192261349770211, 0.0046906973879417176, 0.004236519504775875, 0.0038251253043386367, 0.0034514736255158054, 0.003114192069205615, 0.0028099418455047992, 0.0025341980824933647, 0.002285937027736776, 0.002060313424476261, 0.0018573559244572928, 0.0016744984324479908, 0.0015093589270856194, 0.001359717604733933, 0.001225389358161722, 0.0011039955173036218, 0.0009947963388321633, 0.0008966628241944521, 0.0008077775843725707, 0.0007276217638056234, 0.0006553246755516625, 0.0005901272069612398, 0.0005316580368644475, 0.000478774078780921, 0.000431196370777964, 0.000388249857846059, 0.000349549944285042, 0.0003146637922005095, 0.0002832437300815971, 0.0002550818178334101, 0.0002296721199967573, 0.00020675311116739718, 0.00018621110963049843, 0.00016765705788324687, 0.00015092269606509969, 0.00013589647253150924, 0.00012231007620073356, 0.00011013984045831894, 9.916347913995903e-05, 8.925672492818717e-05, 8.038610699308941e-05, 7.234940200986742e-05, 6.511448223902485e-05, 5.8623617299256e-05, 5.276567130600471e-05, 4.755407926399777e-05, 4.280344404480399e-05, 3.8524358466858796e-05, 3.467221176708771e-05, 3.126867662459576e-05, 2.8143321854835618e-05, 2.5330197102241534e-05, 2.280359363375603e-05, 2.0548229384028604e-05, 1.8496069808959574e-05, 1.6646729562698112e-05, 1.4991447831151448e-05, 1.3499909940062048e-05, 1.2160500330919328e-05, 1.09481567390371e-05, 9.916909164411092e-06, 8.943632909957371e-06, 8.071779326822644e-06, 7.276077659986299e-06, 6.55420107361755e-06, 5.906674248492863e-06, 5.3248260289525565e-06, 4.797338638270025e-06, 4.317643880417917e-06, 3.889640070687398e-06, 3.5011399797267933e-06, 3.153251790721917e-06, 2.8402517966028234e-06, 2.5562286828584872e-06, 2.300675991874702e-06, 2.0788945868133437e-06, 1.8721149406755401e-06, 1.6855701761858435e-06, 1.5617775835286887e-06, 1.406965765036488e-06, 1.27300810214103e-06, 1.1500142122411152e-06, 1.0370990354702464e-06, 9.334487340668245e-07, 8.403007523827926e-07, 7.565039464303845e-07, 6.878763711364858e-07, 6.220703138265222e-07, 5.630004876539962e-07, 5.075760952260962e-07, 4.680335372150024e-07, 4.233908436018734e-07, 3.822694411752764e-07, 3.573104848534652e-07, 3.248142404034213e-07, 2.9518906601117113e-07], "duration": 113527.057421, "accuracy_train": [0.5499519469246031, 0.6440831631829088, 0.7179808984519196, 0.75861368499677, 0.779093559662237, 0.8038542315545404, 0.8179599540882245, 0.8310043993978405, 0.8464862855873938, 0.8537160564322629, 0.8668753172296051, 0.876567582883444, 0.881450395383444, 0.8871935129429678, 0.8976334310977298, 0.9024228771571613, 0.9068624694306018, 0.9135810680255629, 0.9169292823112772, 0.9231367086563308, 0.9268573072397563, 0.9295773708587117, 0.9315998898348099, 0.9353444608827058, 0.9387384571682356, 0.9417378991325213, 0.9411562514419527, 0.9459689489894795, 0.9468532265134736, 0.9482940977990033, 0.9502468623108158, 0.9512245062753784, 0.9531772707871908, 0.9541073303110004, 0.956804502930048, 0.9567347484657622, 0.9581065862633813, 0.9594548120847176, 0.9596179329895718, 0.9605941350013842, 0.9616644244301403, 0.9616640639419527, 0.9625487019541344, 0.9629890382751938, 0.9630831256921374, 0.9634780405015688, 0.9641054701919527, 0.9644774940014765, 0.9647797633467147, 0.9647561513704319, 0.9659659497277593, 0.9662213556086194, 0.9663147220491879, 0.9657337953349945, 0.9661523221207088, 0.9666634943706165, 0.9662689400493725, 0.9669657637158545, 0.9658965557516611, 0.9665704884182356, 0.9669657637158545, 0.9669428727159468, 0.966175573608804, 0.9670355181801403, 0.9667565003229974, 0.967198278596807, 0.9677788448228128, 0.9672440605966224, 0.9672215300849022, 0.9671288846207088, 0.9670122666920451, 0.967733423311185, 0.9672912845491879, 0.9675234389419527, 0.9672680330610927, 0.9671986390849945, 0.9670122666920451, 0.9669425122277593, 0.967640417358804, 0.9674772964539498, 0.9672676725729051, 0.9678260687753784, 0.9675939143826136, 0.9674071815014765, 0.9677327023348099, 0.9671517756206165, 0.9678024567990956, 0.9677094508467147, 0.9674540449658545, 0.9679423262158545, 0.9673839300133813, 0.9672680330610927, 0.9677327023348099, 0.9675237994301403, 0.9681741201204319, 0.967639696382429, 0.9673141755490956, 0.9674772964539498, 0.9682674865610004, 0.9676400568706165, 0.967407541989664, 0.9672447815729974, 0.9680120806801403, 0.9677098113349022, 0.9675005479420451, 0.9678725717515688, 0.9678725717515688, 0.9672680330610927, 0.9677795657991879, 0.9667565003229974, 0.9679652172157622, 0.9680582231681433, 0.9679652172157622, 0.9678961837278516, 0.9679652172157622, 0.9680117201919527, 0.9679655777039498, 0.9677327023348099, 0.9675470509182356, 0.9678028172872831, 0.9678954627514765, 0.9675931934062385, 0.9677330628229974, 0.9677094508467147, 0.9674311539659468, 0.9675931934062385, 0.9677792053110004, 0.9684069954895718, 0.9678954627514765, 0.9680582231681433, 0.9675931934062385, 0.9679423262158545, 0.9677795657991879, 0.9677559538229051, 0.9678489597752861, 0.9675237994301403, 0.9675931934062385, 0.9675703024063308, 0.967639696382429, 0.9680814746562385, 0.9679187142395718, 0.9676629478705242, 0.9681515896087117, 0.9676164448943337, 0.9679652172157622], "end": "2016-01-25 06:23:43.665000", "learning_rate_per_epoch": [0.0003831217181868851, 0.0003509100351948291, 0.00032140660914592445, 0.00029438373167067766, 0.0002696328447200358, 0.0002469629398547113, 0.00022619905939791352, 0.00020718094310723245, 0.00018976180581375957, 0.0001738072169246152, 0.00015919403813313693, 0.0001458094920963049, 0.00013355027476791292, 0.0001223217841470614, 0.00011203734175069258, 0.00010261758870910853, 9.398981637787074e-05, 8.60874424688518e-05, 7.884947262937203e-05, 7.222004933282733e-05, 6.614800804527476e-05, 6.0586487961700186e-05, 5.549256093217991e-05, 5.082691859570332e-05, 4.6553548600059e-05, 4.263946902938187e-05, 3.905447374563664e-05, 3.5770892282016575e-05, 3.276338611613028e-05, 3.0008741305209696e-05, 2.748569932009559e-05, 2.51747860602336e-05, 2.305816815351136e-05, 2.1119509256095625e-05, 1.9343846361152828e-05, 1.7717475202516653e-05, 1.6227844753302634e-05, 1.4863457181490958e-05, 1.36137832669192e-05, 1.2469178727769759e-05, 1.1420808732509613e-05, 1.0460582416271791e-05, 9.581089216226246e-06, 8.775540663918946e-06, 8.037720363063272e-06, 7.3619335125840735e-06, 6.742964615114033e-06, 6.17603700447944e-06, 5.656774646922713e-06, 5.181170308787841e-06, 4.745553269458469e-06, 4.3465615817694925e-06, 3.9811161514080595e-06, 3.6463961805566214e-06, 3.3398184768884676e-06, 3.0590167625632603e-06, 2.8018241664540255e-06, 2.5662554890004685e-06, 2.350492650293745e-06, 2.152870365534909e-06, 1.9718636394827627e-06, 1.806075374588545e-06, 1.6542261391805368e-06, 1.5151439356486662e-06, 1.3877553328711656e-06, 1.2710771670754184e-06, 1.1642089248198317e-06, 1.0663258080967353e-06, 9.766723678694689e-07, 8.945567628870776e-07, 8.193451890292636e-07, 7.504571613026201e-07, 6.87361023210542e-07, 6.295697971836489e-07, 5.766374897575588e-07, 5.281555672809191e-07, 4.837498863707879e-07, 4.4307768121143454e-07, 4.058250624439097e-07, 3.717045444773248e-07, 3.4045277175209776e-07, 3.118285576420021e-07, 2.856109801996354e-07, 2.6159767685385305e-07, 2.396033380591689e-07, 2.1945822936686454e-07, 2.0100685560464626e-07, 1.8410682400826772e-07, 1.6862769314229809e-07, 1.5444999235114665e-07, 1.4146431226436107e-07, 1.2957042372363503e-07, 1.1867653881836304e-07, 1.0869857902662261e-07, 9.955953572671206e-08, 9.118887334125247e-08, 8.352199643013591e-08, 7.649972388890092e-08, 7.006786262309106e-08, 6.417677411718614e-08, 5.8780987188811196e-08, 5.383886403365068e-08, 4.931225916493531e-08, 4.516623519634777e-08, 4.1368796388496776e-08, 3.7890636406245903e-08, 3.470490739232446e-08, 3.178702456807514e-08, 2.9114469057844872e-08, 2.666661380601454e-08, 2.442456725759712e-08, 2.2371024144263174e-08, 2.0490137586648416e-08, 1.8767389420304426e-08, 1.7189485390645132e-08, 1.5744246795179606e-08, 1.4420519001134835e-08, 1.3208086180327427e-08, 1.2097591373105843e-08, 1.1080462769541555e-08, 1.0148851536939674e-08, 9.295566982814307e-09, 8.514024152361799e-09, 7.798191425933965e-09, 7.14254344558185e-09, 6.5420202588484244e-09, 5.9919873507396915e-09, 5.488199672498695e-09, 5.026768779003987e-09, 4.604133518881781e-09, 4.217032056885728e-09, 3.862477004901166e-09, 3.5377318852169992e-09, 3.240290258332834e-09, 2.9678566271229556e-09, 2.7183284512233286e-09, 2.4897797157308332e-09, 2.28044672034855e-09, 2.0887138685310447e-09, 1.9131012329864916e-09, 1.752253564468731e-09, 1.6049295226139293e-09, 1.4699920170002656e-09, 1.3463996584306415e-09, 1.233198543282299e-09], "accuracy_valid": [0.540783250188253, 0.6317506353539157, 0.7079563370670181, 0.7404991057981928, 0.7545371917356928, 0.7666927475527108, 0.7830207548945783, 0.7928893307605422, 0.8013430675828314, 0.8040786191641567, 0.8149443477033133, 0.8189829631024097, 0.8198271602033133, 0.8239672557417168, 0.8346388483621988, 0.8341814523719879, 0.8346285532756024, 0.8450765954442772, 0.837059664439006, 0.8451574854103916, 0.8461546380835843, 0.8446589090737951, 0.8483416086219879, 0.8490740304969879, 0.850243258189006, 0.8524714090737951, 0.8521051981362951, 0.8547804499246988, 0.8551363657756024, 0.8550348856362951, 0.8557673075112951, 0.8560011530496988, 0.8564791392131024, 0.8572218561746988, 0.8591852762612951, 0.8590529108621988, 0.859642672251506, 0.8575571818524097, 0.8573336314006024, 0.8608839655496988, 0.8593882365399097, 0.8590426157756024, 0.8599279932228916, 0.8597750376506024, 0.859764742564006, 0.8596632624246988, 0.8615046121987951, 0.8604162744728916, 0.8607618952371988, 0.8607721903237951, 0.860253023814006, 0.8607618952371988, 0.8591440959149097, 0.8612398814006024, 0.8617384577371988, 0.8621046686746988, 0.8612501764871988, 0.8622370340737951, 0.8609957407756024, 0.860741305064006, 0.8609957407756024, 0.8618502329631024, 0.860253023814006, 0.8610060358621988, 0.8607516001506024, 0.8605074595256024, 0.8613619517131024, 0.8611178110881024, 0.8611178110881024, 0.8603853892131024, 0.8608839655496988, 0.860863375376506, 0.8607516001506024, 0.8614943171121988, 0.8598971079631024, 0.8606398249246988, 0.8608839655496988, 0.8606295298381024, 0.8601412485881024, 0.8607618952371988, 0.8608736704631024, 0.8616163874246988, 0.860619234751506, 0.8608736704631024, 0.860863375376506, 0.860741305064006, 0.860497164439006, 0.860375094126506, 0.8611178110881024, 0.8616060923381024, 0.8612398814006024, 0.8613619517131024, 0.8603853892131024, 0.8598971079631024, 0.8623385142131024, 0.861107516001506, 0.8600191782756024, 0.8611178110881024, 0.861839937876506, 0.861351656626506, 0.8605074595256024, 0.859764742564006, 0.860619234751506, 0.8621046686746988, 0.860985445689006, 0.8611178110881024, 0.8609957407756024, 0.860375094126506, 0.8609957407756024, 0.8606295298381024, 0.8608839655496988, 0.860985445689006, 0.8611178110881024, 0.860863375376506, 0.860985445689006, 0.8608736704631024, 0.8611178110881024, 0.8612501764871988, 0.8608736704631024, 0.860863375376506, 0.8606295298381024, 0.860497164439006, 0.860253023814006, 0.8611178110881024, 0.8606398249246988, 0.861351656626506, 0.8603853892131024, 0.860008883189006, 0.8614943171121988, 0.8606295298381024, 0.860253023814006, 0.8613619517131024, 0.8609957407756024, 0.8613722467996988, 0.860253023814006, 0.8609957407756024, 0.860375094126506, 0.8603853892131024, 0.859886812876506, 0.8609957407756024, 0.860863375376506, 0.8614840220256024, 0.8612398814006024, 0.860619234751506, 0.8613619517131024], "accuracy_test": 0.8621332908163264, "start": "2016-01-23 22:51:36.608000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0], "accuracy_train_last": 0.9679652172157622, "batch_size_eval": 1024, "accuracy_train_std": [0.01899480872074576, 0.014021242045736931, 0.016170915040753638, 0.018955268680261893, 0.017597813641832882, 0.017733771454792255, 0.019536551796836353, 0.01982802496004565, 0.019250185161134234, 0.019948480804186125, 0.0184276967789316, 0.019208236249731944, 0.018343439577518446, 0.017779872368648805, 0.015448573317083445, 0.015671132911288684, 0.015168613867211895, 0.014699761777436784, 0.014477394907502807, 0.013625467283841557, 0.013206746850312364, 0.01276360119209847, 0.012685329366729248, 0.012540351140305576, 0.011808828470020296, 0.011823812566045398, 0.01057409634977297, 0.010802439709194856, 0.010270083253007777, 0.010444929473318204, 0.010148639638508358, 0.008750719289891835, 0.009525363830086103, 0.009555854730065778, 0.00883769244611933, 0.008813756169380342, 0.008649858202170426, 0.00849463559283584, 0.008567108513468353, 0.008463335434831699, 0.008241015720583453, 0.007506840814552392, 0.007600263637020041, 0.007744408432730426, 0.007803138839873425, 0.00737285688608403, 0.007320407013858395, 0.00730503179934618, 0.006959165342526667, 0.006935700534912931, 0.0067349822649883165, 0.0069796924253474, 0.006559348145554468, 0.0066116857999404055, 0.00672992773126974, 0.00671746762342331, 0.006513535412480046, 0.006161256437825428, 0.006547970650809235, 0.006609319619776742, 0.00613170256871264, 0.006453239323796953, 0.0063126132631383, 0.006338816950071867, 0.006382434055197824, 0.005921003514575212, 0.005846666184820847, 0.006158112605153459, 0.005785527189750375, 0.006198538649527247, 0.006078167761002818, 0.006117836523258268, 0.006061854180823541, 0.006028991040025673, 0.005925449367374934, 0.0063411297445106945, 0.005861363216675053, 0.006126742200661111, 0.005969985013205733, 0.005990973266448173, 0.005927363677708749, 0.005776282458715014, 0.005837991327063128, 0.005987124909985095, 0.006088354645961391, 0.006049139119351282, 0.006052888189405583, 0.005904497761589252, 0.0058497753648789905, 0.005869608652326917, 0.006055430102129629, 0.00579369537290317, 0.0061255361334221, 0.005985508742551221, 0.005969518563410688, 0.00625989208440498, 0.006074883799038482, 0.005949135818734563, 0.005868683956645795, 0.00609631987081854, 0.005633444639018329, 0.005940645212863948, 0.005965057557214336, 0.005933244461835796, 0.006043018106366447, 0.005969006167568702, 0.005965200882266011, 0.005975058160406655, 0.005954592548740202, 0.005890290920804282, 0.006014157796256317, 0.005826314750503971, 0.00599525051059925, 0.005891721394834436, 0.006021704088017167, 0.005867230900032467, 0.006087295160930331, 0.006140345688363467, 0.006014813476644348, 0.006210244890473666, 0.005892033499425745, 0.006182068694634758, 0.006086463427724727, 0.005889095139821434, 0.005933014362702644, 0.0059766424435005695, 0.005887511873336678, 0.006179778598179214, 0.00612630954530441, 0.005976374571317849, 0.005857682985121855, 0.005857991681746652, 0.006037901936981471, 0.005992610689109102, 0.005900514505673276, 0.005831791525742035, 0.0062732208563331645, 0.006066386446906149, 0.0062671424934947935, 0.006248290094999814, 0.006131359805936547, 0.0060143128425518225, 0.00597686611230453, 0.005960179306905743, 0.006033005827255655], "accuracy_test_std": 0.006596256684394499, "error_valid": [0.459216749811747, 0.36824936464608427, 0.2920436629329819, 0.2595008942018072, 0.24546280826430722, 0.23330725244728923, 0.21697924510542166, 0.20711066923945776, 0.19865693241716864, 0.19592138083584332, 0.18505565229668675, 0.1810170368975903, 0.18017283979668675, 0.1760327442582832, 0.16536115163780118, 0.16581854762801207, 0.16537144672439763, 0.15492340455572284, 0.16294033556099397, 0.1548425145896084, 0.15384536191641573, 0.15534109092620485, 0.15165839137801207, 0.15092596950301207, 0.14975674181099397, 0.14752859092620485, 0.14789480186370485, 0.14521955007530118, 0.14486363422439763, 0.14496511436370485, 0.14423269248870485, 0.14399884695030118, 0.14352086078689763, 0.14277814382530118, 0.14081472373870485, 0.14094708913780118, 0.14035732774849397, 0.1424428181475903, 0.14266636859939763, 0.13911603445030118, 0.1406117634600903, 0.14095738422439763, 0.1400720067771084, 0.14022496234939763, 0.14023525743599397, 0.14033673757530118, 0.13849538780120485, 0.1395837255271084, 0.13923810476280118, 0.13922780967620485, 0.13974697618599397, 0.13923810476280118, 0.1408559040850903, 0.13876011859939763, 0.13826154226280118, 0.13789533132530118, 0.13874982351280118, 0.13776296592620485, 0.13900425922439763, 0.13925869493599397, 0.13900425922439763, 0.13814976703689763, 0.13974697618599397, 0.13899396413780118, 0.13924839984939763, 0.13949254047439763, 0.13863804828689763, 0.13888218891189763, 0.13888218891189763, 0.13961461078689763, 0.13911603445030118, 0.13913662462349397, 0.13924839984939763, 0.13850568288780118, 0.14010289203689763, 0.13936017507530118, 0.13911603445030118, 0.13937047016189763, 0.13985875141189763, 0.13923810476280118, 0.13912632953689763, 0.13838361257530118, 0.13938076524849397, 0.13912632953689763, 0.13913662462349397, 0.13925869493599397, 0.13950283556099397, 0.13962490587349397, 0.13888218891189763, 0.13839390766189763, 0.13876011859939763, 0.13863804828689763, 0.13961461078689763, 0.14010289203689763, 0.13766148578689763, 0.13889248399849397, 0.13998082172439763, 0.13888218891189763, 0.13816006212349397, 0.13864834337349397, 0.13949254047439763, 0.14023525743599397, 0.13938076524849397, 0.13789533132530118, 0.13901455431099397, 0.13888218891189763, 0.13900425922439763, 0.13962490587349397, 0.13900425922439763, 0.13937047016189763, 0.13911603445030118, 0.13901455431099397, 0.13888218891189763, 0.13913662462349397, 0.13901455431099397, 0.13912632953689763, 0.13888218891189763, 0.13874982351280118, 0.13912632953689763, 0.13913662462349397, 0.13937047016189763, 0.13950283556099397, 0.13974697618599397, 0.13888218891189763, 0.13936017507530118, 0.13864834337349397, 0.13961461078689763, 0.13999111681099397, 0.13850568288780118, 0.13937047016189763, 0.13974697618599397, 0.13863804828689763, 0.13900425922439763, 0.13862775320030118, 0.13974697618599397, 0.13900425922439763, 0.13962490587349397, 0.13961461078689763, 0.14011318712349397, 0.13900425922439763, 0.13913662462349397, 0.13851597797439763, 0.13876011859939763, 0.13938076524849397, 0.13863804828689763], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-3, 3], "momentum": 0.538229267994419, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.0004182902944163993, "patience_threshold": 1, "do_flip": true, "batch_size": 16, "optimization": "adam", "nb_data_augmentation": 2, "learning_rate_decay_method": "exp", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 2.0265151527119214e-06, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.08407692028791108}, "accuracy_valid_max": 0.8623385142131024, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-3, 3)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8613619517131024, "loss_train": [1.62763512134552, 1.1932612657546997, 1.0181118249893188, 0.9050304889678955, 0.8242946267127991, 0.7623927593231201, 0.7099831104278564, 0.6671826839447021, 0.6314148902893066, 0.5973740220069885, 0.568533718585968, 0.5423997044563293, 0.5163834691047668, 0.49485474824905396, 0.47601380944252014, 0.45882847905158997, 0.44152432680130005, 0.42604735493659973, 0.4099051356315613, 0.3984754979610443, 0.38480129837989807, 0.37606480717658997, 0.36492905020713806, 0.35509297251701355, 0.34720519185066223, 0.33765462040901184, 0.33182796835899353, 0.3244633376598358, 0.31831055879592896, 0.3137664496898651, 0.3063359260559082, 0.30264538526535034, 0.2998262345790863, 0.2940517067909241, 0.29055002331733704, 0.2844613492488861, 0.2834981381893158, 0.2787363827228546, 0.27715829014778137, 0.2750924229621887, 0.2748889625072479, 0.27255314588546753, 0.26981064677238464, 0.2663559913635254, 0.26479572057724, 0.2640022039413452, 0.2627620995044708, 0.26173892617225647, 0.2607404589653015, 0.2599896788597107, 0.2575419843196869, 0.25670549273490906, 0.25905686616897583, 0.254886269569397, 0.25530195236206055, 0.25565823912620544, 0.2545822858810425, 0.25356847047805786, 0.2532033324241638, 0.25048351287841797, 0.2493538111448288, 0.2503335773944855, 0.25129660964012146, 0.2499106377363205, 0.25216132402420044, 0.25092869997024536, 0.25038811564445496, 0.2507639527320862, 0.24840477108955383, 0.24929367005825043, 0.2506054937839508, 0.2499060034751892, 0.2501847743988037, 0.249642476439476, 0.24973109364509583, 0.24907976388931274, 0.24805761873722076, 0.24680554866790771, 0.24905116856098175, 0.24641172587871552, 0.24895641207695007, 0.2483464628458023, 0.24793940782546997, 0.2466275542974472, 0.24660040438175201, 0.24801930785179138, 0.24701687693595886, 0.24869969487190247, 0.24790062010288239, 0.2476072758436203, 0.24758583307266235, 0.24714399874210358, 0.24713587760925293, 0.24736900627613068, 0.24760974943637848, 0.24675707519054413, 0.24632355570793152, 0.24883268773555756, 0.24766094982624054, 0.24506063759326935, 0.2489529848098755, 0.24755431711673737, 0.24636201560497284, 0.24564869701862335, 0.24689361453056335, 0.24558112025260925, 0.24652768671512604, 0.2473180890083313, 0.2457932084798813, 0.2463376373052597, 0.2460283488035202, 0.24864675104618073, 0.24630972743034363, 0.2467396855354309, 0.24712392687797546, 0.2467595934867859, 0.24764133989810944, 0.24703341722488403, 0.2481699138879776, 0.24797795712947845, 0.24646411836147308, 0.24652938544750214, 0.24754896759986877, 0.24758146703243256, 0.2462926208972931, 0.24669252336025238, 0.2468491494655609, 0.24534235894680023, 0.24649791419506073, 0.24627016484737396, 0.24754272401332855, 0.24625436961650848, 0.24689389765262604, 0.2476675659418106, 0.24646419286727905, 0.24822595715522766, 0.24763323366641998, 0.24577084183692932, 0.24579010903835297, 0.24807503819465637, 0.2457643449306488, 0.24639785289764404, 0.2473871409893036, 0.24523594975471497, 0.2457718700170517], "accuracy_train_first": 0.5499519469246031, "model": "residualv3", "loss_std": [0.3590163290500641, 0.2548168897628784, 0.2479826807975769, 0.24521131813526154, 0.2404922991991043, 0.23630289733409882, 0.2317059487104416, 0.2255723923444748, 0.2224699854850769, 0.21872171759605408, 0.21368403732776642, 0.20980717241764069, 0.20142273604869843, 0.1965704709291458, 0.19202619791030884, 0.19053928554058075, 0.1834186613559723, 0.17882123589515686, 0.1729651391506195, 0.16856041550636292, 0.164876788854599, 0.16124621033668518, 0.15887558460235596, 0.15607410669326782, 0.15301749110221863, 0.14753669500350952, 0.148383229970932, 0.1439974457025528, 0.14076754450798035, 0.14044460654258728, 0.13647645711898804, 0.13610073924064636, 0.13635669648647308, 0.13317643105983734, 0.13146770000457764, 0.12835901975631714, 0.12920507788658142, 0.12778180837631226, 0.12435003370046616, 0.12510448694229126, 0.12364929914474487, 0.1231188178062439, 0.12201571464538574, 0.11952626705169678, 0.12077260762453079, 0.11959642171859741, 0.11671121418476105, 0.11788729578256607, 0.11803508549928665, 0.1183008924126625, 0.11759411543607712, 0.11528245359659195, 0.1164180189371109, 0.11666359752416611, 0.11592257767915726, 0.11440302431583405, 0.11506590247154236, 0.11426763981580734, 0.11344897747039795, 0.11286468058824539, 0.11265657097101212, 0.11295274645090103, 0.11296084523200989, 0.1140773743391037, 0.11297512799501419, 0.11190035194158554, 0.11337129771709442, 0.11370757967233658, 0.11217639595270157, 0.11321595311164856, 0.11408260464668274, 0.11321543902158737, 0.11291294544935226, 0.1123354509472847, 0.11236502230167389, 0.11176372319459915, 0.11160111427307129, 0.10961160808801651, 0.11345185339450836, 0.10950226336717606, 0.11351341009140015, 0.11048538237810135, 0.11122149229049683, 0.11198122799396515, 0.11135605722665787, 0.11056098341941833, 0.11030145734548569, 0.11147243529558182, 0.11094100773334503, 0.11024323105812073, 0.11151185631752014, 0.11042191833257675, 0.10982437431812286, 0.11107347160577774, 0.10982099175453186, 0.11141006648540497, 0.11129877716302872, 0.11253408342599869, 0.11027169972658157, 0.1096472442150116, 0.11154905706644058, 0.11237163841724396, 0.11021992564201355, 0.11047075688838959, 0.11188001185655594, 0.10919558256864548, 0.1110023707151413, 0.11236468702554703, 0.11148456484079361, 0.11022619903087616, 0.10926199704408646, 0.11355170607566833, 0.10873634368181229, 0.11136893182992935, 0.11174454540014267, 0.11234395205974579, 0.11322125792503357, 0.11146191507577896, 0.11187059432268143, 0.11219542473554611, 0.11192306131124496, 0.11176221817731857, 0.11062468588352203, 0.11172103881835938, 0.10968112200498581, 0.1097489520907402, 0.11009950190782547, 0.11073783785104752, 0.11133752763271332, 0.11249321699142456, 0.11230973154306412, 0.11013472825288773, 0.11115841567516327, 0.11227767169475555, 0.11214675009250641, 0.11374014616012573, 0.11346033960580826, 0.11064299941062927, 0.10903607308864594, 0.11209811270236969, 0.10925019532442093, 0.11210057139396667, 0.11213492602109909, 0.11141140758991241, 0.11120180785655975]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:12 2016", "state": "available"}], "summary": "71ec1329ad0816d35476c4fde8af0463"}