{"content": {"hp_model": {"f0": 16, "f1": 16, "f2": 64, "f3": 32, "nonlin": "rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.767856240272522, 1.500647783279419, 1.3634008169174194, 1.263371467590332, 1.1849197149276733, 1.1174228191375732, 1.0575988292694092, 1.0064939260482788, 0.9585381746292114, 0.9162760972976685, 0.8797330856323242, 0.8464217782020569, 0.8140091300010681, 0.786148726940155, 0.7599462270736694, 0.7385357022285461, 0.7163505554199219, 0.6956016421318054, 0.6778666377067566, 0.6578667163848877, 0.6425669193267822, 0.6258981227874756, 0.6115795373916626, 0.5949980616569519, 0.5815078020095825, 0.5683770775794983, 0.5561622977256775, 0.5432507395744324, 0.5316299796104431, 0.5194078683853149, 0.5104656219482422, 0.4977870285511017, 0.4873654842376709, 0.4781370460987091, 0.46823304891586304, 0.4590841233730316, 0.4503127336502075, 0.44124504923820496, 0.43082553148269653, 0.42449524998664856, 0.41540756821632385, 0.40814921259880066, 0.4015456438064575, 0.39143210649490356, 0.38499510288238525, 0.3778446912765503, 0.3702101707458496, 0.36427998542785645, 0.3564048409461975, 0.35155436396598816, 0.343364953994751, 0.33806225657463074, 0.33213213086128235, 0.3268950879573822, 0.32196131348609924, 0.31412726640701294, 0.3085244596004486, 0.30422937870025635, 0.2997642755508423, 0.2936950623989105, 0.2900886535644531, 0.284384161233902, 0.28055283427238464, 0.2745066285133362, 0.270399808883667, 0.26626208424568176, 0.26354554295539856, 0.25499290227890015, 0.25355449318885803, 0.2492530643939972, 0.2477291226387024, 0.24375149607658386, 0.23721636831760406, 0.23661157488822937, 0.23340077698230743, 0.2290283590555191, 0.22726525366306305, 0.22419007122516632, 0.22144964337348938, 0.21683911979198456, 0.21549080312252045, 0.21098355948925018, 0.20857861638069153, 0.20579221844673157, 0.2032407820224762, 0.20095553994178772, 0.19951875507831573, 0.19496946036815643, 0.19424571096897125, 0.1907133013010025, 0.18657927215099335, 0.1877257525920868, 0.18397779762744904, 0.1675914227962494, 0.1521630436182022, 0.14775097370147705, 0.14573374390602112, 0.14383910596370697, 0.14196625351905823, 0.14056871831417084, 0.1395910233259201, 0.13885070383548737, 0.13922958076000214, 0.13699790835380554, 0.13737520575523376, 0.13542313873767853, 0.1347488909959793, 0.13411268591880798, 0.1330375373363495, 0.13340985774993896, 0.13217687606811523, 0.1312081217765808, 0.13011430203914642, 0.1305379718542099, 0.13041552901268005, 0.13103561103343964, 0.13136819005012512, 0.13037246465682983, 0.1308324933052063, 0.13105279207229614, 0.13005581498146057, 0.1315164715051651, 0.13040776550769806, 0.12974289059638977, 0.13026554882526398, 0.12969675660133362, 0.1303882747888565, 0.1307820826768875, 0.1311025470495224, 0.129476398229599, 0.13045427203178406, 0.13032940030097961, 0.13114696741104126, 0.12970806658267975, 0.13015182316303253, 0.1304224580526352, 0.13030412793159485, 0.13067422807216644, 0.13095104694366455, 0.13038897514343262, 0.12984006106853485, 0.13105390965938568, 0.1305895745754242, 0.12960270047187805, 0.13163994252681732, 0.13080543279647827, 0.1311083883047104, 0.12967832386493683, 0.1304423063993454, 0.13017025589942932, 0.13063892722129822, 0.1305903047323227, 0.13067062199115753, 0.13044051826000214, 0.1307508498430252, 0.13101258873939514, 0.13024219870567322, 0.1310436725616455, 0.13140936195850372, 0.13093113899230957, 0.13027451932430267, 0.13034003973007202, 0.13092100620269775, 0.13152791559696198, 0.13092079758644104, 0.13086986541748047, 0.13017882406711578, 0.1304578185081482, 0.13047248125076294, 0.12992489337921143, 0.13107998669147491, 0.13016045093536377, 0.1305261105298996, 0.13032224774360657, 0.13090690970420837, 0.13082818686962128, 0.13018234074115753, 0.13087716698646545, 0.13141153752803802, 0.13113415241241455, 0.13108940422534943, 0.13078147172927856, 0.13091303408145905, 0.13164225220680237, 0.13084128499031067, 0.13117453455924988, 0.1299239993095398, 0.12991319596767426, 0.13062521815299988, 0.12990808486938477, 0.12960177659988403, 0.13097631931304932, 0.1299246996641159, 0.13026846945285797, 0.13024061918258667, 0.1299004703760147, 0.13130101561546326, 0.13071298599243164, 0.13064494729042053, 0.13080717623233795, 0.13022278249263763, 0.130759134888649, 0.13070076704025269], "moving_avg_accuracy_train": [0.04566823529411764, 0.09168729411764705, 0.13515621176470588, 0.17704529647058823, 0.21913606094117644, 0.26201774896470587, 0.3015477387741176, 0.34027767077905874, 0.3757628448776234, 0.41189714862515514, 0.4450180219979337, 0.47581269038637564, 0.5050431860536204, 0.5329529850953172, 0.5577047454093149, 0.5809436826330894, 0.6036093143697804, 0.6244554417563317, 0.6442898975806985, 0.6622256137049816, 0.6781842288050717, 0.6940034529833881, 0.7094642841556374, 0.7232025616224266, 0.7366493642837133, 0.7473726631494596, 0.7585742203639254, 0.7684509159745917, 0.7781752361418385, 0.7871741831158899, 0.7961179412748892, 0.8033390883238709, 0.8101134147856014, 0.816885602718806, 0.8235264542116313, 0.8293526323198799, 0.8348597220290683, 0.8394843380614556, 0.8444817866082512, 0.8493677255944849, 0.8536827177409189, 0.858309740084474, 0.8629752366642619, 0.8676565365272475, 0.8714414711098168, 0.8750667357635411, 0.8781224151283634, 0.88135487949788, 0.8837276268422097, 0.8864748641579887, 0.8902203189186604, 0.8919347576150297, 0.8947248112652915, 0.8976288007269976, 0.8999647441837096, 0.9019988580006327, 0.9041354427888048, 0.9067971926275713, 0.9096186498354024, 0.912155608381274, 0.9137518122490289, 0.9157483957300084, 0.9179523796864193, 0.9190418476001303, 0.9206482510754114, 0.9224093083208115, 0.9238695539593185, 0.9257531867986808, 0.927123750471754, 0.9291972577775198, 0.9305269437644736, 0.9318413082115556, 0.9334430597433413, 0.935122283180772, 0.9364594666274006, 0.9355970493764253, 0.9368444032623121, 0.9378399629360809, 0.939272437230708, 0.9394510758605783, 0.9404377329804028, 0.939351606741186, 0.9409835048905968, 0.942094566166243, 0.943381580137854, 0.9449351868299509, 0.9463357857940147, 0.9475092660381426, 0.9486524570813871, 0.9499213290203072, 0.9513880196476883, 0.9520962765064489, 0.9531807665028628, 0.9566038663231647, 0.9599011267496718, 0.9628757199570576, 0.9655810891378225, 0.9681265096358049, 0.9704079763192832, 0.9725201198638255, 0.9744116372892075, 0.9761398853249926, 0.9777517791454345, 0.979226012995597, 0.9805245881666256, 0.9817168352323159, 0.982808681120849, 0.9838313424205287, 0.9847658552372994, 0.9855457403018048, 0.9862688133304478, 0.9870042849385795, 0.9876615035035451, 0.9882412355061317, 0.9887771119555185, 0.989266459583496, 0.9897139312722053, 0.9900860675567495, 0.990449225506957, 0.9907643029562614, 0.9910455197194589, 0.99129626186516, 0.9915384003845262, 0.9917445603460736, 0.9919253984291132, 0.9921140350567901, 0.9922743962569933, 0.9923951919254116, 0.9925062609681645, 0.9926085760478186, 0.9926983066783308, 0.992764946598733, 0.992829628409448, 0.9929043126273268, 0.9929809401881236, 0.9930310814634289, 0.9930762086112036, 0.9931097642206714, 0.9931634936809572, 0.9932071443128615, 0.9932417239992224, 0.9932822574816531, 0.9933234434981937, 0.993339334442492, 0.9933606951158898, 0.9933681550160656, 0.9933795748085766, 0.9933851467394837, 0.9934066320655354, 0.9934259688589818, 0.9934527837377894, 0.993460446540481, 0.993474401886433, 0.9934869616977897, 0.9934723831750696, 0.9934686742693274, 0.9934841597835711, 0.9935192732169786, 0.9935155811893984, 0.9935263760116351, 0.9935384442928245, 0.9935469528047185, 0.9935263751713055, 0.9935384435365279, 0.9935281285946398, 0.9935188451469406, 0.9935246076910701, 0.9935439116278455, 0.9935471675238845, 0.9935265684185549, 0.9935033233414053, 0.9935059321837354, 0.9935176919065384, 0.9935047462452963, 0.9935095657384136, 0.9935256679881017, 0.993540160012821, 0.9935484969527153, 0.9935371766692085, 0.9935481648846406, 0.9935627601608824, 0.9935570723800883, 0.9935660710244324, 0.9935929933337538, 0.9935960469415549, 0.9936105598944582, 0.9936047980226594, 0.9936043182203935, 0.9936062393395306, 0.9936126742291069, 0.9936278773944316, 0.9936368543608707, 0.9936072865718424, 0.9936065579146581, 0.9935988432996629, 0.9935848413226378, 0.993586357190374, 0.9935924273536896, 0.9936025963830265, 0.9936023367447239, 0.9935997501290751, 0.9935644809985205, 0.9935633270163156], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.04667999999999999, 0.09309199999999998, 0.13610279999999997, 0.17713251999999996, 0.21849926799999997, 0.2608626745333333, 0.29941640707999995, 0.33736809970533327, 0.37193795640146665, 0.40655749409465336, 0.4383284113518547, 0.4676955702166692, 0.4958060131950023, 0.5218520785421688, 0.5453602040212852, 0.5670375169524899, 0.587493765257241, 0.6063177220648501, 0.6241526165250318, 0.6406040215391953, 0.6551036193852757, 0.6688465907800815, 0.68210859836874, 0.694017738531866, 0.7050826313453461, 0.7145743682108114, 0.7241969313897303, 0.7323772382507573, 0.7400995144256814, 0.7470895629831132, 0.7537006066848019, 0.7594372126829884, 0.7647334914146895, 0.7698201422732205, 0.7744381280458985, 0.7786609819079754, 0.7823682170505112, 0.78501139534546, 0.787830255810914, 0.7915138968964893, 0.7940958405401737, 0.796992923152823, 0.7998402975042074, 0.8028962677537866, 0.8050333076450746, 0.8070099768805672, 0.8086289791925104, 0.8105794146065927, 0.8122414731459334, 0.8139106591646733, 0.815879593248206, 0.8167983005900521, 0.8180118038643802, 0.8194639568112755, 0.8201042277968147, 0.8214938050171332, 0.8227177578487532, 0.8240726487305445, 0.8259187171908234, 0.8274468454717411, 0.8283554942579003, 0.8293599448321104, 0.8306639503488994, 0.8310642219806762, 0.8315577997826086, 0.8321620198043477, 0.8331858178239129, 0.8340939027081883, 0.8348178457707027, 0.8358827278602992, 0.8366811217409359, 0.837839676233509, 0.8390290419434915, 0.8400594710824757, 0.8406401906408947, 0.8396161715768052, 0.8404545544191248, 0.8409824323105456, 0.8416708557461577, 0.8415304368382087, 0.8421240598210545, 0.8409383205056158, 0.8418844884550543, 0.8425627062762155, 0.843026435648594, 0.8439637920837346, 0.8449407462086945, 0.8459000049211584, 0.8465500044290425, 0.8469216706528049, 0.8476961702541911, 0.848206553228772, 0.8487725645725614, 0.851401974781972, 0.853715110637108, 0.8558502662400639, 0.8574785729493908, 0.8590907156544517, 0.8603949774223398, 0.8616888130134391, 0.8628532650454285, 0.8639146052075524, 0.8649764780201304, 0.8658121635514506, 0.8666309471963056, 0.8673411858100084, 0.8681537338956743, 0.8686850271727735, 0.8691098577888295, 0.8696655386766133, 0.870018984808952, 0.8705504196613901, 0.8710020443619177, 0.871501839925726, 0.8718983225998201, 0.8722551570065048, 0.8726163079725209, 0.8729946771752688, 0.8732285427910753, 0.8734256885119678, 0.8736564529941043, 0.8739708076946939, 0.8741870602585579, 0.8744216875660354, 0.8746461854760986, 0.8747682335951554, 0.8749580769023065, 0.8751289358787425, 0.8752293756242016, 0.8753864380617814, 0.8754611275889366, 0.8755816814967096, 0.8756101800137053, 0.8757691620123348, 0.8758322458111013, 0.8758756878966578, 0.8759547857736587, 0.8758926405296262, 0.8759033764766636, 0.8758997054956639, 0.8759630682794308, 0.8759934281181544, 0.8760607519730056, 0.876054676775705, 0.8759958757648012, 0.8759962881883211, 0.875956659369489, 0.8759743267658734, 0.875976894089286, 0.8759658713470242, 0.8759692842123218, 0.8759723557910897, 0.8761084535453141, 0.8760976081907828, 0.8760878473717045, 0.8760790626345341, 0.8761378230377473, 0.8762307074006392, 0.876274303327242, 0.8761802063278512, 0.8761488523617327, 0.8761606337922261, 0.8761979037463369, 0.8762181133717032, 0.8763296353678662, 0.8763233384977462, 0.8763310046479715, 0.8762979041831744, 0.8763747804315236, 0.8763106357217045, 0.8762662388162008, 0.8762662816012474, 0.8763196534411227, 0.8762743547636771, 0.8763002526206428, 0.8762702273585785, 0.8763498712893872, 0.8764348841604486, 0.8764447290777371, 0.8764535895032968, 0.8764348972196337, 0.8765247408310037, 0.8764322667479033, 0.8764023734064463, 0.876455469399135, 0.8764632557925549, 0.8764569302132994, 0.8763845705253027, 0.8764394468061057, 0.8764488354588286, 0.876497285246279, 0.8764342233883178, 0.8764441343828194, 0.8764930542778708, 0.8765504155167504, 0.8765487072984087, 0.8765071699019011, 0.8763631195783776, 0.8763668076205399, 0.8764367935251525, 0.8763131141726371, 0.8763351360887068, 0.8762749558131695], "moving_var_accuracy_train": [0.018770289433910035, 0.03595304446555017, 0.0493636612316562, 0.060219553865959854, 0.07014229056283067, 0.07967761401627337, 0.08577343346363582, 0.09069615881523854, 0.09295932116096371, 0.09541458021073734, 0.09574605246644433, 0.09470625163018684, 0.0929254233597435, 0.09064349296670024, 0.08709299041780454, 0.08324412520563881, 0.07954329044328462, 0.07550001064210311, 0.07149066031853164, 0.06723680350269629, 0.06280521971564207, 0.05877692842651232, 0.055050571288692285, 0.05124417656961345, 0.0477471074289564, 0.0440072989331378, 0.040735842996084584, 0.03754020074214792, 0.03463724229236922, 0.031902347482908405, 0.029432030024677553, 0.02695813170453695, 0.02467534202517418, 0.02262057058728054, 0.02075542170550032, 0.01898537869709161, 0.017359793160967897, 0.01581629750589423, 0.014459438183105633, 0.013228345962789856, 0.012073083781524944, 0.01105845942528228, 0.010148515207778163, 0.009330894802665047, 0.008526736890547507, 0.0077923460957786345, 0.007097146073626182, 0.006481470899365313, 0.005883993179068992, 0.005363519676984971, 0.00495342359156462, 0.004484534932800634, 0.00410614103386262, 0.003771425323619661, 0.0034433924777542957, 0.0031362918011606455, 0.0028637475720580136, 0.0026411370246897776, 0.0024486689092013984, 0.002261727446252494, 0.0020584855027141677, 0.0018885140628114314, 0.0017433805640513392, 0.0015797249706612575, 0.0014449772627236884, 0.0013283914400455035, 0.0012147431519639639, 0.001125201490829283, 0.001029587344783883, 0.0009653235032290711, 0.0008847037363212782, 0.0008117813477869301, 0.000753693684734435, 0.0007037024384363368, 0.0006494247307221431, 0.0005911761292829477, 0.000546061541804387, 0.0005003756392002598, 0.0004688059187231434, 0.00042221253269156674, 0.0003887527098713138, 0.0003604944707518184, 0.0003484128478070906, 0.0003246816774505468, 0.00030712115437358926, 0.00029813228271978633, 0.0002859741515710351, 0.0002697702393641589, 0.0002545551872799343, 0.00024358999252835085, 0.00023859162584354227, 0.00021924711326102065, 0.0002079074689058156, 0.0002925752334329955, 0.0003611650469715813, 0.000404682385019253, 0.0004300853481554168, 0.0004453893029438183, 0.00044769618469983093, 0.000443076919404612, 0.0004309697709988702, 0.00041475436535773965, 0.0003966627440173751, 0.0003765567586203203, 0.00035407776003159436, 0.0003314630616192611, 0.0003090459024560951, 0.0002875538374152507, 0.00026665828151610336, 0.00024546643978903967, 0.00022562530725289502, 0.00020793104290491582, 0.00019102536479364296, 0.00017494763106768696, 0.0001600373400819852, 0.00014618875598285206, 0.00013337195859433338, 0.00012128113146336913, 0.00011033997158822172, 0.00010019943862094047, 9.089124056997534e-05, 8.236796112565529e-05, 7.465884457613815e-05, 6.75754774862313e-05, 6.111225144810547e-05, 5.532128029900681e-05, 5.002059369988205e-05, 4.5149858671471496e-05, 4.074589979464687e-05, 3.676552519490388e-05, 3.316143694988263e-05, 2.9885261165815285e-05, 2.6934388678970014e-05, 2.4291149402674424e-05, 2.1914880510069886e-05, 1.974601978646607e-05, 1.778974594301603e-05, 1.6020905159055272e-05, 1.4444796337273175e-05, 1.3017465102536645e-05, 1.1726480384662353e-05, 1.056861901497776e-05, 9.527023705106307e-06, 8.576594033591896e-06, 7.7230411355448e-06, 6.951237872986006e-06, 6.2572877906363735e-06, 5.631838429299032e-06, 5.072809159489033e-06, 4.568893447767252e-06, 4.118475442519774e-06, 3.707156365173617e-06, 3.33819349378199e-06, 3.005793884155635e-06, 2.7071272956623873e-06, 2.4365383699323927e-06, 2.1950427433016893e-06, 1.986635047822549e-06, 1.7880942226491716e-06, 1.6103335540683374e-06, 1.4506109893593102e-06, 1.3062014433952357e-06, 1.1793922500276386e-06, 1.0627638339771448e-06, 9.574450328148203e-07, 8.624761711440045e-07, 7.765274162632007e-07, 7.022284524121158e-07, 6.321010149020539e-07, 5.727098216752769e-07, 5.203018420129737e-07, 4.683329123364041e-07, 4.2274424082637854e-07, 3.819781280486901e-07, 3.4398936286900256e-07, 3.1192396858724683e-07, 2.8262174075268866e-07, 2.549851077786414e-07, 2.3063993636885075e-07, 2.0866261063740474e-07, 1.8971354837084093e-07, 1.710333511870153e-07, 1.5465879646860076e-07, 1.4571621347454673e-07, 1.3122851281251772e-07, 1.2000129374903544e-07, 1.0829995687375977e-07, 9.747203307831341e-08, 8.775804605913303e-08, 7.935491168795786e-08, 7.34996466421471e-08, 6.687495531598625e-08, 6.805574711655244e-08, 6.125495087652641e-08, 5.5665093349594316e-08, 5.186308226014184e-08, 4.669745472907118e-08, 4.235933120026009e-08, 3.9054080499131455e-08, 3.514927915765213e-08, 3.1694566466521396e-08, 3.9720313950546036e-08, 3.576026762985627e-08], "duration": 174980.966843, "accuracy_train": [0.4566823529411765, 0.5058588235294118, 0.5263764705882353, 0.5540470588235294, 0.5979529411764706, 0.6479529411764706, 0.6573176470588236, 0.6888470588235294, 0.6951294117647059, 0.7371058823529412, 0.7431058823529412, 0.752964705882353, 0.7681176470588236, 0.7841411764705882, 0.7804705882352941, 0.7900941176470588, 0.8076, 0.8120705882352941, 0.8228, 0.8236470588235294, 0.8218117647058824, 0.8363764705882353, 0.8486117647058824, 0.8468470588235294, 0.8576705882352941, 0.8438823529411764, 0.8593882352941177, 0.8573411764705883, 0.8656941176470588, 0.868164705882353, 0.8766117647058823, 0.8683294117647059, 0.8710823529411764, 0.877835294117647, 0.8832941176470588, 0.8817882352941177, 0.8844235294117647, 0.8811058823529412, 0.8894588235294117, 0.8933411764705882, 0.8925176470588235, 0.8999529411764706, 0.9049647058823529, 0.9097882352941177, 0.9055058823529412, 0.9076941176470589, 0.9056235294117647, 0.9104470588235294, 0.9050823529411764, 0.9112, 0.9239294117647059, 0.907364705882353, 0.9198352941176471, 0.9237647058823529, 0.9209882352941177, 0.9203058823529412, 0.923364705882353, 0.9307529411764706, 0.9350117647058823, 0.9349882352941177, 0.9281176470588235, 0.9337176470588235, 0.9377882352941177, 0.9288470588235294, 0.9351058823529412, 0.9382588235294118, 0.9370117647058823, 0.9427058823529412, 0.9394588235294118, 0.9478588235294118, 0.9424941176470588, 0.9436705882352941, 0.9478588235294118, 0.9502352941176471, 0.9484941176470588, 0.9278352941176471, 0.9480705882352941, 0.9468, 0.9521647058823529, 0.9410588235294117, 0.9493176470588235, 0.9295764705882353, 0.9556705882352942, 0.9520941176470589, 0.9549647058823529, 0.9589176470588235, 0.9589411764705882, 0.9580705882352941, 0.9589411764705882, 0.9613411764705883, 0.9645882352941176, 0.9584705882352941, 0.9629411764705882, 0.9874117647058823, 0.9895764705882353, 0.9896470588235294, 0.9899294117647058, 0.991035294117647, 0.9909411764705882, 0.9915294117647059, 0.9914352941176471, 0.9916941176470588, 0.9922588235294117, 0.9924941176470589, 0.9922117647058823, 0.9924470588235295, 0.9926352941176471, 0.993035294117647, 0.9931764705882353, 0.9925647058823529, 0.9927764705882353, 0.9936235294117647, 0.9935764705882353, 0.9934588235294117, 0.9936, 0.9936705882352941, 0.9937411764705882, 0.9934352941176471, 0.9937176470588235, 0.9936, 0.9935764705882353, 0.9935529411764706, 0.9937176470588235, 0.9936, 0.9935529411764706, 0.9938117647058824, 0.9937176470588235, 0.9934823529411765, 0.9935058823529411, 0.9935294117647059, 0.9935058823529411, 0.9933647058823529, 0.9934117647058823, 0.9935764705882353, 0.9936705882352941, 0.9934823529411765, 0.9934823529411765, 0.9934117647058823, 0.9936470588235294, 0.9936, 0.9935529411764706, 0.9936470588235294, 0.9936941176470588, 0.9934823529411765, 0.9935529411764706, 0.9934352941176471, 0.9934823529411765, 0.9934352941176471, 0.9936, 0.9936, 0.9936941176470588, 0.9935294117647059, 0.9936, 0.9936, 0.9933411764705883, 0.9934352941176471, 0.9936235294117647, 0.993835294117647, 0.9934823529411765, 0.9936235294117647, 0.9936470588235294, 0.9936235294117647, 0.9933411764705883, 0.9936470588235294, 0.9934352941176471, 0.9934352941176471, 0.9935764705882353, 0.9937176470588235, 0.9935764705882353, 0.9933411764705883, 0.9932941176470588, 0.9935294117647059, 0.9936235294117647, 0.9933882352941177, 0.9935529411764706, 0.9936705882352941, 0.9936705882352941, 0.9936235294117647, 0.9934352941176471, 0.9936470588235294, 0.9936941176470588, 0.9935058823529411, 0.9936470588235294, 0.993835294117647, 0.9936235294117647, 0.9937411764705882, 0.9935529411764706, 0.9936, 0.9936235294117647, 0.9936705882352941, 0.993764705882353, 0.9937176470588235, 0.9933411764705883, 0.9936, 0.9935294117647059, 0.9934588235294117, 0.9936, 0.9936470588235294, 0.9936941176470588, 0.9936, 0.9935764705882353, 0.9932470588235294, 0.9935529411764706], "end": "2016-02-08 00:02:37.253000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0], "moving_var_accuracy_valid": [0.019611201599999996, 0.037036745136, 0.04998243087216, 0.060135129094449596, 0.06952248674586417, 0.07872216198925398, 0.08422745842984773, 0.08876769134501299, 0.09064659713843246, 0.09236854893539896, 0.09221621469213445, 0.09075646340104229, 0.0887925901008811, 0.08601890877141276, 0.08239070556614844, 0.07838078807279002, 0.07430883211786167, 0.07006702105512815, 0.0659230700932677, 0.061766601626401324, 0.05748208650304375, 0.053433701217565424, 0.049673258703343603, 0.04598238140783407, 0.04248602994381489, 0.03904826456794251, 0.03597678161033883, 0.032981360232370034, 0.03021992615302926, 0.027637680547243634, 0.02526726558195001, 0.02303671685916087, 0.020985500288879612, 0.019119816412601007, 0.01739976690471081, 0.015820282666903843, 0.01436194673183193, 0.012988629582138736, 0.011761280392838159, 0.010707275258380385, 0.009696545629354807, 0.00880242885539996, 0.007995153836132259, 0.007279689040015854, 0.006592822591486875, 0.005968705323737071, 0.005395425307738064, 0.004890120561704815, 0.004425970452828093, 0.004008449045231698, 0.0036424944535361978, 0.003285841216802235, 0.002970510406893257, 0.002692438099834522, 0.002426883812265379, 0.0022015737546998947, 0.001994898924036181, 0.0018119305953466151, 0.0016614092546522814, 0.0015162849135735164, 0.0013720872057654625, 0.001243958773793193, 0.0011348667699042183, 0.0010228220493266433, 0.0009227324158130225, 0.0008337449107437544, 0.0007598038811331703, 0.0006912450564332982, 0.0006268373928098345, 0.0005743594183115397, 0.0005226603715781296, 0.0004824745710306679, 0.00044695843105633924, 0.0004118186458449146, 0.00037367189811019744, 0.0003457422436917457, 0.00031749399143523227, 0.00028825248790596744, 0.00026369258055567043, 0.00023750077972748999, 0.00021692219596660566, 0.00020788377588754024, 0.00019515250239568682, 0.00017977706687258443, 0.0001637347645625845, 0.00015526902188482154, 0.00014833207395682435, 0.00014178046205808278, 0.00013140491009452148, 0.00011950764112204164, 0.00011295552370276363, 0.0001040043883591656, 9.648726909493426e-05, 0.00014906272462961127, 0.00018231182952549793, 0.00020511065161245247, 0.00020846203110795955, 0.000211006864910494, 0.00020521606725201739, 0.00019976055535797426, 0.00019198803663541544, 0.00018292721942950686, 0.00017478266231738898, 0.00016358972885097278, 0.00015326441587961285, 0.0001424779242872015, 0.000134172241382154, 0.00012329547016055618, 0.00011259025261554708, 0.00010411025859542522, 9.482355025206914e-05, 8.788300224833566e-05, 8.093038585464234e-05, 7.508550771959951e-05, 6.899174354535051e-05, 6.323854633496133e-05, 5.808856188375507e-05, 5.356817497767235e-05, 4.870359561621387e-05, 4.4183033971988376e-05, 4.024400079073133e-05, 3.710897061170301e-05, 3.3818960092932254e-05, 3.093251384436652e-05, 2.829285626453426e-05, 2.5597632328368573e-05, 2.3362233426962415e-05, 2.128874519272505e-05, 1.9250663955663514e-05, 1.754761504378387e-05, 1.584306026860548e-05, 1.4389553443859032e-05, 1.2957907588711723e-05, 1.1889594312834532e-05, 1.0736450972552408e-05, 9.679790808474677e-06, 8.76811999494164e-06, 7.926066277650257e-06, 7.134496994914336e-06, 6.421168580336403e-06, 5.81518530360304e-06, 5.2419622515086335e-06, 4.758558539246039e-06, 4.283034857521601e-06, 3.885849401719266e-06, 3.497265992385777e-06, 3.1616733826854487e-06, 2.8483152764719467e-06, 2.563543069170299e-06, 2.3082822698760277e-06, 2.0775588717342795e-06, 1.8698878959259937e-06, 1.849602494677623e-06, 1.6657008406440689e-06, 1.499988218881372e-06, 1.35068394145762e-06, 1.2466906121839261e-06, 1.1996690947940972e-06, 1.096807628661856e-06, 1.066815073444966e-06, 9.689812068226807e-07, 8.73332305080649e-07, 7.985005198873251e-07, 7.223263285156111e-07, 7.6202809631769e-07, 6.861821418456896e-07, 6.180928563946229e-07, 5.661443376832508e-07, 5.627195219571912e-07, 5.434784639413709e-07, 5.068703845120445e-07, 4.5618336253588203e-07, 4.362020059073253e-07, 4.110495369214799e-07, 3.7598087418803915e-07, 3.4649643402749365e-07, 3.6893519205686486e-07, 3.97086367065901e-07, 3.5825003192706414e-07, 3.231315930042371e-07, 2.939630469206601e-07, 3.37213612764551e-07, 3.8045535589537623e-07, 3.504523270770219e-07, 3.4077975432570933e-07, 3.072474301955345e-07, 2.7688280375223893e-07, 2.9631784339970194e-07, 2.937887148127225e-07, 2.6520316453098765e-07, 2.598092852138585e-07, 2.6961953805817715e-07, 2.4354163456044826e-07, 2.407258762909552e-07, 2.462660941940662e-07, 2.2166574686378596e-07, 2.1502736995502726e-07, 3.802790943244085e-07, 3.4237359978688127e-07, 3.5221848140829414e-07, 4.54665873414775e-07, 4.135639691596945e-07, 4.0480256231748484e-07], "accuracy_test": 0.8658, "start": "2016-02-05 23:26:16.287000", "learning_rate_per_epoch": [0.0041618444956839085, 0.0041618444956839085, 0.0041618444956839085, 0.0041618444956839085, 0.0041618444956839085, 0.0041618444956839085, 0.0041618444956839085, 0.0041618444956839085, 0.0041618444956839085, 0.0041618444956839085, 0.0041618444956839085, 0.0041618444956839085, 0.0041618444956839085, 0.0041618444956839085, 0.0041618444956839085, 0.0041618444956839085, 0.0041618444956839085, 0.0041618444956839085, 0.0041618444956839085, 0.0041618444956839085, 0.0041618444956839085, 0.0041618444956839085, 0.0041618444956839085, 0.0041618444956839085, 0.0041618444956839085, 0.0041618444956839085, 0.0041618444956839085, 0.0041618444956839085, 0.0041618444956839085, 0.0041618444956839085, 0.0041618444956839085, 0.0041618444956839085, 0.0041618444956839085, 0.0041618444956839085, 0.0041618444956839085, 0.0041618444956839085, 0.0041618444956839085, 0.0041618444956839085, 0.0041618444956839085, 0.0041618444956839085, 0.0041618444956839085, 0.0041618444956839085, 0.0041618444956839085, 0.0041618444956839085, 0.0041618444956839085, 0.0041618444956839085, 0.0041618444956839085, 0.0041618444956839085, 0.0041618444956839085, 0.0041618444956839085, 0.0041618444956839085, 0.0041618444956839085, 0.0041618444956839085, 0.0041618444956839085, 0.0041618444956839085, 0.0041618444956839085, 0.0041618444956839085, 0.0041618444956839085, 0.0041618444956839085, 0.0041618444956839085, 0.0041618444956839085, 0.0041618444956839085, 0.0041618444956839085, 0.0041618444956839085, 0.0041618444956839085, 0.0041618444956839085, 0.0041618444956839085, 0.0041618444956839085, 0.0041618444956839085, 0.0041618444956839085, 0.0041618444956839085, 0.0041618444956839085, 0.0041618444956839085, 0.0041618444956839085, 0.0041618444956839085, 0.0041618444956839085, 0.0041618444956839085, 0.0041618444956839085, 0.0041618444956839085, 0.0041618444956839085, 0.0041618444956839085, 0.0041618444956839085, 0.0041618444956839085, 0.0041618444956839085, 0.0041618444956839085, 0.0041618444956839085, 0.0041618444956839085, 0.0041618444956839085, 0.0041618444956839085, 0.0041618444956839085, 0.0041618444956839085, 0.0041618444956839085, 0.00041618445538915694, 0.00041618445538915694, 0.00041618445538915694, 0.00041618445538915694, 0.00041618445538915694, 0.00041618445538915694, 0.00041618445538915694, 0.00041618445538915694, 0.00041618445538915694, 0.00041618445538915694, 0.00041618445538915694, 0.00041618445538915694, 0.00041618445538915694, 0.00041618445538915694, 0.00041618445538915694, 0.00041618445538915694, 0.00041618445538915694, 0.00041618445538915694, 4.161844481131993e-05, 4.161844572081463e-06, 4.1618446289248823e-07, 4.161844557870609e-08, 4.161844646688451e-09, 4.161844757710753e-10, 4.161844827099692e-11, 4.161844913835866e-12, 4.161845022256083e-13, 4.1618448867308117e-14, 4.161844717324222e-15, 4.1618447702637814e-16, 4.1618447702637814e-17, 4.16184468754572e-18, 4.1618447392445084e-19, 4.161844868491479e-20, 4.161844949270836e-21, 4.16184484829664e-22, 4.1618449114055124e-23, 4.161844753633331e-24, 4.161844753633331e-25, 4.161844630373815e-26, 4.161844476299419e-27, 4.161844572595917e-28, 4.1618446327812273e-29, 4.161844708012866e-30, 4.161844802052414e-31, 4.161844802052414e-32, 4.161844728584017e-33, 4.161844636748521e-34, 4.161844636748521e-35, 4.1618447802414835e-36, 4.1618447802414835e-37, 4.1618446681376064e-38, 4.161845228656992e-39, 4.1618424260600635e-40, 4.1618564390447067e-41, 4.161856439044707e-42, 4.161856439044707e-43, 4.203895392974451e-44, 4.203895392974451e-45, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "accuracy_train_first": 0.4566823529411765, "accuracy_train_last": 0.9935529411764706, "batch_size_eval": 1024, "accuracy_train_std": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "accuracy_test_std": 0, "error_valid": [0.5332, 0.48919999999999997, 0.4768, 0.4536, 0.4092, 0.35786666666666667, 0.3536, 0.3210666666666666, 0.3169333333333333, 0.2818666666666667, 0.2757333333333334, 0.268, 0.2512, 0.24373333333333336, 0.24306666666666665, 0.23786666666666667, 0.22840000000000005, 0.22426666666666661, 0.21533333333333338, 0.21133333333333337, 0.21440000000000003, 0.2074666666666667, 0.19853333333333334, 0.19879999999999998, 0.19533333333333336, 0.19999999999999996, 0.18920000000000003, 0.19399999999999995, 0.1904, 0.18999999999999995, 0.18679999999999997, 0.1889333333333333, 0.1876, 0.1844, 0.18400000000000005, 0.18333333333333335, 0.1842666666666667, 0.19120000000000004, 0.18679999999999997, 0.17533333333333334, 0.18266666666666664, 0.1769333333333334, 0.17453333333333332, 0.16959999999999997, 0.1757333333333333, 0.17520000000000002, 0.17679999999999996, 0.1718666666666666, 0.17279999999999995, 0.1710666666666667, 0.1664, 0.17493333333333339, 0.1710666666666667, 0.16746666666666665, 0.17413333333333336, 0.16600000000000004, 0.16626666666666667, 0.1637333333333333, 0.15746666666666664, 0.15880000000000005, 0.16346666666666665, 0.16159999999999997, 0.15759999999999996, 0.16533333333333333, 0.16400000000000003, 0.1624, 0.15759999999999996, 0.15773333333333328, 0.15866666666666662, 0.1545333333333333, 0.15613333333333335, 0.1517333333333334, 0.15026666666666666, 0.15066666666666662, 0.15413333333333334, 0.16959999999999997, 0.15200000000000002, 0.15426666666666666, 0.15213333333333334, 0.15973333333333328, 0.1525333333333333, 0.1697333333333333, 0.14959999999999996, 0.15133333333333332, 0.15280000000000005, 0.14759999999999995, 0.14626666666666666, 0.14546666666666663, 0.14759999999999995, 0.14973333333333338, 0.14533333333333331, 0.1472, 0.14613333333333334, 0.12493333333333334, 0.12546666666666662, 0.12493333333333334, 0.12786666666666668, 0.12639999999999996, 0.12786666666666668, 0.1266666666666667, 0.1266666666666667, 0.1265333333333334, 0.12546666666666662, 0.1266666666666667, 0.126, 0.12626666666666664, 0.12453333333333338, 0.1265333333333334, 0.12706666666666666, 0.1253333333333333, 0.12680000000000002, 0.1246666666666667, 0.12493333333333334, 0.124, 0.12453333333333338, 0.12453333333333338, 0.12413333333333332, 0.12360000000000004, 0.1246666666666667, 0.12480000000000002, 0.12426666666666664, 0.12319999999999998, 0.12386666666666668, 0.12346666666666661, 0.1233333333333333, 0.12413333333333332, 0.1233333333333333, 0.1233333333333333, 0.12386666666666668, 0.12319999999999998, 0.12386666666666668, 0.1233333333333333, 0.12413333333333332, 0.12280000000000002, 0.12360000000000004, 0.12373333333333336, 0.1233333333333333, 0.1246666666666667, 0.124, 0.12413333333333332, 0.12346666666666661, 0.12373333333333336, 0.1233333333333333, 0.124, 0.12453333333333338, 0.124, 0.12439999999999996, 0.12386666666666668, 0.124, 0.12413333333333332, 0.124, 0.124, 0.1226666666666667, 0.124, 0.124, 0.124, 0.1233333333333333, 0.12293333333333334, 0.1233333333333333, 0.1246666666666667, 0.12413333333333332, 0.12373333333333336, 0.12346666666666661, 0.12360000000000004, 0.1226666666666667, 0.12373333333333336, 0.12360000000000004, 0.124, 0.12293333333333334, 0.12426666666666664, 0.12413333333333332, 0.12373333333333336, 0.12319999999999998, 0.12413333333333332, 0.12346666666666661, 0.124, 0.12293333333333334, 0.12280000000000002, 0.12346666666666661, 0.12346666666666661, 0.12373333333333336, 0.1226666666666667, 0.12439999999999996, 0.12386666666666668, 0.12306666666666666, 0.12346666666666661, 0.12360000000000004, 0.12426666666666664, 0.12306666666666666, 0.12346666666666661, 0.12306666666666666, 0.12413333333333332, 0.12346666666666661, 0.12306666666666666, 0.12293333333333334, 0.12346666666666661, 0.12386666666666668, 0.12493333333333334, 0.12360000000000004, 0.12293333333333334, 0.12480000000000002, 0.12346666666666661, 0.12426666666666664], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.055615349644049045, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 32, "valid_ratio": 0.15, "learning_rate": 0.004161844583812969, "optimization": "nesterov_momentum", "nb_data_augmentation": 4, "learning_rate_decay_method": "discrete", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 2.0226854746628947e-05, "rotation_range": [0, 0], "momentum": 0.6627858755207852}, "accuracy_valid_max": 0.8773333333333333, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        y_pred = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            y_pred.extend((nnet.predict(X[mini_batch]) == y[mini_batch]).tolist())\n        return np.mean(y_pred)\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            acc = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = acc\n            status[\"accuracy_train_std\"] = 0\n            acc = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = acc\n            status[\"accuracy_valid_std\"] = 0\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n\n    # rescaling to [-1, 1]\n    X_min = X_train.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X_train.max(axis=(0, 2, 3))[None, :, None, None]\n    def preprocess(a):\n        return (a / 255.) * 2 - 1\n        # return 2 * ((a - X_min) / (X_max - X_min)) - 1\n    X_train = preprocess(X_train)\n    X_valid = preprocess(X_valid)\n\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = preprocess(X_test)\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    acc = evaluate(X_test, y_test, batch_size_eval)\n    light.set(\"accuracy_test\", acc)\n    light.set(\"accuracy_test_std\", 0)\n    print(\"Test accuracy : {}+-{}\".format(acc, 0))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8757333333333334, "accuracy_valid_std": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "accuracy_valid": [0.4668, 0.5108, 0.5232, 0.5464, 0.5908, 0.6421333333333333, 0.6464, 0.6789333333333334, 0.6830666666666667, 0.7181333333333333, 0.7242666666666666, 0.732, 0.7488, 0.7562666666666666, 0.7569333333333333, 0.7621333333333333, 0.7716, 0.7757333333333334, 0.7846666666666666, 0.7886666666666666, 0.7856, 0.7925333333333333, 0.8014666666666667, 0.8012, 0.8046666666666666, 0.8, 0.8108, 0.806, 0.8096, 0.81, 0.8132, 0.8110666666666667, 0.8124, 0.8156, 0.816, 0.8166666666666667, 0.8157333333333333, 0.8088, 0.8132, 0.8246666666666667, 0.8173333333333334, 0.8230666666666666, 0.8254666666666667, 0.8304, 0.8242666666666667, 0.8248, 0.8232, 0.8281333333333334, 0.8272, 0.8289333333333333, 0.8336, 0.8250666666666666, 0.8289333333333333, 0.8325333333333333, 0.8258666666666666, 0.834, 0.8337333333333333, 0.8362666666666667, 0.8425333333333334, 0.8412, 0.8365333333333334, 0.8384, 0.8424, 0.8346666666666667, 0.836, 0.8376, 0.8424, 0.8422666666666667, 0.8413333333333334, 0.8454666666666667, 0.8438666666666667, 0.8482666666666666, 0.8497333333333333, 0.8493333333333334, 0.8458666666666667, 0.8304, 0.848, 0.8457333333333333, 0.8478666666666667, 0.8402666666666667, 0.8474666666666667, 0.8302666666666667, 0.8504, 0.8486666666666667, 0.8472, 0.8524, 0.8537333333333333, 0.8545333333333334, 0.8524, 0.8502666666666666, 0.8546666666666667, 0.8528, 0.8538666666666667, 0.8750666666666667, 0.8745333333333334, 0.8750666666666667, 0.8721333333333333, 0.8736, 0.8721333333333333, 0.8733333333333333, 0.8733333333333333, 0.8734666666666666, 0.8745333333333334, 0.8733333333333333, 0.874, 0.8737333333333334, 0.8754666666666666, 0.8734666666666666, 0.8729333333333333, 0.8746666666666667, 0.8732, 0.8753333333333333, 0.8750666666666667, 0.876, 0.8754666666666666, 0.8754666666666666, 0.8758666666666667, 0.8764, 0.8753333333333333, 0.8752, 0.8757333333333334, 0.8768, 0.8761333333333333, 0.8765333333333334, 0.8766666666666667, 0.8758666666666667, 0.8766666666666667, 0.8766666666666667, 0.8761333333333333, 0.8768, 0.8761333333333333, 0.8766666666666667, 0.8758666666666667, 0.8772, 0.8764, 0.8762666666666666, 0.8766666666666667, 0.8753333333333333, 0.876, 0.8758666666666667, 0.8765333333333334, 0.8762666666666666, 0.8766666666666667, 0.876, 0.8754666666666666, 0.876, 0.8756, 0.8761333333333333, 0.876, 0.8758666666666667, 0.876, 0.876, 0.8773333333333333, 0.876, 0.876, 0.876, 0.8766666666666667, 0.8770666666666667, 0.8766666666666667, 0.8753333333333333, 0.8758666666666667, 0.8762666666666666, 0.8765333333333334, 0.8764, 0.8773333333333333, 0.8762666666666666, 0.8764, 0.876, 0.8770666666666667, 0.8757333333333334, 0.8758666666666667, 0.8762666666666666, 0.8768, 0.8758666666666667, 0.8765333333333334, 0.876, 0.8770666666666667, 0.8772, 0.8765333333333334, 0.8765333333333334, 0.8762666666666666, 0.8773333333333333, 0.8756, 0.8761333333333333, 0.8769333333333333, 0.8765333333333334, 0.8764, 0.8757333333333334, 0.8769333333333333, 0.8765333333333334, 0.8769333333333333, 0.8758666666666667, 0.8765333333333334, 0.8769333333333333, 0.8770666666666667, 0.8765333333333334, 0.8761333333333333, 0.8750666666666667, 0.8764, 0.8770666666666667, 0.8752, 0.8765333333333334, 0.8757333333333334], "seed": 268264851, "model": "residualv3", "loss_std": [0.2571012079715729, 0.17377206683158875, 0.17651717364788055, 0.17832231521606445, 0.17708803713321686, 0.1746286153793335, 0.17529088258743286, 0.17511799931526184, 0.17257951200008392, 0.17315086722373962, 0.17057009041309357, 0.1712837666273117, 0.16691263020038605, 0.16788293421268463, 0.16451415419578552, 0.16333800554275513, 0.16224810481071472, 0.1604464054107666, 0.16051320731639862, 0.15537093579769135, 0.1561877429485321, 0.15176571905612946, 0.15300340950489044, 0.14987210929393768, 0.14746809005737305, 0.14596469700336456, 0.14441199600696564, 0.14502237737178802, 0.14111018180847168, 0.1388743370771408, 0.13778293132781982, 0.13669027388095856, 0.13177403807640076, 0.1316649168729782, 0.12968750298023224, 0.12617482244968414, 0.12618301808834076, 0.12335330247879028, 0.12154334038496017, 0.11879752576351166, 0.11533350497484207, 0.11475090682506561, 0.11341720074415207, 0.11298675090074539, 0.11110780388116837, 0.1062680184841156, 0.10712631791830063, 0.10679282248020172, 0.10152658820152283, 0.10046138614416122, 0.09947733581066132, 0.09710954874753952, 0.0935877114534378, 0.09380301088094711, 0.09375179558992386, 0.08938279002904892, 0.08752815425395966, 0.08754322677850723, 0.08580724149942398, 0.08494600653648376, 0.08226058632135391, 0.08144430071115494, 0.07971571385860443, 0.07765911519527435, 0.07582961767911911, 0.07435502856969833, 0.0740170031785965, 0.07086813449859619, 0.0740492194890976, 0.06915454566478729, 0.06917896866798401, 0.06847844272851944, 0.06594062596559525, 0.06508507579565048, 0.06546926498413086, 0.06376775354146957, 0.06335511803627014, 0.0611548125743866, 0.06001860648393631, 0.05945046246051788, 0.06087784841656685, 0.05746760219335556, 0.05776399001479149, 0.05655244365334511, 0.05497751757502556, 0.05313677713274956, 0.05421805754303932, 0.05506134033203125, 0.052224576473236084, 0.051981132477521896, 0.04998669773340225, 0.0505497008562088, 0.04755603149533272, 0.04814045876264572, 0.0382678396999836, 0.03556093946099281, 0.03434770554304123, 0.033187657594680786, 0.032248206436634064, 0.031953971832990646, 0.030109582468867302, 0.030767565593123436, 0.03157311677932739, 0.030780300498008728, 0.030013153329491615, 0.029821962118148804, 0.02966623567044735, 0.02905622497200966, 0.028627658262848854, 0.028432346880435944, 0.02825677953660488, 0.028001675382256508, 0.02749771438539028, 0.02776394598186016, 0.027282461524009705, 0.02812185138463974, 0.028436819091439247, 0.027031583711504936, 0.027153965085744858, 0.027480054646730423, 0.027464941143989563, 0.02771722339093685, 0.027542157098650932, 0.027170028537511826, 0.02703632228076458, 0.026723185554146767, 0.026968469843268394, 0.02794450707733631, 0.027943480759859085, 0.02724304050207138, 0.02697976864874363, 0.027434194460511208, 0.027531668543815613, 0.02827371656894684, 0.028128817677497864, 0.027592109516263008, 0.02736327424645424, 0.027909621596336365, 0.02807595022022724, 0.027273178100585938, 0.026818707585334778, 0.02701231651008129, 0.027451056987047195, 0.02791072241961956, 0.02820310741662979, 0.028226733207702637, 0.02775341458618641, 0.026566937565803528, 0.0277705118060112, 0.027743501588702202, 0.02842087298631668, 0.02695116214454174, 0.027177173644304276, 0.027992378920316696, 0.02812500298023224, 0.026752999052405357, 0.026978129521012306, 0.02875979244709015, 0.02910573035478592, 0.026821542531251907, 0.02736726403236389, 0.02715812809765339, 0.028365425765514374, 0.02783273346722126, 0.027750175446271896, 0.02858637645840645, 0.02802118845283985, 0.02825780399143696, 0.02812434546649456, 0.027105670422315598, 0.0284606721252203, 0.027863286435604095, 0.02841222658753395, 0.02721266634762287, 0.02798740565776825, 0.02857670933008194, 0.027900075539946556, 0.028926225379109383, 0.02748522162437439, 0.02967767044901848, 0.027270395308732986, 0.02743852697312832, 0.028301645070314407, 0.02818119525909424, 0.027990518137812614, 0.028702430427074432, 0.02818041481077671, 0.026910856366157532, 0.027698490768671036, 0.026914214715361595, 0.0270911306142807, 0.028653070330619812, 0.027636472135782242, 0.026759061962366104, 0.026713397353887558, 0.027188174426555634, 0.02788676880300045, 0.028036609292030334, 0.028685875236988068, 0.027623271569609642, 0.028089649975299835, 0.02696917951107025, 0.0281318761408329]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:45 2016", "state": "available"}], "summary": "a9781ec31713397c5ee57482005443b6"}