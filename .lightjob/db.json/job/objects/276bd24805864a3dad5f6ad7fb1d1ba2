{"content": {"hp_model": {"f0": 16, "f1": 64, "f2": 16, "f3": 64, "nonlin": "leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.4343700408935547, 1.044107437133789, 0.8025228381156921, 0.6822765469551086, 0.6001755595207214, 0.5350324511528015, 0.47987979650497437, 0.4308226704597473, 0.38663798570632935, 0.3459029197692871, 0.3084728419780731, 0.2732895314693451, 0.24065066874027252, 0.21037037670612335, 0.18231168389320374, 0.15636952221393585, 0.13272325694561005, 0.11131864786148071, 0.09211831539869308, 0.07493659853935242, 0.05995064973831177, 0.047112058848142624, 0.036397743970155716, 0.027747731655836105, 0.0208914652466774, 0.01566808670759201, 0.01171138882637024, 0.008802961558103561, 0.006682072766125202, 0.005184069275856018, 0.004160663578659296, 0.0034495247527956963, 0.002933322452008724, 0.00254833628423512, 0.0022534620948135853, 0.002018557395786047, 0.0018307011341676116, 0.0016768332570791245, 0.0015491744270548224, 0.0014417993370443583, 0.0013506432296708226, 0.0012721095699816942, 0.0012042579473927617, 0.0011446241987869143, 0.0010924286907538772, 0.0010459739714860916, 0.0010049722623080015, 0.000968192471191287, 0.0009350977488793433, 0.0009053275571204722, 0.0008783597149886191, 0.0008539442205801606, 0.0008315579616464674, 0.0008111695642583072, 0.0007923867087811232, 0.000775128835812211, 0.0007591825560666621, 0.0007445071241818368, 0.0007307918858714402], "moving_avg_accuracy_train": [0.048110825604466584, 0.10833898578811367, 0.1731796146813861, 0.23513497031019284, 0.2934773818247623, 0.34784985289846915, 0.39826605246119695, 0.4447682931914523, 0.48773870642606304, 0.5272326756228809, 0.5635701236440646, 0.5969015449440638, 0.6275322645902535, 0.6556974394670531, 0.68165753889544, 0.7054541421083785, 0.7272617460488419, 0.7473745456964493, 0.7658364634447723, 0.782770698756349, 0.7983207913772534, 0.8124809963503624, 0.8254530454094937, 0.8373162626651022, 0.8483093784332449, 0.8584589129448023, 0.8678190334397278, 0.8765454112303988, 0.8846734827527077, 0.8922329598454233, 0.8992550532169625, 0.9057283970727765, 0.9117194560596665, 0.917216040844296, 0.9222931394349771, 0.9269485947213614, 0.9312198846874404, 0.9351384504188164, 0.9387278664972744, 0.9420397211762198, 0.9450854945539374, 0.9478824941653118, 0.9504602476845964, 0.9528151030840954, 0.95495078503413, 0.9569031617725037, 0.958688238671573, 0.9603203845176401, 0.9618125672671958, 0.9631811083787006, 0.9644453474623884, 0.9656017638281835, 0.9666471888550182, 0.967595046825598, 0.9684574195943579, 0.969252156276718, 0.9699743947372707, 0.9706476608398632, 0.9712722015226728], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.04738769531249999, 0.10614549016378011, 0.16810071241999244, 0.2261981324712914, 0.27977269634200863, 0.32839263585690415, 0.37243031363040047, 0.4125433863712761, 0.4488150207669045, 0.4813852200268104, 0.510758405008316, 0.5370711716705115, 0.5608259038539876, 0.5822316358989352, 0.6013269258105477, 0.6182929601684989, 0.6336112192156551, 0.6472491089657764, 0.6595730673745451, 0.6705649146751178, 0.6805694995442024, 0.6894881767076285, 0.6973674722710523, 0.7044211876757243, 0.7106342246875192, 0.7162249284894752, 0.7210703679165066, 0.7255086236229734, 0.7295874734688839, 0.7333683016114534, 0.7368707622070851, 0.7400941599133345, 0.7430450754826184, 0.7458239993161337, 0.7483240012576379, 0.7506238606386513, 0.752594018814244, 0.7543305400785274, 0.7558323740601325, 0.7570853388849175, 0.7581885931647239, 0.7591581374627093, 0.7600307273308963, 0.7608526793060145, 0.7616422937172805, 0.7623529466874199, 0.7629437062355453, 0.7634753898288582, 0.7639783191253399, 0.7643943343984233, 0.7647433045730389, 0.7649963425739428, 0.7651986332035967, 0.7653684877390352, 0.7655213568209298, 0.7656467319633851, 0.7657839836540947, 0.7659075101757334, 0.7660064770139583], "moving_var_accuracy_train": [0.02083186386309058, 0.05139555898874501, 0.08409476748734618, 0.11023148555843881, 0.12984286983461327, 0.14346587334710173, 0.15199542461753074, 0.15625800769218925, 0.15725031464494924, 0.15556324560672707, 0.15189061220428438, 0.14670040379675833, 0.14047453229147377, 0.13356657274489275, 0.1262752563313891, 0.1187442356185165, 0.11114995636328188, 0.1036756831139369, 0.0963756964650558, 0.08931904174884087, 0.08256338599862513, 0.07611165004268675, 0.07001495154955066, 0.06428007970747948, 0.058939709085357644, 0.05397285563403353, 0.04936407677174589, 0.045113016118680946, 0.04119630442685878, 0.037590985229410526, 0.03427567486433701, 0.03122524500398374, 0.028425755593644915, 0.025855092032932007, 0.023501575200534273, 0.021346477055792547, 0.01937602461198224, 0.017576618567304043, 0.01593491188063227, 0.014440136125299123, 0.013079613131984932, 0.011842060680220697, 0.010717657931054286, 0.009695800233521804, 0.008767270446694952, 0.00792484937638234, 0.0071610429345644235, 0.00646891374167349, 0.005842061851728784, 0.005274711809520816, 0.004761625332715246, 0.0042974984887434306, 0.003877584861249678, 0.0034979122877162342, 0.003154814240075298, 0.002845017273616366, 0.0025652102017998425, 0.002312768766823961, 0.0020850023497219227], "duration": 33777.78082, "accuracy_train": [0.48110825604466595, 0.6503924274409376, 0.756745274720838, 0.7927331709694537, 0.8185590854558878, 0.8372020925618309, 0.8520118485257475, 0.8632884597637505, 0.87447242553756, 0.8826783983942414, 0.8906071558347176, 0.8968843366440569, 0.9032087414059615, 0.9091840133582503, 0.9152984337509228, 0.9196235710248246, 0.9235301815130121, 0.9283897425249169, 0.9319937231796788, 0.9351788165605389, 0.9382716249653931, 0.9399228411083426, 0.9422014869416758, 0.9440852179655776, 0.9472474203465301, 0.9498047235488187, 0.9520601178940569, 0.9550828113464378, 0.9578261264534883, 0.9602682536798633, 0.9624538935608158, 0.9639884917751015, 0.9656389869416758, 0.9666853039059615, 0.9679870267511074, 0.9688476922988187, 0.9696614943821521, 0.9704055420011997, 0.971032611203396, 0.9718464132867294, 0.972497454953396, 0.9730554906676817, 0.973660029358158, 0.9740088016795865, 0.9741719225844407, 0.9744745524178663, 0.9747539307631967, 0.9750096971322444, 0.9752422120131967, 0.9754979783822444, 0.9758234992155776, 0.9760095111203396, 0.9760560140965301, 0.9761257685608158, 0.9762187745131967, 0.9764047864179586, 0.9764745408822444, 0.9767070557631967, 0.9768930676679586], "end": "2016-01-30 01:06:54.353000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0], "moving_var_accuracy_valid": [0.02021034300327301, 0.04926161480501179, 0.07888149940786063, 0.10137114141662822, 0.11706613232040344, 0.12663460575425198, 0.13142499875196742, 0.1327640263192037, 0.13132830684285454, 0.12774283707703887, 0.12273360933295477, 0.11669150360443162, 0.11010093895396647, 0.1032146933379907, 0.0961748948754684, 0.0891480222844382, 0.08234506159811641, 0.07578448376983293, 0.06957295495059919, 0.06370304581925122, 0.05823356670295055, 0.053126095253764265, 0.04837223541556996, 0.04398280598310391, 0.039931941844982936, 0.036220051381495484, 0.032809350792515214, 0.02970569873670756, 0.02688486200762616, 0.02432502775985635, 0.022002930055886296, 0.01989614968525155, 0.01798490584099978, 0.016255917015952116, 0.014686575401724623, 0.013265522040104075, 0.01197390354522536, 0.010803652745614602, 0.009743587018827876, 0.008783357604614423, 0.007915976374206183, 0.007132838882097371, 0.006426407711590196, 0.0057898473858757835, 0.005216474065554515, 0.004699371907794775, 0.004232575688608611, 0.003811862306738333, 0.0034329525169598356, 0.0030912148836308006, 0.0027831894169126612, 0.0025054467292905083, 0.0022552703498510697, 0.002030002969934844, 0.0018272129935471538, 0.0016446331645295494, 0.0014803393903160187, 0.00133244278049835, 0.0011992866523641292], "accuracy_test": 0.46384327168367345, "start": "2016-01-29 15:43:56.572000", "learning_rate_per_epoch": [0.0013716497924178839, 0.0006858248962089419, 0.0004572166071739048, 0.00034291244810447097, 0.00027432997012510896, 0.0002286083035869524, 0.00019594996410887688, 0.00017145622405223548, 0.0001524055260233581, 0.00013716498506255448, 0.0001246954343514517, 0.0001143041517934762, 0.00010551152081461623, 9.797498205443844e-05, 9.144331852439791e-05, 8.572811202611774e-05, 8.068528404692188e-05, 7.620276301167905e-05, 7.219209510367364e-05, 6.858249253127724e-05, 6.531665712827817e-05, 6.234771717572585e-05, 5.963694638921879e-05, 5.71520758967381e-05, 5.486599184223451e-05, 5.2755760407308117e-05, 5.0801842007786036e-05, 4.898749102721922e-05, 4.7298268327722326e-05, 4.5721659262198955e-05, 4.424676808412187e-05, 4.286405601305887e-05, 4.156514478381723e-05, 4.034264202346094e-05, 3.9189992094179615e-05, 3.810138150583953e-05, 3.7071615224704146e-05, 3.609604755183682e-05, 3.517050572554581e-05, 3.429124626563862e-05, 3.345487129990943e-05, 3.265832856413908e-05, 3.189883136656135e-05, 3.1173858587862924e-05, 3.0481105568469502e-05, 2.9818473194609396e-05, 2.9184038794483058e-05, 2.857603794836905e-05, 2.7992853574687615e-05, 2.7432995921117254e-05, 2.689509346964769e-05, 2.6377880203654058e-05, 2.5880184693960473e-05, 2.5400921003893018e-05, 2.493908687029034e-05, 2.449374551360961e-05, 2.4064031094894744e-05, 2.3649134163861163e-05, 2.3248301658895798e-05], "accuracy_train_first": 0.48110825604466595, "accuracy_train_last": 0.9768930676679586, "batch_size_eval": 1024, "accuracy_train_std": [0.023703808465029688, 0.024537573289933296, 0.025434146500981324, 0.023319469419709256, 0.025697391961936086, 0.026847305929973725, 0.02600291316450211, 0.0252066785453313, 0.023750951777943608, 0.024690180840711274, 0.023799548364079087, 0.023555515942677015, 0.022880558166530337, 0.022826801257955, 0.022394540521060362, 0.02211022333083543, 0.021076357053597186, 0.02024242552329158, 0.01996624279537348, 0.019320307382523858, 0.018982775423919063, 0.018313777319813093, 0.017922170825937816, 0.01705550858423744, 0.015850309582412667, 0.015272542635056046, 0.013950518372888937, 0.01297316919287276, 0.012082841692750212, 0.011088066692321201, 0.0109566949276375, 0.010101985012206707, 0.009904404220440437, 0.009912993821745765, 0.00971243690967689, 0.00944415897722577, 0.009124795395596082, 0.009022045597597917, 0.008957921242820794, 0.008585439486632547, 0.008173238183736376, 0.008123204369206221, 0.008011528881452762, 0.007715268509604534, 0.007548420934628697, 0.007136960249465836, 0.007084300002609256, 0.007228449254601556, 0.007211813485569541, 0.007176878695384457, 0.0068695022493239135, 0.006686224857919027, 0.006566582164609851, 0.0065560228921905835, 0.006530337337262652, 0.00656520250078035, 0.006463693626292877, 0.006342092052389158, 0.006381545486907513], "accuracy_test_std": 0.01185387958277933, "error_valid": [0.526123046875, 0.3650343561746988, 0.27430228727409633, 0.2509250870670181, 0.2380562288215362, 0.2340279085090362, 0.23123058640813254, 0.22643895896084332, 0.22474026967243976, 0.2254829866340362, 0.22488293015813254, 0.22611392836972888, 0.22538150649472888, 0.2251167756965362, 0.22681546498493976, 0.22901273060993976, 0.22852444935993976, 0.23000988328313254, 0.2295113069465362, 0.23050845961972888, 0.2293892366340362, 0.2302437288215362, 0.23171886765813254, 0.23209537368222888, 0.23344844220632532, 0.23345873729292166, 0.23532067724021077, 0.23454707501882532, 0.23370287791792166, 0.23260424510542166, 0.23160709243222888, 0.23089526073042166, 0.23039668439382532, 0.22916568618222888, 0.22917598126882532, 0.22867740493222888, 0.22967455760542166, 0.23004076854292166, 0.23065112010542166, 0.2316379776920181, 0.2318821183170181, 0.23211596385542166, 0.23211596385542166, 0.23174975291792166, 0.23125117658132532, 0.23125117658132532, 0.23173945783132532, 0.23173945783132532, 0.23149531720632532, 0.23186152814382532, 0.23211596385542166, 0.23272631541792166, 0.2329807511295181, 0.2331028214420181, 0.2331028214420181, 0.2332248917545181, 0.2329807511295181, 0.2329807511295181, 0.2331028214420181], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.05350013631763202, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 16, "valid_ratio": 0.15, "learning_rate": 0.0013716497824496237, "optimization": "rmsprop", "nb_data_augmentation": 0, "learning_rate_decay_method": "lin", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 3.545482021690661e-08, "rotation_range": [0, 0], "momentum": 0.6004053265418419}, "accuracy_valid_max": 0.7752597303275602, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.7668971785579819, "accuracy_valid_std": [0.013843010256186363, 0.012773303601993764, 0.012859475454787666, 0.012513961472615515, 0.011665978038215414, 0.010479948630652873, 0.010284865014312205, 0.009425126964677306, 0.008539373712979411, 0.0076198170661322786, 0.006044307235335246, 0.008497828459056469, 0.009390723990345691, 0.009470004145424255, 0.00999500295715834, 0.00907071854812766, 0.009718328048205131, 0.00921569481219358, 0.010824485746656155, 0.011914660735655072, 0.011250635192135027, 0.009827404653838703, 0.009181070259695778, 0.00838407025532166, 0.007242796465794249, 0.006855420122151072, 0.0081633547873586, 0.0074533198428151535, 0.00698677935682589, 0.007689283031502208, 0.007266887050706173, 0.007710407618014521, 0.007384886142207577, 0.006919215460196343, 0.006986942818424466, 0.007033067771200329, 0.007167437544077043, 0.007073301565534062, 0.007340094841710983, 0.006787749920999113, 0.007912595660184298, 0.007685359507890447, 0.007400892942185691, 0.008224796715845967, 0.00849520845718738, 0.008195228890637233, 0.007865604560914565, 0.007603592523534743, 0.006868027679739571, 0.0071700636630045046, 0.007037613771830591, 0.006699234506217259, 0.006713002254303292, 0.007305950540185347, 0.007124205845251622, 0.0066345045142108955, 0.00655123133879174, 0.006991358704279596, 0.007073828750274683], "accuracy_valid": [0.473876953125, 0.6349656438253012, 0.7256977127259037, 0.7490749129329819, 0.7619437711784638, 0.7659720914909638, 0.7687694135918675, 0.7735610410391567, 0.7752597303275602, 0.7745170133659638, 0.7751170698418675, 0.7738860716302711, 0.7746184935052711, 0.7748832243034638, 0.7731845350150602, 0.7709872693900602, 0.7714755506400602, 0.7699901167168675, 0.7704886930534638, 0.7694915403802711, 0.7706107633659638, 0.7697562711784638, 0.7682811323418675, 0.7679046263177711, 0.7665515577936747, 0.7665412627070783, 0.7646793227597892, 0.7654529249811747, 0.7662971220820783, 0.7673957548945783, 0.7683929075677711, 0.7691047392695783, 0.7696033156061747, 0.7708343138177711, 0.7708240187311747, 0.7713225950677711, 0.7703254423945783, 0.7699592314570783, 0.7693488798945783, 0.7683620223079819, 0.7681178816829819, 0.7678840361445783, 0.7678840361445783, 0.7682502470820783, 0.7687488234186747, 0.7687488234186747, 0.7682605421686747, 0.7682605421686747, 0.7685046827936747, 0.7681384718561747, 0.7678840361445783, 0.7672736845820783, 0.7670192488704819, 0.7668971785579819, 0.7668971785579819, 0.7667751082454819, 0.7670192488704819, 0.7670192488704819, 0.7668971785579819], "seed": 889554187, "model": "residualv3", "loss_std": [0.38143420219421387, 0.2938787639141083, 0.2753176689147949, 0.2648093104362488, 0.25432640314102173, 0.24443048238754272, 0.23496992886066437, 0.2249147891998291, 0.21500560641288757, 0.20449362695217133, 0.19397057592868805, 0.18276558816432953, 0.17175732553005219, 0.1603306531906128, 0.14843226969242096, 0.13617508113384247, 0.12334700673818588, 0.11083462834358215, 0.09820111095905304, 0.08572886139154434, 0.07364533096551895, 0.0621432401239872, 0.05148005113005638, 0.04178622364997864, 0.03297429159283638, 0.025420313701033592, 0.018673904240131378, 0.013054494746029377, 0.008334130048751831, 0.004843018017709255, 0.003059574170038104, 0.0021784882992506027, 0.0016905405791476369, 0.0013824701309204102, 0.0011698752641677856, 0.0010099121136590838, 0.0008859726949594915, 0.0007872559363022447, 0.0007064568344503641, 0.0006401337450370193, 0.0005840430385433137, 0.000536316423676908, 0.000495774089358747, 0.00045994241372682154, 0.0004289415664970875, 0.000401223951485008, 0.0003770954208448529, 0.0003555644943844527, 0.00033633544808253646, 0.00031901762122288346, 0.0003034523979295045, 0.0002894798817578703, 0.00027666293317452073, 0.000264988950220868, 0.00025418977020308375, 0.0002443178091198206, 0.00023524599964730442, 0.00022695082589052618, 0.00021920738799963146]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:23 2016", "state": "available"}], "summary": "4e68c6a783f8627593dcd586a5dc76c5"}