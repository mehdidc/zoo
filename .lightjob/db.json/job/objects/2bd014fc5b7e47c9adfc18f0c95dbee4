{"content": {"hp_model": {"f0": 16, "f1": 64, "f2": 16, "f3": 16, "nonlin": "rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.4839463233947754, 1.1302334070205688, 0.9811999797821045, 0.8710511326789856, 0.7901274561882019, 0.726807713508606, 0.674673318862915, 0.6289120316505432, 0.5864819288253784, 0.548059344291687, 0.5111637711524963, 0.4747675657272339, 0.4439457356929779, 0.413330078125, 0.38343730568885803, 0.36072829365730286, 0.33718404173851013, 0.31627216935157776, 0.2942681610584259, 0.2757626473903656, 0.2592456340789795, 0.24545666575431824, 0.2316407561302185, 0.2166658192873001, 0.20487622916698456, 0.19606594741344452, 0.18625232577323914, 0.17191551625728607, 0.17501887679100037, 0.15911225974559784, 0.15235647559165955, 0.15139171481132507, 0.1406555324792862, 0.13382859528064728, 0.13552342355251312, 0.12439079582691193, 0.11746150255203247, 0.12523658573627472, 0.11281846463680267, 0.11064594238996506, 0.1065014973282814, 0.10391443222761154, 0.10187666118144989, 0.09785417467355728, 0.09586741030216217, 0.09178175777196884, 0.08708608895540237, 0.09032988548278809, 0.08672093600034714, 0.08093781769275665, 0.08031687885522842, 0.07960838079452515, 0.07922250777482986, 0.07862433046102524, 0.07305625081062317, 0.07332199066877365, 0.0730258896946907, 0.07157056778669357, 0.06980092823505402, 0.06387382745742798, 0.06323741376399994], "moving_avg_accuracy_train": [0.05674549101375045, 0.11717775239940935, 0.1769695322593669, 0.234429355346184, 0.28822620501828394, 0.33822883282071914, 0.38432619873692153, 0.4267339741601563, 0.46564476731838395, 0.5012988862976013, 0.5341082092657555, 0.5641130391501341, 0.5911311206460196, 0.6160751120732504, 0.6378110278685094, 0.656987449479499, 0.6753364713793251, 0.6928015048546262, 0.707520509384852, 0.7221229589250951, 0.736597329583896, 0.7470696095627065, 0.7600093853231672, 0.771925117062399, 0.7811498790849335, 0.7911049131229609, 0.8005388101631473, 0.8081085585707436, 0.81760226476378, 0.8251212097125127, 0.8326112012020402, 0.8396685940248041, 0.8461712740914543, 0.852628188793097, 0.8582697843079364, 0.8638400797212734, 0.8687395574968851, 0.873723255055613, 0.8787852918608675, 0.8821809478272816, 0.8857603048256535, 0.8892396373979515, 0.8927755765570582, 0.8961206822169209, 0.8980967663346529, 0.9014329475477176, 0.904784499253817, 0.9074007539011558, 0.9101715847206656, 0.9132070200332059, 0.9155831279978163, 0.9180959380754802, 0.9205901622704238, 0.9229904245767424, 0.9232352066680254, 0.9244598306406193, 0.9268081638314023, 0.9282240830114312, 0.9295426241496569, 0.9307549238597835, 0.9326317497012415], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.05586496376129517, 0.1151472197383283, 0.1729418151120105, 0.22797223473974015, 0.27954539318406435, 0.32721253002341394, 0.3711159887586478, 0.4109963420665179, 0.44693851767726067, 0.4796090060827274, 0.5093511539966685, 0.5355890372321673, 0.5584137931475048, 0.5790984398116701, 0.5970463236166477, 0.6127192267877388, 0.6267811590600192, 0.6398234050704329, 0.649969364874007, 0.6606652877145431, 0.670046448137366, 0.6773726226948644, 0.6861125582792937, 0.69346580499278, 0.6978873438459117, 0.7035686537007784, 0.7095770638860469, 0.7124882216258308, 0.7176239415377959, 0.7223459517985493, 0.7263535323528811, 0.730072277150349, 0.7338383045652086, 0.7367465074765341, 0.7399235015895734, 0.742605720801851, 0.7446168860616509, 0.7463028054656514, 0.748511962748529, 0.749834847563661, 0.7517233036958492, 0.7534829198624088, 0.754623003252674, 0.7559105735034608, 0.7557794123786418, 0.7567020535485336, 0.7584561940144634, 0.7598944660380924, 0.7612267085344337, 0.7627725241267735, 0.763542229074789, 0.7642734965664065, 0.7650313525761815, 0.7657338660855062, 0.7653314720409616, 0.7661645769547119, 0.767096594409843, 0.7677889257444611, 0.7676368039249095, 0.7674510661623132, 0.7676277580682956], "moving_var_accuracy_train": [0.0289804567535247, 0.05895093502383364, 0.0852313539708449, 0.10642289999627527, 0.12182751930843043, 0.13214713246192714, 0.13805712351544472, 0.14043718591102788, 0.14001991573774647, 0.1374588699656295, 0.13340104803172442, 0.12816355157606651, 0.12191698696792463, 0.11532511264602806, 0.10804465170055254, 0.10054980284271915, 0.09352500200056989, 0.08691774834915342, 0.08017581536348535, 0.07407731762031494, 0.06855515251199866, 0.06268665509239014, 0.05792492975373017, 0.05341029874428918, 0.04883513497921183, 0.04484354580557522, 0.04116017694530123, 0.037559869069359875, 0.03461505627794116, 0.031662361448425706, 0.0290010250562019, 0.0265491836916749, 0.024274828954950305, 0.02222257178663388, 0.020286763007548, 0.018537340425719792, 0.016899650325411224, 0.015433220465081934, 0.0141205163681335, 0.012812239046300349, 0.011646321310366458, 0.010590640975667701, 0.009644102669733067, 0.008780399989640567, 0.007937504166639686, 0.0072439246957533765, 0.00662062831572596, 0.006020168579570864, 0.0054872492524868826, 0.005021449135067744, 0.004570117223096335, 0.004169933431164386, 0.0038089304770597293, 0.003479888761605963, 0.0031324391498952823, 0.002832692569774019, 0.0025990553317710163, 0.002357193242713277, 0.0021371208750406894, 0.0019366358228211774, 0.0017746745176915395], "duration": 32701.770299, "accuracy_train": [0.5674549101375046, 0.6610681048703396, 0.7150955509989848, 0.7515677631275379, 0.7723978520671835, 0.7882524830426356, 0.7992024919827427, 0.8084039529692691, 0.8158419057424326, 0.8221859571105574, 0.8293921159791436, 0.8341565081095422, 0.8342938541089886, 0.8405710349183279, 0.8334342700258398, 0.8295752439784054, 0.8404776684777593, 0.8499868061323367, 0.8399915501568845, 0.8535450047872831, 0.8668666655131044, 0.8413201293720007, 0.8764673671673128, 0.8791667027154854, 0.8641727372877446, 0.8807002194652085, 0.8854438835248246, 0.8762362942391103, 0.9030456205011074, 0.8927917142511074, 0.9000211246077889, 0.9031851294296788, 0.9046953946913067, 0.9107404211078812, 0.9090441439414912, 0.9139727384413067, 0.9128348574773901, 0.9185765330841639, 0.924343623108158, 0.9127418515250092, 0.9179745178110004, 0.9205536305486341, 0.924599028989018, 0.9262266331556847, 0.9158815233942414, 0.9314585784653008, 0.9349484646087117, 0.9309470457272055, 0.9351090620962532, 0.9405259378460686, 0.9369680996793098, 0.9407112287744556, 0.9430381800249169, 0.9445927853336102, 0.9254382454895718, 0.9354814463939645, 0.9479431625484496, 0.9409673556316908, 0.9414094943936876, 0.9416656212509228, 0.9495231822743633], "end": "2016-02-03 20:44:26.912000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0], "moving_var_accuracy_valid": [0.028088047584457414, 0.05690871568955, 0.08127978141026386, 0.10040692702887362, 0.11430435037329655, 0.12332331874611839, 0.128338610071754, 0.12981873228422355, 0.12846341894450247, 0.12522332436391784, 0.12066235019033883, 0.11479195382142197, 0.10800148378263238, 0.10105202687296139, 0.09384596298335804, 0.08667212572931586, 0.07978455460945606, 0.07333700077747583, 0.06692976520274993, 0.06126641357117124, 0.055931827751962745, 0.0508217004795889, 0.04642700869780976, 0.042270939963093355, 0.038219796019251795, 0.03468831195232966, 0.03154438969368664, 0.02846622427879111, 0.025856982422039394, 0.023471960607959412, 0.0212693108642586, 0.019266841343650993, 0.017467803871691164, 0.015797142282083027, 0.014308267678223305, 0.012942189609525374, 0.011684373719892869, 0.010541517266034655, 0.009531288922535611, 0.008593910248389012, 0.007766615622618901, 0.0070178203018395734, 0.006327736382886442, 0.005709883278954199, 0.005139049780224753, 0.004632806202757692, 0.004197218661449834, 0.003796114433030434, 0.0034324768203489115, 0.003110735050923707, 0.002804993557194331, 0.0025293069707735664, 0.0022815453852801787, 0.002057832573829213, 0.0018535066051500566, 0.0016744025188108838, 0.001514780175759819, 0.001367616062275884, 0.0012310627254801486, 0.0011082669395802225, 0.000997721225888958], "accuracy_test": 0.09968909438775511, "start": "2016-02-03 11:39:25.142000", "learning_rate_per_epoch": [0.0008289603283628821, 0.0008289603283628821, 0.0008289603283628821, 0.0008289603283628821, 0.0008289603283628821, 0.0008289603283628821, 0.0008289603283628821, 0.0008289603283628821, 0.0008289603283628821, 0.0008289603283628821, 0.0008289603283628821, 0.0008289603283628821, 0.0008289603283628821, 0.0008289603283628821, 0.0008289603283628821, 0.0008289603283628821, 0.0008289603283628821, 0.0008289603283628821, 0.0008289603283628821, 0.0008289603283628821, 0.0008289603283628821, 0.0008289603283628821, 0.0008289603283628821, 0.0008289603283628821, 0.0008289603283628821, 0.0008289603283628821, 0.0008289603283628821, 0.0008289603283628821, 0.0008289603283628821, 0.0008289603283628821, 0.0008289603283628821, 0.0008289603283628821, 0.0008289603283628821, 0.0008289603283628821, 0.0008289603283628821, 0.0008289603283628821, 0.0008289603283628821, 0.0008289603283628821, 0.0008289603283628821, 0.0008289603283628821, 0.0008289603283628821, 0.0008289603283628821, 0.0008289603283628821, 0.0008289603283628821, 0.0008289603283628821, 0.0008289603283628821, 0.0008289603283628821, 0.0008289603283628821, 0.0008289603283628821, 0.0008289603283628821, 0.0008289603283628821, 0.0008289603283628821, 0.0008289603283628821, 0.0008289603283628821, 0.0008289603283628821, 0.0008289603283628821, 0.0008289603283628821, 0.0008289603283628821, 0.0008289603283628821, 0.0008289603283628821, 0.0008289603283628821], "accuracy_train_first": 0.5674549101375046, "accuracy_train_last": 0.9495231822743633, "batch_size_eval": 1024, "accuracy_train_std": [0.01740420412809833, 0.01636893012434928, 0.01788701572197109, 0.01995971635053658, 0.018063524562996442, 0.01976925786882506, 0.019901473152025465, 0.020901231457937427, 0.02272050992367784, 0.02451428300564137, 0.02501159932148086, 0.026027852797834206, 0.026496689532087177, 0.02777515995901333, 0.027747971570476117, 0.026327013027911055, 0.028022044393454043, 0.028779792745575735, 0.027528028258761533, 0.027047422896905315, 0.02687209832500009, 0.02361627928652142, 0.027196758471652673, 0.024970208084785995, 0.02444222881699528, 0.02580643402705685, 0.02385878640117049, 0.02528302524085661, 0.021764013599491018, 0.023741204817153177, 0.02353857123201546, 0.02293963752232397, 0.021112488554365247, 0.019712260598347124, 0.021121674761611003, 0.01884320994212334, 0.02129097139669465, 0.0188666752956763, 0.0174935877296825, 0.018044223170423788, 0.01715572343946017, 0.01757290368199176, 0.018685679090524675, 0.01577228145224722, 0.017196341620596495, 0.015395848649450605, 0.013487246461545017, 0.015171717191247942, 0.014651223698489035, 0.01542139381681574, 0.015355621950831812, 0.013682435783834374, 0.01269186215088382, 0.015111451782512888, 0.013084366690755068, 0.014330688472686483, 0.013465074247279041, 0.013624831644429052, 0.013566622200448103, 0.01191519109886045, 0.01305888307272846], "accuracy_test_std": 0.00846709877750175, "error_valid": [0.44135036238704817, 0.3513124764683735, 0.30690682652484935, 0.2767539886106928, 0.2562961808170181, 0.24378323842243976, 0.23375288262424698, 0.23008047816265065, 0.2295819018260542, 0.2263565982680723, 0.22296951477786142, 0.22827001364834332, 0.23616340361445776, 0.23473974021084332, 0.2414227221385542, 0.24622464467243976, 0.24666145048945776, 0.24279638083584332, 0.2587169968938253, 0.24307140672063254, 0.24552310805722888, 0.25669180628765065, 0.23522802146084332, 0.24035497458584332, 0.26231880647590367, 0.24529955760542166, 0.2363472444465362, 0.26131135871611444, 0.2361545792545181, 0.23515595585466864, 0.23757824265813254, 0.23645901967243976, 0.2322674487010542, 0.2370796663215362, 0.2314835513930723, 0.23325430628765065, 0.23728262660015065, 0.23852391989834332, 0.2316056217055723, 0.23825918910015065, 0.23128059111445776, 0.2306805346385542, 0.23511624623493976, 0.23250129423945776, 0.24540103774472888, 0.23499417592243976, 0.22575654179216864, 0.22716108574924698, 0.22678310899849397, 0.22331513554216864, 0.2295304263930723, 0.2291450960090362, 0.22814794333584332, 0.2279435123305723, 0.23829007435993976, 0.2263374788215362, 0.22451524849397586, 0.22598009224397586, 0.2337322924510542, 0.2342205737010542, 0.23078201477786142], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.05576666946440106, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 16, "valid_ratio": 0.15, "learning_rate": 0.00082896033101594, "optimization": "adam", "nb_data_augmentation": 0, "learning_rate_decay_method": "discrete", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 2.0960946237915485e-08, "rotation_range": [0, 0], "momentum": 0.6601042501363665}, "accuracy_valid_max": 0.7770304852221386, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.7692179852221386, "accuracy_valid_std": [0.025880327623646514, 0.012074931689433802, 0.01275160544691845, 0.01637267920153502, 0.01391510808773615, 0.012647736106888977, 0.015035531372074746, 0.012401196471543098, 0.01407680103000973, 0.015300958367438304, 0.013359575720997883, 0.010961101401339123, 0.016584867455547367, 0.014153378409085872, 0.015652854744554755, 0.014731713770953772, 0.01814496863803862, 0.015876984924351262, 0.012926671566931664, 0.012123078325067947, 0.012220264161326056, 0.020580144386591972, 0.01554622719895087, 0.01281460973290441, 0.012479941323747884, 0.01345714858666222, 0.013050435039308115, 0.011367174590717309, 0.011036498639614877, 0.019752316911541546, 0.010043658131852878, 0.009404770528542347, 0.015933663834966555, 0.008638309605989885, 0.01723966866338226, 0.01577025979898536, 0.01457857670411107, 0.012079946682826049, 0.017719775636881423, 0.01806387689960584, 0.012029263839593227, 0.012664650554361305, 0.01308736641502064, 0.015171063523603048, 0.013094761367693301, 0.011426129726332338, 0.020308854512974265, 0.009437151240405171, 0.02414208679364008, 0.01621235568899301, 0.019175219089601785, 0.01251982740097118, 0.015050740002710557, 0.01594704505527569, 0.017245468351627574, 0.010086329514123906, 0.017253411782535498, 0.02048290640333176, 0.016096659143606324, 0.01820697866165012, 0.01618018465774013], "accuracy_valid": [0.5586496376129518, 0.6486875235316265, 0.6930931734751506, 0.7232460113893072, 0.7437038191829819, 0.7562167615775602, 0.766247117375753, 0.7699195218373494, 0.7704180981739458, 0.7736434017319277, 0.7770304852221386, 0.7717299863516567, 0.7638365963855422, 0.7652602597891567, 0.7585772778614458, 0.7537753553275602, 0.7533385495105422, 0.7572036191641567, 0.7412830031061747, 0.7569285932793675, 0.7544768919427711, 0.7433081937123494, 0.7647719785391567, 0.7596450254141567, 0.7376811935240963, 0.7547004423945783, 0.7636527555534638, 0.7386886412838856, 0.7638454207454819, 0.7648440441453314, 0.7624217573418675, 0.7635409803275602, 0.7677325512989458, 0.7629203336784638, 0.7685164486069277, 0.7667456937123494, 0.7627173733998494, 0.7614760801016567, 0.7683943782944277, 0.7617408108998494, 0.7687194088855422, 0.7693194653614458, 0.7648837537650602, 0.7674987057605422, 0.7545989622552711, 0.7650058240775602, 0.7742434582078314, 0.772838914250753, 0.773216891001506, 0.7766848644578314, 0.7704695736069277, 0.7708549039909638, 0.7718520566641567, 0.7720564876694277, 0.7617099256400602, 0.7736625211784638, 0.7754847515060241, 0.7740199077560241, 0.7662677075489458, 0.7657794262989458, 0.7692179852221386], "seed": 928936795, "model": "residualv3", "loss_std": [0.33215126395225525, 0.27256134152412415, 0.2690959572792053, 0.2656010389328003, 0.2627757787704468, 0.25759994983673096, 0.25372037291526794, 0.24833497405052185, 0.24292300641536713, 0.2385147362947464, 0.2306871861219406, 0.22517338395118713, 0.2178916335105896, 0.2104717642068863, 0.2009696364402771, 0.19473280012607574, 0.18644483387470245, 0.1836654096841812, 0.1754893958568573, 0.1722366362810135, 0.16160033643245697, 0.16071201860904694, 0.15533235669136047, 0.14884579181671143, 0.14823655784130096, 0.14129815995693207, 0.14120981097221375, 0.13151931762695312, 0.13635747134685516, 0.12761647999286652, 0.12155793607234955, 0.12440868467092514, 0.11745815724134445, 0.11793016642332077, 0.12270866334438324, 0.11228027939796448, 0.1088331937789917, 0.11663687229156494, 0.1108478382229805, 0.10971587151288986, 0.11152901500463486, 0.10300008952617645, 0.10486330837011337, 0.1034165471792221, 0.10052729398012161, 0.09979596734046936, 0.09278086572885513, 0.10039160400629044, 0.09617763012647629, 0.09249049425125122, 0.09526190161705017, 0.09527458250522614, 0.09633906930685043, 0.09553992003202438, 0.09576599299907684, 0.09634730219841003, 0.09225157648324966, 0.09076645970344543, 0.0917016789317131, 0.08454425632953644, 0.08423493057489395]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:34 2016", "state": "available"}], "summary": "acb287bb7dc65a74d9887d6c7b0642b7"}