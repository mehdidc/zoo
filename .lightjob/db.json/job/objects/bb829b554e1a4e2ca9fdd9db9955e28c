{"content": {"hp_model": {"f0": 16, "f1": 16, "f2": 32, "f3": 32, "nonlin": "very_leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [2.327225685119629, 2.2163734436035156, 2.140606641769409, 2.046199083328247, 1.9575918912887573, 1.8835002183914185, 1.8270171880722046, 1.7841320037841797, 1.7502915859222412, 1.7229385375976562, 1.70017409324646, 1.680912971496582, 1.6643850803375244, 1.6499282121658325, 1.637058973312378, 1.625537395477295, 1.6151466369628906, 1.6058006286621094, 1.5973666906356812, 1.5896912813186646, 1.5827100276947021, 1.5763812065124512, 1.5705643892288208, 1.5652567148208618, 1.5604013204574585, 1.5559600591659546, 1.551885724067688, 1.5481311082839966, 1.5446878671646118, 1.5415295362472534, 1.5386343002319336, 1.5359752178192139, 1.5335283279418945, 1.5312861204147339, 1.529219627380371, 1.5273288488388062, 1.52558434009552, 1.5239818096160889, 1.5225095748901367, 1.521152138710022, 1.5199024677276611, 1.5187506675720215, 1.5176936388015747, 1.5167158842086792, 1.515813946723938, 1.5149818658828735, 1.5142126083374023, 1.5135020017623901, 1.512846827507019, 1.512243390083313, 1.5116864442825317, 1.5111730098724365, 1.510701298713684, 1.510265588760376, 1.5098628997802734, 1.5094915628433228, 1.5091499090194702, 1.5088343620300293, 1.5085437297821045, 1.508275032043457, 1.5080277919769287, 1.5077993869781494, 1.5075887441635132, 1.5073951482772827, 1.5072160959243774, 1.5070513486862183, 1.5068995952606201, 1.5067596435546875, 1.506630778312683, 1.5065122842788696, 1.5064034461975098, 1.5063031911849976, 1.5062109231948853, 1.5061262845993042, 1.5060484409332275, 1.505976915359497, 1.5059114694595337, 1.5058515071868896, 1.505796194076538, 1.505745768547058, 1.5056993961334229, 1.5056568384170532, 1.5056182146072388, 1.505582571029663, 1.5055502653121948, 1.5055207014083862, 1.5054937601089478, 1.5054693222045898, 1.5054471492767334, 1.5054271221160889, 1.5054088830947876, 1.5053924322128296, 1.5053776502609253, 1.5053645372390747, 1.50535249710083, 1.50534188747406, 1.505332350730896, 1.505323886871338, 1.5053162574768066, 1.5053095817565918, 1.5053038597106934, 1.5052987337112427, 1.5052942037582397, 1.5052903890609741, 1.5052869319915771, 1.5052838325500488, 1.5052813291549683, 1.5052789449691772, 1.5052770376205444, 1.5052751302719116, 1.5052738189697266, 1.505272626876831, 1.5052714347839355, 1.5052704811096191, 1.5052695274353027, 1.5052690505981445, 1.5052682161331177, 1.5052677392959595, 1.5052672624588013, 1.5052669048309326, 1.505266547203064, 1.5052663087844849, 1.5052659511566162, 1.505265712738037, 1.5052655935287476, 1.505265474319458, 1.5052653551101685, 1.505265235900879, 1.5052651166915894, 1.5052651166915894, 1.5052649974822998, 1.5052648782730103, 1.5052648782730103, 1.5052647590637207, 1.5052647590637207, 1.5052646398544312, 1.5052646398544312, 1.5052646398544312, 1.5052646398544312, 1.5052646398544312, 1.5052646398544312, 1.5052646398544312, 1.5052646398544312, 1.5052646398544312, 1.5052646398544312, 1.5052646398544312, 1.5052646398544312, 1.5052646398544312, 1.5052646398544312, 1.5052646398544312, 1.5052645206451416, 1.5052646398544312, 1.5052645206451416, 1.5052645206451416], "moving_avg_accuracy_train": [0.017165438122923583, 0.03696349624515503, 0.05780819108469452, 0.07967892481514163, 0.10202423168071126, 0.12475959618221377, 0.14678367189183478, 0.16799567087213266, 0.1882583810032195, 0.2073643897271408, 0.22522926019171907, 0.241768203318219, 0.2572181190975082, 0.27156939977265954, 0.2848411559528779, 0.2971646276245705, 0.30856267177195107, 0.3190789669736319, 0.32885287744681135, 0.33790283809291094, 0.346261680316058, 0.3540448386906445, 0.3612146225979736, 0.36792551963242687, 0.37416532580987266, 0.3799414784909747, 0.38531440206468087, 0.39027094101911164, 0.3948805274554526, 0.39912913664696903, 0.4030645281110097, 0.4066924469834174, 0.41007147021143225, 0.4131846346809221, 0.4160701880606058, 0.41873922966659766, 0.42117620829531444, 0.4234322680790167, 0.4255115500093487, 0.42742472037640017, 0.4291814869877081, 0.4308207056581233, 0.43236800997695474, 0.43378383535199827, 0.4350998948192902, 0.436328490118415, 0.4374621276733417, 0.4384893769192043, 0.43945578996787077, 0.4403511022997566, 0.44114990795202536, 0.44187580848549574, 0.4425476841072765, 0.44317326245734623, 0.4437479087164566, 0.44427206579608447, 0.44473915687013055, 0.4451618639855815, 0.44555392613353495, 0.44590445691788355, 0.44624780036069284, 0.4465684352032688, 0.4468756077520633, 0.447159038492407, 0.4474118010099068, 0.44764858787089473, 0.44786867149221243, 0.44807601129781766, 0.44826959256929094, 0.4484438157136169, 0.44859829139470075, 0.44873964465648575, 0.4488691877409018, 0.4489811262192571, 0.44908419599858646, 0.4491792839487924, 0.4492671882527873, 0.44934630212638266, 0.4494175046126185, 0.44948623714784974, 0.449552746727177, 0.44961028019976196, 0.449669035771517, 0.4497242409349061, 0.44977625073076577, 0.44982073439823, 0.44986076969894784, 0.44989447632078433, 0.4499248122804372, 0.4499521146441247, 0.4499766867714435, 0.45000112683483995, 0.4500231228918968, 0.4500452444920574, 0.4500628287833925, 0.45007865464559405, 0.4500928979215755, 0.4501033917211492, 0.45011051099195604, 0.4501169183356821, 0.4501250100938452, 0.4501322926761919, 0.450138847000304, 0.4501470710408144, 0.4501521475284642, 0.45015671636734905, 0.4501608283223454, 0.4501645290818421, 0.4501678597653892, 0.45017085738058155, 0.4501735552342547, 0.4501759833025605, 0.4501781685640357, 0.4501801352993634, 0.4501819053611583, 0.45018349841677374, 0.45018493216682764, 0.4501862225418761, 0.45018738387941976, 0.45018842908320905, 0.4501893697666194, 0.4501902163816887, 0.4501909783352511, 0.4501916640934573, 0.4501922812758428, 0.4501928367399898, 0.4501956618065316, 0.4501982043664192, 0.4502004926703181, 0.4502025521438271, 0.45020208052117566, 0.4502016560607894, 0.4502012740464417, 0.4502009302335288, 0.4502006208019072, 0.45020034231344774, 0.45020009167383424, 0.4501998660981821, 0.4501996630800951, 0.45019948036381685, 0.45019931591916645, 0.45019916791898107, 0.45019903471881423, 0.4501989148386641, 0.4501988069465289, 0.45019870984360727, 0.4501986224509778, 0.45019854379761126, 0.4501984730095814, 0.4501984093003545, 0.4501983519620503, 0.4501983003575766, 0.4501982539135502, 0.4501982121139264], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.01766430958207831, 0.03776258353727409, 0.05902397578595632, 0.08069932081842995, 0.10313799542898153, 0.1260468815506165, 0.14773306783079582, 0.16810203914447827, 0.18701593279215395, 0.20515045350013733, 0.22243087713543985, 0.2385346338307814, 0.2535560057263177, 0.2678636969929179, 0.28129611259106585, 0.29376679312412796, 0.30517351107263385, 0.3156735498373584, 0.3253188972256105, 0.3341573717726278, 0.34239379009235293, 0.34996731700367484, 0.35700321778636457, 0.3635053974196257, 0.3694794294020607, 0.3749282708650926, 0.37991870690923096, 0.38464409196002475, 0.38890914553698913, 0.3927731373274167, 0.3964104508537111, 0.3997572752148762, 0.40289148745242476, 0.4058242007647877, 0.4085490919646643, 0.4110014940445533, 0.41318218283663405, 0.4152302519682568, 0.41718543648528655, 0.4190793798943633, 0.42082157956494204, 0.4223651452059629, 0.42386524707279133, 0.4251909246904369, 0.4264206556400679, 0.4275640345884858, 0.4286419037670619, 0.42958654245662076, 0.43049878194213337, 0.4313197974790947, 0.43210753958736, 0.4328531285785487, 0.4334997446081185, 0.43410611309723135, 0.4346518447374329, 0.4351552102448643, 0.43568354040637186, 0.43614683052047865, 0.43657599865442476, 0.43696224997497624, 0.4373220831947226, 0.4376459330924943, 0.4379496050317388, 0.43822290977705886, 0.4384566770165969, 0.43867927456343114, 0.438891819386832, 0.4391075237903928, 0.4393016577535975, 0.4394885853517317, 0.4396446131588025, 0.43978503818516623, 0.4398992136776436, 0.4400263856833732, 0.44012863345727987, 0.4402206564537959, 0.44030347715066026, 0.4403780157778382, 0.44044510054229835, 0.4405054768303125, 0.4405598154895252, 0.4405965132515667, 0.440617334206154, 0.44063607306528263, 0.44065293803849837, 0.4406803235456425, 0.44070497050207225, 0.440727152762859, 0.4407471167975671, 0.4407772914600544, 0.4408044486562929, 0.4408288901329076, 0.4408630944931108, 0.44089387841729366, 0.4409215839490583, 0.4409465189276464, 0.44096896040837574, 0.44098915774103215, 0.4410073353404229, 0.4410236951798746, 0.4410384190353811, 0.44105167050533695, 0.4410635968282972, 0.44108653755021143, 0.4411071841999342, 0.44112576618468474, 0.4411424899709602, 0.44115754137860813, 0.4411710876454913, 0.4411832792856861, 0.44119425176186144, 0.4412041269904193, 0.4412130146961213, 0.4412210136312531, 0.44122821267287177, 0.44123469181032854, 0.44124052303403966, 0.44124577113537966, 0.44125049442658565, 0.44125474538867104, 0.44125857125454787, 0.441262014533837, 0.44126511348519726, 0.4412679025414215, 0.44127041269202333, 0.44127267182756497, 0.4412747050495524, 0.4412765349493411, 0.44127818185915096, 0.4412796640779798, 0.44128099807492577, 0.44128219867217716, 0.4412832792097034, 0.441284251693477, 0.4412851269288733, 0.4412859146407299, 0.44128662358140086, 0.44128726162800475, 0.44128783586994824, 0.4412883526876974, 0.44128881782367163, 0.4412892364460484, 0.4412896132061875, 0.4412899522903127, 0.4412902574660254, 0.4412905321241668, 0.4412907793164941, 0.44129100178958863, 0.4412912020153737, 0.4412913822185803, 0.4412915444014662, 0.4412916903660635, 0.4412918217342011, 0.44129193996552496], "moving_var_accuracy_train": [0.0026518703935672653, 0.005914351302911831, 0.009233427899202368, 0.01261504605445519, 0.015847356099257928, 0.018914691680477762, 0.021388761710198045, 0.023299425645839665, 0.024664679877963745, 0.025483568014394387, 0.0258075935834004, 0.025688663982734755, 0.02526809666274545, 0.024594920309623663, 0.023720683887625774, 0.0227154270852503, 0.02161312307019599, 0.020447142946096453, 0.019262192584926348, 0.018073089415697276, 0.016894612663930747, 0.01575034938609274, 0.01463796665898362, 0.013579495244148563, 0.012571962349922519, 0.01161504157308889, 0.010713352185339987, 0.009863122472467097, 0.009068044809247276, 0.008323696448882686, 0.007630712557771613, 0.0069860974600974, 0.0063902478958668355, 0.0058384492434069984, 0.005329542083829336, 0.004860701923297042, 0.004428081514498738, 0.004031081614777611, 0.0036668841734120968, 0.0033331377437511617, 0.0030276000297155027, 0.002749023367388894, 0.0024956683865456743, 0.0022641426013246617, 0.0020533164538852327, 0.0018615698261779954, 0.0016869790505136572, 0.0015277783145804186, 0.001383406070748073, 0.001252279721217908, 0.0011327945633269852, 0.0010242574912547205, 0.0009258944937895387, 0.0008368271788592681, 0.0007561164258813269, 0.0006829774490903104, 0.0006166432708243607, 0.0005565870754910005, 0.0005023117824926216, 0.00045318645072034387, 0.00040892876812579065, 0.0003689611516336751, 0.00033291423124290413, 0.00030034580497975965, 0.0002708862244940592, 0.0002443022142024819, 0.00022030792398558446, 0.00019866403974192186, 0.00017913489914571669, 0.0001614945925673145, 0.00014555989793499998, 0.00013118373484305532, 0.00011821639405522994, 0.0001065075266561356, 9.595238440522108e-05, 8.64385214291683e-05, 7.78642137861989e-05, 7.013412345253643e-05, 6.316633925369829e-05, 5.689222278092239e-05, 5.124281222011074e-05, 4.6148321902308865e-05, 4.156455966698837e-05, 3.7435532190872876e-05, 3.37163241415739e-05, 3.0362500897456107e-05, 2.7340676235442567e-05, 2.4616833839098957e-05, 2.2163432889221588e-05, 1.9953798371865773e-05, 1.7963852639647936e-05, 1.6172843225972544e-05, 1.4559913342109702e-05, 1.3108326294641734e-05, 1.1800276530893364e-05, 1.0622502999033827e-05, 9.562078537326581e-06, 8.606861762059363e-06, 7.746631742004815e-06, 6.972338054286958e-06, 6.275693537809786e-06, 5.648601508079542e-06, 5.084127989752683e-06, 4.576323904358264e-06, 4.118923450464167e-06, 3.7072189740165517e-06, 3.3366492501799256e-06, 3.0031075857496063e-06, 2.7028966682506614e-06, 2.4326878726971675e-06, 2.1894845911574245e-06, 1.970589191682961e-06, 1.7735732508241006e-06, 1.5962507381723334e-06, 1.4366538624239208e-06, 1.2930113166172735e-06, 1.1637286857084996e-06, 1.0473708027475418e-06, 9.426458608168003e-07, 8.483911067937707e-07, 7.635599600819004e-07, 6.872104148873908e-07, 6.18494598557733e-07, 5.566493710808154e-07, 5.00987862199607e-07, 4.5089185284341355e-07, 4.058744965677632e-07, 3.653452284080266e-07, 3.288578325798261e-07, 2.9601022220205116e-07, 2.6641120183317425e-07, 2.3977170314943267e-07, 2.1579584624914585e-07, 1.94217325490103e-07, 1.747964546724488e-07, 1.5731750720760237e-07, 1.4158632186878487e-07, 1.2742814764128001e-07, 1.1468570382424464e-07, 1.032174339089652e-07, 9.289593389645615e-08, 8.360653764330439e-08, 7.524604355953397e-08, 6.77215685448342e-08, 6.09495164567662e-08, 5.4854649671886076e-08, 4.9369253441942625e-08, 4.443238377491694e-08, 3.998919049593179e-08, 3.599030797612891e-08, 3.239130676764616e-08, 2.915220005807696e-08, 2.623699946569755e-08, 2.3613315244004707e-08], "duration": 33503.636185, "accuracy_train": [0.17165438122923588, 0.21514601934523808, 0.24541044464055, 0.27651552838916577, 0.3031319934708379, 0.3293778766957364, 0.34500035327842377, 0.3589036616948136, 0.3706227721830011, 0.3793184682424326, 0.3860130943729236, 0.39061869145671835, 0.39626736111111116, 0.4007309258490218, 0.40428696157484306, 0.4080758726698044, 0.4111450690983758, 0.4137256237887597, 0.41681807170542634, 0.4193524839078073, 0.42149126032438167, 0.4240932640619232, 0.425742677763935, 0.42832359294250644, 0.4303235814068844, 0.43192685262089336, 0.4336707142280362, 0.4348797916089886, 0.43636680538252126, 0.4373666193706165, 0.4384830512873754, 0.4393437168350868, 0.44048267926356593, 0.44120311490633074, 0.44204016847775934, 0.4427606041205242, 0.44310901595376523, 0.4437368061323366, 0.4442250873823366, 0.4446432536798634, 0.4449923864894795, 0.44557367369186046, 0.44629374884643774, 0.4465262637273901, 0.44694443002491696, 0.447385847810539, 0.4476648656676818, 0.4477346201319675, 0.44815350740586934, 0.44840891328672944, 0.44833915882244374, 0.44840891328672944, 0.44859456470330383, 0.4488034676079734, 0.4489197250484496, 0.4489894795127353, 0.44894297653654486, 0.4489662280246401, 0.4490824854651163, 0.4490592339770211, 0.44933789134597635, 0.44945414878645257, 0.4496401606912145, 0.4497099151555002, 0.44968666366740495, 0.4497796696197859, 0.44984942408407164, 0.44994206954826504, 0.45001182401255074, 0.45001182401255074, 0.4499885725244555, 0.45001182401255074, 0.450035075500646, 0.4499885725244555, 0.45001182401255074, 0.450035075500646, 0.4500583269887412, 0.4500583269887412, 0.4500583269887412, 0.4501048299649317, 0.4501513329411222, 0.45012808145302696, 0.45019783591731266, 0.4502210874054079, 0.4502443388935031, 0.4502210874054079, 0.4502210874054079, 0.45019783591731266, 0.45019783591731266, 0.45019783591731266, 0.45019783591731266, 0.4502210874054079, 0.4502210874054079, 0.4502443388935031, 0.4502210874054079, 0.4502210874054079, 0.4502210874054079, 0.45019783591731266, 0.45017458442921743, 0.45017458442921743, 0.45019783591731266, 0.45019783591731266, 0.45019783591731266, 0.4502210874054079, 0.45019783591731266, 0.45019783591731266, 0.45019783591731266, 0.45019783591731266, 0.45019783591731266, 0.45019783591731266, 0.45019783591731266, 0.45019783591731266, 0.45019783591731266, 0.45019783591731266, 0.45019783591731266, 0.45019783591731266, 0.45019783591731266, 0.45019783591731266, 0.45019783591731266, 0.45019783591731266, 0.45019783591731266, 0.45019783591731266, 0.45019783591731266, 0.45019783591731266, 0.45019783591731266, 0.45019783591731266, 0.4502210874054079, 0.4502210874054079, 0.4502210874054079, 0.4502210874054079, 0.45019783591731266, 0.45019783591731266, 0.45019783591731266, 0.45019783591731266, 0.45019783591731266, 0.45019783591731266, 0.45019783591731266, 0.45019783591731266, 0.45019783591731266, 0.45019783591731266, 0.45019783591731266, 0.45019783591731266, 0.45019783591731266, 0.45019783591731266, 0.45019783591731266, 0.45019783591731266, 0.45019783591731266, 0.45019783591731266, 0.45019783591731266, 0.45019783591731266, 0.45019783591731266, 0.45019783591731266, 0.45019783591731266, 0.45019783591731266], "end": "2016-02-01 19:06:28.536000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0], "moving_var_accuracy_valid": [0.002808250497103533, 0.0061628909911960895, 0.009615023095247404, 0.012881906026213683, 0.016125162488096225, 0.019235999809292963, 0.021545015906771405, 0.02312456924749284, 0.024031730678983825, 0.02458830518285995, 0.024816992033713656, 0.02466927164766711, 0.024233119005516474, 0.023652197369387944, 0.02291084573166147, 0.02201942201511457, 0.02098849874281401, 0.019881906195079076, 0.018731010111731724, 0.017560976791423, 0.01641542639291824, 0.015290108542515075, 0.014206632786678256, 0.013166474567859193, 0.012171028634217682, 0.011221134630399213, 0.010323161234555006, 0.009491808486003893, 0.008706343775532895, 0.007970083290988032, 0.007292145409086312, 0.006663741967918069, 0.006085777348276253, 0.005554606879801226, 0.00506597148028159, 0.004613502815906426, 0.004194951166786946, 0.0038132073346194045, 0.0034662913196181594, 0.0031519453823874114, 0.002864068181378152, 0.002599104717233598, 0.002359446996007997, 0.002139319086720535, 0.001938997321924805, 0.0017568634285094918, 0.0015916333033536617, 0.001440501053302599, 0.0013039405758826941, 0.0011796131169018122, 0.0010672366438738382, 0.0009655161059804904, 0.0008727275059897112, 0.0007887639000920411, 0.0007125679172908908, 0.0006435915170684469, 0.0005817445601976295, 0.0005255018437463284, 0.00047460932695644865, 0.0004284911050044538, 0.0003868073140183057, 0.0003490704914230559, 0.0003149933921009112, 0.00028416631224515045, 0.0002562415051211667, 0.00023106330161975987, 0.00020836354917537443, 0.0001879459497652766, 0.000169490546749775, 0.00015285596941729557, 0.00013778947456477995, 0.00012418799980056525, 0.00011188652420825073, 0.0001008434262587973, 9.085317509833737e-05, 8.184407167549366e-05, 7.372139791840623e-05, 6.639926218903978e-05, 5.979983926073987e-05, 5.3852663000055185e-05, 4.8493970909015e-05, 4.365669434976316e-05, 3.929492652413619e-05, 3.536859417529554e-05, 3.1834294603660085e-05, 2.8657614837307952e-05, 2.5797320605728395e-05, 2.3222017019398056e-05, 2.0903402381594678e-05, 1.8821256735741195e-05, 1.6945768681934922e-05, 1.5256568285753385e-05, 1.374144090149025e-05, 1.2375825661234111e-05, 1.114515146352394e-05, 1.0036232095586269e-05, 9.037141466543568e-06, 8.13709871010711e-06, 7.326362665172898e-06, 6.596135197777573e-06, 5.9384728052886065e-06, 5.3462059378636665e-06, 4.812865478691477e-06, 4.336315421319854e-06, 3.906520436490866e-06, 3.518976004257209e-06, 3.1695955690779864e-06, 2.8546749160198524e-06, 2.5708589365360964e-06, 2.3151107676982523e-06, 2.084683248029198e-06, 1.877092604477903e-06, 1.690094265843929e-06, 1.521660685928728e-06, 1.3699610531379009e-06, 1.233342760823768e-06, 1.1103145132711137e-06, 9.995309450530775e-07, 8.997786358661208e-07, 8.099634083873733e-07, 7.290988027960064e-07, 6.562956280667763e-07, 5.907524967558989e-07, 5.317472565919074e-07, 4.786292386371112e-07, 4.3081224801395977e-07, 3.8776822913741724e-07, 3.490215430228069e-07, 3.141437995278227e-07, 2.827491923289507e-07, 2.54490289026723e-07, 2.290542330278913e-07, 2.061593177772131e-07, 1.855518975217017e-07, 1.6700360210252155e-07, 1.5030882630199134e-07, 1.3528246704366702e-07, 1.2175788427051895e-07, 1.0958506362775414e-07, 9.862896117025127e-08, 8.87680122164969e-08, 7.989278819709656e-08, 7.19047869112089e-08, 6.471534302248385e-08, 5.824464691017609e-08, 5.242086115301041e-08, 4.7179324974129446e-08, 4.2461837925216766e-08, 3.821601494598031e-08, 3.439470571014333e-08, 3.095547186872546e-08, 2.786011643282605e-08, 2.5074260107831695e-08, 2.2566959904862014e-08], "accuracy_test": 0.4411491549744898, "start": "2016-02-01 09:48:04.900000", "learning_rate_per_epoch": [0.001115501974709332, 0.0010288392659276724, 0.0009489093790762126, 0.0008751892019063234, 0.0008071963093243539, 0.0007444857037626207, 0.0006866470794193447, 0.0006333019118756056, 0.0005841010715812445, 0.0005387226119637489, 0.0004968695575371385, 0.00045826807036064565, 0.0004226655000820756, 0.00038982887053862214, 0.0003595433081500232, 0.0003316106158308685, 0.0003058479924220592, 0.00028208683943375945, 0.00026017168420366943, 0.00023995910305529833, 0.00022131683363113552, 0.00020412287267390639, 0.00018826469022314996, 0.00017363851657137275, 0.0001601486437721178, 0.00014770679990760982, 0.0001362315524602309, 0.00012564781354740262, 0.000115886316052638, 0.00010688317706808448, 9.857949044089764e-05, 9.092091204365715e-05, 8.385732508031651e-05, 7.73424981161952e-05, 7.133380859158933e-05, 6.579192995559424e-05, 6.0680595197482035e-05, 5.596635674010031e-05, 5.161836452316493e-05, 4.7608165914425626e-05, 4.3909516534768045e-05, 4.04982129111886e-05, 3.7351932405726984e-05, 3.445008405833505e-05, 3.1773677619639784e-05, 2.9305199859663844e-05, 2.702849633351434e-05, 2.4928669517976232e-05, 2.299197694810573e-05, 2.1205743905738927e-05, 1.955828338395804e-05, 1.8038812413578853e-05, 1.663738839852158e-05, 1.5344839994213544e-05, 1.4152708899928257e-05, 1.305319347011391e-05, 1.203909869218478e-05, 1.1103788892796729e-05, 1.0241142263112124e-05, 9.445514479011763e-06, 8.711698683327995e-06, 8.034891834540758e-06, 7.4106660576944705e-06, 6.834935902588768e-06, 6.303933787421556e-06, 5.814184987684712e-06, 5.362484444049187e-06, 4.94587629873422e-06, 4.561634341371246e-06, 4.207243819109863e-06, 3.88038552046055e-06, 3.578920768632088e-06, 3.300876642242656e-06, 3.0444336971413577e-06, 2.8079134608560707e-06, 2.5897684281517286e-06, 2.388570919720223e-06, 2.2030042146070627e-06, 2.0318541373853805e-06, 1.874000645329943e-06, 1.7284106661463738e-06, 1.594131504134566e-06, 1.4702843600389315e-06, 1.3560588740801904e-06, 1.2507075553003233e-06, 1.1535408930285485e-06, 1.063923036781489e-06, 9.812674761633389e-07, 9.050334028870566e-07, 8.347219022653007e-07, 7.69872883665812e-07, 7.100619541233755e-07, 6.548976898557157e-07, 6.040190783096477e-07, 5.570931875809038e-07, 5.138129495207977e-07, 4.738951417948556e-07, 4.3707851204999315e-07, 4.031221578770783e-07, 3.7180384993007465e-07, 3.429186392622796e-07, 3.1627749308427155e-07, 2.917061010521138e-07, 2.690436247121397e-07, 2.4814178800625086e-07, 2.2886381145781343e-07, 2.1108351688781113e-07, 1.9468457423954533e-07, 1.795596489273521e-07, 1.6560977655899478e-07, 1.527436523929282e-07, 1.408770913258195e-07, 1.2993243103665009e-07, 1.1983806302851008e-07, 1.1052792103782849e-07, 1.019410760250139e-07, 9.402133827052239e-08, 8.671688078720763e-08, 7.997989825980767e-08, 7.376630861699596e-08, 6.803544749800494e-08, 6.27498124572412e-08, 5.7874817827041625e-08, 5.337856023857057e-08, 4.923161256442654e-08, 4.5406839177530856e-08, 4.1879211210016365e-08, 3.8625639575684545e-08, 3.562483641417202e-08, 3.285716587697607e-08, 3.030451267704848e-08, 2.795017373102837e-08, 2.577874269604763e-08, 2.377600871739105e-08, 2.192886583429754e-08, 2.0225225938474978e-08, 1.865394061439929e-08, 1.7204728308684025e-08, 1.586810327580679e-08, 1.4635320511047212e-08, 1.3498311801640739e-08, 1.2449635988787122e-08, 1.1482431894194178e-08, 1.0590368582086285e-08, 9.767609832067592e-09, 9.008770618379458e-09, 8.30888513547734e-09, 7.663373047250843e-09, 7.068010177135875e-09, 6.51890097458363e-09, 6.012451425618792e-09, 5.54534773655746e-09, 5.114533241368235e-09, 4.7171884176577805e-09], "accuracy_train_first": 0.17165438122923588, "accuracy_train_last": 0.45019783591731266, "batch_size_eval": 1024, "accuracy_train_std": [0.009228194271550786, 0.011814250189231603, 0.015514694414200494, 0.015129350775003565, 0.016012804768100134, 0.018397522516382235, 0.01742748969431273, 0.016344222360300423, 0.01668163232203501, 0.01575796206882777, 0.015322096998150151, 0.015396453427413775, 0.015469485279303318, 0.015549782302679668, 0.015520644430220892, 0.015656195296072907, 0.015441190722280654, 0.015441379057715775, 0.0159289405893503, 0.01584675386378899, 0.01621031902623156, 0.017127355568903928, 0.017018865861775936, 0.017102986867304215, 0.01649856108446463, 0.01668825074670004, 0.016186182108656688, 0.015976803457078387, 0.015997190483467316, 0.015566106288454304, 0.015295381750136915, 0.015160850415464534, 0.015002167937012291, 0.015305630391155761, 0.01530571910764059, 0.015446080789063732, 0.015741777936147172, 0.015757266935192783, 0.015915180515450277, 0.01620810572683325, 0.016314338912374715, 0.0164118287514463, 0.01648057606929093, 0.016456808199413284, 0.01648396805551588, 0.016328023815032237, 0.01631941123109669, 0.01644300768654113, 0.016376049919056047, 0.016418907850839114, 0.016388323134308005, 0.016296753833140425, 0.016429559761446017, 0.01645726354670432, 0.016596480552406846, 0.016506363894665536, 0.01634946136399555, 0.016440551272807837, 0.01638312761087578, 0.01642790058143206, 0.0163946481667597, 0.016352479482765227, 0.016300059928056727, 0.01633498526415351, 0.016409864181145654, 0.016357049995865215, 0.016284244100391393, 0.01641879713087261, 0.016504552903593525, 0.016504552903593525, 0.016473293515403057, 0.016477014453286046, 0.016458642747097966, 0.016474671842173563, 0.01642594660816888, 0.01639090238531686, 0.016478844660691924, 0.016437455078893277, 0.016411187686441227, 0.016396283589008718, 0.01635765272475062, 0.016305435504786064, 0.016250474598321206, 0.016228781969726984, 0.016269951147088978, 0.01625115298137064, 0.01625115298137064, 0.016254665910400645, 0.016254665910400645, 0.016254665910400645, 0.01619308460734261, 0.0161614831377571, 0.0161614831377571, 0.01614526394177583, 0.016176930515445543, 0.016176930515445543, 0.016176930515445543, 0.01615658532830251, 0.016151652561457463, 0.016151652561457463, 0.01615658532830251, 0.01615658532830251, 0.01615658532830251, 0.01616288805488342, 0.016159395892052772, 0.016159395892052772, 0.016159395892052772, 0.016159395892052772, 0.016159395892052772, 0.016159395892052772, 0.016159395892052772, 0.016159395892052772, 0.016159395892052772, 0.016159395892052772, 0.016159395892052772, 0.016159395892052772, 0.016159395892052772, 0.016159395892052772, 0.016159395892052772, 0.016159395892052772, 0.016159395892052772, 0.016159395892052772, 0.016159395892052772, 0.016159395892052772, 0.016159395892052772, 0.016159395892052772, 0.016189558315955605, 0.016189558315955605, 0.016189558315955605, 0.016189558315955605, 0.016246282204942297, 0.016246282204942297, 0.016246282204942297, 0.016246282204942297, 0.016246282204942297, 0.016246282204942297, 0.016246282204942297, 0.016246282204942297, 0.016246282204942297, 0.016246282204942297, 0.016246282204942297, 0.016246282204942297, 0.016246282204942297, 0.016246282204942297, 0.016246282204942297, 0.016246282204942297, 0.016246282204942297, 0.016246282204942297, 0.016246282204942297, 0.016246282204942297, 0.016246282204942297, 0.016246282204942297, 0.016246282204942297, 0.016246282204942297], "accuracy_test_std": 0.010812045562256421, "error_valid": [0.8233569041792168, 0.7813529508659638, 0.7496234939759037, 0.7242225738893072, 0.6949139330760542, 0.6677731433546686, 0.6570912556475903, 0.6485772190323795, 0.6427590243787651, 0.6316388601280121, 0.6220453101468373, 0.6165315559111446, 0.6112516472138554, 0.6033670816076807, 0.5978121470256024, 0.5939970820783133, 0.5921660273908133, 0.5898261012801205, 0.5878729762801205, 0.5862963573042168, 0.5834784450301205, 0.5818709407944277, 0.5796736751694277, 0.5779749858810241, 0.5767542827560241, 0.5760321559676205, 0.5751673686935241, 0.5728274425828314, 0.5727053722703314, 0.5724509365587349, 0.5708537274096386, 0.5701213055346386, 0.5689006024096386, 0.5677813794239458, 0.5669268872364458, 0.5669268872364458, 0.5671916180346386, 0.5663371258471386, 0.5652179028614458, 0.5638751294239458, 0.5634986233998494, 0.5637427640248494, 0.562633836125753, 0.562877976750753, 0.562511765813253, 0.562145554875753, 0.561657273625753, 0.5619117093373494, 0.561291062688253, 0.561291062688253, 0.560802781438253, 0.560436570500753, 0.560680711125753, 0.560436570500753, 0.560436570500753, 0.560314500188253, 0.5595614881400602, 0.5596835584525602, 0.5595614881400602, 0.5595614881400602, 0.5594394178275602, 0.5594394178275602, 0.5593173475150602, 0.5593173475150602, 0.5594394178275602, 0.5593173475150602, 0.5591952772025602, 0.5589511365775602, 0.5589511365775602, 0.5588290662650602, 0.5589511365775602, 0.5589511365775602, 0.5590732068900602, 0.5588290662650602, 0.5589511365775602, 0.5589511365775602, 0.5589511365775602, 0.5589511365775602, 0.5589511365775602, 0.5589511365775602, 0.5589511365775602, 0.5590732068900602, 0.5591952772025602, 0.5591952772025602, 0.5591952772025602, 0.5590732068900602, 0.5590732068900602, 0.5590732068900602, 0.5590732068900602, 0.5589511365775602, 0.5589511365775602, 0.5589511365775602, 0.5588290662650602, 0.5588290662650602, 0.5588290662650602, 0.5588290662650602, 0.5588290662650602, 0.5588290662650602, 0.5588290662650602, 0.5588290662650602, 0.5588290662650602, 0.5588290662650602, 0.5588290662650602, 0.5587069959525602, 0.5587069959525602, 0.5587069959525602, 0.5587069959525602, 0.5587069959525602, 0.5587069959525602, 0.5587069959525602, 0.5587069959525602, 0.5587069959525602, 0.5587069959525602, 0.5587069959525602, 0.5587069959525602, 0.5587069959525602, 0.5587069959525602, 0.5587069959525602, 0.5587069959525602, 0.5587069959525602, 0.5587069959525602, 0.5587069959525602, 0.5587069959525602, 0.5587069959525602, 0.5587069959525602, 0.5587069959525602, 0.5587069959525602, 0.5587069959525602, 0.5587069959525602, 0.5587069959525602, 0.5587069959525602, 0.5587069959525602, 0.5587069959525602, 0.5587069959525602, 0.5587069959525602, 0.5587069959525602, 0.5587069959525602, 0.5587069959525602, 0.5587069959525602, 0.5587069959525602, 0.5587069959525602, 0.5587069959525602, 0.5587069959525602, 0.5587069959525602, 0.5587069959525602, 0.5587069959525602, 0.5587069959525602, 0.5587069959525602, 0.5587069959525602, 0.5587069959525602, 0.5587069959525602, 0.5587069959525602, 0.5587069959525602, 0.5587069959525602], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.07768937968695716, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 128, "valid_ratio": 0.15, "learning_rate": 0.0012094644626859418, "optimization": "nesterov_momentum", "nb_data_augmentation": 0, "learning_rate_decay_method": "exp", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 1.232174302163307e-06, "rotation_range": [0, 0], "momentum": 0.6852716614161947}, "accuracy_valid_max": 0.44129300404743976, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.44129300404743976, "accuracy_valid_std": [0.01683464297848697, 0.015606865372852616, 0.015595690875060764, 0.018082981735777985, 0.011515761452594546, 0.01945087414197459, 0.016740837000863873, 0.020283566288285326, 0.02737208600270951, 0.019613778100804755, 0.01814640546240266, 0.01870767059058215, 0.015753459541355184, 0.016335897377047427, 0.017912174311979674, 0.02016911360516546, 0.019251207624509405, 0.01861548827677673, 0.018446370385909034, 0.016135355488400293, 0.01655510550558174, 0.01808838428003008, 0.018542656597104093, 0.018065519051032083, 0.018198415790145195, 0.01709594168451999, 0.016506289078236293, 0.017681034271582602, 0.017381933364199555, 0.017494148856514093, 0.017361064474050236, 0.01709867266405575, 0.016837267254412116, 0.01858532404855273, 0.017894471147296146, 0.01680189107107488, 0.015506810421663684, 0.015376086366896493, 0.016048644754105407, 0.015600816667702041, 0.0158600363417385, 0.015592489407537888, 0.015845582447855926, 0.015901725611676836, 0.01546927720097056, 0.015461365881421208, 0.01536506077867612, 0.015123084523691816, 0.015758412471171192, 0.015545154981431353, 0.01569054767851188, 0.01615268863344384, 0.01637242078728831, 0.015922260333785448, 0.015817095625601293, 0.015766704230582722, 0.016864827894213966, 0.016924427052263182, 0.01687895900402808, 0.01687895900402808, 0.016945417611117782, 0.016945417611117782, 0.016898242493901195, 0.016898242493901195, 0.016874921991646807, 0.016692407273630208, 0.016622118514093134, 0.016578902365731167, 0.016578902365731167, 0.016606228352654923, 0.016657809220379603, 0.016657809220379603, 0.01672972950646615, 0.016620579329816497, 0.016434464763734023, 0.016434464763734023, 0.016434464763734023, 0.016434464763734023, 0.016434464763734023, 0.016434464763734023, 0.016434464763734023, 0.016252637421010196, 0.016332730351429157, 0.016332730351429157, 0.016332730351429157, 0.016296586593929123, 0.016296586593929123, 0.016296586593929123, 0.016296586593929123, 0.016252112711671558, 0.016252112711671558, 0.016252112711671558, 0.016213951302166782, 0.016213951302166782, 0.016213951302166782, 0.016213951302166782, 0.016213951302166782, 0.016213951302166782, 0.016213951302166782, 0.016213951302166782, 0.016213951302166782, 0.016213951302166782, 0.016213951302166782, 0.016182147024480696, 0.016182147024480696, 0.016182147024480696, 0.016182147024480696, 0.016182147024480696, 0.016182147024480696, 0.016182147024480696, 0.016182147024480696, 0.016182147024480696, 0.016182147024480696, 0.016182147024480696, 0.016182147024480696, 0.016182147024480696, 0.016182147024480696, 0.016182147024480696, 0.016182147024480696, 0.016182147024480696, 0.016182147024480696, 0.016182147024480696, 0.016182147024480696, 0.016182147024480696, 0.016182147024480696, 0.016182147024480696, 0.016182147024480696, 0.016182147024480696, 0.016182147024480696, 0.016182147024480696, 0.016182147024480696, 0.016182147024480696, 0.016182147024480696, 0.016182147024480696, 0.016182147024480696, 0.016182147024480696, 0.016182147024480696, 0.016182147024480696, 0.016182147024480696, 0.016182147024480696, 0.016182147024480696, 0.016182147024480696, 0.016182147024480696, 0.016182147024480696, 0.016182147024480696, 0.016182147024480696, 0.016182147024480696, 0.016182147024480696, 0.016182147024480696, 0.016182147024480696, 0.016182147024480696, 0.016182147024480696, 0.016182147024480696, 0.016182147024480696], "accuracy_valid": [0.17664309582078314, 0.21864704913403615, 0.2503765060240964, 0.2757774261106928, 0.3050860669239458, 0.33222685664533136, 0.34290874435240964, 0.3514227809676205, 0.3572409756212349, 0.36836113987198793, 0.37795468985316266, 0.3834684440888554, 0.3887483527861446, 0.3966329183923193, 0.4021878529743976, 0.40600291792168675, 0.40783397260918675, 0.4101738987198795, 0.4121270237198795, 0.41370364269578314, 0.4165215549698795, 0.4181290592055723, 0.4203263248305723, 0.4220250141189759, 0.4232457172439759, 0.4239678440323795, 0.4248326313064759, 0.42717255741716864, 0.42729462772966864, 0.4275490634412651, 0.4291462725903614, 0.4298786944653614, 0.4310993975903614, 0.4322186205760542, 0.4330731127635542, 0.4330731127635542, 0.4328083819653614, 0.4336628741528614, 0.4347820971385542, 0.4361248705760542, 0.4365013766001506, 0.4362572359751506, 0.437366163874247, 0.437122023249247, 0.437488234186747, 0.437854445124247, 0.438342726374247, 0.4380882906626506, 0.438708937311747, 0.438708937311747, 0.439197218561747, 0.439563429499247, 0.439319288874247, 0.439563429499247, 0.439563429499247, 0.439685499811747, 0.44043851185993976, 0.44031644154743976, 0.44043851185993976, 0.44043851185993976, 0.44056058217243976, 0.44056058217243976, 0.44068265248493976, 0.44068265248493976, 0.44056058217243976, 0.44068265248493976, 0.44080472279743976, 0.44104886342243976, 0.44104886342243976, 0.44117093373493976, 0.44104886342243976, 0.44104886342243976, 0.44092679310993976, 0.44117093373493976, 0.44104886342243976, 0.44104886342243976, 0.44104886342243976, 0.44104886342243976, 0.44104886342243976, 0.44104886342243976, 0.44104886342243976, 0.44092679310993976, 0.44080472279743976, 0.44080472279743976, 0.44080472279743976, 0.44092679310993976, 0.44092679310993976, 0.44092679310993976, 0.44092679310993976, 0.44104886342243976, 0.44104886342243976, 0.44104886342243976, 0.44117093373493976, 0.44117093373493976, 0.44117093373493976, 0.44117093373493976, 0.44117093373493976, 0.44117093373493976, 0.44117093373493976, 0.44117093373493976, 0.44117093373493976, 0.44117093373493976, 0.44117093373493976, 0.44129300404743976, 0.44129300404743976, 0.44129300404743976, 0.44129300404743976, 0.44129300404743976, 0.44129300404743976, 0.44129300404743976, 0.44129300404743976, 0.44129300404743976, 0.44129300404743976, 0.44129300404743976, 0.44129300404743976, 0.44129300404743976, 0.44129300404743976, 0.44129300404743976, 0.44129300404743976, 0.44129300404743976, 0.44129300404743976, 0.44129300404743976, 0.44129300404743976, 0.44129300404743976, 0.44129300404743976, 0.44129300404743976, 0.44129300404743976, 0.44129300404743976, 0.44129300404743976, 0.44129300404743976, 0.44129300404743976, 0.44129300404743976, 0.44129300404743976, 0.44129300404743976, 0.44129300404743976, 0.44129300404743976, 0.44129300404743976, 0.44129300404743976, 0.44129300404743976, 0.44129300404743976, 0.44129300404743976, 0.44129300404743976, 0.44129300404743976, 0.44129300404743976, 0.44129300404743976, 0.44129300404743976, 0.44129300404743976, 0.44129300404743976, 0.44129300404743976, 0.44129300404743976, 0.44129300404743976, 0.44129300404743976, 0.44129300404743976, 0.44129300404743976], "seed": 12346333, "model": "residualv4", "loss_std": [0.06976495683193207, 0.03522694483399391, 0.041123002767562866, 0.047121964395046234, 0.049005839973688126, 0.05187598988413811, 0.05426419898867607, 0.0567464716732502, 0.0589943565428257, 0.060919035226106644, 0.06252454966306686, 0.06390172243118286, 0.06518968939781189, 0.06629033386707306, 0.06723423302173615, 0.06800968199968338, 0.06865299493074417, 0.06920292973518372, 0.06967086344957352, 0.07009653002023697, 0.0704478994011879, 0.07077040523290634, 0.07105603069067001, 0.0713045597076416, 0.07151685655117035, 0.07170657068490982, 0.07187008857727051, 0.07201813906431198, 0.0721602812409401, 0.07229562103748322, 0.0724114403128624, 0.07250665873289108, 0.07258938252925873, 0.07266032695770264, 0.07272972166538239, 0.07279659807682037, 0.07285631448030472, 0.07291632890701294, 0.07297290116548538, 0.07302214950323105, 0.07306892424821854, 0.07310732454061508, 0.07314642518758774, 0.07318314909934998, 0.0732174664735794, 0.07324881851673126, 0.07327599078416824, 0.07330043613910675, 0.07332449406385422, 0.0733468234539032, 0.07336658239364624, 0.0733850970864296, 0.0734018012881279, 0.07341712713241577, 0.07343057543039322, 0.07344242185354233, 0.07345394045114517, 0.07346417754888535, 0.07347361743450165, 0.0734824389219284, 0.0734904557466507, 0.07349773496389389, 0.07350407540798187, 0.07351002842187881, 0.07351502776145935, 0.07351981848478317, 0.0735241174697876, 0.07352772355079651, 0.07353100180625916, 0.0735338032245636, 0.0735364481806755, 0.07353874295949936, 0.07354063540697098, 0.07354243844747543, 0.0735439583659172, 0.07354528456926346, 0.07354643940925598, 0.07354736328125, 0.07354822754859924, 0.07354889065027237, 0.07354941219091415, 0.07354988902807236, 0.0735502615571022, 0.0735505148768425, 0.07355070114135742, 0.07355079799890518, 0.07355085760354996, 0.07355087995529175, 0.07355087250471115, 0.07355082780122757, 0.07355077564716339, 0.07355066388845444, 0.07355055212974548, 0.07355041056871414, 0.07355024665594101, 0.07355009764432907, 0.07354994863271713, 0.07354976236820221, 0.07354959845542908, 0.07354942709207535, 0.07354924827814102, 0.0735490620136261, 0.07354889065027237, 0.07354868948459625, 0.07354851067066193, 0.073548324406147, 0.07354814559221268, 0.07354797422885895, 0.07354781031608582, 0.07354766130447388, 0.07354752719402313, 0.07354740053415298, 0.07354728132486343, 0.07354717701673508, 0.07354708760976791, 0.07354699820280075, 0.07354692369699478, 0.07354685664176941, 0.07354680448770523, 0.07354675233364105, 0.07354671508073807, 0.07354668527841568, 0.07354665547609329, 0.07354661822319031, 0.07354658842086792, 0.07354656606912613, 0.07354652881622314, 0.07354652136564255, 0.07354650646448135, 0.07354647666215897, 0.07354647666215897, 0.07354645431041718, 0.07354645431041718, 0.07354644685983658, 0.07354643195867538, 0.07354643195867538, 0.07354641705751419, 0.07354642450809479, 0.07354641705751419, 0.07354641705751419, 0.07354641705751419, 0.07354641705751419, 0.07354641705751419, 0.07354641705751419, 0.07354641705751419, 0.07354641705751419, 0.07354641705751419, 0.0735464096069336, 0.07354641705751419, 0.07354641705751419, 0.0735464096069336, 0.0735464096069336, 0.0735464096069336, 0.07354641705751419]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:28 2016", "state": "available"}], "summary": "0c9d2763f04c8cd20fcacc00803bdf47"}