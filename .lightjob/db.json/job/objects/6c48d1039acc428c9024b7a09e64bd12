{"content": {"hp_model": {"f0": 32, "f1": 64, "f2": 32, "f3": 32, "nonlin": "leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [2.101865530014038, 1.7598506212234497, 1.6239022016525269, 1.535587191581726, 1.4644668102264404, 1.4057633876800537, 1.3567900657653809, 1.3142908811569214, 1.280788779258728, 1.2507569789886475, 1.2245808839797974, 1.2001668214797974, 1.1815950870513916, 1.1615060567855835, 1.1465518474578857, 1.1331466436386108, 1.1195646524429321, 1.1092771291732788, 1.098799467086792, 1.089795470237732, 1.080903172492981, 1.0720367431640625, 1.0666636228561401, 1.0610603094100952, 1.0524814128875732, 1.0483015775680542, 1.0458718538284302, 1.0420712232589722, 1.036621332168579, 1.034269094467163, 1.0300719738006592, 1.0270048379898071, 1.0268516540527344, 1.0231927633285522, 1.0218552350997925, 1.0197523832321167, 1.019195795059204, 1.0171924829483032, 1.0143972635269165, 1.0127761363983154, 1.0133612155914307, 1.0101317167282104, 1.0112392902374268, 1.011586308479309, 1.010453224182129, 1.009203314781189, 1.0066503286361694, 1.0067424774169922, 1.0077619552612305, 1.0059384107589722, 1.0056740045547485, 1.0047129392623901, 1.0054051876068115, 1.003773808479309, 1.0054304599761963, 1.0038856267929077, 1.0044856071472168, 1.0033156871795654, 1.0039184093475342, 1.0031917095184326, 1.0042885541915894, 1.0025324821472168, 1.0030676126480103, 1.0028692483901978, 1.0036041736602783, 1.0027141571044922, 1.003146767616272, 1.000393033027649, 1.0037747621536255, 1.0023399591445923, 1.0021253824234009, 1.0017521381378174, 1.0036951303482056, 1.0015562772750854, 1.002549648284912, 1.0026609897613525, 1.0033897161483765, 1.0019253492355347, 1.0045812129974365, 1.0034329891204834, 1.0012457370758057, 1.0015926361083984, 1.001846432685852, 1.000969409942627, 1.0003952980041504, 1.0012023448944092, 1.0005786418914795, 1.001986026763916, 1.0026240348815918, 1.0002611875534058, 1.001982569694519, 1.0007622241973877, 1.0028928518295288, 1.0013231039047241, 1.001624345779419, 1.0015426874160767, 1.0012773275375366, 1.0017552375793457, 1.0018352270126343, 1.0007656812667847, 1.0026822090148926, 1.0039451122283936, 1.0017204284667969, 1.0008316040039062, 1.00188410282135, 1.0032851696014404, 1.0022270679473877, 1.0022934675216675, 1.0016772747039795, 1.001812219619751, 1.002753734588623, 1.003346562385559, 1.0018131732940674, 1.0030564069747925, 1.0017907619476318, 1.0031211376190186, 1.0021218061447144, 1.0037192106246948, 1.0033190250396729, 1.0022417306900024, 1.0018116235733032, 1.0020995140075684, 1.0021731853485107, 1.002056360244751, 1.0022929906845093, 1.000753402709961, 1.0036981105804443, 1.0013313293457031, 1.0010600090026855, 1.003577709197998, 1.0034990310668945, 1.0016509294509888, 1.0035691261291504, 1.0029069185256958, 1.0002553462982178, 1.003528356552124, 1.002865195274353, 1.002860188484192, 1.0006035566329956, 1.0022099018096924, 1.0030100345611572, 1.003393530845642, 1.0021830797195435, 1.0031418800354004, 1.0026006698608398, 1.0027261972427368, 1.00081467628479, 1.0023106336593628, 1.002397894859314, 1.0018621683120728, 1.0033563375473022, 1.0015873908996582, 1.0033684968948364, 1.0012084245681763, 1.0028526782989502, 1.0015395879745483, 1.0024068355560303, 1.0018497705459595], "moving_avg_accuracy_train": [0.03107510915582317, 0.06768130212659189, 0.10528600537992568, 0.14249629675892278, 0.1783890779415115, 0.21348930243787345, 0.24683434295705214, 0.2783934285314558, 0.30798922664361106, 0.3359413349754663, 0.36224896484193775, 0.3869484646121275, 0.40999414163744113, 0.4316626248470359, 0.451636337041642, 0.4703798329286554, 0.48787887826143583, 0.5037814067847293, 0.5188679930580836, 0.5328410157576278, 0.5457885437038288, 0.5576228607066464, 0.5685710766662513, 0.5788034342370293, 0.5881870503579, 0.596890216140447, 0.6048903318637501, 0.6123370099349886, 0.6191250867538743, 0.6252971349087286, 0.631000787771907, 0.6362804515773114, 0.6412227030581, 0.6455754703872568, 0.6494859855370694, 0.6531868107790435, 0.6565778992194117, 0.6597019223800195, 0.6626089103745756, 0.665113448231544, 0.6673676764980906, 0.6696241662772221, 0.6716713191689259, 0.6734998779762396, 0.6751291967146991, 0.676618799018589, 0.678006016165918, 0.6793569262925895, 0.6805493497232236, 0.6816296144036791, 0.6826413080482133, 0.6834820418151897, 0.6843689465876205, 0.6852322650494749, 0.685858080943706, 0.6865794614163805, 0.6871705390727306, 0.6876582950872461, 0.6882088826431672, 0.6887531314220399, 0.6891664056564047, 0.6894941746399521, 0.6899031350656302, 0.6903084378785116, 0.6905150281934197, 0.6907055737256373, 0.6909537946153474, 0.6911096920029729, 0.6913315240554442, 0.6914984766240599, 0.6917208856465469, 0.6918512993024994, 0.6920452933571147, 0.6921340016955912, 0.6922974364109066, 0.692411975571357, 0.6925454319455613, 0.692681782675193, 0.692632401271138, 0.692799582597974, 0.6928082117147455, 0.6928508191031639, 0.692933271482484, 0.6930843166810424, 0.6930667975383166, 0.6931067617836544, 0.6929777161366197, 0.6929870609923654, 0.692953618683965, 0.6929397966480713, 0.6929646312943568, 0.6929659840390905, 0.6930533401617596, 0.6930993004423721, 0.6929709288318281, 0.6929693266740052, 0.693005087112917, 0.6930862077793938, 0.6930917870637469, 0.6931223129589318, 0.6932498037122266, 0.6931576071461443, 0.6931280726104704, 0.6930735176450121, 0.6931383865165851, 0.6930039253450853, 0.6930828730883545, 0.6932746896001171, 0.6932544092071503, 0.6933221152617952, 0.6933250301371937, 0.6932112879381199, 0.693234586141124, 0.693334537485714, 0.6932129051541784, 0.6930964966581866, 0.6930033187070228, 0.6930914835140614, 0.6931569530451764, 0.6930902454898282, 0.6930698083174143, 0.692951469512251, 0.6928728303244995, 0.6928067053531423, 0.6927261944419976, 0.692649120373167, 0.6926773378635819, 0.6926842045121167, 0.6927740177553033, 0.6927479649265706, 0.6928477502676161, 0.692932906776938, 0.6929583943615181, 0.6929860555828967, 0.6929992528404523, 0.6931366884079667, 0.693272042211596, 0.6931845972420052, 0.6933756140312783, 0.6932778118797194, 0.6932548220123455, 0.6933155113400422, 0.6932701503361598, 0.6934084339886365, 0.6934189209353802, 0.6933121377957919, 0.6932763065951164, 0.6933929761847741, 0.6932191412024171, 0.6931881396075538, 0.6932463047269479, 0.693282233097461, 0.6933239052749796, 0.6933938541716234, 0.693315082247678, 0.6932790647482702, 0.6932931880238122, 0.6932803223348953], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.031484286756400595, 0.06748645460749245, 0.10420366475668295, 0.14031723676934296, 0.17503304227162553, 0.20914400762277624, 0.2416527231537968, 0.27255557489715204, 0.3012327816676025, 0.3281053089884175, 0.3535499668132203, 0.37712669311759106, 0.3988096139790247, 0.41890709172833607, 0.43752384208110484, 0.4548710319505546, 0.47072867296671905, 0.48509717662260743, 0.4988824396644129, 0.5115291989923993, 0.5232897003563372, 0.53406034557861, 0.5440113034435653, 0.5532855778431847, 0.5615683011206132, 0.5692557151727085, 0.5764805931095038, 0.5830328408862793, 0.5888444146666272, 0.5943220602199193, 0.5993363609279725, 0.6038736456277204, 0.6082156085310628, 0.611964683737821, 0.6154720992589937, 0.6187437840525973, 0.6218439032471117, 0.6246360695394939, 0.6270106238242493, 0.6292372899339178, 0.6311181896114598, 0.6329310106164283, 0.6344781298108096, 0.6358461230232528, 0.636978631155792, 0.6382796797024869, 0.6394638599344219, 0.6405275631258442, 0.6413017905293743, 0.642231558294961, 0.6432179221849679, 0.6438238584585645, 0.6445878981586418, 0.6451890551613018, 0.645842018762265, 0.646353355289653, 0.6467647300393021, 0.6471858544563056, 0.647750030917678, 0.6481326308944344, 0.648428142748515, 0.6487805821445972, 0.6491354282034809, 0.6495503868891569, 0.6496563245274249, 0.6498605021744566, 0.6501185337529447, 0.6502663424634936, 0.6505702687404876, 0.6506474603811225, 0.650924452388944, 0.6511127100397334, 0.6510980069480341, 0.651172282401574, 0.6512625148636003, 0.6514281437895144, 0.6516158899339064, 0.6516485251027898, 0.6517521684509446, 0.6516246913931243, 0.6515343761035859, 0.6516250202891611, 0.6516832155023384, 0.6516236688956287, 0.6516443486457495, 0.651526624059789, 0.6513840508386746, 0.6513432431757408, 0.6513431373728504, 0.6513653971954299, 0.6513630759905706, 0.6513253953211069, 0.6512517730988607, 0.6513086129199988, 0.651225491415273, 0.6512859889134295, 0.6513882352781106, 0.6515301146399832, 0.6516832496368282, 0.6517101783440791, 0.6517832423056049, 0.651615007259909, 0.6516843517899421, 0.651649105616972, 0.6516051770300489, 0.6515178426854777, 0.6515602825792041, 0.6518436486172173, 0.6520010218014293, 0.6520460309258796, 0.6520722730893158, 0.651974850232568, 0.6520835116701545, 0.6520571776341633, 0.6519134657065903, 0.6517841249717745, 0.6517643450517807, 0.652003920288696, 0.6521086452120102, 0.6519699345405833, 0.6519427511862991, 0.6519447592472626, 0.6519099454083797, 0.6519274410783851, 0.6518322943914803, 0.6518432891146063, 0.652001727757739, 0.6519103299254893, 0.6519379351577144, 0.6518895376792171, 0.6518205363774099, 0.6518602094904219, 0.6518806197349039, 0.6519122254948472, 0.6519650847412961, 0.6521336988669406, 0.6522742740574303, 0.652266514385121, 0.6523826305012023, 0.6521300425734464, 0.6521600906033759, 0.6521871338303125, 0.6523111880018746, 0.6523862156625305, 0.6522075409148016, 0.652082325226936, 0.6519828676477665, 0.6519920415851735, 0.6518660207850899, 0.6517892231587646, 0.6517445193575719, 0.6516798718739985, 0.6517071383575324, 0.6516451994653032, 0.6516891697296163, 0.6517653640612481, 0.6517209871524878, 0.6517807632019228], "moving_var_accuracy_train": [0.008690961681416927, 0.019881985787593655, 0.03062081056977593, 0.040020181573387124, 0.04761278908523916, 0.05393974201397032, 0.0585527933576043, 0.06166129696247668, 0.06337836865928809, 0.0640724150351212, 0.06389399603433044, 0.06299518403097583, 0.06147559469387387, 0.05955374370592689, 0.05718891194483082, 0.05463188849294612, 0.051924648931679855, 0.049008197759419336, 0.04615582375192767, 0.0432974496469927, 0.040476451001552345, 0.037689265431705705, 0.03499910978281845, 0.03244150907764292, 0.029989828433413412, 0.027672551441818547, 0.025481312961912854, 0.02343225879439155, 0.021503734797044247, 0.019696208923172396, 0.018019372934707952, 0.016468309290320025, 0.015041311008582237, 0.013707699158519986, 0.012474558401300207, 0.011350367528414881, 0.010318826102866975, 0.0093747791789524, 0.008513356473861604, 0.007718475215368336, 0.006992361599530781, 0.006338951154687627, 0.005742773553876965, 0.005198588844295511, 0.004702622075829416, 0.004252330103460264, 0.00384441643583883, 0.003476399415788037, 0.0031415563369505584, 0.0028379034492740578, 0.0025633248206201717, 0.002313353837960563, 0.0020890978548427524, 0.0018868959382576853, 0.001701731154233168, 0.001536241546887055, 0.0013857617473608769, 0.001249326725992053, 0.0011271223733034634, 0.0010170759965728565, 0.0009169055572506795, 0.0008261818940847929, 0.000745068942344251, 0.0006720404814409923, 0.0006052205493208177, 0.0005450252627873689, 0.0004910772589994286, 0.00044218826905870174, 0.00039841232728836406, 0.0003588219530010343, 0.0003233849496604833, 0.00029119952418936516, 0.0002624182750094635, 0.00023624727003235434, 0.00021286294118465085, 0.00019169472003967614, 0.00017268554347005033, 0.00015558431281628533, 0.0001400478282422547, 0.00012629459178241302, 0.00011366580275907802, 0.00010231556098910075, 9.214519044389043e-05, 8.313600326756971e-05, 7.482516522406939e-05, 6.73570229698112e-05, 6.07711956839975e-05, 5.46948620525579e-05, 4.9235441339222464e-05, 4.4313616643386446e-05, 3.9887805815952945e-05, 3.5899041703622485e-05, 3.2377817362770124e-05, 2.9159046753038947e-05, 2.639145551127791e-05, 2.3752333062337325e-05, 2.1388609037024056e-05, 1.9308973196088494e-05, 1.7378356032204666e-05, 1.564890690147577e-05, 1.4230301240909066e-05, 1.2883772977994616e-05, 1.1603246279372388e-05, 1.0469707849740522e-05, 9.460608799258845e-06, 8.677266179102785e-06, 7.86563427669828e-06, 7.410213016691235e-06, 6.6728933640720556e-06, 6.0468610161850256e-06, 5.4422513830538255e-06, 5.014461835399636e-06, 4.517900908228689e-06, 4.1560232589741e-06, 3.873570749749772e-06, 3.6081721162265814e-06, 3.325494079851679e-06, 3.0629019706678678e-06, 2.795188109140952e-06, 2.5557183796916687e-06, 2.3039056438690104e-06, 2.199551734749525e-06, 2.035253657926496e-06, 1.871080898666876e-06, 1.7423108701204795e-06, 1.6215434918833933e-06, 1.4665551835829088e-06, 1.3203240229835177e-06, 1.2608893885504703e-06, 1.1409091986601653e-06, 1.1164323073820815e-06, 1.0700537563627593e-06, 9.688949334360374e-07, 8.788917286058617e-07, 7.92570064208189e-07, 8.833098747493793e-07, 9.598647566867478e-07, 9.326978853786232e-07, 1.1678148208985555e-06, 1.137120686454811e-06, 1.02816542382617e-06, 9.584976319101553e-07, 8.811664547781152e-07, 9.651511261809174e-07, 8.696257980308494e-07, 8.852869683306147e-07, 8.083131459742039e-07, 8.49987969735049e-07, 1.0369565825812944e-06, 9.419108142798032e-07, 8.781683628790183e-07, 8.019691568606089e-07, 7.374013745868085e-07, 7.076968704032941e-07, 6.927723273813385e-07, 6.351704370155838e-07, 5.734485955223745e-07, 5.175934695318806e-07], "duration": 230786.200817, "accuracy_train": [0.3107510915582318, 0.3971370388635105, 0.44372833465992984, 0.4773889191698967, 0.5014241085848099, 0.5293913229051311, 0.5469397076296604, 0.562425198701089, 0.5743514096530085, 0.5875103099621631, 0.5990176336401809, 0.6092439625438354, 0.6174052348652639, 0.6266789737333887, 0.6313997467930971, 0.6390712959117756, 0.6453702862564599, 0.6469041634943706, 0.6546472695182723, 0.6585982200535253, 0.6623162952196383, 0.6641317137320044, 0.6671050203026947, 0.670894652374031, 0.6726395954457365, 0.6752187081833703, 0.6768913733734773, 0.6793571125761352, 0.6802177781238464, 0.6808455683024179, 0.6823336635405132, 0.6837974258259505, 0.6857029663851975, 0.6847503763496677, 0.684680621885382, 0.6864942379568106, 0.6870976951827242, 0.6878181308254891, 0.6887718023255813, 0.6876542889442598, 0.6876557308970099, 0.6899325742894057, 0.6900956951942598, 0.6899569072420635, 0.6897930653608343, 0.6900252197535991, 0.6904909704918789, 0.691515117432632, 0.6912811605989295, 0.6913519965277778, 0.6917465508490218, 0.6910486457179771, 0.692351089539498, 0.6930021312061646, 0.6914904239917866, 0.6930718856704503, 0.6924902379798819, 0.6920480992178848, 0.6931641706464562, 0.6936513704318937, 0.6928858737656884, 0.6924440954918789, 0.6935837788967332, 0.6939561631944444, 0.6923743410275932, 0.6924204835155962, 0.6931877826227391, 0.692512768491602, 0.6933280125276855, 0.693001049741602, 0.6937225668489295, 0.6930250222060723, 0.6937912398486527, 0.6929323767418789, 0.6937683488487449, 0.6934428280154116, 0.6937465393133998, 0.6939089392418789, 0.6921879686346438, 0.694304214539498, 0.6928858737656884, 0.6932342855989295, 0.6936753428963639, 0.6944437234680694, 0.6929091252537837, 0.6934664399916943, 0.6918163053133075, 0.6930711646940753, 0.692652637908361, 0.6928153983250277, 0.6931881431109266, 0.6929781587416943, 0.6938395452657807, 0.6935129429678848, 0.6918155843369325, 0.6929549072535991, 0.6933269310631229, 0.6938162937776855, 0.6931420006229236, 0.6933970460155962, 0.6943972204918789, 0.6923278380514027, 0.6928622617894057, 0.6925825229558877, 0.693722206360742, 0.6917937748015873, 0.6937934027777778, 0.69500103820598, 0.6930718856704503, 0.6939314697535991, 0.6933512640157807, 0.6921876081464562, 0.6934442699681617, 0.6942340995870248, 0.692118214170358, 0.6920488201942598, 0.6921647171465486, 0.6938849667774086, 0.6937461788252123, 0.6924898774916943, 0.6928858737656884, 0.6918864202657807, 0.6921650776347361, 0.6922115806109266, 0.6920015962416943, 0.6919554537536914, 0.6929312952773163, 0.6927460043489295, 0.693582336943983, 0.6925134894679771, 0.6937458183370248, 0.6936993153608343, 0.6931877826227391, 0.6932350065753046, 0.6931180281584532, 0.6943736085155962, 0.6944902264442598, 0.6923975925156884, 0.6950947651347361, 0.6923975925156884, 0.69304791320598, 0.6938617152893134, 0.6928619013012182, 0.6946529868609266, 0.6935133034560723, 0.692351089539498, 0.6929538257890366, 0.6944430024916943, 0.6916546263612035, 0.6929091252537837, 0.693769790801495, 0.6936055884320782, 0.6936989548726468, 0.6940233942414176, 0.6926061349321705, 0.6929549072535991, 0.6934202975036914, 0.6931645311346438], "end": "2016-02-06 04:03:15.307000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0], "moving_var_accuracy_valid": [0.00892134281303336, 0.01969461334153376, 0.029858533697638752, 0.0386103910794971, 0.0455960363365982, 0.051508454317625005, 0.05586895815515386, 0.058876938552484624, 0.06039068439063269, 0.06085081047304099, 0.06059260493212859, 0.059536102647996594, 0.05781383389694569, 0.05566762801420772, 0.05322011575606281, 0.05060642914775719, 0.04780896924035933, 0.044886157392106885, 0.04210784294708212, 0.039336523345874015, 0.03664765554226725, 0.03402695117457716, 0.03151545011899048, 0.02913801459784635, 0.02684164468207582, 0.02468934722714341, 0.02269020225524341, 0.020807569588073407, 0.019030782137505923, 0.017397745331022837, 0.015884259702237603, 0.014481116304032943, 0.013202678450315666, 0.01200891068943746, 0.010918737293237184, 0.009923198856411743, 0.009017375621952336, 0.008185803793195941, 0.0074179699863376, 0.006720795365379353, 0.006080555881214214, 0.005502077173057287, 0.0049734116559661656, 0.0044929131392331635, 0.004055164997342254, 0.0036648830434957407, 0.003311015284541518, 0.0029900969364023435, 0.002696482095413501, 0.002434614098753467, 0.0021999089123897067, 0.0019832224500596772, 0.001790154015023357, 0.0016143911211976448, 0.0014567892622555257, 0.0013134635214281437, 0.0011836402319471695, 0.0010668723207238216, 0.0009630497443675403, 0.0008680622146107114, 0.00078204193845276, 0.0007049556663586812, 0.0006355933412523604, 0.000573583723524486, 0.0005163263562208537, 0.00046506891720269456, 0.0004191612481418985, 0.00037744175006193585, 0.00034052891569236925, 0.0003065296510675876, 0.0002765672071124016, 0.0002492294548888879, 0.00022430845502814875, 0.00020192726111232097, 0.00018180781207591888, 0.000163873927338222, 0.00014780377213700614, 0.000133032980411538, 0.00011982635986293473, 0.00010798997747907609, 9.726439139488801e-05, 8.761189957080642e-05, 7.888118975925649e-05, 7.102498296866666e-05, 6.392633354038555e-05, 5.765843188960311e-05, 5.2075532811053326e-05, 4.6882966918135006e-05, 4.2194670327069774e-05, 3.79796627916742e-05, 3.4181745004434775e-05, 3.0776348999652384e-05, 2.774749618416341e-05, 2.500182345315009e-05, 2.2563823768765983e-05, 2.0340380917438066e-05, 1.840043169750885e-05, 1.6741556307686526e-05, 1.5278453622246484e-05, 1.3757134657489681e-05, 1.2429466274005333e-05, 1.144124692200792e-05, 1.034040020441686e-05, 9.31754081835653e-06, 8.403154223262427e-06, 7.631484390611257e-06, 6.884546252765644e-06, 6.918758430983401e-06, 6.449779459866272e-06, 5.823033905433809e-06, 5.246928375166704e-06, 4.807656454802248e-06, 4.433156581487456e-06, 3.99608225640306e-06, 3.782352093903351e-06, 3.5546781156568816e-06, 3.202731511205815e-06, 3.3990250073724363e-06, 3.157828292703536e-06, 3.0152113167425543e-06, 2.7203405978195787e-06, 2.448342828817118e-06, 2.2144165763352767e-06, 1.995729804922192e-06, 1.877632852690575e-06, 1.6909575228510875e-06, 1.7477870033056827e-06, 1.6481903766347927e-06, 1.4902297785871649e-06, 1.3622876440525575e-06, 1.2689094965071588e-06, 1.1561841499210306e-06, 1.0443149376472267e-06, 9.488737604368707e-07, 8.79133283809603e-07, 1.0470964657303233e-06, 1.1202392767881399e-06, 1.0087572617384555e-06, 1.0292281072888122e-06, 1.5005112477914762e-06, 1.3585860799361002e-06, 1.2293094970507428e-06, 1.2448834846833575e-06, 1.1710574849865107e-06, 1.3412737257719247e-06, 1.3482570695838635e-06, 1.3024576531137996e-06, 1.172969337950361e-06, 1.1986035826387816e-06, 1.1318241030576562e-06, 1.0366275613216092e-06, 9.705784793808805e-07, 8.802117815615236e-07, 8.26718440740528e-07, 7.614470539603099e-07, 7.375525341195468e-07, 6.815210709877134e-07, 6.45527548663382e-07], "accuracy_test": 0.6509885204081632, "start": "2016-02-03 11:56:49.106000", "learning_rate_per_epoch": [0.0006155892042443156, 0.0005560653517022729, 0.0005022971308790147, 0.00045372796012088656, 0.0004098551580682397, 0.00037022458855062723, 0.000334426062181592, 0.0003020890289917588, 0.00027287882403470576, 0.00024649305851198733, 0.00022265863663051277, 0.0002011288597714156, 0.0001816808944568038, 0.00016411342949140817, 0.00014824463869445026, 0.00013391026004683226, 0.00012096192949684337, 0.00010926563118118793, 9.870029316516593e-05, 8.91565578058362e-05, 8.053564670262858e-05, 7.274832751136273e-05, 6.571399717358872e-05, 5.9359841543482617e-05, 5.3620096878148615e-05, 4.8435351345688105e-05, 4.3751941120717674e-05, 3.952138649765402e-05, 3.5699904401553795e-05, 3.224793545086868e-05, 2.9129751055734232e-05, 2.6313076887163334e-05, 2.3768758182995953e-05, 2.1470459614647552e-05, 1.9394394257687964e-05, 1.7519070752314292e-05, 1.5825080481590703e-05, 1.4294889297161717e-05, 1.2912658348795958e-05, 1.1664080375339836e-05, 1.0536233276070561e-05, 9.517441867501475e-06, 8.597161468060222e-06, 7.765866939735133e-06, 7.014953553152736e-06, 6.33664922133903e-06, 5.723932645196328e-06, 5.170462372916518e-06, 4.670509497373132e-06, 4.218898993713083e-06, 3.810956513916608e-06, 3.4424595014570514e-06, 3.1095939903025283e-06, 2.8089145871490473e-06, 2.537309228500817e-06, 2.2919664388609817e-06, 2.0703469090221915e-06, 1.8701566659728996e-06, 1.6893236534087919e-06, 1.5259760175467818e-06, 1.3784231214231113e-06, 1.2451378097466659e-06, 1.124740379054856e-06, 1.0159845942325774e-06, 9.17744898742967e-07, 8.290044206660241e-07, 7.488446271963767e-07, 6.764357749489136e-07, 6.11028440289374e-07, 5.519456180991256e-07, 4.98575730034645e-07, 4.503664001731522e-07, 4.0681862856217776e-07, 3.674816753118648e-07, 3.319483710129134e-07, 2.998509387452941e-07, 2.708571287257655e-07, 2.4466683612445195e-07, 2.2100900309851568e-07, 1.9963874819950433e-07, 1.8033486526292108e-07, 1.628975496714702e-07, 1.4714632357026858e-07, 1.3291814582316874e-07, 1.2006574934275704e-07, 1.0845610631804448e-07, 9.796904265613193e-08, 8.84960158487047e-08, 7.993897810365524e-08, 7.220935316354371e-08, 6.522713391632351e-08, 5.892005816576784e-08, 5.322283769260139e-08, 4.807650455518342e-08, 4.342779291732768e-08, 3.9228581272254814e-08, 3.543540927353206e-08, 3.200901588229499e-08, 2.8913934357888138e-08, 2.6118128104712923e-08, 2.3592660269855514e-08, 2.1311389986067297e-08, 1.9250704141882125e-08, 1.7389275797086157e-08, 1.570783503268558e-08, 1.4188980657081629e-08, 1.2816990135888773e-08, 1.1577663272532845e-08, 1.0458172106098118e-08, 9.446928572742763e-09, 8.533466377969035e-09, 7.708330862499224e-09, 6.962980858560286e-09, 6.2897016483987045e-09, 5.681524584133513e-09, 5.132154701215086e-09, 4.635905881400504e-09, 4.187641344799431e-09, 3.782721247347354e-09, 3.416954497126312e-09, 3.0865552336223345e-09, 2.7881035258303655e-09, 2.518510511251293e-09, 2.274985533290419e-09, 2.0550079415926348e-09, 1.856300890779039e-09, 1.6768075816742112e-09, 1.514670278091046e-09, 1.3682106558832174e-09, 1.2359128165329025e-09, 1.1164074109615285e-09, 1.008457428675058e-09, 9.109455967326596e-10, 8.228625558714953e-10, 7.43296646454894e-10, 6.714242495320377e-10, 6.065015156764275e-10, 5.478564268912578e-10, 4.948819687378148e-10, 4.4702980206423604e-10, 4.0380465637923635e-10, 3.6475913955946737e-10, 3.2948910266838993e-10, 2.9762944886435605e-10, 2.6885044190905205e-10, 2.42854208964971e-10, 2.193716458487316e-10, 1.9815971086245554e-10, 1.7899884352523543e-10, 1.6169071637150978e-10, 1.4605618103846751e-10, 1.319334086424817e-10, 1.1917622444457265e-10, 1.076525882326429e-10, 9.724322042048428e-11, 8.784037386355692e-11, 7.934673057485853e-11, 7.167437476862659e-11], "accuracy_train_first": 0.3107510915582318, "accuracy_train_last": 0.6931645311346438, "batch_size_eval": 1024, "accuracy_train_std": [0.01737019603517055, 0.017504936793671693, 0.01854121578404599, 0.018271858248274663, 0.01732824803790363, 0.018810464044379315, 0.017265022715066997, 0.018551278154143345, 0.018049556350094448, 0.018369672674753736, 0.0171963669826153, 0.017709898075130508, 0.018037555303059054, 0.016582810019249508, 0.017270854604848437, 0.016708456925857283, 0.01522140002869791, 0.016430462668320368, 0.015606210156862343, 0.0173038373364568, 0.016079187593106857, 0.016913558863345453, 0.016875267749617107, 0.016277314161184112, 0.01728040136970089, 0.016983262738585466, 0.015734705137974217, 0.01711299724204875, 0.016818078049373145, 0.016671845074058358, 0.015992467103784196, 0.016787334298203484, 0.01685427726028053, 0.016167066666157484, 0.01653636802809403, 0.016281418890902923, 0.015656270887080572, 0.016491962485182456, 0.016681365048071956, 0.016396830416297914, 0.016506327262364805, 0.01585576389375179, 0.015814077623893, 0.016992419475952394, 0.01666229491609866, 0.0162732066532318, 0.016557422411613003, 0.01671180641122419, 0.016855987440189447, 0.01658284691360452, 0.016673442060716224, 0.015808442967046706, 0.017143317854787427, 0.016988413315745576, 0.016413297530952775, 0.016804965453130333, 0.01594707770868013, 0.01673740740048031, 0.016479041044907126, 0.016609034590324808, 0.01679128997837308, 0.01670042775275588, 0.01651197062759379, 0.016855675348798026, 0.01631331339177162, 0.016333563428789957, 0.016719996905290826, 0.01640659068014482, 0.016337208513643113, 0.0167061948904305, 0.016268800823460935, 0.016754844311121696, 0.016131068447446523, 0.016152945055251035, 0.016788418283271492, 0.016560363822632415, 0.01687959402627199, 0.016404527761748753, 0.016565734419270093, 0.016618361268462417, 0.016169914281356623, 0.016640093821854912, 0.017163585197764085, 0.016747344886148766, 0.016457989059906714, 0.016513571861433762, 0.016797381872136804, 0.016601235163706225, 0.016841360588860304, 0.016162982227655595, 0.016626573995558336, 0.016475787467182854, 0.01609488762332725, 0.016904582144140524, 0.01663587019947798, 0.016048663922299602, 0.01649737885768255, 0.016371813467137914, 0.01649761929301851, 0.016808196330799465, 0.017093799844341905, 0.01645734492296211, 0.016334216039081344, 0.016530671620126672, 0.01618981658295485, 0.016448466351110604, 0.016611690777832932, 0.015978129682257772, 0.016621555294805967, 0.016476467002193386, 0.016046248450261206, 0.016482629612603224, 0.016873730635860083, 0.017303682304117517, 0.015982226222719963, 0.016899282373561832, 0.016339795581625148, 0.016930669225976438, 0.01673051995434118, 0.01627200751947478, 0.016449746304010564, 0.01697420093132987, 0.016203621346917566, 0.01685126064694004, 0.016064936297183537, 0.01614481237666783, 0.016409009033151383, 0.016126683198324453, 0.01630898948338311, 0.01674902370025258, 0.01691759286495434, 0.016360464073784144, 0.016789114411127272, 0.016709184244174537, 0.016588928376252345, 0.01700479405843817, 0.01683218335277464, 0.016172817761144535, 0.01642781398475397, 0.016580454600670022, 0.016528849926748305, 0.016455989668837195, 0.016251339480792885, 0.016650178689629255, 0.016310455352529202, 0.016249829687894072, 0.016412726140212417, 0.01670006545345142, 0.016865857139090456, 0.016478671057955467, 0.016840661426460044, 0.016619653262903097, 0.016341781126841197, 0.016952952315912862, 0.01608062028416893, 0.016738438365288287, 0.016463486056131692, 0.01664147004385599], "accuracy_test_std": 0.010831775553486368, "error_valid": [0.685157132435994, 0.6084940347326807, 0.5653414439006024, 0.5346606151167168, 0.5125247082078314, 0.48385730421686746, 0.4657688370670181, 0.44931875941265065, 0.4406723573983433, 0.430041945124247, 0.4174481127635542, 0.4106827701430723, 0.4060440982680723, 0.4002156085278614, 0.39492540474397586, 0.38900425922439763, 0.3865525578878012, 0.38558629047439763, 0.3770501929593373, 0.37464996705572284, 0.37086578736822284, 0.36900384742093373, 0.3664300757718373, 0.36324595256024095, 0.3638871893825302, 0.36155755835843373, 0.3584955054593373, 0.35799692912274095, 0.35885142131024095, 0.35637912980045183, 0.35553493269954817, 0.35529079207454817, 0.3527067253388554, 0.3542936394013554, 0.35296116105045183, 0.3518110528049698, 0.35025502400225905, 0.35023443382906627, 0.35161838761295183, 0.35072271507906627, 0.3519537132906627, 0.3507536003388554, 0.35159779743975905, 0.35184193806475905, 0.3528287956513554, 0.35001088337725905, 0.3498785179781627, 0.3498991081513554, 0.3517301628388554, 0.34940053181475905, 0.3479048028049698, 0.35072271507906627, 0.3485357445406627, 0.34940053181475905, 0.34828130882906627, 0.3490446159638554, 0.3495328972138554, 0.3490240257906627, 0.3471723809299698, 0.34842396931475905, 0.34891225056475905, 0.3480474632906627, 0.34767095726656627, 0.34671498493975905, 0.3493902367281627, 0.34830189900225905, 0.3475591820406627, 0.34840337914156627, 0.34669439476656627, 0.3486578148531627, 0.3465826195406627, 0.3471929711031627, 0.34903432087725905, 0.34815923851656627, 0.3479253929781627, 0.34708119587725905, 0.34669439476656627, 0.34805775837725905, 0.3473150414156627, 0.34952260212725905, 0.34927846150225905, 0.3475591820406627, 0.34779302757906627, 0.34891225056475905, 0.3481695336031627, 0.3495328972138554, 0.3498991081513554, 0.3490240257906627, 0.3486578148531627, 0.3484342644013554, 0.3486578148531627, 0.34901373070406627, 0.3494108269013554, 0.34817982868975905, 0.34952260212725905, 0.3481695336031627, 0.34769154743975905, 0.3471929711031627, 0.34693853539156627, 0.3480474632906627, 0.3475591820406627, 0.3498991081513554, 0.34769154743975905, 0.34866810993975905, 0.34879018025225905, 0.3492681664156627, 0.34805775837725905, 0.3456060570406627, 0.3465826195406627, 0.34754888695406627, 0.34769154743975905, 0.3489019554781627, 0.34693853539156627, 0.34817982868975905, 0.34937994164156627, 0.34937994164156627, 0.3484136742281627, 0.34583990257906627, 0.3469488304781627, 0.34927846150225905, 0.34830189900225905, 0.34803716820406627, 0.34840337914156627, 0.34791509789156627, 0.3490240257906627, 0.34805775837725905, 0.34657232445406627, 0.34891225056475905, 0.34781361775225905, 0.34854603962725905, 0.3488004753388554, 0.3477827324924698, 0.34793568806475905, 0.3478033226656627, 0.3475591820406627, 0.34634877400225905, 0.3464605492281627, 0.3478033226656627, 0.34657232445406627, 0.3501432487763554, 0.34756947712725905, 0.34756947712725905, 0.34657232445406627, 0.34693853539156627, 0.34940053181475905, 0.3490446159638554, 0.34891225056475905, 0.3479253929781627, 0.3492681664156627, 0.3489019554781627, 0.3486578148531627, 0.3489019554781627, 0.3480474632906627, 0.34891225056475905, 0.34791509789156627, 0.34754888695406627, 0.3486784050263554, 0.3476812523531627], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.09669407490240131, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 32, "valid_ratio": 0.15, "learning_rate": 0.0006814847782059074, "optimization": "nesterov_momentum", "nb_data_augmentation": 2, "learning_rate_decay_method": "exp", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 3.4546289458140335e-08, "rotation_range": [0, 0], "momentum": 0.8102826918313606}, "accuracy_valid_max": 0.6543939429593373, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.6523187476468373, "accuracy_valid_std": [0.01878823681848045, 0.014331175234597333, 0.015261733846455315, 0.020161126506227447, 0.02097432444616173, 0.020357888269190882, 0.010693739081449053, 0.009288232115534766, 0.011340997355944826, 0.015128000983766212, 0.016793084133807576, 0.016689535228437413, 0.017266952300836292, 0.020726252704976387, 0.02069712400683744, 0.015226897527659308, 0.01646171351547139, 0.01673041939968347, 0.01213492455752446, 0.015441141447469366, 0.014372143089471469, 0.013567008116651907, 0.010937098608136445, 0.012660083402373254, 0.01420145235318997, 0.016740379084236888, 0.015523554625817355, 0.013016489526011133, 0.01457462067270563, 0.014479573278875805, 0.013918278187137388, 0.014104481116342677, 0.013638480552828616, 0.013424451901473646, 0.013322760619336132, 0.012323608842482762, 0.012992302306885016, 0.011537439485349197, 0.015019423254075221, 0.012693003204818087, 0.012183000730867094, 0.013618773507869771, 0.012320402118008658, 0.012642060762237407, 0.012913544118511667, 0.012554341125432415, 0.01259907027356634, 0.013841382844836062, 0.013633003378339665, 0.013523395826859689, 0.01222790330627121, 0.013026718666079285, 0.013026759596398839, 0.014033849466804119, 0.013439317593513824, 0.01591284050269671, 0.013675262820584557, 0.012611995181248656, 0.012347012189828166, 0.01368366578902684, 0.012975868815641804, 0.013839659747648154, 0.012036832011852093, 0.014673715731589364, 0.014108640189151379, 0.013910244911755127, 0.012572302411547913, 0.012875752845958183, 0.012777804216780748, 0.013285337487539788, 0.014706656790352303, 0.014072352801095553, 0.012782897453401967, 0.01395351014436037, 0.01331136611287981, 0.013688762027239917, 0.013903705317978254, 0.0135373612525355, 0.01236725501705957, 0.013307030677296708, 0.013149980052845922, 0.012909138498562548, 0.013733777424446585, 0.013382899819557634, 0.013727546666226217, 0.013805400823539524, 0.014530457754098808, 0.014486047763295276, 0.013525438765288621, 0.01522405242374518, 0.013104648815431826, 0.012172460177548682, 0.014241982875520334, 0.015376403234940484, 0.013235169693423018, 0.014019692192158558, 0.013652851166997123, 0.013377504514309716, 0.01272844852209387, 0.01332176736636999, 0.013478361445608593, 0.014332206970498739, 0.013200037782294225, 0.012672103145617803, 0.014492430251567654, 0.012440204172269681, 0.01575150796581948, 0.014205317784021622, 0.013311089623939855, 0.012469307965225036, 0.01393801126769979, 0.01378471031039784, 0.012719079495014255, 0.013484655050435303, 0.012650464035826927, 0.012555877290366248, 0.013906132273551865, 0.014805123124420923, 0.014213966115715494, 0.014626345100390405, 0.014046693944506458, 0.013927450559650922, 0.012838665703015792, 0.013375813835643508, 0.013409132665227653, 0.013885132078799683, 0.01406950153138749, 0.01359500104739802, 0.014755638564285611, 0.012591638359984102, 0.015267428889827597, 0.0115737194977381, 0.01284000907890066, 0.012890182754197529, 0.013434066188721876, 0.013655776680805134, 0.014164418012209651, 0.014268445197552418, 0.013560390008208566, 0.01370425205769045, 0.013847688593977324, 0.015596694921736055, 0.012697811313768775, 0.012803153924420581, 0.014939041955831376, 0.014186022790765715, 0.013507030306019812, 0.0132034633654169, 0.01274315794373616, 0.013434340146710817, 0.012810246915830653, 0.012788814490065725, 0.013761919005674991, 0.01329352555905694, 0.012950163031715811, 0.014245312856263828, 0.014137221271181546, 0.014644370213703259], "accuracy_valid": [0.31484286756400603, 0.3915059652673193, 0.4346585560993976, 0.46533938488328314, 0.48747529179216864, 0.5161426957831325, 0.5342311629329819, 0.5506812405873494, 0.5593276426016567, 0.569958054875753, 0.5825518872364458, 0.5893172298569277, 0.5939559017319277, 0.5997843914721386, 0.6050745952560241, 0.6109957407756024, 0.6134474421121988, 0.6144137095256024, 0.6229498070406627, 0.6253500329442772, 0.6291342126317772, 0.6309961525790663, 0.6335699242281627, 0.636754047439759, 0.6361128106174698, 0.6384424416415663, 0.6415044945406627, 0.642003070877259, 0.641148578689759, 0.6436208701995482, 0.6444650673004518, 0.6447092079254518, 0.6472932746611446, 0.6457063605986446, 0.6470388389495482, 0.6481889471950302, 0.649744975997741, 0.6497655661709337, 0.6483816123870482, 0.6492772849209337, 0.6480462867093373, 0.6492463996611446, 0.648402202560241, 0.648158061935241, 0.6471712043486446, 0.649989116622741, 0.6501214820218373, 0.6501008918486446, 0.6482698371611446, 0.650599468185241, 0.6520951971950302, 0.6492772849209337, 0.6514642554593373, 0.650599468185241, 0.6517186911709337, 0.6509553840361446, 0.6504671027861446, 0.6509759742093373, 0.6528276190700302, 0.651576030685241, 0.651087749435241, 0.6519525367093373, 0.6523290427334337, 0.653285015060241, 0.6506097632718373, 0.651698100997741, 0.6524408179593373, 0.6515966208584337, 0.6533056052334337, 0.6513421851468373, 0.6534173804593373, 0.6528070288968373, 0.650965679122741, 0.6518407614834337, 0.6520746070218373, 0.652918804122741, 0.6533056052334337, 0.651942241622741, 0.6526849585843373, 0.650477397872741, 0.650721538497741, 0.6524408179593373, 0.6522069724209337, 0.651087749435241, 0.6518304663968373, 0.6504671027861446, 0.6501008918486446, 0.6509759742093373, 0.6513421851468373, 0.6515657355986446, 0.6513421851468373, 0.6509862692959337, 0.6505891730986446, 0.651820171310241, 0.650477397872741, 0.6518304663968373, 0.652308452560241, 0.6528070288968373, 0.6530614646084337, 0.6519525367093373, 0.6524408179593373, 0.6501008918486446, 0.652308452560241, 0.651331890060241, 0.651209819747741, 0.6507318335843373, 0.651942241622741, 0.6543939429593373, 0.6534173804593373, 0.6524511130459337, 0.652308452560241, 0.6510980445218373, 0.6530614646084337, 0.651820171310241, 0.6506200583584337, 0.6506200583584337, 0.6515863257718373, 0.6541600974209337, 0.6530511695218373, 0.650721538497741, 0.651698100997741, 0.6519628317959337, 0.6515966208584337, 0.6520849021084337, 0.6509759742093373, 0.651942241622741, 0.6534276755459337, 0.651087749435241, 0.652186382247741, 0.651453960372741, 0.6511995246611446, 0.6522172675075302, 0.652064311935241, 0.6521966773343373, 0.6524408179593373, 0.653651225997741, 0.6535394507718373, 0.6521966773343373, 0.6534276755459337, 0.6498567512236446, 0.652430522872741, 0.652430522872741, 0.6534276755459337, 0.6530614646084337, 0.650599468185241, 0.6509553840361446, 0.651087749435241, 0.6520746070218373, 0.6507318335843373, 0.6510980445218373, 0.6513421851468373, 0.6510980445218373, 0.6519525367093373, 0.651087749435241, 0.6520849021084337, 0.6524511130459337, 0.6513215949736446, 0.6523187476468373], "seed": 113727063, "model": "residualv4", "loss_std": [0.1855970025062561, 0.12088874727487564, 0.1351783275604248, 0.14505243301391602, 0.152145653963089, 0.15693287551403046, 0.1619521528482437, 0.16379734873771667, 0.16316837072372437, 0.16231665015220642, 0.16472579538822174, 0.16484682261943817, 0.1651967316865921, 0.16394583880901337, 0.16466422379016876, 0.16503167152404785, 0.16316014528274536, 0.16353006660938263, 0.16284668445587158, 0.16362512111663818, 0.16459329426288605, 0.16453057527542114, 0.16118796169757843, 0.16430732607841492, 0.1632918417453766, 0.16277649998664856, 0.16267627477645874, 0.1620497703552246, 0.162203848361969, 0.16168470680713654, 0.1618078500032425, 0.16248738765716553, 0.16227197647094727, 0.1620403230190277, 0.16038654744625092, 0.16329239308834076, 0.16206130385398865, 0.16136744618415833, 0.16057752072811127, 0.1624959409236908, 0.16207154095172882, 0.1612517088651657, 0.1609874814748764, 0.16317468881607056, 0.1626245081424713, 0.16110379993915558, 0.1617552936077118, 0.16131435334682465, 0.16245250403881073, 0.16145789623260498, 0.16138343513011932, 0.16278569400310516, 0.16261844336986542, 0.1620432436466217, 0.16168315708637238, 0.1614139825105667, 0.1614857167005539, 0.16042381525039673, 0.16157272458076477, 0.15961705148220062, 0.16214196383953094, 0.16153378784656525, 0.16075754165649414, 0.1624484658241272, 0.16280227899551392, 0.16231398284435272, 0.1606784462928772, 0.16161267459392548, 0.16159434616565704, 0.16055037081241608, 0.16026653349399567, 0.1624363213777542, 0.1630711853504181, 0.16116677224636078, 0.1620391309261322, 0.1621103733778, 0.16090427339076996, 0.1625962108373642, 0.16177231073379517, 0.16139008104801178, 0.15948764979839325, 0.16172616183757782, 0.1620250791311264, 0.16229790449142456, 0.1607397049665451, 0.16062432527542114, 0.16121885180473328, 0.1605972945690155, 0.16223829984664917, 0.16101013123989105, 0.16275891661643982, 0.16270071268081665, 0.16300836205482483, 0.16034270823001862, 0.16055874526500702, 0.160654678940773, 0.160723477602005, 0.1605626940727234, 0.16243061423301697, 0.1615549772977829, 0.1614818572998047, 0.16108600795269012, 0.15979140996932983, 0.16144298017024994, 0.15943115949630737, 0.1617821305990219, 0.1623275727033615, 0.16344009339809418, 0.16140396893024445, 0.159469336271286, 0.16198517382144928, 0.16212673485279083, 0.16194766759872437, 0.15930074453353882, 0.16115672886371613, 0.16049933433532715, 0.16035246849060059, 0.16125687956809998, 0.15965501964092255, 0.16078928112983704, 0.1608525961637497, 0.1625044345855713, 0.1623256653547287, 0.1616441160440445, 0.1613020896911621, 0.159197136759758, 0.16045597195625305, 0.16008198261260986, 0.16049568355083466, 0.16226312518119812, 0.16170373558998108, 0.16041219234466553, 0.1625710129737854, 0.16061115264892578, 0.16210319101810455, 0.1635107398033142, 0.16080033779144287, 0.16015535593032837, 0.1615811437368393, 0.1588355451822281, 0.16193963587284088, 0.1611260324716568, 0.1602410525083542, 0.16256973147392273, 0.16386443376541138, 0.1612555831670761, 0.16073565185070038, 0.16085714101791382, 0.16150163114070892, 0.16013571619987488, 0.16092239320278168, 0.16156919300556183, 0.16225914657115936, 0.16144706308841705, 0.16121025383472443, 0.16074778139591217, 0.15989771485328674, 0.16185717284679413]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:41 2016", "state": "available"}], "summary": "3c5b4b97a07bfac6a8e33899c84daaaf"}