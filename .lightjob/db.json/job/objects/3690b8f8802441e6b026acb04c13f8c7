{"content": {"hp_model": {"f0": 32, "f1": 32, "f2": 16, "f3": 64, "nonlin": "rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.05348220062809373, 0.048093428137897944, 0.04897152597114703, 0.050979460566355245, 0.051713050669885685, 0.05301933974666397, 0.051954592421543724, 0.05060145321745815, 0.05060145321745815, 0.08410503906272758, 0.07911541650709346, 0.08472776264204913, 0.0861724943463322, 0.0842337633136828, 0.08609764351562732, 0.08787037977000667, 0.07723959985439674, 0.07880642808005678, 0.07686984837054674, 0.0795532255077797, 0.0794316231843872, 0.076370792526579, 0.07692829342622091, 0.07924650680222689, 0.07419026141253117, 0.0744178048311039, 0.07442247801045361, 0.07091346153846154, 0.07376498967491589, 0.07304972538470222, 0.07123966252338432, 0.07395381893047653, 0.07122463997821726, 0.06769239537463026, 0.06932736370450308, 0.07146699254898985, 0.06941156355034524, 0.07086050081037244, 0.0712656941001208, 0.07000016489530321, 0.07039434977755575, 0.0692057040200925, 0.06997111368395888, 0.06960464448410651, 0.06949078520392535, 0.07079062267814017, 0.07161420199179246, 0.06963436082975823, 0.06823716359619181, 0.07048549878605528, 0.06948462533316618, 0.07078659156540225, 0.07157571496424096, 0.06954966141586062, 0.06530598922468149, 0.06768567659835557, 0.06826434014022072, 0.06624793161507476, 0.07219163921105633, 0.06799838096663995, 0.06850024302363106, 0.06874701147051811, 0.07057653007645769, 0.06937982385420942, 0.0685235413863116, 0.06846104745361019, 0.07032844646932776, 0.07039650326730887, 0.06995059218342121, 0.06767619013158, 0.0697524603995583, 0.06992368842727426, 0.07114947967435567, 0.07054923302261261, 0.06740301552824707, 0.07009526149161621, 0.06651834260341072, 0.06745062662094538, 0.0678362280184553, 0.06984904325208607, 0.07000819007794465, 0.06738779934008496, 0.06937918120029717, 0.06741796374183737, 0.06569807046383294, 0.06700081501681382, 0.06376684367693328, 0.0679591583808999, 0.06748499152008591, 0.06757966879594579, 0.0660489536126171, 0.06798159279714873, 0.06587212119215761, 0.06595126769363144, 0.06700560622696441, 0.06819951665699538, 0.06568503879526555, 0.06559986204060299, 0.07016201932250296, 0.0668914556520524, 0.06386284414679125, 0.06642174978266223, 0.06499556057373988, 0.06565204082347341, 0.06689412182935307, 0.0669298383582102, 0.06594531809797714, 0.06591069161637948, 0.06579409930641951, 0.06627215635471775, 0.06803574599487743, 0.06746437468117035, 0.06739415086242398, 0.06569169069910319, 0.06622692967313897, 0.0656709182360989, 0.06782623671343177, 0.06527443901718036, 0.06734649984379364, 0.06694422619937636, 0.06573063833182212, 0.06693849807758699, 0.06652155994855, 0.06855437675706146, 0.06549415437604318, 0.06645557324010183, 0.06557090121847024, 0.06569807046383294, 0.06535731110386316, 0.06353526125189389, 0.06628022930040049, 0.06568340965490173, 0.06617358711561676, 0.06894402310743593, 0.06514151487349304, 0.06448606016945303, 0.06556559714717924, 0.06718423358003026, 0.06521594201660422, 0.06601829877336278, 0.06819755530952273, 0.06835741620094664, 0.06593179429299803, 0.0684580515203665, 0.06603761163025454, 0.06604260774346761, 0.06745062662094538, 0.06311110040419944, 0.06208499742028756, 0.06519406050058844, 0.06600195271131364, 0.06791807501944741, 0.0663221927774039, 0.06895320582312749, 0.0692922395139829, 0.06898850262645115, 0.06699282890509844, 0.06771188922539421, 0.0676276829637151, 0.06707836389889855, 0.06608418235124816, 0.06861093726271406, 0.06527484885787231, 0.0681740147385122, 0.06579789417633944, 0.06751986721393871, 0.07099628273978854, 0.06658908828740956, 0.06733418451637586, 0.06724950531456761, 0.06614083288332254, 0.06626690841263062, 0.06824500409453395, 0.06852822613381483, 0.06830312636634454, 0.06688305649886604, 0.06988171830261264, 0.06695381637580251, 0.06721223389586785, 0.06658841869973312, 0.06865823024619204, 0.0680771513136596, 0.06774980712797427, 0.06733537642069569, 0.0657676646754473, 0.06485216541835125, 0.06708899826076109, 0.06476754554019862, 0.06342414439930864, 0.06480223241356466, 0.06388629829238802, 0.06306883848609293, 0.062379463173811246, 0.0636956233782282, 0.06395451765443547, 0.06337730747847817, 0.06451758125793666, 0.06481048845666233, 0.06538309332378756, 0.06642980454886271, 0.06583312180770896, 0.06463744194199976, 0.06422764239535815, 0.06384287341291912, 0.06318015679892272, 0.06429161603254156, 0.06127872555465755, 0.0645296049831258, 0.06448149663014256, 0.06465882225455881, 0.06422542090800291, 0.06271535874791505, 0.062111276542316546, 0.06337730747847817, 0.06477126287942812, 0.06244761506349533, 0.06376110982076205, 0.06530107331191815, 0.06576115605959548, 0.06413496888388534, 0.06557049322821895, 0.06421500666159158, 0.0647377991419809, 0.060490898469276785, 0.06310558958517669, 0.0630026324842914, 0.06324321600217148, 0.06405858979574804, 0.06346462413179492, 0.06641409684992754, 0.06276894091663579, 0.06314457888573263, 0.06060400916360317, 0.06444345456834873, 0.06330578980668121, 0.0626465017218809, 0.06334254420813136, 0.06266528841432514, 0.06485876525202128, 0.0636412801080105, 0.06363245194997806, 0.06197905012585798, 0.06478406541519854, 0.06538309332378756, 0.06350886929626397, 0.061952139157987425, 0.06462254051047724, 0.062161362759633736, 0.06462695610764915, 0.06487539935321644, 0.06415498768949901, 0.06311237206333135, 0.06551212446176584, 0.06702636417998326], "moving_avg_accuracy_train": [0.009906814759036143, 0.016667451054216867, 0.02419451242469879, 0.03186542262801205, 0.03856922298569277, 0.04470382930158132, 0.050123788991905116, 0.05516412093608809, 0.05970041968585278, 0.08366495979558075, 0.11814955568349256, 0.1551533124645409, 0.19965304145905066, 0.24574336607820585, 0.28785295266315636, 0.32933780046912986, 0.369053210934265, 0.40648900429866985, 0.44316738173024867, 0.4756202218704768, 0.5081575031472845, 0.5397353898807488, 0.5684896370372522, 0.5928483164058161, 0.6190703560001742, 0.6439738437435303, 0.6655233720197796, 0.6870734444563559, 0.706548517179395, 0.7249867565759134, 0.7425083181171173, 0.7585153929319116, 0.7724064176447446, 0.7855978165429207, 0.798336039406701, 0.8103699053455489, 0.8214027566784639, 0.8321606361310995, 0.8417274226685919, 0.850598731606552, 0.8592982710964991, 0.8675820170290178, 0.8748914922839475, 0.8815500275435045, 0.887761553403612, 0.8939096262259013, 0.8996170258021063, 0.904652499426715, 0.9093656192129591, 0.9138545090988921, 0.9185463360504487, 0.9222230465719098, 0.9255509113424296, 0.9293131206599938, 0.9327061685337534, 0.9359834620719443, 0.938998914810533, 0.941646933721046, 0.944460779505568, 0.9470308913140473, 0.9496004866705944, 0.951851940262571, 0.954050029368844, 0.9559859526367789, 0.9575965064694866, 0.9594531020574776, 0.9611993392914889, 0.9625262238864365, 0.9637486579736965, 0.9651994698871702, 0.9665310853984531, 0.9677883684248728, 0.9689269826366025, 0.970099984674147, 0.9710333220501058, 0.9720309875860591, 0.9729947751226339, 0.9740951470079608, 0.975127838632466, 0.9761937445282556, 0.977035401701936, 0.9776658223751159, 0.9784614577580862, 0.9788880905967354, 0.9795191422298329, 0.9802588695731147, 0.9805787092724297, 0.9811183534054277, 0.9817216912576561, 0.9824270635475532, 0.982946593638581, 0.9833929922566506, 0.9835264904707445, 0.9839525500080074, 0.9845689666939537, 0.984853128006486, 0.985257122434753, 0.9854959997997115, 0.985953365181186, 0.986360287699212, 0.9867265179654354, 0.9870914226447955, 0.9874080710429666, 0.9877918874326458, 0.9882949840809475, 0.9887971874800816, 0.989079742828459, 0.9894422881239263, 0.989655627082618, 0.9898546916333923, 0.9901562141869207, 0.9905146515031684, 0.9907807691841769, 0.9910391003982894, 0.9913680781596652, 0.9914806114581566, 0.9916360141677626, 0.9918276461847212, 0.9920612972288997, 0.9922033414517928, 0.9924088356198665, 0.9926173119976388, 0.9928825951051038, 0.9931072309259187, 0.9932505740983871, 0.9933513450018014, 0.9934232135136695, 0.9936549697225435, 0.9938447250093253, 0.9940037389541759, 0.9941068477394812, 0.9941266976040873, 0.9943022243798232, 0.9943472466707566, 0.994521897003681, 0.9946320190503009, 0.994759366844066, 0.9949398684126715, 0.9950717287099585, 0.995148046049806, 0.9952661480713314, 0.9953889120292585, 0.9954123325733206, 0.9954992996171934, 0.9956011015831849, 0.9957374334429387, 0.9958389536528618, 0.9959185560285395, 0.9959737260280952, 0.995971609449382, 0.99604735889601, 0.995957871500385, 0.9961314744105874, 0.9961841778731431, 0.9962316109894432, 0.9963284235350772, 0.9964743838924129, 0.9966081013766657, 0.9966743243715291, 0.9967503972054605, 0.9968118032680471, 0.9968976598388327, 0.9969513991260338, 0.9969715265327075, 0.9970225854758222, 0.9970732448499268, 0.9970906003348137, 0.9971391645483203, 0.9971805191778257, 0.9971565561154647, 0.9972267627027134, 0.9972570043541288, 0.9972324522620895, 0.9972668312828685, 0.9973989583955456, 0.9974425715921356, 0.9974818234690666, 0.9975500944354129, 0.9976444825822329, 0.9977435508902747, 0.9977927086024521, 0.9978698948205201, 0.9979040649770223, 0.9979465839311273, 0.9979848509898218, 0.9980287039932493, 0.9980869969975388, 0.9981394607013994, 0.9982125628240305, 0.9982430572946395, 0.998315212408549, 0.998330735595405, 0.9983494127888765, 0.9984109323533623, 0.998416883545737, 0.9984457712453803, 0.9984600043618062, 0.9984257509135773, 0.9984208075993279, 0.9983928269899977, 0.9984617709476243, 0.998502642045633, 0.9985488386844431, 0.9985480587316614, 0.9984932340331941, 0.9984744829190313, 0.9985234954705017, 0.9985981978812828, 0.9986607237256846, 0.998688759033839, 0.9987587009015395, 0.9988004701186144, 0.9988498282272349, 0.9988824847117402, 0.998923641361048, 0.9989630355080757, 0.9989937839150995, 0.9990332232946739, 0.9990545997603872, 0.9990761917421798, 0.9991026840137449, 0.9991053485942981, 0.999095980903543, 0.9991204942589719, 0.9991213778150024, 0.9991645299431408, 0.9991468909548508, 0.9991663133051488, 0.9991414364927063, 0.9991802295904235, 0.9991586674747547, 0.999202796962219, 0.9992378071756357, 0.9992740226930119, 0.9992407281044335, 0.999234294601219, 0.9992449765868803, 0.9992851814884331, 0.9992954811106742, 0.9992788659815346, 0.999280384503863, 0.999314695451067, 0.9993314563276471, 0.9993183031647619, 0.9993535285711773, 0.9993899377622524, 0.9993615238053043, 0.9993736018464606, 0.9993350556678386, 0.9993427210347897, 0.9993519730276963, 0.999367359309264, 0.9993435563602654, 0.9993386058447208], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 1234423, "moving_var_accuracy_train": [0.0008833048080287074, 0.001206330155267271, 0.0015956070156155555, 0.001965632084179658, 0.002173537328882454, 0.0022948841478526664, 0.002329779400470013, 0.0023254459753909744, 0.0022781034349759264, 0.007218985735515182, 0.017199773343936915, 0.02780329815274205, 0.042845001262731164, 0.05767936334794996, 0.06787038255435396, 0.07657227767628164, 0.08311087436438101, 0.08741273455134466, 0.09077919143533034, 0.09117995379030214, 0.09159003046724686, 0.0914054937954857, 0.08970620498177269, 0.08607569182881995, 0.08365648089033073, 0.08087248611734903, 0.07696467702597397, 0.07344785992157173, 0.06951658004752351, 0.06562464009116104, 0.06182522215162468, 0.0579487379336, 0.053890509248392816, 0.05006757536757076, 0.046521178726359684, 0.043172386218631204, 0.039950661873575925, 0.03699718341907527, 0.034121175719053265, 0.031417359247602514, 0.028956761208876622, 0.02667866910805965, 0.024491658053775532, 0.02244151707462286, 0.02054461284875762, 0.01883034075873542, 0.017240476372164068, 0.015744632686564827, 0.01437009090098372, 0.013114433002557633, 0.012001108863592043, 0.010922661779560448, 0.009930067756982209, 0.009064448951826495, 0.00826161902150647, 0.007532122995775035, 0.006860747293165491, 0.006237780601202851, 0.005685262093974211, 0.005176185156949555, 0.004717992023922094, 0.004291814211021309, 0.0039061171513912227, 0.0035492356263460852, 0.0032176570165439204, 0.002926913839485661, 0.002661666555834121, 0.0024113455048054896, 0.0021836600602041967, 0.0019842377510582727, 0.0018017727747814505, 0.0016358223427800145, 0.0014839080894103861, 0.001347900684490101, 0.0012209506839573458, 0.0011078136442562716, 0.0010053922575715569, 0.0009157503963885637, 0.0008337734246716146, 0.0007606214806125633, 0.000690934813733377, 0.0006254182043865926, 0.0005685737049116426, 0.0005133544746316033, 0.0004656030626411587, 0.00042396752525863116, 0.0003824914496320894, 0.0003468632467813928, 0.00031545307117863676, 0.0002883857146669645, 0.00026197634683961847, 0.00023757215769158683, 0.00021397533788092472, 0.00019421154465646635, 0.0001782101159672362, 0.0001611158332343736, 0.00014647315339357371, 0.00013233939961362172, 0.00012098810748180257, 0.00011037957015471171, 0.00010054873461032296, 9.16922599744605e-05, 8.342542984959308e-05, 7.64087220535115e-05, 7.10458059859518e-05, 6.621109967427379e-05, 6.030852743091636e-05, 5.5460626509214334e-05, 5.0324185459953873e-05, 4.56484071723334e-05, 4.1901809107675706e-05, 3.8867923984017776e-05, 3.561849916692388e-05, 3.26572643958947e-05, 3.0365575263624217e-05, 2.7442991426685803e-05, 2.4916042303393305e-05, 2.2754943542366843e-05, 2.0970784482141422e-05, 1.9055295085243204e-05, 1.7529816254729753e-05, 1.6167996230058672e-05, 1.5184572751009545e-05, 1.4120266743847344e-05, 1.2893165455302267e-05, 1.1695241884546456e-05, 1.0572203443074959e-05, 9.998381561932345e-06, 9.322607025493868e-06, 8.61791523485704e-06, 7.851806505835538e-06, 7.070172009375906e-06, 6.640441649440312e-06, 5.994640544624254e-06, 5.669701139277069e-06, 5.2118728117150675e-06, 4.836642675735101e-06, 4.646205754582838e-06, 4.338069421130259e-06, 3.956681506269855e-06, 3.6865461430383964e-06, 3.4535304330278905e-06, 3.113114086682585e-06, 2.8698720784940137e-06, 2.6761576331622193e-06, 2.5758192537013406e-06, 2.4109945055364714e-06, 2.2269238989045934e-06, 2.0316250686728995e-06, 1.8285028809546493e-06, 1.6972944008393021e-06, 1.599636906537239e-06, 1.7109149497604074e-06, 1.5648223494726326e-06, 1.4285892192228647e-06, 1.3700843182297686e-06, 1.4248157196287388e-06, 1.443257438019656e-06, 1.3384010596559776e-06, 1.2566446382516102e-06, 1.1649165151278155e-06, 1.114767020338691e-06, 1.0292815172047334e-06, 9.29999377978949e-07, 8.604625812290507e-07, 7.975136727681026e-07, 7.204732211922068e-07, 6.69652244574603e-07, 6.180788685508399e-07, 5.614390369151551e-07, 5.496558172616416e-07, 5.029212528584569e-07, 4.580543745842074e-07, 4.2288619075329514e-07, 5.377157368172149e-07, 5.010631613866935e-07, 4.6482323383149935e-07, 4.602892340611474e-07, 4.944424109963334e-07, 5.333289368210382e-07, 5.01744369137533e-07, 5.051893425606126e-07, 4.6517880466301503e-07, 4.34931677320359e-07, 4.046178196184764e-07, 3.8146381084310137e-07, 3.739000989006894e-07, 3.612820510155595e-07, 3.732491289126263e-07, 3.442934306608721e-07, 3.567213317644773e-07, 3.2321792255950855e-07, 2.9403566830732236e-07, 2.986940128073426e-07, 2.6914336174272444e-07, 2.497395182845127e-07, 2.2658880088478222e-07, 2.1448960923637396e-07, 1.9326057551464167e-07, 1.8098074844967426e-07, 2.0566209724385468e-07, 2.001299073913667e-07, 1.9932408158846764e-07, 1.7939714836669542e-07, 1.8850916158834997e-07, 1.728226839706263e-07, 1.7716048738827662e-07, 2.0966849023810385e-07, 2.2388697217776294e-07, 2.085720814898104e-07, 2.3174165705765945e-07, 2.2426949880739338e-07, 2.2376855490598204e-07, 2.1098971323766233e-07, 2.051355699541848e-07, 1.985891023391472e-07, 1.8723937291571596e-07, 1.825146175750598e-07, 1.683757353950925e-07, 1.557340849551732e-07, 1.4647724053379555e-07, 1.3189341638613765e-07, 1.194938574182749e-07, 1.1295261302587024e-07, 1.0166437776461494e-07, 1.0825689545394137e-07, 1.0023141107959376e-07, 9.360331919153918e-08, 8.981268944812386e-08, 9.437556037786863e-08, 8.912232782913814e-08, 9.773680002097422e-08, 9.899455541019232e-08, 1.0089917315860036e-07, 1.0078602250012437e-07, 9.107992992260376e-08, 8.299888028934422e-08, 8.924689924034029e-08, 8.127694928107658e-08, 7.563381699993249e-08, 6.809118849049785e-08, 7.187723952381191e-08, 6.721785842501392e-08, 6.205312382747238e-08, 6.701527475887726e-08, 7.224441003563603e-08, 7.228614557712313e-08, 6.637044272297365e-08, 7.310566942788557e-08, 6.632392313954811e-08, 6.046192518028171e-08, 5.65463716065839e-08, 5.5990957875233405e-08, 5.0612430525126945e-08], "duration": 103053.768547, "accuracy_train": [0.09906814759036145, 0.07751317771084337, 0.09193806475903614, 0.10090361445783133, 0.09890342620481928, 0.09991528614457831, 0.09890342620481928, 0.10052710843373494, 0.10052710843373494, 0.29934582078313254, 0.4285109186746988, 0.4881871234939759, 0.6001506024096386, 0.6605562876506024, 0.6668392319277109, 0.7027014307228916, 0.7264919051204819, 0.7434111445783133, 0.7732727786144579, 0.7676957831325302, 0.8009930346385542, 0.8239363704819277, 0.8272778614457831, 0.8120764307228916, 0.8550687123493976, 0.8681052334337349, 0.8594691265060241, 0.8810240963855421, 0.881824171686747, 0.8909309111445783, 0.9002023719879518, 0.9025790662650602, 0.897425640060241, 0.904320406626506, 0.9129800451807228, 0.9186746987951807, 0.9206984186746988, 0.9289815512048193, 0.9278285015060241, 0.9304405120481928, 0.9375941265060241, 0.9421357304216867, 0.9406767695783133, 0.9414768448795181, 0.9436652861445783, 0.949242281626506, 0.9509836219879518, 0.9499717620481928, 0.9517836972891566, 0.9542545180722891, 0.9607727786144579, 0.9553134412650602, 0.9555016942771084, 0.9631730045180723, 0.9632435993975904, 0.9654791039156626, 0.9661379894578314, 0.9654791039156626, 0.9697853915662651, 0.9701618975903614, 0.9727268448795181, 0.9721150225903614, 0.9738328313253012, 0.9734092620481928, 0.9720914909638554, 0.9761624623493976, 0.9769154743975904, 0.9744681852409639, 0.9747505647590361, 0.9782567771084337, 0.978515625, 0.9791039156626506, 0.9791745105421686, 0.9806570030120482, 0.9794333584337349, 0.9810099774096386, 0.9816688629518072, 0.9839984939759037, 0.9844220632530121, 0.9857868975903614, 0.9846103162650602, 0.9833396084337349, 0.9856221762048193, 0.9827277861445783, 0.9851986069277109, 0.9869164156626506, 0.9834572665662651, 0.9859751506024096, 0.9871517319277109, 0.9887754141566265, 0.9876223644578314, 0.9874105798192772, 0.9847279743975904, 0.9877870858433735, 0.9901167168674698, 0.9874105798192772, 0.9888930722891566, 0.9876458960843374, 0.9900696536144579, 0.9900225903614458, 0.9900225903614458, 0.9903755647590361, 0.990257906626506, 0.991246234939759, 0.9928228539156626, 0.9933170180722891, 0.9916227409638554, 0.9927051957831325, 0.9915756777108434, 0.9916462725903614, 0.9928699171686747, 0.9937405873493976, 0.993175828313253, 0.9933640813253012, 0.9943288780120482, 0.9924934111445783, 0.9930346385542169, 0.9935523343373494, 0.994164156626506, 0.9934817394578314, 0.9942582831325302, 0.9944935993975904, 0.9952701430722891, 0.995128953313253, 0.9945406626506024, 0.9942582831325302, 0.9940700301204819, 0.9957407756024096, 0.9955525225903614, 0.9954348644578314, 0.9950348268072289, 0.9943053463855421, 0.9958819653614458, 0.9947524472891566, 0.99609375, 0.9956231174698795, 0.9959054969879518, 0.9965643825301205, 0.9962584713855421, 0.9958349021084337, 0.9963290662650602, 0.9964937876506024, 0.9956231174698795, 0.9962820030120482, 0.9965173192771084, 0.9969644201807228, 0.9967526355421686, 0.9966349774096386, 0.9964702560240963, 0.9959525602409639, 0.9967291039156626, 0.995152484939759, 0.9976939006024096, 0.9966585090361446, 0.9966585090361446, 0.9971997364457831, 0.9977880271084337, 0.9978115587349398, 0.9972703313253012, 0.9974350527108434, 0.9973644578313253, 0.9976703689759037, 0.9974350527108434, 0.9971526731927711, 0.9974821159638554, 0.9975291792168675, 0.9972467996987951, 0.9975762424698795, 0.9975527108433735, 0.9969408885542169, 0.9978586219879518, 0.9975291792168675, 0.9970114834337349, 0.9975762424698795, 0.9985881024096386, 0.9978350903614458, 0.9978350903614458, 0.9981645331325302, 0.9984939759036144, 0.9986351656626506, 0.9982351280120482, 0.9985645707831325, 0.9982115963855421, 0.9983292545180723, 0.9983292545180723, 0.9984233810240963, 0.9986116340361446, 0.9986116340361446, 0.9988704819277109, 0.9985175075301205, 0.9989646084337349, 0.9984704442771084, 0.9985175075301205, 0.9989646084337349, 0.9984704442771084, 0.9987057605421686, 0.9985881024096386, 0.9981174698795181, 0.9983763177710844, 0.9981410015060241, 0.9990822665662651, 0.9988704819277109, 0.9989646084337349, 0.9985410391566265, 0.9979998117469879, 0.9983057228915663, 0.9989646084337349, 0.9992705195783133, 0.9992234563253012, 0.9989410768072289, 0.9993881777108434, 0.9991763930722891, 0.9992940512048193, 0.9991763930722891, 0.9992940512048193, 0.9993175828313253, 0.9992705195783133, 0.9993881777108434, 0.9992469879518072, 0.9992705195783133, 0.9993411144578314, 0.9991293298192772, 0.999011671686747, 0.9993411144578314, 0.9991293298192772, 0.9995528990963856, 0.998988140060241, 0.9993411144578314, 0.9989175451807228, 0.9995293674698795, 0.9989646084337349, 0.9995999623493976, 0.9995528990963856, 0.9995999623493976, 0.9989410768072289, 0.9991763930722891, 0.9993411144578314, 0.9996470256024096, 0.9993881777108434, 0.9991293298192772, 0.9992940512048193, 0.9996234939759037, 0.9994823042168675, 0.9991999246987951, 0.9996705572289156, 0.9997176204819277, 0.9991057981927711, 0.9994823042168675, 0.998988140060241, 0.9994117093373494, 0.9994352409638554, 0.9995058358433735, 0.9991293298192772, 0.9992940512048193], "end": "2016-01-21 12:46:54.387000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0, 211.0, 212.0, 213.0, 214.0, 215.0, 216.0, 217.0, 218.0, 219.0, 220.0, 221.0, 222.0, 223.0, 224.0, 225.0, 226.0, 227.0, 228.0, 229.0, 230.0, 231.0, 232.0, 233.0, 234.0, 235.0, 236.0, 237.0, 238.0, 239.0, 240.0, 241.0, 242.0, 243.0, 244.0, 245.0, 246.0, 247.0, 248.0, 249.0, 250.0, 251.0, 252.0, 253.0], "accuracy_valid": [0.10576923076923077, 0.07705662393162394, 0.09001068376068376, 0.0968215811965812, 0.10670405982905982, 0.10202991452991453, 0.10670405982905982, 0.09695512820512821, 0.09695512820512821, 0.3016826923076923, 0.4285523504273504, 0.49599358974358976, 0.5836004273504274, 0.6394230769230769, 0.6395566239316239, 0.6796207264957265, 0.6907051282051282, 0.7073985042735043, 0.7255608974358975, 0.7235576923076923, 0.7449252136752137, 0.7562767094017094, 0.7584134615384616, 0.7509348290598291, 0.7771100427350427, 0.7813835470085471, 0.7793803418803419, 0.7912660256410257, 0.7867254273504274, 0.7928685897435898, 0.7982104700854701, 0.796875, 0.7923344017094017, 0.7954059829059829, 0.7987446581196581, 0.8012820512820513, 0.8068910256410257, 0.8071581196581197, 0.8024839743589743, 0.8046207264957265, 0.8036858974358975, 0.8082264957264957, 0.8075587606837606, 0.8083600427350427, 0.8067574786324786, 0.8134348290598291, 0.8083600427350427, 0.8056891025641025, 0.8114316239316239, 0.8068910256410257, 0.8131677350427351, 0.8112980769230769, 0.8111645299145299, 0.8079594017094017, 0.813034188034188, 0.8147702991452992, 0.8149038461538461, 0.8167735042735043, 0.8134348290598291, 0.8154380341880342, 0.8169070512820513, 0.8181089743589743, 0.8157051282051282, 0.8163728632478633, 0.8159722222222222, 0.8170405982905983, 0.8143696581196581, 0.8150373931623932, 0.8173076923076923, 0.8219818376068376, 0.8175747863247863, 0.8187767094017094, 0.8195779914529915, 0.8205128205128205, 0.8205128205128205, 0.8191773504273504, 0.8222489316239316, 0.8189102564102564, 0.8217147435897436, 0.8218482905982906, 0.8239850427350427, 0.8203792735042735, 0.8213141025641025, 0.8206463675213675, 0.8215811965811965, 0.8227831196581197, 0.8230502136752137, 0.8241185897435898, 0.8234508547008547, 0.8230502136752137, 0.8226495726495726, 0.8245192307692307, 0.8183760683760684, 0.8222489316239316, 0.8214476495726496, 0.8210470085470085, 0.8247863247863247, 0.8243856837606838, 0.8235844017094017, 0.8269230769230769, 0.8234508547008547, 0.8243856837606838, 0.8250534188034188, 0.8254540598290598, 0.8250534188034188, 0.8245192307692307, 0.8249198717948718, 0.8211805555555556, 0.8258547008547008, 0.8239850427350427, 0.8243856837606838, 0.8223824786324786, 0.8230502136752137, 0.8249198717948718, 0.8255876068376068, 0.8245192307692307, 0.8230502136752137, 0.8211805555555556, 0.8254540598290598, 0.8221153846153846, 0.8258547008547008, 0.8227831196581197, 0.8230502136752137, 0.8238514957264957, 0.8247863247863247, 0.8222489316239316, 0.8250534188034188, 0.8250534188034188, 0.8261217948717948, 0.8258547008547008, 0.8231837606837606, 0.8263888888888888, 0.8234508547008547, 0.8233173076923077, 0.8254540598290598, 0.827590811965812, 0.8267895299145299, 0.8235844017094017, 0.8257211538461539, 0.827323717948718, 0.8251869658119658, 0.8262553418803419, 0.8246527777777778, 0.8233173076923077, 0.8255876068376068, 0.8257211538461539, 0.8269230769230769, 0.8274572649572649, 0.8246527777777778, 0.8251869658119658, 0.8247863247863247, 0.8226495726495726, 0.8266559829059829, 0.8234508547008547, 0.8210470085470085, 0.827590811965812, 0.8259882478632479, 0.8253205128205128, 0.8265224358974359, 0.8269230769230769, 0.828659188034188, 0.8266559829059829, 0.8255876068376068, 0.8227831196581197, 0.8245192307692307, 0.8287927350427351, 0.8263888888888888, 0.827857905982906, 0.8274572649572649, 0.8262553418803419, 0.8270566239316239, 0.8270566239316239, 0.8266559829059829, 0.8234508547008547, 0.8265224358974359, 0.8265224358974359, 0.8293269230769231, 0.8269230769230769, 0.8285256410256411, 0.8285256410256411, 0.8261217948717948, 0.828125, 0.8237179487179487, 0.827590811965812, 0.828659188034188, 0.8263888888888888, 0.8282585470085471, 0.8243856837606838, 0.8297275641025641, 0.8270566239316239, 0.8266559829059829, 0.8305288461538461, 0.8274572649572649, 0.8293269230769231, 0.8267895299145299, 0.8285256410256411, 0.8259882478632479, 0.828392094017094, 0.827323717948718, 0.8259882478632479, 0.8279914529914529, 0.8257211538461539, 0.8285256410256411, 0.8321314102564102, 0.827857905982906, 0.828659188034188, 0.8263888888888888, 0.8298611111111112, 0.8271901709401709, 0.8263888888888888, 0.8274572649572649, 0.8277243589743589, 0.8274572649572649, 0.8285256410256411, 0.8258547008547008, 0.827590811965812, 0.8269230769230769, 0.8298611111111112, 0.8270566239316239, 0.8246527777777778, 0.8270566239316239, 0.8294604700854701, 0.8291933760683761, 0.8307959401709402, 0.8291933760683761, 0.8305288461538461, 0.8287927350427351, 0.8234508547008547, 0.8257211538461539, 0.8242521367521367, 0.8294604700854701, 0.8302617521367521, 0.8267895299145299, 0.8305288461538461, 0.8271901709401709, 0.8287927350427351, 0.8302617521367521, 0.8306623931623932, 0.8266559829059829, 0.8282585470085471, 0.8291933760683761, 0.8277243589743589, 0.8262553418803419, 0.827323717948718, 0.8285256410256411, 0.8299946581196581, 0.8241185897435898, 0.8265224358974359, 0.8254540598290598, 0.828125, 0.8291933760683761, 0.828659188034188, 0.8274572649572649, 0.828392094017094], "accuracy_test": 0.8178084935897436, "start": "2016-01-20 08:09:20.619000", "learning_rate_per_epoch": [0.005994757637381554, 0.002997378818690777, 0.0019982524681836367, 0.0014986894093453884, 0.0011989515041932464, 0.0009991262340918183, 0.0008563939481973648, 0.0007493447046726942, 0.0006660841754637659, 0.0005994757520966232, 0.0005449779564514756, 0.0004995631170459092, 0.0004611352051142603, 0.0004281969740986824, 0.0003996505110990256, 0.0003746723523363471, 0.000352632807334885, 0.00033304208773188293, 0.0003155135491397232, 0.0002997378760483116, 0.0002854646591003984, 0.0002724889782257378, 0.00026064165285788476, 0.0002497815585229546, 0.0002397903153905645, 0.00023056760255713016, 0.00022202805848792195, 0.0002140984870493412, 0.00020671577658504248, 0.0001998252555495128, 0.0001933792809722945, 0.00018733617616817355, 0.00018165932851843536, 0.0001763164036674425, 0.000171278792549856, 0.00016652104386594146, 0.00016202048573177308, 0.0001577567745698616, 0.00015371173503808677, 0.0001498689380241558, 0.00014621359878219664, 0.0001427323295501992, 0.0001394129649270326, 0.0001362444891128689, 0.00013321683218237013, 0.00013032082642894238, 0.00012754803174175322, 0.0001248907792614773, 0.0001223419967573136, 0.00011989515769528225, 0.00011754426668630913, 0.00011528380127856508, 0.00011310863919788972, 0.00011101402924396098, 0.00010899559129029512, 0.0001070492435246706, 0.0001051711878972128, 0.00010335788829252124, 0.00010160606325371191, 9.99126277747564e-05, 9.827471512835473e-05, 9.668964048614725e-05, 9.515488636679947e-05, 9.366808808408678e-05, 9.222704102285206e-05, 9.082966425921768e-05, 8.94739932846278e-05, 8.815820183372125e-05, 8.688054367667064e-05, 8.5639396274928e-05, 8.443320984952152e-05, 8.326052193297073e-05, 8.21199719212018e-05, 8.101024286588654e-05, 7.993010513018817e-05, 7.88783872849308e-05, 7.78539979364723e-05, 7.685586751904339e-05, 7.588300650240853e-05, 7.49344690120779e-05, 7.400935282930732e-05, 7.310679939109832e-05, 7.222599379019812e-05, 7.13661647750996e-05, 7.052656292216852e-05, 6.97064824635163e-05, 6.890526128700003e-05, 6.812224455643445e-05, 6.735682836733758e-05, 6.660841609118506e-05, 6.587645475519821e-05, 6.516041321447119e-05, 6.445976032409817e-05, 6.377401587087661e-05, 6.31027141935192e-05, 6.244538963073865e-05, 6.180162745295092e-05, 6.11709983786568e-05, 6.055310950614512e-05, 5.9947578847641125e-05, 5.93540353293065e-05, 5.8772133343154564e-05, 5.820153091917746e-05, 5.764190063928254e-05, 5.70929296372924e-05, 5.655431959894486e-05, 5.602577220997773e-05, 5.550701462198049e-05, 5.4997777624521405e-05, 5.449779564514756e-05, 5.4006824939278886e-05, 5.35246217623353e-05, 5.305095328367315e-05, 5.25855939486064e-05, 5.212832911638543e-05, 5.167894414626062e-05, 5.123724622535519e-05, 5.0803031626855955e-05, 5.037611481384374e-05, 4.99563138873782e-05, 4.9543450586497784e-05, 4.9137357564177364e-05, 4.8737867473391816e-05, 4.834482024307363e-05, 4.79580630781129e-05, 4.757744318339974e-05, 4.7202815039781854e-05, 4.683404404204339e-05, 4.647098830901086e-05, 4.611352051142603e-05, 4.576150968205184e-05, 4.541483212960884e-05, 4.5073367800796404e-05, 4.47369966423139e-05, 4.4405613152775913e-05, 4.4079100916860625e-05, 4.375735443318263e-05, 4.344027183833532e-05, 4.3127754906890914e-05, 4.2819698137464e-05, 4.251601058058441e-05, 4.221660492476076e-05, 4.1921382944565266e-05, 4.1630260966485366e-05, 4.1343155317008495e-05, 4.10599859606009e-05, 4.07806655857712e-05, 4.050512143294327e-05, 4.023327346658334e-05, 3.9965052565094084e-05, 3.970038233092055e-05, 3.94391936424654e-05, 3.9181424654088914e-05, 3.892699896823615e-05, 3.867585473926738e-05, 3.842793375952169e-05, 3.818317054538056e-05, 3.794150325120427e-05, 3.77028773073107e-05, 3.746723450603895e-05, 3.7234520277706906e-05, 3.700467641465366e-05, 3.6777655623154715e-05, 3.655339969554916e-05, 3.633186497609131e-05, 3.611299689509906e-05, 3.589675179682672e-05, 3.56830823875498e-05, 3.547193773556501e-05, 3.526328146108426e-05, 3.505706263240427e-05, 3.485324123175815e-05, 3.465177724137902e-05, 3.4452630643500015e-05, 3.425575778237544e-05, 3.4061122278217226e-05, 3.38686877512373e-05, 3.367841418366879e-05, 3.349026519572362e-05, 3.330420804559253e-05, 3.3120209991466254e-05, 3.2938227377599105e-05, 3.275823837611824e-05, 3.2580206607235596e-05, 3.240409569116309e-05, 3.2229880162049085e-05, 3.205752727808431e-05, 3.1887007935438305e-05, 3.171829303028062e-05, 3.15513570967596e-05, 3.138616739306599e-05, 3.122269481536932e-05, 3.106092117377557e-05, 3.090081372647546e-05, 3.0742347007617354e-05, 3.05854991893284e-05, 3.0430242986767553e-05, 3.027655475307256e-05, 3.0124410841381177e-05, 2.9973789423820563e-05, 2.9824665034539066e-05, 2.967701766465325e-05, 2.953082548629027e-05, 2.9386066671577282e-05, 2.924272121163085e-05, 2.910076545958873e-05, 2.8960183044546284e-05, 2.882095031964127e-05, 2.8683050913969055e-05, 2.85464648186462e-05, 2.841117384377867e-05, 2.827715979947243e-05, 2.8144402676844038e-05, 2.8012886104988866e-05, 2.788259371300228e-05, 2.7753507310990244e-05, 2.762561234703753e-05, 2.7498888812260702e-05, 2.7373322154744528e-05, 2.724889782257378e-05, 2.7125601263833232e-05, 2.7003412469639443e-05, 2.6882322345045395e-05, 2.676231088116765e-05, 2.6643367164069787e-05, 2.6525476641836576e-05, 2.640862476255279e-05, 2.62927969743032e-05, 2.6177980544161983e-05, 2.6064164558192715e-05, 2.5951332645490766e-05, 2.583947207313031e-05, 2.572857374616433e-05, 2.5618623112677597e-05, 2.5509607439744286e-05, 2.5401515813427977e-05, 2.5294335500802845e-05, 2.518805740692187e-05, 2.508266879885923e-05, 2.49781569436891e-05, 2.487451274646446e-05, 2.4771725293248892e-05, 2.4669785489095375e-05, 2.4568678782088682e-05, 2.4468399715260603e-05, 2.4368933736695908e-05, 2.4270273570436984e-05, 2.4172410121536814e-05, 2.4075332476058975e-05, 2.397903153905645e-05, 2.3883496396592818e-05, 2.378872159169987e-05, 2.3694694391451776e-05, 2.3601407519890927e-05], "accuracy_train_last": 0.9992940512048193, "error_valid": [0.8942307692307693, 0.9229433760683761, 0.9099893162393162, 0.9031784188034188, 0.8932959401709402, 0.8979700854700855, 0.8932959401709402, 0.9030448717948718, 0.9030448717948718, 0.6983173076923077, 0.5714476495726496, 0.5040064102564102, 0.4163995726495726, 0.36057692307692313, 0.36044337606837606, 0.32037927350427353, 0.3092948717948718, 0.29260149572649574, 0.27443910256410253, 0.2764423076923077, 0.2550747863247863, 0.24372329059829057, 0.24158653846153844, 0.2490651709401709, 0.2228899572649573, 0.21861645299145294, 0.2206196581196581, 0.20873397435897434, 0.2132745726495726, 0.20713141025641024, 0.20178952991452992, 0.203125, 0.20766559829059827, 0.20459401709401714, 0.2012553418803419, 0.19871794871794868, 0.19310897435897434, 0.19284188034188032, 0.19751602564102566, 0.19537927350427353, 0.19631410256410253, 0.19177350427350426, 0.19244123931623935, 0.1916399572649573, 0.1932425213675214, 0.1865651709401709, 0.1916399572649573, 0.19431089743589747, 0.18856837606837606, 0.19310897435897434, 0.1868322649572649, 0.18870192307692313, 0.18883547008547008, 0.19204059829059827, 0.18696581196581197, 0.1852297008547008, 0.18509615384615385, 0.18322649572649574, 0.1865651709401709, 0.18456196581196582, 0.18309294871794868, 0.18189102564102566, 0.1842948717948718, 0.1836271367521367, 0.1840277777777778, 0.18295940170940173, 0.1856303418803419, 0.1849626068376068, 0.1826923076923077, 0.17801816239316237, 0.1824252136752137, 0.18122329059829057, 0.18042200854700852, 0.17948717948717952, 0.17948717948717952, 0.1808226495726496, 0.17775106837606836, 0.1810897435897436, 0.1782852564102564, 0.17815170940170943, 0.1760149572649573, 0.17962072649572647, 0.17868589743589747, 0.17935363247863245, 0.17841880341880345, 0.17721688034188032, 0.1769497863247863, 0.17588141025641024, 0.17654914529914534, 0.1769497863247863, 0.1773504273504274, 0.17548076923076927, 0.18162393162393164, 0.17775106837606836, 0.1785523504273504, 0.17895299145299148, 0.17521367521367526, 0.17561431623931623, 0.17641559829059827, 0.17307692307692313, 0.17654914529914534, 0.17561431623931623, 0.17494658119658124, 0.17454594017094016, 0.17494658119658124, 0.17548076923076927, 0.1750801282051282, 0.17881944444444442, 0.1741452991452992, 0.1760149572649573, 0.17561431623931623, 0.1776175213675214, 0.1769497863247863, 0.1750801282051282, 0.1744123931623932, 0.17548076923076927, 0.1769497863247863, 0.17881944444444442, 0.17454594017094016, 0.17788461538461542, 0.1741452991452992, 0.17721688034188032, 0.1769497863247863, 0.17614850427350426, 0.17521367521367526, 0.17775106837606836, 0.17494658119658124, 0.17494658119658124, 0.17387820512820518, 0.1741452991452992, 0.17681623931623935, 0.17361111111111116, 0.17654914529914534, 0.1766826923076923, 0.17454594017094016, 0.17240918803418803, 0.17321047008547008, 0.17641559829059827, 0.17427884615384615, 0.17267628205128205, 0.17481303418803418, 0.1737446581196581, 0.1753472222222222, 0.1766826923076923, 0.1744123931623932, 0.17427884615384615, 0.17307692307692313, 0.1725427350427351, 0.1753472222222222, 0.17481303418803418, 0.17521367521367526, 0.1773504273504274, 0.17334401709401714, 0.17654914529914534, 0.17895299145299148, 0.17240918803418803, 0.17401175213675213, 0.17467948717948723, 0.1734775641025641, 0.17307692307692313, 0.17134081196581197, 0.17334401709401714, 0.1744123931623932, 0.17721688034188032, 0.17548076923076927, 0.1712072649572649, 0.17361111111111116, 0.17214209401709402, 0.1725427350427351, 0.1737446581196581, 0.17294337606837606, 0.17294337606837606, 0.17334401709401714, 0.17654914529914534, 0.1734775641025641, 0.1734775641025641, 0.17067307692307687, 0.17307692307692313, 0.17147435897435892, 0.17147435897435892, 0.17387820512820518, 0.171875, 0.17628205128205132, 0.17240918803418803, 0.17134081196581197, 0.17361111111111116, 0.17174145299145294, 0.17561431623931623, 0.1702724358974359, 0.17294337606837606, 0.17334401709401714, 0.16947115384615385, 0.1725427350427351, 0.17067307692307687, 0.17321047008547008, 0.17147435897435892, 0.17401175213675213, 0.17160790598290598, 0.17267628205128205, 0.17401175213675213, 0.17200854700854706, 0.17427884615384615, 0.17147435897435892, 0.16786858974358976, 0.17214209401709402, 0.17134081196581197, 0.17361111111111116, 0.17013888888888884, 0.1728098290598291, 0.17361111111111116, 0.1725427350427351, 0.17227564102564108, 0.1725427350427351, 0.17147435897435892, 0.1741452991452992, 0.17240918803418803, 0.17307692307692313, 0.17013888888888884, 0.17294337606837606, 0.1753472222222222, 0.17294337606837606, 0.17053952991452992, 0.17080662393162394, 0.16920405982905984, 0.17080662393162394, 0.16947115384615385, 0.1712072649572649, 0.17654914529914534, 0.17427884615384615, 0.1757478632478633, 0.17053952991452992, 0.16973824786324787, 0.17321047008547008, 0.16947115384615385, 0.1728098290598291, 0.1712072649572649, 0.16973824786324787, 0.1693376068376068, 0.17334401709401714, 0.17174145299145294, 0.17080662393162394, 0.17227564102564108, 0.1737446581196581, 0.17267628205128205, 0.17147435897435892, 0.1700053418803419, 0.17588141025641024, 0.1734775641025641, 0.17454594017094016, 0.171875, 0.17080662393162394, 0.17134081196581197, 0.1725427350427351, 0.17160790598290598], "accuracy_train_std": [0.05324680372246714, 0.04761812866970481, 0.05047466965321544, 0.05422438399619695, 0.05420047762503763, 0.05291967208935282, 0.054132597771574496, 0.05417779742901447, 0.05417779742901447, 0.08165814480153663, 0.08829727283341592, 0.08874635895217269, 0.08575456791513675, 0.08286532887563566, 0.08226741436850889, 0.08039691688703607, 0.07947577193654475, 0.07956579864521808, 0.07729445146804632, 0.07772000010263104, 0.07198653014203676, 0.07115015990669511, 0.06992459145074137, 0.0732965556197842, 0.06569389122093912, 0.06328546038649834, 0.0654876329616803, 0.06103956954622387, 0.06143597101621706, 0.05799532982987891, 0.05686463678277498, 0.05489334845726786, 0.05699279783985359, 0.05500885003400426, 0.05264169487492959, 0.0517091309203878, 0.051723671287382464, 0.047585929511814756, 0.047326207936184514, 0.047189495592940174, 0.0459239933564501, 0.04473806566725038, 0.04387082232917992, 0.044389199751497514, 0.043879486475480275, 0.04156603885547121, 0.04007318541388419, 0.04087906137551814, 0.03921800245455952, 0.03954209594312085, 0.03600070461834398, 0.03990513593418479, 0.03852528296425363, 0.035155769598720105, 0.034544523889019456, 0.03337172107932749, 0.034362613057704196, 0.032500975803627756, 0.03165380773499113, 0.03156944638481532, 0.029872368758957137, 0.029947459548903724, 0.02911660110237253, 0.030445196184133327, 0.029442456625500813, 0.028880956802874207, 0.027364995139076746, 0.02921327041204892, 0.029232143492394856, 0.02627883261637082, 0.026808203460921175, 0.02609800602349777, 0.026998303852372084, 0.025018844778089005, 0.02638628887088045, 0.02474398361978111, 0.024972355875430927, 0.022878744130149613, 0.022720537879331103, 0.021408969819176914, 0.022880631897184006, 0.023865663767756208, 0.02145350138543486, 0.024015682151988734, 0.021847375930125826, 0.020495168622173397, 0.024206976294832466, 0.02187160567417206, 0.019955207767160605, 0.019283403379922633, 0.019994683325244466, 0.020064613602464896, 0.022555416916284588, 0.020186995005101783, 0.01801164248407271, 0.019769241405860956, 0.019634346946584978, 0.020355373800805692, 0.018189018786017277, 0.017877594749938806, 0.01864275195669736, 0.017927068921964193, 0.01780154713943556, 0.017316974948485955, 0.015505734920488049, 0.015607696090476821, 0.016552086535900436, 0.015138208266167491, 0.016213751725699877, 0.0167625638245131, 0.015143822070593328, 0.01384643774982721, 0.015308150515508509, 0.014855819786721798, 0.013040831671481469, 0.01600564082718313, 0.01543617303517185, 0.014180885227870997, 0.013550294362144659, 0.014684168609164047, 0.013150453199092521, 0.013361240123833813, 0.01196182270031903, 0.012624514760002904, 0.01276169376389924, 0.01342713866110927, 0.01323440097343279, 0.012076631556225702, 0.011761082445000721, 0.011747255696429315, 0.01281933814759145, 0.013983331219292875, 0.011438960604295808, 0.012821584115727643, 0.0108893221410018, 0.011880745131927344, 0.011220317766315683, 0.010216845818262729, 0.010845782562603105, 0.011739404662407998, 0.010836001012986306, 0.010582196248693915, 0.012003897625482662, 0.010751764700769537, 0.01041491818622561, 0.009795009500229835, 0.009914050467310287, 0.010276156488111905, 0.010399062135978689, 0.011302629948218734, 0.010377607367013859, 0.012892840511134708, 0.00834799167802563, 0.010176006945523378, 0.010176006945523378, 0.009249306283322434, 0.008285137823736024, 0.008423369138631322, 0.009388617662206241, 0.008747405415507255, 0.009178179023530802, 0.008472660076059891, 0.00907744502212774, 0.009776449219681155, 0.00859155764496013, 0.008605210482247657, 0.009103272899566169, 0.008532835142243305, 0.008654521471364906, 0.00975004768105062, 0.008259265688457333, 0.00894050077290366, 0.009734217894896743, 0.008618584699434636, 0.006822061628496642, 0.008208457372222496, 0.00855930365497684, 0.0074471556747177335, 0.006909167659451268, 0.006386571871807339, 0.007414777798618785, 0.00676299928313797, 0.007359344544198749, 0.007133732017779026, 0.007236080372557516, 0.006946534550294492, 0.006552068602931156, 0.006663357234468428, 0.005832806244288278, 0.006752838841340134, 0.005723170223479014, 0.007061958340347996, 0.006860871597234456, 0.005974623897800584, 0.007061958340347996, 0.00645843484106439, 0.006713404367858363, 0.007630635617285859, 0.0069356849462124445, 0.007684400269768737, 0.005276071943105206, 0.005832806244288278, 0.005723170223479014, 0.006592720198288761, 0.008024215630555123, 0.0070764104402155515, 0.00559320576684698, 0.004871842377244268, 0.005013451546813746, 0.0057827984663598965, 0.004329563491004113, 0.005428772245127813, 0.004643547797533743, 0.005005935289193275, 0.0046435477975337435, 0.0045672577471588295, 0.004871842377244268, 0.004329563491004113, 0.004792139330330518, 0.004718487203764208, 0.004489548199439692, 0.005284042320100856, 0.0057314849949757435, 0.004489548199439692, 0.005142992959745359, 0.0037110650789145354, 0.005531434043697957, 0.004489548199439692, 0.005714455762943081, 0.0039945585794282645, 0.005723170223479014, 0.0035129967918325522, 0.0039041939855193482, 0.0035129967918325522, 0.0057827984663598965, 0.005005935289193276, 0.004489548199439692, 0.0033024020045028615, 0.004496203585166293, 0.0054214232353915455, 0.004799298157502509, 0.003409407055021618, 0.003988732166869363, 0.004935811247915851, 0.0031916381463076003, 0.0029571306612208225, 0.005210010518593235, 0.004169017984567214, 0.0056628163696527565, 0.004247116351604866, 0.004335953616822673, 0.003898516600052375, 0.005284042320100856, 0.00479929815750251], "accuracy_test_std": 0.0669956823855661, "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-3, 3], "momentum": 0.5039790701731423, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.005994757708930311, "patience_threshold": 1, "do_flip": true, "batch_size": 32, "optimization": "rmsprop", "nb_data_augmentation": 3, "learning_rate_decay_method": "lin", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 1.214924303592963e-07, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.05229691950888345}, "accuracy_valid_max": 0.8321314102564102, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = 1234423\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-10, -4], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128, 256],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-3, 3)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_optimizer.learning_rate = learning_rate\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.828392094017094, "loss_train": [7221180628992.0, 18170632994816.0, 35730747392.0, 197302.953125, 523.8547973632812, 65.88185119628906, 7.306777477264404, 4.610654830932617, 2.4930853843688965, 1.973430871963501, 1.6008843183517456, 1.338867425918579, 1.153771996498108, 1.030871868133545, 0.9358386993408203, 0.8662382364273071, 0.804082453250885, 0.7545285224914551, 0.7101228833198547, 0.6707032322883606, 0.636753261089325, 0.6073739528656006, 0.5803119540214539, 0.5553648471832275, 0.5323837995529175, 0.509765625, 0.4871108829975128, 0.4681362509727478, 0.4525693953037262, 0.4371049404144287, 0.4228556156158447, 0.40664246678352356, 0.3933378756046295, 0.3788222372531891, 0.36817625164985657, 0.35674333572387695, 0.34540238976478577, 0.33487987518310547, 0.327134370803833, 0.3152509331703186, 0.30594852566719055, 0.3001842200756073, 0.2888740301132202, 0.2821969985961914, 0.2758446931838989, 0.2690775990486145, 0.26321762800216675, 0.25421178340911865, 0.24902793765068054, 0.2417096197605133, 0.23538191616535187, 0.22861146926879883, 0.22531187534332275, 0.2206268459558487, 0.21506249904632568, 0.2078198790550232, 0.20433242619037628, 0.20072749257087708, 0.19645391404628754, 0.19251760840415955, 0.18849579989910126, 0.18321672081947327, 0.17812877893447876, 0.17671945691108704, 0.1714104413986206, 0.16857735812664032, 0.16525806486606598, 0.1613602191209793, 0.1584552824497223, 0.15773800015449524, 0.15089508891105652, 0.15036652982234955, 0.14714694023132324, 0.14310452342033386, 0.14075665175914764, 0.13950258493423462, 0.1354770064353943, 0.13569872081279755, 0.13194012641906738, 0.12814706563949585, 0.1272777020931244, 0.12522120773792267, 0.12093842774629593, 0.12121395021677017, 0.11879837512969971, 0.11779338866472244, 0.11428102850914001, 0.11298366636037827, 0.11075003445148468, 0.10963098704814911, 0.10780824720859528, 0.104853555560112, 0.10449999570846558, 0.104632169008255, 0.10066112130880356, 0.10164516419172287, 0.09821057319641113, 0.09647919982671738, 0.09462325274944305, 0.09385133534669876, 0.09292914718389511, 0.09114687889814377, 0.08932141214609146, 0.0912335067987442, 0.08722008764743805, 0.0852828174829483, 0.08564919978380203, 0.08355176448822021, 0.08394753932952881, 0.08278755843639374, 0.07920843362808228, 0.08056587725877762, 0.07862920314073563, 0.07809557765722275, 0.07514262199401855, 0.0753660649061203, 0.0747404620051384, 0.0729997307062149, 0.07399589568376541, 0.07187401503324509, 0.07125312834978104, 0.0700489804148674, 0.06955435127019882, 0.06788568198680878, 0.06754163652658463, 0.06823756545782089, 0.0664965882897377, 0.064612977206707, 0.06484761834144592, 0.06337116658687592, 0.0636020228266716, 0.06298864632844925, 0.06116073951125145, 0.06169270724058151, 0.061452534049749374, 0.05988192930817604, 0.05997871607542038, 0.05897166207432747, 0.05681132897734642, 0.05706335976719856, 0.05649653822183609, 0.05642049014568329, 0.055144183337688446, 0.054359979927539825, 0.05480867996811867, 0.05388939380645752, 0.05289594084024429, 0.052392080426216125, 0.05352163314819336, 0.05208927020430565, 0.05226627364754677, 0.05184444040060043, 0.05036116763949394, 0.04982348158955574, 0.04873669892549515, 0.048950888216495514, 0.04788903892040253, 0.047501202672719955, 0.04909957945346832, 0.04767278581857681, 0.046035800129175186, 0.04701553285121918, 0.04784128814935684, 0.046082038432359695, 0.04515191540122032, 0.04399479180574417, 0.04500539228320122, 0.04529602453112602, 0.043474193662405014, 0.043217919766902924, 0.0425548329949379, 0.042640846222639084, 0.042667489498853683, 0.04249386861920357, 0.04051078483462334, 0.04077770560979843, 0.041542042046785355, 0.0406917929649353, 0.04027101397514343, 0.039587780833244324, 0.041183989495038986, 0.03920723870396614, 0.040323760360479355, 0.0387260839343071, 0.03913802281022072, 0.0384586825966835, 0.039058078080415726, 0.03701924905180931, 0.03647110238671303, 0.036852218210697174, 0.036343008279800415, 0.036495670676231384, 0.036607708781957626, 0.03640851750969887, 0.0350191630423069, 0.03713282570242882, 0.03516945242881775, 0.03493136912584305, 0.03348749503493309, 0.03470969945192337, 0.03288682922720909, 0.034388989210128784, 0.03421677276492119, 0.03373820334672928, 0.03320950269699097, 0.03382875397801399, 0.0318550169467926, 0.03242356330156326, 0.033250898122787476, 0.033207401633262634, 0.031532615423202515, 0.03152499720454216, 0.0311591699719429, 0.032957110553979874, 0.031219270080327988, 0.031094495207071304, 0.03108849748969078, 0.031754471361637115, 0.0299751628190279, 0.03058800846338272, 0.031201232224702835, 0.03026769682765007, 0.030953356996178627, 0.030123203992843628, 0.028681011870503426, 0.03023695759475231, 0.028452346101403236, 0.02925158105790615, 0.028791584074497223, 0.029131339862942696, 0.02805866301059723, 0.028299611061811447, 0.028580931946635246, 0.027829542756080627, 0.02716953121125698, 0.02787698619067669, 0.027526631951332092, 0.027300015091896057, 0.02740316651761532, 0.027090292423963547, 0.027413204312324524, 0.027454333379864693, 0.02625691518187523, 0.02704453282058239, 0.027009839192032814, 0.026306014508008957, 0.02634345553815365, 0.02541346661746502, 0.02639363706111908, 0.02548726089298725, 0.02662680856883526, 0.025769734755158424, 0.025095650926232338, 0.02560281939804554], "accuracy_train_first": 0.09906814759036145, "model": "residualv2", "loss_std": [112570748895232.0, 230825064923136.0, 246008889344.0, 1736674.625, 3106.1767578125, 950.3092651367188, 44.383811950683594, 29.10633087158203, 2.056483507156372, 0.29642677307128906, 0.1987248659133911, 0.1994803249835968, 0.2012709528207779, 0.19613038003444672, 0.18796397745609283, 0.18580390512943268, 0.18260712921619415, 0.17949062585830688, 0.17685897648334503, 0.17427565157413483, 0.1710190325975418, 0.16661803424358368, 0.16320765018463135, 0.15795885026454926, 0.15671296417713165, 0.15589457750320435, 0.1489751636981964, 0.14633364975452423, 0.14349812269210815, 0.14294014871120453, 0.14022214710712433, 0.13864029943943024, 0.13345837593078613, 0.1310279220342636, 0.12969300150871277, 0.12615831196308136, 0.12547357380390167, 0.1211174800992012, 0.12103747576475143, 0.11758199334144592, 0.11724680662155151, 0.11332067847251892, 0.11206574738025665, 0.10897111892700195, 0.10734222829341888, 0.10438656806945801, 0.10516196489334106, 0.10321676731109619, 0.09856849163770676, 0.09957531839609146, 0.09756317734718323, 0.09609141945838928, 0.09452302753925323, 0.09428077936172485, 0.09226007014513016, 0.08968295156955719, 0.08807582408189774, 0.08597460389137268, 0.08298229426145554, 0.08470586687326431, 0.08221165090799332, 0.08063221722841263, 0.07960917800664902, 0.07864062488079071, 0.07789898663759232, 0.07591861486434937, 0.07631827890872955, 0.07302162051200867, 0.07255839556455612, 0.07209927588701248, 0.07171668857336044, 0.07013507932424545, 0.06799548864364624, 0.06758318096399307, 0.06725788116455078, 0.06552908569574356, 0.06440825760364532, 0.06475389003753662, 0.061054036021232605, 0.06293328106403351, 0.06263311207294464, 0.06168613210320473, 0.05948184058070183, 0.058680351823568344, 0.05895521491765976, 0.05588756501674652, 0.05615897849202156, 0.05657193437218666, 0.057461801916360855, 0.05543779581785202, 0.054091405123472214, 0.05441724509000778, 0.054475508630275726, 0.05291212350130081, 0.052707280963659286, 0.05132373422384262, 0.05181797966361046, 0.05078722536563873, 0.05038757249712944, 0.04920323193073273, 0.04773996025323868, 0.04786450415849686, 0.04791668802499771, 0.04806498438119888, 0.045882806181907654, 0.04525288939476013, 0.04686659574508667, 0.045014865696430206, 0.046283796429634094, 0.04452478140592575, 0.044233422726392746, 0.044820886105298996, 0.042403694242239, 0.04228857532143593, 0.04201986640691757, 0.04190216585993767, 0.041021134704351425, 0.041922204196453094, 0.041273679584264755, 0.04013430327177048, 0.039866141974925995, 0.040384385734796524, 0.039528194814920425, 0.03801837936043739, 0.038383886218070984, 0.03806845843791962, 0.037675440311431885, 0.037507131695747375, 0.03710809350013733, 0.03633921220898628, 0.03686671704053879, 0.03740977495908737, 0.035597022622823715, 0.03718272224068642, 0.03553367406129837, 0.035869747400283813, 0.035199690610170364, 0.03468485549092293, 0.03440985083580017, 0.03456677496433258, 0.033579885959625244, 0.035355404019355774, 0.03413301333785057, 0.03300292044878006, 0.034213170409202576, 0.03173943981528282, 0.03288021683692932, 0.03245793282985687, 0.033293891698122025, 0.032865580171346664, 0.03319680690765381, 0.03301575034856796, 0.030764130875468254, 0.03074520267546177, 0.03092951513826847, 0.03093518503010273, 0.029722603037953377, 0.030224543064832687, 0.03130346164107323, 0.030024684965610504, 0.03008991852402687, 0.02952258475124836, 0.031522978097200394, 0.03024357371032238, 0.02824844792485237, 0.028608083724975586, 0.030053503811359406, 0.029579855501651764, 0.029502755030989647, 0.028700819239020348, 0.027435049414634705, 0.028393810614943504, 0.02817557565867901, 0.028610849753022194, 0.028186844661831856, 0.027864402160048485, 0.02933233603835106, 0.02805303782224655, 0.027344923466444016, 0.02745973877608776, 0.029138954356312752, 0.02639138512313366, 0.027929408475756645, 0.026085486635565758, 0.025859689339995384, 0.026039239019155502, 0.027242131531238556, 0.02581489458680153, 0.02437303029000759, 0.02551594376564026, 0.02432684414088726, 0.024548804387450218, 0.02468215487897396, 0.02591310627758503, 0.023941759020090103, 0.025931330397725105, 0.02420356310904026, 0.025128835812211037, 0.023711705580353737, 0.025080420076847076, 0.022952847182750702, 0.024414217099547386, 0.024321557953953743, 0.024458643049001694, 0.023511135950684547, 0.023911384865641594, 0.02290983311831951, 0.02382940612733364, 0.024118710309267044, 0.023475417867302895, 0.02319778874516487, 0.022286109626293182, 0.022502006962895393, 0.024077558889985085, 0.022965727373957634, 0.02237717993557453, 0.023264776915311813, 0.024459056556224823, 0.022259073331952095, 0.022942662239074707, 0.022567855194211006, 0.02231580950319767, 0.022567695006728172, 0.02233654074370861, 0.021954677999019623, 0.02339925989508629, 0.02152036875486374, 0.022169198840856552, 0.020118502900004387, 0.02212832123041153, 0.022085711359977722, 0.020978301763534546, 0.021688196808099747, 0.021606234833598137, 0.020462404936552048, 0.021148325875401497, 0.022736113518476486, 0.021247785538434982, 0.02133728191256523, 0.021103717386722565, 0.021767033264040947, 0.02075195126235485, 0.020534396171569824, 0.020559262484312057, 0.021393422037363052, 0.02009350061416626, 0.019646048545837402, 0.01934184692800045, 0.020692866295576096, 0.019771527498960495, 0.02159767597913742, 0.019941246137022972, 0.019243160262703896, 0.019997401162981987]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:05 2016", "state": "available"}], "summary": "03af8d15725e6d8cd1df4cf90bb818c6"}