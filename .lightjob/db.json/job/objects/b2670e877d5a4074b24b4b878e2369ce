{"content": {"hp_model": {"f0": 16, "f1": 16, "f2": 64, "f3": 16, "nonlin": "leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.571225881576538, 1.1740710735321045, 0.9576485753059387, 0.8402062058448792, 0.7609788775444031, 0.7064828276634216, 0.662962794303894, 0.6281747221946716, 0.5979443192481995, 0.5707085728645325, 0.5473015308380127, 0.5251173973083496, 0.505510151386261, 0.48749855160713196, 0.47055140137672424, 0.4545256793498993, 0.4414685368537903, 0.42664849758148193, 0.4137212634086609, 0.4006568193435669, 0.39045044779777527, 0.3803817927837372, 0.3695368766784668, 0.3578138053417206, 0.35213759541511536, 0.3414522111415863, 0.33456018567085266, 0.32594674825668335, 0.3164643943309784, 0.3100017309188843, 0.3015988767147064, 0.29474538564682007, 0.28743240237236023, 0.28003373742103577, 0.27490606904029846, 0.2679794132709503, 0.26077044010162354, 0.25736555457115173, 0.252063512802124, 0.24463331699371338, 0.24210231006145477, 0.2356625199317932, 0.2299068570137024, 0.2251681089401245, 0.22188571095466614, 0.21643473207950592, 0.2104196399450302, 0.20954184234142303, 0.20196354389190674, 0.19889970123767853, 0.19434691965579987, 0.1918531060218811, 0.18758392333984375, 0.1836571991443634, 0.17956802248954773, 0.1768254041671753, 0.17277634143829346, 0.17022854089736938, 0.1653043031692505, 0.1632307767868042, 0.16322186589241028, 0.15708915889263153, 0.1554432213306427, 0.15078973770141602, 0.15014825761318207, 0.1446370631456375, 0.14390595257282257, 0.14153899252414703, 0.13993918895721436, 0.13592016696929932, 0.13623832166194916, 0.13227421045303345, 0.12858587503433228, 0.1276092231273651, 0.12525764107704163, 0.12375608086585999, 0.12026045471429825, 0.11978688836097717, 0.11596160382032394, 0.11412563920021057, 0.11384914070367813, 0.111455537378788, 0.1102147027850151, 0.10750984400510788, 0.10711316019296646, 0.10455939173698425, 0.10262081027030945, 0.10232210904359818, 0.0979405865073204, 0.09707185626029968, 0.09865662455558777, 0.09468004107475281, 0.09399963915348053, 0.09276217967271805, 0.09105779230594635, 0.09039098024368286, 0.08752348273992538, 0.08869680017232895, 0.08642062544822693, 0.08442475646734238, 0.08306578546762466, 0.08259154111146927, 0.08272969722747803, 0.08059562742710114, 0.07899661362171173, 0.07842855900526047, 0.07659091800451279, 0.07596508413553238, 0.07519044727087021, 0.0744728073477745, 0.07272720336914062, 0.07409191131591797, 0.07205961644649506, 0.07045429199934006, 0.06800617277622223, 0.07065363228321075, 0.06883623450994492, 0.06793683767318726, 0.06625598669052124, 0.06660310924053192, 0.06608504801988602, 0.062975212931633, 0.06526626646518707, 0.06430759280920029, 0.06247046962380409, 0.06203491613268852, 0.060613568872213364, 0.0610402375459671, 0.059381384402513504, 0.05854356661438942, 0.05855579301714897, 0.059289008378982544, 0.05774638056755066, 0.05684512108564377, 0.055016808211803436, 0.05617595463991165, 0.0566612109541893, 0.05383054539561272, 0.05490102991461754, 0.05302639305591583, 0.053828585892915726, 0.05234406888484955, 0.052526332437992096, 0.05234881862998009, 0.05174890160560608, 0.0514867939054966, 0.05080633610486984, 0.04914931580424309, 0.05071588233113289, 0.04973559081554413, 0.04831613600254059, 0.04979104921221733, 0.0491153784096241, 0.04871251806616783, 0.04697483032941818, 0.04557136818766594, 0.04795847833156586, 0.045469220727682114, 0.046751126646995544, 0.04521369934082031, 0.044167324900627136, 0.04579847306013107, 0.044846419245004654, 0.0439695380628109, 0.04435517638921738, 0.04505448043346405, 0.04273412004113197, 0.04342259094119072, 0.04228994995355606, 0.04351169243454933, 0.042746592313051224, 0.042497411370277405, 0.04006514698266983, 0.04164823517203331, 0.04053513705730438, 0.04175398871302605, 0.0398813895881176, 0.04007863998413086, 0.041678622364997864, 0.03994698077440262, 0.039971910417079926, 0.038918863981962204, 0.03955044224858284, 0.03752891719341278, 0.03858101740479469, 0.03752522170543671, 0.03840474784374237, 0.03873319551348686, 0.0374920554459095, 0.03680938854813576, 0.037009697407484055, 0.03670363500714302, 0.03704733029007912, 0.0376577153801918, 0.03457196056842804, 0.03602486848831177, 0.036219578236341476, 0.036148905754089355, 0.036300286650657654, 0.03586264327168465, 0.03482218086719513, 0.03445255011320114, 0.03448934853076935, 0.03467879444360733, 0.03516816720366478, 0.034033071249723434, 0.03382005915045738, 0.03368750587105751, 0.034204933792352676, 0.035234853625297546, 0.033622972667217255, 0.033394187688827515, 0.0340728722512722, 0.03256205841898918, 0.03330812230706215, 0.03246953338384628, 0.03363993018865585, 0.03262491524219513, 0.032529234886169434, 0.03235624358057976, 0.03162066638469696, 0.030811632052063942, 0.03210269659757614, 0.031511396169662476, 0.03256625309586525, 0.03142737224698067, 0.03133811429142952, 0.03082108683884144, 0.03079341910779476, 0.0308347400277853, 0.030110280960798264, 0.030414199456572533, 0.031035808846354485, 0.029649974778294563, 0.03081420063972473, 0.030322743579745293, 0.03024948202073574, 0.030022552236914635, 0.029145171865820885, 0.029066020622849464, 0.02963627688586712, 0.02917507477104664, 0.028066063299775124, 0.02925535850226879, 0.02984045445919037, 0.028675001114606857, 0.027577465400099754, 0.028111563995480537, 0.02898310497403145, 0.02918447181582451, 0.028234416618943214, 0.02788528800010681, 0.028045551851391792, 0.027480849996209145, 0.028377315029501915, 0.028470756486058235, 0.02823485992848873, 0.027958808466792107, 0.02915290743112564, 0.027673719450831413, 0.02698522061109543, 0.027533842250704765, 0.02704738639295101, 0.027220072224736214, 0.027778280898928642, 0.02753707394003868, 0.026693565770983696, 0.02676595374941826, 0.026542499661445618, 0.027020037174224854, 0.026257723569869995, 0.026357287541031837, 0.026792434975504875, 0.025979865342378616, 0.02644651010632515, 0.026023386046290398, 0.02556440979242325, 0.026597389951348305, 0.025800591334700584, 0.025819456204771996, 0.025520583614706993, 0.025712307542562485, 0.026265420019626617, 0.026029277592897415, 0.02477334998548031, 0.027384057641029358, 0.024545684456825256, 0.02418803982436657, 0.02501099742949009, 0.025042060762643814, 0.026036687195301056, 0.024468667805194855, 0.02612461894750595, 0.025121595710515976, 0.024259554222226143, 0.02521533891558647, 0.025049487128853798, 0.024309467524290085, 0.024587979540228844, 0.024868382140994072, 0.023755675181746483, 0.024109365418553352, 0.024389419704675674, 0.024517960846424103, 0.024084018543362617, 0.025083502754569054, 0.024081118404865265, 0.024415871128439903, 0.023459739983081818, 0.024226732552051544, 0.022976526990532875, 0.02382102981209755, 0.023905416950583458, 0.023808280006051064, 0.02363770827651024, 0.023986684158444405], "moving_avg_accuracy_train": [0.05614352941176469, 0.11687035294117645, 0.17558802352941172, 0.23291863294117643, 0.2872808872941176, 0.33781632797647054, 0.3856535187082352, 0.42997522566094115, 0.4705871148595529, 0.5078060504324211, 0.5421266218597672, 0.5736621949679082, 0.6030606813534704, 0.6298863779240057, 0.6547753871904286, 0.6772531425890328, 0.6984172400948354, 0.7178908102029989, 0.735781729182699, 0.7520694386173703, 0.7671166124026921, 0.7807743629271289, 0.7935228089873572, 0.8053917045592097, 0.816353710573877, 0.8266783395164892, 0.836269329094252, 0.8446518079495328, 0.852513685978109, 0.8599446703214745, 0.8668890268187388, 0.873430712372159, 0.8793888176055313, 0.8847158181979194, 0.8899242363781275, 0.8945365186226677, 0.8991487491133421, 0.9033821094961255, 0.9072603691347482, 0.9109390381036263, 0.9143180754697342, 0.9175992090992313, 0.9204086999540141, 0.9232525358409656, 0.9260190469627514, 0.9286383187370645, 0.9309438986280639, 0.9329765675887869, 0.9350342049475553, 0.9372037256292703, 0.9390127648310492, 0.9406550177597091, 0.9424130453955029, 0.9441129173265408, 0.9457769197115338, 0.9471592277403803, 0.9482927167310481, 0.9498493274108843, 0.9513632181992077, 0.9524292493204634, 0.953461618506064, 0.9544848684201634, 0.9560246168722647, 0.9570339198909206, 0.957728174960652, 0.9585906515822339, 0.9595504099534222, 0.9603977218992564, 0.9613179497093307, 0.96227791944428, 0.9629371863233814, 0.9635399382792785, 0.964157709157233, 0.9644572323591567, 0.9649150385350058, 0.9656988287991523, 0.9665524753310017, 0.967054874856725, 0.9679117403122289, 0.9676358603986531, 0.9682863920058465, 0.9690648116287913, 0.969687742230618, 0.9701777915369679, 0.9703058947362123, 0.9704800111449441, 0.9711425982657438, 0.971576573733287, 0.9722095045952523, 0.9727579659004331, 0.9728468751927427, 0.9732515994381743, 0.9737429100825922, 0.9740462661331565, 0.9738722277551349, 0.9742285343913861, 0.9746409750698946, 0.9749227599158462, 0.9754187192183792, 0.9756250825906588, 0.9755402213904165, 0.9759956110160808, 0.9760972263850609, 0.9766357390406725, 0.9766286357248405, 0.9765893015641212, 0.9769821361135913, 0.9772745107375262, 0.9771611773108325, 0.9769415301679846, 0.9771650242100097, 0.9770908747301852, 0.9771841401983431, 0.9772680791196853, 0.9778024476783049, 0.9780316146751804, 0.9778402179135447, 0.9783267843574843, 0.9785317529805595, 0.9790550482707389, 0.9790718963848415, 0.9791411773345926, 0.9787588243070157, 0.9787088242292552, 0.9790050006298591, 0.9795515593904025, 0.9800905210984211, 0.9803873513415201, 0.9804192044426623, 0.9805819898807491, 0.9807073203044389, 0.9805965882739949, 0.9808875176818895, 0.9796599423842888, 0.9796092422635069, 0.9800859650959797, 0.9802185450569699, 0.9807049258453906, 0.9811567862020281, 0.9814858134641782, 0.981645467411878, 0.9815444500824548, 0.9819758874271506, 0.9823830045667885, 0.9826223511689332, 0.982790704287334, 0.9832104573880123, 0.9835576469433287, 0.9836065881313488, 0.9835518116711551, 0.9834719246216866, 0.9838023792183416, 0.9839139060023898, 0.984148397755092, 0.9842370873913475, 0.9839616139463304, 0.9841913349046385, 0.9845369072965275, 0.9844949812727571, 0.9847160713807755, 0.9840679936544626, 0.9845035472301928, 0.984580251330703, 0.9843951673741033, 0.984626238871987, 0.9845118502789059, 0.9848089005451329, 0.9852338928435608, 0.9847387388533223, 0.9850295708503429, 0.9853101431770733, 0.9855155994476014, 0.9857922747969589, 0.9858859884937335, 0.986156213173772, 0.9857641212681595, 0.9859782973766377, 0.9862063499919151, 0.9863598326397824, 0.9868438493758042, 0.9869641703205767, 0.9871336356414602, 0.9871355661949612, 0.9874337742813475, 0.9877186321473304, 0.9878691218737738, 0.9878822096863965, 0.9879763416589333, 0.9881810604342165, 0.9882829543907948, 0.9884640707164213, 0.9883706048212497, 0.9883311913979483, 0.9883663075522711, 0.9882073238558675, 0.9880712973526337, 0.9880806382056055, 0.9883549273262214, 0.9883641404759522, 0.9883418440754158, 0.9885853067266978, 0.9887738348775573, 0.9890046866839192, 0.9889536297802332, 0.9888229726845628, 0.9890065577690477, 0.989096490227437, 0.989090370616458, 0.989240157084224, 0.989447906081684, 0.989590174297045, 0.9895276274555758, 0.9894666294159006, 0.9895858488272516, 0.9898060874739383, 0.9901313610794856, 0.9902758720303605, 0.9902106377685009, 0.9902625151681215, 0.9905280283571917, 0.9904822843450019, 0.9907234676752076, 0.9908017091429809, 0.991062714699271, 0.991027031464638, 0.990823151847586, 0.9908867190157685, 0.9907909882906623, 0.9909354188733608, 0.9911195240448482, 0.9912216892874222, 0.9912006968292683, 0.9913818036169296, 0.9913989173728837, 0.9915954962238306, 0.9916594760132122, 0.9917264695883615, 0.9917655873354078, 0.9916407933077492, 0.9915755375063862, 0.991625042579277, 0.991686067733114, 0.9914892256656849, 0.9914461854520575, 0.9914686257303812, 0.9915405866867548, 0.9914853515474911, 0.9916074046280361, 0.9915925465181737, 0.9918262330428269, 0.9918200803267796, 0.9919651311176311, 0.9918909709470444, 0.9918383444405752, 0.9918333335259294, 0.9919488237027483, 0.9921045295677675, 0.9922470177874613, 0.9922929042440093, 0.9923036138196084, 0.9923650171435299, 0.9923402801350593, 0.9924803697686122, 0.9924582151446921, 0.9925559230419877, 0.9926697425024947, 0.9927651211934218, 0.9927921384858442, 0.9927576305196127, 0.9928277498205926, 0.9929473277797098, 0.9930902420605624, 0.9929741590309767, 0.9928179195984672, 0.9929290688150911, 0.9929349854629939, 0.9927450163284592, 0.9928963970485545, 0.9929667573436991, 0.9928300816093293, 0.9926929558013375, 0.9928283661035567, 0.9927008236108481, 0.9927154471321162, 0.9926580200659634, 0.9927781004123082, 0.9929026433122539, 0.9930782613339697, 0.9932951410829257, 0.9931068034452213, 0.9931020054536404, 0.9931965107906293, 0.9933427420645076, 0.9933378796227628, 0.9934252681310748, 0.9934733295532614, 0.9934742318920529, 0.9933903381146122, 0.9935430690090332, 0.9935440562257769, 0.9935308270737875, 0.9936224502487616, 0.9936060875768267, 0.9936078317603205, 0.9933223427019355, 0.9934489319611537, 0.9935816858238617, 0.9936141054767698, 0.9936409302232104, 0.9933709548479481, 0.9935162123043298, 0.993595179309191, 0.9935791907900365, 0.9936989187698564, 0.9937525563046354, 0.9938996536153484], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.056333333333333326, 0.11571333333333332, 0.17306199999999997, 0.2286091333333333, 0.28128155333333327, 0.33011339799999995, 0.37607539153333325, 0.4188945190466666, 0.4578450671419999, 0.49327389376113323, 0.52578650438502, 0.555327853946518, 0.5826350685518662, 0.6071582283633463, 0.6302157388603451, 0.6505274983076439, 0.6694747484768795, 0.6871939402958582, 0.703101212932939, 0.7171910916396451, 0.7301653158090139, 0.7417754508947791, 0.7524645724719679, 0.7625647818914377, 0.7715749703689606, 0.7798441399987313, 0.7878330593321915, 0.7944230867323057, 0.8004607780590751, 0.806548033586501, 0.8118132302278509, 0.8166852405383992, 0.8212033831512259, 0.82506971150277, 0.8287494070191597, 0.8323144663172437, 0.8354963530188527, 0.8385867177169674, 0.841221379278604, 0.8437392413507435, 0.8460853172156692, 0.8482501188274356, 0.8496784402780254, 0.8514972629168895, 0.8531608699585339, 0.8548847829626806, 0.8559163046664124, 0.8567913408664379, 0.8581922067797941, 0.8594129861018147, 0.8602716874916333, 0.86104451874247, 0.8620467335348897, 0.8630153935147341, 0.8643005208299273, 0.8646838020802678, 0.8651487552055744, 0.8659405463516836, 0.8665598250498485, 0.8668105092115304, 0.8674094582903774, 0.8678285124613397, 0.8686456612152058, 0.8689010950936852, 0.8689176522509834, 0.869265887025885, 0.8696992983232965, 0.8701027018243002, 0.8704257649752034, 0.8711165218110164, 0.8711115362965814, 0.8711870493335899, 0.871228344400231, 0.8714255099602078, 0.8713362922975204, 0.8714159964011017, 0.8720210634276582, 0.871965623751559, 0.8725423947097364, 0.8723548219054293, 0.8729860063815531, 0.8733407390767312, 0.8735799985023913, 0.8740753319854856, 0.8740944654536037, 0.8739116855749101, 0.874160517017419, 0.8741044653156771, 0.8744940187841094, 0.8746712835723651, 0.874497488548462, 0.8743410730269492, 0.8747869657242543, 0.8750282691518289, 0.8745387755699793, 0.874711564679648, 0.8748270748783498, 0.8754243673905148, 0.8756552639847965, 0.8756497375863169, 0.8757114304943519, 0.87603362077825, 0.875790258700425, 0.8762512328303825, 0.8760394428806776, 0.8760221652592766, 0.8762866154000156, 0.876377953860014, 0.876313491807346, 0.8753354759599448, 0.8750819283639504, 0.874880402194222, 0.8752056953081332, 0.8756584591106532, 0.8757059465329212, 0.8759220185462958, 0.8757031500249995, 0.8759595016891663, 0.8761368848535829, 0.8765231963682246, 0.8768442100647356, 0.8766264557249287, 0.8761904768191026, 0.8762247624705257, 0.8762956195568065, 0.8766127242677925, 0.8772447851743466, 0.8774136399902452, 0.8773256093245541, 0.8773130483920987, 0.8774217435528888, 0.8772529025309332, 0.87744761227784, 0.8764361843833893, 0.8764192326117171, 0.8765506426838787, 0.8762555784154908, 0.8766433539072751, 0.8771123518498809, 0.8770144499982262, 0.876966338331737, 0.87685637116523, 0.8771574007153736, 0.8774283273105029, 0.8775254945794526, 0.8778262784548407, 0.8783636506093566, 0.8782739522150876, 0.8779798903269123, 0.878261901294221, 0.8781290444981322, 0.8783294733816523, 0.8783231927101537, 0.8786108734391384, 0.8787764527618911, 0.8787654741523686, 0.8790355934037983, 0.8791853673967518, 0.8792134973237432, 0.8794121475913689, 0.8788175994988987, 0.8792291728823421, 0.8791862555941079, 0.8787876300346971, 0.8792022003645608, 0.8788286469947714, 0.8789457822952942, 0.8791978707324314, 0.8787847503258549, 0.878772941959936, 0.8789489810972758, 0.8791207496542148, 0.87939534135546, 0.878935807219914, 0.879215559831256, 0.8789740038481304, 0.8791832701299841, 0.8797716097836523, 0.8801011154719538, 0.8807443372580916, 0.8808032368656158, 0.8810962465123875, 0.8813999551944821, 0.8817132930083672, 0.8820486303741971, 0.8822437673367773, 0.8823660572697664, 0.882382784876123, 0.8824645063885107, 0.882831389082993, 0.8828815835080269, 0.8824067584905575, 0.8824327493081684, 0.8825761410440182, 0.882425193606283, 0.882316007578988, 0.8823244068210891, 0.8824386328056469, 0.8826747695250823, 0.8827539592392407, 0.8831985633153167, 0.8833053736504517, 0.8830415029520732, 0.8832173526568659, 0.883362284057846, 0.8833993889853947, 0.8831794500868553, 0.8833415050781698, 0.8836606879036861, 0.8836812857799842, 0.8838464905353192, 0.8840218414817872, 0.8838463240002752, 0.8841283582669144, 0.8844488557735563, 0.8848839701962006, 0.8849422398432472, 0.8846080158589225, 0.8844938809396969, 0.884591159512394, 0.8845587102278213, 0.8846361725383726, 0.8847858886178687, 0.8849206330894152, 0.8849352364471402, 0.8846683794690929, 0.8845882081888503, 0.8841293873699653, 0.884289781966302, 0.8845008037696718, 0.8846240567260379, 0.8846016510534341, 0.8846748192814241, 0.8847006706866151, 0.8851106036179536, 0.8851995432561582, 0.8853462555972091, 0.8855582967041549, 0.8851224670337394, 0.8849702203303655, 0.885073198297329, 0.8847792118009294, 0.8844212906208365, 0.8845391615587529, 0.884498578736211, 0.8844753875292566, 0.8845345154429975, 0.8848543972320311, 0.885088957508828, 0.8853267284246118, 0.885167388915484, 0.8852239833572688, 0.8853282516882086, 0.8852087598527211, 0.8853412172007822, 0.8854470954807039, 0.8857690525993002, 0.8856188140060368, 0.8856835992720998, 0.8853685726782231, 0.8853917154104007, 0.8853725438693606, 0.8855819561490912, 0.8854237605341821, 0.8856413844807639, 0.8861572460326874, 0.8863281880960854, 0.8861353692864768, 0.8861618323578291, 0.8859989824553794, 0.8858257508765082, 0.8859231757888574, 0.8858775248766383, 0.8858497723889744, 0.885944795150077, 0.8858703156350692, 0.8856166174048956, 0.8857482889977394, 0.8860134600979654, 0.8854387807548356, 0.8852815693460186, 0.8854334124114168, 0.8855834045036084, 0.8856517307199142, 0.8857265576479227, 0.8856872352164639, 0.8859585116948174, 0.8858693271920023, 0.8857890611394686, 0.8855568216921885, 0.8854411395229697, 0.8856170255706727, 0.885921989680272, 0.8860497907122449, 0.8861648116410205, 0.8858549971435851, 0.8858294974292266, 0.8857398810196372, 0.8859258929176734, 0.8859199702925729, 0.8860746399299823, 0.8859738426036506, 0.8860564583432856, 0.8862108125089571, 0.8858830645913947, 0.8859214247989219, 0.8859959489856963, 0.8862630207537934, 0.8866633853450807, 0.886610380143906, 0.8868293421295154, 0.8869997412498972, 0.8870464337915741, 0.8869951237457501, 0.8872689447045085, 0.8873287169007243], "moving_var_accuracy_train": [0.02836886305328719, 0.05872170061170933, 0.083879414104315, 0.10507266167360241, 0.12116268779124702, 0.13203089589675884, 0.1394233776610481, 0.1431607632597571, 0.14368861683230427, 0.1417869976356697, 0.13820941247999885, 0.13333890257332906, 0.1277834513318549, 0.12148166816711928, 0.11490866639078415, 0.10796504514154105, 0.10119981183650302, 0.09449281004767075, 0.08792429388034742, 0.0815194697999672, 0.07540527977030156, 0.06954355913776115, 0.06405190911653999, 0.05891455434374585, 0.05410458909216168, 0.049653511848169164, 0.04551604439307905, 0.04159683351960419, 0.03799343230286965, 0.03469106482738478, 0.031655975129096174, 0.028875520465105, 0.026307459580341953, 0.023932106040109483, 0.02178304401555783, 0.019796197941531753, 0.018008032178270537, 0.01636852102161817, 0.014867036999877485, 0.013502126748333017, 0.0122546751151937, 0.011126100144725886, 0.010084529280021268, 0.009148862974986363, 0.008302858931570409, 0.007534318300062788, 0.006828727757760538, 0.006183040669919465, 0.005602841446429312, 0.005084918681681885, 0.004605880419015851, 0.004169565329249491, 0.0037804247468384744, 0.0034283883533920037, 0.003110469653488164, 0.0028166196675188724, 0.0025465208763946726, 0.0023136761200324317, 0.002102935295899921, 0.0019028695674733003, 0.0017221746859443713, 0.0015593805808302758, 0.0014247799504089834, 0.001291470188619296, 0.0011666610806739973, 0.0010566897659115748, 0.0009593110145000129, 0.0008698413508519925, 0.0007904785887687016, 0.0007197246069199988, 0.00065166384158892, 0.0005897672467130686, 0.0005342252897605999, 0.00048161018812095587, 0.00043533544776067, 0.0003973308475881403, 0.00036415617444137413, 0.0003300122045482602, 0.0003036189497729584, 0.00027394204233609405, 0.00025035656045010475, 0.00023077433838956344, 0.0002111892873628373, 0.0001922316935304403, 0.0001731562180443061, 0.00015611344495398212, 0.00014445329569243073, 0.00013170297848105227, 0.00012213809391720167, 0.0001126315727550056, 0.00010143955923983604, 9.276981874941411e-05, 8.566531221833737e-05, 7.792700503722941e-05, 7.040690874672577e-05, 6.450880764338309e-05, 5.958889269864132e-05, 5.4344627723449494e-05, 5.112394561902594e-05, 4.6394823629891316e-05, 4.182015407666135e-05, 3.950455606945864e-05, 3.5647031611429545e-05, 3.4692291372570886e-05, 3.122351634917608e-05, 2.8115089300053925e-05, 2.66924512193659e-05, 2.479255238391945e-05, 2.2428897335982937e-05, 2.062021140863608e-05, 1.9007736549158805e-05, 1.7156446202467122e-05, 1.5519087610176917e-05, 1.4030590531803998e-05, 1.5197479286595418e-05, 1.4150388970047801e-05, 1.3065044556324668e-05, 1.388926224000491e-05, 1.2878445244012113e-05, 1.4055142366125894e-05, 1.2652182860052628e-05, 1.1430163224033159e-05, 1.1602891440904888e-05, 1.0465102366798805e-05, 1.0208076272590997e-05, 1.1875806953873584e-05, 1.3302543762878866e-05, 1.2765263125555255e-05, 1.1497868393471052e-05, 1.0586573443801673e-05, 9.669285535341882e-06, 8.812711224903707e-06, 8.69319938581464e-06, 2.1386349448748446e-05, 1.9270849024099218e-05, 1.9389146052697683e-05, 1.7608428461933537e-05, 1.7976682057842495e-05, 1.8016613889162626e-05, 1.7189282953388517e-05, 1.5699759105194942e-05, 1.4221623702269407e-05, 1.4474704973625092e-05, 1.451893376474493e-05, 1.3582621551894254e-05, 1.247944434898229e-05, 1.2817233903846004e-05, 1.2620375799348711e-05, 1.1379895378377179e-05, 1.0268909985861636e-05, 9.299456453330444e-06, 9.352312972050319e-06, 8.529025686886485e-06, 8.171000556965957e-06, 7.424693165481558e-06, 7.3651944191197285e-06, 7.103620445381828e-06, 7.468040903167204e-06, 6.737056936073249e-06, 6.5032787652382416e-06, 9.632993542799816e-06, 1.0377056444502158e-05, 9.392302471367547e-06, 8.761376863146241e-06, 8.365785511039983e-06, 7.646969711979536e-06, 7.676422486771695e-06, 8.534346321601683e-06, 9.88750895588283e-06, 9.660007314714078e-06, 9.40249405798492e-06, 8.842155164079735e-06, 8.646882888150687e-06, 7.861234912004243e-06, 7.732303820120563e-06, 8.342698000130006e-06, 7.921270849102678e-06, 7.597215722206184e-06, 7.04950645875281e-06, 8.453005619619906e-06, 7.737999225416486e-06, 7.222665757714206e-06, 6.500432725274169e-06, 6.650742017821987e-06, 6.7159638503507924e-06, 6.248191885200889e-06, 5.6249143142340015e-06, 5.142170337093646e-06, 5.005141295965142e-06, 4.59806857185334e-06, 4.4334898253436555e-06, 4.068763704851245e-06, 3.6758680957931674e-06, 3.3193795848636657e-06, 3.214923967876618e-06, 3.0599604573271105e-06, 2.7547496754025884e-06, 3.1563854030565857e-06, 2.841510801902587e-06, 2.5618338870042457e-06, 2.8391170614269262e-06, 2.875091128283225e-06, 3.0672150239599742e-06, 2.7839547882900197e-06, 2.6592007993021662e-06, 2.6966120685799176e-06, 2.4997414853695054e-06, 2.250104383579363e-06, 2.22701781855367e-06, 2.392752850208841e-06, 2.335639771106042e-06, 2.137284760395398e-06, 1.9570431319538374e-06, 1.8892582311446358e-06, 2.1368779614792663e-06, 2.8754164315232927e-06, 2.7758255226760574e-06, 2.536542550691733e-06, 2.307109676945016e-06, 2.710873991382615e-06, 2.4586192241052963e-06, 2.736281890616749e-06, 2.5177492470690337e-06, 2.8790894260910786e-06, 2.6026401225868193e-06, 2.71647819457171e-06, 2.4811974389513077e-06, 2.315557040620525e-06, 2.271743075526081e-06, 2.3496211954892897e-06, 2.2085987070522064e-06, 1.991704986041106e-06, 2.0877315042703237e-06, 1.8815942796290014e-06, 2.041224053422628e-06, 1.8739423691242492e-06, 1.7269413842134734e-06, 1.5680190289978608e-06, 1.5513790701509698e-06, 1.434566039639815e-06, 1.3131662058531425e-06, 1.2153662098753098e-06, 1.442550784475448e-06, 1.3149678459296625e-06, 1.188003156157884e-06, 1.115808253721977e-06, 1.031685713835121e-06, 1.0625897326863584e-06, 9.583176302758822e-07, 1.3539703934887595e-06, 1.2189140573727226e-06, 1.286380238975135e-06, 1.2072397931905654e-06, 1.111441756519828e-06, 1.0005235642581283e-06, 1.020513036307093e-06, 1.1366605802890248e-06, 1.205720557023806e-06, 1.104098603372203e-06, 9.947209981205912e-07, 9.291822120060312e-07, 8.417712670981137e-07, 9.342200892491628e-07, 8.452155265735936e-07, 8.466154726613913e-07, 8.785477517064356e-07, 8.725668286823149e-07, 7.91879552622683e-07, 7.234087949613097e-07, 6.953183627943657e-07, 7.544765212746675e-07, 8.628492941916035e-07, 8.978417925925023e-07, 1.02775445577109e-06, 1.03616634539886e-06, 9.328647713606087e-07, 1.1643727429072733e-06, 1.2541805703655585e-06, 1.1733176535244591e-06, 1.2241081954620046e-06, 1.2709287608723329e-06, 1.3088594343089185e-06, 1.3243772778949562e-06, 1.193864176473976e-06, 1.1041585701688423e-06, 1.1235163193566368e-06, 1.1507630927628347e-06, 1.3132619894488697e-06, 1.6052672200688028e-06, 1.7639800900463631e-06, 1.587789267550625e-06, 1.5093916692700335e-06, 1.5509047714835322e-06, 1.3960270843926805e-06, 1.32515513841831e-06, 1.213428727299903e-06, 1.0920931825075647e-06, 1.0462273572960472e-06, 1.1515451565625868e-06, 1.0363994122784203e-06, 9.343345652118203e-07, 9.16454364421817e-07, 8.272185612753046e-07, 7.445240847323144e-07, 1.403607698377023e-06, 1.4074704934840705e-06, 1.4253357367111025e-06, 1.2922614680920347e-06, 1.1695114244772865e-06, 1.708540611261332e-06, 1.7275841078454003e-06, 1.6109477877715027e-06, 1.4521537036971036e-06, 1.435951435693126e-06, 1.3182491583584e-06, 1.3811628118933425e-06], "duration": 288494.102642, "accuracy_train": [0.561435294117647, 0.6634117647058824, 0.7040470588235294, 0.7488941176470588, 0.7765411764705883, 0.7926352941176471, 0.8161882352941177, 0.8288705882352941, 0.8360941176470589, 0.8427764705882353, 0.8510117647058824, 0.8574823529411765, 0.8676470588235294, 0.8713176470588235, 0.8787764705882353, 0.8795529411764705, 0.8888941176470588, 0.8931529411764706, 0.8968, 0.8986588235294117, 0.9025411764705882, 0.9036941176470589, 0.9082588235294118, 0.9122117647058824, 0.9150117647058823, 0.9196, 0.9225882352941176, 0.9200941176470588, 0.9232705882352941, 0.9268235294117647, 0.9293882352941176, 0.9323058823529412, 0.9330117647058823, 0.9326588235294118, 0.9368, 0.9360470588235295, 0.9406588235294118, 0.9414823529411764, 0.9421647058823529, 0.9440470588235295, 0.9447294117647059, 0.9471294117647059, 0.9456941176470588, 0.9488470588235294, 0.9509176470588235, 0.9522117647058823, 0.9516941176470588, 0.9512705882352941, 0.9535529411764706, 0.9567294117647058, 0.9552941176470588, 0.955435294117647, 0.9582352941176471, 0.9594117647058824, 0.9607529411764706, 0.9596, 0.9584941176470588, 0.9638588235294118, 0.9649882352941177, 0.9620235294117647, 0.9627529411764706, 0.9636941176470588, 0.9698823529411764, 0.9661176470588235, 0.9639764705882353, 0.9663529411764706, 0.9681882352941177, 0.9680235294117647, 0.9696, 0.9709176470588236, 0.9688705882352942, 0.968964705882353, 0.9697176470588236, 0.9671529411764705, 0.9690352941176471, 0.9727529411764706, 0.9742352941176471, 0.9715764705882353, 0.9756235294117647, 0.9651529411764705, 0.9741411764705883, 0.9760705882352941, 0.9752941176470589, 0.9745882352941176, 0.9714588235294118, 0.9720470588235294, 0.9771058823529412, 0.9754823529411765, 0.9779058823529412, 0.9776941176470588, 0.9736470588235294, 0.9768941176470588, 0.9781647058823529, 0.9767764705882352, 0.9723058823529411, 0.9774352941176471, 0.9783529411764705, 0.9774588235294117, 0.9798823529411764, 0.9774823529411765, 0.9747764705882352, 0.9800941176470588, 0.9770117647058824, 0.9814823529411765, 0.9765647058823529, 0.9762352941176471, 0.9805176470588235, 0.9799058823529412, 0.9761411764705883, 0.974964705882353, 0.9791764705882353, 0.9764235294117647, 0.9780235294117647, 0.9780235294117647, 0.9826117647058824, 0.9800941176470588, 0.9761176470588235, 0.9827058823529412, 0.9803764705882353, 0.983764705882353, 0.9792235294117647, 0.979764705882353, 0.9753176470588235, 0.9782588235294117, 0.9816705882352941, 0.9844705882352941, 0.9849411764705882, 0.9830588235294118, 0.9807058823529412, 0.9820470588235294, 0.981835294117647, 0.9796, 0.9835058823529412, 0.9686117647058824, 0.9791529411764706, 0.9843764705882353, 0.9814117647058823, 0.9850823529411765, 0.9852235294117647, 0.9844470588235295, 0.9830823529411765, 0.980635294117647, 0.9858588235294118, 0.9860470588235294, 0.9847764705882353, 0.9843058823529411, 0.9869882352941176, 0.9866823529411765, 0.9840470588235294, 0.9830588235294118, 0.9827529411764706, 0.9867764705882353, 0.9849176470588236, 0.9862588235294117, 0.985035294117647, 0.9814823529411765, 0.9862588235294117, 0.9876470588235294, 0.9841176470588235, 0.9867058823529412, 0.9782352941176471, 0.9884235294117647, 0.9852705882352941, 0.9827294117647059, 0.9867058823529412, 0.9834823529411765, 0.9874823529411765, 0.9890588235294118, 0.9802823529411765, 0.9876470588235294, 0.987835294117647, 0.9873647058823529, 0.9882823529411765, 0.9867294117647059, 0.9885882352941177, 0.9822352941176471, 0.9879058823529412, 0.9882588235294117, 0.9877411764705882, 0.9912, 0.9880470588235294, 0.9886588235294118, 0.9871529411764706, 0.9901176470588235, 0.9902823529411765, 0.9892235294117647, 0.988, 0.9888235294117647, 0.9900235294117647, 0.9892, 0.9900941176470588, 0.9875294117647059, 0.9879764705882353, 0.9886823529411765, 0.9867764705882353, 0.9868470588235294, 0.988164705882353, 0.9908235294117647, 0.9884470588235295, 0.9881411764705882, 0.9907764705882353, 0.9904705882352941, 0.9910823529411765, 0.9884941176470589, 0.9876470588235294, 0.9906588235294118, 0.9899058823529412, 0.989035294117647, 0.9905882352941177, 0.9913176470588235, 0.9908705882352942, 0.988964705882353, 0.9889176470588236, 0.9906588235294118, 0.9917882352941176, 0.9930588235294118, 0.9915764705882353, 0.9896235294117647, 0.9907294117647059, 0.9929176470588236, 0.9900705882352941, 0.9928941176470588, 0.9915058823529411, 0.9934117647058823, 0.9907058823529412, 0.9889882352941176, 0.9914588235294117, 0.9899294117647058, 0.9922352941176471, 0.9927764705882353, 0.9921411764705882, 0.9910117647058824, 0.9930117647058824, 0.9915529411764706, 0.9933647058823529, 0.9922352941176471, 0.9923294117647059, 0.9921176470588235, 0.9905176470588235, 0.9909882352941176, 0.9920705882352941, 0.9922352941176471, 0.9897176470588235, 0.9910588235294118, 0.9916705882352941, 0.9921882352941176, 0.9909882352941176, 0.9927058823529412, 0.9914588235294117, 0.9939294117647058, 0.991764705882353, 0.9932705882352941, 0.9912235294117647, 0.9913647058823529, 0.9917882352941176, 0.9929882352941176, 0.9935058823529411, 0.9935294117647059, 0.9927058823529412, 0.9924, 0.9929176470588236, 0.9921176470588235, 0.9937411764705882, 0.9922588235294117, 0.9934352941176471, 0.9936941176470588, 0.9936235294117647, 0.993035294117647, 0.9924470588235295, 0.9934588235294117, 0.9940235294117648, 0.9943764705882353, 0.9919294117647058, 0.9914117647058823, 0.9939294117647058, 0.9929882352941176, 0.991035294117647, 0.9942588235294118, 0.9936, 0.9916, 0.9914588235294117, 0.9940470588235294, 0.9915529411764706, 0.9928470588235294, 0.9921411764705882, 0.9938588235294118, 0.9940235294117648, 0.9946588235294118, 0.9952470588235294, 0.9914117647058823, 0.9930588235294118, 0.9940470588235294, 0.9946588235294118, 0.9932941176470588, 0.9942117647058824, 0.9939058823529412, 0.9934823529411765, 0.9926352941176471, 0.9949176470588236, 0.9935529411764706, 0.9934117647058823, 0.9944470588235295, 0.9934588235294117, 0.9936235294117647, 0.9907529411764706, 0.9945882352941177, 0.9947764705882353, 0.9939058823529412, 0.9938823529411764, 0.9909411764705882, 0.9948235294117647, 0.9943058823529412, 0.9934352941176471, 0.9947764705882353, 0.9942352941176471, 0.9952235294117647], "end": "2016-02-07 16:43:31.390000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0, 211.0, 212.0, 213.0, 214.0, 215.0, 216.0, 217.0, 218.0, 219.0, 220.0, 221.0, 222.0, 223.0, 224.0, 225.0, 226.0, 227.0, 228.0, 229.0, 230.0, 231.0, 232.0, 233.0, 234.0, 235.0, 236.0, 237.0, 238.0, 239.0, 240.0, 241.0, 242.0, 243.0, 244.0, 245.0, 246.0, 247.0, 248.0, 249.0, 250.0, 251.0, 252.0, 253.0, 254.0, 255.0, 256.0, 257.0, 258.0, 259.0, 260.0, 261.0, 262.0, 263.0, 264.0, 265.0, 266.0, 267.0, 268.0, 269.0, 270.0, 271.0, 272.0, 273.0, 274.0, 275.0, 276.0, 277.0, 278.0, 279.0, 280.0, 281.0, 282.0, 283.0, 284.0, 285.0, 286.0, 287.0, 288.0, 289.0, 290.0, 291.0, 292.0, 293.0, 294.0, 295.0, 296.0, 297.0, 298.0, 299.0, 300.0, 301.0, 302.0, 303.0, 304.0, 305.0, 306.0, 307.0, 308.0, 309.0, 310.0, 311.0, 312.0, 313.0, 314.0, 315.0], "moving_var_accuracy_valid": [0.028560999999999996, 0.05743875959999999, 0.08129470975599999, 0.10093459497436, 0.11581058993483159, 0.1256904724232936, 0.1321339688269878, 0.13542187107331694, 0.13553399073832711, 0.1332774074649719, 0.12946329536469894, 0.12437118783346063, 0.11864522477563744, 0.1121931706023282, 0.10575869265496823, 0.09889593153607582, 0.09223732298324863, 0.08583931851338363, 0.07953275856679905, 0.07336620484784635, 0.06754455879823502, 0.06200326004879898, 0.05683124992474639, 0.05206625300512609, 0.04759027917221786, 0.04344666375228936, 0.03967640286610923, 0.03609961872970661, 0.03281774030575207, 0.029869458393882446, 0.02713201321554294, 0.024632440254183444, 0.022352918742793666, 0.020252163322811885, 0.01834880842237074, 0.016628314410323367, 0.015056602596127915, 0.013636895522221306, 0.012335678943898464, 0.01115916771423749, 0.01009278759048962, 0.009125686125605416, 0.008231478432540809, 0.00743810363141153, 0.006719201563771457, 0.006074028291807102, 0.005476201795853823, 0.004935472811430635, 0.004459587358052402, 0.004027041341624819, 0.0036309735201542235, 0.003273251581419229, 0.002954966333688609, 0.002667914419728717, 0.002415986947702148, 0.002175710393583697, 0.0019600849869039184, 0.0017697188871850392, 0.0015961985534205434, 0.0014371442810187521, 0.0012966585129083415, 0.001168573119201315, 0.001057725396054687, 0.0009525400746456936, 0.0008572885344362444, 0.0007726510881186772, 0.0006970765874813248, 0.0006288335381947907, 0.0005668895125705559, 0.0005144948663695016, 0.0004630456034307391, 0.00041679236305648947, 0.00037512847429360045, 0.00033796549518660946, 0.0003042405837899672, 0.0002738737001081198, 0.00024978128505694113, 0.00022483081857042084, 0.0002053417193571511, 0.00018512419943367622, 0.00017019732407640495, 0.00015431010923401886, 0.00013939430396552234, 0.00012766307090423824, 0.00011490005862023445, 0.0001037107291147086, 9.389690998426769e-05, 8.453549512525438e-05, 7.744771275563761e-05, 6.998574672647197e-05, 6.325901444682634e-05, 5.7153305340475e-05, 5.322735748401751e-05, 4.8428667833048876e-05, 4.574223674979113e-05, 4.1436717762593055e-05, 3.7413129440371076e-05, 3.688264160212965e-05, 3.367419657717536e-05, 3.0307051789179224e-05, 2.7310600744377608e-05, 2.5513799881285188e-05, 2.3495445801466368e-05, 2.3058375557730332e-05, 2.115623284712141e-05, 1.9043296208220787e-05, 1.7768371479830654e-05, 1.6066618760321563e-05, 1.4497355090397033e-05, 2.1656254561269407e-05, 2.0069206556053685e-05, 1.842780107421682e-05, 1.7537361456417116e-05, 1.7628580858626875e-05, 1.588601827022713e-05, 1.471760047787798e-05, 1.3676971296619822e-05, 1.2900719748447213e-05, 1.1893830856768883e-05, 1.2047577048194952e-05, 1.1770267483503618e-05, 1.1019993307695855e-05, 1.1628692433855043e-05, 1.0476402743511083e-05, 9.473949009245771e-06, 9.431552687886919e-06, 1.2083906325444002e-05, 1.113212323256919e-05, 1.0088655492230572e-05, 9.08120993622486e-06, 8.279420684415099e-06, 7.708044232228521e-06, 7.278446778869809e-06, 1.575747957203905e-05, 1.4184317877900605e-05, 1.2921303553700267e-05, 1.2412739500643647e-05, 1.2524794038835916e-05, 1.3251946266468762e-05, 1.2013014592838765e-05, 1.0832545725626275e-05, 9.858126152449914e-06, 9.687882647742241e-06, 9.379705362503252e-06, 8.526708129649226e-06, 8.488275773925462e-06, 1.0238367688574723e-05, 9.286943137127185e-06, 9.136500370110197e-06, 8.938622004241078e-06, 8.203618158219717e-06, 7.744801978539802e-06, 6.9706768021960775e-06, 7.01845093843861e-06, 6.563354453704334e-06, 5.908103777137316e-06, 5.973973089360571e-06, 5.57846602111162e-06, 5.027741054133376e-06, 4.88012430816982e-06, 7.573498785692733e-06, 8.340682756755377e-06, 7.523191523744244e-06, 8.200993400909905e-06, 8.927711086448026e-06, 9.290819058532475e-06, 8.485223260336468e-06, 8.208638155547515e-06, 8.92379057296195e-06, 8.032666453216809e-06, 7.508307808772945e-06, 7.023016962272098e-06, 6.999320687579408e-06, 8.199933214409573e-06, 8.084293604942285e-06, 7.801007881302021e-06, 7.415038483659377e-06, 9.788826567999821e-06, 9.787109898806845e-06, 1.253200730438866e-05, 1.1310029047848301e-05, 1.0951718020975282e-05, 1.0686696891094401e-05, 1.0501652472477707e-05, 1.0463547565525853e-05, 9.759898716458831e-06, 8.918502294206865e-06, 8.029170380116e-06, 7.286358992386796e-06, 7.769149296743201e-06, 7.01490968980936e-06, 8.342547895761396e-06, 7.514372809586015e-06, 6.947986237817668e-06, 6.458253774665164e-06, 5.919722694206766e-06, 5.328385350196961e-06, 4.91297499511094e-06, 4.923522447990868e-06, 4.487609300648232e-06, 5.817903430753364e-06, 5.338789116902881e-06, 5.431559914377397e-06, 5.166711991020731e-06, 4.839086790829103e-06, 4.367569092581766e-06, 4.366170255140408e-06, 4.165909611515776e-06, 4.6662177353056094e-06, 4.2034144143469585e-06, 4.028706473579667e-06, 3.902567416067002e-06, 3.7895681513073216e-06, 4.12650128420474e-06, 4.638319021657302e-06, 5.878408166629951e-06, 5.32112551586938e-06, 5.794364009563407e-06, 5.332168626686748e-06, 4.884119850371701e-06, 4.405184469958072e-06, 4.018669708965636e-06, 3.818536878206148e-06, 3.600087843896558e-06, 3.241998382018542e-06, 3.5587123644098202e-06, 3.2606880355505116e-06, 4.829268126576426e-06, 4.577879152725225e-06, 4.520863050929671e-06, 4.205498367113689e-06, 3.7894666578857775e-06, 3.458702298381856e-06, 3.1188467248967777e-06, 4.319367126169051e-06, 3.958622746747899e-06, 3.7564810712226833e-06, 3.7854858434134245e-06, 5.116464773602361e-06, 4.813429824436218e-06, 4.4275269971119176e-06, 4.762626837988365e-06, 5.439332294621681e-06, 5.0204410872070385e-06, 4.533219667855583e-06, 4.084738189790055e-06, 3.7077293624613294e-06, 4.2578756568131424e-06, 4.3272548021911315e-06, 4.403344397506306e-06, 4.1915116702775945e-06, 3.801186880818381e-06, 3.5189151562692546e-06, 3.295528329375935e-06, 3.12388003793706e-06, 2.9123839255761123e-06, 3.5540530089518916e-06, 3.401792422208461e-06, 3.099387356277277e-06, 3.6826244142952706e-06, 3.3191822473395814e-06, 2.9905719544782834e-06, 3.0861962851481725e-06, 3.0028093298216327e-06, 3.128770035971938e-06, 5.210911299152143e-06, 4.952810870585593e-06, 4.7921416235767305e-06, 4.319230108527638e-06, 4.125987914225405e-06, 3.983471742066994e-06, 3.6705490897765127e-06, 3.3222502328767645e-06, 2.9969570147328635e-06, 2.778525239407567e-06, 2.550597498868845e-06, 2.8748028769208746e-06, 2.7433592644869367e-06, 3.101864749594127e-06, 5.763985401416461e-06, 5.410025704834532e-06, 5.076529982936604e-06, 4.77135563412322e-06, 4.336236317222933e-06, 3.953004307897448e-06, 3.5716201596502958e-06, 3.87677649305671e-06, 3.560683723632384e-06, 3.262599103973024e-06, 3.4217556414328655e-06, 3.200021355766198e-06, 3.1584423361789234e-06, 3.6796260758548626e-06, 3.4586614022291745e-06, 3.23186358851368e-06, 3.772542435052536e-06, 3.401140310438569e-06, 3.133306187203836e-06, 3.131379404382979e-06, 2.81855716133743e-06, 2.752005715830817e-06, 2.5682460532080464e-06, 2.372849691806092e-06, 2.349991598766599e-06, 3.0817607160881644e-06, 2.786828194173115e-06, 2.558130064485388e-06, 2.9442630218671516e-06, 4.092462973290452e-06, 3.7085026381255805e-06, 3.769151534591139e-06, 3.6535591231740103e-06, 3.307824951890887e-06, 3.0007369439240255e-06, 3.3754645066295906e-06, 3.070072494930794e-06], "accuracy_test": 0.8338, "start": "2016-02-04 08:35:17.288000", "learning_rate_per_epoch": [0.001566707738675177, 0.0011078297393396497, 0.0009045391343533993, 0.0007833538693375885, 0.0007006530067883432, 0.0006396057433448732, 0.0005921598640270531, 0.0005539148696698248, 0.0005222359322942793, 0.0004954364849254489, 0.00047238016850315034, 0.00045226956717669964, 0.0004345265624579042, 0.00041872027213685215, 0.00040452220127917826, 0.0003916769346687943, 0.00037998243351466954, 0.0003692765603773296, 0.0003594274166971445, 0.0003503265033941716, 0.0003418836568016559, 0.0003340232069604099, 0.0003266811545472592, 0.0003198028716724366, 0.0003133415593765676, 0.000307256676023826, 0.0003015130350831896, 0.00029607993201352656, 0.00029093032935634255, 0.00028604039107449353, 0.00028138901689089835, 0.0002769574348349124, 0.00027272882289253175, 0.00026868816348724067, 0.00026482195244170725, 0.00026111796614713967, 0.0002575651742517948, 0.0002541535650379956, 0.000250874029006809, 0.00024771824246272445, 0.0002446786384098232, 0.00024174826103262603, 0.0002389206929365173, 0.00023619008425157517, 0.00023355100711341947, 0.00023099845566321164, 0.0002285278169438243, 0.00022613478358834982, 0.0002238153974758461, 0.0002215659333160147, 0.00021938297140877694, 0.0002172632812289521, 0.00021520386508200318, 0.00021320191444829106, 0.0002112548245349899, 0.00020936013606842607, 0.00020751550619024783, 0.00020571881032083184, 0.00020396798208821565, 0.00020226110063958913, 0.0002005963760893792, 0.00019897209131158888, 0.00019738663104362786, 0.00019583846733439714, 0.00019432617409620434, 0.00019284839800093323, 0.00019140381482429802, 0.00018999121675733477, 0.00018860945419874042, 0.00018725739209912717, 0.0001859339972725138, 0.0001846382801886648, 0.00018336928042117506, 0.00018212608119938523, 0.0001809078239602968, 0.00017971370834857225, 0.00017854291945695877, 0.0001773947151377797, 0.00017626839689910412, 0.0001751632516970858, 0.0001740786392474547, 0.00017301393381785601, 0.00017196852422785014, 0.00017094182840082794, 0.0001699333224678412, 0.00016894243890419602, 0.00016796869749668986, 0.00016701160348020494, 0.00016607069119345397, 0.00016514549497514963, 0.0001642355928197503, 0.0001633405772736296, 0.0001624600263312459, 0.00016159356164280325, 0.00016074081941042095, 0.0001599014358362183, 0.00015907507622614503, 0.00015826137678232044, 0.0001574600610183552, 0.0001566707796882838, 0.0001558932417538017, 0.0001551271852804348, 0.00015437230467796326, 0.000153628338011913, 0.0001528950233478099, 0.0001521721133030951, 0.0001514593604952097, 0.0001507565175415948, 0.00015006338071543723, 0.00014937971718609333, 0.0001487053232267499, 0.00014803996600676328, 0.00014738347090315074, 0.00014673561963718385, 0.0001460962521377951, 0.00014546516467817128, 0.00014484218263532966, 0.00014422714593820274, 0.00014361986541189253, 0.00014302019553724676, 0.00014242797624319792, 0.0001418430620105937, 0.0001412652782164514, 0.00014069450844544917, 0.00014013060717843473, 0.00013957341434434056, 0.00013902282807976007, 0.0001384787174174562, 0.00013794092228636146, 0.00013740935537498444, 0.00013688388571608812, 0.00013636441144626588, 0.00013585078704636544, 0.00013534293975681067, 0.0001348407386103645, 0.00013434408174362034, 0.0001338528818450868, 0.0001333670224994421, 0.00013288641639519483, 0.00013241097622085363, 0.0001319406001130119, 0.00013147520076017827, 0.00013101469085086137, 0.00013055898307356983, 0.00013010800466872752, 0.00012966165377292782, 0.00012921987217850983, 0.0001287825871258974, 0.00012834969675168395, 0.00012792115740012378, 0.00012749686720781028, 0.0001270767825189978, 0.00012666081602219492, 0.00012624890950974077, 0.0001258409902220592, 0.0001254370145034045, 0.00012503689504228532, 0.00012464057363104075, 0.0001242480066139251, 0.00012385912123136222, 0.00012347387382760644, 0.0001230921916430816, 0.0001227140164701268, 0.0001223393192049116, 0.00012196803436381742, 0.0001216001037391834, 0.00012123548367526382, 0.00012087413051631302, 0.00012051597877871245, 0.00012016099935863167, 0.00011980913404840976, 0.00011946034646825865, 0.00011911458568647504, 0.00011877180804731324, 0.00011843197717098519, 0.00011809504212578759, 0.00011776096653193235, 0.00011742971400963143, 0.0001171012336271815, 0.00011677550355670974, 0.00011645247286651284, 0.00011613210517680272, 0.00011581437138374895, 0.00011549922783160582, 0.00011518664541654289, 0.00011487658775877208, 0.00011456901847850531, 0.00011426390847191215, 0.0001139612213592045, 0.00011366092803655192, 0.00011336299212416634, 0.00011306739179417491, 0.00011277409066678956, 0.00011248305963817984, 0.00011219426960451528, 0.00011190769873792306, 0.00011162330338265747, 0.0001113410689868033, 0.00011106096644653007, 0.00011078296665800735, 0.00011050704779336229, 0.00011023317347280681, 0.00010996132914442569, 0.00010969148570438847, 0.00010942361404886469, 0.00010915769962593913, 0.00010889371333178133, 0.00010863164061447605, 0.00010837144509423524, 0.00010811310494318604, 0.00010785661288537085, 0.00010760193254100159, 0.00010734904935816303, 0.00010709793423302472, 0.00010684857988962904, 0.00010660095722414553, 0.00010635505168465897, 0.00010611084144329652, 0.00010586829739622772, 0.00010562741226749495, 0.00010538817150518298, 0.00010515053872950375, 0.00010491451394045725, 0.00010468006803421304, 0.00010444718645885587, 0.0001042158473865129, 0.00010398604354122654, 0.00010375775309512392, 0.00010353096149628982, 0.0001033056469168514, 0.00010308180208085105, 0.00010285940516041592, 0.00010263844160363078, 0.0001024188895826228, 0.00010220074182143435, 0.00010198399104410782, 0.00010176860814681277, 0.00010155457857763395, 0.00010134190233657137, 0.00010113055031979457, 0.00010092051525134593, 0.00010071178985526785, 0.00010050435230368748, 0.0001002981880446896, 0.00010009328980231658, 9.988963574869558e-05, 9.968722588382661e-05, 9.948604565579444e-05, 9.928607323672622e-05, 9.908730135066435e-05, 9.888971544569358e-05, 9.869331552181393e-05, 9.849807247519493e-05, 9.830398630583659e-05, 9.811104246182367e-05, 9.791923366719857e-05, 9.772854537004605e-05, 9.753896301845089e-05, 9.735048661241308e-05, 9.716308704810217e-05, 9.697677887743339e-05, 9.679152572061867e-05, 9.660734212957323e-05, 9.642419900046661e-05, 9.624208905734122e-05, 9.606101957615465e-05, 9.588096145307645e-05, 9.570190741214901e-05, 9.552385745337233e-05, 9.534679702483118e-05, 9.517071885056794e-05, 9.499560837866738e-05, 9.482146560912952e-05, 9.464827599003911e-05, 9.447603224543855e-05, 9.430472709937021e-05, 9.413434599991888e-05, 9.396488894708455e-05, 9.379634138895199e-05, 9.362869604956359e-05, 9.346194565296173e-05, 9.329609019914642e-05, 9.313110786024481e-05, 9.29669986362569e-05, 9.280375525122508e-05, 9.264137042919174e-05, 9.247983689419925e-05, 9.23191400943324e-05, 9.215928002959117e-05, 9.200024942401797e-05, 9.184203372569755e-05, 9.168464021058753e-05, 9.152804705081508e-05, 9.137225424638018e-05, 9.121725452132523e-05, 9.106304059969261e-05, 9.090960520552471e-05, 9.075694106286392e-05, 9.060504817171022e-05, 9.04539119801484e-05, 9.030353248817846e-05, 9.015390241984278e-05, 9.000500722322613e-05, 8.985685417428613e-05, 8.970942872110754e-05, 8.956272358773276e-05, 8.941673149820417e-05, 8.927145972847939e-05, 8.912689372664317e-05, 8.898301894078031e-05, 8.883984992280602e-05, 8.869735756888986e-05, 8.855555643094704e-05, 8.841443195706233e-05, 8.827398414723575e-05, 8.813419844955206e-05], "accuracy_train_first": 0.561435294117647, "accuracy_train_last": 0.9952235294117647, "batch_size_eval": 1024, "accuracy_train_std": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "accuracy_test_std": 0, "error_valid": [0.43666666666666665, 0.34986666666666666, 0.31079999999999997, 0.27146666666666663, 0.2446666666666667, 0.23040000000000005, 0.2102666666666667, 0.19573333333333331, 0.1916, 0.18786666666666663, 0.18159999999999998, 0.17879999999999996, 0.17159999999999997, 0.17213333333333336, 0.16226666666666667, 0.16666666666666663, 0.16000000000000003, 0.15333333333333332, 0.1537333333333334, 0.15600000000000003, 0.15306666666666668, 0.1537333333333334, 0.15133333333333332, 0.1465333333333333, 0.14733333333333332, 0.14573333333333338, 0.14026666666666665, 0.14626666666666666, 0.1452, 0.13866666666666672, 0.14080000000000004, 0.13946666666666663, 0.13813333333333333, 0.14013333333333333, 0.13813333333333333, 0.13560000000000005, 0.1358666666666667, 0.13360000000000005, 0.13506666666666667, 0.13360000000000005, 0.13280000000000003, 0.13226666666666664, 0.13746666666666663, 0.13213333333333332, 0.1318666666666667, 0.12960000000000005, 0.13480000000000003, 0.1353333333333333, 0.12919999999999998, 0.12960000000000005, 0.132, 0.132, 0.12893333333333334, 0.12826666666666664, 0.12413333333333332, 0.1318666666666667, 0.1306666666666667, 0.12693333333333334, 0.12786666666666668, 0.13093333333333335, 0.12719999999999998, 0.12839999999999996, 0.124, 0.12880000000000003, 0.13093333333333335, 0.12760000000000005, 0.12639999999999996, 0.12626666666666664, 0.1266666666666667, 0.1226666666666667, 0.12893333333333334, 0.12813333333333332, 0.12839999999999996, 0.12680000000000002, 0.12946666666666662, 0.12786666666666668, 0.12253333333333338, 0.1285333333333334, 0.12226666666666663, 0.1293333333333333, 0.1213333333333333, 0.12346666666666661, 0.12426666666666664, 0.12146666666666661, 0.12573333333333336, 0.12773333333333337, 0.12360000000000004, 0.12639999999999996, 0.122, 0.12373333333333336, 0.12706666666666666, 0.12706666666666666, 0.12119999999999997, 0.12280000000000002, 0.12986666666666669, 0.12373333333333336, 0.12413333333333332, 0.11919999999999997, 0.12226666666666663, 0.12439999999999996, 0.12373333333333336, 0.12106666666666666, 0.12639999999999996, 0.11960000000000004, 0.12586666666666668, 0.12413333333333332, 0.1213333333333333, 0.12280000000000002, 0.12426666666666664, 0.13346666666666662, 0.12719999999999998, 0.12693333333333334, 0.12186666666666668, 0.12026666666666663, 0.12386666666666668, 0.12213333333333332, 0.12626666666666664, 0.12173333333333336, 0.12226666666666663, 0.12, 0.12026666666666663, 0.1253333333333333, 0.12773333333333337, 0.12346666666666661, 0.12306666666666666, 0.12053333333333338, 0.11706666666666665, 0.12106666666666666, 0.12346666666666661, 0.12280000000000002, 0.12160000000000004, 0.12426666666666664, 0.12080000000000002, 0.1326666666666667, 0.12373333333333336, 0.12226666666666663, 0.12639999999999996, 0.11986666666666668, 0.1186666666666667, 0.12386666666666668, 0.12346666666666661, 0.12413333333333332, 0.12013333333333331, 0.12013333333333331, 0.12160000000000004, 0.11946666666666672, 0.11680000000000001, 0.12253333333333338, 0.1246666666666667, 0.11919999999999997, 0.12306666666666666, 0.11986666666666668, 0.12173333333333336, 0.11880000000000002, 0.11973333333333336, 0.1213333333333333, 0.11853333333333338, 0.11946666666666672, 0.12053333333333338, 0.11880000000000002, 0.1265333333333334, 0.11706666666666665, 0.12119999999999997, 0.12480000000000002, 0.11706666666666665, 0.12453333333333338, 0.12, 0.11853333333333338, 0.12493333333333334, 0.1213333333333333, 0.11946666666666672, 0.11933333333333329, 0.11813333333333331, 0.12519999999999998, 0.11826666666666663, 0.12319999999999998, 0.11893333333333334, 0.11493333333333333, 0.11693333333333333, 0.11346666666666672, 0.1186666666666667, 0.11626666666666663, 0.11586666666666667, 0.11546666666666672, 0.11493333333333333, 0.11599999999999999, 0.11653333333333338, 0.11746666666666672, 0.11680000000000001, 0.11386666666666667, 0.1166666666666667, 0.12186666666666668, 0.11733333333333329, 0.11613333333333331, 0.11893333333333334, 0.1186666666666667, 0.11760000000000004, 0.11653333333333338, 0.11519999999999997, 0.11653333333333338, 0.11280000000000001, 0.11573333333333335, 0.11933333333333329, 0.11519999999999997, 0.11533333333333329, 0.11626666666666663, 0.11880000000000002, 0.11519999999999997, 0.11346666666666672, 0.11613333333333331, 0.1146666666666667, 0.11439999999999995, 0.11773333333333336, 0.11333333333333329, 0.11266666666666669, 0.11119999999999997, 0.11453333333333338, 0.11839999999999995, 0.11653333333333338, 0.11453333333333338, 0.11573333333333335, 0.1146666666666667, 0.11386666666666667, 0.11386666666666667, 0.11493333333333333, 0.11773333333333336, 0.11613333333333331, 0.12, 0.11426666666666663, 0.11360000000000003, 0.11426666666666663, 0.11560000000000004, 0.1146666666666667, 0.11506666666666665, 0.11119999999999997, 0.11399999999999999, 0.11333333333333329, 0.11253333333333337, 0.11880000000000002, 0.11639999999999995, 0.11399999999999999, 0.11786666666666668, 0.11880000000000002, 0.11439999999999995, 0.11586666666666667, 0.11573333333333335, 0.11493333333333333, 0.11226666666666663, 0.11280000000000001, 0.11253333333333337, 0.11626666666666663, 0.11426666666666663, 0.11373333333333335, 0.11586666666666667, 0.11346666666666672, 0.11360000000000003, 0.11133333333333328, 0.11573333333333335, 0.11373333333333335, 0.11746666666666672, 0.11439999999999995, 0.11480000000000001, 0.11253333333333337, 0.11599999999999999, 0.11240000000000006, 0.10919999999999996, 0.11213333333333331, 0.11560000000000004, 0.11360000000000003, 0.11546666666666672, 0.11573333333333335, 0.11319999999999997, 0.11453333333333338, 0.11439999999999995, 0.11319999999999997, 0.11480000000000001, 0.1166666666666667, 0.11306666666666665, 0.11160000000000003, 0.11973333333333336, 0.11613333333333331, 0.11319999999999997, 0.11306666666666665, 0.11373333333333335, 0.11360000000000003, 0.1146666666666667, 0.11160000000000003, 0.11493333333333333, 0.11493333333333333, 0.11653333333333338, 0.11560000000000004, 0.11280000000000001, 0.11133333333333328, 0.11280000000000001, 0.11280000000000001, 0.11693333333333333, 0.11439999999999995, 0.11506666666666665, 0.11240000000000006, 0.11413333333333331, 0.11253333333333337, 0.11493333333333333, 0.11319999999999997, 0.11240000000000006, 0.11706666666666665, 0.11373333333333335, 0.11333333333333329, 0.11133333333333328, 0.10973333333333335, 0.11386666666666667, 0.11119999999999997, 0.11146666666666671, 0.11253333333333337, 0.11346666666666672, 0.11026666666666662, 0.11213333333333331], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.019934797979756516, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 32, "valid_ratio": 0.15, "learning_rate": 0.001566707759964551, "optimization": "adam", "nb_data_augmentation": 4, "learning_rate_decay_method": "sqrt", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 1.0605612821492927e-06, "rotation_range": [0, 0], "momentum": 0.9487543747291647}, "accuracy_valid_max": 0.8908, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        y_pred = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            y_pred.extend((nnet.predict(X[mini_batch]) == y[mini_batch]).tolist())\n        return np.mean(y_pred)\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            acc = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = acc\n            status[\"accuracy_train_std\"] = 0\n            acc = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = acc\n            status[\"accuracy_valid_std\"] = 0\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n\n    # rescaling to [-1, 1]\n    X_min = X_train.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X_train.max(axis=(0, 2, 3))[None, :, None, None]\n    def preprocess(a):\n        return (a / 255.) * 2 - 1\n        # return 2 * ((a - X_min) / (X_max - X_min)) - 1\n    X_train = preprocess(X_train)\n    X_valid = preprocess(X_valid)\n\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = preprocess(X_test)\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    acc = evaluate(X_test, y_test, batch_size_eval)\n    light.set(\"accuracy_test\", acc)\n    light.set(\"accuracy_test_std\", 0)\n    print(\"Test accuracy : {}+-{}\".format(acc, 0))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8878666666666667, "accuracy_valid_std": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "accuracy_valid": [0.5633333333333334, 0.6501333333333333, 0.6892, 0.7285333333333334, 0.7553333333333333, 0.7696, 0.7897333333333333, 0.8042666666666667, 0.8084, 0.8121333333333334, 0.8184, 0.8212, 0.8284, 0.8278666666666666, 0.8377333333333333, 0.8333333333333334, 0.84, 0.8466666666666667, 0.8462666666666666, 0.844, 0.8469333333333333, 0.8462666666666666, 0.8486666666666667, 0.8534666666666667, 0.8526666666666667, 0.8542666666666666, 0.8597333333333333, 0.8537333333333333, 0.8548, 0.8613333333333333, 0.8592, 0.8605333333333334, 0.8618666666666667, 0.8598666666666667, 0.8618666666666667, 0.8644, 0.8641333333333333, 0.8664, 0.8649333333333333, 0.8664, 0.8672, 0.8677333333333334, 0.8625333333333334, 0.8678666666666667, 0.8681333333333333, 0.8704, 0.8652, 0.8646666666666667, 0.8708, 0.8704, 0.868, 0.868, 0.8710666666666667, 0.8717333333333334, 0.8758666666666667, 0.8681333333333333, 0.8693333333333333, 0.8730666666666667, 0.8721333333333333, 0.8690666666666667, 0.8728, 0.8716, 0.876, 0.8712, 0.8690666666666667, 0.8724, 0.8736, 0.8737333333333334, 0.8733333333333333, 0.8773333333333333, 0.8710666666666667, 0.8718666666666667, 0.8716, 0.8732, 0.8705333333333334, 0.8721333333333333, 0.8774666666666666, 0.8714666666666666, 0.8777333333333334, 0.8706666666666667, 0.8786666666666667, 0.8765333333333334, 0.8757333333333334, 0.8785333333333334, 0.8742666666666666, 0.8722666666666666, 0.8764, 0.8736, 0.878, 0.8762666666666666, 0.8729333333333333, 0.8729333333333333, 0.8788, 0.8772, 0.8701333333333333, 0.8762666666666666, 0.8758666666666667, 0.8808, 0.8777333333333334, 0.8756, 0.8762666666666666, 0.8789333333333333, 0.8736, 0.8804, 0.8741333333333333, 0.8758666666666667, 0.8786666666666667, 0.8772, 0.8757333333333334, 0.8665333333333334, 0.8728, 0.8730666666666667, 0.8781333333333333, 0.8797333333333334, 0.8761333333333333, 0.8778666666666667, 0.8737333333333334, 0.8782666666666666, 0.8777333333333334, 0.88, 0.8797333333333334, 0.8746666666666667, 0.8722666666666666, 0.8765333333333334, 0.8769333333333333, 0.8794666666666666, 0.8829333333333333, 0.8789333333333333, 0.8765333333333334, 0.8772, 0.8784, 0.8757333333333334, 0.8792, 0.8673333333333333, 0.8762666666666666, 0.8777333333333334, 0.8736, 0.8801333333333333, 0.8813333333333333, 0.8761333333333333, 0.8765333333333334, 0.8758666666666667, 0.8798666666666667, 0.8798666666666667, 0.8784, 0.8805333333333333, 0.8832, 0.8774666666666666, 0.8753333333333333, 0.8808, 0.8769333333333333, 0.8801333333333333, 0.8782666666666666, 0.8812, 0.8802666666666666, 0.8786666666666667, 0.8814666666666666, 0.8805333333333333, 0.8794666666666666, 0.8812, 0.8734666666666666, 0.8829333333333333, 0.8788, 0.8752, 0.8829333333333333, 0.8754666666666666, 0.88, 0.8814666666666666, 0.8750666666666667, 0.8786666666666667, 0.8805333333333333, 0.8806666666666667, 0.8818666666666667, 0.8748, 0.8817333333333334, 0.8768, 0.8810666666666667, 0.8850666666666667, 0.8830666666666667, 0.8865333333333333, 0.8813333333333333, 0.8837333333333334, 0.8841333333333333, 0.8845333333333333, 0.8850666666666667, 0.884, 0.8834666666666666, 0.8825333333333333, 0.8832, 0.8861333333333333, 0.8833333333333333, 0.8781333333333333, 0.8826666666666667, 0.8838666666666667, 0.8810666666666667, 0.8813333333333333, 0.8824, 0.8834666666666666, 0.8848, 0.8834666666666666, 0.8872, 0.8842666666666666, 0.8806666666666667, 0.8848, 0.8846666666666667, 0.8837333333333334, 0.8812, 0.8848, 0.8865333333333333, 0.8838666666666667, 0.8853333333333333, 0.8856, 0.8822666666666666, 0.8866666666666667, 0.8873333333333333, 0.8888, 0.8854666666666666, 0.8816, 0.8834666666666666, 0.8854666666666666, 0.8842666666666666, 0.8853333333333333, 0.8861333333333333, 0.8861333333333333, 0.8850666666666667, 0.8822666666666666, 0.8838666666666667, 0.88, 0.8857333333333334, 0.8864, 0.8857333333333334, 0.8844, 0.8853333333333333, 0.8849333333333333, 0.8888, 0.886, 0.8866666666666667, 0.8874666666666666, 0.8812, 0.8836, 0.886, 0.8821333333333333, 0.8812, 0.8856, 0.8841333333333333, 0.8842666666666666, 0.8850666666666667, 0.8877333333333334, 0.8872, 0.8874666666666666, 0.8837333333333334, 0.8857333333333334, 0.8862666666666666, 0.8841333333333333, 0.8865333333333333, 0.8864, 0.8886666666666667, 0.8842666666666666, 0.8862666666666666, 0.8825333333333333, 0.8856, 0.8852, 0.8874666666666666, 0.884, 0.8876, 0.8908, 0.8878666666666667, 0.8844, 0.8864, 0.8845333333333333, 0.8842666666666666, 0.8868, 0.8854666666666666, 0.8856, 0.8868, 0.8852, 0.8833333333333333, 0.8869333333333334, 0.8884, 0.8802666666666666, 0.8838666666666667, 0.8868, 0.8869333333333334, 0.8862666666666666, 0.8864, 0.8853333333333333, 0.8884, 0.8850666666666667, 0.8850666666666667, 0.8834666666666666, 0.8844, 0.8872, 0.8886666666666667, 0.8872, 0.8872, 0.8830666666666667, 0.8856, 0.8849333333333333, 0.8876, 0.8858666666666667, 0.8874666666666666, 0.8850666666666667, 0.8868, 0.8876, 0.8829333333333333, 0.8862666666666666, 0.8866666666666667, 0.8886666666666667, 0.8902666666666667, 0.8861333333333333, 0.8888, 0.8885333333333333, 0.8874666666666666, 0.8865333333333333, 0.8897333333333334, 0.8878666666666667], "seed": 26453858, "model": "residualv3", "loss_std": [0.28138574957847595, 0.19028770923614502, 0.17673495411872864, 0.1736084520816803, 0.1703227013349533, 0.16723686456680298, 0.1639600694179535, 0.16174672544002533, 0.16035039722919464, 0.15574681758880615, 0.15488597750663757, 0.1496269553899765, 0.14961399137973785, 0.145308256149292, 0.1418163776397705, 0.1412724256515503, 0.13750366866588593, 0.13524721562862396, 0.13293609023094177, 0.1310185343027115, 0.13094569742679596, 0.12711143493652344, 0.12456957250833511, 0.12158497422933578, 0.1207868903875351, 0.11919853836297989, 0.11785054206848145, 0.11540693044662476, 0.11246339976787567, 0.11260946840047836, 0.10964054614305496, 0.10803784430027008, 0.1060728132724762, 0.1051080971956253, 0.1042095199227333, 0.10095435380935669, 0.09986451268196106, 0.09946932643651962, 0.09813745319843292, 0.094114288687706, 0.09466666728258133, 0.09087512642145157, 0.0903107151389122, 0.08814756572246552, 0.08602584153413773, 0.0848236009478569, 0.08575582504272461, 0.08533862233161926, 0.08019507676362991, 0.0793624073266983, 0.07988867163658142, 0.07788851112127304, 0.07619038969278336, 0.07468971610069275, 0.07314220815896988, 0.07471518218517303, 0.07092460244894028, 0.06948103755712509, 0.06806182116270065, 0.06880079209804535, 0.06772498041391373, 0.06686904281377792, 0.06535529345273972, 0.06253695487976074, 0.06336846947669983, 0.061773400753736496, 0.06141885742545128, 0.058954257518053055, 0.05917999893426895, 0.05905155837535858, 0.0579061396420002, 0.05833575874567032, 0.05496259406208992, 0.054352860897779465, 0.054793089628219604, 0.053468603640794754, 0.052443213760852814, 0.05091886222362518, 0.0509529784321785, 0.04932331293821335, 0.04976239055395126, 0.049820102751255035, 0.0476679801940918, 0.048809852451086044, 0.047416575253009796, 0.04579939320683479, 0.045403383672237396, 0.04644466191530228, 0.04298398271203041, 0.04246789216995239, 0.043920423835515976, 0.042031560093164444, 0.04104873910546303, 0.04050055891275406, 0.04135734215378761, 0.040920037776231766, 0.039024390280246735, 0.03960806876420975, 0.04034844785928726, 0.038703553378582, 0.03852177783846855, 0.038482218980789185, 0.03675604611635208, 0.03779561445116997, 0.037208396941423416, 0.03605825453996658, 0.03482977673411369, 0.035456061363220215, 0.03486611694097519, 0.03508392721414566, 0.0335637666285038, 0.03412075713276863, 0.034091439098119736, 0.032615214586257935, 0.031367287039756775, 0.033696405589580536, 0.032447729259729385, 0.03181157633662224, 0.030308552086353302, 0.031508561223745346, 0.031440526247024536, 0.028143957257270813, 0.03118162415921688, 0.03106774389743805, 0.02895757369697094, 0.029381517320871353, 0.029361119493842125, 0.029598010703921318, 0.028767218813300133, 0.028908612206578255, 0.02747448906302452, 0.028865160420536995, 0.02767239697277546, 0.028156595304608345, 0.025980191305279732, 0.02684374339878559, 0.027780868113040924, 0.02609095349907875, 0.027049744501709938, 0.025136198848485947, 0.02622220106422901, 0.02533450350165367, 0.02574298344552517, 0.02571069821715355, 0.02591424621641636, 0.026275230571627617, 0.024735087528824806, 0.02421567589044571, 0.026046311482787132, 0.02548222243785858, 0.024159839376807213, 0.024954821914434433, 0.025519557297229767, 0.025105636566877365, 0.02391369640827179, 0.02272777631878853, 0.02561436966061592, 0.022993069142103195, 0.024863790720701218, 0.02288929931819439, 0.022929063066840172, 0.0237002894282341, 0.0222224872559309, 0.02168816514313221, 0.022623134776949883, 0.024284426122903824, 0.021978728473186493, 0.022648079320788383, 0.022009707987308502, 0.022510899230837822, 0.021782277151942253, 0.02126375399529934, 0.020112769678235054, 0.021979454904794693, 0.020427685230970383, 0.021341392770409584, 0.019206520169973373, 0.020938796922564507, 0.02244163304567337, 0.021712204441428185, 0.021578818559646606, 0.02013041079044342, 0.021014561876654625, 0.019969392567873, 0.02007172629237175, 0.019592557102441788, 0.020273728296160698, 0.021308189257979393, 0.01961982063949108, 0.019444486126303673, 0.018648015335202217, 0.018753962591290474, 0.019479554146528244, 0.01999865472316742, 0.017890460789203644, 0.019096529111266136, 0.018953416496515274, 0.019971098750829697, 0.01985175907611847, 0.018956946209073067, 0.01776650734245777, 0.01812531240284443, 0.018355021253228188, 0.018004821613430977, 0.019048606976866722, 0.017462188377976418, 0.017502522096037865, 0.01767929084599018, 0.018428858369588852, 0.020134350284934044, 0.01773090288043022, 0.017898667603731155, 0.01856844127178192, 0.017697550356388092, 0.017807474359869957, 0.017177805304527283, 0.018670205026865005, 0.017419589683413506, 0.019146135076880455, 0.01763042062520981, 0.016277575865387917, 0.0158772524446249, 0.01723000407218933, 0.016298269852995872, 0.018152574077248573, 0.01718207262456417, 0.01646820828318596, 0.016714243218302727, 0.016040503978729248, 0.016080038622021675, 0.015632975846529007, 0.016252804547548294, 0.015826329588890076, 0.015222975984215736, 0.017145901918411255, 0.016058271750807762, 0.016732625663280487, 0.015804491937160492, 0.015628419816493988, 0.01579366996884346, 0.015705768018960953, 0.0155610591173172, 0.014727587811648846, 0.016531318426132202, 0.015924639999866486, 0.014865490607917309, 0.013848531991243362, 0.015328842215240002, 0.015621873550117016, 0.016354242339730263, 0.016051145270466805, 0.015210610814392567, 0.015040361322462559, 0.01462763175368309, 0.015027065761387348, 0.01586654782295227, 0.015387142077088356, 0.015474135987460613, 0.016917340457439423, 0.014721478335559368, 0.014520777389407158, 0.013895643875002861, 0.013587914407253265, 0.015024377033114433, 0.016313336789608, 0.015011363662779331, 0.014245860278606415, 0.014191048219799995, 0.01408106368035078, 0.014799730852246284, 0.013738131150603294, 0.013712083920836449, 0.014466512948274612, 0.014185620471835136, 0.014171731658279896, 0.013880284503102303, 0.01370968110859394, 0.014213420450687408, 0.013613398186862469, 0.014255604706704617, 0.013534788973629475, 0.013194535858929157, 0.014514550566673279, 0.014525842852890491, 0.01306669320911169, 0.016767365857958794, 0.012770768254995346, 0.012265566736459732, 0.013047244399785995, 0.013464486226439476, 0.014379923231899738, 0.012772214598953724, 0.01450397726148367, 0.014196296222507954, 0.012609806843101978, 0.014059437438845634, 0.013861485756933689, 0.012679897248744965, 0.013232899829745293, 0.01384949404746294, 0.012932593002915382, 0.01274680532515049, 0.013688221573829651, 0.013344022445380688, 0.013562671840190887, 0.013962594792246819, 0.01313552726060152, 0.013786902651190758, 0.012295876629650593, 0.012382660992443562, 0.011792676523327827, 0.012948544695973396, 0.013287863694131374, 0.012733572162687778, 0.011709054931998253, 0.013658245094120502]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:44 2016", "state": "available"}], "summary": "fdb5d5200340238f426c98abe6610d1d"}