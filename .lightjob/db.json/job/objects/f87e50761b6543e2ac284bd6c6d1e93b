{"content": {"hp_model": {"f0": 32, "f1": 64, "f2": 16, "f3": 64, "nonlin": "leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.08989568854519814, 0.08964148345983354, 0.08708780542945278, 0.08695623017837587, 0.08425461612442033, 0.08330304433246272, 0.08609308617763915, 0.08708995570897689, 0.08111098252451913, 0.07789958531670155, 0.07593941358131351, 0.0749568796952354, 0.07084086635376548, 0.0720451145452728, 0.07395659223410732, 0.06766670233485855, 0.06915891435852001, 0.0660015473856182, 0.06601451657348725, 0.06578271338315009, 0.06539168513815116, 0.06521047232594937, 0.0680561897436538, 0.06643141538490659, 0.06378865558908713, 0.06455792789421051, 0.063595444436252, 0.06462129856941011, 0.06699216335281566, 0.06434194530586791, 0.06359600531683088, 0.06715130833029305, 0.06597452005583392, 0.06493475245465864, 0.06612627023900923, 0.061537934071887714, 0.06027750415017713, 0.06321232908485973, 0.05971463058372686, 0.06245504012407699, 0.06401012744093507, 0.061235199022775896, 0.06265973838689697, 0.06337730747847817, 0.06150590084101117, 0.05754868955261678, 0.05781953169078069, 0.06455171173436805, 0.060884109024971, 0.061498361185341556, 0.05885060742671867, 0.060683854730876924, 0.06396344077545373, 0.06412537432480224, 0.06476754554019862, 0.059699545983183916, 0.05738482479485378, 0.058626376228932006, 0.062269720824608014, 0.06161092493053399, 0.06303602710811308, 0.05881726224269152, 0.06006884469721463, 0.06349370299890235, 0.06147530150172115, 0.06306827291729901, 0.060341380011424195, 0.06137658371360414, 0.06109684419710402, 0.057653343323557205, 0.05956496006424936, 0.059765382446180314, 0.06170522151870605, 0.06176531098379733, 0.05793108528820513, 0.060731006882144306, 0.059389842708809516, 0.0578653193335894, 0.05941326153580414, 0.059551335000467144, 0.06121815847728864, 0.058889991029050404, 0.062252963460112276, 0.06229549258335755, 0.05707131058693547, 0.062002641574986785, 0.06015785057125298, 0.05895960526502777, 0.06246517676232084, 0.0643099221437616, 0.05863322058264767, 0.061663586818047886, 0.05937062036167533, 0.06032305219441498, 0.05885606211274638, 0.061911678756672625, 0.061276979265962005, 0.05832030126060849, 0.05924747976133962, 0.06035630420126616, 0.05923603981534356, 0.06113492663409827, 0.05955418005148093, 0.05878511171534228, 0.06097016972436411, 0.05969715599375251, 0.060444886788965806, 0.059899369172616304, 0.05881589771901913, 0.05984932692440337, 0.06025752905739847, 0.05898001993454469, 0.05893101284678491, 0.05910100192954656, 0.06112894590628414, 0.06088630596297442, 0.05955418005148092, 0.061512424797120734, 0.06055985044280196, 0.05837822307720187, 0.0589426632840492, 0.05931005931815412, 0.0610793270626501, 0.06000126055066924, 0.062364022216946154, 0.05968460697909558, 0.06026581582538119, 0.06273312979053546, 0.06045831047960779, 0.060448869953252, 0.06136379686464792, 0.060209561796876254, 0.057140175362439705, 0.061325856552798354, 0.06366313504007623, 0.06262827894124721, 0.06009867634781945, 0.05928840463885104, 0.057965709462189485, 0.06328719324623883, 0.06086697018708923, 0.06110866542421005, 0.05938984270880952, 0.06136379686464792, 0.06024317249418818, 0.056997512795404486, 0.05996900128659443, 0.061214225370057135, 0.05795817084832701, 0.06229878487324988, 0.06018823074380101, 0.06021430100469157, 0.06012893811362622, 0.059861990374535004, 0.05861177226975027, 0.06125427298251451, 0.05998862650013325, 0.06145310382554541, 0.06000423288184871, 0.05941866456970882, 0.06006543018057706, 0.0589771471861173, 0.058681108706238265, 0.058131465029041506, 0.05983278591304702, 0.05581434220278205, 0.057719505301747624, 0.058157843947513, 0.060317286649135105, 0.059942378043305246, 0.06039057154263313, 0.058490693104331075, 0.06113142579126364, 0.05848261224706001, 0.06002919465331652, 0.05950100004507992, 0.061694815477649094, 0.059716422560237736, 0.05806101145880869, 0.06149444599952456, 0.05780472388832919, 0.060898753781286376, 0.05586305049488681, 0.057981706537649585, 0.05891724117803014, 0.05888272219279631, 0.05994237804330524, 0.05891587896992409, 0.059704923109606275, 0.05603136588627545, 0.05868293224298914, 0.05919191512007238, 0.06137527608998293, 0.05658249799393206, 0.05893721661302895, 0.05938759041184242, 0.05944087188260588, 0.05980058488478416, 0.05882332640938644, 0.05699328842212931, 0.058906342631152686, 0.05896504986798193, 0.05630111272625376, 0.057963709517331354, 0.056967466001147195, 0.06135565839146565, 0.05611835350927285, 0.057451451221305724, 0.05725974969145965, 0.057735725046500604, 0.05620536516121261, 0.05912936133709491, 0.0594582718162025, 0.05797540052535171, 0.05667052791694526, 0.06034669994795747, 0.05722750326902092, 0.05726675738057594, 0.05804488261038408, 0.058848182959526325, 0.05917820416727847, 0.05778682606443202, 0.06089348207475111, 0.05898122947097004, 0.05927035302915877, 0.05832259477407113, 0.05931592276664962, 0.05802967130449366, 0.058823174812838254, 0.060223630230197736], "moving_avg_accuracy_train": [0.04016848644578312, 0.08347373870481925, 0.12534991528614453, 0.1669447242093373, 0.205740180252259, 0.24461411026920174, 0.2823114906579442, 0.3218467196041979, 0.3615535197823323, 0.3972661083161473, 0.4330548401050145, 0.46819673937764555, 0.5011845767350618, 0.5326996845736038, 0.5610279841885326, 0.5891284048961853, 0.6161530644065668, 0.6406093882369944, 0.6651497295337769, 0.6879443386587124, 0.7091607293410339, 0.7294297091177738, 0.7471164445312977, 0.7647264303492523, 0.7813213701456524, 0.796282700751569, 0.8108374126041229, 0.8238848836931082, 0.8351946257454841, 0.8461711157311766, 0.8564899981339625, 0.8647557197061084, 0.873187903759594, 0.8814781118776105, 0.8882545288525, 0.8955887145214668, 0.9024436231898021, 0.9084530259310629, 0.9145815561692819, 0.9201513561246429, 0.9254065518374798, 0.9303974290332498, 0.9351339474251056, 0.9393426912368119, 0.9433847022336126, 0.9473425422512152, 0.9505210327550093, 0.9536146373108336, 0.9561376803568586, 0.9591543716585221, 0.9615493637095374, 0.963864871615692, 0.9660217767734, 0.968306553162325, 0.9701110635087431, 0.971713944356664, 0.9736224633246121, 0.9754413163897413, 0.9766641275218515, 0.9780140927817146, 0.9793796639252298, 0.9803286515989719, 0.9811639152041349, 0.9820944928102274, 0.9830473176255903, 0.9838978004714649, 0.9848444285568485, 0.9856563900686336, 0.9864930477485172, 0.9873636977929426, 0.988067275302805, 0.9886887292484281, 0.989306866865754, 0.9898749565346003, 0.9904615384413813, 0.9910459380610986, 0.9914707117248683, 0.9917918257933453, 0.9922361371899144, 0.9926148389829711, 0.9929180199943125, 0.9931179348623511, 0.9933543341472003, 0.9936518073589863, 0.9940113065929672, 0.9942689673493331, 0.9945455721204238, 0.9948062822276585, 0.9950479808121216, 0.995279628514042, 0.9954810519578184, 0.9956717457078197, 0.9957445372514956, 0.9958524065685147, 0.9960812660622657, 0.9962566484921836, 0.9964568496068207, 0.9966417369352952, 0.9968199013441753, 0.9969661303362638, 0.9971000895917941, 0.9969429797290003, 0.9971051388344135, 0.9972416693786831, 0.9971668812058749, 0.9972195831455284, 0.9973423160960359, 0.9974598352394443, 0.9975044202395963, 0.9975139556252751, 0.9975625412374464, 0.9975992088004486, 0.9975780868661869, 0.9976343783301707, 0.9977179849248645, 0.9977438144444263, 0.9978800128192608, 0.9979367028023949, 0.9980347870402277, 0.9981136502036749, 0.9981658017495725, 0.9982903925083502, 0.9983460482876356, 0.9983984916516432, 0.9984998134202138, 0.9985133486444574, 0.9984243443523008, 0.9985136682002032, 0.9984905205066889, 0.9984343901427669, 0.9984803524839119, 0.9985311312415448, 0.9986168358884746, 0.9986869105827597, 0.9987382119943632, 0.998786736427457, 0.9987809920015787, 0.9988299447592521, 0.9989069465182667, 0.9989244785230665, 0.9989614357912417, 0.9990370542603104, 0.9990556944668095, 0.9991312997189238, 0.9991899317952242, 0.9992615259650994, 0.9992765443023244, 0.9993182987576341, 0.9992688107493406, 0.9992948664213944, 0.9993347886647972, 0.9993589528706066, 0.99939952595704, 0.9994195695962758, 0.9994352557089373, 0.9994564326982845, 0.9994849046392994, 0.9994752319464537, 0.9994829986614469, 0.9995088140061456, 0.9995414604669768, 0.9995567233058213, 0.9995681066981308, 0.9995830580765104, 0.9996129864556064, 0.999609330882335, 0.99960368770374, 0.9996268467948117, 0.9996076862117161, 0.9996210328013878, 0.9996565763585984, 0.9996579744456301, 0.9996474669107056, 0.999647422779876, 0.9996567957127317, 0.9996605250270008, 0.9996521155965898, 0.9996704318983767, 0.9996704444314306, 0.9996869278497333, 0.9996970566009046, 0.9997014661516574, 0.9996983752593833, 0.9997097124322402, 0.9997293285384138, 0.9997258045701146, 0.999713220348043, 0.9997348388252869, 0.9997378233162522, 0.9996887397798077, 0.9996963341753209, 0.9997055222939334, 0.9997090852753834, 0.9997217046092908, 0.9997330620098075, 0.9997409305076219, 0.9997480121556549, 0.999763798289487, 0.9997803589725865, 0.9998023230753279, 0.9998126781171928, 0.9998125850042687, 0.9998148543652876, 0.9998216031155058, 0.9998276769907022, 0.9998260839904272, 0.9997893528504206, 0.9997892391015231, 0.999779724076913, 0.9997829263680169, 0.999799927405914, 0.9997964030388166, 0.9998097032469832, 0.999821673434333, 0.9998347997655985, 0.9998160223492796, 0.9998155948131469, 0.999805797380025, 0.9998087455034683, 0.9998184583025191, 0.999820140333713, 0.9998240073244381, 0.9998321939413919, 0.9998442682219515, 0.9998527819118046, 0.9998651505579735, 0.9998762823395255, 0.9998863009429223, 0.9998929645233288, 0.9998966085830441], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 1234, "moving_var_accuracy_train": [0.014521565730105565, 0.029947513016063798, 0.04273528920009046, 0.05403291344429367, 0.062175408786068725, 0.06955850982212133, 0.07539249123347111, 0.08192055106061832, 0.08791816577803316, 0.09060485001829999, 0.09307186492396924, 0.09487925619196205, 0.09518510729443992, 0.09460541476367032, 0.09236730631896185, 0.09023727848258939, 0.08778654062919893, 0.08439089254396773, 0.08137185844823407, 0.07791102044983801, 0.0741711355071183, 0.07045150582711553, 0.06622174073069596, 0.06239057106220342, 0.05863004219759836, 0.054781610699334383, 0.05121000636339881, 0.0476211342434201, 0.04401021320669961, 0.040693541877683724, 0.03758250169629808, 0.03443915090464242, 0.03163515136538488, 0.029090184184606637, 0.026594444209304114, 0.024419112303215612, 0.02240010902855501, 0.020485114417459562, 0.01877463292164049, 0.01717637367336109, 0.015707290043846956, 0.014360740736111582, 0.013126578120787913, 0.011973342028962306, 0.010923048502150399, 0.009971724130379788, 0.009065476934286196, 0.008245062743187934, 0.007477848184778, 0.006811967203985992, 0.00618239436590723, 0.005612409121087691, 0.005093038367713044, 0.004630716359268245, 0.004196951041654391, 0.0038003789806026356, 0.00345312308440153, 0.0031375848142141446, 0.0028372837363760442, 0.0025699570185639736, 0.0023297443776395914, 0.002104875138319862, 0.0019006666120988656, 0.0017183937230176272, 0.0015547252268748046, 0.0014057625938274686, 0.0012732512770330556, 0.001151859682799332, 0.0010429736791791732, 0.0009454985947599773, 0.0008554039270954367, 0.0007733393794446678, 0.0006994442885257808, 0.0006324043925198517, 0.0005722606582681313, 0.0005181082986810496, 0.0004679213628018352, 0.0004220572547264165, 0.0003816282428078653, 0.0003447561539596583, 0.0003111078070944347, 0.00028035671997515734, 0.00025282400957453666, 0.00022833802142265547, 0.0002066673765734855, 0.00018659814050447652, 0.00016862691824854055, 0.0001523759542638156, 0.00013766412268901706, 0.00012438065634036026, 0.0001123077333396494, 0.00010140423696229047, 9.131150074553786e-05, 8.228507277697161e-05, 7.452795551019389e-05, 6.735199092969003e-05, 6.0977516213438304e-05, 5.518741451016861e-05, 4.995435606847615e-05, 4.515136672477349e-05, 4.07977357915761e-05, 3.694011379330248e-05, 3.3482762593187995e-05, 3.0302251639535888e-05, 2.732236591271002e-05, 2.4615126771428157e-05, 2.2289184488547594e-05, 2.0184562781299968e-05, 1.8183996903316924e-05, 1.6366415525205646e-05, 1.4751019028075607e-05, 1.3288017716856828e-05, 1.1963231170133754e-05, 1.0795426613377256e-05, 9.778794516126127e-06, 8.806919541240625e-06, 8.093177562884557e-06, 7.312783594285867e-06, 6.668089894258438e-06, 6.057255491772554e-06, 5.4760079962508585e-06, 5.068112911180972e-06, 4.58917971197373e-06, 4.155014498632188e-06, 3.831907955845472e-06, 3.450365980918868e-06, 3.176625259027622e-06, 2.9307714813617756e-06, 2.642516674660882e-06, 2.406620566980839e-06, 2.1849713415145216e-06, 1.9896805474037425e-06, 1.856820071211725e-06, 1.7153322291028665e-06, 1.5674855196852292e-06, 1.431928553180379e-06, 1.289032683720375e-06, 1.1816967677028648e-06, 1.1168905289545886e-06, 1.0079678167898435e-06, 9.194635921496744e-07, 8.789806087132032e-07, 7.942096635268361e-07, 7.662340844994578e-07, 7.205501593911648e-07, 6.946266698930714e-07, 6.271939569807972e-07, 5.801654721266259e-07, 5.441904915977084e-07, 4.958815248535594e-07, 4.6063744203295844e-07, 4.1982887741127205e-07, 3.9266156775467104e-07, 3.5701113824352556e-07, 3.235245115930605e-07, 2.952082443340675e-07, 2.7298328272707545e-07, 2.465270033363344e-07, 2.2241719975876955e-07, 2.0617336798010077e-07, 1.9514815382533693e-07, 1.7772992668913723e-07, 1.6112316860445638e-07, 1.4702274518308636e-07, 1.403818415425717e-07, 1.2646392633180298e-07, 1.1410414288051184e-07, 1.0752082008590434e-07, 1.0007288957839712e-07, 9.166878372335529e-08, 9.387200548367964e-08, 8.450239676144615e-08, 7.704583169699862e-08, 6.934126605506984e-08, 6.319780628243347e-08, 5.700319571844234e-08, 5.193934282512247e-08, 4.9764790742916685e-08, 4.4788313082321975e-08, 4.275480948457345e-08, 3.9402652938717296e-08, 3.5637384885428574e-08, 3.2159628932341293e-08, 3.01004494345824e-08, 3.055352908383802e-08, 2.7609941348619082e-08, 2.627421102010662e-08, 2.7853016943196737e-08, 2.5147879925773993e-08, 4.431583388227791e-08, 4.040332408294963e-08, 3.712278538738938e-08, 3.3524760379970166e-08, 3.160551263638302e-08, 2.9605876291217158e-08, 2.7202507982797288e-08, 2.4933604834286203e-08, 2.4683062543140776e-08, 2.4683062311320822e-08, 2.6556552363279845e-08, 2.4865939155146224e-08, 2.2379423269781305e-08, 2.0187830937710743e-08, 1.8578958509511203e-08, 1.7053090297673294e-08, 1.5370620116792344e-08, 2.5976147920755953e-08, 2.3378649577985453e-08, 2.1855605860171444e-08, 1.9762337288980427e-08, 2.0387421166291786e-08, 1.846046952059901e-08, 1.8206482403996476e-08, 1.7675402630316564e-08, 1.74585675197072e-08, 1.8886033040252913e-08, 1.6999074820531096e-08, 1.6163074600483912e-08, 1.4624990026967258e-08, 1.4011537212872828e-08, 1.263584655201997e-08, 1.1506844452229243e-08, 1.0959346281336883e-08, 1.1175505912491504e-08, 1.071030155546563e-08, 1.1016122072390198e-08, 1.1029758909853413e-08, 1.0830134745078048e-08, 1.014675100508347e-08, 9.251588445454033e-09], "duration": 202402.096268, "accuracy_train": [0.4016848644578313, 0.47322100903614456, 0.5022355045180723, 0.5412980045180723, 0.5548992846385542, 0.5944794804216867, 0.6215879141566265, 0.6776637801204819, 0.7189147213855421, 0.7186794051204819, 0.7551534262048193, 0.7844738328313253, 0.7980751129518072, 0.8163356551204819, 0.8159826807228916, 0.8420321912650602, 0.859375, 0.8607163027108434, 0.8860128012048193, 0.8930958207831325, 0.9001082454819277, 0.9118505271084337, 0.9062970632530121, 0.9232163027108434, 0.930675828313253, 0.9309346762048193, 0.9418298192771084, 0.9413121234939759, 0.9369823042168675, 0.9449595256024096, 0.9493599397590361, 0.9391472138554217, 0.9490775602409639, 0.956089984939759, 0.949242281626506, 0.9615963855421686, 0.9641378012048193, 0.9625376506024096, 0.969738328313253, 0.9702795557228916, 0.9727033132530121, 0.9753153237951807, 0.9777626129518072, 0.9772213855421686, 0.9797628012048193, 0.9829631024096386, 0.9791274472891566, 0.981457078313253, 0.9788450677710844, 0.986304593373494, 0.9831042921686747, 0.9847044427710844, 0.9854339231927711, 0.9888695406626506, 0.986351656626506, 0.9861398719879518, 0.9907991340361446, 0.9918109939759037, 0.9876694277108434, 0.9901637801204819, 0.9916698042168675, 0.9888695406626506, 0.9886812876506024, 0.9904696912650602, 0.9916227409638554, 0.9915521460843374, 0.9933640813253012, 0.9929640436746988, 0.9940229668674698, 0.9951995481927711, 0.9943994728915663, 0.9942818147590361, 0.9948701054216867, 0.9949877635542169, 0.9957407756024096, 0.9963055346385542, 0.9952936746987951, 0.9946818524096386, 0.9962349397590361, 0.9960231551204819, 0.9956466490963856, 0.9949171686746988, 0.9954819277108434, 0.9963290662650602, 0.9972467996987951, 0.9965879141566265, 0.997035015060241, 0.9971526731927711, 0.9972232680722891, 0.9973644578313253, 0.9972938629518072, 0.9973879894578314, 0.9963996611445783, 0.9968232304216867, 0.9981410015060241, 0.9978350903614458, 0.9982586596385542, 0.9983057228915663, 0.9984233810240963, 0.9982821912650602, 0.9983057228915663, 0.9955289909638554, 0.9985645707831325, 0.9984704442771084, 0.9964937876506024, 0.9976939006024096, 0.9984469126506024, 0.9985175075301205, 0.9979056852409639, 0.9975997740963856, 0.9979998117469879, 0.9979292168674698, 0.9973879894578314, 0.9981410015060241, 0.9984704442771084, 0.9979762801204819, 0.9991057981927711, 0.9984469126506024, 0.9989175451807228, 0.9988234186746988, 0.9986351656626506, 0.9994117093373494, 0.9988469503012049, 0.9988704819277109, 0.9994117093373494, 0.9986351656626506, 0.9976233057228916, 0.9993175828313253, 0.9982821912650602, 0.9979292168674698, 0.9988940135542169, 0.998988140060241, 0.9993881777108434, 0.9993175828313253, 0.9991999246987951, 0.9992234563253012, 0.9987292921686747, 0.9992705195783133, 0.9995999623493976, 0.9990822665662651, 0.9992940512048193, 0.9997176204819277, 0.9992234563253012, 0.9998117469879518, 0.9997176204819277, 0.9999058734939759, 0.9994117093373494, 0.9996940888554217, 0.9988234186746988, 0.9995293674698795, 0.9996940888554217, 0.9995764307228916, 0.9997646837349398, 0.9995999623493976, 0.9995764307228916, 0.9996470256024096, 0.9997411521084337, 0.9993881777108434, 0.9995528990963856, 0.9997411521084337, 0.9998352786144579, 0.9996940888554217, 0.9996705572289156, 0.9997176204819277, 0.9998823418674698, 0.9995764307228916, 0.9995528990963856, 0.9998352786144579, 0.9994352409638554, 0.9997411521084337, 0.999976468373494, 0.9996705572289156, 0.9995528990963856, 0.9996470256024096, 0.9997411521084337, 0.9996940888554217, 0.9995764307228916, 0.9998352786144579, 0.9996705572289156, 0.9998352786144579, 0.9997882153614458, 0.9997411521084337, 0.9996705572289156, 0.9998117469879518, 0.9999058734939759, 0.9996940888554217, 0.9995999623493976, 0.9999294051204819, 0.9997646837349398, 0.9992469879518072, 0.9997646837349398, 0.9997882153614458, 0.9997411521084337, 0.9998352786144579, 0.9998352786144579, 0.9998117469879518, 0.9998117469879518, 0.9999058734939759, 0.9999294051204819, 1.0, 0.9999058734939759, 0.9998117469879518, 0.9998352786144579, 0.9998823418674698, 0.9998823418674698, 0.9998117469879518, 0.9994587725903614, 0.9997882153614458, 0.9996940888554217, 0.9998117469879518, 0.9999529367469879, 0.9997646837349398, 0.9999294051204819, 0.9999294051204819, 0.9999529367469879, 0.9996470256024096, 0.9998117469879518, 0.9997176204819277, 0.9998352786144579, 0.9999058734939759, 0.9998352786144579, 0.9998588102409639, 0.9999058734939759, 0.9999529367469879, 0.9999294051204819, 0.999976468373494, 0.999976468373494, 0.999976468373494, 0.9999529367469879, 0.9999294051204819], "end": "2016-01-20 22:20:44.319000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0, 211.0, 212.0, 213.0, 214.0, 215.0, 216.0, 217.0, 218.0, 219.0, 220.0, 221.0, 222.0, 223.0, 224.0, 225.0, 226.0, 227.0, 228.0, 229.0], "accuracy_valid": [0.39476495726495725, 0.45926816239316237, 0.48985042735042733, 0.5261752136752137, 0.5383279914529915, 0.5754540598290598, 0.6021634615384616, 0.6601228632478633, 0.7001869658119658, 0.6948450854700855, 0.7276976495726496, 0.7526709401709402, 0.7657585470085471, 0.7753739316239316, 0.7737713675213675, 0.7931356837606838, 0.8051549145299145, 0.8035523504273504, 0.8222489316239316, 0.8242521367521367, 0.8309294871794872, 0.8350694444444444, 0.8348023504273504, 0.8412126068376068, 0.8438835470085471, 0.8449519230769231, 0.8494925213675214, 0.8461538461538461, 0.8421474358974359, 0.843482905982906, 0.8465544871794872, 0.8385416666666666, 0.8433493589743589, 0.8481570512820513, 0.8390758547008547, 0.8476228632478633, 0.8517628205128205, 0.8532318376068376, 0.8564369658119658, 0.8553685897435898, 0.8576388888888888, 0.859375, 0.8620459401709402, 0.8589743589743589, 0.8597756410256411, 0.8667200854700855, 0.8656517094017094, 0.8589743589743589, 0.8604433760683761, 0.8659188034188035, 0.8609775641025641, 0.8635149572649573, 0.860176282051282, 0.8648504273504274, 0.8631143162393162, 0.8645833333333334, 0.8656517094017094, 0.8667200854700855, 0.8617788461538461, 0.8639155982905983, 0.8671207264957265, 0.8663194444444444, 0.8675213675213675, 0.8645833333333334, 0.8687232905982906, 0.8668536324786325, 0.8693910256410257, 0.8633814102564102, 0.8680555555555556, 0.8693910256410257, 0.8685897435897436, 0.8681891025641025, 0.8689903846153846, 0.8671207264957265, 0.8685897435897436, 0.8703258547008547, 0.8696581196581197, 0.8644497863247863, 0.8699252136752137, 0.8703258547008547, 0.8645833333333334, 0.8679220085470085, 0.8669871794871795, 0.8713942307692307, 0.8693910256410257, 0.8696581196581197, 0.8696581196581197, 0.8745993589743589, 0.8688568376068376, 0.8679220085470085, 0.8727297008547008, 0.8721955128205128, 0.8669871794871795, 0.8720619658119658, 0.8729967948717948, 0.8701923076923077, 0.8739316239316239, 0.8748664529914529, 0.8721955128205128, 0.8743322649572649, 0.8740651709401709, 0.8687232905982906, 0.874198717948718, 0.8729967948717948, 0.8719284188034188, 0.8720619658119658, 0.8724626068376068, 0.8691239316239316, 0.8728632478632479, 0.8723290598290598, 0.8729967948717948, 0.8717948717948718, 0.8709935897435898, 0.8754006410256411, 0.8766025641025641, 0.8723290598290598, 0.875801282051282, 0.8737980769230769, 0.8740651709401709, 0.8774038461538461, 0.8735309829059829, 0.8791399572649573, 0.875534188034188, 0.8762019230769231, 0.875534188034188, 0.8736645299145299, 0.8732638888888888, 0.8748664529914529, 0.874198717948718, 0.8736645299145299, 0.8732638888888888, 0.8737980769230769, 0.8794070512820513, 0.8768696581196581, 0.8768696581196581, 0.875801282051282, 0.8724626068376068, 0.8732638888888888, 0.8778044871794872, 0.8739316239316239, 0.8760683760683761, 0.8759348290598291, 0.8768696581196581, 0.8767361111111112, 0.8792735042735043, 0.8819444444444444, 0.8739316239316239, 0.8786057692307693, 0.8728632478632479, 0.875267094017094, 0.8770032051282052, 0.8759348290598291, 0.8783386752136753, 0.8762019230769231, 0.8768696581196581, 0.8795405982905983, 0.8776709401709402, 0.8728632478632479, 0.8780715811965812, 0.874732905982906, 0.8802083333333334, 0.8775373931623932, 0.8784722222222222, 0.8760683760683761, 0.8804754273504274, 0.8798076923076923, 0.8776709401709402, 0.8798076923076923, 0.8772702991452992, 0.8754006410256411, 0.8786057692307693, 0.8772702991452992, 0.8775373931623932, 0.8782051282051282, 0.8775373931623932, 0.8783386752136753, 0.8770032051282052, 0.8764690170940171, 0.8796741452991453, 0.8828792735042735, 0.8787393162393162, 0.8762019230769231, 0.8772702991452992, 0.8812767094017094, 0.8772702991452992, 0.8740651709401709, 0.8754006410256411, 0.8808760683760684, 0.8790064102564102, 0.8767361111111112, 0.8768696581196581, 0.8782051282051282, 0.8774038461538461, 0.8799412393162394, 0.8783386752136753, 0.8807425213675214, 0.8774038461538461, 0.8791399572649573, 0.8778044871794872, 0.8783386752136753, 0.8775373931623932, 0.8778044871794872, 0.8822115384615384, 0.8795405982905983, 0.8792735042735043, 0.8770032051282052, 0.8774038461538461, 0.8791399572649573, 0.8792735042735043, 0.8780715811965812, 0.8811431623931624, 0.8796741452991453, 0.8784722222222222, 0.8766025641025641, 0.8782051282051282, 0.8774038461538461, 0.8815438034188035, 0.8804754273504274, 0.8816773504273504, 0.8804754273504274, 0.8799412393162394, 0.8790064102564102, 0.8802083333333334, 0.8763354700854701, 0.8796741452991453, 0.8795405982905983, 0.8795405982905983, 0.8791399572649573, 0.8784722222222222, 0.8795405982905983], "accuracy_test": 0.8812099358974359, "start": "2016-01-18 14:07:22.222000", "learning_rate_per_epoch": [0.0033141106832772493, 0.0016570553416386247, 0.001104703638702631, 0.0008285276708193123, 0.0006628221599385142, 0.0005523518193513155, 0.0004734444082714617, 0.00041426383540965617, 0.0003682345268316567, 0.0003314110799692571, 0.0003012827946804464, 0.00027617590967565775, 0.00025493159773759544, 0.00023672220413573086, 0.00022094072483014315, 0.00020713191770482808, 0.00019494770094752312, 0.00018411726341582835, 0.00017442688113078475, 0.00016570553998462856, 0.0001578147930558771, 0.0001506413973402232, 0.00014409176947083324, 0.00013808795483782887, 0.00013256442616693676, 0.00012746579886879772, 0.0001227448374265805, 0.00011836110206786543, 0.0001142796827480197, 0.00011047036241507158, 0.00010690680210245773, 0.00010356595885241404, 0.00010042759822681546, 9.747385047376156e-05, 9.468888310948387e-05, 9.205863170791417e-05, 8.957055979408324e-05, 8.721344056539237e-05, 8.497719682054594e-05, 8.285276999231428e-05, 8.083196735242382e-05, 7.890739652793854e-05, 7.707234180998057e-05, 7.53206986701116e-05, 7.364690827671438e-05, 7.204588473541662e-05, 7.051299326121807e-05, 6.904397741891444e-05, 6.763491546735168e-05, 6.628221308346838e-05, 6.49825669825077e-05, 6.373289943439886e-05, 6.253039464354515e-05, 6.137241871329024e-05, 6.025655966368504e-05, 5.9180551033932716e-05, 5.814229371026158e-05, 5.713984137400985e-05, 5.617136775981635e-05, 5.523518120753579e-05, 5.4329684644471854e-05, 5.3453401051228866e-05, 5.26049334439449e-05, 5.178297942620702e-05, 5.098632027511485e-05, 5.021379911340773e-05, 4.946433909935877e-05, 4.873692523688078e-05, 4.803058982361108e-05, 4.7344441554741934e-05, 4.667761822929606e-05, 4.602931585395709e-05, 4.5398777729133144e-05, 4.478527989704162e-05, 4.418814569362439e-05, 4.360672028269619e-05, 4.304039975977503e-05, 4.248859841027297e-05, 4.195077053736895e-05, 4.142638499615714e-05, 4.0914947021519765e-05, 4.041598367621191e-05, 3.99290474888403e-05, 3.945369826396927e-05, 3.898953946190886e-05, 3.8536170904990286e-05, 3.8093228795332834e-05, 3.76603493350558e-05, 3.723720146808773e-05, 3.682345413835719e-05, 3.641880175564438e-05, 3.602294236770831e-05, 3.563559948815964e-05, 3.5256496630609035e-05, 3.488537549856119e-05, 3.452198870945722e-05, 3.416609251871705e-05, 3.381745773367584e-05, 3.3475866075605154e-05, 3.314110654173419e-05, 3.281297904322855e-05, 3.249128349125385e-05, 3.217583434889093e-05, 3.186644971719943e-05, 3.156295861117542e-05, 3.1265197321772575e-05, 3.0972998501965776e-05, 3.068620935664512e-05, 3.0404686185647734e-05, 3.012827983184252e-05, 2.9856853871024214e-05, 2.9590275516966358e-05, 2.9328413802431896e-05, 2.907114685513079e-05, 2.881835462176241e-05, 2.8569920687004924e-05, 2.8325734092504717e-05, 2.8085683879908174e-05, 2.78496700047981e-05, 2.7617590603767894e-05, 2.7389345632400364e-05, 2.7164842322235927e-05, 2.6943989723804407e-05, 2.6726700525614433e-05, 2.6512885597185232e-05, 2.630246672197245e-05, 2.6095360226463526e-05, 2.589148971310351e-05, 2.5690780603326857e-05, 2.5493160137557425e-05, 2.529855555621907e-05, 2.5106899556703866e-05, 2.491812665539328e-05, 2.4732169549679384e-05, 2.454896821291186e-05, 2.436846261844039e-05, 2.419058910163585e-05, 2.401529491180554e-05, 2.3842523660277948e-05, 2.3672220777370967e-05, 2.350433169340249e-05, 2.333880911464803e-05, 2.3175600290414877e-05, 2.3014657926978543e-05, 2.285593654960394e-05, 2.2699388864566572e-05, 2.2544971216120757e-05, 2.239263994852081e-05, 2.224235504399985e-05, 2.2094072846812196e-05, 2.1947753339190967e-05, 2.1803360141348094e-05, 2.16608550545061e-05, 2.1520199879887514e-05, 2.1381360056693666e-05, 2.1244299205136485e-05, 2.1108986402396113e-05, 2.0975385268684477e-05, 2.0843464881181717e-05, 2.071319249807857e-05, 2.058453901554458e-05, 2.0457473510759883e-05, 2.033196869888343e-05, 2.0207991838105954e-05, 2.0085519281565212e-05, 1.996452374442015e-05, 1.9844974303850904e-05, 1.9726849131984636e-05, 1.9610122762969695e-05, 1.949476973095443e-05, 1.9380764570087194e-05, 1.9268085452495143e-05, 1.9156710550305434e-05, 1.9046614397666417e-05, 1.893777516670525e-05, 1.88301746675279e-05, 1.8723789253272116e-05, 1.8618600734043866e-05, 1.8514585462980904e-05, 1.8411727069178596e-05, 1.83100037247641e-05, 1.820940087782219e-05, 1.8109894881490618e-05, 1.8011471183854155e-05, 1.7914111595018767e-05, 1.781779974407982e-05, 1.7722517441143282e-05, 1.7628248315304518e-05, 1.75349778146483e-05, 1.7442687749280594e-05, 1.7351365386275575e-05, 1.726099435472861e-05, 1.7171558283735067e-05, 1.7083046259358525e-05, 1.699544009170495e-05, 1.690872886683792e-05, 1.6822898032842204e-05, 1.6737933037802577e-05, 1.6653822967782617e-05, 1.6570553270867094e-05, 1.648811303311959e-05, 1.6406489521614276e-05, 1.6325668184435926e-05, 1.6245641745626926e-05, 1.6166393834282644e-05, 1.6087917174445465e-05, 1.601019721420016e-05, 1.5933224858599715e-05, 1.5856989193707705e-05, 1.578147930558771e-05, 1.570668609929271e-05, 1.5632598660886288e-05, 1.555920607643202e-05, 1.5486499250982888e-05, 1.5414469089591876e-05, 1.534310467832256e-05, 1.5272400560206734e-05, 1.5202343092823867e-05, 1.5132925909711048e-05, 1.506413991592126e-05, 1.4995976016507484e-05, 1.4928426935512107e-05, 1.486148357798811e-05, 1.4795137758483179e-05, 1.4729381291544996e-05, 1.4664206901215948e-05, 1.459960731153842e-05, 1.4535573427565396e-05, 1.4472099792328663e-05, 1.4409177310881205e-05], "accuracy_train_last": 0.9999294051204819, "error_valid": [0.6052350427350428, 0.5407318376068376, 0.5101495726495726, 0.4738247863247863, 0.4616720085470085, 0.42454594017094016, 0.39783653846153844, 0.3398771367521367, 0.2998130341880342, 0.3051549145299145, 0.2723023504273504, 0.24732905982905984, 0.23424145299145294, 0.22462606837606836, 0.22622863247863245, 0.20686431623931623, 0.1948450854700855, 0.1964476495726496, 0.17775106837606836, 0.1757478632478633, 0.16907051282051277, 0.16493055555555558, 0.1651976495726496, 0.1587873931623932, 0.15611645299145294, 0.15504807692307687, 0.1505074786324786, 0.15384615384615385, 0.1578525641025641, 0.15651709401709402, 0.15344551282051277, 0.16145833333333337, 0.15665064102564108, 0.15184294871794868, 0.16092414529914534, 0.1523771367521367, 0.14823717948717952, 0.14676816239316237, 0.14356303418803418, 0.14463141025641024, 0.14236111111111116, 0.140625, 0.13795405982905984, 0.14102564102564108, 0.14022435897435892, 0.1332799145299145, 0.13434829059829057, 0.14102564102564108, 0.13955662393162394, 0.13408119658119655, 0.1390224358974359, 0.1364850427350427, 0.13982371794871795, 0.1351495726495726, 0.13688568376068377, 0.13541666666666663, 0.13434829059829057, 0.1332799145299145, 0.13822115384615385, 0.13608440170940173, 0.13287927350427353, 0.13368055555555558, 0.13247863247863245, 0.13541666666666663, 0.13127670940170943, 0.13314636752136755, 0.13060897435897434, 0.13661858974358976, 0.13194444444444442, 0.13060897435897434, 0.1314102564102564, 0.13181089743589747, 0.13100961538461542, 0.13287927350427353, 0.1314102564102564, 0.12967414529914534, 0.13034188034188032, 0.1355502136752137, 0.1300747863247863, 0.12967414529914534, 0.13541666666666663, 0.13207799145299148, 0.13301282051282048, 0.12860576923076927, 0.13060897435897434, 0.13034188034188032, 0.13034188034188032, 0.12540064102564108, 0.13114316239316237, 0.13207799145299148, 0.1272702991452992, 0.12780448717948723, 0.13301282051282048, 0.12793803418803418, 0.12700320512820518, 0.1298076923076923, 0.12606837606837606, 0.12513354700854706, 0.12780448717948723, 0.1256677350427351, 0.1259348290598291, 0.13127670940170943, 0.12580128205128205, 0.12700320512820518, 0.12807158119658124, 0.12793803418803418, 0.1275373931623932, 0.13087606837606836, 0.12713675213675213, 0.12767094017094016, 0.12700320512820518, 0.1282051282051282, 0.12900641025641024, 0.12459935897435892, 0.1233974358974359, 0.12767094017094016, 0.12419871794871795, 0.12620192307692313, 0.1259348290598291, 0.12259615384615385, 0.12646901709401714, 0.1208600427350427, 0.12446581196581197, 0.12379807692307687, 0.12446581196581197, 0.12633547008547008, 0.12673611111111116, 0.12513354700854706, 0.12580128205128205, 0.12633547008547008, 0.12673611111111116, 0.12620192307692313, 0.12059294871794868, 0.12313034188034189, 0.12313034188034189, 0.12419871794871795, 0.1275373931623932, 0.12673611111111116, 0.12219551282051277, 0.12606837606837606, 0.12393162393162394, 0.12406517094017089, 0.12313034188034189, 0.12326388888888884, 0.12072649572649574, 0.11805555555555558, 0.12606837606837606, 0.12139423076923073, 0.12713675213675213, 0.12473290598290598, 0.12299679487179482, 0.12406517094017089, 0.12166132478632474, 0.12379807692307687, 0.12313034188034189, 0.12045940170940173, 0.12232905982905984, 0.12713675213675213, 0.12192841880341876, 0.12526709401709402, 0.11979166666666663, 0.12246260683760679, 0.12152777777777779, 0.12393162393162394, 0.11952457264957261, 0.12019230769230771, 0.12232905982905984, 0.12019230769230771, 0.12272970085470081, 0.12459935897435892, 0.12139423076923073, 0.12272970085470081, 0.12246260683760679, 0.1217948717948718, 0.12246260683760679, 0.12166132478632474, 0.12299679487179482, 0.12353098290598286, 0.12032585470085466, 0.11712072649572647, 0.12126068376068377, 0.12379807692307687, 0.12272970085470081, 0.11872329059829057, 0.12272970085470081, 0.1259348290598291, 0.12459935897435892, 0.11912393162393164, 0.12099358974358976, 0.12326388888888884, 0.12313034188034189, 0.1217948717948718, 0.12259615384615385, 0.12005876068376065, 0.12166132478632474, 0.1192574786324786, 0.12259615384615385, 0.1208600427350427, 0.12219551282051277, 0.12166132478632474, 0.12246260683760679, 0.12219551282051277, 0.11778846153846156, 0.12045940170940173, 0.12072649572649574, 0.12299679487179482, 0.12259615384615385, 0.1208600427350427, 0.12072649572649574, 0.12192841880341876, 0.11885683760683763, 0.12032585470085466, 0.12152777777777779, 0.1233974358974359, 0.1217948717948718, 0.12259615384615385, 0.11845619658119655, 0.11952457264957261, 0.1183226495726496, 0.11952457264957261, 0.12005876068376065, 0.12099358974358976, 0.11979166666666663, 0.12366452991452992, 0.12032585470085466, 0.12045940170940173, 0.12045940170940173, 0.1208600427350427, 0.12152777777777779, 0.12045940170940173], "accuracy_train_std": [0.08386157989605461, 0.08634647458831915, 0.090282430150456, 0.08930583989901925, 0.08912753503475111, 0.08662476911225289, 0.08650962995929667, 0.08433712616199882, 0.07871091987713205, 0.07740155070882662, 0.07588508089336034, 0.07389164161931956, 0.0703806637868499, 0.06782103386828, 0.06876062062903177, 0.06771248731583082, 0.06312050786235171, 0.062332427038434335, 0.05971457056519608, 0.05655624129525226, 0.05584974144871697, 0.053134871192275855, 0.05338788294727919, 0.05096052304771725, 0.04804231528609791, 0.048314864833725664, 0.043307604398794906, 0.044938654636973004, 0.04478619375114033, 0.04285376671352233, 0.04093518352036263, 0.043373811610844866, 0.041230525580934645, 0.03899698177036998, 0.04113925310911108, 0.0364669457284401, 0.035471958129468885, 0.03617447072774028, 0.03239015344169502, 0.03194606155654097, 0.030254607714662863, 0.029286799085051423, 0.027367827927085836, 0.02808221360794839, 0.02605366597674788, 0.023442035778261053, 0.02682521821184658, 0.025959639860843802, 0.026213249664574872, 0.02129959294502624, 0.024523116254348182, 0.023007674020772338, 0.021769781619533368, 0.0197517554012441, 0.021191428428066234, 0.021519887941547126, 0.017862209627007034, 0.016557438348810558, 0.019839268550788776, 0.018520152505125297, 0.016796175432175504, 0.019108361736892173, 0.019190023380992537, 0.017812912805899722, 0.016596454300499337, 0.016538414833306692, 0.014706570013625724, 0.015427973942126553, 0.013597208295330813, 0.012446428720652356, 0.013756565508936988, 0.013627371942493508, 0.013011180843671246, 0.012627497029031121, 0.01157925802855323, 0.010929395597093924, 0.011816532248851819, 0.012992100607487789, 0.011138789301571547, 0.011098124790047, 0.011796247348091196, 0.013029638178010243, 0.01201284347405408, 0.011170163438221161, 0.009263424355886372, 0.010402975185614474, 0.009853993281107438, 0.009470800004144225, 0.009216590430799292, 0.009337045758762985, 0.009197675701803968, 0.00898251841060606, 0.010620013618998194, 0.009899825468580325, 0.007966315803174097, 0.00855930365497684, 0.007370622317961598, 0.007281279972068418, 0.0072571704009424115, 0.007122545620565173, 0.007179575988525934, 0.012030439050212682, 0.006870872285259798, 0.006850532992954462, 0.010924125169962311, 0.00918674216589593, 0.007004523728781913, 0.006860871597234456, 0.007907711967857035, 0.00858243293416555, 0.008205454880681397, 0.008232706929845585, 0.008817266225062268, 0.007873465616821784, 0.006742334583134472, 0.007972326138214783, 0.005210010518593235, 0.007211435778007172, 0.005714455762943081, 0.006308407308544259, 0.006723130241460462, 0.004416868116742445, 0.005890948945582346, 0.005957546083752507, 0.004247116351604867, 0.006500694342175088, 0.008716458785305596, 0.0045672577471588295, 0.007326125130673455, 0.007960231367419418, 0.005899965853214186, 0.005531434043697957, 0.004329563491004113, 0.0045672577471588295, 0.005082613435200353, 0.004864562647928654, 0.006172108338025525, 0.004871842377244268, 0.0037164328483173634, 0.005547827364119666, 0.00479929815750251, 0.0029571306612208225, 0.005013451546813746, 0.002418153723393285, 0.0029571306612208225, 0.0017124816828558219, 0.004416868116742444, 0.0030767095475028312, 0.006308407308544259, 0.003806017812341063, 0.0030767095475028312, 0.0036134649544624603, 0.002961247405259671, 0.0035129967918325522, 0.0036134649544624603, 0.0033024020045028615, 0.002832312549927618, 0.004329563491004113, 0.0037110650789145346, 0.0030810259715075413, 0.002262832332131093, 0.0030767095475028312, 0.0031916381463076003, 0.0029571306612208225, 0.001913889548907091, 0.0036134649544624603, 0.003711065078914535, 0.0022628323321310924, 0.004162903687405116, 0.002832312549927618, 0.0008572103539197562, 0.003414276016023356, 0.0037110650789145354, 0.0033024020045028615, 0.002832312549927618, 0.0030767095475028312, 0.0036134649544624603, 0.002262832332131093, 0.0031916381463076003, 0.0022628323321310924, 0.00256386762951829, 0.002832312549927618, 0.0031916381463076003, 0.002418153723393285, 0.0017124816828558219, 0.0030767095475028312, 0.0035129967918325522, 0.0014836126003527968, 0.002701529111175859, 0.004943210092432561, 0.002701529111175859, 0.00256386762951829, 0.002832312549927618, 0.0022628323321310924, 0.002262832332131093, 0.002418153723393285, 0.002418153723393285, 0.0017124816828558219, 0.0014836126003527968, 0.0, 0.0017124816828558219, 0.002418153723393285, 0.002262832332131093, 0.001913889548907091, 0.001913889548907091, 0.002418153723393285, 0.004076816091297321, 0.00256386762951829, 0.0030767095475028312, 0.002418153723393285, 0.0012118216481159388, 0.002961247405259671, 0.0014836126003527968, 0.0014836126003527968, 0.0012118216481159388, 0.0033024020045028615, 0.002418153723393285, 0.0029571306612208225, 0.002262832332131093, 0.0017124816828558219, 0.0022628323321310924, 0.0020957684561570327, 0.0017124816828558219, 0.0012118216481159388, 0.0014836126003527968, 0.0008572103539197562, 0.0008572103539197563, 0.0008572103539197562, 0.0012118216481159388, 0.0014836126003527968], "accuracy_test_std": 0.05872193476403675, "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-3, 3], "momentum": 0.8377032613764267, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.0033141107996355855, "patience_threshold": 1, "do_flip": true, "batch_size": 32, "optimization": "adam", "nb_data_augmentation": 4, "learning_rate_decay_method": "lin", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 2.481509077210505e-10, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.05955933219163261}, "accuracy_valid_max": 0.8828792735042735, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = 1234\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-10, -6], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128, 256],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-3, 3)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_optimizer.learning_rate = learning_rate\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8795405982905983, "loss_train": [27.418947219848633, 1.6202329397201538, 1.4468989372253418, 1.3523683547973633, 1.2793385982513428, 1.2066458463668823, 1.1279985904693604, 1.0362393856048584, 0.9426625967025757, 0.855665385723114, 0.7763687372207642, 0.702342689037323, 0.638515830039978, 0.5772051811218262, 0.5302383303642273, 0.4812266528606415, 0.4484318494796753, 0.41503220796585083, 0.3831101655960083, 0.3574693202972412, 0.3315655291080475, 0.30777040123939514, 0.2885991036891937, 0.2676313519477844, 0.2522037923336029, 0.233863964676857, 0.22144602239131927, 0.20562811195850372, 0.19424523413181305, 0.1829615831375122, 0.1723148077726364, 0.1611727923154831, 0.15438371896743774, 0.14251551032066345, 0.13697893917560577, 0.1294075846672058, 0.12060856819152832, 0.11637043952941895, 0.10848665237426758, 0.10499732196331024, 0.09872714430093765, 0.09538470953702927, 0.09129758924245834, 0.08472919464111328, 0.08204837888479233, 0.07803218066692352, 0.07355120033025742, 0.07097893953323364, 0.06780841946601868, 0.06501805037260056, 0.06116226688027382, 0.06051589921116829, 0.0578031986951828, 0.05565071105957031, 0.053537432104349136, 0.04951131343841553, 0.0480521023273468, 0.048189397901296616, 0.043992944061756134, 0.04291541874408722, 0.04189852625131607, 0.04040460288524628, 0.037650216370821, 0.038244932889938354, 0.03715559467673302, 0.036513231694698334, 0.03395494446158409, 0.03330833092331886, 0.03197387605905533, 0.030219871550798416, 0.03007463924586773, 0.029635880142450333, 0.029676714912056923, 0.02622061036527157, 0.027057791128754616, 0.02549414150416851, 0.02629336714744568, 0.024247366935014725, 0.023987308144569397, 0.022674111649394035, 0.02308368682861328, 0.02212512120604515, 0.021965058520436287, 0.02042219415307045, 0.020339472219347954, 0.019271353259682655, 0.018752461299300194, 0.01940295659005642, 0.018269626423716545, 0.017968129366636276, 0.018564380705356598, 0.017419904470443726, 0.016476904973387718, 0.01591920480132103, 0.015221084468066692, 0.016352932900190353, 0.014351321384310722, 0.015391102060675621, 0.015375354327261448, 0.015047060325741768, 0.014211036264896393, 0.013187405653297901, 0.01431482844054699, 0.013247291557490826, 0.011559482663869858, 0.012355759739875793, 0.011946804821491241, 0.011506550945341587, 0.012504630722105503, 0.012613270431756973, 0.01120299194008112, 0.011245610192418098, 0.010745102539658546, 0.010456571355462074, 0.010975050739943981, 0.010722018778324127, 0.00997318234294653, 0.010138295590877533, 0.009289180859923363, 0.01027636881917715, 0.00918444525450468, 0.008504933677613735, 0.009672125801444054, 0.009373392909765244, 0.009103160351514816, 0.008556753396987915, 0.008402417413890362, 0.009253384545445442, 0.008346405811607838, 0.008913956582546234, 0.00754669401794672, 0.008150073699653149, 0.008095956407487392, 0.007843585684895515, 0.007852363400161266, 0.008099881000816822, 0.0073307715356349945, 0.006909835617989302, 0.0075870999135077, 0.007285289466381073, 0.006679593585431576, 0.007385837845504284, 0.0076440563425421715, 0.006401594262570143, 0.007621739525347948, 0.0066627636551856995, 0.006549726705998182, 0.006004638969898224, 0.0069305505603551865, 0.005800374783575535, 0.005957095418125391, 0.0061135138384997845, 0.006311626173555851, 0.0053684646263718605, 0.006310420576483011, 0.005399213172495365, 0.005249158013612032, 0.005168644245713949, 0.005062437150627375, 0.005220117513090372, 0.005690918304026127, 0.0055242376402020454, 0.00441081915050745, 0.004949414171278477, 0.0055041806772351265, 0.004784299526363611, 0.004998419433832169, 0.0050947098061442375, 0.004747198428958654, 0.005111284088343382, 0.004346264060586691, 0.004687121137976646, 0.004520579241216183, 0.005412254948168993, 0.004254437517374754, 0.004674699157476425, 0.004348768386989832, 0.005040551070123911, 0.00450880965217948, 0.004003956913948059, 0.0046121226623654366, 0.0038113812915980816, 0.0044653150252997875, 0.004464251454919577, 0.003915721084922552, 0.0040082852356135845, 0.0039602527394890785, 0.00442586001008749, 0.003183532739058137, 0.0036972707603126764, 0.0038536221254616976, 0.0038147023878991604, 0.004049467854201794, 0.0040344479493796825, 0.0034771873615682125, 0.0036881286650896072, 0.003485108492895961, 0.003957023844122887, 0.003675800981000066, 0.003533693728968501, 0.0030983155593276024, 0.003970201592892408, 0.0030111551750451326, 0.0033615967258810997, 0.0028560226783156395, 0.00333399954251945, 0.003684715600684285, 0.0032132058404386044, 0.0030436692759394646, 0.0033706631511449814, 0.003697694279253483, 0.002756071975454688, 0.0027716325130313635, 0.003487475449219346, 0.002936108037829399, 0.003070824546739459, 0.002713436260819435, 0.0027798223309218884, 0.0027951889205724, 0.0027729852590709925, 0.003094893414527178, 0.0026598635595291853, 0.0027123126201331615, 0.003255299525335431, 0.00207765051163733, 0.0027832461055368185, 0.0028647230938076973, 0.002386457286775112, 0.0028669447638094425, 0.0025642041582614183], "accuracy_train_first": 0.4016848644578313, "model": "residualv2", "loss_std": [228.76898193359375, 0.18250252306461334, 0.17702670395374298, 0.18063512444496155, 0.18178272247314453, 0.18518805503845215, 0.18815818428993225, 0.1906001716852188, 0.19193634390830994, 0.187998428940773, 0.18654336035251617, 0.17929162085056305, 0.1730773150920868, 0.16711284220218658, 0.15917055308818817, 0.15265589952468872, 0.14616155624389648, 0.13972121477127075, 0.13349132239818573, 0.12943710386753082, 0.12441107630729675, 0.11865305155515671, 0.1110633835196495, 0.10688351094722748, 0.10395257920026779, 0.09738965332508087, 0.09328783303499222, 0.08938170224428177, 0.08644093573093414, 0.08339666575193405, 0.07930951565504074, 0.07568975538015366, 0.07375793904066086, 0.06836842000484467, 0.0687546506524086, 0.06402023881673813, 0.06145954504609108, 0.059900470077991486, 0.05751316621899605, 0.05609562620520592, 0.0532723106443882, 0.05254831537604332, 0.05241714417934418, 0.04849319905042648, 0.04787725955247879, 0.045863233506679535, 0.04484019801020622, 0.043619755655527115, 0.04159388318657875, 0.040437277406454086, 0.03986668214201927, 0.03961596265435219, 0.03755311295390129, 0.03658771887421608, 0.0361136719584465, 0.032616887241601944, 0.03185490518808365, 0.03305993601679802, 0.03145059198141098, 0.0317363440990448, 0.031046483665704727, 0.030547671020030975, 0.02751036547124386, 0.028716761618852615, 0.028077302500605583, 0.02834194526076317, 0.025632038712501526, 0.025973115116357803, 0.026544496417045593, 0.0241764634847641, 0.024343039840459824, 0.025041522458195686, 0.026779456064105034, 0.021192865446209908, 0.021997731178998947, 0.022317329421639442, 0.0242549329996109, 0.02074611745774746, 0.021199725568294525, 0.020864641293883324, 0.02132403664290905, 0.021669091656804085, 0.019770225510001183, 0.018579307943582535, 0.018965603783726692, 0.01863180287182331, 0.018477214500308037, 0.01933220960199833, 0.017544087022542953, 0.018277235329151154, 0.01946932077407837, 0.018634244799613953, 0.017329411581158638, 0.017101433128118515, 0.015831204131245613, 0.016978593543171883, 0.015778563916683197, 0.016896702349185944, 0.017312338575720787, 0.0162692628800869, 0.01564744859933853, 0.014875981956720352, 0.01613805629312992, 0.015583592467010021, 0.013425609096884727, 0.014786922372877598, 0.014222615398466587, 0.013582575134932995, 0.015199375338852406, 0.015539496205747128, 0.01374614704400301, 0.014311547391116619, 0.013055260293185711, 0.013264111243188381, 0.014349021948873997, 0.013734119944274426, 0.01244477927684784, 0.01355811022222042, 0.012768515385687351, 0.013254452496767044, 0.012262039817869663, 0.011353281326591969, 0.013012896291911602, 0.013103616423904896, 0.013128518126904964, 0.011736773885786533, 0.011654296889901161, 0.014345564879477024, 0.012262534350156784, 0.012396849691867828, 0.011279016733169556, 0.012178074568510056, 0.012444455176591873, 0.011297588236629963, 0.01161506213247776, 0.011939296498894691, 0.011304051615297794, 0.010210372507572174, 0.01151985488831997, 0.011996537446975708, 0.009875082410871983, 0.011539301835000515, 0.013992146588861942, 0.00984096247702837, 0.011256118305027485, 0.010757762007415295, 0.011317389085888863, 0.009725320152938366, 0.011082149110734463, 0.009464931674301624, 0.009970147162675858, 0.009503831155598164, 0.009894941933453083, 0.008705198764801025, 0.010184559971094131, 0.009012301452457905, 0.009257977828383446, 0.008259949274361134, 0.008456448093056679, 0.008957707323133945, 0.009884703904390335, 0.009724396280944347, 0.007845837622880936, 0.00850695837289095, 0.009038041345775127, 0.008547156117856503, 0.00862080603837967, 0.008926434442400932, 0.00881692860275507, 0.009331101551651955, 0.008052368648350239, 0.008104167878627777, 0.00856919214129448, 0.009749867022037506, 0.0073812901973724365, 0.009161601774394512, 0.008450142107903957, 0.009575439617037773, 0.008414926938712597, 0.007582321763038635, 0.008745473809540272, 0.007235257420688868, 0.00940012652426958, 0.008487622253596783, 0.008311813697218895, 0.007865703664720058, 0.00829700380563736, 0.008029310964047909, 0.005815901327878237, 0.007193715311586857, 0.007437259424477816, 0.007282935082912445, 0.008011927828192711, 0.007859655655920506, 0.007387012243270874, 0.00790041871368885, 0.00772832753136754, 0.008027439936995506, 0.007397634908556938, 0.006969905924052, 0.006384689826518297, 0.008605530485510826, 0.006180056370794773, 0.008107307367026806, 0.006375989411026239, 0.0069443280808627605, 0.007888060063123703, 0.007153989747166634, 0.006408640183508396, 0.007041094824671745, 0.008044639602303505, 0.005940222647041082, 0.005932644009590149, 0.0075251637026667595, 0.006651601754128933, 0.007392726838588715, 0.0060195461846888065, 0.006634335033595562, 0.005807216744869947, 0.0062332795932888985, 0.006233413703739643, 0.006252011749893427, 0.006087161600589752, 0.007284953724592924, 0.004780509974807501, 0.00785900093615055, 0.00624462403357029, 0.005637739319354296, 0.0065731629729270935, 0.006136580370366573]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:04 2016", "state": "available"}], "summary": "5b90bc62e6377fdd712ed65566e2ce29"}