{"content": {"hp_model": {"f0": 32, "f1": 64, "f2": 16, "f3": 64, "nonlin": "rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.7653586864471436, 1.402312159538269, 1.2363882064819336, 1.1190094947814941, 1.0302245616912842, 0.9592106938362122, 0.9042921662330627, 0.8567206859588623, 0.815151035785675, 0.7809814214706421, 0.7465791702270508, 0.7146836519241333, 0.6878287196159363, 0.6643129587173462, 0.6380575895309448, 0.6183640360832214, 0.6007513403892517, 0.5813387632369995, 0.564191460609436, 0.5467090010643005, 0.5324080586433411, 0.5201358795166016, 0.5036641955375671, 0.4931773245334625, 0.48056864738464355, 0.46974050998687744, 0.4563537538051605, 0.45021679997444153, 0.43859633803367615, 0.4346899092197418, 0.4215524196624756, 0.4154224395751953, 0.40850284695625305, 0.4006825387477875, 0.395020455121994, 0.3902774751186371, 0.3834989368915558, 0.37692126631736755, 0.37243616580963135, 0.3686542510986328, 0.3602389991283417, 0.3526822626590729, 0.3496917188167572, 0.3473454713821411, 0.34180325269699097, 0.3368240296840668, 0.33551421761512756, 0.3279081881046295, 0.32542183995246887, 0.32200878858566284, 0.31773024797439575, 0.3155868649482727, 0.3113101124763489, 0.31012189388275146, 0.3039235770702362, 0.3011421859264374, 0.299095094203949, 0.29478615522384644, 0.29241952300071716, 0.2907094955444336, 0.2877383232116699, 0.28482797741889954, 0.280577152967453, 0.2806398868560791, 0.2786327302455902, 0.2774764597415924, 0.272560179233551, 0.2721628248691559, 0.26971784234046936, 0.26708412170410156, 0.266948938369751, 0.2641529440879822, 0.2633904814720154, 0.26124200224876404, 0.25780367851257324, 0.2585771083831787, 0.2560688555240631, 0.2543252408504486, 0.25325414538383484, 0.253246933221817, 0.2517744302749634, 0.24960802495479584, 0.24803833663463593, 0.24793118238449097, 0.24659374356269836, 0.24503718316555023, 0.24464108049869537, 0.2429085373878479, 0.24242433905601501, 0.241464301943779, 0.24103614687919617, 0.23951108753681183, 0.2388276308774948, 0.23669229447841644, 0.23763121664524078, 0.23649939894676208, 0.2358303815126419, 0.23470212519168854, 0.23486825823783875, 0.2325497269630432, 0.233016237616539, 0.23260846734046936, 0.23145833611488342, 0.23134788870811462, 0.230501189827919, 0.22955745458602905, 0.23007388412952423, 0.2298985868692398, 0.22767870128154755, 0.2288823425769806, 0.227799192070961, 0.227451890707016, 0.22713464498519897, 0.2263852208852768, 0.2262762188911438, 0.22545188665390015, 0.22604238986968994, 0.22501930594444275, 0.22456298768520355, 0.22424978017807007, 0.2245965451002121, 0.22463616728782654, 0.22334907948970795, 0.22431348264217377, 0.2224321812391281, 0.22211873531341553, 0.22286289930343628, 0.22194623947143555, 0.22110453248023987, 0.2217499017715454, 0.22131414711475372, 0.22121942043304443, 0.22043955326080322, 0.22147364914417267, 0.2208174765110016, 0.22070010006427765, 0.21992214024066925, 0.2200665920972824, 0.22069527208805084, 0.22069084644317627, 0.2200401872396469, 0.22033818066120148, 0.21930564939975739, 0.21922601759433746, 0.21928907930850983, 0.21799920499324799, 0.21789123117923737, 0.2189016044139862, 0.21818865835666656, 0.2182222455739975, 0.21857763826847076, 0.21848633885383606, 0.21810947358608246, 0.2182505577802658, 0.21710753440856934, 0.21781499683856964, 0.21765664219856262, 0.21771758794784546, 0.2175028920173645, 0.21655391156673431, 0.21716423332691193, 0.2173483967781067, 0.21721211075782776, 0.21727818250656128, 0.21774785220623016, 0.21721456944942474, 0.21707555651664734, 0.21695086359977722, 0.21715395152568817, 0.21618682146072388, 0.21640802919864655, 0.21688434481620789, 0.21704767644405365, 0.21699900925159454, 0.2171340137720108, 0.2159164994955063, 0.21531955897808075, 0.21733346581459045, 0.21657931804656982, 0.2162259966135025, 0.21654382348060608], "moving_avg_accuracy_train": [0.05915129905523255, 0.12053069809258489, 0.17771751370576086, 0.23499765460977873, 0.289500323165043, 0.34101175898715075, 0.38969912864837475, 0.43477570289824746, 0.4769252522789653, 0.5155479465739554, 0.5520125973703712, 0.5847794495692422, 0.6158367668458452, 0.6444088787363806, 0.670572641304557, 0.6945640228920784, 0.7171024937398677, 0.7373568905683542, 0.7555601989794498, 0.7731823727184558, 0.7896724083621233, 0.8045761834104624, 0.8184709228063578, 0.8303810943627006, 0.8425555395764398, 0.8538194599116622, 0.8636827288002947, 0.8730153278690932, 0.8818378080655264, 0.8901873024816112, 0.8975623385275161, 0.9044719133795449, 0.9104324752773324, 0.9165200301674655, 0.9225173017042904, 0.9271871466076893, 0.9320154360016916, 0.936435337267017, 0.9405155189046103, 0.9440667746403489, 0.9476721670418087, 0.9507798364233605, 0.9538115067988816, 0.956858591572574, 0.9597660173855548, 0.9624524911303418, 0.9645726624042125, 0.9667110062828388, 0.9688657055057454, 0.9705584690325518, 0.9724423903209725, 0.9742541408722085, 0.9757405571421305, 0.9769923373279266, 0.9784723981630095, 0.9795649625872032, 0.9807853636499114, 0.9819860311539679, 0.9830551143100273, 0.9840497330873579, 0.9849286139452887, 0.9857010055269504, 0.9864519615218744, 0.9871278940149435, 0.9878664055432202, 0.988563581953184, 0.9892468442935799, 0.9897594738523171, 0.9902998955147044, 0.9908025510525197, 0.9911968483651341, 0.9916098446667252, 0.991997781331005, 0.9924724823645712, 0.9928997493435995, 0.9932261248556681, 0.9935593903462917, 0.9938686298830911, 0.9942097244840676, 0.9944841575416132, 0.9947264969957852, 0.994967890041454, 0.9952385861563562, 0.9954357096835776, 0.9956479980902199, 0.9957762786383407, 0.9959661358935543, 0.9961649452577795, 0.9962601683284394, 0.9963946972170333, 0.9965482892512824, 0.9967027981237732, 0.9968582042483191, 0.9970236463973151, 0.9971213189599646, 0.9972208500103966, 0.9972964770629285, 0.9973994907399875, 0.9974805412564741, 0.9975534506724935, 0.9976237554933486, 0.9977149316178324, 0.9977923037834301, 0.9978944908158014, 0.9979771585496974, 0.9980469092125848, 0.998116660255612, 0.9981608350038603, 0.9981843522844358, 0.9982357087226589, 0.9982982055587263, 0.9983428630159581, 0.9983853438274576, 0.9984026862673401, 0.9984275950584725, 0.99846392781453, 0.9984780621533242, 0.9985139984975157, 0.9985370766608685, 0.9985485103638293, 0.9985820521845892, 0.9986215404185113, 0.9986384786385649, 0.9986606984830417, 0.9986806963430709, 0.9987219459051924, 0.9987544202134826, 0.9987813219421343, 0.9988032083491114, 0.9988275564130098, 0.9988494696705184, 0.998864541304657, 0.9988734915265814, 0.998909412463209, 0.9989254652645071, 0.9989375876368658, 0.9989508229207983, 0.9989627346763376, 0.9989897312979895, 0.9989954270670001, 0.9989982281103, 0.9990170250909367, 0.9990246417782715, 0.9990454476897301, 0.9990455718195665, 0.999057309280467, 0.9990725232928965, 0.9990745901600354, 0.999074125191651, 0.9990737067201049, 0.9990779803933324, 0.9990748873016274, 0.9990860183631313, 0.9990914220696845, 0.9990939602567729, 0.9991055091715718, 0.9991159031948909, 0.999125257815878, 0.9991313878747756, 0.9991299294813549, 0.9991309420760858, 0.9991295282625341, 0.9991351952279474, 0.9991449457944384, 0.9991560464530898, 0.9991683621946856, 0.9991794463621219, 0.9991824466663859, 0.9991851469402235, 0.9991782765914392, 0.9991720932775333, 0.9991781540390657, 0.9991859338732544, 0.9991813099799766, 0.9991934245176932, 0.9991996773040192, 0.9992006545140935, 0.9991992088543508, 0.9992048832070108, 0.9992076649755954, 0.9992008679720835], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.058768178181475894, 0.1194652732021837, 0.17491876588384786, 0.23010049387001125, 0.2820233793474679, 0.33094727094020604, 0.37609270174339926, 0.4180450373672521, 0.4568072340258582, 0.4921001610844922, 0.5252706925569918, 0.5548759122226028, 0.5825267340274509, 0.6078347192749317, 0.6301449502841856, 0.6510033491751948, 0.6699112150295127, 0.6861073346786698, 0.7016411383437094, 0.7161088133835855, 0.7296363862526817, 0.7418356158973684, 0.7530824477564266, 0.7625636537526213, 0.771784450933835, 0.7801889136435388, 0.7872342047904801, 0.7938211664650465, 0.8005275934461774, 0.8071534332638337, 0.8123394100616974, 0.8169864931519132, 0.8214272746066766, 0.825841075995783, 0.8304482628709788, 0.8334950687374955, 0.8370947747308393, 0.8402480313974391, 0.8432781863714, 0.8461151891292147, 0.8488313010521367, 0.8506216225658989, 0.8530071024403331, 0.8555355408220527, 0.857538462643462, 0.8593532993139802, 0.8609154691472659, 0.8625024684486539, 0.8644008120593607, 0.8656473660661506, 0.8673135806078789, 0.8686636007944554, 0.8699061215508532, 0.8706184696743824, 0.8721283412216279, 0.8730101218867392, 0.87400918499927, 0.8747161178265267, 0.8753401503398078, 0.8763279961868512, 0.8772892701280306, 0.8779479266525017, 0.8786271962519352, 0.8793626682212446, 0.880043859512825, 0.8808950423209552, 0.8816143377405916, 0.8822128754932643, 0.8827840625297813, 0.8833713730501465, 0.8837422906208848, 0.8841768612105283, 0.8848710915051381, 0.885151013369308, 0.8852707247206303, 0.8854628846469106, 0.8860560151863611, 0.8862103851944569, 0.8863716732469239, 0.8869074574941441, 0.887268622512802, 0.8876658837084345, 0.8880366553244133, 0.8881262091537943, 0.8885841960603275, 0.889082863003617, 0.8898043359747161, 0.8900599481227265, 0.8900957160645954, 0.890519561720937, 0.890555107902006, 0.8908709497096969, 0.891169473385188, 0.8915194758772415, 0.8916879937450896, 0.8915547800727643, 0.8919690556893584, 0.8921668872721544, 0.8923093441115806, 0.8924853538834044, 0.8925328698881363, 0.8925135696274853, 0.8926325357539686, 0.8926673925889633, 0.8929093717976875, 0.8931331830648314, 0.8933021101461494, 0.8933401632034471, 0.8931770394376958, 0.8934615921769081, 0.8933850112724703, 0.8936630328767744, 0.8937148512946692, 0.8938103159957745, 0.8940102155426579, 0.8939896650915548, 0.8939101345293119, 0.8938263499920434, 0.8939940550248421, 0.893961884085611, 0.8940448525388721, 0.8942670380304668, 0.8943327276291521, 0.8942321273530591, 0.8942799824829641, 0.8944054127926496, 0.894419614312707, 0.8947172754341471, 0.894934283301124, 0.8950919397789935, 0.8953579599388953, 0.8954976628154877, 0.8955725082621016, 0.8955564789626234, 0.8954932244680929, 0.8954108518518559, 0.8952858293549233, 0.895420538258663, 0.8955428057806882, 0.8953944398769417, 0.8953840103847294, 0.8953491802705787, 0.8953432767390028, 0.8954101762394249, 0.8954714152984643, 0.8955997726390997, 0.8955932239331716, 0.8956005666377461, 0.895729245384363, 0.8956731283101587, 0.8955351147073055, 0.8956215105219665, 0.895526309300342, 0.8954894563258802, 0.8955549744075241, 0.8955030478910939, 0.8954451365037164, 0.8954418443800767, 0.895573158812551, 0.8955672124719586, 0.8955649492914043, 0.8954998182553361, 0.8953801651666247, 0.8954555828555344, 0.8954135954943032, 0.8955487643240144, 0.8955961445745949, 0.8955289235188673, 0.8956657960860318, 0.8957767743652298, 0.8957148748842791, 0.8957822651725831, 0.895768644735897, 0.8957207947577892, 0.8957764155361518, 0.8957654390804283, 0.8958176249351867, 0.8958757697270595, 0.8958914789459952, 0.8959666523992873, 0.8960098944447501], "moving_var_accuracy_train": [0.03148988561929401, 0.062247772693043366, 0.08545598234351753, 0.10643951498702303, 0.12253043140512568, 0.13415824044870925, 0.1420765560845964, 0.1461559783910754, 0.147529641168946, 0.1462020896834898, 0.14354891753448176, 0.13885702520823748, 0.13365233529518994, 0.12763439196663845, 0.12103183501548972, 0.11410892902824331, 0.10726988013882877, 0.10023505744291811, 0.09319379563261146, 0.0866692851349397, 0.08044964810121055, 0.07440378588731297, 0.06870098134450031, 0.06310755288856394, 0.05813075164606858, 0.05345955959332585, 0.04898916029251848, 0.04487412091167708, 0.04108723423185748, 0.037605937321709824, 0.034334863999644416, 0.03133105762140207, 0.028517706542498068, 0.025999460809111714, 0.02372322012117823, 0.021547165171856616, 0.019602260060921063, 0.017817853799585993, 0.01618589935938917, 0.014680812179155806, 0.01332972065055676, 0.0120836670663664, 0.010958019587122069, 0.00994578015897248, 0.00902728026679711, 0.008189506510750273, 0.00741101199575016, 0.006711063427064474, 0.006081741643028776, 0.0054993565139450735, 0.004981363297339249, 0.004512768928144463, 0.0040813769352774166, 0.0036873418244516426, 0.003338322862686396, 0.00301523384960688, 0.002727114873430927, 0.002467377808185509, 0.002230926476518089, 0.002016737227476248, 0.0018220153887905592, 0.0016451831487102988, 0.0014857402479960796, 0.0013412781858131511, 0.0012120589607284155, 0.0010952275591750626, 0.0009899064300897859, 0.0008932808886612275, 0.0008065812999537022, 0.0007281971332655992, 0.0006567766532756537, 0.0005926340814542394, 0.0005347251270082488, 0.0004832806839488429, 0.0004365956291962703, 0.0003938947550505458, 0.00035550487253065697, 0.00032081504709767045, 0.0002897806521292419, 0.0002614804084439823, 0.00023586092329901958, 0.00021279926639159298, 0.00019217882723204208, 0.00017331066367369617, 0.00015638519461467885, 0.0001408947782444467, 0.000127129712416217, 0.00011477246764432805, 0.00010337682777856825, 9.320202719750809e-05, 8.409413909462014e-05, 7.589958211026357e-05, 6.852698347115463e-05, 6.192062506601913e-05, 5.5814421924867804e-05, 5.03221376023821e-05, 4.534139890181567e-05, 4.090276537058495e-05, 3.6871611509531265e-05, 3.323229240507655e-05, 2.9953548075088138e-05, 2.703301103866245e-05, 2.4383588002879728e-05, 2.2039208908855398e-05, 1.9896793606017433e-05, 1.7950900640174777e-05, 1.6199597448187784e-05, 1.4597200378814232e-05, 1.3142457903303847e-05, 1.1851949466696114e-05, 1.070190721069248e-05, 9.64966508600096e-06, 8.700940151511708e-06, 7.833552978350247e-06, 7.055781711396302e-06, 6.3620841627212905e-06, 5.7276737622475375e-06, 5.166529173527361e-06, 4.6546696707883174e-06, 4.19037926978004e-06, 3.7814668264610594e-06, 3.417354029379485e-06, 3.0782007561288064e-06, 2.7748241739131032e-06, 2.500940986173508e-06, 2.266160624933083e-06, 2.0490357887301695e-06, 1.85064553689722e-06, 1.6698921165007808e-06, 1.5082383587911e-06, 1.3617362406037115e-06, 1.2276070039438223e-06, 1.105567261801918e-06, 1.0066233588155416e-06, 9.082802547996488e-07, 8.187747965241368e-07, 7.3847387153868e-07, 6.659034936650461e-07, 6.058725025241101e-07, 5.455772283332954e-07, 4.910901180920818e-07, 4.4516104461236795e-07, 4.011670654847501e-07, 3.6494633250086533e-07, 3.284518379247256e-07, 2.968465660277673e-07, 2.6924510499283107e-07, 2.423590419514775e-07, 2.1812508351671698e-07, 1.9631415123095895e-07, 1.768471146535706e-07, 1.5924850813487135e-07, 1.4443876209322177e-07, 1.3025768628452168e-07, 1.1728989919933084e-07, 1.0676130617668718e-07, 9.705749704582226e-08, 8.81393277455502e-08, 7.966359356978911e-08, 7.171637641513654e-08, 6.455396690642391e-08, 5.8116560034613143e-08, 5.259393450410687e-08, 4.819020297575004e-08, 4.448020428062862e-08, 4.1397281272053546e-08, 3.836328205463217e-08, 3.460797028025806e-08, 3.1212796561414236e-08, 2.851633213702884e-08, 2.6008799261047554e-08, 2.373851480811373e-08, 2.1909395707329727e-08, 1.9910879637998937e-08, 1.9240649890988568e-08, 1.7668460933427066e-08, 1.591020929584764e-08, 1.4337997755087378e-08, 1.3193982482585517e-08, 1.1944228362452738e-08, 1.116559883687786e-08], "duration": 139445.006743, "accuracy_train": [0.5915129905523256, 0.672945289428756, 0.6923988542243448, 0.7505189227459395, 0.7800243401624216, 0.8046146813861205, 0.8278854555993909, 0.8404648711471022, 0.8562711967054264, 0.8631521952288667, 0.8801944545381136, 0.8796811193590809, 0.8953526223352714, 0.9015578857511997, 0.9060465044181433, 0.9104864571797711, 0.9199487313699704, 0.9196464620247323, 0.9193899746793098, 0.9317819363695091, 0.9380827291551311, 0.938710158845515, 0.9435235773694168, 0.937572638369786, 0.9521255465000923, 0.9551947429286637, 0.9524521487979882, 0.9570087194882798, 0.9612401298334257, 0.965332752226375, 0.9639376629406607, 0.9666580870478036, 0.9640775323574198, 0.9713080241786637, 0.9764927455357143, 0.9692157507382798, 0.9754700405477114, 0.9762144486549464, 0.9772371536429494, 0.9760280762619971, 0.9801206986549464, 0.9787488608573275, 0.9810965401785714, 0.9842823545358066, 0.9859328497023809, 0.9866307548334257, 0.9836542038690477, 0.9859561011904762, 0.9882579985119048, 0.9857933407738095, 0.9893976819167589, 0.9905598958333334, 0.9891183035714286, 0.9882583590000923, 0.991792945678756, 0.9893980424049464, 0.9917689732142857, 0.9927920386904762, 0.9926768627145626, 0.9930013020833334, 0.9928385416666666, 0.9926525297619048, 0.9932105654761905, 0.9932112864525655, 0.9945130092977114, 0.9948381696428571, 0.9953962053571429, 0.9943731398809523, 0.9951636904761905, 0.9953264508928571, 0.9947455241786637, 0.9953268113810447, 0.9954892113095238, 0.9967447916666666, 0.9967451521548542, 0.9961635044642857, 0.9965587797619048, 0.9966517857142857, 0.9972795758928571, 0.9969540550595238, 0.9969075520833334, 0.9971404274524732, 0.9976748511904762, 0.9972098214285714, 0.99755859375, 0.9969308035714286, 0.9976748511904762, 0.9979542295358066, 0.997117175964378, 0.997605457214378, 0.9979306175595238, 0.9980933779761905, 0.9982568593692323, 0.9985126257382798, 0.9980003720238095, 0.9981166294642857, 0.9979771205357143, 0.998326613833518, 0.9982099959048542, 0.9982096354166666, 0.9982564988810447, 0.9985355167381875, 0.9984886532738095, 0.9988141741071429, 0.9987211681547619, 0.9986746651785714, 0.9987444196428571, 0.9985584077380952, 0.9983960078096161, 0.9986979166666666, 0.9988606770833334, 0.9987447801310447, 0.9987676711309523, 0.9985587682262828, 0.9986517741786637, 0.9987909226190477, 0.9986052712024732, 0.9988374255952381, 0.9987447801310447, 0.9986514136904762, 0.9988839285714286, 0.9989769345238095, 0.9987909226190477, 0.9988606770833334, 0.9988606770833334, 0.9990931919642857, 0.9990466889880952, 0.9990234375, 0.9990001860119048, 0.9990466889880952, 0.9990466889880952, 0.9990001860119048, 0.9989540435239018, 0.9992327008928571, 0.9990699404761905, 0.9990466889880952, 0.9990699404761905, 0.9990699404761905, 0.9992327008928571, 0.9990466889880952, 0.9990234375, 0.9991861979166666, 0.9990931919642857, 0.9992327008928571, 0.9990466889880952, 0.9991629464285714, 0.9992094494047619, 0.9990931919642857, 0.9990699404761905, 0.9990699404761905, 0.9991164434523809, 0.9990470494762828, 0.9991861979166666, 0.9991400554286637, 0.9991168039405685, 0.9992094494047619, 0.9992094494047619, 0.9992094494047619, 0.9991865584048542, 0.9991168039405685, 0.9991400554286637, 0.9991168039405685, 0.9991861979166666, 0.9992327008928571, 0.9992559523809523, 0.9992792038690477, 0.9992792038690477, 0.9992094494047619, 0.9992094494047619, 0.9991164434523809, 0.9991164434523809, 0.9992327008928571, 0.9992559523809523, 0.9991396949404762, 0.9993024553571429, 0.9992559523809523, 0.9992094494047619, 0.9991861979166666, 0.9992559523809523, 0.9992327008928571, 0.9991396949404762], "end": "2016-02-03 00:31:10.074000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0], "moving_var_accuracy_valid": [0.0310832889009273, 0.06113219610641009, 0.0826947851511275, 0.10183051456786511, 0.11591133743782436, 0.12586212821124387, 0.13161890469177268, 0.13429700040126294, 0.13438987136934075, 0.13216120053570124, 0.12884763790564388, 0.12385109539812125, 0.11834709737666033, 0.11227683469457465, 0.10552886889429362, 0.09889163724353232, 0.092220040039663, 0.08535886466090503, 0.07899466970155052, 0.07297902532093054, 0.06732807983839588, 0.06193466268987051, 0.05687961746267683, 0.052000695120689645, 0.0475658335145344, 0.04344496510403019, 0.0395471937397337, 0.035982966942680196, 0.03278945571409134, 0.029905625922285368, 0.02715711252818865, 0.02463575970659611, 0.022349668595297245, 0.020290036520089823, 0.01845206840620763, 0.016690408799481023, 0.01513798886867956, 0.013713677230260709, 0.012424946059730617, 0.011254888715588196, 0.010195795219829924, 0.009205062957950691, 0.008335771290237598, 0.007559731167065219, 0.006839863312762791, 0.006185519670752432, 0.0055889310749694405, 0.005052705068515947, 0.004579867937843157, 0.004135866216085435, 0.0037472660325684943, 0.003388942419849123, 0.003063942898334926, 0.0027621155671432927, 0.002506421419231597, 0.0022627771115807144, 0.0020454825443480223, 0.001845432076113499, 0.001664393617700836, 0.0015067368106884387, 0.001364379557929509, 0.0012318460578916125, 0.0011128141168008831, 0.001006400976279555, 0.0009099370728331236, 0.0008254639751055182, 0.0007475740507013556, 0.0006760408726035912, 0.0006113730770193972, 0.0005533401721434428, 0.0004992443735276406, 0.000451019600551324, 0.000410255241813778, 0.0003699349238827634, 0.00033307040876320556, 0.0003000956968222976, 0.00027325236167152756, 0.0002461415963989703, 0.00022176156128189055, 0.00020216898798982596, 0.0001831260507271626, 0.00016623379377244426, 0.00015084765871613986, 0.00013583507183973702, 0.0001241393327147662, 0.00011396341792625714, 0.00010725178536587103, 9.711464496117796e-05, 8.741469457605001e-05, 8.029003138204232e-05, 7.227240002273537e-05, 6.594296444783137e-05, 6.015071546650694e-05, 5.523815961984909e-05, 4.9969928103920935e-05, 4.5132648235978195e-05, 4.216400199091982e-05, 3.829983780819231e-05, 3.4652499587266616e-05, 3.146606458653738e-05, 2.833977806423473e-05, 2.550915275836202e-05, 2.3085613935780005e-05, 2.0787987532714642e-05, 1.923617421653608e-05, 1.7763380144587513e-05, 1.6243869359352556e-05, 1.463251473994463e-05, 1.3408747532526244e-05, 1.2796605131812893e-05, 1.1569726332952197e-05, 1.1108417811795821e-05, 1.0021742366514364e-05, 9.101589712277157e-06, 8.551069200647093e-06, 7.699763169947267e-06, 6.9867128459284754e-06, 6.3512201995033565e-06, 5.9692229817870755e-06, 5.381615407587522e-06, 4.90540774495776e-06, 4.8591645045386655e-06, 4.412084164463659e-06, 4.06195948796704e-06, 3.6763745602942894e-06, 3.4503319675551016e-06, 3.1071139193470375e-06, 3.5938218163653513e-06, 3.6582713636974306e-06, 3.51614431245572e-06, 3.8014304104780448e-06, 3.5969394129839546e-06, 3.2876620395950427e-06, 2.961208281611403e-06, 2.701097633154934e-06, 2.492055100990998e-06, 2.3835252135451864e-06, 2.3084910909111805e-06, 2.2121861042998774e-06, 2.1890794664206317e-06, 1.971150488548814e-06, 1.7849536713596612e-06, 1.6067719693893091e-06, 1.4863746608607962e-06, 1.3714891959430435e-06, 1.3826207384039049e-06, 1.2447446345075022e-06, 1.120755408850955e-06, 1.1577038464440735e-06, 1.0702755959550005e-06, 1.1346778275122348e-06, 1.0883881758794915e-06, 1.0611188116804769e-06, 9.672302060526496e-07, 9.091407566480698e-07, 8.424939489603716e-07, 7.884281131561729e-07, 7.096828445430879e-07, 7.939058816731129e-07, 7.148335242037743e-07, 6.433962696593885e-07, 6.172351094272356e-07, 6.843633532279409e-07, 6.671174681096362e-07, 6.162721678270711e-07, 7.190804637739547e-07, 6.673764107021865e-07, 6.413068026301141e-07, 7.457830191468518e-07, 7.820503233159611e-07, 7.383292026620596e-07, 7.053693410151426e-07, 6.365020535732896e-07, 5.934584318601877e-07, 5.619556275451762e-07, 5.068444080129305e-07, 4.806702381434001e-07, 4.6303056572655556e-07, 4.18948525190005e-07, 4.279131053896746e-07, 4.0195066531300467e-07], "accuracy_test": 0.878760762117347, "start": "2016-02-01 09:47:05.067000", "learning_rate_per_epoch": [0.006899558939039707, 0.0066808671690523624, 0.006469107232987881, 0.006264059338718653, 0.006065510679036379, 0.005873255431652069, 0.005687093827873468, 0.005506833083927631, 0.00533228600397706, 0.0051632714457809925, 0.004999613855034113, 0.004841143731027842, 0.004687696695327759, 0.004539113026112318, 0.004395239055156708, 0.004255925305187702, 0.004121027421206236, 0.003990405239164829, 0.0038639234844595194, 0.003741450607776642, 0.0036228597164154053, 0.0035080278757959604, 0.003396835643798113, 0.0032891680020838976, 0.003184912959113717, 0.003083962481468916, 0.00298621179535985, 0.002891559386625886, 0.0027999072335660458, 0.002711160108447075, 0.0026252258103340864, 0.0025420153979212046, 0.0024614424910396338, 0.0023834235034883022, 0.0023078774102032185, 0.002234725747257471, 0.0021638928446918726, 0.0020953051280230284, 0.0020288913510739803, 0.001964582595974207, 0.0019023122731596231, 0.0018420156557112932, 0.001783630228601396, 0.0017270954558625817, 0.0016723526641726494, 0.0016193449264392257, 0.0015680174110457301, 0.001518316799774766, 0.001470191520638764, 0.0014235916314646602, 0.0013784688198938966, 0.0013347761705517769, 0.0012924685142934322, 0.001251501846127212, 0.0012118336744606495, 0.0011734227882698178, 0.0011362293735146523, 0.001100214896723628, 0.0010653419885784388, 0.0010315744439139962, 0.0009988772217184305, 0.0009672163287177682, 0.000936558993998915, 0.0009068733779713511, 0.0008781286887824535, 0.0008502951241098344, 0.0008233437547460198, 0.0007972466410137713, 0.000771976716350764, 0.000747507787309587, 0.000723814417142421, 0.0007008720422163606, 0.0006786568555980921, 0.0006571458070538938, 0.0006363166030496359, 0.0006161475903354585, 0.0005966178723610938, 0.0005777071928605437, 0.0005593959358520806, 0.0005416650674305856, 0.0005244961939752102, 0.0005078715039417148, 0.000491773767862469, 0.0004761862801387906, 0.00046109285904094577, 0.0004464778467081487, 0.0004323260800447315, 0.00041862286161631346, 0.0004053539887536317, 0.0003925056953448802, 0.0003800646518357098, 0.0003680179361253977, 0.0003563530626706779, 0.00034505792427808046, 0.0003341207921039313, 0.00032353034475818276, 0.0003132755809929222, 0.0003033458488062024, 0.00029373084544204175, 0.000284420617390424, 0.0002754054730758071, 0.00026667609927244484, 0.00025822341558523476, 0.0002500386326573789, 0.0002421132812742144, 0.00023443913960363716, 0.0002270082477480173, 0.00021981287864036858, 0.00021284558170009404, 0.0002060991246253252, 0.0001995665079448372, 0.00019324095046613365, 0.00018711588927544653, 0.00018118497973773628, 0.00017544205184094608, 0.0001698811538517475, 0.00016449652321171016, 0.00015928255743347108, 0.00015423385775648057, 0.00014934518549125642, 0.0001446114620193839, 0.0001400277833454311, 0.00013558939099311829, 0.00013129168655723333, 0.00012713020259980112, 0.00012310061720199883, 0.00011919876124011353, 0.00011542058200575411, 0.00011176215775776654, 0.00010821969044627622, 0.00010478950571268797, 0.00010146804561372846, 9.825186862144619e-05, 9.513762779533863e-05, 9.21220998861827e-05, 8.920215623220429e-05, 8.63747627590783e-05, 8.36369872558862e-05, 8.098599209915847e-05, 7.841901970095932e-05, 7.59334143367596e-05, 7.352659304160625e-05, 7.119606016203761e-05, 6.893939280416816e-05, 6.675425538560376e-05, 6.463837780756876e-05, 6.25895700068213e-05, 6.060570012778044e-05, 5.868471271242015e-05, 5.682461414835416e-05, 5.502347266883589e-05, 5.327942199073732e-05, 5.15906504006125e-05, 4.9955408030655235e-05, 4.83719959447626e-05, 4.683877341449261e-05, 4.535415064310655e-05, 4.3916585127590224e-05, 4.252458529663272e-05, 4.117670687264763e-05, 3.987154923379421e-05, 3.8607762689935043e-05, 3.738403393072076e-05, 3.619909330154769e-05, 3.5051711165579036e-05, 3.394069426576607e-05, 3.286489300080575e-05, 3.18231905112043e-05, 3.081450631725602e-05, 2.983779450005386e-05, 2.8892041882500052e-05, 2.797626621031668e-05, 2.7089517971035093e-05, 2.6230874937027693e-05, 2.539944944146555e-05, 2.4594375645392574e-05, 2.3814820451661944e-05, 2.305997440998908e-05, 2.2329053535941057e-05, 2.1621301129925996e-05, 2.0935982320224866e-05], "accuracy_train_first": 0.5915129905523256, "accuracy_train_last": 0.9991396949404762, "batch_size_eval": 1024, "accuracy_train_std": [0.020002771655508577, 0.02175234982406709, 0.024400893707561995, 0.021686109072012395, 0.019096710097028204, 0.02098788733256821, 0.022953403466360076, 0.021098429901461813, 0.020863362612601178, 0.021192384016873962, 0.02141033176566811, 0.020341085157638442, 0.01883547897021477, 0.019187989718083064, 0.019682375325833567, 0.017936069171908615, 0.01682452224651998, 0.017803962907223435, 0.017666341782800892, 0.016319066714538977, 0.014968140320479032, 0.01494377781739131, 0.013815625589175717, 0.014340363144482261, 0.013267663607011754, 0.012816150996858, 0.013035961402494203, 0.011762708994848192, 0.012338066353448673, 0.010774326275246417, 0.009768913061162217, 0.010377734631133421, 0.010930706512296147, 0.009289883758384205, 0.00789459073361397, 0.009425079805591924, 0.007819939645958715, 0.007209238628472532, 0.007694829139422924, 0.00797801806112277, 0.006726541139159168, 0.007259169062013433, 0.00704151652541165, 0.005970806778261553, 0.005353349018452536, 0.005580174744613455, 0.006050258811275664, 0.005650063764372204, 0.005031624172745119, 0.0052598237531934645, 0.004318743738009991, 0.004407000469939967, 0.0049957703846509405, 0.004720342094858954, 0.003554399878111815, 0.003974161586958893, 0.0033905740575815076, 0.0032103898054488852, 0.0031846008462682435, 0.0030875489861979517, 0.0034471850452467904, 0.0031191678455606405, 0.003065671451608912, 0.002859434819090124, 0.0028191624403120126, 0.0026640970214662396, 0.002629783392038447, 0.0028336305313582818, 0.00281870953509891, 0.0029875221694402984, 0.0027948649411146747, 0.0021688339200202915, 0.0022848062679572635, 0.001949245899845116, 0.0019378056184044172, 0.0024612554079767508, 0.001872863705878187, 0.0022058298410772737, 0.001959343178737185, 0.002111793285836968, 0.0020651981673908723, 0.0015427181275007294, 0.0016637410546873435, 0.0018769006762100603, 0.0017475778854272278, 0.002016857183984509, 0.0016908164957866298, 0.001592028198981994, 0.0016775348695938592, 0.0014486253698430698, 0.001611749146728364, 0.0017306356088103048, 0.0013217140608701296, 0.0015683279204913411, 0.0014909988413208143, 0.0015571554734238487, 0.0015716698861719855, 0.001384603358775626, 0.0016632198843093725, 0.001444027860418921, 0.0013228300202845573, 0.0014491402409716908, 0.0016534734651749467, 0.0014060903676284275, 0.0013890696643641968, 0.001327780090705844, 0.0012658721715788875, 0.0011910573358320032, 0.0012392290051728967, 0.0013755765864721951, 0.00122263921364722, 0.001071356216545532, 0.0011487741189579264, 0.0013347503900464546, 0.0013964843853804354, 0.0012206477630586578, 0.0013331166575333217, 0.0013004214522051303, 0.0014496477017320473, 0.001444589338449504, 0.0013405445782551687, 0.0012045976420190308, 0.0012573015099140415, 0.001412228854079261, 0.001396057761558601, 0.0009856546872315218, 0.0011766723334151782, 0.0013308303591196228, 0.0013559824508764262, 0.0011958138830565283, 0.0010758682385843819, 0.001214653821299198, 0.0010735543554269303, 0.0011182491309906107, 0.0011662887094479588, 0.0011374232522912025, 0.0011662887094479586, 0.0009984612422611555, 0.001076872784310771, 0.0009645865193773968, 0.00108661843801599, 0.001105607647690545, 0.0010738563280092532, 0.0010338418003820549, 0.001117281787270644, 0.0010563445583705866, 0.0010272843207949462, 0.0011748330633054112, 0.0011466543904332174, 0.0010856229101496235, 0.0012019017841629544, 0.0010755356584283343, 0.0010201548999479307, 0.0010904805359909205, 0.0010822770981100116, 0.0010705789093397823, 0.0010705789093397823, 0.001112189429231901, 0.001105238514415066, 0.0011434871687742995, 0.0011900475254396795, 0.0010822770981100114, 0.0011259579625003025, 0.001076872784310771, 0.0010614501683885163, 0.0010667847606914492, 0.0010667847606914492, 0.0010705789093397823, 0.001049154963976793, 0.0010614501683885163, 0.0011438219750975487, 0.0010116401561480232, 0.0010614501683885163, 0.0011114600448955654, 0.00105018506237316, 0.0010614501683885163, 0.0010705789093397823, 0.001063739698074579, 0.0010398381591795896, 0.0010338418003820549, 0.0010698211560944328], "accuracy_test_std": 0.008577233073902554, "error_valid": [0.41231821818524095, 0.3342608716114458, 0.3259997999811747, 0.2732639542545181, 0.25067065135542166, 0.22873770472515065, 0.21759842102786142, 0.2043839420180723, 0.19433299604668675, 0.19026349538780118, 0.17619452419051207, 0.17867711078689763, 0.16861586972891573, 0.16439341349774095, 0.16906297063253017, 0.16127106080572284, 0.1599179922816265, 0.16812758847891573, 0.15855462867093373, 0.15368211125753017, 0.14861545792545183, 0.14837131730045183, 0.14569606551204817, 0.1521054922816265, 0.14522837443524095, 0.1441709219691265, 0.14935817488704817, 0.1468961784638554, 0.1391145637236446, 0.13321400837725905, 0.14098679875753017, 0.1411897590361446, 0.13860569230045183, 0.13443471150225905, 0.12808705525225905, 0.1390836784638554, 0.13050787132906627, 0.13137265860316272, 0.12945041886295183, 0.12835178605045183, 0.12672369164156627, 0.13326548381024095, 0.12552357868975905, 0.12170851374246983, 0.12443524096385539, 0.12431317065135539, 0.12502500235316272, 0.12321453783885539, 0.11851409544427716, 0.12313364787274095, 0.11769048851656627, 0.11918621752635539, 0.11891119164156627, 0.12297039721385539, 0.11428281485316272, 0.11905385212725905, 0.11699924698795183, 0.11892148672816272, 0.11904355704066272, 0.11478139118975905, 0.11405926440135539, 0.11612416462725905, 0.11525937735316272, 0.11401808405496983, 0.11382541886295183, 0.11144431240587349, 0.11191200348268071, 0.11240028473268071, 0.11207525414156627, 0.11134283226656627, 0.11291945124246983, 0.11191200348268071, 0.10888083584337349, 0.11232968985316272, 0.11365187311746983, 0.11280767601656627, 0.10860580995858427, 0.11240028473268071, 0.11217673428087349, 0.10827048428087349, 0.10948089231927716, 0.10875876553087349, 0.10862640013177716, 0.11106780638177716, 0.10729392178087349, 0.10642913450677716, 0.1037024072853916, 0.10763954254518071, 0.10958237245858427, 0.10566582737198793, 0.10912497646837349, 0.10628647402108427, 0.1061438135353916, 0.10533050169427716, 0.10679534544427716, 0.10964414297816272, 0.10430246376129515, 0.10605262848268071, 0.10640854433358427, 0.10593055817018071, 0.10703948606927716, 0.10766013271837349, 0.10629676910768071, 0.10701889589608427, 0.10491281532379515, 0.10485251553087349, 0.10517754612198793, 0.10631735928087349, 0.10829107445406627, 0.10397743317018071, 0.10730421686746983, 0.10383477268448793, 0.10581878294427716, 0.10533050169427716, 0.1041906885353916, 0.10619528896837349, 0.10680564053087349, 0.10692771084337349, 0.10449659967996983, 0.10632765436746983, 0.10520843138177716, 0.10373329254518071, 0.10507606598268071, 0.10667327513177716, 0.1052893213478916, 0.10446571442018071, 0.10545257200677716, 0.1026037744728916, 0.10311264589608427, 0.10348915192018071, 0.10224785862198793, 0.10324501129518071, 0.10375388271837349, 0.10458778473268071, 0.10507606598268071, 0.10533050169427716, 0.10583937311746983, 0.10336708160768071, 0.10335678652108427, 0.10594085325677716, 0.10470985504518071, 0.10496429075677716, 0.10470985504518071, 0.10398772825677716, 0.10397743317018071, 0.10324501129518071, 0.10446571442018071, 0.10433334902108427, 0.10311264589608427, 0.10483192535768071, 0.10570700771837349, 0.10360092714608427, 0.10533050169427716, 0.10484222044427716, 0.10385536285768071, 0.10496429075677716, 0.10507606598268071, 0.10458778473268071, 0.10324501129518071, 0.10448630459337349, 0.10445541933358427, 0.10508636106927716, 0.10569671263177716, 0.10386565794427716, 0.10496429075677716, 0.10323471620858427, 0.10397743317018071, 0.10507606598268071, 0.10310235080948793, 0.10322442112198793, 0.10484222044427716, 0.10361122223268071, 0.10435393919427716, 0.10470985504518071, 0.10372299745858427, 0.10433334902108427, 0.10371270237198793, 0.10360092714608427, 0.10396713808358427, 0.10335678652108427, 0.10360092714608427], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.031696479007213, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 32, "valid_ratio": 0.15, "learning_rate": 0.007125409327603191, "optimization": "nesterov_momentum", "nb_data_augmentation": 1, "learning_rate_decay_method": "exp", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 6.530702438910241e-05, "rotation_range": [0, 0], "momentum": 0.9092961117123517}, "accuracy_valid_max": 0.8977521413780121, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8963990728539157, "accuracy_valid_std": [0.018737111862919948, 0.0176398118912677, 0.02291490676086897, 0.01806580161845479, 0.013366336710500276, 0.01108736795445518, 0.011427310178654193, 0.008638795494462174, 0.008292302261032506, 0.01268108446429248, 0.011017718929343084, 0.006230221534758347, 0.009986167718103626, 0.012986220813555516, 0.012211470265432206, 0.010072961669211901, 0.011920384264775732, 0.008051778081158931, 0.008671614771622628, 0.008146916112587365, 0.011268282560115913, 0.012664125781211382, 0.009687861247287315, 0.009320642203431564, 0.007789169984848584, 0.006328030590752791, 0.012445156181661119, 0.011013682662764058, 0.004698910444785668, 0.008516822384932236, 0.006972606348057533, 0.007893567399467755, 0.0109864489129309, 0.010310457137240228, 0.005442906215222105, 0.00883994176822934, 0.009284785677715639, 0.008740561929000216, 0.00580039900836942, 0.007759960676844622, 0.0073381826577509565, 0.005285656560510936, 0.006776012960239217, 0.009988253390095807, 0.006429099186900516, 0.007079634968418985, 0.008268626563871928, 0.006483877904584278, 0.00847298633816741, 0.008017611703460494, 0.007677408963832579, 0.009009212815045802, 0.005711484363935024, 0.007265794568860725, 0.006853673168940397, 0.006251752824667087, 0.007303080661709185, 0.009677279365608762, 0.008781699383714548, 0.00482780222437703, 0.00809936834531895, 0.005388806492671067, 0.007106355034203972, 0.006597088808926558, 0.009473676825614915, 0.010308792104665555, 0.007606789215588598, 0.009757130849629954, 0.0075749087913334606, 0.007948738112917233, 0.007975157654388781, 0.007883830071369063, 0.006179597891319449, 0.006558764022586003, 0.00979906134361428, 0.011014240018880373, 0.008562056633493813, 0.01024581691032807, 0.009338432730822318, 0.006506740565673432, 0.007314274849001042, 0.009175286286939874, 0.006898188889666811, 0.008299593821693645, 0.007682153557049266, 0.007509660933242963, 0.00887104367768368, 0.008687561321831594, 0.01025135589279261, 0.008365697487988925, 0.0069557671660279725, 0.01018919767370523, 0.009331758550157844, 0.005255009042899291, 0.007184955372525325, 0.004702258593597638, 0.009567376016028743, 0.007726498429313044, 0.009717308944540717, 0.008713189964876486, 0.00908987889903584, 0.007498884910460823, 0.007932865504000023, 0.01020937004527044, 0.009980941445673519, 0.005256873119280793, 0.007181970736622667, 0.0073482789182873955, 0.008718526047433192, 0.006991063542659563, 0.0077892527384119175, 0.009642813351761193, 0.009243777598719527, 0.007870666771727687, 0.009434162858264776, 0.006867032387900198, 0.007720478268594256, 0.006958192933208821, 0.006517139979117943, 0.008119500261844666, 0.008951969813615863, 0.009104740266242976, 0.008966018577160704, 0.008656786428471909, 0.008659576108056899, 0.008215684465973396, 0.008320938225990294, 0.01077798747983688, 0.007704525366896443, 0.00889540760091358, 0.008516113899756506, 0.007744821437901923, 0.0076532019758616746, 0.008351886597587416, 0.008187676636036827, 0.00862236403458237, 0.008950782017686523, 0.008980903787208984, 0.008728469176325743, 0.008421564096767545, 0.00858193893970397, 0.00804314995112674, 0.008484146740309033, 0.007424548493291302, 0.007453233504482965, 0.007821403454655245, 0.008330955520061852, 0.009288248882828497, 0.008414499564800379, 0.00798042995585927, 0.006849231359164785, 0.00810639931707398, 0.008649971023249887, 0.008188870255459857, 0.007451800142039561, 0.006502377619053143, 0.007614337967968323, 0.007135750975891526, 0.007135989987319617, 0.008512813885843079, 0.008768004972579324, 0.007569037833723946, 0.0072290102096008646, 0.007293676770316266, 0.007329734408396952, 0.007686713580562014, 0.007548589348146897, 0.007472106495395386, 0.007638338500953463, 0.007589186114878635, 0.006787986527669136, 0.0067012515456723295, 0.00659981569422545, 0.007174729828248725, 0.007595456576632008, 0.007840665474486353, 0.007945145273286209, 0.007136648932346372, 0.007740431878806862, 0.007218441498884005, 0.008150396654467939], "accuracy_valid": [0.587681781814759, 0.6657391283885542, 0.6740002000188253, 0.7267360457454819, 0.7493293486445783, 0.7712622952748494, 0.7824015789721386, 0.7956160579819277, 0.8056670039533133, 0.8097365046121988, 0.8238054758094879, 0.8213228892131024, 0.8313841302710843, 0.835606586502259, 0.8309370293674698, 0.8387289391942772, 0.8400820077183735, 0.8318724115210843, 0.8414453713290663, 0.8463178887424698, 0.8513845420745482, 0.8516286826995482, 0.8543039344879518, 0.8478945077183735, 0.854771625564759, 0.8558290780308735, 0.8506418251129518, 0.8531038215361446, 0.8608854362763554, 0.866785991622741, 0.8590132012424698, 0.8588102409638554, 0.8613943076995482, 0.865565288497741, 0.871912944747741, 0.8609163215361446, 0.8694921286709337, 0.8686273413968373, 0.8705495811370482, 0.8716482139495482, 0.8732763083584337, 0.866734516189759, 0.874476421310241, 0.8782914862575302, 0.8755647590361446, 0.8756868293486446, 0.8749749976468373, 0.8767854621611446, 0.8814859045557228, 0.876866352127259, 0.8823095114834337, 0.8808137824736446, 0.8810888083584337, 0.8770296027861446, 0.8857171851468373, 0.880946147872741, 0.8830007530120482, 0.8810785132718373, 0.8809564429593373, 0.885218608810241, 0.8859407355986446, 0.883875835372741, 0.8847406226468373, 0.8859819159450302, 0.8861745811370482, 0.8885556875941265, 0.8880879965173193, 0.8875997152673193, 0.8879247458584337, 0.8886571677334337, 0.8870805487575302, 0.8880879965173193, 0.8911191641566265, 0.8876703101468373, 0.8863481268825302, 0.8871923239834337, 0.8913941900414157, 0.8875997152673193, 0.8878232657191265, 0.8917295157191265, 0.8905191076807228, 0.8912412344691265, 0.8913735998682228, 0.8889321936182228, 0.8927060782191265, 0.8935708654932228, 0.8962975927146084, 0.8923604574548193, 0.8904176275414157, 0.8943341726280121, 0.8908750235316265, 0.8937135259789157, 0.8938561864646084, 0.8946694983057228, 0.8932046545557228, 0.8903558570218373, 0.8956975362387049, 0.8939473715173193, 0.8935914556664157, 0.8940694418298193, 0.8929605139307228, 0.8923398672816265, 0.8937032308923193, 0.8929811041039157, 0.8950871846762049, 0.8951474844691265, 0.8948224538780121, 0.8936826407191265, 0.8917089255459337, 0.8960225668298193, 0.8926957831325302, 0.8961652273155121, 0.8941812170557228, 0.8946694983057228, 0.8958093114646084, 0.8938047110316265, 0.8931943594691265, 0.8930722891566265, 0.8955034003200302, 0.8936723456325302, 0.8947915686182228, 0.8962667074548193, 0.8949239340173193, 0.8933267248682228, 0.8947106786521084, 0.8955342855798193, 0.8945474279932228, 0.8973962255271084, 0.8968873541039157, 0.8965108480798193, 0.8977521413780121, 0.8967549887048193, 0.8962461172816265, 0.8954122152673193, 0.8949239340173193, 0.8946694983057228, 0.8941606268825302, 0.8966329183923193, 0.8966432134789157, 0.8940591467432228, 0.8952901449548193, 0.8950357092432228, 0.8952901449548193, 0.8960122717432228, 0.8960225668298193, 0.8967549887048193, 0.8955342855798193, 0.8956666509789157, 0.8968873541039157, 0.8951680746423193, 0.8942929922816265, 0.8963990728539157, 0.8946694983057228, 0.8951577795557228, 0.8961446371423193, 0.8950357092432228, 0.8949239340173193, 0.8954122152673193, 0.8967549887048193, 0.8955136954066265, 0.8955445806664157, 0.8949136389307228, 0.8943032873682228, 0.8961343420557228, 0.8950357092432228, 0.8967652837914157, 0.8960225668298193, 0.8949239340173193, 0.8968976491905121, 0.8967755788780121, 0.8951577795557228, 0.8963887777673193, 0.8956460608057228, 0.8952901449548193, 0.8962770025414157, 0.8956666509789157, 0.8962872976280121, 0.8963990728539157, 0.8960328619164157, 0.8966432134789157, 0.8963990728539157], "seed": 495530054, "model": "residualv3", "loss_std": [0.2765917181968689, 0.19740067422389984, 0.18569086492061615, 0.18151631951332092, 0.17418892681598663, 0.16758641600608826, 0.16237638890743256, 0.15679073333740234, 0.15269267559051514, 0.14708559215068817, 0.14428256452083588, 0.13660494983196259, 0.13090378046035767, 0.12853766977787018, 0.12330321222543716, 0.11761470139026642, 0.1146978810429573, 0.10951994359493256, 0.1052694171667099, 0.1016845554113388, 0.1000448688864708, 0.09755637496709824, 0.0937623605132103, 0.09013781696557999, 0.08721138536930084, 0.08583764731884003, 0.08369691669940948, 0.07866989076137543, 0.07880572974681854, 0.07851974666118622, 0.07357707619667053, 0.07296323776245117, 0.07227607816457748, 0.0687146931886673, 0.06618732213973999, 0.06732873618602753, 0.06829454004764557, 0.06270178407430649, 0.06274596601724625, 0.06187017634510994, 0.06202365830540657, 0.057799406349658966, 0.05868811532855034, 0.05772603675723076, 0.05452941358089447, 0.05366411432623863, 0.05497968941926956, 0.05320286750793457, 0.0534343346953392, 0.05139274522662163, 0.04983493313193321, 0.05029934644699097, 0.049737609922885895, 0.04956139624118805, 0.04609738290309906, 0.04627812281250954, 0.04681522399187088, 0.04326052591204643, 0.0431053601205349, 0.04233550280332565, 0.0416046679019928, 0.04090859740972519, 0.03831884637475014, 0.041859906166791916, 0.03884164243936539, 0.04012226685881615, 0.03691726550459862, 0.03699015453457832, 0.037076275795698166, 0.0344952754676342, 0.03554016724228859, 0.03392845019698143, 0.035130612552165985, 0.033271949738264084, 0.0328863300383091, 0.03391329199075699, 0.031101854518055916, 0.03124185837805271, 0.03165386617183685, 0.031376857310533524, 0.031812187284231186, 0.03102082572877407, 0.02932186983525753, 0.0306185781955719, 0.029481716454029083, 0.028647396713495255, 0.028096897527575493, 0.027260538190603256, 0.02664376050233841, 0.027048384770751, 0.0263009425252676, 0.026079030707478523, 0.02675566077232361, 0.025801029056310654, 0.027096832171082497, 0.025590943172574043, 0.025791974738240242, 0.025798406451940536, 0.025826822966337204, 0.024733586236834526, 0.024648621678352356, 0.025489894673228264, 0.026570823043584824, 0.024192549288272858, 0.022605255246162415, 0.023079069331288338, 0.023715956136584282, 0.022981075569987297, 0.022174766287207603, 0.023421209305524826, 0.022254476323723793, 0.0229977834969759, 0.021604910492897034, 0.021532168611884117, 0.02200389839708805, 0.021512696519494057, 0.022315049543976784, 0.021616173908114433, 0.021555613726377487, 0.02134537324309349, 0.021523067727684975, 0.02068488486111164, 0.02114834263920784, 0.022649185732007027, 0.020167922601103783, 0.020060665905475616, 0.02046831324696541, 0.019482720643281937, 0.01897716522216797, 0.01972256414592266, 0.01980563998222351, 0.01993573270738125, 0.01942404918372631, 0.02020566165447235, 0.018393732607364655, 0.019216250628232956, 0.01860230229794979, 0.019812967628240585, 0.01964399591088295, 0.019892707467079163, 0.02049252763390541, 0.02117159776389599, 0.018290335312485695, 0.019397573545575142, 0.01875842548906803, 0.01794493943452835, 0.017454814165830612, 0.01881708763539791, 0.01754763163626194, 0.018241971731185913, 0.018467392772436142, 0.019035518169403076, 0.017271824181079865, 0.017957987263798714, 0.017272746190428734, 0.017827559262514114, 0.019078420475125313, 0.018712002784013748, 0.01793491467833519, 0.01705170050263405, 0.018105575814843178, 0.018117817118763924, 0.01754624955356121, 0.018455304205417633, 0.018476776778697968, 0.018023250624537468, 0.01841973327100277, 0.017094049602746964, 0.017798462882637978, 0.017105236649513245, 0.017111394554376602, 0.018088527023792267, 0.018127337098121643, 0.01809966005384922, 0.01833631470799446, 0.016944706439971924, 0.01628372073173523, 0.018856152892112732, 0.018139919266104698, 0.0179135762155056, 0.018001416698098183]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:34 2016", "state": "available"}], "summary": "c7beb62a3af7b79ecbe0c7f82c8dc37d"}