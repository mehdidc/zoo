{"content": {"hp_model": {"f0": 64, "f1": 16, "f2": 32, "f3": 16, "nonlin": "rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.6238617897033691, 1.1583166122436523, 0.9913771748542786, 0.8731563687324524, 0.7871657609939575, 0.7239758968353271, 0.6732126474380493, 0.6296001076698303, 0.5921421647071838, 0.5574209094047546, 0.5259082913398743, 0.49711787700653076, 0.4694247841835022, 0.4465787708759308, 0.4250746965408325, 0.40413016080856323, 0.38547784090042114, 0.367684006690979, 0.3519015312194824, 0.3356035649776459, 0.3229120969772339, 0.3111441433429718, 0.29784300923347473, 0.28876808285713196, 0.27568894624710083, 0.26883864402770996, 0.26192352175712585, 0.2512977123260498, 0.24156281352043152, 0.23209378123283386, 0.23118117451667786, 0.22168906033039093, 0.2191264033317566, 0.21402154862880707, 0.21029222011566162, 0.20543354749679565, 0.2005811631679535, 0.19549302756786346, 0.19528406858444214, 0.18809951841831207, 0.18808695673942566, 0.1819624900817871, 0.182021364569664, 0.17603637278079987, 0.1744079887866974, 0.17138217389583588, 0.17058473825454712, 0.16728097200393677, 0.16300876438617706, 0.15768864750862122, 0.15756475925445557, 0.15661278367042542, 0.15508760511875153, 0.15111322700977325, 0.15004044771194458, 0.14788755774497986, 0.14504672586917877, 0.1423465460538864, 0.1404360830783844, 0.14056558907032013, 0.1398538202047348, 0.13487699627876282, 0.13465413451194763, 0.13174240291118622, 0.1322367787361145, 0.13113534450531006, 0.1271669864654541, 0.12719005346298218, 0.1240810975432396, 0.12327282875776291, 0.12209883332252502, 0.11983342468738556, 0.12014288455247879, 0.11815392225980759, 0.1168508529663086, 0.11495434492826462, 0.11419056355953217, 0.1108061820268631, 0.10933811217546463, 0.10810081660747528, 0.10623618215322495, 0.10317632555961609, 0.10095011442899704, 0.0993746742606163, 0.09762414544820786, 0.0961570292711258, 0.09420996904373169, 0.09254136681556702, 0.09136633574962616, 0.0902726948261261, 0.08921745419502258, 0.08819428831338882, 0.08720121532678604, 0.08623690158128738, 0.08530011028051376, 0.084389828145504, 0.08350508660078049, 0.08264501392841339, 0.08180870115756989, 0.08099536597728729, 0.08020422607660294, 0.07943450659513474, 0.07868549972772598, 0.07795652747154236, 0.07724693417549133, 0.07655608654022217, 0.07588335871696472, 0.07522816956043243, 0.07458997517824173, 0.07396822422742844, 0.07336241751909256, 0.07277204841375351, 0.07219661772251129, 0.07163570821285248, 0.07108883559703827, 0.07055558264255524, 0.07003554701805115, 0.06952834874391556, 0.06903359293937683, 0.06855090707540512, 0.06807994097471237, 0.06762035191059113, 0.06717180460691452, 0.06673399358987808, 0.06630660593509674, 0.06588933616876602, 0.06548190861940384, 0.06508402526378632, 0.06469544023275375, 0.06431588530540466, 0.06394512951374054, 0.06358292698860168, 0.0632290244102478, 0.06288320571184158, 0.06254525482654572, 0.06221495196223259, 0.06189211085438728, 0.06157652288675308, 0.061267998069524765, 0.06096634641289711, 0.06067138910293579, 0.060382962226867676, 0.06010089069604874, 0.059825003147125244, 0.05955514684319496, 0.059291187673807144, 0.05903294309973717, 0.058780282735824585, 0.058533065021038055, 0.05829117074608803, 0.058054450899362564, 0.05782278627157211, 0.05759605020284653, 0.05737411603331566, 0.05715687572956085, 0.05694420635700226, 0.05673600733280182, 0.05653215944766998, 0.056332577019929886, 0.05613714084029198, 0.055945757776498795, 0.05575834959745407, 0.05557479336857796, 0.05539500713348389, 0.05521891638636589, 0.0550464428961277, 0.054877471178770065, 0.05471194535493851, 0.054549768567085266, 0.054390888661146164, 0.05423520877957344, 0.05408266931772232, 0.053933195769786835, 0.053786713629961014, 0.05364316701889038, 0.05350248143076897, 0.0533645898103714, 0.05322945490479469], "moving_avg_accuracy_train": [0.05196247966846621, 0.1053658576590762, 0.1607843348776647, 0.2165124266586782, 0.2696524793566439, 0.31807816283593887, 0.36416016405880497, 0.4054663381521862, 0.4448921702492046, 0.4808520385936548, 0.5118561406359134, 0.5420892710929016, 0.5697988513029636, 0.5948746933206, 0.6171338144912624, 0.6364791039366082, 0.6568072412658156, 0.6755072489014157, 0.6906050920079979, 0.7072828770347729, 0.7198328761183942, 0.7336925865281957, 0.7448854572935194, 0.7581299311285824, 0.7715008864861006, 0.7813724660614662, 0.7917240926269234, 0.8009291296370716, 0.8106294262294738, 0.820273260351866, 0.8292456437632002, 0.8364443879763043, 0.8439789474252593, 0.851387732961434, 0.8588459742463168, 0.8660607316920248, 0.8714171237717611, 0.8766355491875898, 0.8814388726141611, 0.8866523235944855, 0.8920279871779587, 0.8968962392399709, 0.9015846217874577, 0.9057462176040515, 0.9095402296222548, 0.9125318435993981, 0.9158683623990651, 0.9190687227723, 0.923297669466554, 0.9258807292663918, 0.929284316085046, 0.931724620633795, 0.9343695763502697, 0.9356990692331922, 0.9369701257361834, 0.9402621536006881, 0.9420532478740172, 0.9446580712616801, 0.946662976633205, 0.9487555296758645, 0.9503553034547713, 0.9525251244843403, 0.9541851028074088, 0.9558253874290765, 0.9573505798600337, 0.9592116063955327, 0.960491218931044, 0.9618357133689197, 0.9632621053487498, 0.964701643100835, 0.9661832029336548, 0.9676329363213063, 0.9686610036618593, 0.9698955451088423, 0.971036859345651, 0.9721919253433026, 0.9730244743995039, 0.9739759483500573, 0.9749765422782022, 0.9759143512921225, 0.9772209037736522, 0.9782480275320382, 0.9794444452764811, 0.9805445087833937, 0.9816973263562818, 0.9826162795825953, 0.983764171973173, 0.9849554212925593, 0.9860484720192927, 0.9870345428221622, 0.9879313071399829, 0.988736069877212, 0.9894603563407182, 0.9901168644554927, 0.990717022354028, 0.9912618147603288, 0.9917521279259994, 0.9921934097751031, 0.9925975388857249, 0.992958929936475, 0.9932818567333407, 0.9935701657017102, 0.9938273186244332, 0.9940657317013125, 0.9942756531728848, 0.9944622573484904, 0.9946278759577258, 0.9947838721036476, 0.9949219434861677, 0.9950438825816262, 0.9951536277675389, 0.9952523984348604, 0.9953412920354496, 0.995425946573599, 0.9955021356579333, 0.9955683806850248, 0.9956326515070263, 0.9956928203956371, 0.9957469723953868, 0.9958026485927712, 0.9958527571704173, 0.9959001800391084, 0.9959451857697398, 0.995985690927308, 0.9960174952715004, 0.9960507694788926, 0.9960807162655456, 0.9961076683735334, 0.9961272749731032, 0.9961472460615256, 0.9961628948922963, 0.9961769788399898, 0.9961873292441045, 0.9961966446078077, 0.9962050284351406, 0.9962102487309307, 0.9962149469971417, 0.9962168502879222, 0.9962185632496245, 0.9962177797663472, 0.996219399780207, 0.9962185326438714, 0.9962177522211694, 0.996219374989547, 0.996220835481087, 0.9962244750722825, 0.9962300758531679, 0.9962327914071554, 0.996235235405744, 0.9962397601532833, 0.9962415072772591, 0.9962454048376469, 0.9962489126419959, 0.99625206966591, 0.9962549109874327, 0.996252817879184, 0.9962532592305697, 0.996253656446817, 0.9962563390902489, 0.9962541031717186, 0.9962520908450413, 0.9962549300486508, 0.9962621356295185, 0.9962662955034898, 0.996270039390064, 0.9962734088879808, 0.9962764414361058, 0.9962791707294184, 0.9962816270933997, 0.9962861629697926, 0.996290245258546, 0.9962939193184243, 0.9962995511211241, 0.996304619743554, 0.9963091815037408, 0.996313287087909, 0.9963169821136604, 0.9963203076368367], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.051954154508659624, 0.10523674286991713, 0.16056661950536516, 0.21478054184511478, 0.2659269699648201, 0.31174726477933207, 0.3544942015167603, 0.39263685473669574, 0.4284006718515051, 0.46099902828269496, 0.4875705239389887, 0.5137605253854512, 0.5370792970656562, 0.5577904303297231, 0.5762483743072929, 0.5915949635990787, 0.6071048478867612, 0.6217177758898772, 0.6330240903566123, 0.6455894100483608, 0.655263485092094, 0.6655458891713635, 0.6735822911710193, 0.682103997812577, 0.6910879222030061, 0.6973606364774343, 0.7031149130970101, 0.7086173219190862, 0.7147891634752951, 0.7199246637787445, 0.7245038159061411, 0.728185599695798, 0.7323782584291549, 0.736341816245878, 0.7403933285689559, 0.7441230798611567, 0.7463731342150259, 0.7487988090747734, 0.7509473543721152, 0.7535037508061387, 0.7559681994736424, 0.7583876928263534, 0.7605244977154049, 0.7616571065376144, 0.763090611104109, 0.764579166239933, 0.7655260347721897, 0.7671524069632689, 0.7701829070425444, 0.7711310720047057, 0.7720291305610122, 0.7731547660741881, 0.7735726349581097, 0.773706782418624, 0.7741651939734484, 0.7750426610689499, 0.775525146656332, 0.7768779995547048, 0.7778259829670807, 0.7787259371458998, 0.7792205721116562, 0.7804968512144965, 0.7809242580546433, 0.7815551238530947, 0.7820374538529509, 0.7832273572816618, 0.7837530719958902, 0.7835751245478072, 0.7834729184748036, 0.78361389611151, 0.7838841718335667, 0.7838629833305565, 0.7843566089903472, 0.7850673677543396, 0.7858444165116616, 0.7861419578706611, 0.7869520020120588, 0.7876413321195879, 0.7875272483240447, 0.7876697430417156, 0.7883951033102097, 0.7889401232879236, 0.7896015397053662, 0.7901846074498146, 0.7904174291784777, 0.7904966623524974, 0.7908650589937537, 0.7917246068406132, 0.7923883366215368, 0.7929989299642777, 0.7935362569414945, 0.7940930934084898, 0.7945942462287854, 0.7950320472271418, 0.7954860737732529, 0.7958580765710028, 0.7961562579952279, 0.7964124142457805, 0.7966673689337777, 0.7968724140904752, 0.797056954731503, 0.7973227565757472, 0.7975863922980672, 0.797799250385655, 0.7979908226644841, 0.7981377941442707, 0.7982700684760786, 0.7983647013122057, 0.7984986989897201, 0.7986691545331427, 0.7987727068885634, 0.798890318070942, 0.7989839611038327, 0.7990438257709344, 0.7990854969400758, 0.7991474150548032, 0.7991909343268079, 0.7992423087028621, 0.7993007526725607, 0.7993289381827896, 0.7993665121732456, 0.799412535795906, 0.7994783711188003, 0.7995498299406552, 0.7996385569428246, 0.799718411244777, 0.7998269012102843, 0.7999367492104908, 0.7999735477457671, 0.8000066664275157, 0.8000364732410895, 0.8000877134358059, 0.8000972085173006, 0.8001057540906458, 0.8001134451066565, 0.8001203670210661, 0.8001265967440349, 0.8001199964634567, 0.8001140562109363, 0.800096502952418, 0.8000929120510015, 0.8001018872709766, 0.8001221720002043, 0.8001526352877593, 0.8001678452153087, 0.8001693271188531, 0.8001706608320431, 0.8001596541426642, 0.8001253340597232, 0.8000822389538262, 0.8000556603897688, 0.8000317396821173, 0.800010211045231, 0.8000030423032831, 0.7999965904355301, 0.7999785767233023, 0.7999501573510474, 0.7999622305184275, 0.7999608893378197, 0.7999474752440227, 0.7998977519571957, 0.7998530009990514, 0.7998127251367216, 0.7997764768606248, 0.7997815040145473, 0.7997982354843275, 0.7998010867758797, 0.7997914459070267, 0.799782769125059, 0.799762752990038, 0.7997691525310191, 0.7997871191491521, 0.7998032891054718, 0.7998300490974095, 0.7998541330901534, 0.7998758086836231, 0.7999075237489957, 0.7999482743390811], "moving_var_accuracy_train": [0.024300893639661883, 0.04753809130296741, 0.07042515072771557, 0.09133321757692162, 0.10761468262591259, 0.11795863574725313, 0.12527472970286588, 0.12810305689667376, 0.12928231733588694, 0.12799209478444995, 0.12384417439702608, 0.11968613655238688, 0.11462791041590893, 0.1088243000503592, 0.10240108632293538, 0.09552913970415974, 0.0896953242392198, 0.08387300438544132, 0.07753720774513596, 0.07228682359121624, 0.06647566352508467, 0.061556921326368225, 0.056528752397454536, 0.05245462194221809, 0.04881820177255101, 0.04481341434511088, 0.04129647846355593, 0.03792942497442412, 0.03498334426280682, 0.032322041665748084, 0.02981437047589315, 0.02729933069251514, 0.025080323898072163, 0.02306630243635423, 0.021260300460290583, 0.019602744939265105, 0.017900688870345346, 0.016355707657695912, 0.014927784135388568, 0.01367962636196792, 0.012571743556435247, 0.011527868104045295, 0.01057290967184498, 0.009671488622326705, 0.008833890504842467, 0.008031049242052371, 0.00732813553715192, 0.006687502742103877, 0.006179708379179061, 0.005621787322626997, 0.0051638682194533525, 0.004701077174123823, 0.00429393157339045, 0.0038804463779830776, 0.0035069420018889364, 0.003253784828846114, 0.0029572785142250742, 0.0027226166067308064, 0.002486531755996649, 0.002277287584524079, 0.0020725923113647684, 0.0019077061899315311, 0.0017417353232358902, 0.001591776593673019, 0.0014535348418687607, 0.0013393521355743688, 0.0012201535961862717, 0.001114407224208952, 0.0010212778485091689, 0.0009378004841153576, 0.0008637756115478472, 0.0007963135924505061, 0.0007261945353158613, 0.0006672919150431485, 0.0006122861072231125, 0.0005630650936311792, 0.0005129968256468966, 0.0004698448671894439, 0.00043187107435186333, 0.0003965993386359883, 0.00037230311925531297, 0.00034456765626515024, 0.000322993629411596, 0.0003015855239436044, 0.0002833878667564788, 0.0002626493553701988, 0.00024824333229629415, 0.00023619067353510939, 0.00022332444520250955, 0.00020974302133670492, 0.0001960063953784822, 0.00018223454340972635, 0.00016873240699971854, 0.00015573819244263108, 0.00014340607872693627, 0.00013173665974790968, 0.00012072665677698905, 0.00011040655813242515, 0.00010083578536165075, 9.192763824954633e-05, 8.367340986979661e-05, 7.605416743399764e-05, 6.904389932158288e-05, 6.265107654646817e-05, 5.678257210986523e-05, 5.141770496405964e-05, 4.6522800181179576e-05, 4.2089533340943624e-05, 3.805215336688828e-05, 3.438076031721066e-05, 3.105108033796868e-05, 2.803377310667986e-05, 2.5301514446043393e-05, 2.2835860518902633e-05, 2.060451745615788e-05, 1.8583561343071267e-05, 1.676238185581076e-05, 1.5118726326639588e-05, 1.3633245645667652e-05, 1.229781963169772e-05, 1.1090635494511388e-05, 1.0001812301334215e-05, 9.019860713307816e-06, 8.132640652083727e-06, 7.328480233660946e-06, 6.605596766193083e-06, 5.953108379851344e-06, 5.3643352869910395e-06, 4.831361527012183e-06, 4.351814973665954e-06, 3.918837449439759e-06, 3.528738922739508e-06, 3.1768292082536007e-06, 2.8599272714365564e-06, 2.5745671413396374e-06, 2.317355690598901e-06, 2.0858187848875257e-06, 1.8772695090409285e-06, 1.6895689662769816e-06, 1.5206175942636962e-06, 1.368579454841484e-06, 1.231728276686156e-06, 1.108560930553885e-06, 9.977285378933657e-07, 8.979748814238733e-07, 8.082966128981178e-07, 7.277492703270477e-07, 6.550407113954685e-07, 5.895903984178332e-07, 5.308156186386998e-07, 4.777615287545127e-07, 4.3012209467184966e-07, 3.872206274268233e-07, 3.485882658840896e-07, 3.138020972676391e-07, 2.824613174601391e-07, 2.542169388335368e-07, 2.2879666497690653e-07, 2.059817676612656e-07, 1.85428584880204e-07, 1.669221715200859e-07, 1.503025040623069e-07, 1.357395372168404e-07, 1.2232132445827636e-07, 1.102153421925764e-07, 9.929598961922242e-08, 8.94491577904825e-08, 8.057128338931234e-08, 7.256845856646264e-08, 6.549678028167098e-08, 5.909708798670599e-08, 5.330886763192885e-08, 4.826343568358582e-08, 4.3668310515255466e-08, 3.948876636775294e-08, 3.56915921232365e-08, 3.224531184864252e-08, 2.9120312603339152e-08], "duration": 80184.452459, "accuracy_train": [0.5196247966846622, 0.5859962595745662, 0.6595506298449612, 0.7180652526878, 0.7479129536383352, 0.753909314149594, 0.7788981750645996, 0.7772219049926172, 0.7997246591223699, 0.8044908536937062, 0.7908930590162422, 0.8141874452057956, 0.8191850731935216, 0.8205572714793282, 0.8174659050272242, 0.8105867089447213, 0.8397604772286821, 0.8438073176218162, 0.8264856799672389, 0.8573829422757475, 0.8327828678709857, 0.8584299802164084, 0.8456212941814323, 0.8773301956441492, 0.8918394847037652, 0.8702166822397563, 0.8848887317160392, 0.8837744627284054, 0.8979320955610927, 0.907067767453396, 0.9099970944652085, 0.9012330858942414, 0.9117899824658545, 0.9180668027870063, 0.9259701458102622, 0.930993548703396, 0.9196246524893872, 0.923601377930048, 0.9246687834533037, 0.9335733824174051, 0.9404089594292175, 0.9407105077980805, 0.9437800647148394, 0.943200579953396, 0.9436863377860835, 0.9394563693936876, 0.9458970315960686, 0.9478719661314139, 0.9613581897148394, 0.9491282674649317, 0.9599165974529347, 0.953687361572536, 0.9581741777985419, 0.9476645051794942, 0.9484096342631044, 0.9698904043812293, 0.9581730963339794, 0.968101481750646, 0.9647071249769288, 0.9675885070598007, 0.9647532674649317, 0.9720535137504615, 0.9691249077150241, 0.9705879490240864, 0.971077311738649, 0.9759608452150241, 0.972007731750646, 0.9739361633098007, 0.9760996331672205, 0.9776574828696014, 0.979517241429033, 0.9806805368101699, 0.9779136097268365, 0.9810064181316908, 0.9813086874769288, 0.9825875193221669, 0.9805174159053157, 0.9825392139050388, 0.9839818876315062, 0.9843546324174051, 0.9889798761074198, 0.9874921413575121, 0.9902122049764673, 0.9904450803456073, 0.9920726845122739, 0.9908868586194168, 0.9940952034883721, 0.9956766651670359, 0.995885928559893, 0.9959091800479882, 0.9960021860003692, 0.9959789345122739, 0.9959789345122739, 0.9960254374884644, 0.9961184434408453, 0.9961649464170359, 0.9961649464170359, 0.9961649464170359, 0.9962347008813216, 0.9962114493932264, 0.9961881979051311, 0.9961649464170359, 0.9961416949289406, 0.9962114493932264, 0.9961649464170359, 0.9961416949289406, 0.9961184434408453, 0.9961878374169435, 0.9961645859288483, 0.996141334440753, 0.996141334440753, 0.996141334440753, 0.996141334440753, 0.9961878374169435, 0.9961878374169435, 0.9961645859288483, 0.9962110889050388, 0.9962343403931341, 0.9962343403931341, 0.9963037343692323, 0.9963037343692323, 0.9963269858573275, 0.9963502373454227, 0.9963502373454227, 0.9963037343692323, 0.9963502373454227, 0.9963502373454227, 0.9963502373454227, 0.9963037343692323, 0.9963269858573275, 0.9963037343692323, 0.9963037343692323, 0.996280482881137, 0.996280482881137, 0.996280482881137, 0.9962572313930418, 0.9962572313930418, 0.9962339799049464, 0.9962339799049464, 0.9962107284168512, 0.9962339799049464, 0.9962107284168512, 0.9962107284168512, 0.9962339799049464, 0.9962339799049464, 0.9962572313930418, 0.996280482881137, 0.9962572313930418, 0.9962572313930418, 0.996280482881137, 0.9962572313930418, 0.996280482881137, 0.996280482881137, 0.996280482881137, 0.996280482881137, 0.9962339799049464, 0.9962572313930418, 0.9962572313930418, 0.996280482881137, 0.9962339799049464, 0.9962339799049464, 0.996280482881137, 0.9963269858573275, 0.9963037343692323, 0.9963037343692323, 0.9963037343692323, 0.9963037343692323, 0.9963037343692323, 0.9963037343692323, 0.9963269858573275, 0.9963269858573275, 0.9963269858573275, 0.9963502373454227, 0.9963502373454227, 0.9963502373454227, 0.9963502373454227, 0.9963502373454227, 0.9963502373454227], "end": "2016-02-04 12:21:14.395000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0], "moving_var_accuracy_valid": [0.0242931075363871, 0.047415104785025336, 0.0702261515429679, 0.08965588076781472, 0.10423390667567096, 0.112706010760103, 0.11788111508799592, 0.11918676153110258, 0.1187795409095861, 0.11646546239676156, 0.11117331558979736, 0.10622922961270903, 0.10050019266549985, 0.09431073276868732, 0.08794592075473055, 0.08127098890527429, 0.07530889861030256, 0.06969984773269057, 0.06388035768080785, 0.058913307243329625, 0.053864266070562695, 0.04942938996635084, 0.0450677047836164, 0.04121450966201767, 0.037819456772892425, 0.034391633594920736, 0.03125047553016004, 0.028397916502751384, 0.025900949506230765, 0.023548214825908253, 0.021382111051169998, 0.019365899732917025, 0.01758751524491487, 0.01597015183552194, 0.014520869420906213, 0.013193981881130653, 0.011920148394375883, 0.01078108864166519, 0.009744525999551243, 0.008828889864147106, 0.008000662442845238, 0.007253281731315032, 0.006569046974388395, 0.005923687501646877, 0.005349813169561637, 0.004834774020136966, 0.004359365658279672, 0.003947234870986947, 0.0036351667604626525, 0.003279741235575619, 0.002959025694553056, 0.002674526622874455, 0.002408645490224358, 0.002167942901072384, 0.001953039881347514, 0.001764665429745954, 0.001590294017849642, 0.0014477365147463996, 0.0013110509162230183, 0.0011872350823164811, 0.0010707135478289726, 0.0009783021881811995, 0.0008821160588261174, 0.0007974863778444065, 0.0007198315201188175, 0.000660591199633858, 0.0005970194633172785, 0.0005376025046340639, 0.0004839362689028869, 0.00043572151425906006, 0.00039280680352655415, 0.00035353016374783713, 0.00032037014400108796, 0.00029287973178630713, 0.00026902600154897835, 0.000242920179136918, 0.00022453370482234154, 0.0002063569183144215, 0.00018583836249462909, 0.00016743726894624298, 0.00015542886972360733, 0.00014255940373621262, 0.00013224070845795449, 0.00012207634956370344, 0.00011035656822337226, 9.937741246382216e-05, 9.066111598504015e-05, 8.824440689590444e-05, 8.338480120507817e-05, 7.840173915636589e-05, 7.316004776473387e-05, 6.863464464704094e-05, 6.403156752594898e-05, 5.935343820081078e-05, 5.5273355321892065e-05, 5.0991494523507424e-05, 4.66925545269324e-05, 4.2613843296513234e-05, 3.893747600324793e-05, 3.54221200494892e-05, 3.21864052782588e-05, 2.9603620334065885e-05, 2.7268792447407286e-05, 2.494969029173062e-05, 2.2785020704699456e-05, 2.070092417706528e-05, 1.8788300249055707e-05, 1.6990068587211216e-05, 1.5452660126703274e-05, 1.4168889944584287e-05, 1.2848508762944367e-05, 1.168814939863435e-05, 1.059825561725169e-05, 9.57068406083126e-06, 8.629244031786711e-06, 7.800824304990544e-06, 7.037787217813791e-06, 6.357762434666987e-06, 5.7527274695476055e-06, 5.184604529474596e-06, 4.678850319356207e-06, 4.2300288520056465e-06, 3.846034574470499e-06, 3.507388386011475e-06, 3.2275018756360757e-06, 2.9621420739353243e-06, 2.771858520083627e-06, 2.603271916419572e-06, 2.3551319145638877e-06, 2.129490346834378e-06, 1.92453732736971e-06, 1.7557136126238646e-06, 1.5809536605147993e-06, 1.4235155358775102e-06, 1.2816963478352542e-06, 1.1539579291435798e-06, 1.038911421263621e-06, 9.354123524706555e-07, 8.421886966236415e-07, 7.607428789227793e-07, 6.847846421873484e-07, 6.170311691310348e-07, 5.59031284376478e-07, 5.114802629366847e-07, 4.624143137075305e-07, 4.1619264667981243e-07, 3.745893911296895e-07, 3.3822077691649603e-07, 3.149995120626177e-07, 3.0021425422683514e-07, 2.7655060941029197e-07, 2.540453507602351e-07, 2.32812155539902e-07, 2.0999345773593802e-07, 1.893687513398649e-07, 1.7335232065988043e-07, 1.6328603506816963e-07, 1.48269283896666e-07, 1.3345854439580448e-07, 1.2173213116778502e-07, 1.3181056532689558e-07, 1.3665334308767662e-07, 1.3758731455662193e-07, 1.356540207809099e-07, 1.2231606919185924e-07, 1.1260394100172707e-07, 1.0141671567319444e-07, 9.211156127604766e-08, 8.357798405628312e-08, 7.88259966012502e-08, 7.131198406404868e-08, 6.708597996188535e-08, 6.273058935213281e-08, 6.290240493349856e-08, 6.18325127985813e-08, 5.987774368905386e-08, 6.29425776644607e-08, 7.159381522876368e-08], "accuracy_test": 0.1666194993622449, "start": "2016-02-03 14:04:49.943000", "learning_rate_per_epoch": [0.006657639518380165, 0.006542461458593607, 0.006429275963455439, 0.006318048574030399, 0.006208745297044516, 0.006101333070546389, 0.0059957788325846195, 0.005892050918191671, 0.005790117662400007, 0.0056899478659033775, 0.00559151079505682, 0.005494776647537947, 0.0053997160866856575, 0.005306300241500139, 0.005214500240981579, 0.005124288611114025, 0.00503563741222024, 0.004948520101606846, 0.004862910136580467, 0.004778780974447727, 0.004696107469499111, 0.004614864010363817, 0.004535025916993618, 0.004456569440662861, 0.0043794699013233185, 0.004303704481571913, 0.004229249898344278, 0.004156083334237337, 0.004084182437509298, 0.0040135253220796585, 0.0039440905675292015, 0.0038758572190999985, 0.0038088043220341206, 0.0037429113872349262, 0.0036781583912670612, 0.0036145257763564587, 0.0035519939847290516, 0.0034905439242720604, 0.0034301569685339928, 0.003370814723894, 0.0033124990295618773, 0.0032551921904087067, 0.0031988767441362143, 0.0031435356941074133, 0.003089152043685317, 0.003035709261894226, 0.0029831910505890846, 0.0029315813444554806, 0.002880864543840289, 0.0028310250490903854, 0.0027820479590445757, 0.0027339181397110224, 0.0026866209227591753, 0.002640141872689128, 0.002594467019662261, 0.0025495823938399553, 0.0025054742582142353, 0.002462129108607769, 0.002419533906504512, 0.002377675613388419, 0.00233654142357409, 0.0022961189970374107, 0.002256395760923624, 0.0022173598408699036, 0.0021789991296827793, 0.0021413019858300686, 0.002104257233440876, 0.002067853230983019, 0.0020320790354162455, 0.001996923703700304, 0.0019623765256255865, 0.0019284271402284503, 0.001895065070129931, 0.0018622801871970296, 0.001830062479712069, 0.001798402052372694, 0.0017672893591225147, 0.0017367149703204632, 0.0017066695727407932, 0.0016771439695730805, 0.0016481290804222226, 0.0016196161741390824, 0.001591596519574523, 0.0015640616184100509, 0.0015370030887424946, 0.0015104126650840044, 0.0014842823147773743, 0.0014586040051653981, 0.0014333699364215136, 0.00140857242513448, 0.0013842039043083787, 0.0013602569233626127, 0.0013367242645472288, 0.0013135987101122737, 0.0012908732751384377, 0.0012685409747064114, 0.0012465950567275286, 0.0012250287691131234, 0.0012038355926051736, 0.0011830090079456568, 0.0011625427287071943, 0.0011424305848777294, 0.0011226664064452052, 0.0011032441398128867, 0.0010841578477993608, 0.001065401709638536, 0.0010469701373949647, 0.0010288574267178774, 0.0010110579896718264, 0.0009935664711520076, 0.0009763776324689388, 0.0009594861185178161, 0.0009428868652321398, 0.0009265747503377497, 0.000910544884391129, 0.0008947923197411001, 0.000879312283359468, 0.0008641000604256988, 0.0008491509943269193, 0.0008344605448655784, 0.0008200242300517857, 0.0008058376843109727, 0.0007918965420685709, 0.0007781966123729944, 0.0007647337042726576, 0.0007515036850236356, 0.0007385025382973254, 0.0007257263059727848, 0.0007131711463443935, 0.0007008331594988704, 0.0006887086201459169, 0.0006767938612028956, 0.0006650852155871689, 0.0006535791326314211, 0.0006422721198759973, 0.0006311606848612428, 0.0006202415097504854, 0.0006095112184993923, 0.0005989665514789522, 0.0005886043072678149, 0.0005784213426522911, 0.0005684145726263523, 0.0005585809121839702, 0.0005489173927344382, 0.0005394210456870496, 0.0005300889606587589, 0.0005209183436818421, 0.0005119064007885754, 0.000503050338011235, 0.0004943474777974188, 0.0004857952008023858, 0.00047739085857756436, 0.00046913191908970475, 0.00046101585030555725, 0.0004530402075033635, 0.0004452025459613651, 0.00043750047916546464, 0.0004299316497053951, 0.0004224937583785504, 0.00041518453508615494, 0.000408001767937094, 0.00040094327414408326, 0.000394006900023669, 0.00038719052099622786, 0.0003804920706897974, 0.0003739094827324152, 0.0003674407780636102, 0.0003610840067267418, 0.0003548371896613389, 0.0003486984351184219, 0.000342665909556672, 0.00033673772122710943, 0.0003309120947960764, 0.00032518725492991507, 0.000319561455398798, 0.0003140329790767282, 0.00030860016704536974, 0.00030326133128255606], "accuracy_train_first": 0.5196247966846622, "accuracy_train_last": 0.9963502373454227, "batch_size_eval": 1024, "accuracy_train_std": [0.022241758459572297, 0.02410874572836862, 0.02371809966206577, 0.024334702864481837, 0.025850967134220994, 0.027463985865341666, 0.027190339755395906, 0.028718972643073496, 0.02928466332283062, 0.029962322818875738, 0.028100071555073148, 0.02834367691279919, 0.030371544888502838, 0.029531524748478366, 0.029015994664345932, 0.029747177647643695, 0.030395435578840262, 0.02857060115178432, 0.027272251878015154, 0.030452827579941003, 0.03128916785930257, 0.02790828467411926, 0.029481612782936366, 0.027792773004506426, 0.025684590768533924, 0.027277145315827648, 0.027402459723744544, 0.026602939745932835, 0.02503476194068158, 0.0235249261687941, 0.02296095728384247, 0.023727457038458924, 0.02247361924269995, 0.02080566773073162, 0.0203730244900033, 0.020969830833266267, 0.020125920944947247, 0.01809411017578563, 0.018805627223304044, 0.018664438113191063, 0.01862138164683238, 0.01720896778063367, 0.018008670771471356, 0.01589171260067836, 0.017778174866793537, 0.016894610597989895, 0.015797400794730576, 0.015305392082453542, 0.012207890343362246, 0.01360352687448896, 0.01214064670416952, 0.012231687442502814, 0.011141596568798633, 0.01472308580743247, 0.013243900665613996, 0.010876779514740227, 0.011558874270193843, 0.009565229645018271, 0.01032194528216183, 0.009699140840021103, 0.009824074537663662, 0.0082297618696212, 0.009185466732597295, 0.008849118080749035, 0.00847228932567618, 0.00756150508919396, 0.007438084838593582, 0.007271953583009485, 0.006531551312392258, 0.0073117105066872155, 0.006443839370332797, 0.005216331677012324, 0.00564178356805291, 0.006218962458111393, 0.00493815709141665, 0.0050481294776232605, 0.005980945094059322, 0.006097722471325295, 0.0046467954045562145, 0.005222114381626336, 0.004157060452634776, 0.004317846536486062, 0.004518979777678718, 0.003340006461809516, 0.002747395069618494, 0.003258122053359658, 0.0025657123957116405, 0.0021302161470520267, 0.0020138308006932017, 0.0019179747508171338, 0.0019246582153539078, 0.001905620901886545, 0.0018816388857415937, 0.0018718087056743625, 0.0019207750817297432, 0.0018958089633304101, 0.0018837936624688787, 0.0018837936624688787, 0.0019098201481622203, 0.0018814606946175655, 0.0018887915758802006, 0.0018958089633304101, 0.0018541618374970676, 0.0018693531722777268, 0.0018837936624688787, 0.0019144141226653552, 0.0019207750817297432, 0.0018542828729504754, 0.0018614258560232207, 0.001806460872866818, 0.001806460872866818, 0.001806460872866818, 0.0018189870664424082, 0.0017664862111308386, 0.0017920099496061424, 0.0017994001511730847, 0.001745691256630959, 0.0017504705809624184, 0.0017243320821242302, 0.0017250788576726435, 0.0017512062101766247, 0.0016888039787532193, 0.0016786797056290972, 0.0016786797056290972, 0.0016985496577424897, 0.0016786797056290972, 0.0016786797056290972, 0.0016786797056290972, 0.0016579601176623922, 0.0016753046934712182, 0.001711865649665049, 0.0017381916257637317, 0.0017985808784667851, 0.0017985808784667851, 0.0017985808784667851, 0.0018196581474118204, 0.0018196581474118204, 0.0018889122154946334, 0.0018889122154946334, 0.0018479783858790337, 0.0018768527633138494, 0.0018602250340649894, 0.0018602250340649894, 0.0019008951629510913, 0.001827819478070738, 0.0018196581474118204, 0.0018727977874032273, 0.00190500582997646, 0.00190500582997646, 0.0019206831605568034, 0.001928697316225728, 0.001967403378565392, 0.001967403378565392, 0.0019206831605568034, 0.0019206831605568034, 0.001912803043498433, 0.0018930489031079704, 0.0018930489031079704, 0.0018968915830575156, 0.0019008951629510913, 0.0019008951629510913, 0.0018968915830575156, 0.0019274094193209004, 0.002005092892353725, 0.002005092892353725, 0.002005092892353725, 0.002005092892353725, 0.002016385521088028, 0.002016385521088028, 0.0020529127500293486, 0.0020529127500293486, 0.0020529127500293486, 0.0020556678956867914, 0.0020556678956867914, 0.0020556678956867914, 0.0020556678956867914, 0.0020556678956867914, 0.0020556678956867914], "accuracy_test_std": 0.009125023794114062, "error_valid": [0.48045845491340367, 0.4152199618787651, 0.34146449077560237, 0.2972941570971386, 0.27375517695783136, 0.27587008189006024, 0.26078336784638556, 0.26407926628388556, 0.24972497411521077, 0.24561576383659633, 0.27328601515436746, 0.25052946159638556, 0.2530517578125, 0.2558093702936747, 0.25763012989457834, 0.27028573277484935, 0.25330619352409633, 0.24676587208207834, 0.2652190794427711, 0.24132271272590367, 0.2576698395143072, 0.24191247411521077, 0.25409009083207834, 0.24120064241340367, 0.22805675828313254, 0.24618493505271077, 0.24509659732680722, 0.24186099868222888, 0.22966426251882532, 0.23385583349021077, 0.23428381494728923, 0.23867834619728923, 0.22988781297063254, 0.22798616340361444, 0.22314306052334332, 0.2223091585090362, 0.23337637660015065, 0.2293701171875, 0.22971573795180722, 0.22348868128765065, 0.22185176251882532, 0.21983686699924698, 0.22024425828313254, 0.2281494140625, 0.22400784779743976, 0.22202383753765065, 0.2259521484375, 0.2182102433170181, 0.20254259224397586, 0.22033544333584332, 0.21988834243222888, 0.21671451430722888, 0.22266654508659633, 0.22508589043674698, 0.22170910203313254, 0.2170601350715362, 0.22013248305722888, 0.21094632435993976, 0.2136421663215362, 0.21317447524472888, 0.2163277131965362, 0.20801663685993976, 0.2152290803840362, 0.21276708396084332, 0.21362157614834332, 0.20606351185993976, 0.2115154955760542, 0.21802640248493976, 0.21744693618222888, 0.21511730515813254, 0.21368334666792166, 0.2163277131965362, 0.2112007600715362, 0.20853580336972888, 0.20716214467243976, 0.21118016989834332, 0.20575760071536142, 0.20615469691265065, 0.21349950583584332, 0.21104780449924698, 0.20507665427334332, 0.20615469691265065, 0.20444571253765065, 0.20456778285015065, 0.2074871752635542, 0.20879023908132532, 0.20581937123493976, 0.20053946253765065, 0.20163809535015065, 0.2015057299510542, 0.2016278002635542, 0.2008953783885542, 0.2008953783885542, 0.20102774378765065, 0.20042768731174698, 0.20079389824924698, 0.20116010918674698, 0.20128217949924698, 0.20103803887424698, 0.20128217949924698, 0.20128217949924698, 0.2002850268260542, 0.2000408862010542, 0.2002850268260542, 0.2002850268260542, 0.20053946253765065, 0.20053946253765065, 0.20078360316265065, 0.20029532191265065, 0.1997967455760542, 0.20029532191265065, 0.20005118128765065, 0.20017325160015065, 0.20041739222515065, 0.20053946253765065, 0.20029532191265065, 0.20041739222515065, 0.20029532191265065, 0.20017325160015065, 0.20041739222515065, 0.20029532191265065, 0.20017325160015065, 0.19992911097515065, 0.19980704066265065, 0.19956290003765065, 0.19956290003765065, 0.19919668910015065, 0.19907461878765065, 0.19969526543674698, 0.19969526543674698, 0.19969526543674698, 0.19945112481174698, 0.19981733574924698, 0.19981733574924698, 0.19981733574924698, 0.19981733574924698, 0.19981733574924698, 0.19993940606174698, 0.19993940606174698, 0.20006147637424698, 0.19993940606174698, 0.19981733574924698, 0.19969526543674698, 0.19957319512424698, 0.19969526543674698, 0.19981733574924698, 0.19981733574924698, 0.19993940606174698, 0.20018354668674698, 0.20030561699924698, 0.20018354668674698, 0.20018354668674698, 0.20018354668674698, 0.20006147637424698, 0.20006147637424698, 0.20018354668674698, 0.20030561699924698, 0.19992911097515065, 0.20005118128765065, 0.20017325160015065, 0.20054975762424698, 0.20054975762424698, 0.20054975762424698, 0.20054975762424698, 0.20017325160015065, 0.20005118128765065, 0.20017325160015065, 0.20029532191265065, 0.20029532191265065, 0.20041739222515065, 0.20017325160015065, 0.20005118128765065, 0.20005118128765065, 0.19992911097515065, 0.19992911097515065, 0.19992911097515065, 0.19980704066265065, 0.19968497035015065], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.0173001602187552, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 32, "valid_ratio": 0.15, "learning_rate": 0.006774845521652885, "optimization": "rmsprop", "nb_data_augmentation": 0, "learning_rate_decay_method": "exp", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 1.5601559483604917e-06, "rotation_range": [0, 0], "momentum": 0.8147434456343838}, "accuracy_valid_max": 0.8009253812123494, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            print(X_train.min(), X_train.max())\n            print(X_valid.min(), X_valid.max())\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n\n    # rescaling to [-1, 1]\n    X_min = X_train.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X_train.max(axis=(0, 2, 3))[None, :, None, None]\n    def preprocess(a):\n        return (a / 255.) * 2 - 1\n        # return 2 * ((a - X_min) / (X_max - X_min)) - 1\n    X_train = preprocess(X_train)\n    X_valid = preprocess(X_valid)\n\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = preprocess(X_test)\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8003150296498494, "accuracy_valid_std": [0.01669157423442554, 0.016783091220944766, 0.01013729754568965, 0.0082516944515083, 0.016524119210407907, 0.008631123754125667, 0.011276941241829102, 0.012827828562539471, 0.01075318885820201, 0.011592290924938011, 0.01309712790126892, 0.011909752314045688, 0.010747041472587383, 0.013314232630838667, 0.011081389656702408, 0.013334507454190325, 0.014360026857101825, 0.011574076362727111, 0.017373031078155222, 0.010972160243048167, 0.01138638277766098, 0.013810401080748873, 0.011946143917953872, 0.009334950258398629, 0.01091502579946855, 0.008987057404526017, 0.012649982191520678, 0.01220288664114599, 0.01262044169751941, 0.0119417504561999, 0.014953069659407327, 0.016254587847016926, 0.012855674706137653, 0.011611525456757534, 0.012799252628866397, 0.011744530237735994, 0.015205183842578162, 0.013526690589322111, 0.008130645193066435, 0.00912603749932187, 0.01254728129144725, 0.013366078998193361, 0.00810646602449592, 0.009542588540621618, 0.011964451565526914, 0.010803598440347131, 0.011481749565374183, 0.014199654284586938, 0.011942564165399475, 0.00714322898320024, 0.009721518330306475, 0.009758935332327258, 0.015117944267128905, 0.007586477412676296, 0.011495538246885439, 0.008231435053045915, 0.007337449246632657, 0.009795316449822877, 0.01032041821771394, 0.010171542899032621, 0.008590960316320253, 0.01185682098533097, 0.009539279123358899, 0.01224042316949337, 0.010355097921503964, 0.01392070462203146, 0.008325506911780101, 0.013773183566549078, 0.01190753889964782, 0.009259888913593374, 0.014359718290221172, 0.009168238547177783, 0.013803178210282279, 0.014944430317477012, 0.011146869997528026, 0.010783356513038602, 0.011375591052780904, 0.00968118934171968, 0.010311097083578732, 0.012633692219649568, 0.011090453071928734, 0.012459430985339174, 0.012875233736991624, 0.012801741376563813, 0.012223557056015143, 0.013103039806528185, 0.01201574281617874, 0.012803529316105889, 0.012806003973389686, 0.012177771515192923, 0.012334326771811115, 0.012710836281243783, 0.01286001773848731, 0.012790397919034411, 0.01323519559720973, 0.013228201122597262, 0.013011025237914324, 0.013331864410184958, 0.013223911701018543, 0.01307910120905533, 0.013287080783830906, 0.012966382545787046, 0.012592638100815958, 0.012276910036702107, 0.012218510898414239, 0.012606489312542575, 0.012859271924296194, 0.012729249240993164, 0.012657965991760056, 0.012737904988367955, 0.01295582871065391, 0.01310180991014885, 0.012905491489829519, 0.012810881783937658, 0.012615941953193655, 0.0123529230758272, 0.012200815670254415, 0.01220731050767452, 0.012094880378508475, 0.01194407886623682, 0.012314261415159981, 0.012212581804010489, 0.012258420397695043, 0.012103449545592577, 0.012132517245660739, 0.012394969800366496, 0.012614184355196944, 0.012768369761993893, 0.012982182091171118, 0.013389021170641804, 0.013504270785059619, 0.013607387799083836, 0.013714056536505148, 0.013714056536505148, 0.013714056536505148, 0.013468467859100226, 0.013468467859100226, 0.01347576395163321, 0.01347576395163321, 0.013596412795093821, 0.01347576395163321, 0.01367924239020763, 0.013557132473750639, 0.013441674959182309, 0.013460060792608142, 0.013486158223520105, 0.013486158223520105, 0.01351992259503156, 0.013469340933089985, 0.013570295876777659, 0.0134604876060532, 0.0134604876060532, 0.013671385170901641, 0.013813866416471893, 0.014129557416950038, 0.014243623741553761, 0.014036649652063454, 0.013787123462005656, 0.013896585733222935, 0.013694252281705621, 0.013938847327562906, 0.013938847327562906, 0.013938847327562906, 0.013938847327562906, 0.013694252281705621, 0.013584256556512724, 0.01391874378712534, 0.014042043398640255, 0.014042043398640255, 0.01389984743558059, 0.013995613449347417, 0.01396504363307282, 0.01396504363307282, 0.014103413089863908, 0.014379640951487496, 0.014379640951487496, 0.014264484964693909, 0.013943641579638934], "accuracy_valid": [0.5195415450865963, 0.5847800381212349, 0.6585355092243976, 0.7027058429028614, 0.7262448230421686, 0.7241299181099398, 0.7392166321536144, 0.7359207337161144, 0.7502750258847892, 0.7543842361634037, 0.7267139848456325, 0.7494705384036144, 0.7469482421875, 0.7441906297063253, 0.7423698701054217, 0.7297142672251506, 0.7466938064759037, 0.7532341279179217, 0.7347809205572289, 0.7586772872740963, 0.7423301604856928, 0.7580875258847892, 0.7459099091679217, 0.7587993575865963, 0.7719432417168675, 0.7538150649472892, 0.7549034026731928, 0.7581390013177711, 0.7703357374811747, 0.7661441665097892, 0.7657161850527108, 0.7613216538027108, 0.7701121870293675, 0.7720138365963856, 0.7768569394766567, 0.7776908414909638, 0.7666236233998494, 0.7706298828125, 0.7702842620481928, 0.7765113187123494, 0.7781482374811747, 0.780163133000753, 0.7797557417168675, 0.7718505859375, 0.7759921522025602, 0.7779761624623494, 0.7740478515625, 0.7817897566829819, 0.7974574077560241, 0.7796645566641567, 0.7801116575677711, 0.7832854856927711, 0.7773334549134037, 0.774914109563253, 0.7782908979668675, 0.7829398649284638, 0.7798675169427711, 0.7890536756400602, 0.7863578336784638, 0.7868255247552711, 0.7836722868034638, 0.7919833631400602, 0.7847709196159638, 0.7872329160391567, 0.7863784238516567, 0.7939364881400602, 0.7884845044239458, 0.7819735975150602, 0.7825530638177711, 0.7848826948418675, 0.7863166533320783, 0.7836722868034638, 0.7887992399284638, 0.7914641966302711, 0.7928378553275602, 0.7888198301016567, 0.7942423992846386, 0.7938453030873494, 0.7865004941641567, 0.788952195500753, 0.7949233457266567, 0.7938453030873494, 0.7955542874623494, 0.7954322171498494, 0.7925128247364458, 0.7912097609186747, 0.7941806287650602, 0.7994605374623494, 0.7983619046498494, 0.7984942700489458, 0.7983721997364458, 0.7991046216114458, 0.7991046216114458, 0.7989722562123494, 0.799572312688253, 0.799206101750753, 0.798839890813253, 0.798717820500753, 0.798961961125753, 0.798717820500753, 0.798717820500753, 0.7997149731739458, 0.7999591137989458, 0.7997149731739458, 0.7997149731739458, 0.7994605374623494, 0.7994605374623494, 0.7992163968373494, 0.7997046780873494, 0.8002032544239458, 0.7997046780873494, 0.7999488187123494, 0.7998267483998494, 0.7995826077748494, 0.7994605374623494, 0.7997046780873494, 0.7995826077748494, 0.7997046780873494, 0.7998267483998494, 0.7995826077748494, 0.7997046780873494, 0.7998267483998494, 0.8000708890248494, 0.8001929593373494, 0.8004370999623494, 0.8004370999623494, 0.8008033108998494, 0.8009253812123494, 0.800304734563253, 0.800304734563253, 0.800304734563253, 0.800548875188253, 0.800182664250753, 0.800182664250753, 0.800182664250753, 0.800182664250753, 0.800182664250753, 0.800060593938253, 0.800060593938253, 0.799938523625753, 0.800060593938253, 0.800182664250753, 0.800304734563253, 0.800426804875753, 0.800304734563253, 0.800182664250753, 0.800182664250753, 0.800060593938253, 0.799816453313253, 0.799694383000753, 0.799816453313253, 0.799816453313253, 0.799816453313253, 0.799938523625753, 0.799938523625753, 0.799816453313253, 0.799694383000753, 0.8000708890248494, 0.7999488187123494, 0.7998267483998494, 0.799450242375753, 0.799450242375753, 0.799450242375753, 0.799450242375753, 0.7998267483998494, 0.7999488187123494, 0.7998267483998494, 0.7997046780873494, 0.7997046780873494, 0.7995826077748494, 0.7998267483998494, 0.7999488187123494, 0.7999488187123494, 0.8000708890248494, 0.8000708890248494, 0.8000708890248494, 0.8001929593373494, 0.8003150296498494], "seed": 374679060, "model": "residualv4", "loss_std": [0.2849131226539612, 0.2017713189125061, 0.19918276369571686, 0.19310829043388367, 0.19016090035438538, 0.18377482891082764, 0.17947737872600555, 0.17595064640045166, 0.173685684800148, 0.17189791798591614, 0.17036794126033783, 0.1660217046737671, 0.15857529640197754, 0.15679755806922913, 0.1507054567337036, 0.14549201726913452, 0.14546853303909302, 0.141366109251976, 0.13166214525699615, 0.13177938759326935, 0.1254613697528839, 0.12569348514080048, 0.11857999861240387, 0.11599347740411758, 0.10968939960002899, 0.10696540772914886, 0.10635565221309662, 0.10351882874965668, 0.09624368697404861, 0.09053479880094528, 0.09419857710599899, 0.08532405644655228, 0.08789712190628052, 0.08761238306760788, 0.08035378903150558, 0.0811268761754036, 0.07508593052625656, 0.07351631671190262, 0.0789518728852272, 0.0716802328824997, 0.07677531242370605, 0.07091124355792999, 0.06726473569869995, 0.06418587267398834, 0.06260163336992264, 0.05899258330464363, 0.059590671211481094, 0.05747426673769951, 0.05681623890995979, 0.05209629237651825, 0.05338563397526741, 0.05599020794034004, 0.05488056689500809, 0.04931927099823952, 0.05148530378937721, 0.04840978607535362, 0.04493381083011627, 0.045948226004838943, 0.04221596568822861, 0.0444708876311779, 0.04318553954362869, 0.03829297423362732, 0.04014930874109268, 0.03822692483663559, 0.041894715279340744, 0.04038992151618004, 0.034301768988370895, 0.03639465197920799, 0.03169838339090347, 0.033588957041502, 0.03070981428027153, 0.02915899083018303, 0.030185457319021225, 0.027468059211969376, 0.028621265664696693, 0.026563946157693863, 0.02705448865890503, 0.021697470918297768, 0.019526420161128044, 0.0200582854449749, 0.018877534195780754, 0.013542945496737957, 0.011074051260948181, 0.00973296444863081, 0.008050688542425632, 0.008285792544484138, 0.002460651798173785, 0.00042010939796455204, 0.00033093156525865197, 0.0003138499450869858, 0.0003032652020920068, 0.00029384790104813874, 0.0002851288882084191, 0.0002768515842035413, 0.0002689318498596549, 0.0002613640099298209, 0.0002540790883358568, 0.0002470942272339016, 0.00024037508410401642, 0.0002339250931981951, 0.00022769572387915105, 0.0002216784778283909, 0.0002158949209842831, 0.00021032833319623023, 0.00020494709315244108, 0.00019975723989773542, 0.00019474442524369806, 0.0001899116177810356, 0.00018523629114497453, 0.00018072253442369401, 0.0001763448235578835, 0.00017210846999660134, 0.00016800413141027093, 0.00016403681365773082, 0.00016019218310248107, 0.0001564692793181166, 0.00015287594578694552, 0.0001493981690146029, 0.00014604460739064962, 0.00014276852016337216, 0.00013960972137283534, 0.00013654239592142403, 0.00013357787975110114, 0.00013070410932414234, 0.00012790386972483248, 0.00012520316522568464, 0.00012259292998351157, 0.00012005124881397933, 0.00011760020424844697, 0.00011522787826834247, 0.00011291976261418313, 0.00011067804007325321, 0.0001084973628167063, 0.00010639277752488852, 0.00010435385047458112, 0.00010236359958071262, 0.00010044845112133771, 9.858224802883342e-05, 9.676662739366293e-05, 9.500394662609324e-05, 9.329873137176037e-05, 9.164698712993413e-05, 9.003920422401279e-05, 8.848033030517399e-05, 8.697625162312761e-05, 8.55137623148039e-05, 8.40913926367648e-05, 8.271749538835138e-05, 8.138667908497155e-05, 8.008695294847712e-05, 7.884126534918323e-05, 7.7631208114326e-05, 7.645558071089908e-05, 7.530924631282687e-05, 7.420111069222912e-05, 7.312771049328148e-05, 7.208474562503397e-05, 7.106817793101072e-05, 7.008412649156526e-05, 6.912562093930319e-05, 6.819806731073186e-05, 6.730059249093756e-05, 6.642723019467667e-05, 6.557884626090527e-05, 6.475434202002361e-05, 6.395403761416674e-05, 6.317849329207093e-05, 6.242882955120876e-05, 6.169893458718434e-05, 6.0999111155979335e-05, 6.0312391724437475e-05, 5.965010859654285e-05, 5.900844553252682e-05, 5.83901783102192e-05, 5.779189814347774e-05, 5.7210534578189254e-05, 5.664587297360413e-05, 5.609674917650409e-05]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:37 2016", "state": "available"}], "summary": "2ece2b7d3344ad9e74b3616b1fb248bf"}