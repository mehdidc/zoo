{"content": {"hp_model": {"f0": 32, "f1": 32, "f2": 32, "f3": 64, "nonlin": "rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.586126446723938, 1.1595501899719238, 0.954192042350769, 0.8267167210578918, 0.7375243306159973, 0.6686130166053772, 0.6171283721923828, 0.5752633213996887, 0.5372961759567261, 0.508899450302124, 0.47955644130706787, 0.4532827138900757, 0.43259671330451965, 0.4116652309894562, 0.39207854866981506, 0.3775757849216461, 0.3616633713245392, 0.3492262363433838, 0.3376537263393402, 0.3258996605873108, 0.3147221505641937, 0.3041558861732483, 0.299814373254776, 0.2907053530216217, 0.28201472759246826, 0.275794118642807, 0.26923668384552, 0.26356545090675354, 0.2566613256931305, 0.25561991333961487, 0.24866534769535065, 0.24315015971660614, 0.24163666367530823, 0.23781244456768036, 0.2333960086107254, 0.22984862327575684, 0.22840099036693573, 0.2232234627008438, 0.22397665679454803, 0.21493971347808838, 0.21797287464141846, 0.2138288915157318, 0.21191461384296417, 0.20938964188098907, 0.2097136378288269, 0.20363202691078186, 0.2043098658323288, 0.20167604088783264, 0.20276062190532684, 0.2003931999206543, 0.19685210287570953, 0.198465496301651, 0.19513411819934845, 0.19396989047527313, 0.1902538537979126, 0.19226281344890594, 0.18933948874473572, 0.18839232623577118, 0.19046293199062347, 0.1862667053937912, 0.18539461493492126, 0.18618535995483398, 0.18497233092784882, 0.18286065757274628, 0.18123677372932434, 0.1826370358467102, 0.181762233376503, 0.17777681350708008, 0.17900867760181427, 0.17840448021888733, 0.17654673755168915, 0.1764506697654724, 0.17374198138713837, 0.1744450330734253, 0.17474935948848724, 0.17254707217216492, 0.1721743494272232, 0.1727898120880127, 0.17335174977779388, 0.1732977330684662, 0.17118151485919952, 0.1688380092382431, 0.17009873688220978, 0.17050200700759888, 0.17003439366817474, 0.17102821171283722, 0.16780585050582886, 0.16810128092765808, 0.16839447617530823, 0.1660536229610443, 0.162343829870224, 0.16821305453777313, 0.13869711756706238, 0.12335013598203659, 0.11873646080493927, 0.11774281412363052, 0.115288145840168, 0.11399827897548676, 0.11240015178918839, 0.11111309379339218, 0.1098741814494133, 0.10963784158229828, 0.1079331710934639, 0.10828452557325363, 0.10697314143180847, 0.10649648308753967, 0.10444106161594391, 0.1047300174832344, 0.1044183224439621, 0.10284736007452011, 0.10259053111076355, 0.10258050262928009, 0.10332455486059189, 0.10401111096143723, 0.1039271280169487, 0.10313593596220016, 0.10341746360063553, 0.10376695543527603, 0.10314443707466125, 0.10347677022218704, 0.10274848341941833, 0.1039450541138649, 0.10369960218667984, 0.1038832813501358, 0.1040881872177124, 0.10382946580648422, 0.10396046191453934, 0.10384843498468399, 0.10329262912273407, 0.1033870130777359, 0.10423354804515839, 0.10341022163629532, 0.1042197123169899, 0.10364290326833725, 0.10377795994281769, 0.1031598225235939, 0.1041584238409996, 0.1033366471529007, 0.10314514487981796, 0.10340266674757004, 0.10399167984724045, 0.10437345504760742, 0.10378626734018326, 0.1033208966255188, 0.10363393276929855, 0.1039569154381752, 0.10315019637346268, 0.10348504036664963, 0.10412980616092682, 0.10344596207141876, 0.10312013328075409, 0.10396535694599152, 0.10213993489742279, 0.10377626121044159, 0.10397414863109589, 0.10408302396535873, 0.10423923283815384, 0.1038055419921875, 0.10463034361600876, 0.1029774621129036, 0.10388752818107605, 0.10334085673093796, 0.10299836099147797, 0.10288208723068237, 0.10406813025474548, 0.10241974890232086, 0.10405108332633972, 0.103759765625, 0.10443291813135147, 0.10357187688350677, 0.10342875123023987, 0.10318585485219955, 0.1038106232881546, 0.10359589010477066, 0.10397953540086746, 0.10287684202194214, 0.10345280170440674, 0.10276828706264496, 0.10266292095184326, 0.1043628677725792, 0.10348068922758102, 0.104507215321064, 0.10327715426683426, 0.10351322591304779, 0.10369158536195755, 0.10352665930986404, 0.10340122878551483, 0.1040218397974968, 0.10349277406930923, 0.10342239588499069, 0.10299843549728394, 0.1043383851647377, 0.10356750339269638, 0.10329651832580566, 0.10336077958345413, 0.10297633707523346, 0.10327720642089844, 0.1028214618563652, 0.10445733368396759, 0.10343286395072937, 0.10318440943956375], "moving_avg_accuracy_train": [0.03042069692460317, 0.06804474090992985, 0.11101890904738831, 0.16063361098133533, 0.21125180471201316, 0.25919542196944784, 0.31559506971228707, 0.3644388300617948, 0.4107322672441074, 0.45417205342228983, 0.49485161176130854, 0.5324023220437417, 0.5639011468599471, 0.5965718919526992, 0.6271521959802514, 0.6540462107918331, 0.6786347620663781, 0.70158756089204, 0.7212499521934783, 0.7405363619552121, 0.756943325121771, 0.7721745135871225, 0.787756364755865, 0.802068385308933, 0.8157116363233055, 0.8278931222791532, 0.8388191851608263, 0.8496290240102383, 0.8585744480211747, 0.8671554635595888, 0.8757059502762582, 0.8818972693832301, 0.8890271620866107, 0.8952464639196624, 0.9023527850491525, 0.9077090964990177, 0.9138202704420006, 0.9177415869978375, 0.9217776543385667, 0.9255519850714229, 0.9299882242488506, 0.928803309890826, 0.9325981602993995, 0.9364086928182782, 0.9400056548971923, 0.9425942042503579, 0.9449262238170164, 0.9473133959282088, 0.9496221779496828, 0.9514862041273521, 0.9534474597931976, 0.9554590917150867, 0.9563210439257763, 0.9576524033344168, 0.9590832137807832, 0.9604568655420092, 0.9611071425294749, 0.9626132588932218, 0.963817628947975, 0.9648178566401099, 0.9655553011463647, 0.9656842530246222, 0.9670255910400356, 0.9681908704776987, 0.9690397673204327, 0.9691946215396169, 0.9703197092368457, 0.9713812244358079, 0.9714739579065405, 0.9724200121897144, 0.9723879405457707, 0.9731960214911937, 0.9739790979135028, 0.974767572050724, 0.9747565107897268, 0.9753951640262303, 0.975709643418873, 0.9763809386746232, 0.9748322689489106, 0.9755168608409335, 0.9761957725616113, 0.9765185467554778, 0.9768972910382634, 0.977689239761818, 0.9785043001606362, 0.978166105113657, 0.9782522464249195, 0.9791179630026656, 0.9793879363821701, 0.9797146177808671, 0.9799900658980369, 0.9799985149249276, 0.9817638234026729, 0.9834432818362151, 0.9849733956168794, 0.9863714243587628, 0.9876412759705055, 0.9887771669746455, 0.9898250455152762, 0.990747209862558, 0.9915957589655879, 0.992364103455934, 0.9930509631996263, 0.9936737872665684, 0.994245954670864, 0.9947771813763966, 0.9952297087744713, 0.9956276828375004, 0.9959951600894646, 0.9963258896162325, 0.9966212210415141, 0.996898645068315, 0.9971436763948169, 0.9973595542910495, 0.9975608198440874, 0.9977349833953929, 0.9978987060379965, 0.9980460564163397, 0.9981740214592295, 0.9982938402954494, 0.9983993520992378, 0.9984966378714569, 0.9985748944712161, 0.9986546260062373, 0.998728709536566, 0.9987930595650523, 0.9988509745906899, 0.9989007729649544, 0.9989479166506018, 0.9989880208188748, 0.9990264397191302, 0.9990610167293601, 0.9990874857409479, 0.9991113078513768, 0.9991350728995725, 0.9991587865917582, 0.9991847792123442, 0.9991988719756335, 0.9992138806114035, 0.9992250632347869, 0.9992328024470225, 0.9992397677380346, 0.9992483616487549, 0.9992607464660223, 0.9992672425039438, 0.9992754140868827, 0.9992850936603374, 0.9992984555740655, 0.9993011807011827, 0.9993082836132073, 0.999317001382839, 0.9993248473755074, 0.99932725847129, 0.9993317536063039, 0.9993311489301973, 0.9993329298705109, 0.9993438333120312, 0.999346670962971, 0.9993562002952454, 0.9993508258014352, 0.999345988757006, 0.9993509360122578, 0.9993507382443654, 0.9993459099556431, 0.9993485399422216, 0.9993462566325232, 0.9993488519514138, 0.9993418871431772, 0.9993402691133833, 0.9993434631841878, 0.9993416875502927, 0.9993424146285967, 0.9993453941478799, 0.9993434254176157, 0.9993416535603779, 0.999344709186483, 0.9993474592499776, 0.9993476091583131, 0.9993477440758152, 0.9993501906503766, 0.9993454171210532, 0.9993411209446622, 0.9993442298323388, 0.9993447026824382, 0.9993451282475279, 0.9993501615537275, 0.9993570166781166, 0.9993562108436382, 0.9993624610390363, 0.9993680862148945, 0.9993684985755479, 0.9993735199977549, 0.9993757141289318, 0.999373038549372, 0.9993729556765777, 0.9993682307934437, 0.9993639783986231, 0.9993555009456656, 0.9993548466844324, 0.9993496075517034, 0.9993495426298664], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.030374329348644574, 0.0681780932323042, 0.11092803734469124, 0.16067051810758656, 0.20993814485480983, 0.25698705792015714, 0.3114166289071324, 0.35798713304390406, 0.402816155291095, 0.44400147194383793, 0.4822330724433547, 0.5163760467313987, 0.545190131629042, 0.5742825171860173, 0.6015084093868434, 0.6254471004041079, 0.6470601640365135, 0.666927989876838, 0.6839442458590337, 0.7003301535396965, 0.7140103111902449, 0.7264343753743078, 0.7390707288760336, 0.7506815576732947, 0.7611627771412814, 0.7708013351764003, 0.7793611736561247, 0.7877424449859188, 0.794561256304345, 0.8011591122251154, 0.8077207706562183, 0.8119100723085934, 0.8175034094866195, 0.8221548769434545, 0.8274236524738832, 0.8311381008089497, 0.8358890309388981, 0.8388128290404149, 0.8418644339375781, 0.8451866678310944, 0.8481481462381205, 0.8471462198861458, 0.850121321636236, 0.8531570351522058, 0.8562330332088979, 0.858101052600734, 0.8593448759457057, 0.86084376443359, 0.8630482857688455, 0.8642755190330754, 0.8654247390612437, 0.8667825969510531, 0.8678449481369718, 0.8690849144490276, 0.8704001675918507, 0.8712482755973493, 0.8717144860176596, 0.8729132663786194, 0.8737877376982124, 0.8745401998094153, 0.8747809040378112, 0.8746975096054156, 0.8758147727167866, 0.8767958954545206, 0.8771478265227733, 0.8770240818505413, 0.8781273847912401, 0.8793675865888481, 0.8791042266027794, 0.8798600901812063, 0.8798079455267904, 0.8803276863186144, 0.8813977155971445, 0.8820168860555023, 0.8822414610982955, 0.8831710000411768, 0.8833289918104025, 0.883735621055567, 0.8819093222218477, 0.8822809900090756, 0.8825635743666017, 0.882828048302306, 0.8826398582593495, 0.8831764360158694, 0.8841720513092373, 0.8837518047155877, 0.8833928493005048, 0.8846690575933458, 0.8849509458381528, 0.8854914369565213, 0.8857906554596644, 0.8853773878711528, 0.8875701001853177, 0.8894295599521774, 0.8912861792111012, 0.8929825801152922, 0.8944360987415642, 0.8956944078715493, 0.8969245423385359, 0.8980438703900739, 0.8990268515739581, 0.9000600780317732, 0.9009035031163971, 0.901661556183899, 0.9021983490783102, 0.9027750008986418, 0.9030742609744403, 0.9035918537022974, 0.9041797574698689, 0.9047332849231833, 0.9052080750773258, 0.9055977356136444, 0.9060094652525812, 0.9063932584675339, 0.9066766076960817, 0.9069560360642747, 0.9071831075331483, 0.9074485070113847, 0.907712810112957, 0.9079506829043721, 0.9080915262291458, 0.9082050486815324, 0.9083438399824304, 0.9084453675993982, 0.9086333691960097, 0.90864387922671, 0.9087021663793402, 0.9086691755979573, 0.9087737612384629, 0.9088445037610774, 0.9088705214290209, 0.90896717951767, 0.9090053436724542, 0.90903969141176, 0.9091072254708853, 0.9091812426640076, 0.9092356511065676, 0.9092469681024621, 0.9092703899386767, 0.9092660260201102, 0.9092498914621504, 0.9093218490873962, 0.9093744039188675, 0.9094217032671915, 0.9094144150470236, 0.9094821273450321, 0.9094932107795801, 0.909516422410583, 0.909536283369826, 0.9095063596168041, 0.9095892915203345, 0.9096018655686023, 0.909638625783203, 0.9096218523426839, 0.9096688209111263, 0.9095635787390649, 0.9095787240654597, 0.909531319702965, 0.9095618979642196, 0.9095161762118489, 0.9095238547597152, 0.9095195879302045, 0.9095269253062352, 0.9095579430071629, 0.9094759956567477, 0.9094887217687838, 0.9094869387297065, 0.9094731269632871, 0.9094606963735096, 0.9094261242888694, 0.9094550150602837, 0.9093589464420566, 0.9093579339044022, 0.9094068802541728, 0.9093644532415568, 0.9093750970552023, 0.9093612919336429, 0.9093610743554895, 0.9094341207226514, 0.9094622118506875, 0.9094630798034199, 0.9094150328358791, 0.9094704763237521, 0.9094827248604281, 0.9094815415121865, 0.9095303341324288, 0.9095121828257371, 0.9094836396184646, 0.9095200153968289, 0.9095028959636973, 0.9095129320450384, 0.9095463785807454, 0.909514415797972, 0.909497856324726, 0.9094829527988046, 0.9094451255629754, 0.909447702144479, 0.909499878701492, 0.9095102165090536, 0.9095317275671091, 0.9095134369169494], "moving_var_accuracy_train": [0.008328769212407048, 0.02023601046345452, 0.03483342156106803, 0.05350464723691088, 0.07121399634222769, 0.08477991062955144, 0.10493020195624349, 0.11590859808634044, 0.12360547921308088, 0.1282280665006287, 0.13029869805048447, 0.12995933082987313, 0.12589298143010377, 0.12291008155133364, 0.11903546834595799, 0.11364151380553239, 0.10771873410900752, 0.10168833946348854, 0.09499899220235758, 0.08884678339559932, 0.08238480101917876, 0.07623422283586441, 0.07079594732488156, 0.06555985798319638, 0.060679116869047325, 0.05594670258297524, 0.051426441975526184, 0.047335471321525886, 0.043322109685992226, 0.03965260316642754, 0.03634534025761225, 0.03305579812241022, 0.030207736639824675, 0.027535080413457615, 0.02523607057207078, 0.022970674165995308, 0.021009724772048335, 0.019047142806623223, 0.01728903708217102, 0.01568834352628276, 0.014296631136008582, 0.0128796042207304, 0.011721251805268406, 0.010679808047438462, 0.009728270468468934, 0.008815748711406007, 0.007983118677598912, 0.007236094126035114, 0.006560458983235739, 0.005935684427231491, 0.0053767346985896395, 0.004875481195633148, 0.0043946197305914465, 0.003971110418407082, 0.003592424343367254, 0.0032501641814805986, 0.0029289535047763867, 0.0026564736328090624, 0.0024038808345872307, 0.0021724968500535305, 0.0019601415846464258, 0.0017642770834639387, 0.0016040420641618828, 0.0014558587432562586, 0.001316758501577067, 0.0011852984698821523, 0.001078161023832037, 0.0009804862521074825, 0.0008825150223660814, 0.000802318688489879, 0.0007220960769539984, 0.0006557634225877991, 0.0005957059584776085, 0.0005417305858154474, 0.00048755862839735626, 0.00044247366716608825, 0.000399116376045052, 0.00036326047432408144, 0.00034851982816572213, 0.0003178858398767619, 0.0002902455460093488, 0.0002621586400304498, 0.0002372338011130895, 0.0002191550660284386, 0.0002032184705090899, 0.00018392600646639152, 0.00016560018874930673, 0.00015578535661123653, 0.000140862791580883, 0.00012773699904908625, 0.00011564614413144933, 0.000104082172192803, 0.00012172078116791867, 0.00013493392872109209, 0.0001425117694849892, 0.00014585095180468148, 0.00014577856466682466, 0.00014281294355971623, 0.00013841409412697326, 0.00013222616846485675, 0.0001254838718406468, 0.0001182486639591872, 0.00011066978433081413, 0.00010309399426299385, 9.573097468353915e-05, 8.86976935292249e-05, 8.167095359037616e-05, 7.492930842493324e-05, 6.865173335884071e-05, 6.277099820184128e-05, 5.727888423848645e-05, 5.224367263045603e-05, 4.755966852611564e-05, 4.322313106824039e-05, 3.9265388366973256e-05, 3.561184601370619e-05, 3.229190734564544e-05, 2.9258125817061888e-05, 2.6479688705171897e-05, 2.396092881627255e-05, 2.166503060129342e-05, 1.9583708234450438e-05, 1.7680454269658057e-05, 1.596962290178386e-05, 1.4422055936799075e-05, 1.301711867861481e-05, 1.1745594162504797e-05, 1.0593353648968657e-05, 9.5540210279396e-06, 8.61309402396158e-06, 7.765068728636918e-06, 6.9993219825011356e-06, 6.305695261420941e-06, 5.680233171786484e-06, 5.117292852249511e-06, 4.610624619798224e-06, 4.155642704742797e-06, 3.7418658880626966e-06, 3.369706631585512e-06, 3.0338614280185895e-06, 2.7310143438709797e-06, 2.458349546993823e-06, 2.213179290007667e-06, 1.9932418142956163e-06, 1.794297419444163e-06, 1.6154686504093014e-06, 1.454765032648736e-06, 1.310895396030175e-06, 1.179872693287405e-06, 1.0623394861917258e-06, 9.567895331387081e-07, 8.616646162334231e-07, 7.755504750559365e-07, 6.981772836994782e-07, 6.283628460282752e-07, 5.655551071610535e-07, 5.100695617778351e-07, 4.5913507596575356e-07, 4.140388419315285e-07, 3.7289492439182364e-07, 3.3581600494193437e-07, 3.024546824584778e-07, 2.7220956622188355e-07, 2.4519842094756346e-07, 2.207408303174374e-07, 1.987136688143006e-07, 1.7890292305416287e-07, 1.6144920773270138e-07, 1.4532784914315685e-07, 1.308868830235807e-07, 1.1782657060278298e-07, 1.0604867132824688e-07, 9.552370201185209e-08, 8.600621490034457e-08, 7.74338487129488e-08, 6.977449549969987e-08, 6.286511159274685e-08, 5.6578802686053856e-08, 5.092108624203964e-08, 4.588284916159348e-08, 4.1499643485243334e-08, 3.7515793320964646e-08, 3.385120063214186e-08, 3.046809285387667e-08, 2.7422913519297668e-08, 2.4908629709058026e-08, 2.2840701311670097e-08, 2.0562475503361146e-08, 1.8857812435653426e-08, 1.7256814623017734e-08, 1.5532663532492307e-08, 1.4206329308082776e-08, 1.2829024281867233e-08, 1.161055038750998e-08, 1.0449557159859359e-08, 9.605522129539184e-09, 8.8077156719743e-09, 8.573748982603628e-09, 7.720226604194984e-09, 7.195240549539437e-09, 6.475754428189775e-09], "duration": 92032.222053, "accuracy_train": [0.30420696924603174, 0.4066611367778701, 0.49778642228451464, 0.6071659283868586, 0.6668155482881137, 0.6906879772863602, 0.8231918993978405, 0.8040326732073644, 0.8273732018849206, 0.8451301290259321, 0.8609676368124769, 0.8703587145856404, 0.8473905702057956, 0.8906085977874677, 0.9023749322282208, 0.8960923440960686, 0.8999317235372831, 0.9081627503229974, 0.8982114739064231, 0.9141140498108158, 0.9046059936208011, 0.9092552097752861, 0.9279930252745479, 0.9308765702865448, 0.9385008954526578, 0.9375264958817828, 0.9371537510958842, 0.9469175736549464, 0.9390832641196014, 0.9443846034053157, 0.9526603307262828, 0.9376191413459765, 0.9531961964170359, 0.9512201804171282, 0.9663096752145626, 0.9559158995478036, 0.9688208359288483, 0.9530334360003692, 0.9581022604051311, 0.9595209616671282, 0.9699143768456996, 0.9181390806686047, 0.9667518139765596, 0.9707034854881875, 0.9723783136074198, 0.9658911484288483, 0.9659143999169435, 0.9687979449289406, 0.9704012161429494, 0.968262439726375, 0.9710987607858066, 0.9735637790120893, 0.9640786138219823, 0.9696346380121816, 0.9719605077980805, 0.9728197313930418, 0.9669596354166666, 0.9761683061669435, 0.974656959440753, 0.9738199058693245, 0.9721923017026578, 0.9668448199289406, 0.979097633178756, 0.9786783854166666, 0.9766798389050388, 0.9705883095122739, 0.9804454985119048, 0.9809348612264673, 0.9723085591431341, 0.9809345007382798, 0.9720992957502769, 0.98046875, 0.9810267857142857, 0.9818638392857143, 0.974656959440753, 0.9811430431547619, 0.9785399579526578, 0.982422595976375, 0.9608942414174971, 0.98167818786914, 0.9823059780477114, 0.9794235145002769, 0.9803059895833334, 0.9848167782738095, 0.98583984375, 0.9751223496908453, 0.9790275182262828, 0.9869094122023809, 0.9818176967977114, 0.98265475036914, 0.9824690989525655, 0.9800745561669435, 0.9976515997023809, 0.9985584077380952, 0.9987444196428571, 0.9989536830357143, 0.9990699404761905, 0.9990001860119048, 0.9992559523809523, 0.9990466889880952, 0.9992327008928571, 0.9992792038690477, 0.9992327008928571, 0.9992792038690477, 0.9993954613095238, 0.9995582217261905, 0.9993024553571429, 0.9992094494047619, 0.9993024553571429, 0.9993024553571429, 0.9992792038690477, 0.9993954613095238, 0.9993489583333334, 0.9993024553571429, 0.9993722098214286, 0.9993024553571429, 0.9993722098214286, 0.9993722098214286, 0.9993257068452381, 0.9993722098214286, 0.9993489583333334, 0.9993722098214286, 0.9992792038690477, 0.9993722098214286, 0.9993954613095238, 0.9993722098214286, 0.9993722098214286, 0.9993489583333334, 0.9993722098214286, 0.9993489583333334, 0.9993722098214286, 0.9993722098214286, 0.9993257068452381, 0.9993257068452381, 0.9993489583333334, 0.9993722098214286, 0.9994187127976191, 0.9993257068452381, 0.9993489583333334, 0.9993257068452381, 0.9993024553571429, 0.9993024553571429, 0.9993257068452381, 0.9993722098214286, 0.9993257068452381, 0.9993489583333334, 0.9993722098214286, 0.9994187127976191, 0.9993257068452381, 0.9993722098214286, 0.9993954613095238, 0.9993954613095238, 0.9993489583333334, 0.9993722098214286, 0.9993257068452381, 0.9993489583333334, 0.9994419642857143, 0.9993722098214286, 0.9994419642857143, 0.9993024553571429, 0.9993024553571429, 0.9993954613095238, 0.9993489583333334, 0.9993024553571429, 0.9993722098214286, 0.9993257068452381, 0.9993722098214286, 0.9992792038690477, 0.9993257068452381, 0.9993722098214286, 0.9993257068452381, 0.9993489583333334, 0.9993722098214286, 0.9993257068452381, 0.9993257068452381, 0.9993722098214286, 0.9993722098214286, 0.9993489583333334, 0.9993489583333334, 0.9993722098214286, 0.9993024553571429, 0.9993024553571429, 0.9993722098214286, 0.9993489583333334, 0.9993489583333334, 0.9993954613095238, 0.9994187127976191, 0.9993489583333334, 0.9994187127976191, 0.9994187127976191, 0.9993722098214286, 0.9994187127976191, 0.9993954613095238, 0.9993489583333334, 0.9993722098214286, 0.9993257068452381, 0.9993257068452381, 0.9992792038690477, 0.9993489583333334, 0.9993024553571429, 0.9993489583333334], "end": "2016-02-04 15:40:27.070000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0], "moving_var_accuracy_valid": [0.00830339895041938, 0.02033518012932084, 0.0347496816108987, 0.053543542981831935, 0.07003488009138212, 0.0829537940679194, 0.10132161844156307, 0.11070876329738438, 0.11772465808839819, 0.1212182650496387, 0.12225133603546672, 0.12051788667102638, 0.11593836140032109, 0.11196182733685034, 0.10743688745834429, 0.10185074706109039, 0.09586979303118703, 0.08983538826066154, 0.0834578261434599, 0.07752852526378677, 0.07145999315750282, 0.06570321017939997, 0.06056998602984518, 0.05572628953509443, 0.051142364235409944, 0.04686424402083614, 0.042837257131943265, 0.03918574280068161, 0.03568563421078012, 0.03250885611446331, 0.029645468755315155, 0.02683887411479498, 0.024436555490399293, 0.022187625286877323, 0.020218702718499983, 0.018321006584554888, 0.016692047959996265, 0.015099780522042538, 0.013673613101873806, 0.01240558693407548, 0.011243961429865468, 0.010128599994611954, 0.009195401068961268, 0.008358800971024514, 0.0076080767503250235, 0.006878674543327003, 0.006204730957615774, 0.005604477862146204, 0.005087769304789952, 0.004592547287674448, 0.004145178918965293, 0.003747255029509021, 0.0033826868369381266, 0.003058255801339614, 0.0027679992386730086, 0.002497672899506626, 0.0022498617789600164, 0.002037809270248424, 0.0018409106440226964, 0.0016619153726795894, 0.0014962452821417392, 0.0013466833456097565, 0.0012232495027890565, 0.0011095879689486388, 0.0009997438713449886, 0.0008999072989056423, 0.0008208720654256717, 0.0007526277633722137, 0.0006779892133753504, 0.0006153322597805459, 0.0005538235053873487, 0.0005008723292647862, 0.0004610897602505131, 0.00041843113273398925, 0.0003770419250092002, 0.00034711411632527606, 0.0003126273572850361, 0.0002828527476437404, 0.00028458577974976846, 0.00025737043427135747, 0.00023235207611628806, 0.00020974638666866152, 0.0001890904874322071, 0.00017277267988811344, 0.00016441666021079415, 0.0001495644589849818, 0.00013576765399663939, 0.000136849257057423, 0.00012387948019472404, 0.00011412070801656817, 0.00010351442262851985, 9.470009126309579e-05, 0.00012850196777099715, 0.00014677008661503042, 0.00016311639360698933, 0.00017270473849595358, 0.00017444871221863472, 0.00017125391779620731, 0.00016774760327840324, 0.00016224890053320121, 0.0001547202785507142, 0.0001488562629138036, 0.0001403729294827785, 0.0001315074366128415, 0.00012095001245497164, 0.00011184775710650053, 0.00010146899073255262, 9.373321174667307e-05, 8.747056813132827e-05, 8.14810450923491e-05, 7.536177179735023e-05, 6.919211261969228e-05, 6.379859301793404e-05, 5.874440880273433e-05, 5.359254899032868e-05, 4.893601600785504e-05, 4.450646747485786e-05, 4.068975267480533e-05, 3.724948257283179e-05, 3.4033785499609304e-05, 3.080893852884798e-05, 2.7844030800726054e-05, 2.5232994947497994e-05, 2.2802466165812636e-05, 2.084032095218723e-05, 1.8757283003676385e-05, 1.691213123276438e-05, 1.523071363439418e-05, 1.3806085676754214e-05, 1.247051764963163e-05, 1.1229558156075414e-05, 1.0190687415379575e-05, 9.184727198235152e-06, 8.2768723831704e-06, 7.490232787130698e-06, 6.79051641231698e-06, 6.138107278681613e-06, 5.5254492203781256e-06, 4.977841540045299e-06, 4.480228780108062e-06, 4.0345488177422825e-06, 3.6776950344471884e-06, 3.334783623801181e-06, 3.02144031658802e-06, 2.719774348308166e-06, 2.4890615111916533e-06, 2.241260942764906e-06, 2.0219838668127184e-06, 1.8233355994498823e-06, 1.6490609184590553e-06, 1.5460541322217727e-06, 1.3928716792081532e-06, 1.2657463316846803e-06, 1.1417038332778396e-06, 1.0473878677438555e-06, 1.0423323139913313e-06, 9.401635107966196e-07, 8.663717219687731e-07, 7.881498203241499e-07, 7.281491460504087e-07, 6.558648723213859e-07, 5.904422375959061e-07, 5.318825476194663e-07, 4.873531727950741e-07, 4.9905616967603e-07, 4.5060813805641743e-07, 4.055759373059321e-07, 3.66735227599967e-07, 3.314523808999188e-07, 3.0906420413719466e-07, 2.856698737796777e-07, 3.40165501074405e-07, 3.061581780594798e-07, 2.9710406665634176e-07, 2.8359412258643096e-07, 2.562543272480751e-07, 2.323441269546926e-07, 2.091101403214991e-07, 2.3622107208929496e-07, 2.1970096814941188e-07, 1.9773765141198274e-07, 1.9874048607956893e-07, 2.0653226059924977e-07, 1.8722927439565338e-07, 1.6851894977363625e-07, 1.730935329071546e-07, 1.5874940902797484e-07, 1.5020690025778595e-07, 1.4709498549654141e-07, 1.350231618636399e-07, 1.224273520354452e-07, 1.2025265358909934e-07, 1.1742196357372389e-07, 1.0814771260401431e-07, 9.933197710762038e-08, 1.0227687733119956e-07, 9.210893854828508e-08, 1.0739958260897677e-07, 9.762145673471333e-08, 9.202384162923766e-08, 8.583238841569591e-08], "accuracy_test": 0.9031210140306122, "start": "2016-02-03 14:06:34.848000", "learning_rate_per_epoch": [0.0019072178984060884, 0.0019072178984060884, 0.0019072178984060884, 0.0019072178984060884, 0.0019072178984060884, 0.0019072178984060884, 0.0019072178984060884, 0.0019072178984060884, 0.0019072178984060884, 0.0019072178984060884, 0.0019072178984060884, 0.0019072178984060884, 0.0019072178984060884, 0.0019072178984060884, 0.0019072178984060884, 0.0019072178984060884, 0.0019072178984060884, 0.0019072178984060884, 0.0019072178984060884, 0.0019072178984060884, 0.0019072178984060884, 0.0019072178984060884, 0.0019072178984060884, 0.0019072178984060884, 0.0019072178984060884, 0.0019072178984060884, 0.0019072178984060884, 0.0019072178984060884, 0.0019072178984060884, 0.0019072178984060884, 0.0019072178984060884, 0.0019072178984060884, 0.0019072178984060884, 0.0019072178984060884, 0.0019072178984060884, 0.0019072178984060884, 0.0019072178984060884, 0.0019072178984060884, 0.0019072178984060884, 0.0019072178984060884, 0.0019072178984060884, 0.0019072178984060884, 0.0019072178984060884, 0.0019072178984060884, 0.0019072178984060884, 0.0019072178984060884, 0.0019072178984060884, 0.0019072178984060884, 0.0019072178984060884, 0.0019072178984060884, 0.0019072178984060884, 0.0019072178984060884, 0.0019072178984060884, 0.0019072178984060884, 0.0019072178984060884, 0.0019072178984060884, 0.0019072178984060884, 0.0019072178984060884, 0.0019072178984060884, 0.0019072178984060884, 0.0019072178984060884, 0.0019072178984060884, 0.0019072178984060884, 0.0019072178984060884, 0.0019072178984060884, 0.0019072178984060884, 0.0019072178984060884, 0.0019072178984060884, 0.0019072178984060884, 0.0019072178984060884, 0.0019072178984060884, 0.0019072178984060884, 0.0019072178984060884, 0.0019072178984060884, 0.0019072178984060884, 0.0019072178984060884, 0.0019072178984060884, 0.0019072178984060884, 0.0019072178984060884, 0.0019072178984060884, 0.0019072178984060884, 0.0019072178984060884, 0.0019072178984060884, 0.0019072178984060884, 0.0019072178984060884, 0.0019072178984060884, 0.0019072178984060884, 0.0019072178984060884, 0.0019072178984060884, 0.0019072178984060884, 0.0019072178984060884, 0.00019072179566137493, 0.00019072179566137493, 0.00019072179566137493, 0.00019072179566137493, 0.00019072179566137493, 0.00019072179566137493, 0.00019072179566137493, 0.00019072179566137493, 0.00019072179566137493, 0.00019072179566137493, 0.00019072179566137493, 0.00019072179566137493, 0.00019072179566137493, 0.00019072179566137493, 0.00019072179566137493, 0.00019072179566137493, 1.9072180293733254e-05, 1.9072180066359579e-06, 1.9072180634793767e-07, 1.90721802795224e-08, 1.907217939134398e-09, 1.9072179946455492e-10, 1.9072179599510797e-11, 1.907217873214906e-12, 1.907217873214906e-13, 1.907217839333588e-14, 1.9072179240368827e-15, 1.907217897567103e-16, 1.9072179306543276e-17, 1.9072179306543276e-18, 1.9072179306543276e-19, 1.907217898342585e-20, 1.9072179791219416e-21, 1.9072180296090396e-22, 1.9072179665001671e-23, 1.907217966500167e-24, 1.9072179171963606e-25, 1.9072179171963606e-26, 1.9072179942335583e-27, 1.907217970159434e-28, 1.9072180303447448e-29, 1.9072179551131064e-30, 1.9072179551131064e-31, 1.907217984500465e-32, 1.9072179110320682e-33, 1.9072178880731942e-34, 1.9072179167717867e-35, 1.907217880898546e-36, 1.907217880898546e-37, 1.9072179369504846e-38, 1.9072176566907917e-39, 1.907223261884649e-40, 1.907167209946076e-41, 1.907167209946076e-42, 1.9057659114817512e-43, 1.961817850054744e-44, 1.401298464324817e-45, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "accuracy_train_first": 0.30420696924603174, "accuracy_train_last": 0.9993489583333334, "batch_size_eval": 1024, "accuracy_train_std": [0.015362875189196604, 0.014801819604948536, 0.01543254680553971, 0.018064722306356064, 0.018907084576245552, 0.019318664526431353, 0.02100221508512953, 0.02148694470602053, 0.02037259273035489, 0.023388302302443993, 0.01859619905363091, 0.020781637567091903, 0.02010260939065994, 0.020421640714012406, 0.016601066735235273, 0.020897616701123924, 0.019336501152394804, 0.016672278038671867, 0.01826967564778714, 0.01727713892785975, 0.0158645629130062, 0.01675342091111677, 0.01602706000715629, 0.0149247115732336, 0.014972147448265884, 0.015603059722749127, 0.01515627081501708, 0.013973122987486276, 0.01484956663480285, 0.015348088910705443, 0.01304979359445667, 0.013833921898892645, 0.013309789473834079, 0.014212659229488694, 0.010741292570666144, 0.013663315183805438, 0.010506235544233876, 0.011486447531662254, 0.011899447098044937, 0.0126893076942984, 0.009266578599408406, 0.0132426041021297, 0.008934014723037948, 0.0102032572839322, 0.009442485095523577, 0.01127786128710444, 0.009922830158228809, 0.009585958110571438, 0.008814671891089185, 0.008561704103990896, 0.010113968304726738, 0.008267502442161161, 0.011284113405519753, 0.0098212187115216, 0.008141942973747278, 0.00770614500872915, 0.009249563539895055, 0.00735721305663391, 0.008247629016462163, 0.008019619623227276, 0.008182406548778725, 0.009397265217783599, 0.006437697280645112, 0.007198091420845465, 0.00801630077473464, 0.008523006766521048, 0.007346624011791165, 0.006721607721838204, 0.009033328434803103, 0.0065256979430269695, 0.009384280856314275, 0.007689460814464659, 0.007760425806747688, 0.006902520578600994, 0.008400387842451399, 0.007004874988216222, 0.006613873723623463, 0.005888115346973641, 0.007818421932808394, 0.0070197101031154435, 0.006500361023790691, 0.007003600348572197, 0.007179139344429318, 0.005570126798684484, 0.005337166249031499, 0.00664842938237348, 0.006676432072879757, 0.004398712068632623, 0.007049215168633828, 0.00670532511502005, 0.005810284976114106, 0.006108788900018153, 0.002011085711139203, 0.001525411036724026, 0.0014960665868810836, 0.001231451141821555, 0.0013966385247099337, 0.0012332059715565588, 0.0009952071502526366, 0.0011766723334151782, 0.0010116401561480232, 0.0010452830539894448, 0.0011383734807378764, 0.001243682953898468, 0.00085114421585394, 0.0008031024943810707, 0.0011131611982501696, 0.0010272843207949462, 0.0008860006921157762, 0.000959810613864876, 0.0009544445569091236, 0.0008774163999308598, 0.0008957105786950438, 0.0009112685054996943, 0.0008740209323480516, 0.0009112685054996942, 0.0009487632888347825, 0.0008740209323480516, 0.0008911722422217633, 0.0009487632888347825, 0.0008957105786950438, 0.0009487632888347825, 0.0009544445569091236, 0.0008996252846983399, 0.0008774163999308598, 0.0008740209323480516, 0.0008740209323480516, 0.0009687810014880792, 0.0009487632888347825, 0.0009687810014880792, 0.0009487632888347825, 0.0008996252846983399, 0.0008911722422217635, 0.000940751875192704, 0.0008699910218503398, 0.0008996252846983399, 0.0008801847670548597, 0.000940751875192704, 0.0008699910218503398, 0.0009162974560657567, 0.000959810613864876, 0.000959810613864876, 0.0008911722422217635, 0.0008740209323480516, 0.0008911722422217633, 0.0008699910218503398, 0.0009724015841855693, 0.0008801847670548597, 0.0008911722422217633, 0.0009487632888347825, 0.0008774163999308598, 0.0008774163999308598, 0.0009207119546699836, 0.0009487632888347825, 0.000940751875192704, 0.0009450521499675211, 0.0008823319364309095, 0.0009487632888347825, 0.0008823319364309095, 0.000959810613864876, 0.000959810613864876, 0.0008774163999308598, 0.0009450521499675211, 0.0008860006921157762, 0.0009487632888347825, 0.0008911722422217633, 0.0009487632888347825, 0.0009544445569091236, 0.0009645865193773967, 0.0009245208032027084, 0.000940751875192704, 0.0009687810014880792, 0.0009724015841855693, 0.0009162974560657566, 0.0009162974560657566, 0.0008740209323480516, 0.0008996252846983399, 0.0008957105786950438, 0.0009450521499675211, 0.0008996252846983399, 0.000959810613864876, 0.0009112685054996943, 0.0008996252846983399, 0.0009450521499675211, 0.0009450521499675211, 0.0008774163999308598, 0.0008801847670548597, 0.0009450521499675211, 0.0008801847670548597, 0.0008801847670548597, 0.0008740209323480516, 0.0008801847670548597, 0.0008774163999308598, 0.0008699910218503398, 0.0009487632888347825, 0.0009407518751927041, 0.0009645865193773967, 0.0009303501220118376, 0.0008699910218503398, 0.0009358543432616307, 0.0009450521499675211], "accuracy_test_std": 0.0082816703809417, "error_valid": [0.6962567065135542, 0.591588031814759, 0.5043224656438253, 0.3916471550263554, 0.3466532144201807, 0.3195727244917168, 0.1987172322100903, 0.22287832972515065, 0.19372264448418675, 0.18533067818147586, 0.17368252306099397, 0.17633718467620485, 0.19548310429216864, 0.16388601280120485, 0.15345856080572284, 0.15910468044051207, 0.15842226327183728, 0.15426157756024095, 0.16290945030120485, 0.15219667733433728, 0.1628682699548193, 0.1617490469691265, 0.14720208960843373, 0.1448209831513554, 0.14450624764683728, 0.14245164250753017, 0.1436002800263554, 0.13682611304593373, 0.1440694418298193, 0.13946018448795183, 0.1332243034638554, 0.15038621282003017, 0.1321565559111446, 0.13598191594503017, 0.12515736775225905, 0.13543186417545183, 0.12135259789156627, 0.13487298804593373, 0.13067112198795183, 0.12491322712725905, 0.1251985480986446, 0.1618711172816265, 0.12310276261295183, 0.11952154320406627, 0.11608298428087349, 0.12508677287274095, 0.12946071394954817, 0.12566623917545183, 0.11711102221385539, 0.12467938158885539, 0.12423228068524095, 0.12099668204066272, 0.12259389118975905, 0.11975538874246983, 0.11776255412274095, 0.12111875235316272, 0.12408962019954817, 0.11629771037274095, 0.11834202042545183, 0.11868764118975905, 0.12305275790662651, 0.1260530402861446, 0.11412985928087349, 0.11437399990587349, 0.11968479386295183, 0.12408962019954817, 0.11194288874246983, 0.10947059723268071, 0.12326601327183728, 0.11333713761295183, 0.12066135636295183, 0.11499464655496983, 0.10897202089608427, 0.11241057981927716, 0.11573736351656627, 0.1084631494728916, 0.11524908226656627, 0.11260471573795183, 0.1345273672816265, 0.11437399990587349, 0.11489316641566272, 0.11479168627635539, 0.11905385212725905, 0.11199436417545183, 0.10686741105045183, 0.12003041462725905, 0.11983774943524095, 0.10384506777108427, 0.11251205995858427, 0.10964414297816272, 0.11151637801204817, 0.11834202042545183, 0.09269548898719882, 0.09383530214608427, 0.09200424745858427, 0.09174981174698793, 0.09248223362198793, 0.09298080995858427, 0.09200424745858427, 0.09188217714608427, 0.09212631777108427, 0.0906408838478916, 0.09150567112198793, 0.09151596620858427, 0.09297051487198793, 0.09203513271837349, 0.09423239834337349, 0.09174981174698793, 0.09052910862198793, 0.09028496799698793, 0.0905188135353916, 0.09089531955948793, 0.09028496799698793, 0.0901526025978916, 0.09077324924698793, 0.09052910862198793, 0.09077324924698793, 0.09016289768448793, 0.0899084619728916, 0.0899084619728916, 0.0906408838478916, 0.09077324924698793, 0.09040703830948793, 0.0906408838478916, 0.08967461643448793, 0.09126153049698793, 0.09077324924698793, 0.09162774143448793, 0.09028496799698793, 0.0905188135353916, 0.09089531955948793, 0.09016289768448793, 0.09065117893448793, 0.09065117893448793, 0.09028496799698793, 0.0901526025978916, 0.0902746729103916, 0.09065117893448793, 0.0905188135353916, 0.09077324924698793, 0.09089531955948793, 0.0900305322853916, 0.0901526025978916, 0.0901526025978916, 0.09065117893448793, 0.0899084619728916, 0.09040703830948793, 0.0902746729103916, 0.09028496799698793, 0.0907629541603916, 0.0896643213478916, 0.09028496799698793, 0.0900305322853916, 0.09052910862198793, 0.0899084619728916, 0.09138360080948793, 0.09028496799698793, 0.09089531955948793, 0.09016289768448793, 0.09089531955948793, 0.09040703830948793, 0.0905188135353916, 0.09040703830948793, 0.09016289768448793, 0.09126153049698793, 0.0903967432228916, 0.09052910862198793, 0.09065117893448793, 0.09065117893448793, 0.0908850244728916, 0.09028496799698793, 0.09150567112198793, 0.09065117893448793, 0.0901526025978916, 0.09101738987198793, 0.09052910862198793, 0.0907629541603916, 0.0906408838478916, 0.0899084619728916, 0.09028496799698793, 0.09052910862198793, 0.09101738987198793, 0.0900305322853916, 0.09040703830948793, 0.09052910862198793, 0.0900305322853916, 0.09065117893448793, 0.09077324924698793, 0.0901526025978916, 0.09065117893448793, 0.0903967432228916, 0.0901526025978916, 0.09077324924698793, 0.09065117893448793, 0.09065117893448793, 0.09089531955948793, 0.09052910862198793, 0.0900305322853916, 0.0903967432228916, 0.0902746729103916, 0.09065117893448793], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.040734403123478584, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 64, "valid_ratio": 0.15, "learning_rate": 0.0019072179247629924, "optimization": "rmsprop", "nb_data_augmentation": 1, "learning_rate_decay_method": "discrete", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 3.193716834810887e-06, "rotation_range": [0, 0], "momentum": 0.985470225904203}, "accuracy_valid_max": 0.9103356786521084, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            print(X_train.min(), X_train.max())\n            print(X_valid.min(), X_valid.max())\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n\n    # rescaling to [-1, 1]\n    X_min = X_train.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X_train.max(axis=(0, 2, 3))[None, :, None, None]\n    def preprocess(a):\n        return (a / 255.) * 2 - 1\n        # return 2 * ((a - X_min) / (X_max - X_min)) - 1\n    X_train = preprocess(X_train)\n    X_valid = preprocess(X_valid)\n\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = preprocess(X_test)\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.9093488210655121, "accuracy_valid_std": [0.01200660852653314, 0.016256003675614455, 0.015733015145771675, 0.012599588056632431, 0.009358836849469851, 0.011837841401186794, 0.012368673639666865, 0.010847322795336956, 0.012594744511389205, 0.009643512294950196, 0.007870500090071815, 0.010765447078821036, 0.006537577020094248, 0.013281918689535209, 0.011151999952841114, 0.011548590644646122, 0.012831027909184662, 0.012745899306168068, 0.010305727477404011, 0.009454418489040363, 0.008372357934434822, 0.008486675344333956, 0.006919318621726655, 0.012171631360085578, 0.007370649170173953, 0.004185276695837901, 0.01242094960004792, 0.006752030157205414, 0.006895496243994216, 0.00795123744467756, 0.008870110364810202, 0.00653870168929103, 0.006603280984042853, 0.00898414598571883, 0.0064616544289268244, 0.008859971710026098, 0.0076665040608882165, 0.009492983476008954, 0.006277662313433122, 0.0066448776382934715, 0.0049564018572624555, 0.011068977253096825, 0.004860586724668622, 0.0045787873439177736, 0.006568473867065629, 0.0068914627573044675, 0.00569912310503636, 0.005916618912092912, 0.004412552538235869, 0.007046680399152502, 0.007599897424941025, 0.006291968031144594, 0.009685292375232172, 0.007603997037143869, 0.007556983274224617, 0.004945510995141211, 0.005550384251846583, 0.01062608782846175, 0.005365789547940556, 0.0071028198390682825, 0.010507254967362423, 0.007405976642264146, 0.004830437534311422, 0.006746030472976768, 0.004220007639868315, 0.004262696406121803, 0.004251330208647781, 0.00634115238118239, 0.009406949695296532, 0.005495496844492381, 0.004003795565808713, 0.005179156560362988, 0.007271619440357769, 0.006953287046824343, 0.006861423300080182, 0.008301508763977353, 0.003767191042656811, 0.008079002331446308, 0.0067877239495289605, 0.005094958651059204, 0.003931432544685894, 0.005100500189176762, 0.008081488543696932, 0.00542051647857836, 0.008729734794596385, 0.002746674466932201, 0.009629857692226, 0.007112811210848065, 0.008972747844959375, 0.004851983857729971, 0.008395468406749913, 0.004136325313223267, 0.007114290068663043, 0.007897656441466313, 0.005488745380117415, 0.005885641542367185, 0.0056381813631058006, 0.005407815118546917, 0.005011939313941007, 0.006385270510967606, 0.005317203898822964, 0.005598764894477987, 0.0055231400282562656, 0.006218801193389383, 0.005535608042102091, 0.007581552976987498, 0.006068194659272103, 0.004588127870542549, 0.005493153627615554, 0.005320151565936728, 0.004827052823335814, 0.004616623625537775, 0.004726896255155169, 0.004192980567779369, 0.0047064944317313255, 0.004516588871418546, 0.004977317973169401, 0.0050414877290460135, 0.004203842091416405, 0.004451747470542137, 0.0048207817472051126, 0.0055866802165308645, 0.004726535422252858, 0.005662280819682453, 0.004706589794769762, 0.004634845661576322, 0.005119004364194714, 0.004466726154276029, 0.005518131920899779, 0.004047999526349891, 0.005176566528038588, 0.005181419689306291, 0.005637433021893254, 0.004665519344517487, 0.0047520486935743326, 0.004332802665398369, 0.004444744977584685, 0.00528828458110071, 0.004441189060685305, 0.005211322288733521, 0.004693449630834258, 0.004326022648209404, 0.005275707056228823, 0.004277422328819086, 0.004716344761811843, 0.0045576013272207744, 0.004996272757559198, 0.004576882215140792, 0.004777068700071528, 0.004736494092618012, 0.004632383683745046, 0.004726896255155169, 0.005534925555768463, 0.005471409160123592, 0.0045576013272207744, 0.005005463203737227, 0.005561170475237764, 0.005107013342640177, 0.00489755929494602, 0.005176566528038588, 0.005137436417371758, 0.004727236469954513, 0.005137436417371758, 0.005111932536666169, 0.00468600378412523, 0.004830231768970805, 0.0057683774412659785, 0.005885717800366516, 0.00517434642396503, 0.005181141027078046, 0.004826719644168927, 0.004605078846965129, 0.00517434642396503, 0.00470223766357094, 0.004651626396965823, 0.004620957622596003, 0.004582997361421448, 0.005909521726008167, 0.004146739824411598, 0.004996614111287477, 0.004798143328782399, 0.004625927967213462, 0.004695991062623798, 0.004650256053219685, 0.0052942392810177985, 0.00474649019997363, 0.005509096078461796, 0.006018121467942481, 0.0041068029298312715, 0.005865428837386887, 0.00444464399648064, 0.00441457072370439, 0.0047064944317313255, 0.005010482098709541, 0.004456424813822666, 0.004125733786376118, 0.0048229242148970786, 0.004771539353033491, 0.004755615964801, 0.0038713545864119746, 0.004456424813822666], "accuracy_valid": [0.3037432934864458, 0.40841196818524095, 0.4956775343561747, 0.6083528449736446, 0.6533467855798193, 0.6804272755082832, 0.8012827677899097, 0.7771216702748494, 0.8062773555158133, 0.8146693218185241, 0.826317476939006, 0.8236628153237951, 0.8045168957078314, 0.8361139871987951, 0.8465414391942772, 0.8408953195594879, 0.8415777367281627, 0.845738422439759, 0.8370905496987951, 0.8478033226656627, 0.8371317300451807, 0.8382509530308735, 0.8527979103915663, 0.8551790168486446, 0.8554937523531627, 0.8575483574924698, 0.8563997199736446, 0.8631738869540663, 0.8559305581701807, 0.8605398155120482, 0.8667756965361446, 0.8496137871799698, 0.8678434440888554, 0.8640180840549698, 0.874842632247741, 0.8645681358245482, 0.8786474021084337, 0.8651270119540663, 0.8693288780120482, 0.875086772872741, 0.8748014519013554, 0.8381288827183735, 0.8768972373870482, 0.8804784567959337, 0.8839170157191265, 0.874913227127259, 0.8705392860504518, 0.8743337608245482, 0.8828889777861446, 0.8753206184111446, 0.875767719314759, 0.8790033179593373, 0.877406108810241, 0.8802446112575302, 0.882237445877259, 0.8788812476468373, 0.8759103798004518, 0.883702289627259, 0.8816579795745482, 0.881312358810241, 0.8769472420933735, 0.8739469597138554, 0.8858701407191265, 0.8856260000941265, 0.8803152061370482, 0.8759103798004518, 0.8880571112575302, 0.8905294027673193, 0.8767339867281627, 0.8866628623870482, 0.8793386436370482, 0.8850053534450302, 0.8910279791039157, 0.8875894201807228, 0.8842626364834337, 0.8915368505271084, 0.8847509177334337, 0.8873952842620482, 0.8654726327183735, 0.8856260000941265, 0.8851068335843373, 0.8852083137236446, 0.880946147872741, 0.8880056358245482, 0.8931325889495482, 0.879969585372741, 0.880162250564759, 0.8961549322289157, 0.8874879400414157, 0.8903558570218373, 0.8884836219879518, 0.8816579795745482, 0.9073045110128012, 0.9061646978539157, 0.9079957525414157, 0.9082501882530121, 0.9075177663780121, 0.9070191900414157, 0.9079957525414157, 0.9081178228539157, 0.9078736822289157, 0.9093591161521084, 0.9084943288780121, 0.9084840337914157, 0.9070294851280121, 0.9079648672816265, 0.9057676016566265, 0.9082501882530121, 0.9094708913780121, 0.9097150320030121, 0.9094811864646084, 0.9091046804405121, 0.9097150320030121, 0.9098473974021084, 0.9092267507530121, 0.9094708913780121, 0.9092267507530121, 0.9098371023155121, 0.9100915380271084, 0.9100915380271084, 0.9093591161521084, 0.9092267507530121, 0.9095929616905121, 0.9093591161521084, 0.9103253835655121, 0.9087384695030121, 0.9092267507530121, 0.9083722585655121, 0.9097150320030121, 0.9094811864646084, 0.9091046804405121, 0.9098371023155121, 0.9093488210655121, 0.9093488210655121, 0.9097150320030121, 0.9098473974021084, 0.9097253270896084, 0.9093488210655121, 0.9094811864646084, 0.9092267507530121, 0.9091046804405121, 0.9099694677146084, 0.9098473974021084, 0.9098473974021084, 0.9093488210655121, 0.9100915380271084, 0.9095929616905121, 0.9097253270896084, 0.9097150320030121, 0.9092370458396084, 0.9103356786521084, 0.9097150320030121, 0.9099694677146084, 0.9094708913780121, 0.9100915380271084, 0.9086163991905121, 0.9097150320030121, 0.9091046804405121, 0.9098371023155121, 0.9091046804405121, 0.9095929616905121, 0.9094811864646084, 0.9095929616905121, 0.9098371023155121, 0.9087384695030121, 0.9096032567771084, 0.9094708913780121, 0.9093488210655121, 0.9093488210655121, 0.9091149755271084, 0.9097150320030121, 0.9084943288780121, 0.9093488210655121, 0.9098473974021084, 0.9089826101280121, 0.9094708913780121, 0.9092370458396084, 0.9093591161521084, 0.9100915380271084, 0.9097150320030121, 0.9094708913780121, 0.9089826101280121, 0.9099694677146084, 0.9095929616905121, 0.9094708913780121, 0.9099694677146084, 0.9093488210655121, 0.9092267507530121, 0.9098473974021084, 0.9093488210655121, 0.9096032567771084, 0.9098473974021084, 0.9092267507530121, 0.9093488210655121, 0.9093488210655121, 0.9091046804405121, 0.9094708913780121, 0.9099694677146084, 0.9096032567771084, 0.9097253270896084, 0.9093488210655121], "seed": 345800052, "model": "residualv3", "loss_std": [0.37007564306259155, 0.14855244755744934, 0.1344335824251175, 0.13147670030593872, 0.12496928125619888, 0.12405720353126526, 0.11765755712985992, 0.1134609580039978, 0.11053669452667236, 0.10956111550331116, 0.10496790707111359, 0.10398846864700317, 0.09535252302885056, 0.09834896773099899, 0.0932207703590393, 0.09035686403512955, 0.08786359429359436, 0.0869298055768013, 0.08247563987970352, 0.08198194205760956, 0.07925587147474289, 0.07538606971502304, 0.07294902950525284, 0.07289233803749084, 0.0710328221321106, 0.07099996507167816, 0.06788579374551773, 0.06907106190919876, 0.06625880300998688, 0.06552137434482574, 0.06565424799919128, 0.06140691414475441, 0.06258708238601685, 0.06233840435743332, 0.05946829542517662, 0.058135926723480225, 0.057856250554323196, 0.05437561124563217, 0.05586962029337883, 0.05477338656783104, 0.054591450840234756, 0.05237280949950218, 0.05357186496257782, 0.05208573490381241, 0.05115080997347832, 0.050707388669252396, 0.05156361311674118, 0.05253671854734421, 0.05091041326522827, 0.04969483241438866, 0.0500919483602047, 0.04744575172662735, 0.04893837496638298, 0.04830057546496391, 0.0458388514816761, 0.04896106570959091, 0.04770376905798912, 0.047827061265707016, 0.049208950251340866, 0.044866494834423065, 0.047241996973752975, 0.04533708468079567, 0.04794770106673241, 0.04601023346185684, 0.042873136699199677, 0.046014655381441116, 0.04518326371908188, 0.04270705208182335, 0.04238211363554001, 0.043630458414554596, 0.04153910279273987, 0.04361848905682564, 0.04095900431275368, 0.041064828634262085, 0.04269980639219284, 0.040935128927230835, 0.04169658198952675, 0.04107542335987091, 0.04193037748336792, 0.043040551245212555, 0.041866376996040344, 0.040734630078077316, 0.04207117483019829, 0.040902696549892426, 0.04060795530676842, 0.04157576337456703, 0.03920265659689903, 0.03986712172627449, 0.03917587175965309, 0.03935249149799347, 0.03758372366428375, 0.04220352694392204, 0.03342164307832718, 0.023396693170070648, 0.020569734275341034, 0.019949469715356827, 0.01992538943886757, 0.020032048225402832, 0.01841004192829132, 0.018791107460856438, 0.017377063632011414, 0.018489837646484375, 0.016778450459241867, 0.017534999176859856, 0.017475994303822517, 0.016792599111795425, 0.014960612170398235, 0.015349836088716984, 0.01590072363615036, 0.013986444100737572, 0.014415444806218147, 0.01352723315358162, 0.014879411086440086, 0.01618843525648117, 0.015944182872772217, 0.015246299095451832, 0.015129966661334038, 0.016036000102758408, 0.014659025706350803, 0.016065247356891632, 0.013998202979564667, 0.01551010087132454, 0.01523131225258112, 0.01621909625828266, 0.017087938264012337, 0.01606191322207451, 0.016749175265431404, 0.015370680019259453, 0.014668231830000877, 0.014554694294929504, 0.016244394704699516, 0.015349393710494041, 0.01523316465318203, 0.015675421804189682, 0.015901153907179832, 0.015404846519231796, 0.015908490866422653, 0.014816048555076122, 0.01429598219692707, 0.0154333570972085, 0.016114426776766777, 0.015630822628736496, 0.015357532538473606, 0.016104329377412796, 0.015377036295831203, 0.015957634896039963, 0.015264862217009068, 0.01523522101342678, 0.015143511816859245, 0.015144855715334415, 0.015253651887178421, 0.015561309643089771, 0.014860713854432106, 0.01596762426197529, 0.015897490084171295, 0.016471371054649353, 0.016310978680849075, 0.016045089811086655, 0.01724640280008316, 0.015587729401886463, 0.015448309481143951, 0.015055807307362556, 0.015408435836434364, 0.014849474653601646, 0.01612281985580921, 0.015048535540699959, 0.01618027500808239, 0.015682244673371315, 0.01621776446700096, 0.01555607095360756, 0.01515010092407465, 0.01502975169569254, 0.01568363420665264, 0.015171738341450691, 0.015787549316883087, 0.01555608119815588, 0.015914730727672577, 0.015086859464645386, 0.014123122207820415, 0.015959830954670906, 0.014234744943678379, 0.01620206981897354, 0.015265807509422302, 0.015800336375832558, 0.015930626541376114, 0.015255345962941647, 0.015285083092749119, 0.015838952735066414, 0.01493991818279028, 0.016100935637950897, 0.01499143522232771, 0.015863150358200073, 0.01582588627934456, 0.016334187239408493, 0.015258719213306904, 0.014800237491726875, 0.015431427396833897, 0.015257521532475948, 0.015890082344412804, 0.016028456389904022, 0.015011169947683811]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:38 2016", "state": "available"}], "summary": "31033ac0246e7292b9afa2b27aabc2db"}