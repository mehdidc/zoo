{"content": {"hp_model": {"f0": 64, "f1": 64, "f2": 32, "f3": 32, "nonlin": "rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.4945745468139648, 1.0734171867370605, 0.8709400296211243, 0.7510899305343628, 0.6662005186080933, 0.6001984477043152, 0.5437301397323608, 0.49448198080062866, 0.449959933757782, 0.4081277847290039, 0.36841535568237305, 0.3293076753616333, 0.29182666540145874, 0.25645899772644043, 0.22202642261981964, 0.19167299568653107, 0.1670304834842682, 0.14693613350391388, 0.1281476467847824, 0.11427626013755798, 0.10078967362642288, 0.09093199670314789, 0.07953938096761703, 0.07014891505241394, 0.0659598857164383, 0.06104128435254097, 0.05277177318930626, 0.04987258091568947, 0.04632193595170975, 0.04162292554974556, 0.03853818401694298, 0.03645479679107666, 0.03517129272222519, 0.02919917367398739, 0.029776455834507942, 0.02590593509376049, 0.024705231189727783, 0.02139337733387947, 0.021169481799006462, 0.018512431532144547, 0.017985094338655472, 0.015538142062723637, 0.016262443736195564, 0.015158666297793388, 0.013559115119278431, 0.011925718747079372, 0.010751280933618546, 0.00994187779724598, 0.009838496334850788, 0.008527659811079502, 0.008208494633436203, 0.0078003983944654465, 0.005785051733255386, 0.005038302857428789, 0.003891074564307928, 0.0035711280070245266, 0.0031040131580084562, 0.002491768915206194, 0.001794834970496595, 0.0013506935210898519, 0.001116061001084745, 0.0009640646167099476, 0.0009301795507781208, 0.0009217409533448517, 0.0009174339938908815, 0.0009144381037913263, 0.0009121237089857459, 0.0009102319017983973, 0.0009086361969821155, 0.0009072610409930348, 0.0009060499141924083, 0.0009049784857779741, 0.000904003216419369, 0.0009031257359310985, 0.000902312807738781, 0.0009015658870339394, 0.0009008715278469026, 0.0009002292645163834, 0.0008996247197501361, 0.0008990593487396836, 0.000898524362128228, 0.0008980185957625508, 0.0008975444361567497, 0.0008970851195044816, 0.0008966523455455899, 0.0008962370920926332, 0.0008958414546214044, 0.0008954622899182141, 0.000895094417501241, 0.0008947441237978637, 0.0008944070432335138, 0.0008940801490098238, 0.0008937690872699022, 0.0008934647194109857, 0.0008931723423302174, 0.000892886717338115, 0.0008926108712330461, 0.0008923421264626086, 0.000892082171048969, 0.0008918290841393173, 0.0008915820508264005, 0.0008913410711102188, 0.0008911072509363294, 0.0008908776217140257, 0.0008906549774110317, 0.0008904372225515544, 0.0008902236586436629, 0.0008900148677639663, 0.0008898114319890738, 0.0008896112558431923, 0.000889415736310184, 0.0008892234181985259, 0.0008890338940545917, 0.0008888483280315995, 0.0008886671857908368, 0.0008884886628948152, 0.0008883132832124829, 0.0008881409885361791, 0.00088797154603526, 0.0008878051885403693, 0.0008876421488821507, 0.0008874815539456904, 0.000887323752976954, 0.0008871688041836023, 0.0008870158344507217, 0.0008868654258549213, 0.0008867161232046783, 0.0008865697891451418, 0.00088642502669245, 0.0008862814283929765, 0.0008861405658535659, 0.0008860011002980173, 0.000885864021256566, 0.0008857285138219595, 0.0008855948108248413], "moving_avg_accuracy_train": [0.06044575806339977, 0.12298860208448686, 0.185751403380514, 0.24562000669787049, 0.30165244023466337, 0.3538506000271808, 0.4022374432867366, 0.44687613306082224, 0.4879294996193118, 0.5253726780719248, 0.5594457794911535, 0.5905648305910415, 0.619092809914274, 0.6451958907837732, 0.6691350560889322, 0.6911592494695185, 0.7110855110131997, 0.7299166898918077, 0.747190163569432, 0.7634571581078837, 0.7786065886841386, 0.7917227482623378, 0.804585018298138, 0.8166889061589389, 0.8271475663574599, 0.8376018469587017, 0.8473944932486658, 0.8564381727882326, 0.8646309988452806, 0.872841487721596, 0.879663591400756, 0.8862174693465517, 0.8923251147441685, 0.8985777050139377, 0.9037166829090925, 0.9089484826587424, 0.9136548854310741, 0.9183160780606779, 0.9221696068940527, 0.9262073001071484, 0.9295529415953723, 0.9332592023800026, 0.9363760928539809, 0.9395046341603602, 0.9421599221170632, 0.9446636496185814, 0.9473493018044253, 0.9496059895526465, 0.9511954104963298, 0.9531907923599029, 0.9552074891252046, 0.9571502912520437, 0.9590987759638179, 0.9605967539818232, 0.9623702561860772, 0.9640479686223329, 0.9654160036399446, 0.9669215306176815, 0.9685531615571592, 0.9703564508312607, 0.972223551802952, 0.9740410904084171, 0.9757094272366691, 0.9772295315725722, 0.9786092512189325, 0.9798672749423234, 0.9809971711445657, 0.9820257034706316, 0.982974634052186, 0.9838402973196325, 0.9846286948555726, 0.985335927489109, 0.9859770871569109, 0.9865611063043612, 0.987089048685876, 0.9875665219780487, 0.9880008982386234, 0.9883964871707596, 0.9887594926561107, 0.9890908478905458, 0.9893960430479659, 0.9896683935408346, 0.9899135089844163, 0.9901410883300685, 0.9903435845923458, 0.9905304815260145, 0.9907010139151259, 0.9908568182141356, 0.9909970420832445, 0.9911209544654516, 0.9912324756094381, 0.9913305194902162, 0.9914187589829166, 0.991498174526347, 0.9915696485154343, 0.9916363002544224, 0.9916962868195117, 0.9917502747280922, 0.9917988638458146, 0.9918449192005743, 0.9918886941686674, 0.9919257664911417, 0.9919591315813686, 0.9919891601625729, 0.9920161858856567, 0.9920428341852415, 0.9920668176548679, 0.9920907279263413, 0.9921122471706674, 0.9921316144905609, 0.9921513702272744, 0.9921668252415071, 0.9921807347543166, 0.9921932533158451, 0.9922068451700302, 0.9922214029876063, 0.9922345050234248, 0.9922509471532805, 0.9922657450701508, 0.9922790631953339, 0.9922910495079987, 0.9923088126358257, 0.9923271245996794, 0.9923436053671478, 0.9923607632066789, 0.9923762052622568, 0.9923901031122769, 0.992402611177295, 0.9924115432870019, 0.992419582185738, 0.9924268171946006, 0.9924333287025768, 0.9924391890597555, 0.9924444633812163, 0.9924538605681501], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.059005112245858415, 0.12001001564853159, 0.18082292598126876, 0.23831348141589787, 0.29152394309170265, 0.3403787435773366, 0.3852757983894072, 0.42585507566643027, 0.4627599907277993, 0.4957913088142814, 0.525081071475775, 0.5515893717547788, 0.5751416662246323, 0.5962125429003617, 0.6150451430907472, 0.6318846199808442, 0.6471988405881816, 0.6613304954977369, 0.6739839787981138, 0.6857464137025644, 0.6963884927295219, 0.7059013577355606, 0.7146133915779684, 0.7230514841313914, 0.7301258655563546, 0.7368712268075716, 0.7432604642548264, 0.7487809033809252, 0.7537950381934351, 0.7592325524463808, 0.7634630175518933, 0.7673538263482853, 0.7712227947111978, 0.7747089842724576, 0.7770886894314317, 0.7805386354355778, 0.7832254592508302, 0.7861074678720574, 0.7876463234003638, 0.7900628610527972, 0.7918694749851681, 0.7940966605815307, 0.7956585859672782, 0.7965983926096317, 0.7978267545911384, 0.7988416836124463, 0.8000420585023312, 0.8007591264190408, 0.8010312171186277, 0.8019127763179095, 0.8025669017828806, 0.8035269731821527, 0.803869076551061, 0.8043213949407592, 0.8047784861978128, 0.8052579629733628, 0.8058583314915386, 0.8064394022862854, 0.8067782310241478, 0.807336436018495, 0.8081582623432269, 0.809204111325395, 0.8101575824405965, 0.8109281982082086, 0.8116095453678094, 0.8122105507802001, 0.8127392486201017, 0.8132028696447633, 0.8136445426294586, 0.8140664623781845, 0.8144583971832877, 0.8148365820790403, 0.8151769484852176, 0.8155087218219368, 0.8158439389187341, 0.8161832849082612, 0.8164886962988357, 0.8167879806128526, 0.8170207154017181, 0.8172423837429468, 0.8174540922813027, 0.817656836997073, 0.8178271002100164, 0.8179925441329153, 0.8181913012971841, 0.8183701827450259, 0.8185311760480836, 0.8186760700208355, 0.8187942675650622, 0.8189006453548663, 0.8189841783344398, 0.8190471509848061, 0.8191038263701357, 0.8191426271856823, 0.8191775479196743, 0.8192333906427671, 0.8192836490935506, 0.8193532957617558, 0.8194037707318904, 0.8194369911737616, 0.8194790966026957, 0.8195169914887364, 0.819551096886173, 0.8195817917438659, 0.8195972100845396, 0.8196110865911458, 0.8196357824783415, 0.8196580087768175, 0.819678012445446, 0.8196838087159616, 0.8196890253594256, 0.8196937203385432, 0.819697945819749, 0.8197017487528343, 0.819705171392611, 0.8197082517684101, 0.8196988170753794, 0.8196903258516517, 0.8196826837502967, 0.8196880128903272, 0.8196928091163547, 0.8196971257197794, 0.8197010106628616, 0.8197045071116357, 0.8196954468842823, 0.8196628786171644, 0.8196213601455081, 0.8195839935210175, 0.819550363558976, 0.8195323036243887, 0.8195160496832601, 0.8195014211362444, 0.8194882554439302, 0.8194886133520974, 0.8194889354694479], "moving_var_accuracy_train": [0.032883207010731524, 0.06479935235387266, 0.09377194015720666, 0.1166529931100249, 0.1332443962715184, 0.1444417876158932, 0.1510691882599273, 0.1538957830566581, 0.15367461490306367, 0.15092507792646526, 0.14628135629674416, 0.14036877873928671, 0.1336565113037591, 0.1264231976512997, 0.1189386306057393, 0.11141035339175452, 0.10384282114454378, 0.09665005871171262, 0.08967040887656558, 0.0830849039907352, 0.07684196081272457, 0.07070606751017922, 0.06512440267342588, 0.05993049931820487, 0.05492190154471753, 0.05041333923625129, 0.04623506860486929, 0.04234765500091177, 0.038716991090029976, 0.03545200112931988, 0.03232567090387078, 0.02947968365863917, 0.02686744528350253, 0.024532554720886786, 0.02231698109306011, 0.02033162854133804, 0.018497817730702946, 0.016843576408205118, 0.015292865927611464, 0.013910306033198025, 0.012620015282587747, 0.011481641075362172, 0.010420912023866948, 0.009466910757831747, 0.008583674669245681, 0.007781725064937836, 0.007068467107414008, 0.006407454153009352, 0.005789445068134389, 0.005246334500354234, 0.004758304642907415, 0.004316444499553129, 0.003918969383645977, 0.0035472678885632276, 0.003220848890323349, 0.0029240964724599173, 0.0026485305034986313, 0.0024040769564750084, 0.0021876292365314554, 0.0019981329827331107, 0.001829694278806212, 0.0016764558701667998, 0.0015338604131026389, 0.001401270826520656, 0.001278276380591565, 0.0011646923557299394, 0.0010597131090075212, 0.0009632627068186293, 0.0008750406593742493, 0.0007942809494702813, 0.0007204469905953403, 0.0006529038935172571, 0.0005913132756420742, 0.0005352516533591639, 0.00048423499644704315, 0.00043786332350498417, 0.0003957751357762428, 0.00035760603762767637, 0.0003230313907064639, 0.0002917164182583052, 0.000263383073189489, 0.000237712338989232, 0.00021448183931644925, 0.00019349978661191138, 0.00017451885057684705, 0.00015738133969349526, 0.00014190493738577001, 0.0001279329184635023, 0.0001153165912183626, 0.00010392312060270454, 9.364274123243843e-05, 8.436498053221713e-05, 7.599855835164367e-05, 6.84554639733244e-05, 6.165589435603649e-05, 5.553028700922414e-05, 5.0009643800222675e-05, 4.5034911668656375e-05, 4.0552668623040066e-05, 3.651649162205438e-05, 3.288208869033298e-05, 2.9606249035142464e-05, 2.6655643194840875e-05, 2.3998194316559042e-05, 2.1604948392276962e-05, 1.9450844739886194e-05, 1.751093712723549e-05, 1.57649887242493e-05, 1.419265755271163e-05, 1.2776767635159147e-05, 1.15026034738411e-05, 1.0354492843641387e-05, 9.320784830196609e-06, 8.39011677662163e-06, 7.552767745461182e-06, 6.799398341388294e-06, 6.1210034773327825e-06, 5.5113362223072455e-06, 4.962173405169792e-06, 4.467552416778362e-06, 4.022090220322221e-06, 3.6227209566817614e-06, 3.2634668131952178e-06, 2.939564673142819e-06, 2.6482577289449026e-06, 2.3855780697746693e-06, 2.1487586149138517e-06, 1.935290818636954e-06, 1.7424797800275912e-06, 1.5688134170608413e-06, 1.4124031835339245e-06, 1.2715444628056575e-06, 1.1446991106014433e-06, 1.0304795657431437e-06, 9.282263732692385e-07], "duration": 72863.998043, "accuracy_train": [0.6044575806339978, 0.6858741982742709, 0.7506166150447582, 0.784437436554079, 0.8059443420657992, 0.8236340381598376, 0.8377190326227391, 0.8486243410275931, 0.857409798645718, 0.8623612841454411, 0.8661036922642118, 0.8706362904900333, 0.8758446238233666, 0.8801236186092655, 0.8845875438353636, 0.8893769898947952, 0.8904218649063308, 0.8993972997992802, 0.902651426668051, 0.9098601089539498, 0.9149514638704319, 0.9097681844661315, 0.9203454486203396, 0.9256238969061462, 0.9212755081441492, 0.9316903723698781, 0.9355283098583426, 0.9378312886443337, 0.9383664333587117, 0.9467358876084349, 0.9410625245131967, 0.9452023708587117, 0.9472939233227206, 0.9548510174418604, 0.9499674839654854, 0.9560346804055924, 0.9560125103820598, 0.9602668117271133, 0.956851366394426, 0.9625465390250092, 0.9596637149893872, 0.9666155494416758, 0.964428107119786, 0.967661505917774, 0.9660575137273901, 0.9671971971322444, 0.971520171477021, 0.9699161792866371, 0.9655001989894795, 0.9711492291320598, 0.9733577600129198, 0.9746355103935955, 0.976635138369786, 0.9740785561438722, 0.9783317760243633, 0.9791473805486341, 0.9777283187984496, 0.9804712734173128, 0.9832378400124585, 0.9865860542981728, 0.9890274605481728, 0.9903989378576044, 0.9907244586909376, 0.9909104705956996, 0.9910267280361758, 0.9911894884528424, 0.9911662369647471, 0.9912824944052234, 0.9915150092861758, 0.9916312667266519, 0.991724272679033, 0.9917010211909376, 0.9917475241671282, 0.9918172786314139, 0.9918405301195091, 0.9918637816076044, 0.9919102845837948, 0.9919567875599853, 0.992026542024271, 0.9920730450004615, 0.9921427994647471, 0.9921195479766519, 0.9921195479766519, 0.9921893024409376, 0.9921660509528424, 0.992212553929033, 0.9922358054171282, 0.9922590569052234, 0.9922590569052234, 0.9922361659053157, 0.9922361659053157, 0.9922129144172205, 0.9922129144172205, 0.9922129144172205, 0.9922129144172205, 0.9922361659053157, 0.9922361659053157, 0.9922361659053157, 0.9922361659053157, 0.992259417393411, 0.9922826688815062, 0.992259417393411, 0.992259417393411, 0.992259417393411, 0.992259417393411, 0.9922826688815062, 0.9922826688815062, 0.9923059203696014, 0.9923059203696014, 0.9923059203696014, 0.9923291718576966, 0.9923059203696014, 0.9923059203696014, 0.9923059203696014, 0.9923291718576966, 0.9923524233457919, 0.9923524233457919, 0.9923989263219823, 0.9923989263219823, 0.9923989263219823, 0.9923989263219823, 0.992468680786268, 0.9924919322743633, 0.9924919322743633, 0.9925151837624585, 0.9925151837624585, 0.9925151837624585, 0.9925151837624585, 0.9924919322743633, 0.9924919322743633, 0.9924919322743633, 0.9924919322743633, 0.9924919322743633, 0.9924919322743633, 0.9925384352505537], "end": "2016-01-30 13:51:28.015000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0], "moving_var_accuracy_valid": [0.03133442944031717, 0.06169537064881084, 0.08880972415216756, 0.10967522741459032, 0.1241898837589019, 0.13325201915743162, 0.13806852701887107, 0.1390817740159137, 0.1374313514155041, 0.13350782804472694, 0.12787805701115385, 0.12141446116317496, 0.11426541022000966, 0.10683470579296292, 0.0993432366830448, 0.09196102485212931, 0.08487565054220812, 0.07818541852233214, 0.07180787242679519, 0.06587227905804863, 0.06030433576638783, 0.055088353595357104, 0.05026261403886272, 0.04587716528843765, 0.041739870612506676, 0.03797538263694078, 0.03454524556966334, 0.031364998246001655, 0.028454772352663703, 0.025875394168656217, 0.02344892626687123, 0.02124027917799484, 0.01925097150593431, 0.017435256014254205, 0.01574269738262162, 0.014275546791243163, 0.012912963312046716, 0.011696420744077497, 0.010548091356702738, 0.009545839109063124, 0.00862062988326254, 0.007803210096062096, 0.00704484558465167, 0.00634831015491161, 0.005727058997838949, 0.00516362382631969, 0.004660229542574116, 0.004198834265891273, 0.0037796171394413607, 0.0034086497450937702, 0.0030716356916997063, 0.0027727677563550396, 0.0024965442931547004, 0.002248731191170162, 0.0020257384638086196, 0.0018252336992323846, 0.0016459543105276952, 0.0014843976688914942, 0.0013369911462247578, 0.0012060963669437103, 0.0010915653168215391, 0.0009922529859809073, 0.0009012096518905282, 0.0008164333246531059, 0.0007389680977548603, 0.0006683221555308811, 0.0006040056326310432, 0.0005455395694585125, 0.0004927412877413485, 0.0004450693054364974, 0.00040194489091590946, 0.00036303761616269685, 0.0003277764981605136, 0.00029598951026708247, 0.0002674018937582411, 0.00024169810568789072, 0.00021836778017653543, 0.00019733714206443174, 0.0001780909171955223, 0.00016072405715749822, 0.00014505503598866363, 0.0001309194811677524, 0.00011808843910611315, 0.00010652594042011964, 9.62288860712409e-05, 8.689398461555471e-05, 7.843785574666393e-05, 7.078301854205593e-05, 6.383045262300131e-05, 5.754925346817346e-05, 5.1857127949444126e-05, 4.670710514674707e-05, 4.206530352579272e-05, 3.787232270279723e-05, 3.4096065551480375e-05, 3.071452468383412e-05, 2.766580542232715e-05, 2.4942880805623177e-05, 2.2471522228551715e-05, 2.0234302385519605e-05, 1.822682795127915e-05, 1.6417069357643552e-05, 1.4785831025087974e-05, 1.3315727491178283e-05, 1.198629426912261e-05, 1.0789397859130698e-05, 9.715947054817053e-06, 8.748798424430884e-06, 7.87751990281518e-06, 7.090070283300676e-06, 6.381308175291891e-06, 5.743375743222941e-06, 5.169198861123442e-06, 4.652409135711562e-06, 4.187273652307782e-06, 3.7686316855125793e-06, 3.3925696378545866e-06, 3.0539615819926733e-06, 2.7490910392114773e-06, 2.474437532891518e-06, 2.2272008136593295e-06, 2.0046484298795375e-06, 1.8043194219363584e-06, 1.6239975061289903e-06, 1.4623365449933373e-06, 1.3256491187016261e-06, 1.2085982582294616e-06, 1.1003048140388915e-06, 1.000453101757227e-06, 9.033432427172039e-07, 8.15386633865401e-07, 7.357739199689954e-07, 6.637565470591061e-07, 5.973820452375009e-07, 5.376447745500383e-07], "accuracy_test": 0.8081971460459183, "start": "2016-01-29 17:37:04.017000", "learning_rate_per_epoch": [0.00100860430393368, 0.0007131909369491041, 0.0005823179963044822, 0.00050430215196684, 0.00045106158358976245, 0.00041176099330186844, 0.0003812166105490178, 0.00035659546847455204, 0.00033620145404711366, 0.0003189487033523619, 0.00030410566250793636, 0.0002911589981522411, 0.000279736501397565, 0.00026956084184348583, 0.0002604205219540745, 0.00025215107598342, 0.0002446224680170417, 0.00023773031716700643, 0.00023138969845604151, 0.00022553079179488122, 0.00022009550593793392, 0.00021503516472876072, 0.00021030854259151965, 0.00020588049665093422, 0.000201720860786736, 0.00019780358707066625, 0.00019410598906688392, 0.0001906083052745089, 0.00018729311705101281, 0.00018414511578157544, 0.00018115068087354302, 0.00017829773423727602, 0.00017557547835167497, 0.0001729742216411978, 0.0001704852475086227, 0.00016810072702355683, 0.00016581352974753827, 0.0001636172237340361, 0.00016150594456121325, 0.00015947435167618096, 0.000157517526531592, 0.00015563103079330176, 0.00015381071716547012, 0.00015205283125396818, 0.00015035385149531066, 0.00014871059102006257, 0.00014712006668560207, 0.00014557949907612056, 0.00014408632705453783, 0.0001426381932105869, 0.0001412328565493226, 0.0001398682506987825, 0.00013854245480615646, 0.00013725366443395615, 0.00013600017700809985, 0.00013478042092174292, 0.00013359291187953204, 0.00013243623834569007, 0.0001313091051997617, 0.00013021026097703725, 0.00012913855607621372, 0.00012809288455173373, 0.00012707219866570085, 0.00012607553799171, 0.00012510197120718658, 0.00012415061064530164, 0.00012322062684688717, 0.00012231123400852084, 0.000121421689982526, 0.00012055127444909886, 0.00011969931074418128, 0.00011886515858350322, 0.00011804820678662509, 0.00011724787327693775, 0.00011646359780570492, 0.00011569484922802076, 0.0001149411327787675, 0.00011420195369282737, 0.0001134768535848707, 0.00011276539589744061, 0.00011206715134903789, 0.00011138171248603612, 0.000110708708234597, 0.00011004775296896696, 0.00010939849744318053, 0.00010876059968722984, 0.00010813373228302225, 0.00010751758236438036, 0.00010691184434108436, 0.0001063162344507873, 0.00010573046165518463, 0.00010515427129575983, 0.00010458739416208118, 0.00010402959742350504, 0.00010348061914555728, 0.00010294024832546711, 0.00010240825213259086, 0.00010188442684011534, 0.0001013685468933545, 0.000100860430393368, 0.00010035988088930026, 9.986670920625329e-05, 9.938074072124436e-05, 9.890179353533313e-05, 9.842970757745206e-05, 9.796431550057605e-05, 9.750546450959519e-05, 9.705299453344196e-05, 9.660677460487932e-05, 9.616665192879736e-05, 9.573248826200143e-05, 9.530415263725445e-05, 9.488151408731937e-05, 9.446444892091677e-05, 9.40528407227248e-05, 9.364655852550641e-05, 9.3245500465855e-05, 9.284955740440637e-05, 9.245860564988106e-05, 9.207255789078772e-05, 9.169130498776212e-05, 9.131474507739767e-05, 9.0942790848203e-05, 9.057534043677151e-05, 9.021231380756944e-05, 8.985361637314782e-05, 8.949916082201526e-05, 8.914886711863801e-05, 8.880266250343993e-05, 8.846045238897204e-05, 8.812217129161581e-05, 8.778773917583749e-05, 8.745709055801854e-05, 8.71301454026252e-05, 8.680683822603896e-05], "accuracy_train_first": 0.6044575806339978, "accuracy_train_last": 0.9925384352505537, "batch_size_eval": 1024, "accuracy_train_std": [0.018988422080736235, 0.01702025965282816, 0.019222405496322495, 0.0212420735553711, 0.019344043895074513, 0.02084661704463835, 0.023000243340657534, 0.022360736185753217, 0.02302896082486621, 0.02289639480371372, 0.024173895702896186, 0.025542865758446186, 0.025660468087127127, 0.024079972330518364, 0.023664651508487306, 0.023325379329308085, 0.022855958236794247, 0.021954026935449907, 0.023569585315755395, 0.023027882140259057, 0.021096283704529577, 0.022818346354918032, 0.0195913715653319, 0.018568974931949974, 0.01976974523626524, 0.01809398666666781, 0.01766648429531185, 0.016137644480867016, 0.014796362284753844, 0.014026005372757076, 0.0161028748635613, 0.012715089681144116, 0.012754464796099397, 0.01338759680721108, 0.012598207963944313, 0.01353713023886534, 0.011160771188210452, 0.011856878413794056, 0.010515649960947043, 0.01032690375678755, 0.01123683578962657, 0.008632420889151076, 0.010230482169425395, 0.010932673924598391, 0.009302629712906853, 0.009936875036070174, 0.008699434257950165, 0.008457079908179083, 0.00850796075774304, 0.009913893487159505, 0.006624059365559196, 0.007958586267907065, 0.007327732771086669, 0.007166715911277347, 0.006899386236875998, 0.005893093358254982, 0.00710922263337784, 0.006611898265521671, 0.005918758750870999, 0.004481384965309973, 0.0035959763517801483, 0.003124031133656111, 0.0031623725367028884, 0.0029791635699926195, 0.002892075297638964, 0.0028860791739179553, 0.0029054169371382924, 0.002869488899384305, 0.0029015695165449467, 0.0029760945671470714, 0.0028837773663178802, 0.0028838856338374345, 0.002859759954343252, 0.0029897565686610676, 0.0029658682344704534, 0.0029493111446435238, 0.0029463483120340515, 0.0029961753802049463, 0.0030569569246422356, 0.003110576249017474, 0.0031666045506447745, 0.0031840693612504756, 0.0031840693612504756, 0.0031236038310110134, 0.003141651912130157, 0.0031271365454490527, 0.003137737537454036, 0.003126418123437407, 0.003126418123437407, 0.003114374037860102, 0.003114374037860102, 0.0030890293630902134, 0.0030890293630902134, 0.0030890293630902134, 0.0030890293630902134, 0.00307770374374581, 0.003070317128829034, 0.003070317128829034, 0.003062912700175268, 0.0030438623180172094, 0.0030839884324176795, 0.0030587454840861022, 0.0030587454840861022, 0.0030587454840861022, 0.0030587454840861022, 0.003024513208732599, 0.003039491121087539, 0.0030424080164540056, 0.00301240672682816, 0.00301240672682816, 0.0030076303338625013, 0.003042408016454005, 0.003042408016454005, 0.003042408016454005, 0.0030226919046674084, 0.0030102189225684065, 0.0030102189225684065, 0.002999750576208478, 0.002999750576208478, 0.002999750576208478, 0.002999750576208478, 0.0029749973643364343, 0.0029303960650273854, 0.0029303960650273854, 0.0028927780207502894, 0.0028927780207502894, 0.0028927780207502894, 0.0028927780207502894, 0.002938134469457447, 0.002938134469457447, 0.002938134469457447, 0.002938134469457447, 0.002938134469457447, 0.002938134469457447, 0.0029637492928638247], "accuracy_test_std": 0.00921005840073223, "error_valid": [0.40994887754141573, 0.3309458537274097, 0.27186088102409633, 0.24427151967243976, 0.2295819018260542, 0.21992805205195776, 0.21065070830195776, 0.20893142884036142, 0.20509577371987953, 0.20692682840737953, 0.2113110645707832, 0.20983592573418675, 0.21288768354668675, 0.2141495670180723, 0.2154614551957832, 0.2165600880082832, 0.2149731739457832, 0.2114846103162651, 0.21213467149849397, 0.20839167215737953, 0.20783279602786142, 0.2084828572100903, 0.20697830384036142, 0.20100568288780118, 0.20620470161897586, 0.20242052193147586, 0.19923639871987953, 0.20153514448418675, 0.20107774849397586, 0.1918298192771084, 0.19846279649849397, 0.19762889448418675, 0.1939564900225903, 0.19391530967620485, 0.20149396413780118, 0.1884118505271084, 0.19259312641189763, 0.18795445453689763, 0.19850397684487953, 0.18818830007530118, 0.19187099962349397, 0.18585866905120485, 0.19028408556099397, 0.19494334760918675, 0.19111798757530118, 0.1920239551957832, 0.18915456748870485, 0.1927872623305723, 0.1965199665850903, 0.1901531908885542, 0.19154596903237953, 0.18783238422439763, 0.1930519931287651, 0.19160773955195776, 0.19110769248870485, 0.19042674604668675, 0.18873835184487953, 0.18833096056099397, 0.1901723103350903, 0.18763971903237953, 0.18444530073418675, 0.1813832478350903, 0.1812611775225903, 0.1821362598832832, 0.1822583301957832, 0.1823804005082832, 0.1825024708207832, 0.1826245411332832, 0.1823804005082832, 0.1821362598832832, 0.1820141895707832, 0.18175975385918675, 0.18175975385918675, 0.1815053181475903, 0.1811391072100903, 0.18076260118599397, 0.18076260118599397, 0.18051846056099397, 0.18088467149849397, 0.18076260118599397, 0.18064053087349397, 0.18051846056099397, 0.18064053087349397, 0.18051846056099397, 0.18001988422439763, 0.18001988422439763, 0.18001988422439763, 0.18001988422439763, 0.18014195453689763, 0.18014195453689763, 0.18026402484939763, 0.18038609516189763, 0.18038609516189763, 0.18050816547439763, 0.18050816547439763, 0.18026402484939763, 0.18026402484939763, 0.18001988422439763, 0.18014195453689763, 0.18026402484939763, 0.18014195453689763, 0.18014195453689763, 0.18014195453689763, 0.18014195453689763, 0.18026402484939763, 0.18026402484939763, 0.18014195453689763, 0.18014195453689763, 0.18014195453689763, 0.18026402484939763, 0.18026402484939763, 0.18026402484939763, 0.18026402484939763, 0.18026402484939763, 0.18026402484939763, 0.18026402484939763, 0.18038609516189763, 0.18038609516189763, 0.18038609516189763, 0.18026402484939763, 0.18026402484939763, 0.18026402484939763, 0.18026402484939763, 0.18026402484939763, 0.18038609516189763, 0.18063023578689763, 0.18075230609939763, 0.18075230609939763, 0.18075230609939763, 0.18063023578689763, 0.18063023578689763, 0.18063023578689763, 0.18063023578689763, 0.18050816547439763, 0.18050816547439763], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.08760721420980107, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 16, "valid_ratio": 0.15, "learning_rate": 0.0010086043331012943, "optimization": "rmsprop", "nb_data_augmentation": 0, "learning_rate_decay_method": "sqrt", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 8.890126717270466e-08, "rotation_range": [0, 0], "momentum": 0.7869851978287388}, "accuracy_valid_max": 0.8199801157756024, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8194918345256024, "accuracy_valid_std": [0.013280501035957133, 0.017310280308995656, 0.014154317355075335, 0.0137536499530285, 0.012892177381609114, 0.011872163890115316, 0.008036056777221369, 0.007849334165582493, 0.010436301226756041, 0.010708786382212346, 0.012487491332682525, 0.013630219769476802, 0.013653270369381895, 0.010895165307874658, 0.013556230030411405, 0.014513848928064918, 0.016466905087497583, 0.00909763502309861, 0.01717138794081522, 0.013113583370679442, 0.007362679012604009, 0.016397427378676145, 0.007381807848552224, 0.015712201157704855, 0.011239524821094127, 0.01196092783765888, 0.009065423585977719, 0.0132514628864665, 0.01230483578120511, 0.016980340201446526, 0.01602440026905079, 0.014104726487222018, 0.014681527210722266, 0.018445933935896795, 0.019056374529988956, 0.014870520643415116, 0.012094239402374205, 0.010374537005861077, 0.010748337259299946, 0.016428542099686656, 0.015017868089052961, 0.015900619483528885, 0.01436154865685779, 0.01268932450450721, 0.012554758155988095, 0.01029149067201941, 0.014808384898597239, 0.010550979401604562, 0.013057920562825285, 0.009847539368952449, 0.00888714727646899, 0.012912616546118609, 0.011587642341833987, 0.01010220022555431, 0.013660126842422718, 0.012717017831784477, 0.007781924250996004, 0.009798610914937524, 0.011834782966687024, 0.011122209386165191, 0.011055052159404628, 0.01081393687431807, 0.011561660917242844, 0.011829224007371119, 0.011958816231678866, 0.011664160838436398, 0.011402450721699228, 0.011460379159801212, 0.011806375742467932, 0.011738175897670438, 0.01211740787338375, 0.012367472503699038, 0.01273784521474149, 0.012743936012915777, 0.012980477082842381, 0.013158367013356516, 0.013158367013356516, 0.013444721155064317, 0.013323453295373114, 0.013445148455462992, 0.013199419009916207, 0.013194120157198442, 0.013208447326557307, 0.013239218240443733, 0.013523920445343512, 0.013283791724362365, 0.013283791724362365, 0.013283791724362365, 0.013257715733677719, 0.013257715733677719, 0.01384682016162899, 0.013741796120979722, 0.013741796120979722, 0.013748059204068921, 0.013748059204068921, 0.01357731389621477, 0.01357731389621477, 0.013638030126829188, 0.013542393474059736, 0.013568531020609604, 0.013603872649358589, 0.013603872649358589, 0.013603872649358589, 0.013603872649358589, 0.01357731389621477, 0.01357731389621477, 0.013673796096650672, 0.013673796096650672, 0.013673796096650672, 0.013515713913169209, 0.013515713913169209, 0.013515713913169209, 0.013515713913169209, 0.013515713913169209, 0.013515713913169209, 0.013515713913169209, 0.013416984504866043, 0.013416984504866043, 0.013416984504866043, 0.013338146460457109, 0.01347154147405292, 0.01347154147405292, 0.01347154147405292, 0.01347154147405292, 0.01343474266080369, 0.013249967614881867, 0.013503700552570672, 0.013503700552570672, 0.013503700552570672, 0.013613846023023115, 0.013613846023023115, 0.013613846023023115, 0.013613846023023115, 0.013582311249402824, 0.013582311249402824], "accuracy_valid": [0.5900511224585843, 0.6690541462725903, 0.7281391189759037, 0.7557284803275602, 0.7704180981739458, 0.7800719479480422, 0.7893492916980422, 0.7910685711596386, 0.7949042262801205, 0.7930731715926205, 0.7886889354292168, 0.7901640742658133, 0.7871123164533133, 0.7858504329819277, 0.7845385448042168, 0.7834399119917168, 0.7850268260542168, 0.7885153896837349, 0.787865328501506, 0.7916083278426205, 0.7921672039721386, 0.7915171427899097, 0.7930216961596386, 0.7989943171121988, 0.7937952983810241, 0.7975794780685241, 0.8007636012801205, 0.7984648555158133, 0.7989222515060241, 0.8081701807228916, 0.801537203501506, 0.8023711055158133, 0.8060435099774097, 0.8060846903237951, 0.7985060358621988, 0.8115881494728916, 0.8074068735881024, 0.8120455454631024, 0.8014960231551205, 0.8118116999246988, 0.808129000376506, 0.8141413309487951, 0.809715914439006, 0.8050566523908133, 0.8088820124246988, 0.8079760448042168, 0.8108454325112951, 0.8072127376694277, 0.8034800334149097, 0.8098468091114458, 0.8084540309676205, 0.8121676157756024, 0.8069480068712349, 0.8083922604480422, 0.8088923075112951, 0.8095732539533133, 0.8112616481551205, 0.811669039439006, 0.8098276896649097, 0.8123602809676205, 0.8155546992658133, 0.8186167521649097, 0.8187388224774097, 0.8178637401167168, 0.8177416698042168, 0.8176195994917168, 0.8174975291792168, 0.8173754588667168, 0.8176195994917168, 0.8178637401167168, 0.8179858104292168, 0.8182402461408133, 0.8182402461408133, 0.8184946818524097, 0.8188608927899097, 0.819237398814006, 0.819237398814006, 0.819481539439006, 0.819115328501506, 0.819237398814006, 0.819359469126506, 0.819481539439006, 0.819359469126506, 0.819481539439006, 0.8199801157756024, 0.8199801157756024, 0.8199801157756024, 0.8199801157756024, 0.8198580454631024, 0.8198580454631024, 0.8197359751506024, 0.8196139048381024, 0.8196139048381024, 0.8194918345256024, 0.8194918345256024, 0.8197359751506024, 0.8197359751506024, 0.8199801157756024, 0.8198580454631024, 0.8197359751506024, 0.8198580454631024, 0.8198580454631024, 0.8198580454631024, 0.8198580454631024, 0.8197359751506024, 0.8197359751506024, 0.8198580454631024, 0.8198580454631024, 0.8198580454631024, 0.8197359751506024, 0.8197359751506024, 0.8197359751506024, 0.8197359751506024, 0.8197359751506024, 0.8197359751506024, 0.8197359751506024, 0.8196139048381024, 0.8196139048381024, 0.8196139048381024, 0.8197359751506024, 0.8197359751506024, 0.8197359751506024, 0.8197359751506024, 0.8197359751506024, 0.8196139048381024, 0.8193697642131024, 0.8192476939006024, 0.8192476939006024, 0.8192476939006024, 0.8193697642131024, 0.8193697642131024, 0.8193697642131024, 0.8193697642131024, 0.8194918345256024, 0.8194918345256024], "seed": 472658884, "model": "residualv3", "loss_std": [0.4008043110370636, 0.27878496050834656, 0.26734644174575806, 0.26164528727531433, 0.25560906529426575, 0.24858401715755463, 0.2415609359741211, 0.23496313393115997, 0.22763477265834808, 0.21932578086853027, 0.20970091223716736, 0.19970552623271942, 0.18825775384902954, 0.1782006174325943, 0.16486334800720215, 0.15301905572414398, 0.14124049246311188, 0.12945489585399628, 0.11829730123281479, 0.11014897376298904, 0.10213720798492432, 0.09345562756061554, 0.08336813002824783, 0.07576452940702438, 0.07757852226495743, 0.07091823220252991, 0.06369371712207794, 0.06509460508823395, 0.05954886972904205, 0.056046660989522934, 0.056407928466796875, 0.05542873963713646, 0.05589393898844719, 0.04688822105526924, 0.04747721552848816, 0.045519713312387466, 0.04500649869441986, 0.039962295442819595, 0.04134047031402588, 0.03777520731091499, 0.0374787263572216, 0.033098023384809494, 0.03470279276371002, 0.03525819256901741, 0.032276701182127, 0.027627548202872276, 0.027887333184480667, 0.02672620117664337, 0.02524229697883129, 0.022681865841150284, 0.02304868958890438, 0.022173916921019554, 0.016674473881721497, 0.015610468573868275, 0.013368871994316578, 0.010124736465513706, 0.008868908509612083, 0.006332977209240198, 0.004174013622105122, 0.0019648163579404354, 0.0009291311143897474, 0.00017008473514579237, 5.138841152074747e-05, 3.696229759952985e-05, 3.1613450119039044e-05, 2.8266425943002105e-05, 2.5828947400441393e-05, 2.3914817575132474e-05, 2.2354834072757512e-05, 2.1056042896816507e-05, 1.9947541659348644e-05, 1.8996051949216053e-05, 1.8144339264836162e-05, 1.7401489458279684e-05, 1.6720088751753792e-05, 1.6109795978991315e-05, 1.5550538591924123e-05, 1.5043417988636065e-05, 1.4574427950719837e-05, 1.4144588021736126e-05, 1.3743547242484055e-05, 1.3369129192142282e-05, 1.3027539353061002e-05, 1.2700113984465133e-05, 1.239645280293189e-05, 1.211072776641231e-05, 1.1841133527923375e-05, 1.1586835171328858e-05, 1.1343435289745685e-05, 1.1116078894701786e-05, 1.089885518013034e-05, 1.0691230272641405e-05, 1.0498406481929123e-05, 1.031050669553224e-05, 1.0133743671758566e-05, 9.961040632333606e-06, 9.796518497751094e-06, 9.640403732191771e-06, 9.491069249634165e-06, 9.346836122858804e-06, 9.20721868169494e-06, 9.07331286725821e-06, 8.944745786720887e-06, 8.820351467875298e-06, 8.701196748006623e-06, 8.585901014157571e-06, 8.474207788822241e-06, 8.366147994820494e-06, 8.262208211817779e-06, 8.160762263287324e-06, 8.063236236921512e-06, 7.968010322656482e-06, 7.875334631535225e-06, 7.78476896812208e-06, 7.698101399000734e-06, 7.612541594426148e-06, 7.529755293944618e-06, 7.45024817661033e-06, 7.37213531465386e-06, 7.2964426180988085e-06, 7.223473858175566e-06, 7.152329999371432e-06, 7.083033779053949e-06, 7.015411483735079e-06, 6.949855105631286e-06, 6.886026767460862e-06, 6.822265277151018e-06, 6.761222266504774e-06, 6.7009636950388085e-06, 6.641038453381043e-06, 6.583631147805136e-06, 6.5268322941847146e-06, 6.4714990912762005e-06, 6.417506938305451e-06, 6.364958608173765e-06]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:23 2016", "state": "available"}], "summary": "3be19ba4d52707f6c7767c96646ea816"}