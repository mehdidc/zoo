{"content": {"hp_model": {"f0": 32, "f1": 32, "f2": 16, "f3": 64, "nonlin": "very_leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.61478590965271, 1.317549467086792, 1.1639235019683838, 1.077081561088562, 1.014987826347351, 0.9664980173110962, 0.926373302936554, 0.8917880058288574, 0.8612333536148071, 0.83359694480896, 0.8081861138343811, 0.7843411564826965, 0.7618149518966675, 0.7404083609580994, 0.7200611233711243, 0.700415313243866, 0.6814570426940918, 0.6630432605743408, 0.6449874043464661, 0.6274891495704651, 0.6103257536888123, 0.5935862064361572, 0.5770390033721924, 0.5606343150138855, 0.5444400310516357, 0.5285645723342896, 0.5127862691879272, 0.49735426902770996, 0.48196110129356384, 0.4666275978088379, 0.45152541995048523, 0.4363555312156677, 0.42130741477012634, 0.4064056873321533, 0.3914230465888977, 0.3764352798461914, 0.36158356070518494, 0.3469815254211426, 0.33234143257141113, 0.3179328739643097, 0.3035738170146942, 0.28942906856536865, 0.2754952907562256, 0.2618139982223511, 0.2483496367931366, 0.23507384955883026, 0.2222297340631485, 0.20967638492584229, 0.19739533960819244, 0.1855989396572113, 0.1742684692144394, 0.16340172290802002, 0.1529475748538971, 0.14289487898349762, 0.13337503373622894, 0.12442008405923843, 0.1159152016043663, 0.10803581774234772, 0.10069283097982407, 0.09378892183303833, 0.08749636262655258, 0.0816921591758728, 0.0763547495007515, 0.07147910445928574, 0.06700651347637177, 0.06293605268001556, 0.05920783430337906, 0.055804599076509476, 0.05271060764789581, 0.04985300824046135, 0.047253165394067764, 0.044852592051029205, 0.04264649748802185, 0.04063623771071434, 0.03876696899533272, 0.037072744220495224, 0.03552461788058281, 0.03407207876443863, 0.03274589404463768, 0.0315287783741951, 0.030397137627005577], "moving_avg_accuracy_train": [0.04587922347960501, 0.09264059039313399, 0.14142102352690106, 0.18890320524467052, 0.234148149260855, 0.2765773669575085, 0.3163258384451279, 0.3534967690720529, 0.38808288200888597, 0.4201866577614857, 0.4499611431423601, 0.4776857341160532, 0.5033888530090345, 0.5271632929376899, 0.5493019031972617, 0.569942581971297, 0.5890748313357675, 0.6068587587780491, 0.6234151734998657, 0.6388041198530445, 0.6533446326208777, 0.6668427175488508, 0.6795117552197224, 0.6913973758806128, 0.7026313636063204, 0.7131139042713437, 0.7230270273293515, 0.7323347406863019, 0.7411161504539582, 0.7493425428317352, 0.7569252963812491, 0.7641494478293183, 0.7709395025849616, 0.7773435206150405, 0.7834256461801976, 0.7891831912947822, 0.7946182789228713, 0.7998817734512191, 0.8047351759672083, 0.8094565887530087, 0.8139384111900003, 0.8181812426785122, 0.8222438955943543, 0.8261791929292986, 0.8301232112747963, 0.8339262329571635, 0.8376069718915137, 0.8411544769621906, 0.8445332434305618, 0.8478298996211434, 0.8512454636688113, 0.854531059853379, 0.8577391764420997, 0.8609031019826443, 0.8640598076631636, 0.8672287248065925, 0.870322421516594, 0.8732974107579763, 0.8763073613061635, 0.879211629299532, 0.8820393120864023, 0.8847493121600618, 0.8873836607751742, 0.8899870533609089, 0.8924904698582897, 0.8948830175856852, 0.8972246836427313, 0.8995134726035782, 0.9016896040599978, 0.9037760055552992, 0.9058467182034422, 0.907866144557009, 0.909846352643067, 0.9117448334098142, 0.9135673983915532, 0.915330975810842, 0.9170112014405828, 0.9186559019406736, 0.9202175125990887, 0.9217647962690432, 0.9232829096077165], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.04595829607492469, 0.0924418327607304, 0.14050186635212722, 0.18657689738445968, 0.2300128458707878, 0.27056710180516985, 0.3082907533057523, 0.34326640077261683, 0.3756825129543913, 0.40523646139539793, 0.43227240909998466, 0.45710730933268195, 0.47996435536665777, 0.5008185175333052, 0.5198059605371282, 0.5372008645304787, 0.5528451006019036, 0.5670103622849361, 0.580065303089575, 0.5919591751714308, 0.602797937388851, 0.6126982782508696, 0.6217052117680266, 0.629896901152218, 0.6372683920893305, 0.6441723181288914, 0.6504479162294059, 0.6562525869087997, 0.6614157553640041, 0.665964950723688, 0.6702677755873132, 0.6740426617145758, 0.6773657875329526, 0.6802589445194918, 0.6828871998698769, 0.6851173228328139, 0.6869016184109783, 0.6883721775789167, 0.6896814147814918, 0.6908098706301499, 0.6914572109391228, 0.6919655455210388, 0.6922765622697633, 0.6925554478349556, 0.692820710892198, 0.6931825474648757, 0.6932375866754665, 0.6932382938399982, 0.6930680318505767, 0.6929758312163474, 0.6928684365830411, 0.6928084025068153, 0.6926811296507122, 0.6925431995263789, 0.6925676058067982, 0.6924919152091756, 0.6923373149439056, 0.6921249325176627, 0.6918462800979748, 0.6916687351077557, 0.6916208669151278, 0.6916286726840819, 0.6916601119386406, 0.6916884072677435, 0.6916396013677764, 0.6916577407227156, 0.6916008239546609, 0.6914275285509117, 0.691223764071197, 0.6911146477356134, 0.6910907147297478, 0.6911057961182189, 0.6909718554841832, 0.690753652663551, 0.6906305123124821, 0.69044644380902, 0.6901678303486752, 0.6899913499305246, 0.6897582458580294, 0.6894985945591241, 0.6893391800862689], "moving_var_accuracy_train": [0.01894412832382386, 0.03672934441203662, 0.05447218588129424, 0.06931598551927834, 0.08080833159859951, 0.0889296450678897, 0.09425614943151957, 0.09726563724141266, 0.09830486638998574, 0.09775025150914608, 0.09595390617549457, 0.09327639205987238, 0.0898946057413257, 0.08599216111248434, 0.08180400757926283, 0.07745794540361278, 0.07300653755495005, 0.06855229647690604, 0.06416410064518255, 0.05987906760941343, 0.05579399945243579, 0.051854384177697194, 0.04811348639948117, 0.044573549565984326, 0.04125201693137805, 0.038115768167385024, 0.03518862142951537, 0.032449461037980215, 0.029898533351749593, 0.027517741800552952, 0.025283450983031634, 0.023224801162030243, 0.021317264638088924, 0.01955464119664622, 0.017932107339494636, 0.016437240537263458, 0.015059378081262181, 0.013802779644985486, 0.01263450132432683, 0.011571676840139423, 0.010595289747335963, 0.009697775343961546, 0.008876544147996789, 0.008128268819226896, 0.0074554394636908005, 0.006840062282570714, 0.006277986606239208, 0.005763451075653598, 0.005289850533718344, 0.004858676958696616, 0.004477803962700446, 0.004127179847022816, 0.0038070899707419647, 0.003516474797102759, 0.003254510434173286, 0.003019437713513218, 0.002803632576163164, 0.002602924367423913, 0.0024241701514043155, 0.002257666089459627, 0.0021038615900021303, 0.0019595723345950285, 0.001826073234769023, 0.0017044647878912468, 0.0015904221565363507, 0.0014828985025335025, 0.0013839592515846487, 0.001292710320591833, 0.001206059221573219, 0.0011246309402122617, 0.0010507585040316508, 0.000982385398805807, 0.0009194378755020333, 0.000859932150947211, 0.0008038346238664461, 0.0007514430093042281, 0.000701707131875347, 0.000655881776302804, 0.0006122412493088045, 0.0005725639051756962, 0.0005360495276396489], "duration": 21724.246492, "accuracy_train": [0.4587922347960502, 0.5134928926148948, 0.5804449217308048, 0.6162428407045958, 0.6413526454065154, 0.6584403262273901, 0.6740620818337025, 0.688035144714378, 0.6993578984403839, 0.7091206395348837, 0.7179315115702289, 0.7272070528792912, 0.7347169230458656, 0.7411332522955888, 0.7485493955334072, 0.7557086909376154, 0.7612650756160022, 0.7669141057585824, 0.7724229059962164, 0.7773046370316538, 0.7842092475313769, 0.7883254819006091, 0.7935330942575674, 0.7983679618286268, 0.8037372531376891, 0.8074567702565523, 0.8122451348514212, 0.8161041608988556, 0.8201488383628645, 0.8233800742317275, 0.8251700783268733, 0.8291668108619417, 0.8320499953857512, 0.8349796828857512, 0.8381647762666113, 0.8410010973260429, 0.8435340675756736, 0.8472532242063492, 0.848415798611111, 0.8519493038252122, 0.8542748131229235, 0.8563667260751201, 0.8588077718369325, 0.8615968689437985, 0.8656193763842747, 0.8681534280984681, 0.8707336223006644, 0.8730820225982835, 0.8749421416459026, 0.8774998053363787, 0.881985540097822, 0.8841014255144887, 0.8866122257405868, 0.8893784318475452, 0.8924701587878369, 0.895748979097453, 0.8981656919066077, 0.9000723139304172, 0.9033969162398486, 0.9053500412398486, 0.9074884571682356, 0.9091393128229974, 0.911092798311185, 0.9134175866325213, 0.9150212183347176, 0.9164159471322444, 0.9182996781561462, 0.9201125732511997, 0.921274787167774, 0.9225536190130121, 0.9244831320367294, 0.9260409817391103, 0.9276682254175894, 0.9288311603105389, 0.9299704832272055, 0.9312031725844407, 0.9321332321082503, 0.9334582064414912, 0.9342720085248246, 0.9356903492986341, 0.9369459296557769], "end": "2016-01-29 17:33:21.920000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0], "moving_var_accuracy_valid": [0.019009484802993946, 0.036555008968080366, 0.05368740953052806, 0.06742484493914884, 0.0776624950333965, 0.08469807459957912, 0.08903593208245775, 0.09114200211574947, 0.091485040865007, 0.09019745959458941, 0.08775619584969681, 0.08453152669083938, 0.08078037500234904, 0.07661640221916961, 0.07219946892366354, 0.06770276619573809, 0.06313516867649056, 0.05862754355578061, 0.05429867251491619, 0.0501419830013205, 0.046185093598840425, 0.042448734981613764, 0.038933985145894766, 0.03564452060600894, 0.032569118453131436, 0.02974118436065584, 0.027121514108262884, 0.024712610512702515, 0.02248127423790362, 0.020419403419898383, 0.018544091794171824, 0.016817930502218845, 0.015235525938839818, 0.013787306561094675, 0.012470745440666661, 0.011268431932468365, 0.010170242135613843, 0.009172680820450125, 0.008270839656878576, 0.007455216404612054, 0.006713466209431439, 0.006044445224912841, 0.005440871285183441, 0.004897484151091351, 0.004408369016388054, 0.003968710446097193, 0.003571866665319796, 0.003214680003288552, 0.0028934729052650728, 0.0026042021233511365, 0.0023438857134813903, 0.002109529578946026, 0.0018987224064705294, 0.0017090213882962639, 0.0015381246104653528, 0.0013843637110179338, 0.0012461424510943342, 0.0011219341626396925, 0.0010104395709147047, 0.0009096793138352017, 0.0008187320047264707, 0.0007368593526240843, 0.0006631823132022208, 0.00059687128751284, 0.0005372055969044005, 0.00048348799853973896, 0.0004351683543521372, 0.00039192180058956897, 0.00035310330019935253, 0.0003179001275516379, 0.00028611526989540196, 0.0002575057899403657, 0.00023191667178734223, 0.00020915351684699455, 0.0001883746370768476, 0.00016984210429486372, 0.00015355652300794493, 0.00013848117874906614, 0.00012512209845168428, 0.000113216657779725, 0.00010212370876915374], "accuracy_test": 0.47337173150510203, "start": "2016-01-29 11:31:17.673000", "learning_rate_per_epoch": [0.003528366796672344, 0.0024949321523308754, 0.002037103520706296, 0.001764183398336172, 0.0015779336681589484, 0.0014404497342184186, 0.0013335973490029573, 0.0012474660761654377, 0.0011761222267523408, 0.0011157675180584192, 0.0010638426756486297, 0.001018551760353148, 0.0009785928996279836, 0.000942995713558048, 0.0009110203827731311, 0.000882091699168086, 0.0008557546534575522, 0.0008316440507769585, 0.0008094628574326634, 0.0007889668340794742, 0.0007699527777731419, 0.0007522503146901727, 0.0007357153226621449, 0.0007202248671092093, 0.0007056733593344688, 0.000691969646140933, 0.0006790345069020987, 0.0006667986745014787, 0.0006552012637257576, 0.000644188723526895, 0.0006337133818306029, 0.0006237330380827188, 0.0006142097990959883, 0.0006051099044270813, 0.0005964028532616794, 0.0005880611133761704, 0.0005800599465146661, 0.0005723766516894102, 0.0005649908562190831, 0.0005578837590292096, 0.0005510383052751422, 0.0005444388370960951, 0.0005380709189921618, 0.0005319213378243148, 0.0005259778699837625, 0.0005202292813919485, 0.0005146652110852301, 0.000509275880176574, 0.0005040523828938603, 0.0004989863955415785, 0.0004940701765008271, 0.0004892964498139918, 0.0004846584633924067, 0.0004801499017048627, 0.00047576488577760756, 0.000471497856779024, 0.0004673436051234603, 0.00046329727047123015, 0.00045935422531329095, 0.00045551019138656557, 0.0004517610650509596, 0.0004481030337046832, 0.0004445324302650988, 0.000441045849584043, 0.0004376400320325047, 0.0004343119217082858, 0.00043105860822834074, 0.0004278773267287761, 0.00042476545786485076, 0.00042172049870714545, 0.0004187400918453932, 0.00041582202538847923, 0.0004129640874452889, 0.0004101642989553511, 0.0004074207099620253, 0.0004047314287163317, 0.000402094708988443, 0.0003995088627561927, 0.0003969722893089056, 0.0003944834170397371, 0.0003920407616533339], "accuracy_train_first": 0.4587922347960502, "accuracy_train_last": 0.9369459296557769, "batch_size_eval": 1024, "accuracy_train_std": [0.021028448724895676, 0.019818633840042208, 0.02058760057996271, 0.021204231504334575, 0.020395324314531754, 0.020465125719124417, 0.021623578513562478, 0.021606789088526767, 0.020825051852158338, 0.02091371788994502, 0.020696366283021055, 0.020712891107303653, 0.019702631192979762, 0.019929837908487808, 0.019320100521323373, 0.019274612257868376, 0.019216529152048346, 0.019255289913912402, 0.01933044831911254, 0.019894968032248665, 0.01926230144739782, 0.01852762068829208, 0.018443380804816837, 0.018789429835879126, 0.019663190507294228, 0.019573732896800294, 0.019711036862461572, 0.0194473036562422, 0.019386023740238564, 0.019888677713279384, 0.01944876881787718, 0.01993225743872387, 0.019830229949864255, 0.019622554840329716, 0.019513526936076452, 0.019380601563467093, 0.019216828003938088, 0.01906806640508558, 0.019135671774561172, 0.019196351684295572, 0.019113483492066343, 0.01923322508822459, 0.019190556339192474, 0.01941246332491545, 0.01915884859593055, 0.01840097777715476, 0.018797898035235183, 0.019050238359109132, 0.018623431182110725, 0.018639846628386005, 0.019105547704330327, 0.01913758136026137, 0.019166624371739212, 0.01954738961607654, 0.018992677460230006, 0.018920345040343484, 0.01914644434069768, 0.01874830663567589, 0.0184163555761915, 0.01812856806936453, 0.01843818905941849, 0.018104447393692633, 0.01789098305921019, 0.017888135244618693, 0.017948539970927345, 0.017864604785714642, 0.017105678894343293, 0.016841246140452647, 0.01707555877765243, 0.01659991448100751, 0.01664270434361988, 0.015990195977983367, 0.015904122753033576, 0.015389096978018251, 0.015413989639688748, 0.01504283900205304, 0.015015448246016694, 0.014807934605619194, 0.01482751019640471, 0.014495833854576394, 0.014109018551412847], "accuracy_test_std": 0.013939811589615813, "error_valid": [0.540417039250753, 0.4892063370670181, 0.4269578313253012, 0.39874782332454817, 0.37906361775225905, 0.3644445947853916, 0.35219638318900603, 0.34195277202560237, 0.3325724774096386, 0.32877800263554224, 0.3244040615587349, 0.31937858857304224, 0.31432223032756024, 0.31149402296686746, 0.3093070524284638, 0.30624499952936746, 0.3063567747552711, 0.3055022825677711, 0.3024402296686747, 0.30099597609186746, 0.29965320265436746, 0.2981986539909638, 0.29723238657756024, 0.29637789439006024, 0.2963881894766567, 0.29369234751506024, 0.2930717008659638, 0.2915053769766567, 0.2921157285391567, 0.2930922910391567, 0.29100680064006024, 0.29198336314006024, 0.2927260801016567, 0.2937026426016567, 0.2934585019766567, 0.294811570500753, 0.29703972138554224, 0.2983927899096386, 0.29853545039533136, 0.2990340267319277, 0.3027167262801205, 0.3034594432417168, 0.3049242869917168, 0.30493458207831325, 0.3047919215926205, 0.30356092338102414, 0.3062670604292168, 0.3067553416792168, 0.3084643260542168, 0.3078539744917168, 0.3080981151167168, 0.3077319041792168, 0.3084643260542168, 0.3086981715926205, 0.3072127376694277, 0.3081893001694277, 0.30905408744352414, 0.30978650931852414, 0.3106615916792168, 0.3099291698042168, 0.30880994681852414, 0.30830107539533136, 0.30805693477033136, 0.30805693477033136, 0.3087996517319277, 0.30817900508283136, 0.30891142695783136, 0.31013213008283136, 0.3106101162462349, 0.3098673992846386, 0.30912468232304224, 0.30875847138554224, 0.3102336102221386, 0.3112101727221386, 0.3104777508471386, 0.3112101727221386, 0.3123396907944277, 0.31159697383283136, 0.3123396907944277, 0.31283826713102414, 0.3120955501694277], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.0440101841560964, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 32, "valid_ratio": 0.15, "learning_rate": 0.003528366816714989, "optimization": "nesterov_momentum", "nb_data_augmentation": 0, "learning_rate_decay_method": "sqrt", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 1.5027330828733231e-06, "rotation_range": [0, 0], "momentum": 0.5467226178300999}, "accuracy_valid_max": 0.7089931993599398, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.6879044498305723, "accuracy_valid_std": [0.01438408137131042, 0.014499277549966102, 0.018216708907444482, 0.023342056131607627, 0.01916134117862021, 0.018709782845638984, 0.015876326271663282, 0.010790775501726982, 0.016787120165912335, 0.01639157473119524, 0.01582672100418168, 0.016568258921267242, 0.016742736174570526, 0.01782366444743171, 0.015689110808057436, 0.01612142799079347, 0.0180231343528023, 0.017879715121119983, 0.018922580181276932, 0.015954314515829043, 0.015152264501714847, 0.01326316270330243, 0.013443074891941317, 0.01254396517319493, 0.013740355468588974, 0.01378068962330789, 0.012034958657522958, 0.012945751327294603, 0.010188170151536653, 0.011579080190997886, 0.012094999574146954, 0.012804265876267098, 0.010514144840967792, 0.00974921338274257, 0.010005712094419134, 0.011615299601358299, 0.010413067120502675, 0.010963698943738803, 0.012286280335381164, 0.011891550355087738, 0.012872348184802079, 0.013244692333277654, 0.012087925116462332, 0.013058013408992137, 0.011034829761572057, 0.010103324376048755, 0.010930668084444374, 0.010914625883929396, 0.010981106690562278, 0.010820071841990174, 0.010778843891412905, 0.011142271353376766, 0.009363916927947195, 0.00919095379011285, 0.009146945455030467, 0.008071223409459836, 0.007564307218517403, 0.007641476641141707, 0.008081083440307552, 0.006894269613410251, 0.0063605183811914195, 0.005779842554646363, 0.0060252393577545985, 0.004639133261067002, 0.00697823331654173, 0.00839013194679885, 0.008422722002991555, 0.008735327460445481, 0.009892859117629037, 0.00956607967183876, 0.011156295232725568, 0.012144408327322104, 0.011888872439313088, 0.010657641494929027, 0.010666868770872586, 0.011287763278095607, 0.010358789535673167, 0.011028529184552197, 0.011199300543162982, 0.011228529827447835, 0.011278022905848544], "accuracy_valid": [0.459582960749247, 0.5107936629329819, 0.5730421686746988, 0.6012521766754518, 0.620936382247741, 0.6355554052146084, 0.647803616810994, 0.6580472279743976, 0.6674275225903614, 0.6712219973644578, 0.6755959384412651, 0.6806214114269578, 0.6856777696724398, 0.6885059770331325, 0.6906929475715362, 0.6937550004706325, 0.6936432252447289, 0.6944977174322289, 0.6975597703313253, 0.6990040239081325, 0.7003467973456325, 0.7018013460090362, 0.7027676134224398, 0.7036221056099398, 0.7036118105233433, 0.7063076524849398, 0.7069282991340362, 0.7084946230233433, 0.7078842714608433, 0.7069077089608433, 0.7089931993599398, 0.7080166368599398, 0.7072739198983433, 0.7062973573983433, 0.7065414980233433, 0.705188429499247, 0.7029602786144578, 0.7016072100903614, 0.7014645496046686, 0.7009659732680723, 0.6972832737198795, 0.6965405567582832, 0.6950757130082832, 0.6950654179216867, 0.6952080784073795, 0.6964390766189759, 0.6937329395707832, 0.6932446583207832, 0.6915356739457832, 0.6921460255082832, 0.6919018848832832, 0.6922680958207832, 0.6915356739457832, 0.6913018284073795, 0.6927872623305723, 0.6918106998305723, 0.6909459125564759, 0.6902134906814759, 0.6893384083207832, 0.6900708301957832, 0.6911900531814759, 0.6916989246046686, 0.6919430652296686, 0.6919430652296686, 0.6912003482680723, 0.6918209949171686, 0.6910885730421686, 0.6898678699171686, 0.6893898837537651, 0.6901326007153614, 0.6908753176769578, 0.6912415286144578, 0.6897663897778614, 0.6887898272778614, 0.6895222491528614, 0.6887898272778614, 0.6876603092055723, 0.6884030261671686, 0.6876603092055723, 0.6871617328689759, 0.6879044498305723], "seed": 373745540, "model": "residualv3", "loss_std": [0.27001896500587463, 0.19525370001792908, 0.19339558482170105, 0.19275914132595062, 0.19095642864704132, 0.18889284133911133, 0.18679511547088623, 0.1848210096359253, 0.18297657370567322, 0.18120335042476654, 0.1795828938484192, 0.17771197855472565, 0.17589055001735687, 0.17444562911987305, 0.17279395461082458, 0.17117895185947418, 0.16934865713119507, 0.16751264035701752, 0.1657087355852127, 0.16398103535175323, 0.1621626317501068, 0.16026856005191803, 0.15829671919345856, 0.15637798607349396, 0.15436001121997833, 0.15222717821598053, 0.1500968337059021, 0.14795435965061188, 0.14575816690921783, 0.14345385134220123, 0.14123061299324036, 0.13877424597740173, 0.1361926794052124, 0.13353446125984192, 0.1306929588317871, 0.1277981549501419, 0.12477684020996094, 0.1217874065041542, 0.11871756613254547, 0.11542060226202011, 0.11211922019720078, 0.10868725925683975, 0.10499884188175201, 0.10138867795467377, 0.09818150103092194, 0.09441382437944412, 0.09070702642202377, 0.08684223145246506, 0.0829642042517662, 0.07898364216089249, 0.07518270611763, 0.07137463241815567, 0.06759753078222275, 0.06372356414794922, 0.06009649112820625, 0.05652203410863876, 0.05308796465396881, 0.04968851059675217, 0.04661177098751068, 0.04356038197875023, 0.04069671779870987, 0.03801834583282471, 0.03549424931406975, 0.03313051909208298, 0.030925815925002098, 0.02880258299410343, 0.02684652991592884, 0.02496400661766529, 0.023221323266625404, 0.021609244868159294, 0.02009148895740509, 0.01871665194630623, 0.017369674518704414, 0.016180217266082764, 0.015050405636429787, 0.014008449390530586, 0.013093637302517891, 0.012266657315194607, 0.01152050867676735, 0.010827090591192245, 0.010194540955126286]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:21 2016", "state": "available"}], "summary": "c344b367a6b9b062bebae52055fdec23"}