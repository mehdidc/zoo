{"content": {"hp_model": {"f0": 32, "f1": 32, "f2": 32, "f3": 64, "nonlin": "rectify", "nbg1": 5, "nbg3": 3, "nbg2": 8, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.5395113229751587, 1.0939172506332397, 0.8996734619140625, 0.7860168218612671, 0.7091983556747437, 0.6517075300216675, 0.6068485379219055, 0.5710739493370056, 0.5438486933708191, 0.5191857814788818, 0.4971383213996887, 0.4785763919353485, 0.46091488003730774, 0.4460415244102478, 0.4326046109199524, 0.4208928346633911, 0.4053950011730194, 0.39857217669487, 0.38694801926612854, 0.3786412179470062, 0.3706895709037781, 0.3616195023059845, 0.3547550439834595, 0.34898561239242554, 0.3440096080303192, 0.33715057373046875, 0.32970038056373596, 0.32826298475265503, 0.3207986652851105, 0.31584858894348145, 0.31044021248817444, 0.3067607283592224, 0.30247125029563904, 0.3013933002948761, 0.29666250944137573, 0.29552212357521057, 0.29134809970855713, 0.2873440682888031, 0.28284430503845215, 0.28285032510757446, 0.2776672840118408, 0.28008678555488586, 0.2778758704662323, 0.2732805013656616, 0.27034562826156616, 0.2708054482936859, 0.2638378143310547, 0.2661970257759094, 0.2675805985927582, 0.26359570026397705, 0.2595467269420624, 0.25964441895484924, 0.2542749047279358, 0.2606910765171051, 0.2522265315055847, 0.2527165412902832, 0.2530479431152344, 0.2511311173439026, 0.25055763125419617, 0.25049322843551636, 0.2468625009059906, 0.24626685678958893, 0.24919584393501282, 0.24517515301704407, 0.24196857213974, 0.24494539201259613, 0.24157816171646118, 0.24051770567893982, 0.24074970185756683, 0.24315781891345978, 0.2341998815536499, 0.23723290860652924, 0.23807433247566223, 0.2377028614282608, 0.23690493404865265, 0.2357783019542694, 0.23454679548740387, 0.2315777987241745, 0.23417934775352478, 0.23145301640033722, 0.23401424288749695, 0.22711828351020813, 0.2334171086549759, 0.2264508455991745, 0.22747968137264252, 0.2264898270368576, 0.22913014888763428, 0.22550229728221893, 0.1988004595041275, 0.17777389287948608, 0.1713012158870697, 0.16680030524730682, 0.1644822210073471, 0.16229748725891113, 0.15891431272029877, 0.15842600166797638, 0.15511064231395721, 0.1539260447025299, 0.15217487514019012, 0.15092356503009796, 0.14873726665973663, 0.14818601310253143, 0.14713335037231445, 0.14589668810367584, 0.14491324126720428, 0.14310477674007416, 0.14317633211612701, 0.14129841327667236, 0.13992567360401154, 0.14019234478473663, 0.1398845762014389, 0.1401992291212082, 0.14048826694488525, 0.13996994495391846, 0.1395675539970398, 0.1395946443080902, 0.13958393037319183, 0.13940005004405975, 0.13980458676815033, 0.14040659368038177, 0.1404198706150055, 0.14043211936950684, 0.14011988043785095, 0.13970912992954254, 0.1400526911020279, 0.14057238399982452, 0.1401565968990326, 0.14035020768642426, 0.1404963731765747, 0.1398821771144867, 0.13984882831573486, 0.14008355140686035, 0.13972048461437225, 0.13967138528823853, 0.140009343624115, 0.13983000814914703, 0.14002934098243713, 0.14048604667186737, 0.1397361159324646, 0.1399896889925003, 0.14027617871761322, 0.14000794291496277, 0.13976101577281952, 0.14023159444332123, 0.1402180939912796, 0.13995245099067688, 0.1400357186794281, 0.13962428271770477, 0.14028090238571167], "moving_avg_accuracy_train": [0.06008470588235293, 0.1231915294117647, 0.1860182588235294, 0.24641878588235294, 0.30397690729411764, 0.35784509891764704, 0.4084582360847059, 0.45477711835858825, 0.4976147006403765, 0.5362249952822212, 0.5713342604598814, 0.6035278932374227, 0.6333068686195629, 0.6602797111693712, 0.6842682106406693, 0.7065355072236612, 0.7273125447365891, 0.746155407909989, 0.7622786906484019, 0.7779261157012088, 0.7930299747193232, 0.8067481537179793, 0.8172592206991225, 0.8279685927468573, 0.8381952628839363, 0.8477216189484839, 0.8563612217595178, 0.8646356878188601, 0.8725132955075624, 0.8791607894862179, 0.8855435340670079, 0.8915515336014835, 0.8970881449472176, 0.902080506923084, 0.9067689268190108, 0.9107932106076979, 0.9143727130763399, 0.9183989711804706, 0.9209873093565412, 0.9235379901855929, 0.926875955872916, 0.9290424779326832, 0.9319899948452972, 0.9340004071254734, 0.9353368370011613, 0.9366337415363393, 0.9384621320885878, 0.9393947424091407, 0.9413211505211677, 0.9425160942925804, 0.9432197789809694, 0.9440789775534607, 0.9456428445039969, 0.9473467953477149, 0.9489579981658847, 0.9501351395257668, 0.9509733902790725, 0.9516572277217535, 0.9524091520084017, 0.9528811779840322, 0.9544095307738643, 0.9558109306376543, 0.9558227787503595, 0.9562263832282647, 0.9564555096113205, 0.956722899826659, 0.9565988451381108, 0.9574448429772409, 0.9573685939736344, 0.9584834992821533, 0.9589175022951144, 0.9594375167714853, 0.9602420003884543, 0.960768388584903, 0.9609291967852363, 0.9615186300478891, 0.9615997082195707, 0.9613950315152607, 0.9615614107166758, 0.9622852696450082, 0.9627838015040368, 0.9630183625301036, 0.9633847615712109, 0.9636204030611486, 0.9642254215785632, 0.9650664088324715, 0.9659127091256949, 0.9654955558601843, 0.9685060002741659, 0.9713659884820435, 0.9739846837514862, 0.976343862435161, 0.9785188879563508, 0.9804528815136568, 0.9822358286564088, 0.9838451869672385, 0.9853289035646323, 0.9866289543846397, 0.9878084118873522, 0.9888652177574405, 0.9898751665699317, 0.9907488263835268, 0.9915704143334094, 0.9922839611353627, 0.992928506198297, 0.993489773225526, 0.99398314884415, 0.9945142457244408, 0.9949922329167026, 0.9954341860956205, 0.9958178263095878, 0.9961701613256879, 0.9964849098990014, 0.9967658306738071, 0.9970233652534852, 0.997252793434019, 0.9974592787964993, 0.9976474685639083, 0.9978191922957528, 0.9979690377720598, 0.9981062516419127, 0.9982297441247803, 0.9983385344181845, 0.998438798623425, 0.9985313893493178, 0.9986147210026214, 0.9986897194905946, 0.9987572181297705, 0.9988203198462052, 0.9988676996262905, 0.9989126943695439, 0.9989531896384718, 0.9989966942040364, 0.9990358483130445, 0.9990687340699753, 0.9990912724276836, 0.9991139098907975, 0.9991342836076, 0.9991573258350753, 0.9991733579574502, 0.999185433926411, 0.9991963022984758, 0.9992060838333341, 0.9992195930970595, 0.9992341043755889, 0.9992448115850888, 0.9992544480736388, 0.9992631209133338, 0.9992662205867062], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.05991999999999998, 0.12195466666666663, 0.1829725333333333, 0.24175527999999996, 0.29764641866666663, 0.3485884434666666, 0.39624959911999996, 0.4400913058746666, 0.48058884195386664, 0.5169032910918133, 0.5488929619826319, 0.5784836657843687, 0.6056219658725985, 0.6298864359520053, 0.6515644590234715, 0.671154679787791, 0.6896792118090118, 0.7062579572947774, 0.7198721615652997, 0.7329649454087697, 0.7457084508678927, 0.7566309391144368, 0.7652211785363264, 0.7736990606826938, 0.7818758212810911, 0.7892349058196487, 0.7958314152376837, 0.8025282737139154, 0.8091421130091906, 0.8144412350416048, 0.8195171115374443, 0.8243520670503666, 0.8284635270119965, 0.8326305076441302, 0.8364474568797171, 0.8391893778584121, 0.8414171067392375, 0.8442220627319804, 0.8460531897921156, 0.8479278708129041, 0.8501750837316138, 0.851984242025119, 0.8541724844892737, 0.8554085693736797, 0.8563077124363118, 0.8561569411926806, 0.8572345804067458, 0.8576711223660712, 0.858904010129464, 0.8597336091165176, 0.8598402482048658, 0.8604828900510458, 0.8615279343792747, 0.8622418076080138, 0.8636842935138791, 0.8645825308291578, 0.8654709444129087, 0.8656571833049512, 0.8660514649744562, 0.8661129851436773, 0.8669816866293096, 0.867856851299712, 0.8677911661697408, 0.8680787162194334, 0.8685241779308234, 0.8687250934710744, 0.8683459174573003, 0.8692446590449036, 0.8688401931404132, 0.8698761738263718, 0.870421889777068, 0.8707930341326946, 0.8708337307194252, 0.8712703576474826, 0.8710766552160677, 0.8717156563611276, 0.8719040907250148, 0.8718203483191801, 0.8719849801539288, 0.8724931488052026, 0.8726438339246823, 0.8727527838655473, 0.8729975054789926, 0.8735777549310932, 0.8740333127713172, 0.8749766481608522, 0.875518983344767, 0.8750070850102903, 0.8777863765092614, 0.8803810721916686, 0.8825829649725018, 0.884618001808585, 0.8863562016277264, 0.8878805814649539, 0.8894125233184584, 0.8906046043199459, 0.8916374772212847, 0.892580396165823, 0.893735689882574, 0.8943887875609833, 0.8952032421382182, 0.895829584591063, 0.8964332927986234, 0.8970432968520944, 0.8974723005002182, 0.8978984037835297, 0.8979085634051768, 0.8983710403979924, 0.8987739363581931, 0.8991898760557072, 0.8994708884501365, 0.8997504662717895, 0.8999754196446105, 0.9002712110134827, 0.9004974232454678, 0.9007143475875876, 0.9008962461621621, 0.9010732882126126, 0.9012326260580179, 0.9013360301188829, 0.901375760440328, 0.9014915177296284, 0.9016490326233323, 0.9018041293609991, 0.9019170497582325, 0.9020186781157427, 0.9019901436375017, 0.902084462607085, 0.9021426830130431, 0.9021817480450721, 0.9022169065738982, 0.902141882583175, 0.9022076943248575, 0.9022402582257051, 0.9023228990698012, 0.9023439424961545, 0.9024162149132057, 0.9024279267552184, 0.9024251340796966, 0.9023959540050603, 0.9023963586045544, 0.9023833894107656, 0.902385050469689, 0.9024265454227202, 0.9023972242137814, 0.9024508351257367, 0.9024324182798297, 0.90244250978518, 0.9024515921399954], "moving_var_accuracy_train": [0.032491546928719715, 0.06508463281959861, 0.09410095089485065, 0.11752486882621864, 0.1355888180076598, 0.14814597482599737, 0.15638658422842125, 0.16005687550149508, 0.16056671405308637, 0.15792683631874826, 0.1532280971987108, 0.14723315740157636, 0.14049092803470967, 0.1329896433481895, 0.12486971197533081, 0.11684523325183203, 0.10904587751697138, 0.10133677119841747, 0.09354273629494095, 0.08639203986249573, 0.07980597489139785, 0.07351907331761055, 0.06716150874758822, 0.061477573719740636, 0.056271079386600184, 0.05146073458675705, 0.04698644575867318, 0.042904002279910744, 0.039172112377993576, 0.035652603725960574, 0.03245399820881695, 0.02953346291359159, 0.026856003208975826, 0.02439471599096094, 0.022153075921949553, 0.02008352206986161, 0.018190485404182576, 0.016517333652654023, 0.01492589573801196, 0.013491859918435994, 0.01224295206096011, 0.011060901215383221, 0.010033001797396211, 0.009066077435483138, 0.008175544095248507, 0.007373127338083944, 0.0066659017123795105, 0.006007139399231577, 0.005439824893235174, 0.004908693419463197, 0.004422280626782936, 0.003986696563787383, 0.0036100380259594615, 0.003275165259663779, 0.0029710125043889025, 0.0026863822099803185, 0.0024240679679110445, 0.0021858698739520522, 0.0019723713977525094, 0.0017771395346722872, 0.001620448341456746, 0.0014760788015151481, 0.0013284721847636055, 0.0011970910354585115, 0.001077854422007371, 0.000970712457551963, 0.000873779717888524, 0.0007928431571939867, 0.0007136111666695468, 0.000653437174625265, 0.0005897886847000726, 0.0005332435517307832, 0.0004857439415674504, 0.0004396633082109499, 0.00039592971088550465, 0.00035946362393704843, 0.00032357642457265276, 0.00029159581509497233, 0.00026268537193344685, 0.00024113258047324125, 0.00021925612855611568, 0.0001978256855750502, 0.0001792513513334642, 0.00016182595840613846, 0.0001489377892232552, 0.00014040934635205704, 0.000132814429393642, 0.00012109913807661377, 0.00019055420439600805, 0.0002451145768991937, 0.00028232120343708937, 0.00030418059964693533, 0.0003163391638426836, 0.00031836822717572894, 0.0003151415090827827, 0.00030693766572823433, 0.00029605663362784724, 0.00028166215947647875, 0.0002660160235351741, 0.00024946596900513463, 0.00023369934153929356, 0.0002171989406143828, 0.00020155410738747543, 0.00018598103799592636, 0.00017112187923971079, 0.0001568448773984314, 0.0001433511651680606, 0.00013155462371754694, 0.00012045540714948912, 0.00011016776994574155, 0.00010047561127512373, 9.154530981974332e-05, 8.328237881739532e-05, 7.566438927111292e-05, 6.86948668815715e-05, 6.229911580362195e-05, 5.645293006752795e-05, 5.1126375557791874e-05, 4.627913936271999e-05, 4.185330842737524e-05, 3.783742639935766e-05, 3.419093729934504e-05, 3.087836152086154e-05, 2.7881001566447733e-05, 2.5170058792495145e-05, 2.2715550393226294e-05, 2.0494618312687997e-05, 1.8486161078034502e-05, 1.667338140978412e-05, 1.5026246860854142e-05, 1.35418429170526e-05, 1.2202417426597277e-05, 1.0999209508962222e-05, 9.913085956335989e-06, 8.931510617782621e-06, 8.042931354118065e-06, 7.243250311332397e-06, 6.522661075226333e-06, 5.875173465926906e-06, 5.289969379864785e-06, 4.7622849031154e-06, 4.287119506405906e-06, 3.859268661582972e-06, 3.474984297282301e-06, 3.1293810623950754e-06, 2.8174747551730485e-06, 2.5365630368599025e-06, 2.28358369650928e-06, 2.055311798633498e-06], "duration": 139302.799722, "accuracy_train": [0.6008470588235294, 0.6911529411764706, 0.7514588235294117, 0.7900235294117647, 0.822, 0.8426588235294118, 0.8639764705882353, 0.8716470588235294, 0.8831529411764706, 0.8837176470588235, 0.8873176470588235, 0.8932705882352941, 0.9013176470588236, 0.903035294117647, 0.900164705882353, 0.9069411764705883, 0.9143058823529412, 0.9157411764705883, 0.9073882352941176, 0.9187529411764706, 0.9289647058823529, 0.9302117647058824, 0.9118588235294117, 0.9243529411764706, 0.930235294117647, 0.9334588235294118, 0.9341176470588235, 0.9391058823529411, 0.9434117647058824, 0.9389882352941177, 0.9429882352941177, 0.9456235294117648, 0.9469176470588235, 0.9470117647058823, 0.9489647058823529, 0.9470117647058823, 0.9465882352941176, 0.954635294117647, 0.9442823529411765, 0.9464941176470588, 0.9569176470588235, 0.9485411764705882, 0.9585176470588235, 0.9520941176470589, 0.9473647058823529, 0.9483058823529412, 0.9549176470588235, 0.9477882352941176, 0.9586588235294118, 0.9532705882352941, 0.9495529411764706, 0.9518117647058824, 0.9597176470588236, 0.9626823529411764, 0.9634588235294118, 0.9607294117647058, 0.9585176470588235, 0.9578117647058824, 0.9591764705882353, 0.9571294117647059, 0.9681647058823529, 0.9684235294117647, 0.9559294117647059, 0.9598588235294118, 0.9585176470588235, 0.9591294117647059, 0.9554823529411764, 0.9650588235294117, 0.9566823529411764, 0.9685176470588235, 0.9628235294117647, 0.9641176470588235, 0.9674823529411765, 0.9655058823529412, 0.9623764705882353, 0.9668235294117647, 0.9623294117647059, 0.9595529411764706, 0.9630588235294117, 0.9688, 0.9672705882352941, 0.9651294117647059, 0.9666823529411764, 0.9657411764705882, 0.9696705882352942, 0.972635294117647, 0.9735294117647059, 0.9617411764705882, 0.9956, 0.9971058823529412, 0.9975529411764706, 0.9975764705882353, 0.9980941176470588, 0.9978588235294118, 0.9982823529411765, 0.9983294117647059, 0.9986823529411765, 0.9983294117647059, 0.9984235294117647, 0.9983764705882353, 0.998964705882353, 0.9986117647058823, 0.998964705882353, 0.9987058823529412, 0.9987294117647059, 0.9985411764705883, 0.9984235294117647, 0.9992941176470588, 0.9992941176470588, 0.9994117647058823, 0.9992705882352941, 0.9993411764705883, 0.9993176470588235, 0.9992941176470588, 0.9993411764705883, 0.9993176470588235, 0.9993176470588235, 0.9993411764705883, 0.9993647058823529, 0.9993176470588235, 0.9993411764705883, 0.9993411764705883, 0.9993176470588235, 0.9993411764705883, 0.9993647058823529, 0.9993647058823529, 0.9993647058823529, 0.9993647058823529, 0.9993882352941177, 0.9992941176470588, 0.9993176470588235, 0.9993176470588235, 0.9993882352941177, 0.9993882352941177, 0.9993647058823529, 0.9992941176470588, 0.9993176470588235, 0.9993176470588235, 0.9993647058823529, 0.9993176470588235, 0.9992941176470588, 0.9992941176470588, 0.9992941176470588, 0.9993411764705883, 0.9993647058823529, 0.9993411764705883, 0.9993411764705883, 0.9993411764705883, 0.9992941176470588], "end": "2016-02-07 08:42:32.957000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0], "moving_var_accuracy_valid": [0.03231365759999999, 0.06371699065599999, 0.09085391206335999, 0.1128672226081216, 0.1296948747804185, 0.140081196318891, 0.1465173485109033, 0.14916447092027263, 0.14900847768462033, 0.14597628286189093, 0.14058870596902784, 0.13441032313546414, 0.12759767680702705, 0.12013678970043419, 0.11235254088897394, 0.10457127754642953, 0.09720257437123372, 0.08995601015104646, 0.08262852815721737, 0.0759084642404421, 0.06977919019887818, 0.0638749779244534, 0.05815161005193655, 0.05298331941793214, 0.04828672220109045, 0.04394545510819214, 0.03994253502589296, 0.03635191274435975, 0.033110407301937296, 0.03005209282057333, 0.02727876423832514, 0.02476127896780006, 0.022437287998164836, 0.020349832746645546, 0.018445971385184425, 0.016669037422546648, 0.015046798663990158, 0.013612928800682163, 0.012281813157407184, 0.011085261702033805, 0.010022185224948563, 0.009049424186032337, 0.008187577413166475, 0.007382570824422939, 0.006651589866204359, 0.005986635467295079, 0.005398423677046792, 0.004860296429282379, 0.004387946896488256, 0.003955346317153314, 0.0035599140324944563, 0.0032076395261271665, 0.0028967046323461177, 0.0026116207039919006, 0.0023691855238902907, 0.0021395284439722946, 0.001932679107837203, 0.0017397233613776655, 0.0015671501475540668, 0.001410469195379649, 0.0012762140562819417, 0.001155485869456632, 0.001039976113337663, 0.0009367226672836006, 0.0008448363257820711, 0.0007607159966926932, 0.0006859383670682185, 0.000624614158332986, 0.0005636250765107444, 0.0005169218726947848, 0.00046790993851490405, 0.0004223586778578345, 0.00038013771598159466, 0.0003438397320521795, 0.0003097934445343861, 0.00028248900225143816, 0.0002545596696117367, 0.00022916681776537806, 0.00020649406875795462, 0.00018816878028539595, 0.00016955625630395, 0.00015270746148008562, 0.00013797571334486233, 0.00012720834685034507, 0.00011635530867741663, 0.00011272871272401626, 0.00010410298851702152, 9.605104880887959e-05, 0.00015596609505426215, 0.00020096149670755894, 0.00022450033340136875, 0.00023932267437917, 0.00024258245444262513, 0.00023923781399167013, 0.00023643564517517555, 0.0002255815946846247, 0.00021262487308904143, 0.00019936425100385848, 0.00019144015805115246, 0.00017613497144392936, 0.0001644915006249479, 0.00015157309437657438, 0.00013969595733779875, 0.00012907530611127823, 0.00011782417267108293, 0.00010767583147641436, 9.690917728998103e-05, 8.91432242809372e-05, 8.16898282455583e-05, 7.507789790871496e-05, 6.828081981024938e-05, 6.215621165446668e-05, 5.6396026668512146e-05, 5.1543856806755026e-05, 4.685001889117648e-05, 4.258852253389614e-05, 3.862745410339688e-05, 3.504680368170649e-05, 3.177062025434178e-05, 2.8689789827137716e-05, 2.583501733040309e-05, 2.3372113347598654e-05, 2.125820048848553e-05, 1.934887542195085e-05, 1.7528747024757966e-05, 1.5868827229733943e-05, 1.4289272454796886e-05, 1.2940409821526256e-05, 1.1676875380403034e-05, 1.0522922532909583e-05, 9.48175537896158e-06, 8.58423723372167e-06, 7.764794178439068e-06, 6.9978584293408615e-06, 6.359538168423109e-06, 5.727569783714944e-06, 5.201822525741283e-06, 4.6828747783571416e-06, 4.2146574918505605e-06, 3.8008550334675407e-06, 3.4207710034275415e-06, 3.080207702972563e-06, 2.7722117647260316e-06, 2.510487068396909e-06, 2.2671759611998494e-06, 2.0663255340058705e-06, 1.8627456025237431e-06, 1.6773875885934964e-06, 1.5103912322550701e-06], "accuracy_test": 0.8757, "start": "2016-02-05 18:00:50.158000", "learning_rate_per_epoch": [0.0009717176435515285, 0.0009717176435515285, 0.0009717176435515285, 0.0009717176435515285, 0.0009717176435515285, 0.0009717176435515285, 0.0009717176435515285, 0.0009717176435515285, 0.0009717176435515285, 0.0009717176435515285, 0.0009717176435515285, 0.0009717176435515285, 0.0009717176435515285, 0.0009717176435515285, 0.0009717176435515285, 0.0009717176435515285, 0.0009717176435515285, 0.0009717176435515285, 0.0009717176435515285, 0.0009717176435515285, 0.0009717176435515285, 0.0009717176435515285, 0.0009717176435515285, 0.0009717176435515285, 0.0009717176435515285, 0.0009717176435515285, 0.0009717176435515285, 0.0009717176435515285, 0.0009717176435515285, 0.0009717176435515285, 0.0009717176435515285, 0.0009717176435515285, 0.0009717176435515285, 0.0009717176435515285, 0.0009717176435515285, 0.0009717176435515285, 0.0009717176435515285, 0.0009717176435515285, 0.0009717176435515285, 0.0009717176435515285, 0.0009717176435515285, 0.0009717176435515285, 0.0009717176435515285, 0.0009717176435515285, 0.0009717176435515285, 0.0009717176435515285, 0.0009717176435515285, 0.0009717176435515285, 0.0009717176435515285, 0.0009717176435515285, 0.0009717176435515285, 0.0009717176435515285, 0.0009717176435515285, 0.0009717176435515285, 0.0009717176435515285, 0.0009717176435515285, 0.0009717176435515285, 0.0009717176435515285, 0.0009717176435515285, 0.0009717176435515285, 0.0009717176435515285, 0.0009717176435515285, 0.0009717176435515285, 0.0009717176435515285, 0.0009717176435515285, 0.0009717176435515285, 0.0009717176435515285, 0.0009717176435515285, 0.0009717176435515285, 0.0009717176435515285, 0.0009717176435515285, 0.0009717176435515285, 0.0009717176435515285, 0.0009717176435515285, 0.0009717176435515285, 0.0009717176435515285, 0.0009717176435515285, 0.0009717176435515285, 0.0009717176435515285, 0.0009717176435515285, 0.0009717176435515285, 0.0009717176435515285, 0.0009717176435515285, 0.0009717176435515285, 0.0009717176435515285, 0.0009717176435515285, 0.0009717176435515285, 9.717176726553589e-05, 9.717176726553589e-05, 9.717176726553589e-05, 9.717176726553589e-05, 9.717176726553589e-05, 9.717176726553589e-05, 9.717176726553589e-05, 9.717176726553589e-05, 9.717176726553589e-05, 9.717176726553589e-05, 9.717176726553589e-05, 9.717176726553589e-05, 9.717176726553589e-05, 9.717176726553589e-05, 9.717176726553589e-05, 9.717176726553589e-05, 9.717176726553589e-05, 9.717176726553589e-05, 9.717176726553589e-05, 9.71717690845253e-06, 9.717176681078854e-07, 9.71717639686176e-08, 9.71717639686176e-09, 9.71717595277255e-10, 9.717175675216794e-11, 9.717175848689141e-12, 9.717176065529576e-13, 9.717176201054847e-14, 9.717176370461437e-15, 9.71717679397791e-16, 9.717176529280114e-17, 9.71717686015236e-18, 9.717177066947513e-19, 9.717176937700542e-20, 9.717176614583115e-21, 9.717177018479899e-22, 9.717177270915388e-23, 9.71717758645975e-24, 9.717177783674977e-25, 9.717177537155944e-26, 9.717177845304735e-27, 9.717177460118746e-28, 9.717177219377503e-29, 9.717177219377503e-30, 9.717177031298407e-31, 9.717177031298407e-32, 9.717177031298407e-33, 9.717177214969399e-34, 9.717176985380659e-35, 9.717176985380659e-36, 9.717176626648252e-37, 9.717176402440498e-38, 9.71717668270019e-39, 9.717178083998655e-40, 9.717164071014011e-41, 9.716603551628282e-42, 9.710998357770982e-43, 9.668959403841238e-44, 9.80908925027372e-45, 1.401298464324817e-45, 0.0, 0.0], "accuracy_train_first": 0.6008470588235294, "accuracy_train_last": 0.9992941176470588, "batch_size_eval": 1024, "accuracy_train_std": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "accuracy_test_std": 0, "error_valid": [0.40080000000000005, 0.3197333333333333, 0.2678666666666667, 0.22919999999999996, 0.19933333333333336, 0.1929333333333333, 0.17479999999999996, 0.16533333333333333, 0.15493333333333337, 0.15626666666666666, 0.1632, 0.1552, 0.15013333333333334, 0.1517333333333334, 0.15333333333333332, 0.1525333333333333, 0.14359999999999995, 0.1445333333333333, 0.15759999999999996, 0.1492, 0.13959999999999995, 0.14506666666666668, 0.15746666666666664, 0.15000000000000002, 0.1445333333333333, 0.1445333333333333, 0.14480000000000004, 0.1372, 0.1313333333333333, 0.1378666666666667, 0.13480000000000003, 0.13213333333333332, 0.13453333333333328, 0.12986666666666669, 0.12919999999999998, 0.13613333333333333, 0.1385333333333333, 0.13053333333333328, 0.13746666666666663, 0.1352, 0.12960000000000005, 0.13173333333333337, 0.12613333333333332, 0.13346666666666662, 0.13560000000000005, 0.1452, 0.13306666666666667, 0.13839999999999997, 0.13, 0.13280000000000003, 0.1392, 0.13373333333333337, 0.12906666666666666, 0.1313333333333333, 0.1233333333333333, 0.1273333333333333, 0.1265333333333334, 0.1326666666666667, 0.13039999999999996, 0.1333333333333333, 0.12519999999999998, 0.12426666666666664, 0.13280000000000003, 0.1293333333333333, 0.12746666666666662, 0.12946666666666662, 0.13506666666666667, 0.1226666666666667, 0.13480000000000003, 0.12080000000000002, 0.1246666666666667, 0.12586666666666668, 0.12880000000000003, 0.12480000000000002, 0.1306666666666667, 0.12253333333333338, 0.12639999999999996, 0.12893333333333334, 0.1265333333333334, 0.12293333333333334, 0.126, 0.12626666666666664, 0.12480000000000002, 0.12119999999999997, 0.12186666666666668, 0.11653333333333338, 0.11960000000000004, 0.12960000000000005, 0.09719999999999995, 0.09626666666666661, 0.09760000000000002, 0.09706666666666663, 0.09799999999999998, 0.09840000000000004, 0.0968, 0.09866666666666668, 0.09906666666666664, 0.09893333333333332, 0.09586666666666666, 0.09973333333333334, 0.0974666666666667, 0.09853333333333336, 0.0981333333333333, 0.0974666666666667, 0.09866666666666668, 0.09826666666666661, 0.10199999999999998, 0.0974666666666667, 0.09760000000000002, 0.09706666666666663, 0.09799999999999998, 0.09773333333333334, 0.09799999999999998, 0.09706666666666663, 0.0974666666666667, 0.09733333333333338, 0.0974666666666667, 0.09733333333333338, 0.09733333333333338, 0.09773333333333334, 0.09826666666666661, 0.0974666666666667, 0.09693333333333332, 0.0968, 0.09706666666666663, 0.09706666666666663, 0.09826666666666661, 0.09706666666666663, 0.09733333333333338, 0.0974666666666667, 0.0974666666666667, 0.09853333333333336, 0.09719999999999995, 0.0974666666666667, 0.09693333333333332, 0.0974666666666667, 0.09693333333333332, 0.0974666666666667, 0.09760000000000002, 0.09786666666666666, 0.09760000000000002, 0.09773333333333334, 0.09760000000000002, 0.09719999999999995, 0.09786666666666666, 0.09706666666666663, 0.09773333333333334, 0.0974666666666667, 0.0974666666666667], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.032264133542047935, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 64, "valid_ratio": 0.15, "learning_rate": 0.0009717176661835874, "optimization": "adam", "nb_data_augmentation": 3, "learning_rate_decay_method": "discrete", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 1.548598876070436e-05, "rotation_range": [0, 0], "momentum": 0.728335388914357}, "accuracy_valid_max": 0.9041333333333333, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        y_pred = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            y_pred.extend((nnet.predict(X[mini_batch]) == y[mini_batch]).tolist())\n        return np.mean(y_pred)\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            acc = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = acc\n            status[\"accuracy_train_std\"] = 0\n            acc = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = acc\n            status[\"accuracy_valid_std\"] = 0\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n\n    # rescaling to [-1, 1]\n    X_min = X_train.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X_train.max(axis=(0, 2, 3))[None, :, None, None]\n    def preprocess(a):\n        return (a / 255.) * 2 - 1\n        # return 2 * ((a - X_min) / (X_max - X_min)) - 1\n    X_train = preprocess(X_train)\n    X_valid = preprocess(X_valid)\n\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = preprocess(X_test)\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    acc = evaluate(X_test, y_test, batch_size_eval)\n    light.set(\"accuracy_test\", acc)\n    light.set(\"accuracy_test_std\", 0)\n    print(\"Test accuracy : {}+-{}\".format(acc, 0))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.9025333333333333, "accuracy_valid_std": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "accuracy_valid": [0.5992, 0.6802666666666667, 0.7321333333333333, 0.7708, 0.8006666666666666, 0.8070666666666667, 0.8252, 0.8346666666666667, 0.8450666666666666, 0.8437333333333333, 0.8368, 0.8448, 0.8498666666666667, 0.8482666666666666, 0.8466666666666667, 0.8474666666666667, 0.8564, 0.8554666666666667, 0.8424, 0.8508, 0.8604, 0.8549333333333333, 0.8425333333333334, 0.85, 0.8554666666666667, 0.8554666666666667, 0.8552, 0.8628, 0.8686666666666667, 0.8621333333333333, 0.8652, 0.8678666666666667, 0.8654666666666667, 0.8701333333333333, 0.8708, 0.8638666666666667, 0.8614666666666667, 0.8694666666666667, 0.8625333333333334, 0.8648, 0.8704, 0.8682666666666666, 0.8738666666666667, 0.8665333333333334, 0.8644, 0.8548, 0.8669333333333333, 0.8616, 0.87, 0.8672, 0.8608, 0.8662666666666666, 0.8709333333333333, 0.8686666666666667, 0.8766666666666667, 0.8726666666666667, 0.8734666666666666, 0.8673333333333333, 0.8696, 0.8666666666666667, 0.8748, 0.8757333333333334, 0.8672, 0.8706666666666667, 0.8725333333333334, 0.8705333333333334, 0.8649333333333333, 0.8773333333333333, 0.8652, 0.8792, 0.8753333333333333, 0.8741333333333333, 0.8712, 0.8752, 0.8693333333333333, 0.8774666666666666, 0.8736, 0.8710666666666667, 0.8734666666666666, 0.8770666666666667, 0.874, 0.8737333333333334, 0.8752, 0.8788, 0.8781333333333333, 0.8834666666666666, 0.8804, 0.8704, 0.9028, 0.9037333333333334, 0.9024, 0.9029333333333334, 0.902, 0.9016, 0.9032, 0.9013333333333333, 0.9009333333333334, 0.9010666666666667, 0.9041333333333333, 0.9002666666666667, 0.9025333333333333, 0.9014666666666666, 0.9018666666666667, 0.9025333333333333, 0.9013333333333333, 0.9017333333333334, 0.898, 0.9025333333333333, 0.9024, 0.9029333333333334, 0.902, 0.9022666666666667, 0.902, 0.9029333333333334, 0.9025333333333333, 0.9026666666666666, 0.9025333333333333, 0.9026666666666666, 0.9026666666666666, 0.9022666666666667, 0.9017333333333334, 0.9025333333333333, 0.9030666666666667, 0.9032, 0.9029333333333334, 0.9029333333333334, 0.9017333333333334, 0.9029333333333334, 0.9026666666666666, 0.9025333333333333, 0.9025333333333333, 0.9014666666666666, 0.9028, 0.9025333333333333, 0.9030666666666667, 0.9025333333333333, 0.9030666666666667, 0.9025333333333333, 0.9024, 0.9021333333333333, 0.9024, 0.9022666666666667, 0.9024, 0.9028, 0.9021333333333333, 0.9029333333333334, 0.9022666666666667, 0.9025333333333333, 0.9025333333333333], "seed": 939087717, "model": "residualv5", "loss_std": [0.26999956369400024, 0.1412585824728012, 0.12798911333084106, 0.12184060364961624, 0.1160857155919075, 0.10983159393072128, 0.10453513264656067, 0.10143787413835526, 0.10023291409015656, 0.09585949033498764, 0.0939287319779396, 0.09070547670125961, 0.08900120109319687, 0.08672121912240982, 0.08548032492399216, 0.08240679651498795, 0.07919077575206757, 0.0789904072880745, 0.07741272449493408, 0.07405418902635574, 0.07335568219423294, 0.06987364590167999, 0.06874650716781616, 0.06996754556894302, 0.06665289402008057, 0.06345953792333603, 0.06335291266441345, 0.06642860174179077, 0.0610252320766449, 0.059965912252664566, 0.05723248049616814, 0.058965861797332764, 0.055911045521497726, 0.058023933321237564, 0.056170493364334106, 0.05448093265295029, 0.05396082624793053, 0.050193559378385544, 0.051018036901950836, 0.054815031588077545, 0.04871737211942673, 0.05025535076856613, 0.04765314608812332, 0.04696843400597572, 0.04709416627883911, 0.04553770646452904, 0.04571341350674629, 0.0453423336148262, 0.046637412160634995, 0.04507967829704285, 0.04333396255970001, 0.0438184030354023, 0.041788529604673386, 0.04360160604119301, 0.04213244467973709, 0.042037274688482285, 0.04312629625201225, 0.03999284654855728, 0.04368029162287712, 0.04028484970331192, 0.04116930067539215, 0.04140292853116989, 0.041033849120140076, 0.03923375532031059, 0.04072151333093643, 0.03924969956278801, 0.03847387433052063, 0.0382753424346447, 0.03821595013141632, 0.039434973150491714, 0.03794408217072487, 0.03771226108074188, 0.03878914192318916, 0.03756295517086983, 0.039260417222976685, 0.03703465312719345, 0.03781751170754433, 0.03509513661265373, 0.0383625254034996, 0.03654920682311058, 0.03727841004729271, 0.03538928180932999, 0.039064791053533554, 0.03296823799610138, 0.03475465625524521, 0.03334219008684158, 0.03518950194120407, 0.034350037574768066, 0.032594405114650726, 0.0189540833234787, 0.015781456604599953, 0.014276603236794472, 0.013820189982652664, 0.013347664847970009, 0.011448010802268982, 0.012308225966989994, 0.010903831571340561, 0.01075396966189146, 0.010286407545208931, 0.010093499906361103, 0.009964060969650745, 0.009280841797590256, 0.009571903385221958, 0.009226369671523571, 0.0095253586769104, 0.008228400722146034, 0.00901886261999607, 0.008863963186740875, 0.008252894505858421, 0.007365076802670956, 0.007574256043881178, 0.008178303018212318, 0.008202495984733105, 0.007253783755004406, 0.007269673515111208, 0.007286377716809511, 0.007580948062241077, 0.0075177294202148914, 0.007266966626048088, 0.008086906746029854, 0.008056635037064552, 0.008034550584852695, 0.007299448829144239, 0.007525234948843718, 0.00799501407891512, 0.008122524246573448, 0.007314426824450493, 0.008509737439453602, 0.008030534721910954, 0.007356112357228994, 0.007385737262666225, 0.007515036500990391, 0.007067797239869833, 0.007354363799095154, 0.007570913061499596, 0.007282453589141369, 0.0076903305016458035, 0.008170831017196178, 0.008186304941773415, 0.00806557759642601, 0.00784209556877613, 0.008166598156094551, 0.007572558242827654, 0.008026955649256706, 0.007839001715183258, 0.007561441510915756, 0.007768262643367052, 0.007850860245525837, 0.008181506767868996]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:43 2016", "state": "available"}], "summary": "e5d98c69d5deb76f72f0952ec2f4e278"}