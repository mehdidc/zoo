{"content": {"hp_model": {"f1": 13, "f2": 91, "f3": 13, "nonlin": "rectify", "ds2": 2569, "ds1": 3942, "do2": 0.09617842479765037, "do3": 0.9851653739790414, "do1": 0.7974526769868961, "do4": 0.30480217077582294, "do5": 0.004071075443253469}, "accuracy_valid_std": [0.02667842090354967, 0.024326374369545923, 0.025960450110665455, 0.025544774256870965, 0.02494646979721462, 0.02727831317796278, 0.027611503840960807, 0.02742061241444453, 0.02742061241444453, 0.02742061241444453, 0.02742061241444453, 0.02742061241444453, 0.02742061241444453, 0.02742061241444453, 0.02742061241444453, 0.02742061241444453, 0.02742061241444453, 0.02916458750587146, 0.025386605170907014, 0.025257632844671835, 0.02550318975439411, 0.02550318975439411, 0.02550318975439411, 0.02554761516930023, 0.02554761516930023, 0.025920932472898537, 0.025920932472898537, 0.026203921611325662, 0.026402580103318373, 0.026443436379356006, 0.026403610870271207, 0.026720892483167228, 0.02643142638330064, 0.026461612568050723, 0.026402236505391415, 0.026225723178213287, 0.0260149067945573, 0.025993975417359514, 0.026317230420071377, 0.02654752323112636, 0.026809017843483902, 0.026924834285203536, 0.02670323251162547, 0.026391582750860103, 0.02671953443821396, 0.02678294940007881, 0.026683861038374477, 0.026690999532636536, 0.02730357642094812, 0.027265340057291223, 0.02834558717926228, 0.02866542614692782, 0.029132219746073787, 0.029427488891414017, 0.02943827660287713, 0.030548012887603154, 0.030142633182516716, 0.032567190806625484, 0.03345971327917809, 0.03265620733033186, 0.03331080464263405, 0.03304063867309102, 0.032795918411653115, 0.033088652713457456, 0.03157029902775578, 0.03134507312434652, 0.030985288251485506, 0.03189164805692296, 0.03171593830341751, 0.03248993894678862, 0.03248993894678862, 0.03269146857580258, 0.0332330975484752, 0.03327483654912615, 0.032808087173541486, 0.032904172465931004, 0.032712551615277946, 0.032924017171438655, 0.03318638595422092, 0.032131962439723154, 0.03201316428454534, 0.03194252533464999, 0.03232786682708663, 0.03233235641889053, 0.03217710351027596, 0.032542389807201505, 0.032381700846152744, 0.032410823604836575, 0.03352796752497683, 0.03312071470511559, 0.03269923759290502, 0.032800067360397464, 0.03205366166226068, 0.03073364840988363, 0.030476359323757765, 0.030476359323757765, 0.02829080685617152, 0.027311881590482912, 0.026990791983334497, 0.02689921533699702, 0.02698003439343595, 0.02691539856236771, 0.0282831098990319, 0.028204416627689074, 0.028601744482730597, 0.028391955519787453, 0.02906893653632565, 0.02857667652193504, 0.029137513088240308, 0.029892093764396974, 0.0293815197403365, 0.02973874379252279, 0.030013544666585224], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 1234, "duration": 22194.458926, "accuracy_train": [0.09287932981927711, 0.09577371987951808, 0.11262236445783133, 0.10431570030120482, 0.1014683734939759, 0.10055064006024096, 0.10055064006024096, 0.1003859186746988, 0.10050357680722892, 0.10052710843373494, 0.10050357680722892, 0.10050357680722892, 0.10050357680722892, 0.10050357680722892, 0.10050357680722892, 0.10050357680722892, 0.10052710843373494, 0.10902202560240964, 0.10043298192771084, 0.10015060240963855, 0.10022119728915663, 0.10024472891566265, 0.10031532379518072, 0.10026826054216867, 0.10022119728915663, 0.1001976656626506, 0.10015060240963855, 0.10008000753012049, 0.10026826054216867, 0.1001976656626506, 0.1002917921686747, 0.10045651355421686, 0.1008800828313253, 0.10123305722891567, 0.1017507530120482, 0.1021507906626506, 0.10238610692771084, 0.10262142319277108, 0.10311558734939759, 0.10372740963855422, 0.10391566265060241, 0.10483339608433735, 0.1052804969879518, 0.10636295180722892, 0.10725715361445783, 0.10793957078313253, 0.10850432981927711, 0.11128106174698796, 0.11332831325301204, 0.11561088102409639, 0.11805817018072289, 0.12144672439759036, 0.12419992469879518, 0.1265766189759036, 0.12963573042168675, 0.13140060240963855, 0.1333066641566265, 0.1399190512048193, 0.14020143072289157, 0.14210749246987953, 0.14135448042168675, 0.1414956701807229, 0.14154273343373494, 0.14142507530120482, 0.14147213855421686, 0.14055440512048192, 0.1398484563253012, 0.13942488704819278, 0.13820124246987953, 0.13763648343373494, 0.1378953313253012, 0.13697759789156627, 0.13648343373493976, 0.1360363328313253, 0.1342714608433735, 0.13434205572289157, 0.13373023343373494, 0.1334949171686747, 0.13373023343373494, 0.13133000753012047, 0.13024755271084337, 0.1321300828313253, 0.13140060240963855, 0.13031814759036145, 0.13069465361445784, 0.13071818524096385, 0.13092996987951808, 0.13031814759036145, 0.1303652108433735, 0.13043580572289157, 0.13038874246987953, 0.13147119728915663, 0.12947100903614459, 0.13038874246987953, 0.12972985692771086, 0.1292827560240964, 0.12876506024096385, 0.12864740210843373, 0.12907097138554216, 0.12867093373493976, 0.12780026355421686, 0.1280120481927711, 0.12652955572289157, 0.12650602409638553, 0.1272590361445783, 0.12808264307228914, 0.1261765813253012, 0.1280355798192771, 0.12796498493975902, 0.12810617469879518, 0.12935335090361447, 0.1290945030120482, 0.13064759036144577], "end": "2016-01-17 03:29:15.778000", "learning_rate_per_epoch": [0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771, 0.00154264981392771], "accuracy_valid": [0.0894396551724138, 0.0972521551724138, 0.11301185344827586, 0.10762392241379311, 0.10075431034482758, 0.09886853448275862, 0.09752155172413793, 0.09765625, 0.09765625, 0.09765625, 0.09765625, 0.09765625, 0.09765625, 0.09765625, 0.09765625, 0.09765625, 0.09765625, 0.10290948275862069, 0.09792564655172414, 0.09819504310344827, 0.09832974137931035, 0.09832974137931035, 0.09832974137931035, 0.09819504310344827, 0.09819504310344827, 0.09752155172413793, 0.09752155172413793, 0.09765625, 0.09738685344827586, 0.09752155172413793, 0.09752155172413793, 0.09765625, 0.09846443965517242, 0.09873383620689655, 0.09913793103448276, 0.09940732758620689, 0.09954202586206896, 0.09981142241379311, 0.10061961206896551, 0.10102370689655173, 0.10142780172413793, 0.10264008620689655, 0.10277478448275862, 0.10466056034482758, 0.10519935344827586, 0.10641163793103449, 0.10748922413793104, 0.10924030172413793, 0.11179956896551724, 0.11301185344827586, 0.11557112068965517, 0.1175915948275862, 0.12095905172413793, 0.12351831896551724, 0.12661637931034483, 0.1275592672413793, 0.12998383620689655, 0.1437230603448276, 0.1437230603448276, 0.14560883620689655, 0.14480064655172414, 0.14560883620689655, 0.1466864224137931, 0.14601293103448276, 0.14641702586206898, 0.14547413793103448, 0.14412715517241378, 0.14453125, 0.1437230603448276, 0.14331896551724138, 0.14331896551724138, 0.1427801724137931, 0.14224137931034483, 0.14156788793103448, 0.14102909482758622, 0.14102909482758622, 0.13954741379310345, 0.1388739224137931, 0.13995150862068967, 0.13860452586206898, 0.1375269396551724, 0.1384698275862069, 0.13860452586206898, 0.13779633620689655, 0.1375269396551724, 0.13712284482758622, 0.13698814655172414, 0.1361799568965517, 0.1361799568965517, 0.13644935344827586, 0.13604525862068967, 0.13712284482758622, 0.13631465517241378, 0.1349676724137931, 0.13469827586206898, 0.13469827586206898, 0.13348599137931033, 0.13321659482758622, 0.1341594827586207, 0.1349676724137931, 0.1333512931034483, 0.13523706896551724, 0.13186961206896552, 0.13254310344827586, 0.1336206896551724, 0.13510237068965517, 0.13267780172413793, 0.13510237068965517, 0.1353717672413793, 0.13550646551724138, 0.13712284482758622, 0.13523706896551724, 0.13833512931034483], "accuracy_test": 0.13631810897435898, "start": "2016-01-16 21:19:21.319000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0], "accuracy_train_last": 0.13064759036144577, "error_valid": [0.9105603448275862, 0.9027478448275862, 0.8869881465517242, 0.8923760775862069, 0.8992456896551724, 0.9011314655172413, 0.9024784482758621, 0.90234375, 0.90234375, 0.90234375, 0.90234375, 0.90234375, 0.90234375, 0.90234375, 0.90234375, 0.90234375, 0.90234375, 0.8970905172413793, 0.9020743534482758, 0.9018049568965517, 0.9016702586206896, 0.9016702586206896, 0.9016702586206896, 0.9018049568965517, 0.9018049568965517, 0.9024784482758621, 0.9024784482758621, 0.90234375, 0.9026131465517242, 0.9024784482758621, 0.9024784482758621, 0.90234375, 0.9015355603448276, 0.9012661637931034, 0.9008620689655172, 0.9005926724137931, 0.900457974137931, 0.9001885775862069, 0.8993803879310345, 0.8989762931034483, 0.8985721982758621, 0.8973599137931034, 0.8972252155172413, 0.8953394396551724, 0.8948006465517242, 0.8935883620689655, 0.892510775862069, 0.8907596982758621, 0.8882004310344828, 0.8869881465517242, 0.8844288793103449, 0.8824084051724138, 0.8790409482758621, 0.8764816810344828, 0.8733836206896551, 0.8724407327586207, 0.8700161637931034, 0.8562769396551724, 0.8562769396551724, 0.8543911637931034, 0.8551993534482758, 0.8543911637931034, 0.8533135775862069, 0.8539870689655172, 0.853582974137931, 0.8545258620689655, 0.8558728448275862, 0.85546875, 0.8562769396551724, 0.8566810344827587, 0.8566810344827587, 0.8572198275862069, 0.8577586206896551, 0.8584321120689655, 0.8589709051724138, 0.8589709051724138, 0.8604525862068966, 0.8611260775862069, 0.8600484913793103, 0.861395474137931, 0.8624730603448276, 0.8615301724137931, 0.861395474137931, 0.8622036637931034, 0.8624730603448276, 0.8628771551724138, 0.8630118534482758, 0.8638200431034483, 0.8638200431034483, 0.8635506465517242, 0.8639547413793103, 0.8628771551724138, 0.8636853448275862, 0.8650323275862069, 0.865301724137931, 0.865301724137931, 0.8665140086206897, 0.8667834051724138, 0.8658405172413793, 0.8650323275862069, 0.8666487068965517, 0.8647629310344828, 0.8681303879310345, 0.8674568965517242, 0.8663793103448276, 0.8648976293103449, 0.8673221982758621, 0.8648976293103449, 0.8646282327586207, 0.8644935344827587, 0.8628771551724138, 0.8647629310344828, 0.8616648706896551], "accuracy_train_std": [0.02754429906082041, 0.028128277540937812, 0.028923920520822922, 0.025545886480892937, 0.02670067203275158, 0.026022393606261674, 0.025958733290008563, 0.025841754684038248, 0.025956856054447336, 0.0259259154394843, 0.025921419095229267, 0.025921419095229267, 0.025921419095229267, 0.025921419095229267, 0.025921419095229267, 0.025921419095229267, 0.02591882346309596, 0.0280123530110729, 0.025311980849823706, 0.025268715918209513, 0.025130311847401998, 0.0251790602822284, 0.025149782928531112, 0.025147404917086273, 0.025130311847401998, 0.025161944702865997, 0.025100823211193144, 0.02520274550306499, 0.025015469296199012, 0.024949162169691067, 0.02476926978619767, 0.02493618603345643, 0.024788745230645618, 0.02497693439024044, 0.02500046779301779, 0.02530492469637402, 0.02515990897842384, 0.0250852326054264, 0.024952890589096208, 0.025025085453393884, 0.024986022417345884, 0.025099532637051, 0.025245608046132833, 0.02515749890791798, 0.02484480671641664, 0.025161669614593055, 0.025463383740283303, 0.025551174923441895, 0.025743814493826185, 0.026177525196431205, 0.02653811456107482, 0.027030403559435454, 0.027629737197987822, 0.0277590520640408, 0.027840480039912734, 0.028535238559052604, 0.0293999869906538, 0.03288192652883006, 0.033126278942480394, 0.0333861538808007, 0.032956805317404254, 0.03267324606835811, 0.03251967766782836, 0.032497338080195844, 0.032749734868287896, 0.032373933959922906, 0.03240443388255449, 0.032351759033804765, 0.03225466748848708, 0.03221866396080311, 0.03231282204349103, 0.032281986847481826, 0.032361796096717944, 0.032349773499885465, 0.03217299190071303, 0.03228665216450328, 0.032001100046423425, 0.03190918923059362, 0.03170676015157041, 0.031247191317454633, 0.03097587154504913, 0.031625621824877326, 0.0312592836905043, 0.030841875097347527, 0.03091762011320332, 0.030850770060557493, 0.03094472411334281, 0.031167992412874466, 0.03113631691114716, 0.03116833885226152, 0.031158814808449672, 0.03128884379736147, 0.029999160020217117, 0.03046671408644349, 0.030170399310858798, 0.029507833955897015, 0.029991886516375226, 0.030477544557604187, 0.03059859840973103, 0.030483766703136798, 0.030161367955213604, 0.030956389067210485, 0.0305487553549585, 0.030780734720562995, 0.03099671736550415, 0.03144736667363719, 0.031027036271671634, 0.03168488726103317, 0.03171766475293052, 0.031935252283004104, 0.031838377623457585, 0.03196197842222289, 0.03173599070481228], "accuracy_test_std": 0.030014838721193864, "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1.0, 1.2], "translation_range": [-3, 3], "momentum": 0.9506894900351656, "shear_range": [1, 1.1], "patience_check_each": 1, "learning_rate": 0.0015426498504043425, "patience_threshold": 1, "do_flip": true, "batch_size": 128, "optimization": "adadelta", "nb_data_augmentation": 3, "learning_rate_decay_method": "none", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 2.725150229625668e-07, "valid_ratio": 0.15, "rotation_range": [-90, 90], "learning_rate_decay": 0.8334879269910432}, "accuracy_valid_max": 0.1466864224137931, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    args = parser.parse_args()\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = 1234\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.98, interval=[0.8, 1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-10, -6], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128, 256, 512],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1.0, 1.2)),\n        rotation_range=make_constant_param((-90, 90)),\n        shear_range=make_constant_param((1, 1.1)),\n        translation_range=make_constant_param((-3, 3)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    model_class = vgg\n\n    instantiate = instantiate_random\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n    )\n    batch_optimizer.learning_rate = learning_rate\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.13833512931034483, "loss_train": [2.358508586883545, 2.356346607208252, 2.355914354324341, 2.355266809463501, 2.354820966720581, 2.3545775413513184, 2.354119300842285, 2.3537843227386475, 2.353546142578125, 2.3531341552734375, 2.352951765060425, 2.3524181842803955, 2.351715564727783, 2.351134777069092, 2.3503623008728027, 2.34934139251709, 2.3483097553253174, 2.3476574420928955, 2.3463761806488037, 2.344921112060547, 2.3434295654296875, 2.341622829437256, 2.339930295944214, 2.3381617069244385, 2.335881471633911, 2.333190679550171, 2.330662250518799, 2.3278794288635254, 2.325434446334839, 2.323286294937134, 2.3204379081726074, 2.3180959224700928, 2.3162529468536377, 2.314079999923706, 2.311896562576294, 2.30964732170105, 2.3081552982330322, 2.3056399822235107, 2.303208827972412, 2.3008182048797607, 2.298792600631714, 2.295499563217163, 2.2921829223632812, 2.2896409034729004, 2.286118984222412, 2.2833621501922607, 2.2799317836761475, 2.2750327587127686, 2.2704508304595947, 2.2657673358917236, 2.260089635848999, 2.2549760341644287, 2.24977445602417, 2.242973804473877, 2.237649917602539, 2.232468605041504, 2.227283239364624, 2.221796989440918, 2.218705892562866, 2.212646484375, 2.2078185081481934, 2.2034640312194824, 2.2002131938934326, 2.195546865463257, 2.191359758377075, 2.18866229057312, 2.1854779720306396, 2.1801769733428955, 2.176314115524292, 2.1721959114074707, 2.169416666030884, 2.165421724319458, 2.162020206451416, 2.159210443496704, 2.155595541000366, 2.152630090713501, 2.1501009464263916, 2.146052598953247, 2.14411997795105, 2.141516923904419, 2.1395211219787598, 2.1354525089263916, 2.133434534072876, 2.130990982055664, 2.1286375522613525, 2.1255362033843994, 2.123685121536255, 2.1221659183502197, 2.1211187839508057, 2.1180827617645264, 2.1159234046936035, 2.1143856048583984, 2.1131951808929443, 2.1107852458953857, 2.1093521118164062, 2.1070187091827393, 2.106485366821289, 2.106306552886963, 2.1030609607696533, 2.101447105407715, 2.1007163524627686, 2.0996816158294678, 2.09846830368042, 2.0968520641326904, 2.0961570739746094, 2.094101905822754, 2.0933191776275635, 2.092257022857666, 2.091942071914673, 2.0915825366973877, 2.0893473625183105, 2.088895797729492, 2.0871737003326416], "accuracy_train_first": 0.09287932981927711, "model": "vgg", "loss_std": [0.0048392158932983875, 0.0033061220310628414, 0.0029939040541648865, 0.0029151986818760633, 0.002657719422131777, 0.0028091291896998882, 0.003019341267645359, 0.0034255622886121273, 0.004324490204453468, 0.003721316112205386, 0.00402172701433301, 0.004517224617302418, 0.005443740636110306, 0.005189904011785984, 0.005651009734719992, 0.006363226566463709, 0.007090494502335787, 0.00736785214394331, 0.007564600557088852, 0.008583095856010914, 0.008406526409089565, 0.009428559802472591, 0.010086898691952229, 0.01103238295763731, 0.01031311135739088, 0.01130648236721754, 0.012258403934538364, 0.013418898917734623, 0.014502624049782753, 0.015660176053643227, 0.016062742099165916, 0.016924388706684113, 0.017529241740703583, 0.018560422584414482, 0.018586022779345512, 0.0188872329890728, 0.019331442192196846, 0.02019500359892845, 0.020937208086252213, 0.020177965983748436, 0.020958187058568, 0.02147802896797657, 0.02196577377617359, 0.02242794632911682, 0.022942405194044113, 0.022803276777267456, 0.02248420938849449, 0.0243170578032732, 0.02528028003871441, 0.024418579414486885, 0.026281066238880157, 0.026246443390846252, 0.02690359205007553, 0.027264239266514778, 0.027713434770703316, 0.029575088992714882, 0.030051767826080322, 0.031054353341460228, 0.031149303540587425, 0.03108169697225094, 0.0319121815264225, 0.032156895846128464, 0.033712927252054214, 0.03336228430271149, 0.03429534658789635, 0.034329865127801895, 0.036249399185180664, 0.03437177836894989, 0.035228513181209564, 0.03558316081762314, 0.03615374490618706, 0.03678590804338455, 0.036511097103357315, 0.03675706684589386, 0.035928964614868164, 0.038308240473270416, 0.037846893072128296, 0.038675617426633835, 0.03811860457062721, 0.03831743821501732, 0.04004378989338875, 0.0406639501452446, 0.03932168334722519, 0.040808193385601044, 0.03984970971941948, 0.040142226964235306, 0.04094887152314186, 0.041086405515670776, 0.042697615921497345, 0.041917793452739716, 0.04292602092027664, 0.042244698852300644, 0.043355438858270645, 0.04184674844145775, 0.04306243732571602, 0.04332256317138672, 0.0445866622030735, 0.04526284337043762, 0.04367782548069954, 0.0436151959002018, 0.04419543966650963, 0.04562345892190933, 0.04563470557332039, 0.044184911996126175, 0.04417626932263374, 0.04577669873833656, 0.046615831553936005, 0.04658970609307289, 0.0468359999358654, 0.047917064279317856, 0.0456291139125824, 0.0466446653008461, 0.047645486891269684]}, "state": "available", "life": [{"dt": "Sun May 15 22:04:57 2016", "state": "available"}], "summary": "83d00dfa2365ecda0208586ed8941efe"}