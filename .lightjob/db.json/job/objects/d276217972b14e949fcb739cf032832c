{"content": {"hp_model": {"f0": 16, "f1": 64, "f2": 32, "f3": 32, "nonlin": "very_leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.11961548433425975, 0.12517765108157966, 0.1276096698587902, 0.11704082991000966, 0.11498606828255131, 0.11403467939543462, 0.11641776156903313, 0.11499715768652236, 0.11460854572896882, 0.11699144803514953, 0.11259866167366375, 0.11322486815000986, 0.10851867540137142, 0.11017644392632502, 0.10948737287475331, 0.1101762011136714, 0.10701207709269087, 0.1079261452983395, 0.10751089928301755, 0.10832522999308783, 0.10853223324636958, 0.10515663091891213, 0.10649408916573268, 0.10596330291549064, 0.10807212763488969, 0.11147224081215677, 0.10721986902252625, 0.10848127983794911, 0.1087945112834482, 0.10716096897814732, 0.10746610010025275, 0.10612904391561469, 0.10697132057137791, 0.10912907202977143, 0.10694005504709195, 0.10926267478234146, 0.10601403657660441, 0.1093647266707142, 0.10813613909809046, 0.10873712026240184, 0.10792713679422887, 0.10689501662654556, 0.10805025936390225, 0.1073966244359424, 0.10813539691428584, 0.10673071575848014, 0.1072610299266987, 0.10650212752980215, 0.10697265436381564, 0.10673372353311388, 0.1060995473431288, 0.10506500577473502, 0.10545310177155193, 0.10681047665679713, 0.10688125110145978, 0.10563787856861749, 0.10655545006332878, 0.1071465718132473, 0.10595008966778557, 0.105577082408844, 0.10585384414474808, 0.10587844012281367, 0.10624166053825142, 0.10640939827340797, 0.10638526033949959, 0.1077586997039556, 0.1064518777959277, 0.10696139996825475, 0.1055724368183845, 0.10655837909953758, 0.10609273927717199, 0.10628160612442386, 0.10677682565915221, 0.10641300173126829, 0.10678108480823309, 0.10534844574706294, 0.10719990653153406, 0.10622050679339232, 0.10689326474907632, 0.10724481691091295, 0.10588820954216428, 0.1060987909129289, 0.10660857863425381, 0.1048883170194948, 0.10553222280095148, 0.10640738698805152, 0.1064486107421931, 0.10696806938263546, 0.1072610299266987, 0.1066467144773254, 0.10578364679340517, 0.10608139152962968, 0.1066424499616831, 0.1065343586253684, 0.10566488785587844, 0.1065343586253684, 0.10532516532321042, 0.10605482464725104, 0.10538839143174175, 0.10680989223863002, 0.10769048929134825, 0.10649685242181693, 0.1064016043308747, 0.10604902276550325, 0.10681515188702277, 0.10619825723861837, 0.10660431259305506, 0.10862093371544719, 0.10694330708749956, 0.10653025702751075, 0.10698132360932687, 0.10742784008974487, 0.10619934883493945, 0.1063231302592039, 0.1055900045217259, 0.10683894229934572, 0.10674809284218519, 0.10562572213611203, 0.10590572486495524, 0.1061334971016575, 0.10654632769184465, 0.10604574330062495, 0.10558155887534958, 0.10489112257811561, 0.10756596010895703, 0.10625038942746612, 0.10588820954216428, 0.10644316542980096, 0.10592988788710087, 0.10681189594474748, 0.10696665216687375, 0.10696064963311291, 0.10628160612442386, 0.10623183968056142, 0.1057155116701919, 0.10654005037710491, 0.10637344081212542, 0.10731887794869309], "moving_avg_accuracy_train": [0.04596903237951806, 0.09471597326807227, 0.14462878859186745, 0.1937624835278614, 0.2411360469220632, 0.2864595657238328, 0.3294768245128953, 0.36967014356762984, 0.4076984228855656, 0.44267453315725, 0.47528725679935635, 0.5057564603362882, 0.5340188225857919, 0.5602220796344417, 0.5842732903456963, 0.6068229944436568, 0.6275695353607369, 0.6467002889029765, 0.6644027185970162, 0.6807843593879169, 0.6956078436298481, 0.7093937271885501, 0.7224622610962012, 0.7344357262516413, 0.7455483471505736, 0.7554908768933476, 0.7647685964329285, 0.7732526742896356, 0.7813636832160937, 0.7888400784487013, 0.7957688529833492, 0.8020141627151347, 0.8077431869556694, 0.812979316302271, 0.8180706919009595, 0.8227847070482129, 0.8271920420662832, 0.8314574952391729, 0.8351175627333278, 0.8384186829660192, 0.841610888464598, 0.8447121301904273, 0.8474585376533124, 0.8498385310265354, 0.8522064286768939, 0.8543751871646261, 0.8564659063999708, 0.8583499068744316, 0.8601349274821691, 0.8618873421134703, 0.8635586417876655, 0.8650204545667303, 0.8664372720618644, 0.8676206344641118, 0.868723311228544, 0.8697651367321956, 0.8708886795348796, 0.8720410578163315, 0.8728240567033729, 0.873672298623397, 0.874475720116479, 0.8752599816891684, 0.8760152335202516, 0.876662015891118, 0.8772464731875484, 0.8778571986097574, 0.8783221376343239, 0.8787852928467951, 0.8790891807307902, 0.8795368138625305, 0.8798737951268798, 0.880207669379252, 0.8805034498810859, 0.8808355408869531, 0.8811108911657277, 0.881281052049155, 0.8814671411213479, 0.8815569669188517, 0.8817672340823881, 0.881980006156077, 0.8821362035826379, 0.8823309040075067, 0.8824967217392861, 0.882643604535237, 0.8826557877564121, 0.8828761841313733, 0.8830698345435372, 0.8832182351253279, 0.8831706021248433, 0.8831536172135638, 0.8831477434440147, 0.8832507025333481, 0.8834421985450736, 0.8834921804977951, 0.8834736288636783, 0.8835063488086358, 0.8835616815482541, 0.8836138341765613, 0.8835572323854112, 0.8836851311348218, 0.8837155261538698, 0.8837946512493262, 0.8838682169978876, 0.883927366683641, 0.8839829545634696, 0.883964741938448, 0.8840118859674948, 0.8840260776418296, 0.8839659021065622, 0.8839964579802434, 0.8840827873328215, 0.8840804762200213, 0.8841513442606697, 0.8842504229370123, 0.8841748723601786, 0.884055107262715, 0.8840485046689737, 0.8840825660996666, 0.8839861506041577, 0.8839605588871154, 0.884045771823705, 0.8840565749124188, 0.8840662976922613, 0.8839315052724328, 0.8839019654379606, 0.8839459744664536, 0.8838843965981215, 0.88389721823349, 0.88394170198243, 0.8840217411215364, 0.883863166406973, 0.8839275274771191, 0.8839054449101301, 0.8838596858106833, 0.8838467405729885, 0.8839809859434005, 0.8839229664153255, 0.8839860538099376], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 682591484, "moving_var_accuracy_train": [0.019018367441182626, 0.03850290891099416, 0.05707422022182029, 0.07309387800220814, 0.08598278077996745, 0.09587249491113994, 0.10293960640355253, 0.10718517183291848, 0.10948200490057312, 0.10954375901814906, 0.10816169080654175, 0.10570087300346238, 0.1023196357824157, 0.09826716832379263, 0.0936465981215079, 0.08885834070350729, 0.08384627727337328, 0.07875552112588116, 0.07370035316694482, 0.06874554124526928, 0.0638486082863794, 0.05917420272718825, 0.05479386166092824, 0.05060475030529227, 0.046655688363953514, 0.042879804606731665, 0.039366508864754596, 0.03607767417198716, 0.033062002947034194, 0.03025887102339818, 0.027665055170026234, 0.02524958469583703, 0.023020021694991035, 0.020964772980301027, 0.019101594631653215, 0.01739143261776471, 0.015827110773641816, 0.014408146513208672, 0.013087896708443734, 0.011877183590715518, 0.010781176815150378, 0.009789618435813566, 0.008878541377801922, 0.008041666556130997, 0.007287962354061057, 0.00660149773905795, 0.005980687927441514, 0.005414564254787278, 0.00490178451643898, 0.004439244678155068, 0.004020459393748244, 0.0036376455237827534, 0.0032919473177351432, 0.0029753557051371013, 0.0026887631990447592, 0.0024296554825608144, 0.002198051070169901, 0.0019901977444849676, 0.0017966957553504457, 0.0016235018090093776, 0.0014669610029683546, 0.0013258004986010951, 0.0011983540966961758, 0.0010822836339439304, 0.0009771295835316934, 0.0008827734950505156, 0.000796441660214547, 0.0007187281089506458, 0.0006476864286699328, 0.0005847211645886239, 0.0005272710554824641, 0.0004755471980817919, 0.0004287798532209984, 0.0003868944278245003, 0.00034888734502624134, 0.0003142592030598558, 0.0002831449450389769, 0.00025490306860015394, 0.00022981067226069383, 0.00020723705263270137, 0.0001867329260940097, 0.00016840080778360536, 0.00015180818668679717, 0.00013682153901983465, 0.000123140720995755, 0.00011126381995504388, 0.00010047494229872066, 9.062565266293141e-05, 8.158350752125477e-05, 7.342775315402985e-05, 6.608528834914532e-05, 5.957216468091818e-05, 5.3944984715387055e-05, 4.857297000422909e-05, 4.371877047196183e-05, 3.935652877794783e-05, 3.544843130881609e-05, 3.192806724768855e-05, 2.876409438777229e-05, 2.6034907759902442e-05, 2.3439731698558517e-05, 2.1152105555281512e-05, 1.908560227400589e-05, 1.7208530214527836e-05, 1.551548730452967e-05, 1.3966923871468321e-05, 1.2590234519594347e-05, 1.1333023700218755e-05, 1.0232311185599234e-05, 9.217483019787062e-06, 8.362809531857295e-06, 7.526576649852944e-06, 6.819119497535842e-06, 6.2255568047345975e-06, 5.654372131200366e-06, 5.218028025214617e-06, 4.696617570890179e-06, 4.237397443348849e-06, 3.897321228981876e-06, 3.5134835299142384e-06, 3.227486377982926e-06, 2.9057881007164656e-06, 2.6160600826756033e-06, 2.51797504239716e-06, 2.274030954543245e-06, 2.0640590103890812e-06, 1.8917796141651075e-06, 1.7040812017503198e-06, 1.5514823168532335e-06, 1.4539904592679426e-06, 1.534904874230551e-06, 1.4186955129606202e-06, 1.2812147195479812e-06, 1.1719383042327795e-06, 1.0562526864202849e-06, 1.1128237930717312e-06, 1.031837804506953e-06, 9.644741982866601e-07], "duration": 202265.812777, "accuracy_train": [0.4596903237951807, 0.5334384412650602, 0.5938441265060241, 0.6359657379518072, 0.6674981174698795, 0.694371234939759, 0.7166321536144579, 0.731410015060241, 0.7499529367469879, 0.7574595256024096, 0.7688017695783133, 0.7799792921686747, 0.7883800828313253, 0.7960513930722891, 0.8007341867469879, 0.8097703313253012, 0.8142884036144579, 0.8188770707831325, 0.8237245858433735, 0.8282191265060241, 0.8290192018072289, 0.8334666792168675, 0.8400790662650602, 0.8421969126506024, 0.8455619352409639, 0.8449736445783133, 0.8482680722891566, 0.849609375, 0.8543627635542169, 0.8561276355421686, 0.8581278237951807, 0.8582219503012049, 0.8593044051204819, 0.8601044804216867, 0.8638930722891566, 0.865210843373494, 0.8668580572289156, 0.8698465737951807, 0.8680581701807228, 0.868128765060241, 0.8703407379518072, 0.8726233057228916, 0.8721762048192772, 0.8712584713855421, 0.8735175075301205, 0.8738940135542169, 0.8752823795180723, 0.8753059111445783, 0.8762001129518072, 0.8776590737951807, 0.8786003388554217, 0.8781767695783133, 0.8791886295180723, 0.8782708960843374, 0.8786474021084337, 0.8791415662650602, 0.8810005647590361, 0.8824124623493976, 0.879871046686747, 0.8813064759036144, 0.8817065135542169, 0.8823183358433735, 0.8828125, 0.8824830572289156, 0.8825065888554217, 0.8833537274096386, 0.8825065888554217, 0.8829536897590361, 0.881824171686747, 0.8835655120481928, 0.8829066265060241, 0.8832125376506024, 0.8831654743975904, 0.883824359939759, 0.8835890436746988, 0.8828125, 0.8831419427710844, 0.8823653990963856, 0.8836596385542169, 0.8838949548192772, 0.8835419804216867, 0.8840832078313253, 0.8839890813253012, 0.8839655496987951, 0.8827654367469879, 0.8848597515060241, 0.8848126882530121, 0.8845538403614458, 0.8827419051204819, 0.8830007530120482, 0.8830948795180723, 0.8841773343373494, 0.8851656626506024, 0.8839420180722891, 0.8833066641566265, 0.883800828313253, 0.8840596762048193, 0.8840832078313253, 0.8830478162650602, 0.8848362198795181, 0.8839890813253012, 0.8845067771084337, 0.8845303087349398, 0.8844597138554217, 0.8844832454819277, 0.883800828313253, 0.8844361822289156, 0.8841538027108434, 0.8834243222891566, 0.8842714608433735, 0.8848597515060241, 0.8840596762048193, 0.884789156626506, 0.8851421310240963, 0.8834949171686747, 0.8829772213855421, 0.8839890813253012, 0.8843891189759037, 0.8831184111445783, 0.8837302334337349, 0.8848126882530121, 0.8841538027108434, 0.8841538027108434, 0.8827183734939759, 0.8836361069277109, 0.8843420557228916, 0.8833301957831325, 0.8840126129518072, 0.8843420557228916, 0.884742093373494, 0.8824359939759037, 0.8845067771084337, 0.8837067018072289, 0.8834478539156626, 0.8837302334337349, 0.8851891942771084, 0.8834007906626506, 0.8845538403614458], "end": "2016-01-23 11:42:15.751000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0], "accuracy_valid": [0.44898504273504275, 0.5205662393162394, 0.5733173076923077, 0.6100427350427351, 0.6418269230769231, 0.6649305555555556, 0.6802884615384616, 0.6897702991452992, 0.7067307692307693, 0.7110042735042735, 0.7191506410256411, 0.7283653846153846, 0.7317040598290598, 0.7362446581196581, 0.7385149572649573, 0.7462606837606838, 0.750267094017094, 0.7545405982905983, 0.7577457264957265, 0.7565438034188035, 0.7585470085470085, 0.7616185897435898, 0.765357905982906, 0.7660256410256411, 0.7684294871794872, 0.765892094017094, 0.7705662393162394, 0.7716346153846154, 0.7748397435897436, 0.7735042735042735, 0.7745726495726496, 0.7744391025641025, 0.7771100427350427, 0.7769764957264957, 0.7785790598290598, 0.7788461538461539, 0.7792467948717948, 0.7808493589743589, 0.781784188034188, 0.7793803418803419, 0.7777777777777778, 0.780982905982906, 0.7797809829059829, 0.7781784188034188, 0.7805822649572649, 0.7799145299145299, 0.7789797008547008, 0.7805822649572649, 0.7816506410256411, 0.7823183760683761, 0.7828525641025641, 0.7832532051282052, 0.7836538461538461, 0.7800480769230769, 0.7829861111111112, 0.7816506410256411, 0.7836538461538461, 0.7848557692307693, 0.7821848290598291, 0.7848557692307693, 0.7833867521367521, 0.7852564102564102, 0.7836538461538461, 0.7831196581196581, 0.7841880341880342, 0.7851228632478633, 0.7845886752136753, 0.7828525641025641, 0.7849893162393162, 0.7859241452991453, 0.7832532051282052, 0.7849893162393162, 0.7841880341880342, 0.7856570512820513, 0.7861912393162394, 0.7845886752136753, 0.7855235042735043, 0.7844551282051282, 0.7853899572649573, 0.7841880341880342, 0.7849893162393162, 0.7856570512820513, 0.7845886752136753, 0.7861912393162394, 0.7836538461538461, 0.7857905982905983, 0.7865918803418803, 0.7871260683760684, 0.7835202991452992, 0.7829861111111112, 0.7856570512820513, 0.7860576923076923, 0.7857905982905983, 0.7844551282051282, 0.7845886752136753, 0.7844551282051282, 0.7852564102564102, 0.7847222222222222, 0.7856570512820513, 0.7855235042735043, 0.7848557692307693, 0.7855235042735043, 0.7859241452991453, 0.7867254273504274, 0.7853899572649573, 0.7851228632478633, 0.7847222222222222, 0.7847222222222222, 0.7837873931623932, 0.7845886752136753, 0.7851228632478633, 0.7856570512820513, 0.7868589743589743, 0.7859241452991453, 0.7844551282051282, 0.7847222222222222, 0.7868589743589743, 0.7856570512820513, 0.7844551282051282, 0.7847222222222222, 0.7861912393162394, 0.7849893162393162, 0.7847222222222222, 0.7839209401709402, 0.7860576923076923, 0.7857905982905983, 0.7849893162393162, 0.7848557692307693, 0.7859241452991453, 0.7871260683760684, 0.7824519230769231, 0.7856570512820513, 0.7849893162393162, 0.7840544871794872, 0.7853899572649573, 0.7863247863247863, 0.7845886752136753, 0.7843215811965812], "accuracy_test": 0.785, "start": "2016-01-21 03:31:09.939000", "learning_rate_per_epoch": [0.0011555071687325835, 0.0010685765882954001, 0.0009881858713924885, 0.0009138431050814688, 0.0008450932218693197, 0.0007815154967829585, 0.0007227208698168397, 0.0006683494430035353, 0.0006180684431456029, 0.0005715701845474541, 0.000528570031747222, 0.0004888048861175776, 0.0004520313232205808, 0.0004180242831353098, 0.00038657564437016845, 0.00035749294329434633, 0.0003305981808807701, 0.0003057267458643764, 0.0002827264543157071, 0.0002614565019030124, 0.0002417867217445746, 0.00022359672584570944, 0.0002067751920549199, 0.00019121916557196528, 0.0001768334477674216, 0.00016352998500224203, 0.00015122735931072384, 0.00013985027908347547, 0.0001293291279580444, 0.00011959949188167229, 0.00011060183169320226, 0.00010228108294541016, 9.458631393499672e-05, 8.747043466428295e-05, 8.088989852694795e-05, 7.48044258216396e-05, 6.91767709213309e-05, 6.397249671863392e-05, 5.915974543313496e-05, 5.47090639884118e-05, 5.059321483713575e-05, 4.67870086140465e-05, 4.326714770286344e-05, 4.001209163106978e-05, 3.700191882671788e-05, 3.421820656512864e-05, 3.164391819154844e-05, 2.9263295800774358e-05, 2.7061771106673405e-05, 2.5025870854733512e-05, 2.314313496754039e-05, 2.140204014722258e-05, 1.9791930753854103e-05, 1.8302953321835957e-05, 1.6925992895266972e-05, 1.5652623915229924e-05, 1.4475052012130618e-05, 1.3386071259446908e-05, 1.23790159705095e-05, 1.144772340921918e-05, 1.0586492862785235e-05, 9.790054718905594e-06, 9.053533176484052e-06, 8.372421689273324e-06, 7.742551133560482e-06, 7.160066616052063e-06, 6.621403372264467e-06, 6.1232649386511184e-06, 5.662602234224323e-06, 5.236595825408585e-06, 4.842638645641273e-06, 4.478319169720635e-06, 4.141408226132626e-06, 3.8298435356409755e-06, 3.541718342603417e-06, 3.2752693641668884e-06, 3.028865648957435e-06, 2.8009992547595175e-06, 2.5902756988216424e-06, 2.3954050902830204e-06, 2.215195081589627e-06, 2.048542455668212e-06, 1.8944273278975743e-06, 1.7519065522719757e-06, 1.620107809685578e-06, 1.4982244920247467e-06, 1.38551069994719e-06, 1.2812765817216132e-06, 1.1848841268147225e-06, 1.0957434142255806e-06, 1.0133088608199614e-06, 9.370760380988941e-07, 8.665783184369502e-07, 8.013842602849763e-07, 7.410948796859884e-07, 6.853411491647421e-07, 6.337818376778159e-07, 5.861014074071136e-07, 5.420080810836225e-07, 5.012319661545916e-07, 4.6352349158951256e-07, 4.286519015295198e-07, 3.964037489367911e-07, 3.6658167346104165e-07, 3.3900315088430943e-07, 3.134994130959967e-07, 2.899143680679117e-07, 2.681036619378574e-07, 2.4793379793663917e-07, 2.2928134058020078e-07, 2.1203214828346972e-07, 1.9608063439591206e-07, 1.8132918455648905e-07, 1.6768750299434032e-07, 1.5507210093801405e-07, 1.4340578502469725e-07, 1.3261714570944605e-07, 1.226401451503989e-07, 1.1341373351569928e-07, 1.0488143686870899e-07, 9.699103742377702e-08, 8.96942466965811e-08, 8.294640707617873e-08, 7.670621471334016e-08, 7.093548504144565e-08, 6.559889698110055e-08, 6.066378688274199e-08, 5.609995312738647e-08, 5.187946428009127e-08, 4.797648855969783e-08, 4.436714107214357e-08, 4.102933104377371e-08, 3.794263037093515e-08, 3.5088145722284025e-08, 3.244840840466168e-08, 3.00072642289706e-08, 2.7749770481477753e-08, 2.5662110658686288e-08], "accuracy_train_last": 0.8845538403614458, "error_valid": [0.5510149572649572, 0.47943376068376065, 0.4266826923076923, 0.3899572649572649, 0.35817307692307687, 0.3350694444444444, 0.31971153846153844, 0.3102297008547008, 0.2932692307692307, 0.28899572649572647, 0.2808493589743589, 0.2716346153846154, 0.26829594017094016, 0.2637553418803419, 0.2614850427350427, 0.2537393162393162, 0.24973290598290598, 0.24545940170940173, 0.24225427350427353, 0.24345619658119655, 0.24145299145299148, 0.23838141025641024, 0.23464209401709402, 0.23397435897435892, 0.23157051282051277, 0.23410790598290598, 0.22943376068376065, 0.22836538461538458, 0.2251602564102564, 0.22649572649572647, 0.2254273504273504, 0.22556089743589747, 0.2228899572649573, 0.22302350427350426, 0.22142094017094016, 0.22115384615384615, 0.22075320512820518, 0.21915064102564108, 0.21821581196581197, 0.2206196581196581, 0.2222222222222222, 0.21901709401709402, 0.22021901709401714, 0.22182158119658124, 0.2194177350427351, 0.22008547008547008, 0.2210202991452992, 0.2194177350427351, 0.21834935897435892, 0.21768162393162394, 0.2171474358974359, 0.21674679487179482, 0.21634615384615385, 0.21995192307692313, 0.21701388888888884, 0.21834935897435892, 0.21634615384615385, 0.21514423076923073, 0.2178151709401709, 0.21514423076923073, 0.21661324786324787, 0.21474358974358976, 0.21634615384615385, 0.2168803418803419, 0.21581196581196582, 0.2148771367521367, 0.21541132478632474, 0.2171474358974359, 0.21501068376068377, 0.21407585470085466, 0.21674679487179482, 0.21501068376068377, 0.21581196581196582, 0.21434294871794868, 0.21380876068376065, 0.21541132478632474, 0.21447649572649574, 0.2155448717948718, 0.2146100427350427, 0.21581196581196582, 0.21501068376068377, 0.21434294871794868, 0.21541132478632474, 0.21380876068376065, 0.21634615384615385, 0.21420940170940173, 0.21340811965811968, 0.21287393162393164, 0.2164797008547008, 0.21701388888888884, 0.21434294871794868, 0.2139423076923077, 0.21420940170940173, 0.2155448717948718, 0.21541132478632474, 0.2155448717948718, 0.21474358974358976, 0.2152777777777778, 0.21434294871794868, 0.21447649572649574, 0.21514423076923073, 0.21447649572649574, 0.21407585470085466, 0.2132745726495726, 0.2146100427350427, 0.2148771367521367, 0.2152777777777778, 0.2152777777777778, 0.2162126068376068, 0.21541132478632474, 0.2148771367521367, 0.21434294871794868, 0.21314102564102566, 0.21407585470085466, 0.2155448717948718, 0.2152777777777778, 0.21314102564102566, 0.21434294871794868, 0.2155448717948718, 0.2152777777777778, 0.21380876068376065, 0.21501068376068377, 0.2152777777777778, 0.21607905982905984, 0.2139423076923077, 0.21420940170940173, 0.21501068376068377, 0.21514423076923073, 0.21407585470085466, 0.21287393162393164, 0.21754807692307687, 0.21434294871794868, 0.21501068376068377, 0.21594551282051277, 0.2146100427350427, 0.2136752136752137, 0.21541132478632474, 0.21567841880341876], "accuracy_train_std": [0.12316394641404073, 0.1211081328868355, 0.12073870034368761, 0.12000711478367612, 0.1180657003424775, 0.11550879452375194, 0.11287529372281849, 0.11270625252189824, 0.10995167530565458, 0.11001296627270042, 0.10772637516297545, 0.10456915759573712, 0.10347007627997917, 0.10362351255862282, 0.10165205080900222, 0.09953988159477722, 0.09837237654152438, 0.09818923845717109, 0.09602331266680078, 0.09532124854847118, 0.09503122014091643, 0.09283643595474585, 0.091918906802369, 0.09153025952734413, 0.09131636483713143, 0.09043607885187521, 0.08949723218050197, 0.08992786176987053, 0.08754754028516877, 0.08848671329953395, 0.08712675201307844, 0.08709428187085498, 0.0868058928966743, 0.08631862176907465, 0.08539344213425844, 0.08479476557824454, 0.08451751793974738, 0.08423877608556317, 0.08459846860063985, 0.084855919618245, 0.08385640298955706, 0.08334552315623589, 0.08344621341431235, 0.08350702550839495, 0.08310117521582622, 0.08324849263898096, 0.08338774426939426, 0.08259907613564442, 0.08298176101942421, 0.08203857714854522, 0.08162515454535968, 0.08178672298105627, 0.08133530970092803, 0.08187287967517366, 0.08162306507530273, 0.08113857691512634, 0.08139357622339774, 0.08102342922509925, 0.08154115369118828, 0.08063510730663058, 0.08073955501779288, 0.08032101818791038, 0.08050314908390042, 0.08115297552051973, 0.08103291466154242, 0.08049447832195253, 0.0806873326216266, 0.08026975487774125, 0.08082983200858944, 0.0804082254418894, 0.08014605300959697, 0.08038102919285431, 0.08060507487985104, 0.07985705959767936, 0.08037827358276416, 0.08019197158132102, 0.0801591930489684, 0.08051789170647591, 0.08028830958235607, 0.08040903805388404, 0.08067550036336395, 0.08020482793665461, 0.07960808719247808, 0.07985974998611892, 0.08039801888680909, 0.07986485315435045, 0.0800729455235755, 0.07985599520506512, 0.08026297685764738, 0.08035664220661619, 0.08036551732169137, 0.07977118650936006, 0.0798725384927998, 0.08056827548484628, 0.08029812666619979, 0.08007116478506346, 0.08074660505965696, 0.08027814296308419, 0.08047996534565935, 0.07972952619800823, 0.0801237058504927, 0.0800639307759725, 0.07977588928586422, 0.07973819333559136, 0.07976306098533319, 0.08045596279714202, 0.07998958524284404, 0.0806266155661257, 0.08005655777676682, 0.07981560020710333, 0.0801773002519495, 0.08010657502787373, 0.07984591224163176, 0.07986632302818926, 0.08033337138293875, 0.08025825777591415, 0.07997672544378305, 0.08027042747447306, 0.08068277562320657, 0.0801614968244623, 0.07999944244536006, 0.08002239173075543, 0.08022430532423609, 0.08001749583575056, 0.07987763737784737, 0.08009249992237832, 0.08030484655600405, 0.07978074795196453, 0.079724397003371, 0.07975952381560766, 0.08072120350563138, 0.07978791394441216, 0.0802465727149479, 0.08024674867622467, 0.08017984177118045, 0.07956512705151865, 0.08034326911256555, 0.08013177752858211], "accuracy_test_std": 0.1063014581273465, "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-3, 3], "momentum": 0.7713868554517257, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.0012495097294798938, "patience_threshold": 1, "do_flip": true, "batch_size": 16, "optimization": "nesterov_momentum", "nb_data_augmentation": 2, "learning_rate_decay_method": "exp", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 2.432988917448716e-06, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.07523157887922013}, "accuracy_valid_max": 0.7871260683760684, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import os\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-6, -3], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128, 256],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-3, 3)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_optimizer.learning_rate = learning_rate\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.7843215811965812, "loss_train": [1.9603612422943115, 1.5816930532455444, 1.4161945581436157, 1.2989469766616821, 1.2069076299667358, 1.1367985010147095, 1.0787073373794556, 1.0277955532073975, 0.9853548407554626, 0.945629894733429, 0.9146831631660461, 0.8874768018722534, 0.860671877861023, 0.8394302725791931, 0.816596269607544, 0.797948956489563, 0.7813512682914734, 0.7648137211799622, 0.7503273487091064, 0.7364691495895386, 0.7254315614700317, 0.7131733894348145, 0.701454222202301, 0.6929517984390259, 0.6823961734771729, 0.6761868596076965, 0.6677396893501282, 0.6596769690513611, 0.6521849632263184, 0.6479397416114807, 0.6413615345954895, 0.6380175352096558, 0.6329678893089294, 0.6273082494735718, 0.6220582127571106, 0.6201814413070679, 0.6166004538536072, 0.6127969026565552, 0.608690619468689, 0.6057529449462891, 0.6036810278892517, 0.5996831059455872, 0.5986490249633789, 0.5971440672874451, 0.5930941104888916, 0.5931916832923889, 0.5892491340637207, 0.5897964239120483, 0.5861466526985168, 0.5877025127410889, 0.583596408367157, 0.5834417939186096, 0.5815663933753967, 0.5810333490371704, 0.5799094438552856, 0.578580379486084, 0.578641951084137, 0.5781998634338379, 0.5778166651725769, 0.5771132707595825, 0.5783584117889404, 0.5756805539131165, 0.5749820470809937, 0.5752702951431274, 0.5754127502441406, 0.5752863883972168, 0.5744534730911255, 0.5742635726928711, 0.5727030038833618, 0.5739815831184387, 0.5733259916305542, 0.5736912488937378, 0.5747458934783936, 0.5729327201843262, 0.5719988942146301, 0.5714563131332397, 0.5716085433959961, 0.5706039071083069, 0.5719558596611023, 0.5695857405662537, 0.5713208913803101, 0.5682036280632019, 0.5699924826622009, 0.5697149634361267, 0.5707191228866577, 0.5700380206108093, 0.5698506236076355, 0.5686055421829224, 0.5707840323448181, 0.5697097182273865, 0.5705432295799255, 0.571203887462616, 0.569537878036499, 0.5700309872627258, 0.5717303156852722, 0.5706060528755188, 0.5685285329818726, 0.5681233406066895, 0.5685338973999023, 0.5702233910560608, 0.5694314241409302, 0.5687212944030762, 0.5717719197273254, 0.5686801671981812, 0.5697426199913025, 0.5704492330551147, 0.5711775422096252, 0.5682991147041321, 0.5714626908302307, 0.5698433518409729, 0.5683740973472595, 0.5688932538032532, 0.5688485503196716, 0.5680950284004211, 0.5712956190109253, 0.5682340264320374, 0.5679751038551331, 0.5686305165290833, 0.568085253238678, 0.5693174004554749, 0.5690478086471558, 0.5676113963127136, 0.5688027739524841, 0.5674073696136475, 0.5692295432090759, 0.5695828199386597, 0.5694331526756287, 0.5678963661193848, 0.5685879588127136, 0.5691216588020325, 0.5696631669998169, 0.5704680681228638, 0.5697207450866699, 0.5683559775352478, 0.5689039826393127, 0.5694904327392578, 0.5690900683403015, 0.568778395652771], "accuracy_train_first": 0.4596903237951807, "model": "residualv4", "loss_std": [0.2574722468852997, 0.2215624898672104, 0.23588532209396362, 0.24150188267230988, 0.24179042875766754, 0.24354411661624908, 0.24133697152137756, 0.2385256588459015, 0.2371368110179901, 0.2349274605512619, 0.2326570302248001, 0.2309049367904663, 0.22810353338718414, 0.22710883617401123, 0.2243109494447708, 0.22404077649116516, 0.22030124068260193, 0.22148583829402924, 0.21698179841041565, 0.2161509394645691, 0.21543259918689728, 0.20971162617206573, 0.20965002477169037, 0.20785871148109436, 0.20637516677379608, 0.20782744884490967, 0.20507599413394928, 0.20422065258026123, 0.20304866135120392, 0.20203441381454468, 0.1998387724161148, 0.1998862624168396, 0.1994917392730713, 0.19645346701145172, 0.1946021318435669, 0.1976710706949234, 0.19485117495059967, 0.19440217316150665, 0.19301636517047882, 0.19220761954784393, 0.1942257136106491, 0.1932382434606552, 0.19123664498329163, 0.19419363141059875, 0.1904967576265335, 0.1897614449262619, 0.19085226953029633, 0.19050417840480804, 0.18976105749607086, 0.1889793872833252, 0.18875116109848022, 0.18792998790740967, 0.18724650144577026, 0.18881578743457794, 0.1872590333223343, 0.1871318370103836, 0.1892145574092865, 0.18762311339378357, 0.1869896501302719, 0.18720945715904236, 0.1877616047859192, 0.185932919383049, 0.1854737401008606, 0.18480806052684784, 0.18621084094047546, 0.18706344068050385, 0.18681615591049194, 0.18546117842197418, 0.1868392676115036, 0.18797093629837036, 0.1863127201795578, 0.18612462282180786, 0.18620683252811432, 0.187734454870224, 0.18583178520202637, 0.18765613436698914, 0.18529453873634338, 0.18462516367435455, 0.18633446097373962, 0.18566308915615082, 0.1868870109319687, 0.18376140296459198, 0.1856309473514557, 0.18430514633655548, 0.18665827810764313, 0.18640205264091492, 0.18584738671779633, 0.18619126081466675, 0.1843232363462448, 0.18432702124118805, 0.1857568323612213, 0.18571293354034424, 0.1848185509443283, 0.18653129041194916, 0.1866237372159958, 0.18830694258213043, 0.18458254635334015, 0.1842299848794937, 0.18553680181503296, 0.1845543384552002, 0.1858537644147873, 0.18447813391685486, 0.18589618802070618, 0.18726134300231934, 0.18624062836170197, 0.18835172057151794, 0.18445052206516266, 0.18434906005859375, 0.1856633722782135, 0.18761137127876282, 0.18528412282466888, 0.1833028793334961, 0.18427163362503052, 0.18593652546405792, 0.1867847442626953, 0.18636557459831238, 0.18508793413639069, 0.18596962094306946, 0.1842087209224701, 0.18550275266170502, 0.18513847887516022, 0.18650880455970764, 0.18343481421470642, 0.18430432677268982, 0.18446259200572968, 0.1850920468568802, 0.18517622351646423, 0.18475939333438873, 0.1854160726070404, 0.18608222901821136, 0.18465609848499298, 0.18578101694583893, 0.18568682670593262, 0.18481187522411346, 0.18528930842876434, 0.18555119633674622, 0.18704217672348022, 0.1842137724161148]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:08 2016", "state": "available"}], "summary": "6822f7e5d53479bb306150e55c6fbc40"}