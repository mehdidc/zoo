{"content": {"hp_model": {"f0": 64, "f1": 32, "f2": 32, "f3": 32, "nonlin": "very_leaky_rectify", "nbg1": 2, "nbg3": 6, "nbg2": 6, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.01639788326431189, 0.01983309573023875, 0.019044736936154095, 0.017092380755024998, 0.01590057704062824, 0.01710502011250112, 0.01921477061585184, 0.012265971413818721, 0.015520616732910776, 0.011172242886104645, 0.008338985255391675, 0.012015356853723905, 0.007935739583527088, 0.014169361873216341, 0.01041198430638593, 0.00930215159316887, 0.011671548494898253, 0.011529500077323746, 0.010228016347220422, 0.012110630225580963, 0.01002092480450219, 0.00661779930555436, 0.010056090153790442, 0.011397731544184555, 0.009887741142712781, 0.00929239273657782, 0.012186493171587702, 0.00868101052969559, 0.011246917440601913, 0.010188324286468261, 0.009661424942977288, 0.015213292010801474, 0.011269118794080971, 0.013314056197565993, 0.010868599540653388, 0.010219711160745644, 0.011290297831612355, 0.012879099840310252, 0.010881222232378022, 0.009537956962582573, 0.015095250603278427, 0.008053135006320022, 0.016154835366047525, 0.014438116322108632, 0.009985754863676454, 0.010304580486766853, 0.011369524785117, 0.015955820636584038, 0.014071283565800589, 0.014935850062062722, 0.012192869949651868, 0.016345313850216737, 0.011393687325091183, 0.01529772745739164, 0.01344081883650354, 0.011221426573906707, 0.010590412426452549, 0.0130019836087209, 0.01085008951987408, 0.014179112620523595, 0.013915592213420914, 0.014974271472597427, 0.011810277454158793, 0.014311863796163577, 0.010844451446649769, 0.016733478583356106, 0.012374953045207223, 0.014429002036681288, 0.010422695630323142, 0.013112094893374313, 0.01860578940581562, 0.01088225785923492, 0.01083328180494201, 0.013711968741711836, 0.009474643029713684, 0.011987376110621121, 0.012025863784832174, 0.013787021798081197, 0.018520912816422354, 0.012931016765435557, 0.013478840315942557, 0.01556755016522394, 0.010752780912047797, 0.014384325957635643, 0.0169152397438203, 0.01392908064895637, 0.014892919368599782, 0.009285025163400308, 0.013802112335214525, 0.017475706060615374, 0.01281391993521408, 0.013811229908798258, 0.010977506031050976, 0.01679824173324585, 0.009463954849031267, 0.014096483982968264, 0.015436585285641303, 0.012836924782683983, 0.013710212971918352, 0.015550168973241553, 0.011345964044923723, 0.010927974906635763, 0.012982002061530658, 0.00742309749913908, 0.01306637389792001, 0.01144965972983898, 0.014211611820624187, 0.010046678365540103, 0.011925330753695085, 0.01380327348702763, 0.008606308368524237, 0.01666611295434291, 0.013828137121401232, 0.00989133022465974, 0.009739293617067015, 0.01744968779167276, 0.014367679182900597, 0.013459570647521629, 0.013343026780264574, 0.01945003079962333, 0.014742912536538439, 0.018275451827696833, 0.012665264626624323, 0.013435564476631247, 0.01318151317373286, 0.011636456225842922, 0.01154096291148274, 0.011918803578971512, 0.011207317633780103, 0.016178585714331065, 0.012803626963444278, 0.00813326880684704, 0.010667273209644756, 0.010658013406102758, 0.012164658902338412, 0.012266770228504266, 0.01039818588610895, 0.01648545893674795, 0.013690908341847297, 0.01612378923706107, 0.014603405909778914, 0.010566366447318343, 0.01006577701547162, 0.012190859910442753, 0.015996000395787392, 0.014164321270603089, 0.01511894041104822, 0.012938806761434217, 0.013760366259615136, 0.008727931775113724, 0.01357816491734322, 0.008806758377321401, 0.012655156356020682, 0.012413241673856185, 0.009472857118697704, 0.014725503741324619, 0.014213192199165295, 0.013131331210068629, 0.011608967501018131, 0.013543447367793967, 0.012936662823247718, 0.010367671920005869, 0.01519253659937811, 0.01299437322811395, 0.01402367073091541, 0.015714190375249546, 0.012883512794112676, 0.015224667334570388, 0.012206150820009393, 0.008542445202220151, 0.012640411607888676, 0.01036255748472646, 0.009412769638595725, 0.013313249058637854, 0.015150767801409937, 0.011285960405879425, 0.013954574143244573, 0.01246984917638826, 0.009523958436446813, 0.008312574125412125, 0.009885676165936334, 0.009319316528096954, 0.011876682170043078, 0.013621840043864809, 0.0115744162105671, 0.010952938552825378, 0.01370738129082137, 0.012919019583916353, 0.012043640054989991, 0.015535479903886196, 0.009167535575119328, 0.008163423679037973, 0.009677797911262984, 0.011108516340355118, 0.011188341805598984, 0.011016766424817153, 0.011371812865810824, 0.012537482215084309, 0.0110708822812845, 0.011456162772286031, 0.013237890866545577, 0.012062618154046894, 0.010954799785963318, 0.009802656856545572, 0.014428125567241854, 0.013713684696954475, 0.011509779304720196, 0.010371037873215331, 0.010375260874513185, 0.007242340567335292, 0.01016572520735974, 0.009708563764464795, 0.008935439956434577, 0.01003193077534692, 0.014756050771102333, 0.012181609671268637, 0.010516010202260227, 0.010667685540934878, 0.00904314110666023, 0.010130918533149439, 0.00978619130203668], "moving_avg_accuracy_train": [0.05438542892326503, 0.11619959834406146, 0.1778745834558993, 0.23619481518852087, 0.29131244261208294, 0.3435336671181781, 0.3916138912724548, 0.4364923382528597, 0.4762878466352611, 0.5149468122947324, 0.5494005178085224, 0.5818407479005938, 0.6115042738453536, 0.6381155969337787, 0.6632234955275917, 0.6865016926191201, 0.7070360486086846, 0.7263213983897504, 0.7437853142332228, 0.7602299972398101, 0.7743490514910044, 0.7880118725266123, 0.8007245770467455, 0.8117543516291233, 0.8225646692520636, 0.8314570096877376, 0.8405854160060163, 0.8495983635388588, 0.8574450575005875, 0.864909188614916, 0.8710528111547716, 0.8766493926096618, 0.8824511456821212, 0.8862012647390937, 0.8912988385235822, 0.8958100331653638, 0.8995864762370241, 0.9037221669300437, 0.9071188398180656, 0.9112500641672853, 0.9140382147042296, 0.9176912710600341, 0.9204744644885915, 0.9226795025243208, 0.925791769977915, 0.9286461809623126, 0.9308990027078126, 0.9326894491978286, 0.9350656447530826, 0.9368973732075916, 0.9391319384142872, 0.9409847927859907, 0.9427476207240767, 0.9436624141553143, 0.9449740455422468, 0.946593714573755, 0.9478864392830924, 0.9491312356810105, 0.9503423413891646, 0.9516786941538565, 0.9531488758528119, 0.9541835767342158, 0.9554078123262981, 0.9561119878639249, 0.9561275086061869, 0.9572271415241765, 0.9583701988741583, 0.9589782067010744, 0.9592255776953265, 0.9599758761746402, 0.9610789000893375, 0.9614206520435359, 0.9621467916368476, 0.9631071648160476, 0.963518168757108, 0.9645530288147675, 0.9653308348987761, 0.9662493883136605, 0.96704593155017, 0.9675094152916093, 0.9681148516636573, 0.9682623602449845, 0.9686228383562372, 0.969240201357546, 0.9698028035051524, 0.9705927414951225, 0.9709316618765719, 0.9716203758222665, 0.9721379118257726, 0.9726199702705948, 0.9732746399102019, 0.9733432255454275, 0.973567604887341, 0.9734137624783872, 0.9739078168841568, 0.9745430199052743, 0.9750683077945457, 0.9753805234805766, 0.97587775643729, 0.9763671548257223, 0.9765914105848444, 0.9769838308727978, 0.9772997707021848, 0.9776910733938711, 0.9779735274009218, 0.9783742564310862, 0.9785326967094431, 0.9790961088456693, 0.9791729911896923, 0.9795002407683513, 0.97972497487604, 0.9798040026860552, 0.9802426454222484, 0.980630412389575, 0.9808305931363596, 0.9812315367989141, 0.9811855932000026, 0.9815232792657536, 0.981652810564215, 0.9820785253780409, 0.9824569823640463, 0.982851108122889, 0.9828292192963421, 0.9830698639214791, 0.9829935113829211, 0.9832177988970376, 0.9834266331061712, 0.9835261921908106, 0.9838877656800629, 0.9840575770942456, 0.9842613803967258, 0.9844402612177952, 0.9844268677960433, 0.9847589357402762, 0.9850345093531717, 0.9853847600547593, 0.9856210387731206, 0.9856500749613308, 0.9857715025830918, 0.9857784087974201, 0.9858056588760575, 0.9859207205551276, 0.9860661287448621, 0.9862527996870517, 0.9863186411826783, 0.9865637301894105, 0.9867099055335646, 0.9869484562373603, 0.9871003728529192, 0.9872557710950358, 0.9874420603914938, 0.9876307191952293, 0.98761678931382, 0.9878437427479326, 0.9881037683612438, 0.988223895170376, 0.9882552793878807, 0.9881439802062447, 0.9882460988892009, 0.9882356991562424, 0.9884030146572849, 0.988376923347518, 0.9885557291151563, 0.9887050285619833, 0.9887348384653364, 0.9889498962854695, 0.9890574899152743, 0.9891543241820987, 0.989090268251984, 0.9891141062696703, 0.9892355058355788, 0.9894004969186876, 0.9895094613637235, 0.9895215349071222, 0.9895812292211812, 0.9896279426085869, 0.9896142171346423, 0.989752998880711, 0.9896291475783726, 0.9898733931253065, 0.9899838960746806, 0.9901671261838976, 0.9903040954476598, 0.9904692204636174, 0.9905271521744078, 0.9903793639653188, 0.9905159997414059, 0.9907087624529888, 0.990754437806527, 0.9908745646354166, 0.9910152308647505, 0.991158070463999, 0.9910006327997511, 0.9910844783364519, 0.9911250620873397, 0.9912011149929008, 0.9913068370864956, 0.9911554851481026, 0.990956453336873, 0.9910168150341473, 0.9911060538426558, 0.9912560871857805, 0.9914189829314882, 0.991554035456215, 0.9916778357796412, 0.9916683843814482, 0.9918086515980653, 0.9918605233799347, 0.99184442896576, 0.9919624414263268, 0.9920733389872749, 0.9921475341064045, 0.9922050451672019, 0.9922382399802621, 0.9923773252084263, 0.9923467529923549, 0.9924982744562239, 0.9925137360356107, 0.9926160792094583, 0.9926012672695017, 0.9926088268140077], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 871027566, "moving_var_accuracy_train": [0.026619973912507623, 0.05834690039190317, 0.08674644444962165, 0.10868304486877962, 0.1251563160571251, 0.13718419105165652, 0.1442711435390203, 0.14797070421547542, 0.14742677618065186, 0.1461347391953283, 0.1422047856884741, 0.1374556238754655, 0.13162938443119787, 0.12483990863672703, 0.11802957691922908, 0.11110348936579453, 0.10378807841237057, 0.09675659301673602, 0.08982582892435337, 0.0832770944246023, 0.07674351421867559, 0.0707492169046675, 0.06512881092014668, 0.05971083317417463, 0.054791516560736865, 0.05002402837047849, 0.04577157575063495, 0.04192551718463939, 0.03828710092133673, 0.03495981010883003, 0.03180352597915704, 0.028905068897072315, 0.026317505055789215, 0.023812325086683506, 0.021664959904409902, 0.01968162180783327, 0.017841813327511367, 0.016211567432335195, 0.01469424716947568, 0.013378425584140381, 0.012110547076476314, 0.01101959575547683, 0.00998735167087603, 0.009032376238439542, 0.0082163144929199, 0.007468012002238553, 0.006766887654367677, 0.0061190501766334076, 0.005557961906821349, 0.005032362778318736, 0.004574066035453635, 0.004147557055812936, 0.0037607694112853133, 0.003392224093353301, 0.003068485076074651, 0.0027852465184118225, 0.0025217621011378215, 0.0022835315536744697, 0.002068379391633932, 0.0018776140008758353, 0.0017093055088397433, 0.0015480104111815717, 0.0014066981451277046, 0.0012704910993050615, 0.0011434441574155187, 0.001039982474662905, 0.0009477434481447403, 0.0008562961649885867, 0.0007712172801689036, 0.0006991620824245586, 0.0006401958299896501, 0.000577227396574471, 0.0005242501652977965, 0.00048012599855795846, 0.00043363371685826746, 0.000399908763222891, 0.00036536272763949005, 0.00033642011825950135, 0.0003084884365822132, 0.00027957294753119864, 0.00025491463158346664, 0.00022961899745920633, 0.00020782659793151695, 0.0001904741718168296, 0.00017427544522356897, 0.00016246391895319523, 0.00014725133028253173, 0.00013679513934522668, 0.00012552621704502962, 0.00011506501843854583, 0.0001074158476279019, 9.671659876934548e-05, 8.749805369410879e-05, 7.89612557058321e-05, 7.326193793799229e-05, 6.956709004652261e-05, 6.509372734140714e-05, 5.946166231869982e-05, 5.574066160600811e-05, 5.232219248880863e-05, 4.7542589049422696e-05, 4.417427328605696e-05, 4.065520773958834e-05, 3.7967747134317925e-05, 3.488899481577705e-05, 3.2845349134747624e-05, 2.9786744117525063e-05, 2.9664968822995674e-05, 2.6751669994098286e-05, 2.504033357528119e-05, 2.299084899018072e-05, 2.0747972643964603e-05, 2.0404842429704544e-05, 1.9717627175281545e-05, 1.8106515440202085e-05, 1.7742666281065585e-05, 1.5987396981487357e-05, 1.5414944194359778e-05, 1.4024454990454048e-05, 1.42531074158054e-05, 1.4116863886531729e-05, 1.4103193521928493e-05, 1.2697186256284056e-05, 1.1948656151121245e-05, 1.0806257927307411e-05, 1.0178376135473818e-05, 9.553044064065944e-06, 8.686947759667526e-06, 8.994871476871374e-06, 8.354907576664853e-06, 7.893238893914916e-06, 7.391900137841583e-06, 6.654324577773425e-06, 6.9813141962795546e-06, 6.9666501217706614e-06, 7.374065095257033e-06, 7.13910728048538e-06, 6.4327844544688905e-06, 5.922208014960739e-06, 5.330416475631805e-06, 4.804057929140362e-06, 4.4428048461402924e-06, 4.188816236303215e-06, 4.08354897859476e-06, 3.714210003652531e-06, 3.883406594275933e-06, 3.6873710159956957e-06, 3.830791858928619e-06, 3.655420595781624e-06, 3.507216059080182e-06, 3.468827770945617e-06, 3.4422742918930515e-06, 3.099793237068422e-06, 3.2533846646612802e-06, 3.536566074395999e-06, 3.3127835194068874e-06, 2.9903698894416224e-06, 2.8028204709930127e-06, 2.616392452572069e-06, 2.355726597325331e-06, 2.37210422959448e-06, 2.1410206146431914e-06, 2.2146620760456384e-06, 2.193808791846527e-06, 1.982425585703206e-06, 2.200431821136186e-06, 2.0845761415938113e-06, 1.9605104045171366e-06, 1.8013878237110795e-06, 1.6263633011249397e-06, 1.5963676624375428e-06, 1.6817294137424652e-06, 1.6204157249062736e-06, 1.4596860864674679e-06, 1.3457881779993946e-06, 1.2308486252656857e-06, 1.109459260454176e-06, 1.1718566917859552e-06, 1.192723328425507e-06, 1.6103539803561924e-06, 1.5592166987039774e-06, 1.7054544851468566e-06, 1.7037542495725363e-06, 1.7787752626700635e-06, 1.6311024844389247e-06, 1.6645644287065915e-06, 1.6661320035984222e-06, 1.83393597002982e-06, 1.6693185143145055e-06, 1.63226075805463e-06, 1.6471175749244258e-06, 1.6660341774531535e-06, 1.7225103228220819e-06, 1.613529956761652e-06, 1.4670003286106341e-06, 1.3723566957479512e-06, 1.3357154758396637e-06, 1.408310611553585e-06, 1.6240025073302413e-06, 1.494394067077741e-06, 1.4166267448661083e-06, 1.4775541068217806e-06, 1.5686139118664809e-06, 1.5759051805955175e-06, 1.5562533432596542e-06, 1.4014319692839098e-06, 1.4383628008730566e-06, 1.3187426565744733e-06, 1.1891996624256718e-06, 1.1956221638245134e-06, 1.186744368659942e-06, 1.117614173117914e-06, 1.035620454832446e-06, 9.419754698761024e-07, 1.0218802291300681e-06, 9.281041497767609e-07, 1.0419225209160902e-06, 9.398818127587027e-07, 9.401607585815826e-07, 8.481192248109058e-07, 7.63821622748071e-07], "duration": 222749.30178, "accuracy_train": [0.5438542892326504, 0.6725271231312293, 0.73294944946244, 0.7610769007821152, 0.7873710894241418, 0.8135246876730344, 0.824335908660945, 0.8403983610765043, 0.8344474220768733, 0.862877503229974, 0.8594838674326319, 0.8738028187292359, 0.8784760073481912, 0.8776175047296051, 0.8891945828719084, 0.8960054664428755, 0.8918452525147655, 0.8998895464193429, 0.900960556824474, 0.9082321442990956, 0.9014205397517534, 0.9109772618470838, 0.9151389177279439, 0.9110223228705242, 0.9198575278585271, 0.911488073608804, 0.9227410728705242, 0.9307148913344407, 0.9280653031561462, 0.9320863686438722, 0.9263454140134736, 0.9270186257036729, 0.9346669233342562, 0.9199523362518457, 0.9371770025839794, 0.936410784941399, 0.9335744638819674, 0.9409433831672205, 0.9376888958102622, 0.9484310833102622, 0.9391315695367294, 0.9505687782622739, 0.9455232053456073, 0.9425248448458842, 0.9538021770602622, 0.95433587982189, 0.9511743984173128, 0.9488034676079733, 0.9564514047503692, 0.9533829292981728, 0.9592430252745479, 0.9576604821313216, 0.9586130721668512, 0.9518955550364526, 0.95677872802464, 0.9611707358573275, 0.9595209616671282, 0.9603344032622739, 0.9612422927625508, 0.9637058690360835, 0.966380511143411, 0.9634958846668512, 0.9664259326550388, 0.9624495677025655, 0.9562671952865448, 0.9671238377860835, 0.9686577150239941, 0.9644502771433187, 0.9614519166435955, 0.9667285624884644, 0.9710061153216132, 0.9644964196313216, 0.9686820479766519, 0.9717505234288483, 0.9672172042266519, 0.9738667693337025, 0.9723310896548542, 0.9745163690476191, 0.974214820678756, 0.9716807689645626, 0.9735637790120893, 0.9695899374769288, 0.9718671413575121, 0.9747964683693245, 0.9748662228336102, 0.9777021834048542, 0.9739819453096161, 0.977818801333518, 0.9767957358573275, 0.9769584962739941, 0.9791666666666666, 0.9739604962624585, 0.9755870189645626, 0.9720291807978036, 0.9783543065360835, 0.9802598470953304, 0.9797958987979882, 0.9781904646548542, 0.9803528530477114, 0.9807717403216132, 0.9786097124169435, 0.980515613464378, 0.9801432291666666, 0.9812127976190477, 0.980515613464378, 0.9819808177025655, 0.9799586592146549, 0.9841668180717055, 0.9798649322858989, 0.9824454869762828, 0.9817475818452381, 0.9805152529761905, 0.9841904300479882, 0.984120315095515, 0.9826322198574198, 0.9848400297619048, 0.9807721008098007, 0.9845624538575121, 0.9828185922503692, 0.9859099587024732, 0.9858630952380952, 0.9863982399524732, 0.9826322198574198, 0.9852356655477114, 0.9823063385358989, 0.9852363865240864, 0.9853061409883721, 0.9844222239525655, 0.9871419270833334, 0.98558587982189, 0.9860956101190477, 0.9860501886074198, 0.9843063270002769, 0.9877475472383721, 0.9875146718692323, 0.9885370163690477, 0.9877475472383721, 0.9859114006552234, 0.9868643511789406, 0.985840564726375, 0.9860509095837948, 0.9869562756667589, 0.9873748024524732, 0.9879328381667589, 0.9869112146433187, 0.98876953125, 0.9880254836309523, 0.9890954125715209, 0.9884676223929494, 0.9886543552740864, 0.9891186640596161, 0.9893286484288483, 0.987491420381137, 0.9898863236549464, 0.9904439988810447, 0.9893050364525655, 0.9885377373454227, 0.9871422875715209, 0.9891651670358066, 0.9881421015596161, 0.9899088541666666, 0.9881421015596161, 0.9901649810239018, 0.9900487235834257, 0.989003127595515, 0.9908854166666666, 0.990025832583518, 0.990025832583518, 0.9885137648809523, 0.9893286484288483, 0.990328101928756, 0.9908854166666666, 0.9904901413690477, 0.9896301967977114, 0.9901184780477114, 0.9900483630952381, 0.98949068786914, 0.9910020345953304, 0.9885144858573275, 0.9920716030477114, 0.9909784226190477, 0.9918161971668512, 0.9915368188215209, 0.9919553456072352, 0.9910485375715209, 0.989049270083518, 0.9917457217261905, 0.9924436268572352, 0.9911655159883721, 0.9919557060954227, 0.992281226928756, 0.9924436268572352, 0.9895836938215209, 0.9918390881667589, 0.9914903158453304, 0.9918855911429494, 0.9922583359288483, 0.9897933177025655, 0.9891651670358066, 0.9915600703096161, 0.9919092031192323, 0.9926063872739018, 0.9928850446428571, 0.992769508178756, 0.9927920386904762, 0.9915833217977114, 0.9930710565476191, 0.9923273694167589, 0.9916995792381875, 0.9930245535714286, 0.9930714170358066, 0.9928152901785714, 0.992722644714378, 0.9925369932978036, 0.9936290922619048, 0.9920716030477114, 0.9938619676310447, 0.9926528902500923, 0.9935371677740864, 0.992467959809893, 0.9926768627145626], "end": "2016-01-28 04:13:16.137000", "learning_rate_per_epoch": [0.007454215083271265, 0.00739863608032465, 0.0073434715159237385, 0.00728871813043952, 0.00723437312990427, 0.007180433254688978, 0.00712689571082592, 0.007073757238686085, 0.007021015044301748, 0.0069686658680438995, 0.006916707381606102, 0.0068651363253593445, 0.006813949439674616, 0.00676314439624548, 0.0067127179354429245, 0.006662667728960514, 0.006612990517169237, 0.006563683971762657, 0.006514744833111763, 0.006466170772910118, 0.006417958531528711, 0.006370105780661106, 0.006322610192000866, 0.00627546850591898, 0.006228678394109011, 0.006182237062603235, 0.006136142183095217, 0.006090390961617231, 0.006044980604201555, 0.005999908782541752, 0.005955173168331385, 0.00591077096760273, 0.005866699852049351, 0.005822957493364811, 0.0057795410975813866, 0.005736448802053928, 0.0056936778128147125, 0.005651225335896015, 0.005609089508652687, 0.005567268002778292, 0.005525758024305105, 0.005484557710587978, 0.005443664733320475, 0.0054030767641961575, 0.005362791009247303, 0.005322805605828762, 0.005283118691295385, 0.005243727471679449, 0.005204630084335804, 0.0051658242009580135, 0.005127307493239641, 0.005089078098535538, 0.005051133688539267, 0.0050134724006056786, 0.004976091906428337, 0.004938989877700806, 0.004902164451777935, 0.004865613766014576, 0.004829335492104292, 0.004793327767401934, 0.004757588729262352, 0.00472211604937911, 0.0046869078651070595, 0.00465196231380105, 0.004617277067154646, 0.0045828502625226974, 0.004548680502921343, 0.004514765460044146, 0.004481103271245956, 0.004447692073881626, 0.0044145300053060055, 0.004381615202873945, 0.004348945803940296, 0.004316519945859909, 0.004284335765987635, 0.004252391401678324, 0.004220685455948114, 0.004189216066151857, 0.004157980903983116, 0.004126979038119316, 0.004096208140254021, 0.004065666813403368, 0.004035353194922209, 0.004005265422165394, 0.003975402098149061, 0.003945761360228062, 0.003916341811418533, 0.0038871413562446833, 0.0038581585977226496, 0.0038293919060379267, 0.0038008398842066526, 0.0037725006695836782, 0.0037443728651851416, 0.0037164546083658934, 0.0036887445021420717, 0.0036612411495298147, 0.003633942687883973, 0.0036068479530513287, 0.003579955082386732, 0.0035532626789063215, 0.003526769345626235, 0.0035004736855626106, 0.003474374068900943, 0.0034484690986573696, 0.0034227571450173855, 0.003397237043827772, 0.003371907165274024, 0.003346766112372279, 0.0033218124881386757, 0.0032970448955893517, 0.003272461937740445, 0.003248062217608094, 0.0032238445710390806, 0.0031998073682188988, 0.0031759494449943304, 0.0031522694043815136, 0.0031287658493965864, 0.0031054376158863306, 0.0030822833068668842, 0.003059301758185029, 0.003036491572856903, 0.0030138513538986444, 0.002991379937157035, 0.002969076158478856, 0.002946938620880246, 0.002924966160207987, 0.002903157379478216, 0.0028815113473683596, 0.002860026666894555, 0.0028387021739035845, 0.0028175367042422295, 0.0027965290937572718, 0.0027756779454648495, 0.002754982328042388, 0.002734441077336669, 0.0027140530291944742, 0.0026938170194625854, 0.0026737318839877844, 0.0026537964586168528, 0.0026340095791965723, 0.0026143703144043684, 0.0025948775000870228, 0.002575529972091317, 0.002556326799094677, 0.002537266816943884, 0.0025183488614857197, 0.0024995720013976097, 0.0024809350725263357, 0.002462437143549323, 0.0024440770503133535, 0.0024258538614958525, 0.0024077666457742453, 0.0023898142389953136, 0.002371995709836483, 0.0023543101269751787, 0.0023367563262581825, 0.00231933337636292, 0.002302040345966816, 0.0022848763037472963, 0.0022678400855511427, 0.002250930992886424, 0.002234147861599922, 0.002217489993199706, 0.0022009562235325575, 0.0021845458541065454, 0.0021682577207684517, 0.0021520911250263453, 0.0021360451355576515, 0.0021201185882091522, 0.0021043107844889164, 0.002088621025905013, 0.002073048148304224, 0.0020575914531946182, 0.002042250009253621, 0.002027022885158658, 0.002011909382417798, 0.0019969085697084665, 0.001982019515708089, 0.001967241521924734, 0.0019525736570358276, 0.0019380152225494385, 0.001923565287142992, 0.0019092231523245573, 0.0018949878867715597, 0.0018808587919920683, 0.0018668350530788302, 0.0018529158551245928, 0.0018391003832221031, 0.0018253879388794303, 0.0018117778236046433, 0.0017982691060751677, 0.0017848610877990723, 0.0017715530702844262, 0.0017583443550392985, 0.0017452340107411146, 0.0017322214553132653, 0.0017193059902638197, 0.001706486800685525, 0.00169376318808645, 0.0016811344539746642, 0.0016685998998582363, 0.0016561587108299136, 0.001643810304813087, 0.0016315539833158255, 0.001619389047846198, 0.0016073147999122739, 0.001595330541022122, 0.001583435689099133, 0.0015716295456513762, 0.0015599114121869206, 0.001548280706629157, 0.0015367366140708327, 0.0015252786688506603, 0.0015139061724767089, 0.0015026184264570475, 0.001491414848715067, 0.0014802947407588363, 0.0014692576369270682, 0.0014583028387278318, 0.0014474296476691961, 0.0014366375980898738], "accuracy_valid": [0.5378344432417168, 0.6675392978162651, 0.7203780944088856, 0.7475497693900602, 0.7728903896837349, 0.794090914439006, 0.8051081278237951, 0.8182917215737951, 0.8091570383094879, 0.8370302499058735, 0.8295118952371988, 0.8448736351656627, 0.8419939523719879, 0.8379244517131024, 0.8506815347326807, 0.8503153237951807, 0.8400408273719879, 0.8513124764683735, 0.8521669686558735, 0.8541803934487951, 0.8508756706513554, 0.8580160485692772, 0.861414897872741, 0.8565012001129518, 0.8634489128388554, 0.859166156814759, 0.8680272849209337, 0.866856586502259, 0.8694818335843373, 0.867884624435241, 0.864954936935241, 0.8644975409450302, 0.8702348456325302, 0.8483621987951807, 0.869175922439759, 0.8669580666415663, 0.8679655144013554, 0.867466938064759, 0.8677316688629518, 0.8798784003200302, 0.8657682487763554, 0.8770604880459337, 0.8755250494164157, 0.8724527014307228, 0.8793386436370482, 0.8733571983245482, 0.8762162909450302, 0.8714452536709337, 0.8774369940700302, 0.8782811911709337, 0.8859113210655121, 0.8792180440512049, 0.8796136695218373, 0.8719541250941265, 0.8774575842432228, 0.8812417639307228, 0.877528179122741, 0.8811299887048193, 0.8789121329066265, 0.8818932958396084, 0.8850568288780121, 0.8833684346762049, 0.8846906179405121, 0.8774678793298193, 0.8668668815888554, 0.8887807087725903, 0.8838464208396084, 0.8846803228539157, 0.8770501929593373, 0.8834596197289157, 0.8883027226091867, 0.8782003012048193, 0.8854230398155121, 0.8856877706137049, 0.8802446112575302, 0.8860539815512049, 0.8840802663780121, 0.8866437429405121, 0.8851391895707832, 0.8829610433923193, 0.8864098974021084, 0.8833684346762049, 0.8846700277673193, 0.891822171498494, 0.8911103397966867, 0.8867658132530121, 0.8854642201618976, 0.8857583654932228, 0.8855451101280121, 0.886329007435994, 0.8930325795368976, 0.8838258306664157, 0.8850465337914157, 0.8897572712725903, 0.8872643895896084, 0.891455960560994, 0.889869046498494, 0.8891057393637049, 0.8908044286521084, 0.8912221150225903, 0.8856568853539157, 0.8868569983057228, 0.8876203054405121, 0.8874673498682228, 0.8919236516378012, 0.8894204748682228, 0.8894925404743976, 0.8907838384789157, 0.8834287344691265, 0.8907029485128012, 0.8879247458584337, 0.8900117069841867, 0.8926663685993976, 0.8922795674887049, 0.8930016942771084, 0.8951489551957832, 0.8902249623493976, 0.8910691594503012, 0.8882615422628012, 0.8936841114457832, 0.892432523060994, 0.8940606174698795, 0.8888615987387049, 0.8965726185993976, 0.8894719503012049, 0.8909367940512049, 0.8921369070030121, 0.8905705831137049, 0.895850491810994, 0.8967358692582832, 0.8937547063253012, 0.8923501623682228, 0.8899396413780121, 0.8953313253012049, 0.8966843938253012, 0.8965726185993976, 0.8943650578878012, 0.893653226185994, 0.8921677922628012, 0.8947621540850903, 0.8952504353350903, 0.8949754094503012, 0.8969079442771084, 0.8948224538780121, 0.8955048710466867, 0.8973462208207832, 0.8935105657003012, 0.9015980915850903, 0.8970917851091867, 0.8964299581137049, 0.8989228397966867, 0.8955548757530121, 0.8958607868975903, 0.9003773884600903, 0.8939782567771084, 0.8973462208207832, 0.8965932087725903, 0.8933987904743976, 0.8942223974021084, 0.8973050404743976, 0.8978139118975903, 0.8949651143637049, 0.896949124623494, 0.8988007694841867, 0.8970814900225903, 0.9017407520707832, 0.9000111775225903, 0.8978242069841867, 0.8950768895896084, 0.8962975927146084, 0.9040497929216867, 0.8994920110128012, 0.8989934346762049, 0.8975491810993976, 0.8977124317582832, 0.9004788685993976, 0.8961049275225903, 0.8957181264118976, 0.8975285909262049, 0.9003259130271084, 0.8981595326618976, 0.8959107916039157, 0.8998685170368976, 0.9007538944841867, 0.897193265248494, 0.8929811041039157, 0.8984242634600903, 0.8985154485128012, 0.899634671498494, 0.9001744281814759, 0.8987492940512049, 0.9008244893637049, 0.8967858739646084, 0.900611233998494, 0.8981389424887049, 0.8977727315512049, 0.898780179310994, 0.9017304569841867, 0.8962975927146084, 0.9015672063253012, 0.9001332478350903, 0.9009877400225903, 0.9008656697100903, 0.9000508871423193, 0.8989125447100903, 0.896704983998494, 0.900000882435994, 0.8982507177146084, 0.8980065770896084, 0.8970094244164157, 0.8975285909262049, 0.8994817159262049, 0.9022790380271084, 0.8960431570030121, 0.9007538944841867, 0.898780179310994, 0.900977444935994, 0.8966535085655121, 0.9020657826618976, 0.8991155049887049, 0.8983110175075302], "accuracy_test": 0.8620137117346939, "start": "2016-01-25 14:20:46.836000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0, 211.0, 212.0, 213.0, 214.0, 215.0, 216.0, 217.0, 218.0, 219.0, 220.0], "accuracy_train_last": 0.9926768627145626, "batch_size_eval": 1024, "accuracy_train_std": [0.01958113134711317, 0.021405045147904266, 0.021409665552071796, 0.021691272649424657, 0.020038073790606063, 0.019620238676010638, 0.019077243038628212, 0.017615722278634855, 0.019979240531168695, 0.019978919863919024, 0.01913091058272998, 0.01896415800242524, 0.01866123368123795, 0.01849250711311962, 0.01811265062803141, 0.01703001365718191, 0.018401523402278276, 0.018512241968951685, 0.01713912811256811, 0.01741967805043885, 0.01867123830571841, 0.01810856069456205, 0.016328864151831393, 0.017362236609358844, 0.016091112107760257, 0.01665360339908459, 0.015867149259353442, 0.01502145444254184, 0.015440585513627814, 0.01563709404730068, 0.0151589726994699, 0.014845871886322129, 0.012828919548043775, 0.014133353021962499, 0.014567819683313159, 0.013779750386060791, 0.014475904995545161, 0.015126967583175057, 0.014309977923168314, 0.012995483938471495, 0.012956546008549431, 0.013368901483839757, 0.013791760605165945, 0.01326120678700206, 0.011715608604663377, 0.012116315112333827, 0.011679763563996682, 0.01314514058907637, 0.011995149375928795, 0.011865838556582678, 0.010542179208804929, 0.011513533621054227, 0.01172871088417753, 0.011855891904368594, 0.010252762387576997, 0.012317522435623332, 0.010686330620531178, 0.010824262737268366, 0.009881846454056806, 0.01052810064580863, 0.01021170867383359, 0.010168824665605702, 0.009773233366997354, 0.010394709687100441, 0.010723745412592418, 0.008866895251402522, 0.00874680120237388, 0.00933798938291376, 0.00943486883586697, 0.009814358422965066, 0.007848124831748437, 0.010039821284657838, 0.009267197502540913, 0.00835044654497718, 0.00924479092840608, 0.00829181755705504, 0.008630227759400193, 0.009010544155344392, 0.007454827265435983, 0.008606504606766074, 0.008226202081814583, 0.00813278418135069, 0.008525549323636174, 0.007597847891100907, 0.006859078837252976, 0.0073551238512394815, 0.00875075254174376, 0.006653696129305019, 0.006863857885655743, 0.007089228799570123, 0.007137108563427024, 0.007844507626695703, 0.008462338165031337, 0.008063045674659321, 0.006608944836271156, 0.007166164865285927, 0.006658507913351239, 0.007341967673002295, 0.006659211851274611, 0.006375230184599598, 0.007066395710631797, 0.006685576741729737, 0.008144528647914586, 0.006595230393702795, 0.006853290414381854, 0.007433158648804941, 0.006554503268764795, 0.004707665332555113, 0.006482927116495925, 0.006313531158977863, 0.006409022902736354, 0.006967381250122039, 0.005214496746668894, 0.0060894569398046525, 0.006187654593143394, 0.0059449285448767274, 0.006095375589392994, 0.005510791034645646, 0.005708850808678238, 0.005062431328238972, 0.005492275144007697, 0.0054729747975605005, 0.006098947110738398, 0.005675939953041678, 0.0060185799693286334, 0.005321732642694471, 0.005238939395493303, 0.005471919034860373, 0.004731263624267133, 0.0036798559139429995, 0.005258127524946781, 0.004662399431248948, 0.005637631847191445, 0.005187253793136276, 0.0055111384738270675, 0.00427833699244882, 0.005040731181660879, 0.005359409175265418, 0.004107660874554183, 0.005884803649469812, 0.004894985167808333, 0.005189396722679268, 0.0039153215540163894, 0.005101065870300327, 0.004111992433886677, 0.004670077446355383, 0.00483166817766942, 0.0042141645042876215, 0.005221643634749964, 0.004331899013876903, 0.004308238911029854, 0.0036702269216917324, 0.004338691863975942, 0.004009624406669027, 0.003731784116256898, 0.0042522991167256345, 0.004645947233802166, 0.003929952217164178, 0.004330514273349229, 0.004806551035992871, 0.004427562014562319, 0.004007980556997788, 0.004866593752589109, 0.003717815992845854, 0.0039999079436977905, 0.003582842314780175, 0.003414505011851959, 0.004103161418000455, 0.00417492107281686, 0.0038395483343642263, 0.0036101572026018005, 0.003822024136152705, 0.0041871404747504965, 0.003889275146053843, 0.003735869745896089, 0.004166365632062243, 0.003690723925645433, 0.003938942441035572, 0.004294182650495305, 0.0034905880817656973, 0.004778506087040771, 0.003274371794386489, 0.003804777062313275, 0.0036843253212282823, 0.0035851308960937445, 0.00375061927624003, 0.0034176523885822547, 0.003496121053180139, 0.003958766138727072, 0.004026139899111096, 0.0033622495938677994, 0.003309274340891228, 0.003974256593223704, 0.003335052044296445, 0.0031275415825271825, 0.003270884186100538, 0.0029023058061035108, 0.003748457600964802, 0.004213581195680124, 0.0032243083445093954, 0.0032207467649225225, 0.0032891829609643674, 0.003282662184470442, 0.0029541203336258576, 0.0034292594986210234, 0.002627791593441435, 0.0033950357604992524, 0.0035381735085372977, 0.003418638941770656, 0.0032707827889891525, 0.003238110583173807, 0.0031161331709872258, 0.003711225264641626, 0.003287149292410897, 0.002923776396878376, 0.0028851507461922538, 0.002681652824738247, 0.002764796862107492, 0.002457464208686572, 0.0029930969074047683, 0.0032967092005556997], "accuracy_test_std": 0.006521429641905436, "error_valid": [0.4621655567582832, 0.3324607021837349, 0.27962190559111444, 0.25245023060993976, 0.2271096103162651, 0.20590908556099397, 0.19489187217620485, 0.18170827842620485, 0.19084296169051207, 0.1629697500941265, 0.17048810476280118, 0.15512636483433728, 0.15800604762801207, 0.16207554828689763, 0.1493184652673193, 0.1496846762048193, 0.15995917262801207, 0.1486875235316265, 0.1478330313441265, 0.14581960655120485, 0.1491243293486446, 0.14198395143072284, 0.13858510212725905, 0.14349879988704817, 0.1365510871611446, 0.14083384318524095, 0.13197271507906627, 0.13314341349774095, 0.13051816641566272, 0.13211537556475905, 0.13504506306475905, 0.13550245905496983, 0.12976515436746983, 0.1516378012048193, 0.13082407756024095, 0.13304193335843373, 0.1320344855986446, 0.13253306193524095, 0.13226833113704817, 0.12012159967996983, 0.1342317512236446, 0.12293951195406627, 0.12447495058358427, 0.12754729856927716, 0.12066135636295183, 0.12664280167545183, 0.12378370905496983, 0.12855474632906627, 0.12256300592996983, 0.12171880882906627, 0.11408867893448793, 0.12078195594879515, 0.12038633047816272, 0.1280458749058735, 0.12254241575677716, 0.11875823606927716, 0.12247182087725905, 0.11887001129518071, 0.12108786709337349, 0.1181067041603916, 0.11494317112198793, 0.11663156532379515, 0.11530938205948793, 0.12253212067018071, 0.1331331184111446, 0.1112192912274097, 0.1161535791603916, 0.11531967714608427, 0.12294980704066272, 0.11654038027108427, 0.11169727739081325, 0.12179969879518071, 0.11457696018448793, 0.11431222938629515, 0.11975538874246983, 0.11394601844879515, 0.11591973362198793, 0.11335625705948793, 0.11486081042921681, 0.11703895660768071, 0.1135901025978916, 0.11663156532379515, 0.11532997223268071, 0.10817782850150603, 0.10888966020331325, 0.11323418674698793, 0.11453577983810237, 0.11424163450677716, 0.11445488987198793, 0.11367099256400603, 0.10696742046310237, 0.11617416933358427, 0.11495346620858427, 0.1102427287274097, 0.1127356104103916, 0.10854403943900603, 0.11013095350150603, 0.11089426063629515, 0.1091955713478916, 0.1087778849774097, 0.11434311464608427, 0.11314300169427716, 0.11237969455948793, 0.11253265013177716, 0.10807634836219882, 0.11057952513177716, 0.11050745952560237, 0.10921616152108427, 0.11657126553087349, 0.10929705148719882, 0.11207525414156627, 0.10998829301581325, 0.10733363140060237, 0.10772043251129515, 0.1069983057228916, 0.10485104480421681, 0.10977503765060237, 0.10893084054969882, 0.11173845773719882, 0.10631588855421681, 0.10756747693900603, 0.10593938253012047, 0.11113840126129515, 0.10342738140060237, 0.11052804969879515, 0.10906320594879515, 0.10786309299698793, 0.10942941688629515, 0.10414950818900603, 0.10326413074171681, 0.10624529367469882, 0.10764983763177716, 0.11006035862198793, 0.10466867469879515, 0.10331560617469882, 0.10342738140060237, 0.10563494211219882, 0.10634677381400603, 0.10783220773719882, 0.1052378459149097, 0.1047495646649097, 0.10502459054969882, 0.1030920557228916, 0.10517754612198793, 0.10449512895331325, 0.10265377917921681, 0.10648943429969882, 0.0984019084149097, 0.10290821489081325, 0.10357004188629515, 0.10107716020331325, 0.10444512424698793, 0.1041392131024097, 0.0996226115399097, 0.1060217432228916, 0.10265377917921681, 0.1034067912274097, 0.10660120952560237, 0.1057776025978916, 0.10269495952560237, 0.1021860881024097, 0.10503488563629515, 0.10305087537650603, 0.10119923051581325, 0.1029185099774097, 0.09825924792921681, 0.0999888224774097, 0.10217579301581325, 0.1049231104103916, 0.1037024072853916, 0.09595020707831325, 0.10050798898719882, 0.10100656532379515, 0.10245081890060237, 0.10228756824171681, 0.09952113140060237, 0.1038950724774097, 0.10428187358810237, 0.10247140907379515, 0.0996740869728916, 0.10184046733810237, 0.10408920839608427, 0.10013148296310237, 0.09924610551581325, 0.10280673475150603, 0.10701889589608427, 0.1015757365399097, 0.10148455148719882, 0.10036532850150603, 0.09982557181852414, 0.10125070594879515, 0.09917551063629515, 0.1032141260353916, 0.09938876600150603, 0.10186105751129515, 0.10222726844879515, 0.10121982068900603, 0.09826954301581325, 0.1037024072853916, 0.09843279367469882, 0.0998667521649097, 0.0990122599774097, 0.0991343302899097, 0.09994911285768071, 0.1010874552899097, 0.10329501600150603, 0.09999911756400603, 0.1017492822853916, 0.1019934229103916, 0.10299057558358427, 0.10247140907379515, 0.10051828407379515, 0.0977209619728916, 0.10395684299698793, 0.09924610551581325, 0.10121982068900603, 0.09902255506400603, 0.10334649143448793, 0.09793421733810237, 0.10088449501129515, 0.10168898249246983], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "momentum": 0.5212300886742134, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.007510211433592419, "patience_threshold": 1, "do_flip": true, "batch_size": 16, "optimization": "rmsprop", "nb_data_augmentation": 3, "learning_rate_decay_method": "exp", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 2.8650888088297172e-09, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.007456040694239475}, "accuracy_valid_max": 0.9040497929216867, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8983110175075302, "loss_train": [1.6005109548568726, 1.1023943424224854, 0.8955860137939453, 0.7882084846496582, 0.7162615656852722, 0.66038978099823, 0.6176533699035645, 0.5836182832717896, 0.5520766973495483, 0.5251914858818054, 0.5029808878898621, 0.4801703095436096, 0.45825299620628357, 0.44352731108665466, 0.4231399893760681, 0.4071734547615051, 0.39215993881225586, 0.38107600808143616, 0.3691774308681488, 0.353352427482605, 0.3429720103740692, 0.3325638175010681, 0.318532258272171, 0.3107073903083801, 0.301089882850647, 0.2910248041152954, 0.28362658619880676, 0.2758936583995819, 0.2675246596336365, 0.2577831447124481, 0.2513793408870697, 0.24818654358386993, 0.23682492971420288, 0.2303352653980255, 0.2262359857559204, 0.22243034839630127, 0.2152174711227417, 0.20764628052711487, 0.20588412880897522, 0.19912967085838318, 0.19341357052326202, 0.18876336514949799, 0.1845661848783493, 0.1778208464384079, 0.1763840913772583, 0.17260797321796417, 0.16965092718601227, 0.16439901292324066, 0.1607576459646225, 0.15885081887245178, 0.15503886342048645, 0.1511264443397522, 0.14969120919704437, 0.1438743621110916, 0.14104992151260376, 0.13895054161548615, 0.13665032386779785, 0.13192595541477203, 0.1293061375617981, 0.12539081275463104, 0.12434494495391846, 0.12386862188577652, 0.12317434698343277, 0.11945503205060959, 0.11687926948070526, 0.11124766618013382, 0.11166918277740479, 0.10900428891181946, 0.10820449888706207, 0.10538235306739807, 0.10240490734577179, 0.10214988142251968, 0.1006680577993393, 0.0951274186372757, 0.09525652974843979, 0.09382503479719162, 0.09314269572496414, 0.09231151640415192, 0.08995125442743301, 0.08878444135189056, 0.08639167994260788, 0.08729054778814316, 0.0831400603055954, 0.08278996497392654, 0.08144640922546387, 0.07856624573469162, 0.07993680238723755, 0.07932163774967194, 0.0770907774567604, 0.07442547380924225, 0.07487927377223969, 0.07067549228668213, 0.0709853321313858, 0.07060554623603821, 0.06789004802703857, 0.06729786098003387, 0.06725552678108215, 0.06650948524475098, 0.06318888068199158, 0.06345193088054657, 0.06483329087495804, 0.06182073429226875, 0.05917438864707947, 0.05885481461882591, 0.06018843874335289, 0.05947716161608696, 0.058510225266218185, 0.05464998632669449, 0.05303254351019859, 0.054888419806957245, 0.05441883206367493, 0.05444004014134407, 0.0525275282561779, 0.0512465201318264, 0.05245712772011757, 0.049971804022789, 0.049002453684806824, 0.046390973031520844, 0.04745166748762131, 0.04874290153384209, 0.04781769588589668, 0.04643549025058746, 0.04496222361922264, 0.04493281990289688, 0.04687469080090523, 0.045162785798311234, 0.04310133680701256, 0.042115990072488785, 0.04306482896208763, 0.04244599863886833, 0.03980866074562073, 0.0389154814183712, 0.03918641805648804, 0.03973209485411644, 0.03936875984072685, 0.0367872454226017, 0.03796272352337837, 0.03874684497714043, 0.03621561452746391, 0.03571417182683945, 0.036626510322093964, 0.03440349921584129, 0.03457613289356232, 0.033041812479496, 0.034467268735170364, 0.032228704541921616, 0.03251257166266441, 0.033336784690618515, 0.031332921236753464, 0.03133414313197136, 0.03135739266872406, 0.030301133170723915, 0.0306048933416605, 0.03068363666534424, 0.029610641300678253, 0.029001522809267044, 0.03035629354417324, 0.028826948255300522, 0.02824334427714348, 0.028404129669070244, 0.02815268374979496, 0.02615831047296524, 0.026795972138643265, 0.027606038376688957, 0.025550587102770805, 0.025382719933986664, 0.025526724755764008, 0.024176428094506264, 0.024928459897637367, 0.025414956733584404, 0.02352679893374443, 0.0243937149643898, 0.024174947291612625, 0.024485653266310692, 0.023724788799881935, 0.022474972531199455, 0.023518657311797142, 0.023043405264616013, 0.022085947915911674, 0.020555442199110985, 0.022979827597737312, 0.021776901558041573, 0.022224124521017075, 0.020747823640704155, 0.021565932780504227, 0.021208947524428368, 0.019858479499816895, 0.020301170647144318, 0.019802315160632133, 0.01940349116921425, 0.020306089892983437, 0.019736826419830322, 0.019636886194348335, 0.01866183988749981, 0.018277965486049652, 0.018049104139208794, 0.01924605295062065, 0.0178453978151083, 0.017734292894601822, 0.017135215923190117, 0.017225274816155434, 0.01675073429942131, 0.017768919467926025, 0.016991589218378067, 0.016622105613350868, 0.016682909801602364, 0.016042878851294518, 0.016035443171858788, 0.016473375260829926, 0.016457678750157356, 0.015896735712885857, 0.014979417435824871, 0.01635473407804966, 0.01581082120537758, 0.014291894622147083, 0.015369457192718983, 0.013496440835297108, 0.014436009339988232, 0.01494445651769638, 0.014612010680139065, 0.014884158037602901], "accuracy_train_first": 0.5438542892326504, "model": "residualv5", "loss_std": [0.3389670252799988, 0.2851511836051941, 0.26861655712127686, 0.2628467381000519, 0.2554861605167389, 0.2484036237001419, 0.2462501972913742, 0.24062494933605194, 0.23266281187534332, 0.22949962317943573, 0.22498874366283417, 0.21892575919628143, 0.2159431278705597, 0.21462105214595795, 0.2089305818080902, 0.20337195694446564, 0.20057250559329987, 0.1947157084941864, 0.19688817858695984, 0.18905846774578094, 0.186570942401886, 0.18068154156208038, 0.17946235835552216, 0.17355979979038239, 0.17176979780197144, 0.16662049293518066, 0.16395771503448486, 0.16086532175540924, 0.15915775299072266, 0.15369580686092377, 0.15295659005641937, 0.15513873100280762, 0.14542677998542786, 0.1430918127298355, 0.14416328072547913, 0.14386332035064697, 0.13891103863716125, 0.13183610141277313, 0.13400602340698242, 0.1281808316707611, 0.1296483874320984, 0.12667062878608704, 0.12310300767421722, 0.12038378417491913, 0.12306241691112518, 0.11763409525156021, 0.11696671694517136, 0.11432820558547974, 0.10997135192155838, 0.11162960529327393, 0.10953867435455322, 0.10664618015289307, 0.1084083765745163, 0.10283830761909485, 0.0983346477150917, 0.10327751189470291, 0.10003510862588882, 0.09683483093976974, 0.097172811627388, 0.09207765758037567, 0.09590369462966919, 0.0957389622926712, 0.09536851942539215, 0.09259875118732452, 0.08871034532785416, 0.08626934885978699, 0.08770737797021866, 0.085862897336483, 0.0866406261920929, 0.08290904015302658, 0.08243903517723083, 0.08242668956518173, 0.08464605361223221, 0.07807876169681549, 0.07875724881887436, 0.07809589058160782, 0.07923571020364761, 0.07559961825609207, 0.07770347595214844, 0.07597123086452484, 0.07598483562469482, 0.07724203169345856, 0.07201456278562546, 0.07275810092687607, 0.07028234004974365, 0.06825706362724304, 0.069757379591465, 0.07069218158721924, 0.06970999389886856, 0.06839815527200699, 0.06815802305936813, 0.06533775478601456, 0.06518994271755219, 0.06649815291166306, 0.06158948317170143, 0.06315804272890091, 0.06259136646986008, 0.062363214790821075, 0.0592314712703228, 0.061004363000392914, 0.06200728937983513, 0.06092656031250954, 0.0579199492931366, 0.05760873854160309, 0.058917466551065445, 0.05627918988466263, 0.05872635170817375, 0.05359490215778351, 0.053758297115564346, 0.05455508455634117, 0.05517222359776497, 0.05516617372632027, 0.05358675867319107, 0.05483419820666313, 0.05303801968693733, 0.053901556879282, 0.051967207342386246, 0.04898051917552948, 0.05020252615213394, 0.05208596587181091, 0.050913650542497635, 0.05086008086800575, 0.047784388065338135, 0.050290558487176895, 0.053290240466594696, 0.04888874664902687, 0.047870274633169174, 0.04647057503461838, 0.047514066100120544, 0.048805516213178635, 0.04380781948566437, 0.04332955926656723, 0.0443364679813385, 0.045994844287633896, 0.04489194601774216, 0.04170988127589226, 0.044438838958740234, 0.04487556591629982, 0.04254276305437088, 0.043993640691041946, 0.04371654614806175, 0.04145119711756706, 0.0411519855260849, 0.04014607518911362, 0.041443489491939545, 0.03953167796134949, 0.038943927735090256, 0.04146205633878708, 0.03877914324402809, 0.03962866589426994, 0.039019014686346054, 0.037686314433813095, 0.039589203894138336, 0.04071079567074776, 0.03865644708275795, 0.03724587336182594, 0.04033582657575607, 0.03930315747857094, 0.036024462431669235, 0.037341825664043427, 0.03750210627913475, 0.03500538319349289, 0.036396708339452744, 0.036771293729543686, 0.03504269942641258, 0.03583713248372078, 0.03680192679166794, 0.032789696007966995, 0.03428918495774269, 0.03679056465625763, 0.03414897993206978, 0.03379929065704346, 0.033200979232788086, 0.034291088581085205, 0.035201866179704666, 0.03481955826282501, 0.03380695730447769, 0.032409630715847015, 0.03203246369957924, 0.030197054147720337, 0.03538591042160988, 0.03302479535341263, 0.033431053161621094, 0.030748311430215836, 0.03217082470655441, 0.031227044761180878, 0.029478801414370537, 0.03247286379337311, 0.029609188437461853, 0.030226226896047592, 0.03136841580271721, 0.029517903923988342, 0.03136197105050087, 0.02963154762983322, 0.02861281856894493, 0.028168627992272377, 0.030856940895318985, 0.02998626045882702, 0.02703934721648693, 0.02785775251686573, 0.026861852034926414, 0.027464425191283226, 0.029339656233787537, 0.02788221649825573, 0.027471181005239487, 0.02693731151521206, 0.026990266516804695, 0.027098121121525764, 0.026197344064712524, 0.028384294360876083, 0.02585265412926674, 0.025703348219394684, 0.027655918151140213, 0.027026725932955742, 0.0242533627897501, 0.02676479145884514, 0.024369366466999054, 0.02461932599544525, 0.026764703914523125, 0.02517489716410637, 0.02630799077451229]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:19 2016", "state": "available"}], "summary": "56d0f1d0252dc75817d16febf49232a1"}