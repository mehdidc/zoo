{"content": {"hp_model": {"f0": 32, "f1": 64, "f2": 16, "f3": 16, "nonlin": "leaky_rectify", "nbg1": 6, "nbg3": 8, "nbg2": 2, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.015448111790559865, 0.018172719214133846, 0.016673501880325396, 0.016512129230077142, 0.014552510817846472, 0.015222717947720095, 0.012300193323149518, 0.010721424933661588, 0.016102964872844735, 0.015491990324344375, 0.012102044069427494, 0.008222872458033843, 0.00990560247073016, 0.004843481154438009, 0.005733665737653536, 0.009525673911100934, 0.011807831066820809, 0.008189090019907858, 0.011865737717586894, 0.013295717780836104, 0.011841768547053813, 0.015448372784704296, 0.009717253740504533, 0.01844912387535392, 0.017633747414040894, 0.015003237506204572, 0.01217604073482849, 0.013634452835520392, 0.01540594196855362, 0.018456775464889007, 0.01740228956006294, 0.01304189773473341, 0.011857814820962207, 0.014100048983348545, 0.015740524376295726, 0.015835650890076942, 0.013390900788952168, 0.015080950183108956, 0.018000711783262267, 0.017297475250116226, 0.018087194643722573, 0.016373043299758354, 0.013215572229269853, 0.015067858593490713, 0.018500532393948606, 0.018977748387142532, 0.019925660013504973, 0.012777689359242868, 0.022236816453400834, 0.018439745131247014, 0.014631617555766195, 0.01803152699080158, 0.0168911613197579, 0.013573999073008526, 0.016256739418673453, 0.012076579614420022, 0.015981183844126827, 0.014551828782456207, 0.013921680624350065, 0.01241587513210266, 0.016799948821795967, 0.012323334414278372, 0.01582051743348331, 0.01619976244491032, 0.012493189541347663, 0.01669999879823651, 0.014239815798271943, 0.014976359138411121, 0.016714488422481844, 0.015466383890670996, 0.017565465695117202, 0.012195412650417665, 0.012573506946476921, 0.017959963543906173, 0.013960670963517333, 0.01621486370620398, 0.014696237864139004, 0.01150510189537668, 0.019112423544705277, 0.01955899690129557, 0.011411412673176679, 0.014297164928419346, 0.018311712787446983, 0.014897134777862307, 0.011997013300837207, 0.018447096392117, 0.018103962395228785, 0.019257020456846692, 0.01679166232159784, 0.01277384818925129, 0.014027953296024343, 0.013722533163151317, 0.01546270661981422, 0.01066585045814186, 0.01617806321595877, 0.011531225950086177, 0.015486980472289443, 0.015883282400718443, 0.00943677075337627, 0.011096594710817777, 0.013310736021722355, 0.011381682118945042, 0.012516643901880094, 0.012197637311531978, 0.016236363413291148, 0.011833425277256839, 0.009898030172010753, 0.01271425390995753, 0.014307384635256867, 0.0135958347336767, 0.012682309196807157, 0.017036905652440235, 0.011943604382661753, 0.013488477335116858, 0.0146129345330345, 0.009259280035766154, 0.013377561187273367, 0.009612451493972505, 0.01232904392981052, 0.012941893769484422, 0.008501581203917178, 0.009350119821796817, 0.01224951898635982, 0.012262018808001804, 0.012851810728708118, 0.011839068500151225, 0.014380811199227846, 0.012471006890441099, 0.01724069145297827, 0.013112473647572776, 0.010829865082523316, 0.015985025127863744, 0.014669417173181421, 0.012817438671302356, 0.013902489619795693, 0.010710536760518504, 0.013194550327322023, 0.015221835810759095, 0.013150422851791011, 0.013474008064488773, 0.01269288979529131, 0.01791277833290742, 0.01050823389505125, 0.010387000530813173, 0.0169319776576805, 0.01645097268914688, 0.019929878896022764, 0.018261088885593798, 0.015658096098397703, 0.014039095070871515, 0.013504999157971343, 0.01390582841201337, 0.012976262214324524, 0.0142320466202261, 0.013514513889937204, 0.014000184941305796, 0.011660472906942209, 0.006118895287815229, 0.016668955555983914, 0.011638187053624655, 0.0170850595482231, 0.010741826658483743, 0.013847148749281456, 0.01094935826271507, 0.011575538469186917, 0.014621145083245232, 0.010882955013171908, 0.012018277097012534, 0.016206719088983964, 0.0138794249670033, 0.004899429130725906, 0.011129453317184004, 0.018417511435784045, 0.015849381057587316, 0.01342959923322143], "moving_avg_accuracy_train": [0.05661027189461054, 0.11837863227897746, 0.17794279052464007, 0.234149760924234, 0.28741871527777774, 0.3369442365840716, 0.38290721216200624, 0.4250571048427694, 0.4641172360839908, 0.49973391461891026, 0.5320001173049982, 0.561255794366488, 0.5882206693468288, 0.612351837000555, 0.6349231093555475, 0.6560068426821744, 0.6750308505570448, 0.6926222098015895, 0.7086612992680899, 0.7224780262534256, 0.7364218497758774, 0.7489223546746279, 0.7608400907430177, 0.7719522441998621, 0.7816415763217271, 0.7910525083790155, 0.7997757163531756, 0.8081194269310825, 0.815165989740466, 0.8220217541558158, 0.8285523041462881, 0.8344858190020542, 0.8395305803269779, 0.8437524282789608, 0.8482054582512215, 0.8525223939691042, 0.8563865295318192, 0.8603128971120444, 0.8636770723152456, 0.8669980150410392, 0.8703750191501689, 0.8744466168221767, 0.8782317822209853, 0.8808435546264246, 0.8832892645995979, 0.8861321085980637, 0.8884839462455478, 0.8914794703294648, 0.8931943013050085, 0.8954443862746368, 0.897092572347247, 0.8993297566615257, 0.9013268744050723, 0.9033100399372949, 0.9047901862757912, 0.9063155576733597, 0.9069210567752097, 0.9082402084228087, 0.9101875883222684, 0.9114172519937329, 0.9120660752266686, 0.9132822042707699, 0.9137421350295548, 0.9155627156445856, 0.916605891956419, 0.9180074552501643, 0.9185899908597916, 0.918414331019152, 0.9203139217613675, 0.9216560717710004, 0.922422588994048, 0.9232797931114392, 0.9243371619742062, 0.9256050502376106, 0.9264741072639604, 0.9280586151710546, 0.92895679141004, 0.9296885282608688, 0.9308815872622551, 0.931283444455188, 0.9322404621705219, 0.9331226684047894, 0.9336003616798975, 0.9343326270703702, 0.9352031463170062, 0.9355844710413871, 0.9361902248647124, 0.9363914254771706, 0.9367816973236027, 0.9370445863306297, 0.9374906300739049, 0.937945475767834, 0.9385780872269034, 0.9393146680102576, 0.9399102695462567, 0.9400789734655697, 0.9403307523429423, 0.9407224388980537, 0.9409099072809965, 0.9421269464993716, 0.9425204473436205, 0.9431139081867317, 0.9438457687407975, 0.9443651866526166, 0.9442021869576853, 0.9440323078417894, 0.9441792887362742, 0.9447698421496992, 0.9456057905205543, 0.9465207963245343, 0.9472770164279152, 0.9479623008673956, 0.9482420184320034, 0.9487376525234192, 0.9489210895366734, 0.9490140311378695, 0.949488447774221, 0.9497525902326333, 0.9499229251785467, 0.9501924840703451, 0.950109313897899, 0.9506554557189341, 0.9512074732757321, 0.9514857250887551, 0.9524429969585709, 0.952876678211634, 0.9531111342715153, 0.9534222342706742, 0.9529723438366392, 0.9525043029399631, 0.9528852424722404, 0.9529097229084789, 0.9536200714463501, 0.9530434044006908, 0.9532914508250089, 0.9533170910069044, 0.9536820721920667, 0.9538689374742443, 0.9543970096102178, 0.9542075703636201, 0.9540948432737328, 0.9546748737869373, 0.955252812966706, 0.955115049261859, 0.9556744393845841, 0.9555990726367403, 0.9558430648458883, 0.9558741405364564, 0.9559483953412456, 0.9561502274917834, 0.955650571777258, 0.9556192281758057, 0.9559399354512021, 0.9559588547371543, 0.9562711759933207, 0.9569404928774234, 0.9571151588386199, 0.9576117217834308, 0.95748195548018, 0.9577279611191776, 0.9578239523538361, 0.9577545234459719, 0.9582546152943425, 0.9590184849007055, 0.9588968878583555, 0.9591806530107814, 0.9596522804872503, 0.9600348925375009, 0.960260480549347, 0.9604122843885614, 0.960572267478406, 0.9604303310533321, 0.9604977205266718, 0.9611071422205439, 0.9610626367009627], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 747534221, "moving_var_accuracy_train": [0.028842505955835596, 0.06029622846140929, 0.08619760614289738, 0.10601085722211512, 0.12094800498118294, 0.13092819982900614, 0.1368487359619059, 0.139153383442714, 0.13896928977167558, 0.13648929090324619, 0.13221033233493895, 0.12669235086438072, 0.12056705612229136, 0.11375116978105237, 0.10696121382445636, 0.10026580674090557, 0.0934964419474232, 0.08693190103331652, 0.08055398244821449, 0.07421670170467878, 0.06854490346403842, 0.06309677672214772, 0.058065390946895175, 0.053370171442241786, 0.04887810271072786, 0.04478738321933715, 0.04099349411364749, 0.03752070225815362, 0.03421551845917753, 0.031216980164728774, 0.028479114896858417, 0.025948062794064957, 0.02358230306608748, 0.021384488760645692, 0.0194245051679858, 0.0176497780571182, 0.01601918414422972, 0.014556012991182143, 0.013202270765244438, 0.01198130163401201, 0.010885808881388521, 0.009946429161673967, 0.009080733539373609, 0.008234052381316576, 0.007464480618640834, 0.006790768414773269, 0.0061614718361770555, 0.00562608313339529, 0.00508994062752792, 0.004626512506110054, 0.004188309911468563, 0.003814523863226196, 0.0034689677904378735, 0.0031574675211478464, 0.002861438267683341, 0.002596235262019689, 0.0023399113982787915, 0.0021215817080751805, 0.0019435541335230371, 0.0017628073748750107, 0.0015903153816758837, 0.0014445945721754563, 0.0013020389416837995, 0.0012016656714978547, 0.001091293055706203, 0.0009998431671329508, 0.0009029129800480109, 0.0008128993894597315, 0.0007640854554049545, 0.0007038892096996778, 0.0006387882266087677, 0.0005815225940377404, 0.0005334325948415113, 0.0004945572011936679, 0.0004518988221097307, 0.00042930492766755343, 0.00039363491990729885, 0.00035909037728631645, 0.0003359918475847859, 0.0003038460656579141, 0.0002817044052592892, 0.00026053855529138394, 0.00023653841754799602, 0.00021771048921195477, 0.0002027596741196313, 0.0001837923836164861, 0.00016871558450509642, 0.00015220836123266897, 0.0001383583341364595, 0.00012514449639295405, 0.00011442064194189289, 0.00010484053919527691, 9.795826059906141e-05, 9.304539579281714e-05, 8.693352692069436e-05, 7.84963233401492e-05, 7.121722443395297e-05, 6.54762672076534e-05, 5.9244940038317e-05, 6.665110616605216e-05, 6.137958177926818e-05, 5.841138555209747e-05, 5.739082583226582e-05, 5.407989795310596e-05, 4.89110282627246e-05, 4.4279655662610245e-05, 4.0046120546441104e-05, 3.9180288498768285e-05, 4.155154675750929e-05, 4.493151267361219e-05, 4.5585181009066674e-05, 4.525319577510746e-05, 4.143205344114777e-05, 3.949972647019462e-05, 3.585259606365985e-05, 3.234507972838987e-05, 3.1136212059174265e-05, 2.8650531998280964e-05, 2.6046604742647026e-05, 2.4095902233710254e-05, 2.174856750860164e-05, 2.2258148755893606e-05, 2.2774844327423233e-05, 2.11941765377359e-05, 2.7322083778629623e-05, 2.6282590264092646e-05, 2.4149058033818787e-05, 2.2605201115727006e-05, 2.2166293627879502e-05, 2.1921224793744e-05, 2.1035136659633826e-05, 1.8937016619496288e-05, 2.158467036484837e-05, 2.2419107262309176e-05, 2.0730939793631394e-05, 1.8663762584616992e-05, 1.7996287715857407e-05, 1.6510926647421518e-05, 1.7369575609804015e-05, 1.5955603102186924e-05, 1.4474409363118364e-05, 1.6054886993040058e-05, 1.7455521553342616e-05, 1.5880778943367016e-05, 1.7108956833652298e-05, 1.5449182470412004e-05, 1.4440054006495354e-05, 1.3004739892744348e-05, 1.175388988777846e-05, 1.0945126851916566e-05, 1.2097516664245219e-05, 1.0896606789988685e-05, 1.0732624519420273e-05, 9.662583521906696e-06, 9.574226273196525e-06, 1.2648669467982337e-05, 1.1658376303189976e-05, 1.271171149630491e-05, 1.1592093987808828e-05, 1.0977553558795062e-05, 9.962727057096859e-06, 9.009837710612156e-06, 1.0359680650812263e-05, 1.4575183565458639e-05, 1.3250737775287172e-05, 1.2650367953339607e-05, 1.3387223447049703e-05, 1.3366028931317505e-05, 1.2487435597983693e-05, 1.1446091688587478e-05, 1.0531833821054415e-05, 9.659963977813797e-06, 8.734839650085473e-06, 1.1203908893734657e-05, 1.010134467581989e-05], "duration": 155123.332993, "accuracy_train": [0.5661027189461055, 0.6742938757382798, 0.7140202147356035, 0.7400124945205795, 0.7668393044596714, 0.7826739283407161, 0.7965739923634183, 0.8044061389696382, 0.8156584172549834, 0.8202840214331857, 0.8223959414797897, 0.8245568879198967, 0.8309045441698967, 0.8295323458840901, 0.8380645605504798, 0.8457604426218162, 0.8462469214308784, 0.8509444430024916, 0.8530131044665927, 0.846828569121447, 0.8619162614779439, 0.8614268987633813, 0.8680997153585271, 0.8719616253114618, 0.8688455654185124, 0.8757508968946106, 0.8782845881206165, 0.8832128221322444, 0.8785850550249169, 0.8837236338939645, 0.8873272540605389, 0.8878874527039498, 0.884933432251292, 0.881749059846807, 0.8882827280015688, 0.891374815430048, 0.8911637495962532, 0.8956502053340717, 0.8939546491440569, 0.896886499573182, 0.9007680561323367, 0.9110909958702473, 0.9122982708102622, 0.9043495062753784, 0.905300654358158, 0.9117177045842562, 0.9096504850729051, 0.9184391870847176, 0.9086277800849022, 0.915695151001292, 0.9119262470007383, 0.9194644154900333, 0.9193009340969915, 0.9211585297272978, 0.9181115033222591, 0.9200439002514765, 0.9123705486918604, 0.9201125732511997, 0.9277140074174051, 0.922484225036914, 0.9179054843230897, 0.9242273656676817, 0.9178815118586194, 0.9319479411798633, 0.9259944787629198, 0.9306215248938722, 0.9238328113464378, 0.916833392453396, 0.9374102384413067, 0.9337354218576966, 0.9293212440014765, 0.9309946301679586, 0.9338534817391103, 0.9370160446082503, 0.9342956205011074, 0.9423191863349022, 0.9370403775609081, 0.9362741599183279, 0.9416191182747323, 0.9349001591915835, 0.9408536216085271, 0.9410625245131967, 0.9378996011558692, 0.9409230155846253, 0.9430378195367294, 0.9390163935608158, 0.94164200927464, 0.9382022309892949, 0.9402941439414912, 0.9394105873938722, 0.9415050237633813, 0.9420390870131967, 0.9442715903585271, 0.9459438950604466, 0.9452706833702473, 0.9415973087393872, 0.9425967622392949, 0.9442476178940569, 0.9425971227274824, 0.9530802994647471, 0.9460619549418604, 0.9484550557747323, 0.9504325137273901, 0.9490399478589886, 0.9427351897033037, 0.9425033957987264, 0.9455021167866371, 0.9500848228705242, 0.9531293258582503, 0.9547558485603543, 0.9540829973583426, 0.9541298608227206, 0.9507594765134736, 0.9531983593461609, 0.9505720226559615, 0.9498505055486341, 0.9537581975013842, 0.9521298723583426, 0.9514559396917681, 0.9526185140965301, 0.9493607823458842, 0.9555707321082503, 0.956175631286914, 0.9539899914059615, 0.961058443786914, 0.9567798094892026, 0.9552212388104466, 0.9562221342631044, 0.9489233299303249, 0.9482919348698781, 0.9563136982627353, 0.9531300468346253, 0.9600132082871908, 0.9478534009897563, 0.9555238686438722, 0.9535478526439645, 0.9569669028585271, 0.9555507250138427, 0.9591496588339794, 0.9525026171442414, 0.9530802994647471, 0.9598951484057769, 0.9604542655846253, 0.9538751759182356, 0.9607089504891103, 0.9549207719061462, 0.9580389947282208, 0.9561538217515688, 0.9566166885843485, 0.9579667168466224, 0.9511536703465301, 0.9553371357627353, 0.9588263009297711, 0.9561291283107235, 0.9590820672988187, 0.9629643448343485, 0.9586871524893872, 0.9620807882867294, 0.9563140587509228, 0.959942011870155, 0.9586878734657622, 0.9571296632751938, 0.9627554419296788, 0.9658933113579733, 0.9578025144772055, 0.9617345393826136, 0.9638969277754706, 0.9634784009897563, 0.9622907726559615, 0.9617785189414912, 0.9620121152870063, 0.959152903227667, 0.9611042257867294, 0.9665919374653931, 0.9606620870247323], "end": "2016-01-26 05:12:52.715000", "learning_rate_per_epoch": [0.002719841431826353, 0.0027101323939859867, 0.0027004580479115248, 0.0026908181607723236, 0.0026812127325683832, 0.0026716417632997036, 0.0026621047873049974, 0.0026526020374149084, 0.002643133047968149, 0.0026336978189647198, 0.00262429635040462, 0.0026149284094572067, 0.0026055939961224794, 0.0025962928775697947, 0.0025870250537991524, 0.002577790291979909, 0.0025685883592814207, 0.0025594192557036877, 0.00255028298124671, 0.0025411793030798435, 0.0025321082212030888, 0.002523069502785802, 0.0025140629149973392, 0.002505088457837701, 0.0024961461313068867, 0.002487235702574253, 0.0024783571716398, 0.002469510305672884, 0.002460694871842861, 0.0024519108701497316, 0.0024431583005934954, 0.0024344369303435087, 0.0024257467593997717, 0.0024170875549316406, 0.0024084593169391155, 0.0023998618125915527, 0.0023912950418889523, 0.0023827587720006704, 0.0023742530029267073, 0.0023657777346670628, 0.0023573327343910933, 0.002348917769268155, 0.0023405328392982483, 0.002332177944481373, 0.002323852851986885, 0.0023155573289841413, 0.0023072916083037853, 0.0022990552242845297, 0.002290848409757018, 0.0022826706990599632, 0.0022745223250240088, 0.002266403054818511, 0.0022583126556128263, 0.0022502511274069548, 0.0022422184702008963, 0.002234214451164007, 0.0022262390702962875, 0.0022182920947670937, 0.0022103735245764256, 0.0022024831268936396, 0.0021946209017187357, 0.002186786849051714, 0.0021789807360619307, 0.002171202562749386, 0.002163452096283436, 0.0021557293366640806, 0.0021480340510606766, 0.0021403662394732237, 0.002132725901901722, 0.0021251128055155277, 0.0021175267174839973, 0.0021099678706377745, 0.0021024360321462154, 0.0020949309691786766, 0.002087452681735158, 0.0020800011698156595, 0.0020725762005895376, 0.0020651777740567923, 0.00205780565738678, 0.0020504598505795, 0.0020431403536349535, 0.002035846933722496, 0.002028579590842128, 0.002021338324993849, 0.0020141229033470154, 0.002006933093070984, 0.0019997688941657543, 0.0019926303066313267, 0.001985517330467701, 0.0019784297328442335, 0.0019713672809302807, 0.001964330207556486, 0.001957318279892206, 0.0019503312651067972, 0.0019433691632002592, 0.0019364319741725922, 0.0019295195816084743, 0.0019226318690925837, 0.0019157687202095985, 0.001908930018544197, 0.0019021157640963793, 0.0018953258404508233, 0.0018885601311922073, 0.0018818185199052095, 0.00187510100658983, 0.0018684074748307467, 0.001861737808212638, 0.0018550920067355037, 0.0018484699539840221, 0.0018418715335428715, 0.0018352966289967299, 0.0018287452403455973, 0.001822217251174152, 0.001815712545067072, 0.0018092310056090355, 0.0018027726328000426, 0.0017963373102247715, 0.0017899249214679003, 0.001783535466529429, 0.0017771688289940357, 0.0017708248924463987, 0.001764503656886518, 0.0017582048894837499, 0.0017519287066534162, 0.0017456748755648732, 0.001739443396218121, 0.0017332341521978378, 0.0017270470270887017, 0.0017208820208907127, 0.001714739017188549, 0.0017086178995668888, 0.001702518668025732, 0.001696441206149757, 0.0016903853975236416, 0.001684351242147386, 0.0016783386236056685, 0.001672347541898489, 0.0016663777641952038, 0.001660429290495813, 0.0016545021208003163, 0.0016485960222780704, 0.001642711111344397, 0.0016368471551686525, 0.0016310041537508368, 0.0016251819906756282, 0.0016193806659430265, 0.0016135999467223883, 0.0016078399494290352, 0.0016021004412323236, 0.0015963814221322536, 0.0015906828921288252, 0.0015850046183913946, 0.001579346600919962, 0.0015737088397145271, 0.0015680912183597684, 0.0015624936204403639, 0.0015569160459563136, 0.0015513583784922957, 0.0015458205016329885, 0.0015403024153783917, 0.0015348040033131838, 0.0015293252654373646, 0.0015238660853356123, 0.0015184263465926051, 0.001513006049208343, 0.0015076050767675042, 0.0015022234292700887, 0.0014968609903007746, 0.00149151764344424, 0.0014861933887004852, 0.0014808881096541882, 0.0014756018063053489, 0.0014703343622386456, 0.0014650857774540782, 0.001459855935536325], "accuracy_valid": [0.5542815794427711, 0.6678128529743976, 0.7046295533697289, 0.7236534026731928, 0.7438464796686747, 0.7549754682793675, 0.7634586196347892, 0.7645881377070783, 0.7686164580195783, 0.771862351750753, 0.7618011106927711, 0.7677825560052711, 0.7684134977409638, 0.7612613540097892, 0.7639880812311747, 0.7671722044427711, 0.7576698395143072, 0.7667045133659638, 0.7652396696159638, 0.7615775602409638, 0.7734389707266567, 0.769054734563253, 0.7715770307793675, 0.7735213314194277, 0.7622599774096386, 0.7708754941641567, 0.7707431287650602, 0.7694400649472892, 0.7742728727409638, 0.7727477291980422, 0.7741213878953314, 0.7702239622552711, 0.7653102644954819, 0.7627673781061747, 0.7688414791980422, 0.7716388012989458, 0.7684943877070783, 0.769543015813253, 0.7692179852221386, 0.7749964702560241, 0.7723918133471386, 0.7820456631212349, 0.7773246305534638, 0.7742125729480422, 0.7817103374435241, 0.7750979503953314, 0.7740096126694277, 0.7819838926016567, 0.7729109798569277, 0.7809470303087349, 0.7768878247364458, 0.7780173428087349, 0.7802043133471386, 0.7821574383471386, 0.7801837231739458, 0.7792880506400602, 0.7751994305346386, 0.7808043698230422, 0.784313523625753, 0.7773349256400602, 0.7744670086596386, 0.7838149472891567, 0.771007859563253, 0.7881388836596386, 0.7828589749623494, 0.7797763318900602, 0.7742728727409638, 0.7703563276543675, 0.7829104503953314, 0.7837649425828314, 0.7765319088855422, 0.7759406767695783, 0.7788100644766567, 0.7851591914533133, 0.7765216137989458, 0.7853518566453314, 0.7858504329819277, 0.7856665921498494, 0.7826766048569277, 0.7818324077560241, 0.7866843349962349, 0.7873152767319277, 0.7848032756024097, 0.7805190488516567, 0.7889830807605422, 0.7872343867658133, 0.7821986186935241, 0.7868475856551205, 0.7796145519578314, 0.785168015813253, 0.7846900296498494, 0.7862872387989458, 0.7857695430158133, 0.7888713055346386, 0.7818015224962349, 0.7792880506400602, 0.7853209713855422, 0.7906317653426205, 0.778210008000753, 0.7840590879141567, 0.7829001553087349, 0.785656297063253, 0.7827677899096386, 0.7836825818900602, 0.7815779720444277, 0.7836428722703314, 0.7840796780873494, 0.7876300122364458, 0.7878947430346386, 0.7886477550828314, 0.7877623776355422, 0.7925848903426205, 0.7885256847703314, 0.7869387707078314, 0.7829192747552711, 0.7851989010730422, 0.7817706372364458, 0.7837134671498494, 0.7764907285391567, 0.7890139660203314, 0.7871417309864458, 0.7898375729480422, 0.7916995128953314, 0.7904993999435241, 0.7977015483810241, 0.7891257412462349, 0.7895228374435241, 0.7894007671310241, 0.7825648296310241, 0.7751376600150602, 0.7865004941641567, 0.7789115446159638, 0.7874373470444277, 0.7795630765248494, 0.7870917262801205, 0.7882300687123494, 0.7811602856739458, 0.7852606715926205, 0.7902552593185241, 0.7765216137989458, 0.779064500188253, 0.7880682887801205, 0.7886374599962349, 0.7782408932605422, 0.7913230068712349, 0.7855959972703314, 0.787499117564006, 0.7874579372176205, 0.7803160885730422, 0.7851886059864458, 0.7800513577748494, 0.7913127117846386, 0.7869593608810241, 0.7901434840926205, 0.7868064053087349, 0.7929202160203314, 0.7905494046498494, 0.7836825818900602, 0.7823309840926205, 0.7869387707078314, 0.7839884930346386, 0.7868975903614458, 0.7845988445971386, 0.7900817135730422, 0.7794718914721386, 0.7873255718185241, 0.7877932628953314, 0.7877726727221386, 0.7882918392319277, 0.7854224515248494, 0.789806687688253, 0.7824927640248494, 0.7865931499435241, 0.7904891048569277, 0.7842017483998494], "accuracy_test": 0.09998804209183673, "start": "2016-01-24 10:07:29.382000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0], "accuracy_train_last": 0.9606620870247323, "batch_size_eval": 1024, "accuracy_train_std": [0.02301698048165912, 0.0209985467712634, 0.023882778542791874, 0.024042137559940526, 0.023947565829511613, 0.021612406934751176, 0.02270473316892965, 0.025018282051965275, 0.02412262093960513, 0.02598323891246376, 0.028281152825873717, 0.0290769581353952, 0.029262009806637762, 0.028988687540591244, 0.030552618504801957, 0.029581959615717402, 0.030946932382081214, 0.030200745066081294, 0.03044417260929952, 0.031401435311178075, 0.029274825993462542, 0.029844447806879316, 0.03060008710118428, 0.029410794777685413, 0.029602800577380517, 0.028698739003618273, 0.028482405153089967, 0.027576078114786226, 0.028262003670019097, 0.02688745006182709, 0.028202118285056168, 0.026762573231025426, 0.02699056958040513, 0.02626433481881514, 0.026845350656737144, 0.027862401128520703, 0.02649302398317474, 0.02640492095162364, 0.02456401720679113, 0.023332646988974883, 0.024878063168679982, 0.023416256514395724, 0.02240769954067145, 0.020901351288260057, 0.022645472505105697, 0.023411314585343616, 0.022384260485478594, 0.017684645283075246, 0.019973687802167874, 0.02181303750164583, 0.02251591308775711, 0.01779568895276252, 0.01850937055826126, 0.020400833882250585, 0.023024805372487203, 0.018902171404418334, 0.02009733323688755, 0.019284982899264346, 0.01988574813255533, 0.01851024983976618, 0.018641184752607856, 0.01751673083987177, 0.01558391059658402, 0.016291696600130877, 0.018467254973934182, 0.016945850233485656, 0.016615918118992268, 0.019266367280304335, 0.016288477746226722, 0.0166803916533331, 0.01717830183440405, 0.01653983217003845, 0.015456791434144168, 0.015461091815429274, 0.01674232896813538, 0.015140895546683068, 0.014049127607551063, 0.015043445898458137, 0.014059521838200315, 0.015299906907556593, 0.013853817818095112, 0.012538501264240458, 0.013417805234391508, 0.014163486848817792, 0.01238546531559841, 0.013676364146176305, 0.01294793278686751, 0.014557974156915964, 0.015040423029094417, 0.013174197495485703, 0.01372452406211459, 0.013686234595947413, 0.013206764484450219, 0.013753534314437738, 0.01277026210020068, 0.012293989902703955, 0.012768724320667954, 0.012119002930399506, 0.011270815207529247, 0.012230547924131278, 0.012410790585319666, 0.012534449206427902, 0.011354273037038953, 0.011063053673020637, 0.013749421840860369, 0.013479036964180843, 0.011431963463258171, 0.011191429633880147, 0.010730517391943944, 0.011160251457489712, 0.011170654799075343, 0.011383851543746742, 0.010495222303774733, 0.012341124731885639, 0.011564535479230881, 0.012339482815304382, 0.012462239900154692, 0.010736158135795839, 0.010964003417122864, 0.009960654536797712, 0.012601397029468737, 0.009819574969575232, 0.01001365563001163, 0.010622042633558938, 0.009301225136687984, 0.009915255443709728, 0.011164243344921528, 0.009669195505178637, 0.010844735215271234, 0.011901093614621041, 0.010312531539243122, 0.01025858666008268, 0.009920651844702545, 0.010072275011013444, 0.011238517856082712, 0.010666980745436684, 0.009112984687013517, 0.008720167313378472, 0.009503893786493045, 0.009961720211308889, 0.011597808659945894, 0.010637381778830881, 0.008192672259747609, 0.010313704208555322, 0.008960318174068744, 0.00872606482274095, 0.009396375896781755, 0.009614997081270088, 0.009742304001416227, 0.008914788556629442, 0.00992771785331989, 0.010017989777383078, 0.009064274987785082, 0.009071869279156023, 0.008739929015052123, 0.009200845602726127, 0.0097740315235567, 0.008172735659436296, 0.010725066298221276, 0.009022789782403928, 0.008843343488398605, 0.008593299720928323, 0.007068545784060103, 0.0073936886659459, 0.009789238217491478, 0.008016672861776626, 0.00808894765165686, 0.00787182634898686, 0.0075869469885246985, 0.008751545497771497, 0.00822921222479736, 0.008273754523184029, 0.0088635961244916, 0.008686291876518195, 0.00902724379778262], "accuracy_test_std": 0.007350288587607382, "error_valid": [0.4457184205572289, 0.33218714702560237, 0.2953704466302711, 0.2763465973268072, 0.2561535203313253, 0.24502453172063254, 0.23654138036521077, 0.23541186229292166, 0.23138354198042166, 0.22813764824924698, 0.23819888930722888, 0.23221744399472888, 0.2315865022590362, 0.23873864599021077, 0.23601191876882532, 0.23282779555722888, 0.24233016048569278, 0.2332954866340362, 0.2347603303840362, 0.2384224397590362, 0.22656102927334332, 0.23094526543674698, 0.22842296922063254, 0.2264786685805723, 0.23774002259036142, 0.22912450583584332, 0.22925687123493976, 0.23055993505271077, 0.2257271272590362, 0.22725227080195776, 0.22587861210466864, 0.22977603774472888, 0.2346897355045181, 0.23723262189382532, 0.23115852080195776, 0.2283611987010542, 0.23150561229292166, 0.23045698418674698, 0.23078201477786142, 0.22500352974397586, 0.22760818665286142, 0.2179543368787651, 0.2226753694465362, 0.22578742705195776, 0.21828966255647586, 0.22490204960466864, 0.2259903873305723, 0.21801610739834332, 0.2270890201430723, 0.2190529696912651, 0.2231121752635542, 0.2219826571912651, 0.21979568665286142, 0.21784256165286142, 0.2198162768260542, 0.22071194935993976, 0.22480056946536142, 0.21919563017695776, 0.21568647637424698, 0.22266507435993976, 0.22553299134036142, 0.21618505271084332, 0.22899214043674698, 0.21186111634036142, 0.21714102503765065, 0.22022366810993976, 0.2257271272590362, 0.22964367234563254, 0.21708954960466864, 0.21623505741716864, 0.22346809111445776, 0.22405932323042166, 0.22118993552334332, 0.21484080854668675, 0.2234783862010542, 0.21464814335466864, 0.2141495670180723, 0.21433340785015065, 0.2173233951430723, 0.21816759224397586, 0.2133156650037651, 0.2126847232680723, 0.2151967243975903, 0.21948095114834332, 0.21101691923945776, 0.21276561323418675, 0.21780138130647586, 0.21315241434487953, 0.22038544804216864, 0.21483198418674698, 0.21530997035015065, 0.2137127612010542, 0.21423045698418675, 0.21112869446536142, 0.2181984775037651, 0.22071194935993976, 0.21467902861445776, 0.20936823465737953, 0.22178999199924698, 0.21594091208584332, 0.2170998446912651, 0.21434370293674698, 0.21723221009036142, 0.21631741810993976, 0.2184220279555723, 0.21635712772966864, 0.21592032191265065, 0.2123699877635542, 0.21210525696536142, 0.21135224491716864, 0.21223762236445776, 0.20741510965737953, 0.21147431522966864, 0.21306122929216864, 0.21708072524472888, 0.21480109892695776, 0.2182293627635542, 0.21628653285015065, 0.22350927146084332, 0.21098603397966864, 0.2128582690135542, 0.21016242705195776, 0.20830048710466864, 0.20950060005647586, 0.20229845161897586, 0.2108742587537651, 0.21047716255647586, 0.21059923286897586, 0.21743517036897586, 0.22486233998493976, 0.21349950583584332, 0.2210884553840362, 0.2125626529555723, 0.22043692347515065, 0.21290827371987953, 0.21176993128765065, 0.2188397143260542, 0.21473932840737953, 0.20974474068147586, 0.2234783862010542, 0.22093549981174698, 0.21193171121987953, 0.2113625400037651, 0.22175910673945776, 0.2086769931287651, 0.21440400272966864, 0.21250088243599397, 0.21254206278237953, 0.21968391142695776, 0.2148113940135542, 0.21994864222515065, 0.20868728821536142, 0.21304063911897586, 0.20985651590737953, 0.2131935946912651, 0.20707978397966864, 0.20945059535015065, 0.21631741810993976, 0.21766901590737953, 0.21306122929216864, 0.21601150696536142, 0.2131024096385542, 0.21540115540286142, 0.20991828642695776, 0.22052810852786142, 0.21267442818147586, 0.21220673710466864, 0.21222732727786142, 0.2117081607680723, 0.21457754847515065, 0.21019331231174698, 0.21750723597515065, 0.21340685005647586, 0.2095108951430723, 0.21579825160015065], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-3, 3], "momentum": 0.9718918391843776, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.002729585127997982, "patience_threshold": 1, "do_flip": true, "batch_size": 16, "optimization": "adam", "nb_data_augmentation": 0, "learning_rate_decay_method": "exp", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 3.1101077176146207e-06, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.0035696800132388297}, "accuracy_valid_max": 0.7977015483810241, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-3, 3)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.7842017483998494, "loss_train": [1.679328203201294, 1.2376649379730225, 1.0415856838226318, 0.9187281131744385, 0.8398072719573975, 0.7811595797538757, 0.7350327372550964, 0.69672691822052, 0.6604788899421692, 0.629440188407898, 0.6021382808685303, 0.5782900452613831, 0.5574332475662231, 0.5343225002288818, 0.5180734992027283, 0.500130295753479, 0.48512303829193115, 0.46998071670532227, 0.45780161023139954, 0.44678303599357605, 0.4356198310852051, 0.4190499186515808, 0.41119956970214844, 0.39996352791786194, 0.39550119638442993, 0.38432741165161133, 0.3791460692882538, 0.3746683895587921, 0.36242902278900146, 0.3550148904323578, 0.3562764525413513, 0.3498421907424927, 0.346782386302948, 0.33990323543548584, 0.33235085010528564, 0.33498233556747437, 0.31958478689193726, 0.3256974220275879, 0.3228052258491516, 0.3153449594974518, 0.311553031206131, 0.30945560336112976, 0.3074181377887726, 0.30498597025871277, 0.2990054488182068, 0.300819993019104, 0.2975931763648987, 0.29944783449172974, 0.29426878690719604, 0.28862857818603516, 0.285373717546463, 0.29023972153663635, 0.2922256588935852, 0.2824885845184326, 0.2813326418399811, 0.28427261114120483, 0.27570846676826477, 0.28293728828430176, 0.2786118686199188, 0.27600690722465515, 0.274652361869812, 0.277800589799881, 0.2716326415538788, 0.27221086621284485, 0.2707388699054718, 0.2723918557167053, 0.26745960116386414, 0.2679201662540436, 0.26806187629699707, 0.26303181052207947, 0.2676974833011627, 0.2626893222332001, 0.2650444507598877, 0.2664540410041809, 0.26126962900161743, 0.25922414660453796, 0.2589209973812103, 0.25955864787101746, 0.2582511305809021, 0.258224755525589, 0.2563229501247406, 0.2591717541217804, 0.25204718112945557, 0.2548444867134094, 0.25592273473739624, 0.25315842032432556, 0.2559604048728943, 0.25044873356819153, 0.2522903382778168, 0.2535841763019562, 0.24270185828208923, 0.24938149750232697, 0.24808326363563538, 0.24543152749538422, 0.24880768358707428, 0.24835239350795746, 0.2475627213716507, 0.24643519520759583, 0.24334217607975006, 0.24741798639297485, 0.24290937185287476, 0.24445509910583496, 0.2421073317527771, 0.23770365118980408, 0.2423408180475235, 0.24016697704792023, 0.24293328821659088, 0.24362128973007202, 0.23867760598659515, 0.23977532982826233, 0.2396053969860077, 0.23959322273731232, 0.23733708262443542, 0.23873496055603027, 0.23417909443378448, 0.23679807782173157, 0.23539257049560547, 0.23320455849170685, 0.23461514711380005, 0.23682700097560883, 0.23354944586753845, 0.23032012581825256, 0.23894895613193512, 0.23020027577877045, 0.23106403648853302, 0.23221871256828308, 0.2325935810804367, 0.23081520199775696, 0.22664877772331238, 0.23272183537483215, 0.22696243226528168, 0.22803667187690735, 0.22546570003032684, 0.23098315298557281, 0.22447991371154785, 0.2269657403230667, 0.2230195701122284, 0.23043091595172882, 0.2251691371202469, 0.22253353893756866, 0.2234637439250946, 0.2241777628660202, 0.22264564037322998, 0.22379200160503387, 0.22395838797092438, 0.22114475071430206, 0.22130125761032104, 0.22172333300113678, 0.21975959837436676, 0.21904954314231873, 0.2202833741903305, 0.2194148451089859, 0.21855193376541138, 0.214138463139534, 0.221170112490654, 0.21759912371635437, 0.21671462059020996, 0.2152681201696396, 0.2164594531059265, 0.21901141107082367, 0.21351154148578644, 0.21330857276916504, 0.21441347897052765, 0.2161734253168106, 0.21116234362125397, 0.21195054054260254, 0.2128075212240219, 0.21271876990795135, 0.21350394189357758, 0.21293790638446808, 0.21139419078826904, 0.20689183473587036, 0.21240933239459991, 0.20971886813640594, 0.21069671213626862], "accuracy_train_first": 0.5661027189461055, "model": "residualv5", "loss_std": [0.3184055685997009, 0.27441540360450745, 0.2738869786262512, 0.2679903507232666, 0.25982651114463806, 0.2539948523044586, 0.2467164695262909, 0.24155865609645844, 0.23748989403247833, 0.22895970940589905, 0.22550277411937714, 0.22211779654026031, 0.2165713608264923, 0.2129645198583603, 0.20968542993068695, 0.2066073715686798, 0.19914524257183075, 0.19452467560768127, 0.1908242106437683, 0.18583662807941437, 0.18205179274082184, 0.17361247539520264, 0.17476339638233185, 0.1677892804145813, 0.1656893640756607, 0.16188685595989227, 0.158311128616333, 0.15633520483970642, 0.14842383563518524, 0.15208175778388977, 0.1499827355146408, 0.14518596231937408, 0.1455649584531784, 0.14343006908893585, 0.13444626331329346, 0.1382044106721878, 0.12905055284500122, 0.13294020295143127, 0.13051795959472656, 0.12567594647407532, 0.12381336092948914, 0.12254035472869873, 0.1169285848736763, 0.11941900849342346, 0.11253557354211807, 0.11740708351135254, 0.11618507653474808, 0.11628972738981247, 0.1152912974357605, 0.11052888631820679, 0.10741616040468216, 0.11253672093153, 0.11373110115528107, 0.10715315490961075, 0.10671606659889221, 0.10989566147327423, 0.10066916793584824, 0.10688075423240662, 0.10220485180616379, 0.10106538981199265, 0.1016416847705841, 0.10313595831394196, 0.1017148345708847, 0.09968185424804688, 0.10204582661390305, 0.10178028047084808, 0.09711109846830368, 0.09953126311302185, 0.0969996452331543, 0.09120162576436996, 0.09458362311124802, 0.09418731182813644, 0.09709910303354263, 0.09344135224819183, 0.09078876674175262, 0.09196238219738007, 0.09050020575523376, 0.09075997769832611, 0.0906466469168663, 0.09219028800725937, 0.0885813906788826, 0.09096874296665192, 0.0824422612786293, 0.08483897894620895, 0.08810539543628693, 0.08565021306276321, 0.08664111793041229, 0.08679691702127457, 0.08728495985269547, 0.08802895247936249, 0.07527250796556473, 0.08657355606555939, 0.08241589367389679, 0.07771410793066025, 0.08211128413677216, 0.08400654047727585, 0.07877250760793686, 0.08518652617931366, 0.07680320739746094, 0.08338361233472824, 0.07726899534463882, 0.08000290393829346, 0.0793851763010025, 0.07155247777700424, 0.08120252937078476, 0.0798388123512268, 0.07982625812292099, 0.08089455962181091, 0.0779922753572464, 0.07781065255403519, 0.07486427575349808, 0.0777052789926529, 0.07504747807979584, 0.0788760632276535, 0.07329397648572922, 0.07473275065422058, 0.07129313051700592, 0.07036613672971725, 0.07229817658662796, 0.07270437479019165, 0.07441860437393188, 0.07269544899463654, 0.08240260928869247, 0.06882870942354202, 0.07265596091747284, 0.07147306203842163, 0.07611887902021408, 0.07131452858448029, 0.06545042991638184, 0.07287664711475372, 0.06656288355588913, 0.07027942687273026, 0.06485848873853683, 0.07207605987787247, 0.0643063634634018, 0.06751265376806259, 0.06730514764785767, 0.07219741493463516, 0.06775631755590439, 0.06539938598871231, 0.06675811111927032, 0.06689669191837311, 0.06712384521961212, 0.06800481677055359, 0.06685902923345566, 0.06336157023906708, 0.0660565048456192, 0.06851939111948013, 0.06819511204957962, 0.059644665569067, 0.06453675031661987, 0.062449656426906586, 0.06711936742067337, 0.06038891524076462, 0.06739397346973419, 0.06060323119163513, 0.06227589026093483, 0.06178431212902069, 0.062955342233181, 0.07117364555597305, 0.0599120557308197, 0.05797416716814041, 0.062386658042669296, 0.06436565518379211, 0.058233607560396194, 0.05710265040397644, 0.05867094174027443, 0.06189388409256935, 0.06678043305873871, 0.06934035569429398, 0.05928213521838188, 0.055254437029361725, 0.061706408858299255, 0.05909687280654907, 0.058885686099529266]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:15 2016", "state": "available"}], "summary": "e1d4f820f1ea7aa49a79e6594e0e58dc"}