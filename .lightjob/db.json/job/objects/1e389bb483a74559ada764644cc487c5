{"content": {"hp_model": {"f0": 16, "f1": 16, "f2": 32, "f3": 64, "nonlin": "rectify", "nbg1": 2, "nbg3": 2, "nbg2": 2, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.08734637984798871, 0.08674554138438124, 0.0822224540550599, 0.08735444477543763, 0.07975171970109929, 0.08240835196524962, 0.07972611005989866, 0.07821842704286394, 0.07844303918353278, 0.07481827385326044, 0.07872253500332478, 0.07025397748981466, 0.07385234095589022, 0.07199063283723142, 0.06958247705245026, 0.07393536778475816, 0.07152336929406841, 0.06906330317803834, 0.06870146701564608, 0.06731723070387251, 0.06909080010396033, 0.07535554892228326, 0.07164308481166455, 0.07473002306835923, 0.0725601589663602, 0.07084439088770426, 0.07073013186388616, 0.07285500351257888, 0.07249929959313398, 0.07466794660349069, 0.07454076749716497, 0.07120210021760684, 0.07107009939011769, 0.07117554423436365, 0.07266295066776567, 0.07117090844810582, 0.07111337451229478, 0.07054076373448859, 0.07421369600781669, 0.06933752452815364, 0.0721371445486957, 0.07148158990961181, 0.07093295018472093, 0.07098472625112212, 0.07271889073781608, 0.06965267105480093, 0.06937918120029717, 0.07092389806022148, 0.07307852894253199, 0.07228237235001242, 0.07045158487772425, 0.06843877020409089, 0.06933752452815363, 0.0717112618327477, 0.06943121689440471, 0.07108816525290292, 0.07062263306410815, 0.07009373485509351, 0.07184841471250361, 0.07369278339197001, 0.0678345190837708, 0.07181117082775039, 0.07355725102903471, 0.07207196876871903, 0.07012845761379205, 0.07177291366648603, 0.06505699721212961, 0.07119358333979744, 0.06993899041088275, 0.06801529610120617, 0.07117842578715948, 0.07040258335339575, 0.06745485709176119, 0.06449145302685078, 0.06663661181503779, 0.06726475278669819, 0.06679660391043682, 0.07142155945879601, 0.07084955149606101, 0.069075697513647, 0.06792870919891034, 0.06897079183655551, 0.0684027986363468, 0.0690427701653579, 0.06618611839708431, 0.06648817242125894, 0.06917129156221043, 0.06801936035161604, 0.06909170357307633, 0.06849659787025004, 0.064622402518204, 0.067227755111125, 0.06896523203768203, 0.06880769051611699, 0.06963115924700429, 0.06581862656554766, 0.07122401397007336, 0.06826551580418314, 0.06752792305114173, 0.06433875756698278, 0.06877502383628659, 0.06770714799447951, 0.06584734300877515, 0.06803299348261953, 0.06490989121016744, 0.0710456278784296, 0.06688305649886603, 0.06532783325025339, 0.0684563581087963, 0.06633832550812652, 0.0668345073907145, 0.06566181972911223, 0.06608944480300316, 0.0682749203870835, 0.06726382477848832, 0.06466999238000164, 0.06554369630943933, 0.06905400596641897, 0.07056351474291425, 0.06888812442743798, 0.06401361015512283, 0.06719219694239487, 0.0703402375565639, 0.06620161078761169, 0.06645074237166076, 0.06635888905155363, 0.06703155266383082, 0.06943930783106825, 0.06725520694789518, 0.06946768287217846, 0.06819951665699538, 0.06731034199777045, 0.07148495810798379, 0.06573226630164751, 0.07102202684789999, 0.07064119206964849, 0.0652941084686086, 0.06838767450410602, 0.06566874557288663, 0.06751247084496592, 0.06847667628371348, 0.0679255585044858, 0.0635264183606035, 0.06762399077652095, 0.06760025047227604, 0.0689698867835596, 0.0674687364609459, 0.06887245948786543, 0.0691035766408013, 0.06649675555500838, 0.06723876371536897, 0.06694542504656083, 0.06743423103977085, 0.06831396167773214, 0.06745062662094538, 0.06593923272897752, 0.0705240749970109, 0.06455240244835515, 0.06574854378288335, 0.06639650518765368, 0.06834684872462374, 0.06594896904984697, 0.0699099137783577, 0.06645933033945128, 0.06878228443921686, 0.06665694955755774, 0.06856881387144395, 0.06713364418535366, 0.0674268252769597, 0.06882583197209166, 0.06934215428307672, 0.06746014480720894, 0.06714599629412324, 0.06506097213814364, 0.06711451385432843, 0.07049979341752413, 0.0662943545898164, 0.06814575531911463, 0.06846221973962642, 0.06990379084091075, 0.06678485480451764, 0.06597925063775617, 0.06704751471115798, 0.06617035285129069, 0.06820317768803416, 0.06738038847397669, 0.06792240766391101, 0.0681159132128205, 0.0681159132128205, 0.0660851269247284, 0.06987712429686843, 0.06926598123863428, 0.06605705383629486, 0.06543817046035, 0.06788773876585276, 0.06694169522925844, 0.06631910022250613, 0.06618935189104563, 0.06645879362397683, 0.06831513648773661, 0.06708221904986686, 0.0684153126311634, 0.06915891435852001, 0.06716551596201516, 0.06839536736859866, 0.06402809621295324, 0.06978633071538647, 0.06744970117008482, 0.06510865232191088, 0.06811591321282051, 0.06894854995111117, 0.06647864921063858, 0.0662301611746091, 0.06696939748342692, 0.06570580679414714, 0.06619191162840828, 0.06576929172876946, 0.0679491851516621, 0.07009029980134107, 0.06774296241009937, 0.06619676138565027, 0.06887297739405222, 0.06859494902132798, 0.06651995129543188, 0.07116539522812346, 0.06656055788381479, 0.06791295426508188, 0.06635405114407633, 0.06606690776921575, 0.06757848120029261, 0.07215543759520977, 0.0670161190708539, 0.06798382271464812, 0.06595451270141292, 0.06661733875264914, 0.06694049631527613, 0.06578230670658486, 0.06658841869973312, 0.06819127861851398, 0.06813201386753509, 0.06650761699553952], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 1234, "duration": 71620.575597, "accuracy_train": [0.4942347515060241, 0.5766660391566265, 0.6412132906626506, 0.6804640436746988, 0.7172675075301205, 0.731457078313253, 0.7563064759036144, 0.7708960843373494, 0.776402484939759, 0.7900978915662651, 0.7912980045180723, 0.8082407756024096, 0.8122646837349398, 0.8258189006024096, 0.8268542921686747, 0.8350903614457831, 0.841796875, 0.8483857304216867, 0.8532332454819277, 0.8590220256024096, 0.8639401355421686, 0.8654226280120482, 0.8706466490963856, 0.8757059487951807, 0.8791180346385542, 0.8816829819277109, 0.8846244352409639, 0.8881071159638554, 0.8873776355421686, 0.887730609939759, 0.895543109939759, 0.8993552334337349, 0.9007671310240963, 0.9044380647590361, 0.906273531626506, 0.9064147213855421, 0.9106268825301205, 0.9141566265060241, 0.9136859939759037, 0.9186982304216867, 0.9163215361445783, 0.920886671686747, 0.9243693524096386, 0.9270519578313253, 0.9261106927710844, 0.9329348644578314, 0.9268166415662651, 0.9294286521084337, 0.9341585090361446, 0.9333584337349398, 0.9338055346385542, 0.9325583584337349, 0.9368175828313253, 0.9402532003012049, 0.9461361069277109, 0.9425357680722891, 0.9431240587349398, 0.9434535015060241, 0.9438064759036144, 0.9444653614457831, 0.9499482304216867, 0.9454536897590361, 0.9538309487951807, 0.9498070406626506, 0.955054593373494, 0.9582548945783133, 0.9570783132530121, 0.9594079442771084, 0.9588431852409639, 0.9578548569277109, 0.9625376506024096, 0.9615257906626506, 0.9656673569277109, 0.9627964984939759, 0.9635965737951807, 0.9643025225903614, 0.9626082454819277, 0.964820218373494, 0.9668910015060241, 0.9672439759036144, 0.9706325301204819, 0.9693853539156626, 0.9676675451807228, 0.9695265436746988, 0.9699971762048193, 0.9695971385542169, 0.9696677334337349, 0.9717385165662651, 0.9738563629518072, 0.9731268825301205, 0.9749858810240963, 0.9747505647590361, 0.9725150602409639, 0.9753153237951807, 0.9708678463855421, 0.9738798945783133, 0.974609375, 0.9789627259036144, 0.9762330572289156, 0.9787509412650602, 0.9777861445783133, 0.9796451430722891, 0.9789862575301205, 0.9771978539156626, 0.9787274096385542, 0.9806099397590361, 0.9772449171686747, 0.9792686370481928, 0.9816453313253012, 0.9800922439759037, 0.9809864457831325, 0.9796686746987951, 0.9793862951807228, 0.9833160768072289, 0.9819747740963856, 0.9810805722891566, 0.9817159262048193, 0.9827513177710844, 0.9842338102409639, 0.9829631024096386, 0.9826807228915663, 0.9813158885542169, 0.9836690512048193, 0.9846573795180723, 0.9835513930722891, 0.985363328313253, 0.9823042168674698, 0.9842573418674698, 0.9827513177710844, 0.9850574171686747, 0.9827277861445783, 0.9852221385542169, 0.9858810240963856, 0.9867281626506024, 0.9838337725903614, 0.9861869352409639, 0.9859751506024096, 0.9870340737951807, 0.9844926581325302, 0.9876223644578314, 0.9885165662650602, 0.987292921686747, 0.9881635918674698, 0.9879753388554217, 0.9865163780120482, 0.9868458207831325, 0.9885871611445783, 0.9849162274096386, 0.9891519201807228, 0.9880459337349398, 0.9871987951807228, 0.9881400602409639, 0.9874576430722891, 0.9869164156626506, 0.9887048192771084, 0.9868928840361446, 0.9877635542168675, 0.9900461219879518, 0.989222515060241, 0.9895754894578314, 0.9895048945783133, 0.9886342243975904, 0.9893637048192772, 0.9886812876506024, 0.9881635918674698, 0.9879047439759037, 0.9881635918674698, 0.9891989834337349, 0.9884930346385542, 0.9896931475903614, 0.9889166039156626, 0.9894578313253012, 0.9910815135542169, 0.9923286897590361, 0.9893637048192772, 0.9894107680722891, 0.9903049698795181, 0.9901167168674698, 0.9921169051204819, 0.9914109563253012, 0.9898814006024096, 0.9909873870481928, 0.9909638554216867, 0.9907285391566265, 0.9911285768072289, 0.9915756777108434, 0.9926110692771084, 0.9927993222891566, 0.9927757906626506, 0.9918345256024096, 0.9915050828313253, 0.9913403614457831, 0.9915756777108434, 0.9923051581325302, 0.9911285768072289, 0.9920463102409639, 0.9926346009036144, 0.9914344879518072, 0.993199359939759, 0.9926110692771084, 0.9900696536144579, 0.9899049322289156, 0.9909167921686747, 0.9933170180722891, 0.9928934487951807, 0.9920698418674698, 0.9930817018072289, 0.9929405120481928, 0.993175828313253, 0.9919051204819277, 0.9940464984939759, 0.993128765060241, 0.9927757906626506, 0.994164156626506, 0.9901402484939759, 0.9927051957831325, 0.9944700677710844, 0.9915050828313253, 0.9925404743975904, 0.9947995105421686, 0.9937170557228916, 0.9918815888554217, 0.9945171310240963, 0.9922345632530121, 0.9930581701807228, 0.9939288403614458, 0.9937641189759037, 0.9938582454819277, 0.9943994728915663, 0.9932228915662651, 0.9930581701807228, 0.9914109563253012, 0.9942112198795181, 0.9943759412650602, 0.9947289156626506, 0.9947995105421686, 0.9926346009036144, 0.9916933358433735, 0.9943524096385542, 0.9935993975903614, 0.9957172439759037], "end": "2016-01-17 17:02:31.848000", "learning_rate_per_epoch": [0.0010000000474974513, 0.0007071067811921239, 0.0005773502634838223, 0.0005000000237487257, 0.00044721359154209495, 0.0004082482773810625, 0.000377964461222291, 0.00035355339059606194, 0.00033333332976326346, 0.0003162277571391314, 0.0003015113470610231, 0.00028867513174191117, 0.00027735010371543467, 0.0002672612317837775, 0.00025819888105615973, 0.0002500000118743628, 0.00024253562150988728, 0.00023570226039737463, 0.00022941573115531355, 0.00022360679577104747, 0.00021821788686793298, 0.00021320072119124234, 0.00020851440785918385, 0.00020412413869053125, 0.00019999999494757503, 0.0001961161324288696, 0.00019245008297730237, 0.0001889822306111455, 0.00018569533131085336, 0.00018257419287692755, 0.00017960529658012092, 0.00017677669529803097, 0.00017407764971721917, 0.00017149858467746526, 0.00016903085634112358, 0.00016666666488163173, 0.0001643989817239344, 0.00016222141857724637, 0.00016012815467547625, 0.0001581138785695657, 0.00015617375902365893, 0.00015430334315169603, 0.00015249857096932828, 0.00015075567353051156, 0.00014907120203133672, 0.00014744195505045354, 0.00014586499310098588, 0.00014433756587095559, 0.0001428571413271129, 0.00014142136205919087, 0.00014002800162415951, 0.00013867505185771734, 0.00013736056280322373, 0.00013608275912702084, 0.0001348399673588574, 0.00013363061589188874, 0.0001324532349826768, 0.00013130642764735967, 0.00013018891331739724, 0.00012909944052807987, 0.00012803687423001975, 0.00012700012302957475, 0.00012598815374076366, 0.0001250000059371814, 0.00012403473374433815, 0.00012309149315115064, 0.00012216944014653563, 0.00012126781075494364, 0.0001203858555527404, 0.00011952286149607971, 0.0001186781664728187, 0.00011785113019868731, 0.00011704114876920357, 0.00011624764010775834, 0.00011547005124157295, 0.00011470786557765678, 0.00011396057379897684, 0.00011322770296828821, 0.00011250878742430359, 0.00011180339788552374, 0.00011111111234640703, 0.00011043152335332707, 0.00010976425983244553, 0.00010910894343396649, 0.00010846523218788207, 0.00010783276957226917, 0.00010721124999690801, 0.00010660036059562117, 0.00010599978850223124, 0.00010540925723034889, 0.00010482848301762715, 0.00010425720392959192, 0.00010369517258368433, 0.00010314212704543024, 0.00010259783448418602, 0.00010206206934526563, 0.00010153461334994063, 0.00010101525549544021, 0.00010050378477899358, 9.999999747378752e-05, 9.95037189568393e-05, 9.901475277729332e-05, 9.853292431216687e-05, 9.80580662144348e-05, 9.759000386111438e-05, 9.712858445709571e-05, 9.667364793131128e-05, 9.622504148865119e-05, 9.578262688592076e-05, 9.534625860396773e-05, 9.49157983995974e-05, 9.449111530557275e-05, 9.407208563061431e-05, 9.365857840748504e-05, 9.32504772208631e-05, 9.284766565542668e-05, 9.245003457181156e-05, 9.205746027873829e-05, 9.16698481887579e-05, 9.128709643846378e-05, 9.09090886125341e-05, 9.053574467543513e-05, 9.016696276376024e-05, 8.980264829006046e-05, 8.944272121880203e-05, 8.908707968657836e-05, 8.87356509338133e-05, 8.838834764901549e-05, 8.804508979665115e-05, 8.770580461714417e-05, 8.737040479900315e-05, 8.703882485860959e-05, 8.671099931234494e-05, 8.638684084871784e-05, 8.606629853602499e-05, 8.574929233873263e-05, 8.543576404917985e-05, 8.512565545970574e-05, 8.481889381073415e-05, 8.451542817056179e-05, 8.421519305557013e-05, 8.391813753405586e-05, 8.362420339835808e-05, 8.333333244081587e-05, 8.304548100568354e-05, 8.276059088530019e-05, 8.247861114796251e-05, 8.21994908619672e-05, 8.19231936475262e-05, 8.164966129697859e-05, 8.137884287862107e-05, 8.111070928862318e-05, 8.084520959528163e-05, 8.058229286689311e-05, 8.032192999962717e-05, 8.006407733773813e-05, 7.980869122548029e-05, 7.955572800710797e-05, 7.930515857879072e-05, 7.905693928478286e-05, 7.881104102125391e-05, 7.856742013245821e-05, 7.832604751456529e-05, 7.808687951182947e-05, 7.78498942963779e-05, 7.761505548842251e-05, 7.738232670817524e-05, 7.715167157584801e-05, 7.69230755395256e-05, 7.669650221941993e-05, 7.647190795978531e-05, 7.624928548466414e-05, 7.602859113831073e-05, 7.580980309285223e-05, 7.55928922444582e-05, 7.537783676525578e-05, 7.51646002754569e-05, 7.495316822314635e-05, 7.474351150449365e-05, 7.453560101566836e-05, 7.432941492879763e-05, 7.412493141600862e-05, 7.392212864942849e-05, 7.372097752522677e-05, 7.352146349148825e-05, 7.332355744438246e-05, 7.312724483199418e-05, 7.293249655049294e-05, 7.273929804796353e-05, 7.25476274965331e-05, 7.23574630683288e-05, 7.216878293547779e-05, 7.198157254606485e-05, 7.179581734817475e-05, 7.161148823797703e-05, 7.142857066355646e-05, 7.124705007299781e-05, 7.106690463842824e-05, 7.088811980793253e-05, 7.071068102959543e-05, 7.053455919958651e-05, 7.035975431790575e-05, 7.018623728072271e-05, 7.001400081207976e-05, 6.984303036006168e-05, 6.967330409679562e-05, 6.950480747036636e-05, 6.933752592885867e-05, 6.917144492035732e-05, 6.900655716890469e-05, 6.884284084662795e-05, 6.868028140161186e-05, 6.851887155789882e-05, 6.835858948761597e-05, 6.819943519076332e-05, 6.804137956351042e-05, 6.788442260585725e-05, 6.77285497658886e-05, 6.757373921573162e-05, 6.74199836794287e-05, 6.726727588102221e-05, 6.711560854455456e-05, 6.696495256619528e-05, 6.681530794594437e-05, 6.666666740784422e-05, 6.651900912402198e-05, 6.637233309447765e-05, 6.62266174913384e-05, 6.608186231460422e-05, 6.593804573640227e-05, 6.579516775673255e-05, 6.565321382367983e-05, 6.55121766612865e-05, 6.537204171763733e-05, 6.523280899273232e-05, 6.509445665869862e-05, 6.495697743957862e-05, 6.482037133537233e-05, 6.468462379416451e-05, 6.454972026403993e-05, 6.44156607449986e-05], "accuracy_valid": [0.48864850427350426, 0.5718482905982906, 0.625534188034188, 0.6610576923076923, 0.6895032051282052, 0.6984508547008547, 0.7232905982905983, 0.7291666666666666, 0.7314369658119658, 0.7398504273504274, 0.7369123931623932, 0.7512019230769231, 0.7521367521367521, 0.7612179487179487, 0.7564102564102564, 0.7596153846153846, 0.7638888888888888, 0.7617521367521367, 0.7704326923076923, 0.7700320512820513, 0.7723023504273504, 0.7705662393162394, 0.7776442307692307, 0.7772435897435898, 0.7799145299145299, 0.7788461538461539, 0.7781784188034188, 0.782051282051282, 0.7787126068376068, 0.780715811965812, 0.7828525641025641, 0.7821848290598291, 0.7839209401709402, 0.7848557692307693, 0.7861912393162394, 0.7871260683760684, 0.7835202991452992, 0.7845886752136753, 0.7871260683760684, 0.7916666666666666, 0.7833867521367521, 0.7856570512820513, 0.7889957264957265, 0.7900641025641025, 0.7893963675213675, 0.7897970085470085, 0.7900641025641025, 0.7913995726495726, 0.7915331196581197, 0.7841880341880342, 0.7879273504273504, 0.7918002136752137, 0.7916666666666666, 0.7909989316239316, 0.7944711538461539, 0.7930021367521367, 0.7893963675213675, 0.7922008547008547, 0.7899305555555556, 0.7922008547008547, 0.7911324786324786, 0.7915331196581197, 0.797142094017094, 0.7904647435897436, 0.7939369658119658, 0.796073717948718, 0.7956730769230769, 0.796875, 0.7982104700854701, 0.796607905982906, 0.7983440170940171, 0.7950053418803419, 0.7978098290598291, 0.8026175213675214, 0.8020833333333334, 0.7980769230769231, 0.7944711538461539, 0.7959401709401709, 0.7995459401709402, 0.8023504273504274, 0.8022168803418803, 0.8012820512820513, 0.8006143162393162, 0.8027510683760684, 0.8022168803418803, 0.8015491452991453, 0.796607905982906, 0.7996794871794872, 0.8012820512820513, 0.8027510683760684, 0.8018162393162394, 0.8024839743589743, 0.8022168803418803, 0.7972756410256411, 0.7986111111111112, 0.8000801282051282, 0.7988782051282052, 0.8032852564102564, 0.8032852564102564, 0.8056891025641025, 0.8036858974358975, 0.8068910256410257, 0.8038194444444444, 0.8031517094017094, 0.8042200854700855, 0.8030181623931624, 0.8056891025641025, 0.8044871794871795, 0.8007478632478633, 0.8039529914529915, 0.8019497863247863, 0.8067574786324786, 0.8050213675213675, 0.8016826923076923, 0.8046207264957265, 0.8046207264957265, 0.8036858974358975, 0.8026175213675214, 0.8043536324786325, 0.8048878205128205, 0.8056891025641025, 0.8070245726495726, 0.8062232905982906, 0.8060897435897436, 0.8051549145299145, 0.8075587606837606, 0.8018162393162394, 0.8036858974358975, 0.8028846153846154, 0.8019497863247863, 0.8039529914529915, 0.8047542735042735, 0.8076923076923077, 0.8058226495726496, 0.8032852564102564, 0.8066239316239316, 0.8062232905982906, 0.8032852564102564, 0.8058226495726496, 0.8060897435897436, 0.8042200854700855, 0.8062232905982906, 0.8054220085470085, 0.8078258547008547, 0.8054220085470085, 0.8035523504273504, 0.8059561965811965, 0.8026175213675214, 0.8034188034188035, 0.8052884615384616, 0.8023504273504274, 0.8088942307692307, 0.8047542735042735, 0.8034188034188035, 0.8060897435897436, 0.8012820512820513, 0.8078258547008547, 0.8110309829059829, 0.8074252136752137, 0.8080929487179487, 0.8058226495726496, 0.8060897435897436, 0.8067574786324786, 0.8080929487179487, 0.8082264957264957, 0.8087606837606838, 0.8050213675213675, 0.8095619658119658, 0.8036858974358975, 0.8079594017094017, 0.8060897435897436, 0.8068910256410257, 0.8091613247863247, 0.8107638888888888, 0.8058226495726496, 0.8038194444444444, 0.8067574786324786, 0.8059561965811965, 0.8067574786324786, 0.8083600427350427, 0.8094284188034188, 0.8090277777777778, 0.8106303418803419, 0.8106303418803419, 0.8044871794871795, 0.8056891025641025, 0.8099626068376068, 0.8078258547008547, 0.8102297008547008, 0.8076923076923077, 0.8125, 0.8047542735042735, 0.8042200854700855, 0.8051549145299145, 0.8091613247863247, 0.8102297008547008, 0.8088942307692307, 0.8078258547008547, 0.8091613247863247, 0.8107638888888888, 0.8107638888888888, 0.8062232905982906, 0.8051549145299145, 0.8063568376068376, 0.8103632478632479, 0.8091613247863247, 0.8120993589743589, 0.8046207264957265, 0.8102297008547008, 0.8078258547008547, 0.8090277777777778, 0.8120993589743589, 0.8079594017094017, 0.8112980769230769, 0.8110309829059829, 0.811698717948718, 0.8110309829059829, 0.8091613247863247, 0.8075587606837606, 0.8082264957264957, 0.8125, 0.8044871794871795, 0.8099626068376068, 0.8107638888888888, 0.8092948717948718, 0.8111645299145299, 0.8075587606837606, 0.8123664529914529, 0.8088942307692307, 0.8098290598290598, 0.8092948717948718, 0.8084935897435898, 0.8099626068376068, 0.8091613247863247, 0.8108974358974359, 0.8066239316239316, 0.8083600427350427, 0.8076923076923077, 0.8027510683760684, 0.8098290598290598, 0.8088942307692307], "accuracy_test": 0.8037860576923077, "start": "2016-01-16 21:08:51.273000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0, 211.0, 212.0, 213.0, 214.0, 215.0, 216.0, 217.0, 218.0, 219.0, 220.0, 221.0, 222.0, 223.0, 224.0, 225.0, 226.0, 227.0, 228.0, 229.0, 230.0, 231.0, 232.0, 233.0, 234.0, 235.0, 236.0, 237.0, 238.0, 239.0, 240.0], "accuracy_train_last": 0.9957172439759037, "error_valid": [0.5113514957264957, 0.42815170940170943, 0.37446581196581197, 0.3389423076923077, 0.3104967948717948, 0.30154914529914534, 0.2767094017094017, 0.27083333333333337, 0.2685630341880342, 0.2601495726495726, 0.2630876068376068, 0.24879807692307687, 0.24786324786324787, 0.23878205128205132, 0.2435897435897436, 0.24038461538461542, 0.23611111111111116, 0.2382478632478633, 0.2295673076923077, 0.22996794871794868, 0.2276976495726496, 0.22943376068376065, 0.22235576923076927, 0.22275641025641024, 0.22008547008547008, 0.22115384615384615, 0.22182158119658124, 0.21794871794871795, 0.2212873931623932, 0.21928418803418803, 0.2171474358974359, 0.2178151709401709, 0.21607905982905984, 0.21514423076923073, 0.21380876068376065, 0.21287393162393164, 0.2164797008547008, 0.21541132478632474, 0.21287393162393164, 0.20833333333333337, 0.21661324786324787, 0.21434294871794868, 0.21100427350427353, 0.20993589743589747, 0.21060363247863245, 0.21020299145299148, 0.20993589743589747, 0.2086004273504274, 0.20846688034188032, 0.21581196581196582, 0.2120726495726496, 0.2081997863247863, 0.20833333333333337, 0.20900106837606836, 0.20552884615384615, 0.2069978632478633, 0.21060363247863245, 0.20779914529914534, 0.21006944444444442, 0.20779914529914534, 0.2088675213675214, 0.20846688034188032, 0.20285790598290598, 0.2095352564102564, 0.20606303418803418, 0.20392628205128205, 0.20432692307692313, 0.203125, 0.20178952991452992, 0.20339209401709402, 0.20165598290598286, 0.2049946581196581, 0.2021901709401709, 0.1973824786324786, 0.19791666666666663, 0.20192307692307687, 0.20552884615384615, 0.2040598290598291, 0.20045405982905984, 0.1976495726495726, 0.19778311965811968, 0.19871794871794868, 0.19938568376068377, 0.19724893162393164, 0.19778311965811968, 0.19845085470085466, 0.20339209401709402, 0.20032051282051277, 0.19871794871794868, 0.19724893162393164, 0.19818376068376065, 0.19751602564102566, 0.19778311965811968, 0.20272435897435892, 0.20138888888888884, 0.1999198717948718, 0.20112179487179482, 0.1967147435897436, 0.1967147435897436, 0.19431089743589747, 0.19631410256410253, 0.19310897435897434, 0.19618055555555558, 0.19684829059829057, 0.1957799145299145, 0.19698183760683763, 0.19431089743589747, 0.19551282051282048, 0.1992521367521367, 0.19604700854700852, 0.1980502136752137, 0.1932425213675214, 0.19497863247863245, 0.1983173076923077, 0.19537927350427353, 0.19537927350427353, 0.19631410256410253, 0.1973824786324786, 0.19564636752136755, 0.19511217948717952, 0.19431089743589747, 0.1929754273504274, 0.19377670940170943, 0.1939102564102564, 0.1948450854700855, 0.19244123931623935, 0.19818376068376065, 0.19631410256410253, 0.19711538461538458, 0.1980502136752137, 0.19604700854700852, 0.19524572649572647, 0.1923076923076923, 0.1941773504273504, 0.1967147435897436, 0.19337606837606836, 0.19377670940170943, 0.1967147435897436, 0.1941773504273504, 0.1939102564102564, 0.1957799145299145, 0.19377670940170943, 0.19457799145299148, 0.19217414529914534, 0.19457799145299148, 0.1964476495726496, 0.19404380341880345, 0.1973824786324786, 0.19658119658119655, 0.19471153846153844, 0.1976495726495726, 0.19110576923076927, 0.19524572649572647, 0.19658119658119655, 0.1939102564102564, 0.19871794871794868, 0.19217414529914534, 0.18896901709401714, 0.1925747863247863, 0.19190705128205132, 0.1941773504273504, 0.1939102564102564, 0.1932425213675214, 0.19190705128205132, 0.19177350427350426, 0.19123931623931623, 0.19497863247863245, 0.19043803418803418, 0.19631410256410253, 0.19204059829059827, 0.1939102564102564, 0.19310897435897434, 0.19083867521367526, 0.18923611111111116, 0.1941773504273504, 0.19618055555555558, 0.1932425213675214, 0.19404380341880345, 0.1932425213675214, 0.1916399572649573, 0.19057158119658124, 0.1909722222222222, 0.1893696581196581, 0.1893696581196581, 0.19551282051282048, 0.19431089743589747, 0.1900373931623932, 0.19217414529914534, 0.1897702991452992, 0.1923076923076923, 0.1875, 0.19524572649572647, 0.1957799145299145, 0.1948450854700855, 0.19083867521367526, 0.1897702991452992, 0.19110576923076927, 0.19217414529914534, 0.19083867521367526, 0.18923611111111116, 0.18923611111111116, 0.19377670940170943, 0.1948450854700855, 0.19364316239316237, 0.18963675213675213, 0.19083867521367526, 0.18790064102564108, 0.19537927350427353, 0.1897702991452992, 0.19217414529914534, 0.1909722222222222, 0.18790064102564108, 0.19204059829059827, 0.18870192307692313, 0.18896901709401714, 0.18830128205128205, 0.18896901709401714, 0.19083867521367526, 0.19244123931623935, 0.19177350427350426, 0.1875, 0.19551282051282048, 0.1900373931623932, 0.18923611111111116, 0.1907051282051282, 0.18883547008547008, 0.19244123931623935, 0.18763354700854706, 0.19110576923076927, 0.19017094017094016, 0.1907051282051282, 0.19150641025641024, 0.1900373931623932, 0.19083867521367526, 0.1891025641025641, 0.19337606837606836, 0.1916399572649573, 0.1923076923076923, 0.19724893162393164, 0.19017094017094016, 0.19110576923076927], "accuracy_train_std": [0.08728239994925728, 0.08698408364900734, 0.08464652494450423, 0.08341326317849843, 0.08019660825427345, 0.07845572733046134, 0.07565716178258611, 0.07590122390574791, 0.0743538480948081, 0.07380538505947516, 0.07303497768034198, 0.07054914667636242, 0.07051575322089111, 0.06885286360422613, 0.06899708845860117, 0.06533044066228541, 0.065688462684996, 0.06377982025850691, 0.06273900124838264, 0.06295620500540895, 0.06232125488807973, 0.06055341375028688, 0.06222423728753468, 0.05932074279378009, 0.05863884438202755, 0.057686341331362564, 0.05661691243592707, 0.05689123427947379, 0.05632146224147392, 0.05573707531719007, 0.055143508657230736, 0.05300328975161543, 0.052387631989993376, 0.05224999887103267, 0.05307707939431696, 0.050024136905249504, 0.05142303836285959, 0.04995076489071745, 0.050854017234546287, 0.04867796077762492, 0.049752158366226595, 0.04779678264844625, 0.047849582203510474, 0.04668285651663187, 0.045776556361540266, 0.0452275412778748, 0.04664533478220318, 0.04695657502114395, 0.04327626699794041, 0.04525204586287486, 0.0455578548817987, 0.044433069947798824, 0.044247111206338556, 0.04337704783106145, 0.03986909666779568, 0.04228530486519631, 0.04201026748802279, 0.042313862551409516, 0.04135689885693404, 0.041357862870981366, 0.039542200971072215, 0.04162326353084255, 0.03807411351210055, 0.039827130136780894, 0.03818610959628796, 0.03575218491285677, 0.03695109802016509, 0.03560899618006262, 0.036296410681128724, 0.037098376027734595, 0.03448859917777745, 0.03514871245617199, 0.033935762951427716, 0.0343741491265777, 0.03381427588604948, 0.033849236624586686, 0.034639505380287164, 0.03367387263920784, 0.03256708247093486, 0.03177407482807552, 0.030813746183147778, 0.031745542183420364, 0.033349188455350585, 0.03211092850642565, 0.031142567463456505, 0.0315663591416991, 0.03203841967073625, 0.029815220820977352, 0.029625775735014917, 0.029130252774858728, 0.027868112942083325, 0.028129645694978514, 0.029764224082568232, 0.027499753841460833, 0.030678852111806425, 0.02880359518830972, 0.028522651387179205, 0.0263217415364031, 0.02825767226805444, 0.026721309441260294, 0.02666589130483935, 0.025170866935781286, 0.026186534893219433, 0.026596682667268078, 0.026495232113833825, 0.02518759172597099, 0.026911612119317727, 0.025676520001596058, 0.023762608828873964, 0.02530896171332876, 0.025124196489940385, 0.02656802960682933, 0.02581384033499431, 0.022402392690024798, 0.023859862496514558, 0.024121558587257775, 0.023986185244341787, 0.023584736282977085, 0.022817039726873562, 0.023504690909316894, 0.023858794908954397, 0.024122063616658796, 0.02312674078443893, 0.02249111450522831, 0.02328131886134906, 0.021569505263121472, 0.02407468255032144, 0.023168974647845862, 0.023678090410914798, 0.022515217150719257, 0.023269197439239584, 0.02177905108952915, 0.02153955063532451, 0.020978388939516812, 0.022971676965776174, 0.02148177167219988, 0.021634972802619605, 0.020372232990116486, 0.0223614225803472, 0.020359141137166716, 0.019754460578756815, 0.01993505179975531, 0.019825182358487483, 0.018989526874287135, 0.021279344219697183, 0.020395921040969135, 0.01882422754374295, 0.022057143882072073, 0.019116821674546217, 0.02021442049541585, 0.020920237994228402, 0.019341565570845366, 0.01976195745090241, 0.020278746926562348, 0.018992107372201188, 0.020245392162312626, 0.020008027493353047, 0.018196369406684305, 0.0187884970616897, 0.0184961428468813, 0.01803296571696201, 0.019238784911744522, 0.018474949573033247, 0.01968188110672313, 0.020229084229647542, 0.01930987549159022, 0.019450722068371094, 0.01871613620954345, 0.01860944047795136, 0.01785521764557879, 0.018942579319693405, 0.018608027027200998, 0.016646675756688187, 0.016224676791483025, 0.018234566125530993, 0.018181223578893905, 0.01749411448360156, 0.017847586905448784, 0.016008408299395493, 0.016554612138201405, 0.018206545799132334, 0.017459626432010922, 0.016955839170417753, 0.017639053577756053, 0.016449785150072886, 0.01670520093400637, 0.015237959207665498, 0.015375716737874648, 0.015147770601449234, 0.01572657524866922, 0.01597100696251659, 0.01649551912108789, 0.01603130806877997, 0.015869722634978177, 0.01653895053492849, 0.01590424630064884, 0.015176841048393171, 0.016139608248234568, 0.014605976505034646, 0.015759866561534375, 0.01882477173577713, 0.01803706462945715, 0.01718930150547028, 0.014381655358025472, 0.015081918580727431, 0.016572397523860256, 0.014825970511974061, 0.015152613458322122, 0.015163353594623582, 0.01629068517063839, 0.013580480660986617, 0.014698510194532614, 0.015001425739717213, 0.013331450434002419, 0.017674350740831686, 0.015803655358891906, 0.012988583891883341, 0.016735453879695168, 0.015419644800837117, 0.012667776368363344, 0.01422881394304566, 0.015982652313069886, 0.013232706313996062, 0.015904803362191404, 0.01508549789956119, 0.014243225255303334, 0.013777219710766623, 0.013766122180502896, 0.01337714498707737, 0.01454125050286382, 0.014386871573934827, 0.016330999655965732, 0.013461653560546828, 0.01300477419932641, 0.012783197380545903, 0.012783348991250994, 0.01546482667694504, 0.01616098589200755, 0.013357343860472225, 0.014407813635788348, 0.011853121097572942], "accuracy_test_std": 0.0675712149696232, "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1.0, 1.2], "translation_range": [-3, 3], "rotation_range": [-90, 90], "shear_range": [1, 1.1], "patience_check_each": 1, "learning_rate": 0.001, "patience_threshold": 1, "do_flip": true, "nb_data_augmentation": 1, "optimization": "adam", "batch_size": 32, "learning_rate_decay_method": "sqrt", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0, "valid_ratio": 0.15, "momentum": 0.9, "learning_rate_decay": 0.98}, "accuracy_valid_max": 0.8125, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    args = parser.parse_args()\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = 1234\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.98, interval=[0.8, 1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-10, -6], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128, 256, 512],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1.0, 1.2)),\n        rotation_range=make_constant_param((-90, 90)),\n        shear_range=make_constant_param((1, 1.1)),\n        translation_range=make_constant_param((-3, 3)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    model_class = residual\n\n    instantiate = instantiate_default\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        rng=rng,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n    )\n    batch_optimizer.learning_rate = learning_rate\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8088942307692307, "loss_train": [1.7786142826080322, 1.4857583045959473, 1.2937769889831543, 1.1794019937515259, 1.096811056137085, 1.0385041236877441, 0.9904228448867798, 0.9532075524330139, 0.9196030497550964, 0.893940806388855, 0.8677552342414856, 0.8489963412284851, 0.8281699419021606, 0.8132979869842529, 0.7947240471839905, 0.7839638590812683, 0.7708674073219299, 0.7541284561157227, 0.7435594797134399, 0.733149528503418, 0.7243813872337341, 0.7142372727394104, 0.7063810229301453, 0.6972538828849792, 0.688054621219635, 0.6799580454826355, 0.6749764680862427, 0.6683932542800903, 0.6566908359527588, 0.654796838760376, 0.6441231369972229, 0.6405017375946045, 0.6307357549667358, 0.6260923147201538, 0.6236526966094971, 0.6173564195632935, 0.6115822792053223, 0.605974555015564, 0.6027835011482239, 0.6006983518600464, 0.595291256904602, 0.5921463966369629, 0.5836024284362793, 0.5858473181724548, 0.5802157521247864, 0.5723628997802734, 0.5721883773803711, 0.5649133920669556, 0.5611863136291504, 0.561048686504364, 0.5617759227752686, 0.5539435744285583, 0.5494697690010071, 0.5506978631019592, 0.5483857989311218, 0.542410671710968, 0.5411816835403442, 0.5366262197494507, 0.5368645191192627, 0.5321986675262451, 0.5291666984558105, 0.5278261303901672, 0.524242103099823, 0.5237736105918884, 0.5228599905967712, 0.5201043486595154, 0.5157912969589233, 0.5148710012435913, 0.5144222974777222, 0.5079481601715088, 0.5115101337432861, 0.5055056810379028, 0.5110374689102173, 0.5033078789710999, 0.5023247003555298, 0.5005049109458923, 0.5007139444351196, 0.49671655893325806, 0.4930342733860016, 0.49057725071907043, 0.4901975989341736, 0.4904020130634308, 0.48775631189346313, 0.4838380515575409, 0.4841257929801941, 0.4805324971675873, 0.47930747270584106, 0.4810667037963867, 0.4811381697654724, 0.4786781966686249, 0.4752470552921295, 0.47411614656448364, 0.47172459959983826, 0.47435659170150757, 0.47351881861686707, 0.4710652828216553, 0.46988171339035034, 0.4685427248477936, 0.4697745144367218, 0.4655579924583435, 0.46820974349975586, 0.45831605792045593, 0.46216556429862976, 0.4629988372325897, 0.4609845280647278, 0.4596371650695801, 0.45613762736320496, 0.4586658477783203, 0.45826613903045654, 0.4542514681816101, 0.45685291290283203, 0.453387975692749, 0.45640885829925537, 0.4524114429950714, 0.44994601607322693, 0.451378732919693, 0.44855549931526184, 0.4473906457424164, 0.4496071934700012, 0.44775035977363586, 0.44691944122314453, 0.44217953085899353, 0.44266021251678467, 0.4432293176651001, 0.44048187136650085, 0.4421786367893219, 0.4370819926261902, 0.438093364238739, 0.43875569105148315, 0.4386768937110901, 0.44017425179481506, 0.4347827732563019, 0.4374297261238098, 0.4340548813343048, 0.4394087493419647, 0.433226078748703, 0.43115565180778503, 0.4303177297115326, 0.4332205653190613, 0.43538999557495117, 0.43019115924835205, 0.4323166608810425, 0.42994844913482666, 0.4263398051261902, 0.4277309775352478, 0.4295886158943176, 0.4257242977619171, 0.42894116044044495, 0.4286015033721924, 0.42557293176651, 0.4259859025478363, 0.42641952633857727, 0.42464321851730347, 0.42226335406303406, 0.4241974651813507, 0.4212009906768799, 0.41932621598243713, 0.4217478632926941, 0.4180169403553009, 0.41996973752975464, 0.4158271849155426, 0.4195566773414612, 0.41697701811790466, 0.41511350870132446, 0.4184165596961975, 0.41661062836647034, 0.4160067141056061, 0.4161619544029236, 0.41349801421165466, 0.4106517732143402, 0.41079241037368774, 0.41410383582115173, 0.41182923316955566, 0.4108339250087738, 0.4131673574447632, 0.40710213780403137, 0.4082338511943817, 0.41194844245910645, 0.40674883127212524, 0.4083314538002014, 0.40939903259277344, 0.40789422392845154, 0.405180424451828, 0.4083888828754425, 0.4056820869445801, 0.4042854309082031, 0.4057612121105194, 0.4080129563808441, 0.40654411911964417, 0.4040849208831787, 0.40644335746765137, 0.40488335490226746, 0.40095996856689453, 0.403182715177536, 0.40465840697288513, 0.40300509333610535, 0.399722695350647, 0.4011369049549103, 0.40155014395713806, 0.4008103907108307, 0.3995380699634552, 0.40023690462112427, 0.3981090784072876, 0.39614880084991455, 0.39716553688049316, 0.3989703357219696, 0.3958507478237152, 0.3940734267234802, 0.3957875967025757, 0.39826664328575134, 0.3969075381755829, 0.3915765881538391, 0.39051762223243713, 0.3939078450202942, 0.39192500710487366, 0.39456817507743835, 0.3922012448310852, 0.3942931592464447, 0.39167889952659607, 0.39110156893730164, 0.3914065659046173, 0.3919868469238281, 0.3926137685775757, 0.3887106478214264, 0.3903242349624634, 0.39042842388153076, 0.3900861144065857, 0.389877051115036, 0.3904462456703186, 0.3862840533256531, 0.38415318727493286, 0.3839610517024994, 0.38982152938842773, 0.3860754370689392, 0.3862249553203583, 0.38607606291770935, 0.3858371675014496, 0.3856174051761627, 0.38739582896232605, 0.3848758935928345, 0.38591936230659485], "accuracy_train_first": 0.4942347515060241, "model": "residual", "loss_std": [0.2180943489074707, 0.1818145513534546, 0.17624948918819427, 0.1709369421005249, 0.17287464439868927, 0.1679558902978897, 0.16738222539424896, 0.16759920120239258, 0.163529634475708, 0.1657872200012207, 0.16524408757686615, 0.15969350934028625, 0.15946920216083527, 0.1581999957561493, 0.15580622851848602, 0.15648795664310455, 0.15410752594470978, 0.1518852263689041, 0.15518106520175934, 0.15404602885246277, 0.15236879885196686, 0.15143927931785583, 0.1492599993944168, 0.1483239233493805, 0.14447465538978577, 0.1476699858903885, 0.1466853767633438, 0.142576664686203, 0.14381806552410126, 0.14609535038471222, 0.14456796646118164, 0.14303193986415863, 0.142680823802948, 0.14043423533439636, 0.13793064653873444, 0.13899247348308563, 0.13783273100852966, 0.13787871599197388, 0.13425780832767487, 0.1389024555683136, 0.13710664212703705, 0.1322115659713745, 0.13450750708580017, 0.13436491787433624, 0.13382765650749207, 0.13114364445209503, 0.13168151676654816, 0.1346077024936676, 0.13081417977809906, 0.13101093471050262, 0.1302950084209442, 0.13153767585754395, 0.1300584375858307, 0.1278495192527771, 0.12681014835834503, 0.1307946890592575, 0.12718729674816132, 0.12306012213230133, 0.12860107421875, 0.12401606887578964, 0.12524764239788055, 0.12681545317173004, 0.12341073155403137, 0.12324235588312149, 0.12420594692230225, 0.12229659408330917, 0.12358638644218445, 0.12286096811294556, 0.12470540404319763, 0.1203308030962944, 0.12348060309886932, 0.12309633195400238, 0.12311119586229324, 0.12023831903934479, 0.1225586086511612, 0.12190956622362137, 0.11884522438049316, 0.11983420699834824, 0.11781804263591766, 0.12087427079677582, 0.11964612454175949, 0.11840879917144775, 0.11819494515657425, 0.121151864528656, 0.12040966004133224, 0.11997916549444199, 0.11483020335435867, 0.11561597883701324, 0.11660365760326385, 0.11840158700942993, 0.11751631647348404, 0.11771310120820999, 0.11339271068572998, 0.11972402781248093, 0.11602120101451874, 0.11818411201238632, 0.11673752218484879, 0.11627260595560074, 0.1177598163485527, 0.116289883852005, 0.11722689121961594, 0.11441095173358917, 0.11285149306058884, 0.11592096090316772, 0.11304976046085358, 0.11576429009437561, 0.11559625715017319, 0.11349180340766907, 0.11417680233716965, 0.1125371903181076, 0.11577090620994568, 0.11312731355428696, 0.11048543453216553, 0.11472463607788086, 0.11173949390649796, 0.11586134135723114, 0.11234030872583389, 0.11171746253967285, 0.11364401131868362, 0.11090606451034546, 0.11221633106470108, 0.11318957805633545, 0.11359647661447525, 0.11244233697652817, 0.1151806190609932, 0.11018657684326172, 0.11277998238801956, 0.11421581357717514, 0.11312432587146759, 0.11315692216157913, 0.10822419077157974, 0.11359357088804245, 0.11205364763736725, 0.10923886299133301, 0.11096518486738205, 0.11209183931350708, 0.11285395920276642, 0.11063126474618912, 0.1138523519039154, 0.11008746922016144, 0.10899503529071808, 0.11306022852659225, 0.1114836111664772, 0.10841505974531174, 0.11281327903270721, 0.1121329814195633, 0.11012034118175507, 0.1106325015425682, 0.10961507260799408, 0.11216698586940765, 0.10893317312002182, 0.11102835834026337, 0.11013758182525635, 0.10879086703062057, 0.10785970091819763, 0.1102929562330246, 0.11149667948484421, 0.10799172520637512, 0.10937429219484329, 0.11170797049999237, 0.10983984172344208, 0.11196725070476532, 0.11106979101896286, 0.10598084330558777, 0.10992413759231567, 0.1101047471165657, 0.11363007873296738, 0.11059463769197464, 0.10867872834205627, 0.1072627380490303, 0.10848956555128098, 0.10851383954286575, 0.1069185733795166, 0.10522078722715378, 0.10893984884023666, 0.10536012798547745, 0.10802418738603592, 0.10717030614614487, 0.10678606480360031, 0.10692443698644638, 0.10552478581666946, 0.1086161881685257, 0.10852651298046112, 0.10819395631551743, 0.11052416265010834, 0.10636509954929352, 0.10979746282100677, 0.10933893918991089, 0.10903709381818771, 0.10955488681793213, 0.10732974857091904, 0.10463734716176987, 0.10522925108671188, 0.10933990776538849, 0.10655463486909866, 0.10678543895483017, 0.10737703740596771, 0.1052485853433609, 0.108627088367939, 0.10539814829826355, 0.10972192883491516, 0.10890349000692368, 0.10455064475536346, 0.10425238311290741, 0.1088605746626854, 0.10856451839208603, 0.1090988889336586, 0.10567792505025864, 0.10592248290777206, 0.10692396759986877, 0.10522541403770447, 0.10648324340581894, 0.10354769974946976, 0.10704595595598221, 0.10367613285779953, 0.10670842975378036, 0.10325095802545547, 0.10721141844987869, 0.1039828434586525, 0.10421198606491089, 0.10603450238704681, 0.1040622815489769, 0.10646423697471619, 0.10433460026979446, 0.10571510344743729, 0.10501040518283844, 0.10536948591470718, 0.10467897355556488, 0.10537191480398178, 0.10478179901838303, 0.10612284392118454, 0.10546962171792984, 0.10722756385803223, 0.10431303083896637, 0.10900609940290451, 0.10522684454917908, 0.10533393919467926, 0.10765255987644196, 0.10779114812612534, 0.10370209068059921, 0.11133801192045212]}, "state": "available", "life": [{"dt": "Sun May 15 22:04:58 2016", "state": "available"}], "summary": "ffdfbaf0cbf2286cdccdd015981268eb"}