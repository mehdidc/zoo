{"content": {"hp_model": {"f0": 16, "f1": 32, "f2": 32, "f3": 32, "nonlin": "leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.015420332077570561, 0.01811182626520861, 0.014002975021279456, 0.012911673327209832, 0.011440366525087788, 0.010915236551836483, 0.010131352479817448, 0.007808190595693821, 0.008694915594833935, 0.010693453976199354, 0.00959267411140567, 0.009635259918284313, 0.010505397110971952, 0.01031915987881191, 0.01165369344805694, 0.01349449204829486, 0.012253887978205431, 0.01116284710229694, 0.013287144598331769, 0.01306990145812155, 0.011560457510783823, 0.01243147320651916, 0.014704090556804788, 0.014413759861330928, 0.015178949019242424, 0.01728667737554203, 0.01622711019064019, 0.017044594821218693, 0.01897910649480254, 0.018848928308968806, 0.019498586232341835, 0.018863036012380173, 0.019983066546517603, 0.017304095592896012, 0.018053549698726048, 0.01866831245658289, 0.019092185113795276, 0.01859380075801778, 0.01878830917577965, 0.017872653681944455, 0.018842794666659612, 0.01772237942341984, 0.01707902016626297, 0.016613247068947262, 0.015166209728778496, 0.01562792678202596, 0.016380647878539458, 0.015987861109142174, 0.01670690063354953, 0.017011078797112673, 0.016718495862182402, 0.017260255991969252, 0.01577512895480395, 0.016533686671008123, 0.016884579235524153, 0.01692864826499972, 0.017763946215806854, 0.019123134310417688, 0.01905059573687092, 0.020800441245150153, 0.01938149257012032, 0.020280894623599305, 0.020074990031112083, 0.018989976060577115, 0.019220839685301208, 0.020314739614586175, 0.020619987818417767, 0.01842116812083763, 0.0200303277855827, 0.018579394320089693, 0.018774463075904795, 0.01882437071741209, 0.018687726265866077, 0.018696447342037462, 0.019661820744673052, 0.019055769583433517, 0.018392340013288867, 0.019662866216245932, 0.019522582251766642, 0.020020685799800212, 0.020679617554364885, 0.018278251241737077, 0.01963232838190014, 0.0200474667485032, 0.018538730610338745, 0.019417307547906097, 0.019840714155640374, 0.020382159649010596, 0.019529321791032415, 0.020059688315450557, 0.019644816092031017, 0.020260919546259352, 0.01931589967345747, 0.02004588153509723, 0.02020590107686881, 0.0196178423801544, 0.020491639045730623, 0.021229656603382767, 0.019679709364748135, 0.021262413587801104, 0.019958895227774245, 0.02105077604976582, 0.019880718657248168, 0.01970070889041706, 0.020445305387211853, 0.020189617010086282, 0.019989890728787374, 0.020577904517718966, 0.019465607244668677, 0.01973726696773731, 0.020776036292709903, 0.020025953358200246, 0.019894562050082366, 0.020371688301292348, 0.019465962384006434, 0.019775342935159496, 0.019245015056952995, 0.021139908795704584, 0.02066707752920271, 0.019999632840670176, 0.020062734228782287, 0.020972609927212667, 0.020409487792643604, 0.020051927095104984, 0.021202006449122127, 0.02011551597353846, 0.020635496278063725, 0.02046198815291374, 0.020262421430549897, 0.019279616959550255, 0.02009831783888545, 0.020774375013523128, 0.020191863261380914, 0.019966810421400357, 0.02091094924767541, 0.02036945524340453, 0.020050937724744346, 0.020418170370044942, 0.019341024959669943, 0.020446715814335957, 0.01952969986125235, 0.01997323166641012, 0.02037140968679803, 0.020357747167966726, 0.020630247161968943, 0.020020278862627392, 0.0208255072706501, 0.02007880049523134, 0.019978334230175147, 0.0194402515967159, 0.021325803836657452, 0.02023715812569997, 0.02029211727013647, 0.019497161356675015, 0.020327334669837563, 0.020127364955201763, 0.020345289004795816, 0.01837378076347984, 0.020168590189076286, 0.02010063573853669, 0.019675644782654513, 0.020575628399333735, 0.020556071946401886, 0.01970999622619893, 0.019934610616785794, 0.019594420044522335, 0.020929630720694762, 0.019546013649873316, 0.020409272277251877, 0.019115337506553243, 0.020298207776923028, 0.020072721756971206, 0.01927993003140593, 0.020757131396338863, 0.019060046419925883, 0.019726131356409504, 0.02011551597353846, 0.02017540318810776, 0.019919877951123516, 0.02114405276156354, 0.01922494889288412, 0.019460313667041318, 0.019892568748572762, 0.020473787959947712, 0.019344705100264488, 0.020701172709718607, 0.020579255773279483, 0.018791141022754736, 0.021142885230416738, 0.019754256860164538, 0.019250364299282376, 0.020356635961648672, 0.019778380581174083, 0.020054897232234338, 0.01963642588872955, 0.020453598555421393, 0.018582883138336687, 0.02006896489110041, 0.02078695733305177, 0.020130847835616764, 0.02075928831558147, 0.019967529730393643, 0.020984419831192154, 0.01929847034749679, 0.020258473761896576, 0.02025866429555865, 0.020514212020802024, 0.019225578049532843, 0.01972781046191931, 0.01980584723218694, 0.020311364463604265, 0.02053209248728027, 0.019119677325703927, 0.01932044410434804, 0.019958279000396283, 0.020163747155001247, 0.018655577600181165, 0.019521475195554527, 0.021794940036870863, 0.019463800611878013, 0.018932421070313853, 0.01963220030037317, 0.020103854322312313, 0.019113615177308835, 0.021217680780587778, 0.018824914158438674, 0.019562690215987007, 0.020197766226625304, 0.019780445533461484, 0.021317114136356164, 0.02140680469958252, 0.019272664231354052, 0.020431781421549628, 0.020374727489477957, 0.020831152180021406, 0.019144801117022835], "moving_avg_accuracy_train": [0.02379117496077888, 0.04928282317045034, 0.07458690072443705, 0.09956920955034143, 0.1239617299828119, 0.14719136087318888, 0.1692744097768722, 0.19023002354724697, 0.20991070927507116, 0.22825358595276893, 0.24540369974121296, 0.2612967843686513, 0.2760981423785839, 0.28974706637439107, 0.30245667230039786, 0.3143346626123477, 0.32536432561929307, 0.33563975859815376, 0.34519220672435724, 0.3540264670700555, 0.36232131525853717, 0.37001218201387576, 0.3771477676377004, 0.38375587870154204, 0.38994259688874294, 0.3957222678477092, 0.40107746758102614, 0.4061366016195736, 0.4108083327459145, 0.41514767729293617, 0.4192461107852649, 0.4230580059129031, 0.42670945646940717, 0.43016789507980313, 0.43345011754579843, 0.43652266430566117, 0.43946710099432723, 0.4421938599736597, 0.4448083783229161, 0.44713815730033285, 0.4494326320776362, 0.4515929904783997, 0.45361636809861067, 0.4555304139091815, 0.4573367965446568, 0.45899970724871825, 0.4606079700740494, 0.4620693575097046, 0.46353802997440413, 0.46493191480572893, 0.46625147927176913, 0.4675205035483574, 0.4687067671758491, 0.46983725555608624, 0.47097316954112917, 0.4719886248276955, 0.47293040032250067, 0.4738337657904352, 0.4747305000687191, 0.4756120017298981, 0.4764262435154262, 0.47718696290811574, 0.4779227636353459, 0.47864772725889143, 0.47935134779389194, 0.48009614132061196, 0.48080830817323145, 0.48150273676320804, 0.4821603106263391, 0.4827939437329097, 0.4833666107752702, 0.4839610301240998, 0.4845377881189803, 0.48501501763580135, 0.48545843904497865, 0.4858994430894472, 0.48630099702708796, 0.48670886249833634, 0.4871852234165075, 0.48762328488691836, 0.48800358931743104, 0.48837372904178794, 0.4887603332163282, 0.48910133757580465, 0.4894500581290861, 0.48981734900083973, 0.4901502719830463, 0.4904219648324991, 0.49068741473629235, 0.4910053747092301, 0.49123802421343626, 0.4914288436255644, 0.49166811855841214, 0.49189265844675695, 0.4921226461319816, 0.49229243266773137, 0.49251034471657285, 0.49274130674385425, 0.49287712900413105, 0.49305756985625576, 0.49318970363982545, 0.493385281858115, 0.49350793197841275, 0.4936717234116623, 0.49379588421349163, 0.493951806762519, 0.49410147370070046, 0.49425706423553073, 0.49440879355856315, 0.4945220263635596, 0.4946216467880655, 0.49469502912845426, 0.49477506017648, 0.4948749178077799, 0.49490433580690224, 0.4950075779656454, 0.4950771723227814, 0.49516545597874617, 0.49530064274290553, 0.49536425420804836, 0.4954401057171531, 0.4955222869193857, 0.49561027299188976, 0.49566617292022935, 0.49575604643431564, 0.4958113199112698, 0.49584951239411845, 0.49589311412628284, 0.49592072994118314, 0.4959548847698315, 0.4959972498596627, 0.4960400287381298, 0.49611569606088385, 0.49613965487280026, 0.4962285750213637, 0.496304061003908, 0.4963511441465497, 0.49638879657967067, 0.4964529107040034, 0.4964965904254081, 0.4965662012068337, 0.49659393762915516, 0.49660727466519683, 0.49664721583216737, 0.49661573356696465, 0.496643167050892, 0.4966190651102454, 0.49660431276127326, 0.4966700907067222, 0.4966920884766738, 0.49669099617916335, 0.49675511727807053, 0.49686852169205936, 0.4968823021075249, 0.49690167992787243, 0.4968587381947751, 0.4968897730016357, 0.49690371738613437, 0.49691168913220163, 0.4969769563750815, 0.4970496477865305, 0.49710344431278697, 0.49714721088879876, 0.49718195050959035, 0.497215577365931, 0.4972620815294856, 0.49728071983740824, 0.49728586857049095, 0.49730445332312256, 0.4973257938492913, 0.49733806092523336, 0.49731415196380085, 0.49733688382353003, 0.49733409100919107, 0.49734785351795263, 0.4973509031317812, 0.497362948379465, 0.4973970405904756, 0.4974416744732424, 0.497484170116542, 0.4975108265002828, 0.4974952536670688, 0.4975114290028813, 0.49751675830751196, 0.4975541067650129, 0.49753424195414464, 0.49750935212911596, 0.4974590495008758, 0.49743241437475466, 0.49741309305886466, 0.49740267932099225, 0.49743973783546, 0.49742429842229974, 0.4974243538433126, 0.4974709066984147, 0.49746862644062567, 0.4974479730181393, 0.49742709583791084, 0.4974175709221246, 0.497420696339602, 0.49740254682722723, 0.4974210894982328, 0.497453981846167, 0.4974790067593262, 0.49748060284188383, 0.49749362901141453, 0.49749837711756356, 0.4975259739988305, 0.49754608879671414, 0.4975316760802948, 0.49758842305098444, 0.4975372248258048, 0.49756555118504786, 0.4975352052881193, 0.49751486942731216, 0.4975710079633093, 0.4975796799671352, 0.49759206297056013, 0.4975358504558039, 0.4975828793937045, 0.4976066402961576, 0.49758849757860346, 0.49755360399114734, 0.49753611460647523, 0.4975319638554992, 0.49753055332843027, 0.4974897923731251, 0.4975112362335885, 0.4975049230222821, 0.49749927718092507, 0.4975221337582368, 0.4975287177361414, 0.4975160421257793, 0.4974999837788345, 0.49748553126658407, 0.4974515616174543], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 678805161, "moving_var_accuracy_train": [0.005094180054129528, 0.010433179204727392, 0.015152528351978221, 0.019254317305236142, 0.022683841052148934, 0.02527199870866245, 0.02713374827773843, 0.028372613186402683, 0.02902131638421887, 0.029147334869116037, 0.02887973900881364, 0.02826507635870652, 0.027410290513279627, 0.026345899598141553, 0.025165116383476883, 0.023918384629786175, 0.02262142736122857, 0.021309545331233228, 0.019999834184944176, 0.018702248169149952, 0.017451263910464584, 0.01623848240245348, 0.015072883401962533, 0.013958599248254874, 0.012907218660779969, 0.01191713816224723, 0.010983527823675968, 0.010115528576288274, 0.009300401364110849, 0.008539830427579668, 0.00783702179864108, 0.007184094518953975, 0.006585682887557923, 0.0060347617773990276, 0.005528242458505685, 0.0050603831049790046, 0.00463237216120317, 0.004236051875865187, 0.0038739680440660544, 0.0035354220704119674, 0.0032292613939039027, 0.002948339590291258, 0.002690352144207868, 0.002454289072071755, 0.0022382273288963014, 0.002039292044093811, 0.0018586414235225108, 0.0016919981603040783, 0.0015422113335507696, 0.0014054764345026692, 0.0012806000444727274, 0.001167033843556589, 0.0010629954517461177, 0.0009681979423721672, 0.0008829908533676951, 0.0008039721129820646, 0.0007315573714273984, 0.0006657462568025673, 0.0006064088224149548, 0.0005527613467814119, 0.0005034521192709705, 0.00045831515329359943, 0.0004173562623559711, 0.0003803507864195522, 0.00034677144449306633, 0.00031708675662075683, 0.00028994271559241086, 0.0002652885236323617, 0.00024265130173838312, 0.00022199958978822557, 0.00020275115868205706, 0.0001856560520742181, 0.0001700842949287238, 0.00015512559754137873, 0.00014138264070229226, 0.00012899473773720157, 0.00011754647404699444, 0.00010728901482602502, 9.86023908626704e-05, 9.046923244313041e-05, 8.272399233762544e-05, 7.568462384378615e-05, 6.946132654935535e-05, 6.356174965305728e-05, 5.830002890628003e-05, 5.368414927591389e-05, 4.931327375705416e-05, 4.504629942134303e-05, 4.117584234202347e-05, 3.796814500733638e-05, 3.4658462632869194e-05, 3.152032480198662e-05, 2.8883564763191712e-05, 2.6448971739993737e-05, 2.428012358418924e-05, 2.2111558435267547e-05, 2.0327773541013413e-05, 1.8775087309325628e-05, 1.7063607755873387e-05, 1.5650277090325542e-05, 1.4242383412136987e-05, 1.316240262614698e-05, 1.1981549831613794e-05, 1.1024843550905834e-05, 1.006110233821339e-05, 9.273798676048756e-06, 8.548020539905424e-06, 7.911094216673853e-06, 7.327180882217254e-06, 6.709857807141699e-06, 6.128190087236309e-06, 5.563835789441015e-06, 5.06509692832981e-06, 4.648331154256359e-06, 4.191286806881981e-06, 3.868088616271286e-06, 3.524870125550828e-06, 3.2425291481902653e-06, 3.0827553842060927e-06, 2.810897612264023e-06, 2.5815889139387974e-06, 2.3842137725485364e-06, 2.215466335885835e-06, 2.0220429201926366e-06, 1.892533864981409e-06, 1.7307768937747151e-06, 1.5708271961125504e-06, 1.4308544759309066e-06, 1.2946327274312968e-06, 1.1756684255681765e-06, 1.0742547905389863e-06, 9.832996034712122e-07, 9.364995367189374e-07, 8.480158050630583e-07, 8.343753599416749e-07, 8.022210259936501e-07, 7.419503242834408e-07, 6.805146433344555e-07, 6.494587674515198e-07, 6.016841532643416e-07, 5.851266859540518e-07, 5.335377994673795e-07, 4.817849082940318e-07, 4.47964088835343e-07, 4.1208787715245175e-07, 3.776524538007516e-07, 3.4511534030706786e-07, 3.125624924781175e-07, 3.202468861976133e-07, 2.925773145234546e-07, 2.63330321095772e-07, 2.740009269118175e-07, 3.6234588422994283e-07, 3.278203944605725e-07, 2.984178543073122e-07, 2.8517200084919247e-07, 2.6532323389615665e-07, 2.4054092323796085e-07, 2.170587695324125e-07, 2.3369120951743608e-07, 2.578784602517682e-07, 2.5813721036200044e-07, 2.495631079079845e-07, 2.3546836839187771e-07, 2.220984207589181e-07, 2.1935231373420116e-07, 2.0054356106074393e-07, 1.8072779002588412e-07, 1.6576354829669563e-07, 1.5328595598145131e-07, 1.3931169075281632e-07, 1.3052526760857433e-07, 1.221233778684412e-07, 1.0998123838898502e-07, 1.0068777437680219e-07, 9.070269823965329e-08, 8.293822034157026e-08, 8.51049079717874e-08, 9.452406859213963e-08, 1.0132457902795154e-07, 9.758718627237836e-08, 9.001108585392765e-08, 8.336475066635805e-08, 7.528388899033944e-08, 8.03096655905947e-08, 7.583019542900398e-08, 7.382270639574345e-08, 8.92136254269504e-08, 8.667713237567201e-08, 8.136923836759849e-08, 7.42083279591217e-08, 7.914749661423383e-08, 7.33781262614062e-08, 6.604034127866364e-08, 7.894082201421723e-08, 7.109353599305626e-08, 6.782325713734296e-08, 6.496364131223815e-08, 5.9283793367636996e-08, 5.3443328140549536e-08, 5.106363852146261e-08, 4.905175050150989e-08, 5.388373442497553e-08, 5.413157749012311e-08, 4.8741347056886773e-08, 4.5394342184980983e-08, 4.105780857450775e-08, 4.38063184179912e-08, 4.306713242129568e-08, 4.062995673041068e-08, 6.554892919938755e-08, 8.258536063337015e-08, 8.154826822174055e-08, 8.168130254315371e-08, 7.723509740174111e-08, 9.787540467664167e-08, 8.876469706219501e-08, 8.126827632036581e-08, 1.0158007002534683e-07, 1.1132755202333932e-07, 1.052760211894778e-07, 9.77108428727634e-08, 9.88978205955189e-08, 9.176094572185729e-08, 8.273990975265873e-08, 7.44838250569025e-08, 8.198854184773826e-08, 7.792824002713869e-08, 7.049412575742176e-08, 6.373159290333916e-08, 6.206024175065765e-08, 5.624435646103051e-08, 5.206596069737277e-08, 4.918019918704864e-08, 4.614205526146808e-08, 5.1913283293337876e-08], "duration": 159143.533982, "accuracy_train": [0.23791174960778885, 0.2787076570574935, 0.3023235987103175, 0.32440998898348095, 0.34349441387504615, 0.35625803888658175, 0.3680218499100222, 0.3788305474806202, 0.38703688082548915, 0.3933394760520487, 0.39975472383720934, 0.40433454601559615, 0.4093103644679771, 0.41258738233665565, 0.4168431256344592, 0.4212365754198967, 0.42463129268180144, 0.4281186554078996, 0.43116423986018826, 0.43353481018133994, 0.43697494895487266, 0.4392299828119232, 0.44136803825212256, 0.4432288782761167, 0.4456230605735511, 0.4477393064784053, 0.44927426518087854, 0.45166880796650055, 0.4528539128829826, 0.45420177821613145, 0.4561320122162237, 0.45736506206164634, 0.4595725114779439, 0.46129384257336653, 0.4629901197397564, 0.464175585144426, 0.4659670311923219, 0.46673469078765223, 0.4683390434662237, 0.46810616809708383, 0.47008290507336653, 0.4710362160852713, 0.4718267666805094, 0.4727568262043189, 0.473594240263935, 0.4739659035852713, 0.4750823355020303, 0.47522184443060167, 0.47675608215669985, 0.47747687828765223, 0.47812755946613145, 0.47894172203765223, 0.4793831398232743, 0.48001165097822074, 0.4811963954065154, 0.48112772240679214, 0.48140637977574746, 0.48196405500184575, 0.4828011085732743, 0.4835455166805094, 0.48375441958517906, 0.4840334374423219, 0.48454497018041714, 0.4851723998708011, 0.4856839326088963, 0.48679928306109266, 0.48721780984680696, 0.4877525940729974, 0.4880784753945183, 0.48849664169204504, 0.4885206141565154, 0.48931080426356593, 0.48972861007290514, 0.48931008328719083, 0.48944923172757476, 0.4898684794896641, 0.4899149824658546, 0.4903796517395718, 0.491472471680048, 0.4915658381206165, 0.49142632919204504, 0.49170498656100037, 0.49223977078719083, 0.49217037681109266, 0.49258854310861944, 0.4931229668466224, 0.49314657882290514, 0.49286720047757476, 0.49307646387043186, 0.49386701446567, 0.49333186975129195, 0.4931462183347176, 0.4938215929540421, 0.49391351744186046, 0.4941925352990033, 0.4938205114894795, 0.49447155315614616, 0.4948199649893872, 0.4940995293466224, 0.49468153752537836, 0.49437890769195275, 0.49514548582272055, 0.49461178306109266, 0.4951458463109081, 0.4949133314299557, 0.49535510970376523, 0.4954484761443337, 0.4956573790490033, 0.4957743574658546, 0.49554112160852715, 0.49551823060861944, 0.49535547019195275, 0.4954953396087117, 0.4957736364894795, 0.4951690977990033, 0.4959367573943337, 0.49570352153700625, 0.4959600088824289, 0.49651732362033957, 0.4959367573943337, 0.4961227692990956, 0.4962619177394795, 0.496402147644426, 0.49616927227528607, 0.49656490806109266, 0.4963087812038575, 0.4961932447397564, 0.4962855297157623, 0.49616927227528607, 0.49626227822766705, 0.4963785356681432, 0.4964250386443337, 0.49679670196567, 0.496355284180048, 0.4970288563584348, 0.49698343484680696, 0.49677489243032485, 0.49672766847775934, 0.4970299378229974, 0.4968897079180509, 0.4971926982396641, 0.496843565430048, 0.4967273079895718, 0.4970066863349022, 0.4963323931801403, 0.49689006840623845, 0.496402147644426, 0.4964715416205242, 0.4972620922157623, 0.49689006840623845, 0.4966811655015689, 0.4973322071682355, 0.49788916141795864, 0.49700632584671467, 0.49707608031100037, 0.49647226259689925, 0.4971690862633813, 0.4970292168466224, 0.49698343484680696, 0.49756436156100037, 0.4977038704895718, 0.4975876130490956, 0.49754111007290514, 0.49749460709671467, 0.4975182190729974, 0.49768061900147653, 0.4974484646087117, 0.4973322071682355, 0.49747171609680696, 0.4975178585848099, 0.4974484646087117, 0.4970989713109081, 0.49754147056109266, 0.4973089556801403, 0.49747171609680696, 0.49737834965623845, 0.49747135560861944, 0.4977038704895718, 0.4978433794181432, 0.49786663090623845, 0.4977507339539498, 0.4973550981681432, 0.4976570070251938, 0.4975647220491879, 0.49789024288252126, 0.49735545865633074, 0.4972853437038575, 0.49700632584671467, 0.4971926982396641, 0.4972392012158546, 0.4973089556801403, 0.49777326446567, 0.4972853437038575, 0.4974248526324289, 0.4978898823943337, 0.4974481041205242, 0.4972620922157623, 0.4972392012158546, 0.497331846680048, 0.49744882509689925, 0.4972392012158546, 0.4975879735372831, 0.49775001297757476, 0.49770423097775934, 0.4974949675849022, 0.49761086453719083, 0.49754111007290514, 0.49777434593023256, 0.49772712197766705, 0.49740196163252126, 0.49809914578719083, 0.4970764407991879, 0.4978204884182355, 0.4972620922157623, 0.497331846680048, 0.4980762547872831, 0.4976577280015689, 0.49770351000138424, 0.4970299378229974, 0.4980061398348099, 0.4978204884182355, 0.4974252131206165, 0.4972395617040421, 0.497378710144426, 0.49749460709671467, 0.4975178585848099, 0.49712294377537836, 0.49770423097775934, 0.4974481041205242, 0.4974484646087117, 0.4977278429540421, 0.4975879735372831, 0.49740196163252126, 0.49735545865633074, 0.49735545865633074, 0.49714583477528607], "end": "2016-01-26 01:31:43.083000", "learning_rate_per_epoch": [0.00017954810755327344, 0.000173307053046301, 0.00016728293849155307, 0.0001614682114450261, 0.00015585561050102115, 0.0001504381070844829, 0.00014520890545099974, 0.0001401614717906341, 0.00013528949057217687, 0.00013058684999123216, 0.00012604767107404768, 0.0001216662785736844, 0.00011743717914214358, 0.00011335508315823972, 0.00010941488289972767, 0.00010561164526734501, 0.00010194060450885445, 9.839716949500144e-05, 9.497690189164132e-05, 9.167552343569696e-05, 8.84889013832435e-05, 8.541304123355076e-05, 8.244410128099844e-05, 7.95783635112457e-05, 7.681223360123113e-05, 7.414225547108799e-05, 7.156508218031377e-05, 6.907749047968537e-05, 6.667637353530154e-05, 6.435871910071e-05, 6.212162406882271e-05, 5.99622871959582e-05, 5.78780091018416e-05, 5.586618135566823e-05, 5.392428283812478e-05, 5.204988701734692e-05, 5.024064375902526e-05, 4.84942902403418e-05, 4.6808640036033466e-05, 4.518157948041335e-05, 4.3611074943328276e-05, 4.209516191622242e-05, 4.063194137415849e-05, 3.92195834137965e-05, 3.7856319977436215e-05, 3.65404412150383e-05, 3.527030276018195e-05, 3.404431481612846e-05, 3.286094215582125e-05, 3.1718700483907014e-05, 3.061616371269338e-05, 2.955195122922305e-05, 2.8524731533252634e-05, 2.753321678028442e-05, 2.6576168238534592e-05, 2.565238537499681e-05, 2.4760713131399825e-05, 2.390003646723926e-05, 2.3069276721798815e-05, 2.226739343313966e-05, 2.1493382519111037e-05, 2.0746276277350262e-05, 2.002513974730391e-05, 1.9329070710227825e-05, 1.8657196051208302e-05, 1.8008675397140905e-05, 1.7382697478751652e-05, 1.6778478311607614e-05, 1.6195261196116917e-05, 1.5632316717528738e-05, 1.5088940017449204e-05, 1.456445170333609e-05, 1.4058194210520014e-05, 1.3569534530688543e-05, 1.3097860573907383e-05, 1.2642581168620382e-05, 1.2203127880638931e-05, 1.1778949556173757e-05, 1.1369515959813725e-05, 1.0974314136547036e-05, 1.0592849321255926e-05, 1.0224644029221963e-05, 9.869237146631349e-06, 9.526184840069618e-06, 9.195056918542832e-06, 8.875438652466983e-06, 8.56692986417329e-06, 8.2691449279082e-06, 7.98171095084399e-06, 7.704268682573456e-06, 7.436469786625821e-06, 7.177979568950832e-06, 6.928474249434657e-06, 6.6876418713945895e-06, 6.455180937336991e-06, 6.230799954209942e-06, 6.014218797645299e-06, 5.80516552872723e-06, 5.603379122476326e-06, 5.408606739365496e-06, 5.220604634814663e-06, 5.0391372496960685e-06, 4.86397766508162e-06, 4.694906692748191e-06, 4.531712875177618e-06, 4.374191576062003e-06, 4.222145435051061e-06, 4.075384367752122e-06, 3.933724656235427e-06, 3.7969891764078056e-06, 3.6650064885179745e-06, 3.5376115192775615e-06, 3.414644879740081e-06, 3.295952410553582e-06, 3.181385864081676e-06, 3.070801540161483e-06, 2.964061195598333e-06, 2.8610309072973905e-06, 2.7615819817583542e-06, 2.665590045580757e-06, 2.5729345907166135e-06, 2.4834998839651234e-06, 2.3971738301042933e-06, 2.3138484266382875e-06, 2.233419536423753e-06, 2.155786205548793e-06, 2.080851572827669e-06, 2.008521505558747e-06, 1.938705736392876e-06, 1.871316726465011e-06, 1.806270120141562e-06, 1.7434845176467206e-06, 1.682881361375621e-06, 1.6243847085206653e-06, 1.5679214584451984e-06, 1.5134207842493197e-06, 1.460814587517234e-06, 1.4100369298830628e-06, 1.3610243740913575e-06, 1.3137154155629105e-06, 1.2680509371421067e-06, 1.2239737543495721e-06, 1.181428615382174e-06, 1.1403624284866964e-06, 1.1007236935256515e-06, 1.0624627293509548e-06, 1.0255317874907632e-06, 9.898844837152865e-07, 9.554762527841376e-07, 9.22264064229239e-07, 8.902063655114034e-07, 8.592629683334962e-07, 8.293951623272733e-07, 8.005655445231241e-07, 7.727380193500721e-07, 7.458777986357745e-07, 7.199512310762657e-07, 6.949258590793761e-07, 6.707703619213135e-07, 6.474544989032438e-07, 6.249491093512916e-07, 6.032259989297017e-07, 5.822579964842589e-07, 5.620188403554494e-07, 5.424831783784612e-07, 5.236265678831842e-07, 5.054254188507912e-07, 4.87856937070319e-07, 4.708991241386684e-07, 4.5453077746060444e-07, 4.387313765619183e-07, 4.2348116835455585e-07, 4.0876105344977987e-07, 3.9455258615817e-07, 3.808380029113323e-07, 3.6760013699677074e-07, 3.5482241855788743e-07, 3.4248887459398247e-07, 3.305840152734163e-07, 3.190929760421568e-07, 3.080013755152322e-07, 2.9729531547673105e-07, 2.8696138087980216e-07, 2.7698666826836416e-07, 2.6735867209026765e-07, 2.5806534154071414e-07, 2.490950521405466e-07, 2.404365488928306e-07, 2.3207901733712788e-07, 2.2401199828436802e-07, 2.1622538781684852e-07, 2.0870943728823477e-07, 2.0145473911270528e-07, 1.9445221255409706e-07, 1.8769308951505081e-07, 1.81168914537011e-07, 1.748715163785164e-07, 1.6879302222605475e-07, 1.6292581506149872e-07, 1.5726254787296057e-07, 1.517961294439374e-07, 1.4651972435331118e-07, 1.4142672455363936e-07, 1.3651076358200953e-07, 1.317656739274753e-07, 1.271855296636204e-07, 1.2276458960513992e-07, 1.1849731862412227e-07, 1.1437837343919455e-07, 1.1040260261552248e-07, 1.0656503235395576e-07, 1.0286085228017328e-07, 9.928542965553788e-08, 9.583428806081429e-08, 9.250310739616907e-08, 8.928771677574332e-08, 8.618409452765263e-08, 8.31883539831324e-08, 8.029674347653781e-08, 7.750564634534385e-08, 7.481156671929057e-08, 7.221112952038311e-08, 6.970108756831905e-08, 6.727829315877898e-08, 6.493971227428119e-08, 6.268241747875436e-08, 6.050358791753752e-08, 5.840049510652534e-08, 5.6370506484881844e-08, 5.4411078309613004e-08, 5.251975920828045e-08, 5.0694183073574095e-08, 4.89320619578848e-08, 4.72311931787317e-08, 4.558944510790752e-08, 4.4004764276905917e-08], "accuracy_valid": [0.24218602927334337, 0.28302075489457834, 0.30589937876506024, 0.3242819912462349, 0.34167774614081325, 0.35945000705948793, 0.37135259789156627, 0.3795930793486446, 0.38876894295933734, 0.3972226797816265, 0.4040792074548193, 0.4107018895896084, 0.4162156438253012, 0.41989834337349397, 0.42270596056099397, 0.42283832596009036, 0.4265313205948795, 0.4293492328689759, 0.43339814335466864, 0.4345982563064759, 0.43742646366716864, 0.4392986987010542, 0.4408959078501506, 0.4438255953501506, 0.4450462984751506, 0.447009718561747, 0.4491863940135542, 0.4504070971385542, 0.4528587984751506, 0.4537132906626506, 0.4560326266001506, 0.457263624811747, 0.4593491152108434, 0.459949171686747, 0.461414015436747, 0.46364216632153615, 0.46437458819653615, 0.46571736163403615, 0.46633800828313254, 0.46486286944653615, 0.46706013507153615, 0.46790433217243976, 0.46668362904743976, 0.47024425828313254, 0.46975597703313254, 0.4724621140813253, 0.4745578995670181, 0.4739475480045181, 0.4757786026920181, 0.47737581184111444, 0.47823030402861444, 0.47872888036521083, 0.4786171051393072, 0.4799598785768072, 0.480712890625, 0.4814453125, 0.4821880294615964, 0.48343961784638556, 0.4852809676204819, 0.4849353468561747, 0.48590161426957834, 0.4865222609186747, 0.4862781202936747, 0.48773266895707834, 0.48773266895707834, 0.4890857374811747, 0.4887195265436747, 0.48834302051957834, 0.48934017319277107, 0.4884547957454819, 0.48956372364457834, 0.49029614551957834, 0.49017407520707834, 0.49139477833207834, 0.49129329819277107, 0.49153743881777107, 0.4928699171686747, 0.49312435288027107, 0.4929919874811747, 0.4931140577936747, 0.49374499952936746, 0.49296110222138556, 0.4940906202936747, 0.4944568312311747, 0.49430387565888556, 0.49346997364457834, 0.4954333937311747, 0.49556575913027107, 0.49495540756777107, 0.49517895801957834, 0.4948230421686747, 0.49618640577936746, 0.4961658156061747, 0.49568782944277107, 0.49629818100527107, 0.4968982374811747, 0.4968982374811747, 0.49728503859186746, 0.49715267319277107, 0.49789539015436746, 0.4970203077936747, 0.49765124952936746, 0.49727474350527107, 0.49862781202936746, 0.49800716538027107, 0.4972644484186747, 0.4976306593561747, 0.49764095444277107, 0.49800716538027107, 0.49776302475527107, 0.49887195265436746, 0.49764095444277107, 0.49764095444277107, 0.49788509506777107, 0.4971423781061747, 0.4979968702936747, 0.4984851515436747, 0.49888224774096385, 0.49911609327936746, 0.49788509506777107, 0.49800716538027107, 0.49973673992846385, 0.49960437452936746, 0.49887195265436746, 0.49887195265436746, 0.49948230421686746, 0.49961466961596385, 0.49960437452936746, 0.49899402296686746, 0.4988513624811747, 0.49873958725527107, 0.49998088055346385, 0.49936023390436746, 0.49948230421686746, 0.49961466961596385, 0.49997058546686746, 0.49997058546686746, 0.5001029508659638, 0.49886165756777107, 0.49911609327936746, 0.5000926557793675, 0.5004691618034638, 0.5001029508659638, 0.49997058546686746, 0.49960437452936746, 0.49960437452936746, 0.49949259930346385, 0.49972644484186746, 0.5002147260918675, 0.49960437452936746, 0.5006015272025602, 0.5000926557793675, 0.5002147260918675, 0.49922786850527107, 0.5002147260918675, 0.49948230421686746, 0.5003470914909638, 0.49859692676957834, 0.49960437452936746, 0.49934993881777107, 0.5002044310052711, 0.49984851515436746, 0.5009574430534638, 0.49922786850527107, 0.5010795133659638, 0.5005809370293675, 0.5002353162650602, 0.49922786850527107, 0.5002147260918675, 0.49971614975527107, 0.49960437452936746, 0.5002250211784638, 0.5002147260918675, 0.5003470914909638, 0.49959407944277107, 0.5002147260918675, 0.49948230421686746, 0.49997058546686746, 0.5003367964043675, 0.5003470914909638, 0.5007030073418675, 0.5004588667168675, 0.5000926557793675, 0.5003470914909638, 0.49971614975527107, 0.5017001600150602, 0.5009574430534638, 0.4988513624811747, 0.5002250211784638, 0.5000926557793675, 0.49983822006777107, 0.5002147260918675, 0.5000926557793675, 0.5002250211784638, 0.49997058546686746, 0.49984851515436746, 0.5000823606927711, 0.5009574430534638, 0.5007235975150602, 0.5000926557793675, 0.5006015272025602, 0.49997058546686746, 0.5002250211784638, 0.5002147260918675, 0.5005809370293675, 0.5004588667168675, 0.5012015836784638, 0.49983822006777107, 0.5005809370293675, 0.5003367964043675, 0.5005809370293675, 0.5016898649284638, 0.49997058546686746, 0.49934993881777107, 0.49959407944277107, 0.5003367964043675, 0.5000926557793675, 0.5004588667168675, 0.5006118222891567, 0.49972644484186746, 0.5000823606927711, 0.49971614975527107, 0.5005912321159638, 0.5003367964043675, 0.5004691618034638, 0.5005809370293675, 0.49998088055346385, 0.49936023390436746, 0.5002147260918675, 0.49986910532756024, 0.5008456678275602, 0.49996029038027107, 0.5002250211784638, 0.5016898649284638, 0.5010898084525602, 0.49983822006777107], "accuracy_test": 0.4891800860969388, "start": "2016-01-24 05:19:19.549000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0, 211.0, 212.0, 213.0, 214.0, 215.0, 216.0, 217.0, 218.0, 219.0, 220.0, 221.0, 222.0, 223.0, 224.0, 225.0, 226.0, 227.0, 228.0, 229.0, 230.0, 231.0, 232.0, 233.0, 234.0, 235.0], "accuracy_train_last": 0.49714583477528607, "batch_size_eval": 1024, "accuracy_train_std": [0.012412761203037859, 0.012978571588805539, 0.01455462469779373, 0.013817895955899448, 0.016152763077535874, 0.014503277450223416, 0.014008211616529787, 0.01513482539251701, 0.014043182928481248, 0.01342032388689244, 0.01360532782532995, 0.014256958600152738, 0.014297600298006958, 0.014494433002130427, 0.013642757376064582, 0.01406901940365866, 0.014514960492416483, 0.014230218428183212, 0.014291725854850022, 0.01529639076220696, 0.01530744670115404, 0.015185791078924746, 0.014653123744449654, 0.014897133808044732, 0.014347740972446458, 0.014786307176256315, 0.014280762748622658, 0.014017727284092058, 0.014443639510891702, 0.014498860370260602, 0.014591590596638046, 0.015400900452947914, 0.01573520824293165, 0.015582953748929805, 0.01518932435793554, 0.015599156570654284, 0.015215678383298546, 0.015100091897282926, 0.01564254258109026, 0.015622732371072822, 0.015615079355340735, 0.01598659515619609, 0.015361702388814227, 0.01587337485893043, 0.01621579291350983, 0.016291927921787335, 0.01645869367694242, 0.016674791557225885, 0.01631371294413842, 0.016731375199778558, 0.016697914121189972, 0.016437365632083884, 0.016703004848058742, 0.017140286492076787, 0.017055705870689143, 0.0175862740682223, 0.017586559717322196, 0.017361861746412757, 0.017544584351375157, 0.017451643172186573, 0.017453189749483015, 0.017634274521562725, 0.017807171318254128, 0.01763173012971331, 0.017843304576059277, 0.017612806777557832, 0.017371958828261185, 0.01709958291202959, 0.017265237628699574, 0.016963773800841724, 0.01691875030929245, 0.01712534890739991, 0.01696139909488799, 0.017174021139263182, 0.017164816933875587, 0.01725579912381855, 0.017230386242956742, 0.01705374410791925, 0.01712444568336871, 0.01743177030804572, 0.017311191439340363, 0.01720663295803627, 0.01682979646977269, 0.016975763964260825, 0.01677712079436881, 0.01686702508307155, 0.017162860182804673, 0.01662239047958746, 0.01654059900490748, 0.016588970684721996, 0.0162453375011368, 0.01675156973933496, 0.017003496266952245, 0.01640385533335034, 0.016610294979365166, 0.016364883815301302, 0.016381310639128666, 0.016798153818381546, 0.016291485127129173, 0.016741455996313968, 0.01682212293515353, 0.016936712534108854, 0.016454242084034256, 0.0164320485593859, 0.016708989664651345, 0.01711203995065525, 0.016929206695605825, 0.016917629149229705, 0.016759308132276066, 0.016904159237552556, 0.016588278504130333, 0.016684898425170923, 0.01645765831427874, 0.016784817847631785, 0.016664569020277582, 0.01695617944095918, 0.016710570146628592, 0.016822609235444225, 0.016673709008666854, 0.016539958010509007, 0.016500819274509394, 0.01714768564101508, 0.017140633824060458, 0.016846575116560732, 0.01733388877172803, 0.016714110543873166, 0.01692246467956574, 0.016560856882416753, 0.016506175434861353, 0.016804064688794813, 0.01646877029790663, 0.016573730770799613, 0.016864949737085067, 0.017020385067809774, 0.0170330281069872, 0.017075817997126628, 0.017522895457590063, 0.016558805292641532, 0.016627294668966234, 0.01660023536150686, 0.01702832833847737, 0.0164168548184581, 0.016860434432032636, 0.01730771655101065, 0.01684039313063949, 0.01687709247933165, 0.017156523093061645, 0.01689970785036253, 0.016642580905171858, 0.016708070436772807, 0.01689310595309955, 0.01732158966244135, 0.016742297316181428, 0.01700058644380306, 0.016906282250631312, 0.01697881109115854, 0.016856226541843232, 0.016902083987958758, 0.016725153240252973, 0.016630208519071744, 0.016686824501899757, 0.01726130293636149, 0.0168768699818817, 0.0170174417394195, 0.016793233466334653, 0.01681173188262821, 0.017212259980171465, 0.017236171830177494, 0.017029370853777277, 0.016872793115621468, 0.017399883953323134, 0.01642311234593378, 0.017032250359945895, 0.01745316932490377, 0.01715557380651111, 0.01690011814335381, 0.01700532533028551, 0.016849322819612766, 0.016944303539750377, 0.01727961641049692, 0.01686235101553351, 0.01727327824576408, 0.01708375532540456, 0.01679733675322884, 0.016750741786690188, 0.016414598721922687, 0.017352322122515817, 0.016819309616348962, 0.017002973793191507, 0.01678077523059335, 0.01663791829286112, 0.017122519953424998, 0.01686389859952613, 0.017094253086800695, 0.017410100065372247, 0.016850934392936445, 0.017163792340487097, 0.016620339224659223, 0.01710522372296155, 0.01719372760082989, 0.016895399600719906, 0.017216544312635165, 0.01738338051596261, 0.017528341992917598, 0.017123880943405476, 0.017044146991412105, 0.017473123122576883, 0.017308556221760297, 0.017288108448683663, 0.017246165604631755, 0.017218608684960994, 0.017133120910594938, 0.017382593145080234, 0.01698960435959284, 0.0167063161407541, 0.017414710423461363, 0.017301799984845153, 0.017289529150619622, 0.016988002111660985, 0.01688977594415864, 0.01751403048354883, 0.01732648747190732, 0.017196400459780502, 0.0171097883714088, 0.01694657579076606, 0.016969410362059022, 0.01683220605083053, 0.017151204359085313, 0.01691146290939725, 0.01694368481064369, 0.017337909607883402, 0.016839165695746002, 0.017054188165166694, 0.016956221576482737, 0.01678529091763545, 0.01699385962039008], "accuracy_test_std": 0.02156484671714366, "error_valid": [0.7578139707266567, 0.7169792451054217, 0.6941006212349398, 0.6757180087537651, 0.6583222538591867, 0.6405499929405121, 0.6286474021084337, 0.6204069206513554, 0.6112310570406627, 0.6027773202183735, 0.5959207925451807, 0.5892981104103916, 0.5837843561746988, 0.580101656626506, 0.577294039439006, 0.5771616740399097, 0.5734686794051205, 0.5706507671310241, 0.5666018566453314, 0.5654017436935241, 0.5625735363328314, 0.5607013012989458, 0.5591040921498494, 0.5561744046498494, 0.5549537015248494, 0.552990281438253, 0.5508136059864458, 0.5495929028614458, 0.5471412015248494, 0.5462867093373494, 0.5439673733998494, 0.542736375188253, 0.5406508847891567, 0.540050828313253, 0.538585984563253, 0.5363578336784638, 0.5356254118034638, 0.5342826383659638, 0.5336619917168675, 0.5351371305534638, 0.5329398649284638, 0.5320956678275602, 0.5333163709525602, 0.5297557417168675, 0.5302440229668675, 0.5275378859186747, 0.5254421004329819, 0.5260524519954819, 0.5242213973079819, 0.5226241881588856, 0.5217696959713856, 0.5212711196347892, 0.5213828948606928, 0.5200401214231928, 0.519287109375, 0.5185546875, 0.5178119705384037, 0.5165603821536144, 0.5147190323795181, 0.5150646531438253, 0.5140983857304217, 0.5134777390813253, 0.5137218797063253, 0.5122673310429217, 0.5122673310429217, 0.5109142625188253, 0.5112804734563253, 0.5116569794804217, 0.5106598268072289, 0.5115452042545181, 0.5104362763554217, 0.5097038544804217, 0.5098259247929217, 0.5086052216679217, 0.5087067018072289, 0.5084625611822289, 0.5071300828313253, 0.5068756471197289, 0.5070080125188253, 0.5068859422063253, 0.5062550004706325, 0.5070388977786144, 0.5059093797063253, 0.5055431687688253, 0.5056961243411144, 0.5065300263554217, 0.5045666062688253, 0.5044342408697289, 0.5050445924322289, 0.5048210419804217, 0.5051769578313253, 0.5038135942206325, 0.5038341843938253, 0.5043121705572289, 0.5037018189947289, 0.5031017625188253, 0.5031017625188253, 0.5027149614081325, 0.5028473268072289, 0.5021046098456325, 0.5029796922063253, 0.5023487504706325, 0.5027252564947289, 0.5013721879706325, 0.5019928346197289, 0.5027355515813253, 0.5023693406438253, 0.5023590455572289, 0.5019928346197289, 0.5022369752447289, 0.5011280473456325, 0.5023590455572289, 0.5023590455572289, 0.5021149049322289, 0.5028576218938253, 0.5020031297063253, 0.5015148484563253, 0.5011177522590362, 0.5008839067206325, 0.5021149049322289, 0.5019928346197289, 0.5002632600715362, 0.5003956254706325, 0.5011280473456325, 0.5011280473456325, 0.5005176957831325, 0.5003853303840362, 0.5003956254706325, 0.5010059770331325, 0.5011486375188253, 0.5012604127447289, 0.5000191194465362, 0.5006397660956325, 0.5005176957831325, 0.5003853303840362, 0.5000294145331325, 0.5000294145331325, 0.4998970491340362, 0.5011383424322289, 0.5008839067206325, 0.49990734422063254, 0.4995308381965362, 0.4998970491340362, 0.5000294145331325, 0.5003956254706325, 0.5003956254706325, 0.5005074006965362, 0.5002735551581325, 0.49978527390813254, 0.5003956254706325, 0.49939847279743976, 0.49990734422063254, 0.49978527390813254, 0.5007721314947289, 0.49978527390813254, 0.5005176957831325, 0.4996529085090362, 0.5014030732304217, 0.5003956254706325, 0.5006500611822289, 0.4997955689947289, 0.5001514848456325, 0.4990425569465362, 0.5007721314947289, 0.4989204866340362, 0.49941906297063254, 0.49976468373493976, 0.5007721314947289, 0.49978527390813254, 0.5002838502447289, 0.5003956254706325, 0.4997749788215362, 0.49978527390813254, 0.4996529085090362, 0.5004059205572289, 0.49978527390813254, 0.5005176957831325, 0.5000294145331325, 0.49966320359563254, 0.4996529085090362, 0.49929699265813254, 0.49954113328313254, 0.49990734422063254, 0.4996529085090362, 0.5002838502447289, 0.49829983998493976, 0.4990425569465362, 0.5011486375188253, 0.4997749788215362, 0.49990734422063254, 0.5001617799322289, 0.49978527390813254, 0.49990734422063254, 0.4997749788215362, 0.5000294145331325, 0.5001514848456325, 0.4999176393072289, 0.4990425569465362, 0.49927640248493976, 0.49990734422063254, 0.49939847279743976, 0.5000294145331325, 0.4997749788215362, 0.49978527390813254, 0.49941906297063254, 0.49954113328313254, 0.4987984163215362, 0.5001617799322289, 0.49941906297063254, 0.49966320359563254, 0.49941906297063254, 0.4983101350715362, 0.5000294145331325, 0.5006500611822289, 0.5004059205572289, 0.49966320359563254, 0.49990734422063254, 0.49954113328313254, 0.4993881777108433, 0.5002735551581325, 0.4999176393072289, 0.5002838502447289, 0.4994087678840362, 0.49966320359563254, 0.4995308381965362, 0.49941906297063254, 0.5000191194465362, 0.5006397660956325, 0.49978527390813254, 0.5001308946724398, 0.49915433217243976, 0.5000397096197289, 0.4997749788215362, 0.4983101350715362, 0.49891019154743976, 0.5001617799322289], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-3, 3], "momentum": 0.8156541523830558, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.00018601391008394514, "patience_threshold": 1, "do_flip": true, "batch_size": 128, "optimization": "nesterov_momentum", "nb_data_augmentation": 4, "learning_rate_decay_method": "exp", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 1.2009287845617613e-05, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.034759803206613614}, "accuracy_valid_max": 0.5017001600150602, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-3, 3)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.49983822006777107, "loss_train": [2.9678151607513428, 2.303189277648926, 2.1509015560150146, 2.074103593826294, 2.0264761447906494, 1.9914608001708984, 1.9650753736495972, 1.942702293395996, 1.9246881008148193, 1.9081121683120728, 1.8952083587646484, 1.8819444179534912, 1.8707029819488525, 1.8600661754608154, 1.8510117530822754, 1.8422743082046509, 1.8341732025146484, 1.825875163078308, 1.8185813426971436, 1.8130930662155151, 1.8055614233016968, 1.799452781677246, 1.793028473854065, 1.7877116203308105, 1.7825053930282593, 1.7774794101715088, 1.7722039222717285, 1.7688425779342651, 1.7636098861694336, 1.7596080303192139, 1.7561233043670654, 1.7508879899978638, 1.7477699518203735, 1.7439159154891968, 1.7408965826034546, 1.737663984298706, 1.7344609498977661, 1.731590986251831, 1.7283587455749512, 1.7258214950561523, 1.7230678796768188, 1.7204607725143433, 1.718019723892212, 1.7164995670318604, 1.713740587234497, 1.7118722200393677, 1.710503101348877, 1.7077302932739258, 1.7051951885223389, 1.703857421875, 1.7015626430511475, 1.7004247903823853, 1.698896884918213, 1.695987343788147, 1.6963763236999512, 1.694526195526123, 1.693408489227295, 1.6912856101989746, 1.6903494596481323, 1.6888738870620728, 1.6881921291351318, 1.686346411705017, 1.6856536865234375, 1.683657169342041, 1.6828750371932983, 1.6832685470581055, 1.681932806968689, 1.6812447309494019, 1.6798685789108276, 1.6785157918930054, 1.6780917644500732, 1.677207589149475, 1.6770834922790527, 1.6757416725158691, 1.6752084493637085, 1.6756393909454346, 1.6742690801620483, 1.6728862524032593, 1.6721265316009521, 1.6716715097427368, 1.6712162494659424, 1.6723536252975464, 1.669860601425171, 1.6691018342971802, 1.6693089008331299, 1.669342279434204, 1.6683199405670166, 1.6684907674789429, 1.6668572425842285, 1.6667205095291138, 1.6662203073501587, 1.6675742864608765, 1.6653623580932617, 1.665838599205017, 1.6647915840148926, 1.6644197702407837, 1.6633566617965698, 1.6641288995742798, 1.6633388996124268, 1.6632716655731201, 1.6639649868011475, 1.6629680395126343, 1.662468671798706, 1.6622202396392822, 1.6625406742095947, 1.663182020187378, 1.6622461080551147, 1.6612834930419922, 1.6617318391799927, 1.6614025831222534, 1.6609469652175903, 1.660578966140747, 1.6609832048416138, 1.6608586311340332, 1.6600195169448853, 1.6596320867538452, 1.6607409715652466, 1.6600645780563354, 1.6599748134613037, 1.6600341796875, 1.65956711769104, 1.6587356328964233, 1.659954309463501, 1.6590640544891357, 1.6580851078033447, 1.6586382389068604, 1.658607840538025, 1.6577496528625488, 1.658351182937622, 1.6579006910324097, 1.6587721109390259, 1.658502221107483, 1.6572401523590088, 1.658011555671692, 1.658278226852417, 1.6575570106506348, 1.6577019691467285, 1.6576200723648071, 1.6567833423614502, 1.6573781967163086, 1.6584326028823853, 1.6583046913146973, 1.6579395532608032, 1.6579862833023071, 1.6570407152175903, 1.657393217086792, 1.657555341720581, 1.6568176746368408, 1.6561832427978516, 1.658281922340393, 1.6572977304458618, 1.657655119895935, 1.6568280458450317, 1.6556105613708496, 1.657128095626831, 1.6564098596572876, 1.6571311950683594, 1.656861424446106, 1.6563423871994019, 1.6556596755981445, 1.6574407815933228, 1.6570513248443604, 1.6559728384017944, 1.6561074256896973, 1.655169129371643, 1.6565752029418945, 1.6559631824493408, 1.657584309577942, 1.6565148830413818, 1.656935453414917, 1.6560879945755005, 1.6555765867233276, 1.655785322189331, 1.655730962753296, 1.656190037727356, 1.6570290327072144, 1.6559820175170898, 1.6564381122589111, 1.656590223312378, 1.6563676595687866, 1.6563570499420166, 1.6560611724853516, 1.655756950378418, 1.6553473472595215, 1.6568723917007446, 1.6561594009399414, 1.6567614078521729, 1.6568236351013184, 1.6556942462921143, 1.6565701961517334, 1.6571296453475952, 1.6555328369140625, 1.6568284034729004, 1.656068205833435, 1.6557753086090088, 1.656184434890747, 1.6561954021453857, 1.6553382873535156, 1.6558681726455688, 1.6556673049926758, 1.656594157218933, 1.655954360961914, 1.655617594718933, 1.6572585105895996, 1.6560674905776978, 1.6565333604812622, 1.6562635898590088, 1.655877709388733, 1.6562316417694092, 1.6562331914901733, 1.6561435461044312, 1.6553815603256226, 1.6557514667510986, 1.6562577486038208, 1.656865119934082, 1.655522108078003, 1.6563621759414673, 1.655456304550171, 1.6554838418960571, 1.6553539037704468, 1.6558438539505005, 1.657605767250061, 1.6563830375671387, 1.656050443649292, 1.655147671699524, 1.6553541421890259, 1.6560304164886475, 1.6551364660263062, 1.6568387746810913, 1.6547545194625854, 1.657341718673706, 1.6563694477081299, 1.6556594371795654, 1.656008005142212, 1.6555813550949097, 1.6560183763504028], "accuracy_train_first": 0.23791174960778885, "model": "residualv3", "loss_std": [0.4919123649597168, 0.10568893700838089, 0.07060633599758148, 0.06698182225227356, 0.06807631254196167, 0.0707123726606369, 0.07212536036968231, 0.07265426963567734, 0.07658454775810242, 0.07578256726264954, 0.07611434161663055, 0.07672053575515747, 0.07534050941467285, 0.07699541747570038, 0.07781986892223358, 0.07821296155452728, 0.0778457298874855, 0.0790354460477829, 0.07974497973918915, 0.08077070862054825, 0.07928742468357086, 0.07926305383443832, 0.07730606198310852, 0.07895667105913162, 0.0786663070321083, 0.07958053052425385, 0.079075887799263, 0.08097565174102783, 0.08003595471382141, 0.07875243574380875, 0.0796407014131546, 0.07954832911491394, 0.07963459193706512, 0.07891257852315903, 0.07997727394104004, 0.07974795997142792, 0.08068647235631943, 0.08061650395393372, 0.08012537658214569, 0.08011845499277115, 0.07803403586149216, 0.07991813123226166, 0.07952253520488739, 0.081211157143116, 0.08056877553462982, 0.08084613084793091, 0.08025236427783966, 0.08005646616220474, 0.08038199692964554, 0.08007826656103134, 0.07957383245229721, 0.08035185188055038, 0.08083348721265793, 0.07906689494848251, 0.08083176612854004, 0.08013921976089478, 0.07985992729663849, 0.07955701649188995, 0.08064046502113342, 0.08173542469739914, 0.08082038164138794, 0.07961505651473999, 0.08211144059896469, 0.08038695156574249, 0.08017503470182419, 0.0800311341881752, 0.08006832003593445, 0.08150789886713028, 0.0797395259141922, 0.08048617094755173, 0.07935437560081482, 0.08106651157140732, 0.0810328796505928, 0.08101759105920792, 0.08118890225887299, 0.08080850541591644, 0.08128944784402847, 0.08128850907087326, 0.08048657327890396, 0.0810675397515297, 0.08159609138965607, 0.08253125846385956, 0.08011020720005035, 0.08078468590974808, 0.08053306490182877, 0.08129457384347916, 0.08139755576848984, 0.08151219040155411, 0.07986147701740265, 0.07996158301830292, 0.0814623236656189, 0.08157345652580261, 0.0806865468621254, 0.08040005713701248, 0.08067958801984787, 0.08035844564437866, 0.08042889833450317, 0.08139525353908539, 0.08098207414150238, 0.08132051676511765, 0.08091168850660324, 0.08050376921892166, 0.08106530457735062, 0.08122093230485916, 0.08130308985710144, 0.08163785189390182, 0.08095373958349228, 0.0804498940706253, 0.08095434308052063, 0.08099376410245895, 0.0804716944694519, 0.07899096608161926, 0.08044903725385666, 0.08152653276920319, 0.08148406445980072, 0.08155091106891632, 0.08173850178718567, 0.08141963928937912, 0.08157015591859818, 0.08145060390233994, 0.08124881982803345, 0.08157748728990555, 0.08183406293392181, 0.08209185302257538, 0.08019934594631195, 0.08042686432600021, 0.08082284778356552, 0.08127996325492859, 0.08063431084156036, 0.0811629667878151, 0.083096943795681, 0.08219205588102341, 0.08182629197835922, 0.08162294328212738, 0.0806027352809906, 0.08240056782960892, 0.08075710386037827, 0.08056224137544632, 0.08212954550981522, 0.08141537755727768, 0.08141901344060898, 0.08189009875059128, 0.08148273825645447, 0.08199625462293625, 0.08143767714500427, 0.0825396329164505, 0.08085620403289795, 0.08041976392269135, 0.08103660494089127, 0.08111052215099335, 0.0821823999285698, 0.08126599341630936, 0.08134032040834427, 0.08112457394599915, 0.08142289519309998, 0.08033401519060135, 0.08181675523519516, 0.08073821663856506, 0.08058121055364609, 0.08238627016544342, 0.08143385499715805, 0.08013749867677689, 0.08090823143720627, 0.0811888724565506, 0.08124013245105743, 0.08130715042352676, 0.0818309485912323, 0.08232205361127853, 0.08118933439254761, 0.08090733736753464, 0.08078354597091675, 0.08104600757360458, 0.08109751343727112, 0.0800810232758522, 0.08071421086788177, 0.08186884224414825, 0.08166411519050598, 0.08115213364362717, 0.08240504562854767, 0.08047714829444885, 0.08129097521305084, 0.08046942204236984, 0.08153702318668365, 0.08302312344312668, 0.08076822757720947, 0.0812646672129631, 0.08130097389221191, 0.08105175197124481, 0.07919576019048691, 0.08149644732475281, 0.08191907405853271, 0.08121408522129059, 0.07985973358154297, 0.08274750411510468, 0.08114219456911087, 0.0804043635725975, 0.08198580890893936, 0.08213814347982407, 0.08074252307415009, 0.0799892470240593, 0.08084800094366074, 0.0813569575548172, 0.08204588294029236, 0.08157896250486374, 0.08129763603210449, 0.08041585981845856, 0.08214447647333145, 0.08065491169691086, 0.08220426738262177, 0.08097608387470245, 0.0817520022392273, 0.08208396285772324, 0.08212681859731674, 0.08133513480424881, 0.08085905015468597, 0.0815788060426712, 0.08091439306735992, 0.0817708745598793, 0.08095298707485199, 0.08172477036714554, 0.08073145151138306, 0.08190499246120453, 0.08127988874912262, 0.08113879710435867, 0.08094345778226852, 0.08113879710435867, 0.08211153745651245, 0.08025848865509033, 0.08145283907651901, 0.08090093731880188, 0.08139147609472275, 0.08107826113700867, 0.08115816861391068, 0.08248671889305115, 0.08139533549547195, 0.08150122314691544]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:14 2016", "state": "available"}], "summary": "968f98f0f1ed2a5029fca05c94f4fbd8"}