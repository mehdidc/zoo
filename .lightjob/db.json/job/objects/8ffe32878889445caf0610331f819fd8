{"content": {"hp_model": {"f0": 32, "f1": 32, "f2": 32, "f3": 16, "nonlin": "very_leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.8299005031585693, 1.5591062307357788, 1.4310775995254517, 1.334371566772461, 1.2568553686141968, 1.190246343612671, 1.1351666450500488, 1.088541030883789, 1.0463731288909912, 1.0125702619552612, 0.9806434512138367, 0.951485812664032, 0.9264234900474548, 0.9049138426780701, 0.8813681602478027, 0.8628659248352051, 0.8438817262649536, 0.827466607093811, 0.8126073479652405, 0.799801766872406, 0.7854652404785156, 0.7717716693878174, 0.7602219581604004, 0.7497857809066772, 0.7390154004096985, 0.7287663817405701, 0.7202366590499878, 0.7107332348823547, 0.7025901675224304, 0.6923050880432129, 0.6868239045143127, 0.6808757781982422, 0.6728266477584839, 0.6679471135139465, 0.6595519185066223, 0.6537454724311829, 0.6486835479736328, 0.6434587240219116, 0.639464259147644, 0.6338914632797241, 0.6303632259368896, 0.6248482465744019, 0.6210229992866516, 0.6166543960571289, 0.6125761866569519, 0.6096835136413574, 0.6057514548301697, 0.6014833450317383, 0.5991078615188599, 0.5957520604133606, 0.5943511724472046, 0.5895645022392273, 0.5884749889373779, 0.5847601890563965, 0.5829983353614807, 0.5803672075271606, 0.5786117315292358, 0.5762494206428528, 0.5714939832687378, 0.571742832660675, 0.5696823596954346, 0.5677881240844727, 0.5659542679786682, 0.5642369985580444, 0.5625351071357727, 0.5617587566375732, 0.5595943927764893, 0.5575725436210632, 0.5575428605079651, 0.5557432174682617, 0.5542426705360413, 0.5518496036529541, 0.5517781376838684, 0.5498117804527283, 0.5494059324264526, 0.5478424429893494, 0.5469411611557007, 0.5474792122840881, 0.5439229011535645, 0.5440025925636292, 0.5426546335220337, 0.5415801405906677, 0.5407344698905945, 0.5415250062942505, 0.5412718653678894, 0.5386340618133545, 0.5398554801940918, 0.5389317274093628, 0.5369967222213745, 0.5372149348258972, 0.5380508899688721, 0.5346260666847229, 0.536318838596344, 0.5345627069473267, 0.5336785912513733, 0.533424973487854, 0.5323556065559387, 0.5339178442955017, 0.5340573191642761, 0.5315768718719482, 0.5315966010093689, 0.5297721028327942, 0.5311220288276672, 0.5316555500030518, 0.5310807824134827, 0.5289536118507385, 0.5285375118255615, 0.5299807190895081, 0.5272123217582703, 0.5290552973747253, 0.5288929343223572, 0.5288853645324707, 0.5292274951934814, 0.5276666879653931, 0.5276774168014526, 0.5286007523536682, 0.5280616283416748, 0.5288538932800293, 0.5256273746490479, 0.5258411765098572, 0.5269762277603149, 0.5265354514122009, 0.5271501541137695, 0.5244397521018982, 0.5262488126754761, 0.5261465311050415, 0.5257765650749207, 0.5262823104858398, 0.526332676410675, 0.524070143699646, 0.5253141522407532, 0.523621678352356, 0.5239918231964111, 0.5252646207809448, 0.5242136716842651, 0.5237436890602112, 0.5239658951759338, 0.524612545967102, 0.5235507488250732, 0.5221716165542603, 0.5253477096557617, 0.5248862504959106, 0.5247568488121033, 0.5237283706665039, 0.5241246223449707, 0.5239046812057495, 0.5236475467681885, 0.5251349210739136, 0.5237823128700256, 0.5240582227706909, 0.5226554870605469, 0.5232032537460327, 0.5232456922531128, 0.5245376229286194, 0.5242374539375305, 0.5242912769317627, 0.5234069228172302, 0.5238258242607117, 0.5223193168640137, 0.5244529843330383, 0.521868884563446, 0.5225011706352234, 0.5227255821228027, 0.5226246118545532, 0.523654580116272, 0.5224671363830566, 0.5221606492996216, 0.5214616656303406, 0.5233656764030457, 0.5239700078964233, 0.5239839553833008, 0.5227989554405212, 0.5236510634422302, 0.522125244140625, 0.5236537456512451, 0.5232595205307007, 0.5233984589576721, 0.5218128561973572, 0.5206595659255981, 0.5230495929718018, 0.5216599106788635, 0.5216394066810608, 0.521657407283783], "moving_avg_accuracy_train": [0.04334933540397747, 0.08891474490413896, 0.13410597938151758, 0.178290056456055, 0.21987831805043656, 0.26010411442922454, 0.2973278552046022, 0.33252603980111317, 0.36602238391647063, 0.39805217576545665, 0.4284063588556699, 0.45659444090107376, 0.48275887958597563, 0.5069555548714257, 0.5299715425044382, 0.551880877618151, 0.5719365339443295, 0.5906747605390531, 0.6080251566243136, 0.6245401293974211, 0.6397335597312579, 0.6540492439078645, 0.6667987533775893, 0.6785546188574754, 0.6900716443690608, 0.7003531898747073, 0.7105273037095419, 0.7198815356632462, 0.7284469008942176, 0.7365347288580443, 0.743997424732622, 0.7509579195471043, 0.7576825280515117, 0.7639810333328315, 0.7698287966419902, 0.7754102208606816, 0.78040327572298, 0.7849435641240577, 0.7892760731659247, 0.7932290260191367, 0.7972980000322101, 0.8012786941285185, 0.8046937999544539, 0.8080022352275578, 0.8113725968780671, 0.8145802724754209, 0.8174627104595141, 0.8201172503677894, 0.8227061548875811, 0.8252779483827654, 0.8278251855558398, 0.829994357978246, 0.8320559672500966, 0.8341716880661537, 0.8360665001565484, 0.8377487597939021, 0.8395789055591594, 0.8410958644633763, 0.8426425251331331, 0.8441553193275533, 0.8454517659846835, 0.8468439992641682, 0.8481179716038089, 0.8493227835761799, 0.8503535277822385, 0.8514370546355668, 0.8524818390725731, 0.8534221811146975, 0.8543195340799628, 0.8552086401034911, 0.8560297258151335, 0.8567896292948974, 0.8576270022481135, 0.8584760050560172, 0.8590518426271966, 0.8597791795900214, 0.8604943809208864, 0.861152013011522, 0.8618136363573798, 0.8625135848698054, 0.8631970530024262, 0.8637680325432228, 0.8644331569490149, 0.8650339859165811, 0.8655305541600097, 0.8660727966802859, 0.8665282628652011, 0.866996311151863, 0.8674734663277435, 0.8678679566562556, 0.8682439603400209, 0.8686450345268106, 0.8690688163615973, 0.8693827906974291, 0.8696979917806484, 0.8699537709698317, 0.8702350894650874, 0.8704650246227223, 0.8707859706538978, 0.8710236327593275, 0.871328173408967, 0.871609235440071, 0.8718087488942643, 0.8720696912113717, 0.8722836490063013, 0.8724645492288716, 0.8725739531042033, 0.8727607722467637, 0.872940499170297, 0.8730557864741052, 0.8731571117522668, 0.8732717001859827, 0.8734049485643943, 0.8735737362787835, 0.8736767810479151, 0.8737557146425514, 0.873987081999125, 0.8741092460652701, 0.8742727442450572, 0.8744361325997135, 0.8745204031010472, 0.8746799519093902, 0.8747816931583275, 0.8748755854311806, 0.8749159827470049, 0.874919716150276, 0.8750184433632292, 0.8750816851691635, 0.8751804194242572, 0.8752228133264697, 0.8752563175408419, 0.8753864527325864, 0.8754221941968232, 0.8755147793348651, 0.8755818299174362, 0.8756770526738931, 0.8756837701928178, 0.8756990805062695, 0.8757779639550427, 0.8758350081660815, 0.8758398449798259, 0.8758767501955292, 0.875942444875358, 0.8759644398038889, 0.8759517192050522, 0.8760053748327659, 0.8760582430976898, 0.8760988490896927, 0.8761493814241712, 0.8761785484347164, 0.8762187496370643, 0.8762851576537011, 0.8763100476365314, 0.8763325207187161, 0.876352674395045, 0.8763847996454169, 0.8763880996850281, 0.8764026954647257, 0.8764274574105012, 0.8764381534664702, 0.876454755363271, 0.8764928764608494, 0.8764899830677175, 0.8765083053531846, 0.8764737142339329, 0.876465797665883, 0.8764447579105995, 0.8764676027117784, 0.8764626584935722, 0.8764419687043387, 0.8764140112499715, 0.8763772237969936, 0.8764184477535806, 0.8764323338752324, 0.8764262301942429, 0.876455650162314, 0.8764541902990449, 0.8764598518685313, 0.8764858375715361, 0.876465118974497, 0.8764604231300188, 0.8764608471676075, 0.8764518921573805], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.04369896578501505, 0.08904830748776354, 0.13385764983763176, 0.17817290377329628, 0.219192915730304, 0.25918415691329766, 0.29646727783717874, 0.33106468479555423, 0.3641719481966464, 0.39555642882878894, 0.425154500413154, 0.45279580041890183, 0.4785682017399634, 0.5018467531303495, 0.5243549489110646, 0.5455818281257563, 0.5649698696637078, 0.5830029855305449, 0.5997343076006078, 0.6152685716824144, 0.629680773484429, 0.6426385185663325, 0.6544571215289764, 0.6653533003775848, 0.6758190410288323, 0.6851039302712051, 0.6942212845614791, 0.7027229606987047, 0.7098841289548885, 0.7169252658993845, 0.7234220100643407, 0.7295529300575301, 0.7351531187441717, 0.7403184474006279, 0.7450984320092098, 0.749758540097822, 0.7539139572665037, 0.7575073483433172, 0.7609356833037897, 0.7639092624696456, 0.7672752545208739, 0.7703799485717985, 0.7728791454503113, 0.7755577277520422, 0.7781261137211904, 0.7807418073660142, 0.7829270922261747, 0.7849538542479098, 0.7867911766073808, 0.788849657779474, 0.7907521484680177, 0.7923382017405682, 0.7935937217397042, 0.7951540243586555, 0.7965562376983923, 0.7975120244142458, 0.798851395203243, 0.8001697807205693, 0.8012942630212534, 0.8022818830293691, 0.8032317761929232, 0.8044172993925315, 0.805362199959679, 0.8061006881715425, 0.8069972611559695, 0.8077685852568635, 0.8082898194928488, 0.8088810006177356, 0.8093286439200434, 0.8101852420656898, 0.8106865962006118, 0.8112487077119512, 0.8118268207509971, 0.8122860873298884, 0.8124521980999115, 0.8127848032616825, 0.8131085619697762, 0.8136715880205395, 0.8140307975825669, 0.8143164355859819, 0.8148329459712842, 0.8152855982868064, 0.8155698855496167, 0.8158989862736461, 0.8161941474166129, 0.8165483301900118, 0.816839592097592, 0.8171404079254834, 0.8174121716792453, 0.817681173120131, 0.8179598955106782, 0.8182341302160109, 0.8184952074993797, 0.818655905358252, 0.8187628828288274, 0.8189832918821645, 0.8191195953652582, 0.8193033036562926, 0.8194574635956332, 0.8195951780323801, 0.8196702929004523, 0.819897617196627, 0.8199170445771149, 0.819971150313304, 0.8200697031095339, 0.8201960512285503, 0.8201988717457556, 0.8203468650775806, 0.8203946098574731, 0.8204375801593764, 0.820538318095999, 0.8205933906538689, 0.8206907545722922, 0.8207407314964636, 0.8206148122907179, 0.8206123777954564, 0.8205725361473113, 0.8205844772803211, 0.820732590169759, 0.8207661765029336, 0.8208096407427005, 0.820835522018581, 0.8209086728005331, 0.8209999520754496, 0.8210576893603745, 0.8211096529168069, 0.8211675976401864, 0.821292990078728, 0.8213437786085058, 0.8213640447141463, 0.8213355151015419, 0.8213586665751978, 0.8214771591514882, 0.8214596731403304, 0.821518207426448, 0.8215576517440442, 0.8214100461611308, 0.8213148517389183, 0.8213135964690175, 0.8212636386011066, 0.8212441200911466, 0.8212509674946824, 0.8212683076804551, 0.8213581855438102, 0.8214167205756491, 0.8214439585331444, 0.8213921419814113, 0.8214452223521708, 0.8214807876546043, 0.8214517612705445, 0.8214622586186408, 0.8214717062319273, 0.8215178596862948, 0.8215339542240658, 0.8214996111830599, 0.8215297376024046, 0.821543614839905, 0.8215581633709748, 0.8215336064465278, 0.8215481263082757, 0.821560164675189, 0.8215109935578206, 0.8215145381685296, 0.8215309648580772, 0.8214725066911701, 0.8214565154347037, 0.821429916272634, 0.8215036332767712, 0.8214967363929947, 0.8214172870100958, 0.8214190247529868, 0.8214083816903387, 0.8213865959027055, 0.8213425746313355, 0.8214138482770122, 0.8213793087994616, 0.821372637332166, 0.8213666330116, 0.8213978502168405, 0.821462566795307, 0.821471983590927, 0.8214428081045753, 0.8215152359255183], "moving_var_accuracy_train": [0.01691248391969882, 0.0339070944139856, 0.04889661403489179, 0.06157704663376058, 0.07098559349236895, 0.0784500663919015, 0.08307552164852397, 0.08591817927368214, 0.08742440696816393, 0.08791513436435189, 0.08741600880758442, 0.08582551965141175, 0.08340416835153447, 0.0803330633702066, 0.07706737821369279, 0.07368081107844818, 0.06993279412666746, 0.06609960493723757, 0.0621989706423526, 0.05843377250938571, 0.05466795818622948, 0.051045611688605635, 0.047404000445212494, 0.04390740375932181, 0.040710440273100224, 0.03759078784765233, 0.03476332239380465, 0.03207450505341752, 0.029527343881935172, 0.027163326144293762, 0.024948219997312327, 0.022889434390143015, 0.02100747418696664, 0.019263767287279302, 0.01764515758003086, 0.01616101248880872, 0.014769286611649144, 0.013477885919368883, 0.012299033038812732, 0.011209762261268902, 0.01023779498081361, 0.009356628812127704, 0.008525932461135984, 0.007771850910629248, 0.007096899858463335, 0.006479812517257738, 0.005906607304121254, 0.005379365812830762, 0.0049017510710610696, 0.004471103059991813, 0.004082388508935666, 0.003716497439025242, 0.0033830997902307394, 0.0030850762823511437, 0.0028088814698371788, 0.0025534633002406845, 0.0023282618719154178, 0.0021161461635776243, 0.001926060980266217, 0.0017540517987116378, 0.0015937735842535316, 0.0014518410473687228, 0.0013212639923313786, 0.0012022017400971584, 0.0010915434686523538, 0.000992955395764072, 0.0009034840268659589, 0.0008210938125850428, 0.0007462316124249708, 0.0006787230368721421, 0.0006169183688976976, 0.000560423611694943, 0.0005106919916904487, 0.0004661100444318588, 0.0004224833401641088, 0.0003849961776651191, 0.0003511001763916464, 0.0003198824784521857, 0.0002918339396730233, 0.0002670598969861415, 0.00024455806548230054, 0.00022303641765814412, 0.0002047142901689531, 0.00018749182018645784, 0.00017096185835124825, 0.00015651191507328328, 0.00014272776857636666, 0.000130426614506554, 0.00011943304661272463, 0.0001088903455250584, 9.927371990439865e-05, 9.079409244374001e-05, 8.333100259082211e-05, 7.588512128378896e-05, 6.919077466117446e-05, 6.286050413763e-05, 5.7286714585823145e-05, 5.20338747176899e-05, 4.7757544440266826e-05, 4.3490139483455675e-05, 3.997583060065528e-05, 3.6689210328544715e-05, 3.337853986132769e-05, 3.0653503910911085e-05, 2.800015496192038e-05, 2.5494663480462238e-05, 2.3052920003854443e-05, 2.1061740531712293e-05, 1.9246282381925756e-05, 1.7441274605507558e-05, 1.5789548452907615e-05, 1.4328768189889672e-05, 1.3055687544044834e-05, 1.2006522422399191e-05, 1.0901434200167756e-05, 9.867365391410883e-06, 9.362406535460389e-06, 8.560482413428248e-06, 7.945019065228787e-06, 7.390778948641647e-06, 6.715614710332563e-06, 6.273155639492487e-06, 5.739001611161156e-06, 5.244443280158738e-06, 4.734686440275184e-06, 4.261343240947527e-06, 3.922932480050346e-06, 3.566634966205876e-06, 3.2977075477453002e-06, 2.9841119794740156e-06, 2.695803572952916e-06, 2.5786397288310912e-06, 2.3322728263400545e-06, 2.1761936137822494e-06, 1.9990362780121053e-06, 1.8807390103360102e-06, 1.6930712348469594e-06, 1.5258737626441673e-06, 1.4292897727929293e-06, 1.3156471736309202e-06, 1.1842930091726084e-06, 1.0781216627702857e-06, 1.0091516151134932e-06, 9.125904455319038e-07, 8.22787723691581e-07, 7.66419288790564e-07, 7.149328408360359e-07, 6.58279176031283e-07, 6.15432909878771e-07, 5.615460494282029e-07, 5.199366745172948e-07, 5.076332291283618e-07, 4.624455074231742e-07, 4.2074631148682315e-07, 3.823272163642433e-07, 3.5338278013089113e-07, 3.1814251447071736e-07, 2.8824559408848245e-07, 2.649394203069272e-07, 2.394751287958767e-07, 2.1800822271273025e-07, 2.0928636316669675e-07, 1.8843307236436433e-07, 1.7261112043056159e-07, 1.6611891816724136e-07, 1.500710747977287e-07, 1.3904800903941535e-07, 1.2984017260359644e-07, 1.1707616298627718e-07, 1.0922115309440927e-07, 1.0533361107714054e-07, 1.0698010023887659e-07, 1.1157682158523967e-07, 1.0215455879750024e-07, 9.227439721234046e-08, 9.08367681828418e-08, 8.177227217143648e-08, 7.388352527574001e-08, 7.257248359402339e-08, 6.917857760406611e-08, 6.245917844192355e-08, 5.62148788686212e-08, 5.13151208552456e-08], "duration": 155319.674762, "accuracy_train": [0.43349335403977485, 0.4990034304055925, 0.5408270896779255, 0.5759467501268918, 0.5941726723998708, 0.6221362818383167, 0.6323415221830011, 0.6493097011697121, 0.6674894809546881, 0.6863203024063308, 0.7015940066675895, 0.7102871793097084, 0.7182388277500923, 0.7247256324404762, 0.7371154312015504, 0.7490648936415651, 0.7524374408799372, 0.7593187998915651, 0.7641787213916574, 0.7731748843553894, 0.7764744327357882, 0.7828904014973238, 0.7815443386051125, 0.7843574081764488, 0.7937248739733297, 0.792887099425526, 0.8020943282230528, 0.8040696232465854, 0.8055351879729604, 0.8093251805324843, 0.8111616876038206, 0.8136023728774455, 0.8182040045911776, 0.8206675808647103, 0.8224586664244187, 0.8256430388289037, 0.8253407694836655, 0.8258061597337578, 0.8282686545427279, 0.8288056016980436, 0.8339187661498707, 0.8371049409952934, 0.8354297523878736, 0.8377781526854927, 0.8417058517326504, 0.8434493528516058, 0.8434046523163529, 0.8440081095422666, 0.846006295565707, 0.8484240898394242, 0.8507503201135106, 0.8495169097799004, 0.8506104506967516, 0.8532131754106681, 0.8531198089700996, 0.8528890965300849, 0.8560502174464747, 0.854748494601329, 0.856562471160945, 0.8577704670773348, 0.8571197858988556, 0.8593740987795312, 0.8595837226605758, 0.8601660913275194, 0.8596302256367663, 0.8611887963155224, 0.8618848990056294, 0.8618852594938169, 0.8623957107673496, 0.8632105943152455, 0.8634194972199151, 0.8636287606127722, 0.865163358827058, 0.8661170303271503, 0.864234380767811, 0.8663252122554448, 0.866931192898671, 0.8670707018272426, 0.8677682464700996, 0.8688131214816353, 0.8693482661960132, 0.8689068484103912, 0.8704192766011444, 0.8704414466246769, 0.8699996683508674, 0.8709529793627722, 0.8706274585294389, 0.8712087457318198, 0.8717678629106681, 0.8714183696128645, 0.8716279934939092, 0.872254702207918, 0.8728828528746769, 0.8722085597199151, 0.8725348015296235, 0.8722557836724806, 0.8727669559223883, 0.872534441041436, 0.8736744849344776, 0.8731625917081949, 0.8740690392557217, 0.8741387937200074, 0.8736043699820044, 0.8744181720653378, 0.8742092691606681, 0.8740926512320044, 0.873558587982189, 0.8744421445298081, 0.8745580414820967, 0.8740933722083795, 0.8740690392557217, 0.8743029960894242, 0.8746041839700996, 0.8750928257082872, 0.8746041839700996, 0.8744661169942783, 0.8760693882082872, 0.8752087226605758, 0.8757442278631414, 0.8759066277916205, 0.8752788376130491, 0.8761158911844776, 0.8756973643987633, 0.8757206158868586, 0.8752795585894242, 0.8749533167797158, 0.8759069882798081, 0.8756508614225729, 0.8760690277200996, 0.8756043584463824, 0.8755578554701919, 0.8765576694582872, 0.8757438673749538, 0.8763480455772426, 0.8761852851605758, 0.8765340574820044, 0.8757442278631414, 0.8758368733273348, 0.8764879149940015, 0.8763484060654301, 0.8758833763035253, 0.8762088971368586, 0.8765336969938169, 0.8761623941606681, 0.8758372338155224, 0.876488275482189, 0.8765340574820044, 0.8764643030177187, 0.8766041724344776, 0.8764410515296235, 0.8765805604581949, 0.876882829803433, 0.8765340574820044, 0.8765347784583795, 0.8765340574820044, 0.8766739268987633, 0.8764178000415282, 0.8765340574820044, 0.8766503149224806, 0.8765344179701919, 0.8766041724344776, 0.876835966339055, 0.8764639425295312, 0.8766732059223883, 0.8761623941606681, 0.876394548553433, 0.8762554001130491, 0.8766732059223883, 0.8764181605297158, 0.8762557606012367, 0.8761623941606681, 0.8760461367201919, 0.8767894633628645, 0.8765573089700996, 0.8763712970653378, 0.8767204298749538, 0.8764410515296235, 0.8765108059939092, 0.8767197088985788, 0.8762786516011444, 0.8764181605297158, 0.8764646635059062, 0.8763712970653378], "end": "2016-02-03 04:55:30.187000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0], "moving_var_accuracy_valid": [0.01718639649611925, 0.03397682198236111, 0.04865003424057424, 0.0614596063989587, 0.07045741818764033, 0.07780537071108362, 0.0825351135923979, 0.08505442734734917, 0.08641380262179792, 0.08663729298056212, 0.08585797625612471, 0.08414855182458204, 0.08171164667080857, 0.07841750059724137, 0.07513532043324422, 0.07167701200067582, 0.06789237619270422, 0.06402987798423473, 0.06014632442972085, 0.05630351223181855, 0.05254256505567463, 0.04879943696857551, 0.04517660765561549, 0.04172748731156169, 0.0385405241268181, 0.03546235422832446, 0.03266425414878142, 0.030048335208183716, 0.02750504266450571, 0.02520073688329547, 0.023060532357669944, 0.021092772741568958, 0.019265754487345958, 0.017579304619774243, 0.01602700843352133, 0.014619757056747132, 0.01331318877768439, 0.012098082034794256, 0.010994055156725612, 0.009974229198553565, 0.009078775401098593, 0.008257649987337357, 0.0074880988539417406, 0.006803862196871881, 0.006182845435563349, 0.005626137571199155, 0.005106503043359665, 0.004632822617658422, 0.00419992213696609, 0.0038180660258922425, 0.003468834660682973, 0.003144591279464993, 0.002844319125732567, 0.002581798111523666, 0.0023413141206225234, 0.00211540446277609, 0.0019200092434922494, 0.0017436515824936862, 0.0015806665682452825, 0.001431378450944627, 0.0012963612790496649, 0.0011793743384559837, 0.0010694724383465456, 0.0009674334780634422, 0.0008779247183047384, 0.0007954867142918443, 0.0007183832090215288, 0.0006496903442211777, 0.0005865247705339689, 0.0005344761369286935, 0.0004832907269532546, 0.00043780537841855174, 0.00039703277274993, 0.0003592278275893153, 0.0003235533799216431, 0.0002921936776722086, 0.0002639176872145869, 0.0002403789034976718, 0.0002175022967329717, 0.00019648636868062867, 0.00017923877861569313, 0.00016315894782285219, 0.00014757042627073269, 0.00013378814922266886, 0.00012119341520325871, 0.00011020308261568598, 9.994627584338236e-05, 9.076605971983405e-05, 8.235415358857947e-05, 7.47699942065088e-05, 6.799217032478872e-05, 6.18697953547905e-05, 5.6296267950332527e-05, 5.0899055371914466e-05, 4.591214744761943e-05, 4.175815405999429e-05, 3.7749546409526357e-05, 3.42783303943268e-05, 3.106438493697166e-05, 2.8128633838071177e-05, 2.5366550644913625e-05, 2.3294982601103945e-05, 2.0968881149007146e-05, 1.8898339910303525e-05, 1.7095919802075646e-05, 1.5530002446479145e-05, 1.3977073799686973e-05, 1.2776484656100386e-05, 1.1519352266553236e-05, 1.0384035061508853e-05, 9.436964742232775e-06, 8.520565147682583e-06, 7.753826226411058e-06, 7.000922840316637e-06, 6.4435313736657615e-06, 5.799231577203792e-06, 5.233594631825625e-06, 4.711518484561091e-06, 4.437803488263413e-06, 4.004175515422176e-06, 3.6207602251265727e-06, 3.264712766584765e-06, 2.986400822028169e-06, 2.762747894089008e-06, 2.5164754513146223e-06, 2.289129806957119e-06, 2.090435144969196e-06, 2.0229010032629432e-06, 1.8438261757495642e-06, 1.663139993515053e-06, 1.5041514433217572e-06, 1.3585602155816033e-06, 1.3490686097469898e-06, 1.2169135940482046e-06, 1.1260585985050168e-06, 1.0274554263701636e-06, 1.1207965566981205e-06, 1.090274703211607e-06, 9.812614142131624e-07, 9.055973698875588e-07, 8.184663829783511e-07, 7.370417270971737e-07, 6.660436927711517e-07, 6.721415963855232e-07, 6.357645863183571e-07, 5.788652846431766e-07, 5.45143351480457e-07, 5.159867481720798e-07, 4.7577208998956544e-07, 4.3577765973490774e-07, 3.93191642614882e-07, 3.546757949246994e-07, 3.383794875827121e-07, 3.06872846139038e-07, 2.868005617149821e-07, 2.6628891582622973e-07, 2.413932237294188e-07, 2.1915883916306036e-07, 2.0267033809135783e-07, 1.8430074174880396e-07, 1.6717496807539988e-07, 1.7221766031707807e-07, 1.551089726710693e-07, 1.4202660056941518e-07, 1.5858015601573773e-07, 1.4502362296453311e-07, 1.3688889947340275e-07, 1.7210777981674407e-07, 1.553251048875109e-07, 1.966024343858496e-07, 1.769693687004625e-07, 1.602919048731937e-07, 1.4853429927109101e-07, 1.5112172034121773e-07, 1.817289414195028e-07, 1.742928268627818e-07, 1.5726412045938491e-07, 1.4186217520257955e-07, 1.3644658280963077e-07, 1.6049604428442446e-07, 1.4524452421370408e-07, 1.3838095282707302e-07, 1.7175496076342794e-07], "accuracy_test": 0.819319993622449, "start": "2016-02-01 09:46:50.512000", "learning_rate_per_epoch": [0.0018492622766643763, 0.0017718150047585368, 0.0016976111801341176, 0.0016265150625258684, 0.001558396383188665, 0.0014931305777281523, 0.0014305980876088142, 0.001370684476569295, 0.0013132800813764334, 0.0012582797789946198, 0.001205582870170474, 0.0011550929630175233, 0.0011067175073549151, 0.0010603680275380611, 0.0010159596567973495, 0.0009734111372381449, 0.0009326445870101452, 0.0008935853256843984, 0.0008561618742533028, 0.0008203057222999632, 0.0007859512115828693, 0.0007530354778282344, 0.0007214982761070132, 0.0006912818644195795, 0.0006623308872804046, 0.0006345923757180572, 0.0006080155726522207, 0.0005825518164783716, 0.0005581544828601182, 0.0005347789265215397, 0.0005123823066242039, 0.0004909237031824887, 0.00047036376781761646, 0.0004506648692768067, 0.000431790977017954, 0.0004137075156904757, 0.00039638139423914254, 0.00037978088948875666, 0.00036387561704032123, 0.0003486364730633795, 0.00033403554698452353, 0.0003200460923835635, 0.000306642526993528, 0.000293800316285342, 0.00028149591526016593, 0.0002697068266570568, 0.0002584114845376462, 0.0002475891960784793, 0.0002372201270190999, 0.00022728531621396542, 0.00021776658832095563, 0.00020864649559371173, 0.000199908361537382, 0.00019153617904521525, 0.000183514624950476, 0.0001758290163706988, 0.00016846528160385787, 0.0001614099310245365, 0.00015465007163584232, 0.0001481733052060008, 0.00014196778647601604, 0.00013602216495200992, 0.00013032554124947637, 0.0001248674961971119, 0.00011963803262915462, 0.00011462757538538426, 0.00010982695675920695, 0.00010522739466978237, 0.00010082045628223568, 9.659808711148798e-05, 9.255254553863779e-05, 8.867643191479146e-05, 8.496265218127519e-05, 8.14044033177197e-05, 7.799518061801791e-05, 7.472873403457925e-05, 7.159908273024485e-05, 6.860050780232996e-05, 6.572750862687826e-05, 6.297483196249232e-05, 6.03374392085243e-05, 5.78104991291184e-05, 5.538938785321079e-05, 5.3069674322614446e-05, 5.084710937808268e-05, 4.871762575930916e-05, 4.667732719099149e-05, 4.472247383091599e-05, 4.284949318389408e-05, 4.105495099793188e-05, 3.933556581614539e-05, 3.7688187148887664e-05, 3.610980274970643e-05, 3.459752042545006e-05, 3.314857167424634e-05, 3.1760304409544915e-05, 3.043017932213843e-05, 2.9155760785215534e-05, 2.7934715035371482e-05, 2.6764806534629315e-05, 2.564389433246106e-05, 2.456992478983011e-05, 2.354093339818064e-05, 2.2555035684490576e-05, 2.1610429030261002e-05, 2.0705381757579744e-05, 1.9838238586089574e-05, 1.9007411538041197e-05, 1.8211378119303845e-05, 1.744868313835468e-05, 1.6717929611331783e-05, 1.6017780581023544e-05, 1.5346953659900464e-05, 1.4704221939609852e-05, 1.408840762451291e-05, 1.3498383850674145e-05, 1.2933070138387848e-05, 1.2391431482683402e-05, 1.1872476534335874e-05, 1.1375255780876614e-05, 1.0898858818109147e-05, 1.0442413440614473e-05, 1.0005083822761662e-05, 9.586069609213155e-06, 9.184604095935356e-06, 8.799951501714531e-06, 8.43140878714621e-06, 8.078300197666977e-06, 7.73997999203857e-06, 7.4158288043690845e-06, 7.105253189365612e-06, 6.807684258092195e-06, 6.522577677969821e-06, 6.249411399039673e-06, 5.987685199215775e-06, 5.736920229537645e-06, 5.496657195180887e-06, 5.266456355457194e-06, 5.045896614319645e-06, 4.8345737013733014e-06, 4.63210108136991e-06, 4.438108135218499e-06, 4.252239705238026e-06, 4.0741556404100265e-06, 3.9035294321365654e-06, 3.7400491237349343e-06, 3.5834154914482497e-06, 3.4333415896981023e-06, 3.289552751084557e-06, 3.1517859042651253e-06, 3.0197886644600658e-06, 2.8933195608260576e-06, 2.7721471269614995e-06, 2.6560492187854834e-06, 2.5448134692851454e-06, 2.438236379020964e-06, 2.3361228613794083e-06, 2.238285787825589e-06, 2.1445462152769323e-06, 2.0547324766084785e-06, 1.9686799532792065e-06, 1.8862314163925475e-06, 1.8072358898280072e-06, 1.7315486502411659e-06, 1.6590312270636787e-06, 1.5895508340690867e-06, 1.5229802556859795e-06, 1.4591976196243195e-06, 1.3980862831886043e-06, 1.339534264843678e-06, 1.2834344715884072e-06, 1.2296841305214912e-06, 1.1781847888414632e-06, 1.1288423138466896e-06, 1.081566324501182e-06, 1.0362701914345962e-06, 9.928710369422333e-07, 9.512895076113637e-07, 9.114493764172948e-07, 8.732777700970473e-07, 8.367048280888412e-07, 8.016635320018395e-07, 7.680897624595673e-07], "accuracy_train_first": 0.43349335403977485, "accuracy_train_last": 0.8763712970653378, "batch_size_eval": 1024, "accuracy_train_std": [0.013588358526392817, 0.01593598748116042, 0.01571666450240414, 0.01398516177416248, 0.014649501778647614, 0.012935412777544508, 0.013962344188994266, 0.013183716529212092, 0.014251975213789625, 0.013641608987574853, 0.013686381724801773, 0.013972181628925868, 0.01332827060367336, 0.013907675029227625, 0.012504096384449485, 0.012845895977339566, 0.011780472481032186, 0.011400973017490294, 0.01153506701582328, 0.011036001288572307, 0.011559572616567315, 0.012010900006790493, 0.012318699487531507, 0.013444858704138309, 0.013223362584440904, 0.012634764797717324, 0.012903298474393195, 0.01300180954773403, 0.012519217186350413, 0.013060197296948219, 0.012801735441656286, 0.012446670097383258, 0.01234238950001706, 0.011121176869671609, 0.01218563974905803, 0.010517302532074772, 0.010182622912212846, 0.01152170619399267, 0.011268352975250542, 0.010737671151455257, 0.01114228998501576, 0.011113564919131088, 0.010933740345395453, 0.011114118389683328, 0.011191038672092545, 0.01143060286332072, 0.010257336595235018, 0.011205577876784927, 0.010973161595491476, 0.01115917193941709, 0.011348679296635439, 0.011317269544638962, 0.010613458994986858, 0.010833342759514558, 0.011002141148331056, 0.009878145713631088, 0.011208109344510532, 0.011049514069113633, 0.011954210657723327, 0.01161692520170411, 0.01171995573480778, 0.01179443484230089, 0.011284623532046009, 0.01055565923895487, 0.011445506164471216, 0.011690957363589344, 0.011558843139408461, 0.011549790524078772, 0.011727997890046474, 0.0114218892561134, 0.011078404887422205, 0.01167178717214916, 0.01199162236526236, 0.01134715905350556, 0.01150639005546657, 0.011694525306920498, 0.011033732020960469, 0.010665701183324819, 0.011770598147416609, 0.011846311289204095, 0.011534204074124556, 0.011356817509699248, 0.011277266489781007, 0.01137068952345159, 0.011419531837106565, 0.011107918249903008, 0.011251409519912013, 0.011646083487352184, 0.011835308844039492, 0.011423081140313707, 0.011318685351673798, 0.011484705084876402, 0.0112752058598623, 0.01111623445463809, 0.010910207519971571, 0.011161214988907472, 0.010618889717077846, 0.011576335799352947, 0.011180931250317352, 0.011132667961305073, 0.011316016533630104, 0.010744248589671221, 0.011308625336414937, 0.011855477517376799, 0.011308994715110217, 0.0112325950671195, 0.011635605762946578, 0.011515749262708933, 0.011561112587494355, 0.011825592460533668, 0.011499140487727896, 0.01140298959200912, 0.011362590221135099, 0.011396156895862859, 0.011300471299471036, 0.011282473207756416, 0.011441907379285436, 0.011297437962649442, 0.011221459521537951, 0.011504411401694515, 0.011516026575139685, 0.01113569320148833, 0.011055289575501365, 0.011150945158328252, 0.011153290803375744, 0.011264171835821252, 0.011043153379228761, 0.011433676301656277, 0.011318109385638422, 0.011656482670667405, 0.010958032477604822, 0.011474117951477448, 0.011152472645245338, 0.011062745748463516, 0.010991869778829617, 0.011226140912285488, 0.010863667929670378, 0.01113364942111731, 0.011200083816929792, 0.011066932862341472, 0.010743897013892214, 0.011058427232528369, 0.01106704639848379, 0.01118993721369371, 0.011251288973814728, 0.011264724702836484, 0.010924766339269976, 0.01129893359071966, 0.011382248723809421, 0.01102427190555106, 0.011284264211038614, 0.011106691655690856, 0.01109184284261246, 0.011252430956785102, 0.010733961564269699, 0.011238636717999913, 0.01148025592806209, 0.010995206396378864, 0.011385502877881487, 0.011294776808891781, 0.01127803647586709, 0.011320750717292547, 0.01121805787923844, 0.011214098070579444, 0.010983076778167407, 0.011193725023670692, 0.011208283029301287, 0.011116481841286165, 0.01129489461711524, 0.010895958778045877, 0.011317054289682722, 0.011094470118163063, 0.010992489924079822, 0.011019789524985163, 0.010951571984927603, 0.011219475442766013, 0.010877051236287138, 0.010987064642617676, 0.011045840074310462, 0.011047496351368124, 0.011009850845297459, 0.011085994689050135, 0.011132532436845633], "accuracy_test_std": 0.005937768704342708, "error_valid": [0.5630103421498494, 0.5028076171875, 0.4628582690135542, 0.42298981080572284, 0.4116269766566265, 0.38089467243975905, 0.3679846338478916, 0.35755865257906627, 0.33786268119352414, 0.3219832454819277, 0.30846285532756024, 0.29843249952936746, 0.2894801863704819, 0.2886462843561747, 0.2730712890625, 0.2633762589420181, 0.2605377564947289, 0.25469897166792166, 0.24968379376882532, 0.24492305158132532, 0.24060941029743976, 0.2407417756965362, 0.23917545180722888, 0.23658108998493976, 0.22998929310993976, 0.23133206654743976, 0.2237225268260542, 0.2207619540662651, 0.22566535673945776, 0.21970450160015065, 0.2181072924510542, 0.2152687900037651, 0.2144451830760542, 0.2131935946912651, 0.2118817065135542, 0.20830048710466864, 0.20868728821536142, 0.21015213196536142, 0.20820930205195776, 0.20932852503765065, 0.2024308170180723, 0.20167780496987953, 0.2046280826430723, 0.20033503153237953, 0.19875841255647586, 0.1957169498305723, 0.19740534403237953, 0.19680528755647586, 0.19667292215737953, 0.19262401167168675, 0.1921254353350903, 0.19338731880647586, 0.1951065982680723, 0.1908032520707832, 0.19082384224397586, 0.1938858951430723, 0.1890942676957832, 0.18796474962349397, 0.1885853962725903, 0.1888295368975903, 0.1882191853350903, 0.18491299181099397, 0.18613369493599397, 0.18725291792168675, 0.18493358198418675, 0.1852894978350903, 0.1870190723832832, 0.1857983692582832, 0.18664256635918675, 0.18210537462349397, 0.1848012165850903, 0.18369228868599397, 0.1829701618975903, 0.1835805134600903, 0.18605280496987953, 0.18422175028237953, 0.18397760965737953, 0.1812611775225903, 0.18273631635918675, 0.1831128223832832, 0.18051846056099397, 0.18064053087349397, 0.1818715290850903, 0.1811391072100903, 0.18114940229668675, 0.18026402484939763, 0.18053905073418675, 0.18015224962349397, 0.18014195453689763, 0.17989781391189763, 0.17953160297439763, 0.17929775743599397, 0.17915509695030118, 0.17989781391189763, 0.18027431993599397, 0.17903302663780118, 0.17965367328689763, 0.17904332172439763, 0.17915509695030118, 0.17916539203689763, 0.17965367328689763, 0.17805646413780118, 0.17990810899849397, 0.17954189806099397, 0.17904332172439763, 0.17866681570030118, 0.17977574359939763, 0.17832119493599397, 0.17917568712349397, 0.17917568712349397, 0.17855504047439763, 0.17891095632530118, 0.17843297016189763, 0.17880947618599397, 0.18051846056099397, 0.17940953266189763, 0.17978603868599397, 0.1793080525225903, 0.17793439382530118, 0.17893154649849397, 0.17879918109939763, 0.17893154649849397, 0.17843297016189763, 0.17817853445030118, 0.17842267507530118, 0.17842267507530118, 0.17831089984939763, 0.17757847797439763, 0.17819912462349397, 0.1784535603350903, 0.17892125141189763, 0.17843297016189763, 0.17745640766189763, 0.1786977009600903, 0.17795498399849397, 0.1780873493975903, 0.1799184040850903, 0.17954189806099397, 0.1786977009600903, 0.1791859822100903, 0.17893154649849397, 0.17868740587349397, 0.1785756306475903, 0.17783291368599397, 0.17805646413780118, 0.17831089984939763, 0.17907420698418675, 0.17807705431099397, 0.17819912462349397, 0.17880947618599397, 0.17844326524849397, 0.17844326524849397, 0.17806675922439763, 0.17832119493599397, 0.17880947618599397, 0.17819912462349397, 0.1783314900225903, 0.17831089984939763, 0.17868740587349397, 0.17832119493599397, 0.1783314900225903, 0.17893154649849397, 0.1784535603350903, 0.17832119493599397, 0.17905361681099397, 0.17868740587349397, 0.17880947618599397, 0.17783291368599397, 0.17856533556099397, 0.17929775743599397, 0.17856533556099397, 0.17868740587349397, 0.17880947618599397, 0.17905361681099397, 0.17794468891189763, 0.17893154649849397, 0.17868740587349397, 0.17868740587349397, 0.17832119493599397, 0.17795498399849397, 0.17844326524849397, 0.1788197712725903, 0.17783291368599397], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.041880111262501386, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 32, "valid_ratio": 0.15, "learning_rate": 0.0019300949364939482, "optimization": "nesterov_momentum", "nb_data_augmentation": 4, "learning_rate_decay_method": "exp", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 1.817250678729546e-05, "rotation_range": [0, 0], "momentum": 0.7685172429024987}, "accuracy_valid_max": 0.8225435923381024, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.822167086314006, "accuracy_valid_std": [0.015603078700029747, 0.01389190190800097, 0.01499549177559035, 0.013723942036616983, 0.010996172267368343, 0.009480560934568108, 0.016904914519571584, 0.011819137062721518, 0.013747920985470981, 0.011733731204182203, 0.016130364022510377, 0.015732765472956916, 0.014288506022396254, 0.013167586434214016, 0.01775986708818997, 0.015677944435052785, 0.017552110052826236, 0.014625020570200514, 0.013559390791936715, 0.012751172632216506, 0.01618840308642257, 0.012339546963807556, 0.01230401166158684, 0.013907776059472791, 0.012236550077006263, 0.012161678299082176, 0.014480955924157757, 0.015514528685518862, 0.013019084697641256, 0.011285350823318102, 0.01213369983668575, 0.012032144034283179, 0.010306314199451695, 0.011880961784200544, 0.01205853189509198, 0.012689285639304587, 0.011431009470761859, 0.009859314051666733, 0.010006634709000381, 0.010655280139673255, 0.012559770053569745, 0.012229807328364034, 0.012827841799254325, 0.015355422460402963, 0.010609998549938361, 0.012108818216701461, 0.01265208683384662, 0.009990326393679852, 0.0132828019083574, 0.01291056367316266, 0.013413787526741593, 0.012181814935569118, 0.011667691973022802, 0.01263237561808434, 0.013019439907697047, 0.011258385946954979, 0.012811401221102324, 0.013898417641206714, 0.012477227167150412, 0.01483845971055964, 0.013221932354364524, 0.011845546848360597, 0.011858028882399963, 0.012477331528700435, 0.011607498703983999, 0.01257189869491673, 0.011775679806109999, 0.011621045769249239, 0.012426327403314842, 0.013931343961613912, 0.012008806472703805, 0.013226845169425646, 0.012389753319194842, 0.012496385506515176, 0.010914453467853453, 0.012963442197159374, 0.012274155566216433, 0.012720274977345616, 0.01245415145112939, 0.011737564093100623, 0.012257363044816533, 0.011787226666706044, 0.012241565298043272, 0.011857394804227166, 0.010951927384900233, 0.012189366914286907, 0.01186540548456179, 0.011447616314235238, 0.011495060961100633, 0.011356093667127226, 0.010680806628792856, 0.011106908138788391, 0.011738434338665088, 0.012038891484533439, 0.010284278961501761, 0.01126045793915733, 0.011092496860222515, 0.011323979062091837, 0.011929827900103227, 0.01115531102969082, 0.011810773410410267, 0.011753970793156227, 0.011360874787988466, 0.010551528809175464, 0.01117562712742656, 0.01163458496196223, 0.011570863807722055, 0.011359555189086486, 0.011351484125787743, 0.010945148964365525, 0.01134881114244722, 0.012578302351017067, 0.012510728070492385, 0.011089761062837272, 0.011515206205662078, 0.011354639116988835, 0.011367833729232753, 0.011424572482283889, 0.013471916545002715, 0.012517191015404121, 0.011983070691646112, 0.012411988823149837, 0.012529770708676564, 0.012831444375891263, 0.011371981105651472, 0.01306018255813694, 0.013162221028368217, 0.012982501656378364, 0.012735337950922633, 0.011981926127717448, 0.01209542689829313, 0.012978414610763682, 0.012621936143238751, 0.012376161145248905, 0.012284627174222934, 0.012613436095048882, 0.013312488584367496, 0.012955306382248552, 0.011298609504239956, 0.012113834626819536, 0.012459918115361122, 0.012642295164929682, 0.011449782241944775, 0.01303477273702192, 0.013736832537957972, 0.012625996388868797, 0.011037434155431737, 0.012412169626925056, 0.013445649180836608, 0.012599428054322783, 0.01293777132552876, 0.012333983047222587, 0.012756846317542307, 0.013248815683626458, 0.012370268444848662, 0.012810003175527428, 0.012754666281979242, 0.013351061851431568, 0.012013759586224575, 0.012258007490368929, 0.011793136674931037, 0.011710241134570436, 0.012276771200243366, 0.012584314813425238, 0.012433211661263696, 0.013622658829324516, 0.013136737223677218, 0.011551071712324827, 0.01238906326175699, 0.012289018007117777, 0.013271723833717502, 0.01282949720835683, 0.01250445526622573, 0.012122514495043245, 0.013222840600457692, 0.012373511717761866, 0.012288455542111328, 0.012063271210211212, 0.012422232628584417, 0.012049482101369297, 0.01273344879172331, 0.011859204008122516, 0.012859834653014796], "accuracy_valid": [0.4369896578501506, 0.4971923828125, 0.5371417309864458, 0.5770101891942772, 0.5883730233433735, 0.619105327560241, 0.6320153661521084, 0.6424413474209337, 0.6621373188064759, 0.6780167545180723, 0.6915371446724398, 0.7015675004706325, 0.7105198136295181, 0.7113537156438253, 0.7269287109375, 0.7366237410579819, 0.7394622435052711, 0.7453010283320783, 0.7503162062311747, 0.7550769484186747, 0.7593905897025602, 0.7592582243034638, 0.7608245481927711, 0.7634189100150602, 0.7700107068900602, 0.7686679334525602, 0.7762774731739458, 0.7792380459337349, 0.7743346432605422, 0.7802954983998494, 0.7818927075489458, 0.7847312099962349, 0.7855548169239458, 0.7868064053087349, 0.7881182934864458, 0.7916995128953314, 0.7913127117846386, 0.7898478680346386, 0.7917906979480422, 0.7906714749623494, 0.7975691829819277, 0.7983221950301205, 0.7953719173569277, 0.7996649684676205, 0.8012415874435241, 0.8042830501694277, 0.8025946559676205, 0.8031947124435241, 0.8033270778426205, 0.8073759883283133, 0.8078745646649097, 0.8066126811935241, 0.8048934017319277, 0.8091967479292168, 0.8091761577560241, 0.8061141048569277, 0.8109057323042168, 0.812035250376506, 0.8114146037274097, 0.8111704631024097, 0.8117808146649097, 0.815087008189006, 0.813866305064006, 0.8127470820783133, 0.8150664180158133, 0.8147105021649097, 0.8129809276167168, 0.8142016307417168, 0.8133574336408133, 0.817894625376506, 0.8151987834149097, 0.816307711314006, 0.8170298381024097, 0.8164194865399097, 0.8139471950301205, 0.8157782497176205, 0.8160223903426205, 0.8187388224774097, 0.8172636836408133, 0.8168871776167168, 0.819481539439006, 0.819359469126506, 0.8181284709149097, 0.8188608927899097, 0.8188505977033133, 0.8197359751506024, 0.8194609492658133, 0.819847750376506, 0.8198580454631024, 0.8201021860881024, 0.8204683970256024, 0.820702242564006, 0.8208449030496988, 0.8201021860881024, 0.819725680064006, 0.8209669733621988, 0.8203463267131024, 0.8209566782756024, 0.8208449030496988, 0.8208346079631024, 0.8203463267131024, 0.8219435358621988, 0.820091891001506, 0.820458101939006, 0.8209566782756024, 0.8213331842996988, 0.8202242564006024, 0.821678805064006, 0.820824312876506, 0.820824312876506, 0.8214449595256024, 0.8210890436746988, 0.8215670298381024, 0.821190523814006, 0.819481539439006, 0.8205904673381024, 0.820213961314006, 0.8206919474774097, 0.8220656061746988, 0.821068453501506, 0.8212008189006024, 0.821068453501506, 0.8215670298381024, 0.8218214655496988, 0.8215773249246988, 0.8215773249246988, 0.8216891001506024, 0.8224215220256024, 0.821800875376506, 0.8215464396649097, 0.8210787485881024, 0.8215670298381024, 0.8225435923381024, 0.8213022990399097, 0.822045016001506, 0.8219126506024097, 0.8200815959149097, 0.820458101939006, 0.8213022990399097, 0.8208140177899097, 0.821068453501506, 0.821312594126506, 0.8214243693524097, 0.822167086314006, 0.8219435358621988, 0.8216891001506024, 0.8209257930158133, 0.821922945689006, 0.821800875376506, 0.821190523814006, 0.821556734751506, 0.821556734751506, 0.8219332407756024, 0.821678805064006, 0.821190523814006, 0.821800875376506, 0.8216685099774097, 0.8216891001506024, 0.821312594126506, 0.821678805064006, 0.8216685099774097, 0.821068453501506, 0.8215464396649097, 0.821678805064006, 0.820946383189006, 0.821312594126506, 0.821190523814006, 0.822167086314006, 0.821434664439006, 0.820702242564006, 0.821434664439006, 0.821312594126506, 0.821190523814006, 0.820946383189006, 0.8220553110881024, 0.821068453501506, 0.821312594126506, 0.821312594126506, 0.821678805064006, 0.822045016001506, 0.821556734751506, 0.8211802287274097, 0.822167086314006], "seed": 356849434, "model": "residualv3", "loss_std": [0.280922532081604, 0.16558478772640228, 0.17242226004600525, 0.1746048480272293, 0.17385725677013397, 0.17465947568416595, 0.17311722040176392, 0.17130927741527557, 0.17127498984336853, 0.16862043738365173, 0.1678241789340973, 0.16656839847564697, 0.16511453688144684, 0.1650223582983017, 0.16194365918636322, 0.16324585676193237, 0.1612546294927597, 0.15968510508537292, 0.1587810218334198, 0.15864914655685425, 0.15749049186706543, 0.15683585405349731, 0.15657269954681396, 0.15802942216396332, 0.1566062569618225, 0.15535478293895721, 0.15390916168689728, 0.15445303916931152, 0.1524858921766281, 0.15191146731376648, 0.15167252719402313, 0.15119726955890656, 0.1511489897966385, 0.15193985402584076, 0.14856983721256256, 0.14945663511753082, 0.14851780235767365, 0.1486498862504959, 0.14726199209690094, 0.14657318592071533, 0.14563149213790894, 0.14498351514339447, 0.14493264257907867, 0.14369581639766693, 0.14224539697170258, 0.1424768716096878, 0.14266547560691833, 0.143172025680542, 0.14257608354091644, 0.14042928814888, 0.14116553962230682, 0.14174753427505493, 0.13894201815128326, 0.13948804140090942, 0.1396826058626175, 0.13894541561603546, 0.13989463448524475, 0.13953527808189392, 0.13800734281539917, 0.13697496056556702, 0.1373298466205597, 0.1365687996149063, 0.13727016746997833, 0.1372271627187729, 0.1376333385705948, 0.13778522610664368, 0.13668890297412872, 0.13568833470344543, 0.13458846509456635, 0.13440150022506714, 0.1332014799118042, 0.1321917027235031, 0.13391557335853577, 0.13605348765850067, 0.13460752367973328, 0.13594280183315277, 0.1349492073059082, 0.13587366044521332, 0.13404616713523865, 0.13420791923999786, 0.13393859565258026, 0.13336560130119324, 0.13305111229419708, 0.13256262242794037, 0.13302436470985413, 0.13294531404972076, 0.13399748504161835, 0.13301584124565125, 0.13277220726013184, 0.13313311338424683, 0.1322087198495865, 0.1321146935224533, 0.13387174904346466, 0.13242705166339874, 0.13074886798858643, 0.13280349969863892, 0.13289381563663483, 0.13231752812862396, 0.13291412591934204, 0.13276100158691406, 0.13096705079078674, 0.13059717416763306, 0.13377529382705688, 0.13166004419326782, 0.1306445151567459, 0.1319623440504074, 0.13164229691028595, 0.13156117498874664, 0.1302793025970459, 0.129942387342453, 0.13177739083766937, 0.13172033429145813, 0.13160373270511627, 0.13169972598552704, 0.1315324306488037, 0.13149061799049377, 0.13295917212963104, 0.1326337307691574, 0.13192546367645264, 0.1332177072763443, 0.13222427666187286, 0.12976501882076263, 0.13127528131008148, 0.13200370967388153, 0.13063232600688934, 0.13119149208068848, 0.13037492334842682, 0.13006353378295898, 0.13076116144657135, 0.13009144365787506, 0.13034331798553467, 0.13005557656288147, 0.13088971376419067, 0.13027437031269073, 0.1298728585243225, 0.13046731054782867, 0.130585178732872, 0.13115091621875763, 0.1302172690629959, 0.12995515763759613, 0.13161487877368927, 0.13076044619083405, 0.13146086037158966, 0.12965811789035797, 0.13024847209453583, 0.1309518963098526, 0.1315240114927292, 0.1299312561750412, 0.13043127954006195, 0.12981416285037994, 0.13131143152713776, 0.13058598339557648, 0.1306873857975006, 0.13040398061275482, 0.1307085007429123, 0.1307699978351593, 0.1301766037940979, 0.12898096442222595, 0.12937143445014954, 0.13122378289699554, 0.12970715761184692, 0.1319657266139984, 0.12945891916751862, 0.12990231812000275, 0.13135822117328644, 0.12881390750408173, 0.13097965717315674, 0.1300916075706482, 0.13107647001743317, 0.13002339005470276, 0.1306404322385788, 0.12910735607147217, 0.13152576982975006, 0.1286919265985489, 0.13096551597118378, 0.13011173903942108, 0.12995857000350952, 0.12923942506313324, 0.1292499452829361, 0.12921085953712463, 0.12954279780387878, 0.1296970397233963, 0.1296994984149933]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:34 2016", "state": "available"}], "summary": "88a86ffba22a38553adba1abe7582351"}