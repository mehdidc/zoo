{"content": {"hp_model": {"f0": 64, "f1": 32, "f2": 16, "f3": 64, "nonlin": "rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.5071624517440796, 1.1218674182891846, 0.9065253138542175, 0.7849831581115723, 0.699554979801178, 0.6348904967308044, 0.5868086218833923, 0.5463643670082092, 0.5090647339820862, 0.47868409752845764, 0.44823768734931946, 0.42490333318710327, 0.4017016887664795, 0.3813953697681427, 0.36029258370399475, 0.3443669080734253, 0.32910674810409546, 0.3152832090854645, 0.30078330636024475, 0.2897937595844269, 0.27692627906799316, 0.2664955258369446, 0.25921472907066345, 0.2495620995759964, 0.2439073771238327, 0.23451614379882812, 0.22521604597568512, 0.22138091921806335, 0.2129562944173813, 0.20626124739646912, 0.1986091583967209, 0.19691166281700134, 0.19151902198791504, 0.18555870652198792, 0.18150649964809418, 0.1758430153131485, 0.1736377328634262, 0.16908366978168488, 0.16774962842464447, 0.16548733413219452, 0.15890635550022125, 0.15609918534755707, 0.1535259336233139, 0.1486981064081192, 0.14666339755058289, 0.14641927182674408, 0.14336764812469482, 0.14102794229984283, 0.1349819302558899, 0.13397909700870514, 0.13047508895397186, 0.131074458360672, 0.12831741571426392, 0.1275903582572937, 0.12335845082998276, 0.11899420619010925, 0.117795430123806, 0.1154184564948082, 0.11387071758508682, 0.11453764140605927, 0.11122142523527145, 0.11157065629959106, 0.10535265505313873, 0.10713879764080048, 0.10535193979740143, 0.10415954142808914, 0.1013624295592308, 0.1006564050912857, 0.09927602112293243, 0.09772224724292755, 0.09569622576236725, 0.09461021423339844, 0.09586683660745621, 0.09248104691505432, 0.09104502946138382, 0.08865612745285034, 0.08685988932847977, 0.08785408735275269, 0.08769187331199646, 0.08454862236976624, 0.08541768789291382, 0.08264919370412827, 0.081308513879776, 0.08404286950826645, 0.07888469099998474, 0.07941583544015884, 0.07828213274478912, 0.07756775617599487, 0.07564859837293625, 0.07516292482614517, 0.07364068180322647, 0.07330892235040665, 0.07328605651855469, 0.07099409401416779, 0.0715387612581253, 0.07040205597877502, 0.06890170276165009, 0.06891245394945145, 0.06679122895002365, 0.06616264581680298, 0.066880963742733, 0.06437686830759048, 0.06653789430856705, 0.06395004689693451, 0.06341864168643951, 0.06255646049976349, 0.06319592148065567, 0.06265805661678314, 0.060747552663087845, 0.06015707179903984, 0.058641254901885986, 0.05864766240119934, 0.05628325417637825, 0.05719156190752983, 0.056590501219034195, 0.057370152324438095, 0.057012517005205154, 0.05537775903940201, 0.056366462260484695, 0.05421331152319908, 0.05602142959833145, 0.053436726331710815, 0.0548446886241436, 0.05319724977016449, 0.0545734241604805, 0.051571324467659, 0.04950232431292534, 0.050823912024497986, 0.049918510019779205, 0.04920680820941925, 0.05109131708741188, 0.047702789306640625, 0.04933631047606468, 0.048232801258563995, 0.048258502036333084, 0.04837321490049362, 0.046463217586278915, 0.047327734529972076, 0.047375526279211044, 0.046005021780729294, 0.04481770470738411, 0.04323061183094978, 0.04665284976363182, 0.04388546571135521, 0.044589925557374954, 0.045607417821884155, 0.0431986078619957, 0.042084384709596634, 0.04275976121425629, 0.04218936711549759, 0.041738271713256836, 0.04115530475974083, 0.04161150008440018, 0.04107789322733879, 0.04111502319574356, 0.04085575044155121, 0.04017363861203194, 0.039584048092365265, 0.0401974692940712, 0.038852840662002563, 0.03949299454689026, 0.03930703550577164, 0.037993885576725006, 0.03929311782121658, 0.037438515573740005, 0.038679931312799454, 0.036856137216091156, 0.03769820183515549, 0.03686138987541199, 0.03691114857792854], "moving_avg_accuracy_train": [0.06036463019679769, 0.12529080762331576, 0.1895898886307793, 0.25059961660278807, 0.3079726690275037, 0.3611682305727507, 0.41023456795867425, 0.4553310542345122, 0.49748246468984464, 0.5362722800056516, 0.5718664551981467, 0.6046916913689928, 0.6345691532536882, 0.6619749798879909, 0.6874072706242749, 0.7112311142059965, 0.7328027817628793, 0.7523614057414455, 0.7706035111471367, 0.7871308322015813, 0.8025773356600868, 0.8167348109465145, 0.8293974836447755, 0.8411660210291906, 0.8520668413203745, 0.8615101700217165, 0.8703813338577231, 0.8788604938112827, 0.8868126083052006, 0.8938113651818602, 0.900505521668473, 0.9065232870599959, 0.9117000379267168, 0.9167472333138624, 0.9215919424587128, 0.926066076931926, 0.9299254953899885, 0.9334152119950926, 0.9369767728253914, 0.9399404341941073, 0.942444969009285, 0.9449570337143458, 0.9474783807132047, 0.9496405640693021, 0.9513656039040662, 0.9533250407970206, 0.9549305680769069, 0.9568498369371287, 0.9583469891791855, 0.9599153153339414, 0.9612314056743937, 0.9624787020474767, 0.9637546925558611, 0.9651123834550831, 0.9661506185084304, 0.9667664486207196, 0.9673090699777321, 0.9682531223168913, 0.9693631500399825, 0.9702134736134114, 0.9712065573151931, 0.9716982621468429, 0.9722453560941187, 0.9728027366668774, 0.9734044326788073, 0.9740761674228775, 0.9747364962151505, 0.9749214577912637, 0.9754599109704707, 0.9759050273508137, 0.9764986554931318, 0.9765539762152747, 0.9769153708544985, 0.9774405527786001, 0.9778248969043484, 0.9780661028234559, 0.9786295992744528, 0.9789438668756159, 0.9789197880738054, 0.9793421845259763, 0.9794037959460253, 0.9798475460752599, 0.9797912280737232, 0.9800242460759209, 0.9803756882088234, 0.9800294629129964, 0.9801989857062299, 0.9803376774249203, 0.9803253522407984, 0.9805258120679368, 0.9807155986052369, 0.9811932900828455, 0.9813696990948174, 0.9811100846151529, 0.9814459488465131, 0.9818342571606897, 0.9820675132517913, 0.9819077811218873, 0.9820477264085542, 0.9823991805522595, 0.9823737645042318, 0.9822857498455213, 0.9824924578586159, 0.9825878142668296, 0.9823832437747627, 0.9826314998640178, 0.9826152958216913, 0.9824148444741105, 0.9824505689541174, 0.9828151812170667, 0.9829365382049484, 0.9831944248225765, 0.9829522284701177, 0.9831457670433716, 0.983008381818824, 0.9829244068417681, 0.9831601469611627, 0.9836281875841217, 0.9838052474709661, 0.9840832560560493, 0.9840126292957286, 0.9841536062090406, 0.9841758537345927, 0.9842447046325897, 0.9845694843539007, 0.9844315975244998, 0.9845656629935338, 0.9846373495953893, 0.9844345475216016, 0.9844008351790022, 0.984765733319463, 0.9848918176506305, 0.9850100159439377, 0.9849187207102859, 0.9853294865476182, 0.9856548898273801, 0.9858502046756222, 0.985728513186696, 0.9859351389871202, 0.9861396313003407, 0.9863143737870009, 0.9865273734987863, 0.9867237235370122, 0.9868376595535583, 0.9869309374220305, 0.9867869868715033, 0.9869830243069998, 0.9870199851191939, 0.986964822097769, 0.9867640767546865, 0.9869437679625697, 0.9869753177651499, 0.9872989704374628, 0.9867695163616014, 0.9869091360790312, 0.9871649661092325, 0.9871254598256903, 0.9871226004491105, 0.9871897454256556, 0.9868943920878704, 0.9870703163088546, 0.9870985118720443, 0.98725867441223, 0.9873865446567305, 0.9875924528756275, 0.9874707424833212, 0.9876518106826174, 0.9877078512655646, 0.9878653167330926, 0.987930269694335], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.05928072642131022, 0.12272697724491713, 0.18498217626364827, 0.24298336446371416, 0.2971397649337584, 0.3466648638827771, 0.3918792780498458, 0.4333474708209154, 0.47165055435817627, 0.5063186420417111, 0.5380957104429617, 0.5670246618478372, 0.5927758383588366, 0.616357817775965, 0.6379122186037902, 0.6576895973175829, 0.6753092212172253, 0.6914110233519033, 0.7063207728615924, 0.7196917488039724, 0.7316085573102468, 0.7423123594293727, 0.7522010994841763, 0.761298337050819, 0.7699650136055263, 0.7776613362754706, 0.7845280210308302, 0.7907486293663767, 0.7968863452606878, 0.8020593741852967, 0.8068117269587851, 0.8115069720434036, 0.8152860329339428, 0.818662773672928, 0.8226723728587225, 0.8260979066571876, 0.8287159903796466, 0.8314018555736097, 0.8339066424842457, 0.8362383109259567, 0.8381698850479694, 0.8400090465337599, 0.8415533990810616, 0.8426329930490849, 0.8441468845386041, 0.8459031004051504, 0.8471011586816535, 0.8487979987622833, 0.8497655433420038, 0.8506994276373215, 0.8521818956887249, 0.8532657992580301, 0.8543399982290645, 0.8555488589106761, 0.8568037609996536, 0.8574773946888449, 0.8577133360369784, 0.8586408976234161, 0.8596435129627311, 0.8602477504748164, 0.8614345659299703, 0.8619980935237201, 0.8624829133129144, 0.8628520389149814, 0.8633338248578206, 0.8640156908660145, 0.8647758546483889, 0.8645871257818181, 0.8653613292427929, 0.8657519070677605, 0.8662163789174604, 0.8663882039398709, 0.8667493364826309, 0.8670896513283437, 0.8672178296913677, 0.8678307370460261, 0.8679855516132609, 0.867997666867974, 0.8679363579183754, 0.868209740198827, 0.8684029851642606, 0.8686613253432411, 0.8685915971763869, 0.8687812185204952, 0.8690779190046656, 0.8691791985462172, 0.8691635753783424, 0.8697242745046648, 0.869676498786126, 0.8700292731827393, 0.8704260423064835, 0.8706610642053532, 0.8706427093379052, 0.8704288184398827, 0.8703807419893432, 0.8706457374910866, 0.8706878914339959, 0.8707125934427048, 0.8708732206289312, 0.871371789002785, 0.8713688403830034, 0.8710304197294922, 0.8713158966759708, 0.8712289700354822, 0.8711008784253826, 0.8717252244119106, 0.8721915385671051, 0.8718939479163433, 0.8721571957263656, 0.8724032372606568, 0.872764099528567, 0.8731753542970959, 0.8730001381444947, 0.8728017044787652, 0.8726414982628163, 0.8727414532934624, 0.8726615438922035, 0.873338372371959, 0.8731600909517812, 0.8739356081178982, 0.874458557095265, 0.8746606564873951, 0.8747082685965623, 0.8747897996058819, 0.87494759722436, 0.874522944100192, 0.8749606863995102, 0.8753567134862158, 0.8751251413469315, 0.8750032051489853, 0.8755445532617223, 0.875246398528547, 0.8749434971922586, 0.8750635700069183, 0.8754726932867235, 0.875851052252479, 0.876013617396207, 0.875950347476993, 0.8761162196381792, 0.8764241959894968, 0.8763829623845231, 0.876613377318887, 0.876734272032405, 0.8771401640592097, 0.8770221861039664, 0.8770777858764764, 0.877446237992895, 0.8772498540279429, 0.8769561856902841, 0.8767498308166624, 0.8770443036837913, 0.877380512434388, 0.8773382149089461, 0.8770549770023889, 0.8771815693812163, 0.8770991605135013, 0.8770402880897867, 0.8769465637800551, 0.8769629566772755, 0.876596203790045, 0.8765468879102876, 0.8766011893771654, 0.8767863970584248, 0.8765929030133504, 0.8765958338622413, 0.8762637342391948, 0.8764663623683627, 0.8765094498702313, 0.8766529443598647, 0.8766498710741042], "moving_var_accuracy_train": [0.03279499720916527, 0.06745417412522609, 0.0979181030783427, 0.12162597493547506, 0.1390883817426901, 0.15064745348144856, 0.15725025731352718, 0.15982846925201608, 0.15983629495717966, 0.1573945134115714, 0.1530575698391205, 0.14744927802225485, 0.14073831477807222, 0.13342419730185007, 0.12590299028051893, 0.1184208709595241, 0.11076681543423372, 0.10313299183822489, 0.09581466234109354, 0.0886915671781144, 0.08196976068214558, 0.07557669157230332, 0.06946211193384257, 0.06376238698997368, 0.0584555992381625, 0.053412627427000564, 0.04877964261454796, 0.0445487437347556, 0.040662994485599444, 0.037037538417406826, 0.03373709015527149, 0.030689302642511043, 0.027861561124084805, 0.02530467264316054, 0.02298544623912826, 0.02086706252877499, 0.018914412273407392, 0.01713257414392211, 0.015533479169461168, 0.014059180850890826, 0.012709717017565677, 0.01149553953755082, 0.010403200299993632, 0.00940495560178273, 0.008491241903488166, 0.0076766722495765875, 0.006932204485237062, 0.0062721363733337105, 0.005665095919523401, 0.005120723149920283, 0.0046242396789863425, 0.004175817445268461, 0.003772889066739001, 0.0034121900812655714, 0.003080672461373007, 0.002776018435780525, 0.0025010665336362477, 0.0022589809936442713, 0.002044172348194124, 0.001846262564990471, 0.0016705122456401257, 0.0015056369838493228, 0.0013577670915487037, 0.0012247864403198325, 0.0011055661391047996, 0.000999070573291839, 0.0009030878229877974, 0.0008130869377507623, 0.0007343876304114689, 0.0006627320246987698, 0.0005996303715710606, 0.0005396948778546404, 0.0004869008448365132, 0.00044069310483348996, 0.00039795327801311626, 0.0003586815728705168, 0.0003256711698360409, 0.0002939929299787039, 0.0002645988550791031, 0.00023974473843645238, 0.00021580442829653135, 0.0001959962130616404, 0.00017642513721115012, 0.00015927129999416872, 0.00014445577414976378, 0.00013108904433402162, 0.00011823878169745034, 0.00010658802206320503, 9.593058704835724e-05, 8.669918562418844e-05, 7.835343742943279e-05, 7.257179601650731e-05, 6.559469767040103e-05, 5.964182500582396e-05, 5.469288554240683e-05, 5.05806471098947e-05, 4.601225803522911e-05, 4.164066141161957e-05, 3.765285741980025e-05, 3.499925181396892e-05, 3.1505140412048156e-05, 2.842434559217444e-05, 2.5966464857054656e-05, 2.3451653972635972e-05, 2.14831303513927e-05, 1.988949708892312e-05, 1.7902910518920273e-05, 1.6474246151750545e-05, 1.4838307682821305e-05, 1.455095583517615e-05, 1.3228407918228116e-05, 1.2504116694370222e-05, 1.178163668323243e-05, 1.0940587628943989e-05, 1.0016401165365457e-05, 9.078227219772743e-06, 8.670565132825184e-06, 9.775066842201576e-06, 9.079711989745516e-06, 8.867339751190457e-06, 8.025499029531982e-06, 7.401819537361506e-06, 6.666092155164119e-06, 6.042146955042626e-06, 6.38726906591195e-06, 5.919657158820513e-06, 5.48945339282413e-06, 4.9867587735120955e-06, 4.858241026354309e-06, 4.382645622110754e-06, 5.142736936105428e-06, 4.771538569588248e-06, 4.420122241496048e-06, 4.053123394534254e-06, 5.16636821315564e-06, 5.602717042158852e-06, 5.385776347437318e-06, 4.980478078987234e-06, 4.866678263696848e-06, 4.756364392823365e-06, 4.555542383339007e-06, 4.508308039990774e-06, 4.404457273593298e-06, 4.080844289031511e-06, 3.7510667068487254e-06, 3.562455885137504e-06, 3.552086381667786e-06, 3.2091726582434193e-06, 2.915642022813432e-06, 2.9867660554560753e-06, 2.9786898216250025e-06, 2.6897793498481735e-06, 3.363560885521435e-06, 5.550099362986075e-06, 5.170532416143929e-06, 5.2425202137049675e-06, 4.732314910288396e-06, 4.259157003569378e-06, 3.873817334089754e-06, 4.271537947948497e-06, 4.122928136913379e-06, 3.7177902312743773e-06, 3.5768795616557126e-06, 3.3663488003477467e-06, 3.411297671796639e-06, 3.2034886809749226e-06, 3.1782110480445838e-06, 2.888654865673677e-06, 2.822947740280826e-06, 2.578622950819958e-06], "duration": 111044.051172, "accuracy_train": [0.6036463019679771, 0.7096264044619786, 0.7682816176979512, 0.7996871683508674, 0.8243301408499446, 0.839928284479974, 0.851831604431986, 0.8611994307170543, 0.8768451587878369, 0.8853806178479143, 0.8922140319306018, 0.9001188169066077, 0.9034663102159468, 0.9086274195967147, 0.9162978872508305, 0.9256457064414912, 0.9269477897748246, 0.9283890215485419, 0.9347824597983574, 0.9358767216915835, 0.9415958667866371, 0.9441520885243633, 0.9433615379291252, 0.9470828574889257, 0.95017422394103, 0.9465001283337948, 0.9502218083817828, 0.9551729333933187, 0.9583816387504615, 0.9568001770717978, 0.9607529300479882, 0.9606831755837025, 0.9582907957272055, 0.9621719917981728, 0.9651943247623662, 0.9663332871908453, 0.9646602615125508, 0.96482266144103, 0.9690308202980805, 0.9666133865125508, 0.9649857823458842, 0.967565616059893, 0.9701705037029347, 0.9691002142741787, 0.9668909624169435, 0.9709599728336102, 0.9693803135958842, 0.9741232566791252, 0.9718213593576966, 0.9740302507267442, 0.9730762187384644, 0.9737043694052234, 0.9752386071313216, 0.9773316015480805, 0.9754947339885567, 0.9723089196313216, 0.9721926621908453, 0.9767495933693245, 0.9793533995478036, 0.977866385774271, 0.9801443106312293, 0.9761236056316908, 0.9771692016196014, 0.9778191618217055, 0.9788196967861758, 0.9801217801195091, 0.9806794553456073, 0.9765861119762828, 0.9803059895833334, 0.9799110747739018, 0.9818413087739941, 0.9770518627145626, 0.9801679226075121, 0.982167190095515, 0.9812839940360835, 0.9802369560954227, 0.9837010673334257, 0.9817722752860835, 0.9787030788575121, 0.983143752595515, 0.9799582987264673, 0.9838412972383721, 0.979284366059893, 0.9821214080956996, 0.9835386674049464, 0.9769134352505537, 0.9817246908453304, 0.9815859028931341, 0.9802144255837025, 0.9823299505121816, 0.9824236774409376, 0.9854925133813216, 0.9829573802025655, 0.9787735542981728, 0.984468726928756, 0.9853290319882798, 0.9841668180717055, 0.9804701919527501, 0.9833072339885567, 0.9855622678456073, 0.9821450200719823, 0.9814936179171282, 0.9843528299764673, 0.983446021940753, 0.9805421093461609, 0.9848658046673128, 0.982469459440753, 0.9806107823458842, 0.9827720892741787, 0.9860966915836102, 0.9840287510958842, 0.9855154043812293, 0.9807724612979882, 0.9848876142026578, 0.9817719147978959, 0.9821686320482651, 0.9852818080357143, 0.987840553190753, 0.9853987864525655, 0.9865853333217978, 0.9833769884528424, 0.9854223984288483, 0.9843760814645626, 0.9848643627145626, 0.9874925018456996, 0.983190616059893, 0.9857722522148394, 0.9852825290120893, 0.9826093288575121, 0.9840974240956073, 0.9880498165836102, 0.986026576631137, 0.9860738005837025, 0.9840970636074198, 0.9890263790836102, 0.9885835193452381, 0.9876080383098007, 0.9846332897863603, 0.9877947711909376, 0.9879800621193245, 0.9878870561669435, 0.9884443709048542, 0.9884908738810447, 0.9878630837024732, 0.9877704382382798, 0.9854914319167589, 0.9887473612264673, 0.9873526324289406, 0.9864683549049464, 0.9849573686669435, 0.988560988833518, 0.9872592659883721, 0.9902118444882798, 0.9820044296788483, 0.9881657135358989, 0.9894674363810447, 0.9867699032738095, 0.987096866059893, 0.9877940502145626, 0.9842362120478036, 0.9886536342977114, 0.987352271940753, 0.9887001372739018, 0.9885373768572352, 0.9894456268456996, 0.9863753489525655, 0.9892814244762828, 0.9882122165120893, 0.9892825059408453, 0.988514846345515], "end": "2016-02-02 18:12:42.195000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0], "moving_var_accuracy_valid": [0.03162784072534406, 0.06469389734495803, 0.09310589585421866, 0.11407254676237197, 0.12906153349298105, 0.13822999897687382, 0.1428060883154274, 0.14400197858917202, 0.14280591660641628, 0.13934221167847397, 0.13449602919622639, 0.1285783843410746, 0.12168865373227299, 0.11452477613811449, 0.107253628279722, 0.10004856783084881, 0.09283777136504755, 0.0858874065164017, 0.07929937153873662, 0.07297848136372828, 0.06695872615213239, 0.06129399595516779, 0.05604468097849433, 0.05118505046274029, 0.04674254695900148, 0.042601392706860426, 0.03876561567193977, 0.03523731781732423, 0.03205263004318532, 0.029088209093160358, 0.026382651895797467, 0.023942794643859427, 0.021677046890403107, 0.0196119636035277, 0.01779545921385145, 0.016121521828706145, 0.01457105890723577, 0.013178877863073474, 0.011917455693975367, 0.010774640224076468, 0.009730755008968285, 0.00878812214280879, 0.007930775151641126, 0.007148187344699145, 0.00645399541720758, 0.005836354523016, 0.005265637163419477, 0.004764986843410616, 0.0042969134416932725, 0.003875071356417315, 0.003507343624486469, 0.0031671828845657958, 0.0028608497269735573, 0.00258791685160412, 0.002343298179719991, 0.002113052402872913, 0.0019022481774634533, 0.0017197666941868213, 0.0015568371624358071, 0.001404439372931326, 0.001276672213959523, 0.0011518630627038293, 0.0010387922084853956, 0.0009361392710277679, 0.0008446144031774483, 0.0007643374341378753, 0.0006931043315083923, 0.000624114465623247, 0.0005670975380517909, 0.0005117607435828203, 0.0004625262761170107, 0.00041653936305024726, 0.00037605917716618454, 0.00033949558719747894, 0.0003056938957124588, 0.0002785054049697623, 0.00025087057242483875, 0.00022578483619692575, 0.00020324018166294116, 0.0001835888043380314, 0.00016556601645421705, 0.00014961007164147677, 0.00013469282263260478, 0.00012154714665661724, 0.00011018471258671786, 9.925855923787803e-05, 8.93349000644602e-05, 8.323086165034235e-05, 7.492831815884516e-05, 6.855553431711361e-05, 6.31168125234125e-05, 5.730224890760623e-05, 5.15750561272769e-05, 4.6829294360860856e-05, 4.2167167030643045e-05, 3.858245387107683e-05, 3.474020107809444e-05, 3.127167267339326e-05, 2.8376715242649335e-05, 2.7776177529049976e-05, 2.4998638025372522e-05, 2.35295310713417e-05, 2.1910051746943677e-05, 1.9787052739689103e-05, 1.7956014610920945e-05, 1.9668684347871573e-05, 1.965885593509709e-05, 1.84900121003744e-05, 1.726470557567076e-05, 1.6083062947470775e-05, 1.5646750840335485e-05, 1.5604250118041267e-05, 1.432013140742833e-05, 1.3242501543939448e-05, 1.21492456742032e-05, 1.1024240180146011e-05, 9.979285773817403e-06, 1.3104228315509638e-05, 1.2079863866984281e-05, 1.6284719354766125e-05, 1.7117528115650443e-05, 1.577337278277971e-05, 1.421643772095581e-05, 1.2854619698186405e-05, 1.1793258523943954e-05, 1.2236905154339788e-05, 1.2737779524416832e-05, 1.287553865261593e-05, 1.2070615688588406e-05, 1.099737004705567e-05, 1.2535153054825376e-05, 1.208170395357606e-05, 1.169927653394631e-05, 1.065910620793436e-05, 1.1099632309847646e-05, 1.127806864157008e-05, 1.0388108611010847e-05, 9.385325494005962e-06, 8.694415109314718e-06, 8.67861849512146e-06, 7.826058537221517e-06, 7.521272061300926e-06, 6.900684640980066e-06, 7.693351213694597e-06, 7.049285273635548e-06, 6.37217875860045e-06, 6.956773541580719e-06, 6.6081961426356e-06, 6.723546361261308e-06, 6.434432729942344e-06, 6.571417882224404e-06, 6.931603009802609e-06, 6.254544434748856e-06, 6.35110339667245e-06, 5.8602237303997866e-06, 5.335322350662429e-06, 4.832983776062464e-06, 4.428743614568359e-06, 3.9882877968250285e-06, 4.800028139770173e-06, 4.34191382975948e-06, 3.934260290529332e-06, 3.849551228253643e-06, 3.8015556147415815e-06, 3.421477362144413e-06, 4.071941062578066e-06, 4.034270384890885e-06, 3.6475521417572427e-06, 3.468112944578131e-06, 3.1213866558886132e-06], "accuracy_test": 0.8450095663265307, "start": "2016-02-01 11:21:58.144000", "learning_rate_per_epoch": [0.0019942803774029016, 0.00141016929410398, 0.0011513984063640237, 0.0009971401887014508, 0.0008918693638406694, 0.0008141616126522422, 0.0007537671481259167, 0.00070508464705199, 0.0006647601840086281, 0.0006306468858383596, 0.0006012981757521629, 0.0005756992031820118, 0.0005531138740479946, 0.0005329938721843064, 0.0005149209755472839, 0.0004985700943507254, 0.00048368406714871526, 0.0004700564022641629, 0.00045751931611448526, 0.0004459346819203347, 0.0004351876850705594, 0.000425182020990178, 0.0004158362280577421, 0.0004070808063261211, 0.0003988560929428786, 0.00039111057412810624, 0.0003837994590867311, 0.00037688357406295836, 0.00037032857653684914, 0.0003641041403170675, 0.00035818334436044097, 0.000352542323525995, 0.0003471596573945135, 0.0003420162829570472, 0.0003370949125383049, 0.00033238009200431406, 0.00032785767689347267, 0.00032351500703953207, 0.00031934044091030955, 0.0003153234429191798, 0.00031145429238677025, 0.0003077241708524525, 0.0003041249292436987, 0.00030064908787608147, 0.000297289778245613, 0.00029404062661342323, 0.00029089569579809904, 0.0002878496015910059, 0.0002848972217179835, 0.00028203384135849774, 0.00027925512404181063, 0.0002765569370239973, 0.0002739354968070984, 0.00027138719451613724, 0.0002689087123144418, 0.0002664969360921532, 0.00026414889725856483, 0.0002618618600536138, 0.00025963320513255894, 0.00025746048777364194, 0.00025534143787808716, 0.00025327387265861034, 0.00025125572574324906, 0.0002492850471753627, 0.0002473600616212934, 0.0002454789646435529, 0.00024364014097955078, 0.00024184203357435763, 0.00024008315813262016, 0.0002383621031185612, 0.000236677544307895, 0.00023502820113208145, 0.00023341288033407182, 0.00023183038865681738, 0.0002302796783624217, 0.00022875965805724263, 0.0002272693527629599, 0.00022580780205316842, 0.0002243740891572088, 0.00022296734096016735, 0.00022158671345096081, 0.0002202314353780821, 0.00021890072093810886, 0.0002175938425352797, 0.0002163100871257484, 0.0002150487998733297, 0.0002138093113899231, 0.000212591010495089, 0.00021139330056030303, 0.0002102156140608713, 0.00020905739802401513, 0.00020791811402887106, 0.0002067972527584061, 0.0002056943194475025, 0.00020460886298678815, 0.00020354040316306055, 0.00020248850341886282, 0.00020145275630056858, 0.00020043272525072098, 0.0001994280464714393, 0.0001984383270610124, 0.0001974631886696443, 0.00019650229660328478, 0.00019555528706405312, 0.0001946218399098143, 0.00019370164955034852, 0.0001927943667396903, 0.00019189972954336554, 0.00019101743237115443, 0.0001901471841847524, 0.00018928872304968536, 0.00018844178703147918, 0.00018760612874757499, 0.00018678148626349866, 0.00018596761219669133, 0.00018516428826842457, 0.00018437129619996995, 0.0001835884031606838, 0.00018281539087183774, 0.00018205207015853375, 0.00018129822274204344, 0.00018055367399938405, 0.00017981822020374238, 0.00017909167218022048, 0.00017837386985775083, 0.00017766462406143546, 0.00017696377472020686, 0.0001762711617629975, 0.00017558661056682467, 0.00017490996106062084, 0.0001742410968290642, 0.00017357982869725674, 0.0001729260548017919, 0.0001722795859677717, 0.00017164033488370478, 0.0001710081414785236, 0.00017038287478499115, 0.00016976443293970078, 0.000169152655871585, 0.00016854745626915246, 0.0001679487177170813, 0.00016735629469621927, 0.0001667701144469902, 0.00016619004600215703, 0.00016561597294639796, 0.00016504782252013683, 0.00016448547830805182, 0.00016392883844673634, 0.000163377815624699, 0.00016283232253044844, 0.00016229224274866283, 0.00016175750351976603, 0.00016122801753226668, 0.0001607036974746734, 0.0001601844560354948, 0.00015967022045515478, 0.00015916090342216194, 0.00015865643217694014, 0.00015815673395991325, 0.0001576617214595899, 0.00015717132191639394, 0.00015668547712266445, 0.00015620409976691008, 0.00015572714619338512, 0.00015525452909059823, 0.00015478619025088847, 0.0001543220569146797, 0.00015386208542622626, 0.00015340618847403675, 0.00015295433695428073], "accuracy_train_first": 0.6036463019679771, "accuracy_train_last": 0.988514846345515, "batch_size_eval": 1024, "accuracy_train_std": [0.023566700330871982, 0.021608033556268412, 0.02008925798048745, 0.023774776698598076, 0.02242219925908973, 0.02352614211654076, 0.021912185396431284, 0.021624361103656762, 0.02110997031830782, 0.0189419436863821, 0.01889501620662604, 0.01800647642734194, 0.01884266913345317, 0.01915140119579368, 0.018255816297973498, 0.016899542734950444, 0.016811223376177156, 0.01679864192941216, 0.01572616845902838, 0.01505426717437607, 0.014026708270666405, 0.014837027090521163, 0.01427360726764788, 0.013989224225350766, 0.0139466737214091, 0.013761284244851993, 0.013608782540235472, 0.012741010237054868, 0.011881320432748803, 0.01365036494274708, 0.011734697964096137, 0.012156353397078427, 0.01061852304057889, 0.010534188876716253, 0.01088041014589759, 0.010760945445045569, 0.009725204931151064, 0.009871374723406498, 0.00990867128179666, 0.010247670870582747, 0.009881081974692693, 0.009789083775403312, 0.009096126126763504, 0.01030700257308097, 0.009943297424785981, 0.009335692035906885, 0.008364711627307456, 0.009750739820254978, 0.008706372961120697, 0.008188334359833195, 0.008879941209070734, 0.009130450494246187, 0.008871092576951924, 0.008358879018571186, 0.008561302142092744, 0.007862001852680408, 0.009766810320746354, 0.008231607070248653, 0.007713352742069731, 0.007069933246174062, 0.007775601074825884, 0.0073242443172485076, 0.00822870410130357, 0.008387356476155515, 0.00758060455369368, 0.0072506136609493755, 0.00730875875936336, 0.007712544537744274, 0.007647760477269985, 0.008031570062319741, 0.007318830354901031, 0.008151212362931138, 0.006485676650616341, 0.006604936889892916, 0.0069073486582070815, 0.007708081667016881, 0.007375089864614106, 0.0057826466587518055, 0.0069873756992050885, 0.006580667720314744, 0.007335571178312633, 0.00578014868497355, 0.007212085202281555, 0.006783035553743575, 0.005674198171619052, 0.007919046265284823, 0.0064480773115369455, 0.006165912421731643, 0.00671130888145426, 0.005829744138906668, 0.005402173875820414, 0.005638180402805767, 0.006857891439440626, 0.006578546111775201, 0.005691014731454645, 0.005162341058094325, 0.006143190814604681, 0.007162766838898556, 0.005558537914101375, 0.005800792253160799, 0.005733529899538064, 0.006549764709033283, 0.006076083722179452, 0.006002797526401529, 0.006800615306669979, 0.006261827230201282, 0.006222142804285415, 0.006326023535871418, 0.006133608154268533, 0.0056026087636068, 0.0047080084413085085, 0.004898266908665928, 0.0062719593766808425, 0.005027975674689076, 0.006753249601788111, 0.00540597526325765, 0.0060327194578855365, 0.004676168387791028, 0.004846837614947614, 0.005017747045500856, 0.006432177913467867, 0.00546563173185462, 0.005791028455223136, 0.005440162797514028, 0.00435930001992032, 0.00584816100463999, 0.005089497818592968, 0.005608858337158192, 0.0059012651693251365, 0.004539660006952166, 0.004404083552566759, 0.005436469018174859, 0.005053965287395391, 0.005440464389382409, 0.0045658578283240105, 0.00493594646439229, 0.004806888968706677, 0.005021531669776901, 0.004452008847524909, 0.004918870846119601, 0.004436229887981713, 0.004398762217814119, 0.00479216238475348, 0.004874922278972907, 0.00428599044948825, 0.005523329896866078, 0.003681795873350468, 0.004766382435787623, 0.005219413433239617, 0.004968089091482243, 0.00425953887291384, 0.004832709993053911, 0.003506022904430905, 0.005568168132688984, 0.004497716958617183, 0.004453157859095058, 0.005726903351565969, 0.004309017093995292, 0.0037554201071390517, 0.005662091475447115, 0.0036793624510494992, 0.004621332054018126, 0.0043323004091031635, 0.004551694349881004, 0.003542287534539094, 0.00497614232753615, 0.004473280535821376, 0.004834412292849185, 0.003689056624382399, 0.003952930799879246], "accuracy_test_std": 0.010547867679598014, "error_valid": [0.40719273578689763, 0.3062567653426205, 0.2547210325677711, 0.23500594173569278, 0.21545263083584332, 0.2076092455760542, 0.2011909944465362, 0.19343879423945776, 0.18362169380647586, 0.18166856880647586, 0.1759106739457832, 0.1726147755082832, 0.17546357304216864, 0.17140436746987953, 0.1680981739457832, 0.1643139942582832, 0.16611416368599397, 0.16367275743599397, 0.15949148155120485, 0.1599694677146084, 0.1611401661332832, 0.16135342149849397, 0.1588002400225903, 0.15682652484939763, 0.1520348974021084, 0.15307175969503017, 0.15367181617093373, 0.15326589561370485, 0.14787421169051207, 0.15138336549322284, 0.1504170980798193, 0.14623582219503017, 0.15070241905120485, 0.15094655967620485, 0.1412412344691265, 0.1430722891566265, 0.14772125611822284, 0.14442535768072284, 0.14355027532003017, 0.1427766730986446, 0.14444594785391573, 0.1434385000941265, 0.14454742799322284, 0.14765066123870485, 0.14222809205572284, 0.13829095679593373, 0.1421163168298193, 0.13593044051204817, 0.14152655544051207, 0.1408956137048193, 0.1344758918486446, 0.13697906861822284, 0.1359922110316265, 0.1335713949548193, 0.13190212019954817, 0.13645990210843373, 0.1401631918298193, 0.1330110480986446, 0.13133294898343373, 0.13431411191641573, 0.1278840949736446, 0.13293015813253017, 0.13315370858433728, 0.13382583066641573, 0.1323301016566265, 0.12984751506024095, 0.12838267131024095, 0.1371114340173193, 0.12767083960843373, 0.13073289250753017, 0.12960337443524095, 0.13206537085843373, 0.13000047063253017, 0.12984751506024095, 0.13162856504141573, 0.12665309676204817, 0.1306211172816265, 0.1318932958396084, 0.13261542262801207, 0.1293298192771084, 0.12985781014683728, 0.12901361304593373, 0.13203595632530118, 0.12951218938253017, 0.12825177663780118, 0.1299092855798193, 0.13097703313253017, 0.12522943335843373, 0.13075348268072284, 0.12679575724774095, 0.1260030355798193, 0.1272237387048193, 0.1295224844691265, 0.1314961996423193, 0.13005194606551207, 0.12696930299322284, 0.1289327230798193, 0.12906508847891573, 0.12768113469503017, 0.12414109563253017, 0.12865769719503017, 0.1320153661521084, 0.12611481080572284, 0.12955336972891573, 0.13005194606551207, 0.12265566170933728, 0.12361163403614461, 0.13078436794051207, 0.12547357398343373, 0.12538238893072284, 0.12398814006024095, 0.12312335278614461, 0.12857680722891573, 0.12898419851280118, 0.12880035768072284, 0.12635895143072284, 0.1280576407191265, 0.12057017131024095, 0.1284444418298193, 0.11908473738704817, 0.12083490210843373, 0.12352044898343373, 0.12486322242093373, 0.12447642131024095, 0.12363222420933728, 0.1292989340173193, 0.12109963290662651, 0.12107904273343373, 0.1269590079066265, 0.12609422063253017, 0.11958331372364461, 0.12743699407003017, 0.12778261483433728, 0.12385577466114461, 0.12084519719503017, 0.12074371705572284, 0.12252329631024095, 0.12461908179593373, 0.12239093091114461, 0.12080401684864461, 0.12398814006024095, 0.12131288827183728, 0.12217767554593373, 0.11920680769954817, 0.12403961549322284, 0.12242181617093373, 0.11923769295933728, 0.12451760165662651, 0.1256868293486446, 0.12510736304593373, 0.12030544051204817, 0.11959360881024095, 0.12304246282003017, 0.1254941641566265, 0.12167909920933728, 0.12364251929593373, 0.12348956372364461, 0.12389695500753017, 0.12288950724774095, 0.12670457219503017, 0.12389695500753017, 0.12291009742093373, 0.12154673381024095, 0.1251485433923193, 0.12337778849774095, 0.12672516236822284, 0.12170998446912651, 0.12310276261295183, 0.12205560523343373, 0.12337778849774095], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.010792142820406726, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 16, "valid_ratio": 0.15, "learning_rate": 0.001994280471664555, "optimization": "adam", "nb_data_augmentation": 1, "learning_rate_decay_method": "sqrt", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 2.9579556877255253e-08, "rotation_range": [0, 0], "momentum": 0.6857748213056354}, "accuracy_valid_max": 0.8809152626129518, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.876622211502259, "accuracy_valid_std": [0.017487752538902956, 0.014981934835131312, 0.012769624659076245, 0.014862974619899917, 0.0117589352278426, 0.013590227925176545, 0.01620171810140729, 0.014496197874569376, 0.016243772582797397, 0.013176063969797456, 0.013643357206473698, 0.015099252520210268, 0.017186631556657193, 0.015631227197897023, 0.01759991250995573, 0.016390261343825298, 0.01607121363850465, 0.015604687266320425, 0.01502929748798986, 0.013124634733745049, 0.014964098938086628, 0.01471901915355299, 0.014043218674946545, 0.016279196394469547, 0.012914087900402762, 0.015372527430435465, 0.013423705542105412, 0.016784586184550362, 0.01496506622486833, 0.012624479036346196, 0.01321989879874771, 0.011909681754868446, 0.01268024599030991, 0.013641392102890289, 0.014118854491542916, 0.013532292713658687, 0.014564575635325422, 0.013696783784117171, 0.015394980967064723, 0.01602990848993438, 0.01376074705503041, 0.012472625636617728, 0.014011249609577272, 0.014905106928926316, 0.01518120929551944, 0.014049427907817965, 0.011366196944084542, 0.012882453352929561, 0.013339409942144883, 0.010123861862456139, 0.011342271342218532, 0.013030637430727752, 0.014127898862831936, 0.01442300113183966, 0.01327259086354527, 0.01051206443290998, 0.014187891116315795, 0.01405898059197067, 0.015364208002379606, 0.012371096543278667, 0.007377961070471856, 0.011006570852205409, 0.010316704591494839, 0.014624523026975448, 0.01009695999727448, 0.011700110401704981, 0.01005118829187906, 0.013844412962650082, 0.012366159392904509, 0.012676856731568705, 0.013018413460411236, 0.01096126805163344, 0.011209313195309352, 0.007981107669233978, 0.01444761015662605, 0.012541580150272708, 0.013259670571106908, 0.015793866096149755, 0.015549779486852305, 0.018970365367207268, 0.012693335162567746, 0.013786957159517171, 0.018524124933161565, 0.013251154133237296, 0.019005583626329065, 0.011995976182203107, 0.01322011902913366, 0.01428670990160176, 0.01234989411877327, 0.012588786429342285, 0.01466016996808824, 0.014993662915652036, 0.01329775047753461, 0.015511365688189385, 0.014351349132800381, 0.013739637604633839, 0.014225600222060547, 0.015081249658682206, 0.013864397752181946, 0.015111432912509909, 0.016306485933888216, 0.014341111963020759, 0.014205076587056695, 0.01297688662689746, 0.016793354300583572, 0.014980102659550557, 0.012738597204265574, 0.01418181617556333, 0.010350869724108612, 0.013671383905170637, 0.014668233453399375, 0.012806852623664624, 0.012952071016446598, 0.016592978718536395, 0.012610328372674506, 0.015631187344633226, 0.014763963144623426, 0.011171103734677746, 0.01294544096375523, 0.00861687138360498, 0.01264410453902061, 0.011730329573820327, 0.010605284895173818, 0.009024340414870366, 0.010999029279059873, 0.012785584955737546, 0.013992113434772156, 0.009836627575497914, 0.013073437414579977, 0.011079404731541443, 0.01229595873586581, 0.013806159684612565, 0.013817511300272627, 0.011505328723293226, 0.012362125088589108, 0.01498294175406879, 0.00902990907365658, 0.011432580973978203, 0.009532753731339024, 0.011213006666557796, 0.009385092966097443, 0.011500524148163223, 0.013114790062203011, 0.012245289485426568, 0.01506716320652161, 0.013688787309700327, 0.012391184465719475, 0.011860199467844265, 0.010302892670261495, 0.01027448831798254, 0.009248404477646214, 0.0100559026948279, 0.013516784129413076, 0.012029378920367213, 0.010300438230253986, 0.009214055318319732, 0.009374915051676432, 0.01071755600088906, 0.010692649997114254, 0.012892838414004645, 0.012360200674315184, 0.009735072808883445, 0.013360639178602647, 0.01467339607181869, 0.01056529320261081, 0.010880829721932297, 0.011252367217096207, 0.006837536506730799, 0.011367184961408462, 0.011556813130551083], "accuracy_valid": [0.5928072642131024, 0.6937432346573795, 0.7452789674322289, 0.7649940582643072, 0.7845473691641567, 0.7923907544239458, 0.7988090055534638, 0.8065612057605422, 0.8163783061935241, 0.8183314311935241, 0.8240893260542168, 0.8273852244917168, 0.8245364269578314, 0.8285956325301205, 0.8319018260542168, 0.8356860057417168, 0.833885836314006, 0.836327242564006, 0.8405085184487951, 0.8400305322853916, 0.8388598338667168, 0.838646578501506, 0.8411997599774097, 0.8431734751506024, 0.8479651025978916, 0.8469282403049698, 0.8463281838290663, 0.8467341043862951, 0.8521257883094879, 0.8486166345067772, 0.8495829019201807, 0.8537641778049698, 0.8492975809487951, 0.8490534403237951, 0.8587587655308735, 0.8569277108433735, 0.8522787438817772, 0.8555746423192772, 0.8564497246799698, 0.8572233269013554, 0.8555540521460843, 0.8565614999058735, 0.8554525720067772, 0.8523493387612951, 0.8577719079442772, 0.8617090432040663, 0.8578836831701807, 0.8640695594879518, 0.8584734445594879, 0.8591043862951807, 0.8655241081513554, 0.8630209313817772, 0.8640077889683735, 0.8664286050451807, 0.8680978798004518, 0.8635400978915663, 0.8598368081701807, 0.8669889519013554, 0.8686670510165663, 0.8656858880835843, 0.8721159050263554, 0.8670698418674698, 0.8668462914156627, 0.8661741693335843, 0.8676698983433735, 0.870152484939759, 0.871617328689759, 0.8628885659826807, 0.8723291603915663, 0.8692671074924698, 0.870396625564759, 0.8679346291415663, 0.8699995293674698, 0.870152484939759, 0.8683714349585843, 0.8733469032379518, 0.8693788827183735, 0.8681067041603916, 0.8673845773719879, 0.8706701807228916, 0.8701421898531627, 0.8709863869540663, 0.8679640436746988, 0.8704878106174698, 0.8717482233621988, 0.8700907144201807, 0.8690229668674698, 0.8747705666415663, 0.8692465173192772, 0.873204242752259, 0.8739969644201807, 0.8727762612951807, 0.8704775155308735, 0.8685038003576807, 0.8699480539344879, 0.8730306970067772, 0.8710672769201807, 0.8709349115210843, 0.8723188653049698, 0.8758589043674698, 0.8713423028049698, 0.8679846338478916, 0.8738851891942772, 0.8704466302710843, 0.8699480539344879, 0.8773443382906627, 0.8763883659638554, 0.8692156320594879, 0.8745264260165663, 0.8746176110692772, 0.876011859939759, 0.8768766472138554, 0.8714231927710843, 0.8710158014871988, 0.8711996423192772, 0.8736410485692772, 0.8719423592808735, 0.879429828689759, 0.8715555581701807, 0.8809152626129518, 0.8791650978915663, 0.8764795510165663, 0.8751367775790663, 0.875523578689759, 0.8763677757906627, 0.8707010659826807, 0.8789003670933735, 0.8789209572665663, 0.8730409920933735, 0.8739057793674698, 0.8804166862763554, 0.8725630059299698, 0.8722173851656627, 0.8761442253388554, 0.8791548028049698, 0.8792562829442772, 0.877476703689759, 0.8753809182040663, 0.8776090690888554, 0.8791959831513554, 0.876011859939759, 0.8786871117281627, 0.8778223244540663, 0.8807931923004518, 0.8759603845067772, 0.8775781838290663, 0.8807623070406627, 0.8754823983433735, 0.8743131706513554, 0.8748926369540663, 0.8796945594879518, 0.880406391189759, 0.8769575371799698, 0.8745058358433735, 0.8783209007906627, 0.8763574807040663, 0.8765104362763554, 0.8761030449924698, 0.877110492752259, 0.8732954278049698, 0.8761030449924698, 0.8770899025790663, 0.878453266189759, 0.8748514566076807, 0.876622211502259, 0.8732748376317772, 0.8782900155308735, 0.8768972373870482, 0.8779443947665663, 0.876622211502259], "seed": 599930353, "model": "residualv3", "loss_std": [0.33419492840766907, 0.2759252190589905, 0.26242372393608093, 0.25405198335647583, 0.24239341914653778, 0.23737017810344696, 0.2307364046573639, 0.22077083587646484, 0.21580901741981506, 0.20833338797092438, 0.20448042452335358, 0.1969018131494522, 0.1887189745903015, 0.1827351599931717, 0.1767510622739792, 0.17414146661758423, 0.16613255441188812, 0.16436311602592468, 0.15663659572601318, 0.15371903777122498, 0.1497126817703247, 0.1462094783782959, 0.1398443579673767, 0.13744106888771057, 0.13536542654037476, 0.13260658085346222, 0.12857459485530853, 0.12650026381015778, 0.1238827258348465, 0.12176541984081268, 0.11770597100257874, 0.11751304566860199, 0.1175205260515213, 0.11497104912996292, 0.11003829538822174, 0.10972020030021667, 0.10826127231121063, 0.10967275500297546, 0.10713857412338257, 0.1039506271481514, 0.10410644859075546, 0.10414652526378632, 0.10212384909391403, 0.09825142472982407, 0.09791769832372665, 0.09720920771360397, 0.09847278892993927, 0.0964302271604538, 0.0950051099061966, 0.09410011768341064, 0.09110002219676971, 0.09177831560373306, 0.09197675436735153, 0.09179716557264328, 0.09004203230142593, 0.08830750733613968, 0.08646639436483383, 0.0866926982998848, 0.08535623550415039, 0.08606892079114914, 0.08392039686441422, 0.08450865745544434, 0.08288482576608658, 0.08132956922054291, 0.08257497102022171, 0.0796344056725502, 0.08094575256109238, 0.07858090102672577, 0.08100427687168121, 0.07821686565876007, 0.07825891673564911, 0.07753726840019226, 0.07911841571331024, 0.07477725297212601, 0.07408769428730011, 0.07520148903131485, 0.07391620427370071, 0.0727098360657692, 0.07480543851852417, 0.07229947298765182, 0.07378959655761719, 0.07181515544652939, 0.06956476718187332, 0.0724652037024498, 0.06894922256469727, 0.06981928646564484, 0.07103732228279114, 0.07236948609352112, 0.06657934188842773, 0.0689176619052887, 0.06573319435119629, 0.06520169973373413, 0.06658472120761871, 0.06592538207769394, 0.0656990185379982, 0.06522475928068161, 0.06271516531705856, 0.06463675945997238, 0.06190810725092888, 0.0632038488984108, 0.06308679282665253, 0.06323391944169998, 0.06475519388914108, 0.06276865303516388, 0.06080177426338196, 0.060157015919685364, 0.06124018877744675, 0.06272817403078079, 0.060314588248729706, 0.059968773275613785, 0.06118267774581909, 0.059825532138347626, 0.05566589534282684, 0.05810990929603577, 0.05833267793059349, 0.05876379460096359, 0.058357708156108856, 0.05764046683907509, 0.05563996732234955, 0.05608625337481499, 0.05774460732936859, 0.055579766631126404, 0.059332218021154404, 0.05643279105424881, 0.05903508886694908, 0.0549045130610466, 0.0532221645116806, 0.05556082725524902, 0.054272767156362534, 0.05350875481963158, 0.055777084082365036, 0.0532957948744297, 0.052162282168865204, 0.052582841366529465, 0.052335500717163086, 0.05354846641421318, 0.05097096040844917, 0.05257190391421318, 0.05392476171255112, 0.05243338271975517, 0.0514187216758728, 0.04893643036484718, 0.052488841116428375, 0.051839228719472885, 0.051535654813051224, 0.05321884900331497, 0.049321506172418594, 0.04797869920730591, 0.05022783204913139, 0.049155350774526596, 0.0485219806432724, 0.04793141037225723, 0.04876507818698883, 0.04764226824045181, 0.04790399223566055, 0.04928996413946152, 0.048368703573942184, 0.04718579724431038, 0.04846223443746567, 0.04794320836663246, 0.047660574316978455, 0.04833756014704704, 0.04586288332939148, 0.048433125019073486, 0.046407055109739304, 0.047137949615716934, 0.04639396443963051, 0.04651691019535065, 0.04575435444712639, 0.04613931104540825]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:31 2016", "state": "available"}], "summary": "d5928db401baa5f26656fab8173aacf6"}