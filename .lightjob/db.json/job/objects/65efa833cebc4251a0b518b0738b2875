{"content": {"hp_model": {"f0": 64, "f1": 64, "f2": 32, "f3": 64, "nonlin": "very_leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.7427968978881836, 1.4208321571350098, 1.282561182975769, 1.1822822093963623, 1.1050161123275757, 1.0413037538528442, 0.9865904450416565, 0.9400168657302856, 0.8959861993789673, 0.8558763265609741, 0.8232458829879761, 0.7916210293769836, 0.7617256045341492, 0.7322251796722412, 0.7089393734931946, 0.6871231198310852, 0.6658390164375305, 0.6462978720664978, 0.626410961151123, 0.6078201532363892, 0.593164324760437, 0.5756162405014038, 0.5619728565216064, 0.54643315076828, 0.5333606004714966, 0.5219296216964722, 0.5061487555503845, 0.4962003231048584, 0.48577064275741577, 0.4734709560871124, 0.4636297821998596, 0.4542139172554016, 0.44328227639198303, 0.4358360767364502, 0.42711108922958374, 0.41896674036979675, 0.4071609377861023, 0.39879676699638367, 0.39268210530281067, 0.3853776156902313, 0.3751280903816223, 0.3693200349807739, 0.3621785640716553, 0.35388004779815674, 0.35051146149635315, 0.3430846631526947, 0.3362736105918884, 0.32815247774124146, 0.32309484481811523, 0.3163965344429016, 0.3113231658935547, 0.30267438292503357, 0.299070805311203, 0.2930334508419037, 0.28914812207221985, 0.2850192189216614, 0.2774353623390198, 0.2704222500324249, 0.2660343050956726, 0.26034462451934814, 0.254261314868927, 0.25009337067604065, 0.24798905849456787, 0.24084170162677765, 0.23772849142551422, 0.23301060497760773, 0.2304062396287918, 0.22472910583019257, 0.22059151530265808, 0.2162654548883438, 0.21338225901126862, 0.20811524987220764, 0.20348051190376282, 0.20329402387142181, 0.19718334078788757, 0.19431450963020325, 0.18929071724414825, 0.18691186606884003, 0.1843595802783966, 0.18268528580665588, 0.17964164912700653, 0.17608965933322906, 0.17217320203781128, 0.17005686461925507, 0.16384805738925934, 0.16477499902248383, 0.15882673859596252, 0.15892915427684784, 0.15836496651172638, 0.14734946191310883, 0.12460321187973022, 0.1202986091375351, 0.11736969649791718, 0.11601826548576355, 0.11353805661201477, 0.1123570129275322, 0.1118437796831131, 0.1108488142490387, 0.10919639468193054, 0.10909564048051834, 0.10734651237726212, 0.10761399567127228, 0.10552944988012314, 0.10480351001024246, 0.10429756343364716, 0.10373777151107788, 0.10258341580629349, 0.10242919623851776, 0.09787595272064209, 0.09679730981588364, 0.09730098396539688, 0.0975283533334732, 0.09601766616106033, 0.09727144986391068, 0.09657669812440872, 0.09849996864795685, 0.09718996286392212, 0.0971759557723999, 0.09757378697395325, 0.09672755002975464, 0.09780514240264893, 0.09829693287611008, 0.09706024080514908, 0.0975237488746643, 0.09727807343006134, 0.09770408272743225, 0.09741190075874329, 0.09657353907823563, 0.09770482778549194, 0.09719791263341904, 0.09784481674432755, 0.09711503237485886, 0.09713873267173767, 0.09884099662303925, 0.09793885052204132, 0.09724878519773483, 0.09817518293857574, 0.0967777818441391, 0.09835169464349747, 0.09768179059028625, 0.09687956422567368], "moving_avg_accuracy_train": [0.04852095301541158, 0.09891738548011259, 0.147696721648325, 0.19515738629481588, 0.240715100738424, 0.2842743829887787, 0.32425651767001346, 0.3619769285068327, 0.39664351690038163, 0.42838949593662623, 0.45868813819535376, 0.4869053966984005, 0.5131470672248967, 0.53734829119639, 0.5605774918444236, 0.5811721664383589, 0.6005955083205012, 0.6189480863298134, 0.636362661636939, 0.6521730352907515, 0.6675370081494116, 0.6819642918709692, 0.6960717138512772, 0.708835750851393, 0.720288434821717, 0.7313745229044678, 0.7416196105849513, 0.7509564469378626, 0.7602057735780556, 0.7689996313208407, 0.7774256720762612, 0.7848159772096742, 0.7924437782809272, 0.7986719247641578, 0.8055696428001544, 0.8120520647385313, 0.8188087878281702, 0.8245738346636625, 0.8299831217572353, 0.8352236120974309, 0.8409834684379831, 0.8458907545826031, 0.8513721942187041, 0.8556474727780999, 0.8600881364279847, 0.8645357044354722, 0.8686431833874584, 0.872372502576398, 0.876593628910674, 0.8805554030281891, 0.8838722088113337, 0.8871084500875923, 0.8901420110231391, 0.8933020439555244, 0.8959484719946803, 0.8983374489692617, 0.9009874352404326, 0.9033373654082498, 0.9058569865985987, 0.9080644080937775, 0.9105483808941266, 0.912946788928745, 0.9142986016206343, 0.9161871289516494, 0.9176287480805247, 0.9188099478560363, 0.9204147873266157, 0.9217544030072524, 0.9233182021341296, 0.9246721068768813, 0.9254603244203208, 0.9267789452463304, 0.9285051024647298, 0.9294798621518117, 0.9307639748142145, 0.9320450900508164, 0.9330632351328056, 0.9335752061066513, 0.9348819376127728, 0.935130297642101, 0.9365349612149156, 0.9371878245376378, 0.9376801064757161, 0.9387088093294365, 0.9398856137739384, 0.9400871183514633, 0.9402846403664461, 0.9412434078381995, 0.9427133065973382, 0.9467075951697934, 0.9504769131433548, 0.9538576735755125, 0.9569932918191979, 0.9598805605516376, 0.9625627356703388, 0.9650045229652466, 0.9672139014699863, 0.9691999448778049, 0.9709781193984224, 0.9726155707014742, 0.9742125458099443, 0.9755312768671005, 0.9767716132411601, 0.9779251183587662, 0.9790376777265164, 0.9800435954062918, 0.9809372955740422, 0.981895085546446, 0.9827919016561149, 0.9835944219060164, 0.9843143289332996, 0.9849599561578638, 0.9855572967016382, 0.986085602595797, 0.98656107790054, 0.9870029205188471, 0.9874238303634186, 0.9877864092306851, 0.9880778890279007, 0.9883587499382336, 0.9886487271384856, 0.9888841660306263, 0.9891239267704486, 0.9893094845017647, 0.9894788116087587, 0.9896010151193484, 0.9896947222372124, 0.989848777058757, 0.9899804509517184, 0.990078031116098, 0.9901472881223823, 0.9902444606113623, 0.9903087004121677, 0.990389731672169, 0.9904440946645129, 0.9905116225480984, 0.9905305449647541, 0.9905615260326014, 0.9906009986888927, 0.9906086583426593, 0.9906736447024688], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.046956184111445774, 0.09583222538591865, 0.14349218455854665, 0.19013385348032752, 0.23503104206866526, 0.2775728303222656, 0.316796410194783, 0.35357983040459684, 0.3873440882809293, 0.41807371724462855, 0.4473977461225753, 0.4740060101492485, 0.4985882133982544, 0.5216196346694982, 0.5437324558881659, 0.5628720115041685, 0.5811749188345499, 0.5981836152982335, 0.6142391595477776, 0.6285914931223674, 0.642714147979935, 0.6553186450324686, 0.6679017795879567, 0.6785856580109382, 0.6887810561123293, 0.6982335580876928, 0.7069036193064084, 0.7147035858772737, 0.7223156703430103, 0.7295093727458327, 0.736301087720873, 0.7420310951950206, 0.7480059730155035, 0.7528106620938477, 0.7580270250542671, 0.7626149769633735, 0.767937481290756, 0.7719891362584425, 0.7760008071582006, 0.779599103936733, 0.7834917502542795, 0.7867295187058847, 0.7902112108017872, 0.7931626577280092, 0.7958667585779492, 0.7988019671328049, 0.801480275925925, 0.8037829495758024, 0.8063192230481921, 0.8090088192392314, 0.8111313395178685, 0.813281777431669, 0.8149434693232912, 0.8171103787445012, 0.8190810403241173, 0.8204425381365852, 0.8216312650740561, 0.8232444057447378, 0.8251071533762128, 0.8264581544354289, 0.8278662793627445, 0.8293187562833977, 0.8299993088835971, 0.8306229837463669, 0.8314996149180405, 0.8316273442677274, 0.8322713210608342, 0.8326789722284706, 0.8335525236125514, 0.8340395740562662, 0.8344321798565884, 0.8348037620874205, 0.8353142320759676, 0.8352659602237924, 0.8357759499977234, 0.8366938075111288, 0.8367202451899858, 0.8363767986547974, 0.8369660166149954, 0.836649909588285, 0.8375780273926342, 0.8377887158054792, 0.8376202134361512, 0.8380952379321446, 0.8389367695323788, 0.8389993766999994, 0.8387749614321078, 0.8391090675573459, 0.8395674249676506, 0.8421305902269096, 0.8447223187136313, 0.846685574888202, 0.8485756052664751, 0.8501677988343306, 0.8514898802554909, 0.8528994800970352, 0.8540094285481751, 0.8549707315517913, 0.8557148634512055, 0.8565788651520186, 0.8574683889813197, 0.8581224760526908, 0.8587721895731747, 0.8591738262728602, 0.8597163457540079, 0.8600968090231101, 0.8604758470590521, 0.8610845064702403, 0.8615956788465596, 0.862141183203997, 0.8626443441569407, 0.8629873257333399, 0.8633448372770993, 0.8636299765727328, 0.8639720511575528, 0.864231090158891, 0.8645740885413452, 0.864785130835554, 0.864962861869092, 0.8651350268305261, 0.8653143893583168, 0.8654269875083285, 0.8656137750620889, 0.8656964346417234, 0.8657952423258944, 0.8658597551791484, 0.8659422308095769, 0.8660530799707126, 0.8661772582782347, 0.8662890187550045, 0.8663773961528475, 0.8664203147171561, 0.8664955625187839, 0.8664412152277489, 0.8665265800095673, 0.866566787219454, 0.866480903395852, 0.8665134712358602, 0.8665916104168675, 0.8666253145860241, 0.8666434413070151], "moving_var_accuracy_train": [0.02118854593371401, 0.041927894986865125, 0.05914991822128192, 0.07350755859733374, 0.08483635084552843, 0.0934294153924702, 0.09847361369623871, 0.10143171686990071, 0.10210449634054031, 0.10096431137121342, 0.09912994973859324, 0.0963828778615838, 0.09294221752361605, 0.0889192889467199, 0.08488372191676734, 0.08021261531976134, 0.07558674967662046, 0.07105942878524936, 0.0666828928048724, 0.062264314760043696, 0.05816234824205415, 0.05421943205808961, 0.05058866304665495, 0.04699608250685241, 0.043476949987284214, 0.040235367129362376, 0.037156486810653276, 0.034225426747317364, 0.031572834462258496, 0.029111538422035715, 0.026839368045140217, 0.024646980730310748, 0.02270593279992314, 0.020784447797480034, 0.01913420964466904, 0.017598984827886494, 0.016249966107288384, 0.014924091381698326, 0.013695025725274737, 0.012572687803798418, 0.011614002528992773, 0.010669335391840104, 0.009872817477014073, 0.009050037790156526, 0.008322509454003542, 0.007668286259234234, 0.007053300083379897, 0.006473140469558849, 0.005986187590372232, 0.005528829718758921, 0.005074957552310939, 0.004661721115463287, 0.0042783714314640385, 0.003940406561521468, 0.003609398137667198, 0.0032998232227662004, 0.003033042745626129, 0.002779438017206073, 0.002558630633971162, 0.0023466219574904464, 0.0021674908495972686, 0.002002513014542239, 0.0018187082910735908, 0.0016689362812861547, 0.0015207470445721912, 0.0013812294363019894, 0.0012662860802087582, 0.001155808603734152, 0.001062236952743734, 0.0009725107799413701, 0.0008808512840093067, 0.0008084150035534533, 0.0007543900718818007, 0.0006875024727216597, 0.0006335927334171829, 0.0005850047663205443, 0.0005358338643602998, 0.0004846095064268143, 0.00045151648084594786, 0.0004069199770988638, 0.00038398569716410877, 0.00034942320211110044, 0.0003166619554590129, 0.0002945198259643836, 0.0002775316616733403, 0.00025014393235887785, 0.00022548067364061612, 0.00021120572186058493, 0.0002095305709335838, 0.00033216658464064, 0.0004268197480488974, 0.0004870036431407951, 0.0005267921947579081, 0.0005491398618820363, 0.0005589724460002476, 0.0005567361281423813, 0.0005449946957229934, 0.0005259945419103534, 0.0005018522293512758, 0.00047579822734494315, 0.00045117137008410826, 0.0004217056974856719, 0.00039338103662444284, 0.00036601809946908837, 0.00034055638464309775, 0.0003156075795851516, 0.00029123512153516863, 0.0002703678640627878, 0.0002505695898675625, 0.0002313089796443258, 0.00021284247683127862, 0.0001953097397660357, 0.00017899010731656185, 0.00016360306064513218, 0.0001492774454694024, 0.00013610672501663377, 0.0001240905383902856, 0.00011286465546615128, 0.0001023428341692005, 9.281849641085748e-05, 8.429342775976533e-05, 7.636296823118093e-05, 6.924403831930358e-05, 6.262951953223406e-05, 5.662461260147752e-05, 5.109655462333359e-05, 4.60659283764456e-05, 4.167293153117111e-05, 3.7661680504842774e-05, 3.398120945068175e-05, 3.062625730188869e-05, 2.764861400523082e-05, 2.4920893372775464e-05, 2.248789862137465e-05, 2.026570677366636e-05, 1.8280176231853713e-05, 1.6455381129337147e-05, 1.481848145548802e-05, 1.3350656125291484e-05, 1.2016118545424767e-05, 1.0852515733533743e-05], "duration": 152922.048963, "accuracy_train": [0.48520953015411594, 0.5524852776624216, 0.586710747162237, 0.6223033681132337, 0.650734530730897, 0.6763079232419712, 0.6840957298011259, 0.701460626038206, 0.708642812442322, 0.7141033072628277, 0.7313759185239018, 0.7408607232258213, 0.7493221019633628, 0.7551593069398302, 0.7696402976767257, 0.7665242377837762, 0.7754055852597821, 0.7841212884136213, 0.7930938394010705, 0.7944663981750646, 0.8058127638773532, 0.8118098453649871, 0.8230385116740495, 0.8237120838524363, 0.8233625905546327, 0.8311493156492249, 0.8338253997093023, 0.8349879741140642, 0.8434497133397933, 0.8481443510059062, 0.8532600388750462, 0.8513287234103912, 0.8610939879222037, 0.8547252431132337, 0.8676491051241234, 0.8703938621839239, 0.8796192956349206, 0.8764592561830934, 0.8786667055993909, 0.8823880251591916, 0.8928221755029531, 0.8900563298841824, 0.9007051509436139, 0.8941249798126615, 0.9000541092769472, 0.9045638165028608, 0.9056104939553341, 0.9059363752768549, 0.9145837659191584, 0.916211370085825, 0.9137234608596345, 0.9162346215739202, 0.9174440594430602, 0.9217423403469915, 0.9197663243470838, 0.9198382417404946, 0.9248373116809707, 0.9244867369186047, 0.9285335773117387, 0.9279312015503876, 0.9329041360972684, 0.9345324612403102, 0.9264649158476375, 0.9331838749307864, 0.9306033202404023, 0.9294407458356404, 0.9348583425618309, 0.9338109441329827, 0.9373923942760245, 0.9368572495616464, 0.9325542823112772, 0.9386465326804172, 0.9440405174303249, 0.9382526993355482, 0.9423209887758398, 0.9435751271802326, 0.9422265408707088, 0.9381829448712625, 0.9466425211678663, 0.9373655379060539, 0.9491769333702473, 0.9430635944421374, 0.9421106439184201, 0.9479671350129198, 0.9504768537744556, 0.9419006595491879, 0.942062338501292, 0.9498723150839794, 0.9559423954295865, 0.98265619232189, 0.984400774905408, 0.9842845174649317, 0.9852138560123662, 0.9858659791435955, 0.986702311738649, 0.9869806086194168, 0.9870983080126431, 0.9870743355481728, 0.9869816900839794, 0.9873526324289406, 0.9885853217861758, 0.9873998563815062, 0.9879346406076966, 0.9883066644172205, 0.989050712036268, 0.989096854524271, 0.9889805970837948, 0.9905151952980805, 0.9908632466431341, 0.9908171041551311, 0.9907934921788483, 0.9907706011789406, 0.9909333615956073, 0.9908403556432264, 0.9908403556432264, 0.9909795040836102, 0.9912120189645626, 0.9910496190360835, 0.9907012072028424, 0.9908864981312293, 0.991258521940753, 0.991003116059893, 0.9912817734288483, 0.9909795040836102, 0.9910027555717055, 0.9907008467146549, 0.9905380862979882, 0.9912352704526578, 0.9911655159883721, 0.990956252595515, 0.9907706011789406, 0.9911190130121816, 0.9908868586194168, 0.9911190130121816, 0.9909333615956073, 0.9911193735003692, 0.9907008467146549, 0.9908403556432264, 0.990956252595515, 0.9906775952265596, 0.991258521940753], "end": "2016-02-01 15:01:57.706000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0], "moving_var_accuracy_valid": [0.01984394903677194, 0.039359360829070544, 0.05586667012119262, 0.06985901062735453, 0.08101492745284995, 0.08920166843789247, 0.094127904556245, 0.09689229412160619, 0.09746329069890089, 0.0962157524952304, 0.09433326527241927, 0.0912719361757957, 0.08758330500739504, 0.08359899179861707, 0.07963988437899419, 0.07497279924369732, 0.07049048707002833, 0.066045100161569, 0.06176061465575373, 0.05743845850150501, 0.05348965707338828, 0.04957055147957541, 0.04603851380879154, 0.042461969751325816, 0.039151288058205884, 0.036040307394733545, 0.03311280630908669, 0.03034908098473755, 0.027835667355485358, 0.025517844808280184, 0.023381206858171837, 0.021338583043238744, 0.019526017223642153, 0.01778118083553398, 0.016247956734794105, 0.014812604785797155, 0.01358630577805249, 0.012375418372041846, 0.01128271806550936, 0.010270975916315992, 0.009380252582865973, 0.008536575625495264, 0.007792017681601768, 0.007091215264066333, 0.0064479031903195184, 0.005880651914632048, 0.005357146765090584, 0.004869152842022082, 0.004440131705960601, 0.004061223884402215, 0.0036956473269610245, 0.003367702043254918, 0.003055782818413575, 0.0027924640045297773, 0.002548169167629177, 0.0023100353375064505, 0.002091749449342628, 0.0019059945098190332, 0.0017466235174842218, 0.0015883880004938262, 0.00144739454274279, 0.0013216422913137846, 0.0011936464287571494, 0.001077782518891493, 0.0009769206069026913, 0.0008793753792933652, 0.0007951701963545694, 0.0007171487919893908, 0.0006523017409761145, 0.0005892065300910077, 0.0005316731309119265, 0.0004797484780091655, 0.00043411884669111457, 0.00039072793356741484, 0.00035399594633630106, 0.0003261785134369033, 0.00029356695265098304, 0.00026527185708868066, 0.00024186927162139137, 0.00021858165733027333, 0.0002044761155259961, 0.00018442801043916084, 0.00016624074683146723, 0.00015164750659446373, 0.00014285633484275407, 0.00012860597827541587, 0.00011619864036003964, 0.00010558341845032967, 9.691590024552663e-05, 0.00014635265553742598, 0.00019217089892364556, 0.00020764318229418702, 0.00021902879754192737, 0.0002199416410054187, 0.00021367857046247234, 0.00021019345883576012, 0.0002002619830298739, 0.00018855271590974065, 0.00017468103487229927, 0.00016393142183614197, 0.00015465955343857914, 0.000143044067167134, 0.00013253880937871734, 0.00012073673678765432, 0.00011131200959571189, 0.00010148357932836428, 9.262824988974481e-05, 8.669962141022161e-05, 8.038133405400714e-05, 7.502137568445526e-05, 6.979777661711439e-05, 6.387672621114715e-05, 5.863938412532337e-05, 5.350718547402005e-05, 4.9209602120836625e-05, 4.4892552746681064e-05, 4.146212848530921e-05, 3.7716765286282725e-05, 3.422938364019674e-05, 3.1073212241687745e-05, 2.82554292648984e-05, 2.554399142888306e-05, 2.3303598598153342e-05, 2.103473219328615e-05, 1.9019125600018716e-05, 1.7154670214131513e-05, 1.5500423259249624e-05, 1.4060968762045026e-05, 1.2793654154372043e-05, 1.1626702376445376e-05, 1.0534327218846207e-05, 9.49747252542241e-06, 8.598685357728468e-06, 7.765399474341224e-06, 7.054443840681368e-06, 6.363549034155047e-06, 5.793578411148086e-06, 5.2237665478584575e-06, 4.756341477549115e-06, 4.290931068961049e-06, 3.864795164189901e-06], "accuracy_test": 0.8607959980867348, "start": "2016-01-30 20:33:15.657000", "learning_rate_per_epoch": [0.00033102769521065056, 0.00033102769521065056, 0.00033102769521065056, 0.00033102769521065056, 0.00033102769521065056, 0.00033102769521065056, 0.00033102769521065056, 0.00033102769521065056, 0.00033102769521065056, 0.00033102769521065056, 0.00033102769521065056, 0.00033102769521065056, 0.00033102769521065056, 0.00033102769521065056, 0.00033102769521065056, 0.00033102769521065056, 0.00033102769521065056, 0.00033102769521065056, 0.00033102769521065056, 0.00033102769521065056, 0.00033102769521065056, 0.00033102769521065056, 0.00033102769521065056, 0.00033102769521065056, 0.00033102769521065056, 0.00033102769521065056, 0.00033102769521065056, 0.00033102769521065056, 0.00033102769521065056, 0.00033102769521065056, 0.00033102769521065056, 0.00033102769521065056, 0.00033102769521065056, 0.00033102769521065056, 0.00033102769521065056, 0.00033102769521065056, 0.00033102769521065056, 0.00033102769521065056, 0.00033102769521065056, 0.00033102769521065056, 0.00033102769521065056, 0.00033102769521065056, 0.00033102769521065056, 0.00033102769521065056, 0.00033102769521065056, 0.00033102769521065056, 0.00033102769521065056, 0.00033102769521065056, 0.00033102769521065056, 0.00033102769521065056, 0.00033102769521065056, 0.00033102769521065056, 0.00033102769521065056, 0.00033102769521065056, 0.00033102769521065056, 0.00033102769521065056, 0.00033102769521065056, 0.00033102769521065056, 0.00033102769521065056, 0.00033102769521065056, 0.00033102769521065056, 0.00033102769521065056, 0.00033102769521065056, 0.00033102769521065056, 0.00033102769521065056, 0.00033102769521065056, 0.00033102769521065056, 0.00033102769521065056, 0.00033102769521065056, 0.00033102769521065056, 0.00033102769521065056, 0.00033102769521065056, 0.00033102769521065056, 0.00033102769521065056, 0.00033102769521065056, 0.00033102769521065056, 0.00033102769521065056, 0.00033102769521065056, 0.00033102769521065056, 0.00033102769521065056, 0.00033102769521065056, 0.00033102769521065056, 0.00033102769521065056, 0.00033102769521065056, 0.00033102769521065056, 0.00033102769521065056, 0.00033102769521065056, 0.00033102769521065056, 3.3102769521065056e-05, 3.3102769521065056e-05, 3.3102769521065056e-05, 3.3102769521065056e-05, 3.3102769521065056e-05, 3.3102769521065056e-05, 3.3102769521065056e-05, 3.3102769521065056e-05, 3.3102769521065056e-05, 3.3102769521065056e-05, 3.3102769521065056e-05, 3.3102769521065056e-05, 3.3102769521065056e-05, 3.3102769521065056e-05, 3.3102769521065056e-05, 3.3102769521065056e-05, 3.3102769521065056e-05, 3.3102769521065056e-05, 3.3102769521065056e-06, 3.3102770657933434e-07, 3.3102772079018905e-08, 3.3102771634929695e-09, 3.310277052470667e-10, 3.310277052470667e-11, 3.3102769657344933e-12, 3.310276857314276e-13, 3.310276925076912e-14, 3.310276967428559e-15, 3.3102770203681184e-16, 3.3102768880192203e-17, 3.310276929378251e-18, 3.3102770327758275e-19, 3.310276968152342e-20, 3.310277048931699e-21, 3.310276947957503e-22, 3.310276821739758e-23, 3.310276900625849e-24, 3.3102768020182356e-25, 3.310276678758719e-26, 3.310276832833115e-27, 3.3102768328331147e-28, 3.310276953203736e-29, 3.310277103667013e-30, 3.310277197706561e-31, 3.310277315255996e-32, 3.310277315255996e-33, 3.310277269338248e-34, 3.310277326735433e-35, 3.3102771832424705e-36, 3.3102772729255722e-37, 3.310277272925572e-38, 3.310277553185265e-39, 3.310273349289872e-40], "accuracy_train_first": 0.48520953015411594, "accuracy_train_last": 0.991258521940753, "batch_size_eval": 1024, "accuracy_train_std": [0.01903720196890247, 0.01629340666041867, 0.013264137774142227, 0.013061699236148864, 0.011514898792609377, 0.010550006309008104, 0.013471796219589662, 0.013014281548742263, 0.011261460163940692, 0.01145303447008312, 0.012088163154948263, 0.0135211499527816, 0.012192219439812036, 0.011915407440158827, 0.01333912384334414, 0.012032856748178859, 0.011923182105865163, 0.01171719072497009, 0.0137489228564194, 0.012566653713870588, 0.011257223033808047, 0.012396450968684108, 0.012422123265625258, 0.012577798735045819, 0.012436086670034296, 0.0132866461263956, 0.011474733721346402, 0.012114611018433663, 0.01176189651221496, 0.012247475965727539, 0.011525771031562108, 0.01225134417993976, 0.011314949453215904, 0.011903733705182008, 0.011108682461952438, 0.011159998865185344, 0.01149447425700477, 0.011738475925807759, 0.011482301077472955, 0.011601263857579132, 0.012388684205956367, 0.01303997315652905, 0.01107396335391453, 0.012437993011030346, 0.011428519026274776, 0.01117594554644272, 0.011081363969394907, 0.010848018094393739, 0.011311228838798903, 0.011156201035631823, 0.010898086257755386, 0.010748372498609594, 0.010790036340381523, 0.011736229750439831, 0.012253054112038062, 0.010661604731069004, 0.010062532926820609, 0.011486297994722193, 0.00987816219743187, 0.009324989068010062, 0.010101128934342866, 0.010559069481817175, 0.009908651958537634, 0.010493718533418352, 0.011575720270134326, 0.012018657213666637, 0.010618533449172879, 0.010325526380162292, 0.009632810917063217, 0.010666639998308067, 0.01138975028238905, 0.010955761461648264, 0.00918131626030792, 0.010064590149223632, 0.010696836518899305, 0.01152464624074255, 0.010364408206168413, 0.010374989811258947, 0.010592363246058513, 0.012351616549709813, 0.010222795323394047, 0.01056475492510098, 0.010501908677011325, 0.011213037752384546, 0.010719495358967162, 0.01130252914115548, 0.010892254299691366, 0.011328053030615811, 0.010162506658915844, 0.00571186371098568, 0.004544193788186022, 0.005193967335850649, 0.005020261459884491, 0.004892366974253418, 0.004027074903760723, 0.004444620362903567, 0.004476174613688304, 0.004003446406173804, 0.0036885981743269344, 0.004161039180847657, 0.0036578259484955013, 0.003835256986465064, 0.004098344088376097, 0.003687921212401199, 0.004085069768534027, 0.0036290830911005546, 0.004028659573490011, 0.003133899180742195, 0.0031821600418464146, 0.003052281525028883, 0.0032549993991719674, 0.0032479527230437778, 0.0032554454365022255, 0.0030957984662711045, 0.0031394978133266146, 0.0032595849633300623, 0.0032328608192116227, 0.003080733131488172, 0.0031189204657935655, 0.0031451547998613285, 0.0031470890064828672, 0.003033359877621692, 0.0030993920141621687, 0.0031025484289653213, 0.003136939152600134, 0.0030758911630684124, 0.0032007735258897793, 0.0031939045553367952, 0.003197227207880388, 0.0033234041665638346, 0.0032128074292894415, 0.0033826066152585397, 0.0032650838239277357, 0.0030582718371711635, 0.0032693655524298885, 0.003065130282957952, 0.003191820084218178, 0.003117724704303534, 0.0032890652077609828, 0.003233981908751669, 0.003125368782506679], "accuracy_test_std": 0.009914108519535097, "error_valid": [0.5304381588855421, 0.4642834031438253, 0.4275681828878012, 0.3900911262236446, 0.36089426063629515, 0.33955107539533136, 0.33019137095256024, 0.31536938770707834, 0.30877759083207834, 0.30535962208207834, 0.28868599397590367, 0.2865196136106928, 0.2801719573606928, 0.2710975738893072, 0.2572521531438253, 0.2648719879518072, 0.2540989151920181, 0.24873811652861444, 0.24126094220632532, 0.24223750470632532, 0.23018195830195776, 0.23124088149472888, 0.21885000941265065, 0.22525943618222888, 0.21946036097515065, 0.2166939241340362, 0.21506582972515065, 0.21509671498493976, 0.20917556946536142, 0.2057473056287651, 0.2025734775037651, 0.20639883753765065, 0.19822012660015065, 0.2039471362010542, 0.19502570830195776, 0.19609345585466864, 0.18415997976280118, 0.19154596903237953, 0.18789415474397586, 0.18801622505647586, 0.18147443288780118, 0.18413056522966864, 0.1784535603350903, 0.18027431993599397, 0.1797963337725903, 0.17478115587349397, 0.17441494493599397, 0.17549298757530118, 0.17085431570030118, 0.16678481504141573, 0.16976597797439763, 0.1673642813441265, 0.1701013036521084, 0.1633874364646084, 0.16318300545933728, 0.16730398155120485, 0.16767019248870485, 0.1622373282191265, 0.15812811794051207, 0.1613828360316265, 0.15946059629141573, 0.15760895143072284, 0.1638757177146084, 0.16376394248870485, 0.16061070453689763, 0.1672230915850903, 0.16193288780120485, 0.16365216726280118, 0.15858551393072284, 0.16157697195030118, 0.16203436794051207, 0.1618519978350903, 0.1600915380271084, 0.1651684864457832, 0.15963414203689763, 0.15504547486822284, 0.16304181570030118, 0.16671422016189763, 0.15773102174322284, 0.1661950536521084, 0.15406891236822284, 0.16031508847891573, 0.16389630788780118, 0.15762954160391573, 0.15348944606551207, 0.16043715879141573, 0.16324477597891573, 0.15788397731551207, 0.1563073583396084, 0.13480092243975905, 0.1319521249058735, 0.13564511954066272, 0.13441412132906627, 0.13550245905496983, 0.13661138695406627, 0.13441412132906627, 0.13600103539156627, 0.13637754141566272, 0.13758794945406627, 0.13564511954066272, 0.13452589655496983, 0.13599074030496983, 0.13538038874246983, 0.13721144342996983, 0.13540097891566272, 0.13647902155496983, 0.13611281061746983, 0.13343755882906627, 0.13380376976656627, 0.13294927757906627, 0.13282720726656627, 0.13392584007906627, 0.13343755882906627, 0.13380376976656627, 0.13294927757906627, 0.13343755882906627, 0.13233892601656627, 0.13331548851656627, 0.13343755882906627, 0.13331548851656627, 0.13307134789156627, 0.13355962914156627, 0.13270513695406627, 0.13355962914156627, 0.13331548851656627, 0.13355962914156627, 0.13331548851656627, 0.13294927757906627, 0.13270513695406627, 0.13270513695406627, 0.13282720726656627, 0.13319341820406627, 0.13282720726656627, 0.13404791039156627, 0.13270513695406627, 0.13307134789156627, 0.13429205101656627, 0.13319341820406627, 0.13270513695406627, 0.13307134789156627, 0.13319341820406627], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.05582282271682738, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 64, "valid_ratio": 0.15, "learning_rate": 0.00033102769816394393, "optimization": "nesterov_momentum", "nb_data_augmentation": 3, "learning_rate_decay_method": "discrete", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 1.5910374777004374e-08, "rotation_range": [0, 0], "momentum": 0.9665689400442751}, "accuracy_valid_max": 0.8680478750941265, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8668065817959337, "accuracy_valid_std": [0.015699107031640125, 0.012704716675329509, 0.015353994589468928, 0.012742014431904891, 0.018008016750748305, 0.019499731914830162, 0.024222484187120405, 0.02088689809184252, 0.018530471330971257, 0.018207747536018754, 0.019637986492945875, 0.016898043255028774, 0.01544277101365796, 0.01748026133790194, 0.016565460671787376, 0.016123241955112864, 0.01354877323991343, 0.010511395668597526, 0.01289791121292283, 0.010900144539096166, 0.016030568590935428, 0.012476764203935295, 0.009347268494673723, 0.011103496134359864, 0.011615427722132603, 0.010708150204887516, 0.012237019123295612, 0.01064048527560083, 0.011339043854873977, 0.010053361490621918, 0.010879854000283724, 0.010502680145636158, 0.012071583111679657, 0.01126406791006784, 0.015634688439855302, 0.01362233101935272, 0.013885800050776831, 0.012994814005399973, 0.012512255068556518, 0.0106270412565233, 0.014086153339912963, 0.016776200294752668, 0.014287227611467345, 0.01481526429023232, 0.012714063366375743, 0.013624661879371409, 0.012152807124728297, 0.012884809189814418, 0.014889491109131105, 0.015047051430774587, 0.014548106641114084, 0.019073068222533317, 0.016718244345814204, 0.016521301898113915, 0.014187208094541432, 0.015041715649242188, 0.015432734406515453, 0.014164912170550737, 0.016578422231951797, 0.01621593492668239, 0.012245127679950925, 0.013519044867696634, 0.013252268565481952, 0.0093944510345834, 0.015306493192398284, 0.01178888095667367, 0.017672968250419235, 0.010575953460452817, 0.012279449779699896, 0.012968722886317684, 0.011731331451818254, 0.012670107162739575, 0.012276394237243936, 0.01574201948565394, 0.010556377115876554, 0.013652072851792391, 0.009121491306135052, 0.009545368946716775, 0.011499164614052648, 0.008805136704067284, 0.01058414352292961, 0.012013416772554481, 0.013226263630275044, 0.009247219554671057, 0.009760874881657887, 0.010759994528890164, 0.009734171675185215, 0.008504732850484257, 0.009487070128189688, 0.012580533580384872, 0.01494448358121094, 0.012619736658739068, 0.01481913455158362, 0.014638241564839798, 0.01506034142966253, 0.014419578304581687, 0.014808002851802348, 0.014421432946189038, 0.01496409489073023, 0.01442683478978473, 0.014630390120243055, 0.015361311729124694, 0.01563979080206826, 0.015238242467382503, 0.013986736389492946, 0.015999773433220024, 0.014134139531896253, 0.013003489347802783, 0.012828285600692324, 0.013190628440487913, 0.01341979300229865, 0.013243657538595814, 0.01351593852557822, 0.013743493243032843, 0.013476723384645787, 0.013221678453088245, 0.01234700982480247, 0.012865960839919512, 0.013293612366458955, 0.013240396900301231, 0.012686629653660596, 0.012544318662949009, 0.012224677758127236, 0.013083891176003469, 0.013159116340605565, 0.013461121373339113, 0.013657004916718743, 0.013090839326983256, 0.012166027898021662, 0.01305463830410434, 0.012754853505631877, 0.012650613048191149, 0.012949636215402446, 0.013732385153894766, 0.01308200431846332, 0.012696022627116018, 0.013585937933483104, 0.01315868657372954, 0.013172813776080969, 0.013390623680768595, 0.01349855319475052], "accuracy_valid": [0.46956184111445787, 0.5357165968561747, 0.5724318171121988, 0.6099088737763554, 0.6391057393637049, 0.6604489246046686, 0.6698086290474398, 0.6846306122929217, 0.6912224091679217, 0.6946403779179217, 0.7113140060240963, 0.7134803863893072, 0.7198280426393072, 0.7289024261106928, 0.7427478468561747, 0.7351280120481928, 0.7459010848079819, 0.7512618834713856, 0.7587390577936747, 0.7577624952936747, 0.7698180416980422, 0.7687591185052711, 0.7811499905873494, 0.7747405638177711, 0.7805396390248494, 0.7833060758659638, 0.7849341702748494, 0.7849032850150602, 0.7908244305346386, 0.7942526943712349, 0.7974265224962349, 0.7936011624623494, 0.8017798733998494, 0.7960528637989458, 0.8049742916980422, 0.8039065441453314, 0.8158400202371988, 0.8084540309676205, 0.8121058452560241, 0.8119837749435241, 0.8185255671121988, 0.8158694347703314, 0.8215464396649097, 0.819725680064006, 0.8202036662274097, 0.825218844126506, 0.825585055064006, 0.8245070124246988, 0.8291456842996988, 0.8332151849585843, 0.8302340220256024, 0.8326357186558735, 0.8298986963478916, 0.8366125635353916, 0.8368169945406627, 0.8326960184487951, 0.8323298075112951, 0.8377626717808735, 0.8418718820594879, 0.8386171639683735, 0.8405394037085843, 0.8423910485692772, 0.8361242822853916, 0.8362360575112951, 0.8393892954631024, 0.8327769084149097, 0.8380671121987951, 0.8363478327371988, 0.8414144860692772, 0.8384230280496988, 0.8379656320594879, 0.8381480021649097, 0.8399084619728916, 0.8348315135542168, 0.8403658579631024, 0.8449545251317772, 0.8369581842996988, 0.8332857798381024, 0.8422689782567772, 0.8338049463478916, 0.8459310876317772, 0.8396849115210843, 0.8361036921121988, 0.8423704583960843, 0.8465105539344879, 0.8395628412085843, 0.8367552240210843, 0.8421160226844879, 0.8436926416603916, 0.865199077560241, 0.8680478750941265, 0.8643548804593373, 0.8655858786709337, 0.8644975409450302, 0.8633886130459337, 0.8655858786709337, 0.8639989646084337, 0.8636224585843373, 0.8624120505459337, 0.8643548804593373, 0.8654741034450302, 0.8640092596950302, 0.8646196112575302, 0.8627885565700302, 0.8645990210843373, 0.8635209784450302, 0.8638871893825302, 0.8665624411709337, 0.8661962302334337, 0.8670507224209337, 0.8671727927334337, 0.8660741599209337, 0.8665624411709337, 0.8661962302334337, 0.8670507224209337, 0.8665624411709337, 0.8676610739834337, 0.8666845114834337, 0.8665624411709337, 0.8666845114834337, 0.8669286521084337, 0.8664403708584337, 0.8672948630459337, 0.8664403708584337, 0.8666845114834337, 0.8664403708584337, 0.8666845114834337, 0.8670507224209337, 0.8672948630459337, 0.8672948630459337, 0.8671727927334337, 0.8668065817959337, 0.8671727927334337, 0.8659520896084337, 0.8672948630459337, 0.8669286521084337, 0.8657079489834337, 0.8668065817959337, 0.8672948630459337, 0.8669286521084337, 0.8668065817959337], "seed": 887254120, "model": "residualv3", "loss_std": [0.34554538130760193, 0.12134060263633728, 0.1196354404091835, 0.12145961821079254, 0.11975736916065216, 0.11720842123031616, 0.11624793708324432, 0.11599725484848022, 0.11419708281755447, 0.11483325064182281, 0.11257699877023697, 0.11155125498771667, 0.1112600788474083, 0.11043862253427505, 0.10767199844121933, 0.10747065395116806, 0.10746672749519348, 0.10621349513530731, 0.10562173277139664, 0.10462862998247147, 0.10495471954345703, 0.10450311750173569, 0.10276735574007034, 0.10351986438035965, 0.10111039131879807, 0.10266829282045364, 0.09791542589664459, 0.09927031397819519, 0.09660618752241135, 0.0982070118188858, 0.09417039901018143, 0.09713269770145416, 0.0932680293917656, 0.09217517822980881, 0.09023179858922958, 0.0895613506436348, 0.08749523758888245, 0.08808660507202148, 0.08838271349668503, 0.08470191061496735, 0.08404545485973358, 0.08248291909694672, 0.08224758505821228, 0.0809403583407402, 0.08137311041355133, 0.08207016438245773, 0.07888850569725037, 0.07862664014101028, 0.07852199673652649, 0.07805110514163971, 0.07600802928209305, 0.0757385715842247, 0.07546068727970123, 0.07231350243091583, 0.07236993312835693, 0.07070525735616684, 0.070547915995121, 0.06885410100221634, 0.06893087923526764, 0.06970559805631638, 0.06679743528366089, 0.06570614129304886, 0.06742189824581146, 0.06636720150709152, 0.06164202094078064, 0.06483951210975647, 0.06553155183792114, 0.060304634273052216, 0.05971840023994446, 0.05979137122631073, 0.05893265828490257, 0.05790859833359718, 0.056578025221824646, 0.05842138081789017, 0.055062614381313324, 0.05557432398200035, 0.054192762821912766, 0.05383410304784775, 0.05513898655772209, 0.05261494591832161, 0.05337192863225937, 0.051807839423418045, 0.05073797330260277, 0.050293758511543274, 0.05030163750052452, 0.046576548367738724, 0.05048332363367081, 0.04887738451361656, 0.050151776522397995, 0.058304015547037125, 0.04345274716615677, 0.040933068841695786, 0.039467424154281616, 0.040495533496141434, 0.03902769088745117, 0.038643576204776764, 0.037238989025354385, 0.03823769837617874, 0.037964146584272385, 0.03712472692131996, 0.03619108349084854, 0.03729745373129845, 0.036218978464603424, 0.036197762936353683, 0.03605721518397331, 0.035543810576200485, 0.03556475043296814, 0.035492707043886185, 0.03324367478489876, 0.03349018469452858, 0.03364138305187225, 0.03331012651324272, 0.032893527299165726, 0.03366386890411377, 0.033729225397109985, 0.034548066556453705, 0.034080974757671356, 0.034692902117967606, 0.035473600029945374, 0.03283820301294327, 0.03432265669107437, 0.034631241112947464, 0.0343056321144104, 0.0346795991063118, 0.034769512712955475, 0.03428278863430023, 0.03520803526043892, 0.033337824046611786, 0.033888079226017, 0.03221447765827179, 0.03419345244765282, 0.035186830908060074, 0.03278939425945282, 0.0335547998547554, 0.03435320779681206, 0.035211142152547836, 0.036051537841558456, 0.03360646590590477, 0.033485304564237595, 0.0326576791703701, 0.03347540646791458]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:28 2016", "state": "available"}], "summary": "ece6d41d7ddca125b6e0dbcca0890979"}