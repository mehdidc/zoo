{"content": {"hp_model": {"f0": 32, "f1": 64, "f2": 32, "f3": 32, "nonlin": "leaky_rectify", "nbg1": 8, "nbg3": 7, "nbg2": 3, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.021239662044632266, 0.013861008172805408, 0.016572880899323343, 0.013669492693055587, 0.015230233364068896, 0.02361048411616812, 0.02032868406671806, 0.018084305132820857, 0.017080545772533522, 0.0194540150439554, 0.02559292719561579, 0.014214867659001222, 0.014276559255773513, 0.015506256010820644, 0.01526271677929126, 0.020183153900810127, 0.014959281230915921, 0.025251311915410184, 0.01758851146104805, 0.017957014712827502, 0.019022711062284398, 0.01546461736282259, 0.013432380294285359, 0.013517559273131592, 0.021808572371702035, 0.01834652130342749, 0.014836423422884134, 0.023449042537361114, 0.01578272785758051, 0.01688746569336555, 0.02326089953935424, 0.018366539775096457, 0.018432158525834266, 0.014553152245304426, 0.012887011437222588, 0.019240837343619106, 0.01636107302711408, 0.01945732013542156, 0.013317146528436938, 0.017207236835432828, 0.011467334776261643, 0.0161775386282653, 0.01761593481335173, 0.016796238277436676, 0.01338846153926066, 0.010454844963179163, 0.015872525184407932, 0.01266845252472064, 0.014823946443615319, 0.0096064675806539, 0.014126543751472521, 0.012081271655170171, 0.020112049528054253, 0.018554861372743387, 0.012469326699850089, 0.020094768825704747, 0.018987660931706177, 0.016070541747373136, 0.01916896846026643, 0.020548376420356156, 0.01040510734026017, 0.017560216560682653, 0.010550749892470578, 0.015996548042027967, 0.019435633958925405, 0.018882631937497008, 0.01742674329980041, 0.026538929453588617, 0.022722035878423245, 0.01610990960943802, 0.017663552053318772, 0.012363447200622156, 0.01713358044222361, 0.015064035649964495, 0.013539853074964477, 0.011510593108347775, 0.025671781804191492, 0.01941972783901548, 0.016161735864571886, 0.01956863706670352, 0.01635548920060853, 0.017117792547771925, 0.02397499469016184, 0.01560509236212123, 0.012182607283696453, 0.016676896867213256, 0.024669415283329624, 0.01949185837113035, 0.018065974688818705, 0.021569528879560738, 0.015481109668614873, 0.018522464817384406, 0.021723303198069647, 0.02224786832040179, 0.018618902950810673, 0.01619260557988908, 0.00957243445334881, 0.009461324840762567, 0.013823750250057668, 0.01394346884452585, 0.010708461582000974, 0.013093701921249616, 0.01613543243620725, 0.014135055195140805, 0.010439973139823477, 0.017181491278456404, 0.015768392402487372, 0.016448702595676806, 0.013909572593180139, 0.018405784208006627, 0.01858868427854872, 0.02352544344824739, 0.018389665709262617, 0.016027101542721075, 0.021899080951273298, 0.012355043656745886, 0.012688002338915195, 0.008493776362256861, 0.012429281349251704, 0.016654090385647368, 0.015172252279754593, 0.02022056671040511, 0.01586931202711148, 0.013046855877674601, 0.016014649301106455, 0.01970125966871562, 0.016887682924716426, 0.012918659017471298, 0.01892357322185657, 0.015959555872248735, 0.011752672969752572, 0.012870157377814854, 0.016965312338259982, 0.014278319462458574, 0.011366832828334804, 0.014954028040666182, 0.015743797959385044, 0.018950093986900004, 0.013420104081078224, 0.017827810268566075, 0.015052933744248672, 0.01298780549005737, 0.026379375069613697, 0.011644148465380501, 0.022956615631594988, 0.012797336818573512, 0.01709506460171903, 0.011369952551190462, 0.015876508631751603, 0.02404325725031861, 0.01376342615788072, 0.013420712678603555, 0.020394667887010606, 0.014479140726978753, 0.018574171392962385, 0.02398734089837025, 0.023242381296130342, 0.01826759214606392, 0.012350465695136665, 0.02238540160652814, 0.011873290524286798, 0.02160690836969512], "moving_avg_accuracy_train": [0.058079261258767056, 0.1218622459279254, 0.18464835386674044, 0.24412906746031743, 0.30029316531699884, 0.3524054261950904, 0.4008708174995385, 0.44525453548541893, 0.48656234677745475, 0.5258389502664682, 0.5625342627137508, 0.5948668972293358, 0.6261961220505144, 0.6545064287788794, 0.6795999464249394, 0.7014050793087466, 0.7238546105124696, 0.7442871613256098, 0.7623204929966664, 0.7797806394161304, 0.7952435469757633, 0.8095531499770611, 0.8229549151115532, 0.8340748905623764, 0.8449941465573569, 0.8550399688432971, 0.8648020050315957, 0.8740552646094162, 0.8815301570069094, 0.8887014112455411, 0.8950392465710147, 0.9015687622401407, 0.9078753346768409, 0.913151468469908, 0.9172326351265163, 0.9217102947520153, 0.9250564865185081, 0.9289214247702656, 0.9316558576266185, 0.9344865458580504, 0.937559576799654, 0.9395697033816395, 0.9419043369851975, 0.9444703927950295, 0.9469355919452977, 0.9492030993055391, 0.9502837497155168, 0.9523351320344782, 0.95459521656082, 0.9566780847107089, 0.9582387709563231, 0.9598084741428521, 0.9610399895988604, 0.9618203993806688, 0.9627879072462194, 0.9629587945335484, 0.9632475238207343, 0.9635422934601633, 0.9643445513153743, 0.9649758304838554, 0.9663718068093163, 0.9670746919391082, 0.9675515035856828, 0.9682295331366751, 0.9686351105885115, 0.9697279739701826, 0.9696071053291628, 0.9698819010081791, 0.9708012211740646, 0.9712868124483618, 0.972300481499991, 0.9733475701797722, 0.9743991959368042, 0.9748457521240854, 0.9743780470378766, 0.9750615220959937, 0.9751280587245266, 0.9758994011771016, 0.9762982073391533, 0.9768594568802471, 0.9773669066160412, 0.9774330584758933, 0.9772322505807312, 0.977690831251248, 0.9780826275154274, 0.9786377123437111, 0.9791673353795965, 0.9789604744595309, 0.9791741170802538, 0.979278256077055, 0.9785046285705861, 0.9788476332349837, 0.9787843496722365, 0.9795690262371742, 0.9794405427718179, 0.9798758597743981, 0.9801305333945957, 0.9802015574360885, 0.9805887468555934, 0.9813557801676808, 0.9815857667330924, 0.9817603107050858, 0.9819613978631672, 0.9817075734780594, 0.981765088786215, 0.9819098945647549, 0.9817077234856788, 0.9815515263955089, 0.9816526563440902, 0.9818970970704232, 0.9823333325634085, 0.9832862693213718, 0.9838975908250043, 0.9837641503794547, 0.9840415823272513, 0.9842006263255154, 0.9845552823679823, 0.9845861543538215, 0.9844186987387143, 0.9844517475387078, 0.9843651619205882, 0.9842476712856999, 0.9843581325047674, 0.9839786390448039, 0.9845229405784465, 0.9846827129254099, 0.9845311420412299, 0.9848411207680778, 0.9847899665401073, 0.9850067055992288, 0.9849505104857345, 0.985046491356227, 0.9846655552777748, 0.9851504297345395, 0.9852705965075326, 0.9850138063865873, 0.9850848204276996, 0.9850930015909097, 0.9854258494223134, 0.9852906817408332, 0.9851550078370064, 0.9854212011747527, 0.9857492389799427, 0.9854701251998425, 0.9857399003263606, 0.9860429715651808, 0.9862019115349085, 0.9860122809838079, 0.985662721224759, 0.9858619753285196, 0.985460052868342, 0.9855330533839257, 0.9860800596515225, 0.9861980163340263, 0.9862902625042412, 0.9862754836121597, 0.9862227271771619, 0.9865658713856639, 0.9864305256530591, 0.9867482397163523, 0.9866993969935636, 0.9868738943846926], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 228760719, "moving_var_accuracy_train": [0.030358805295277087, 0.06393734696550418, 0.09302247041989457, 0.1155618209843153, 0.13239529187837779, 0.14359695229497646, 0.1503773044541183, 0.15306880380895793, 0.1531189408917081, 0.15169091103723598, 0.1486407335339457, 0.14318525347301675, 0.1377004110767787, 0.13114363117258776, 0.12369642970600711, 0.11560596111613083, 0.10858119806592013, 0.10148048045391214, 0.09425924186894592, 0.08757702809895344, 0.08097123888083785, 0.07471699763524682, 0.06886176365020294, 0.06308847197142486, 0.057852696137637646, 0.05297569343248012, 0.04853580024410697, 0.04445282553502701, 0.04051040912871123, 0.03692221020203595, 0.033591502591347744, 0.030616063506073217, 0.027912412858560016, 0.025371709862925116, 0.022984442168143708, 0.02086644287282575, 0.018880571579586475, 0.017126954150836912, 0.015481552843166341, 0.014005512721621812, 0.012689953121972108, 0.01145732328965534, 0.01036064558725557, 0.009383842810302568, 0.008500153390926657, 0.007696412358492731, 0.006937281370420722, 0.006281426758145578, 0.005699255920926906, 0.005168375386402608, 0.004673459521777592, 0.004228289282444026, 0.0038191100270651085, 0.0034426803792064766, 0.0031068369845149513, 0.0027964161082481924, 0.0025175247788348835, 0.002266554303214357, 0.002045691431889152, 0.0018447089091972598, 0.0016777767673887597, 0.001514445518201027, 0.001365047110497707, 0.0012326799160961076, 0.0011108923621114396, 0.0010105522792392759, 0.0009096285343707859, 0.0008193452949205609, 0.0007450171115351416, 0.0006726375903526891, 0.0006146215558334998, 0.0005630269525800844, 0.0005166775079177542, 0.0004668044689815708, 0.00042209275451240427, 0.00038408772245677753, 0.0003457187943175284, 0.00031650163749807566, 0.00028628289094228214, 0.000260489611274457, 0.00023675819725622822, 0.00021312176214766248, 0.00019217250022973164, 0.00017484791628910294, 0.0001587446634738169, 0.00014564326962575265, 0.00013360344770444197, 0.00012062822589625126, 0.00010897619183112986, 9.817617702390957e-05, 9.374505499040586e-05, 8.542941928955219e-05, 7.692252064442274e-05, 7.477172438404184e-05, 6.744312395346756e-05, 6.240431959273882e-05, 5.674761550888638e-05, 5.111825368822751e-05, 4.7355669138593286e-05, 4.7915163141399794e-05, 4.359969120968866e-05, 3.951391247215311e-05, 3.5926445631244736e-05, 3.291364243439835e-05, 2.9652050287008695e-05, 2.687556367979452e-05, 2.4555865618748133e-05, 2.2319856835671338e-05, 2.0179916550604657e-05, 1.8699686313755855e-05, 1.8542430330441996e-05, 2.486098347949681e-05, 2.57383109587785e-05, 2.3324737035476726e-05, 2.1684979702852506e-05, 1.974413667302195e-05, 1.890175118184443e-05, 1.7020153779246935e-05, 1.5570510848600433e-05, 1.4023289772369461e-05, 1.268843441851885e-05, 1.1543827420245007e-05, 1.0499259806481437e-05, 1.0745471401229827e-05, 1.233730169683744e-05, 1.1333316352841448e-05, 1.0406748313937171e-05, 1.0230854782427354e-05, 9.23132009953801e-06, 8.730970467323784e-06, 7.886294437617148e-06, 7.180575941360145e-06, 7.768529010022649e-06, 9.107605258427017e-06, 8.32680521256845e-06, 8.087595187247068e-06, 7.324222614838397e-06, 6.5924027362377905e-06, 6.930251572445161e-06, 6.40165913425107e-06, 5.927159894442317e-06, 5.972173942543167e-06, 6.3434357629931995e-06, 6.410232706870349e-06, 6.4242170061743595e-06, 6.608464887756989e-06, 6.174975624774871e-06, 5.881115775494088e-06, 6.3927324242617456e-06, 6.110778962625e-06, 6.953576042318983e-06, 6.306180115566413e-06, 8.368504815120694e-06, 7.656878344134158e-06, 6.9677747129946684e-06, 6.272962982555626e-06, 5.67071585720316e-06, 6.163375801938871e-06, 5.7119044277538944e-06, 6.04919401910703e-06, 5.465745121321134e-06, 5.193214664786646e-06], "duration": 136521.370228, "accuracy_train": [0.5807926125876707, 0.6959091079503507, 0.749723325316076, 0.7794554898025102, 0.8057700460271319, 0.8214157740979143, 0.8370593392395718, 0.8447079973583426, 0.8583326484057769, 0.8793283816675894, 0.8927920747392949, 0.8858606078696014, 0.9081591454411223, 0.9092991893341639, 0.9054416052394795, 0.8976512752630121, 0.9259003913459765, 0.9281801186438722, 0.9246204780361758, 0.9369219571913067, 0.9344097150124585, 0.9383395769887413, 0.9435708013219823, 0.934154669619786, 0.9432674505121816, 0.9454523694167589, 0.9526603307262828, 0.9573346008098007, 0.9488041885843485, 0.9532426993932264, 0.9520797645002769, 0.9603344032622739, 0.9646344866071429, 0.9606366726075121, 0.9539631350359912, 0.9620092313815062, 0.9551722124169435, 0.9637058690360835, 0.9562657533337948, 0.9599627399409376, 0.9652168552740864, 0.9576608426195091, 0.9629160394172205, 0.967564895083518, 0.9691223842977114, 0.9696106655477114, 0.9600096034053157, 0.9707975729051311, 0.9749359772978959, 0.9754238980597084, 0.9722849471668512, 0.9739358028216132, 0.9721236287029347, 0.9688440874169435, 0.9714954780361758, 0.9644967801195091, 0.965846087405408, 0.9661952202150241, 0.9715648720122739, 0.9706573430001846, 0.9789355937384644, 0.9734006581072352, 0.9718428084048542, 0.9743317990956073, 0.9722853076550388, 0.9795637444052234, 0.9685192875599853, 0.9723550621193245, 0.9790751026670359, 0.9756571339170359, 0.9814235029646549, 0.9827713682978036, 0.9838638277500923, 0.9788647578096161, 0.9701687012619971, 0.9812127976190477, 0.9757268883813216, 0.9828414832502769, 0.9798874627976191, 0.9819107027500923, 0.9819339542381875, 0.9780284252145626, 0.975424979524271, 0.9818180572858989, 0.9816087938930418, 0.9836334757982651, 0.9839339427025655, 0.9770987261789406, 0.9810969006667589, 0.9802155070482651, 0.9715419810123662, 0.9819346752145626, 0.9782147976075121, 0.9866311153216132, 0.9782841915836102, 0.9837937127976191, 0.982422595976375, 0.9808407738095238, 0.984073451631137, 0.9882590799764673, 0.9836556458217978, 0.9833312064530271, 0.9837711822858989, 0.9794231540120893, 0.9822827265596161, 0.9832131465716132, 0.9798881837739941, 0.9801457525839794, 0.9825628258813216, 0.9840970636074198, 0.9862594520002769, 0.9918627001430418, 0.9893994843576966, 0.9825631863695091, 0.9865384698574198, 0.985632022309893, 0.9877471867501846, 0.984864002226375, 0.9829115982027501, 0.984749186738649, 0.9835858913575121, 0.9831902555717055, 0.985352283476375, 0.9805631979051311, 0.9894216543812293, 0.9861206640480805, 0.9831670040836102, 0.9876309293097084, 0.9843295784883721, 0.9869573571313216, 0.9844447544642857, 0.9859103191906607, 0.9812371305717055, 0.9895142998454227, 0.9863520974644703, 0.9827026952980805, 0.9857239467977114, 0.9851666320598007, 0.9884214799049464, 0.9840741726075121, 0.9839339427025655, 0.9878169412144703, 0.9887015792266519, 0.9829581011789406, 0.9881678764650241, 0.9887706127145626, 0.9876323712624585, 0.9843056060239018, 0.9825166833933187, 0.9876552622623662, 0.9818427507267442, 0.9861900580241787, 0.991003116059893, 0.9872596264765596, 0.9871204780361758, 0.9861424735834257, 0.9857479192621816, 0.9896541692621816, 0.9852124140596161, 0.9896076662859912, 0.9862598124884644, 0.9884443709048542], "end": "2016-01-25 23:55:48.476000", "learning_rate_per_epoch": [0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802, 0.00021256896434351802], "accuracy_valid": [0.575474750564759, 0.6806111163403614, 0.7123920486634037, 0.7245784897402108, 0.7352089020143072, 0.7318026990775602, 0.7380885848079819, 0.7302349044615963, 0.7330837019954819, 0.7425140013177711, 0.7496661450489458, 0.7347515060240963, 0.7536223997552711, 0.7356971832643072, 0.7493602339043675, 0.7382827207266567, 0.7440597350338856, 0.7458922604480422, 0.7381091749811747, 0.753307664250753, 0.7554034497364458, 0.7537341749811747, 0.7589126035391567, 0.7514251341302711, 0.7571639095444277, 0.7632365399096386, 0.7571830289909638, 0.7643763530685241, 0.7542533414909638, 0.7588111233998494, 0.7599921169051205, 0.7702048428087349, 0.7610186841114458, 0.7602141966302711, 0.7579257459525602, 0.7579772213855422, 0.761242234563253, 0.7622393872364458, 0.7563182417168675, 0.7684855633471386, 0.767711961125753, 0.761730515813253, 0.7651999599962349, 0.7684958584337349, 0.7745787838855422, 0.7730933499623494, 0.761120164250753, 0.7759318524096386, 0.7725241787462349, 0.772594773625753, 0.7648940488516567, 0.771374070500753, 0.7685164486069277, 0.7681296474962349, 0.7657382459525602, 0.7677634365587349, 0.7593905897025602, 0.7643351727221386, 0.7737654720444277, 0.7722197383283133, 0.7747008541980422, 0.7690856198230422, 0.7717196912650602, 0.7677428463855422, 0.7768687052899097, 0.7794218867658133, 0.7650881847703314, 0.7743464090737951, 0.7758921427899097, 0.7720461925828314, 0.7818529979292168, 0.7820559582078314, 0.7882109492658133, 0.7744670086596386, 0.769543015813253, 0.7780173428087349, 0.7784967996987951, 0.7805205195783133, 0.7841929240399097, 0.7765833843185241, 0.7809573253953314, 0.7739787274096386, 0.775292086314006, 0.7773055111069277, 0.7755347562123494, 0.7792792262801205, 0.7825045298381024, 0.7791880412274097, 0.7791571559676205, 0.7816603327371988, 0.7632865446159638, 0.7796660273908133, 0.7687708843185241, 0.7866755106362951, 0.7812529414533133, 0.7832869564194277, 0.7747508589043675, 0.7758700818900602, 0.7822898037462349, 0.7873358669051205, 0.778698289250753, 0.7848532803087349, 0.7765524990587349, 0.7752097256212349, 0.7728683287838856, 0.7854136271649097, 0.7716490963855422, 0.7707034191453314, 0.773937547063253, 0.7784350291792168, 0.784935641001506, 0.7924437005835843, 0.789330172251506, 0.7819338878953314, 0.7864107798381024, 0.7828692700489458, 0.7897669780685241, 0.7777320218373494, 0.7806925945971386, 0.7835310970444277, 0.7830325207078314, 0.7826663097703314, 0.7833781414721386, 0.7757788968373494, 0.7889124858810241, 0.7870211314006024, 0.7790042003953314, 0.7885359798569277, 0.7780894084149097, 0.7808558452560241, 0.7780982327748494, 0.7832869564194277, 0.7867564006024097, 0.7884242046310241, 0.7851077160203314, 0.7777834972703314, 0.7819441829819277, 0.785423922251506, 0.7890036709337349, 0.7763289486069277, 0.7773864010730422, 0.7866946300828314, 0.7912229974585843, 0.7796042568712349, 0.7915789133094879, 0.7880271084337349, 0.7868578807417168, 0.7846091396837349, 0.7774172863328314, 0.7918436441076807, 0.7868372905685241, 0.7770201901355422, 0.7897272684487951, 0.7809676204819277, 0.7796351421310241, 0.7789747858621988, 0.7844782450112951, 0.7854033320783133, 0.7853518566453314, 0.7888727762612951, 0.7808249599962349, 0.7885065653237951], "accuracy_test": 0.4864058514030612, "start": "2016-01-24 10:00:27.106000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0], "accuracy_train_last": 0.9884443709048542, "batch_size_eval": 1024, "accuracy_train_std": [0.02768402664387891, 0.027713368483645147, 0.028727026036368408, 0.03175983575477352, 0.03304944090540716, 0.03446446065776237, 0.03524014873115793, 0.03401408044136957, 0.03367262862778989, 0.03085676437681206, 0.027339965213883266, 0.031562307607927144, 0.02550006949463083, 0.024407235270426, 0.023114093678279653, 0.023978905845921075, 0.020301538643507786, 0.018673024458741735, 0.020991983341479618, 0.01797988033596321, 0.019363177362968036, 0.016012435415784395, 0.016053348749862036, 0.018201986718484987, 0.016211600873865275, 0.01603297842091679, 0.013759548906972254, 0.012722545118813338, 0.014371998510394177, 0.013980099280151056, 0.014801711187208343, 0.011838229323870124, 0.011371689315731187, 0.012703347216742433, 0.013156415068212659, 0.011761911532689632, 0.012568886388934471, 0.010779593897973408, 0.011610196030091335, 0.01010929425255918, 0.010243007838056001, 0.011571358460963665, 0.009563355186960642, 0.01050805549880423, 0.00995540053018785, 0.010789966534992633, 0.011047823816756905, 0.008799828671342186, 0.008326270747127183, 0.008353472760796626, 0.008429603848019659, 0.00822367170070635, 0.007291581293759318, 0.009767985792509684, 0.009015379078258562, 0.01024862913170849, 0.008277552873259703, 0.008317230367113066, 0.00824661722369999, 0.008518168760449068, 0.007188841805640694, 0.008420149385475579, 0.009861261627688961, 0.007778775299280063, 0.007930752677040891, 0.00658293974199721, 0.00876198794753455, 0.0070373405548185795, 0.00681923419672294, 0.007187130267208464, 0.0075288370168405125, 0.00627930422360711, 0.005664156258456783, 0.006971205278844919, 0.008380804898338846, 0.0064030731292318115, 0.00628426996461556, 0.005205493490087866, 0.00699699826597192, 0.005968776521759428, 0.0071047088985374705, 0.006925101749210681, 0.007471076386704865, 0.006013920734208059, 0.005798516270542822, 0.003508869210718391, 0.005270116531852163, 0.007945535778936093, 0.007236392604085859, 0.006891333459179579, 0.008422461141621103, 0.0059928393673810915, 0.006719949591100283, 0.004458849775135659, 0.007490716183585141, 0.006194896377517002, 0.005342203560016682, 0.005997081216769276, 0.0055931840736389675, 0.005046409878335691, 0.005456459812535491, 0.005620971283810991, 0.005307704518022314, 0.0060871625161961545, 0.005556328918617014, 0.005477410846373975, 0.006119351013322536, 0.006376600084362874, 0.004989463064118648, 0.005330845443482844, 0.0051664693314377045, 0.0034348772811377474, 0.0039031641220053873, 0.004865031998376776, 0.0051758202792014, 0.005183733210067802, 0.004776882446615672, 0.005931043660438513, 0.005951047750529278, 0.005102450662433992, 0.005609059426139122, 0.005096952264288927, 0.005537989558671083, 0.006638948221954514, 0.0038954377938237297, 0.004849520695416295, 0.004953603657831058, 0.0039864344459439194, 0.004976869249503226, 0.0044607157077728955, 0.005860897221132633, 0.004805312664399352, 0.005345249426218596, 0.0029162399573286254, 0.0040590348540438595, 0.003577895511232491, 0.0048038071824053515, 0.004982550076271719, 0.004844602777336667, 0.004892128815635496, 0.005534902999016353, 0.004369926324434574, 0.004111667805817398, 0.005148952554222835, 0.004281886161097455, 0.003943892233768933, 0.00396814858431581, 0.006280628310585893, 0.0046416702457947445, 0.003691239144948393, 0.0045038852468661265, 0.004137468713376451, 0.0031508540350421305, 0.0036306923004435234, 0.0035241940463166437, 0.004221530716102214, 0.004753019927629273, 0.0034904381007903836, 0.004584325927933283, 0.0038182899845973666, 0.0037868377162956685, 0.004214220195345584], "accuracy_test_std": 0.01711700856334873, "error_valid": [0.42452524943524095, 0.3193888836596386, 0.28760795133659633, 0.2754215102597892, 0.2647910979856928, 0.26819730092243976, 0.2619114151920181, 0.26976509553840367, 0.2669162980045181, 0.2574859986822289, 0.2503338549510542, 0.26524849397590367, 0.24637760024472888, 0.2643028167356928, 0.25063976609563254, 0.2617172792733433, 0.25594026496611444, 0.25410773955195776, 0.2618908250188253, 0.24669233574924698, 0.2445965502635542, 0.24626582501882532, 0.24108739646084332, 0.24857486586972888, 0.2428360904555723, 0.23676346009036142, 0.2428169710090362, 0.23562364693147586, 0.2457466585090362, 0.24118887660015065, 0.24000788309487953, 0.2297951571912651, 0.2389813158885542, 0.23978580336972888, 0.24207425404743976, 0.24202277861445776, 0.23875776543674698, 0.2377606127635542, 0.24368175828313254, 0.23151443665286142, 0.23228803887424698, 0.23826948418674698, 0.2348000400037651, 0.2315041415662651, 0.22542121611445776, 0.22690665003765065, 0.23887983574924698, 0.22406814759036142, 0.2274758212537651, 0.22740522637424698, 0.23510595114834332, 0.22862592949924698, 0.2314835513930723, 0.2318703525037651, 0.23426175404743976, 0.2322365634412651, 0.24060941029743976, 0.23566482727786142, 0.2262345279555723, 0.22778026167168675, 0.22529914580195776, 0.23091438017695776, 0.22828030873493976, 0.23225715361445776, 0.2231312947100903, 0.22057811323418675, 0.23491181522966864, 0.22565359092620485, 0.2241078572100903, 0.22795380741716864, 0.2181470020707832, 0.21794404179216864, 0.21178905073418675, 0.22553299134036142, 0.23045698418674698, 0.2219826571912651, 0.22150320030120485, 0.21947948042168675, 0.2158070759600903, 0.22341661568147586, 0.21904267460466864, 0.22602127259036142, 0.22470791368599397, 0.2226944888930723, 0.22446524378765065, 0.22072077371987953, 0.21749547016189763, 0.2208119587725903, 0.22084284403237953, 0.21833966726280118, 0.2367134553840362, 0.22033397260918675, 0.23122911568147586, 0.21332448936370485, 0.21874705854668675, 0.2167130435805723, 0.22524914109563254, 0.22412991810993976, 0.2177101962537651, 0.21266413309487953, 0.22130171074924698, 0.2151467196912651, 0.2234475009412651, 0.2247902743787651, 0.22713167121611444, 0.2145863728350903, 0.22835090361445776, 0.22929658085466864, 0.22606245293674698, 0.2215649708207832, 0.21506435899849397, 0.20755629941641573, 0.21066982774849397, 0.21806611210466864, 0.21358922016189763, 0.2171307299510542, 0.21023302193147586, 0.22226797816265065, 0.21930740540286142, 0.2164689029555723, 0.21696747929216864, 0.21733369022966864, 0.21662185852786142, 0.22422110316265065, 0.21108751411897586, 0.21297886859939763, 0.22099579960466864, 0.2114640201430723, 0.2219105915850903, 0.21914415474397586, 0.22190176722515065, 0.2167130435805723, 0.2132435993975903, 0.21157579536897586, 0.21489228397966864, 0.22221650272966864, 0.2180558170180723, 0.21457607774849397, 0.2109963290662651, 0.2236710513930723, 0.22261359892695776, 0.21330536991716864, 0.20877700254141573, 0.2203957431287651, 0.20842108669051207, 0.2119728915662651, 0.2131421192582832, 0.2153908603162651, 0.22258271366716864, 0.2081563558923193, 0.21316270943147586, 0.22297980986445776, 0.21027273155120485, 0.2190323795180723, 0.22036485786897586, 0.22102521413780118, 0.21552175498870485, 0.21459666792168675, 0.21464814335466864, 0.21112722373870485, 0.2191750400037651, 0.21149343467620485], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-3, 3], "momentum": 0.8385113671055602, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.00021256896901125, "patience_threshold": 1, "do_flip": true, "batch_size": 32, "optimization": "adam", "nb_data_augmentation": 0, "learning_rate_decay_method": "none", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 1.3149747793999408e-06, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.0009101917622052814}, "accuracy_valid_max": 0.7924437005835843, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-3, 3)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.7885065653237951, "loss_train": [1.6230957508087158, 1.1585975885391235, 0.942870557308197, 0.796146810054779, 0.6778117418289185, 0.5805807113647461, 0.4969766139984131, 0.42916592955589294, 0.37043923139572144, 0.3222963213920593, 0.28854745626449585, 0.2632180154323578, 0.24346336722373962, 0.22534027695655823, 0.2077898532152176, 0.19306090474128723, 0.18893444538116455, 0.17745128273963928, 0.17602494359016418, 0.16909748315811157, 0.1623861938714981, 0.15422691404819489, 0.15410871803760529, 0.14685972034931183, 0.1455400288105011, 0.1420278251171112, 0.1431397795677185, 0.13550904393196106, 0.13492152094841003, 0.1344296783208847, 0.1310165971517563, 0.13538433611392975, 0.13719415664672852, 0.1250361204147339, 0.12358108162879944, 0.12401974201202393, 0.12372191250324249, 0.12263324856758118, 0.1232694536447525, 0.11877428740262985, 0.11877076327800751, 0.12318161129951477, 0.11360416561365128, 0.12079080194234848, 0.1171807274222374, 0.10973244160413742, 0.11103270947933197, 0.1204221323132515, 0.10929398983716965, 0.11008286476135254, 0.11503788083791733, 0.111086405813694, 0.10551951080560684, 0.11337395012378693, 0.11274398863315582, 0.10519459843635559, 0.11013762652873993, 0.10648714751005173, 0.10681719332933426, 0.10550627112388611, 0.11310005187988281, 0.10149301588535309, 0.10692451149225235, 0.10686156153678894, 0.10383809357881546, 0.10804973542690277, 0.10601199418306351, 0.10559073090553284, 0.1068376749753952, 0.1039218157529831, 0.10281894356012344, 0.10494869947433472, 0.09933634847402573, 0.10692056268453598, 0.09878893941640854, 0.10322583466768265, 0.10494781285524368, 0.10316390544176102, 0.0999518632888794, 0.10547832399606705, 0.10053149610757828, 0.1035093367099762, 0.10052074491977692, 0.10218405723571777, 0.10031463205814362, 0.10126127302646637, 0.10000289976596832, 0.10279431939125061, 0.10027655959129333, 0.10166341066360474, 0.0977063924074173, 0.10420728474855423, 0.10231022536754608, 0.10181309282779694, 0.09512779116630554, 0.10506945103406906, 0.09759521484375, 0.10514653474092484, 0.09495533257722855, 0.10150358080863953, 0.10105100274085999, 0.09783671796321869, 0.10426533967256546, 0.09404926002025604, 0.0982685312628746, 0.09965307265520096, 0.1030714213848114, 0.09944594651460648, 0.09725875407457352, 0.1013190969824791, 0.09915248304605484, 0.10323260724544525, 0.09716536104679108, 0.09857671707868576, 0.09634498506784439, 0.1037839949131012, 0.09729190170764923, 0.09882462024688721, 0.09961771219968796, 0.09968554228544235, 0.09852984547615051, 0.09796702861785889, 0.09878748655319214, 0.09951326251029968, 0.10264416038990021, 0.09292221069335938, 0.1027139276266098, 0.09620367735624313, 0.1020742729306221, 0.09399056434631348, 0.09648280590772629, 0.1005316749215126, 0.09573864191770554, 0.10118067264556885, 0.09647343307733536, 0.09784692525863647, 0.10048814117908478, 0.09992637485265732, 0.09698467701673508, 0.09954987466335297, 0.09554734826087952, 0.10016050189733505, 0.09378515928983688, 0.10310955345630646, 0.09631027281284332, 0.09940284490585327, 0.09518629312515259, 0.09778377413749695, 0.09981673210859299, 0.09744026511907578, 0.09428687393665314, 0.10050469636917114, 0.09731420874595642, 0.09681105613708496, 0.0985855683684349, 0.0983792096376419, 0.09776372462511063, 0.09509359300136566, 0.09847129136323929, 0.09934698045253754, 0.0928695872426033, 0.10115187615156174], "accuracy_train_first": 0.5807926125876707, "model": "residualv5", "loss_std": [0.28054529428482056, 0.19170217216014862, 0.18324466049671173, 0.1750006228685379, 0.16540612280368805, 0.15568256378173828, 0.14743049442768097, 0.1341114044189453, 0.12494157254695892, 0.114449642598629, 0.10876431316137314, 0.10168107599020004, 0.0937558114528656, 0.08978146314620972, 0.08640775084495544, 0.0771775096654892, 0.08008266240358353, 0.07543528825044632, 0.07721299678087234, 0.07357041537761688, 0.0711326003074646, 0.06530620157718658, 0.065874844789505, 0.06160520017147064, 0.06373974680900574, 0.06355560570955276, 0.06290382146835327, 0.060338977724313736, 0.05805673077702522, 0.060735464096069336, 0.058266863226890564, 0.083492711186409, 0.0705617293715477, 0.05620167404413223, 0.05514160543680191, 0.05558795481920242, 0.05595840513706207, 0.05495400354266167, 0.05262185260653496, 0.05375494807958603, 0.05172162130475044, 0.05486968532204628, 0.046457696706056595, 0.0549435093998909, 0.05189129710197449, 0.04516008868813515, 0.048696354031562805, 0.05697103962302208, 0.04588138312101364, 0.04608860984444618, 0.050797801464796066, 0.04753020778298378, 0.04387363791465759, 0.04847077280282974, 0.04856555908918381, 0.04362813010811806, 0.0462828166782856, 0.04348476603627205, 0.04285716637969017, 0.041120536625385284, 0.050455398857593536, 0.03975122421979904, 0.04271778091788292, 0.04279710352420807, 0.0395677387714386, 0.045025672763586044, 0.04271259531378746, 0.04279390349984169, 0.044349655508995056, 0.04311481863260269, 0.03914215415716171, 0.042032238095998764, 0.03671085089445114, 0.047070663422346115, 0.036313097923994064, 0.04058220237493515, 0.04174930974841118, 0.040892161428928375, 0.0381510891020298, 0.0430908203125, 0.03772323578596115, 0.03884047642350197, 0.0382094532251358, 0.03974812850356102, 0.03688249737024307, 0.03964356333017349, 0.03513653203845024, 0.04164138063788414, 0.03857753053307533, 0.0375363789498806, 0.03303966298699379, 0.039613332599401474, 0.03936953842639923, 0.03844674676656723, 0.03135063126683235, 0.03977243974804878, 0.034913569688797, 0.04554653540253639, 0.027762187644839287, 0.03760366886854172, 0.03779883682727814, 0.03365928307175636, 0.04234747588634491, 0.03142144903540611, 0.03239649906754494, 0.03211735934019089, 0.039876826107501984, 0.0368678979575634, 0.033870529383420944, 0.03639144077897072, 0.03497380390763283, 0.061539385467767715, 0.03245234116911888, 0.031728312373161316, 0.030341768637299538, 0.03694681450724602, 0.031116550788283348, 0.033308159559965134, 0.03571789339184761, 0.03440311923623085, 0.032883405685424805, 0.03346417099237442, 0.033097174018621445, 0.036496199667453766, 0.038931019604206085, 0.0248641949146986, 0.03715139627456665, 0.03006148338317871, 0.03535713627934456, 0.02684478461742401, 0.027771303430199623, 0.03560684621334076, 0.028218884021043777, 0.036914099007844925, 0.03004157356917858, 0.029918920248746872, 0.03418949991464615, 0.03460251912474632, 0.03078908659517765, 0.03361815959215164, 0.026547513902187347, 0.03418154641985893, 0.02385812997817993, 0.03886241838335991, 0.027571799233555794, 0.03167486563324928, 0.026890914887189865, 0.029678499326109886, 0.031441882252693176, 0.02895302325487137, 0.026044778525829315, 0.03356509655714035, 0.02887824736535549, 0.027361014857888222, 0.029941683635115623, 0.03027542121708393, 0.028638392686843872, 0.029501980170607567, 0.031874798238277435, 0.033724308013916016, 0.023454802110791206, 0.035022277384996414]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:14 2016", "state": "available"}], "summary": "689ce53f94a481d1bf1ec94b89ad0662"}