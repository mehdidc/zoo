{"content": {"hp_model": {"f0": 32, "f1": 16, "f2": 64, "f3": 64, "nonlin": "very_leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.06321247015531108, 0.06407751914591268, 0.06091266303852974, 0.06324800987869929, 0.06066269045546321, 0.060580314652037816, 0.06025382923944274, 0.06083678237905656, 0.06451937804892306, 0.06823376576712337, 0.05639448402347988, 0.06421209036992116, 0.058055635673874915, 0.061704209897284486, 0.05777880110230173, 0.05939765001040623, 0.06268577656656556, 0.06095802906531626, 0.06372333739402292, 0.06792253895185316, 0.0583244295199054, 0.05739880877727631, 0.05445804011756723, 0.06268150875376742, 0.058830451006509694, 0.057394147828376874, 0.05953800637262796, 0.059798348058369456, 0.061709123331699214, 0.05867107824101147, 0.06082124300823314, 0.057888892773247345, 0.0555115575076907, 0.059649635088842, 0.05766726219180364, 0.05754466060663384, 0.05851950066161709, 0.05562421279425834, 0.05992258886782267, 0.06049576304284113, 0.061712880399942854, 0.057309874728994056, 0.05882954153151617, 0.05672368903699335, 0.058599903873032946, 0.0564400057732118, 0.06114994880877857, 0.06268321591375578, 0.060173857647073975, 0.06442670894497338, 0.0590480178587785, 0.06363763688962266, 0.051217671817478695, 0.05718588320445418, 0.060404597818192754, 0.05928404267017281, 0.0612088351459734, 0.05183138160984201, 0.05850761354740627, 0.06505206248331664, 0.057695398975427345, 0.06502930305603533, 0.06086345393150313, 0.06493214315792643, 0.059213605086033065, 0.05988775594898352, 0.06002340088156937, 0.06184942466475292, 0.06018363765310955, 0.061745528363168226, 0.06124990542451189, 0.06246660432569475, 0.05935469714319196, 0.05667336023960575, 0.05959998163986896, 0.060386879876001165, 0.058864091702875446, 0.06187537148162813, 0.06336717602489597, 0.05643336947173652, 0.06143220456804751, 0.05792000118109359, 0.06093169161363738, 0.06491868301451634, 0.06712248548819055, 0.06309499050345337, 0.05966219145929293, 0.06099751392041108, 0.06041050264413706, 0.06446987889060883, 0.05858438003403751, 0.05320268783233877, 0.06125165248508446, 0.06164608607238918, 0.062165235942840756, 0.061354350321870435, 0.062274732835210435, 0.06038510779585222, 0.056954315544114356, 0.05648454360499768, 0.06042762337641905, 0.060068844697214636, 0.06136510473287894, 0.06241976321997366, 0.062308231347984286, 0.05717278298613912, 0.054977266368669266, 0.0557049530108484, 0.0568094596055276, 0.055343917025190204, 0.05865009990682893, 0.0611026821259499, 0.059331255200065616, 0.0518520230656827, 0.05565626658049662, 0.057895054181270125, 0.06150851050649132, 0.055026553568555764, 0.057290265806793195, 0.05751117845061458, 0.0650471273801359, 0.059481663701632874, 0.06373844905086411, 0.06498787691022953, 0.058830451006509694, 0.05922173677521828, 0.05991991013143504, 0.05848535681435847, 0.06336745747603524, 0.06522811043272571, 0.060269810824267946, 0.06146442128960159, 0.06390806943453002, 0.058557888692856316, 0.0617536154556343, 0.05763508904800893, 0.05485417975254745, 0.05751241887806815, 0.0626148932093191, 0.060652987694464686, 0.05558700724603567, 0.053880218755960455, 0.05999219403102617, 0.05551958894934511, 0.05986184140834149, 0.059505645826869624, 0.05725507742221523, 0.05667052791694527, 0.05712409872978916, 0.05559951878607574, 0.0560929231903011, 0.0586224213492201, 0.058102004648310016, 0.05478813837051861, 0.056986873034277394, 0.06196120672542318, 0.061573716196917073, 0.060290225573262884, 0.05662802607757463, 0.057610018672156656, 0.05827211662560575, 0.05967384858040993, 0.05746681558064415, 0.05772568478951242, 0.06218961711107953, 0.05651958061274053, 0.059605666962172364, 0.057735261688457266, 0.05926538787256757, 0.056924872560613404, 0.06194537360034664, 0.060823588828039565, 0.05378447226451014, 0.05555780269534879, 0.0566859466281324, 0.057381095157309964, 0.05730987472899405, 0.056761092142387296, 0.057202721933930695, 0.056775543882184275, 0.05745936670737118, 0.05689384698674092, 0.0560066921763858, 0.05645043267071248, 0.05742956155275539, 0.057578433041321125, 0.058655269178478214, 0.05779546708520023, 0.05782693417006302, 0.05755023907212187, 0.05371213533689954, 0.05532619021139116, 0.055690863947630995, 0.05492858432841735, 0.05542796152767921, 0.05500548221022332, 0.05442822980045415, 0.055439222174903546, 0.055893212364778545, 0.05499153825888939, 0.05558123173918618, 0.055576418358332165, 0.055348750625428704, 0.05614980758366945, 0.0566444009325337, 0.05627766646990205, 0.05788950894355985, 0.058074986170791924, 0.05426282997864427, 0.05354252499544376, 0.05422271687438064, 0.0553232889192624, 0.056472228144394294, 0.05586927570615584, 0.05716810360922779, 0.05647191232803734, 0.05693082502980936, 0.05796370951733136, 0.05731952110118756, 0.05894190683206086, 0.057997545446146055, 0.0576109473991462, 0.057839577880506136, 0.055471382856899176, 0.05786208301387832, 0.056109772065150616, 0.05730925232664464, 0.057578433041321125, 0.05866743037392286, 0.058173788188379365, 0.05801814493281982, 0.05888226786073676, 0.05964843910618346, 0.05866925433581875, 0.059080629064058435, 0.05935920414783875, 0.05888469092454695, 0.059747922699702714, 0.056922052752682986, 0.06135958243293798, 0.062420620385082115, 0.06147457621415588, 0.061946525236807735, 0.06167298599463058, 0.0626385299076701, 0.06270967095065967, 0.062342570099026044, 0.062095768897236475, 0.06118260565960242, 0.06126213380225152, 0.059796558536992714, 0.05953471119565682, 0.06018630465167005, 0.0610784510740509], "moving_avg_accuracy_train": [0.04520425451807228, 0.09372411521084335, 0.14511506965361443, 0.19580969973644574, 0.24229141801581322, 0.286160450160015, 0.3206868185476279, 0.35300301997599765, 0.3953453196350244, 0.4294718381233292, 0.46980778684111674, 0.5065290036389327, 0.5388344993593768, 0.5653845019836801, 0.5861827422973602, 0.614219682676058, 0.6398953235951991, 0.6610408627718237, 0.6867877742356052, 0.7097505630771049, 0.7338197462272258, 0.7497614726587201, 0.7631089323205589, 0.7795408854740451, 0.7966969249386888, 0.8128668408785549, 0.8280245280256392, 0.8410663899821114, 0.8531335085140207, 0.8646786855240645, 0.8751799434776821, 0.8840121938588296, 0.8884738321536695, 0.8968073600828809, 0.9051264358215807, 0.9116911642273744, 0.9175405907263238, 0.9235580866235709, 0.9286985129009728, 0.9343273438397912, 0.9386167480100289, 0.9433008186909538, 0.9475329544423403, 0.9514501221005158, 0.9551661791675727, 0.9579741094435865, 0.9605035998546495, 0.9630625207426785, 0.9656644011985311, 0.9677119482774732, 0.9700112542027379, 0.9720782763728256, 0.9733503056632539, 0.9753022668137958, 0.9768519535360307, 0.9776348492968854, 0.978934805632257, 0.9803659673883084, 0.9819622772759836, 0.9832977701809154, 0.983824356114631, 0.9840394355634089, 0.9842730108323692, 0.9850974040262407, 0.9862417487139781, 0.9870504616437852, 0.9870347038830212, 0.9880206160248396, 0.9889032306271749, 0.9895093307572285, 0.9901112967778912, 0.9905495270398611, 0.9909462874382846, 0.9913127844474682, 0.9915155609726009, 0.9915615764114853, 0.9919324330775657, 0.9914355376613754, 0.9918166450398161, 0.9920890468008948, 0.9924330412171909, 0.9924390782099296, 0.9929410288226717, 0.9933292489825731, 0.9933068474276893, 0.993439641600583, 0.9936273980730548, 0.9939634534464722, 0.9932446306921864, 0.9931836277133292, 0.9936064170504301, 0.9939586895020136, 0.994191020853017, 0.9944518886472333, 0.994569011529498, 0.9943332135391987, 0.9943516052876885, 0.9948270245781967, 0.9952290171504975, 0.9943859911884598, 0.994780317521421, 0.995177568148797, 0.9949491562134353, 0.9950377308029351, 0.9953221730840874, 0.9956770039684497, 0.9957257380595566, 0.9958990226873359, 0.9959726181595662, 0.9960035566448143, 0.9961961226670798, 0.9964494396172392, 0.9962467961073226, 0.9963797407435783, 0.9965558668198229, 0.9968532168848285, 0.9967560917324902, 0.9969463522881569, 0.997075229860546, 0.9964476202781059, 0.9964804749671627, 0.996662999759603, 0.9966766696631608, 0.9967030915522664, 0.9960891641741482, 0.9963484706482997, 0.9965536085232287, 0.9962676000805444, 0.9964408212471888, 0.996535538068253, 0.9967360881770904, 0.9965283114376946, 0.9966848741192263, 0.9965504605024844, 0.9966506855365733, 0.9968867841515907, 0.9965298075436606, 0.9966626889880897, 0.9968646429808471, 0.9971005243152925, 0.9971880998958115, 0.9973116280086399, 0.997272200900547, 0.9973920252382033, 0.997516339280648, 0.9976352814068001, 0.9977540951335899, 0.9976704213130021, 0.9977480704467622, 0.9977685382514835, 0.9979163832215159, 0.9979859083029787, 0.9980767188281026, 0.9980078458910754, 0.9978893843441364, 0.9980227915422529, 0.9981616833217626, 0.9982984517365743, 0.9981109258400253, 0.9982433573524082, 0.9980354561051191, 0.9976906830849687, 0.9978180756198453, 0.9980080301060534, 0.9978895501376167, 0.9979711711780719, 0.9980116858373731, 0.998161100837973, 0.9981920351818866, 0.998212816603457, 0.9981797503045571, 0.9983147120210893, 0.9984761813310287, 0.9986285631979258, 0.9987657068781333, 0.9988891361903199, 0.999000222571288, 0.9991002003141591, 0.9991901802827432, 0.9992711622544689, 0.9993440460290219, 0.9994096414261198, 0.9994686772835077, 0.999521809555157, 0.9995696285996413, 0.9996126657396771, 0.9996513991657094, 0.9996862592491385, 0.9997176333242246, 0.999409367732766, 0.9994684309594895, 0.9995215878635405, 0.9995694290771865, 0.9996124861694679, 0.999651237552521, 0.9996861137972689, 0.999717502417542, 0.9997457521757878, 0.9997711769582089, 0.9997940592623881, 0.9998146533361493, 0.9998331880025343, 0.9998498692022808, 0.9998648822820527, 0.9998783940538475, 0.9998905546484628, 0.9996120601775924, 0.9996437946718814, 0.9996794152046932, 0.9997114736842239, 0.9997403263158015, 0.9997662936842213, 0.9997896643157992, 0.9998106978842193, 0.9998296280957973, 0.9998466652862176, 0.9998619987575958, 0.9998757988818363, 0.9998882189936527, 0.9998993970942874, 0.9999094573848587, 0.9999185116463728, 0.9999266604817355, 0.9999057564817546, 0.9999151808335791, 0.9999236627502212, 0.9999312964751991, 0.9999381668276791, 0.9999443501449112, 0.9999499151304201, 0.999954923617378, 0.9999594312556402, 0.9999634881300762, 0.9999671393170686, 0.9999704253853617, 0.9999733828468256, 0.9999760445621431, 0.9999784401059287, 0.9995923242579865, 0.9996307386695372, 0.9996676648025835, 0.9997008983223251, 0.9997308084900925, 0.9997577276410833, 0.999781954876975, 0.9998037593892775, 0.9998233834503497, 0.9998410451053147, 0.9998569405947833, 0.9998712465353049, 0.9998841218817744, 0.999895709693597, 0.9999061387242373, 0.9999155248518136], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 1234, "moving_var_accuracy_train": [0.01839082163881193, 0.03773933140974394, 0.0577346700556203, 0.07509071272317423, 0.08702659265867917, 0.09564436122423237, 0.09680855612814357, 0.09652673238815991, 0.10300989221307726, 0.10319047636896302, 0.10751432756274261, 0.10889892467447822, 0.10740183769072308, 0.10300577767580536, 0.09659830110953516, 0.09401310123076949, 0.09054493793717064, 0.0855146485870851, 0.08292931477769082, 0.07938199034233548, 0.07665772150572848, 0.07127919712970507, 0.0657546695315543, 0.06160928433834418, 0.05809732311552147, 0.05464078643749033, 0.051244507110581254, 0.047650867869148225, 0.04419631922920175, 0.04097630731602076, 0.03787116435189445, 0.034786125737862515, 0.0314866691105421, 0.0289630313892104, 0.02668959144060538, 0.024408493227921362, 0.02227558601842872, 0.020373919728446325, 0.018574343596422335, 0.017002062876420292, 0.015467447481999139, 0.01411816739709432, 0.012867549414548365, 0.01171889229525384, 0.010671284786859065, 0.009675116560087755, 0.008765189799735918, 0.007947603504763048, 0.007213771191445675, 0.006530126113665466, 0.005924694771940537, 0.005370678520611188, 0.004848173195191437, 0.004397647246671324, 0.003979496282437832, 0.003587062986145328, 0.0032435656657956525, 0.002937643114963945, 0.0026668126507849565, 0.0024161832573985683, 0.0021770605663689966, 0.0019597708422556767, 0.0017642847746865379, 0.0015939729144607993, 0.0014463613458938942, 0.001307611360730038, 0.0011768524594202529, 0.0010679154182406917, 0.0009681349532429223, 0.000874627674227489, 0.0007904261746150324, 0.0007131119690160851, 0.0006432175414382914, 0.0005801046678141269, 0.0005224642659050182, 0.000470236896100058, 0.00042445101849103846, 0.00038422806213361306, 0.00034711244142536973, 0.00031306902175778136, 0.000282827109007989, 0.00025454472611472205, 0.0002313578432619382, 0.00020957849296873, 0.0001886251601388079, 0.00016992135275611787, 0.00015324648991710165, 0.00013893823985141593, 0.0001296947712349852, 0.00011675878638235176, 0.00010669166515621189, 9.713936156189255e-05, 8.791122611563512e-05, 7.973257155860545e-05, 7.188277432869466e-05, 6.51949031258874e-05, 5.867845712101126e-05, 5.484482292499657e-05, 5.081472288616215e-05, 5.21294855515723e-05, 4.8315976308215066e-05, 4.490465122594896e-05, 4.0883734213294845e-05, 3.686596991311096e-05, 3.390753962356384e-05, 3.1649930269683756e-05, 2.850631234743947e-05, 2.5925929172716945e-05, 2.3382082897240417e-05, 2.1052489316341454e-05, 1.9280975441087913e-05, 1.7930403192122087e-05, 1.6506942401912093e-05, 1.5015316648503314e-05, 1.3792968536252804e-05, 1.3209425233057381e-05, 1.1973382366702237e-05, 1.1101835841415198e-05, 1.014113711525792e-05, 1.2672067495468273e-05, 1.1414575621258666e-05, 1.0572955757831354e-05, 9.517341978417727e-06, 8.571890826591136e-06, 1.1106863174359837e-05, 1.0601335484755518e-05, 9.919935865853718e-06, 9.664149742848706e-06, 8.967784921726508e-06, 8.151747915286525e-06, 7.698556239149928e-06, 7.317241176140424e-06, 6.806123917762082e-06, 6.288114709276648e-06, 5.749708755472087e-06, 5.676420884042939e-06, 6.25566948312257e-06, 5.789019839272443e-06, 5.5771865920606766e-06, 5.5202279683123315e-06, 5.0372305122101586e-06, 4.670840212919851e-06, 4.217746663300992e-06, 3.925192844023472e-06, 3.6717593899616993e-06, 3.431908515327685e-06, 3.2157679788582026e-06, 2.9572029552383797e-06, 2.715747151477682e-06, 2.4479428156009236e-06, 2.3998717505156892e-06, 2.2033882080358942e-06, 2.0572683504916776e-06, 1.894232848535181e-06, 1.8311078066099902e-06, 1.8081743505325845e-06, 1.8009752532175203e-06, 1.7892281215067015e-06, 1.926798966244548e-06, 1.8919620188685704e-06, 2.0917721746006346e-06, 2.952410875953652e-06, 2.8032295098389195e-06, 2.8476509203307805e-06, 2.689223354584482e-06, 2.4802589673309304e-06, 2.2470060091625057e-06, 2.2232289898850564e-06, 2.0095184935967987e-06, 1.8124534515794928e-06, 1.6410485275279874e-06, 1.6408756591391556e-06, 1.711439135695562e-06, 1.749277322357505e-06, 1.7436250913092627e-06, 1.7063757381402144e-06, 1.646799820655313e-06, 1.5720797802163702e-06, 1.4877393549122737e-06, 1.3979881371222557e-06, 1.3059977247480138e-06, 1.214122757356981e-06, 1.1240775737391369e-06, 1.0370771609806847e-06, 9.539493940211404e-07, 8.75224213421234e-07, 8.012042967088981e-07, 7.320208957881364e-07, 6.676777994969285e-07, 1.4561590934431309e-06, 1.3419393668576322e-06, 1.2331763382065065e-06, 1.1304577398939106e-06, 1.0340971846660423e-06, 9.442024933962714e-07, 8.607294160860812e-07, 7.835236838213193e-07, 7.123537550077043e-07, 6.469361555574343e-07, 5.869549386025965e-07, 5.320764876090698e-07, 4.819606435702175e-07, 4.362689410380617e-07, 3.946705800123977e-07, 3.5684663380445264e-07, 3.2249289097657533e-07, 9.882761346272363e-07, 8.985122243144905e-07, 8.200804031032533e-07, 7.473220777813007e-07, 6.80082139143753e-07, 6.181426632332496e-07, 5.612440746930607e-07, 5.091013662280946e-07, 4.6141640579880056e-07, 4.178871579356687e-07, 3.7821448024266784e-07, 3.421070230798585e-07, 3.09284653369653e-07, 2.7948073743688947e-07, 2.524435487106035e-07, 2.2793701070363971e-07, 2.057409412931943e-07, 1.890996421006411e-07, 1.7098904355639158e-07, 1.5453762539006293e-07, 1.3960832666439822e-07, 1.2607230968676526e-07, 1.1380917942602238e-07, 1.0270698305684635e-07, 9.266204922563726e-08, 8.357871352739887e-08, 7.53689662463626e-08, 6.795205011980625e-08, 6.125402931127026e-08, 5.520734558493335e-08, 4.975037358231987e-08, 4.482698389435052e-08, 1.382113317794531e-06, 1.2571829891481903e-06, 1.1437365439491335e-06, 1.0393030910639887e-06, 9.434243451805052e-07, 8.556036768730178e-07, 7.753259398162721e-07, 7.020722766453953e-07, 6.353309829375639e-07, 5.746052911487421e-07, 5.19418761302865e-07, 4.6931882458046617e-07, 4.2387891304280834e-07, 3.8269951818404237e-07, 3.454084484865054e-07, 3.116604981557572e-07], "duration": 32287.403345, "accuracy_train": [0.4520425451807229, 0.5304028614457831, 0.6076336596385542, 0.6520613704819277, 0.6606268825301205, 0.6809817394578314, 0.6314241340361446, 0.6438488328313253, 0.7764260165662651, 0.7366105045180723, 0.8328313253012049, 0.8370199548192772, 0.8295839608433735, 0.8043345256024096, 0.7733669051204819, 0.8665521460843374, 0.8709760918674698, 0.8513507153614458, 0.9185099774096386, 0.9164156626506024, 0.9504423945783133, 0.8932370105421686, 0.8832360692771084, 0.9274284638554217, 0.9511012801204819, 0.9583960843373494, 0.9644437123493976, 0.9584431475903614, 0.9617375753012049, 0.9685852786144579, 0.969691265060241, 0.9635024472891566, 0.9286285768072289, 0.9718091114457831, 0.9799981174698795, 0.9707737198795181, 0.9701854292168675, 0.9777155496987951, 0.9749623493975904, 0.9849868222891566, 0.9772213855421686, 0.9854574548192772, 0.9856221762048193, 0.9867046310240963, 0.9886106927710844, 0.9832454819277109, 0.9832690135542169, 0.9860928087349398, 0.9890813253012049, 0.9861398719879518, 0.9907050075301205, 0.9906814759036144, 0.9847985692771084, 0.9928699171686747, 0.9907991340361446, 0.9846809111445783, 0.9906344126506024, 0.9932464231927711, 0.9963290662650602, 0.9953172063253012, 0.9885636295180723, 0.9859751506024096, 0.9863751882530121, 0.9925169427710844, 0.9965408509036144, 0.9943288780120482, 0.9868928840361446, 0.9968938253012049, 0.9968467620481928, 0.9949642319277109, 0.9955289909638554, 0.9944935993975904, 0.9945171310240963, 0.9946112575301205, 0.9933405496987951, 0.9919757153614458, 0.9952701430722891, 0.9869634789156626, 0.9952466114457831, 0.9945406626506024, 0.9955289909638554, 0.9924934111445783, 0.9974585843373494, 0.9968232304216867, 0.9931052334337349, 0.9946347891566265, 0.9953172063253012, 0.9969879518072289, 0.9867752259036144, 0.9926346009036144, 0.9974115210843374, 0.9971291415662651, 0.9962820030120482, 0.9967996987951807, 0.9956231174698795, 0.992211031626506, 0.9945171310240963, 0.9991057981927711, 0.9988469503012049, 0.9867987575301205, 0.9983292545180723, 0.9987528237951807, 0.9928934487951807, 0.9958349021084337, 0.9978821536144579, 0.9988704819277109, 0.9961643448795181, 0.9974585843373494, 0.9966349774096386, 0.9962820030120482, 0.9979292168674698, 0.9987292921686747, 0.9944230045180723, 0.9975762424698795, 0.9981410015060241, 0.9995293674698795, 0.9958819653614458, 0.9986586972891566, 0.9982351280120482, 0.9907991340361446, 0.9967761671686747, 0.9983057228915663, 0.9967996987951807, 0.9969408885542169, 0.9905638177710844, 0.9986822289156626, 0.9983998493975904, 0.9936935240963856, 0.9979998117469879, 0.9973879894578314, 0.9985410391566265, 0.9946583207831325, 0.9980939382530121, 0.9953407379518072, 0.9975527108433735, 0.999011671686747, 0.9933170180722891, 0.9978586219879518, 0.9986822289156626, 0.9992234563253012, 0.9979762801204819, 0.9984233810240963, 0.9969173569277109, 0.9984704442771084, 0.9986351656626506, 0.9987057605421686, 0.9988234186746988, 0.9969173569277109, 0.9984469126506024, 0.9979527484939759, 0.9992469879518072, 0.9986116340361446, 0.9988940135542169, 0.9973879894578314, 0.9968232304216867, 0.9992234563253012, 0.9994117093373494, 0.9995293674698795, 0.9964231927710844, 0.9994352409638554, 0.9961643448795181, 0.9945877259036144, 0.9989646084337349, 0.9997176204819277, 0.9968232304216867, 0.9987057605421686, 0.9983763177710844, 0.9995058358433735, 0.9984704442771084, 0.9983998493975904, 0.9978821536144579, 0.9995293674698795, 0.9999294051204819, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9966349774096386, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.997105609939759, 0.9999294051204819, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9997176204819277, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.996117281626506, 0.999976468373494, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "end": "2016-01-18 09:14:36.734000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0, 211.0, 212.0, 213.0, 214.0, 215.0, 216.0, 217.0, 218.0, 219.0, 220.0, 221.0, 222.0, 223.0, 224.0, 225.0, 226.0, 227.0, 228.0, 229.0, 230.0, 231.0, 232.0, 233.0, 234.0, 235.0, 236.0, 237.0, 238.0, 239.0, 240.0, 241.0, 242.0, 243.0, 244.0, 245.0, 246.0, 247.0, 248.0, 249.0, 250.0, 251.0, 252.0, 253.0], "accuracy_valid": [0.437099358974359, 0.49919871794871795, 0.5579594017094017, 0.5853365384615384, 0.5763888888888888, 0.5797275641025641, 0.530715811965812, 0.5356570512820513, 0.6097756410256411, 0.5757211538461539, 0.6343482905982906, 0.6235309829059829, 0.6121794871794872, 0.6004273504273504, 0.577590811965812, 0.6213942307692307, 0.6176549145299145, 0.6005608974358975, 0.6458333333333334, 0.6402243589743589, 0.6597222222222222, 0.6180555555555556, 0.6051014957264957, 0.6319444444444444, 0.6473023504273504, 0.6617254273504274, 0.6598557692307693, 0.6529113247863247, 0.6566506410256411, 0.6511752136752137, 0.6603899572649573, 0.656517094017094, 0.6370192307692307, 0.6549145299145299, 0.672676282051282, 0.6662660256410257, 0.6575854700854701, 0.6630608974358975, 0.6555822649572649, 0.672409188034188, 0.6635950854700855, 0.671875, 0.6728098290598291, 0.6704059829059829, 0.6668002136752137, 0.6692040598290598, 0.671875, 0.6717414529914529, 0.6761485042735043, 0.6793536324786325, 0.6714743589743589, 0.6838942307692307, 0.6732104700854701, 0.6793536324786325, 0.6728098290598291, 0.6706730769230769, 0.6777510683760684, 0.6816239316239316, 0.6844284188034188, 0.6927083333333334, 0.6797542735042735, 0.6692040598290598, 0.6676014957264957, 0.6828258547008547, 0.6862980769230769, 0.6881677350427351, 0.6745459401709402, 0.6899038461538461, 0.6853632478632479, 0.6907051282051282, 0.686965811965812, 0.6875, 0.6856303418803419, 0.6899038461538461, 0.6810897435897436, 0.6857638888888888, 0.6921741452991453, 0.6786858974358975, 0.6862980769230769, 0.6840277777777778, 0.6865651709401709, 0.6762820512820513, 0.6951121794871795, 0.688301282051282, 0.6792200854700855, 0.6853632478632479, 0.6845619658119658, 0.6959134615384616, 0.6774839743589743, 0.6845619658119658, 0.6916399572649573, 0.6905715811965812, 0.687767094017094, 0.6932425213675214, 0.6927083333333334, 0.6770833333333334, 0.6828258547008547, 0.6972489316239316, 0.6897702991452992, 0.6789529914529915, 0.6936431623931624, 0.6973824786324786, 0.6773504273504274, 0.6793536324786325, 0.6852297008547008, 0.6965811965811965, 0.6905715811965812, 0.6959134615384616, 0.6941773504273504, 0.6904380341880342, 0.6940438034188035, 0.6951121794871795, 0.6789529914529915, 0.6940438034188035, 0.6940438034188035, 0.6967147435897436, 0.6907051282051282, 0.6959134615384616, 0.6870993589743589, 0.6738782051282052, 0.6935096153846154, 0.6892361111111112, 0.6923076923076923, 0.6875, 0.6785523504273504, 0.6943108974358975, 0.6976495726495726, 0.6850961538461539, 0.6977831196581197, 0.6945779914529915, 0.6989850427350427, 0.6784188034188035, 0.6912393162393162, 0.6912393162393162, 0.6892361111111112, 0.7005876068376068, 0.6875, 0.6948450854700855, 0.6961805555555556, 0.7029914529914529, 0.6989850427350427, 0.6952457264957265, 0.6916399572649573, 0.6956463675213675, 0.6993856837606838, 0.6931089743589743, 0.6960470085470085, 0.6895032051282052, 0.6992521367521367, 0.6972489316239316, 0.7001869658119658, 0.6983173076923077, 0.6992521367521367, 0.6897702991452992, 0.6915064102564102, 0.6904380341880342, 0.6983173076923077, 0.7016559829059829, 0.6806891025641025, 0.6984508547008547, 0.6924412393162394, 0.6828258547008547, 0.6976495726495726, 0.7027243589743589, 0.6933760683760684, 0.6944444444444444, 0.6976495726495726, 0.7041933760683761, 0.6952457264957265, 0.6975160256410257, 0.6931089743589743, 0.7027243589743589, 0.7043269230769231, 0.7111378205128205, 0.7126068376068376, 0.7126068376068376, 0.7135416666666666, 0.7135416666666666, 0.7142094017094017, 0.7146100427350427, 0.7152777777777778, 0.7159455128205128, 0.7163461538461539, 0.7174145299145299, 0.7178151709401709, 0.7194177350427351, 0.7223557692307693, 0.7228899572649573, 0.7234241452991453, 0.6951121794871795, 0.7040598290598291, 0.7131410256410257, 0.7134081196581197, 0.7162126068376068, 0.7168803418803419, 0.7172809829059829, 0.717948717948718, 0.7191506410256411, 0.7200854700854701, 0.7211538461538461, 0.7218215811965812, 0.7222222222222222, 0.7232905982905983, 0.7243589743589743, 0.7238247863247863, 0.7208867521367521, 0.703125, 0.7095352564102564, 0.7155448717948718, 0.718215811965812, 0.7188835470085471, 0.7203525641025641, 0.7206196581196581, 0.7216880341880342, 0.7222222222222222, 0.7227564102564102, 0.7230235042735043, 0.7232905982905983, 0.7234241452991453, 0.7243589743589743, 0.7239583333333334, 0.7214209401709402, 0.7211538461538461, 0.7098023504273504, 0.7156784188034188, 0.7171474358974359, 0.719017094017094, 0.7194177350427351, 0.7198183760683761, 0.7208867521367521, 0.7200854700854701, 0.7204861111111112, 0.7207532051282052, 0.7204861111111112, 0.7203525641025641, 0.7196848290598291, 0.717948717948718, 0.7171474358974359, 0.6963141025641025, 0.7123397435897436, 0.7144764957264957, 0.7167467948717948, 0.7186164529914529, 0.7194177350427351, 0.7199519230769231, 0.7214209401709402, 0.7214209401709402, 0.7216880341880342, 0.7212873931623932, 0.7214209401709402, 0.7204861111111112, 0.7196848290598291, 0.718482905982906, 0.7166132478632479], "accuracy_test": 0.7210536858974359, "start": "2016-01-18 00:16:29.331000", "learning_rate_per_epoch": [0.0007310926448553801, 0.0005169605719856918, 0.00042209652019664645, 0.00036554632242769003, 0.00032695455593056977, 0.0002984673192258924, 0.00027632704586721957, 0.0002584802859928459, 0.00024369754828512669, 0.00023119179240893573, 0.0002204327320214361, 0.00021104826009832323, 0.00020276861323509365, 0.00019539272761903703, 0.00018876731337513775, 0.00018277316121384501, 0.00017731600382830948, 0.00017232018581125885, 0.00016772415256127715, 0.00016347727796528488, 0.00015953749243635684, 0.00015586947847623378, 0.00015244336100295186, 0.0001492336596129462, 0.00014621853188145906, 0.00014337906031869352, 0.00014069884491618723, 0.00013816352293360978, 0.00013576049241237342, 0.0001334786502411589, 0.00013130811566952616, 0.00012924014299642295, 0.00012726688873954117, 0.000125381353427656, 0.00012357720697764307, 0.00012184877414256334, 0.00012019088899251074, 0.00011859888763865456, 0.00011706851364579052, 0.00011559589620446786, 0.00011417749192332849, 0.00011281004117336124, 0.00011149058263981715, 0.00011021636601071805, 0.0001089848592528142, 0.00010779372678371146, 0.00010664082219591364, 0.00010552413004916161, 0.00010444180952617899, 0.00010339211439713836, 0.0001023734439513646, 0.00010138430661754683, 0.00010042329813586548, 9.948910883395001e-05, 9.858050907496363e-05, 9.769636380951852e-05, 9.683558891993016e-05, 9.599716577213258e-05, 9.518015576759353e-05, 9.438365668756887e-05, 9.360682452097535e-05, 9.284885891247541e-05, 9.21090177143924e-05, 9.138658060692251e-05, 9.068088547792286e-05, 8.999128476716578e-05, 8.931718184612691e-05, 8.865800191415474e-05, 8.801321382634342e-05, 8.738228643778712e-05, 8.676473225932568e-05, 8.616009290562943e-05, 8.556792454328388e-05, 8.498779061483219e-05, 8.441930549452081e-05, 8.386207628063858e-05, 8.331573917530477e-05, 8.27799376565963e-05, 8.225435158237815e-05, 8.173863898264244e-05, 8.123251609504223e-05, 8.073567732935771e-05, 8.024784619919956e-05, 7.976874621817842e-05, 7.929813000373542e-05, 7.883575017331168e-05, 7.838135934434831e-05, 7.793473923811689e-05, 7.749566429993138e-05, 7.706393080297858e-05, 7.663933502044529e-05, 7.622168050147593e-05, 7.581077807117254e-05, 7.540644583059475e-05, 7.500852370867506e-05, 7.46168298064731e-05, 7.4231211328879e-05, 7.385150820482522e-05, 7.34775749151595e-05, 7.310926594072953e-05, 7.274643576238304e-05, 7.23889606888406e-05, 7.203669520094991e-05, 7.168953015934676e-05, 7.13473345967941e-05, 7.10099920979701e-05, 7.067739352351055e-05, 7.034942245809361e-05, 7.00259770383127e-05, 6.97069481248036e-05, 6.939224113011733e-05, 6.908176146680489e-05, 6.87754072714597e-05, 6.847309850854799e-05, 6.817474059062079e-05, 6.788024620618671e-05, 6.758953531971201e-05, 6.730253517162055e-05, 6.701915117446333e-05, 6.673932512057945e-05, 6.646296969847754e-05, 6.619001942453906e-05, 6.592040153918788e-05, 6.565405783476308e-05, 6.539091555168852e-05, 6.513090920634568e-05, 6.487398059107363e-05, 6.462007149821147e-05, 6.436911644414067e-05, 6.412106449715793e-05, 6.387585744960234e-05, 6.363344436977059e-05, 6.339377432595938e-05, 6.31567818345502e-05, 6.292243779171258e-05, 6.2690676713828e-05, 6.246146222110838e-05, 6.223473610589281e-05, 6.201046926435083e-05, 6.178860348882154e-05, 6.156910967547446e-05, 6.13519296166487e-05, 6.113704148447141e-05, 6.092438707128167e-05, 6.071394091122784e-05, 6.050565934856422e-05, 6.0299506003502756e-05, 6.009544449625537e-05, 5.9893442085012794e-05, 5.969346602796577e-05, 5.949547630734742e-05, 5.929944381932728e-05, 5.9105335822096094e-05, 5.8913123211823404e-05, 5.8722773246699944e-05, 5.853425682289526e-05, 5.834754483657889e-05, 5.8162608183920383e-05, 5.797941776108928e-05, 5.779794810223393e-05, 5.761817374150269e-05, 5.7440061937086284e-05, 5.7263594499090686e-05, 5.7088745961664245e-05, 5.6915483582997695e-05, 5.674379281117581e-05, 5.6573648180346936e-05, 5.640502058668062e-05, 5.6237895478261635e-05, 5.607224738923833e-05, 5.590805085375905e-05, 5.5745291319908574e-05, 5.558394332183525e-05, 5.542398866964504e-05, 5.5265409173443913e-05, 5.5108183005359024e-05, 5.495228833751753e-05, 5.47977106180042e-05, 5.46444280189462e-05, 5.44924296264071e-05, 5.434168997453526e-05, 5.419219087343663e-05, 5.404392504715361e-05, 5.389686339185573e-05, 5.375099863158539e-05, 5.3606312576448545e-05, 5.346279067452997e-05, 5.332041109795682e-05, 5.317916293279268e-05, 5.303903526510112e-05, 5.29000062670093e-05, 5.2762065024580806e-05, 5.262520062387921e-05, 5.248939123703167e-05, 5.235463322605938e-05, 5.222090476308949e-05, 5.2088194934185594e-05, 5.195649282541126e-05, 5.182578388485126e-05, 5.169605719856918e-05, 5.156729821464978e-05, 5.143949965713546e-05, 5.131264333613217e-05, 5.11867219756823e-05, 5.106172466184944e-05, 5.093763684271835e-05, 5.0814451242331415e-05, 5.069215330877341e-05, 5.057073576608673e-05, 5.045018406235613e-05, 5.033049455960281e-05, 5.021164906793274e-05, 5.0093643949367106e-05, 4.997646465199068e-05, 4.9860103899845853e-05, 4.9744554416975006e-05, 4.962980165146291e-05, 4.9515841965330765e-05, 4.9402660806663334e-05, 4.9290254537481815e-05, 4.9178612243849784e-05, 4.906772664980963e-05, 4.895758320344612e-05, 4.884818190475926e-05, 4.8739508201833814e-05, 4.863155845669098e-05, 4.852432175539434e-05, 4.841779445996508e-05, 4.8311962018487975e-05, 4.820682079298422e-05, 4.810236350749619e-05, 4.799858288606629e-05, 4.789547165273689e-05, 4.779302253155038e-05, 4.769122460857034e-05, 4.7590077883796766e-05, 4.7489571443293244e-05, 4.7389698011102155e-05, 4.7290450311265886e-05, 4.7191828343784437e-05, 4.709381755674258e-05, 4.6996414312161505e-05, 4.689961497206241e-05, 4.6803412260487676e-05, 4.6707795263500884e-05, 4.6612763981102034e-05, 4.651831113733351e-05, 4.6424429456237704e-05, 4.63311152998358e-05, 4.6238357754191384e-05, 4.614615681930445e-05, 4.60545088571962e-05, 4.596339931595139e-05, 4.5872831833548844e-05], "accuracy_train_last": 1.0, "error_valid": [0.562900641025641, 0.500801282051282, 0.4420405982905983, 0.41466346153846156, 0.42361111111111116, 0.4202724358974359, 0.46928418803418803, 0.4643429487179487, 0.3902243589743589, 0.42427884615384615, 0.36565170940170943, 0.37646901709401714, 0.3878205128205128, 0.3995726495726496, 0.42240918803418803, 0.3786057692307693, 0.3823450854700855, 0.39943910256410253, 0.35416666666666663, 0.3597756410256411, 0.3402777777777778, 0.3819444444444444, 0.39489850427350426, 0.3680555555555556, 0.3526976495726496, 0.3382745726495726, 0.3401442307692307, 0.34708867521367526, 0.3433493589743589, 0.3488247863247863, 0.3396100427350427, 0.343482905982906, 0.3629807692307693, 0.3450854700854701, 0.32732371794871795, 0.33373397435897434, 0.3424145299145299, 0.33693910256410253, 0.3444177350427351, 0.32759081196581197, 0.3364049145299145, 0.328125, 0.3271901709401709, 0.32959401709401714, 0.3331997863247863, 0.33079594017094016, 0.328125, 0.32825854700854706, 0.32385149572649574, 0.32064636752136755, 0.3285256410256411, 0.3161057692307693, 0.3267895299145299, 0.32064636752136755, 0.3271901709401709, 0.32932692307692313, 0.32224893162393164, 0.31837606837606836, 0.31557158119658124, 0.30729166666666663, 0.32024572649572647, 0.33079594017094016, 0.33239850427350426, 0.31717414529914534, 0.31370192307692313, 0.3118322649572649, 0.32545405982905984, 0.31009615384615385, 0.31463675213675213, 0.3092948717948718, 0.31303418803418803, 0.3125, 0.3143696581196581, 0.31009615384615385, 0.3189102564102564, 0.31423611111111116, 0.30782585470085466, 0.32131410256410253, 0.31370192307692313, 0.3159722222222222, 0.3134348290598291, 0.3237179487179487, 0.3048878205128205, 0.31169871794871795, 0.3207799145299145, 0.31463675213675213, 0.3154380341880342, 0.30408653846153844, 0.32251602564102566, 0.3154380341880342, 0.3083600427350427, 0.30942841880341876, 0.312232905982906, 0.3067574786324786, 0.30729166666666663, 0.32291666666666663, 0.31717414529914534, 0.30275106837606836, 0.3102297008547008, 0.3210470085470085, 0.30635683760683763, 0.3026175213675214, 0.3226495726495726, 0.32064636752136755, 0.3147702991452992, 0.30341880341880345, 0.30942841880341876, 0.30408653846153844, 0.3058226495726496, 0.3095619658119658, 0.30595619658119655, 0.3048878205128205, 0.3210470085470085, 0.30595619658119655, 0.30595619658119655, 0.3032852564102564, 0.3092948717948718, 0.30408653846153844, 0.3129006410256411, 0.3261217948717948, 0.3064903846153846, 0.31076388888888884, 0.3076923076923077, 0.3125, 0.3214476495726496, 0.30568910256410253, 0.3023504273504274, 0.31490384615384615, 0.3022168803418803, 0.3054220085470085, 0.3010149572649573, 0.32158119658119655, 0.3087606837606838, 0.3087606837606838, 0.31076388888888884, 0.2994123931623932, 0.3125, 0.3051549145299145, 0.3038194444444444, 0.29700854700854706, 0.3010149572649573, 0.30475427350427353, 0.3083600427350427, 0.30435363247863245, 0.3006143162393162, 0.30689102564102566, 0.3039529914529915, 0.3104967948717948, 0.3007478632478633, 0.30275106837606836, 0.2998130341880342, 0.3016826923076923, 0.3007478632478633, 0.3102297008547008, 0.30849358974358976, 0.3095619658119658, 0.3016826923076923, 0.29834401709401714, 0.31931089743589747, 0.30154914529914534, 0.30755876068376065, 0.31717414529914534, 0.3023504273504274, 0.2972756410256411, 0.30662393162393164, 0.3055555555555556, 0.3023504273504274, 0.29580662393162394, 0.30475427350427353, 0.30248397435897434, 0.30689102564102566, 0.2972756410256411, 0.29567307692307687, 0.2888621794871795, 0.28739316239316237, 0.28739316239316237, 0.28645833333333337, 0.28645833333333337, 0.2857905982905983, 0.2853899572649573, 0.2847222222222222, 0.2840544871794872, 0.28365384615384615, 0.2825854700854701, 0.2821848290598291, 0.2805822649572649, 0.2776442307692307, 0.2771100427350427, 0.27657585470085466, 0.3048878205128205, 0.2959401709401709, 0.28685897435897434, 0.2865918803418803, 0.2837873931623932, 0.2831196581196581, 0.28271901709401714, 0.28205128205128205, 0.2808493589743589, 0.2799145299145299, 0.27884615384615385, 0.27817841880341876, 0.2777777777777778, 0.2767094017094017, 0.27564102564102566, 0.2761752136752137, 0.27911324786324787, 0.296875, 0.2904647435897436, 0.2844551282051282, 0.28178418803418803, 0.28111645299145294, 0.2796474358974359, 0.2793803418803419, 0.2783119658119658, 0.2777777777777778, 0.27724358974358976, 0.27697649572649574, 0.2767094017094017, 0.27657585470085466, 0.27564102564102566, 0.27604166666666663, 0.27857905982905984, 0.27884615384615385, 0.2901976495726496, 0.28432158119658124, 0.2828525641025641, 0.280982905982906, 0.2805822649572649, 0.28018162393162394, 0.27911324786324787, 0.2799145299145299, 0.27951388888888884, 0.2792467948717948, 0.27951388888888884, 0.2796474358974359, 0.2803151709401709, 0.28205128205128205, 0.2828525641025641, 0.30368589743589747, 0.2876602564102564, 0.28552350427350426, 0.2832532051282052, 0.28138354700854706, 0.2805822649572649, 0.28004807692307687, 0.27857905982905984, 0.27857905982905984, 0.2783119658119658, 0.2787126068376068, 0.27857905982905984, 0.27951388888888884, 0.2803151709401709, 0.281517094017094, 0.28338675213675213], "accuracy_train_std": [0.06428873092028907, 0.06337063085938902, 0.06346833462096553, 0.061000348719535295, 0.06291600068475357, 0.06506933006655385, 0.06725149068282318, 0.06885072028993916, 0.06391847761626798, 0.06417594863394109, 0.06089017411178965, 0.058760784922017285, 0.05894008725981197, 0.06023419353633147, 0.061801878156104786, 0.049651480997410236, 0.05116162549895061, 0.05300040623828745, 0.040650167936616105, 0.04117472530483189, 0.03156856935781422, 0.04206480144334765, 0.04304574516043237, 0.0377028064290339, 0.028984507219424274, 0.02779012362537038, 0.024857852830149865, 0.02656331884128486, 0.025382543577094966, 0.023818179499256863, 0.02279805378825466, 0.02629222021764471, 0.03406982154938225, 0.02186750383180406, 0.017925292754277747, 0.02272755583636363, 0.022025388713275716, 0.020404824107892627, 0.020229754865173966, 0.014815509058150982, 0.019543663949352297, 0.015611030712048998, 0.015610516375399069, 0.015101346828380092, 0.01394915440560875, 0.016478591679163275, 0.01680057609269703, 0.015776915829700488, 0.01344658993104561, 0.014958101940111972, 0.012188178632632825, 0.012424342250269388, 0.015876089275351647, 0.010757402692164684, 0.012199259086298014, 0.016074409111184497, 0.0126240980633888, 0.010554111563923286, 0.007749876021266826, 0.00851854630136234, 0.013535493023523434, 0.015613655338662623, 0.014604763280145244, 0.011010286855130842, 0.007680940613701163, 0.00946261096281639, 0.014973198168952855, 0.006907243903974899, 0.0070962584043261345, 0.008974038065398518, 0.008567451248445868, 0.009266442590297354, 0.009418031923691065, 0.009236365743409452, 0.010509315025049102, 0.011244449285614301, 0.00883207494070022, 0.014217698904679821, 0.009086377342012342, 0.009094296285991578, 0.008695246453059075, 0.011010965786129277, 0.006429605179946561, 0.006954819886036378, 0.0105324731781391, 0.009427228921388722, 0.008475273904726662, 0.007157329013403757, 0.014632585239023239, 0.01103949334353652, 0.006353190530372759, 0.006686129784262879, 0.007631651497040473, 0.007022998062921974, 0.008399834841932489, 0.011440896761282002, 0.009339625188359407, 0.0037292988207186612, 0.004346604182660621, 0.014515161026438433, 0.005403416911200231, 0.004487512640580921, 0.010337537532329343, 0.007948222694663975, 0.005681803683946553, 0.004224172754577725, 0.007645678580995244, 0.006314197978632689, 0.007381845355356158, 0.007679678893065344, 0.005568599193429609, 0.004439688967539605, 0.00940179037446919, 0.006670666190467193, 0.005202086381865964, 0.00267060635525435, 0.008018693066471749, 0.004622212876940188, 0.005092354890864985, 0.012078098730403376, 0.007244340304434368, 0.005292890029935824, 0.006756159053059625, 0.007085638062704877, 0.012139924834833496, 0.004342079290162197, 0.004964671308072512, 0.009736521491706137, 0.005756309150170088, 0.006486837616314365, 0.004858924752295179, 0.009143815981955085, 0.005594344171710278, 0.008723221872217463, 0.006115326005565055, 0.003898871678846846, 0.010685327859712724, 0.005769377056290312, 0.004508256782691283, 0.0035022618643909564, 0.005716005971384741, 0.005009031584704536, 0.0069970491130026576, 0.005096267990464189, 0.004886199312610557, 0.004555603773304554, 0.00429772803343933, 0.007255033627167936, 0.0047529225585729645, 0.005341575552308684, 0.0037603509705267653, 0.004608295215399647, 0.00409795016141225, 0.006486837616314365, 0.007059762482111815, 0.003605717889003586, 0.003095370580151435, 0.0028049067066549965, 0.007698834731430169, 0.0029164031221441396, 0.008066887312880274, 0.009632113039494192, 0.0038866383324665994, 0.002081451843702558, 0.0073656246356784355, 0.004391218405262961, 0.005067064649238621, 0.002734431702126563, 0.004875023840525309, 0.005038186630674938, 0.005996641538767753, 0.00267060635525435, 0.0010478842280785168, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.007480799924274128, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.007024850706724569, 0.0010478842280785168, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.002081451843702558, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.007906031189489482, 0.0006059108240579695, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "accuracy_test_std": 0.055941005140466495, "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-3, 3], "momentum": 0.6697432051757939, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.0007310926428577258, "patience_threshold": 1, "do_flip": true, "batch_size": 64, "optimization": "adam", "nb_data_augmentation": 0, "learning_rate_decay_method": "sqrt", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 6.52459477208362e-07, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.04496055458238987}, "accuracy_valid_max": 0.7243589743589743, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = 1234\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-10, -6], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128, 256],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-3, 3)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_optimizer.learning_rate = learning_rate\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.7166132478632479, "loss_train": [3.113542318344116, 1.434911847114563, 1.2273277044296265, 1.0849289894104004, 0.968772828578949, 0.8690020442008972, 0.7706125378608704, 0.6787658333778381, 0.624999463558197, 0.559730589389801, 0.49746882915496826, 0.4560582637786865, 0.4029501676559448, 0.3459371328353882, 0.3146733343601227, 0.2692560851573944, 0.2253795713186264, 0.1999732404947281, 0.18215377628803253, 0.15870769321918488, 0.14851737022399902, 0.13758563995361328, 0.12534508109092712, 0.12160284072160721, 0.11141311377286911, 0.10441461205482483, 0.10427644103765488, 0.09700403362512589, 0.0876331552863121, 0.0861063301563263, 0.08267929404973984, 0.0784849151968956, 0.0860673114657402, 0.07473525404930115, 0.07735521346330643, 0.06592955440282822, 0.06885435432195663, 0.07402218878269196, 0.0675472542643547, 0.0648861974477768, 0.06703708320856094, 0.05983149632811546, 0.0617026649415493, 0.05620463192462921, 0.06324482709169388, 0.05412495136260986, 0.0558488667011261, 0.0564563162624836, 0.06262725591659546, 0.0560242161154747, 0.05147618055343628, 0.049198705703020096, 0.05373121052980423, 0.06740491092205048, 0.04676608741283417, 0.04553411900997162, 0.04631248489022255, 0.05307906121015549, 0.05331278592348099, 0.04643680900335312, 0.04912358149886131, 0.04726240038871765, 0.04522073641419411, 0.04720441624522209, 0.044931989163160324, 0.04714794456958771, 0.04956265166401863, 0.04919213429093361, 0.04386905953288078, 0.042134884744882584, 0.04455810785293579, 0.04370823875069618, 0.0454796627163887, 0.041860245168209076, 0.044855207204818726, 0.04870213568210602, 0.03864629939198494, 0.04094533249735832, 0.04661979898810387, 0.042094506323337555, 0.04008306935429573, 0.045899055898189545, 0.03841366991400719, 0.039162423461675644, 0.03846577927470207, 0.04954390972852707, 0.03633316606283188, 0.032222822308540344, 0.046085622161626816, 0.045927438884973526, 0.038906753063201904, 0.032809287309646606, 0.04557515308260918, 0.03912141174077988, 0.03245783597230911, 0.049412570893764496, 0.03658324480056763, 0.03259018063545227, 0.029431764036417007, 0.054229047149419785, 0.037756290286779404, 0.034060411155223846, 0.040774885565042496, 0.04077805578708649, 0.03745928779244423, 0.03230411931872368, 0.03931162506341934, 0.03692292049527168, 0.038074322044849396, 0.03953193873167038, 0.035475242882966995, 0.03164632245898247, 0.040062855929136276, 0.03692849725484848, 0.03883609175682068, 0.030330313369631767, 0.0394371822476387, 0.03650348633527756, 0.03719070926308632, 0.03334211930632591, 0.03952675685286522, 0.03403761610388756, 0.033934373408555984, 0.04085046425461769, 0.033927883952856064, 0.03776638209819794, 0.03337743505835533, 0.032643258571624756, 0.039592187851667404, 0.036247219890356064, 0.03354853764176369, 0.036072321236133575, 0.03339499235153198, 0.03499393165111542, 0.039891332387924194, 0.030842486768960953, 0.03203308954834938, 0.039680302143096924, 0.03254975751042366, 0.03313091769814491, 0.036190200597047806, 0.0373816192150116, 0.031985361129045486, 0.03311172127723694, 0.03495324030518532, 0.03187021613121033, 0.030662158504128456, 0.03967500105500221, 0.03371083363890648, 0.03634236380457878, 0.03156261146068573, 0.0320623405277729, 0.033706262707710266, 0.03365127742290497, 0.035970691591501236, 0.035657960921525955, 0.02980036847293377, 0.029643088579177856, 0.03793077915906906, 0.03319206088781357, 0.030782364308834076, 0.036653175950050354, 0.037309080362319946, 0.0305783674120903, 0.03143015876412392, 0.035471249371767044, 0.03281209617853165, 0.03203528746962547, 0.030741795897483826, 0.031618740409612656, 0.043049730360507965, 0.030882788822054863, 0.028200440108776093, 0.027036789804697037, 0.026883967220783234, 0.026866408064961433, 0.0268520787358284, 0.02683551236987114, 0.026814717799425125, 0.026787448674440384, 0.02675079181790352, 0.02670082077383995, 0.026632169261574745, 0.02653750590980053, 0.02640688605606556, 0.026226818561553955, 0.02597922459244728, 0.025640476495027542, 0.025181107223033905, 0.0439077727496624, 0.029465094208717346, 0.02524305135011673, 0.024805745109915733, 0.024740448221564293, 0.024707932025194168, 0.0246796403080225, 0.024650748819112778, 0.02461763471364975, 0.02457660809159279, 0.024523207917809486, 0.024451788514852524, 0.024355005472898483, 0.02422325313091278, 0.02404400333762169, 0.02380119264125824, 0.02347470633685589, 0.04446328431367874, 0.028923535719513893, 0.023712310940027237, 0.023303592577576637, 0.023230070248246193, 0.02318495325744152, 0.023148810490965843, 0.023115402087569237, 0.02308049239218235, 0.02304004319012165, 0.022989556193351746, 0.022923672571778297, 0.022835658863186836, 0.022717086598277092, 0.022557290270924568, 0.022343076765537262, 0.022059526294469833, 0.04397693648934364, 0.023244909942150116, 0.022221142426133156, 0.02211037278175354, 0.02205551043152809, 0.022015146911144257, 0.02198033407330513, 0.021945999935269356, 0.021907880902290344, 0.021861586719751358, 0.021802185103297234, 0.021723762154579163, 0.02161894179880619, 0.02147873118519783, 0.021292172372341156, 0.04217950627207756, 0.025872908532619476, 0.021788258105516434, 0.02138945274055004, 0.021302230656147003, 0.021250614896416664, 0.021211184561252594, 0.02117646299302578, 0.021141575649380684, 0.021102193742990494, 0.021053900942206383, 0.020991651341319084, 0.02090943045914173, 0.020800024271011353, 0.020654747262597084, 0.02046404592692852], "accuracy_train_first": 0.4520425451807229, "model": "residualv2", "loss_std": [15.635823249816895, 0.155987948179245, 0.1507628709077835, 0.14848975837230682, 0.1453314572572708, 0.14407846331596375, 0.13872873783111572, 0.13181568682193756, 0.1323094218969345, 0.1278441846370697, 0.12699361145496368, 0.13428346812725067, 0.12417047470808029, 0.11127282679080963, 0.10949501395225525, 0.09668091684579849, 0.086318239569664, 0.08505772799253464, 0.0825076475739479, 0.07502353191375732, 0.06713977456092834, 0.06834796816110611, 0.06126725301146507, 0.06166834756731987, 0.0574708916246891, 0.05530797317624092, 0.055110398679971695, 0.05839136242866516, 0.04548858106136322, 0.04571019113063812, 0.044015515595674515, 0.04547951743006706, 0.04907327517867088, 0.04114677384495735, 0.04754956439137459, 0.035464879125356674, 0.04156963899731636, 0.04346821829676628, 0.04170874506235123, 0.0382566824555397, 0.03940930217504501, 0.036027923226356506, 0.03502609580755234, 0.031845949590206146, 0.03978867456316948, 0.03152712434530258, 0.03777092695236206, 0.033266518265008926, 0.039146363735198975, 0.034752458333969116, 0.030634528025984764, 0.02759975753724575, 0.03395417332649231, 0.04859219491481781, 0.02872462011873722, 0.02565133385360241, 0.02702992595732212, 0.03343910723924637, 0.036866143345832825, 0.030721619725227356, 0.03157493844628334, 0.030176324769854546, 0.027583451941609383, 0.027300449088215828, 0.024575164541602135, 0.02767302840948105, 0.029426218941807747, 0.033355847001075745, 0.025666404515504837, 0.02409999817609787, 0.025470275431871414, 0.023874733597040176, 0.030782446265220642, 0.025164110586047173, 0.028278809040784836, 0.031261175870895386, 0.019918600097298622, 0.02476133406162262, 0.02896067127585411, 0.02589036524295807, 0.0238062534481287, 0.030313987284898758, 0.01973649114370346, 0.02192668430507183, 0.021181827411055565, 0.0339859277009964, 0.018214017152786255, 0.01132921501994133, 0.03433719277381897, 0.03094504214823246, 0.02237151935696602, 0.012326606549322605, 0.029919521883130074, 0.021082432940602303, 0.011626548133790493, 0.03249262645840645, 0.019993716850876808, 0.013895576819777489, 0.006913179997354746, 0.039359867572784424, 0.021132435649633408, 0.015439635142683983, 0.02380700409412384, 0.02607201598584652, 0.021342875435948372, 0.014968364499509335, 0.02373802289366722, 0.01911497861146927, 0.023968258872628212, 0.023886241018772125, 0.01741134747862816, 0.013081502169370651, 0.021758176386356354, 0.018732275813817978, 0.022689277306199074, 0.010608804412186146, 0.025629738345742226, 0.018724383786320686, 0.02111331932246685, 0.014146458357572556, 0.02503061853349209, 0.01685723103582859, 0.017102962359786034, 0.024861086159944534, 0.016625594347715378, 0.02434028685092926, 0.0149830998852849, 0.01596442610025406, 0.022899411618709564, 0.01983650214970112, 0.016257116571068764, 0.019330700859427452, 0.015146424062550068, 0.017378302291035652, 0.024388978257775307, 0.01297903060913086, 0.014846671372652054, 0.025513194501399994, 0.013868995010852814, 0.01566096767783165, 0.020828720182180405, 0.02183367684483528, 0.014142358675599098, 0.015302743762731552, 0.018857458606362343, 0.012280579656362534, 0.011293935589492321, 0.025080624967813492, 0.014978718012571335, 0.023021770641207695, 0.012686646543443203, 0.014750877395272255, 0.016171082854270935, 0.01689918525516987, 0.021221064031124115, 0.02039354294538498, 0.010990099050104618, 0.009664622135460377, 0.022642754018306732, 0.01931247115135193, 0.011362582445144653, 0.02520083449780941, 0.02176428772509098, 0.013296054676175117, 0.01444688905030489, 0.020081639289855957, 0.01445197407156229, 0.012919079512357712, 0.013202211819589138, 0.012828960083425045, 0.034365154802799225, 0.010827435180544853, 0.004082478582859039, 0.0006919195875525475, 4.861481284024194e-05, 2.189565566368401e-05, 1.6221027181018144e-05, 1.300035546591971e-05, 1.1484595233923756e-05, 1.1666559657896869e-05, 1.366582091577584e-05, 1.7562035282026045e-05, 2.3581498680869117e-05, 3.221787483198568e-05, 4.427148451213725e-05, 6.0863727412652224e-05, 8.346445247298107e-05, 0.00011375521717127413, 0.00015336366777773947, 0.04524892568588257, 0.009572499431669712, 0.0015341093530878425, 0.00015754785272292793, 4.969541623722762e-05, 3.479292354313657e-05, 2.592673263279721e-05, 2.0458926883293316e-05, 1.76396788447164e-05, 1.745803456287831e-05, 1.9976552721345797e-05, 2.519467489037197e-05, 3.331168773001991e-05, 4.4841752242064103e-05, 6.0621303418884054e-05, 8.169493230525404e-05, 0.000109237247670535, 0.06090356782078743, 0.01012092363089323, 0.0008326933602802455, 0.00012427096953615546, 6.96609349688515e-05, 4.8161204176722094e-05, 3.498678415780887e-05, 2.651243266882375e-05, 2.1466155885718763e-05, 1.9455523215583526e-05, 2.0375693566165864e-05, 2.419560041744262e-05, 3.091575490543619e-05, 4.077929406776093e-05, 5.431938188849017e-05, 7.229919719975442e-05, 9.446019248571247e-05, 0.04886064678430557, 0.002300125313922763, 0.000183902244316414, 8.035838254727423e-05, 5.2803810831392184e-05, 3.701586683746427e-05, 2.706752457015682e-05, 2.100249366776552e-05, 1.8172975615016185e-05, 1.8341537725063972e-05, 2.1359968741307966e-05, 2.709307591430843e-05, 3.559072138159536e-05, 4.7187688323901966e-05, 6.237589695956558e-05, 0.05626523494720459, 0.007240333594381809, 0.0008641354506835341, 0.00013655611837748438, 7.323845784412697e-05, 4.946005356032401e-05, 3.4938617318402976e-05, 2.581832086434588e-05, 2.051187948381994e-05, 1.8352122424403206e-05, 1.9105205865344033e-05, 2.2581663870369084e-05, 2.8637503419304267e-05, 3.7337347748689353e-05, 4.901303691440262e-05, 6.397142715286463e-05]}, "state": "available", "life": [{"dt": "Sun May 15 22:04:59 2016", "state": "available"}], "summary": "4bd27cc57deab34030c7d50330558d48"}