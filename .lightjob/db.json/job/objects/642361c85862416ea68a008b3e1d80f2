{"content": {"hp_model": {"f0": 16, "f1": 32, "f2": 16, "f3": 32, "nonlin": "rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.019979239068952208, 0.014484260906287799, 0.011031166831622554, 0.013796242553370855, 0.010017054261930195, 0.013641918525991996, 0.00747712673469968, 0.01098483042153264, 0.025059819180383476, 0.015726035462742635, 0.021469802887126004, 0.017962511982155906, 0.014199647582054319, 0.01705504392250856, 0.01808672231918889, 0.015282499238793252, 0.017281340118485637, 0.018484249370163925, 0.023836602457718986, 0.014906579611790769, 0.01545667406050261, 0.01300483332821689, 0.01904013522086132, 0.02029792032133024, 0.017577748700614466, 0.020459928659780444, 0.01769282230180184, 0.01651020785300489, 0.022338812866698258, 0.01730813060649559, 0.013849031631898072, 0.014629145578068435, 0.01412423943452424, 0.011235051797882187, 0.021213783931144644, 0.017966985311004144, 0.019345871132538627, 0.02070944488355838, 0.021296566807021234, 0.017679854601955612, 0.02044798016495121, 0.02236245099789984, 0.022045215886846624, 0.024714536573110936, 0.02341874991787497, 0.022692933474942837, 0.019489536047323622, 0.021454722888957178, 0.024102540147590425, 0.022723743289410146, 0.01811107314434284, 0.027955360244665436, 0.029693336425170733, 0.022092949978974864, 0.02060391690614277, 0.020227662263366513, 0.02186878727329248, 0.020643157912320177, 0.020581249200685752, 0.02401125978174013, 0.020483064172186295, 0.01894788100771899, 0.013199062088292763, 0.018924723597788594, 0.015913293959656866, 0.023726650569437305, 0.03209317895936511, 0.023924144481732884, 0.024908854286703898, 0.019815244282194236, 0.026711247935086165, 0.023024143418585018, 0.018027219482393768, 0.025470236848041096, 0.01934386003654591, 0.030002384880995468, 0.02434215710840058, 0.023099013513726247, 0.025808104815868864, 0.02593282314446551, 0.014570938348351662, 0.024800313706501664, 0.01975424508920614, 0.025975520005471164, 0.02834029743256367, 0.019469502311223214, 0.025340604374312667, 0.024009925371356183, 0.02618614088295156, 0.018186494594747613, 0.027179167051822765, 0.020327996956485725, 0.022961901723690725, 0.02511640672295723, 0.020058699058314075, 0.02123145626812029, 0.02416966950364641, 0.020318770404785615, 0.020330997619303296, 0.01910697336408467, 0.021560938661963888, 0.031053507841310102, 0.021386134432196374, 0.019334069869300346, 0.020815532615873662, 0.021886977524683267, 0.022575662371210753, 0.018076796524943546, 0.02475696749088698, 0.01690617986479612, 0.023014808743490888, 0.02239909165745904, 0.019414478684246964, 0.02126438381369209, 0.026435369675543226, 0.02321695547264712, 0.02026491413020337, 0.02276599108457713, 0.025346905142254805, 0.016851465631335695, 0.018512057400604083, 0.022818746380275385, 0.019440717850837237, 0.02419791421996577, 0.020302251225014143, 0.02170577924064879, 0.02833480155373868, 0.024391491296558397, 0.02393333005450325, 0.020674324241935136, 0.015717389210480822, 0.02184580503312656, 0.02370931977036033, 0.024816314858720687, 0.0208155881581315, 0.024494209055712462, 0.01989966712259495, 0.021382971891873156, 0.023475721570887505, 0.022714412862494763, 0.02142924120952488, 0.021218670847525774, 0.01948532211977159, 0.024639964843320723, 0.024154997499763263, 0.014917568153227473, 0.01993965423375101, 0.020240873087141495, 0.021604932736671294, 0.018096429909993873, 0.026438379794110345, 0.023869567909196223, 0.01896239898049375, 0.022422507602652277, 0.02537562501844196, 0.022163408175440793, 0.022591982825436195, 0.02434593179814932, 0.022282374909467138, 0.02327825113938519, 0.01704078015072878, 0.017361769147915932, 0.02181156153868879, 0.026979380995958546, 0.022053997531850618, 0.023253690747920486, 0.019996442697166005, 0.020111654818240902, 0.020044806293820893, 0.021015438760268446, 0.026169932588865463, 0.028504932318621532, 0.02720982130281275, 0.023597492004961414, 0.021803868463719924, 0.02146589431999772, 0.023047378184171016, 0.019329543692848797, 0.022032361647710733, 0.01869904312579855, 0.019246037540198137, 0.02018352427835213, 0.01774164229695465, 0.016572091320039298, 0.020901135209239554, 0.02068695486763174, 0.02672771117653901, 0.017636130319911045, 0.0179944487091538, 0.02231198185253034, 0.017012270956856333, 0.018969570846044975, 0.017761218457781128, 0.019889671377537783, 0.015671848977604423, 0.012788248297921008, 0.016964682424587908], "moving_avg_accuracy_train": [0.017875041095653375, 0.0425058417110788, 0.06627744630888933, 0.0904809585993736, 0.1204489859341114, 0.14922896001627242, 0.17449867645812062, 0.19817834807956586, 0.22459252851593375, 0.24429220697547918, 0.26479764058417893, 0.28425914002243874, 0.30156304517048094, 0.31764580344182336, 0.3343747429619821, 0.3511253173288053, 0.36654288347567743, 0.3823759275732001, 0.39271152256966374, 0.4023194683424131, 0.4139610507163667, 0.4230438902506732, 0.4328775566658052, 0.43906660646826307, 0.4410915482346889, 0.4491116529396438, 0.44886978659709137, 0.45248566444578014, 0.4559863121369533, 0.4590346245602088, 0.46421235630061647, 0.4643600661993478, 0.46435556898671276, 0.4658763144309263, 0.46411427159294627, 0.46741248922301115, 0.4661358482914354, 0.47058682112886624, 0.4663475567842982, 0.4692148948212615, 0.4730700771391538, 0.4754471375776932, 0.47816988803065663, 0.4820870997513082, 0.48436009211675507, 0.4819325954731397, 0.48780326345904834, 0.4889183775143524, 0.49226518943184483, 0.49363641397680175, 0.4959978888005132, 0.49522672960392444, 0.4980035927673323, 0.4969713372073765, 0.4960963984559541, 0.501236424798297, 0.5071150187871827, 0.5085892288870543, 0.5124702750540429, 0.5164467033614385, 0.5177708154692315, 0.5197740293495878, 0.5235596248976947, 0.5289198219398097, 0.5355172947453672, 0.5363616646444462, 0.5380300458105849, 0.5442185120790096, 0.5459705978635413, 0.5493905970991971, 0.5514582741925258, 0.5519330343978579, 0.5592649290823799, 0.5608492255855946, 0.563284062826546, 0.5648640824505913, 0.5675067671884133, 0.5642171195335717, 0.5644806970666487, 0.5671920193750264, 0.5719425061487641, 0.5746000011618869, 0.5711475121775734, 0.5720709626140242, 0.568915987897703, 0.5737287376048412, 0.5755078838759647, 0.5769394517545182, 0.5739363230703917, 0.5794165530626326, 0.5784070135048005, 0.5840542660810462, 0.585860983166417, 0.5886030639230034, 0.5923683335908267, 0.593152765429926, 0.5928452415458942, 0.5947746397334975, 0.5979689303570932, 0.5986189128981649, 0.5956425262341329, 0.5914268548734088, 0.5871358176822565, 0.592239928385884, 0.5958904467253133, 0.597715251143775, 0.5988554150751709, 0.5981402063991875, 0.5985196922134494, 0.6015559687700668, 0.5990366111937707, 0.5939449626943641, 0.5992290767862234, 0.5976352306559123, 0.593759831523276, 0.5917829585419063, 0.5910200070837105, 0.5931967805424676, 0.5971995022148266, 0.6030663582698759, 0.6067261162920946, 0.6054309920313791, 0.6076894231726948, 0.6044937969430038, 0.5989376298329134, 0.5997789166835091, 0.5995600170325078, 0.5995066258405158, 0.602980525335414, 0.5992610938336187, 0.5946001487463531, 0.5953405436746895, 0.6012074639230843, 0.6043114249585628, 0.6001308051705785, 0.5963824145471622, 0.5963330327096941, 0.593264615870526, 0.5968512196731468, 0.5975550248064688, 0.5954591933587216, 0.5992688026138885, 0.5996072560780441, 0.6037572637897303, 0.6024823681197365, 0.6060330783938648, 0.6122857836416414, 0.61491911162553, 0.6056029269187725, 0.602792445331085, 0.6038018523414427, 0.601322648932926, 0.5944284872084687, 0.5954326943084968, 0.5904849102677818, 0.5924879416547173, 0.5941043335588285, 0.5949476802820801, 0.6012022266053395, 0.5992083449608632, 0.5993157511093874, 0.6047550678750636, 0.6003302651930851, 0.5989611216506592, 0.6002720645278137, 0.5976926882245339, 0.5973938766741367, 0.5986849048370572, 0.5977079095718362, 0.5917258134167013, 0.5913262888995051, 0.5939701602171957, 0.594366652956781, 0.5933729094034433, 0.59288994734945, 0.59438257752222, 0.5964325736228957, 0.5996608534110768, 0.6030682489727468, 0.6001610389298926, 0.6016471037338395, 0.5981469728005164, 0.6049860006611993, 0.6031879812694518, 0.6057616826810818, 0.6058530988825657, 0.5973571965536022, 0.5993814487224853, 0.6034936553983173, 0.6031378695638658, 0.6033265094139577, 0.6038867660837653, 0.6024335101795232, 0.6000122661717684, 0.5946505946256511, 0.5944606438863234, 0.595010448303062], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 835198913, "moving_var_accuracy_train": [0.0028756538475416737, 0.008048175513399083, 0.012329160628450998, 0.016368534630366506, 0.022814425128350184, 0.027987564789043906, 0.03093583543160224, 0.03288879352133732, 0.03587929452232859, 0.03578406105278103, 0.03598991021483007, 0.03579966883681556, 0.03491452815348598, 0.03375097136066729, 0.03289459098182273, 0.032130367558206724, 0.031056642915424655, 0.030207146192429073, 0.02814785228856449, 0.026163880657456878, 0.024767230553237114, 0.02303298926396705, 0.021599999294047205, 0.01978473840175822, 0.01784316806399914, 0.01663774997290518, 0.014974501469563601, 0.013594722476156985, 0.012345541036860733, 0.011194616810842628, 0.010316435283538993, 0.009284988119112743, 0.008356489489225762, 0.007541654540658052, 0.006815432241258138, 0.006231793172949763, 0.005623282164268358, 0.0052392543806374465, 0.004877071202221836, 0.004463358728763602, 0.0041507847322249485, 0.003786560005958676, 0.0034746243356248217, 0.00326526283104203, 0.0029852349965782455, 0.002739746156513295, 0.0027759542242689174, 0.002509550116049057, 0.0023594054545437743, 0.002140387219863627, 0.0019765375679644728, 0.0017842359897263767, 0.0016752111120083622, 0.0015172799646770633, 0.0013724416285780226, 0.0014729763029200347, 0.0016366994782034984, 0.0014925891891502201, 0.001478892944387867, 0.0014733114887037965, 0.0013417597956994537, 0.001243699608783577, 0.0012483062507898423, 0.0013820610366835425, 0.0016355947597958376, 0.00147845192855449, 0.0013556581971387756, 0.0015647664102237731, 0.0014359180105686176, 0.0013975937624587343, 0.0012963119832733476, 0.001168709360219116, 0.0015356485411815114, 0.0014046736457542415, 0.0013175621726881335, 0.0012082741135306342, 0.0011503007457892272, 0.0011326667064473542, 0.0010200252938461051, 0.000984184182400648, 0.001088869885447692, 0.0010435434146058786, 0.0010464661948265427, 0.0009494944217211199, 0.000944129768694643, 0.0010581798295172039, 0.0009808500996519565, 0.0009012095690049136, 0.0008922576491452387, 0.0010733281711414367, 0.0009751678850967436, 0.0011646742515262901, 0.0010775848660127992, 0.0010374974412922936, 0.0010613429982059375, 0.0009607466981770783, 0.00086552316681262, 0.000812474046428299, 0.0008230580750773864, 0.0007445545633029309, 0.0007498290051370897, 0.0008347930698180464, 0.0009170307644189084, 0.001059795202650974, 0.001073752239704464, 0.0009963462162247522, 0.0009084113587163814, 0.0008221739338965595, 0.0007412526258559368, 0.0007500981412247235, 0.0007322127904774157, 0.0008923154714032594, 0.001054380679885004, 0.000971805721280474, 0.0010097936150875674, 0.0009439864946390361, 0.0008548266995232005, 0.0008119891137876221, 0.0008749862294862115, 0.0010972676062736043, 0.0011080853046769952, 0.0010123728958655418, 0.0009570402072595715, 0.0009532444295326187, 0.0011357589231766033, 0.00102855290294381, 0.0009261288661643057, 0.0008335416351223161, 0.0008587992709159711, 0.0008974268800933045, 0.0010032038740425164, 0.0009078171484874193, 0.0011268222124479065, 0.0011008511581910392, 0.001148064278677114, 0.0011597117412008389, 0.0010437625141736006, 0.001024122899846248, 0.0010374841513943831, 0.0009381938112461594, 0.0008839070152378419, 0.0009261344178075422, 0.0008345519327533784, 0.0009060998155415332, 0.000830118064711698, 0.0008605741492977263, 0.0011263836406081214, 0.0010761550229838887, 0.0017496611980993026, 0.001645784339081947, 0.0014903760277867862, 0.0013966564708753162, 0.0016847560167345254, 0.0015253563021587924, 0.0015931457741648945, 0.001469940409381839, 0.0013464608735327395, 0.0012182158894400372, 0.0014484684478842024, 0.0013394016792054005, 0.0012055653360115279, 0.001351284304306676, 0.0013923657828460044, 0.0012700001909193026, 0.0011584673128718305, 0.0011024992206099349, 0.0009930528936327987, 0.0009087483877266015, 0.0008264642266883203, 0.0010658870737030057, 0.0009607349448912724, 0.0009275719503027126, 0.0008362296137053363, 0.0007614943885830059, 0.0006874442208350819, 0.0006387513022455451, 0.0006126985281360625, 0.000645224788839463, 0.0006851954105787188, 0.0006927427016202991, 0.0006433439288720361, 0.0006892677849384754, 0.0010412917251574059, 0.0009662584162395648, 0.0009292480252216418, 0.0008363984349965214, 0.001402381798946451, 0.0012990219906408552, 0.0013213119852795805, 0.0011903200387915899, 0.0010716082998498148, 0.0009672724576894088, 0.0008895527864294004, 0.0008533593106922585, 0.0010267510755390384, 0.0009244006995354749, 0.0008346811936519155], "duration": 80521.469402, "accuracy_train": [0.1787504109565338, 0.2641830472499077, 0.28022188768918416, 0.308312569213732, 0.3901612319467516, 0.40824872675572166, 0.40192612443475456, 0.4112953926725729, 0.4623201524432447, 0.4215893131113879, 0.4493465430624769, 0.4594126349667774, 0.45729819150286083, 0.4623906278839055, 0.48493519864341084, 0.5018804866302141, 0.5053009787975268, 0.5248733244509044, 0.4857318775378368, 0.4887909802971577, 0.518735292081949, 0.5047894460594315, 0.5213805544019934, 0.4947680546903839, 0.45931602413252126, 0.5212925952842378, 0.4466929895141196, 0.4850285650839793, 0.48749214135751207, 0.486469436369509, 0.5108119419642857, 0.4656894552879291, 0.4643150940729974, 0.4795630234288483, 0.4482558860511259, 0.4970964478935954, 0.45464607990725364, 0.5106455766657438, 0.4281941776831857, 0.4950209371539313, 0.5077667180001846, 0.4968406815245478, 0.5026746421073275, 0.5173420052371723, 0.504817023405777, 0.46008512568060167, 0.540639275332226, 0.49895440401208935, 0.5223864966892765, 0.5059774348814138, 0.5172511622139165, 0.4882862968346253, 0.5229953612380029, 0.4876810371677741, 0.4882219496931524, 0.5474966618793835, 0.5600223646871539, 0.5218571197858989, 0.5473996905569398, 0.5522345581279993, 0.5296878244393688, 0.5378029542727943, 0.5576299848306571, 0.5771615953188446, 0.5948945499953857, 0.5439609937361573, 0.5530454763058325, 0.5999147084948321, 0.5617393699243264, 0.5801705902200996, 0.5700673680324843, 0.5562058762458472, 0.6252519812430786, 0.5751078941145257, 0.585197597995109, 0.5790842590669989, 0.5912909298288114, 0.5346102906399963, 0.5668528948643411, 0.5915939201504246, 0.6146968871124031, 0.5985174562799926, 0.5400751113187523, 0.5803820165420819, 0.5405212154508121, 0.6170434849690846, 0.591520200316076, 0.5898235626614987, 0.5469081649132521, 0.6287386229928018, 0.5693211574843116, 0.6348795392672573, 0.6021214369347545, 0.6132817907322813, 0.6262557606012367, 0.6002126519818198, 0.5900775265896088, 0.612139223421927, 0.6267175459694536, 0.604468755767811, 0.5688550462578442, 0.5534858126268918, 0.5485164829618863, 0.6381769247185308, 0.6287451117801771, 0.6141384909099299, 0.6091168904577335, 0.5917033283153378, 0.6019350645418051, 0.6288824577796235, 0.5763623930071059, 0.5481201261997047, 0.6467861036129567, 0.5832906154831119, 0.5588812393295497, 0.5739911017095791, 0.5841534439599483, 0.6127877416712809, 0.6332239972660576, 0.6558680627653193, 0.6396639384920635, 0.5937748736849391, 0.6280153034445368, 0.5757331608757844, 0.5489321258421004, 0.6073504983388704, 0.5975899201734958, 0.5990261051125877, 0.634245620789498, 0.5657862103174602, 0.5526516429609634, 0.6020040980297158, 0.6540097461586379, 0.6322470742778701, 0.5625052270787191, 0.5626468989364156, 0.5958885961724806, 0.5656488643180141, 0.6291306538967332, 0.6038892710063676, 0.576596710328996, 0.6335552859103912, 0.6026533372554448, 0.6411073331949059, 0.5910083070897932, 0.6379894708610189, 0.6685601308716317, 0.6386190634805279, 0.5217572645579549, 0.5774981110418974, 0.6128865154346622, 0.5790098182562754, 0.5323810316883536, 0.6044705582087486, 0.5459548539013473, 0.6105152241371355, 0.6086518606958288, 0.6025378007913437, 0.6574931435146733, 0.5812634101605758, 0.6002824064461055, 0.6537089187661499, 0.5605070410552787, 0.5866388297688262, 0.6120705504222038, 0.5744783014950167, 0.5947045727205611, 0.6103041583033407, 0.5889149521848468, 0.5378869480204872, 0.5877305682447398, 0.6177650020764119, 0.5979350876130491, 0.5844292174234035, 0.5885432888635105, 0.6078162490771503, 0.6148825385289775, 0.6287153715047066, 0.6337348090277778, 0.5739961485442046, 0.6150216869693613, 0.5666457944006091, 0.6665372514073459, 0.5870058067437246, 0.6289249953857512, 0.6066758446959211, 0.520894075592931, 0.6175997182424326, 0.6405035154808048, 0.5999357970538022, 0.605024268064784, 0.608929076112034, 0.5893542070413437, 0.5782210701019749, 0.5463955507105943, 0.5927510872323736, 0.5999586880537099], "end": "2016-01-24 23:26:37.450000", "learning_rate_per_epoch": [0.00647682836279273, 0.004579809494316578, 0.0037393986713141203, 0.003238414181396365, 0.002896525664255023, 0.0026441540103405714, 0.0024480109568685293, 0.002289904747158289, 0.0021589428652077913, 0.0020481529645621777, 0.0019528372213244438, 0.0018696993356570601, 0.0017963489517569542, 0.0017310051480308175, 0.001672309939749539, 0.0016192070906981826, 0.0015708616701886058, 0.0015266031259670854, 0.0014858862850815058, 0.0014482628321275115, 0.001413359772413969, 0.0013808644143864512, 0.0013505120296031237, 0.0013220770051702857, 0.0012953656259924173, 0.0012702106032520533, 0.0012464661849662662, 0.0012240054784342647, 0.0012027168413624167, 0.001182501669973135, 0.001163272769190371, 0.0011449523735791445, 0.0011274710996076465, 0.0011107668979093432, 0.001094783772714436, 0.0010794714326038957, 0.0010647840099409223, 0.001050680293701589, 0.0010371225653216243, 0.0010240764822810888, 0.0010115106124430895, 0.0009993963176384568, 0.0009877070551738143, 0.0009764186106622219, 0.000965508574154228, 0.0009549562237225473, 0.0009447425254620612, 0.0009348496678285301, 0.0009252611780539155, 0.000915961863938719, 0.000906937406398356, 0.0008981744758784771, 0.0008896607905626297, 0.0008813847089186311, 0.0008733353461138904, 0.0008655025740154088, 0.0008578769047744572, 0.0008504492579959333, 0.0008432112517766654, 0.0008361549698747694, 0.0008292729035019875, 0.0008225580095313489, 0.0008160036522895098, 0.0008096035453490913, 0.0008033516933210194, 0.0007972424500621855, 0.0007912705186754465, 0.0007854308350943029, 0.0007797185098752379, 0.0007741290610283613, 0.0007686581229791045, 0.0007633015629835427, 0.0007580554229207337, 0.00075291603570804, 0.0007478797342628241, 0.0007429431425407529, 0.0007381031173281372, 0.0007333563989959657, 0.0007287001353688538, 0.0007241314160637558, 0.0007196476217359304, 0.000715246016625315, 0.0007109242724254727, 0.0007066798862069845, 0.0007025106460787356, 0.0006984143401496112, 0.0006943888729438186, 0.0006904322071932256, 0.0006865424220450222, 0.0006827176548540592, 0.0006789561011828482, 0.0006752560148015618, 0.0006716158241033554, 0.0006680338410660625, 0.0006645085522904992, 0.0006610385025851429, 0.0006576222949661314, 0.0006542584742419422, 0.0006509457598440349, 0.0006476828129962087, 0.0006444685277529061, 0.0006413015653379261, 0.0006381808780133724, 0.0006351053016260266, 0.0006320737302303314, 0.0006290851742960513, 0.0006261386442929506, 0.0006232330924831331, 0.0006203676457516849, 0.0006175413727760315, 0.0006147533422335982, 0.0006120027392171323, 0.000609288748819381, 0.0006066105561330914, 0.0006039673462510109, 0.0006013584206812084, 0.000598783022724092, 0.0005962403956800699, 0.0005937298992648721, 0.0005912508349865675, 0.0005888025625608861, 0.0005863844999112189, 0.0005839959485456347, 0.0005816363845951855, 0.0005793051677756011, 0.0005770017160102725, 0.0005747255636379123, 0.0005724761867895722, 0.0005702529451809824, 0.0005680554313585162, 0.0005658831214532256, 0.0005637355498038232, 0.0005616122507490218, 0.0005595127586275339, 0.0005574366077780724, 0.0005553834489546716, 0.0005533528164960444, 0.0005513442447409034, 0.000549357442650944, 0.000547391886357218, 0.0005454473430290818, 0.0005435233470052481, 0.0005416196072474122, 0.0005397357163019478, 0.0005378713249228895, 0.0005360261420719326, 0.0005341998185031116, 0.0005323920049704611, 0.0005306024686433375, 0.0005288308020681143, 0.0005270768306218088, 0.0005253401468507946, 0.000523620517924428, 0.0005219177110120654, 0.0005202313768677413, 0.0005185612826608121, 0.0005169071955606341, 0.0005152688245289028, 0.0005136458785273135, 0.0005120382411405444, 0.0005104455631226301, 0.0005088676698505878, 0.0005073043284937739, 0.0005057553062215447, 0.0005042204284109175, 0.0005026994040235877, 0.0005011920002289116, 0.0004996981588192284, 0.0004982175887562335, 0.0004967500572092831, 0.0004952954477630556, 0.0004938535275869071, 0.0004924241220578551, 0.0004910071147605777, 0.0004896022146567702, 0.00048820930533111095, 0.00048682824126444757, 0.0004854588187299669, 0.0004841008922085166, 0.000482754287077114, 0.00048141885781660676, 0.00048009445890784264, 0.00047878094483166933, 0.00047747811186127365, 0.00047618590178899467, 0.0004749041108880192, 0.00047363259363919497, 0.0004723712627310306, 0.0004711199435405433, 0.00046987851965241134, 0.00046864684554748237, 0.00046742483391426504, 0.0004662123101297766, 0.0004650091868825257, 0.00046381531865336, 0.00046263058902695775, 0.00046145491069182754], "accuracy_valid": [0.18831331184111444, 0.27220797251506024, 0.2890036709337349, 0.30870699595256024, 0.3871614387236446, 0.40436452842620485, 0.3934385000941265, 0.4055543462914157, 0.45458984375, 0.41620534873870485, 0.4473671051393072, 0.44923786944653615, 0.45143513507153615, 0.4516998658697289, 0.4781597091490964, 0.5052093138177711, 0.49826160109186746, 0.5145175428275602, 0.48094820689006024, 0.47705078125, 0.512585008000753, 0.4958599044615964, 0.5170310146837349, 0.48800769484186746, 0.4573459855045181, 0.5149455242846386, 0.4458404908697289, 0.48331754753388556, 0.48311458725527107, 0.4769993058170181, 0.5034797392695783, 0.45808723173945787, 0.4585358033697289, 0.4675793015813253, 0.44423298663403615, 0.4860339796686747, 0.44744799510542166, 0.5020766660391567, 0.41646125517695787, 0.4893298781061747, 0.49594226515436746, 0.4918227597891566, 0.49680705242846385, 0.5134100856551205, 0.494762742375753, 0.4547119140625, 0.531761812876506, 0.48888277720256024, 0.5190150249435241, 0.49339937876506024, 0.5127276684864458, 0.4834204983998494, 0.5178252070783133, 0.48458972609186746, 0.4834499129329819, 0.5472956278237951, 0.5567156320594879, 0.5154543957078314, 0.5442541650978916, 0.5535520990210843, 0.5198695171310241, 0.5339076030685241, 0.5494311229292168, 0.5683534920933735, 0.5920557228915663, 0.535423922251506, 0.554600727127259, 0.5938676581325302, 0.5633486092808735, 0.584019672439759, 0.5658723762236446, 0.5541830407567772, 0.6167551063629518, 0.5742746376129518, 0.5809576195406627, 0.5809487951807228, 0.5905511695218373, 0.5355665827371988, 0.5671636742281627, 0.5794221809111446, 0.6087087608245482, 0.6004491599209337, 0.5375800075301205, 0.5800325324736446, 0.5371740869728916, 0.6169595373682228, 0.5933690817959337, 0.5903982139495482, 0.5458719644201807, 0.6209569724209337, 0.5723112175263554, 0.6234498541039157, 0.5975091773343373, 0.6085778661521084, 0.6173463384789157, 0.5962884742093373, 0.5911409309111446, 0.6036935829254518, 0.6224938817771084, 0.6008256659450302, 0.5684652673192772, 0.5492502235504518, 0.5433276073042168, 0.6264810217432228, 0.6167359869164157, 0.6138062994164157, 0.6102765554405121, 0.5911909356174698, 0.6019551840173193, 0.617030132247741, 0.5737451760165663, 0.5424128153237951, 0.6360642766378012, 0.5780176369540663, 0.5557699548192772, 0.5704389824924698, 0.5813135353915663, 0.6126664862575302, 0.6350068241716867, 0.651831937123494, 0.6331242940512049, 0.5880685829254518, 0.6234498541039157, 0.5709375588290663, 0.5418524684676205, 0.6006830054593373, 0.5919454183923193, 0.5955354621611446, 0.6276826054216867, 0.5608454325112951, 0.5474677028426205, 0.5962884742093373, 0.6450474750564759, 0.6304593373493976, 0.554955172251506, 0.554466891001506, 0.5863493034638554, 0.5586790521460843, 0.6267972279743976, 0.597498882247741, 0.5715273202183735, 0.6248132177146084, 0.6033376670745482, 0.6400925969503012, 0.5885568641754518, 0.6299901755459337, 0.6603062641189759, 0.6305196371423193, 0.5175192959337349, 0.5702242564006024, 0.6102662603539157, 0.5800119423004518, 0.5234389707266567, 0.5961766989834337, 0.5436335184487951, 0.6065732657191265, 0.6037656485316265, 0.5960237434111446, 0.6563191241528614, 0.5700610057417168, 0.593785297439759, 0.647925687123494, 0.5560949854103916, 0.586146343185241, 0.6098897543298193, 0.5769190041415663, 0.5862375282379518, 0.603968608810241, 0.5881597679781627, 0.5364504894578314, 0.581263530685241, 0.6126370717243976, 0.6002564947289157, 0.5780073418674698, 0.5815164956701807, 0.6048025108245482, 0.6147828619164157, 0.6286076924887049, 0.6228497976280121, 0.5638971903237951, 0.6150270025414157, 0.5598894601844879, 0.6580987034073795, 0.5812517648719879, 0.6260648060993976, 0.6049863516566265, 0.5179163921310241, 0.6153417380459337, 0.639746976185994, 0.6003270896084337, 0.5996549675263554, 0.6087396460843373, 0.5898687523531627, 0.5746702630835843, 0.5420348385730422, 0.5916380365210843, 0.5972341514495482], "accuracy_test": 0.16098533163265305, "start": "2016-01-24 01:04:35.981000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0], "accuracy_train_last": 0.5999586880537099, "batch_size_eval": 1024, "accuracy_train_std": [0.011376306653363488, 0.01602168112303598, 0.015881017623934887, 0.016077276575299, 0.019150892416875908, 0.018520487296169695, 0.020919853893910958, 0.018057539674100967, 0.015821703813293415, 0.017759837788040276, 0.01784136298129727, 0.018537578421711033, 0.01595487903624125, 0.017993953724041112, 0.01628877759382868, 0.01628485866892347, 0.018376627574856818, 0.016351642436826398, 0.016509428363646897, 0.016602350580033415, 0.01607165756304855, 0.017730990761276783, 0.017124282900923035, 0.0164514002277961, 0.01831539338762392, 0.016353156593423797, 0.01848305843372084, 0.01823973113898462, 0.020308137907328036, 0.016378315249202677, 0.01639981701338822, 0.01443113711682454, 0.016645146626744347, 0.019883916418505752, 0.017807361275955582, 0.01693988502898731, 0.01697235662874819, 0.01739557150202432, 0.01832688069605559, 0.017069032869512522, 0.018320179427857904, 0.016271717469578586, 0.015955527744092386, 0.016423450219203453, 0.01876497982272591, 0.01731941535991758, 0.017689183975988912, 0.016371977102435608, 0.017583024745465588, 0.017249474726703972, 0.015797371660777593, 0.016529669327602397, 0.016384778937142352, 0.01714635359653693, 0.01877469023114057, 0.014579629273684856, 0.015706915912115406, 0.01824759433293919, 0.017124101002598306, 0.014973696757329267, 0.015728226958011914, 0.017008469649939226, 0.01868401324650917, 0.01729629880315702, 0.017293382269965386, 0.015422451840258347, 0.016030897166770596, 0.015676709781969205, 0.01642769574257479, 0.01818968665170932, 0.01680940087754279, 0.014240914664228572, 0.01589399523064252, 0.015526841859268833, 0.015947187300302906, 0.01584880224662773, 0.015175466273092107, 0.014200677423045824, 0.0156119679998284, 0.016978014840426825, 0.01544410011580467, 0.01593161486455884, 0.013655970116833718, 0.015227651856633318, 0.01616113735023861, 0.016059525352081272, 0.01451832677544021, 0.014342989116538985, 0.013824196622868642, 0.014894524584437295, 0.017227410516260047, 0.013661768508481562, 0.014976269973727129, 0.013809379684339314, 0.014824506135762212, 0.015470480718444916, 0.01599890310713654, 0.014723653729597677, 0.014420047852853094, 0.014442874052736584, 0.014607369718797638, 0.01532286867780798, 0.015429976573787259, 0.016034597530375484, 0.015481964373332054, 0.01510556757398973, 0.0163411767827099, 0.01661379526207751, 0.01590218722814177, 0.01279776202722676, 0.014701145985865268, 0.017922828179793127, 0.01348952557128832, 0.016219442418295992, 0.016108556378461262, 0.016570229998738525, 0.017463829654785285, 0.01661925867993173, 0.017257356832841013, 0.016456044011444015, 0.015407268744678184, 0.016507047600029332, 0.015444942910877809, 0.015915300041274033, 0.016523297315262826, 0.01712602002341722, 0.016498305058189763, 0.014025794312022097, 0.017335712001110688, 0.01756747338405296, 0.017119325826421155, 0.01673372328167106, 0.01463915393356056, 0.015067966480657242, 0.01756247867792717, 0.016091615189937097, 0.017283035041640474, 0.017451193106598747, 0.016798239820077745, 0.014124167171293164, 0.01756408101431557, 0.015234891544302156, 0.01623551062501868, 0.014845617667046472, 0.017028645092497272, 0.01682773958787139, 0.014559888841907585, 0.01412166315390786, 0.016625114780102444, 0.01798990871169209, 0.015336456544236517, 0.015387758237059098, 0.016068339686580127, 0.016107755384599062, 0.016300817371811358, 0.015486766657999126, 0.016468451084431773, 0.01620058802681282, 0.014564994183018728, 0.016213269039998764, 0.015749100706988508, 0.014172452069045702, 0.016059747751785248, 0.014384520153791137, 0.015785857617764115, 0.0162784731275792, 0.016161885498881125, 0.015868945418222608, 0.016110926535258675, 0.01574640282505308, 0.015490165685426967, 0.01548552018688335, 0.01559600050280747, 0.017491942406318357, 0.01448890965133542, 0.016809772559460995, 0.01607938960910441, 0.017182803689791776, 0.015777796153640535, 0.015607145289303207, 0.015867610851522367, 0.016004237634206157, 0.014798226505484952, 0.015731189115287977, 0.01622309273419635, 0.01704863925656485, 0.016663064238641792, 0.016867008099489975, 0.01410879405643752, 0.015184972580092694, 0.016097542325942092, 0.0177927739463186, 0.017142432072205824, 0.016992094922740012, 0.01650731181476206, 0.015253761296196782, 0.0165910936642383], "accuracy_test_std": 0.01342178704909985, "error_valid": [0.8116866881588856, 0.7277920274849398, 0.7109963290662651, 0.6912930040474398, 0.6128385612763554, 0.5956354715737951, 0.6065614999058735, 0.5944456537085843, 0.54541015625, 0.5837946512612951, 0.5526328948606928, 0.5507621305534638, 0.5485648649284638, 0.5483001341302711, 0.5218402908509037, 0.4947906861822289, 0.5017383989081325, 0.48548245717243976, 0.5190517931099398, 0.52294921875, 0.487414991999247, 0.5041400955384037, 0.4829689853162651, 0.5119923051581325, 0.5426540144954819, 0.4850544757153614, 0.5541595091302711, 0.5166824524661144, 0.5168854127447289, 0.5230006941829819, 0.49652026073042166, 0.5419127682605421, 0.5414641966302711, 0.5324206984186747, 0.5557670133659638, 0.5139660203313253, 0.5525520048945783, 0.4979233339608433, 0.5835387448230421, 0.5106701218938253, 0.5040577348456325, 0.5081772402108433, 0.5031929475715362, 0.4865899143448795, 0.505237257624247, 0.5452880859375, 0.46823818712349397, 0.5111172227974398, 0.48098497505647586, 0.5066006212349398, 0.4872723315135542, 0.5165795016001506, 0.48217479292168675, 0.5154102739081325, 0.5165500870670181, 0.45270437217620485, 0.44328436794051207, 0.48454560429216864, 0.4557458349021084, 0.44644790097891573, 0.48013048286897586, 0.46609239693147586, 0.4505688770707832, 0.4316465079066265, 0.40794427710843373, 0.46457607774849397, 0.44539927287274095, 0.4061323418674698, 0.4366513907191265, 0.41598032756024095, 0.4341276237763554, 0.44581695924322284, 0.38324489363704817, 0.42572536238704817, 0.4190423804593373, 0.41905120481927716, 0.4094488304781627, 0.4644334172628012, 0.4328363257718373, 0.4205778190888554, 0.39129123917545183, 0.39955084007906627, 0.4624199924698795, 0.4199674675263554, 0.4628259130271084, 0.38304046263177716, 0.40663091820406627, 0.40960178605045183, 0.4541280355798193, 0.37904302757906627, 0.4276887824736446, 0.37655014589608427, 0.4024908226656627, 0.3914221338478916, 0.38265366152108427, 0.4037115257906627, 0.4088590690888554, 0.39630641707454817, 0.3775061182228916, 0.3991743340549698, 0.43153473268072284, 0.45074977644954817, 0.4566723926957832, 0.37351897825677716, 0.38326401308358427, 0.38619370058358427, 0.38972344455948793, 0.4088090643825302, 0.3980448159826807, 0.38296986775225905, 0.42625482398343373, 0.45758718467620485, 0.3639357233621988, 0.42198236304593373, 0.44423004518072284, 0.4295610175075302, 0.41868646460843373, 0.3873335137424698, 0.36499317582831325, 0.34816806287650603, 0.36687570594879515, 0.41193141707454817, 0.37655014589608427, 0.42906244117093373, 0.4581475315323795, 0.3993169945406627, 0.4080545816076807, 0.4044645378388554, 0.37231739457831325, 0.43915456748870485, 0.4525322971573795, 0.4037115257906627, 0.35495252494352414, 0.36954066265060237, 0.44504482774849397, 0.44553310899849397, 0.4136506965361446, 0.44132094785391573, 0.37320277202560237, 0.40250111775225905, 0.4284726797816265, 0.3751867822853916, 0.39666233292545183, 0.3599074030496988, 0.41144313582454817, 0.37000982445406627, 0.33969373588102414, 0.3694803628576807, 0.4824807040662651, 0.42977574359939763, 0.38973373964608427, 0.41998805769954817, 0.4765610292733433, 0.40382330101656627, 0.45636648155120485, 0.3934267342808735, 0.3962343514683735, 0.4039762565888554, 0.3436808758471386, 0.4299389942582832, 0.40621470256024095, 0.35207431287650603, 0.4439050145896084, 0.41385365681475905, 0.3901102456701807, 0.42308099585843373, 0.41376247176204817, 0.39603139118975905, 0.4118402320218373, 0.46354951054216864, 0.41873646931475905, 0.38736292827560237, 0.39974350527108427, 0.4219926581325302, 0.4184835043298193, 0.39519748917545183, 0.38521713808358427, 0.37139230751129515, 0.37715020237198793, 0.43610280967620485, 0.38497299745858427, 0.44011053981551207, 0.3419012965926205, 0.41874823512801207, 0.37393519390060237, 0.3950136483433735, 0.48208360786897586, 0.38465826195406627, 0.36025302381400603, 0.39967291039156627, 0.4003450324736446, 0.3912603539156627, 0.4101312476468373, 0.42532973691641573, 0.45796516142695776, 0.40836196347891573, 0.40276584855045183], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-3, 3], "momentum": 0.7262475082433668, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.006476828393735461, "patience_threshold": 1, "do_flip": true, "batch_size": 128, "optimization": "rmsprop", "nb_data_augmentation": 2, "learning_rate_decay_method": "sqrt", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0004867709259862579, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.075525243883142}, "accuracy_valid_max": 0.6603062641189759, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-3, 3)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.5972341514495482, "loss_train": [4.074851036071777, 2.176482677459717, 1.8655052185058594, 1.744855284690857, 1.667232871055603, 1.60951828956604, 1.5687998533248901, 1.5336754322052002, 1.5082721710205078, 1.4827816486358643, 1.4630719423294067, 1.4463231563568115, 1.4307780265808105, 1.4177641868591309, 1.404890775680542, 1.3909144401550293, 1.3834689855575562, 1.3717949390411377, 1.3609497547149658, 1.3530150651931763, 1.345526099205017, 1.3368812799453735, 1.3304907083511353, 1.3230600357055664, 1.317124605178833, 1.3090671300888062, 1.3047634363174438, 1.2972286939620972, 1.293296217918396, 1.2875868082046509, 1.2831252813339233, 1.2785524129867554, 1.2723844051361084, 1.2696791887283325, 1.2657960653305054, 1.260614275932312, 1.2569808959960938, 1.2535184621810913, 1.2491936683654785, 1.2454771995544434, 1.2425799369812012, 1.242343783378601, 1.2373496294021606, 1.2326765060424805, 1.2312073707580566, 1.227434515953064, 1.2241331338882446, 1.2219105958938599, 1.219074010848999, 1.2157689332962036, 1.2119245529174805, 1.2112812995910645, 1.2075660228729248, 1.2056822776794434, 1.2025325298309326, 1.2015042304992676, 1.1978189945220947, 1.1952928304672241, 1.192333459854126, 1.1904774904251099, 1.1897691488265991, 1.184944987297058, 1.1839114427566528, 1.180309772491455, 1.1806613206863403, 1.179368019104004, 1.174721598625183, 1.1728410720825195, 1.1716103553771973, 1.170128583908081, 1.166801929473877, 1.166807770729065, 1.1635758876800537, 1.161568284034729, 1.160995364189148, 1.1618754863739014, 1.1565532684326172, 1.1564654111862183, 1.1533445119857788, 1.1503844261169434, 1.1507807970046997, 1.1479450464248657, 1.1470087766647339, 1.1455225944519043, 1.1449596881866455, 1.1429325342178345, 1.1414361000061035, 1.1394782066345215, 1.138325572013855, 1.1371124982833862, 1.1353845596313477, 1.1330679655075073, 1.133078932762146, 1.1316709518432617, 1.1298675537109375, 1.1290218830108643, 1.1276758909225464, 1.1262022256851196, 1.1250804662704468, 1.1234346628189087, 1.1223978996276855, 1.1215850114822388, 1.1230332851409912, 1.1171667575836182, 1.1173089742660522, 1.117710828781128, 1.1148710250854492, 1.1137449741363525, 1.1137111186981201, 1.1129502058029175, 1.1108988523483276, 1.1105949878692627, 1.1091057062149048, 1.1086337566375732, 1.1064670085906982, 1.1071149110794067, 1.106400966644287, 1.1031246185302734, 1.1024271249771118, 1.1021437644958496, 1.1019269227981567, 1.099944829940796, 1.098936676979065, 1.097056269645691, 1.0988694429397583, 1.096808671951294, 1.0958842039108276, 1.0923895835876465, 1.0929861068725586, 1.093221664428711, 1.0915316343307495, 1.0922931432724, 1.090329647064209, 1.0877108573913574, 1.0883502960205078, 1.0867688655853271, 1.0874238014221191, 1.0864216089248657, 1.086113691329956, 1.0844917297363281, 1.085772156715393, 1.0832196474075317, 1.0844160318374634, 1.0812962055206299, 1.0801488161087036, 1.0795692205429077, 1.0789371728897095, 1.077979326248169, 1.0784485340118408, 1.0773571729660034, 1.0761066675186157, 1.0734609365463257, 1.0768145322799683, 1.074088215827942, 1.0722784996032715, 1.0736857652664185, 1.0715320110321045, 1.0719201564788818, 1.0702190399169922, 1.0685019493103027, 1.0688773393630981, 1.066613793373108, 1.0691604614257812, 1.065008282661438, 1.0657330751419067, 1.0657033920288086, 1.063807725906372, 1.0653434991836548, 1.0636736154556274, 1.0638757944107056, 1.062025785446167, 1.0608363151550293, 1.0614265203475952, 1.0603164434432983, 1.0588535070419312, 1.0595476627349854, 1.058856725692749, 1.0580319166183472, 1.0593068599700928, 1.0575480461120605, 1.056503176689148, 1.0560919046401978, 1.055673599243164, 1.0543855428695679, 1.0528538227081299, 1.053573727607727, 1.0503263473510742, 1.0515083074569702, 1.0518487691879272, 1.0517535209655762, 1.0501723289489746, 1.0492416620254517, 1.0504018068313599, 1.048696517944336, 1.0466655492782593, 1.0467243194580078, 1.0466465950012207], "accuracy_train_first": 0.1787504109565338, "model": "residualv3", "loss_std": [1.9306104183197021, 0.17027758061885834, 0.09818532317876816, 0.0953168123960495, 0.0955631211400032, 0.09344642609357834, 0.0954756960272789, 0.09515554457902908, 0.0925234705209732, 0.09260933846235275, 0.09141277521848679, 0.09128301590681076, 0.09089167416095734, 0.09299010038375854, 0.08883169293403625, 0.09114342927932739, 0.08822645992040634, 0.09003793448209763, 0.08918549120426178, 0.08751677721738815, 0.0862913578748703, 0.08948783576488495, 0.08700050413608551, 0.08931571245193481, 0.08771133422851562, 0.09032206982374191, 0.0874411091208458, 0.09053029865026474, 0.08981391787528992, 0.08821607381105423, 0.0887993797659874, 0.0846535712480545, 0.08629359304904938, 0.09011927247047424, 0.08569765836000443, 0.0883210301399231, 0.08778346329927444, 0.08569379150867462, 0.08663831651210785, 0.08655673265457153, 0.08671289682388306, 0.08683188259601593, 0.08691240847110748, 0.08846636116504669, 0.08747535198926926, 0.08544810861349106, 0.08641611039638519, 0.08635470271110535, 0.08462371677160263, 0.08675938844680786, 0.08594360947608948, 0.08641199767589569, 0.0850302129983902, 0.08352109044790268, 0.0845712199807167, 0.08599398285150528, 0.08574148267507553, 0.08429093658924103, 0.08452818542718887, 0.0852113589644432, 0.08299927413463593, 0.08480942249298096, 0.08484793454408646, 0.08463513106107712, 0.08320114761590958, 0.08483478426933289, 0.08607835322618484, 0.08645221590995789, 0.08370401710271835, 0.08567722141742706, 0.08476950228214264, 0.08419950306415558, 0.08518368750810623, 0.08357277512550354, 0.08343370258808136, 0.08711342513561249, 0.08306971192359924, 0.08225249499082565, 0.08638742566108704, 0.08480020612478256, 0.08553633093833923, 0.08511342108249664, 0.08454646170139313, 0.08455219119787216, 0.08276182413101196, 0.0855093002319336, 0.08606570959091187, 0.08548375964164734, 0.08459601551294327, 0.08376869559288025, 0.08492521196603775, 0.0841488242149353, 0.08421528339385986, 0.0846981406211853, 0.08369539678096771, 0.08371191471815109, 0.08380115777254105, 0.08473120629787445, 0.08351936936378479, 0.08280733972787857, 0.08260270953178406, 0.08370694518089294, 0.08455460518598557, 0.083847276866436, 0.08572457730770111, 0.08353272080421448, 0.0847543478012085, 0.08455423265695572, 0.08236940205097198, 0.08321502059698105, 0.0846717357635498, 0.08548547327518463, 0.08590307086706161, 0.08236052840948105, 0.08368838578462601, 0.08240892738103867, 0.08180318772792816, 0.08239960670471191, 0.08199578523635864, 0.0857401043176651, 0.08465807139873505, 0.08261054009199142, 0.08133406192064285, 0.08405093103647232, 0.08157356828451157, 0.08237864822149277, 0.0812053233385086, 0.08332992345094681, 0.08452992141246796, 0.0828423872590065, 0.08216114342212677, 0.08341420441865921, 0.0836334079504013, 0.08370400220155716, 0.08375190198421478, 0.08267202973365784, 0.08153665065765381, 0.08344617486000061, 0.08404228091239929, 0.0837717279791832, 0.08423067629337311, 0.0864247977733612, 0.08473724126815796, 0.08386687189340591, 0.08282256871461868, 0.08188927173614502, 0.08541259169578552, 0.08086326718330383, 0.08222586661577225, 0.08321833610534668, 0.08388031274080276, 0.08448562026023865, 0.08196871727705002, 0.08404988795518875, 0.08584804087877274, 0.08253466337919235, 0.08577822148799896, 0.08542957156896591, 0.08214762061834335, 0.0827556699514389, 0.08402896672487259, 0.0846736803650856, 0.0833730697631836, 0.08523119986057281, 0.08161474764347076, 0.08511799573898315, 0.08375313133001328, 0.08547799289226532, 0.08545698970556259, 0.08165452629327774, 0.08263645321130753, 0.08429358899593353, 0.08456709235906601, 0.08294427394866943, 0.08388512581586838, 0.08429481834173203, 0.08632105588912964, 0.08472804725170135, 0.084630586206913, 0.08253785222768784, 0.084833063185215, 0.08427011221647263, 0.08100961148738861, 0.08368463069200516, 0.08304227888584137, 0.08496757596731186, 0.08257615566253662, 0.08370975404977798, 0.08444812148809433, 0.08246464282274246, 0.08111536502838135, 0.08349473029375076, 0.08462551981210709, 0.08476455509662628, 0.08397484570741653, 0.08425207436084747, 0.08374328911304474]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:11 2016", "state": "available"}], "summary": "5d207c1050581b49b0696aad1fbbc10a"}