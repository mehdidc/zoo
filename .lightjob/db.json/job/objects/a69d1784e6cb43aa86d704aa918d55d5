{"content": {"hp_model": {"f0": 32, "f1": 32, "f2": 32, "f3": 16, "nonlin": "very_leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.7472765445709229, 1.4019749164581299, 1.2175413370132446, 1.093256950378418, 1.0090101957321167, 0.951339840888977, 0.9076247811317444, 0.8708122968673706, 0.8403147459030151, 0.8118931651115417, 0.7920976877212524, 0.7675443291664124, 0.7465587258338928, 0.7301044464111328, 0.7111538648605347, 0.6945340633392334, 0.6799395680427551, 0.6635370254516602, 0.648826003074646, 0.6337907910346985, 0.623596727848053, 0.6104500889778137, 0.5973963141441345, 0.5869366526603699, 0.5790537595748901, 0.5623934268951416, 0.5562139749526978, 0.5467089414596558, 0.5368950963020325, 0.528770923614502, 0.5182082056999207, 0.5106735229492188, 0.5025950074195862, 0.4950898587703705, 0.48838838934898376, 0.47968485951423645, 0.4743952751159668, 0.4682982563972473, 0.4617431163787842, 0.4559890031814575, 0.4486416280269623, 0.4456798732280731, 0.4383459985256195, 0.43215614557266235, 0.43032848834991455, 0.4229121208190918, 0.4188654124736786, 0.41700854897499084, 0.4116170108318329, 0.4080304801464081, 0.4020189344882965, 0.39881184697151184, 0.3935345709323883, 0.39189693331718445, 0.3893435597419739, 0.3872198164463043, 0.38547655940055847, 0.3800623416900635, 0.37826865911483765, 0.375301718711853, 0.3741952180862427, 0.37026476860046387, 0.36975282430648804, 0.3680116832256317, 0.36628296971321106, 0.3613485097885132, 0.3610495328903198, 0.35941699147224426, 0.35834065079689026, 0.3558868169784546, 0.354845255613327, 0.3534476161003113, 0.35064810514450073, 0.3515360951423645, 0.35092806816101074, 0.3480148911476135, 0.3484647572040558, 0.34608274698257446, 0.34751400351524353, 0.3433941602706909, 0.3420562446117401, 0.3414844274520874, 0.34198665618896484, 0.34156477451324463, 0.3401101231575012, 0.34011152386665344, 0.33871790766716003, 0.337370365858078, 0.3378501534461975, 0.33710813522338867, 0.3369540870189667, 0.33692553639411926, 0.3324572443962097, 0.3341326415538788, 0.3350904583930969, 0.3336934745311737, 0.332667738199234, 0.3339497447013855, 0.33187925815582275, 0.33295103907585144, 0.3310854732990265, 0.3298729360103607, 0.3312748372554779, 0.3304094076156616, 0.3314371407032013, 0.33091482520103455, 0.33188313245773315, 0.3293353319168091, 0.32960763573646545, 0.3288974463939667, 0.32911646366119385, 0.3302910327911377, 0.32910555601119995, 0.32748943567276, 0.32826337218284607, 0.3297188878059387, 0.3274514675140381, 0.329282283782959, 0.3277774453163147, 0.3275468647480011, 0.3286762833595276, 0.3277173936367035, 0.3271391987800598, 0.3266615569591522, 0.32739943265914917, 0.32672014832496643, 0.32669350504875183, 0.32818007469177246, 0.3260445296764374, 0.32855552434921265, 0.32480835914611816, 0.3287763297557831, 0.32801079750061035, 0.32721784710884094, 0.32510504126548767, 0.3267894983291626, 0.3266127407550812, 0.32565248012542725, 0.32569894194602966, 0.3262603282928467, 0.32492926716804504, 0.327715665102005, 0.32372283935546875, 0.3260486125946045, 0.3259827196598053, 0.32602232694625854, 0.3264964520931244, 0.3281440734863281, 0.32698073983192444, 0.3254791498184204, 0.3267943561077118, 0.3251614570617676, 0.32776761054992676, 0.326592355966568, 0.32484039664268494, 0.3250679075717926, 0.32528311014175415, 0.32449138164520264, 0.3247442841529846, 0.32559362053871155, 0.3253030478954315, 0.3252451419830322, 0.32537585496902466, 0.32693710923194885, 0.32494863867759705, 0.3264869153499603, 0.3252769112586975, 0.32478293776512146, 0.3260747194290161, 0.3256496489048004, 0.3244789242744446, 0.32370638847351074, 0.32385915517807007, 0.3247966468334198, 0.3244014084339142, 0.3260120749473572, 0.32571160793304443, 0.32239606976509094, 0.32367178797721863, 0.3248685598373413, 0.3230056166648865, 0.32402756810188293, 0.3240305185317993, 0.3251878619194031, 0.32594987750053406, 0.3256685137748718, 0.3259923458099365, 0.3248848021030426, 0.32496389746665955, 0.3262154459953308, 0.32615000009536743, 0.3250804841518402, 0.32582396268844604, 0.3227721154689789, 0.3244977593421936, 0.324425607919693, 0.3271598815917969, 0.32499805092811584], "moving_avg_accuracy_train": [0.05064759900447581, 0.10391844351294295, 0.1590425695093323, 0.2117950182154681, 0.26592263430528157, 0.31666069268976815, 0.36301733489758164, 0.4064539843154923, 0.4467931764558792, 0.4830149242691783, 0.5170047199963302, 0.5470188992460051, 0.5751314199087985, 0.6002675308421987, 0.6242618684798781, 0.646491321685877, 0.666706948768858, 0.6860449863578267, 0.7040256768485668, 0.7205315660723941, 0.7355496628393241, 0.7498330687926101, 0.7630762898064831, 0.7754021979070918, 0.7864093044475934, 0.7968597491066547, 0.8058165758236655, 0.8148097801022255, 0.8231735254101092, 0.8311678545609127, 0.8386325041073592, 0.8452158661170275, 0.8516594722078903, 0.8576189006157927, 0.8633777696269801, 0.8684468554942009, 0.873439113206824, 0.8782345226398791, 0.8825201281462861, 0.8865468008186914, 0.8904150189464937, 0.8941008841614788, 0.89739029711807, 0.9004323652802477, 0.9031840693726088, 0.9061603658545062, 0.9089902755072893, 0.9116556686376235, 0.914149961702571, 0.9164505929836336, 0.9186419967770477, 0.9207398182268346, 0.9227486931721005, 0.9245032022002208, 0.9262496710398147, 0.9280097940002019, 0.9295428955860159, 0.9309829966870212, 0.932386188718439, 0.9337071902669531, 0.9348704429260737, 0.9360453616502622, 0.9372864392091655, 0.9383894220705026, 0.9393843957456967, 0.9404031049402761, 0.941403720670178, 0.9423437302592141, 0.9431943891869656, 0.9441158032409989, 0.9449520152872385, 0.9457092564264733, 0.9464139528422423, 0.9471457998176157, 0.9478254605323749, 0.9483767373554295, 0.948924039769988, 0.9494932337073486, 0.9500776960105248, 0.9505525948583926, 0.9510520834345688, 0.9515597158245468, 0.9519816716945653, 0.9523591428775912, 0.9527080954399151, 0.9530477293829114, 0.9534348161887601, 0.9538040846044911, 0.9541574967132096, 0.9544615085717428, 0.9547235655980126, 0.9549988723537799, 0.955216493597084, 0.955403016072001, 0.95564994135895, 0.9558047448017278, 0.9559882817764276, 0.9561813668393717, 0.956380756081745, 0.9565276543165476, 0.956738881738575, 0.9568917479886285, 0.9570433506041713, 0.9571937798998358, 0.9573290581194775, 0.9575136235838311, 0.9576588061624635, 0.9577685441439469, 0.9579277621963296, 0.9580547824018074, 0.9581830514795946, 0.958303143947222, 0.9583833253823725, 0.9584624641204365, 0.9585244604870934, 0.9586383138396852, 0.9587012543272561, 0.9587416968220406, 0.9588059247554234, 0.9588312138609534, 0.9588841649416354, 0.9588667167475826, 0.9589114672419826, 0.9589238409012284, 0.9589558674850166, 0.9589963171544736, 0.9590932117748513, 0.9591339500058195, 0.9591380262815388, 0.9591881618570578, 0.9592379702214627, 0.9592619435077789, 0.9592904228142547, 0.9593323302317496, 0.9593444342217714, 0.9593693508032857, 0.9594428569028205, 0.959481182704325, 0.9595040141328128, 0.9595268875672613, 0.959549726709437, 0.9595772934326424, 0.9595811771442416, 0.959556842796604, 0.9595767945623017, 0.9596411820299826, 0.9596456883770951, 0.9596660201311629, 0.959663428419357, 0.9596795889227516, 0.9596709179365304, 0.9596468740560833, 0.9596344991101003, 0.9596047604682394, 0.9596012471786598, 0.9596190115573239, 0.9596373606957499, 0.959660814317943, 0.9596773443779354, 0.9597084253759577, 0.9597154719348919, 0.9597264641355518, 0.9596759032470982, 0.9597093814093761, 0.9596883945304355, 0.9596625308929604, 0.9596486263121083, 0.9596988551583799, 0.9596975941926527, 0.9597242529627562, 0.9597506070534776, 0.9597023182196691, 0.9597704293632799, 0.9597341812889859, 0.9597572894959123, 0.9597640638916515, 0.9597911592847399, 0.9597574164182814, 0.9597851405098881, 0.9597683116114002, 0.9597949101348763, 0.9597654424810232, 0.9597714376270701, 0.9597652435632834, 0.9597294419713516, 0.9597413262683564, 0.9597706233261368, 0.9597597882971868, 0.9597849861009122, 0.9597681365945031, 0.9597947526196688, 0.9598303327863658, 0.9598717276292685, 0.9598833342533387, 0.959861300229306, 0.9598274826660008, 0.9598807882649877, 0.9598962112207425], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.0504354821630271, 0.10376742811088101, 0.1586196186111634, 0.21128160606292357, 0.2645329458802005, 0.31424446680422863, 0.3599340426200407, 0.40208211049659087, 0.44114047747780527, 0.47597768396571755, 0.5084511281538596, 0.5368390608015761, 0.5634034427955901, 0.5870245948921605, 0.6092285736559565, 0.6301338589390807, 0.649441014978531, 0.6671571932717171, 0.6834913492269249, 0.6984525552775005, 0.7118291029782896, 0.7243328926051594, 0.7361407672188904, 0.7469946404217002, 0.7564742755888675, 0.7654170153398452, 0.7725864277930594, 0.7801294428168408, 0.7872040656002923, 0.793374884096739, 0.7993080682209506, 0.8040781744259639, 0.8092482643158223, 0.8138839906421468, 0.8182677819017272, 0.8220402365805304, 0.8256826749424322, 0.8286037770359149, 0.831453524991209, 0.8342878823471331, 0.8366476095021035, 0.8387662163982786, 0.8409568128495651, 0.8426292038537653, 0.8442127454883436, 0.8459319312181237, 0.8477020134634047, 0.8492309638019286, 0.8505815755354407, 0.8518774277710532, 0.8535422711197009, 0.8547048632377761, 0.8558264973488628, 0.8564768165992929, 0.8572849190131587, 0.8581210449582284, 0.8590098946698604, 0.8596257244329196, 0.8604007272908325, 0.8610727862917944, 0.8616043972051601, 0.86177561222862, 0.8623844544319628, 0.8629588854947906, 0.8635013170224953, 0.8639019971613603, 0.8642992303800887, 0.8646201191831943, 0.8648356769184893, 0.865300292585074, 0.865545489230181, 0.8656674804521178, 0.8659390524840898, 0.8661325801705453, 0.8663555832133552, 0.8665695224917938, 0.8667488313024788, 0.8670486046104839, 0.8672807499852788, 0.867600573612504, 0.8678985628909374, 0.8682654390001869, 0.8685600359134212, 0.8687366353906032, 0.868919988982567, 0.8690280900937228, 0.8694051133038535, 0.8695114710905615, 0.8696417551750294, 0.8697234192659602, 0.8698478040901172, 0.8698854787356989, 0.8700068941527916, 0.8700195412868348, 0.8701997631276542, 0.8701666502843919, 0.8701867063591153, 0.8702037273177068, 0.8702068391491892, 0.8703327396186828, 0.8703819263589984, 0.8704770815676015, 0.8705261001615944, 0.8706790506687784, 0.8708798002988132, 0.8710228243634349, 0.8709531449956156, 0.8710776570679667, 0.871127653268173, 0.8712092709421086, 0.8712461057549008, 0.8712914641176638, 0.87144317943406, 0.871542072616407, 0.8714713555656097, 0.8714941889473018, 0.871440467294665, 0.8714908035659514, 0.8713885923264497, 0.8713963174782173, 0.8714398912085582, 0.871418072409615, 0.8714462341069066, 0.8713973079383093, 0.8713787179577314, 0.8713884600550305, 0.8713941394166208, 0.8713768957968714, 0.8713369624765969, 0.8713376435820999, 0.8714715044121429, 0.871566535588022, 0.8716419156323825, 0.8717209351948972, 0.8717544021987509, 0.8717245168546288, 0.8719560267184883, 0.8719436295248021, 0.8720301283004845, 0.8719726703461892, 0.8720562650397329, 0.8720582580764222, 0.8720844658719427, 0.872156881012911, 0.8722220546397825, 0.8723295390289669, 0.8723652398229829, 0.8725570914525068, 0.8724967948166688, 0.8724547348756645, 0.8725277737186704, 0.872433787762466, 0.8724224425893822, 0.8724620895672662, 0.8724468847050426, 0.872507472025201, 0.8724887584258435, 0.8724973597575815, 0.872565106603736, 0.8725273930066154, 0.8725677224653665, 0.8726294625494022, 0.8726606145625342, 0.8726265867094435, 0.8726325827354118, 0.8726501861900332, 0.872591757603033, 0.8726134435708923, 0.8726553159871464, 0.8726096109603444, 0.8726427481323822, 0.8726725715872162, 0.8726983831879073, 0.8726971995660291, 0.8727205483688388, 0.8727425918000272, 0.8726749226520275, 0.8726882921149874, 0.8726870880917418, 0.8726626199169802, 0.8726772196534449, 0.872653738322513, 0.8726315756160147, 0.8726116291801662, 0.8725947068965623, 0.872615068426409, 0.8725611811244307, 0.8725004755214003, 0.8725557037599229, 0.8724955458933432, 0.8725146460009215, 0.872605078285242, 0.8725511604887208, 0.8725158710117614, 0.8724586669113383, 0.8725058689796172, 0.8725107002386585, 0.8726015270992054], "moving_var_accuracy_train": [0.023086613564263617, 0.04631799807964482, 0.06903422167347262, 0.08717618710656674, 0.10482675780800618, 0.11751323714485372, 0.1251023579214175, 0.12957280474316607, 0.13126077807170095, 0.129942835396383, 0.12734630777890632, 0.1227193356053002, 0.11756022640691402, 0.11149062042192849, 0.10552311252777424, 0.09941813858353607, 0.0931543689304059, 0.08720456931749664, 0.08139385946046107, 0.0757064729260383, 0.07016571470794233, 0.06498528440778542, 0.06006520209240746, 0.05542603397770856, 0.05097383812948328, 0.04685936045868387, 0.042895447116362506, 0.039333801913489445, 0.03602999184231683, 0.03300217634522762, 0.03020344764636624, 0.027573168779882713, 0.025189532436982274, 0.022990212275824273, 0.02098967219883398, 0.019121965662713902, 0.017434072830065716, 0.015897629111734854, 0.014473163931570287, 0.013171774373709522, 0.011989264939696898, 0.010912608867174548, 0.009918730118848, 0.009010144715293227, 0.00817727712247115, 0.007439274476957433, 0.006767422527047921, 0.006154619159196223, 0.005595150724321207, 0.00508327179051172, 0.004618164866732653, 0.004195956073576063, 0.003812680673129912, 0.0034591173231847223, 0.003140656971535306, 0.002854473569902918, 0.002590179817164457, 0.0023498268560780606, 0.0021325647013635653, 0.0019350136370478001, 0.0017536906840835778, 0.0015907455217512583, 0.001445533431141052, 0.0013119292287585762, 0.001189646059411682, 0.0010800213692786006, 0.000981030318901084, 0.0008908798492582939, 0.0008083044498347349, 0.0007351150395819908, 0.0006678967909002796, 0.0006062678390967982, 0.0005501104285326982, 0.0004999197856376974, 0.00045408525525861157, 0.00041141188495348385, 0.0003729665558549702, 0.0003385857359144274, 0.0003078015279774906, 0.0002790511354210968, 0.00025339142141856226, 0.0002303714950668987, 0.00020893676636639692, 0.00018932545017589223, 0.00017148881617507498, 0.0001553780954946844, 0.0001411888117025757, 0.00012829716299802546, 0.00011659154776552244, 0.00010576420188012964, 9.580584665727296e-05, 8.69074062794852e-05, 7.864289670137199e-05, 7.109172273407699e-05, 6.453129933668303e-05, 5.8293846356077865e-05, 5.2767634110207896e-05, 4.7826407272976485e-05, 4.340157117544652e-05, 3.925562588039496e-05, 3.5731616506702555e-05, 3.236876766968116e-05, 2.9338741080067903e-05, 2.6608527729008022e-05, 2.4112376726492174e-05, 2.2007718749531017e-05, 1.9996648704823087e-05, 1.8105365655561318e-05, 1.652298258384609e-05, 1.501589151885803e-05, 1.3662378973819481e-05, 1.2425940883465217e-05, 1.1241208358003752e-05, 1.017345398096459e-05, 9.190700528176136e-06, 8.388293748426215e-06, 7.585117918364423e-06, 6.841326484987562e-06, 6.1943208833285156e-06, 5.580644644722237e-06, 5.0478145327585984e-06, 4.545773034764078e-06, 4.109219192029148e-06, 3.699675239814417e-06, 3.3389390344552534e-06, 3.019770712842342e-06, 2.8022907486812925e-06, 2.5369981049749094e-06, 2.2834478386910736e-06, 2.0777252382156155e-06, 1.8922805728763283e-06, 1.708224981699909e-06, 1.5447021216059502e-06, 1.4060379942150957e-06, 1.2667527539636125e-06, 1.1456650028764601e-06, 1.0797268226082585e-06, 9.849739438961502e-07, 8.911680166476584e-07, 8.06759961014121e-07, 7.307786026505938e-07, 6.645400604400963e-07, 5.982218033381565e-07, 5.437290672788382e-07, 4.929388171410414e-07, 4.809566493761191e-07, 4.330437489171903e-07, 3.934597960367687e-07, 3.5417426916385283e-07, 3.2110729907719074e-07, 2.8967324318791694e-07, 2.659088925516954e-07, 2.4069625688926776e-07, 2.2458611257792302e-07, 2.0223859015315993e-07, 1.8485488948173746e-07, 1.6939961846234006e-07, 1.574103081619315e-07, 1.4412846329588678e-07, 1.3840987290883424e-07, 1.2501577155328354e-07, 1.1360165067608913e-07, 1.2524911657950655e-07, 1.2281129106715962e-07, 1.1449420374944116e-07, 1.0906513306548041e-07, 9.989865607695064e-08, 1.1261522344926602e-07, 1.0136801141542724e-07, 9.762742048476141e-08, 9.411552131605899e-08, 1.0569027241967465e-07, 1.3687339613342232e-07, 1.3501136253031098e-07, 1.263161293234755e-07, 1.1409754832980215e-07, 1.092952364363811e-07, 1.0861294212425884e-07, 1.0466927521058749e-07, 9.675125410836931e-08, 9.344346175749717e-08, 9.19141991941916e-08, 8.304625525987446e-08, 7.50869275696361e-08, 7.911402067639543e-08, 7.247374724642634e-08, 7.295123087310517e-08, 6.671268845691472e-08, 6.575578342447425e-08, 6.173535787808498e-08, 6.193753725090758e-08, 6.713731788537818e-08, 7.584538326731461e-08, 6.947326844134185e-08, 6.689542553285309e-08, 7.049853127073097e-08, 8.90220600937702e-08, 8.226066216233158e-08], "duration": 144221.909433, "accuracy_train": [0.5064759900447582, 0.5833560440891473, 0.6551597034768365, 0.6865670565706903, 0.7530711791136028, 0.7733032181501477, 0.7802271147679033, 0.7973838290766887, 0.8098459057193614, 0.8090106545888703, 0.8229128815406977, 0.8171465124930787, 0.8281441058739387, 0.8264925292428018, 0.8402109072189923, 0.846556400539867, 0.8486475925156883, 0.8600873246585455, 0.8658518912652271, 0.8690845690868402, 0.8707125337416944, 0.8783837223721853, 0.88226527893134, 0.8863353708125692, 0.8854732633121077, 0.8909137510382059, 0.8864280162767626, 0.8957486186092655, 0.8984472331810631, 0.9031168169181433, 0.9058143500253784, 0.9044661242040422, 0.9096519270256552, 0.911253756286914, 0.915207590727667, 0.9140686282991879, 0.9183694326204319, 0.9213932075373754, 0.9210905777039498, 0.9227868548703396, 0.9252289820967147, 0.9272736710963455, 0.9269950137273901, 0.9278109787398486, 0.9279494062038575, 0.9329470341915835, 0.9344594623823367, 0.9356442068106312, 0.9365985992870985, 0.9371562745131967, 0.938364630917774, 0.9396202112749169, 0.9408285676794942, 0.9402937834533037, 0.9419678905961609, 0.9438509006436876, 0.9433408098583426, 0.9439439065960686, 0.9450149170011997, 0.9455962042035806, 0.945339716858158, 0.9466196301679586, 0.9484561372392949, 0.948316267822536, 0.9483391588224437, 0.9495714876914912, 0.9504092622392949, 0.9508038165605389, 0.9508503195367294, 0.9524085297272978, 0.952477923703396, 0.9525244266795865, 0.9527562205841639, 0.9537324225959765, 0.9539424069652085, 0.9533382287629198, 0.9538497615010151, 0.9546159791435955, 0.9553378567391103, 0.9548266844892026, 0.955547480620155, 0.9561284073343485, 0.9557792745247323, 0.9557563835248246, 0.9558486685008305, 0.9561044348698781, 0.956918597441399, 0.9571275003460686, 0.9573382056916758, 0.9571976152985419, 0.9570820788344407, 0.9574766331556847, 0.9571750847868217, 0.9570817183462532, 0.9578722689414912, 0.9571979757867294, 0.9576401145487264, 0.9579191324058692, 0.9581752592631044, 0.9578497384297711, 0.9586399285368217, 0.9582675442391103, 0.9584077741440569, 0.9585476435608158, 0.9585465620962532, 0.9591747127630121, 0.958965449370155, 0.9587561859772978, 0.959360724667774, 0.9591979642511074, 0.9593374731796788, 0.9593839761558692, 0.9591049582987264, 0.9591747127630121, 0.9590824277870063, 0.9596629940130121, 0.9592677187153931, 0.9591056792751015, 0.9593839761558692, 0.9590588158107235, 0.959360724667774, 0.9587096830011074, 0.9593142216915835, 0.9590352038344407, 0.9592441067391103, 0.9593603641795865, 0.9599652633582503, 0.959500594084533, 0.9591747127630121, 0.9596393820367294, 0.9596862455011074, 0.9594777030846253, 0.959546736572536, 0.9597094969892026, 0.9594533701319674, 0.959593600036914, 0.9601044117986341, 0.9598261149178663, 0.9597094969892026, 0.9597327484772978, 0.959755278989018, 0.9598253939414912, 0.9596161305486341, 0.9593378336678663, 0.9597563604535806, 0.9602206692391103, 0.9596862455011074, 0.959849005917774, 0.9596401030131044, 0.9598250334533037, 0.9595928790605389, 0.9594304791320598, 0.9595231245962532, 0.9593371126914912, 0.9595696275724437, 0.9597788909653008, 0.9598025029415835, 0.9598718969176817, 0.9598261149178663, 0.959988154358158, 0.9597788909653008, 0.9598253939414912, 0.9592208552510151, 0.9600106848698781, 0.9594995126199704, 0.9594297581556847, 0.9595234850844407, 0.9601509147748246, 0.9596862455011074, 0.9599641818936876, 0.9599877938699704, 0.9592677187153931, 0.9603834296557769, 0.9594079486203396, 0.9599652633582503, 0.9598250334533037, 0.960035017822536, 0.959453730620155, 0.9600346573343485, 0.9596168515250092, 0.9600342968461609, 0.9595002335963455, 0.9598253939414912, 0.9597094969892026, 0.9594072276439645, 0.959848284941399, 0.9600342968461609, 0.9596622730366371, 0.9600117663344407, 0.9596164910368217, 0.9600342968461609, 0.9601505542866371, 0.9602442812153931, 0.9599877938699704, 0.9596629940130121, 0.9595231245962532, 0.9603605386558692, 0.960035017822536], "end": "2016-02-02 07:48:57.993000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0], "moving_var_accuracy_valid": [0.02289364074915323, 0.04620294480150121, 0.06866151554546451, 0.08675492829224217, 0.10360078219403387, 0.11548182178785128, 0.1227214756531257, 0.12643746471934963, 0.12752372252856764, 0.12569402887864378, 0.12261534718774283, 0.11760668494907012, 0.1121970139706782, 0.10599894201093424, 0.09983619786632875, 0.09378585665461517, 0.0877621674581388, 0.08181071747216855, 0.07603088758187314, 0.07044233800207583, 0.06500849245739163, 0.059914746006949626, 0.05517810453229701, 0.05072055315059135, 0.046457269181655575, 0.042531295611773476, 0.03874077032491486, 0.0353787669732643, 0.03229134286369105, 0.029404919585566743, 0.026781251691676243, 0.024307911741372583, 0.02211768903245827, 0.020099329756365636, 0.01826235541299724, 0.016564202600430136, 0.015027188555369405, 0.013601265236797406, 0.01231422828379599, 0.011155107690006131, 0.010089711731218661, 0.009121137014721484, 0.00825221172856084, 0.007452162580743122, 0.006729514759644796, 0.006083163679841632, 0.005503046032253, 0.004973780631266754, 0.00449281993663238, 0.004058651040118021, 0.0036777312664860473, 0.0033221227237345352, 0.0030012330190734644, 0.0027049159533134364, 0.002440301623583756, 0.0022025634205895483, 0.0019894175628194077, 0.0017938890232110922, 0.0016199057857579407, 0.0014619801768891122, 0.001318325650669087, 0.0011867569168605035, 0.0010714174246315951, 0.0009672454215819092, 0.0008731689670839504, 0.0007872969715386828, 0.0007099874224553666, 0.0006399154068254568, 0.0005763420523781209, 0.0005206506565990332, 0.0004691266834920757, 0.00042234795186693467, 0.0003807769189971856, 0.00034303630378629035, 0.0003091802466215837, 0.0002786741520931547, 0.000251096101730143, 0.0002267952678828598, 0.00020460076436992213, 0.0001850612723057129, 0.00016735432356569253, 0.00015183027392496665, 0.00013742833260405472, 0.00012396618572171777, 0.00011187213400672017, 0.00010079009325814653, 9.199040244112745e-05, 8.289317000615558e-05, 7.475661848953089e-05, 6.734097785430576e-05, 6.0746124329200293e-05, 5.468428630655759e-05, 4.934853300747212e-05, 4.441511925672045e-05, 4.026592653822387e-05, 3.624920202790178e-05, 3.2627902040311415e-05, 2.9367719253562632e-05, 2.6431034479662938e-05, 2.3930589385665337e-05, 2.155930446590453e-05, 1.9484864642832913e-05, 1.755800358156296e-05, 1.6012747942237064e-05, 1.477417687364539e-05, 1.3480862133829054e-05, 1.2176472849143551e-05, 1.1098354869679665e-05, 1.001101596302736e-05, 9.069867369012727e-06, 8.17509186301235e-06, 7.376099106363921e-06, 6.845647030790524e-06, 6.249101081343951e-06, 5.669199084670809e-06, 5.106971446079179e-06, 4.622248445129527e-06, 4.18282726247985e-06, 3.858568773556265e-06, 3.473248997929142e-06, 3.143012127918635e-06, 2.8329954550126737e-06, 2.556833640260456e-06, 2.322694205996899e-06, 2.093535071798193e-06, 1.8850357407564487e-06, 1.6968224630134668e-06, 1.5298162985106867e-06, 1.3911866992729202e-06, 1.2520722044879837e-06, 1.2881334804174626e-06, 1.2405984518763623e-06, 1.1676779664787769e-06, 1.1071069911711307e-06, 1.006476655176479e-06, 9.138671937984828e-07, 1.3048518279964164e-06, 1.1757498588984079e-06, 1.125513216759619e-06, 1.0426746436900108e-06, 1.001299834419047e-06, 9.012056007343487e-07, 8.172666775752788e-07, 7.827355835909936e-07, 7.426904399882248e-07, 7.723974412544905e-07, 7.066286173693711e-07, 9.672291853915631e-07, 9.032274254928617e-07, 8.288260306790837e-07, 7.939554808998486e-07, 7.940601724827483e-07, 7.158125718052183e-07, 6.583782603227541e-07, 5.94621124807628e-07, 5.681964226026446e-07, 5.145285695505787e-07, 4.637415587645145e-07, 4.5867411936300045e-07, 4.2560754609666234e-07, 3.976849786754135e-07, 3.9222302259841126e-07, 3.61734751638195e-07, 3.3598232954807966e-07, 3.0270766753998477e-07, 2.752258353174794e-07, 2.78428349795404e-07, 2.548180456338358e-07, 2.4511593425709476e-07, 2.3940488610611976e-07, 2.2534704703150115e-07, 2.1081728845250783e-07, 1.9573170817935448e-07, 1.7617114600817124e-07, 1.6346053074118795e-07, 1.5148769339408934e-07, 1.7755094637373335e-07, 1.6140453459488486e-07, 1.452771281831827e-07, 1.361376395503099e-07, 1.244422463388072e-07, 1.1696037782590092e-07, 1.0968501007727164e-07, 1.0229725179705132e-07, 9.464479975870549e-08, 8.89116468621516e-08, 1.061550540063898e-07, 1.2870608075933443e-07, 1.4328689765620725e-07, 1.6152892809330478e-07, 1.4865936226950804e-07, 2.0739540846936598e-07, 2.128200266577286e-07, 2.0274614864858942e-07, 2.1192231573070307e-07, 2.1078240140581207e-07, 1.899142308405477e-07, 2.451684751275794e-07], "accuracy_test": 0.8636001275510203, "start": "2016-01-31 15:45:16.083000", "learning_rate_per_epoch": [0.006047689821571112, 0.00573175260797143, 0.0054323202930390835, 0.005148530472069979, 0.004879566375166178, 0.0046246531419456005, 0.00438305689021945, 0.0041540819220244884, 0.003937068395316601, 0.003731392091140151, 0.00353646045550704, 0.003351712366566062, 0.003176615573465824, 0.003010665997862816, 0.0028533858712762594, 0.0027043221052736044, 0.0025630455929785967, 0.002429149579256773, 0.0023022484965622425, 0.0021819768007844687, 0.002067988272756338, 0.001959954621270299, 0.0018575646681711078, 0.0017605236498638988, 0.0016685521695762873, 0.001581385382451117, 0.0014987722970545292, 0.0014204749604687095, 0.001346267992630601, 0.0012759376550093293, 0.0012092813849449158, 0.0011461073299869895, 0.001086233532987535, 0.0010294875828549266, 0.0009757061488926411, 0.0009247342823073268, 0.0008764252415858209, 0.0008306399104185402, 0.0007872464484535158, 0.0007461199420504272, 0.0007071418804116547, 0.000670200097374618, 0.0006351881893351674, 0.0006020053406246006, 0.0005705559742636979, 0.0005407495773397386, 0.0005125002935528755, 0.000485726777696982, 0.00046035193372517824, 0.00043630271102301776, 0.00041350984247401357, 0.00039190766983665526, 0.00037143402732908726, 0.0003520299505908042, 0.0003336395602673292, 0.00031620991649106145, 0.00029969081515446305, 0.00028403467149473727, 0.00026919643278233707, 0.0002551333454903215, 0.0002418049261905253, 0.0002291728014824912, 0.0002172005915781483, 0.00020585382299032062, 0.00019509981211740524, 0.0001849076070357114, 0.00017524785653222352, 0.00016609273734502494, 0.0001574158959556371, 0.00014919233217369765, 0.00014139838458504528, 0.0001340115995844826, 0.00012701070227194577, 0.00012037553824484348, 0.00011408700083848089, 0.0001081269874703139, 0.00010247832688037306, 9.712475730339065e-05, 9.205086826113984e-05, 8.724204235477373e-05, 8.268443343695253e-05, 7.836491568014026e-05, 7.427105447277427e-05, 7.039106276351959e-05, 6.671376468148082e-05, 6.322857370832935e-05, 5.9925452660536394e-05, 5.679488822352141e-05, 5.382786912377924e-05, 5.101584974909201e-05, 4.835073195863515e-05, 4.58248432551045e-05, 4.3430907680885866e-05, 4.116203490411863e-05, 3.9011691114865243e-05, 3.697368083521724e-05, 3.504213964333758e-05, 3.321150506963022e-05, 3.147650204482488e-05, 2.9832139261998236e-05, 2.8273678253754042e-05, 2.6796633392223157e-05, 2.539675188018009e-05, 2.4070001018117182e-05, 2.2812560928286985e-05, 2.162081000278704e-05, 2.0491317627602257e-05, 1.94208314496791e-05, 1.8406268281978555e-05, 1.744470682751853e-05, 1.6533378584426828e-05, 1.566965875099413e-05, 1.4851060768705793e-05, 1.4075227227294818e-05, 1.333992349827895e-05, 1.2643033187487163e-05, 1.1982549040112644e-05, 1.1356569302733988e-05, 1.0763291356852278e-05, 1.0201006261922885e-05, 9.668096026871353e-06, 9.163025424641091e-06, 8.684340173203964e-06, 8.230661478592083e-06, 7.800683306413703e-06, 7.39316783437971e-06, 7.006941359577468e-06, 6.6408920247340575e-06, 6.293965270742774e-06, 5.9651624724210706e-06, 5.653536391037051e-06, 5.358190264814766e-06, 5.0782732614607085e-06, 4.812979113921756e-06, 4.56154430139577e-06, 4.32324486610014e-06, 4.097394139535027e-06, 3.8833422877360135e-06, 3.6804726732952986e-06, 3.488201173240668e-06, 3.305974132672418e-06, 3.133266773147625e-06, 2.969581828438095e-06, 2.8144479529146338e-06, 2.667418357304996e-06, 2.5280698991991812e-06, 2.396001036686357e-06, 2.2708316009811824e-06, 2.1522012048080796e-06, 2.0397681055328576e-06, 1.933208523041685e-06, 1.832215843933227e-06, 1.7364991435897537e-06, 1.6457827314297901e-06, 1.55980546878709e-06, 1.4783197457290953e-06, 1.4010909126227489e-06, 1.3278965980134672e-06, 1.2585260265041143e-06, 1.1927794503208133e-06, 1.1304674671919202e-06, 1.0714107929743477e-06, 1.0154392384720268e-06, 9.623917094359058e-07, 9.121154107560869e-07, 8.644656190881506e-07, 8.193051144189667e-07, 7.765038390061818e-07, 7.359384994742868e-07, 6.974923394409416e-07, 6.610546847696241e-07, 6.265205456656986e-07, 5.937905029895774e-07, 5.627703103527892e-07, 5.33370609900885e-07, 5.055068186265999e-07, 4.790986736225022e-07, 4.54070089972447e-07, 4.303490186430281e-07, 4.0786716226648423e-07, 3.8655977618873294e-07, 3.663655263608234e-07, 3.4722623354355164e-07, 3.2908678804233205e-07, 3.11894979176941e-07, 2.9560126790784125e-07, 2.801587584144727e-07, 2.6552299914328614e-07, 2.516518122774869e-07, 2.3850526531532523e-07, 2.2604551475069457e-07, 2.14236678175439e-07, 2.030447490142251e-07, 1.9243749704855873e-07, 1.8238436894080223e-07, 1.7285643139075546e-07, 1.6382624323796335e-07, 1.552677986182971e-07], "accuracy_train_first": 0.5064759900447582, "accuracy_train_last": 0.960035017822536, "batch_size_eval": 1024, "accuracy_train_std": [0.019589673150917364, 0.017115746514006505, 0.019830332835870604, 0.01710534119324816, 0.021675124414028024, 0.01808554864372391, 0.019851468365415438, 0.018948547166417234, 0.0184093122776339, 0.01785598411102484, 0.016731778287911628, 0.01679027736959522, 0.016015183983599558, 0.017760147930854703, 0.01512714128356914, 0.016628420674267372, 0.017234520370776326, 0.014286576703801623, 0.016431911927809437, 0.015176456481103775, 0.014219178414117519, 0.015752574624020254, 0.015216271613594385, 0.013734801131754881, 0.015157304590734173, 0.014949976010555527, 0.014884567924028397, 0.015682352980328506, 0.01396927507553418, 0.01556127586999248, 0.015314851963803364, 0.01497528840108156, 0.01562001681433059, 0.015448599368691609, 0.015255210731726105, 0.013796312793371034, 0.01456846649302471, 0.013136069229717105, 0.014362931638109228, 0.014067248191911595, 0.01328820545199208, 0.01337407841919454, 0.013002202536105414, 0.012520652823121937, 0.012612645799439187, 0.012645121780146864, 0.011538838443296487, 0.01213870882511546, 0.011651792409577767, 0.011571997585907965, 0.011237032320081018, 0.010424596012172256, 0.011014302990145329, 0.012115002612210239, 0.011135071226984804, 0.01045460894764285, 0.010054241245210122, 0.010118636722248319, 0.01018138599100811, 0.009941438381234458, 0.009552398447745749, 0.00969907985759147, 0.009200427681446598, 0.00941953661347242, 0.009033911449432297, 0.008830021427663245, 0.00898718271498055, 0.008949852849511556, 0.008903318871958662, 0.008080185638451837, 0.007850854126012635, 0.007819780890544931, 0.008963528315332111, 0.008192300659435521, 0.007839020640388532, 0.008222996107134088, 0.00794330470716502, 0.008368279696573548, 0.00785054583887657, 0.0074440475656409036, 0.007562197230698144, 0.007931677234970272, 0.0076069789568366354, 0.007243878437667621, 0.0074870071483188055, 0.007578583668168032, 0.007182417944406367, 0.007417203789726607, 0.006379140971975974, 0.007506031727654915, 0.006741216857031668, 0.006950781517863542, 0.006555424811024366, 0.006731202319460845, 0.006896098022369766, 0.006546314640912582, 0.0068378472902387265, 0.006723333240998722, 0.00652433846497672, 0.006955782877302866, 0.006333344405336252, 0.0067611584390536664, 0.0063160058578911734, 0.006312187447045959, 0.007021764034233128, 0.006476269695741756, 0.006609154961662793, 0.006506530947448406, 0.006058463706259077, 0.006529669190660125, 0.006657975009558583, 0.006627995471267233, 0.00637013805911272, 0.006377342974685827, 0.006131495047203019, 0.006208318473246688, 0.006063525203331751, 0.005937285440959305, 0.006572953290885517, 0.006348990725702853, 0.006746489208554682, 0.006594869152285457, 0.006049924257747815, 0.006846920828787942, 0.006348335538938327, 0.006461330701029309, 0.006275451012314903, 0.006290620427900793, 0.006507748207802535, 0.006371677948706353, 0.006669553026475124, 0.00602824015392548, 0.006258140989943873, 0.006344213865847127, 0.006500813462355231, 0.006350596866168586, 0.006532937140506245, 0.006138130286648925, 0.00625409649817275, 0.006343303173004758, 0.006687633433907032, 0.006417386495395372, 0.0064887690330494244, 0.006364603921555074, 0.006123236322175334, 0.0062212669173997955, 0.006370040359524142, 0.0062363401353446885, 0.006097800276523853, 0.006598849922400323, 0.006722971305688033, 0.006510941950718945, 0.006229527093494123, 0.006499428864941486, 0.006524525701391126, 0.006469101675428381, 0.006125110086596784, 0.006361342960355174, 0.006546276236310394, 0.0062810889037456415, 0.006448007297678425, 0.006406762852113761, 0.006125357004039746, 0.006478359535908297, 0.006269658260966528, 0.006348746577593895, 0.006070516187014536, 0.006243821194882002, 0.0064479843021721705, 0.006383216576201299, 0.006494418413664136, 0.006465847805940585, 0.0060474987988931245, 0.005967320063782968, 0.006759711615248079, 0.006529667605984436, 0.006225973006732806, 0.006225812461081974, 0.0061454943365660845, 0.005909294477280089, 0.006211696889778484, 0.006214828982517469, 0.006083160992787369, 0.006073588966734748, 0.006137336928430488, 0.006569976299537214, 0.0063347630091273414, 0.006153501158273463, 0.006318518600339424, 0.0062858915517045606, 0.006320409016121033, 0.006192757587399268, 0.00622908314043205, 0.006532764669057499, 0.006291877200568552, 0.006831066362820478, 0.005801596578374368, 0.006291277692878749], "accuracy_test_std": 0.008349373836041873, "error_valid": [0.4956451783697289, 0.41624505835843373, 0.34771066688629515, 0.3147605068712349, 0.2562049957643072, 0.2383518448795181, 0.22885977503765065, 0.21858527861445776, 0.2073342196912651, 0.2104874576430723, 0.19928787415286142, 0.20766954536897586, 0.1975171192582832, 0.20038503623870485, 0.19093561746987953, 0.18171857351280118, 0.17679458066641573, 0.1733972020896084, 0.16950124717620485, 0.1668965902673193, 0.1677819677146084, 0.16313300075301207, 0.15758836125753017, 0.15532050075301207, 0.1582090079066265, 0.1540983269013554, 0.16288886012801207, 0.1519834219691265, 0.1491243293486446, 0.15108774943524095, 0.1472932746611446, 0.15299086972891573, 0.14422092667545183, 0.14439447242093373, 0.14227809676204817, 0.14400767131024095, 0.14153537980045183, 0.14510630412274095, 0.1428987434111446, 0.14020290144954817, 0.14211484610316272, 0.1421663215361446, 0.1393278190888554, 0.14231927710843373, 0.14153537980045183, 0.1385953972138554, 0.13636724632906627, 0.1370084831513554, 0.13726291886295183, 0.13645990210843373, 0.13147413874246983, 0.13483180769954817, 0.1340787956513554, 0.13767031014683728, 0.13544215926204817, 0.1343538215361446, 0.13299045792545183, 0.13483180769954817, 0.13262424698795183, 0.13287868269954817, 0.13361110457454817, 0.13668345256024095, 0.13213596573795183, 0.13187123493975905, 0.13161679922816272, 0.1324918815888554, 0.1321256706513554, 0.1324918815888554, 0.1332243034638554, 0.13051816641566272, 0.1322477409638554, 0.13323459855045183, 0.13161679922816272, 0.1321256706513554, 0.1316373894013554, 0.13150502400225905, 0.1316373894013554, 0.13025343561746983, 0.13062994164156627, 0.12952101374246983, 0.12941953360316272, 0.12843267601656627, 0.12878859186746983, 0.12967396931475905, 0.12942982868975905, 0.1299989999058735, 0.12720167780496983, 0.12953130882906627, 0.12918568806475905, 0.12954160391566272, 0.12903273249246983, 0.12977544945406627, 0.1289003670933735, 0.12986663450677716, 0.12817824030496983, 0.13013136530496983, 0.1296327889683735, 0.12964308405496983, 0.12976515436746983, 0.1285341561558735, 0.12917539297816272, 0.12866652155496983, 0.12903273249246983, 0.12794439476656627, 0.1273134530308735, 0.12768995905496983, 0.12967396931475905, 0.1278017342808735, 0.12842238092996983, 0.12805616999246983, 0.12842238092996983, 0.12830031061746983, 0.1271913827183735, 0.12756788874246983, 0.12916509789156627, 0.12830031061746983, 0.12904302757906627, 0.12805616999246983, 0.12953130882906627, 0.1285341561558735, 0.1281679452183735, 0.1287782967808735, 0.12830031061746983, 0.12904302757906627, 0.12878859186746983, 0.12852386106927716, 0.12855474632906627, 0.1287782967808735, 0.1290224374058735, 0.1286562264683735, 0.12732374811746983, 0.12757818382906627, 0.1276796639683735, 0.12756788874246983, 0.12794439476656627, 0.12854445124246983, 0.12596038450677716, 0.1281679452183735, 0.1271913827183735, 0.12854445124246983, 0.1271913827183735, 0.1279238045933735, 0.1276796639683735, 0.1271913827183735, 0.1271913827183735, 0.1267031014683735, 0.1273134530308735, 0.12571624388177716, 0.1280458749058735, 0.1279238045933735, 0.12681487669427716, 0.1284120858433735, 0.1276796639683735, 0.12718108763177716, 0.12768995905496983, 0.1269472420933735, 0.1276796639683735, 0.12742522825677716, 0.1268251717808735, 0.12781202936746983, 0.1270693124058735, 0.12681487669427716, 0.12705901731927716, 0.1276796639683735, 0.1273134530308735, 0.1271913827183735, 0.12793409967996983, 0.1271913827183735, 0.12696783226656627, 0.1278017342808735, 0.12705901731927716, 0.12705901731927716, 0.1270693124058735, 0.1273134530308735, 0.1270693124058735, 0.12705901731927716, 0.12793409967996983, 0.1271913827183735, 0.12732374811746983, 0.1275575936558735, 0.1271913827183735, 0.1275575936558735, 0.12756788874246983, 0.12756788874246983, 0.1275575936558735, 0.12720167780496983, 0.1279238045933735, 0.1280458749058735, 0.1269472420933735, 0.1280458749058735, 0.1273134530308735, 0.1265810311558735, 0.12793409967996983, 0.1278017342808735, 0.12805616999246983, 0.1270693124058735, 0.12744581842996983, 0.1265810311558735], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.052240979274630533, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 16, "valid_ratio": 0.15, "learning_rate": 0.006381041782571219, "optimization": "adam", "nb_data_augmentation": 2, "learning_rate_decay_method": "exp", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 6.55358637090104e-06, "rotation_range": [0, 0], "momentum": 0.9518399332439857}, "accuracy_valid_max": 0.8742837561182228, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8734189688441265, "accuracy_valid_std": [0.010141421975127553, 0.01678125384426147, 0.007424176739482246, 0.01218786690247079, 0.010506983124032945, 0.010828764520435196, 0.009615728754386663, 0.010350952058556965, 0.011929941039088607, 0.012995144661777982, 0.013641970136561085, 0.015034068874973935, 0.014266776257170438, 0.01885352575404685, 0.013037427646955486, 0.016379801691101394, 0.01922247859009939, 0.013630686957211462, 0.015685747348033414, 0.017691679913721517, 0.014449620401823383, 0.013223015388139142, 0.017009646785324826, 0.014231120782347196, 0.014446247153673953, 0.01708867263428048, 0.014026618291360989, 0.012775541324616023, 0.016025651993853862, 0.014956145287884439, 0.01378627098364931, 0.013742234410133641, 0.014203266489366395, 0.014362041454360095, 0.014126884972198361, 0.013330720934985923, 0.014100152685652562, 0.012006404860864506, 0.01552545509372678, 0.015185995553099617, 0.01894368275433829, 0.015337891773044003, 0.017274868323080495, 0.013453970798864007, 0.014744861984048445, 0.018089148688890437, 0.01699791472847389, 0.018824925189112152, 0.016568881190741437, 0.014966320265649629, 0.016623693858736133, 0.01602610981712129, 0.014693037094068525, 0.01502151846837614, 0.015684759138141324, 0.013529232340800782, 0.01637667696859685, 0.0154040862236, 0.013552578797740022, 0.015481930298452047, 0.012512551629472585, 0.012517470611250393, 0.013116571939344481, 0.014597071648708769, 0.016544974871717126, 0.013204222583360157, 0.01295773444222538, 0.01375257961021187, 0.014393973461822678, 0.014513955709542696, 0.01309628699507838, 0.013004050908988304, 0.014395048104692642, 0.012893174621539993, 0.013745116979774685, 0.014391837537153325, 0.014113122960176373, 0.015449525362764035, 0.014716450445839005, 0.016573940464876716, 0.015993306717533576, 0.014921749709389883, 0.015691110396810478, 0.01303902791458954, 0.014504682986828497, 0.017288530850834864, 0.014135339821417079, 0.01617751663358365, 0.015067606296911708, 0.014784165724515514, 0.014975789856391523, 0.015387986276029081, 0.016010545195693004, 0.018235317645352467, 0.01631925236943383, 0.015851826151452267, 0.016878886214728273, 0.015112793173719335, 0.01593957781925989, 0.01600249758486253, 0.014473605596080102, 0.015617373104726127, 0.015492253162258302, 0.014467948594220112, 0.015184030374859561, 0.017215198158417183, 0.015003031051421116, 0.016432248729007507, 0.014876637278199023, 0.01588761441172926, 0.01502018212708917, 0.015995974703281103, 0.016089762260445666, 0.015442574707212775, 0.014257123166689905, 0.01425414679055059, 0.015733419893282544, 0.015515598000715626, 0.015546217668145023, 0.016202383484823252, 0.016700871243566836, 0.01603607449073773, 0.016313283103329513, 0.015287649392579889, 0.015546092514904322, 0.01624204146421612, 0.014897142908958317, 0.0171982410404943, 0.016634568857407116, 0.015041088369488255, 0.015357495355769489, 0.014235226976508409, 0.01550127560730865, 0.014844301990195732, 0.015096959485105524, 0.016859444333558092, 0.015770716464073558, 0.014825359782379996, 0.014108061037689388, 0.015083011682358803, 0.015127448847405325, 0.015625835013756667, 0.015424181004602045, 0.016303207917204725, 0.016185792879126495, 0.01504032443914509, 0.015394547088511568, 0.01633302806421365, 0.015109547295077925, 0.016209977453626075, 0.015985092244683426, 0.016559384223754914, 0.015760576675464488, 0.016254002517558368, 0.01597237094012658, 0.016084115243007124, 0.016141720003685325, 0.016804163588469413, 0.016740947020444544, 0.01535347340322146, 0.015093379086605775, 0.01581263882948391, 0.01649187543731153, 0.015447349785643855, 0.01575429316974181, 0.0157073582140181, 0.015186078165924329, 0.014638858401722089, 0.01475448785704074, 0.01554499878992864, 0.01639763744908025, 0.01699032830691946, 0.015382833928725435, 0.015556312873606419, 0.01696736792155997, 0.015775149865118486, 0.01640129145210569, 0.01597824054491414, 0.015083368766092233, 0.015792172703167937, 0.01576795645276503, 0.015510358837239216, 0.015580906520856731, 0.013786737903878151, 0.01583739972853157, 0.014177444225333678, 0.01481118454582824, 0.01565208356734994, 0.015105667314797257, 0.015529746383430767, 0.01442717685443382, 0.016238421825435233, 0.014675087086592148, 0.01575825839341793, 0.015865088589682774, 0.016190811210146747, 0.014221443946307966, 0.015509988433519858], "accuracy_valid": [0.5043548216302711, 0.5837549416415663, 0.6522893331137049, 0.6852394931287651, 0.7437950042356928, 0.7616481551204819, 0.7711402249623494, 0.7814147213855422, 0.7926657803087349, 0.7895125423569277, 0.8007121258471386, 0.7923304546310241, 0.8024828807417168, 0.7996149637612951, 0.8090643825301205, 0.8182814264871988, 0.8232054193335843, 0.8266027979103916, 0.8304987528237951, 0.8331034097326807, 0.8322180322853916, 0.8368669992469879, 0.8424116387424698, 0.8446794992469879, 0.8417909920933735, 0.8459016730986446, 0.8371111398719879, 0.8480165780308735, 0.8508756706513554, 0.848912250564759, 0.8527067253388554, 0.8470091302710843, 0.8557790733245482, 0.8556055275790663, 0.8577219032379518, 0.855992328689759, 0.8584646201995482, 0.854893695877259, 0.8571012565888554, 0.8597970985504518, 0.8578851538968373, 0.8578336784638554, 0.8606721809111446, 0.8576807228915663, 0.8584646201995482, 0.8614046027861446, 0.8636327536709337, 0.8629915168486446, 0.8627370811370482, 0.8635400978915663, 0.8685258612575302, 0.8651681923004518, 0.8659212043486446, 0.8623296898531627, 0.8645578407379518, 0.8656461784638554, 0.8670095420745482, 0.8651681923004518, 0.8673757530120482, 0.8671213173004518, 0.8663888954254518, 0.863316547439759, 0.8678640342620482, 0.868128765060241, 0.8683832007718373, 0.8675081184111446, 0.8678743293486446, 0.8675081184111446, 0.8667756965361446, 0.8694818335843373, 0.8677522590361446, 0.8667654014495482, 0.8683832007718373, 0.8678743293486446, 0.8683626105986446, 0.868494975997741, 0.8683626105986446, 0.8697465643825302, 0.8693700583584337, 0.8704789862575302, 0.8705804663968373, 0.8715673239834337, 0.8712114081325302, 0.870326030685241, 0.870570171310241, 0.8700010000941265, 0.8727983221950302, 0.8704686911709337, 0.870814311935241, 0.8704583960843373, 0.8709672675075302, 0.8702245505459337, 0.8710996329066265, 0.8701333654932228, 0.8718217596950302, 0.8698686346950302, 0.8703672110316265, 0.8703569159450302, 0.8702348456325302, 0.8714658438441265, 0.8708246070218373, 0.8713334784450302, 0.8709672675075302, 0.8720556052334337, 0.8726865469691265, 0.8723100409450302, 0.870326030685241, 0.8721982657191265, 0.8715776190700302, 0.8719438300075302, 0.8715776190700302, 0.8716996893825302, 0.8728086172816265, 0.8724321112575302, 0.8708349021084337, 0.8716996893825302, 0.8709569724209337, 0.8719438300075302, 0.8704686911709337, 0.8714658438441265, 0.8718320547816265, 0.8712217032191265, 0.8716996893825302, 0.8709569724209337, 0.8712114081325302, 0.8714761389307228, 0.8714452536709337, 0.8712217032191265, 0.8709775625941265, 0.8713437735316265, 0.8726762518825302, 0.8724218161709337, 0.8723203360316265, 0.8724321112575302, 0.8720556052334337, 0.8714555487575302, 0.8740396154932228, 0.8718320547816265, 0.8728086172816265, 0.8714555487575302, 0.8728086172816265, 0.8720761954066265, 0.8723203360316265, 0.8728086172816265, 0.8728086172816265, 0.8732968985316265, 0.8726865469691265, 0.8742837561182228, 0.8719541250941265, 0.8720761954066265, 0.8731851233057228, 0.8715879141566265, 0.8723203360316265, 0.8728189123682228, 0.8723100409450302, 0.8730527579066265, 0.8723203360316265, 0.8725747717432228, 0.8731748282191265, 0.8721879706325302, 0.8729306875941265, 0.8731851233057228, 0.8729409826807228, 0.8723203360316265, 0.8726865469691265, 0.8728086172816265, 0.8720659003200302, 0.8728086172816265, 0.8730321677334337, 0.8721982657191265, 0.8729409826807228, 0.8729409826807228, 0.8729306875941265, 0.8726865469691265, 0.8729306875941265, 0.8729409826807228, 0.8720659003200302, 0.8728086172816265, 0.8726762518825302, 0.8724424063441265, 0.8728086172816265, 0.8724424063441265, 0.8724321112575302, 0.8724321112575302, 0.8724424063441265, 0.8727983221950302, 0.8720761954066265, 0.8719541250941265, 0.8730527579066265, 0.8719541250941265, 0.8726865469691265, 0.8734189688441265, 0.8720659003200302, 0.8721982657191265, 0.8719438300075302, 0.8729306875941265, 0.8725541815700302, 0.8734189688441265], "seed": 566966800, "model": "residualv3", "loss_std": [0.2834795415401459, 0.2678648829460144, 0.2690330743789673, 0.264775812625885, 0.2622690200805664, 0.25870901346206665, 0.25627171993255615, 0.2494029700756073, 0.247719869017601, 0.24162638187408447, 0.24272361397743225, 0.24015745520591736, 0.23514723777770996, 0.2344321459531784, 0.23100033402442932, 0.22738224267959595, 0.22450385987758636, 0.22261708974838257, 0.2178807407617569, 0.2156120389699936, 0.21509313583374023, 0.21117517352104187, 0.2092384546995163, 0.2079039216041565, 0.207746684551239, 0.20279669761657715, 0.20016464591026306, 0.19981060922145844, 0.19671425223350525, 0.1967226266860962, 0.19321879744529724, 0.1917831301689148, 0.18912456929683685, 0.18730904161930084, 0.18512359261512756, 0.18271981179714203, 0.1812673807144165, 0.18077053129673004, 0.17937615513801575, 0.17762117087841034, 0.1769248992204666, 0.17419806122779846, 0.17263942956924438, 0.16948480904102325, 0.17110078036785126, 0.1689598113298416, 0.16420978307724, 0.16677714884281158, 0.1638069748878479, 0.16169996559619904, 0.16100139915943146, 0.1587774008512497, 0.1594291776418686, 0.15776365995407104, 0.15761929750442505, 0.15700818598270416, 0.15650062263011932, 0.15301349759101868, 0.15240471065044403, 0.15302720665931702, 0.15207168459892273, 0.15095484256744385, 0.15063558518886566, 0.15135487914085388, 0.14882969856262207, 0.14757797122001648, 0.14971892535686493, 0.1447780728340149, 0.14538457989692688, 0.14557300508022308, 0.14549683034420013, 0.1428440362215042, 0.1442577838897705, 0.14345528185367584, 0.1426749974489212, 0.14089444279670715, 0.1414964348077774, 0.14060895144939423, 0.14164356887340546, 0.14193327724933624, 0.13916577398777008, 0.1385064423084259, 0.1403627246618271, 0.13992640376091003, 0.1360759735107422, 0.14071786403656006, 0.13707152009010315, 0.13661058247089386, 0.13967595994472504, 0.13671238720417023, 0.13850714266300201, 0.13761183619499207, 0.13434119522571564, 0.13548222184181213, 0.13758566975593567, 0.13631761074066162, 0.13783983886241913, 0.13733190298080444, 0.13367606699466705, 0.13418656587600708, 0.13666458427906036, 0.13497129082679749, 0.13723225891590118, 0.1364111602306366, 0.1344904899597168, 0.13545091450214386, 0.1367567628622055, 0.13501699268817902, 0.13606208562850952, 0.13450226187705994, 0.13500094413757324, 0.13434922695159912, 0.13547362387180328, 0.13556700944900513, 0.13435542583465576, 0.1351039558649063, 0.13397859036922455, 0.1356326788663864, 0.133835569024086, 0.13522730767726898, 0.13710269331932068, 0.13654787838459015, 0.1331661343574524, 0.13441674411296844, 0.13331975042819977, 0.13605636358261108, 0.13373006880283356, 0.13375647366046906, 0.1329396516084671, 0.1346946358680725, 0.1330006718635559, 0.13549509644508362, 0.1344502717256546, 0.1318226009607315, 0.13176220655441284, 0.13319292664527893, 0.13306042551994324, 0.13419334590435028, 0.13259045779705048, 0.13528434932231903, 0.13257955014705658, 0.1349273920059204, 0.13117168843746185, 0.13283061981201172, 0.133459210395813, 0.13345305621623993, 0.1351805329322815, 0.13354302942752838, 0.13529935479164124, 0.1335975080728531, 0.13543371856212616, 0.13355819880962372, 0.13503773510456085, 0.1358402520418167, 0.13454681634902954, 0.13645878434181213, 0.13420331478118896, 0.13178281486034393, 0.13211484253406525, 0.13214632868766785, 0.13268592953681946, 0.13267487287521362, 0.13261504471302032, 0.13327184319496155, 0.13365526497364044, 0.1364198923110962, 0.1332232654094696, 0.13220611214637756, 0.13360971212387085, 0.13300876319408417, 0.13237211108207703, 0.13337813317775726, 0.13354910910129547, 0.13420787453651428, 0.13359835743904114, 0.13355332612991333, 0.13264808058738708, 0.13334989547729492, 0.133080393075943, 0.1339663714170456, 0.13165855407714844, 0.13198921084403992, 0.13267068564891815, 0.13500119745731354, 0.13523514568805695, 0.1324864625930786, 0.1328158974647522, 0.13506771624088287, 0.13341344892978668, 0.13461743295192719, 0.13308782875537872, 0.1330854296684265, 0.13500818610191345, 0.13382340967655182, 0.1324230581521988, 0.1321585774421692, 0.1343926340341568, 0.13476435840129852]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:32 2016", "state": "available"}], "summary": "7fee0e009925b8bda675a2a5ebd7216e"}