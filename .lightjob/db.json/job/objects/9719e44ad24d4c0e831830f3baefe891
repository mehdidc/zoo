{"content": {"hp_model": {"f0": 64, "f1": 64, "f2": 64, "f3": 32, "nonlin": "leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.485149621963501, 1.0949383974075317, 0.8763197064399719, 0.745143711566925, 0.6587974429130554, 0.6017856001853943, 0.5497591495513916, 0.5096578001976013, 0.47640538215637207, 0.4435453712940216, 0.41969284415245056, 0.3948546051979065, 0.37164950370788574, 0.35206326842308044, 0.33404022455215454, 0.316964328289032, 0.30110102891921997, 0.2864890694618225, 0.2738388776779175, 0.2620571553707123, 0.25102755427360535, 0.23755553364753723, 0.22780117392539978, 0.22130939364433289, 0.2107536643743515, 0.20518897473812103, 0.19398392736911774, 0.1888967603445053, 0.18192459642887115, 0.17571578919887543, 0.16882716119289398, 0.1659954935312271, 0.15815886855125427, 0.15462271869182587, 0.14894507825374603, 0.14376670122146606, 0.13932350277900696, 0.13763275742530823, 0.13468381762504578, 0.12663708627223969, 0.1260344684123993, 0.12426844984292984, 0.11677831411361694, 0.11834124475717545, 0.11260419338941574, 0.10822870582342148, 0.10929900407791138, 0.10445854812860489, 0.10564135760068893, 0.09947263449430466, 0.097941555082798, 0.09511560201644897, 0.09554027020931244, 0.09569215774536133, 0.08877149969339371, 0.08989198505878448, 0.08393591642379761, 0.08469025790691376, 0.08379895240068436, 0.08116960525512695, 0.08150650560855865, 0.07931799441576004, 0.07977946847677231, 0.07515137642621994, 0.07489978522062302, 0.0742662101984024, 0.07147683203220367, 0.07479789853096008, 0.06845259666442871, 0.06940164417028427, 0.06725764274597168, 0.06501300632953644, 0.06657061725854874, 0.06672511249780655, 0.061588168144226074, 0.06386807560920715, 0.06294149160385132, 0.06152172386646271, 0.06071789935231209, 0.05998542532324791, 0.05647949129343033, 0.060430001467466354, 0.057379573583602905, 0.05632508918642998, 0.053942009806632996, 0.05518219992518425, 0.05614984408020973, 0.05454959720373154, 0.054311513900756836, 0.05336415022611618, 0.04847580939531326, 0.052754830569028854, 0.05009828135371208, 0.05316412076354027, 0.05288691818714142, 0.048309240490198135, 0.0473947636783123, 0.04822123423218727, 0.048670798540115356, 0.04710600525140762, 0.04928752779960632, 0.04646407440304756, 0.04791143164038658, 0.0437544621527195, 0.04636966437101364, 0.04597361013293266, 0.044652923941612244, 0.04614740610122681, 0.04119747504591942, 0.04362118989229202, 0.04703841730952263, 0.03788154572248459, 0.042583730071783066, 0.04176491126418114, 0.04158264398574829, 0.04144328832626343, 0.04159294068813324, 0.039345987141132355, 0.03942035883665085, 0.04462699219584465, 0.03758539631962776, 0.04044026508927345, 0.039538852870464325, 0.036551252007484436, 0.04120386764407158, 0.03960982710123062, 0.04092995822429657, 0.036611974239349365, 0.035514164716005325, 0.03854123875498772, 0.03983546048402786, 0.03875325620174408, 0.03403988108038902, 0.040837738662958145, 0.03602507710456848, 0.03632957860827446, 0.03860227018594742, 0.032437000423669815, 0.03811055049300194, 0.03207455947995186, 0.034016892313957214, 0.03992949053645134, 0.03527434542775154, 0.03474394604563713, 0.03190724551677704, 0.03995983675122261, 0.03208426386117935, 0.03437012806534767, 0.03450073301792145, 0.034700896590948105, 0.02962956205010414, 0.03564397618174553, 0.033327288925647736, 0.03157774358987808, 0.03455386683344841, 0.03360828757286072, 0.03131943196058273, 0.03667282313108444, 0.032603535801172256, 0.03168279305100441, 0.03526418283581734, 0.029808765277266502, 0.03576527535915375, 0.03096935898065567, 0.028677992522716522, 0.02959054335951805, 0.036290790885686874, 0.03130416199564934, 0.032623931765556335, 0.029919955879449844, 0.031418148428201675, 0.03232474997639656, 0.033481039106845856, 0.029656317085027695, 0.031698957085609436, 0.028456861153244972, 0.03240492567420006, 0.03353584557771683, 0.031205760315060616, 0.02713300660252571, 0.0338873565196991, 0.02937982976436615, 0.030597485601902008, 0.029301315546035767, 0.031070195138454437, 0.03148988261818886, 0.027781719341874123, 0.032168492674827576, 0.02674988843500614], "moving_avg_accuracy_train": [0.05018117647058823, 0.1128995294117647, 0.17519075294117648, 0.23473285411764705, 0.29298192164705883, 0.34716372948235297, 0.397063827122353, 0.44316215029247064, 0.48404122938087063, 0.5219547535016071, 0.5564345722690934, 0.5886122915127723, 0.6188240035379656, 0.6455580737724044, 0.6699646193363404, 0.6929775691674123, 0.7141245181330239, 0.7332932427903097, 0.75166980086422, 0.7685451737189745, 0.7841471269353123, 0.7993677083594282, 0.8127697610528971, 0.8249139614181956, 0.8364390358646113, 0.8451174852193266, 0.8539327955209234, 0.8624312806747134, 0.869352858489595, 0.876970513817106, 0.8832734624353954, 0.8898355279565617, 0.896129622219729, 0.9010931305859914, 0.9062402881156275, 0.9099880240099472, 0.9136856921971878, 0.9180182994480571, 0.9220188224444279, 0.9262804696117498, 0.9302900697093983, 0.933178709797282, 0.936126721170495, 0.9386952255240337, 0.9419362912069245, 0.9451520738509379, 0.9469356899952559, 0.948995062172201, 0.9501449677196867, 0.9530504709477181, 0.9549901297352992, 0.9561734697029457, 0.9580643580267688, 0.9595685104593861, 0.9607951888252122, 0.9622380228838674, 0.9636471617719513, 0.9642330338300503, 0.9660238480941041, 0.9666049926964584, 0.967826846367989, 0.9690441617311901, 0.969949157322777, 0.9713871827669699, 0.9726014056667435, 0.9739647945118338, 0.974650668001827, 0.9754256012016443, 0.9756312763755975, 0.9764469722674496, 0.9766516868054105, 0.977148871066046, 0.9776786898417943, 0.9786261149752619, 0.9771093858306769, 0.9773655060711386, 0.9781889554640247, 0.9790830010940929, 0.9796476421611541, 0.9804687602979798, 0.9805324725034759, 0.9807404017237165, 0.9806381262572271, 0.9810731371609162, 0.981972882268354, 0.9827379469826951, 0.9829488581667786, 0.9834633841148065, 0.9840393986445023, 0.9841342823094639, 0.984238501137341, 0.9848217098471364, 0.9851348329800698, 0.9854331143879452, 0.9860027441256214, 0.9862801167718828, 0.9867038698005768, 0.9871440710558131, 0.9874884874796436, 0.9877796387316793, 0.9875075572114524, 0.9879450367844248, 0.9880846507530412, 0.9881420680306783, 0.9879537435805515, 0.9885113103989669, 0.9887590028884821, 0.9885419261290456, 0.9891394982220234, 0.9893173131057034, 0.9895479347363095, 0.990080200086208, 0.9900533565481755, 0.9903444914815933, 0.9904935717451987, 0.9907924498647964, 0.9910755578194933, 0.9905562373316616, 0.9906206135984955, 0.9906456110621754, 0.9905081087794873, 0.9905608273133032, 0.9908412151702082, 0.9910653289473049, 0.9913187960525744, 0.9908739752708464, 0.9910054012731735, 0.9910554493811503, 0.9913499044430353, 0.9913043257634376, 0.9917009520106232, 0.9920296803389727, 0.9923278887756636, 0.9925562763686854, 0.9926135899082874, 0.9927922309174586, 0.992494184296301, 0.992795354101965, 0.9930805245741213, 0.9931512956461209, 0.99318204843445, 0.9933626671204168, 0.9936264004083751, 0.9934872897793023, 0.9934844431543133, 0.9934912929565289, 0.9936645166020526, 0.9938580649418474, 0.993434611388839, 0.9932464443676021, 0.9934488587543714, 0.9934686787612872, 0.993639457943982, 0.9936237474437014, 0.9936637256405078, 0.9939161766058687, 0.9941951471805759, 0.9937026912860477, 0.99396771627509, 0.9939591799416987, 0.9938973795945877, 0.9939758769292466, 0.9941547598245573, 0.9940639897244544, 0.994257590752009, 0.994521243441514, 0.9946173543914802, 0.9946732660111557, 0.9944788805865107, 0.9944333454690362, 0.9945358932750737, 0.9947340686534487, 0.994881838258692, 0.9948736544328228, 0.9948592301660111, 0.9950768365611746, 0.9951315058462337, 0.9952042376145515, 0.9952085197354493, 0.9952053148207279, 0.9951012539268904, 0.9952287755930249, 0.9937976627396048, 0.9940743670538795, 0.9941751656426092, 0.9942023549607012, 0.9942762371116899, 0.9945615545769915, 0.9945336344134099], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.04898666666666666, 0.10924799999999998, 0.16956319999999997, 0.22707354666666663, 0.283046192, 0.33542157279999996, 0.3831727488533333, 0.4271488073013333, 0.4659405932378667, 0.5014132005807467, 0.533711880522672, 0.5633540258037382, 0.590885289890031, 0.6149167609010279, 0.6372250848109251, 0.6576092429964993, 0.6766883186968494, 0.6933928201604977, 0.7094668714777813, 0.7242801843300032, 0.7376388325636696, 0.7503549493073026, 0.761586121043239, 0.7717875089389151, 0.7814220913783569, 0.7887198822405213, 0.7959012273498025, 0.8024711046148223, 0.8080373274866733, 0.8140202614046727, 0.8190315685975388, 0.8241150784044515, 0.8297169038973397, 0.8337452135076057, 0.8381706921568451, 0.8409669562744939, 0.8432569273137112, 0.8467979012490067, 0.8499314444574393, 0.8528983000116954, 0.8562351366771925, 0.8586916230094732, 0.8600891273751925, 0.8616935479710066, 0.8642708598405726, 0.867230440523182, 0.8684407298041971, 0.8702499901571106, 0.8709583244747329, 0.8730758253605928, 0.8746615761578668, 0.8757820852087468, 0.8773505433545388, 0.878655489019085, 0.8792966067838431, 0.8805002794387922, 0.8813435848282463, 0.8819692263454216, 0.8838123037108795, 0.8839644066731249, 0.8848879660058124, 0.8857591694052313, 0.8862232524647081, 0.8879209272182372, 0.8887555011630801, 0.8902132843801054, 0.8909652892754283, 0.8917220936812188, 0.8917898843130969, 0.8923575625484539, 0.8925484729602752, 0.8925336256642477, 0.8929202630978228, 0.8936682367880405, 0.8918880797759032, 0.8916459384649795, 0.8922946779518149, 0.8931318768233001, 0.8937520224743034, 0.8945501535602064, 0.8943084715375191, 0.8948242910504338, 0.8945818619453905, 0.8947636757508514, 0.8960739748424329, 0.8971999106915229, 0.8975599196223706, 0.8980705943268001, 0.8987568682274535, 0.8988945147380414, 0.8991917299309039, 0.8995925569378135, 0.8995533012440321, 0.8999579711196289, 0.900242174007666, 0.9006579566068994, 0.9009388276128761, 0.9012449448515885, 0.9017071170330964, 0.9020164053297868, 0.9017480981301415, 0.902479954983794, 0.9024052928187479, 0.9020314302035398, 0.9018016205165192, 0.9024347917982006, 0.9024046459517139, 0.9019108480232092, 0.9026130965542216, 0.9028851202321329, 0.9034099415422528, 0.9039089473880275, 0.9039180526492248, 0.9043662473843024, 0.9048629559792055, 0.9054033270479516, 0.9057563276764897, 0.9048606949088408, 0.9049212920846234, 0.9049224962094944, 0.9049369132552116, 0.9046965552630238, 0.9052002330700548, 0.9052002097630493, 0.9055201887867443, 0.9049148365747366, 0.9051166862505963, 0.9053383509588699, 0.9057911825296496, 0.9058387309433513, 0.9060681911823495, 0.9067947053974478, 0.9072885681910363, 0.907213044705266, 0.9070250735680727, 0.9073092328779321, 0.9070583095901389, 0.907352478631125, 0.9083772307680125, 0.9084861743578778, 0.9084508902554234, 0.909005801229881, 0.9093718877735595, 0.9091013656628703, 0.9093512290965833, 0.9094694395202583, 0.9097224955682324, 0.9094569126780757, 0.9088312214102681, 0.9078814326025746, 0.9078932893423172, 0.907930627074752, 0.9084042310339434, 0.9087371412638824, 0.9090500938041608, 0.909478417757078, 0.9096772426480368, 0.9088961850498998, 0.9091932332115764, 0.9093272432237521, 0.9094478522347103, 0.9094364003445725, 0.909359426976782, 0.9087834842791038, 0.9090918025178601, 0.9093426222660742, 0.9092750267061335, 0.9091341907021868, 0.9087941049653014, 0.9090880278021046, 0.9092192250218942, 0.909950635853038, 0.9100089056010676, 0.9096346817076275, 0.9090712135368648, 0.9096574255165116, 0.9096383496315271, 0.9098345146683744, 0.9099310632015369, 0.9099646235480499, 0.9101814945265783, 0.9101366784072538, 0.908469677233195, 0.9089427095098755, 0.9089284385588879, 0.9089289280363324, 0.9088893685660325, 0.9094270983760959, 0.9095643885384863], "moving_var_accuracy_train": [0.022663354247750865, 0.05579934498386159, 0.0851411792445957, 0.10853441763271768, 0.12821756068185966, 0.14181681931638035, 0.15004531508507613, 0.15416628216843795, 0.15378954591563512, 0.1513475091253546, 0.14691247933296747, 0.14153988194119593, 0.13560062163851527, 0.12847295407636286, 0.12098677386600695, 0.11365445921875311, 0.10631375435186548, 0.09898933896156047, 0.09212968604519857, 0.08547972132156158, 0.07912253768688857, 0.07329527880819296, 0.06758228607496033, 0.06215139189007696, 0.057131698770028085, 0.052096368241846505, 0.047586118679082794, 0.04347752506035722, 0.03956094670934865, 0.03612711009261261, 0.03287194353491451, 0.029972295516559777, 0.027331606568246524, 0.024820173649139483, 0.022576595359939545, 0.020445345542947736, 0.018523865738859342, 0.01684042253528599, 0.015300417939957811, 0.013933830875170715, 0.012685139824141213, 0.011491724015743051, 0.010420768553678083, 0.009438066629837602, 0.00858880052770115, 0.007822991797052878, 0.007069324196300036, 0.0064005609005386085, 0.0057724053553979944, 0.005271142360931099, 0.004777888610748155, 0.004312702390984607, 0.003913611279764681, 0.003542612422653148, 0.0032018938387065048, 0.0029004403859231947, 0.002628267398984067, 0.0023685298737018104, 0.002160540027886676, 0.001947525586537618, 0.0017662093654355524, 0.0016029251391333661, 0.0014500037784071545, 0.0013236146551697545, 0.0012045222249057906, 0.001100799464701463, 0.0009949533202297945, 0.0009008626813844268, 0.0008111571337406103, 0.0007360296584584075, 0.0006628038649910397, 0.000598748208193148, 0.0005413997587900525, 0.0004953383123627827, 0.00046650868680880647, 0.0004204481963260934, 0.00038450599681728507, 0.00035324925543335164, 0.00032079370570152563, 0.00029478245008299117, 0.00026534073828085467, 0.00023919577549843821, 0.0002153703403880048, 0.0001955364167261596, 0.00018326864637876791, 0.0001702096978950591, 0.00015358907985369645, 0.00014061280442907408, 0.000129537658631953, 0.00011666491895764649, 0.00010509618113863862, 9.764775461740477e-05, 8.876539402306669e-05, 8.068960080531706e-05, 7.554094306718947e-05, 6.867926902451694e-05, 6.342744178601187e-05, 5.8828691913416346e-05, 5.401342677911214e-05, 4.9375005565258175e-05, 4.5103760191572436e-05, 4.231587956332823e-05, 3.825972014909066e-05, 3.446341882812286e-05, 3.133627183195012e-05, 3.100057146173634e-05, 2.8452678439822247e-05, 2.6031511471226848e-05, 2.6642191980856885e-05, 2.4262535978494445e-05, 2.2314959409175874e-05, 2.263322109258098e-05, 2.037638416312983e-05, 1.9101581691922232e-05, 1.7391447847699836e-05, 1.6456256236298604e-05, 1.553198163878231e-05, 1.6406027396639768e-05, 1.4802723390558936e-05, 1.3328074910216878e-05, 1.21654293188952e-05, 1.0973899581275038e-05, 1.0584065775845405e-05, 9.977702064022326e-06, 9.55814201870344e-06, 1.0383117567547301e-05, 9.500260957581763e-06, 8.572778179832068e-06, 8.495834413075985e-06, 7.66494771607317e-06, 8.314264364075014e-06, 8.45539875240226e-06, 8.410213322585109e-06, 8.038640024143536e-06, 7.264339598124631e-06, 6.825119129731695e-06, 6.9420933122097985e-06, 7.064213247582133e-06, 7.089691706533328e-06, 6.425799437567791e-06, 5.79173109972113e-06, 5.506165977232252e-06, 5.5815466041047855e-06, 5.197557847783587e-06, 4.677874992469684e-06, 4.210509771336268e-06, 4.05951667651894e-06, 3.990713647402762e-06, 5.205458486660399e-06, 5.003574088924654e-06, 4.871960935772539e-06, 4.388300336262568e-06, 4.211960065813488e-06, 3.7929854376037256e-06, 3.4280711998223263e-06, 3.6588474890453317e-06, 3.993383974113259e-06, 5.77666084920185e-06, 5.831138967633442e-06, 5.2486808917600135e-06, 4.75818634871142e-06, 4.337824197777205e-06, 4.192033590111937e-06, 3.846983130754771e-06, 3.7996170385109076e-06, 4.045270000808686e-06, 3.723878833058546e-06, 3.379625932685343e-06, 3.381734579246497e-06, 3.062222143632676e-06, 2.8506444019774526e-06, 2.9190412871262946e-06, 2.8236598645176886e-06, 2.5418966531186365e-06, 2.289579523064267e-06, 2.486794459702699e-06, 2.2650135902921928e-06, 2.0861214223666994e-06, 1.8776743091644789e-06, 1.6899993215533741e-06, 1.618457416034367e-06, 1.6029676524343739e-06, 1.987542688020985e-05, 1.857697169003353e-05, 1.681071772043925e-05, 1.513629927956011e-05, 1.3671796501716557e-05, 1.3037271355600098e-05, 1.1740560039849845e-05], "duration": 187587.972216, "accuracy_train": [0.5018117647058824, 0.677364705882353, 0.7358117647058824, 0.7706117647058823, 0.8172235294117647, 0.8348, 0.8461647058823529, 0.8580470588235294, 0.8519529411764706, 0.8631764705882353, 0.8667529411764706, 0.8782117647058824, 0.8907294117647059, 0.886164705882353, 0.8896235294117647, 0.9000941176470588, 0.9044470588235294, 0.9058117647058823, 0.9170588235294118, 0.9204235294117648, 0.924564705882353, 0.9363529411764706, 0.9333882352941176, 0.9342117647058824, 0.9401647058823529, 0.9232235294117647, 0.9332705882352941, 0.9389176470588235, 0.9316470588235294, 0.9455294117647058, 0.94, 0.9488941176470588, 0.9527764705882353, 0.945764705882353, 0.952564705882353, 0.9437176470588235, 0.9469647058823529, 0.9570117647058823, 0.9580235294117647, 0.964635294117647, 0.9663764705882353, 0.9591764705882353, 0.9626588235294118, 0.9618117647058824, 0.9711058823529412, 0.9740941176470588, 0.9629882352941177, 0.9675294117647059, 0.9604941176470588, 0.9792, 0.9724470588235294, 0.9668235294117647, 0.9750823529411765, 0.9731058823529412, 0.971835294117647, 0.9752235294117647, 0.9763294117647059, 0.9695058823529412, 0.9821411764705882, 0.971835294117647, 0.9788235294117648, 0.98, 0.9780941176470588, 0.9843294117647059, 0.9835294117647059, 0.9862352941176471, 0.9808235294117648, 0.9824, 0.9774823529411765, 0.9837882352941176, 0.9784941176470588, 0.9816235294117647, 0.9824470588235295, 0.9871529411764706, 0.9634588235294118, 0.9796705882352941, 0.9856, 0.9871294117647059, 0.9847294117647059, 0.9878588235294118, 0.9811058823529412, 0.9826117647058824, 0.9797176470588236, 0.9849882352941176, 0.9900705882352941, 0.9896235294117647, 0.9848470588235294, 0.9880941176470588, 0.9892235294117647, 0.9849882352941176, 0.9851764705882353, 0.9900705882352941, 0.9879529411764706, 0.9881176470588235, 0.9911294117647059, 0.9887764705882353, 0.9905176470588235, 0.9911058823529412, 0.9905882352941177, 0.9904, 0.9850588235294118, 0.9918823529411764, 0.9893411764705883, 0.9886588235294118, 0.9862588235294117, 0.9935294117647059, 0.9909882352941176, 0.9865882352941177, 0.9945176470588235, 0.9909176470588236, 0.9916235294117647, 0.9948705882352941, 0.9898117647058824, 0.992964705882353, 0.991835294117647, 0.9934823529411765, 0.9936235294117647, 0.9858823529411764, 0.9912, 0.9908705882352942, 0.9892705882352941, 0.991035294117647, 0.9933647058823529, 0.9930823529411764, 0.9936, 0.9868705882352942, 0.9921882352941176, 0.9915058823529411, 0.994, 0.9908941176470588, 0.9952705882352941, 0.9949882352941176, 0.9950117647058824, 0.9946117647058823, 0.9931294117647059, 0.9944, 0.9898117647058824, 0.9955058823529411, 0.9956470588235294, 0.9937882352941176, 0.9934588235294117, 0.9949882352941176, 0.996, 0.9922352941176471, 0.9934588235294117, 0.9935529411764706, 0.9952235294117647, 0.9956, 0.9896235294117647, 0.9915529411764706, 0.9952705882352941, 0.9936470588235294, 0.9951764705882353, 0.9934823529411765, 0.9940235294117648, 0.9961882352941176, 0.9967058823529412, 0.9892705882352941, 0.9963529411764706, 0.9938823529411764, 0.9933411764705883, 0.9946823529411765, 0.995764705882353, 0.9932470588235294, 0.996, 0.9968941176470588, 0.9954823529411765, 0.9951764705882353, 0.9927294117647059, 0.9940235294117648, 0.9954588235294117, 0.9965176470588235, 0.9962117647058824, 0.9948, 0.9947294117647059, 0.997035294117647, 0.9956235294117647, 0.9958588235294118, 0.9952470588235294, 0.9951764705882353, 0.994164705882353, 0.9963764705882353, 0.9809176470588236, 0.9965647058823529, 0.9950823529411764, 0.9944470588235295, 0.9949411764705882, 0.9971294117647059, 0.9942823529411765], "end": "2016-02-07 13:54:41.776000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0], "moving_var_accuracy_valid": [0.021597241599999997, 0.052120372095999984, 0.07964964504575998, 0.1014516403046656, 0.11950290950469913, 0.1322412431797343, 0.13953869219204879, 0.1429898664224411, 0.14223410368550943, 0.13933544616227778, 0.13479074407996836, 0.129219580663746, 0.1231193571170742, 0.11600502579593824, 0.10888347505736445, 0.1017347526960386, 0.09483737759265194, 0.08786500315572802, 0.0814038789719112, 0.07523839921364031, 0.0693206406359538, 0.06384387319769777, 0.058594738844986866, 0.0536718797954706, 0.04914011842496515, 0.04470542634567964, 0.04069902916931905, 0.03701759583788396, 0.03359468178762763, 0.03055737309326919, 0.027727654581973713, 0.02518746777138915, 0.02295114503392518, 0.02080207603537812, 0.018898132183314174, 0.01707869080212361, 0.015418017428155335, 0.013989062153033784, 0.012678527775082436, 0.011489895084492573, 0.01044111588643317, 0.009451313223695993, 0.008523759067396235, 0.007694550649691064, 0.006984878412979009, 0.006365222632032982, 0.005741883570123345, 0.005197156020332637, 0.004681956055849066, 0.004254114740278723, 0.0038513347165703483, 0.0034775011097112492, 0.0031518915473360354, 0.0028520283412891913, 0.0025705247950548685, 0.0023265117662918298, 0.002100261065481588, 0.0018937578047055507, 0.0017349544318105625, 0.0015616672064296207, 0.0014131771423556063, 0.0012786903863884747, 0.001152759705524468, 0.001063422631090955, 0.0009633489910065559, 0.0008861402790764667, 0.000802615853432125, 0.0007275090442665277, 0.0006547994999678089, 0.00059221987718111, 0.000533325910531075, 0.00047999530345776146, 0.0004333411696573601, 0.00039504223446294533, 0.0003840586419074081, 0.00034618046944676973, 0.0003153501887981076, 0.0002901232874720412, 0.00026457218438096213, 0.0002438480850154282, 0.00021998896831469745, 0.00020038469941236023, 0.00018087517630987346, 0.00016308516501759184, 0.00016222860190042722, 0.00015741532553677955, 0.00014284025085571265, 0.0001309033236538398, 0.00012205173808891713, 0.00011001708333691887, 9.98104068410418e-05, 9.127532676215067e-05, 8.21616631713839e-05, 7.541931622818516e-05, 6.860432613948437e-05, 6.329977005396344e-05, 5.767978974655249e-05, 5.275518064642934e-05, 4.9402090710023686e-05, 4.532281489324797e-05, 4.143843218435693e-05, 4.211511905406479e-05, 3.79537770986626e-05, 3.541635868424872e-05, 3.23500352460606e-05, 3.272318456896928e-05, 2.9459045060615965e-05, 2.8707668102314403e-05, 3.0275278285864842e-05, 2.7913722389377224e-05, 2.7601286818444655e-05, 2.708221964365625e-05, 2.4374743831323857e-05, 2.3745176133152324e-05, 2.359113337409253e-05, 2.386002806412368e-05, 2.2595510251446472e-05, 2.7555381716680823e-05, 2.4832891704428174e-05, 2.23496155832357e-05, 2.0116524685777043e-05, 1.862481989687645e-05, 1.90455599068483e-05, 1.714100392105242e-05, 1.634838270939084e-05, 1.8011606143696075e-05, 1.657713515412838e-05, 1.536163882476215e-05, 1.5670982825739037e-05, 1.412423220797507e-05, 1.318567699870758e-05, 1.661751544149708e-05, 1.7150868027367168e-05, 1.548711539675652e-05, 1.4256402192840435e-05, 1.3557480593974335e-05, 1.2768395001789461e-05, 1.2270374323682725e-05, 2.0494389369813902e-05, 1.85517687847874e-05, 1.67077966172828e-05, 1.7808352661716585e-05, 1.7233691612707618e-05, 1.616896236278295e-05, 1.511395174606617e-05, 1.3728319909848367e-05, 1.2931824189610329e-05, 1.2273450214544515e-05, 1.4569511256585824e-05, 2.123144914390537e-05, 1.9109569470010735e-05, 1.7211159479380123e-05, 1.7508749922898583e-05, 1.6755337921390995e-05, 1.596125776145242e-05, 1.601628466309113e-05, 1.4770438232165253e-05, 1.8783853153416735e-05, 1.7699606331274608e-05, 1.60912738484171e-05, 1.4613065265294046e-05, 1.3152939050854173e-05, 1.1890969239909987e-05, 1.3687262234998608e-05, 1.3174077238646756e-05, 1.242286442962919e-05, 1.1221700424179614e-05, 1.027804340183067e-05, 1.0291163837543283e-05, 1.0039563159738837e-05, 9.19052123808948e-06, 1.3086125349512208e-05, 1.1808071086379805e-05, 1.1887655679534996e-05, 1.355635752674599e-05, 1.5293522139804786e-05, 1.3767444930315783e-05, 1.2737026932415875e-05, 1.1547218812476877e-05, 1.040263360295183e-05, 9.78566743460723e-06, 8.825177052108276e-06, 3.2952695575715596e-05, 3.167126183117784e-05, 2.850596858843886e-05, 2.5655373885888494e-05, 2.3103921062513334e-05, 2.339590909393953e-05, 2.1225955482748254e-05], "accuracy_test": 0.2863, "start": "2016-02-05 09:48:13.804000", "learning_rate_per_epoch": [0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103, 0.0014679068699479103], "accuracy_train_first": 0.5018117647058824, "accuracy_train_last": 0.9942823529411765, "batch_size_eval": 1024, "accuracy_train_std": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "accuracy_test_std": 0, "error_valid": [0.5101333333333333, 0.34840000000000004, 0.28759999999999997, 0.2553333333333333, 0.21319999999999995, 0.19320000000000004, 0.18706666666666671, 0.1770666666666667, 0.18493333333333328, 0.17933333333333334, 0.17559999999999998, 0.16986666666666672, 0.16133333333333333, 0.16879999999999995, 0.16200000000000003, 0.15893333333333337, 0.15159999999999996, 0.15626666666666666, 0.1458666666666667, 0.14239999999999997, 0.14213333333333333, 0.1352, 0.1373333333333333, 0.13639999999999997, 0.1318666666666667, 0.14559999999999995, 0.13946666666666663, 0.13839999999999997, 0.1418666666666667, 0.13213333333333332, 0.1358666666666667, 0.13013333333333332, 0.11986666666666668, 0.13, 0.122, 0.1338666666666667, 0.13613333333333333, 0.1213333333333333, 0.12186666666666668, 0.12039999999999995, 0.11373333333333335, 0.11919999999999997, 0.1273333333333333, 0.12386666666666668, 0.11253333333333337, 0.1061333333333333, 0.1206666666666667, 0.11346666666666672, 0.1226666666666667, 0.10786666666666667, 0.11106666666666665, 0.11413333333333331, 0.10853333333333337, 0.10960000000000003, 0.11493333333333333, 0.10866666666666669, 0.11106666666666665, 0.11240000000000006, 0.09960000000000002, 0.1146666666666667, 0.1068, 0.10640000000000005, 0.10960000000000003, 0.0968, 0.10373333333333334, 0.09666666666666668, 0.10226666666666662, 0.1014666666666667, 0.10760000000000003, 0.10253333333333337, 0.10573333333333335, 0.10760000000000003, 0.10360000000000003, 0.09960000000000002, 0.12413333333333332, 0.11053333333333337, 0.10186666666666666, 0.09933333333333338, 0.10066666666666668, 0.09826666666666661, 0.10786666666666667, 0.10053333333333336, 0.10760000000000003, 0.10360000000000003, 0.09213333333333329, 0.09266666666666667, 0.09919999999999995, 0.09733333333333338, 0.09506666666666663, 0.09986666666666666, 0.0981333333333333, 0.0968, 0.1008, 0.09640000000000004, 0.09719999999999995, 0.09560000000000002, 0.09653333333333336, 0.09599999999999997, 0.09413333333333329, 0.09519999999999995, 0.10066666666666668, 0.09093333333333331, 0.09826666666666661, 0.10133333333333339, 0.10026666666666662, 0.09186666666666665, 0.09786666666666666, 0.10253333333333337, 0.09106666666666663, 0.09466666666666668, 0.09186666666666665, 0.09160000000000001, 0.09599999999999997, 0.09160000000000001, 0.09066666666666667, 0.08973333333333333, 0.09106666666666663, 0.10319999999999996, 0.09453333333333336, 0.09506666666666663, 0.09493333333333331, 0.0974666666666667, 0.09026666666666672, 0.0948, 0.09160000000000001, 0.10053333333333336, 0.09306666666666663, 0.09266666666666667, 0.09013333333333329, 0.09373333333333334, 0.09186666666666665, 0.08666666666666667, 0.08826666666666672, 0.0934666666666667, 0.09466666666666668, 0.09013333333333329, 0.09519999999999995, 0.08999999999999997, 0.08240000000000003, 0.09053333333333335, 0.09186666666666665, 0.08599999999999997, 0.08733333333333337, 0.09333333333333338, 0.08840000000000003, 0.0894666666666667, 0.08799999999999997, 0.09293333333333331, 0.0968, 0.10066666666666668, 0.09199999999999997, 0.09173333333333333, 0.08733333333333337, 0.08826666666666672, 0.08813333333333329, 0.08666666666666667, 0.08853333333333335, 0.0981333333333333, 0.08813333333333329, 0.0894666666666667, 0.0894666666666667, 0.09066666666666667, 0.09133333333333338, 0.09640000000000004, 0.08813333333333329, 0.08840000000000003, 0.09133333333333338, 0.09213333333333329, 0.09426666666666672, 0.08826666666666672, 0.08960000000000001, 0.08346666666666669, 0.0894666666666667, 0.09373333333333334, 0.09599999999999997, 0.08506666666666662, 0.09053333333333335, 0.08840000000000003, 0.08919999999999995, 0.08973333333333333, 0.08786666666666665, 0.09026666666666672, 0.10653333333333337, 0.08679999999999999, 0.09119999999999995, 0.09106666666666663, 0.0914666666666667, 0.08573333333333333, 0.08919999999999995], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.014352071922873078, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 64, "valid_ratio": 0.15, "learning_rate": 0.0014679069263600537, "optimization": "adam", "nb_data_augmentation": 2, "learning_rate_decay_method": "none", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 4.0832298793203845e-08, "rotation_range": [0, 0], "momentum": 0.8430777906874003}, "accuracy_valid_max": 0.9176, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        y_pred = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            y_pred.extend((nnet.predict(X[mini_batch]) == y[mini_batch]).tolist())\n        return np.mean(y_pred)\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            acc = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = acc\n            status[\"accuracy_train_std\"] = 0\n            acc = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = acc\n            status[\"accuracy_valid_std\"] = 0\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n\n    # rescaling to [-1, 1]\n    X_min = X_train.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X_train.max(axis=(0, 2, 3))[None, :, None, None]\n    def preprocess(a):\n        return (a / 255.) * 2 - 1\n        # return 2 * ((a - X_min) / (X_max - X_min)) - 1\n    X_train = preprocess(X_train)\n    X_valid = preprocess(X_valid)\n\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = preprocess(X_test)\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    acc = evaluate(X_test, y_test, batch_size_eval)\n    light.set(\"accuracy_test\", acc)\n    light.set(\"accuracy_test_std\", 0)\n    print(\"Test accuracy : {}+-{}\".format(acc, 0))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.9108, "accuracy_valid_std": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "accuracy_valid": [0.4898666666666667, 0.6516, 0.7124, 0.7446666666666667, 0.7868, 0.8068, 0.8129333333333333, 0.8229333333333333, 0.8150666666666667, 0.8206666666666667, 0.8244, 0.8301333333333333, 0.8386666666666667, 0.8312, 0.838, 0.8410666666666666, 0.8484, 0.8437333333333333, 0.8541333333333333, 0.8576, 0.8578666666666667, 0.8648, 0.8626666666666667, 0.8636, 0.8681333333333333, 0.8544, 0.8605333333333334, 0.8616, 0.8581333333333333, 0.8678666666666667, 0.8641333333333333, 0.8698666666666667, 0.8801333333333333, 0.87, 0.878, 0.8661333333333333, 0.8638666666666667, 0.8786666666666667, 0.8781333333333333, 0.8796, 0.8862666666666666, 0.8808, 0.8726666666666667, 0.8761333333333333, 0.8874666666666666, 0.8938666666666667, 0.8793333333333333, 0.8865333333333333, 0.8773333333333333, 0.8921333333333333, 0.8889333333333334, 0.8858666666666667, 0.8914666666666666, 0.8904, 0.8850666666666667, 0.8913333333333333, 0.8889333333333334, 0.8876, 0.9004, 0.8853333333333333, 0.8932, 0.8936, 0.8904, 0.9032, 0.8962666666666667, 0.9033333333333333, 0.8977333333333334, 0.8985333333333333, 0.8924, 0.8974666666666666, 0.8942666666666667, 0.8924, 0.8964, 0.9004, 0.8758666666666667, 0.8894666666666666, 0.8981333333333333, 0.9006666666666666, 0.8993333333333333, 0.9017333333333334, 0.8921333333333333, 0.8994666666666666, 0.8924, 0.8964, 0.9078666666666667, 0.9073333333333333, 0.9008, 0.9026666666666666, 0.9049333333333334, 0.9001333333333333, 0.9018666666666667, 0.9032, 0.8992, 0.9036, 0.9028, 0.9044, 0.9034666666666666, 0.904, 0.9058666666666667, 0.9048, 0.8993333333333333, 0.9090666666666667, 0.9017333333333334, 0.8986666666666666, 0.8997333333333334, 0.9081333333333333, 0.9021333333333333, 0.8974666666666666, 0.9089333333333334, 0.9053333333333333, 0.9081333333333333, 0.9084, 0.904, 0.9084, 0.9093333333333333, 0.9102666666666667, 0.9089333333333334, 0.8968, 0.9054666666666666, 0.9049333333333334, 0.9050666666666667, 0.9025333333333333, 0.9097333333333333, 0.9052, 0.9084, 0.8994666666666666, 0.9069333333333334, 0.9073333333333333, 0.9098666666666667, 0.9062666666666667, 0.9081333333333333, 0.9133333333333333, 0.9117333333333333, 0.9065333333333333, 0.9053333333333333, 0.9098666666666667, 0.9048, 0.91, 0.9176, 0.9094666666666666, 0.9081333333333333, 0.914, 0.9126666666666666, 0.9066666666666666, 0.9116, 0.9105333333333333, 0.912, 0.9070666666666667, 0.9032, 0.8993333333333333, 0.908, 0.9082666666666667, 0.9126666666666666, 0.9117333333333333, 0.9118666666666667, 0.9133333333333333, 0.9114666666666666, 0.9018666666666667, 0.9118666666666667, 0.9105333333333333, 0.9105333333333333, 0.9093333333333333, 0.9086666666666666, 0.9036, 0.9118666666666667, 0.9116, 0.9086666666666666, 0.9078666666666667, 0.9057333333333333, 0.9117333333333333, 0.9104, 0.9165333333333333, 0.9105333333333333, 0.9062666666666667, 0.904, 0.9149333333333334, 0.9094666666666666, 0.9116, 0.9108, 0.9102666666666667, 0.9121333333333334, 0.9097333333333333, 0.8934666666666666, 0.9132, 0.9088, 0.9089333333333334, 0.9085333333333333, 0.9142666666666667, 0.9108], "seed": 21296070, "model": "residualv3", "loss_std": [0.24420644342899323, 0.15141895413398743, 0.13534538447856903, 0.12247395515441895, 0.12104698270559311, 0.11439193040132523, 0.11383835226297379, 0.11086352914571762, 0.10670546442270279, 0.1046777293086052, 0.10084306448698044, 0.09872656315565109, 0.09681376814842224, 0.09243609011173248, 0.08878064155578613, 0.0859508216381073, 0.08355175703763962, 0.07855278253555298, 0.0779779702425003, 0.07754113525152206, 0.07490379363298416, 0.07297021150588989, 0.06951579451560974, 0.06771761178970337, 0.06696388125419617, 0.06502465158700943, 0.0620444230735302, 0.06382030248641968, 0.05904007703065872, 0.058207981288433075, 0.05614620819687843, 0.05428897589445114, 0.05380789190530777, 0.05438724160194397, 0.05257048085331917, 0.05100366473197937, 0.050309788435697556, 0.05182555690407753, 0.050139203667640686, 0.04538647085428238, 0.04649034142494202, 0.04572383314371109, 0.04386468604207039, 0.04725118353962898, 0.04434918239712715, 0.04142184183001518, 0.043184638023376465, 0.04221661016345024, 0.041138872504234314, 0.039225269109010696, 0.04061345010995865, 0.04170331358909607, 0.04010271653532982, 0.038598790764808655, 0.03705131635069847, 0.03808591514825821, 0.03494797646999359, 0.03544803708791733, 0.037665873765945435, 0.03781600669026375, 0.03482690826058388, 0.03509156033396721, 0.036761898547410965, 0.03228135034441948, 0.03419593349099159, 0.03472430258989334, 0.03136212378740311, 0.035251885652542114, 0.032276708632707596, 0.03169354423880577, 0.033789753913879395, 0.030848123133182526, 0.03133288025856018, 0.03193596005439758, 0.031312912702560425, 0.030247462913393974, 0.03146746754646301, 0.030509207397699356, 0.029682988300919533, 0.030592160299420357, 0.02834727056324482, 0.031419865787029266, 0.02985144965350628, 0.028820980340242386, 0.027546001598238945, 0.027213966473937035, 0.028595784679055214, 0.027549928054213524, 0.030010348185896873, 0.029238272458314896, 0.027454059571027756, 0.027540555223822594, 0.027094557881355286, 0.02928607352077961, 0.02964620105922222, 0.026617426425218582, 0.02576974406838417, 0.025632016360759735, 0.02606464922428131, 0.02687561698257923, 0.026086628437042236, 0.023853741586208344, 0.02481485903263092, 0.02326219528913498, 0.025050712749361992, 0.024992365390062332, 0.024179767817258835, 0.025772374123334885, 0.024152401834726334, 0.02356165088713169, 0.027608182281255722, 0.02203001081943512, 0.023143034428358078, 0.02358473464846611, 0.023011548444628716, 0.02484925463795662, 0.02380693331360817, 0.02380424737930298, 0.02310863696038723, 0.025413675233721733, 0.02211729809641838, 0.023076675832271576, 0.023638978600502014, 0.020908845588564873, 0.024381013587117195, 0.023831728845834732, 0.023705171421170235, 0.022534050047397614, 0.020003167912364006, 0.021547624841332436, 0.022546041756868362, 0.023987257853150368, 0.0193344559520483, 0.025300825014710426, 0.022189119830727577, 0.02246228978037834, 0.022695450112223625, 0.01911487989127636, 0.022907594218850136, 0.01906406693160534, 0.02027040533721447, 0.02394682541489601, 0.021418780088424683, 0.02041909098625183, 0.01862136647105217, 0.02336074225604534, 0.019355028867721558, 0.02028500661253929, 0.021603690460324287, 0.02073686197400093, 0.018443547189235687, 0.021198689937591553, 0.019277771934866905, 0.018887406215071678, 0.01992865838110447, 0.021173648536205292, 0.01812603697180748, 0.022610997781157494, 0.022826846688985825, 0.01953418366611004, 0.021975727751851082, 0.0181285347789526, 0.022165019065141678, 0.018838930875062943, 0.01748068258166313, 0.017804395407438278, 0.02363024838268757, 0.018452266231179237, 0.02285429835319519, 0.01741882972419262, 0.01927027478814125, 0.019110502675175667, 0.022341283038258553, 0.017644178122282028, 0.018522216007113457, 0.016383079811930656, 0.02033359557390213, 0.0217356588691473, 0.01861392706632614, 0.01601748913526535, 0.02010081149637699, 0.01744178496301174, 0.019014321267604828, 0.018760154023766518, 0.019803522154688835, 0.018458442762494087, 0.01650395803153515, 0.02151193469762802, 0.015878353267908096]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:43 2016", "state": "available"}], "summary": "61794bdcbf486d969cc7ad25f8ae944f"}