{"content": {"hp_model": {"f0": 64, "f1": 64, "f2": 16, "f3": 32, "nonlin": "leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.4423401355743408, 1.015919804573059, 0.7697281241416931, 0.6417984366416931, 0.5553199052810669, 0.49021974205970764, 0.43741846084594727, 0.39252033829689026, 0.35238951444625854, 0.3169001340866089, 0.2854747474193573, 0.2569653391838074, 0.23087672889232635, 0.20684032142162323, 0.18472085893154144, 0.16434068977832794, 0.14565563201904297, 0.12852005660533905, 0.11264488101005554, 0.09812914580106735, 0.08481833338737488, 0.0729186087846756, 0.06218687817454338, 0.052662406116724014, 0.04418935999274254, 0.036756157875061035, 0.03033406473696232, 0.0248407069593668, 0.0202015433460474, 0.016332456842064857, 0.013199405744671822, 0.010611999779939651, 0.008574074134230614, 0.0069216336123645306, 0.005583290942013264, 0.004524248652160168, 0.003695193910971284, 0.00304022547788918, 0.00251534441486001, 0.0020950217731297016, 0.001756738405674696, 0.0014865482226014137, 0.0012693438911810517, 0.0010953496675938368, 0.0009568450041115284, 0.0008460864191874862, 0.0007569305598735809, 0.000685760285705328, 0.0006288327858783305, 0.0005833975737914443, 0.0005467999144457281, 0.0005175574915483594, 0.0004941387451253831, 0.00047533423639833927, 0.00046025554183870554, 0.00044817349407821894, 0.0004384004569146782, 0.0004305153270252049, 0.0004240849521011114, 0.0004187589220236987], "moving_avg_accuracy_train": [0.058712206418420065, 0.12163269425987445, 0.18584701241809704, 0.2472943477615125, 0.3047775426169135, 0.35842071628065586, 0.4082782765220532, 0.45446364562141667, 0.49703468975482684, 0.5361250489331832, 0.5719318011746471, 0.6050040881633564, 0.6354202241686799, 0.6632921842234343, 0.6888790722691142, 0.7122606580804549, 0.7337620314796814, 0.7535480703663663, 0.7717275291739064, 0.7885050995423223, 0.8038560289453252, 0.8180252880270754, 0.8311635598542128, 0.8432763229510174, 0.8544266367095793, 0.8646851333779993, 0.8741387055653009, 0.8828654484731489, 0.8908590260187833, 0.898211355928902, 0.9049632754301424, 0.9112793851621837, 0.9169894605579254, 0.922349381502179, 0.9272546905603406, 0.9317671610115047, 0.9359888196854096, 0.939885968741924, 0.9434421589201495, 0.9466915582055524, 0.9496880971755103, 0.9524291600758532, 0.9549612208528286, 0.9573098300163922, 0.9594631418421802, 0.9614592151568087, 0.9633091595625933, 0.9650066616111328, 0.9665785912821993, 0.968028205218302, 0.9693630846953183, 0.9706040037543948, 0.9717743093301827, 0.972846185538868, 0.9738550519540657, 0.974790933513458, 0.9756425275121492, 0.9764578262847899, 0.9772218221146904, 0.977942006493753], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.05770498988140059, 0.11901992246329064, 0.18091237557652481, 0.23900419054146266, 0.29226235700124714, 0.3405578292265742, 0.38462498728659744, 0.4247056161464166, 0.46085039479909423, 0.4935068839336426, 0.5228122749359861, 0.5491403577304146, 0.5728468097679906, 0.593950683008059, 0.6127966550404609, 0.6295016822133727, 0.6445117926064932, 0.6579222062016421, 0.6700058444858453, 0.6809065625127879, 0.6905819018846266, 0.6993416239702605, 0.7072365513699213, 0.7142544777935467, 0.7204485412623094, 0.7259621632279458, 0.7308023526845187, 0.7350852810079342, 0.738953153038918, 0.7423732027105532, 0.7452915265001153, 0.7479678755443809, 0.7503389390818103, 0.7525858480727255, 0.7544982028832994, 0.7562681503378158, 0.7578855171093806, 0.759341147203789, 0.760676657859916, 0.7617443401066805, 0.7626798105576088, 0.763510556440854, 0.7642216066420245, 0.7648737588543281, 0.7653864241492416, 0.7658966510396639, 0.7663924763347939, 0.7667634178955915, 0.7670728512378094, 0.7674001693708057, 0.7676215135030022, 0.7678329302532292, 0.7680232053284334, 0.7682056304187075, 0.7683830495398639, 0.7685549337801547, 0.7687706647526664, 0.7689658521365865, 0.769128284242205, 0.7693355082935116], "moving_var_accuracy_train": [0.031024108642672507, 0.06355258789026476, 0.09430863700996721, 0.11885974849622588, 0.13671263286365928, 0.1489396803037592, 0.15641769909240436, 0.1599737240527639, 0.16028699583496633, 0.1580108018777059, 0.1537488332446695, 0.14821793542017442, 0.1417224138436061, 0.1345417878748899, 0.1269798086461603, 0.11920211477702207, 0.11144268482179656, 0.10382180235304556, 0.09641405662055645, 0.08930603276570515, 0.08249628879095845, 0.07605357103819446, 0.07000174161380879, 0.06432203872098172, 0.059008800321112935, 0.05405505107406551, 0.04945387621056366, 0.045193892965524354, 0.04124957920677457, 0.03761113208206215, 0.034260314626417016, 0.031193322343099975, 0.028367434758015477, 0.025789250054971766, 0.02342688356207933, 0.021267456712025064, 0.019301112658453166, 0.017507691329526072, 0.015870740593826833, 0.014378693895887942, 0.013021637718485433, 0.011787094779049623, 0.010666087287149337, 0.009649122243462975, 0.008725940785488379, 0.007889205485035886, 0.007131085585272742, 0.00644391064558863, 0.00582175824704678, 0.005258494847415791, 0.004748682491637644, 0.004287673163474494, 0.0038712323833935275, 0.0034944494125148853, 0.003154164774256822, 0.002846631165470034, 0.002568494959970491, 0.0023176278727714676, 0.002091118292147269, 0.0018866744527911532], "duration": 27106.627336, "accuracy_train": [0.5871220641842008, 0.6879170848329641, 0.7637758758421004, 0.8003203658522517, 0.8221262963155224, 0.8412092792543374, 0.856996318694629, 0.8701319675156883, 0.8801740869555187, 0.8879382815383905, 0.894192571347822, 0.9026546710617387, 0.9091654482165927, 0.9141398247162238, 0.9191610646802326, 0.9226949303825213, 0.9272743920727206, 0.9316224203465301, 0.9353426584417681, 0.9395032328580657, 0.9420143935723514, 0.9455486197628276, 0.9494080062984496, 0.9522911908222591, 0.9547794605366371, 0.9570116033937799, 0.9592208552510151, 0.9614061346437799, 0.9628012239294942, 0.9643823251199704, 0.9657305509413067, 0.9681243727505537, 0.9683801391196014, 0.9705886700004615, 0.9714024720837948, 0.9723793950719823, 0.9739837477505537, 0.9749603102505537, 0.9754478705241787, 0.9759361517741787, 0.9766569479051311, 0.9770987261789406, 0.9777497678456073, 0.9784473124884644, 0.978842948274271, 0.9794238749884644, 0.9799586592146549, 0.9802841800479882, 0.9807259583217978, 0.9810747306432264, 0.9813769999884644, 0.9817722752860835, 0.9823070595122739, 0.9824930714170359, 0.9829348496908453, 0.9832138675479882, 0.9833068735003692, 0.9837955152385567, 0.9840977845837948, 0.9844236659053157], "end": "2016-02-03 19:24:00.973000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0], "moving_var_accuracy_valid": [0.029968792714912908, 0.06080760206111712, 0.08920292362637045, 0.11065456195701863, 0.12511699641323976, 0.13359737050912185, 0.13771486323359336, 0.13840148819842116, 0.13631934459323858, 0.1322854266772686, 0.1267861374857439, 0.12034603522984205, 0.11336939452074687, 0.10604081626026807, 0.09863327059085594, 0.09128146492739983, 0.08418104916078285, 0.07738149697984116, 0.07095747610950821, 0.06493115938008359, 0.0592805531697175, 0.0540430924319036, 0.04919975209652645, 0.04472303850846058, 0.04059603245791008, 0.03681002945673862, 0.03333987341684443, 0.030170977350371633, 0.027288523521767062, 0.024664941827398434, 0.022275097168325108, 0.02011205304935327, 0.018151445225104687, 0.016381738102715326, 0.014776478200737518, 0.013327024806589509, 0.012017865203394417, 0.0108351484138007, 0.009767685870834292, 0.008801176792171368, 0.007928935057635271, 0.007142252800374504, 0.006432577851834317, 0.005793147789222997, 0.005216198441642177, 0.004696921580795348, 0.00422944200722543, 0.0038077361852766308, 0.003427824307688453, 0.003086006111361301, 0.0027778464392488916, 0.002500464068704491, 0.0022507435032722376, 0.0020259686631670677, 0.001823655094751328, 0.0016415554830047383, 0.0014778187933767717, 0.0013303797970726687, 0.0011975792750658228, 0.0010782078238261998], "accuracy_test": 0.2836933992346939, "start": "2016-02-03 11:52:14.346000", "learning_rate_per_epoch": [0.0019791622180491686, 0.0009895811090245843, 0.0006597207393497229, 0.0004947905545122921, 0.0003958324668928981, 0.00032986036967486143, 0.0002827374846674502, 0.0002473952772561461, 0.00021990692766848952, 0.00019791623344644904, 0.0001799238525563851, 0.00016493018483743072, 0.0001522432576166466, 0.0001413687423337251, 0.00013194415078032762, 0.00012369763862807304, 0.00011642130994005129, 0.00010995346383424476, 0.00010416643635835499, 9.895811672322452e-05, 9.424582094652578e-05, 8.996192627819255e-05, 8.605053153587505e-05, 8.246509241871536e-05, 7.916649337857962e-05, 7.61216288083233e-05, 7.330230437219143e-05, 7.068437116686255e-05, 6.82469762978144e-05, 6.597207539016381e-05, 6.384394509950653e-05, 6.184881931403652e-05, 5.997461630613543e-05, 5.8210654970025644e-05, 5.6547494750702754e-05, 5.497673191712238e-05, 5.349087223294191e-05, 5.208321817917749e-05, 5.0747752538882196e-05, 4.947905836161226e-05, 4.827224984182976e-05, 4.712291047326289e-05, 4.602703120326623e-05, 4.4980963139096275e-05, 4.398138480610214e-05, 4.3025265767937526e-05, 4.210983752273023e-05, 4.123254620935768e-05, 4.039106715936214e-05, 3.958324668928981e-05, 3.8807105738669634e-05, 3.806081440416165e-05, 3.734268466359936e-05, 3.6651152186095715e-05, 3.59847690560855e-05, 3.5342185583431274e-05, 3.47221466654446e-05, 3.41234881489072e-05, 3.354512227815576e-05, 3.2986037695081905e-05], "accuracy_train_first": 0.5871220641842008, "accuracy_train_last": 0.9844236659053157, "batch_size_eval": 1024, "accuracy_train_std": [0.02042466119854992, 0.018230876645531986, 0.02106485492290482, 0.023014959076997955, 0.022330129315671327, 0.02441147030993187, 0.02257169064498402, 0.021443768936885797, 0.020952804577906107, 0.021215757085811353, 0.021001924297279848, 0.020579423117656377, 0.019219797845128606, 0.01842007246443583, 0.018970548971671438, 0.018221095325371313, 0.01816929826297206, 0.01770250993522586, 0.017417020293086168, 0.017209295493004213, 0.01704836549071606, 0.01621318648778294, 0.015270707148313249, 0.014635855489960787, 0.013942531852050138, 0.013567215484138619, 0.012717103635387318, 0.012486617371915875, 0.012252290946031108, 0.01176864277449524, 0.011428790448999322, 0.011182845612609417, 0.011213477269579459, 0.010955183678817378, 0.010590582625779228, 0.010495867255604871, 0.009625548061001304, 0.009267168103767334, 0.009300288894938932, 0.00920511864091164, 0.00894983002406529, 0.008892217036102063, 0.008748808630416246, 0.008651755307395571, 0.008116342011830526, 0.007951371550711447, 0.007803824245920913, 0.007754141604603259, 0.007500481196140342, 0.007411772247179256, 0.007363890988988292, 0.007262200356746464, 0.007263769006569254, 0.007138202248450142, 0.006996245730584231, 0.006822005375197719, 0.006634930891122164, 0.006604740294972177, 0.006441844671736251, 0.006322833317758838], "accuracy_test_std": 0.015791986658593143, "error_valid": [0.42295010118599397, 0.3291456842996988, 0.26205554640436746, 0.23816947477409633, 0.22841414486069278, 0.2247829207454819, 0.21877059017319278, 0.21456872411521077, 0.21384659732680722, 0.21258471385542166, 0.21343920604292166, 0.21390689711972888, 0.21379512189382532, 0.21611445783132532, 0.21758959666792166, 0.22015307323042166, 0.22039721385542166, 0.2213840714420181, 0.22124141095632532, 0.22098697524472888, 0.22234004376882532, 0.2218208772590362, 0.22170910203313254, 0.22258418439382532, 0.22380488751882532, 0.22441523908132532, 0.22563594220632532, 0.22636836408132532, 0.22623599868222888, 0.22684635024472888, 0.22844355939382532, 0.22794498305722888, 0.22832148908132532, 0.2271919710090362, 0.2282906038215362, 0.2278023225715362, 0.2275581819465362, 0.2275581819465362, 0.22730374623493976, 0.22864651967243976, 0.2289009553840362, 0.22901273060993976, 0.22937894154743976, 0.22925687123493976, 0.2299995881965362, 0.2295113069465362, 0.2291450960090362, 0.22989810805722888, 0.23014224868222888, 0.22965396743222888, 0.23038638930722888, 0.23026431899472888, 0.23026431899472888, 0.23015254376882532, 0.23002017836972888, 0.22989810805722888, 0.22928775649472888, 0.22927746140813254, 0.22940982680722888, 0.22879947524472888], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.085571598118162, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 64, "valid_ratio": 0.15, "learning_rate": 0.001979162303811129, "optimization": "adam", "nb_data_augmentation": 0, "learning_rate_decay_method": "lin", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 5.771123959394445e-08, "rotation_range": [0, 0], "momentum": 0.7903434148961134}, "accuracy_valid_max": 0.7874152861445783, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.7712005247552711, "accuracy_valid_std": [0.00936266564898985, 0.007718310158307435, 0.012325575209025386, 0.013954222276135323, 0.012734632648913275, 0.017344309080631625, 0.016747563469117037, 0.013757781759627226, 0.014784391329227408, 0.011602358174556807, 0.011649835100270215, 0.008720297277425917, 0.009144123383067198, 0.008816773612123612, 0.008814739943116458, 0.007304444745998137, 0.007988484666665262, 0.007915862259766114, 0.008431101644794447, 0.007538238280393834, 0.00836690061163384, 0.007581978065968382, 0.007922600145599375, 0.008429323406442856, 0.009524712770119794, 0.009220172035246351, 0.01064522926549016, 0.011634473784619006, 0.01170633786214126, 0.012949948482763412, 0.01321576405303676, 0.01225781629544927, 0.01295488304454416, 0.01250375648874919, 0.011904355464220551, 0.011203565815137581, 0.010882842915024634, 0.011386041591829679, 0.01221655075905731, 0.011886046243391568, 0.013429774873333411, 0.014244919655959377, 0.01474988523019882, 0.01449982063829098, 0.014372118407115227, 0.013517936267511103, 0.01390908118215396, 0.013079746923845779, 0.013322825967670562, 0.01176080840878494, 0.012048758642856825, 0.01185278084624592, 0.011639657367293995, 0.011548086425332395, 0.011956236602195669, 0.012015693851479327, 0.011966142904473622, 0.012080880355824018, 0.011912694177952703, 0.011427649375239778], "accuracy_valid": [0.577049898814006, 0.6708543157003012, 0.7379444535956325, 0.7618305252259037, 0.7715858551393072, 0.7752170792545181, 0.7812294098268072, 0.7854312758847892, 0.7861534026731928, 0.7874152861445783, 0.7865607939570783, 0.7860931028802711, 0.7862048781061747, 0.7838855421686747, 0.7824104033320783, 0.7798469267695783, 0.7796027861445783, 0.7786159285579819, 0.7787585890436747, 0.7790130247552711, 0.7776599562311747, 0.7781791227409638, 0.7782908979668675, 0.7774158156061747, 0.7761951124811747, 0.7755847609186747, 0.7743640577936747, 0.7736316359186747, 0.7737640013177711, 0.7731536497552711, 0.7715564406061747, 0.7720550169427711, 0.7716785109186747, 0.7728080289909638, 0.7717093961784638, 0.7721976774284638, 0.7724418180534638, 0.7724418180534638, 0.7726962537650602, 0.7713534803275602, 0.7710990446159638, 0.7709872693900602, 0.7706210584525602, 0.7707431287650602, 0.7700004118034638, 0.7704886930534638, 0.7708549039909638, 0.7701018919427711, 0.7698577513177711, 0.7703460325677711, 0.7696136106927711, 0.7697356810052711, 0.7697356810052711, 0.7698474562311747, 0.7699798216302711, 0.7701018919427711, 0.7707122435052711, 0.7707225385918675, 0.7705901731927711, 0.7712005247552711], "seed": 522242782, "model": "residualv4", "loss_std": [0.28250977396965027, 0.1548137664794922, 0.1331307291984558, 0.12445122003555298, 0.11899656802415848, 0.11363891512155533, 0.1076105460524559, 0.10233700275421143, 0.09735899418592453, 0.09280097484588623, 0.08794047683477402, 0.08306075632572174, 0.0785764530301094, 0.07409393042325974, 0.06944672763347626, 0.06482843309640884, 0.06006976217031479, 0.05513719841837883, 0.050226494669914246, 0.04548483341932297, 0.04081766679883003, 0.036608170717954636, 0.032440751791000366, 0.02863609418272972, 0.02505636028945446, 0.021530700847506523, 0.01812751404941082, 0.014916983433067799, 0.012006383389234543, 0.009636946022510529, 0.007617922034114599, 0.005887243896722794, 0.0042821490205824375, 0.0027959856670349836, 0.001940004643984139, 0.001417880179360509, 0.00108442734926939, 0.000848533003591001, 0.0006689500878565013, 0.0005279806209728122, 0.00041614173096604645, 0.0003298709343653172, 0.0002621710591483861, 0.00020708209194708616, 0.00016521733778063208, 0.00013126200065016747, 0.00010435926378704607, 8.310754492413253e-05, 6.630038842558861e-05, 5.299588883644901e-05, 4.222802090225741e-05, 3.365169686730951e-05, 2.6855350370169617e-05, 2.1486246623680927e-05, 1.7171420040540397e-05, 1.3745217984251212e-05, 1.1003055078617763e-05, 8.83843495103065e-06, 7.094114153005648e-06, 5.7297938838019036e-06]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:34 2016", "state": "available"}], "summary": "9dca9ec5aa643e0994302b801c8e1756"}