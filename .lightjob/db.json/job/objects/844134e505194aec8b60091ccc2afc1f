{"content": {"hp_model": {"f0": 64, "f1": 32, "f2": 64, "f3": 16, "nonlin": "rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.5513957738876343, 1.1695581674575806, 0.9254511594772339, 0.8050920963287354, 0.7264744639396667, 0.6717440485954285, 0.6292793154716492, 0.5966415405273438, 0.5671690106391907, 0.5433056950569153, 0.5193189382553101, 0.4998231828212738, 0.4849005341529846, 0.46843746304512024, 0.45484933257102966, 0.4409200847148895, 0.42795565724372864, 0.41508445143699646, 0.40682488679885864, 0.3960724174976349, 0.38731151819229126, 0.3808154761791229, 0.37071552872657776, 0.3624780476093292, 0.3570832312107086, 0.3498038351535797, 0.3423130512237549, 0.3355233371257782, 0.33144450187683105, 0.3229124844074249, 0.31764906644821167, 0.3136477768421173, 0.31026700139045715, 0.3040880858898163, 0.299628347158432, 0.2987031638622284, 0.2919568121433258, 0.28875255584716797, 0.2847119867801666, 0.2816084921360016, 0.2772996723651886, 0.2735859155654907, 0.26923641562461853, 0.26689934730529785, 0.26522448658943176, 0.2616587281227112, 0.25847962498664856, 0.25636881589889526, 0.25256264209747314, 0.2506827414035797, 0.24839568138122559, 0.24542482197284698, 0.24423310160636902, 0.24101795256137848, 0.2367660254240036, 0.23434001207351685, 0.23526161909103394, 0.23145757615566254, 0.23102650046348572, 0.22993816435337067, 0.2246675342321396, 0.22478565573692322, 0.22126084566116333, 0.22220726311206818, 0.21816541254520416, 0.2167397439479828, 0.21478331089019775, 0.21410758793354034, 0.21226660907268524, 0.2080700397491455, 0.2098434567451477, 0.20828299224376678, 0.20830300450325012, 0.20559455454349518, 0.20556055009365082, 0.20194664597511292, 0.20054449141025543, 0.19729045033454895, 0.19774632155895233, 0.1984953135251999, 0.1969144344329834, 0.1975109577178955, 0.19240595400333405, 0.19132302701473236, 0.19198331236839294, 0.19153355062007904, 0.18891006708145142, 0.1886734664440155, 0.18519845604896545, 0.18592648208141327, 0.18313336372375488, 0.18552148342132568, 0.18375836312770844, 0.1809520572423935, 0.18339385092258453, 0.17931662499904633, 0.179604634642601, 0.17726361751556396, 0.17782184481620789, 0.1780945062637329, 0.1746397614479065, 0.17776452004909515, 0.1732551008462906, 0.17287185788154602, 0.17268694937229156, 0.17198818922042847, 0.1700018346309662, 0.17085938155651093, 0.16904225945472717, 0.16875053942203522, 0.16780148446559906, 0.1666741520166397, 0.1661575734615326, 0.16544100642204285, 0.16386407613754272, 0.1656184196472168, 0.16194449365139008, 0.16140079498291016, 0.1652117669582367, 0.15940265357494354, 0.16087131202220917, 0.16147777438163757, 0.15960879623889923, 0.15718263387680054, 0.15974000096321106, 0.15891405940055847, 0.15804557502269745, 0.15836238861083984, 0.155405193567276, 0.1573062539100647, 0.15651123225688934, 0.154825359582901, 0.15374760329723358, 0.15466316044330597, 0.15358155965805054, 0.15189532935619354, 0.15269489586353302, 0.1528674066066742, 0.1521761566400528, 0.15177899599075317, 0.14920982718467712, 0.14793294668197632, 0.15049996972084045, 0.14858466386795044, 0.14964459836483002, 0.14669527113437653, 0.1478821039199829, 0.14571541547775269, 0.14701920747756958, 0.14712506532669067, 0.1465688943862915, 0.1476239264011383, 0.14481954276561737, 0.1462935507297516, 0.14541150629520416, 0.14497776329517365, 0.14225536584854126, 0.1434718668460846, 0.14165762066841125, 0.14300575852394104, 0.14387920498847961, 0.1405002474784851, 0.14184781908988953, 0.14072217047214508, 0.14026311039924622, 0.14005917310714722, 0.13917948305606842, 0.13919004797935486, 0.14209553599357605, 0.1387375444173813, 0.1392144411802292], "moving_avg_accuracy_train": [0.031197368724621626, 0.07887443691745107, 0.14252648686957822, 0.2044255618929378, 0.2626714307163332, 0.31641067549538276, 0.36674035000356875, 0.41340875066328014, 0.45613099924151657, 0.4955642725178079, 0.5322352859152519, 0.5658018118872188, 0.5966161879036463, 0.6243327421303083, 0.650149499640238, 0.6740936436396232, 0.6960294200390884, 0.7157739078985978, 0.7338972253959288, 0.7507451042256142, 0.7661873932735678, 0.7800062541619271, 0.7932290571614321, 0.8051134119657761, 0.8164022442361144, 0.8266782344269825, 0.8361219741475826, 0.844867841718751, 0.8527993961577562, 0.8602750498766982, 0.8673472241987368, 0.873795778299258, 0.879457967351715, 0.8849652364964807, 0.8901867014957805, 0.8949558105082549, 0.8994736562004619, 0.9033932329484483, 0.90723710830855, 0.910573327196918, 0.9139478759083355, 0.9170965768914683, 0.9200560018608208, 0.9228916534915992, 0.9256783636961492, 0.9282212080147495, 0.9305377057360228, 0.932722571132797, 0.9346308933672932, 0.9365621889223598, 0.9382423703969566, 0.9400914279597433, 0.9418394293186694, 0.9434312317321789, 0.9450682867554818, 0.9464812905538631, 0.9478135199390914, 0.949056668164359, 0.9501941388063948, 0.9514410406211226, 0.9526677758043499, 0.9537859325573866, 0.9546851725946066, 0.9556944514257237, 0.9566191144642144, 0.957486188430999, 0.958359560953486, 0.9591013462987058, 0.9598388157201457, 0.9605745817637181, 0.9611600773410377, 0.9618800107118157, 0.962527950745516, 0.963176200942513, 0.963759662168629, 0.9642451776447339, 0.9648610698851056, 0.9654410576848009, 0.9660838823449845, 0.9667322150522543, 0.9673296293328354, 0.9679045406151295, 0.9684359116620513, 0.9689513479852334, 0.9694594185034783, 0.9699050201770323, 0.970317615329641, 0.9706565070301117, 0.9710544073664601, 0.9714451778989632, 0.9718317846591776, 0.9721005675373906, 0.9724355501778006, 0.9727554555005521, 0.9729761572684648, 0.973228303331024, 0.9735737452789754, 0.9739450608523607, 0.9743071827029404, 0.974544628567244, 0.9748258673070496, 0.9751510617859699, 0.9754808671003131, 0.9757659940415369, 0.9759180126410285, 0.9761246919913129, 0.9763617485339221, 0.9765426915342121, 0.9768449049677694, 0.9770820558746469, 0.9772885162444082, 0.9774045761129075, 0.9776392022790716, 0.9779666232690954, 0.9781449726220032, 0.9783009448884574, 0.9785575773687424, 0.9788025335426747, 0.9789949841670432, 0.9790797980253942, 0.9792770742848242, 0.9794941143992543, 0.9796336829796315, 0.9798731548960002, 0.980009660610027, 0.9802255217050321, 0.9803896418536504, 0.9805163875993023, 0.9806676611513414, 0.9807920734576727, 0.9809343075167135, 0.9811205189877258, 0.9812161017961791, 0.9813671583928163, 0.9815077956762275, 0.9816111537920211, 0.9816808525105026, 0.9817900482845078, 0.9819069977692261, 0.9819703635780823, 0.9820971833191574, 0.9821415305730206, 0.9823139405348215, 0.9824830243444806, 0.9826189597803261, 0.9826668608618634, 0.9827379457185988, 0.9829019034884702, 0.9830400927884788, 0.983201665539439, 0.9833773800474644, 0.9834656965427642, 0.9835963346623434, 0.983646551752126, 0.9836894219841209, 0.9838441544869362, 0.9839300074144884, 0.984007347146923, 0.9840722665596763, 0.9840912385990298, 0.9841245534272959, 0.9842405672786877, 0.9842915013223212, 0.9844094936723242, 0.9844202475396803, 0.9845763022488444, 0.9846679594109109, 0.9848318310651041, 0.9848863816991344, 0.9849610178578478, 0.9850398521935563], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.0313692759318524, 0.07775127364928462, 0.13948783561982303, 0.20011283547275036, 0.25655212962690604, 0.30805344316082683, 0.35635789741402124, 0.40086744488082693, 0.44088029800193096, 0.4777759196042228, 0.5115048223727162, 0.5424163283225681, 0.5707229059101155, 0.5959170345114986, 0.6193252016364029, 0.6404434391911361, 0.6596726680788749, 0.6768060166230205, 0.6924478158925709, 0.7068459065736451, 0.7195468110217023, 0.7312705937749537, 0.7422411553500187, 0.7519661173752578, 0.7611224447378826, 0.7695706588954949, 0.7771394895609154, 0.7839504076511341, 0.7900812634409906, 0.7958553813081114, 0.8011120930361104, 0.80576886189515, 0.8098297945591741, 0.8140227728404555, 0.8176754124897684, 0.8210217643130807, 0.8242786510877215, 0.8270968973776692, 0.8295621358684414, 0.8317177563365672, 0.83367105129779, 0.8356253587715501, 0.8373985015465035, 0.8389587384588713, 0.8405704712112522, 0.842191929125895, 0.8434803428115736, 0.8445646139238651, 0.8454061805811773, 0.8462235962203487, 0.8469013236653319, 0.8477187978970667, 0.8485368853983992, 0.8492152175193274, 0.8498358644420935, 0.8505785816499926, 0.8511239273159421, 0.8518121099326159, 0.8521527715861916, 0.8526180584806599, 0.8529493084496119, 0.8534081838452381, 0.8536604212777323, 0.8543146810607272, 0.8546614332577418, 0.8547527541638953, 0.8548094994082738, 0.8549958769806242, 0.8550262509260105, 0.8553018461364968, 0.8551927893937056, 0.8552777437939435, 0.8553175816604076, 0.8554988906065656, 0.855529850331677, 0.8555800691294582, 0.8558857317383497, 0.8558291792252828, 0.8559868310034322, 0.8562650539648359, 0.8563414676666203, 0.856557753881886, 0.8565693060068752, 0.8566529451068654, 0.8569245623055163, 0.8569726757756423, 0.8568186063814365, 0.8565934651992416, 0.8565739436040163, 0.8564831319808134, 0.8565855364973405, 0.8566288724372149, 0.856792004112921, 0.8570029463032856, 0.8571144045438154, 0.8570835281425212, 0.8572845844491275, 0.8574920082048925, 0.857654275522581, 0.857677216287341, 0.8580427483766039, 0.8583462836857808, 0.8583732658217208, 0.8583222485392475, 0.8582916285422505, 0.8584593830449532, 0.8585116763387259, 0.8585221192093714, 0.8586800611852715, 0.8586105713976931, 0.8585968587138726, 0.8582549274546841, 0.8579207162415953, 0.8578050906358846, 0.8578628075229738, 0.8577417952665349, 0.8577559840568996, 0.8578694987442066, 0.8579573959142137, 0.8578513388811507, 0.858283878421123, 0.8585256501234383, 0.8585967602805221, 0.8586729664531476, 0.8587405224998509, 0.8587657313567935, 0.8588362179443823, 0.8589739275693717, 0.8589635888881122, 0.858870893873548, 0.8586368659508016, 0.8589043740563991, 0.8591471903687562, 0.8591571760099679, 0.8592007251634891, 0.8591788842454082, 0.8590473051205662, 0.8589899190644583, 0.8591732937336902, 0.85899344553502, 0.8590279241648764, 0.8590955760254972, 0.8590242443736252, 0.8591187372931904, 0.8593726203409797, 0.8594577192349692, 0.8594112084183999, 0.8596145188171472, 0.8596245407212006, 0.859644737957439, 0.8597260096436228, 0.8597106164164593, 0.8597232355918314, 0.8596359070910067, 0.859350821417674, 0.8595347269453344, 0.8595659645764786, 0.8594831856545988, 0.8593211763888378, 0.8593717100583124, 0.8594416044233396, 0.859617461159093, 0.8597482296327922, 0.8599788730663503, 0.8599311340089623, 0.8598972873625841, 0.8599034464745937, 0.8600687105903121, 0.860106555504549, 0.8600663442312025, 0.86011766232126], "moving_var_accuracy_train": [0.008759482338059997, 0.028341459587427384, 0.06197156479665747, 0.09025786771571925, 0.11176531205907698, 0.12657993871797274, 0.136719630071075, 0.14264912364518567, 0.1448109259928927, 0.14432468076514746, 0.14199508170099237, 0.13793597852333367, 0.13268811259453642, 0.1263331677388782, 0.11969839567992897, 0.11288845439870572, 0.10593021353506113, 0.09884579538906385, 0.09191730758413906, 0.08528023601526301, 0.07889839103310162, 0.07272720017605791, 0.06702806283092558, 0.06159639754987284, 0.05658369740113602, 0.051875691430647776, 0.047490780266776944, 0.043430114036250966, 0.039653288634996016, 0.036190928358226196, 0.033021976369175296, 0.030094033382143916, 0.0273731735077214, 0.02490882627784526, 0.022663316920710958, 0.020601684835595643, 0.018725214719323417, 0.016990960984341268, 0.01542484328596311, 0.013982532165606729, 0.012686767160097619, 0.011507319305018493, 0.010435411139859694, 0.00946423830741395, 0.008587706260549841, 0.007787130149552597, 0.007056712589831316, 0.006394004062066378, 0.005787378899615794, 0.005242210132343393, 0.004743396207197259, 0.004299827711312025, 0.0038973445189380866, 0.0035304145813571724, 0.0032014926655653487, 0.002899312616616974, 0.002625354871169069, 0.002376728141642038, 0.0021506998826312756, 0.0019496227715882916, 0.0017682044073173728, 0.0016026364373048912, 0.0014496504873752567, 0.0013138532324681997, 0.0011901629248341383, 0.0010779129877256039, 0.0009769867050203618, 0.0008842402440037723, 0.0008007109699314246, 0.0007255120379761491, 0.0006560460798180814, 0.0005951062083615126, 0.0005393740241108053, 0.0004892186765608838, 0.00044336065192622243, 0.0004011461142314372, 0.00036444541207404256, 0.0003310283434967982, 0.00030164452104078, 0.00027526308663054316, 0.00025094891237126943, 0.0002288287279767245, 0.00020848705188461389, 0.0001900294181254517, 0.00017334969717649264, 0.0001578017751221103, 0.0001435537104495051, 0.00013023196766638617, 0.00011863369299874293, 0.00010814463818052354, 9.867535744586257e-05, 8.945801982186e-05, 8.152213816405891e-05, 7.429097908737577e-05, 6.730026461187619e-05, 6.11424368824656e-05, 5.6102164448859755e-05, 5.1732825299319274e-05, 4.7739732881392975e-05, 4.3473184439527665e-05, 3.983772305448231e-05, 3.6805713791116685e-05, 3.410408632032617e-05, 3.142535404179816e-05, 2.849080552894076e-05, 2.602617216055239e-05, 2.3929317184041667e-05, 2.1831048789822868e-05, 2.0469940545643508e-05, 1.8929111464774715e-05, 1.741983327683469e-05, 1.5799078986836046e-05, 1.471461602879191e-05, 1.4207994968286322e-05, 1.3073471896601166e-05, 1.1985070838066671e-05, 1.1379305823694721e-05, 1.0781406985653363e-05, 1.003660147246669e-05, 9.097681840335555e-06, 8.53817495911403e-06, 8.108315164649125e-06, 7.472798145840985e-06, 7.2416395198206285e-06, 6.685179857496214e-06, 6.436025982777625e-06, 6.034842193142144e-06, 5.575938330195751e-06, 5.224297685095032e-06, 4.841173714285694e-06, 4.539131090818067e-06, 4.397290389165342e-06, 4.039786009695243e-06, 3.841170267214152e-06, 3.6350628498604105e-06, 3.3677026657780703e-06, 3.074653601421982e-06, 2.8745016948249573e-06, 2.7101461631253414e-06, 2.4752685784009125e-06, 2.3724909410980786e-06, 2.1529419573150973e-06, 2.205174515937427e-06, 2.2419610765436416e-06, 2.184070953355587e-06, 1.9863144805320607e-06, 1.8331605441927018e-06, 1.8917838424842465e-06, 1.8744720019677519e-06, 1.9219765864465104e-06, 2.0076592227775622e-06, 1.8770915305780026e-06, 1.842979242105028e-06, 1.681377122850672e-06, 1.5297801216872435e-06, 1.5922814363659106e-06, 1.4993898192529175e-06, 1.4032837452450024e-06, 1.300886142090553e-06, 1.1740369723765796e-06, 1.0666221751805175e-06, 1.08109288109545e-06, 9.963320841937753e-07, 1.021998627707509e-06, 9.208395759047505e-07, 1.0479332685849943e-06, 1.0187492599492084e-06, 1.1585596053862715e-06, 1.0694855899057059e-06, 1.0126720366025048e-06, 9.673385053214971e-07], "duration": 89119.544685, "accuracy_train": [0.3119736872462163, 0.5079680506529162, 0.7153949364387228, 0.7615172371031745, 0.7868842501268919, 0.800063878506829, 0.8197074205772426, 0.8334243566006828, 0.8406312364456442, 0.8504637320044297, 0.8622744064922481, 0.8679005456349206, 0.8739455720514949, 0.8737817301702658, 0.8825003172296051, 0.8895909396340901, 0.8934514076342747, 0.8934742986341824, 0.8970070828719084, 0.9023760136927832, 0.9051679947051495, 0.9043760021571613, 0.9122342841569768, 0.9120726052048725, 0.9180017346691584, 0.9191621461447952, 0.9211156316329827, 0.9235806498592655, 0.924183386108804, 0.9275559333471761, 0.9309967930970838, 0.9318327652039498, 0.9304176688238279, 0.9345306587993725, 0.9371798864894795, 0.9378777916205242, 0.9401342674303249, 0.9386694236803249, 0.9418319865494648, 0.9405992971922297, 0.9443188143110927, 0.945434885739664, 0.9466908265849945, 0.9484125181686047, 0.9507587555370985, 0.9511068068821521, 0.9513861852274824, 0.9523863597037652, 0.9518057934777593, 0.9539438489179586, 0.9533640036683279, 0.9567329460248246, 0.9575714415490033, 0.9577574534537652, 0.9598017819652085, 0.9591983247392949, 0.9598035844061462, 0.9602450021917681, 0.9604313745847176, 0.9626631569536729, 0.963708392453396, 0.9638493433347176, 0.9627783329295865, 0.9647779609057769, 0.9649410818106312, 0.9652898541320598, 0.9662199136558692, 0.9657774144056847, 0.9664760405131044, 0.9671964761558692, 0.966429537536914, 0.9683594110488187, 0.9683594110488187, 0.9690104527154854, 0.9690108132036729, 0.9686148169296788, 0.9704041000484496, 0.9706609478820598, 0.9718693042866371, 0.9725672094176817, 0.9727063578580657, 0.9730787421557769, 0.9732182510843485, 0.9735902748938722, 0.9740320531676817, 0.973915435239018, 0.9740309717031194, 0.9737065323343485, 0.9746355103935955, 0.9749621126914912, 0.9753112455011074, 0.9745196134413067, 0.9754503939414912, 0.9756346034053157, 0.9749624731796788, 0.9754976178940569, 0.9766827228105389, 0.9772869010128276, 0.977566279358158, 0.9766816413459765, 0.9773570159653008, 0.9780778120962532, 0.9784491149294019, 0.9783321365125508, 0.9772861800364526, 0.9779848061438722, 0.9784952574174051, 0.9781711785368217, 0.979564825869786, 0.9792164140365448, 0.9791466595722591, 0.9784491149294019, 0.9797508377745479, 0.9809134121793098, 0.9797501167981728, 0.9797046952865448, 0.9808672696913067, 0.9810071391080657, 0.9807270397863603, 0.9798431227505537, 0.9810525606196937, 0.9814474754291252, 0.9808898002030271, 0.9820284021433187, 0.981238212036268, 0.9821682715600776, 0.9818667231912146, 0.9816570993101699, 0.9820291231196937, 0.9819117842146549, 0.9822144140480805, 0.9827964222268365, 0.9820763470722591, 0.9827266677625508, 0.9827735312269288, 0.9825413768341639, 0.9823081409768365, 0.9827728102505537, 0.9829595431316908, 0.9825406558577889, 0.9832385609888336, 0.9825406558577889, 0.98386563019103, 0.9840047786314139, 0.9838423787029347, 0.9830979705956996, 0.9833777094292175, 0.9843775234173128, 0.9842837964885567, 0.9846558202980805, 0.9849588106196937, 0.9842605450004615, 0.9847720777385567, 0.9840985055601699, 0.9840752540720746, 0.9852367470122739, 0.9847026837624585, 0.9847034047388336, 0.9846565412744556, 0.9842619869532114, 0.9844243868816908, 0.9852846919412146, 0.9847499077150241, 0.9854714248223514, 0.9845170323458842, 0.9859807946313216, 0.9854928738695091, 0.9863066759528424, 0.985377337405408, 0.985632743286268, 0.9857493612149317], "end": "2016-02-02 10:32:57.747000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0], "moving_var_accuracy_valid": [0.008856283252398251, 0.02733226233749742, 0.058901663859226835, 0.0860900129378111, 0.1061495569656038, 0.11940606893051639, 0.12846534474375532, 0.13344870861067826, 0.13451309348362986, 0.13331336617604242, 0.1302207794961365, 0.1257983923473124, 0.12042991412505974, 0.114099619756401, 0.10762113837408799, 0.10087284415344248, 0.09411342893065167, 0.0873440507286031, 0.08081163861524296, 0.07459621989106259, 0.06858841466614445, 0.06296659693793892, 0.05775311623559563, 0.05282897858956715, 0.048300625707554404, 0.0441129140388749, 0.04021720741376368, 0.03661298411947233, 0.033289972241969244, 0.030261038952062953, 0.02748363222057821, 0.024930438464379076, 0.022585815184856816, 0.020485463268176813, 0.018556992929028723, 0.016802076270854324, 0.015217334446934358, 0.01376708361159816, 0.01244507185778581, 0.011242384968430653, 0.010152484722437434, 0.009171610109511645, 0.008282745416263806, 0.007476379927641858, 0.00675212107706355, 0.006100571101277817, 0.00550545407957903, 0.004965489466225674, 0.004475314629551405, 0.004033796681540723, 0.003634550843793802, 0.003277110136490377, 0.002955422527279867, 0.0026640214847484262, 0.0024010861596982344, 0.0021659422033865955, 0.0019520246001062654, 0.0017610844979206676, 0.0015860205013885536, 0.0014293668782971727, 0.0012874177293448323, 0.0011605710560687486, 0.0010450865639630355, 0.0009444304103395321, 0.0008510695030807892, 0.0007660376083438166, 0.0006894628277142712, 0.0006208291743381209, 0.0005587545600933339, 0.0005035626785643873, 0.00045331345106628274, 0.0004080470612107325, 0.0003672566385900989, 0.00033082683113670145, 0.0002977527745642421, 0.0002680001944566731, 0.0002420410416852752, 0.00021786572119735534, 0.00019630283582600292, 0.00017736922438967316, 0.00015968485343508952, 0.00014413738563380616, 0.00012972484813475143, 0.00011681532281270082, 0.00010579777365485692, 9.523883044343941e-05, 8.592858380317396e-05, 7.779192239013744e-05, 7.0016159985245e-05, 6.30887647448991e-05, 5.68742684354555e-05, 5.120374362507312e-05, 4.632287675513433e-05, 4.209105854870295e-05, 3.799375914827079e-05, 3.420296340285563e-05, 3.1146479808405595e-05, 2.841905335766584e-05, 2.5814124163407543e-05, 2.323744825525674e-05, 2.2116226804259076e-05, 2.0733807279087558e-05, 1.866697887211776e-05, 1.682370585290462e-05, 1.5149773525559046e-05, 1.3888070331596309e-05, 1.2523874595599109e-05, 1.1272468617965062e-05, 1.0369732765929935e-05, 9.37621896453611e-06, 8.440289407360553e-06, 8.648513340716387e-06, 8.788936221233935e-06, 8.03036612537432e-06, 7.257310664334353e-06, 6.663375293776956e-06, 5.998849660347349e-06, 5.5149349524223205e-06, 5.0329746696373265e-06, 4.630910051032621e-06, 5.851633128684231e-06, 5.7925518201802905e-06, 5.258806528126721e-06, 4.785192302030102e-06, 4.347747446842712e-06, 3.918692080373627e-06, 3.571538103605386e-06, 3.3850597605775627e-06, 3.047515779491468e-06, 2.8200954930679002e-06, 3.031007561385937e-06, 3.371952084290907e-06, 3.565394729782002e-06, 3.209752674077487e-06, 2.905846165621523e-06, 2.619554780382895e-06, 2.513416897192349e-06, 2.2917136423936045e-06, 2.3651787019973656e-06, 2.419769202881966e-06, 2.188491265844759e-06, 2.010833107469369e-06, 1.8555436377515373e-06, 1.7503494806079676e-06, 2.15542395014063e-06, 2.0050579509504987e-06, 1.8240214603768508e-06, 2.013635378488461e-06, 1.8131757876873048e-06, 1.6355295640835881e-06, 1.5314223904516771e-06, 1.3804127143890692e-06, 1.2438046352338186e-06, 1.1880605752170633e-06, 1.8007190879508481e-06, 1.925038367091913e-06, 1.7413166367782765e-06, 1.62885612226867e-06, 1.7021935297738293e-06, 1.5549570425516077e-06, 1.4434283386594297e-06, 1.5774158283820211e-06, 1.5735777889662054e-06, 1.8949875510609325e-06, 1.725999954357535e-06, 1.5637103181612117e-06, 1.407680698291819e-06, 1.5127226799601283e-06, 1.3743405497664745e-06, 1.2514590133271293e-06, 1.1500150292987299e-06], "accuracy_test": 0.8051000478316327, "start": "2016-02-01 09:47:38.202000", "learning_rate_per_epoch": [0.0017377625918015838, 0.0008688812959007919, 0.000579254177864641, 0.00043444064795039594, 0.00034755253000184894, 0.0002896270889323205, 0.00024825180298648775, 0.00021722032397519797, 0.00019308473565615714, 0.00017377626500092447, 0.00015797841479070485, 0.00014481354446616024, 0.00013367403880693018, 0.00012412590149324387, 0.00011585083848331124, 0.00010861016198759899, 0.00010222133278148249, 9.654236782807857e-05, 9.146118827629834e-05, 8.688813250046223e-05, 8.275060099549592e-05, 7.898920739535242e-05, 7.555489719379693e-05, 7.240677223308012e-05, 6.951050454517826e-05, 6.683701940346509e-05, 6.436157855205238e-05, 6.206295074662194e-05, 5.9922847867710516e-05, 5.792541924165562e-05, 5.605685873888433e-05, 5.430508099379949e-05, 5.265947402222082e-05, 5.1110666390741244e-05, 4.965035986970179e-05, 4.8271183914039284e-05, 4.6966557420091704e-05, 4.573059413814917e-05, 4.45580153609626e-05, 4.344406625023112e-05, 4.2384453990962356e-05, 4.137530049774796e-05, 4.0413084207102656e-05, 3.949460369767621e-05, 3.861694494844414e-05, 3.7777448596898466e-05, 3.697367355925962e-05, 3.620338611654006e-05, 3.546454172465019e-05, 3.475525227258913e-05, 3.407377516850829e-05, 3.3418509701732546e-05, 3.278797521488741e-05, 3.218078927602619e-05, 3.159568223054521e-05, 3.103147537331097e-05, 3.0487062758766115e-05, 2.9961423933855258e-05, 2.9453603929141536e-05, 2.896270962082781e-05, 2.848791154974606e-05, 2.8028429369442165e-05, 2.7583533665165305e-05, 2.7152540496899746e-05, 2.673480958037544e-05, 2.632973701111041e-05, 2.593675526441075e-05, 2.5555333195370622e-05, 2.5184965124935843e-05, 2.4825179934850894e-05, 2.4475530153722502e-05, 2.4135591957019642e-05, 2.3804966986062936e-05, 2.3483278710045852e-05, 2.317016878805589e-05, 2.2865297069074586e-05, 2.256834522995632e-05, 2.22790076804813e-05, 2.1996995201334357e-05, 2.172203312511556e-05, 2.1453859517350793e-05, 2.1192226995481178e-05, 2.0936899090884253e-05, 2.068765024887398e-05, 2.0444265828700736e-05, 2.0206542103551328e-05, 1.9974282622570172e-05, 1.9747301848838106e-05, 1.9525423340382986e-05, 1.930847247422207e-05, 1.9096292817266658e-05, 1.8888724298449233e-05, 1.868561957962811e-05, 1.848683677962981e-05, 1.829223765525967e-05, 1.810169305827003e-05, 1.791507747839205e-05, 1.7732270862325095e-05, 1.7553156794747338e-05, 1.7377626136294566e-05, 1.7205569747602567e-05, 1.7036887584254146e-05, 1.687148142082151e-05, 1.6709254850866273e-05, 1.6550120562897064e-05, 1.6393987607443705e-05, 1.6240772310993634e-05, 1.6090394638013095e-05, 1.5942776371957734e-05, 1.5797841115272604e-05, 1.565551974636037e-05, 1.5515737686655484e-05, 1.537842945253942e-05, 1.5243531379383057e-05, 1.5110978893062565e-05, 1.4980711966927629e-05, 1.4852671483822633e-05, 1.4726801964570768e-05, 1.4603047020500526e-05, 1.4481354810413904e-05, 1.4361674402607605e-05, 1.424395577487303e-05, 1.4128151633485686e-05, 1.4014214684721082e-05, 1.3902100363338832e-05, 1.3791766832582653e-05, 1.368317043670686e-05, 1.3576270248449873e-05, 1.3471028069034219e-05, 1.336740479018772e-05, 1.3265363122627605e-05, 1.3164868505555205e-05, 1.306588455918245e-05, 1.2968377632205375e-05, 1.2872315892309416e-05, 1.2777666597685311e-05, 1.2684398825513199e-05, 1.2592482562467922e-05, 1.2501889614213724e-05, 1.2412589967425447e-05, 1.232455724675674e-05, 1.2237765076861251e-05, 1.2152186172897927e-05, 1.2067795978509821e-05, 1.1984569937339984e-05, 1.1902483493031468e-05, 1.182151390821673e-05, 1.1741639355022926e-05, 1.166283618658781e-05, 1.1585084394027945e-05, 1.1508361239975784e-05, 1.1432648534537293e-05, 1.1357925359334331e-05, 1.128417261497816e-05, 1.1211371202080045e-05, 1.113950384024065e-05, 1.1068551430071238e-05, 1.0998497600667179e-05, 1.0929324162134435e-05, 1.086101656255778e-05, 1.0793556612043176e-05, 1.0726929758675396e-05, 1.0661120541044511e-05, 1.0596113497740589e-05, 1.0531894076848403e-05, 1.0468449545442127e-05, 1.0405764442111831e-05, 1.034382512443699e-05, 1.0282618859491777e-05, 1.0222132914350368e-05, 1.0162354556086939e-05], "accuracy_train_first": 0.3119736872462163, "accuracy_train_last": 0.9857493612149317, "batch_size_eval": 1024, "accuracy_train_std": [0.013156716035381884, 0.017020403666816092, 0.01916812159359841, 0.019564929468302194, 0.015459191839330885, 0.0151671763176241, 0.014876947446365274, 0.013510786849892069, 0.012928140511190343, 0.013332793347925968, 0.013931495083308225, 0.012685220541554526, 0.013332817471171618, 0.013255706146615993, 0.013171800994201635, 0.012399042451843763, 0.011809605712136092, 0.01245546621393627, 0.014108743475840764, 0.013035819661604622, 0.011894860060145163, 0.012143128890032598, 0.012084773880408503, 0.01152163952867652, 0.011149039588221436, 0.011638775898962042, 0.011342907238794575, 0.011906409603210379, 0.011864101799051967, 0.011542595097401857, 0.011030489821421668, 0.011425821335506885, 0.009732792938113796, 0.010519090329611517, 0.010428309650738477, 0.010564913709482383, 0.009815249705750598, 0.009125178228615322, 0.009316442434946356, 0.009617281786716496, 0.009843417582583421, 0.009641612516280111, 0.008014181174093665, 0.009216557444455336, 0.008145235136090686, 0.0084740813278616, 0.008620256486862007, 0.008622657819245002, 0.008318960294585226, 0.007748587240193488, 0.008201380755484782, 0.008622941851303407, 0.007221524721301722, 0.00749860625109074, 0.007660497120278904, 0.007127782314406944, 0.007168451388036498, 0.008272882913718894, 0.006828332991892381, 0.00614334933637464, 0.007228313772507722, 0.0072271529917466125, 0.007354277239975203, 0.0063959672545147745, 0.005925046084240909, 0.006628056172685308, 0.006637230870221217, 0.007151824926796651, 0.0060363814598963115, 0.006574720492490477, 0.0069042261428567024, 0.006077633866183375, 0.006207019462301491, 0.006155095914296903, 0.005970296087367143, 0.0055123523921279125, 0.005948659430069017, 0.004760819877690173, 0.005226969390575349, 0.004816623645220606, 0.005106471012179096, 0.005391588752913585, 0.00540410603703745, 0.005878052647362211, 0.005589070710385416, 0.004776266974334857, 0.005655802883245544, 0.006077175622782037, 0.00530936765206043, 0.004986150211865765, 0.0048292345645059385, 0.005866921429766997, 0.0050344822899144005, 0.004916002697850247, 0.0059978065833617335, 0.004563832249009867, 0.004553324031244321, 0.005041041992675165, 0.004568913090108756, 0.004660037886750409, 0.004842914179085239, 0.00457201792359503, 0.004444473092399511, 0.005043544677636732, 0.004979364108328015, 0.004776131712225403, 0.004606400328069989, 0.0045831239759181325, 0.0045706168534856186, 0.0046039889822813715, 0.004795060483669513, 0.004609986132050904, 0.005126254218425936, 0.0044733948777414285, 0.004992611255511116, 0.004707289430004848, 0.004809209102426438, 0.004581124625459652, 0.0047731739045715595, 0.004725082815254598, 0.003682756379155369, 0.004030164572855234, 0.004053186813388359, 0.004628388158369806, 0.004476242213894352, 0.003827839388857684, 0.003724553207212536, 0.00413256272017531, 0.004607881681717401, 0.004634166788305607, 0.004571430291419456, 0.004145126402977602, 0.004008841403012993, 0.003963347359708483, 0.00411037035315469, 0.003884070641804453, 0.0046330031652847, 0.004295768135020871, 0.004226706293243635, 0.004007416924372923, 0.0035343627727445846, 0.004528782457678852, 0.0047873060000784146, 0.004227028242862894, 0.0040412724897562575, 0.0047030607010808256, 0.004234392124697112, 0.004247246644386996, 0.0049442055427168985, 0.004287202142979092, 0.004252562359220096, 0.004381230174503314, 0.004156861510044084, 0.004118685939901494, 0.004239335954595561, 0.00409716311508092, 0.00404342013831958, 0.003803957512904257, 0.004090683591301969, 0.004392977908788967, 0.0038885035837219004, 0.0038078495950966467, 0.0038882621539673367, 0.0038912265887337833, 0.003615095213938202, 0.003801477793860624, 0.0038237122921769, 0.004064653274743922, 0.004052326961802383, 0.004068782152976181, 0.004475830485668217], "accuracy_test_std": 0.008389930846748654, "error_valid": [0.6863072406814759, 0.5048107468938253, 0.30488310664533136, 0.25426216585090367, 0.23549422298569278, 0.22843473503388556, 0.20890201430722888, 0.19854662791792166, 0.19900402390813254, 0.19016348597515065, 0.18493505271084332, 0.1793801181287651, 0.17451789580195776, 0.1773358080760542, 0.17000129423945776, 0.1694924228162651, 0.16726427193147586, 0.16899384647966864, 0.16677599068147586, 0.16357127729668675, 0.1661450489457832, 0.1632153614457832, 0.15902379047439763, 0.1605092243975903, 0.15647060899849397, 0.15439541368599397, 0.15474103445030118, 0.15475132953689763, 0.15474103445030118, 0.15217755788780118, 0.15157750141189763, 0.15232021837349397, 0.1536218114646084, 0.14824042262801207, 0.14945083066641573, 0.1488610692771084, 0.14640936794051207, 0.14753888601280118, 0.1482507177146084, 0.14888165945030118, 0.14874929405120485, 0.1467858739646084, 0.14664321347891573, 0.1469991293298193, 0.1449239340173193, 0.1432149496423193, 0.1449239340173193, 0.14567694606551207, 0.14701971950301207, 0.1464196630271084, 0.1469991293298193, 0.1449239340173193, 0.1441003270896084, 0.1446797933923193, 0.14457831325301207, 0.14273696347891573, 0.14396796169051207, 0.1419942465173193, 0.1447812735316265, 0.1431943594691265, 0.1440694418298193, 0.1424619375941265, 0.1440694418298193, 0.1397969808923193, 0.1422177969691265, 0.14442535768072284, 0.1446797933923193, 0.14332672486822284, 0.14470038356551207, 0.1422177969691265, 0.14578872129141573, 0.14395766660391573, 0.14432387754141573, 0.14286932887801207, 0.1441915121423193, 0.14396796169051207, 0.1413633047816265, 0.1446797933923193, 0.14259430299322284, 0.14123093938253017, 0.1429708090173193, 0.14149567018072284, 0.14332672486822284, 0.14259430299322284, 0.1406308829066265, 0.14259430299322284, 0.14456801816641573, 0.14543280544051207, 0.14360175075301207, 0.14433417262801207, 0.14249282285391573, 0.14298110410391573, 0.14173981080572284, 0.14109857398343373, 0.14188247129141573, 0.1431943594691265, 0.14090590879141573, 0.14064117799322284, 0.14088531861822284, 0.1421163168298193, 0.13866746282003017, 0.1389218985316265, 0.1413838949548193, 0.14213690700301207, 0.14198395143072284, 0.14003082643072284, 0.1410176840173193, 0.1413838949548193, 0.1398984610316265, 0.14201483669051207, 0.14152655544051207, 0.14482245387801207, 0.14508718467620485, 0.14323553981551207, 0.14161774049322284, 0.14334731504141573, 0.1421163168298193, 0.14110886907003017, 0.14125152955572284, 0.14310317441641573, 0.1378232657191265, 0.13929840455572284, 0.14076324830572284, 0.14064117799322284, 0.1406514730798193, 0.14100738893072284, 0.1405294027673193, 0.13978668580572284, 0.14112945924322284, 0.14196336125753017, 0.14346938535391573, 0.13868805299322284, 0.13866746282003017, 0.1407529532191265, 0.1404073324548193, 0.1410176840173193, 0.14213690700301207, 0.14152655544051207, 0.13917633424322284, 0.14262518825301207, 0.14066176816641573, 0.14029555722891573, 0.14161774049322284, 0.14003082643072284, 0.13834243222891573, 0.1397763907191265, 0.14100738893072284, 0.1385556875941265, 0.1402852621423193, 0.14017348691641573, 0.13954254518072284, 0.14042792262801207, 0.1401631918298193, 0.14115004941641573, 0.1432149496423193, 0.13881012330572284, 0.14015289674322284, 0.1412618246423193, 0.14213690700301207, 0.14017348691641573, 0.13992934629141573, 0.1387998282191265, 0.13907485410391573, 0.1379453360316265, 0.14049851750753017, 0.1404073324548193, 0.1400411215173193, 0.13844391236822284, 0.1395528402673193, 0.14029555722891573, 0.13942047486822284], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.08984076833400649, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 32, "valid_ratio": 0.15, "learning_rate": 0.0017377625969309538, "optimization": "rmsprop", "nb_data_augmentation": 1, "learning_rate_decay_method": "lin", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 2.7197886117624837e-07, "rotation_range": [0, 0], "momentum": 0.8107247979127878}, "accuracy_valid_max": 0.8621767342808735, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8605795251317772, "accuracy_valid_std": [0.013281161632981478, 0.01964385191413632, 0.010683497629493393, 0.015841416586246142, 0.021539629717217134, 0.018873187310998667, 0.015495177378308873, 0.020925164315739465, 0.02165910600328184, 0.019151994691907794, 0.01972306104224578, 0.018032925833862613, 0.020402520623186173, 0.01741081915470099, 0.022918053887162754, 0.01962789015901518, 0.018186716766657944, 0.01905930869383808, 0.02008734980653559, 0.015181156862347764, 0.016408975807186834, 0.017917433030456686, 0.01556894066702434, 0.017464891370576656, 0.016007361634964173, 0.01784070358506132, 0.01588903898075649, 0.01805330713714164, 0.017793149173497072, 0.016832530194379955, 0.017752666169223247, 0.016702936384569184, 0.015321051887104286, 0.017040915968469374, 0.014892534189046031, 0.015089601598916752, 0.018976535171526834, 0.013599069954643822, 0.01640750508756667, 0.016433633785730338, 0.016330105330734253, 0.016542847385892363, 0.015590996745980109, 0.014646643871517722, 0.016951012234853326, 0.01454682755075445, 0.014535518190031913, 0.013686648880554783, 0.01357323233168732, 0.016505793268852355, 0.015812923353548527, 0.015240148424923102, 0.016167785766910067, 0.015538795500861766, 0.015197172759920332, 0.01414122517453139, 0.01640559604677456, 0.015299874828555401, 0.01515040780893903, 0.016551256366455713, 0.015335686818785393, 0.015893842896466375, 0.014314472769123199, 0.014937069411962644, 0.013905757714839477, 0.01581352439380496, 0.017583089662583352, 0.01649582117341139, 0.016039729476284455, 0.014336298167867368, 0.013777182579986495, 0.015509186024492985, 0.014701854399394695, 0.015457138450842355, 0.013105059475157135, 0.013809557166163262, 0.016186414016606344, 0.015275730225692934, 0.01377325635763244, 0.01581401571424992, 0.015931829982800808, 0.014337994015569705, 0.01663972619267513, 0.013885317145149503, 0.015098819956845833, 0.016066384873244226, 0.0159029127961251, 0.014605990647870194, 0.01685862756722955, 0.014894882656410542, 0.01724834489148443, 0.018310074818817083, 0.015083258343511293, 0.01644505991634057, 0.01747152497203551, 0.014656612149161665, 0.015391551756445566, 0.016878826945040133, 0.014506574313234552, 0.013236510821901606, 0.015677494656609663, 0.015340333861674185, 0.01503221708091299, 0.016258045498462586, 0.015872450573375864, 0.016289546430762712, 0.017048257337644677, 0.01474396424032682, 0.015874622939545117, 0.014967561339534459, 0.013525777159399255, 0.01643082840634445, 0.016828591683505974, 0.015074841692041065, 0.015315249955752262, 0.015601155522063417, 0.017081917454401376, 0.015723736643372753, 0.014133107111080626, 0.016298666796124406, 0.014832581515962213, 0.014468747529187422, 0.015005068947162517, 0.01604607934805051, 0.015067169882044552, 0.016347020337351185, 0.015267312927117829, 0.017301210069029863, 0.015501442704700549, 0.015401784081505356, 0.017023636337143617, 0.014892181534776078, 0.015624177055266416, 0.014432203362469385, 0.015277769194285622, 0.01699923963584385, 0.01792501279875005, 0.017681808583880118, 0.014979448468262645, 0.014725243155236828, 0.01519139691620672, 0.01577689389346478, 0.014581576455576601, 0.013930619791258644, 0.017817972353156686, 0.016302112179443315, 0.016757533616811347, 0.0171228315611887, 0.01637896549665226, 0.0173742102914209, 0.017093484486505574, 0.01626848850054836, 0.0175258124904247, 0.01636146428697511, 0.014782568470510599, 0.016570091589039306, 0.013584416662498308, 0.01436900809099913, 0.016699328631872446, 0.015860544223326205, 0.016252735041030825, 0.016096416723864825, 0.01693837952798564, 0.01578498053934572, 0.014002671948231408, 0.015874724587336457, 0.016874072389528216, 0.015183769395557264, 0.01591130761103852, 0.016457365834269776, 0.015048659062179639], "accuracy_valid": [0.3136927593185241, 0.4951892531061747, 0.6951168933546686, 0.7457378341490963, 0.7645057770143072, 0.7715652649661144, 0.7910979856927711, 0.8014533720820783, 0.8009959760918675, 0.8098365140248494, 0.8150649472891567, 0.8206198818712349, 0.8254821041980422, 0.8226641919239458, 0.8299987057605422, 0.8305075771837349, 0.8327357280685241, 0.8310061535203314, 0.8332240093185241, 0.8364287227033133, 0.8338549510542168, 0.8367846385542168, 0.8409762095256024, 0.8394907756024097, 0.843529391001506, 0.845604586314006, 0.8452589655496988, 0.8452486704631024, 0.8452589655496988, 0.8478224421121988, 0.8484224985881024, 0.847679781626506, 0.8463781885353916, 0.8517595773719879, 0.8505491693335843, 0.8511389307228916, 0.8535906320594879, 0.8524611139871988, 0.8517492822853916, 0.8511183405496988, 0.8512507059487951, 0.8532141260353916, 0.8533567865210843, 0.8530008706701807, 0.8550760659826807, 0.8567850503576807, 0.8550760659826807, 0.8543230539344879, 0.8529802804969879, 0.8535803369728916, 0.8530008706701807, 0.8550760659826807, 0.8558996729103916, 0.8553202066076807, 0.8554216867469879, 0.8572630365210843, 0.8560320383094879, 0.8580057534826807, 0.8552187264683735, 0.8568056405308735, 0.8559305581701807, 0.8575380624058735, 0.8559305581701807, 0.8602030191076807, 0.8577822030308735, 0.8555746423192772, 0.8553202066076807, 0.8566732751317772, 0.8552996164344879, 0.8577822030308735, 0.8542112787085843, 0.8560423333960843, 0.8556761224585843, 0.8571306711219879, 0.8558084878576807, 0.8560320383094879, 0.8586366952183735, 0.8553202066076807, 0.8574056970067772, 0.8587690606174698, 0.8570291909826807, 0.8585043298192772, 0.8566732751317772, 0.8574056970067772, 0.8593691170933735, 0.8574056970067772, 0.8554319818335843, 0.8545671945594879, 0.8563982492469879, 0.8556658273719879, 0.8575071771460843, 0.8570188958960843, 0.8582601891942772, 0.8589014260165663, 0.8581175287085843, 0.8568056405308735, 0.8590940912085843, 0.8593588220067772, 0.8591146813817772, 0.8578836831701807, 0.8613325371799698, 0.8610781014683735, 0.8586161050451807, 0.8578630929969879, 0.8580160485692772, 0.8599691735692772, 0.8589823159826807, 0.8586161050451807, 0.8601015389683735, 0.8579851633094879, 0.8584734445594879, 0.8551775461219879, 0.8549128153237951, 0.8567644601844879, 0.8583822595067772, 0.8566526849585843, 0.8578836831701807, 0.8588911309299698, 0.8587484704442772, 0.8568968255835843, 0.8621767342808735, 0.8607015954442772, 0.8592367516942772, 0.8593588220067772, 0.8593485269201807, 0.8589926110692772, 0.8594705972326807, 0.8602133141942772, 0.8588705407567772, 0.8580366387424698, 0.8565306146460843, 0.8613119470067772, 0.8613325371799698, 0.8592470467808735, 0.8595926675451807, 0.8589823159826807, 0.8578630929969879, 0.8584734445594879, 0.8608236657567772, 0.8573748117469879, 0.8593382318335843, 0.8597044427710843, 0.8583822595067772, 0.8599691735692772, 0.8616575677710843, 0.8602236092808735, 0.8589926110692772, 0.8614443124058735, 0.8597147378576807, 0.8598265130835843, 0.8604574548192772, 0.8595720773719879, 0.8598368081701807, 0.8588499505835843, 0.8567850503576807, 0.8611898766942772, 0.8598471032567772, 0.8587381753576807, 0.8578630929969879, 0.8598265130835843, 0.8600706537085843, 0.8612001717808735, 0.8609251458960843, 0.8620546639683735, 0.8595014824924698, 0.8595926675451807, 0.8599588784826807, 0.8615560876317772, 0.8604471597326807, 0.8597044427710843, 0.8605795251317772], "seed": 422572788, "model": "residualv3", "loss_std": [0.27427542209625244, 0.18984198570251465, 0.18055400252342224, 0.173884317278862, 0.16994249820709229, 0.16842912137508392, 0.16528302431106567, 0.1645798534154892, 0.15979118645191193, 0.15867169201374054, 0.15643756091594696, 0.15102145075798035, 0.15098372101783752, 0.14896135032176971, 0.14656281471252441, 0.14439666271209717, 0.14350564777851105, 0.14072415232658386, 0.13999314606189728, 0.13905465602874756, 0.13532455265522003, 0.13566890358924866, 0.13217109441757202, 0.13244777917861938, 0.13032856583595276, 0.12829139828681946, 0.12865321338176727, 0.12646478414535522, 0.1260261833667755, 0.12411635369062424, 0.12126277387142181, 0.1212860718369484, 0.12009543925523758, 0.118068628013134, 0.11981809884309769, 0.11864550411701202, 0.11796905845403671, 0.11520795524120331, 0.11530979722738266, 0.11183282732963562, 0.11358235031366348, 0.11022748053073883, 0.111017145216465, 0.11003417521715164, 0.10894577205181122, 0.11011580377817154, 0.10848040878772736, 0.10879532992839813, 0.10376456379890442, 0.10462131351232529, 0.10532181710004807, 0.10317234694957733, 0.10270962119102478, 0.10403373837471008, 0.10083916038274765, 0.0991664007306099, 0.10001682490110397, 0.10106752067804337, 0.09921560436487198, 0.100234754383564, 0.10039106756448746, 0.1006326973438263, 0.09679027646780014, 0.09775908291339874, 0.09850769490003586, 0.09691301733255386, 0.09559903293848038, 0.09358334541320801, 0.0956011414527893, 0.09223032742738724, 0.0939861610531807, 0.09388365596532822, 0.09425093233585358, 0.09309639036655426, 0.09266479313373566, 0.09168883413076401, 0.09030038118362427, 0.08885440975427628, 0.08864887058734894, 0.09075015783309937, 0.08989150822162628, 0.08994054049253464, 0.08847978711128235, 0.08809340745210648, 0.08963935077190399, 0.08672534674406052, 0.08859755098819733, 0.08871984481811523, 0.08547854423522949, 0.08570060133934021, 0.08310895413160324, 0.08628654479980469, 0.08521950989961624, 0.08506710827350616, 0.08685966581106186, 0.08481782674789429, 0.08600644022226334, 0.08094857633113861, 0.08588366955518723, 0.08484846353530884, 0.08426807820796967, 0.08559028804302216, 0.08434915542602539, 0.0824432522058487, 0.08202958852052689, 0.08257860690355301, 0.08174637705087662, 0.081461600959301, 0.0811019241809845, 0.08095166087150574, 0.08224550634622574, 0.08108105510473251, 0.0781664252281189, 0.07941883057355881, 0.08083760738372803, 0.07902515679597855, 0.07881142944097519, 0.07983461767435074, 0.08215437829494476, 0.07905185222625732, 0.08119671046733856, 0.07736068218946457, 0.07757982611656189, 0.07726305723190308, 0.0799061506986618, 0.07602708786725998, 0.08002690970897675, 0.07978098839521408, 0.07851096242666245, 0.07838491350412369, 0.07802469283342361, 0.07505824416875839, 0.07806370407342911, 0.0775158554315567, 0.07734830677509308, 0.07389850169420242, 0.07518742978572845, 0.076372429728508, 0.07643253356218338, 0.07778217643499374, 0.0737258717417717, 0.07469072937965393, 0.07721162587404251, 0.07302942872047424, 0.07413434982299805, 0.07253578305244446, 0.07481648772954941, 0.07262710481882095, 0.0740559846162796, 0.07661524415016174, 0.07428400963544846, 0.07610165327787399, 0.07206081598997116, 0.07449837774038315, 0.07709286361932755, 0.07452692836523056, 0.07131758332252502, 0.07437028735876083, 0.07244128733873367, 0.07287828624248505, 0.0738687738776207, 0.07290488481521606, 0.07176301628351212, 0.07199954986572266, 0.07027948647737503, 0.07309267669916153, 0.06992022693157196, 0.07194862514734268, 0.07569097727537155, 0.07285813987255096, 0.06995026022195816]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:29 2016", "state": "available"}], "summary": "e279b29b73b5601271d1939c0b3dd76d"}