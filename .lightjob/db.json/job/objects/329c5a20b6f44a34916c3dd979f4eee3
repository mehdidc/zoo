{"content": {"hp_model": {"f0": 16, "f1": 64, "f2": 64, "f3": 16, "nonlin": "very_leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.5946574211120605, 1.1217830181121826, 0.9431467652320862, 0.8158061504364014, 0.7116197347640991, 0.6244564056396484, 0.5459808707237244, 0.4714733958244324, 0.39848417043685913, 0.32729071378707886, 0.2625444829463959, 0.2099779099225998, 0.17113518714904785, 0.13785649836063385, 0.11763553321361542, 0.0962030217051506, 0.07945519685745239, 0.07035259902477264, 0.055413346737623215, 0.0494941808283329, 0.04305816814303398, 0.03633180633187294, 0.029798055067658424, 0.02857302688062191, 0.024878643453121185, 0.02027640864253044, 0.01724778115749359, 0.015172768384218216, 0.01289364229887724, 0.012523348443210125, 0.008616838604211807, 0.00801285170018673, 0.007353720720857382, 0.006117572542279959, 0.004223855212330818, 0.002435089787468314, 0.005622794385999441, 0.001840771990828216, 0.004072937183082104, 0.003267897991463542, 0.000905485765542835, 0.0005933865322731435, 0.0004621980478987098, 0.00037172847078181803, 0.0003835436946246773, 0.002973225899040699, 0.0021076148841530085, 0.001538063632324338, 0.0008362035732716322, 0.0009024592582136393, 0.000588686962146312, 0.0006257835193537176, 0.0010559381917119026, 0.0003906386555172503, 0.0003210990398656577, 0.00028869137167930603, 0.0002756875182967633, 0.00027056082035414875, 0.0003332554770167917, 0.0003526310028973967, 0.0002733134897425771, 0.0002569497737567872, 0.00025353761157020926, 0.0002506486780475825, 0.00024784711422398686, 0.00024487305199727416, 0.00024236805620603263, 0.0002403516846243292, 0.0002386815322097391, 0.00023723579943180084, 0.00023592429351992905, 0.00023468342260457575, 0.00023347213573288172, 0.0002322607469977811, 0.00023103173589333892, 0.0002297807950526476, 0.0002285243826918304, 0.00022732569777872413, 0.0002261865301989019, 0.00022514074225910008, 0.00022410017845686525, 0.00022298652038443834, 0.00022178101062308997, 0.0002205651835538447, 0.000219358887989074, 0.00021816318621858954, 0.00021697639022022486, 0.00021581749024335295, 0.00021468428894877434, 0.0002135789836756885, 0.0002124999591615051, 0.00021145357459317893, 0.00021043923334218562, 0.00020945904543623328, 0.0002085127925965935, 0.00020760194456670433, 0.00020672510436270386, 0.00020588337793014944, 0.0002050753537332639, 0.00020430164295248687], "moving_avg_accuracy_train": [0.05574528048864894, 0.11649118822674417, 0.17758319634782205, 0.2365991995720284, 0.29239406646537003, 0.3450851533704203, 0.39433444620752, 0.4401605675013121, 0.48289653381088926, 0.5216169589585471, 0.5580276613473454, 0.5916108792876846, 0.6232355150173234, 0.6524880935739614, 0.6793410060523443, 0.7036737128483649, 0.7272169210266606, 0.7477293342788303, 0.7677762216450594, 0.7849790776032464, 0.8018194267239203, 0.8177826036182503, 0.8324680803076896, 0.8449456480555751, 0.8561545326893863, 0.8671143875657504, 0.877487428494945, 0.8872160433823921, 0.896332302993027, 0.9041834419259133, 0.9120259945702544, 0.9190262353275607, 0.9257891566222317, 0.9316571497017029, 0.9369663894542163, 0.9421212711921649, 0.9465515816075554, 0.9508875612051978, 0.9535715648668854, 0.9571822946504996, 0.9606341673045434, 0.9637919699181736, 0.9666363534680691, 0.969200948960594, 0.9715067597550571, 0.9732588658831874, 0.9751589570830285, 0.9767364696319238, 0.9783352313354443, 0.9798415822329076, 0.9810485606144526, 0.9822417619542624, 0.9833783861291296, 0.9843408940174626, 0.9852606295395813, 0.9861418338832882, 0.9868489233354909, 0.9875480828603306, 0.9881168725636387, 0.9887032241073395, 0.9892936834657086, 0.989804170548955, 0.9902450077334006, 0.9906115342648779, 0.9909367578455883, 0.9912434099610848, 0.9915333477578889, 0.9918082066190509, 0.9920718556357634, 0.9923207294460334, 0.9925540164705147, 0.9927709502389763, 0.9929731660770204, 0.9931621718265073, 0.993329951852236, 0.9934832790242015, 0.9936189843789797, 0.9937503837446993, 0.993877943769085, 0.993978796898175, 0.9940835156072132, 0.9941870630405857, 0.9942849060282399, 0.9943636641218906, 0.994450822447843, 0.9945292649412001, 0.994602188334031, 0.9946678193875789, 0.994731537633391, 0.9947888840546217, 0.9948497964289676, 0.9949022924170694, 0.9949588394015991, 0.9950027562412473, 0.9950562322897878, 0.9951020355846647, 0.995147908847673, 0.9951845444867613, 0.9952244920083694, 0.995253469331388], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.05571627329631022, 0.11519768036991715, 0.17423146443194648, 0.23091117472820966, 0.28356483323430437, 0.3325319509556781, 0.3774100799847338, 0.41757861053106465, 0.45371705148285274, 0.48515728157553134, 0.5141564960008096, 0.54034123820231, 0.5643916693990217, 0.5857593842832913, 0.6051266640402032, 0.6217412566723275, 0.6380576065792665, 0.6523537555128308, 0.6656495946641079, 0.6773737682925767, 0.6879580276173101, 0.6974706244696603, 0.7062028600742756, 0.7142927762035197, 0.720683616947249, 0.726980571988217, 0.7335532108549073, 0.7391359074651997, 0.744635526197219, 0.749262652700238, 0.7536976802577744, 0.7576993530734879, 0.7617444297672686, 0.7653138156214906, 0.7683553644527904, 0.7716166312360505, 0.7741286432818731, 0.7766772753298153, 0.7784796743969844, 0.7807163031545751, 0.7829256110450664, 0.7847573557575778, 0.7864303400613382, 0.7880214751534724, 0.7891818535229144, 0.7901732478957735, 0.7914103882323256, 0.7923783596688821, 0.7935780942968735, 0.7949417057067947, 0.7955666934098351, 0.7963733229675715, 0.7974044653507841, 0.7978808333394256, 0.7984967880325915, 0.7992373412511697, 0.7994257059118208, 0.7998048126549762, 0.7999099570954273, 0.7998814872706738, 0.8001223600985763, 0.8003167905985078, 0.8006047298556751, 0.8008293131106949, 0.8009470183301224, 0.8009664743001974, 0.8009839846732649, 0.8009498863753661, 0.8008703697822572, 0.8008120413883688, 0.8007839598963693, 0.8006355867324101, 0.8005630860410968, 0.8004612143251649, 0.8003817368120762, 0.8003224140815464, 0.8002191659904099, 0.8001516862795466, 0.80008992503111, 0.7999722752426074, 0.799951839651705, 0.799860205432393, 0.799789941666262, 0.7996768466430846, 0.7995750611222249, 0.7995699328808608, 0.7995032527987235, 0.7994808913272096, 0.7994231154004374, 0.7994087676687521, 0.7993592336164853, 0.7993390670319451, 0.7993209171058591, 0.7993534102973816, 0.7993948612010019, 0.7994311375056005, 0.7994017215148296, 0.7994251047567954, 0.7994084990721549, 0.7993813469247286], "moving_var_accuracy_train": [0.027967826670823308, 0.058381631766067574, 0.08613356969585342, 0.1088662104553039, 0.1259971939545849, 0.13838463031224646, 0.14637560288561144, 0.15063834313255042, 0.15201177416701522, 0.15030403866285208, 0.14720528803257765, 0.14263525197437485, 0.13737278504222844, 0.13133692670791605, 0.1246929442142696, 0.11755237537303263, 0.11078568169766849, 0.10349394540475172, 0.09676145010194506, 0.0897487493698137, 0.08332625065938797, 0.07728703274248622, 0.07149929849860218, 0.06575057592086962, 0.060306270181389475, 0.055356708933449184, 0.05078943784317299, 0.04656230758751002, 0.04265403253235545, 0.03894339272201126, 0.03560260413762349, 0.032483374059803435, 0.029646670593764326, 0.02699190361941439, 0.024546405498219962, 0.022330920199988093, 0.020274477033379732, 0.0184162358016823, 0.016639447102417645, 0.015092838718308388, 0.013690793669855168, 0.012411459758989498, 0.011243128443100792, 0.010178009949153227, 0.00920805982501666, 0.008314882725473082, 0.007515887572035197, 0.006786695727408979, 0.006131030505529873, 0.005538349292213483, 0.004997625534313788, 0.0045106765458183245, 0.004071236121870526, 0.0036724503025994002, 0.003312818493215282, 0.0029885253337520678, 0.0026941725798176083, 0.0024291547382064133, 0.002189150959925075, 0.0019733301371277698, 0.0017791349036999633, 0.00160356678688942, 0.0014449591450091872, 0.001301672305792759, 0.0011724570086105344, 0.0010560576274289276, 0.0009512084400201751, 0.0008567675225601912, 0.0007717163675402932, 0.0006951021743472092, 0.0006260817624346098, 0.0005638971285302397, 0.0005078754368836184, 0.00045740940175530875, 0.00041192181281307974, 0.00037094121452673797, 0.0003340128365639031, 0.00030076694504731644, 0.00027083669458097644, 0.00024384456730570405, 0.00021955880464733722, 0.00019769942282122567, 0.0001780156397912011, 0.00016026990134792047, 0.00014431128037717373, 0.00012993553136233306, 0.00011698983861709749, 0.0001053296216720959, 9.483319963853062e-05, 8.537947718292945e-05, 7.687492232077258e-05, 6.921223254759636e-05, 6.231978734597132e-05, 5.610516681061631e-05, 5.052038731946219e-05, 4.548723006391019e-05, 4.0957446263850385e-05, 3.687378116792807e-05, 3.320076529147894e-05, 2.9888245929575033e-05], "duration": 73197.036094, "accuracy_train": [0.5574528048864895, 0.6632043578696013, 0.7274112694375231, 0.7677432285898856, 0.7945478685054448, 0.8193049355158729, 0.8375780817414176, 0.8525956591454411, 0.8675202305970838, 0.8701007852874677, 0.8857239828465301, 0.8938598407507383, 0.9078572365840717, 0.9157613005837025, 0.9210172183577889, 0.9226680740125508, 0.9391057946313216, 0.9323410535483574, 0.9481982079411223, 0.9398047812269288, 0.9533825688099853, 0.9614511956672205, 0.9646373705126431, 0.9572437577865448, 0.9570344943936876, 0.9657530814530271, 0.9708447968576966, 0.9747735773694168, 0.9783786394887413, 0.97484369232189, 0.9826089683693245, 0.9820284021433187, 0.986655448274271, 0.9844690874169435, 0.9847495472268365, 0.9885152068337025, 0.9864243753460686, 0.9899113775839794, 0.9777275978220746, 0.9896788627030271, 0.9917010211909376, 0.9922121934408453, 0.9922358054171282, 0.9922823083933187, 0.9922590569052234, 0.9890278210363603, 0.9922597778815985, 0.9909340825719823, 0.9927240866671282, 0.9933987403100776, 0.9919113660483574, 0.9929805740125508, 0.9936080037029347, 0.9930034650124585, 0.993538249238649, 0.9940726729766519, 0.9932127284053157, 0.9938405185838871, 0.993235979893411, 0.993980388000646, 0.99460781769103, 0.9943985542981728, 0.994212542393411, 0.9939102730481728, 0.9938637700719823, 0.9940032790005537, 0.9941427879291252, 0.9942819363695091, 0.9944446967861758, 0.9945605937384644, 0.9946535996908453, 0.9947233541551311, 0.9947931086194168, 0.99486322357189, 0.9948399720837948, 0.99486322357189, 0.9948403325719823, 0.9949329780361758, 0.9950259839885567, 0.9948864750599853, 0.9950259839885567, 0.9951189899409376, 0.9951654929171282, 0.9950724869647471, 0.9952352473814139, 0.9952352473814139, 0.9952584988695091, 0.9952584988695091, 0.9953050018456996, 0.9953050018456996, 0.9953980077980805, 0.9953747563099853, 0.9954677622623662, 0.9953980077980805, 0.9955375167266519, 0.9955142652385567, 0.9955607682147471, 0.9955142652385567, 0.9955840197028424, 0.9955142652385567], "end": "2016-02-02 06:07:05.288000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0], "moving_var_accuracy_valid": [0.02793872799026219, 0.05698719527834119, 0.08265336469664787, 0.10330133426039798, 0.11792287062104613, 0.12771059112039143, 0.13306595019468942, 0.1342809527914841, 0.13260673974216858, 0.12824245838247686, 0.12298680247977854, 0.116858888749232, 0.11037880904103871, 0.10345014129131372, 0.09648095088882473, 0.08931725799492521, 0.08278154166400369, 0.0763428063665792, 0.070299539778551, 0.06450669202613024, 0.05906426173259605, 0.05397224104921455, 0.0492612843921837, 0.044924176639769164, 0.04079934458449765, 0.03707627491113962, 0.03375744365507302, 0.030662197802548852, 0.027868190278072157, 0.025274063947339418, 0.022923682777530444, 0.020775434967693587, 0.01884515528005134, 0.01707530439043308, 0.0154510331250284, 0.014001652561809921, 0.012658279146294154, 0.011450910959506922, 0.010335057645132217, 0.009346574454412532, 0.008455846381166162, 0.007640459341275869, 0.006901603295473937, 0.006234228363859334, 0.005622923829115819, 0.005069477211427069, 0.004576304136195281, 0.0041271064408936586, 0.0037273500654027053, 0.0033713499835578372, 0.003037730471862619, 0.0027398132858670833, 0.002475401248810494, 0.0022299034620748654, 0.0020103277175236764, 0.0018142307173972276, 0.0016331269768659446, 0.0014711077764837023, 0.0013240964970155523, 0.0011916941420922907, 0.0010730469053560573, 0.0009660824437941842, 0.0008702203805571282, 0.0007836522812473332, 0.0007054117437907241, 0.0006348739762245957, 0.0005713893381206209, 0.0005142608685538352, 0.00046289168769566853, 0.0004166331387399041, 0.00037497692199765, 0.0003376773611599344, 0.0003039569321961091, 0.00027365463959506053, 0.0002463460257113355, 0.00022174309581741967, 0.00019966472775058782, 0.00017973923657793278, 0.0001617996429864155, 0.00014574425194238626, 0.00013117358526852738, 0.00011813179821301505, 0.00010636305136319158, 9.584186058527992e-05, 8.635091715706198e-05, 7.771606213109118e-05, 6.998447201816658e-05, 6.299052513502434e-05, 5.672151514095135e-05, 5.105121634349684e-05, 4.596817731015287e-05, 4.13750197993257e-05, 3.724048259774551e-05, 3.352593660542883e-05, 3.0188806541584374e-05, 2.7181769619903915e-05, 2.4471380362530823e-05, 2.2029163310321238e-05, 1.9828728718150496e-05, 1.78524909983242e-05], "accuracy_test": 0.3079241071428572, "start": "2016-02-01 09:47:08.252000", "learning_rate_per_epoch": [0.00019071654241997749, 0.0001806444488465786, 0.00017110428598243743, 0.000162067954079248, 0.00015350883768405765, 0.00014540174743160605, 0.00013772280362900347, 0.00013044940715190023, 0.0001235601375810802, 0.00011703469499479979, 0.00011085387814091519, 0.00010499947529751807, 9.945425699697807e-05, 9.420189599040896e-05, 8.922691631596535e-05, 8.451467874692753e-05, 8.005130075616762e-05, 7.582364196423441e-05, 7.181925320765004e-05, 6.802634743507951e-05, 6.443374877562746e-05, 6.103088162490167e-05, 5.7807726989267394e-05, 5.475479338201694e-05, 5.186309135751799e-05, 4.912410440738313e-05, 4.652977077057585e-05, 4.4072447053622454e-05, 4.174490095465444e-05, 3.954027488362044e-05, 3.7452078686328605e-05, 3.547416417859495e-05, 3.360070695634931e-05, 3.182619184372015e-05, 3.0145391065161675e-05, 2.8553356969496235e-05, 2.7045400202041492e-05, 2.5617082428652793e-05, 2.4264196326839738e-05, 2.298275830980856e-05, 2.1768995793536305e-05, 2.0619334463845007e-05, 1.9530389181454666e-05, 1.849895306804683e-05, 1.752198841131758e-05, 1.6596619389019907e-05, 1.57201211550273e-05, 1.4889912563376129e-05, 1.4103548892308027e-05, 1.335871456831228e-05, 1.2653215890168212e-05, 1.1984976481471676e-05, 1.1352027286193334e-05, 1.0752505659183953e-05, 1.0184646271227393e-05, 9.646776561567094e-06, 9.137312190432567e-06, 8.654753401060589e-06, 8.19767956272699e-06, 7.764744623273145e-06, 7.354673925874522e-06, 6.9662601163145155e-06, 6.598359050258296e-06, 6.249887519516051e-06, 5.91981915931683e-06, 5.607182629319141e-06, 5.311057066137437e-06, 5.030570264352718e-06, 4.764896402775776e-06, 4.513253315963084e-06, 4.274900220480049e-06, 4.049134986416902e-06, 3.835292773146648e-06, 3.632743755588308e-06, 3.4408917599648703e-06, 3.2591717626928585e-06, 3.0870487535139546e-06, 2.9240159165055957e-06, 2.769593265838921e-06, 2.623325826789369e-06, 2.484783180989325e-06, 2.353557192691369e-06, 2.2292615540209226e-06, 2.1115301933605224e-06, 2.0000163658551173e-06, 1.8943917439173674e-06, 1.794345394046104e-06, 1.6995826399579528e-06, 1.6098244941531448e-06, 1.5248066347339773e-06, 1.4442787232837873e-06, 1.3680037227459252e-06, 1.2957568742422154e-06, 1.2273255833861185e-06, 1.162508283414354e-06, 1.1011140941263875e-06, 1.0429622534502414e-06, 9.878814353214693e-07, 9.357095791528991e-07, 8.8629303718335e-07], "accuracy_train_first": 0.5574528048864895, "accuracy_train_last": 0.9955142652385567, "batch_size_eval": 1024, "accuracy_train_std": [0.022488686229052632, 0.022506745070275315, 0.022678348849146844, 0.02406246076451511, 0.025052110053000745, 0.026409523801092266, 0.0263011997741112, 0.027863027396612672, 0.028157386240948743, 0.02981355675928456, 0.027200405929167573, 0.026645589937903617, 0.02550354395714707, 0.023444198234904783, 0.02227054655924201, 0.01990472395251474, 0.017709642409572388, 0.01825601353994111, 0.014567914619058231, 0.014658494295251163, 0.013812303443694262, 0.010332325391600035, 0.01046606884081555, 0.011922641924251132, 0.010848695413184901, 0.00929552433163242, 0.008417548976769058, 0.007106366000728881, 0.006624949008148684, 0.007987922833573788, 0.005352199040905405, 0.005653123692941152, 0.004609904224134301, 0.005268747511090187, 0.004659777040415736, 0.0037036164457527753, 0.003925082477380232, 0.003112145152838248, 0.006140188991174169, 0.0034871982406334536, 0.0027384961604328983, 0.0031282571138542287, 0.0034742822173166503, 0.0033195604024726976, 0.003624231907773392, 0.003459342037266171, 0.0027434917512336554, 0.00266198015157897, 0.0023442132807264655, 0.0025866091312674463, 0.0026302211017138655, 0.0030474892096425416, 0.0026590951283707353, 0.0024985979591144987, 0.002396404925317078, 0.0026453271228728276, 0.002674316672784439, 0.0024200420540879054, 0.002556577708370479, 0.002513105341321687, 0.0026242011150991672, 0.0025967264146853257, 0.0023521857343536995, 0.002327395158676847, 0.00234179583237894, 0.002467133802691559, 0.002461746409157288, 0.0023075587475277367, 0.00232621631511286, 0.002152176945110383, 0.002184527780627708, 0.002193003430516553, 0.0022401534207167587, 0.002242098651360794, 0.002234267020946714, 0.0022115078960690724, 0.0023017669297424074, 0.002097398746949504, 0.0022190629191878083, 0.0021036158068849517, 0.0021881501436730256, 0.0021269314767632906, 0.002250880737522099, 0.002339982250828874, 0.002202385204769014, 0.0022126711578703837, 0.0021750717610960206, 0.0023361391587175483, 0.002202696308771902, 0.0022935973039859173, 0.0022133936257895744, 0.002251793820495952, 0.0020815423362503774, 0.0022338167937541215, 0.0020849853948797866, 0.002105775301216051, 0.0020746976584074166, 0.0021165308128863994, 0.0020859821458284705, 0.0021165308128863994], "accuracy_test_std": 0.012414382125979298, "error_valid": [0.44283726703689763, 0.3494696559676205, 0.2944644790097892, 0.25897143260542166, 0.24255224021084332, 0.22676398955195776, 0.2186867587537651, 0.22090461455195776, 0.2210369799510542, 0.23188064759036142, 0.22485057417168675, 0.22399608198418675, 0.2191544498305723, 0.2219311817582832, 0.2205678181475903, 0.2287274096385542, 0.2150952442582832, 0.2189809040850903, 0.21468785297439763, 0.21710866905120485, 0.2167836384600903, 0.21691600385918675, 0.21520701948418675, 0.2128979786332832, 0.22179881635918675, 0.2163468326430723, 0.20729303934487953, 0.21061982304216864, 0.2058679052146084, 0.2090932087725903, 0.20638707172439763, 0.2062855915850903, 0.20184987998870485, 0.20256171169051207, 0.20427069606551207, 0.1990319677146084, 0.20326324830572284, 0.20038503623870485, 0.20529873399849397, 0.1991540380271084, 0.19719061794051207, 0.1987569418298193, 0.1985128012048193, 0.1976583090173193, 0.2003747411521084, 0.20090420274849397, 0.19745534873870485, 0.1989098974021084, 0.19562429405120485, 0.19278579160391573, 0.19880841726280118, 0.19636701101280118, 0.19331525320030118, 0.19783185476280118, 0.19595961972891573, 0.1940976797816265, 0.1988790121423193, 0.1967832266566265, 0.19914374294051207, 0.2003747411521084, 0.19770978445030118, 0.1979333349021084, 0.1968038168298193, 0.1971494375941265, 0.19799363469503017, 0.1988584219691265, 0.1988584219691265, 0.19935699830572284, 0.19984527955572284, 0.1997129141566265, 0.1994687735316265, 0.20069977174322284, 0.20008942018072284, 0.20045563111822284, 0.20033356080572284, 0.20021149049322284, 0.2007100668298193, 0.20045563111822284, 0.2004659262048193, 0.20108657285391573, 0.20023208066641573, 0.20096450254141573, 0.20084243222891573, 0.20134100856551207, 0.20134100856551207, 0.20047622129141573, 0.20109686794051207, 0.20072036191641573, 0.20109686794051207, 0.20072036191641573, 0.20108657285391573, 0.20084243222891573, 0.20084243222891573, 0.20035415097891573, 0.20023208066641573, 0.20024237575301207, 0.2008630224021084, 0.20036444606551207, 0.2007409520896084, 0.2008630224021084], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.05281185977053501, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 16, "valid_ratio": 0.15, "learning_rate": 0.00020135021932694196, "optimization": "adam", "nb_data_augmentation": 0, "learning_rate_decay_method": "exp", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 4.215650392317343e-08, "rotation_range": [0, 0], "momentum": 0.8686496974905924}, "accuracy_valid_max": 0.8072142083960843, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.7991369775978916, "accuracy_valid_std": [0.01884046191127862, 0.015474311794939657, 0.015056119420918784, 0.01229122511012294, 0.011953335222075527, 0.009576111556408475, 0.01009022407441659, 0.008769092343707712, 0.012033023444452826, 0.01368592701418225, 0.019111161608582776, 0.01908098744089754, 0.012596003655137926, 0.019175261672975605, 0.02130342317862169, 0.012393194216068208, 0.016432881084900767, 0.018462338095068608, 0.021915401495652846, 0.022427527387644502, 0.017528329770397456, 0.018121778409016274, 0.01660057479845165, 0.015640355277379777, 0.01906502318287157, 0.011224400459368318, 0.01195295267334832, 0.010793131362124786, 0.02125796139307228, 0.01662489298025991, 0.016272335338354642, 0.014026954078389078, 0.01668434110779234, 0.019085098822601256, 0.020551726908722767, 0.01752854202113605, 0.021461471285571184, 0.015601240373148969, 0.015304082522161925, 0.01579442876261449, 0.01623384959053329, 0.018716436459804742, 0.019149652669843552, 0.0189454830983178, 0.016426440354843426, 0.012034931338610522, 0.014685425279540183, 0.015866851087740462, 0.014176163275367712, 0.01584332370614799, 0.014220207649333317, 0.01331625113078205, 0.012189221757058432, 0.013877729894909274, 0.017468169635413942, 0.01981787508775062, 0.018899676405494538, 0.02067813561577244, 0.0168975430034199, 0.016089148146889467, 0.01345925051689814, 0.015092670902074312, 0.018261453532118995, 0.020466162426707502, 0.022230678618147207, 0.021530145853358922, 0.021530145853358922, 0.02064341169341259, 0.02082986441009681, 0.02194159112884057, 0.021825899728815493, 0.02133283286642011, 0.02124054375285924, 0.021465323169498972, 0.02122913004218899, 0.020955464970463335, 0.020101845242733746, 0.021185824737686074, 0.020023437824165637, 0.019314938674321096, 0.018883180402896164, 0.019255288951632667, 0.019088806433662512, 0.018462710189649862, 0.018294064187335743, 0.019075846514098323, 0.018104740423447795, 0.019076938215236608, 0.018005703182989028, 0.018818990672317973, 0.019141346237345504, 0.019045041375857507, 0.019013718944820252, 0.01880346755588526, 0.01868645301217364, 0.017664147922144418, 0.016751715556229636, 0.017666775028050564, 0.01673834046674573, 0.01675883028901264], "accuracy_valid": [0.5571627329631024, 0.6505303440323795, 0.7055355209902108, 0.7410285673945783, 0.7574477597891567, 0.7732360104480422, 0.7813132412462349, 0.7790953854480422, 0.7789630200489458, 0.7681193524096386, 0.7751494258283133, 0.7760039180158133, 0.7808455501694277, 0.7780688182417168, 0.7794321818524097, 0.7712725903614458, 0.7849047557417168, 0.7810190959149097, 0.7853121470256024, 0.7828913309487951, 0.7832163615399097, 0.7830839961408133, 0.7847929805158133, 0.7871020213667168, 0.7782011836408133, 0.7836531673569277, 0.7927069606551205, 0.7893801769578314, 0.7941320947853916, 0.7909067912274097, 0.7936129282756024, 0.7937144084149097, 0.7981501200112951, 0.7974382883094879, 0.7957293039344879, 0.8009680322853916, 0.7967367516942772, 0.7996149637612951, 0.794701266001506, 0.8008459619728916, 0.8028093820594879, 0.8012430581701807, 0.8014871987951807, 0.8023416909826807, 0.7996252588478916, 0.799095797251506, 0.8025446512612951, 0.8010901025978916, 0.8043757059487951, 0.8072142083960843, 0.8011915827371988, 0.8036329889871988, 0.8066847467996988, 0.8021681452371988, 0.8040403802710843, 0.8059023202183735, 0.8011209878576807, 0.8032167733433735, 0.8008562570594879, 0.7996252588478916, 0.8022902155496988, 0.8020666650978916, 0.8031961831701807, 0.8028505624058735, 0.8020063653049698, 0.8011415780308735, 0.8011415780308735, 0.8006430016942772, 0.8001547204442772, 0.8002870858433735, 0.8005312264683735, 0.7993002282567772, 0.7999105798192772, 0.7995443688817772, 0.7996664391942772, 0.7997885095067772, 0.7992899331701807, 0.7995443688817772, 0.7995340737951807, 0.7989134271460843, 0.7997679193335843, 0.7990354974585843, 0.7991575677710843, 0.7986589914344879, 0.7986589914344879, 0.7995237787085843, 0.7989031320594879, 0.7992796380835843, 0.7989031320594879, 0.7992796380835843, 0.7989134271460843, 0.7991575677710843, 0.7991575677710843, 0.7996458490210843, 0.7997679193335843, 0.7997576242469879, 0.7991369775978916, 0.7996355539344879, 0.7992590479103916, 0.7991369775978916], "seed": 712741267, "model": "residualv3", "loss_std": [0.4504050314426422, 0.27371418476104736, 0.2623794674873352, 0.2535869777202606, 0.2441549301147461, 0.23442304134368896, 0.22231777012348175, 0.20849698781967163, 0.19113166630268097, 0.17288872599601746, 0.15052279829978943, 0.130711629986763, 0.11409634351730347, 0.09694329649209976, 0.09231669455766678, 0.0756593644618988, 0.06818380951881409, 0.0662132129073143, 0.052431587129831314, 0.05025513470172882, 0.055094845592975616, 0.04122946783900261, 0.03899967297911644, 0.06106090918183327, 0.04368501156568527, 0.030148718506097794, 0.024690138176083565, 0.024494359269738197, 0.01995205692946911, 0.022098788991570473, 0.014384768903255463, 0.014201980084180832, 0.017056532204151154, 0.013124700635671616, 0.006995850708335638, 0.0034408248029649258, 0.017485760152339935, 0.0024394020438194275, 0.020459650084376335, 0.008490565232932568, 0.0006941725732758641, 0.00023643301392439753, 0.00014586666657123715, 9.16717981453985e-05, 0.0035494938492774963, 0.07158436626195908, 0.0637684240937233, 0.049754947423934937, 0.01747557893395424, 0.02258707396686077, 0.00709127401933074, 0.00987169798463583, 0.03533042594790459, 0.0016523876693099737, 0.0001826284860726446, 6.790016777813435e-05, 6.357325037242845e-05, 0.0001348906516795978, 0.003624093485996127, 0.004066652152687311, 0.00019794113177340478, 2.323951594007667e-05, 1.4376185390574392e-05, 1.0972696145472582e-05, 8.166071893356275e-06, 6.139354809420183e-06, 4.7162961891444866e-06, 3.7149159197724657e-06, 2.993975613208022e-06, 2.4675152872077888e-06, 2.069791662506759e-06, 1.7673409047347377e-06, 1.5353448361565825e-06, 1.359608745588048e-06, 1.2384135743559455e-06, 1.2071579931216547e-06, 1.3501651210390264e-06, 1.8455525605531875e-06, 1.6880046587175457e-06, 1.962822807399789e-06, 1.25536769246537e-06, 9.201129387292895e-07, 8.941324267652817e-07, 9.446844728699944e-07, 8.08187621714751e-07, 8.103032769213314e-07, 7.341428727158927e-07, 7.52361472677876e-07, 6.785591608604591e-07, 6.819064992669155e-07, 6.31618206625717e-07, 6.33528202342859e-07, 5.934725777478889e-07, 5.902963948756224e-07, 5.623837182611169e-07, 5.563622949011915e-07, 5.344638225324161e-07, 5.267718847790093e-07, 5.107804668114113e-07, 5.015428996557603e-07]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:31 2016", "state": "available"}], "summary": "38e54eb4e2d6902a480a976f9d7b8926"}