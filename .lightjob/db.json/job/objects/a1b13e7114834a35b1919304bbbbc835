{"content": {"hp_model": {"f0": 32, "f1": 16, "f2": 64, "f3": 16, "nonlin": "leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.016504377899123734, 0.00836022094049949, 0.011157561713780226, 0.013778930836610167, 0.013255289752011888, 0.014916239320482586, 0.01501024050860108, 0.009765037466895783, 0.014103857318340173, 0.012402244265218645, 0.010890381626377713, 0.01066233940831085, 0.008230063896621767, 0.010392998222369148, 0.008740775741457308, 0.007423382038719804, 0.011959684394552044, 0.014944439942595157, 0.0056304608671207835, 0.009968261856713765, 0.009822244975807892, 0.011617945250725679, 0.010632435348125085, 0.008639648643990295, 0.007100928442674414, 0.007483279222099298, 0.006561355191947959, 0.019508517587953304, 0.010082420577351939, 0.01814989337240064, 0.011620577454210808, 0.010422563950800821, 0.014556487791989758, 0.015725708171107543, 0.009210211122925001, 0.007005170338085993, 0.011122837146875689, 0.009224516593703876, 0.012675633178684106, 0.011861725142256465, 0.008249341215069894, 0.011574577954824515, 0.008211646184888637, 0.014520597344500204, 0.009054829030928599, 0.012978288612027912, 0.014203407815184421, 0.01164431091286035, 0.01093636366777689, 0.009295529432496225, 0.010549891162157512, 0.01316572413997552, 0.01159340604259369, 0.012041361434736947, 0.00971699374308507, 0.013598953921550791, 0.012067906408825423, 0.01300901607743044, 0.013346192087182254, 0.010720804033095187, 0.011520723018362027, 0.01130442987489886, 0.011185914096591233, 0.00825424617851568, 0.009813236563915636, 0.013262527223260386, 0.011592689573393548, 0.009572027707378658, 0.012833112467281145, 0.009851768858502629, 0.01194871767067767, 0.01200156487049929, 0.008151965889438015, 0.012524562792256666, 0.013972301334764339, 0.014891666191359981, 0.00979319631395793, 0.009922583591024483, 0.008623437036676639, 0.012502701891354152, 0.016001410588791613, 0.012988022244845868, 0.008845379533190973, 0.010313061568279902, 0.009898405602965484, 0.009547626580897393, 0.012530886550960114, 0.010015294665518122, 0.01199488442421103, 0.012491251898317202, 0.014063744734384888, 0.01579505919640786, 0.009783600493435799, 0.008680303484457598, 0.009778451656677454, 0.010390100726312222, 0.00901537686051323, 0.008417672637559851, 0.009438243221748771, 0.009275731942576948, 0.009317361439501294, 0.008659222904099477, 0.010453430131372612, 0.009087059099444107, 0.006857522439407484, 0.009058985581604085, 0.012429159529396935, 0.010769731031630033, 0.01220673178511593, 0.009884474414983755, 0.009105729939943566, 0.010080846622245326, 0.012139953342876347, 0.010145583743524068, 0.011601820868554304, 0.012816180439171551, 0.010917341375145145, 0.010750283513722796, 0.009836462542025031, 0.0097557793471836, 0.010734895826867815, 0.008936232834353061, 0.01095527997643222, 0.008784442377059784, 0.011765967678297587, 0.010367585128378671, 0.010680703345006306, 0.009137507068199274, 0.009566446876025243, 0.009166544079367277, 0.008348726348290297, 0.010324133127037617, 0.00901260287175597, 0.008425114616153225, 0.008842838411926495, 0.0085071854145094, 0.008516518628654648, 0.009074629820715793, 0.009262841397432785, 0.009849039810875622, 0.009046451859188441, 0.007902322446853604, 0.008201412090146748, 0.008618558598144702, 0.0077878073404380604, 0.008024708916882513, 0.008519696866195721, 0.00939269558846626, 0.0097676172548614, 0.007248779470777245, 0.010369099704889756, 0.008807818247897255, 0.008763957227452074, 0.009278512120728078, 0.009550481732637004, 0.00805087096406692, 0.008509682389706204, 0.008664317113616246, 0.00851444195093807, 0.007957550567286026, 0.008008423068148494, 0.00852549886528067, 0.008703561040586137, 0.008823473047016491, 0.010035921450444069, 0.008899343422867112, 0.009581179943207254, 0.010258513049013212, 0.009149602118260986, 0.008471038280146094, 0.008385416229156749, 0.00918592452240873, 0.009564766304063763, 0.008685153716318313, 0.00955842792265923, 0.009529978269640525, 0.011313868674125224, 0.008784906763936933, 0.009087598827474983, 0.010339450655190433, 0.009185244451306905, 0.008665815002128903], "moving_avg_accuracy_train": [0.049769846316675886, 0.1027937167629891, 0.1534097220419781, 0.2052821869628726, 0.2553980272770596, 0.30235521086810596, 0.34795930281885534, 0.389370431184072, 0.4246433962271174, 0.4563912817682115, 0.4904371662228964, 0.5211854911749885, 0.5493118829662659, 0.5743142458820332, 0.598232352082405, 0.6209906881412371, 0.6429913325227113, 0.6604159708708924, 0.6772118916640173, 0.6934157411419493, 0.704946898115102, 0.7199768772312404, 0.7329763019977343, 0.743617913676883, 0.7570614017307986, 0.7671685012794796, 0.7778663416219542, 0.7863043182767115, 0.7939565538885938, 0.8017826016189665, 0.8097328526120163, 0.8171205212890761, 0.8226349307234759, 0.8296274296857813, 0.8358277448971128, 0.8416871906397289, 0.8478693924067822, 0.8526317383901257, 0.8569708595631101, 0.8620665448092724, 0.8670757304676955, 0.8714631258709997, 0.8756142499244959, 0.8780784772714705, 0.8829883716193512, 0.8872493106086712, 0.8915048634383079, 0.8954535517207229, 0.8987097160784587, 0.9028004411587358, 0.9065495590952801, 0.9086591005787015, 0.9121245858696685, 0.914766939223224, 0.9182610205235392, 0.9219241658295279, 0.9242211826168224, 0.9271836800170541, 0.9299871475058433, 0.9320173006493159, 0.934283891603441, 0.9365470737966868, 0.9388861710670273, 0.9415400937293814, 0.9434054295945384, 0.9451377463446177, 0.9468200282577749, 0.94845735091534, 0.9505354439488061, 0.9519755751491635, 0.9537716002235329, 0.9555391574630844, 0.9573206211810616, 0.9584636672094118, 0.9595806561432325, 0.9612858159753379, 0.962188199592136, 0.9638697702579224, 0.965080914511892, 0.9667685436333311, 0.9678619076104834, 0.9690761249220633, 0.9703688472512855, 0.9717183453011662, 0.9726748020282017, 0.973709963194429, 0.974815994404748, 0.9759835195547586, 0.9771016854564256, 0.9779011325726971, 0.9785252678273321, 0.9794404121755513, 0.980324532006815, 0.9811574061870859, 0.9817581834255202, 0.9824825696960635, 0.9830787137681238, 0.9835989673913114, 0.984292735086704, 0.9848636475899384, 0.9853658430988016, 0.9858689723305881, 0.9862706353653864, 0.9866972362633716, 0.9870904776667963, 0.9876048301977356, 0.9880584468803429, 0.9884783276387372, 0.9888888084534442, 0.9892349536497664, 0.9894232514395518, 0.989639222426549, 0.9900033321779417, 0.9903101046149094, 0.9905745740641329, 0.9908754116351097, 0.9910903258287417, 0.9913814048530103, 0.9915712963617569, 0.991800327439867, 0.9920087805589756, 0.9922382410447447, 0.9924284794402701, 0.9925881043010142, 0.9927712942054457, 0.9928919872920532, 0.99300061107, 0.9931262382070477, 0.9932718907625426, 0.9934006168648598, 0.9934699673807548, 0.9935672600772031, 0.9936315720159113, 0.9937336305881297, 0.9938231581543168, 0.9939200090055518, 0.9939885735811871, 0.9940363308064017, 0.9940816374579045, 0.994131714039495, 0.9942116601950693, 0.99426501054461, 0.9943293019008633, 0.9943871641214912, 0.9944252892271992, 0.9944712275663841, 0.9944962960299838, 0.9945560600281759, 0.9945912464360726, 0.99462058905437, 0.9946632734525044, 0.9946830882203492, 0.9947055718090285, 0.994730457336459, 0.994764480055194, 0.9947811496091985, 0.9948263791423262, 0.9948531348292841, 0.9948841903939747, 0.9949307415926725, 0.9949424107369766, 0.9949552381156599, 0.9949877090957605, 0.9950029820849939, 0.995012077477685, 0.995020263331107, 0.9950346060456154, 0.9950428641910539, 0.9950758731588533, 0.9950753542953489, 0.9950958136574807, 0.9951072516369707, 0.9951175458185116, 0.9951291357307082, 0.9951256157588279, 0.9951434101722401, 0.9951710148395398, 0.9951819081472525, 0.9951754360825272, 0.9951858872659411, 0.9952022687774422, 0.9952356133282694], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 987175013, "moving_var_accuracy_train": [0.022293338421469834, 0.04536778211328952, 0.06388882381558436, 0.08171671498675077, 0.09614942054164959, 0.10637927230471372, 0.114458943898114, 0.1184469834806267, 0.11779992369891513, 0.11509128545599732, 0.11401425714511342, 0.11112196681683709, 0.10712961537192134, 0.10204271719707482, 0.09698712771527772, 0.09194989168525064, 0.08711115769552645, 0.08113260412005792, 0.07555827030565244, 0.07036552591621849, 0.06452568155485205, 0.06010621584945085, 0.05561645966284338, 0.051074008788727185, 0.047593154249356416, 0.043753219976003295, 0.04040789207034068, 0.03700789791354268, 0.03383411851092623, 0.03100192986753627, 0.028470595298455054, 0.026114734604947634, 0.023776939547144656, 0.021839300968070775, 0.02000136604974251, 0.01831022738446422, 0.016823181214214816, 0.015344982546178953, 0.013979936044545636, 0.01281563651324267, 0.011759900330563369, 0.010757153443331452, 0.009836524577165938, 0.008907523867207539, 0.008233735043052937, 0.007573761948384007, 0.006979373322518072, 0.006421765242631403, 0.005875012175289561, 0.005438117242902275, 0.00502080848633111, 0.004558779125130481, 0.004210987507334615, 0.0038527270378065667, 0.003577331771224815, 0.0033403662958974133, 0.0030538162413976883, 0.0028274221348753388, 0.002615414791034089, 0.0023909670080042417, 0.0021981072181837197, 0.002024394439123771, 0.0018711973795724254, 0.0017474673910949936, 0.0016040359529940667, 0.0014706406495981045, 0.0013490472365563192, 0.001238269942265471, 0.001153309183940583, 0.0010566440664147125, 0.0009800110143831124, 0.0009101282403006187, 0.0008476779330767814, 0.0007746691277754456, 0.0007084311935024027, 0.0006637562046293913, 0.0006047092498932424, 0.000569687444040218, 0.0005259205332715099, 0.0004989613084081219, 0.00045982418064611846, 0.00042711067569917036, 0.00039943978731348136, 0.00037588611346181907, 0.00034653078735185977, 0.00032152173637726224, 0.00030037930808333356, 0.00028260941205816603, 0.0002656011257052064, 0.00024479305436011817, 0.00022381965226881174, 0.000208975089644628, 0.00019511259156446763, 0.0001818444470094779, 0.00016690840192051644, 0.00015494018094902823, 0.00014264465264599927, 0.00013081616187335797, 0.0001220663682225564, 0.00011279320117744473, 0.00010378368402180253, 9.568356683452495e-05, 8.75672088927831e-05, 8.04483829389606e-05, 7.379529385737177e-05, 6.879679120638872e-05, 6.37690249384075e-05, 5.897882110599452e-05, 5.45973894885773e-05, 5.021599901215226e-05, 4.5513503629679155e-05, 4.1381944471732155e-05, 3.8436933224092346e-05, 3.544022385443124e-05, 3.252569827514061e-05, 3.008765764462807e-05, 2.7494584875785253e-05, 2.5507669373529676e-05, 2.3281431502023202e-05, 2.1425385464483004e-05, 1.967392124382938e-05, 1.818039815021099e-05, 1.668807415937907e-05, 1.52485876089494e-05, 1.4025755717825482e-05, 1.2754281536436584e-05, 1.1585045509011781e-05, 1.0568580556175614e-05, 9.702654502857901e-06, 8.881522737332038e-06, 8.036655910092925e-06, 7.318183138123328e-06, 6.623589053454732e-06, 6.054973717578568e-06, 5.521613011787066e-06, 5.05387249707283e-06, 4.590795156653985e-06, 4.152242414030391e-06, 3.7554924066608234e-06, 3.4025121422088587e-06, 3.1197834181079844e-06, 2.8334214144622024e-06, 2.587279679415966e-06, 2.3586840406583607e-06, 2.135897349759755e-06, 1.9413005938473378e-06, 1.7528263852678423e-06, 1.6096893660602043e-06, 1.4598631791602435e-06, 1.3216257644812022e-06, 1.2058608086299707e-06, 1.0888083529896605e-06, 9.84477123529823e-07, 8.916030164562806e-07, 8.128606233217734e-07, 7.340754272659489e-07, 6.790792805419589e-07, 6.176141535490447e-07, 5.645327710784317e-07, 5.27582620872376e-07, 4.760498791442651e-07, 4.2992576602479957e-07, 3.964224703606067e-07, 3.588796011256786e-07, 3.237361765269555e-07, 2.919656326404738e-07, 2.646204905116359e-07, 2.387722141552163e-07, 2.2470132033631736e-07, 2.0223361127671116e-07, 1.857775196385916e-07, 1.68377214048061e-07, 1.5249322420565193e-07, 1.3845283636757928e-07, 1.247190645491669e-07, 1.150969284323928e-07, 1.1044539449975174e-07, 1.0046883242608478e-07, 9.079893777974657e-08, 8.270208911453844e-08, 7.684706547463664e-08, 7.916909055598909e-08], "duration": 74161.111489, "accuracy_train": [0.497698463166759, 0.5800085507798081, 0.6089537695528793, 0.6721343712509229, 0.7064405901047435, 0.7249698631875231, 0.7583961303755998, 0.7620705864710224, 0.7421000816145257, 0.7421222516380583, 0.7968501263150609, 0.7979204157438169, 0.802449409087763, 0.7993355121239387, 0.8134953078857512, 0.8258157126707272, 0.8409971319559801, 0.817237716004522, 0.8283751788021411, 0.839250386443337, 0.8087273108734773, 0.8552466892764857, 0.8499711248961794, 0.8393924187892212, 0.8780527942160392, 0.858132397217608, 0.8741469047042267, 0.8622461081695275, 0.8628266743955334, 0.872217031192322, 0.8812851115494648, 0.8836095393826136, 0.872264615633075, 0.8925599203465301, 0.8916305817990956, 0.8944222023232743, 0.9035092083102622, 0.8954928522402179, 0.8960229501199704, 0.9079277120247323, 0.9121584013935032, 0.9109496845007383, 0.9129743664059615, 0.9002565233942414, 0.9271774207502769, 0.9255977615125508, 0.9298048389050388, 0.9309917462624585, 0.9280151952980805, 0.9396169668812293, 0.9402916205241787, 0.9276449739294942, 0.9433139534883721, 0.9385481194052234, 0.949707752226375, 0.9548924735834257, 0.9448943337024732, 0.95384615661914, 0.9552183549049464, 0.9502886789405685, 0.9546832101905685, 0.9569157135358989, 0.9599380465000923, 0.9654253976905685, 0.9601934523809523, 0.9607285970953304, 0.9619605654761905, 0.9631932548334257, 0.96923828125, 0.9649367559523809, 0.9699358258928571, 0.9714471726190477, 0.9733537946428571, 0.9687510814645626, 0.9696335565476191, 0.9766322544642857, 0.9703096521433187, 0.97900390625, 0.9759812127976191, 0.9819572057262828, 0.9777021834048542, 0.9800040807262828, 0.9820033482142857, 0.9838638277500923, 0.9812829125715209, 0.9830264136904762, 0.9847702752976191, 0.9864912459048542, 0.9871651785714286, 0.98509615661914, 0.9841424851190477, 0.9876767113095238, 0.9882816104881875, 0.9886532738095238, 0.9871651785714286, 0.9890020461309523, 0.9884440104166666, 0.98828125, 0.9905366443452381, 0.9900018601190477, 0.9898856026785714, 0.9903971354166666, 0.9898856026785714, 0.9905366443452381, 0.9906296502976191, 0.9922340029761905, 0.9921409970238095, 0.9922572544642857, 0.9925831357858066, 0.9923502604166666, 0.9911179315476191, 0.9915829613095238, 0.9932803199404762, 0.9930710565476191, 0.9929547991071429, 0.9935829497739018, 0.9930245535714286, 0.9940011160714286, 0.9932803199404762, 0.9938616071428571, 0.9938848586309523, 0.9943033854166666, 0.994140625, 0.9940247280477114, 0.9944200033453304, 0.9939782250715209, 0.9939782250715209, 0.9942568824404762, 0.9945827637619971, 0.9945591517857143, 0.9940941220238095, 0.9944428943452381, 0.9942103794642857, 0.9946521577380952, 0.99462890625, 0.9947916666666666, 0.9946056547619048, 0.9944661458333334, 0.9944893973214286, 0.9945824032738095, 0.9949311755952381, 0.9947451636904762, 0.9949079241071429, 0.9949079241071429, 0.9947684151785714, 0.9948846726190477, 0.9947219122023809, 0.9950939360119048, 0.9949079241071429, 0.9948846726190477, 0.9950474330357143, 0.9948614211309523, 0.9949079241071429, 0.9949544270833334, 0.9950706845238095, 0.9949311755952381, 0.9952334449404762, 0.9950939360119048, 0.9951636904761905, 0.9953497023809523, 0.9950474330357143, 0.9950706845238095, 0.9952799479166666, 0.9951404389880952, 0.9950939360119048, 0.9950939360119048, 0.9951636904761905, 0.9951171875, 0.9953729538690477, 0.9950706845238095, 0.9952799479166666, 0.9952101934523809, 0.9952101934523809, 0.9952334449404762, 0.9950939360119048, 0.9953035598929494, 0.9954194568452381, 0.9952799479166666, 0.9951171875, 0.9952799479166666, 0.9953497023809523, 0.9955357142857143], "end": "2016-01-24 09:16:50.248000", "learning_rate_per_epoch": [0.007320244796574116, 0.006984519772231579, 0.006664191838353872, 0.006358555052429438, 0.0060669356025755405, 0.005788690410554409, 0.005523206200450659, 0.005269898101687431, 0.005028207320719957, 0.004797601141035557, 0.00457757106050849, 0.004367631860077381, 0.004167321138083935, 0.003976197447627783, 0.0037938388995826244, 0.0036198438610881567, 0.003453828627243638, 0.0032954271882772446, 0.003144290531054139, 0.003000085474923253, 0.002862493973225355, 0.0027312126476317644, 0.0026059523224830627, 0.002486436627805233, 0.0023724022321403027, 0.002263597911223769, 0.0021597836166620255, 0.0020607304759323597, 0.0019662200938910246, 0.0018760442035272717, 0.0017900040838867426, 0.0017079099779948592, 0.0016295808600261807, 0.0015548442024737597, 0.0014835350448265672, 0.0014154963428154588, 0.0013505780370905995, 0.0012886370532214642, 0.0012295368360355496, 0.0011731471167877316, 0.001119343563914299, 0.001068007666617632, 0.0010190261527895927, 0.000972291047219187, 0.0009276993223465979, 0.0008851526654325426, 0.0008445573039352894, 0.0008058237726800144, 0.0007688666228204966, 0.000733604421839118, 0.0006999594625085592, 0.0006678575300611556, 0.0006372279021888971, 0.0006080029997974634, 0.0005801184452138841, 0.0005535127129405737, 0.0005281271878629923, 0.0005039059324190021, 0.0004807955410797149, 0.0004587450239341706, 0.0004377058066893369, 0.00041763149783946574, 0.0003984778595622629, 0.00038020266219973564, 0.0003627655969467014, 0.0003461282467469573, 0.00033025394077412784, 0.0003151076671201736, 0.0003006560436915606, 0.0002868672017939389, 0.0002737107570283115, 0.0002611576928757131, 0.0002491803315933794, 0.00023775229055900127, 0.00022684836585540324, 0.00021644451771862805, 0.000206517826882191, 0.0001970463927136734, 0.0001880093477666378, 0.0001793867559172213, 0.0001711596269160509, 0.00016330981452483684, 0.00015582001651637256, 0.00014867371646687388, 0.0001418551692040637, 0.00013534932804759592, 0.00012914185936097056, 0.00012321908434387296, 0.00011756794265238568, 0.00011217597784707323, 0.00010703130101319402, 0.00010212257620878518, 9.743897680891678e-05, 9.29701782297343e-05, 8.870632882462814e-05, 8.463802805636078e-05, 8.075631194515154e-05, 7.705262396484613e-05, 7.351879321504384e-05, 7.014703442109749e-05, 6.692991155432537e-05, 6.386033783201128e-05, 6.0931539337616414e-05, 5.813706593471579e-05, 5.54707512492314e-05, 5.2926723583368585e-05, 5.049936953582801e-05, 4.818334127776325e-05, 4.597353108692914e-05, 4.3865067709703e-05, 4.185330544714816e-05, 3.993380596511997e-05, 3.81023419322446e-05, 3.635487155406736e-05, 3.468754584901035e-05, 3.30966868204996e-05, 3.1578787456965074e-05, 3.0130504455883056e-05, 2.874864367186092e-05, 2.7430158297647722e-05, 2.61721415881766e-05, 2.497182140359655e-05, 2.382655111432541e-05, 2.273380414408166e-05, 2.1691173969884403e-05, 2.069636138912756e-05, 1.9747174519579858e-05, 1.8841519704437815e-05, 1.7977399693336338e-05, 1.7152910004369915e-05, 1.6366233467124403e-05, 1.5615636584698223e-05, 1.4899464076734148e-05, 1.4216137060429901e-05, 1.3564148503064644e-05, 1.2942062312504277e-05, 1.2348506061243825e-05, 1.1782171895902138e-05, 1.1241811989748385e-05, 1.072623399522854e-05, 1.023430195346009e-05, 9.764930837263819e-06, 9.317086551163811e-06, 8.889781383913942e-06, 8.482073099003173e-06, 8.093064025160857e-06, 7.721895599388517e-06, 7.367749731201911e-06, 7.029846074146917e-06, 6.707439297315432e-06, 6.399819085345371e-06, 6.106306955189211e-06, 5.826256256113993e-06, 5.559049441217212e-06, 5.3040971579321194e-06, 5.0608377932803705e-06, 4.828734745387919e-06, 4.607276878232369e-06, 4.3959753384115174e-06, 4.194364919385407e-06, 4.002000878244871e-06, 3.818458935711533e-06, 3.6433348213904537e-06, 3.476242227407056e-06, 3.3168130357807968e-06, 3.1646954994357657e-06, 3.0195544695743592e-06, 2.881070031435229e-06, 2.7489368221722543e-06, 2.6228635761071928e-06, 2.5025724426086526e-06, 2.3877980765973916e-06, 2.278287638546317e-06, 2.1737996576121077e-06, 2.0741035768878646e-06, 1.978979980776785e-06, 1.8882188896895968e-06, 1.801620328478748e-06, 1.7189934169437038e-06, 1.6401560287704342e-06, 1.564934336784063e-06, 1.493162471888354e-06], "accuracy_valid": [0.48365140248493976, 0.5673651637801205, 0.5964384883283133, 0.6501920769013554, 0.6746885000941265, 0.6977200795368976, 0.7217694018260542, 0.7302231386483433, 0.7062370576054217, 0.7057178910956325, 0.7524208160768072, 0.7483822006777108, 0.7553916839231928, 0.7524517013365963, 0.7639469008847892, 0.7655955854668675, 0.7771613798945783, 0.7615981504141567, 0.7706916533320783, 0.7713828948606928, 0.7489219573606928, 0.7844855986445783, 0.7766422133847892, 0.7701827819088856, 0.8038962490587349, 0.7849032850150602, 0.7926951948418675, 0.7794792451054217, 0.7794601256588856, 0.7859901520143072, 0.7934173216302711, 0.7961440488516567, 0.7885036238704819, 0.7959396178463856, 0.8035094479480422, 0.7952895566641567, 0.8066832760730422, 0.7948733410203314, 0.7966529202748494, 0.8092673428087349, 0.8150252376694277, 0.8097865093185241, 0.8102130200489458, 0.7970794310052711, 0.8239878459149097, 0.814098679875753, 0.8177313747176205, 0.8188094173569277, 0.8193491740399097, 0.8231230586408133, 0.8205492869917168, 0.8187285273908133, 0.8281073512801205, 0.8249952936746988, 0.827782320689006, 0.8344461831701807, 0.8247408579631024, 0.8364096032567772, 0.8325739481362951, 0.8307223032756024, 0.8322077371987951, 0.8292574595256024, 0.8331740046121988, 0.8346388483621988, 0.8346594385353916, 0.8367449289344879, 0.8338961314006024, 0.8406511789344879, 0.8410173898719879, 0.8351271296121988, 0.8377420816076807, 0.8392275155308735, 0.8449545251317772, 0.8387583537274097, 0.8382097726844879, 0.8437132318335843, 0.8443235833960843, 0.8439573724585843, 0.8429602197853916, 0.8451677804969879, 0.846470844314759, 0.8434896813817772, 0.8472532708960843, 0.8488813653049698, 0.8500005882906627, 0.8476194818335843, 0.8471929711031627, 0.8463987787085843, 0.8490137307040663, 0.8472635659826807, 0.8476297769201807, 0.8481283532567772, 0.8498476327183735, 0.8494711266942772, 0.8477518472326807, 0.8490843255835843, 0.8452589655496988, 0.8506918298192772, 0.8502035485692772, 0.8497461525790663, 0.8483930840549698, 0.8492269860692772, 0.8516683923192772, 0.8494505365210843, 0.8514345467808735, 0.8539877282567772, 0.8531641213290663, 0.8497049722326807, 0.8489416650978916, 0.8505285791603916, 0.8492063958960843, 0.8514345467808735, 0.8512918862951807, 0.8504682793674698, 0.8520243081701807, 0.8503153237951807, 0.8490946206701807, 0.8509359704442772, 0.8531435311558735, 0.8512109963290663, 0.8494505365210843, 0.8520346032567772, 0.8536524025790663, 0.8524214043674698, 0.855015766189759, 0.8541406838290663, 0.8527567300451807, 0.8530317559299698, 0.8545068947665663, 0.8524008141942772, 0.8536318124058735, 0.8563379494540663, 0.8542421639683735, 0.8558599632906627, 0.8549745858433735, 0.8553510918674698, 0.8525434746799698, 0.8546083749058735, 0.8541200936558735, 0.8538965432040663, 0.8561849938817772, 0.8550966561558735, 0.8552084313817772, 0.8553510918674698, 0.8543642342808735, 0.8541303887424698, 0.8555849374058735, 0.8558393731174698, 0.8540083184299698, 0.8562158791415663, 0.8550054711031627, 0.8543642342808735, 0.8556055275790663, 0.8559614434299698, 0.8547510353915663, 0.8538759530308735, 0.8554834572665663, 0.8552290215549698, 0.8548628106174698, 0.8536215173192772, 0.8554628670933735, 0.8555746423192772, 0.8545980798192772, 0.8555952324924698, 0.8557275978915663, 0.8553613869540663, 0.8558393731174698, 0.8546083749058735, 0.8549848809299698, 0.8549745858433735, 0.8537435876317772, 0.8550966561558735, 0.8547304452183735, 0.8543642342808735, 0.8539877282567772, 0.8539980233433735, 0.8547407403049698, 0.8543539391942772, 0.8543642342808735, 0.8562158791415663, 0.8552290215549698, 0.8549745858433735], "accuracy_test": 0.8516621492346939, "start": "2016-01-23 12:40:49.137000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0], "accuracy_train_last": 0.9955357142857143, "batch_size_eval": 1024, "accuracy_train_std": [0.01560439006919339, 0.015042658920266903, 0.01301937126430674, 0.017223624674879354, 0.02035020532113167, 0.019893645880803827, 0.02204336902099184, 0.02229537249048484, 0.020870096183361376, 0.022486132692493804, 0.020098803095894388, 0.021238981478393437, 0.022013564012353215, 0.022340657228011568, 0.02161971287598014, 0.022048567554632772, 0.02190760012998366, 0.0215813235710068, 0.021299860134533843, 0.024173278697802364, 0.022808837900719126, 0.02272673704110721, 0.021993615568071306, 0.02310922781280742, 0.023311055999525602, 0.022827286001416397, 0.02361682822562694, 0.022109937103042498, 0.022760620331628515, 0.023794969625585104, 0.023496990212847912, 0.023110170118539924, 0.02247866463198311, 0.02169819146219571, 0.021331756695727767, 0.02074490426030004, 0.021042832970533235, 0.01990585533628675, 0.02119351376820322, 0.020570622233060002, 0.02026410723215279, 0.019818126100820985, 0.017619340163885795, 0.01928482851638937, 0.01844955392902739, 0.01764387247225907, 0.01771358251978773, 0.017824578844618678, 0.01780176097361272, 0.01780722814550169, 0.015524212507064754, 0.016089248224821766, 0.014502582164091388, 0.014709537102695562, 0.014765349635704405, 0.013391516083333246, 0.014480038406492172, 0.013115404866494113, 0.014910950400525907, 0.014641565622903724, 0.01427752773623999, 0.012091523547162449, 0.0119320654335517, 0.010919501710776835, 0.012421953827795778, 0.011596288421804856, 0.01117310819767947, 0.012591222692752096, 0.011408996052404507, 0.011390191832746177, 0.011373685900190882, 0.010476388805543278, 0.009939134092205584, 0.010191625354904426, 0.01014159439519441, 0.009666049389030748, 0.009288092490223823, 0.008535449947954389, 0.009582688344593561, 0.006730471156708118, 0.008422950381165873, 0.008107806630462261, 0.007292817287283677, 0.006669365204687504, 0.007916964954677617, 0.0072359587305468485, 0.006449889036167996, 0.005666708729114356, 0.005543029920294398, 0.006297431987191177, 0.00687835454245026, 0.005083096511138285, 0.005686927707064167, 0.005357134783703407, 0.005822722308791632, 0.004703069708879618, 0.005552628669550269, 0.005609924459509794, 0.004328088229955724, 0.004867953226451537, 0.004584554345641882, 0.004387636511492098, 0.004142225032495561, 0.004338568179901439, 0.0041599375045622685, 0.0035142837853253494, 0.003584652593117888, 0.0040004048434541485, 0.003728678741220573, 0.003520508726219941, 0.003951385004882441, 0.0039636797502529825, 0.0033909726625907712, 0.0037083025613726993, 0.0032356348440795465, 0.002908013275852511, 0.003284637906698345, 0.00315055518711948, 0.0033301618630727, 0.0032618448505859713, 0.0031105763952383085, 0.002826466806960166, 0.0031174341075324344, 0.0028050946587256354, 0.0026389001929648893, 0.003196418882259251, 0.0028983738826146984, 0.002763799064716343, 0.0026295397691304093, 0.002688338622171621, 0.003065671451608912, 0.0029795491476759845, 0.0027735624096081254, 0.002594599373988713, 0.0026294750034836444, 0.0030783424783175904, 0.0027722951191943603, 0.002824457708158522, 0.0027108682519912846, 0.002796468759144464, 0.0024226220754522145, 0.0025120217328998334, 0.0026662269606049656, 0.0024991834695401146, 0.0025821717098915217, 0.0025466483575321925, 0.0025577693768748834, 0.0025704201969942754, 0.00279917402253468, 0.0026429077824372807, 0.002587191732535293, 0.0025754631254690783, 0.0024347539045292614, 0.0025026422494593874, 0.002447930094540896, 0.002646587290761197, 0.0024870398597880475, 0.00255269149403998, 0.0026355333547813903, 0.0025995953979489742, 0.0023863096366160672, 0.0022848062679572635, 0.002343330143858722, 0.0026315302445379897, 0.002534838799287253, 0.002489647048266477, 0.0027203247399943575, 0.0026099730655510193, 0.0025577693768748834, 0.002618245584125604, 0.0024660831247212845, 0.002702379134257373, 0.002474290475153093, 0.0025501487609503556, 0.0025704201969942754, 0.0025949815731154324, 0.002640144785916898, 0.0024568583425482802, 0.0025661048477160333, 0.002353000047981927, 0.002474290475153093, 0.0023648032245333185], "accuracy_test_std": 0.010078872570123212, "error_valid": [0.5163485975150602, 0.4326348362198795, 0.40356151167168675, 0.3498079230986446, 0.3253114999058735, 0.30227992046310237, 0.2782305981739458, 0.2697768613516567, 0.29376294239457834, 0.29428210890436746, 0.24757918392319278, 0.2516177993222892, 0.24460831607680722, 0.24754829866340367, 0.23605309911521077, 0.23440441453313254, 0.22283862010542166, 0.23840184958584332, 0.22930834666792166, 0.22861710513930722, 0.2510780426393072, 0.21551440135542166, 0.22335778661521077, 0.22981721809111444, 0.1961037509412651, 0.21509671498493976, 0.20730480515813254, 0.22052075489457834, 0.22053987434111444, 0.21400984798569278, 0.20658267836972888, 0.20385595114834332, 0.2114963761295181, 0.20406038215361444, 0.19649055205195776, 0.20471044333584332, 0.19331672392695776, 0.20512665897966864, 0.20334707972515065, 0.1907326571912651, 0.1849747623305723, 0.19021349068147586, 0.1897869799510542, 0.20292056899472888, 0.1760121540850903, 0.18590132012424698, 0.18226862528237953, 0.1811905826430723, 0.1806508259600903, 0.17687694135918675, 0.1794507130082832, 0.18127147260918675, 0.17189264871987953, 0.17500470632530118, 0.17221767931099397, 0.1655538168298193, 0.17525914203689763, 0.16359039674322284, 0.16742605186370485, 0.16927769672439763, 0.16779226280120485, 0.17074254047439763, 0.16682599538780118, 0.16536115163780118, 0.1653405614646084, 0.16325507106551207, 0.16610386859939763, 0.15934882106551207, 0.15898261012801207, 0.16487287038780118, 0.1622579183923193, 0.1607724844691265, 0.15504547486822284, 0.1612416462725903, 0.16179022731551207, 0.15628676816641573, 0.15567641660391573, 0.15604262754141573, 0.1570397802146084, 0.15483221950301207, 0.15352915568524095, 0.15651031861822284, 0.15274672910391573, 0.15111863469503017, 0.14999941170933728, 0.15238051816641573, 0.15280702889683728, 0.15360122129141573, 0.15098626929593373, 0.1527364340173193, 0.1523702230798193, 0.15187164674322284, 0.1501523672816265, 0.15052887330572284, 0.1522481527673193, 0.15091567441641573, 0.15474103445030118, 0.14930817018072284, 0.14979645143072284, 0.15025384742093373, 0.15160691594503017, 0.15077301393072284, 0.14833160768072284, 0.15054946347891573, 0.1485654532191265, 0.14601227174322284, 0.14683587867093373, 0.1502950277673193, 0.1510583349021084, 0.1494714208396084, 0.15079360410391573, 0.1485654532191265, 0.1487081137048193, 0.14953172063253017, 0.1479756918298193, 0.1496846762048193, 0.1509053793298193, 0.14906402955572284, 0.1468564688441265, 0.14878900367093373, 0.15054946347891573, 0.14796539674322284, 0.14634759742093373, 0.14757859563253017, 0.14498423381024095, 0.14585931617093373, 0.1472432699548193, 0.14696824407003017, 0.14549310523343373, 0.14759918580572284, 0.1463681875941265, 0.14366205054593373, 0.1457578360316265, 0.14414003670933728, 0.1450254141566265, 0.14464890813253017, 0.14745652532003017, 0.1453916250941265, 0.1458799063441265, 0.14610345679593373, 0.14381500611822284, 0.1449033438441265, 0.14479156861822284, 0.14464890813253017, 0.1456357657191265, 0.14586961125753017, 0.1444150625941265, 0.14416062688253017, 0.14599168157003017, 0.14378412085843373, 0.14499452889683728, 0.1456357657191265, 0.14439447242093373, 0.14403855657003017, 0.14524896460843373, 0.1461240469691265, 0.14451654273343373, 0.14477097844503017, 0.14513718938253017, 0.14637848268072284, 0.1445371329066265, 0.14442535768072284, 0.14540192018072284, 0.14440476750753017, 0.14427240210843373, 0.14463861304593373, 0.14416062688253017, 0.1453916250941265, 0.14501511907003017, 0.1450254141566265, 0.14625641236822284, 0.1449033438441265, 0.1452695547816265, 0.1456357657191265, 0.14601227174322284, 0.1460019766566265, 0.14525925969503017, 0.14564606080572284, 0.1456357657191265, 0.14378412085843373, 0.14477097844503017, 0.1450254141566265], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-3, 3], "momentum": 0.6662444695824572, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.007672107366224854, "patience_threshold": 1, "do_flip": true, "batch_size": 32, "optimization": "nesterov_momentum", "nb_data_augmentation": 1, "learning_rate_decay_method": "exp", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.00015043039532124943, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.045862537453362655}, "accuracy_valid_max": 0.8563379494540663, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import os\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-3, 3)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8549745858433735, "loss_train": [4.875814914703369, 4.321630954742432, 3.9500112533569336, 3.6532604694366455, 3.4015188217163086, 3.1841585636138916, 2.9960625171661377, 2.8292012214660645, 2.687028169631958, 2.5573806762695312, 2.444373846054077, 2.339771270751953, 2.2533915042877197, 2.1693246364593506, 2.0953166484832764, 2.0263845920562744, 1.966183066368103, 1.9108245372772217, 1.860610008239746, 1.8111391067504883, 1.7668076753616333, 1.7267597913742065, 1.6875615119934082, 1.654044508934021, 1.6194045543670654, 1.5907059907913208, 1.5626153945922852, 1.5319111347198486, 1.5061886310577393, 1.4817097187042236, 1.4585192203521729, 1.4340459108352661, 1.4166394472122192, 1.3949590921401978, 1.3796545267105103, 1.3567290306091309, 1.3394888639450073, 1.3200329542160034, 1.3086756467819214, 1.287758231163025, 1.2746787071228027, 1.2598967552185059, 1.2448031902313232, 1.227763056755066, 1.219884991645813, 1.202846646308899, 1.1918535232543945, 1.1795011758804321, 1.170520544052124, 1.1559474468231201, 1.146787166595459, 1.1357022523880005, 1.1261816024780273, 1.1189403533935547, 1.1065452098846436, 1.0965651273727417, 1.090187668800354, 1.082403302192688, 1.07384192943573, 1.064732313156128, 1.0559364557266235, 1.0487099885940552, 1.0422343015670776, 1.0365291833877563, 1.0294151306152344, 1.0267720222473145, 1.0185463428497314, 1.0131549835205078, 1.0071407556533813, 1.001779317855835, 0.9941405057907104, 0.9899789094924927, 0.9851021766662598, 0.9804120063781738, 0.9740217328071594, 0.9734514951705933, 0.9674394726753235, 0.962007462978363, 0.9620437622070312, 0.9554346799850464, 0.9526731967926025, 0.9489068388938904, 0.946183979511261, 0.9440857172012329, 0.9391174912452698, 0.9392160773277283, 0.9333545565605164, 0.930686354637146, 0.9286265969276428, 0.9255087375640869, 0.9232264757156372, 0.9204970598220825, 0.9172021746635437, 0.917966365814209, 0.9156164526939392, 0.9116361737251282, 0.9098401665687561, 0.9103061556816101, 0.9086026549339294, 0.9053347110748291, 0.9050829410552979, 0.9042316675186157, 0.9020880460739136, 0.899928629398346, 0.8971133828163147, 0.8981315493583679, 0.8956051468849182, 0.8933904767036438, 0.8939189910888672, 0.8921660780906677, 0.8909608125686646, 0.8905239701271057, 0.8917946219444275, 0.8893592953681946, 0.8873832821846008, 0.8861725926399231, 0.8845951557159424, 0.8838387727737427, 0.8860852718353271, 0.8845613598823547, 0.8828117847442627, 0.8821230530738831, 0.8815978765487671, 0.8811507225036621, 0.8806988000869751, 0.8789999485015869, 0.8798331022262573, 0.8780570030212402, 0.8792199492454529, 0.8778142333030701, 0.876618504524231, 0.8783349394798279, 0.8769687414169312, 0.8771473169326782, 0.877476155757904, 0.8775123357772827, 0.8763899207115173, 0.8750975728034973, 0.8740573525428772, 0.8738738298416138, 0.8745872378349304, 0.8751248121261597, 0.8750891089439392, 0.8739535212516785, 0.8723487854003906, 0.8723493814468384, 0.8727341890335083, 0.872751772403717, 0.8731704950332642, 0.8739288449287415, 0.8724824786186218, 0.8728704452514648, 0.8714355230331421, 0.8725755214691162, 0.871619701385498, 0.8700374364852905, 0.8714178204536438, 0.8717944025993347, 0.8722078800201416, 0.8692126274108887, 0.8705118894577026, 0.868585467338562, 0.8714869022369385, 0.8704394102096558, 0.8701983690261841, 0.8702549934387207, 0.870392382144928, 0.8706542253494263, 0.8717308044433594, 0.8692423701286316, 0.8710998892784119, 0.86944979429245, 0.8700383901596069, 0.8706942796707153, 0.8701928853988647, 0.8691588044166565, 0.8712372183799744, 0.869167149066925, 0.8686606287956238, 0.8688852787017822, 0.8691843748092651, 0.868339478969574], "accuracy_train_first": 0.497698463166759, "model": "residualv3", "loss_std": [0.29431647062301636, 0.21107254922389984, 0.198348730802536, 0.19170548021793365, 0.185328871011734, 0.18124602735042572, 0.18116286396980286, 0.17719720304012299, 0.17612919211387634, 0.17120523750782013, 0.16840490698814392, 0.16291093826293945, 0.16066795587539673, 0.1606268286705017, 0.15775051712989807, 0.15484578907489777, 0.15516763925552368, 0.15161187946796417, 0.14940176904201508, 0.14454485476016998, 0.1446739137172699, 0.14377081394195557, 0.13998697698116302, 0.14001739025115967, 0.13812783360481262, 0.1367311030626297, 0.1354757398366928, 0.1319090574979782, 0.13004940748214722, 0.12847493588924408, 0.12430744618177414, 0.12310196459293365, 0.1233888790011406, 0.12048816680908203, 0.12296141684055328, 0.11539427936077118, 0.11780271679162979, 0.11126770079135895, 0.11284945905208588, 0.10895740240812302, 0.10935650765895844, 0.10782255232334137, 0.10548116266727448, 0.10405101627111435, 0.10100418329238892, 0.0996311604976654, 0.09739457070827484, 0.09735718369483948, 0.09700058400630951, 0.09388097375631332, 0.08990880101919174, 0.0911577120423317, 0.0898529514670372, 0.08900408446788788, 0.08658389747142792, 0.08468092978000641, 0.08716440945863724, 0.08454808592796326, 0.08299434930086136, 0.08057674020528793, 0.08230577409267426, 0.07859273254871368, 0.08071915805339813, 0.07648155838251114, 0.07775887101888657, 0.07874192297458649, 0.0751013532280922, 0.0741676539182663, 0.07315140962600708, 0.07329396903514862, 0.07109058648347855, 0.07133561372756958, 0.07056241482496262, 0.06933673471212387, 0.06784185022115707, 0.06873549520969391, 0.06659889966249466, 0.06571035832166672, 0.06573591381311417, 0.06590384989976883, 0.06380967795848846, 0.06273932754993439, 0.06293037533760071, 0.06356345862150192, 0.06261393427848816, 0.06483752280473709, 0.06065848097205162, 0.06081997975707054, 0.06119903177022934, 0.06010301038622856, 0.0599934458732605, 0.05905059352517128, 0.05754151567816734, 0.060561079531908035, 0.05865701287984848, 0.05666607245802879, 0.055914781987667084, 0.05737437680363655, 0.05887017771601677, 0.05501199886202812, 0.055644791573286057, 0.05617973953485489, 0.05665125325322151, 0.054806601256132126, 0.054128244519233704, 0.05569080635905266, 0.056787580251693726, 0.052743587642908096, 0.05477883666753769, 0.05182763189077377, 0.053553998470306396, 0.053486984223127365, 0.05391770228743553, 0.05263004079461098, 0.05181696265935898, 0.05183111131191254, 0.05156809464097023, 0.051877211779356, 0.05240201577544212, 0.052345842123031616, 0.05317810922861099, 0.05119289085268974, 0.05016321688890457, 0.05060690641403198, 0.04998912289738655, 0.050487566739320755, 0.05070298910140991, 0.05034549906849861, 0.050561342388391495, 0.05095900222659111, 0.05123024433851242, 0.050290726125240326, 0.04953509941697121, 0.05074469745159149, 0.04997739940881729, 0.05078374966979027, 0.05256113037467003, 0.04961886629462242, 0.04975684732198715, 0.04832030087709427, 0.05096438154578209, 0.0502307265996933, 0.05069982260465622, 0.05026645585894585, 0.04677655175328255, 0.04940636083483696, 0.04896664619445801, 0.050298839807510376, 0.048269495368003845, 0.05047418177127838, 0.04954409599304199, 0.04932446405291557, 0.050163041800260544, 0.04910769686102867, 0.050245050340890884, 0.04856852442026138, 0.05000041425228119, 0.050888046622276306, 0.0487108938395977, 0.049274686723947525, 0.04901069402694702, 0.04800678789615631, 0.049121636897325516, 0.04761621728539467, 0.04801905155181885, 0.050095245242118835, 0.05074199289083481, 0.047654442489147186, 0.04986044019460678, 0.04927601292729378, 0.05084648355841637, 0.049354881048202515, 0.04994424805045128, 0.049549128860235214, 0.04818221926689148, 0.05031079426407814, 0.05061504617333412, 0.049432799220085144, 0.04839972034096718, 0.048747122287750244, 0.0499202199280262, 0.04953918978571892]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:10 2016", "state": "available"}], "summary": "f42b2916b7bac08cfb77770c202d0681"}