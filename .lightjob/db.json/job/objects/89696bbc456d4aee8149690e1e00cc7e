{"content": {"hp_model": {"f0": 16, "f1": 32, "f2": 64, "f3": 16, "nonlin": "rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.008661121143578256, 0.010089094712370052, 0.008749303188407331, 0.013681236523530382, 0.007317498166946443, 0.010921869951504024, 0.013919313877248568, 0.013778509491886842, 0.012883662216999664, 0.009148980935797395, 0.010412824081379238, 0.016749897401410065, 0.00801303010786668, 0.01154171557597004, 0.00954100045831767, 0.010612686511992079, 0.010582169982824347, 0.005800311559906722, 0.006961909693132534, 0.014112408807436116, 0.012329888480536087, 0.008743465396949835, 0.016505437865199292, 0.010292415410241146, 0.01164090089786229, 0.015454443784898401, 0.010731492425539883, 0.012478810872470407, 0.007619163427595121, 0.00974245560868897, 0.010285932821204286, 0.010362853885347044, 0.009998615083578896, 0.006340907629308737, 0.011629546706284855, 0.010894423468507775, 0.010424217969695962, 0.008932616807606679, 0.007632626291330198, 0.010301475866541933, 0.012315924297614491, 0.012320719536246956, 0.007725833378339978, 0.010002416641123685, 0.01047961498352062, 0.010342052386108158, 0.014082850848624711, 0.009465217875534709, 0.007438838892163024, 0.010467616028266557, 0.017295750484398607, 0.00733220342619951, 0.009405934339074922, 0.009940778773826871, 0.007616094054152305, 0.006890960700822218, 0.009328050051430207, 0.009134407040094638, 0.008069388114044786, 0.016377946277418214, 0.012365302887934995, 0.008811735997210433, 0.007847365112490819, 0.01027950821875521, 0.007180327935074799, 0.007718839807853773, 0.009839779941906624, 0.0154287651533235, 0.010868201101164324, 0.006805388842979496, 0.009135519345159961, 0.006768832458129337, 0.009307609815363351, 0.010880934783725489, 0.00749593350860275, 0.01287536175221499, 0.012961845614147784, 0.010506706126870553, 0.010277191103794553, 0.010099453286468461, 0.008855181445846278, 0.009319038465123544, 0.014390420250393901, 0.006501883942923658, 0.010934683166461234, 0.010441483845237893, 0.008972259549675734, 0.014145474367131603, 0.01659258085942552, 0.011032950170797938, 0.008660165581869148, 0.009453034233484836, 0.011601815182150216, 0.01544536765468764, 0.004238106271071025, 0.012623975981697538, 0.00799764171207299, 0.013126086936194138, 0.003883132828886063, 0.008397418025514266, 0.008027740614137547, 0.009384436087052351, 0.007092573926450762, 0.01311003852572562, 0.007142689356462717, 0.01060889159205679, 0.007644206035476343, 0.011826503264010463, 0.011063732946278312, 0.008590018731273267, 0.012186229145489624, 0.0134598894849118, 0.012748872776726499, 0.013895583529117198, 0.008807003984915513, 0.01452945833289093, 0.005289452012104958, 0.01228816967927283, 0.011830154338809124, 0.012898282421345497, 0.010590649858396297, 0.01024043176770541, 0.011455191115356969, 0.008039219399045405, 0.010437402138444187, 0.0067285331450049465, 0.012226395372900097, 0.007484476368696097, 0.008325192018005127, 0.008821343454056496, 0.011868167429990117, 0.00922576503968372, 0.0098666796299922, 0.008641008501585267, 0.006673623929989548, 0.008153676750364885, 0.010737560077298572, 0.006707534599428903, 0.00858271066754401, 0.008409528085168289, 0.013595857961608173, 0.00999018609218247, 0.012839387780812903, 0.009431308500556764, 0.013682981863157634, 0.019575104614024463, 0.01413261268244046, 0.006535743621179763, 0.014755287695677091, 0.01733245842580775, 0.010811583246707081, 0.012457134655545411, 0.011493374825446106, 0.010477654475928418, 0.008785970747673746, 0.0136515278859116, 0.010117570584817676, 0.010744272974824574, 0.012814241080199622, 0.010544716593599025, 0.015159417113048138, 0.011271242931321177, 0.014126290797245918, 0.011905542005446417, 0.012969576816532642, 0.008714113897046379, 0.013023757144853359, 0.0063382797463539834, 0.013103480560084053, 0.010911064443589219, 0.011506399820816882, 0.010991478201997727, 0.011561259327940668, 0.011854358657735153, 0.008450556174142707, 0.008337237831352949, 0.006781061773568098, 0.009973436758279743, 0.015171811390117564, 0.012785249979341988, 0.0071630222527149755, 0.012262171923433714, 0.01582526567582407, 0.006827135159736627, 0.012031168194866456, 0.011949045052975024, 0.013071156614153229, 0.014623793320525887, 0.00677333465637743, 0.009608336828288957, 0.008586339529601571, 0.009685865875142042, 0.012210114070824434, 0.01322789109056936, 0.012423567756990876, 0.011937793146228013, 0.01247563311800846, 0.012855157815107633, 0.010780578079502065, 0.008626261071838587, 0.008535193340241902, 0.004576697188298625, 0.009724084750443463, 0.013253342510372457, 0.006758296728544554, 0.011245333263575893, 0.011473158821092237, 0.012611959507883378, 0.013818909394785096, 0.012734601480542153, 0.008943550109244998, 0.016170307251461326, 0.011272887075507797, 0.0096948404769921, 0.010340658937097262, 0.014986637878070868, 0.010496666612995186, 0.01683047915559485, 0.011313952795014792, 0.006267062251575735, 0.00952416408715167, 0.014152720544337728, 0.010844007838000649, 0.012064719839447326, 0.008155627810613667, 0.012363357273841103, 0.012237216122678777, 0.015099883973142755, 0.01897951501362933, 0.010588224329879244, 0.013922904900484114, 0.018627839426480993, 0.01338271879618125, 0.013773583009753142, 0.01351006136557781, 0.013212509218697216, 0.015305200953601855, 0.015385890566460362, 0.014490038426106041, 0.012779960249052797, 0.009182267016829738, 0.013506485413372463, 0.014494771616183467, 0.013585298843685185, 0.012772605815636315, 0.01157739199922387, 0.011605871490311765, 0.010224046490797351, 0.00861537151338757, 0.01293747464746198, 0.010746486763316116, 0.010634880386685131, 0.009425972852088174, 0.01228381437717983, 0.014174452338888982, 0.0114947395605718, 0.010004552436593964, 0.012117713472775436, 0.008515199474349196, 0.01018498916485168, 0.010347885180637006, 0.013837673125663594, 0.01026144822816027, 0.009762399057301065], "moving_avg_accuracy_train": [0.043248056247692866, 0.08898538076204317, 0.13834074244705147, 0.18366501480164493, 0.2247431416149005, 0.26594883146203907, 0.3028409289244546, 0.33759701604539044, 0.3717529825552386, 0.4049502055829373, 0.43809127796715003, 0.4695590050984527, 0.4984821730582918, 0.5260540931993988, 0.5512709278751676, 0.5775064876462518, 0.60172313827716, 0.623766554279409, 0.642136567219639, 0.6560638801243823, 0.675625908588273, 0.6907703388857764, 0.7071267163401759, 0.7225589876336684, 0.7354461450026383, 0.7490229998810658, 0.7604915787919994, 0.7733586888795306, 0.7846322764619079, 0.7970336112407356, 0.8087016949821567, 0.8172430861959196, 0.8269205935716211, 0.8346028507288554, 0.8416795704893956, 0.8490809843453102, 0.8577393433501018, 0.8645854587913007, 0.8706169345991399, 0.876587202742908, 0.8816486938877295, 0.8881361624811456, 0.893456412079515, 0.898402746837095, 0.9029684885570399, 0.9073356394763913, 0.9103941445002361, 0.9140676660967242, 0.9165854658918599, 0.9201860490265111, 0.9235987069572394, 0.9268328234627429, 0.9299017105343811, 0.932524199970284, 0.9354378619280821, 0.9371208696686902, 0.9394539208697337, 0.936883380261627, 0.9392843221819389, 0.941401100229295, 0.9435712313873825, 0.9456497993189192, 0.94723916745135, 0.9491577718740812, 0.9504406647736517, 0.9520603341939886, 0.952555425065149, 0.9539355003777094, 0.9554055048399845, 0.9574540273822412, 0.9583326888166823, 0.9595257348517269, 0.9614900443261333, 0.9622649401161575, 0.9633577297712546, 0.9639134130798895, 0.9646878956171847, 0.9653872190007412, 0.9660979181566379, 0.9664494091886577, 0.9675817341543435, 0.9687216622639183, 0.9690968803352378, 0.9696321421505973, 0.9687283053868573, 0.9690864389089413, 0.970354950449018, 0.971496899225637, 0.9722267456864251, 0.9722978862940468, 0.9724944102742306, 0.9730502090147107, 0.973982941608533, 0.9743525766881743, 0.9753107493384322, 0.9758359581462833, 0.9755229620686429, 0.9757201020582533, 0.9762696600048828, 0.9760898608556311, 0.9769230971653245, 0.9775358981619242, 0.9779293810374539, 0.9781789199778207, 0.9782964039836378, 0.9789787764936351, 0.9796742559121471, 0.9798538309150169, 0.9801782088342664, 0.9798842475104098, 0.9799709600332707, 0.9804904551382861, 0.97960959466737, 0.9801769962482981, 0.9808945598663624, 0.9808406775750473, 0.980933981541426, 0.9812712160873112, 0.9816585046333881, 0.9817047228819817, 0.9819951823259725, 0.982356541175555, 0.9823725193485127, 0.982389369048259, 0.9824603373494594, 0.9826309493728652, 0.9830239544724927, 0.9831986586526428, 0.9831606159635966, 0.9834332971863122, 0.9836578199962893, 0.983415895249106, 0.9837303697122907, 0.9837183191232599, 0.9837770838621428, 0.9841230129747749, 0.9846784177035064, 0.9846622431189254, 0.9847569319380313, 0.9846469475216829, 0.9850964803731046, 0.9842713446096959, 0.9848257949701549, 0.9848086893564912, 0.9849026844446978, 0.9852336016026182, 0.9854965858614225, 0.9855472958384032, 0.9851885391689225, 0.9855373738306109, 0.9858699262166066, 0.9861623560640306, 0.9868045782314832, 0.9857341918715131, 0.9858310038582266, 0.9863016400045653, 0.9864834331088983, 0.9866098084730269, 0.986932773644781, 0.9870119258553306, 0.986906523632939, 0.986646467920854, 0.9869774649895106, 0.9872009215405779, 0.9873951290877475, 0.9875861558730481, 0.9875905971678953, 0.9869762128963807, 0.9861047937627503, 0.9853228416912926, 0.9855143392162847, 0.9858515562613414, 0.9854738911471581, 0.9858454880801074, 0.9861704805292488, 0.9862328921477894, 0.9864820860044852, 0.9869992931766925, 0.9873297768054519, 0.9873110999796963, 0.9877593205984211, 0.9881882597433593, 0.9881279124511939, 0.9881620276406445, 0.9879788536694926, 0.9881254949382945, 0.9885992329063975, 0.9886141178360419, 0.9879742195548833, 0.9881817781041937, 0.9884895245854872, 0.9886803577662426, 0.9888824066610838, 0.9892014704950216, 0.9892583300669665, 0.9889793325507644, 0.9890421659242871, 0.989189396764029, 0.9893730577936062, 0.9895058006368923, 0.9896857591137163, 0.9896198211107056, 0.9899743173472725, 0.9895913132149816, 0.989537144950654, 0.9894535883782538, 0.9895318472845221, 0.9892975776596599, 0.9893472237615879, 0.9894476006782955, 0.9890939806759883, 0.9891686367739025, 0.9892125757739301, 0.9896916460965924, 0.9897832655631607, 0.989691300873539, 0.9897805215671467, 0.9900283030033169, 0.9902512702470513, 0.9904752283033262, 0.9904395893265835, 0.9902471510772953, 0.9904505226136227, 0.9902429319963172, 0.990272339280028, 0.9905034910282433, 0.9904209200492654, 0.9904209388324525, 0.9903395755289877, 0.9905081640320597, 0.9905645265348153, 0.9903106943420665, 0.989963806974582, 0.9902025604652467, 0.99039880136755, 0.9907149631570131, 0.9909251400544439, 0.991290938474018, 0.9913062980111677, 0.9912154898981738, 0.9908408659441168, 0.9908641024509417, 0.9910197297427615, 0.9908203586280276, 0.9906270458295475, 0.990457642510897, 0.9903331534074632, 0.9900955191298397, 0.9900141457133027, 0.9900408910372289, 0.9901462699394584, 0.9903179129598167, 0.9904537544388443, 0.9904551761295114, 0.9905796885380165, 0.990726626937814, 0.9906519332535841, 0.9909706836401582, 0.9911249534082944, 0.9913800175912745, 0.9912213836512223, 0.9912088214385086, 0.9908605130649605, 0.9907817674120727], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 126713467, "moving_var_accuracy_train": [0.01683354932283246, 0.03397732007412812, 0.05250315361023718, 0.06574144522947376, 0.07435401322889953, 0.08219979178801681, 0.08622905430580241, 0.08847801920284493, 0.09012988771655728, 0.0910353994956585, 0.0918168356550733, 0.09154711274685709, 0.08992134827566896, 0.08777111047051028, 0.0847169981830448, 0.0824400397332595, 0.07947405126994876, 0.07589985584438773, 0.07134698663876693, 0.06595801837760992, 0.06280627315844756, 0.05858982976392626, 0.055138626538410715, 0.05176815886005323, 0.04808605239952125, 0.04493642605507807, 0.041626538169697073, 0.038953947050769225, 0.03620239633849009, 0.03396629464331013, 0.03179496278275045, 0.029272064779274326, 0.02718774564240771, 0.025000124753435844, 0.022950831941215225, 0.02114877709069243, 0.01970860400752589, 0.01815956727648128, 0.01667101885261811, 0.015324713882732738, 0.014022810727941416, 0.012999314893902313, 0.011954128906612628, 0.010978912063757769, 0.010068634834461208, 0.009233419415386614, 0.008394267550675908, 0.007676293643887091, 0.006965718121773847, 0.006385824099782276, 0.005852057797173518, 0.005360987603596695, 0.00490965145396324, 0.004480583366139722, 0.0041089298635646285, 0.0037235295127026895, 0.0034001647125926353, 0.0031196173524946982, 0.0028595363161876327, 0.0026139094282847893, 0.0023949037086460298, 0.0021942973395955435, 0.0019976024251794657, 0.001830971569039839, 0.0016626867398617702, 0.0015200280271561641, 0.0013702312591769047, 0.0012503496040742626, 0.0011447628617388176, 0.0010680545770201378, 0.0009681975325654904, 0.0008841880088845619, 0.0008304958133972942, 0.0007528504034261395, 0.0006883130661561102, 0.0006222608149959588, 0.000565433142301541, 0.0005132913068244877, 0.00046650801575376973, 0.0004209691276887063, 0.0003904116533710723, 0.00036306541288895586, 0.00032802596900946275, 0.00029780191900735474, 0.00027537401516601227, 0.00024899095022617323, 0.00023857394894932605, 0.00022645297713018924, 0.00020860176212409454, 0.00018778713478616028, 0.00016935601638062956, 0.00015520062490184143, 0.00014751047323586524, 0.00013398909674119187, 0.00012885304051639241, 0.00011845033509135282, 0.00010748700048378276, 9.708807801493688e-05, 9.00973956437754e-05, 8.137860568604245e-05, 7.94892898475621e-05, 7.492008641570824e-05, 6.8821536734153e-05, 6.249981020557238e-05, 5.6374051609620514e-05, 5.492733663025788e-05, 5.378782756139739e-05, 4.8699269440158935e-05, 4.477633180661286e-05, 4.107641796526376e-05, 3.7036447723325376e-05, 3.576167942820843e-05, 3.916874800839014e-05, 3.814937419390948e-05, 3.896851468824351e-05, 3.509779293127545e-05, 3.166636430942575e-05, 2.952327212892805e-05, 2.7920876677336723e-05, 2.5148014148130628e-05, 2.3392512930748754e-05, 2.2228483601218337e-05, 2.0007932959196074e-05, 1.8009694874710357e-05, 1.6254053885216646e-05, 1.4890624659470754e-05, 1.4791639268522491e-05, 1.3587169296727654e-05, 1.2241477582763621e-05, 1.1686525267482546e-05, 1.097156717053451e-05, 1.0401158703178733e-05, 1.0251090524818218e-05, 9.227288422600307e-06, 8.335639231164104e-06, 8.579077866745977e-06, 1.0497439794346604e-05, 9.450050369589246e-06, 8.585739084803517e-06, 7.836034322878531e-06, 8.871148951155721e-06, 1.4111675308544003e-05, 1.546724459760744e-05, 1.3923153556016002e-05, 1.2610353889877009e-05, 1.2334873989544376e-05, 1.1723833073999329e-05, 1.0574593282487825e-05, 1.0675491085311436e-05, 1.0703112567537706e-05, 1.062812111566717e-05, 1.033494594507956e-05, 1.3013495161879387e-05, 2.202368828218051e-05, 1.9905672500905346e-05, 1.990859069097929e-05, 1.821517021692884e-05, 1.6537389789163746e-05, 1.5822409329743087e-05, 1.429655404868275e-05, 1.2966885300180406e-05, 1.2278857530654506e-05, 1.2037003312722292e-05, 1.1282698453384518e-05, 1.049387775044476e-05, 9.772911069720647e-06, 8.795797488647867e-06, 1.1313430037542907e-05, 1.7016428791901574e-05, 2.0817827291225014e-05, 1.9066086280805773e-05, 1.818291567201584e-05, 1.7648302551053493e-05, 1.712623082114465e-05, 1.6364188567020188e-05, 1.4762826601477889e-05, 1.3845422145264326e-05, 1.4868409261583328e-05, 1.4364543195326318e-05, 1.2931228290176431e-05, 1.3446220968608475e-05, 1.3757497982290894e-05, 1.2414524345107052e-05, 1.1183546525957578e-05, 1.0367166206729922e-05, 9.523982541499545e-06, 1.059143324915086e-05, 9.534283974410468e-06, 1.2266083869037408e-05, 1.142720044466057e-05, 1.1136851470930979e-05, 1.0350922049733092e-05, 9.683243647918818e-06, 9.631134854270501e-06, 8.69711846713923e-06, 8.527963146847286e-06, 7.71069912761672e-06, 7.13472149639483e-06, 6.724831710823471e-06, 6.210934501734262e-06, 5.881306531987771e-06, 5.332306260958223e-06, 5.930083870523578e-06, 6.65730497163853e-06, 6.017982282217076e-06, 5.479019361116814e-06, 4.986237532698045e-06, 4.981554093626468e-06, 4.505581303193693e-06, 4.145702901543801e-06, 4.856556565675175e-06, 4.421062705709744e-06, 3.9963321566496145e-06, 5.662274307487074e-06, 5.17159401662645e-06, 4.730552152198773e-06, 4.329139926489473e-06, 4.448786694835427e-06, 4.45133755135846e-06, 4.4576186949566625e-06, 4.023288055430419e-06, 3.954251567988971e-06, 3.931066247283254e-06, 3.925804402094127e-06, 3.5410070569019675e-06, 3.667786527538953e-06, 3.3623695739093634e-06, 3.0261326196937005e-06, 2.7830992420808066e-06, 2.7605880681854886e-06, 2.5131198468188622e-06, 2.841684900817872e-06, 3.6404940222196956e-06, 3.7894736837390873e-06, 3.7571207409965464e-06, 4.281033160945856e-06, 4.250498798774203e-06, 5.029725272762772e-06, 4.528875983919574e-06, 4.1502034059971205e-06, 4.998271027976711e-06, 4.503303342423871e-06, 4.270951693814043e-06, 4.201596096944713e-06, 4.117765029756209e-06, 3.964265886109041e-06, 3.707317129361746e-06, 3.84481586553986e-06, 3.5199289752559244e-06, 3.1743738888976285e-06, 2.9568789173236992e-06, 2.9263429635309215e-06, 2.799784833997369e-06, 2.5198245414368077e-06, 2.407372146138838e-06, 2.360952971540316e-06, 2.1750699925608096e-06, 2.871979273774634e-06, 2.7989737986445457e-06, 3.1045960557337627e-06, 3.0206189925885797e-06, 2.7199773760240734e-06, 3.5398481461755234e-06, 3.2416712321963575e-06], "duration": 101501.372965, "accuracy_train": [0.43248056247692873, 0.500621301391196, 0.5825389976121262, 0.5915834659929863, 0.5944462829342008, 0.6368000400862864, 0.6348698060861941, 0.6504018001338132, 0.6791566811438723, 0.703725212832226, 0.7363609294250646, 0.7527685492801771, 0.7587906846968439, 0.7742013744693614, 0.7782224399570875, 0.8136265255860096, 0.8196729939553341, 0.8221572982996493, 0.8074666836817092, 0.7814096962670728, 0.851684164763289, 0.8270702115633074, 0.8543341134297711, 0.8614494292751015, 0.8514305613233666, 0.871214693786914, 0.8637087889904023, 0.8891626796673128, 0.8860945647033037, 0.9086456242501846, 0.9137144486549464, 0.894115607119786, 0.9140181599529347, 0.9037431651439645, 0.9053700483342562, 0.9156937090485419, 0.9356645743932264, 0.9262004977620893, 0.9249002168696937, 0.9303196160368217, 0.9272021141911223, 0.94652337982189, 0.9413386584648394, 0.9429197596553157, 0.9440601640365448, 0.9466399977505537, 0.9379206897148394, 0.9471293604651162, 0.9392456640480805, 0.9525912972383721, 0.9543126283337948, 0.9559398720122739, 0.9575216941791252, 0.956126604893411, 0.9616608195482651, 0.9522679393341639, 0.9604513816791252, 0.9137485147886674, 0.9608927994647471, 0.9604521026555003, 0.9631024118101699, 0.9643569107027501, 0.9615434806432264, 0.9664252116786637, 0.961986700869786, 0.966637358977021, 0.9570112429055924, 0.966356178190753, 0.9686355450004615, 0.9758907302625508, 0.9662406417266519, 0.9702631491671282, 0.9791688295957919, 0.969239002226375, 0.9731928366671282, 0.9689145628576044, 0.9716582384528424, 0.9716811294527501, 0.9724942105597084, 0.9696128284768365, 0.977772658845515, 0.9789810152500923, 0.9724738429771133, 0.9744494984888336, 0.9605937745131967, 0.9723096406076966, 0.9817715543097084, 0.9817744382152085, 0.978795363833518, 0.9729381517626431, 0.9742631260958842, 0.978052397679033, 0.9823775349529347, 0.9776792924049464, 0.983934303190753, 0.9805628374169435, 0.9727059973698781, 0.9774943619647471, 0.9812156815245479, 0.9744716685123662, 0.9844222239525655, 0.9830511071313216, 0.9814707269172205, 0.9804247704411223, 0.9793537600359912, 0.9851201290836102, 0.985933570678756, 0.9814700059408453, 0.9830976101075121, 0.9772385955956996, 0.980751372739018, 0.9851659110834257, 0.9716818504291252, 0.9852836104766519, 0.9873526324289406, 0.9803557369532114, 0.9817737172388336, 0.9843063270002769, 0.9851441015480805, 0.9821206871193245, 0.98460931732189, 0.9856087708217978, 0.9825163229051311, 0.9825410163459765, 0.9830990520602622, 0.984166457583518, 0.98656100036914, 0.9847709962739941, 0.9828182317621816, 0.985887428190753, 0.9856785252860835, 0.9812385725244556, 0.9865606398809523, 0.9836098638219823, 0.9843059665120893, 0.9872363749884644, 0.9896770602620893, 0.9845166718576966, 0.9856091313099853, 0.9836570877745479, 0.9891422760358989, 0.976845122739018, 0.9898158482142857, 0.984654738833518, 0.9857486402385567, 0.9882118560239018, 0.9878634441906607, 0.9860036856312293, 0.9819597291435955, 0.9886768857858066, 0.9888628976905685, 0.9887942246908453, 0.9925845777385567, 0.9761007146317828, 0.986702311738649, 0.9905373653216132, 0.9881195710478959, 0.9877471867501846, 0.9898394601905685, 0.9877242957502769, 0.9859579036314139, 0.9843059665120893, 0.9899564386074198, 0.9892120305001846, 0.9891429970122739, 0.989305396940753, 0.9876305688215209, 0.9814467544527501, 0.9782620215600776, 0.9782852730481728, 0.9872378169412146, 0.9888865096668512, 0.9820749051195091, 0.9891898604766519, 0.9890954125715209, 0.9867945967146549, 0.9887248307147471, 0.9916541577265596, 0.9903041294642857, 0.9871430085478959, 0.9917933061669435, 0.9920487120478036, 0.9875847868217055, 0.9884690643456996, 0.9863302879291252, 0.9894452663575121, 0.9928628746193245, 0.9887480822028424, 0.9822151350244556, 0.9900498050479882, 0.9912592429171282, 0.9903978563930418, 0.9907008467146549, 0.9920730450004615, 0.9897700662144703, 0.9864683549049464, 0.9896076662859912, 0.9905144743217055, 0.9910260070598007, 0.9907004862264673, 0.9913053854051311, 0.9890263790836102, 0.993164783476375, 0.9861442760243633, 0.9890496305717055, 0.9887015792266519, 0.9902361774409376, 0.9871891510358989, 0.9897940386789406, 0.9903509929286637, 0.9859114006552234, 0.9898405416551311, 0.9896080267741787, 0.9940032790005537, 0.9906078407622739, 0.9888636186669435, 0.9905835078096161, 0.9922583359288483, 0.9922579754406607, 0.9924908508098007, 0.9901188385358989, 0.9885152068337025, 0.9922808664405685, 0.9883746164405685, 0.9905370048334257, 0.9925838567621816, 0.9896777812384644, 0.990421107881137, 0.9896073057978036, 0.9920254605597084, 0.9910717890596161, 0.9880262046073275, 0.9868418206672205, 0.9923513418812293, 0.9921649694882798, 0.9935604192621816, 0.9928167321313216, 0.9945831242501846, 0.991444533845515, 0.9903982168812293, 0.9874692503576044, 0.9910732310123662, 0.99242037536914, 0.9890260185954227, 0.9888872306432264, 0.9889330126430418, 0.9892127514765596, 0.9879568106312293, 0.9892817849644703, 0.9902815989525655, 0.9910946800595238, 0.9918627001430418, 0.9916763277500923, 0.990467971345515, 0.9917003002145626, 0.9920490725359912, 0.989979690095515, 0.9938394371193245, 0.9925133813215209, 0.9936755952380952, 0.989793678190753, 0.9910957615240864, 0.9877257377030271, 0.9900730565360835], "end": "2016-01-25 13:42:06.384000", "learning_rate_per_epoch": [0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136, 0.007479142397642136], "accuracy_valid": [0.42309276167168675, 0.4937744140625, 0.5628073818712349, 0.5709757977221386, 0.5771925592996988, 0.6196127282567772, 0.6134283226656627, 0.6274164038968373, 0.6547204442771084, 0.6732457172439759, 0.7024219926581325, 0.709228515625, 0.7096344361822289, 0.7264507247740963, 0.7212825913027108, 0.7494205336972892, 0.7483924957643072, 0.7487895919615963, 0.7374061676393072, 0.7081901826054217, 0.7694106504141567, 0.7447612716490963, 0.7653014401355422, 0.7601421310240963, 0.7566844526543675, 0.765514695500753, 0.7595832548945783, 0.7774878812123494, 0.7677619658320783, 0.782360398625753, 0.7854018613516567, 0.7677413756588856, 0.7839884930346386, 0.7783114881400602, 0.7766025037650602, 0.7768775296498494, 0.8029608669051205, 0.7892375164721386, 0.7782806028802711, 0.7924525249435241, 0.7941820994917168, 0.8007636012801205, 0.7930922910391567, 0.7964190747364458, 0.7904582195971386, 0.7951483669051205, 0.7869578901543675, 0.7977015483810241, 0.7844767742846386, 0.8007121258471386, 0.7991869823042168, 0.7987795910203314, 0.8021460843373494, 0.8004473950489458, 0.8020548992846386, 0.7888095350150602, 0.8076304240399097, 0.7647513883659638, 0.7990340267319277, 0.8063494211219879, 0.8029505718185241, 0.8006106457078314, 0.7984442653426205, 0.8058905544051205, 0.7968161709337349, 0.8003870952560241, 0.7989928463855422, 0.8028387965926205, 0.8051787227033133, 0.8139780802899097, 0.8007533061935241, 0.8018313488328314, 0.8099291698042168, 0.8056052334337349, 0.8006106457078314, 0.7995634883283133, 0.801659273814006, 0.8043139354292168, 0.8049448771649097, 0.8038462443524097, 0.8054125682417168, 0.8093997082078314, 0.8126970773719879, 0.8029505718185241, 0.7990649119917168, 0.8047007365399097, 0.8135206842996988, 0.8179769860692772, 0.8101336008094879, 0.7958910838667168, 0.8083319606551205, 0.812767672251506, 0.812401461314006, 0.8085363916603916, 0.8105189311935241, 0.8137751200112951, 0.8063788356551205, 0.8042330454631024, 0.8087996517319277, 0.8012518825301205, 0.8112925334149097, 0.8169386530496988, 0.8090540874435241, 0.8142839914344879, 0.8027167262801205, 0.8131441782756024, 0.8154429240399097, 0.8058199595256024, 0.8103159709149097, 0.8061346950301205, 0.808617281626506, 0.8155149896460843, 0.8007944865399097, 0.8151987834149097, 0.8209772684487951, 0.8093908838478916, 0.8068671169051205, 0.8121882059487951, 0.8173460443335843, 0.8068980021649097, 0.804466891001506, 0.8036021037274097, 0.8169386530496988, 0.8083422557417168, 0.8090643825301205, 0.8104174510542168, 0.8145281320594879, 0.8086672863328314, 0.8137545298381024, 0.8175592996987951, 0.8112116434487951, 0.8095526637801205, 0.813622164439006, 0.8111189876694277, 0.8039477244917168, 0.818871187876506, 0.8222185617469879, 0.813255953501506, 0.8116381541792168, 0.8145884318524097, 0.8139177804969879, 0.8017710490399097, 0.8189329583960843, 0.8088408320783133, 0.8172342691076807, 0.822789203689759, 0.8113543039344879, 0.8131132930158133, 0.8066847467996988, 0.8205507577183735, 0.8237039956701807, 0.8196447900978916, 0.8207640130835843, 0.8006312358810241, 0.8087187617658133, 0.8176004800451807, 0.8147810970444277, 0.810814547251506, 0.8124323465737951, 0.8090746776167168, 0.8153826242469879, 0.8041712749435241, 0.8161047510353916, 0.8150973032756024, 0.8147619775978916, 0.8127059017319277, 0.8189329583960843, 0.8072230327560241, 0.8006724162274097, 0.8000311794051205, 0.8070097773908133, 0.8124323465737951, 0.8010180369917168, 0.8075083537274097, 0.8115366740399097, 0.809960055064006, 0.8123911662274097, 0.8139677852033133, 0.8135721597326807, 0.8035006235881024, 0.8139677852033133, 0.8162371164344879, 0.8097673898719879, 0.8151781932417168, 0.8040594997176205, 0.8176710749246988, 0.8243246423192772, 0.8114146037274097, 0.8034388530685241, 0.8168268778237951, 0.8178534450301205, 0.8195124246987951, 0.8174475244728916, 0.8202757318335843, 0.8155046945594879, 0.8073862834149097, 0.8136633447853916, 0.8185667474585843, 0.8200212961219879, 0.8182711314006024, 0.8160944559487951, 0.8084231457078314, 0.8192682840737951, 0.8073862834149097, 0.8090437923569277, 0.8126661921121988, 0.812157320689006, 0.8141619211219879, 0.818016695689006, 0.8153002635542168, 0.8038962490587349, 0.8146296121987951, 0.8177931452371988, 0.8245276025978916, 0.8133883189006024, 0.8159017907567772, 0.814598726939006, 0.8186991128576807, 0.8206316476844879, 0.8160326854292168, 0.8207434229103916, 0.8123308664344879, 0.8168268778237951, 0.8151281885353916, 0.8164194865399097, 0.8148737528237951, 0.8148325724774097, 0.8174784097326807, 0.8124941170933735, 0.8169489481362951, 0.8178137354103916, 0.8143251717808735, 0.810814547251506, 0.8191873941076807, 0.8158812005835843, 0.8201433664344879, 0.8151281885353916, 0.8222494470067772, 0.8176813700112951, 0.8172136789344879, 0.8088202419051205, 0.817040133189006, 0.8143648814006024, 0.8162474115210843, 0.8118014048381024, 0.8121367305158133, 0.8124411709337349, 0.8142016307417168, 0.8065509106739458, 0.8159312052899097, 0.815819430064006, 0.8195933146649097, 0.8188917780496988, 0.8185564523719879, 0.8212317041603916, 0.8230524637612951, 0.8156870646649097, 0.8185564523719879, 0.8199801157756024, 0.8207934276167168, 0.8173857539533133, 0.8143237010542168, 0.8143442912274097, 0.8160326854292168], "accuracy_test": 0.2973513233418367, "start": "2016-01-24 09:30:25.011000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0, 211.0, 212.0, 213.0, 214.0, 215.0, 216.0, 217.0, 218.0, 219.0, 220.0, 221.0, 222.0, 223.0, 224.0, 225.0, 226.0, 227.0, 228.0, 229.0, 230.0, 231.0, 232.0, 233.0, 234.0, 235.0, 236.0, 237.0, 238.0, 239.0, 240.0, 241.0, 242.0, 243.0, 244.0, 245.0, 246.0, 247.0, 248.0, 249.0, 250.0, 251.0, 252.0, 253.0, 254.0, 255.0, 256.0, 257.0, 258.0, 259.0, 260.0, 261.0, 262.0, 263.0], "accuracy_train_last": 0.9900730565360835, "batch_size_eval": 1024, "accuracy_train_std": [0.01838647797845414, 0.015285609548898392, 0.02053409741323431, 0.017442419904247605, 0.018028610863856275, 0.018191688272118797, 0.018682486953993202, 0.016807277568760586, 0.020779316694462428, 0.022431286533881368, 0.023813413506360525, 0.02747796674097241, 0.026005319682022743, 0.02789909076493781, 0.028664142425251626, 0.029449475706454075, 0.02879156582165828, 0.030980259015630517, 0.03013898545728091, 0.0284675012980325, 0.028939390441699568, 0.025990278026776253, 0.02877981908886167, 0.027157890112364535, 0.024000859367568055, 0.02636508837166672, 0.02593647414115188, 0.024940432653196343, 0.025833738370403262, 0.024489262459278058, 0.023804173099361007, 0.02483053844890292, 0.021175693135341265, 0.02111556229552295, 0.02306025498304862, 0.02099037487516374, 0.018544642309630495, 0.021084708610918903, 0.016940906260635234, 0.017438903026048346, 0.018154833551483163, 0.015540897359591374, 0.01506999569301649, 0.014318468213999925, 0.013431901147784807, 0.013980285955381951, 0.015668455346906897, 0.0145311276809483, 0.01429157678679143, 0.014478776899814508, 0.013327723389798065, 0.011818128299925774, 0.011163414385018408, 0.011317496897929904, 0.011348245475294156, 0.012470684833800925, 0.010899370408192567, 0.014014460389991407, 0.009154235337562414, 0.01005047789256503, 0.010771998632703577, 0.009123761353745478, 0.00866914609151019, 0.009540881466410332, 0.010282640745266063, 0.008371416855433612, 0.010004810873749352, 0.009785431002201916, 0.009155811228983905, 0.006821486541868701, 0.008742567141742708, 0.008605765710413845, 0.00804317622470052, 0.009451570247079278, 0.00729722273025652, 0.008777719062113373, 0.008459871810109838, 0.008508950387704675, 0.0075122094756889775, 0.007645234426488832, 0.00662539217628246, 0.006472666224427921, 0.007399076989140368, 0.00731471095830286, 0.009593267392923271, 0.007493084480058139, 0.005565720559411801, 0.005333853926145469, 0.006687629972643985, 0.006805130136990084, 0.007122782626518624, 0.006572463501081336, 0.006283910307052179, 0.007154553377776493, 0.005423427711212641, 0.006830811324791153, 0.006945832842003808, 0.006143024128455758, 0.005754654861062558, 0.007881443798166826, 0.0046946805020897, 0.005320776867934244, 0.0056588536430399305, 0.005257285102483611, 0.005341532912740056, 0.004606388080899266, 0.00486129289442955, 0.004496283285013476, 0.005483293182508828, 0.004922286378944757, 0.005387990661313695, 0.0047862604072823976, 0.006700350788276704, 0.004706589953836547, 0.003861004369615594, 0.005559130572106229, 0.005246002252635457, 0.004886821046150673, 0.004840510845185165, 0.0049523585545121875, 0.0055006242380560705, 0.0048477126985017535, 0.0060553836912323735, 0.004700093073817055, 0.005108520425245267, 0.004702371511869299, 0.004413823786389862, 0.005111904090505027, 0.005838410742524312, 0.0037797875328329884, 0.004551052578473466, 0.0053493088145949805, 0.0054946370743680275, 0.0045754503802058685, 0.005335113669455853, 0.00370768846596855, 0.0034582362530511204, 0.004459590217948391, 0.0044671972010324815, 0.00463056877235316, 0.004011914876766829, 0.005024331720779274, 0.003679616863318145, 0.004130301506346707, 0.004814527533456554, 0.004243643490441953, 0.004875388131109353, 0.005101391687176531, 0.004922424554891807, 0.0040613305753530995, 0.004591404062309953, 0.003638116156436264, 0.0029748811534452166, 0.005025587698189333, 0.0040495659682543116, 0.0031222741247873506, 0.004294996208718719, 0.004512903779579206, 0.004284332319480407, 0.004264462570069056, 0.003777974990501269, 0.004554472220729057, 0.0032978474089409597, 0.003704393050209073, 0.004056795397467982, 0.003104177074452549, 0.004697140223203289, 0.0051219529353344504, 0.00542480392674559, 0.005457137371993613, 0.0036541930321026153, 0.0038839913924567956, 0.004522977156301226, 0.003270140001109279, 0.004099452177156294, 0.004843510608039804, 0.004106460571140853, 0.002939268644531763, 0.0042105742237371685, 0.004177321784264372, 0.00293102360647953, 0.0033606911451312138, 0.0041245230887936865, 0.0044363467190370945, 0.003747169214304468, 0.003989158097086894, 0.002290738327127207, 0.003317570106579037, 0.005123348220138091, 0.003445268685029417, 0.003348329303844643, 0.0036045231400918796, 0.002813727477158715, 0.0026364561137615975, 0.003983605441847172, 0.004574916488169649, 0.003740182962310588, 0.00373058517406078, 0.0040846289462956814, 0.003644024973289416, 0.0028260718793909475, 0.003935519827356769, 0.0027464184293202265, 0.0045727745277156055, 0.003916579158279662, 0.0036181645733396736, 0.003342416284858247, 0.0045029189488043336, 0.0029091256971014445, 0.0037358473095893113, 0.004291212122255721, 0.0032890530385866474, 0.003006096865181292, 0.0026189255938352876, 0.0031468285834332726, 0.003112418608192969, 0.003252463778385275, 0.0024712977957250604, 0.0027333379746994108, 0.0025355524791135222, 0.004215962065866217, 0.0037402210032786154, 0.0031384944224198473, 0.0033353454556812926, 0.0037009347190984387, 0.002755013992294898, 0.002970513245215862, 0.0033111271154855175, 0.003830102514275044, 0.0035019467637554757, 0.0030263856960943895, 0.003574273233012014, 0.003517043306117542, 0.0029528704674799565, 0.003353338612584602, 0.0027131023845105525, 0.0024740510802972707, 0.002267833200446787, 0.0032244104877203993, 0.0031326637184873702, 0.0037830461231688306, 0.0032703418436038304, 0.0034814795405429966, 0.0037279968832035298, 0.0035349733780611885, 0.0038411528844885297, 0.0032816766492467235, 0.004320075985432705, 0.004190638731728089, 0.0034370971720993656, 0.003976548321548359, 0.0026938960016814036, 0.003177595183823033, 0.003280585272521226, 0.0029402659262195634, 0.0033128647119865457, 0.003254688443299528, 0.002863078574277444, 0.003166250822689954, 0.0030825545794599236, 0.0030838244550287506, 0.003105381803180791, 0.003396266448727259, 0.003604238999877347], "accuracy_test_std": 0.006940363957461254, "error_valid": [0.5769072383283133, 0.5062255859375, 0.4371926181287651, 0.4290242022778614, 0.4228074407003012, 0.38038727174322284, 0.3865716773343373, 0.3725835961031627, 0.3452795557228916, 0.32675428275602414, 0.29757800734186746, 0.290771484375, 0.2903655638177711, 0.27354927522590367, 0.2787174086972892, 0.2505794663027108, 0.2516075042356928, 0.25121040803840367, 0.2625938323606928, 0.29180981739457834, 0.23058934958584332, 0.25523872835090367, 0.23469855986445776, 0.23985786897590367, 0.24331554734563254, 0.23448530449924698, 0.24041674510542166, 0.22251211878765065, 0.23223803416792166, 0.21763960137424698, 0.21459813864834332, 0.23225862434111444, 0.21601150696536142, 0.22168851185993976, 0.22339749623493976, 0.22312247035015065, 0.19703913309487953, 0.21076248352786142, 0.22171939711972888, 0.20754747505647586, 0.2058179005082832, 0.19923639871987953, 0.20690770896084332, 0.2035809252635542, 0.20954178040286142, 0.20485163309487953, 0.21304210984563254, 0.20229845161897586, 0.21552322571536142, 0.19928787415286142, 0.2008130176957832, 0.20122040897966864, 0.19785391566265065, 0.1995526049510542, 0.19794510071536142, 0.21119046498493976, 0.1923695759600903, 0.2352486116340362, 0.2009659732680723, 0.19365057887801207, 0.19704942818147586, 0.19938935429216864, 0.20155573465737953, 0.19410944559487953, 0.2031838290662651, 0.19961290474397586, 0.20100715361445776, 0.19716120340737953, 0.19482127729668675, 0.1860219197100903, 0.19924669380647586, 0.19816865116716864, 0.1900708301957832, 0.1943947665662651, 0.19938935429216864, 0.20043651167168675, 0.19834072618599397, 0.1956860645707832, 0.1950551228350903, 0.1961537556475903, 0.1945874317582832, 0.19060029179216864, 0.18730292262801207, 0.19704942818147586, 0.2009350880082832, 0.1952992634600903, 0.18647931570030118, 0.18202301393072284, 0.18986639919051207, 0.2041089161332832, 0.19166803934487953, 0.18723232774849397, 0.18759853868599397, 0.1914636083396084, 0.18948106880647586, 0.18622487998870485, 0.19362116434487953, 0.19576695453689763, 0.1912003482680723, 0.19874811746987953, 0.1887074665850903, 0.18306134695030118, 0.19094591255647586, 0.18571600856551207, 0.19728327371987953, 0.18685582172439763, 0.1845570759600903, 0.19418004047439763, 0.1896840290850903, 0.19386530496987953, 0.19138271837349397, 0.18448501035391573, 0.1992055134600903, 0.1848012165850903, 0.17902273155120485, 0.1906091161521084, 0.19313288309487953, 0.18781179405120485, 0.18265395566641573, 0.1931019978350903, 0.19553310899849397, 0.1963978962725903, 0.18306134695030118, 0.1916577442582832, 0.19093561746987953, 0.1895825489457832, 0.18547186794051207, 0.19133271366716864, 0.18624547016189763, 0.18244070030120485, 0.18878835655120485, 0.19044733621987953, 0.18637783556099397, 0.1888810123305723, 0.1960522755082832, 0.18112881212349397, 0.17778143825301207, 0.18674404649849397, 0.1883618458207832, 0.1854115681475903, 0.18608221950301207, 0.1982289509600903, 0.18106704160391573, 0.19115916792168675, 0.1827657308923193, 0.17721079631024095, 0.18864569606551207, 0.18688670698418675, 0.19331525320030118, 0.1794492422816265, 0.1762960043298193, 0.1803552099021084, 0.17923598691641573, 0.19936876411897586, 0.19128123823418675, 0.1823995199548193, 0.1852189029555723, 0.18918545274849397, 0.18756765342620485, 0.1909253223832832, 0.18461737575301207, 0.19582872505647586, 0.1838952489646084, 0.18490269672439763, 0.1852380224021084, 0.1872940982680723, 0.18106704160391573, 0.19277696724397586, 0.1993275837725903, 0.19996882059487953, 0.19299022260918675, 0.18756765342620485, 0.1989819630082832, 0.1924916462725903, 0.1884633259600903, 0.19003994493599397, 0.1876088337725903, 0.18603221479668675, 0.1864278402673193, 0.19649937641189763, 0.18603221479668675, 0.18376288356551207, 0.19023261012801207, 0.1848218067582832, 0.19594050028237953, 0.18232892507530118, 0.17567535768072284, 0.1885853962725903, 0.19656114693147586, 0.18317312217620485, 0.18214655496987953, 0.18048757530120485, 0.1825524755271084, 0.17972426816641573, 0.18449530544051207, 0.1926137165850903, 0.1863366552146084, 0.18143325254141573, 0.17997870387801207, 0.18172886859939763, 0.18390554405120485, 0.19157685429216864, 0.18073171592620485, 0.1926137165850903, 0.1909562076430723, 0.18733380788780118, 0.18784267931099397, 0.18583807887801207, 0.18198330431099397, 0.1846997364457832, 0.1961037509412651, 0.18537038780120485, 0.18220685476280118, 0.1754723974021084, 0.18661168109939763, 0.18409820924322284, 0.18540127306099397, 0.1813008871423193, 0.17936835231551207, 0.1839673145707832, 0.1792565770896084, 0.18766913356551207, 0.18317312217620485, 0.1848718114646084, 0.1835805134600903, 0.18512624717620485, 0.1851674275225903, 0.1825215902673193, 0.1875058829066265, 0.18305105186370485, 0.1821862645896084, 0.1856748282191265, 0.18918545274849397, 0.1808126058923193, 0.18411879941641573, 0.17985663356551207, 0.1848718114646084, 0.17775055299322284, 0.18231862998870485, 0.18278632106551207, 0.19117975809487953, 0.18295986681099397, 0.18563511859939763, 0.18375258847891573, 0.18819859516189763, 0.18786326948418675, 0.1875588290662651, 0.1857983692582832, 0.1934490893260542, 0.1840687947100903, 0.18418056993599397, 0.1804066853350903, 0.18110822195030118, 0.18144354762801207, 0.1787682958396084, 0.17694753623870485, 0.1843129353350903, 0.18144354762801207, 0.18001988422439763, 0.1792065723832832, 0.18261424604668675, 0.1856762989457832, 0.1856557087725903, 0.1839673145707832], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-3, 3], "momentum": 0.7549242354507695, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.007479142404645774, "patience_threshold": 1, "do_flip": true, "batch_size": 32, "optimization": "rmsprop", "nb_data_augmentation": 0, "learning_rate_decay_method": "none", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 5.629696582420949e-09, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.06948846929381536}, "accuracy_valid_max": 0.8245276025978916, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-3, 3)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8160326854292168, "loss_train": [1.6371585130691528, 1.2860509157180786, 1.105702519416809, 0.9839909672737122, 0.8870638608932495, 0.8068066239356995, 0.7408793568611145, 0.6865264773368835, 0.6399397253990173, 0.5997233390808105, 0.5613822340965271, 0.5262463092803955, 0.49258697032928467, 0.463779091835022, 0.4346030056476593, 0.4072265625, 0.3813100755214691, 0.35871610045433044, 0.33686619997024536, 0.31596624851226807, 0.29991862177848816, 0.28112950921058655, 0.2598939538002014, 0.24730053544044495, 0.23198498785495758, 0.2156994789838791, 0.2026413381099701, 0.18754534423351288, 0.1818035989999771, 0.1740214228630066, 0.15789879858493805, 0.15084590017795563, 0.14669737219810486, 0.1363404542207718, 0.1319275200366974, 0.1275136023759842, 0.11641723662614822, 0.11723626405000687, 0.10880375653505325, 0.10663971304893494, 0.10040280967950821, 0.10129127651453018, 0.09015187621116638, 0.09020143002271652, 0.08339398354291916, 0.08144264668226242, 0.07952068001031876, 0.08106450736522675, 0.07609429210424423, 0.07034511864185333, 0.07251887023448944, 0.06541696935892105, 0.0711737722158432, 0.07097046822309494, 0.060485415160655975, 0.06411930173635483, 0.06068304553627968, 0.059697654098272324, 0.054110340774059296, 0.05574711784720421, 0.05571579933166504, 0.05831767991185188, 0.05226029455661774, 0.05201530456542969, 0.05400340259075165, 0.05027267709374428, 0.04551144689321518, 0.04966266080737114, 0.04831570014357567, 0.050610367208719254, 0.044440656900405884, 0.04102650284767151, 0.03976482152938843, 0.03988613560795784, 0.04094089940190315, 0.03609173744916916, 0.038837023079395294, 0.04421485587954521, 0.040970832109451294, 0.038083869963884354, 0.04128763824701309, 0.038516782224178314, 0.0369582436978817, 0.03603566437959671, 0.03845487907528877, 0.04060574620962143, 0.04129350930452347, 0.033271800726652145, 0.030666010454297066, 0.03140071779489517, 0.035761646926403046, 0.033838097006082535, 0.03476369380950928, 0.0345609225332737, 0.03148263320326805, 0.03402053937315941, 0.031278908252716064, 0.026747509837150574, 0.02664770744740963, 0.028872134163975716, 0.03089209645986557, 0.034082893282175064, 0.02855111099779606, 0.029767239466309547, 0.029588643461465836, 0.026892123743891716, 0.025333097204566002, 0.02266690693795681, 0.02836878038942814, 0.02991604059934616, 0.02794557623565197, 0.02750524692237377, 0.02945913001894951, 0.026040226221084595, 0.022271333262324333, 0.027025144547224045, 0.028959356248378754, 0.022421587258577347, 0.022314980626106262, 0.029422713443636894, 0.02687675505876541, 0.023420628160238266, 0.02416081726551056, 0.030097495764493942, 0.030412500724196434, 0.02385888434946537, 0.02208535559475422, 0.023568177595734596, 0.022780105471611023, 0.022757915779948235, 0.02323113940656185, 0.024506542831659317, 0.028223145753145218, 0.023003125563263893, 0.023401832208037376, 0.02501673251390457, 0.01936967670917511, 0.022879168391227722, 0.021782975643873215, 0.02287162095308304, 0.022641796618700027, 0.021620934829115868, 0.022648699581623077, 0.022978462278842926, 0.023269182071089745, 0.019544949755072594, 0.02040426805615425, 0.019918201491236687, 0.019629614427685738, 0.02115515060722828, 0.017483776435256004, 0.013218333944678307, 0.01628466136753559, 0.017352363094687462, 0.016086997464299202, 0.020479846745729446, 0.01844818890094757, 0.02128392457962036, 0.022285345941781998, 0.02445417083799839, 0.021901967003941536, 0.02255265787243843, 0.02200603298842907, 0.019412297755479813, 0.019645648077130318, 0.017557615414261818, 0.016601134091615677, 0.013988474383950233, 0.018197745084762573, 0.015746258199214935, 0.020369967445731163, 0.019141241908073425, 0.01933407410979271, 0.015491914935410023, 0.01896723359823227, 0.01959719881415367, 0.021327393129467964, 0.013788108713924885, 0.014456718228757381, 0.01528466772288084, 0.019021177664399147, 0.01620657928287983, 0.017488930374383926, 0.020827151834964752, 0.02046462893486023, 0.01731656864285469, 0.01606050878763199, 0.012936113402247429, 0.016616474837064743, 0.015472185797989368, 0.015831610187888145, 0.015131307765841484, 0.015586458146572113, 0.012584954500198364, 0.016864102333784103, 0.018242886289954185, 0.018483199179172516, 0.012939734384417534, 0.017674822360277176, 0.019915778189897537, 0.018586089834570885, 0.013374092057347298, 0.013449662365019321, 0.018483968451619148, 0.013933377340435982, 0.01651240698993206, 0.01574127934873104, 0.018505167216062546, 0.015607054345309734, 0.016371188685297966, 0.016084279865026474, 0.017612991854548454, 0.015691516920924187, 0.015267935581505299, 0.013762744143605232, 0.019028039649128914, 0.015867654234170914, 0.01627826876938343, 0.01211237721145153, 0.014371617697179317, 0.011981136165559292, 0.012395933270454407, 0.010596035979688168, 0.01477808877825737, 0.012478829361498356, 0.01330935675650835, 0.012139939703047276, 0.014967787079513073, 0.015160543844103813, 0.018757406622171402, 0.014449715614318848, 0.019863642752170563, 0.01824740320444107, 0.011026926338672638, 0.014180898666381836, 0.013679777272045612, 0.011405323632061481, 0.011611628346145153, 0.01295750867575407, 0.016529226675629616, 0.012427466921508312, 0.015738029032945633, 0.014874616637825966, 0.016567733138799667, 0.015089977532625198, 0.019386932253837585, 0.01490336935967207, 0.018171053379774094, 0.01204750407487154, 0.01673416793346405, 0.015178688801825047, 0.016936169937253, 0.012498334981501102, 0.012620258145034313, 0.013426394201815128, 0.010179370641708374, 0.008644027635455132, 0.012709935195744038, 0.014579741284251213, 0.012161769904196262, 0.016849303618073463, 0.012944639660418034, 0.01421721838414669, 0.01547686941921711], "accuracy_train_first": 0.43248056247692873, "model": "residualv3", "loss_std": [0.27723169326782227, 0.21492910385131836, 0.21276333928108215, 0.20782066881656647, 0.20212778449058533, 0.19669541716575623, 0.1905207633972168, 0.18682174384593964, 0.18360640108585358, 0.17970967292785645, 0.17667900025844574, 0.1746530681848526, 0.16956166923046112, 0.16697557270526886, 0.16167116165161133, 0.15843932330608368, 0.15459488332271576, 0.15075814723968506, 0.14743131399154663, 0.14070652425289154, 0.14113277196884155, 0.1358199119567871, 0.12623852491378784, 0.12875014543533325, 0.12501981854438782, 0.12016942352056503, 0.1144343763589859, 0.11153919994831085, 0.1086215078830719, 0.11010309308767319, 0.09989118576049805, 0.10142859071493149, 0.10049228370189667, 0.09336081147193909, 0.09749463200569153, 0.09575463086366653, 0.08984699100255966, 0.09004240483045578, 0.09085783362388611, 0.08718830347061157, 0.08571849763393402, 0.08950743079185486, 0.0809769332408905, 0.08087316900491714, 0.07613913714885712, 0.07911064475774765, 0.07651494443416595, 0.08152372390031815, 0.08159170299768448, 0.07527071982622147, 0.07435012608766556, 0.07092063128948212, 0.08002714812755585, 0.07939035445451736, 0.0685470849275589, 0.07658043503761292, 0.07049188017845154, 0.07148034125566483, 0.06454818695783615, 0.06792624294757843, 0.06599890440702438, 0.07139603793621063, 0.06888741254806519, 0.06509780883789062, 0.06879758834838867, 0.06485315412282944, 0.06235918030142784, 0.06420622766017914, 0.06718816608190536, 0.06710094213485718, 0.060478825122117996, 0.058235686272382736, 0.058048397302627563, 0.059017930179834366, 0.058254487812519073, 0.05565876513719559, 0.05882000923156738, 0.06503767520189285, 0.0618208609521389, 0.06248274818062782, 0.06250397115945816, 0.06044838950037956, 0.05559954419732094, 0.05863046273589134, 0.059576984494924545, 0.062193259596824646, 0.06461264193058014, 0.05711923912167549, 0.05120289325714111, 0.05295398831367493, 0.05954108387231827, 0.05564320832490921, 0.062436629086732864, 0.06001344323158264, 0.058032091706991196, 0.0607769601047039, 0.05273633450269699, 0.046686798334121704, 0.05114125460386276, 0.052613213658332825, 0.05854244902729988, 0.06059877201914787, 0.05540226399898529, 0.05833087116479874, 0.05936400964856148, 0.048934392631053925, 0.04988107830286026, 0.048376720398664474, 0.05446601286530495, 0.05601806193590164, 0.05210604518651962, 0.05545990169048309, 0.05905984342098236, 0.053248222917318344, 0.045690640807151794, 0.050801221281290054, 0.05552766099572182, 0.04859147220849991, 0.04391045123338699, 0.057482168078422546, 0.05218129605054855, 0.04700620099902153, 0.04754171147942543, 0.06025093048810959, 0.05918540805578232, 0.0489322803914547, 0.04817237704992294, 0.049409713596105576, 0.04694670811295509, 0.04954208433628082, 0.05096380040049553, 0.054571326822042465, 0.05880806967616081, 0.048029519617557526, 0.05118744820356369, 0.05059846490621567, 0.04493418708443642, 0.05022954195737839, 0.049516864120960236, 0.05254938825964928, 0.04908454045653343, 0.052468426525592804, 0.0507361926138401, 0.05444789677858353, 0.05377745255827904, 0.047905780375003815, 0.04603610932826996, 0.04361198469996452, 0.045498356223106384, 0.04885953664779663, 0.03901258483529091, 0.030895007774233818, 0.039488475769758224, 0.041043806821107864, 0.038783784955739975, 0.05129731446504593, 0.046063993126153946, 0.04645414650440216, 0.050836142152547836, 0.05658793821930885, 0.0512072928249836, 0.050242044031620026, 0.05029168352484703, 0.04740443453192711, 0.04992292448878288, 0.04381764680147171, 0.03825881704688072, 0.033175453543663025, 0.04341627657413483, 0.045293498784303665, 0.05495445802807808, 0.04496563971042633, 0.046310532838106155, 0.04101867601275444, 0.04772414267063141, 0.047464024275541306, 0.05491022393107414, 0.038342032581567764, 0.03850901499390602, 0.03947976604104042, 0.04921858757734299, 0.045086201280355453, 0.04495905712246895, 0.05398852378129959, 0.0510704480111599, 0.046588268131017685, 0.0415300652384758, 0.035620249807834625, 0.04178920015692711, 0.0402032807469368, 0.041877631098032, 0.04083390161395073, 0.04169211909174919, 0.03316935896873474, 0.047025416046381, 0.05022389069199562, 0.050045277923345566, 0.034520793706178665, 0.045627694576978683, 0.048986222594976425, 0.05322885885834694, 0.03635859116911888, 0.03828955814242363, 0.050388846546411514, 0.039335813373327255, 0.04286462441086769, 0.041658613830804825, 0.04953736811876297, 0.04406733438372612, 0.04770992323756218, 0.04172584041953087, 0.048130400478839874, 0.04341806471347809, 0.04107549786567688, 0.04159712418913841, 0.05128565803170204, 0.04541442170739174, 0.046226050704717636, 0.03770817071199417, 0.0436696782708168, 0.034651514142751694, 0.035022296011447906, 0.03105943277478218, 0.04704751819372177, 0.03839123249053955, 0.03860555961728096, 0.03348895534873009, 0.042798299342393875, 0.04472571983933449, 0.05168501287698746, 0.04202170670032501, 0.05388098210096359, 0.05480125918984413, 0.035164330154657364, 0.0430295504629612, 0.04452097415924072, 0.03269145265221596, 0.032919418066740036, 0.03618309646844864, 0.048364296555519104, 0.039026856422424316, 0.04655349999666214, 0.040617432445287704, 0.046921662986278534, 0.04294084385037422, 0.05409383773803711, 0.03830808401107788, 0.05380566418170929, 0.035005584359169006, 0.049120835959911346, 0.04565634950995445, 0.05076408013701439, 0.03788933902978897, 0.039198700338602066, 0.038123439997434616, 0.03483038395643234, 0.02630726620554924, 0.036380790174007416, 0.03930523991584778, 0.038918931037187576, 0.0455256849527359, 0.04172798618674278, 0.0410463847219944, 0.048586539924144745]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:13 2016", "state": "available"}], "summary": "d4e223472e5967bb2eca1d337c7ce225"}