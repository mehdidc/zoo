{"content": {"hp_model": {"f0": 64, "f1": 32, "f2": 64, "f3": 16, "nonlin": "rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.667934536933899, 1.339105486869812, 1.155495524406433, 1.02447509765625, 0.9051343202590942, 0.8093467354774475, 0.7421353459358215, 0.6903634071350098, 0.6476138234138489, 0.611771821975708, 0.5815987586975098, 0.554064154624939, 0.5308461785316467, 0.5077106952667236, 0.4886091351509094, 0.46839016675949097, 0.45220518112182617, 0.43650102615356445, 0.421536386013031, 0.40655019879341125, 0.39388227462768555, 0.3822135031223297, 0.36970552802085876, 0.35850632190704346, 0.3490339517593384, 0.3388082981109619, 0.3282451927661896, 0.3207959234714508, 0.3119204640388489, 0.30247706174850464, 0.2967202067375183, 0.2901802062988281, 0.2811894416809082, 0.27428412437438965, 0.26832112669944763, 0.26362109184265137, 0.25387072563171387, 0.252310574054718, 0.24728234112262726, 0.24072633683681488, 0.23826512694358826, 0.23144756257534027, 0.2273215651512146, 0.2226608842611313, 0.21959082782268524, 0.21698427200317383, 0.21135321259498596, 0.20862309634685516, 0.20530304312705994, 0.20090775191783905, 0.19859328866004944, 0.19686731696128845, 0.19292052090168, 0.191964790225029, 0.18769824504852295, 0.1867603361606598, 0.18344826996326447, 0.18169942498207092, 0.18045136332511902, 0.1778169870376587, 0.17526251077651978, 0.17511184513568878, 0.17197655141353607, 0.1695602685213089, 0.1691121757030487, 0.16874922811985016, 0.16655860841274261, 0.16627608239650726, 0.1624593436717987, 0.16089196503162384, 0.1628829687833786, 0.15788429975509644, 0.16000774502754211, 0.15749739110469818, 0.15769758820533752, 0.15518423914909363, 0.15261727571487427, 0.15514513850212097, 0.15119579434394836, 0.15144604444503784, 0.15036866068840027, 0.14952141046524048, 0.14862629771232605, 0.15025320649147034, 0.1472226083278656, 0.1462704837322235, 0.1462797075510025, 0.1468093991279602, 0.14405213296413422, 0.14270269870758057, 0.1429862529039383, 0.14376141130924225, 0.1418100744485855, 0.1442745476961136, 0.13817991316318512, 0.13997693359851837, 0.14146371185779572, 0.137862429022789, 0.13938994705677032, 0.1383906453847885, 0.1386231780052185, 0.13794855773448944, 0.13914453983306885, 0.13673682510852814, 0.13602712750434875, 0.13776469230651855, 0.13637933135032654, 0.13675522804260254, 0.13446718454360962, 0.13585300743579865, 0.13671231269836426, 0.13554880023002625, 0.13396266102790833, 0.13585905730724335, 0.1318066120147705, 0.135142520070076, 0.13227400183677673, 0.132502943277359, 0.13371482491493225, 0.1312214583158493, 0.13461241126060486, 0.13057515025138855, 0.1311103254556656, 0.133096382021904, 0.12965962290763855, 0.1307918131351471, 0.132320836186409, 0.13117530941963196, 0.1322070211172104, 0.13000495731830597, 0.13158956170082092, 0.13168443739414215, 0.1291165053844452, 0.1294373720884323, 0.13089708983898163, 0.12935411930084229, 0.13055652379989624, 0.12792718410491943, 0.1297069936990738, 0.12820619344711304, 0.13046936690807343, 0.13057149946689606, 0.12626604735851288, 0.12905384600162506, 0.12876784801483154, 0.12728692591190338, 0.12883393466472626, 0.1241956576704979, 0.12995709478855133, 0.12770795822143555, 0.12536579370498657, 0.12827281653881073, 0.1286378800868988, 0.12651427090168, 0.1270739585161209, 0.12733985483646393, 0.12477917224168777, 0.12916770577430725, 0.1248730719089508, 0.12901374697685242, 0.1239188015460968, 0.12537401914596558, 0.12958723306655884, 0.12531286478042603, 0.12700986862182617, 0.1249045878648758, 0.12484081834554672, 0.12397994101047516, 0.12631936371326447, 0.12448444962501526, 0.1262209266424179, 0.12597475945949554, 0.12475992739200592, 0.12383072823286057, 0.12468305975198746, 0.12357307225465775, 0.1251627504825592, 0.12486632913351059, 0.12518909573554993, 0.12574738264083862, 0.12537656724452972, 0.12388231605291367, 0.1212908923625946, 0.126473069190979, 0.12240013480186462, 0.12375236302614212, 0.12571954727172852, 0.12299199402332306, 0.12457741796970367, 0.12558045983314514, 0.12278405576944351, 0.12274394929409027, 0.12227210402488708, 0.12108492106199265, 0.12389886379241943, 0.12243474274873734, 0.12563663721084595, 0.1218230277299881, 0.12426316738128662, 0.12327144294977188, 0.12231443077325821, 0.12218669801950455, 0.12399031221866608, 0.1210491880774498, 0.12256957590579987, 0.12151709198951721], "moving_avg_accuracy_train": [0.04795743788067552, 0.1031290122335271, 0.15773742004371996, 0.21263839657017114, 0.26651512928187865, 0.3178868685627624, 0.36599932503082167, 0.41066979615915333, 0.4523916144449084, 0.49073863274847956, 0.5254856450561805, 0.5585966802068009, 0.5881547963661687, 0.615728832867887, 0.6411941101396532, 0.6642242505341873, 0.6860045972023449, 0.7051027124002075, 0.7231812236329627, 0.7397774766734131, 0.7547859677300013, 0.7693724066309122, 0.7824327723262559, 0.7945638476568455, 0.8061025220400628, 0.816603658523072, 0.8259057997363333, 0.8345825376152286, 0.8426565965728826, 0.8502973462513732, 0.8579088400834618, 0.8645174051049699, 0.8705603726278801, 0.8759433479735269, 0.8814900765298103, 0.885800827580456, 0.8908313440402952, 0.894740499514911, 0.8989794995241988, 0.9024968723384825, 0.905302362147593, 0.9080341691222024, 0.9109880520957794, 0.9136138144445717, 0.9160350932299041, 0.9190116620319136, 0.9198213706037777, 0.9212870362957993, 0.9236944492567509, 0.9262958516513508, 0.9280000230326811, 0.9290663863163454, 0.9310329387549859, 0.9322588232259713, 0.9333573968546016, 0.9345297998763212, 0.9349131387851914, 0.9357928559317277, 0.9376983476433722, 0.9381647573707755, 0.9392053048087626, 0.9406508969457804, 0.942014780984591, 0.9429655118135497, 0.943635265801307, 0.9446889790640611, 0.944316816720824, 0.9458348339690074, 0.9468732395590483, 0.9482496549615322, 0.9491488489511211, 0.9490817947822917, 0.9500070567838521, 0.9503888218626652, 0.9513904635955108, 0.9519408076360151, 0.9533730440962508, 0.9544690335104536, 0.9546232370023393, 0.9546876874807692, 0.9554384430613191, 0.9554514196242809, 0.9563304150857178, 0.956984183025974, 0.9578540253246132, 0.9570837560862642, 0.9566274987062275, 0.9575004213046894, 0.9578838008992574, 0.9576732040665299, 0.9584137264408847, 0.9593986698670713, 0.9602131114351815, 0.960181026741691, 0.9612705470949305, 0.961114117644989, 0.9616988136174318, 0.9624808063616779, 0.9628963174279372, 0.9637213202077994, 0.9640686195096939, 0.9646625318873513, 0.9644948580867667, 0.9649276000662499, 0.9653053339572808, 0.9657987542806372, 0.9663940393419146, 0.9671180969018168, 0.9669627778735491, 0.9673138855374677, 0.9677204550921096, 0.9678723458543549, 0.9675533544225278, 0.9682033692017589, 0.9686325614840101, 0.9692000879987228, 0.9688459786024588, 0.9694061142981838, 0.9694314360136681, 0.9694191680813673, 0.9697849812935333, 0.9692377041809297, 0.969607604743826, 0.9699916324754234, 0.9705024150969749, 0.9704738021575524, 0.9709968216799386, 0.9712164592274763, 0.9713746415393171, 0.9710844197949461, 0.9716975122262118, 0.9719819033012558, 0.971828665127138, 0.9718370909501569, 0.9722843075599676, 0.9726286737885593, 0.9733430350918739, 0.974246340882705, 0.9740294193671089, 0.9739946252709295, 0.9741492864403112, 0.9745186351760788, 0.9745534660394696, 0.9750171833486363, 0.9748789625566852, 0.9751311298046158, 0.9754348462872864, 0.9753874287324319, 0.9752562891318447, 0.9753754647187063, 0.9757267912742442, 0.9761615317146954, 0.9762993929396822, 0.9763514244778937, 0.9762401427432364, 0.9768584601641878, 0.9770523307752146, 0.9773196400334259, 0.9771580036705871, 0.9775566157654608, 0.9770249428032559, 0.9770880886634341, 0.9770077722066515, 0.9774538874336145, 0.9774695606307754, 0.9776789429594015, 0.9770791976575551, 0.977948431015646, 0.9780796633224423, 0.9787046548390353, 0.9788299110813222, 0.9785313066530072, 0.9787298815294194, 0.9788224602657817, 0.9792267237618595, 0.9792511612797765, 0.9791963890863689, 0.9790609915087121, 0.9794321012852587, 0.9783291220710463, 0.9784084785747205, 0.9778125456708752, 0.9781875922931103, 0.9785622645364367, 0.9787739115197163, 0.97904581006182, 0.9784838363080837, 0.9777151739189696, 0.977878996481853, 0.977845111230124, 0.9778889832166539, 0.9780750965748057, 0.9781773862840195, 0.9781717907723119, 0.9782179080855845, 0.9784477867699201, 0.9786175112536885, 0.979116710461699, 0.9783755856560699, 0.9781269198726242, 0.9785122736067996, 0.9784848511556803, 0.9785182996699111, 0.9783833177672426, 0.9790662634452987, 0.9797298147781867], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.04844970703124999, 0.10340830607586594, 0.15751912974162272, 0.21183631150578874, 0.26492628845950206, 0.3152160332487175, 0.3621319593383789, 0.40577333795273374, 0.44555106698690317, 0.4820743264873846, 0.5150439457964775, 0.5460523171242093, 0.5742376715846046, 0.6001517479878912, 0.6241102118845087, 0.6454297182751241, 0.6655226533564973, 0.6822461669176849, 0.698312571733823, 0.712919849952007, 0.7262749493882821, 0.7391703566049509, 0.7506725368706606, 0.7614060056045283, 0.7711118670640303, 0.7802021757924917, 0.7873986550788299, 0.7944198023721518, 0.8009310589101626, 0.8068807570477607, 0.8129109901249877, 0.8180930297608323, 0.8224088915061346, 0.8264324448912892, 0.8304433855019344, 0.8338223279664246, 0.837739193908487, 0.8406560807111624, 0.8438092697032992, 0.8467762695966741, 0.8483641146814344, 0.8495490346327186, 0.8517080654220822, 0.8534986787782174, 0.8550816987016004, 0.8570629395995126, 0.8572000191335373, 0.8580924336829094, 0.8598051041418022, 0.861873468915875, 0.862653571901923, 0.8629741580946374, 0.8642251291921766, 0.8649053730030042, 0.8652246236827489, 0.8664936593378174, 0.8661169720090809, 0.8669173253541065, 0.8680038543021296, 0.8679888427894618, 0.8683802234766301, 0.8693428176575816, 0.8697961723752572, 0.8701421269562556, 0.8705459947858559, 0.8707559319695444, 0.8700740581814754, 0.8709110965463098, 0.8714099953630644, 0.8727879152544689, 0.8731510488513865, 0.873048563977543, 0.8733022425007224, 0.8733087675917647, 0.8736106974496816, 0.8739922976030567, 0.8751678748834137, 0.8755168571145754, 0.8755104697841419, 0.8749726122823843, 0.8758213130270375, 0.8755515170029482, 0.8762862925899274, 0.8765447585869588, 0.8771344704165159, 0.8763214081169577, 0.875917182873786, 0.8761850572539526, 0.8762125946855603, 0.8755496665893687, 0.8761391723513656, 0.8769993173809127, 0.8776443181070535, 0.8775737280696915, 0.8782171753399061, 0.8777372076176624, 0.8784253420893299, 0.8792501236277613, 0.8798419716753767, 0.8802321215052035, 0.8806331139857072, 0.881184319247528, 0.8809651896100493, 0.881296993314707, 0.8812019031229201, 0.8811835341585197, 0.8813704035871708, 0.8821367306042067, 0.8820563524421294, 0.8819188589053714, 0.8825347431579065, 0.8822273402370706, 0.8818886129434087, 0.8823233868147304, 0.8820900656878508, 0.8828495796857072, 0.8822574339818202, 0.8826573847666201, 0.8822990375738436, 0.8819796136263237, 0.8823574888130136, 0.8819376520175556, 0.8823248709084205, 0.8825511505250332, 0.8831546927579215, 0.8824333499881534, 0.8823955225665218, 0.8826523876197341, 0.8828245900286944, 0.8824261377558701, 0.8831489560209458, 0.8839256808066523, 0.8839318737858516, 0.8840137781806098, 0.884383549411871, 0.8847428165998255, 0.8854486930723731, 0.8857308604274099, 0.8855556530085394, 0.8856237228733481, 0.8859555994564953, 0.8860457393414181, 0.8854534195017792, 0.8859823433648543, 0.8860189217166219, 0.8866327792625802, 0.8868637502068041, 0.8864865689925995, 0.8862703527936407, 0.8866739027458278, 0.887361540003248, 0.8880221821719744, 0.8871945674468702, 0.8872309641942766, 0.887078556780873, 0.8881612107978158, 0.8881385938125373, 0.8883745861820366, 0.8882940105645859, 0.88871080326754, 0.8880940586429698, 0.8883152380102239, 0.8883148689061142, 0.8887570783633943, 0.8888122404912868, 0.8885393560505919, 0.8875561906356683, 0.8879714641266948, 0.8877144156055916, 0.888509639142924, 0.8887858872015232, 0.8885553477095335, 0.8890467514739717, 0.8888836637700986, 0.8893442949457997, 0.8894607467106324, 0.8893733293249607, 0.8888276979643773, 0.8889337447624426, 0.8873537350734724, 0.8871962571327667, 0.8863941707173063, 0.8870294795021268, 0.887599198391146, 0.8879735500128747, 0.8882841404652769, 0.8874303299108426, 0.8864359967973939, 0.886751799179025, 0.8866859883782159, 0.8870051766262377, 0.8873778952682072, 0.8872972734748202, 0.8870273423434527, 0.8872178274709297, 0.887450299241909, 0.8876808493723115, 0.8883583887291316, 0.8871969780376944, 0.8868750117851297, 0.8870207243208487, 0.8874967510039746, 0.8871102453783513, 0.8864011798484228, 0.8872372772720896, 0.888168752387501], "moving_var_accuracy_train": [0.020699242632709666, 0.04602444191858879, 0.06826070155880891, 0.08856168641494952, 0.10582983872265353, 0.11899835522107602, 0.12793179590548653, 0.13309767523238186, 0.13545429879876975, 0.13514331323386258, 0.13249517568928001, 0.1291127239590626, 0.1240645916411725, 0.1185010798780381, 0.11248729500898563, 0.10601205180741465, 0.09968029813553925, 0.09299491035898279, 0.0866369124386202, 0.08045214172960216, 0.07443422079080314, 0.06890567651001288, 0.06355026722787657, 0.05851970740317688, 0.05386600572155641, 0.049471869956313856, 0.045303451441045556, 0.041450678318912505, 0.0378923243394863, 0.03462852140638186, 0.03168708281094698, 0.028911432714643776, 0.026348946551525924, 0.023974839708519907, 0.02185425151676172, 0.019836069536671337, 0.018080217445678625, 0.016409729169833225, 0.014930478342558581, 0.013548777711934683, 0.012264736898362416, 0.011105428132644899, 0.010073414140974707, 0.009128124378088255, 0.008268075258886138, 0.007521007389495396, 0.006774807302288007, 0.00611666015534613, 0.005557154874292539, 0.005062345036630957, 0.004582248333840372, 0.004134257676331061, 0.0037556378651432596, 0.003393599213254761, 0.0030651010680869816, 0.002770961720886319, 0.002495188087269172, 0.0022526343988634435, 0.0020600490469454124, 0.001856001984555219, 0.0016801464368360096, 0.0015309394227918777, 0.0013945870975545925, 0.0012632633897813272, 0.0011409741844402453, 0.0010368695707571563, 0.0009344291569689538, 0.0008617256285640986, 0.0007852576412325433, 0.0007237825513510427, 0.0006586812446941513, 0.0005928535865787528, 0.0005412732158646619, 0.0004884575954568057, 0.00044864141135992865, 0.00040650317729020314, 0.0003843145710634393, 0.0003566938491214988, 0.0003212384726615364, 0.0002891520101729112, 0.0002653095146311613, 0.0002387800786887219, 0.00022185576801088814, 0.00020351690388716086, 0.0001899748441189625, 0.00017631719200298827, 0.0001605590099742296, 0.00015136105374295397, 0.00013754776759043852, 0.0001241921500649884, 0.00011670829554076971, 0.00011376848796178532, 0.0001083614747763982, 9.753459214676578e-05, 9.846462433319808e-05, 8.883839345515944e-05, 8.303137853136078e-05, 8.023185454670693e-05, 7.376251410769175e-05, 7.25119289779464e-05, 6.634628732601929e-05, 6.288624580442951e-05, 5.6850651754608636e-05, 5.285097716641092e-05, 4.885002548167045e-05, 4.61561954730141e-05, 4.472985466333274e-05, 4.49752033474646e-05, 4.06947990175963e-05, 3.773480844079818e-05, 3.544901682157412e-05, 3.211175237231583e-05, 2.9816376937296426e-05, 3.0637412162537313e-05, 2.92315250825796e-05, 2.920714967843904e-05, 2.7414975891296788e-05, 2.749724628079467e-05, 2.4753292356190826e-05, 2.227931764003816e-05, 2.1255759631791052e-05, 2.182579381042972e-05, 2.087465226726595e-05, 2.0114482728262464e-05, 2.0451124433746664e-05, 1.841338029309355e-05, 1.9033987050957474e-05, 1.756475421645714e-05, 1.6033473588825085e-05, 1.518818417809417e-05, 1.7052306723763798e-05, 1.6074980603469622e-05, 1.467881998518533e-05, 1.3211576937108718e-05, 1.3690443508214025e-05, 1.3388692051942557e-05, 1.664263149180851e-05, 2.2322020508368072e-05, 2.051331295288775e-05, 1.8472877319759394e-05, 1.6840870283614283e-05, 1.6384549652771687e-05, 1.4757013388895356e-05, 1.5216615735393446e-05, 1.3866899047802413e-05, 1.30525040313822e-05, 1.2577446944856076e-05, 1.1339938070945896e-05, 1.0360722617430908e-05, 9.45247574022225e-06, 9.618101303835885e-06, 1.0357284428525416e-05, 9.492607441866408e-06, 8.567712226397718e-06, 7.8223936239728e-06, 1.0481002159043072e-05, 9.771174267517832e-06, 9.437144996495284e-06, 8.728567320971271e-06, 9.285735008490852e-06, 1.0901246756300244e-05, 9.847008677589135e-06, 8.920364408901344e-06, 9.819497129566528e-06, 8.839758258593073e-06, 8.350351068601562e-06, 1.0752565805522758e-05, 1.6477408902332795e-05, 1.4984665277223524e-05, 1.700172831181985e-05, 1.544275761672448e-05, 1.4700963296535658e-05, 1.3585754800760785e-05, 1.230431672252256e-05, 1.254474581861967e-05, 1.129564596729522e-05, 1.0193081309101906e-05, 9.338765714509718e-06, 9.644391339295382e-06, 1.9629020528228556e-05, 1.772279556748434e-05, 1.9146740243704678e-05, 1.8498005938983976e-05, 1.791161895435926e-05, 1.6523607068705364e-05, 1.5536605716617754e-05, 1.6825275643953703e-05, 2.0460324895504472e-05, 1.865583289494123e-05, 1.6800583498009772e-05, 1.5137847909027493e-05, 1.393580675686793e-05, 1.2636394742680487e-05, 1.1373037056173869e-05, 1.0254874609807914e-05, 9.704985034433953e-06, 8.993744134504702e-06, 1.0337168364560293e-05, 1.4246845325773175e-05, 1.3378672839905697e-05, 1.3377283059901284e-05, 1.2046322671339634e-05, 1.0851759632143898e-05, 9.930564695361521e-06, 1.3135241418404982e-05, 1.578442061896149e-05], "duration": 243776.076579, "accuracy_train": [0.4795743788067553, 0.5996731814091916, 0.6492130903354558, 0.7067471853082319, 0.7514057236872462, 0.7802325220907161, 0.7990114332433554, 0.8127040363141381, 0.8278879790167036, 0.8358617974806202, 0.838208755825489, 0.8565959965623846, 0.8541778418004798, 0.8638951613833518, 0.8703816055855482, 0.8714955140849945, 0.8820277172157622, 0.8769857491809707, 0.8858878247277593, 0.8891437540374677, 0.8898623872392949, 0.9006503567391103, 0.8999760635843485, 0.9037435256321521, 0.909950591489018, 0.911113886870155, 0.9096250706556847, 0.9126731785252861, 0.9153231271917681, 0.9190640933577889, 0.9264122845722591, 0.9239944902985419, 0.9249470803340717, 0.9243901260843485, 0.9314106335363603, 0.924597587036268, 0.9361059921788483, 0.9299228987864526, 0.9371304996077889, 0.9341532276670359, 0.9305517704295865, 0.9326204318936876, 0.9375729988579733, 0.9372456755837025, 0.9378266022978959, 0.94580078125, 0.9271087477505537, 0.9344780275239941, 0.9453611659053157, 0.9497084732027501, 0.9433375654646549, 0.9386636558693245, 0.9487319107027501, 0.9432917834648394, 0.9432445595122739, 0.9450814270717978, 0.9383631889650241, 0.9437103102505537, 0.9548477730481728, 0.9423624449174051, 0.948570231750646, 0.9536612261789406, 0.9542897373338871, 0.9515220892741787, 0.9496630516911223, 0.9541723984288483, 0.9409673556316908, 0.9594969892026578, 0.9562188898694168, 0.9606373935838871, 0.9572415948574198, 0.9484783072628276, 0.9583344147978959, 0.9538247075719823, 0.9604052391911223, 0.9568939040005537, 0.9662631722383721, 0.9643329382382798, 0.9560110684293098, 0.9552677417866371, 0.962195243286268, 0.9555682086909376, 0.964241374238649, 0.9628680944882798, 0.9656826060123662, 0.9501513329411223, 0.9525211822858989, 0.9653567246908453, 0.9613342172503692, 0.9557778325719823, 0.9650784278100776, 0.9682631607027501, 0.9675430855481728, 0.9598922645002769, 0.9710762302740864, 0.959706252595515, 0.9669610773694168, 0.969518741059893, 0.966635917024271, 0.9711463452265596, 0.9671943132267442, 0.970007743286268, 0.9629857938815062, 0.9688222778815985, 0.9687049389765596, 0.9702395371908453, 0.971751604893411, 0.9736346149409376, 0.96556490661914, 0.9704738545127353, 0.9713795810838871, 0.9692393627145626, 0.9646824315360835, 0.9740535022148394, 0.972495292024271, 0.974307826631137, 0.9656589940360835, 0.9744473355597084, 0.9696593314530271, 0.9693087566906607, 0.9730773002030271, 0.9643122101674971, 0.972936709809893, 0.9734478820598007, 0.9750994586909376, 0.9702162857027501, 0.9757039973814139, 0.9731931971553157, 0.9727982823458842, 0.9684724240956073, 0.9772153441076044, 0.9745414229766519, 0.9704495215600776, 0.9719129233573275, 0.9763092570482651, 0.9757279698458842, 0.9797722868217055, 0.9823760930001846, 0.9720771257267442, 0.9736814784053157, 0.9755412369647471, 0.9778427737979882, 0.9748669438099853, 0.979190639131137, 0.9736349754291252, 0.9774006350359912, 0.9781682946313216, 0.9749606707387413, 0.9740760327265596, 0.9764480450004615, 0.9788887302740864, 0.980074195678756, 0.9775401439645626, 0.9768197083217978, 0.9752386071313216, 0.9824233169527501, 0.9787971662744556, 0.9797254233573275, 0.9757032764050388, 0.9811441246193245, 0.972239886143411, 0.9776564014050388, 0.9762849240956073, 0.9814689244762828, 0.9776106194052234, 0.9795633839170359, 0.9716814899409376, 0.9857715312384644, 0.9792607540836102, 0.9843295784883721, 0.9799572172619048, 0.9758438667981728, 0.9805170554171282, 0.9796556688930418, 0.9828650952265596, 0.97947109894103, 0.9787034393456996, 0.9778424133098007, 0.9827720892741787, 0.9684023091431341, 0.9791226871077889, 0.972449149536268, 0.9815630118932264, 0.981934314726375, 0.9806787343692323, 0.981492896940753, 0.9734260725244556, 0.9707972124169435, 0.9793533995478036, 0.9775401439645626, 0.9782838310954227, 0.9797501167981728, 0.9790979936669435, 0.9781214311669435, 0.9786329639050388, 0.9805166949289406, 0.9801450316076044, 0.9836095033337948, 0.971705462405408, 0.9758889278216132, 0.981980457214378, 0.9782380490956073, 0.9788193362979882, 0.9771684806432264, 0.9852127745478036, 0.9857017767741787], "end": "2016-02-02 15:37:11.674000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0], "moving_var_accuracy_valid": [0.021126367002725598, 0.0461977587829748, 0.06792981404475693, 0.0876900387534943, 0.10428794575465715, 0.11662067705787126, 0.12476854643973925, 0.12943282114201857, 0.13072994857185893, 0.12966249007552855, 0.12647920324445427, 0.12248495475159532, 0.11738618713095893, 0.11169142262038137, 0.10568835228891296, 0.09921020923467709, 0.09292272267286786, 0.08614753355806211, 0.07985594447570014, 0.07379070322062102, 0.06801686102713411, 0.06271179866997412, 0.05763132016076065, 0.05290505430423282, 0.048462382593848954, 0.04435984774947274, 0.04038996680159372, 0.03679463870526504, 0.03349674299007262, 0.030465658862422213, 0.027746366374871132, 0.0252134115504712, 0.022859710358865155, 0.020719440159567344, 0.018792284944849712, 0.017015811719969763, 0.015452307097245578, 0.013983650445097619, 0.01267476880796905, 0.011486519722477726, 0.01036055901834873, 0.00933713943413242, 0.008445378216263962, 0.007629697060358092, 0.006889280923022733, 0.006235680670180501, 0.005612281720350287, 0.005058221181866639, 0.004578798224586851, 0.004159421597675793, 0.0037489564839277836, 0.003374985815097637, 0.0030515715917697806, 0.002750579017372325, 0.00247643840460375, 0.002243288627587891, 0.002020236804921778, 0.0018239782137216439, 0.0016522052987435088, 0.0014869867969787712, 0.001339666726861489, 0.0012140393421901543, 0.001094485182471487, 0.0009861138253733622, 0.0008889704258501008, 0.0008004700458549463, 0.0007246076080351505, 0.0006584525462494771, 0.0005948473918887613, 0.0005524506217440364, 0.0004983923536525254, 0.00044864764663157325, 0.00040436205710651837, 0.0003639262345871845, 0.00032835406588038155, 0.0002968292273858473, 0.0002795841421260861, 0.0002527218252924765, 0.00022745000994513946, 0.0002073086251803975, 0.00019306039924813092, 0.0001744094683748473, 0.0001618275780063493, 0.00014624606225030706, 0.000134751296402553, 0.00012722579948896382, 0.0001159738019650222, 0.00010502223192046669, 9.452683351967599e-05, 8.902941311419001e-05, 8.325412519361844e-05, 8.158735792094889e-05, 7.717285555935172e-05, 6.950041658378942e-05, 6.627659443132983e-05, 6.17222561177584e-05, 5.981179196585643e-05, 5.9952994044507296e-05, 5.711025164325248e-05, 5.276917848635211e-05, 4.893941536250184e-05, 4.677991899218261e-05, 4.253408727515831e-05, 3.9271521833463767e-05, 3.542574895128415e-05, 3.188621082583404e-05, 2.901187139352995e-05, 3.139599812752976e-05, 2.8314544155227104e-05, 2.5653229993556724e-05, 2.6501727706888074e-05, 2.470202393784486e-05, 2.326444715930386e-05, 2.263925731603065e-05, 2.086528031866318e-05, 2.397050590325654e-05, 2.472918412461728e-05, 2.3695911384514098e-05, 2.2482034641199708e-05, 2.115211610132251e-05, 2.032201140163371e-05, 1.98761766748542e-05, 1.9238005232352764e-05, 1.777502689316729e-05, 1.927589324576821e-05, 2.203132244466116e-05, 1.9841068424640704e-05, 1.8450778482232377e-05, 1.687258366087478e-05, 1.6614203218256622e-05, 1.965497909537357e-05, 2.3119193720415333e-05, 2.080761952529606e-05, 1.878723254169284e-05, 1.81390861587401e-05, 1.7486833753932524e-05, 2.022250472900509e-05, 1.891682000234068e-05, 1.7301416758752003e-05, 1.5612976641332486e-05, 1.5042957575171874e-05, 1.3611788607339675e-05, 1.5408194878473177e-05, 1.6385219466998414e-05, 1.4758739302660874e-05, 1.667425515296424e-05, 1.5486957831348975e-05, 1.5218653063353088e-05, 1.4117532759247273e-05, 1.4171452558514708e-05, 1.700991228279512e-05, 1.923695373041143e-05, 2.3477773556251876e-05, 2.1141918709622547e-05, 1.9236779015603574e-05, 2.786235859766497e-05, 2.5080726490106294e-05, 2.3073885427252653e-05, 2.0824928755675353e-05, 2.0305881295229552e-05, 2.1698658553133563e-05, 1.9969075510311055e-05, 1.7972169185420546e-05, 1.7934895103850555e-05, 1.6168791336648155e-05, 1.5222105464743918e-05, 2.2399423016187468e-05, 2.17115493657131e-05, 2.0135059908953916e-05, 2.3812978187005793e-05, 2.2118497277223516e-05, 2.0384983665802844e-05, 2.0519784236559083e-05, 1.8707184205294507e-05, 1.8746095505013992e-05, 1.699353507630641e-05, 1.5362957762534784e-05, 1.6506084221150356e-05, 1.4956689109454418e-05, 3.5928895753665224e-05, 3.255919989457877e-05, 3.509336346591592e-05, 3.521658238795612e-05, 3.4616140661708466e-05, 3.2415778825756315e-05, 3.004239880529185e-05, 3.3599091090533616e-05, 3.913746704598594e-05, 3.6121300639582174e-05, 3.254815012915221e-05, 3.021026535531393e-05, 2.8439511494427566e-05, 2.5654059207105227e-05, 2.374441862752706e-05, 2.1696538018883768e-05, 2.0013272335715725e-05, 1.8490325365801522e-05, 2.0772829049582535e-05, 3.0835419292287216e-05, 2.8684837773173034e-05, 2.6007443283446567e-05, 2.5446111582533164e-05, 2.4245979812026445e-05, 2.6346347162418883e-05, 3.000324256293656e-05, 3.481173132232124e-05], "accuracy_test": 0.18221261160714286, "start": "2016-01-30 19:54:15.597000", "learning_rate_per_epoch": [0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947, 0.0023891031742095947], "accuracy_train_first": 0.4795743788067553, "accuracy_train_last": 0.9857017767741787, "batch_size_eval": 1024, "accuracy_train_std": [0.019495508816959977, 0.016705662720801578, 0.016332676444905877, 0.017087676291148907, 0.017007133664563766, 0.017497661818937957, 0.020289025784632465, 0.018883056318672316, 0.017039792281395526, 0.016525797807437414, 0.017644799285970177, 0.01854056998079206, 0.0187273567862793, 0.018120375948592837, 0.017647300997015853, 0.019905244700792277, 0.01881993695858219, 0.018678349350901215, 0.01888274616507529, 0.018114451294044962, 0.019504547450952862, 0.018473340025899947, 0.018980413166760603, 0.017123743329202638, 0.018660538963437823, 0.017391085103605583, 0.01827784071359015, 0.016787110239517077, 0.016717487548298278, 0.01713950795692317, 0.01592701607747828, 0.016176071527834533, 0.01681914801430688, 0.015295436813533273, 0.015405355251326096, 0.01685620532261446, 0.016634230202420327, 0.016171632988610178, 0.013930877757870084, 0.01589275761953257, 0.01457788295844505, 0.015230746415016554, 0.01387270617124758, 0.014226229568502016, 0.014926901060026398, 0.01344452835751403, 0.01592390589415561, 0.015421335946222862, 0.012453419677356047, 0.013301390950127412, 0.013423133553260885, 0.014339629156006952, 0.012616390530253327, 0.012967272877486504, 0.012389928960716418, 0.013101619233037223, 0.012389455569091228, 0.012668583604289126, 0.010920377418643625, 0.013042610875505642, 0.011083157324458059, 0.011548260137328157, 0.010583497987924856, 0.01215886294233836, 0.011149368028259039, 0.013173337693105204, 0.013132640601192581, 0.009405297158577158, 0.010856155240338103, 0.00841994668237758, 0.00972670407457404, 0.011062716250770186, 0.010109220389303347, 0.010798209257618754, 0.010299949917904134, 0.00966175622380116, 0.009017400242308524, 0.010023218753190429, 0.009581511018481101, 0.008915395234743929, 0.009005415674057984, 0.00967453065560333, 0.009081923113846101, 0.008944109145357194, 0.009027447290347913, 0.011701257253640962, 0.010138295576895872, 0.008392443837279228, 0.009936243891487567, 0.010111495755853758, 0.009252281892358518, 0.008432465134649195, 0.009080779544197736, 0.008497523842813035, 0.007298759064638216, 0.010183802904920769, 0.008681239170607159, 0.008134236202617214, 0.00996310964688801, 0.008385938756210376, 0.008027729021913293, 0.007409844235059606, 0.009791686515634002, 0.007556444761968756, 0.008090763785623024, 0.007717856568381156, 0.007144438623622031, 0.007419919055154432, 0.009910053935544099, 0.0068443250394615365, 0.007458931354681068, 0.00818005449225442, 0.00943368452633113, 0.006991943817942833, 0.0066244930422581574, 0.00785938640822219, 0.009182267801132524, 0.00739012532946673, 0.0077695156530591, 0.008052923263001109, 0.008547071302294514, 0.008695036705469051, 0.00805719774753587, 0.0073365309296639725, 0.007262589432498874, 0.008227688370234578, 0.0059720642894849265, 0.006717471166038805, 0.007416124310959692, 0.007671112026368624, 0.007387355573204034, 0.006734342973181589, 0.006868288568396008, 0.007322134512796852, 0.006095573209383383, 0.006563984439618658, 0.006000937326496197, 0.005659845336383112, 0.006992284566285556, 0.006188618097969503, 0.0059444843262762125, 0.006820644197509477, 0.006485778566628166, 0.006213071056970866, 0.006932904595628446, 0.006552262143740784, 0.006413363526701462, 0.006246114134338902, 0.007613354505762944, 0.006418976812674925, 0.005372910881213275, 0.006584063088477773, 0.005858567937916453, 0.0061911488165396445, 0.006986736595969229, 0.006338248638419121, 0.00561449097547888, 0.006013632221561453, 0.007727993535057062, 0.0059825084232330815, 0.008285517851512337, 0.005378438723813829, 0.00622866507085153, 0.00532226097177023, 0.006177786600836378, 0.0059980054680943565, 0.006883852989134819, 0.004477972831461565, 0.006359340380957482, 0.005667992914761337, 0.0064821212283120615, 0.006968346836170872, 0.0055354684725803386, 0.006624874294713337, 0.005457263154678207, 0.006773644423776481, 0.006411390540373094, 0.006362003918441748, 0.004636756596649776, 0.007283176170010985, 0.006490946390541647, 0.008250840675657978, 0.0051888778435685195, 0.005912227842323513, 0.006647411182793053, 0.005795299580597641, 0.006728723068494812, 0.007032162009899905, 0.0065096148803879435, 0.0059470428554838, 0.006177703939354266, 0.005262733507646288, 0.00626675161115847, 0.006508638586838253, 0.006131833807546862, 0.005287780056964481, 0.005656162040874707, 0.004683742757561013, 0.007455937263024002, 0.0066238152255516025, 0.005830072960400531, 0.0063007060866728335, 0.006520891153721678, 0.006299749521799794, 0.005078471134965594, 0.004083782333812967], "accuracy_test_std": 0.008340592369198764, "error_valid": [0.5155029296875, 0.4019643025225903, 0.35548345726656627, 0.2993090526167168, 0.25726391895707834, 0.23217626364834332, 0.21562470585466864, 0.2014542545180723, 0.1964493717055723, 0.1892163380082832, 0.18822948042168675, 0.17487234092620485, 0.17209413827183728, 0.16662156438253017, 0.16026361304593373, 0.16269472420933728, 0.1536409309111446, 0.1672422110316265, 0.15708978492093373, 0.15561464608433728, 0.15352915568524095, 0.14477097844503017, 0.14580784073795183, 0.14199277579066272, 0.14153537980045183, 0.1379850456513554, 0.1478330313441265, 0.14238987198795183, 0.14046763224774095, 0.1395719597138554, 0.13281691217996983, 0.13526861351656627, 0.1387483527861446, 0.1373555746423193, 0.13345814900225905, 0.13576718985316272, 0.12700901261295183, 0.13309193806475905, 0.12781202936746983, 0.12652073136295183, 0.13734527955572284, 0.13978668580572284, 0.1288606574736446, 0.13038580101656627, 0.13067112198795183, 0.12510589231927716, 0.14156626506024095, 0.13387583537274095, 0.12478086172816272, 0.11951124811746983, 0.1303255012236446, 0.13414056617093373, 0.12451613092996983, 0.12897243269954817, 0.13190212019954817, 0.12208501976656627, 0.13727321394954817, 0.12587949454066272, 0.12221738516566272, 0.13214626082454817, 0.1280973503388554, 0.12199383471385539, 0.12612363516566272, 0.12674428181475905, 0.12581919474774095, 0.12735463337725905, 0.1360628059111446, 0.12155555817018071, 0.12409991528614461, 0.1148108057228916, 0.12358074877635539, 0.12787379988704817, 0.12441465079066272, 0.1266325065888554, 0.12367193382906627, 0.12257330101656627, 0.11425192959337349, 0.12134230280496983, 0.12454701618975905, 0.12986810523343373, 0.11654038027108427, 0.1268766472138554, 0.11710072712725905, 0.12112904743975905, 0.11755812311746983, 0.13099615257906627, 0.12772084431475905, 0.12140407332454817, 0.12353956842996983, 0.1304166862763554, 0.11855527579066272, 0.11525937735316272, 0.11655067535768071, 0.12306158226656627, 0.11599179922816272, 0.12658250188253017, 0.11538144766566272, 0.11332684252635539, 0.11483139589608427, 0.11625653002635539, 0.11575795368975905, 0.11385483339608427, 0.12100697712725905, 0.11571677334337349, 0.11965390860316272, 0.11898178652108427, 0.11694777155496983, 0.11096632624246983, 0.11866705101656627, 0.11931858292545183, 0.11192229856927716, 0.12053928605045183, 0.12115993269954817, 0.11376364834337349, 0.12000982445406627, 0.11031479433358427, 0.12307187735316272, 0.11374305817018071, 0.12092608716114461, 0.12089520190135539, 0.11424163450677716, 0.12184087914156627, 0.11419015907379515, 0.11541233292545183, 0.11141342714608427, 0.12405873493975905, 0.11794492422816272, 0.11503582690135539, 0.11562558829066272, 0.12115993269954817, 0.11034567959337349, 0.10908379612198793, 0.11601238940135539, 0.11524908226656627, 0.11228850950677716, 0.11202377870858427, 0.10819841867469882, 0.11172963337725905, 0.11602121376129515, 0.11376364834337349, 0.11105751129518071, 0.11314300169427716, 0.11987745905496983, 0.10925734186746983, 0.11365187311746983, 0.10784250282379515, 0.11105751129518071, 0.11690806193524095, 0.11567559299698793, 0.10969414768448793, 0.10644972467996983, 0.10603203830948793, 0.12025396507906627, 0.11244146507906627, 0.11429310993975905, 0.10209490304969882, 0.11206495905496983, 0.10950148249246983, 0.11243116999246983, 0.10753806240587349, 0.11745664297816272, 0.10969414768448793, 0.11168845303087349, 0.10726303652108427, 0.11069130035768071, 0.11391660391566272, 0.12129229809864461, 0.10829107445406627, 0.11459902108433728, 0.10433334902108427, 0.10872788027108427, 0.11351950771837349, 0.10653061464608427, 0.11258412556475905, 0.1065100244728916, 0.10949118740587349, 0.11141342714608427, 0.11608298428087349, 0.11011183405496983, 0.12686635212725905, 0.11422104433358427, 0.12082460702183728, 0.10725274143448793, 0.10727333160768071, 0.10865728539156627, 0.10892054546310237, 0.12025396507906627, 0.12251300122364461, 0.11040597938629515, 0.11390630882906627, 0.11012212914156627, 0.10926763695406627, 0.11342832266566272, 0.11540203783885539, 0.11106780638177716, 0.11045745481927716, 0.11024419945406627, 0.10554375705948793, 0.12325571818524095, 0.11602268448795183, 0.11166786285768071, 0.1082190088478916, 0.11636830525225905, 0.11998040992093373, 0.1052378459149097, 0.10344797157379515], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.04174575949024245, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 16, "valid_ratio": 0.15, "learning_rate": 0.002389103076468668, "optimization": "adam", "nb_data_augmentation": 4, "learning_rate_decay_method": "none", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 1.353576257274931e-07, "rotation_range": [0, 0], "momentum": 0.8133933274359157}, "accuracy_valid_max": 0.8979050969503012, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8965520284262049, "accuracy_valid_std": [0.01251027162051176, 0.014789425720139792, 0.014523632081255834, 0.012427373165144183, 0.014741129522911081, 0.017187055874177282, 0.012940617467536305, 0.011779177509845676, 0.009190180887536448, 0.00959599351579517, 0.009850816039502516, 0.007857074402652974, 0.015335435402274195, 0.011969010276879096, 0.009972294495438539, 0.014456868989449697, 0.014336709531765131, 0.0137642538072751, 0.011685315535793533, 0.015325569074324235, 0.013092232248552483, 0.012561912194946498, 0.013203445754384464, 0.015019265411736161, 0.013182577705177811, 0.009970922696173152, 0.012907406748274978, 0.012437277168802047, 0.008920341032324954, 0.014143943923977354, 0.01473037757010173, 0.013974184071423033, 0.013037261736108632, 0.012146909353333424, 0.015380214916585219, 0.01477413783388478, 0.011516197685844753, 0.010962841484406195, 0.011897638706832194, 0.010680858472826864, 0.013587211565396027, 0.014054434765451547, 0.015251951876704284, 0.014106655337059806, 0.007816977345479561, 0.01270948299616271, 0.013243489066499222, 0.011100206041356666, 0.008076097279003516, 0.012539675945214662, 0.010678162553140035, 0.011392624142342274, 0.01203397037438243, 0.009735085251521666, 0.009185342414791286, 0.011947245923501857, 0.01184918520976581, 0.007176631914346819, 0.009565723759748146, 0.008996706883827147, 0.008421455835890768, 0.006114213847938457, 0.008679498566387481, 0.009994477928264809, 0.007427877132841289, 0.009029561133095854, 0.009178902507083175, 0.014125598442037113, 0.007893021522264856, 0.011976384263881005, 0.008524023011094093, 0.011764939241233799, 0.009210683280854925, 0.00924547534127154, 0.007711486338749661, 0.01114117019983555, 0.0083926833350924, 0.008833325714591476, 0.007578085757381886, 0.007902346123742597, 0.008858157228512048, 0.009691254401180793, 0.0064687632801725585, 0.00793182725344995, 0.005204244777059341, 0.011830311580998603, 0.010062363550436468, 0.008220746729148859, 0.0063746483387819, 0.007965711777659278, 0.007276206061378024, 0.00603602399806064, 0.008825960803075737, 0.007218649455959592, 0.0099159561861693, 0.009548144125632358, 0.0056147127782514815, 0.010331222278640471, 0.009475514172756628, 0.007651462321814656, 0.007587556220305417, 0.008724889512440374, 0.005927211443085962, 0.00782023299244876, 0.007962007063432466, 0.012792290842430207, 0.005336230659599954, 0.009428294865728847, 0.009083904589931538, 0.007230904598305547, 0.005656918707703996, 0.005944831966975857, 0.009014569108373394, 0.008630440567077777, 0.008058483655133498, 0.009112125660228619, 0.008260110902143415, 0.007535449405965932, 0.009252773767747571, 0.0043544972735423135, 0.00772963323869273, 0.010409226023280232, 0.012030939235079932, 0.007391207744259039, 0.009802577970911407, 0.00512142753926522, 0.005326525881588281, 0.0058343352019497845, 0.006072660819755244, 0.006548058107125219, 0.007130276984016115, 0.009352686972843352, 0.004038247700693565, 0.00924547908457106, 0.010211185485078055, 0.007234701454982329, 0.009428524741499768, 0.005515645966298331, 0.011591666195734238, 0.011256028305095185, 0.009806053632445407, 0.00581621575103606, 0.007361977423112101, 0.00820554345303795, 0.007071549861898319, 0.010523518185269724, 0.010140730154049041, 0.009803245224055126, 0.009991763179790137, 0.007908284045855692, 0.00521508287248727, 0.007403058762271203, 0.0049155264734495625, 0.006143585433683574, 0.005796045494566245, 0.00913973824329992, 0.004673620131400196, 0.008560914288350404, 0.00953448350404922, 0.007493819642736751, 0.006941544236923791, 0.006806503740616721, 0.008203208487360945, 0.005751629687810803, 0.006393034608983166, 0.005880347005284739, 0.00838000677357271, 0.0071898540577268796, 0.009474753068448402, 0.006511723946442855, 0.006778226706535131, 0.005780487143711714, 0.004594176258248646, 0.005054597173554442, 0.008345337983611185, 0.0034052956773068586, 0.008830103219333217, 0.007548054486436537, 0.0011644579888953315, 0.008076866455740923, 0.007773667201145752, 0.007170337579678193, 0.009249700912583073, 0.007095482763434264, 0.011739967730334287, 0.010754786699437758, 0.006006940108429717, 0.004587241237898209, 0.011041939826340642, 0.004570781602668241, 0.002336973087757927, 0.0013549144656297906, 0.004161436083638474, 0.0029426517113542916, 0.006955520095748479, 0.006596713870365839, 0.005622222911523157, 0.007179182962688008, 0.005609229613451742, 0.006042882054886246, 0.007898118058425005, 0.008976054318719746, 0.0053650332485777694, 0.008317131671457286, 0.01367554365120774, 0.008499486497859787], "accuracy_valid": [0.4844970703125, 0.5980356974774097, 0.6445165427334337, 0.7006909473832832, 0.7427360810429217, 0.7678237363516567, 0.7843752941453314, 0.7985457454819277, 0.8035506282944277, 0.8107836619917168, 0.8117705195783133, 0.8251276590737951, 0.8279058617281627, 0.8333784356174698, 0.8397363869540663, 0.8373052757906627, 0.8463590690888554, 0.8327577889683735, 0.8429102150790663, 0.8443853539156627, 0.846470844314759, 0.8552290215549698, 0.8541921592620482, 0.8580072242093373, 0.8584646201995482, 0.8620149543486446, 0.8521669686558735, 0.8576101280120482, 0.859532367752259, 0.8604280402861446, 0.8671830878200302, 0.8647313864834337, 0.8612516472138554, 0.8626444253576807, 0.866541850997741, 0.8642328101468373, 0.8729909873870482, 0.866908061935241, 0.8721879706325302, 0.8734792686370482, 0.8626547204442772, 0.8602133141942772, 0.8711393425263554, 0.8696141989834337, 0.8693288780120482, 0.8748941076807228, 0.858433734939759, 0.866124164627259, 0.8752191382718373, 0.8804887518825302, 0.8696744987763554, 0.8658594338290663, 0.8754838690700302, 0.8710275673004518, 0.8680978798004518, 0.8779149802334337, 0.8627267860504518, 0.8741205054593373, 0.8777826148343373, 0.8678537391754518, 0.8719026496611446, 0.8780061652861446, 0.8738763648343373, 0.873255718185241, 0.874180805252259, 0.872645366622741, 0.8639371940888554, 0.8784444418298193, 0.8759000847138554, 0.8851891942771084, 0.8764192512236446, 0.8721262001129518, 0.8755853492093373, 0.8733674934111446, 0.8763280661709337, 0.8774266989834337, 0.8857480704066265, 0.8786576971950302, 0.875452983810241, 0.8701318947665663, 0.8834596197289157, 0.8731233527861446, 0.882899272872741, 0.878870952560241, 0.8824418768825302, 0.8690038474209337, 0.872279155685241, 0.8785959266754518, 0.8764604315700302, 0.8695833137236446, 0.8814447242093373, 0.8847406226468373, 0.8834493246423193, 0.8769384177334337, 0.8840082007718373, 0.8734174981174698, 0.8846185523343373, 0.8866731574736446, 0.8851686041039157, 0.8837434699736446, 0.884242046310241, 0.8861451666039157, 0.878993022872741, 0.8842832266566265, 0.8803460913968373, 0.8810182134789157, 0.8830522284450302, 0.8890336737575302, 0.8813329489834337, 0.8806814170745482, 0.8880777014307228, 0.8794607139495482, 0.8788400673004518, 0.8862363516566265, 0.8799901755459337, 0.8896852056664157, 0.8769281226468373, 0.8862569418298193, 0.8790739128388554, 0.8791047980986446, 0.8857583654932228, 0.8781591208584337, 0.8858098409262049, 0.8845876670745482, 0.8885865728539157, 0.875941265060241, 0.8820550757718373, 0.8849641730986446, 0.8843744117093373, 0.8788400673004518, 0.8896543204066265, 0.8909162038780121, 0.8839876105986446, 0.8847509177334337, 0.8877114904932228, 0.8879762212914157, 0.8918015813253012, 0.888270366622741, 0.8839787862387049, 0.8862363516566265, 0.8889424887048193, 0.8868569983057228, 0.8801225409450302, 0.8907426581325302, 0.8863481268825302, 0.8921574971762049, 0.8889424887048193, 0.883091938064759, 0.8843244070030121, 0.8903058523155121, 0.8935502753200302, 0.8939679616905121, 0.8797460349209337, 0.8875585349209337, 0.885706890060241, 0.8979050969503012, 0.8879350409450302, 0.8904985175075302, 0.8875688300075302, 0.8924619375941265, 0.8825433570218373, 0.8903058523155121, 0.8883115469691265, 0.8927369634789157, 0.8893086996423193, 0.8860833960843373, 0.8787077019013554, 0.8917089255459337, 0.8854009789156627, 0.8956666509789157, 0.8912721197289157, 0.8864804922816265, 0.8934693853539157, 0.887415874435241, 0.8934899755271084, 0.8905088125941265, 0.8885865728539157, 0.8839170157191265, 0.8898881659450302, 0.873133647872741, 0.8857789556664157, 0.8791753929781627, 0.8927472585655121, 0.8927266683923193, 0.8913427146084337, 0.8910794545368976, 0.8797460349209337, 0.8774869987763554, 0.8895940206137049, 0.8860936911709337, 0.8898778708584337, 0.8907323630459337, 0.8865716773343373, 0.8845979621611446, 0.8889321936182228, 0.8895425451807228, 0.8897558005459337, 0.8944562429405121, 0.876744281814759, 0.8839773155120482, 0.8883321371423193, 0.8917809911521084, 0.883631694747741, 0.8800195900790663, 0.8947621540850903, 0.8965520284262049], "seed": 954490147, "model": "residualv3", "loss_std": [0.27538931369781494, 0.2547362744808197, 0.2613429129123688, 0.25903037190437317, 0.2521509528160095, 0.24779877066612244, 0.24302898347377777, 0.23878756165504456, 0.23435351252555847, 0.23004187643527985, 0.22668419778347015, 0.22065986692905426, 0.2170419842004776, 0.21106205880641937, 0.20614373683929443, 0.20320989191532135, 0.20064562559127808, 0.1950627565383911, 0.19139590859413147, 0.18821865320205688, 0.18304139375686646, 0.18072955310344696, 0.17588387429714203, 0.17173084616661072, 0.17034965753555298, 0.16638341546058655, 0.1619798243045807, 0.1590808480978012, 0.15666666626930237, 0.15603169798851013, 0.15271513164043427, 0.14842656254768372, 0.14480291306972504, 0.1399298459291458, 0.1400396227836609, 0.13971567153930664, 0.13455316424369812, 0.13259188830852509, 0.1317504197359085, 0.1278681755065918, 0.12537544965744019, 0.12245882302522659, 0.12093695998191833, 0.11683325469493866, 0.11747381091117859, 0.11508955806493759, 0.1128527820110321, 0.11055197566747665, 0.1085684522986412, 0.10724902898073196, 0.10485339909791946, 0.1062261238694191, 0.10225032269954681, 0.10269308090209961, 0.10083292424678802, 0.09952358901500702, 0.0965469554066658, 0.09691308438777924, 0.09422522038221359, 0.09162584692239761, 0.09068227559328079, 0.09055323153734207, 0.08841349184513092, 0.08716250211000443, 0.08765143156051636, 0.08584185689687729, 0.08667715638875961, 0.08462270349264145, 0.08340923488140106, 0.08347082883119583, 0.08181468397378922, 0.07986971735954285, 0.08140586316585541, 0.08394887298345566, 0.08038534969091415, 0.07956155389547348, 0.07667078822851181, 0.07895071804523468, 0.0761536955833435, 0.07632661610841751, 0.07362031936645508, 0.07538438588380814, 0.07176699489355087, 0.07633687555789948, 0.07208169251680374, 0.07504867762327194, 0.07333864271640778, 0.07418021559715271, 0.07113232463598251, 0.06841334700584412, 0.07145946472883224, 0.07046584784984589, 0.06970488280057907, 0.0731702521443367, 0.06681957095861435, 0.06707356870174408, 0.06915022432804108, 0.06794700026512146, 0.06762117892503738, 0.06577680259943008, 0.06610296666622162, 0.06805682182312012, 0.0663604810833931, 0.06614239513874054, 0.06534460932016373, 0.06554492563009262, 0.06418203562498093, 0.0649087056517601, 0.06077133119106293, 0.0647655576467514, 0.06319160014390945, 0.06448673456907272, 0.06323309987783432, 0.06145164370536804, 0.05955022946000099, 0.0631757378578186, 0.05877011641860008, 0.06065312772989273, 0.06556624919176102, 0.05789044871926308, 0.06204025819897652, 0.05701722577214241, 0.05992816016077995, 0.06130780652165413, 0.058821894228458405, 0.057923104614019394, 0.06158529594540596, 0.061035264283418655, 0.05849354714155197, 0.058958616107702255, 0.0590239055454731, 0.05809416249394417, 0.05567138269543648, 0.05702709034085274, 0.05848817527294159, 0.05800794064998627, 0.059414803981781006, 0.05803653970360756, 0.05621859431266785, 0.05468494072556496, 0.05839475989341736, 0.059682004153728485, 0.053887300193309784, 0.05814735218882561, 0.05671718716621399, 0.05407065898180008, 0.055496759712696075, 0.05238524451851845, 0.058757565915584564, 0.0548870675265789, 0.05279794707894325, 0.058409009128808975, 0.05776749923825264, 0.055815212428569794, 0.05676151067018509, 0.056959815323352814, 0.05082292482256889, 0.056744031608104706, 0.05395623296499252, 0.05635710433125496, 0.05139052867889404, 0.050881799310445786, 0.05663217231631279, 0.05396925285458565, 0.05631384253501892, 0.05297451466321945, 0.052681293338537216, 0.05164364352822304, 0.05629272386431694, 0.05462018400430679, 0.05395961552858353, 0.05195097252726555, 0.05285073071718216, 0.04945472255349159, 0.05275478959083557, 0.05237306281924248, 0.05339941009879112, 0.05241105705499649, 0.05221525579690933, 0.05391264706850052, 0.05296800285577774, 0.0516679547727108, 0.04769916087388992, 0.0538380891084671, 0.04850971698760986, 0.049905724823474884, 0.05432259663939476, 0.04909844323992729, 0.05309014394879341, 0.05343730375170708, 0.04988368600606918, 0.04985431954264641, 0.05014537647366524, 0.047720663249492645, 0.051096923649311066, 0.049226682633161545, 0.054463885724544525, 0.04850996285676956, 0.051682114601135254, 0.05097445845603943, 0.0487983301281929, 0.05044468119740486, 0.05186339467763901, 0.048408061265945435, 0.05112189054489136, 0.04902280122041702]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:31 2016", "state": "available"}], "summary": "9d5f3363b01c6add4be67f31888e5abb"}