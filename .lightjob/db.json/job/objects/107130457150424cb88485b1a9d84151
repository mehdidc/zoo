{"content": {"hp_model": {"f0": 32, "f1": 32, "f2": 16, "f3": 16, "nonlin": "leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.014256322006656592, 0.013568185323255886, 0.012042088481059397, 0.014597632065805666, 0.016347626616114993, 0.011718250887353147, 0.01126091664402852, 0.010897917405313623, 0.009384305512458786, 0.014769690272045364, 0.01586934726143144, 0.011889754441704271, 0.014674631702160972, 0.01057006480062873, 0.015865384920633985, 0.012209261500696785, 0.017711885104055053, 0.01267355659689083, 0.011488823753064136, 0.012552808994225976, 0.010716399903021713, 0.01502574134621215, 0.01231356264772032, 0.009796928660554777, 0.016068830734281576, 0.009658733820271385, 0.008104383960458827, 0.011753666042055202, 0.01954190670468401, 0.017040916666595164, 0.01641766732090475, 0.014631844256018215, 0.009762365711234467, 0.016228021121735946, 0.008840900774917292, 0.012298298175585738, 0.011168237577867795, 0.0110241219259204, 0.01063578909798539, 0.010000352524501785, 0.010848334641211975, 0.015817423624516658, 0.01174504407190492, 0.007162654289575079, 0.008649901380449958, 0.010072978848151901, 0.013353039227738296, 0.01350124819832506, 0.01177991201595505, 0.0075482042171481824, 0.007215981517374109, 0.009320696391505371, 0.013176188816016465, 0.0055306698899483565, 0.008406761951353897, 0.011463528144372482, 0.012369981762299746, 0.010104866039097712, 0.010478285759159933, 0.011445486262020714, 0.00898223714576062, 0.012373588634751883, 0.010349271804586852, 0.006828295923852766, 0.0061680471734919876, 0.011539835978276118, 0.012144486339119239, 0.014003084308243948, 0.009640030647283375, 0.009533318596536887, 0.013624975504050733, 0.01076587463664409, 0.009073318741074882, 0.006515653064723722, 0.006374734016158524, 0.011206858375648283, 0.013068335757352112, 0.009331695618129009, 0.008147158646666281, 0.011041378775006896, 0.008160250878985273, 0.008330644337492948, 0.012183911416447925, 0.010228197056620866, 0.009849679430628524, 0.008188494634779693, 0.012140919636850116, 0.00999449004795667, 0.009891315354403022, 0.00765057022294958, 0.00821047234213456, 0.008685118475682154, 0.009618438993266674, 0.009350964745228231, 0.006194502168332626, 0.009410561479116298, 0.00763854125628623, 0.0048394899228255935, 0.005451298696124308, 0.00826244222229705, 0.007927903716183432, 0.008690980526225075, 0.008869891866409156, 0.009991813295247974, 0.007741072343777202, 0.006984160973298132, 0.009298982006948463, 0.0061802916093551315, 0.011793085318649242, 0.007622038308409115, 0.0072319873960718165, 0.007153309511398836, 0.00724226903660697, 0.009042557581676786, 0.010555070263951902, 0.009192387394663104, 0.0086156978945194, 0.008909227994835784, 0.004269688439624322, 0.012235324214271424, 0.009096135002129298, 0.00868544920902989, 0.008515199474349196, 0.010368372172505082, 0.011097175093729018, 0.00904136996392555, 0.011120552301075259, 0.009187415416245895, 0.010720985918820996, 0.013273933425704761, 0.008567357202311996, 0.00721603772148689, 0.013468581563470748, 0.010502606003034346, 0.009975511106219621, 0.011241410664467415, 0.011106724782986388, 0.01245735552162898, 0.008990168423021546, 0.01050677478496759, 0.009779756344013365, 0.007259250378681048, 0.007645970538267067, 0.012364708818397576, 0.012021793821129802, 0.007890286635261715, 0.008193866722500777, 0.008486084014957481, 0.0072272805330889645, 0.007612988069792139, 0.008397361228145396, 0.008386790486079636, 0.006053024230085505, 0.006031078111835412, 0.012204922790105625, 0.011841912574883411, 0.009807936337901427, 0.008114773478359332, 0.0073100506313504955, 0.008280177884665965, 0.012853398345328811, 0.006614367620187542, 0.007367919428405399, 0.005151394321088266, 0.009168584527896328, 0.010460979927427602, 0.012402428175924568, 0.009986692642408803, 0.015012285278273444, 0.0059590700782785454, 0.012157025831975353, 0.008794860802545688, 0.008882307378523063, 0.010403497375731477, 0.00866879565530974, 0.007626824295852892], "moving_avg_accuracy_train": [0.051342818498523436, 0.10557647108019563, 0.1594967145106589, 0.21115833297284514, 0.2597600139220538, 0.30587242153686206, 0.34931701625794515, 0.3908631359569015, 0.4288103182026196, 0.4648480813712835, 0.49773518385038806, 0.5279125020863349, 0.5564179531223951, 0.5823241193215528, 0.6060671718423783, 0.627675445487321, 0.6458491428059754, 0.6636674482616735, 0.6808451386514529, 0.6959728521130426, 0.709267067888034, 0.7206574421831176, 0.7327314794224471, 0.7444907177389695, 0.754090638472686, 0.7638511446639464, 0.7715661399301561, 0.776875416544837, 0.7819234467111359, 0.7858877839048709, 0.7890975784137471, 0.7927492946229907, 0.799094905921812, 0.8022510501349095, 0.8071743004301118, 0.8107617374102181, 0.8163174360160032, 0.8199132830267136, 0.8237471806780382, 0.8290595920283775, 0.8324085787234726, 0.8361414621705698, 0.8392870354360249, 0.8423712402534412, 0.8437313694523034, 0.8436094769122965, 0.8470590717702087, 0.8501589486982543, 0.8540667298274137, 0.8571585550508831, 0.8589670685472713, 0.8602716431559532, 0.8599112702359374, 0.8607100535805607, 0.8617311197895036, 0.8613435620395121, 0.8628612417286488, 0.8649203722334786, 0.8635189941844109, 0.8667700026078856, 0.8675126315056888, 0.866535477084125, 0.8658186903749281, 0.8673141792185722, 0.8698086345433005, 0.8705425859999155, 0.8712684267216294, 0.8737068208757861, 0.8728961478148317, 0.8744767306097144, 0.8740814934395237, 0.8753113872791722, 0.8781523480632982, 0.8790586455047997, 0.8807136919223891, 0.8809361717923041, 0.8799880325050542, 0.8804107091594953, 0.8777761571677336, 0.8795779454630496, 0.8803834997943286, 0.883605456172177, 0.8839852602861793, 0.8862935831005385, 0.8887918895191669, 0.8904128274590921, 0.8907510580252908, 0.8920225471955375, 0.8930667979034939, 0.8934883316490434, 0.8952926677520885, 0.8963470890305896, 0.8962502919492361, 0.8964138580616215, 0.8980697286519619, 0.8996787029190101, 0.9015359699010108, 0.9000526437384753, 0.9004870523874221, 0.9002246173071793, 0.8995024335849515, 0.9004636882134903, 0.9019472709136899, 0.902691799399794, 0.9020088907718763, 0.9006899331615583, 0.9008162559502844, 0.9009416082530042, 0.8996804786719359, 0.9026419777143492, 0.9037960882727869, 0.9023890917159493, 0.9022897311266264, 0.9025699692081313, 0.9015573746267422, 0.9023457953808915, 0.9044687400964477, 0.9049843010547339, 0.9019659179280811, 0.9034061984532409, 0.9036421470199231, 0.9048799274037558, 0.9056846849575385, 0.9058346549999906, 0.9075646080238933, 0.9100189650394257, 0.9095542175641838, 0.9083083721043693, 0.9080519223524042, 0.9060565098733007, 0.9069551616729769, 0.9073175197212568, 0.9079622594492511, 0.9081913556365704, 0.9091904179492053, 0.9089550095579854, 0.910296089068918, 0.9104500205597744, 0.9116487546610506, 0.9106537628581975, 0.9111298917403364, 0.9123862328080894, 0.9131752510405233, 0.9134063146973145, 0.9151602796538473, 0.9156227045885179, 0.9151345483380917, 0.9123542514961615, 0.9108239325896239, 0.9120154303248014, 0.9121718859484214, 0.9118314623037453, 0.9115248286818056, 0.9099983969971799, 0.9101425701654483, 0.9091844087157639, 0.9092915783669818, 0.9088764262173452, 0.9093164111219119, 0.9086874034480281, 0.910187993345012, 0.9125916724677366, 0.9140574390353318, 0.9158509232544915, 0.9165768522064971, 0.9174789791859213, 0.917953891085297, 0.9171537413697629, 0.9190655308828881, 0.9176845010792524, 0.9171298904012367, 0.917893296594594, 0.9179291763066738, 0.9190914543201556, 0.9196538014822707, 0.9195228231543647], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 825053249, "moving_var_accuracy_train": [0.023724765102350887, 0.047823890243261474, 0.06920803508333909, 0.0863075369741977, 0.09893589379657597, 0.10817949164122644, 0.11434843777141623, 0.11844831455263467, 0.1195633808608789, 0.11929552614259736, 0.11710002711357777, 0.11358605922544245, 0.10954049995182119, 0.10462661498088449, 0.09923754636985616, 0.09351604914210307, 0.08713699369596335, 0.08128072241018008, 0.07580830759330617, 0.07028710626515922, 0.06484902119629157, 0.059531784715901336, 0.05489064762162163, 0.050646100031522374, 0.04641091633121291, 0.042627232028078305, 0.03890019919288921, 0.035263875037141075, 0.03196683101046574, 0.028911591633889847, 0.026113157497603765, 0.023621857029299063, 0.02162207237117071, 0.01954951635069846, 0.0178127102568515, 0.016147266567942454, 0.014810331994133116, 0.013445669836239724, 0.01223339179342324, 0.01126404804327785, 0.01023858464590538, 0.009340135950781352, 0.008495174036218326, 0.007731267506798462, 0.006974790319056994, 0.006277445007273079, 0.005756797848699374, 0.0052676011965507, 0.004878277857076362, 0.004476484520281061, 0.00405827255785252, 0.0036677625362538254, 0.003302155100401769, 0.002977682083846421, 0.0026892970612891827, 0.00242171916424647, 0.002200277412571184, 0.0020184098372373525, 0.0018342435974412958, 0.0017459407396227, 0.001576310144779101, 0.0014272726071734244, 0.001289169395134414, 0.0011803808375541495, 0.0011183435201023197, 0.0010113573307580926, 0.0009149632004619664, 0.0008769787748750013, 0.0007951956146933156, 0.0007381602309672953, 0.000665750119656869, 0.0006127888574024321, 0.0006241494952546635, 0.0005691269212014466, 0.0005368668368806826, 0.000483625628825271, 0.0004433537789149859, 0.0004006263010113727, 0.00042303144868589273, 0.0004099462733675423, 0.0003747919060565696, 0.0004307417415577359, 0.00038896582788708003, 0.00039802443303599153, 0.00041439580438462723, 0.0003966031821919689, 0.00035797246321597114, 0.0003367253792848694, 0.0003128669772259895, 0.00028317949579112326, 0.000284162205166778, 0.0002657522227431014, 0.0002392613275434183, 0.00021557597964716422, 0.00021869554839003182, 0.00022012517727923765, 0.00022915762533318705, 0.00022604417134003058, 0.00020513815207454607, 0.0001852441864091697, 0.0001714137117261087, 0.0001625884347014834, 0.00016613874988631626, 0.00015451377889727082, 0.00014325967875430433, 0.00014459055347921406, 0.00013027511515385638, 0.00011738902243664515, 0.00011996415057518832, 0.00018690202472160212, 0.00018019956287931887, 0.0001799963603899616, 0.00016208557709136075, 0.00014658381982315485, 0.00014115356791716426, 0.00013263267669560847, 0.00015993145741381587, 0.00014633053958781526, 0.00021369321592239478, 0.0002109935662505457, 0.00019039525516056555, 0.0001851446321519187, 0.00017245888142006078, 0.00015541541240075255, 0.0001668085083448696, 0.0002043424727476218, 0.0001858521374145535, 0.00018123610186076078, 0.00016370438995223167, 0.00018316898961286493, 0.00017212026616513006, 0.00015608996974499672, 0.00014422217662218433, 0.00013027232452736387, 0.0001262282216153749, 0.00011410415344974811, 0.00011888018639656583, 0.00010720542189180483, 0.00010941755071268555, 0.00010738587383111763, 9.868757485966737e-05, 0.00010302435328040507, 9.832486589238346e-05, 8.897289302455242e-05, 0.00010776314134080517, 9.891135858857102e-05, 9.116489145318519e-05, 0.00015161885707109203, 0.0001575338549653382, 0.00015455747114520254, 0.0001393220292901429, 0.00012643282068181919, 0.00011463575621657194, 0.0001241421237853791, 0.00011191498452887798, 0.0001089861463489412, 9.819089972132677e-05, 8.992297151532458e-05, 8.26729548100112e-05, 7.796651521325114e-05, 9.043579404229506e-05, 0.00013339127456326943, 0.000139388391783058, 0.0001543988234041262, 0.00014370169665395364, 0.00013665602477160228, 0.0001250202941039602, 0.00011828042079898845, 0.00013934683100154963, 0.00014257733776816783, 0.00013108794102887215, 0.00012322424807049179, 0.00011091340944709305, 0.0001119800801279913, 0.00010362818109184239, 9.341976088408745e-05], "duration": 79137.242673, "accuracy_train": [0.5134281849852345, 0.5936793443152455, 0.6447789053848284, 0.6761128991325213, 0.6971751424649317, 0.7208840900701367, 0.7403183687476929, 0.7647782132475084, 0.7703349584140827, 0.789187949889258, 0.7937191061623293, 0.799508366209856, 0.8129670124469361, 0.8154796151139718, 0.8197546445298081, 0.8221499082918051, 0.8094124186738648, 0.8240321973629567, 0.8354443521594684, 0.8321222732673496, 0.8289150098629567, 0.8231708108388703, 0.841397814576412, 0.8503238625876707, 0.8404899250761352, 0.8516957003852897, 0.8410010973260429, 0.8246589060769656, 0.8273557182078257, 0.8215668186484865, 0.8179857289936323, 0.8256147405061831, 0.8562054076112033, 0.8306563480527871, 0.8514835530869325, 0.8430486702311739, 0.8663187234680694, 0.8522759061231081, 0.8582522595399593, 0.8768712941814323, 0.8625494589793282, 0.8697374131944444, 0.8675971948251201, 0.8701290836101883, 0.8559725322420635, 0.8425124440522334, 0.8781054254914176, 0.8780578410506644, 0.8892367599898486, 0.8849849820621077, 0.8752436900147655, 0.8720128146340901, 0.8566679139557956, 0.8678991036821706, 0.870920715669989, 0.8578555422895902, 0.8765203589308784, 0.8834525467769472, 0.8509065917428018, 0.8960290784191584, 0.8741962915859173, 0.8577410872900517, 0.8593676099921558, 0.8807735788113695, 0.8922587324658545, 0.8771481491094499, 0.8778009932170543, 0.8956523682631967, 0.8656000902662422, 0.8887019757636582, 0.8705243589078073, 0.8863804318360096, 0.9037209951204319, 0.8872153224783131, 0.8956091096806941, 0.8829384906215393, 0.8714547789198044, 0.8842147990494648, 0.8540651892418788, 0.8957940401208934, 0.8876334887758398, 0.9126030635728128, 0.8874034973122, 0.9070684884297711, 0.9112766472868217, 0.9050012689184201, 0.893795133121078, 0.9034659497277593, 0.9024650542751015, 0.8972821353589886, 0.9115316926794942, 0.9058368805370985, 0.8953791182170543, 0.8978859530730897, 0.9129725639650241, 0.9141594713224437, 0.918251372739018, 0.8867027082756552, 0.9043967302279439, 0.8978627015849945, 0.8930027800849022, 0.9091149798703396, 0.9152995152154854, 0.9093925557747323, 0.8958627131206165, 0.888819314668697, 0.9019531610488187, 0.9020697789774824, 0.888330312442322, 0.9292954690960686, 0.9141830832987264, 0.8897261227044113, 0.9013954858227206, 0.9050921119416758, 0.8924440233942414, 0.9094415821682356, 0.9235752425364526, 0.9096243496793098, 0.8748004697882059, 0.9163687231796788, 0.9057656841200628, 0.9160199508582503, 0.9129275029415835, 0.9071843853820598, 0.923134185239018, 0.9321081781792175, 0.9053714902870063, 0.8970957629660392, 0.9057438745847176, 0.8880977975613695, 0.9150430278700628, 0.9105787421557769, 0.9137649170011997, 0.9102532213224437, 0.9181819787629198, 0.9068363340370063, 0.9223658046673128, 0.9118354039774824, 0.922437361572536, 0.9016988366325213, 0.9154150516795865, 0.9236933024178663, 0.920276415132429, 0.9154858876084349, 0.9309459642626431, 0.9197845290005537, 0.9107411420842562, 0.8873315799187893, 0.8970510624307864, 0.922738909941399, 0.9135799865610004, 0.9087676495016611, 0.9087651260843485, 0.8962605118355482, 0.9114401286798633, 0.9005609556686047, 0.9102561052279439, 0.9051400568706165, 0.9132762752630121, 0.903026334383075, 0.9236933024178663, 0.9342247845722591, 0.9272493381436876, 0.9319922812269288, 0.9231102127745479, 0.9255981220007383, 0.9222280981796788, 0.9099523939299556, 0.9362716365010151, 0.9052552328465301, 0.9121383942990956, 0.9247639523348099, 0.9182520937153931, 0.9295519564414912, 0.9247149259413067, 0.9183440182032114], "end": "2016-01-24 08:40:51.976000", "learning_rate_per_epoch": [0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763, 0.000252692261710763], "accuracy_valid": [0.5100406508847892, 0.5801531320594879, 0.6287694724209337, 0.6507936041039157, 0.6722073842243976, 0.6901120105421686, 0.7085240375564759, 0.7298569277108433, 0.7346382600715362, 0.7482086549322289, 0.7514442535768072, 0.7554519837161144, 0.7711696394954819, 0.7698268660579819, 0.7761333419615963, 0.7759303816829819, 0.7626335419804217, 0.7760627470820783, 0.7854415709713856, 0.7813014754329819, 0.7809764448418675, 0.7762348221009037, 0.7844650084713856, 0.7998267483998494, 0.783203125, 0.7959204983998494, 0.7883006635918675, 0.7684326171875, 0.7721653214420181, 0.7655529343938253, 0.7608127823795181, 0.7688591279179217, 0.7921966185052711, 0.7649425828313253, 0.7888904249811747, 0.7794395354856928, 0.8018210537462349, 0.7914641966302711, 0.7915759718561747, 0.8062258800828314, 0.7906302946159638, 0.7977603774472892, 0.7972015013177711, 0.8005591702748494, 0.7876697218561747, 0.7745464278990963, 0.8019725385918675, 0.7979442182793675, 0.809093797063253, 0.8048728115587349, 0.8009459713855422, 0.7954322171498494, 0.7766730986445783, 0.7955542874623494, 0.7913524214043675, 0.7792056899472892, 0.7974868222891567, 0.8000708890248494, 0.7752788497740963, 0.8109351468373494, 0.7909862104668675, 0.7831119399472892, 0.7809146743222892, 0.797985398625753, 0.8069274166980422, 0.7946380247552711, 0.7942718138177711, 0.8107924863516567, 0.7842620481927711, 0.8001620740775602, 0.7864078383847892, 0.7990428510918675, 0.8121661450489458, 0.8049639966114458, 0.8123293957078314, 0.7947806852409638, 0.7864490187311747, 0.7944041792168675, 0.7814338408320783, 0.8105189311935241, 0.8012915921498494, 0.8222376811935241, 0.7981177640248494, 0.8146384365587349, 0.8207125376506024, 0.8109763271837349, 0.8035506282944277, 0.8119322995105422, 0.8119220044239458, 0.8127059017319277, 0.8175990093185241, 0.8097556240587349, 0.8107939570783133, 0.8099997646837349, 0.8208346079631024, 0.8200095303087349, 0.8286471079631024, 0.7958190182605422, 0.8140383800828314, 0.8110778073230422, 0.7992163968373494, 0.8170695477221386, 0.8239069559487951, 0.8172019131212349, 0.8052390224962349, 0.8030623470444277, 0.8101012448230422, 0.8155444041792168, 0.7948012754141567, 0.8321665568524097, 0.8179549251694277, 0.8072833325489458, 0.8104880459337349, 0.8160532756024097, 0.8014548428087349, 0.8145163662462349, 0.8292059840926205, 0.8148619870105422, 0.7840590879141567, 0.8154929287462349, 0.8091246823230422, 0.8219626553087349, 0.8199801157756024, 0.8161650508283133, 0.8296030802899097, 0.8386068688817772, 0.8170901378953314, 0.8091246823230422, 0.8160429805158133, 0.7981486492846386, 0.8180872905685241, 0.8161341655685241, 0.8215876200112951, 0.8147502117846386, 0.8195933146649097, 0.816673922251506, 0.8276808405496988, 0.8163268307605422, 0.8321165521460843, 0.8067450465926205, 0.8202448465737951, 0.827782320689006, 0.8236010448042168, 0.8232760142131024, 0.8357992516942772, 0.8199801157756024, 0.8140692653426205, 0.8007533061935241, 0.8061949948230422, 0.8238657756024097, 0.8183726115399097, 0.820213961314006, 0.8150561229292168, 0.8067038662462349, 0.8167033367846386, 0.8077407285391567, 0.8219832454819277, 0.8144560664533133, 0.8143237010542168, 0.8138251247176205, 0.8330122246799698, 0.8317297510353916, 0.8295324854103916, 0.8340490869728916, 0.823998141001506, 0.8278440912085843, 0.8259100856551205, 0.8122485057417168, 0.8360639824924698, 0.8135500988328314, 0.8211802287274097, 0.8269484186746988, 0.8178637401167168, 0.8338358316076807, 0.8248114528426205, 0.8232451289533133], "accuracy_test": 0.2360223642172524, "start": "2016-01-23 10:41:54.733000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0], "accuracy_train_last": 0.9183440182032114, "batch_size_eval": 1024, "accuracy_train_std": [0.02026134723259013, 0.014845458141645394, 0.017230421169966426, 0.017343635948605555, 0.016497516623891966, 0.01935574195544247, 0.017599637484037284, 0.01603154035868173, 0.016591633194729918, 0.016600002542119435, 0.016911784062419128, 0.016328303878382872, 0.017563548564693977, 0.01774415995079575, 0.01693947408529571, 0.017720149344974713, 0.016662104334985276, 0.01690241188119599, 0.018756905722386422, 0.01746685193625146, 0.01677716880646078, 0.0169267229334349, 0.01689313755326703, 0.017078540220344902, 0.018653112539440356, 0.017425863746272224, 0.01864249029024577, 0.017896855750753615, 0.017601930361785843, 0.018640078896658387, 0.017790310786522705, 0.01699612900508664, 0.018656851544544096, 0.016304630932444924, 0.01791183238932061, 0.017411415028633526, 0.016601435910262186, 0.017891404393598893, 0.017341540753121432, 0.018837338119832014, 0.01905137351536594, 0.017002767147496215, 0.017647387658042316, 0.017915149723902138, 0.01818943007475004, 0.01794640371874529, 0.017242366644649478, 0.01778035447426172, 0.01976860574473174, 0.017090108683857123, 0.01783655003403426, 0.01826953236080359, 0.018348499699270965, 0.019824461614471517, 0.018608436637899014, 0.01637633187992335, 0.019975996160875115, 0.018829261776352815, 0.016852524330927134, 0.01694847513821618, 0.019299247799534697, 0.018809213768977824, 0.016816185651269584, 0.020502621602696874, 0.018768615487057657, 0.019149324487688118, 0.016399239871438823, 0.019089963074739244, 0.016257265539112744, 0.01816375126775918, 0.01878849410058342, 0.01693087226631628, 0.018051051609492837, 0.018246300371338068, 0.01837587807496881, 0.018918093479562584, 0.018803287928744656, 0.019067162377856852, 0.01676932414242427, 0.01930794410378481, 0.01844014597139508, 0.016038227654570334, 0.01706338810741412, 0.017583132480092644, 0.017048308451204093, 0.018410694106021544, 0.017473122386290545, 0.0179686735046511, 0.0180784395298458, 0.01717631864048353, 0.017922257964232503, 0.018113310553672934, 0.015189660455660708, 0.017370630045497243, 0.017757127042032143, 0.018081247538514526, 0.017390041987980904, 0.01774795540269264, 0.015061901433505415, 0.01733519982033791, 0.018172729505316787, 0.01598068112595828, 0.016214524468977965, 0.017689934652355677, 0.018324416887784644, 0.01703955379079394, 0.018417209374879253, 0.01778232530817531, 0.018483480656444985, 0.015836695439569035, 0.017040418143076975, 0.017135899572891102, 0.017721378704485796, 0.01795591462665746, 0.01908939807476735, 0.01571711649338315, 0.016525876581524164, 0.01821966118228158, 0.01716202032255621, 0.015841922717623507, 0.017875260264501504, 0.014610491181587216, 0.01585159585918014, 0.01868661422224965, 0.016270553999556136, 0.01502230859947799, 0.01739084232533696, 0.01656437781779075, 0.016376737172311057, 0.01773019943180493, 0.016388330294673868, 0.018461474269091133, 0.015127439959781408, 0.018146034227886805, 0.016445021728173324, 0.015848237019605024, 0.017112567635733315, 0.015915597905270223, 0.015551398514953484, 0.01670147797908236, 0.01676757809763838, 0.01471200966773444, 0.014118267222358698, 0.015589986734491565, 0.014785874803327052, 0.016639952234014464, 0.017273397456448916, 0.01785465124608464, 0.01584231321282495, 0.01541045603674618, 0.014865846237187095, 0.015192057313649832, 0.0186746429259954, 0.014844831600074574, 0.015612087655803353, 0.01523719029609862, 0.01513143869080392, 0.017344375839307292, 0.01627601270782238, 0.016507689008367403, 0.015781311969290394, 0.015384410131655967, 0.014721332799184394, 0.01445413407135819, 0.015543185435664492, 0.014838962923587957, 0.015778563307357447, 0.015050639324461282, 0.013574288536433882, 0.01734600614152133, 0.015048933323242433, 0.014328977183140874, 0.01587071412675506, 0.014396924849637737, 0.01570337150171234, 0.0160495561207843], "accuracy_test_std": 0.0762480234583916, "error_valid": [0.4899593491152108, 0.41984686794051207, 0.37123052757906627, 0.34920639589608427, 0.32779261577560237, 0.30988798945783136, 0.29147596244352414, 0.2701430722891567, 0.2653617399284638, 0.2517913450677711, 0.24855574642319278, 0.24454801628388556, 0.2288303605045181, 0.2301731339420181, 0.22386665803840367, 0.2240696183170181, 0.23736645801957834, 0.22393725291792166, 0.21455842902861444, 0.2186985245670181, 0.21902355515813254, 0.22376517789909633, 0.21553499152861444, 0.20017325160015065, 0.216796875, 0.20407950160015065, 0.21169933640813254, 0.2315673828125, 0.2278346785579819, 0.23444706560617468, 0.2391872176204819, 0.23114087208207834, 0.20780338149472888, 0.23505741716867468, 0.21110957501882532, 0.22056046451430722, 0.1981789462537651, 0.20853580336972888, 0.20842402814382532, 0.19377411991716864, 0.2093697053840362, 0.20223962255271077, 0.20279849868222888, 0.19944082972515065, 0.21233027814382532, 0.22545357210090367, 0.19802746140813254, 0.20205578172063254, 0.19090620293674698, 0.1951271884412651, 0.19905402861445776, 0.20456778285015065, 0.22332690135542166, 0.20444571253765065, 0.20864757859563254, 0.22079431005271077, 0.20251317771084332, 0.19992911097515065, 0.22472115022590367, 0.18906485316265065, 0.20901378953313254, 0.21688806005271077, 0.21908532567771077, 0.20201460137424698, 0.19307258330195776, 0.20536197524472888, 0.20572818618222888, 0.18920751364834332, 0.21573795180722888, 0.19983792592243976, 0.21359216161521077, 0.20095714890813254, 0.1878338549510542, 0.1950360033885542, 0.18767060429216864, 0.2052193147590362, 0.21355098126882532, 0.20559582078313254, 0.21856615916792166, 0.18948106880647586, 0.19870840785015065, 0.17776231880647586, 0.20188223597515065, 0.1853615634412651, 0.17928746234939763, 0.1890236728162651, 0.1964493717055723, 0.18806770048945776, 0.1880779955760542, 0.1872940982680723, 0.18240099068147586, 0.1902443759412651, 0.18920604292168675, 0.1900002353162651, 0.17916539203689763, 0.1799904696912651, 0.17135289203689763, 0.20418098173945776, 0.18596161991716864, 0.18892219267695776, 0.20078360316265065, 0.18293045227786142, 0.17609304405120485, 0.1827980868787651, 0.1947609775037651, 0.1969376529555723, 0.18989875517695776, 0.1844555958207832, 0.20519872458584332, 0.1678334431475903, 0.1820450748305723, 0.1927166674510542, 0.1895119540662651, 0.1839467243975903, 0.1985451571912651, 0.1854836337537651, 0.17079401590737953, 0.18513801298945776, 0.21594091208584332, 0.1845070712537651, 0.19087531767695776, 0.1780373446912651, 0.18001988422439763, 0.18383494917168675, 0.1703969197100903, 0.16139313111822284, 0.18290986210466864, 0.19087531767695776, 0.18395701948418675, 0.20185135071536142, 0.18191270943147586, 0.18386583443147586, 0.17841237998870485, 0.18524978821536142, 0.1804066853350903, 0.18332607774849397, 0.17231915945030118, 0.18367316923945776, 0.16788344785391573, 0.19325495340737953, 0.17975515342620485, 0.17221767931099397, 0.1763989551957832, 0.17672398578689763, 0.16420074830572284, 0.18001988422439763, 0.18593073465737953, 0.19924669380647586, 0.19380500517695776, 0.1761342243975903, 0.1816273884600903, 0.17978603868599397, 0.1849438770707832, 0.1932961337537651, 0.18329666321536142, 0.19225927146084332, 0.1780167545180723, 0.18554393354668675, 0.1856762989457832, 0.18617487528237953, 0.16698777532003017, 0.1682702489646084, 0.1704675145896084, 0.1659509130271084, 0.17600185899849397, 0.17215590879141573, 0.17408991434487953, 0.1877514942582832, 0.16393601750753017, 0.18644990116716864, 0.1788197712725903, 0.17305158132530118, 0.1821362598832832, 0.1661641683923193, 0.17518854715737953, 0.17675487104668675], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-3, 3], "momentum": 0.7453192751846027, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.00025269226162053323, "patience_threshold": 1, "do_flip": true, "batch_size": 32, "optimization": "adam", "nb_data_augmentation": 1, "learning_rate_decay_method": "none", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 6.579575528331829e-05, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.09545973082901431}, "accuracy_valid_max": 0.8386068688817772, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import os\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-3, 3)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8232451289533133, "loss_train": [2.374901056289673, 2.00679874420166, 1.8328193426132202, 1.713120937347412, 1.6244451999664307, 1.5499656200408936, 1.488446593284607, 1.4310897588729858, 1.3869426250457764, 1.3465553522109985, 1.3105007410049438, 1.2758523225784302, 1.243844747543335, 1.2179330587387085, 1.190075159072876, 1.1679805517196655, 1.146350622177124, 1.1250463724136353, 1.1059623956680298, 1.086954951286316, 1.0718212127685547, 1.0546640157699585, 1.0363792181015015, 1.0247316360473633, 1.0098700523376465, 0.9974353313446045, 0.9850286245346069, 0.9743396043777466, 0.96210777759552, 0.9499212503433228, 0.93839031457901, 0.9276978969573975, 0.9215476512908936, 0.910508394241333, 0.902346670627594, 0.8943005204200745, 0.8828305006027222, 0.877582311630249, 0.8704844117164612, 0.8624452352523804, 0.8546597957611084, 0.8448805212974548, 0.8426430225372314, 0.8360021710395813, 0.828490138053894, 0.8235715627670288, 0.8167277574539185, 0.8131619095802307, 0.8073375225067139, 0.8015773892402649, 0.7973067760467529, 0.7936490774154663, 0.7857487201690674, 0.7825817465782166, 0.7801613807678223, 0.772223949432373, 0.7682313323020935, 0.765311598777771, 0.7597689032554626, 0.7559736371040344, 0.753978967666626, 0.7466195225715637, 0.749449610710144, 0.7410569190979004, 0.7410843372344971, 0.7369478344917297, 0.7321549654006958, 0.7307037711143494, 0.7283744215965271, 0.7235300540924072, 0.7197688817977905, 0.7176378965377808, 0.715061604976654, 0.7117899060249329, 0.7115116119384766, 0.7056787610054016, 0.7020106315612793, 0.7032535076141357, 0.6991002559661865, 0.6986359357833862, 0.6956170797348022, 0.6933614611625671, 0.6897677779197693, 0.6871135830879211, 0.6867190599441528, 0.6845173835754395, 0.6815690398216248, 0.67910236120224, 0.6767148971557617, 0.679435670375824, 0.6756471395492554, 0.6747639179229736, 0.6728600859642029, 0.6709287762641907, 0.6676427721977234, 0.6697672009468079, 0.663712739944458, 0.663665235042572, 0.6598677635192871, 0.6601805686950684, 0.6600391864776611, 0.6585301756858826, 0.6551597118377686, 0.6525834798812866, 0.6518600583076477, 0.6490254402160645, 0.6493353247642517, 0.649063766002655, 0.6460895538330078, 0.6460720300674438, 0.6445158123970032, 0.6404258608818054, 0.6396507620811462, 0.6395044326782227, 0.6391255259513855, 0.6376816034317017, 0.6386302709579468, 0.6315064430236816, 0.6351014971733093, 0.630502462387085, 0.6310189962387085, 0.6348770260810852, 0.6300128102302551, 0.6272904872894287, 0.6303943395614624, 0.6253322958946228, 0.625206708908081, 0.6256662607192993, 0.6281955242156982, 0.6235392689704895, 0.6217128038406372, 0.6185711622238159, 0.6193232536315918, 0.616475522518158, 0.6201616525650024, 0.6186518669128418, 0.6171275973320007, 0.611617922782898, 0.6146913766860962, 0.6148457527160645, 0.610811710357666, 0.6125685572624207, 0.6104899048805237, 0.6109796166419983, 0.6090689301490784, 0.6097049117088318, 0.6058957576751709, 0.6068809628486633, 0.6062533855438232, 0.6020520329475403, 0.6037094593048096, 0.603365957736969, 0.6019710302352905, 0.6017923951148987, 0.5995839238166809, 0.5999191999435425, 0.5996761322021484, 0.5993385314941406, 0.5977081060409546, 0.5947794914245605, 0.5977095365524292, 0.5960260629653931, 0.5967790484428406, 0.5916426777839661, 0.5935940146446228, 0.5928298234939575, 0.5935932993888855, 0.5928237438201904, 0.5914914608001709, 0.5926125049591064, 0.5895386338233948, 0.5891950726509094, 0.588862419128418, 0.5854750275611877, 0.5891902446746826, 0.5857942700386047], "accuracy_train_first": 0.5134281849852345, "model": "residualv3", "loss_std": [0.30063629150390625, 0.1797262281179428, 0.17785903811454773, 0.1775965690612793, 0.17639277875423431, 0.17768564820289612, 0.1773628145456314, 0.17307518422603607, 0.1740466058254242, 0.16863948106765747, 0.16927260160446167, 0.16884037852287292, 0.16402991116046906, 0.16332761943340302, 0.1624397486448288, 0.15968729555606842, 0.15418791770935059, 0.15625055134296417, 0.15438945591449738, 0.1518818885087967, 0.15591008961200714, 0.15222088992595673, 0.1520666778087616, 0.14958389103412628, 0.1527625471353531, 0.14890837669372559, 0.14525055885314941, 0.1474805325269699, 0.1441918909549713, 0.14216366410255432, 0.1455262452363968, 0.1401747465133667, 0.14121747016906738, 0.13969990611076355, 0.13579364120960236, 0.13712073862552643, 0.13575036823749542, 0.1368209719657898, 0.134887233376503, 0.13110241293907166, 0.13248838484287262, 0.1292552500963211, 0.13131175935268402, 0.12833470106124878, 0.12809649109840393, 0.12903617322444916, 0.12973253428936005, 0.12949657440185547, 0.12304229289293289, 0.1277245730161667, 0.12395504117012024, 0.12341605126857758, 0.12405336648225784, 0.12448111921548843, 0.12283457815647125, 0.11983337998390198, 0.1201748475432396, 0.12064213305711746, 0.12096580117940903, 0.11936546117067337, 0.12112831324338913, 0.11824647337198257, 0.11562415212392807, 0.11807913333177567, 0.11916430294513702, 0.11654491722583771, 0.11808312684297562, 0.11687816679477692, 0.11588624119758606, 0.11661426723003387, 0.11537601798772812, 0.11409851908683777, 0.11622391641139984, 0.1162685751914978, 0.11200955510139465, 0.10939545184373856, 0.112605020403862, 0.11151773482561111, 0.11323434114456177, 0.11197436600923538, 0.11167994886636734, 0.11203797161579132, 0.10969036817550659, 0.11132451891899109, 0.11086117476224899, 0.11325892060995102, 0.1104358658194542, 0.1098511591553688, 0.10935845226049423, 0.11202985048294067, 0.10909027606248856, 0.10905774682760239, 0.10921012610197067, 0.10471411049365997, 0.10639669001102448, 0.1102614551782608, 0.10417065024375916, 0.10875407606363297, 0.10909119248390198, 0.10615160316228867, 0.1060231626033783, 0.10421854257583618, 0.10332723706960678, 0.10443511605262756, 0.10414161533117294, 0.10441163927316666, 0.10218755155801773, 0.1060037910938263, 0.10308284312486649, 0.10459675639867783, 0.10463067889213562, 0.1058887243270874, 0.10275214910507202, 0.10206662118434906, 0.10440133512020111, 0.1031365841627121, 0.10653737187385559, 0.10286327451467514, 0.10136399418115616, 0.10231370478868484, 0.10317733138799667, 0.10299967229366302, 0.10047805309295654, 0.10013920813798904, 0.1016240268945694, 0.10415972024202347, 0.10061375051736832, 0.09886281937360764, 0.10170169174671173, 0.10196390002965927, 0.0991593673825264, 0.0974661186337471, 0.09826310724020004, 0.10019683837890625, 0.10108766704797745, 0.0975162610411644, 0.10057422518730164, 0.09647350013256073, 0.09637144953012466, 0.09956620633602142, 0.09981851279735565, 0.09834037721157074, 0.09813840687274933, 0.09703589975833893, 0.10069978982210159, 0.09904748201370239, 0.09898533672094345, 0.09725979715585709, 0.09875912964344025, 0.09385376423597336, 0.09751623868942261, 0.09584682434797287, 0.09643689543008804, 0.09710649400949478, 0.09476053714752197, 0.09617940336465836, 0.09653884172439575, 0.09774842858314514, 0.09696972370147705, 0.09623217582702637, 0.09404382109642029, 0.09628849476575851, 0.09756650030612946, 0.09545467048883438, 0.09547217190265656, 0.09541674703359604, 0.09513610601425171, 0.09623994678258896, 0.09634251147508621, 0.0926205962896347, 0.09246641397476196, 0.09167289733886719, 0.09269119799137115, 0.0958702489733696, 0.094717837870121, 0.09515213221311569]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:10 2016", "state": "available"}], "summary": "278276418b46f4c20d9de8012c0d7271"}