{"content": {"hp_model": {"f0": 32, "f1": 32, "f2": 32, "f3": 32, "nonlin": "leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.8177422285079956, 1.4591546058654785, 1.2996299266815186, 1.1912081241607666, 1.1259294748306274, 1.0816210508346558, 1.0438703298568726, 1.0151211023330688, 0.9921759366989136, 0.9727855324745178, 0.9550370573997498, 0.9400815367698669, 0.9259729981422424, 0.911341667175293, 0.9024304747581482, 0.8882887363433838, 0.8790100812911987, 0.8676469326019287, 0.8535215854644775, 0.846809446811676, 0.8369642496109009, 0.8278121948242188, 0.8191646337509155, 0.8106605410575867, 0.8036919832229614, 0.7935308814048767, 0.78694087266922, 0.7781257033348083, 0.7730038166046143, 0.7642422318458557, 0.7599554657936096, 0.7522946000099182, 0.7446103692054749, 0.7368645668029785, 0.7323684692382812, 0.7267929315567017, 0.7191653847694397, 0.7120112180709839, 0.7072782516479492, 0.7000815868377686, 0.6950802803039551, 0.6863146424293518, 0.68219393491745, 0.6760317087173462, 0.6715826988220215, 0.6637141108512878, 0.6565936803817749, 0.6521140336990356, 0.648705244064331, 0.6434909701347351, 0.637405276298523, 0.6322202682495117, 0.6279284954071045, 0.6199368238449097, 0.613642156124115, 0.6094450950622559, 0.6042959094047546, 0.5965816378593445, 0.5946542620658875, 0.5887161493301392, 0.5831214785575867, 0.5799272656440735, 0.5740151405334473, 0.569500207901001, 0.5661172866821289, 0.5637227892875671, 0.557287871837616, 0.5560576319694519, 0.5477533340454102, 0.5431787967681885, 0.5419641733169556, 0.5374129414558411, 0.5336881279945374, 0.5279327630996704, 0.5254341959953308, 0.5212130546569824, 0.5158217549324036, 0.5130106210708618, 0.5092800855636597, 0.5047744512557983, 0.4987584054470062, 0.4966253638267517, 0.4946162700653076, 0.49228113889694214, 0.48935961723327637, 0.4840472936630249, 0.4811251759529114, 0.4759894013404846, 0.47414711117744446, 0.4696669280529022, 0.46790918707847595, 0.46530869603157043, 0.4602827727794647, 0.4589100778102875, 0.4555674195289612, 0.4520273208618164, 0.4494406580924988, 0.44480353593826294, 0.4436696171760559, 0.4405968189239502, 0.4369928240776062, 0.4364043176174164, 0.4323907792568207, 0.4299493432044983, 0.4280257523059845, 0.42416295409202576, 0.42304515838623047, 0.4182654619216919, 0.41632336378097534, 0.41371747851371765, 0.41176286339759827, 0.41049107909202576, 0.40536361932754517, 0.4061371684074402, 0.4022516906261444, 0.39959993958473206, 0.39606159925460815, 0.39508679509162903, 0.39079412817955017, 0.3913036584854126, 0.38676947355270386, 0.38495147228240967, 0.3831290304660797, 0.37994518876075745, 0.37933453917503357, 0.37684252858161926, 0.37495341897010803, 0.3732738196849823, 0.3717763125896454, 0.3697790503501892, 0.3682072162628174, 0.3645704686641693, 0.3629845678806305, 0.3624497950077057, 0.35991212725639343, 0.358046293258667, 0.3577951490879059, 0.3554243743419647, 0.35348132252693176, 0.35353782773017883, 0.3498377501964569, 0.348252534866333, 0.34682729840278625, 0.34597843885421753, 0.3437773287296295, 0.3418760597705841, 0.34075307846069336, 0.3374015986919403, 0.33929622173309326, 0.33623260259628296, 0.33571577072143555, 0.33606240153312683, 0.33181360363960266, 0.3316316306591034, 0.32716718316078186, 0.32814085483551025, 0.3269956111907959, 0.3241269886493683, 0.3241710960865021, 0.3238063156604767, 0.321342408657074, 0.32132086157798767, 0.3190334141254425, 0.31816238164901733, 0.31714996695518494, 0.31546297669410706, 0.3148655295372009, 0.3142396807670593, 0.31214407086372375, 0.3119654655456543, 0.310301274061203, 0.3103467524051666, 0.308646559715271, 0.3086617588996887, 0.3073756992816925, 0.30519792437553406, 0.30481651425361633, 0.30410826206207275, 0.3031981289386749, 0.30194392800331116, 0.3014158606529236, 0.30079489946365356, 0.30116263031959534, 0.3001295030117035, 0.29914194345474243, 0.29706594347953796, 0.2978558838367462, 0.2948533296585083, 0.2955213487148285, 0.2945864796638489, 0.29348814487457275, 0.2953680157661438, 0.29279282689094543, 0.2922116219997406, 0.29089629650115967, 0.2902070879936218, 0.2907593846321106, 0.2893291711807251, 0.28841373324394226, 0.28764107823371887, 0.2876814007759094, 0.2867301106452942, 0.2852499186992645, 0.2848414480686188, 0.28448259830474854, 0.2842126786708832, 0.2834097743034363, 0.28313055634498596, 0.2830926477909088, 0.2820998430252075, 0.2810722887516022, 0.2821139395236969, 0.2805394232273102, 0.2792119085788727, 0.28029775619506836, 0.27951329946517944, 0.2806299924850464, 0.2784580886363983, 0.27712520956993103, 0.27729299664497375, 0.2767643630504608, 0.27515044808387756, 0.27614784240722656, 0.27442532777786255, 0.27338600158691406, 0.2746196985244751, 0.2734638750553131, 0.274129182100296, 0.2737979292869568, 0.2734522521495819, 0.2723233103752136, 0.2739790976047516, 0.2727184593677521, 0.27081099152565, 0.2719423770904541, 0.2709506154060364, 0.27160879969596863, 0.2689014971256256], "moving_avg_accuracy_train": [0.04135294117647058, 0.08094470588235292, 0.1252596470588235, 0.16936897647058818, 0.2114932552941176, 0.2509815768235294, 0.28808106619999996, 0.3238023713447058, 0.35728095773964696, 0.3869410972597999, 0.4157128698867611, 0.4448451123098497, 0.4719394246082765, 0.49969136450039, 0.5150916398150569, 0.5392295346570806, 0.5607395223678431, 0.5806655701310588, 0.6003684248826588, 0.6170892294532164, 0.6362979535667184, 0.654115217033576, 0.6691507541537478, 0.6792992081501377, 0.6920351696880651, 0.7042645938957293, 0.7142993109767446, 0.7256976151731878, 0.7346855007146925, 0.7426004800549879, 0.7495404320494892, 0.7572146241386579, 0.7646131617247921, 0.7726671396699599, 0.7808757198206109, 0.7860563831326675, 0.7913683918782243, 0.7974456703374606, 0.8020987503625381, 0.8053265223851078, 0.8110197524995382, 0.8176001301907608, 0.8223577642305082, 0.825514928983928, 0.8292128478502411, 0.8334045042416875, 0.8354946420528129, 0.839782824906355, 0.8423033659451313, 0.846496558762383, 0.8502280793567328, 0.8530099773034124, 0.8554548619260123, 0.8591611404392935, 0.860569732277717, 0.8621574649322983, 0.8651723066743625, 0.8675609583598675, 0.8708919213474101, 0.8725344939185514, 0.8751751621737551, 0.8772717636034385, 0.8789445872430945, 0.8810901285187851, 0.8835199391963184, 0.886419709982569, 0.8875847978078415, 0.8895816121447044, 0.8920752156361164, 0.8942018117195635, 0.8962075129005482, 0.8983326439634346, 0.89999349721415, 0.9019212063162644, 0.9038514386258144, 0.9050404124102918, 0.9068751946986743, 0.9079900281699833, 0.9092098488823968, 0.9108794522294512, 0.9126173893594474, 0.9147203563058556, 0.9165000853811524, 0.9179559591959783, 0.9194285985704981, 0.9206222093016836, 0.9225882236656329, 0.9237929307108343, 0.9252277552868098, 0.9265967444640112, 0.9285841288411395, 0.9303610100746726, 0.9319649090672053, 0.9330084181604847, 0.9343240469326716, 0.9359528187099926, 0.9370540074272287, 0.9376921360962706, 0.9391229224866434, 0.9401635714144496, 0.9412601554494753, 0.9424188457868807, 0.9435534317964279, 0.9441345592050204, 0.9450363974021655, 0.9462598164854783, 0.9472432466016363, 0.9482224513532373, 0.9492331473943842, 0.9501921855961223, 0.9511235552718041, 0.9520347291563884, 0.9528500797701613, 0.9537133070872629, 0.9544666822608895, 0.9553047199171535, 0.9557177773372028, 0.9562965878387766, 0.9571516349372519, 0.9574600008552914, 0.9583045890050563, 0.9590058948104331, 0.9597805994470369, 0.9603601865611567, 0.9613006384932763, 0.9622199864086545, 0.9628850465913185, 0.9630788948733631, 0.9639686524448503, 0.9646376695533064, 0.9650562555391522, 0.9657247476322959, 0.9664275669867133, 0.9670342220527479, 0.9678037410239437, 0.9684163080980199, 0.9690029125823355, 0.9693873272064549, 0.9697756533093389, 0.9699933820960521, 0.9702269850629175, 0.9709219336154492, 0.971248563783316, 0.9713990015226315, 0.9717767484291918, 0.9723331912333315, 0.9727963426982337, 0.9731425907813515, 0.9734542140561574, 0.973880557356424, 0.9741654427972521, 0.9745088985175269, 0.974761538077539, 0.9752242077991968, 0.9753323752545713, 0.9754791377291142, 0.975717106309144, 0.976032454501759, 0.9764197972868772, 0.9765919352052483, 0.9767162710964883, 0.97699758516331, 0.9771566501763909, 0.9774551028058106, 0.9777895925252295, 0.9779235744491771, 0.9780818052395536, 0.9784383305979512, 0.978683909302862, 0.9787308124902228, 0.9789424371235534, 0.9791140757641392, 0.9792967858347841, 0.979348283721894, 0.9794581612320575, 0.9796888156970871, 0.9799646400097313, 0.9802340583616994, 0.9806365348784706, 0.9808481755082705, 0.9810598285456789, 0.9812997280440522, 0.9813909317102353, 0.9814588973627412, 0.9814847723323493, 0.9816421774520555, 0.9817273714715558, 0.9819475755008708, 0.9821175238331366, 0.9822728302733523, 0.9824761354813112, 0.9827014631096507, 0.982708963857509, 0.9829392439423463, 0.9829982607245823, 0.9832349052403593, 0.9833725911869116, 0.9835129791270438, 0.983599328273163, 0.9836064542693761, 0.9838105147247914, 0.9839424044287829, 0.9840069875153163, 0.9841404064108435, 0.9842769540050532, 0.9844845527221949, 0.9846643327440932, 0.9847578994696838, 0.9848491683462448, 0.9848889573939733, 0.9850282969486936, 0.9851278201950008, 0.9852715087637359, 0.9854008284755976, 0.9855054515103908, 0.9856443181240575, 0.9857763568998871, 0.9859493094451925, 0.9860155549712616, 0.985966940650606, 0.9860502465855454, 0.986120516044638, 0.986207287969586, 0.9863700885843921, 0.9865095503141882, 0.986517418812181, 0.9866303828133158, 0.9867108739437489, 0.9867080218434916, 0.986879572600319, 0.9869374976932282, 0.9870649243944937, 0.9870878437197501, 0.9870520005242457, 0.9870197416482916, 0.9871789439540507, 0.9871786966174692, 0.9872231798968987], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.04110666666666666, 0.07988933333333331, 0.1231537333333333, 0.16654502666666665, 0.20881052399999997, 0.24799613826666664, 0.2844498577733333, 0.3205382053293333, 0.3530843847964, 0.38249594631676, 0.41015301835175066, 0.43871104984990894, 0.46507994486491805, 0.49209195037842623, 0.5074027553405837, 0.5307024798065253, 0.5512188984925395, 0.5704303419766188, 0.5890939744456236, 0.6053445770010613, 0.6233967859676218, 0.6399637740375264, 0.6543273966337737, 0.6636546569703963, 0.6760491912733566, 0.6870442721460209, 0.6965398449314189, 0.7063525271049437, 0.7141306077277827, 0.7205842136216711, 0.726605792259504, 0.7334518797002202, 0.7402800250635315, 0.7475053558905117, 0.7548881536347938, 0.7591593382713143, 0.7640967377775162, 0.7687937306664312, 0.7723143575997882, 0.7745229218398093, 0.7791506296558284, 0.7849422333569122, 0.7893813433545542, 0.7917232090190989, 0.794870888117189, 0.7984904659721368, 0.7997880860415898, 0.8028359441040975, 0.8046723496936877, 0.8081917813909856, 0.8105059365852203, 0.8121753429266984, 0.8139978086340285, 0.8165580277706257, 0.8175155583268965, 0.8184440024942068, 0.8207729355781195, 0.8221623086869743, 0.8245060778182768, 0.8256288033697824, 0.8273325896994708, 0.8291193307295236, 0.8300073976565713, 0.8313399912242474, 0.8333259921018227, 0.8351400595583072, 0.8352527202691431, 0.8367274482422288, 0.8384947034180059, 0.8397918997428719, 0.8410260431019181, 0.8424167721250596, 0.8431084282458869, 0.844704252087965, 0.8459804935458352, 0.846755777524585, 0.8483868664387931, 0.8490148464615805, 0.8493933618154226, 0.8506540256338803, 0.8514819564038255, 0.8530804274301096, 0.8542257180204319, 0.8551898128850554, 0.8558574982632164, 0.8568184151035615, 0.8579099069265386, 0.8585322495672181, 0.8593590246104963, 0.8601031221494466, 0.8612394766011686, 0.8619688622743851, 0.8633853093802799, 0.8643667784422518, 0.86534343393136, 0.866089090538224, 0.866653514817735, 0.8670681633359615, 0.8678946803356987, 0.8686785456354621, 0.8694106910719159, 0.8701496219647243, 0.8709613264349185, 0.8711851937914267, 0.8716400077456173, 0.8724226736377222, 0.8732070729406167, 0.8738996989798884, 0.8744963957485662, 0.8751000895070429, 0.8754034138896719, 0.8756630725007047, 0.8759234319173009, 0.8760644220589042, 0.8761646465196804, 0.8766815152010456, 0.8770133636809411, 0.8775653606461803, 0.877995491248229, 0.8777692754567393, 0.8783523479110654, 0.8783437797866255, 0.8787360684746296, 0.8791691282938333, 0.88013221546445, 0.8805323272513383, 0.8810124278595379, 0.8808311850735842, 0.881441399899559, 0.8817105932429365, 0.8821928672519763, 0.882826913860112, 0.8834108891407675, 0.8842164668933573, 0.8847414868706882, 0.8852673381836194, 0.885327271031924, 0.8851812105953982, 0.8851297562025251, 0.8852167805822726, 0.8856684358573788, 0.8862482589383075, 0.8861834330444768, 0.8864184230733625, 0.8862299140993596, 0.8867669226894237, 0.8871702304204813, 0.8873065407117665, 0.8875625533072565, 0.8877796313098643, 0.8876683348455445, 0.8878215013609901, 0.8878660178915577, 0.8882260827690686, 0.8882434744921617, 0.8884191270429455, 0.8885638810053176, 0.8886008262381191, 0.8888607436143072, 0.8886013359195432, 0.8885945356609222, 0.8888017487614966, 0.8885349072186803, 0.888668083163479, 0.8890812748471312, 0.889053147362418, 0.8895078326261763, 0.8898103826968919, 0.8899760110938694, 0.8899384099844824, 0.8897979023193675, 0.8900981120874308, 0.8900216342120211, 0.8899528041241523, 0.8899575237117371, 0.8900151046738967, 0.8899069275398402, 0.8899029014525229, 0.890165944640604, 0.8902426835098769, 0.8903917484922226, 0.8906059069763337, 0.8905453162787003, 0.8904241179841635, 0.8903150395190805, 0.8902835355671724, 0.8903351820104551, 0.8907549971427429, 0.8905994974284686, 0.8905795476856218, 0.8906282595837263, 0.890698766958687, 0.890615556929485, 0.8906473345698699, 0.8905426011128829, 0.8906483410015946, 0.8905835069014352, 0.8905518228779583, 0.8904033072568291, 0.8902829765311463, 0.8902013455446983, 0.8902078776568951, 0.890320423224539, 0.8904617142354184, 0.8905088761452099, 0.8904179885306889, 0.8905895230109533, 0.8905172373765247, 0.8905055136388722, 0.8904682956083183, 0.8904881327141532, 0.8903859861094044, 0.890467387498464, 0.8904873154152843, 0.8904119172070892, 0.8905440588197137, 0.8903029862710756, 0.8903926876439681, 0.8905934188795712, 0.8906274103249475, 0.8906046692924527, 0.8906508690298741, 0.89047911546022, 0.8905245372475313, 0.8906054168561115, 0.8904648751705003, 0.8903650543201169, 0.8903952155547719, 0.8905023606659613, 0.8907187912660318, 0.8908335788060953, 0.8910702209254857, 0.8910831988329372, 0.8911482122829768, 0.8911267243880125, 0.8912940519492113, 0.8912179800876235, 0.8910695154121946], "moving_var_accuracy_train": [0.015390591695501726, 0.027959103018685115, 0.042837518820083034, 0.056064463408474874, 0.06642811086524943, 0.07381924761361633, 0.07882467186020839, 0.08242630944535827, 0.08427102022385431, 0.08376143308866332, 0.08283562388067497, 0.08219024942998591, 0.08057814031730946, 0.07945185779555786, 0.07364118833390991, 0.07152081120716028, 0.06853284622829865, 0.06525298802062636, 0.062221511586827545, 0.05851562817752582, 0.0559848410983909, 0.05324345088557857, 0.04995371218544929, 0.045885261033555946, 0.04275657737686045, 0.0398269489872334, 0.03675051401057424, 0.03424475465650871, 0.031547317969422554, 0.028956408254096035, 0.026494233831860264, 0.024374849466667375, 0.022430009745721603, 0.020770807817820692, 0.01930015412884559, 0.017611692167137028, 0.01610447988263917, 0.014826431715615145, 0.013538648927531606, 0.0122785506448456, 0.011342411402583755, 0.010597882597357645, 0.009741810072527354, 0.008857338268796743, 0.008094675877393577, 0.007443338139389807, 0.0067383224100762895, 0.006229986778737384, 0.005664166245017043, 0.005255995414539191, 0.004855714086599792, 0.004439793283611478, 0.004049611102610761, 0.0037682784961117688, 0.003409307825206054, 0.0030910650975272606, 0.002863762024341774, 0.0026287367337795866, 0.0024657208902210404, 0.0022434312030621293, 0.00208184624226228, 0.0019132232560306029, 0.001747085980792075, 0.0016138075090040937, 0.0015055625774615755, 0.0014306843552305448, 0.0012998327864728744, 0.001205734915288698, 0.0011411239491112633, 0.0010677132523193333, 0.0009971474621340358, 0.0009380783542306325, 0.0008690964204912755, 0.0008156313398835209, 0.0007676003768146463, 0.000703563267074752, 0.0006635047747791389, 0.0006083399803199838, 0.0005608976454218807, 0.0005298960589081511, 0.0005040902822277063, 0.0004934834838041046, 0.00047264205565680534, 0.00044445396717338726, 0.00041952657100252384, 0.00039039627310068193, 0.000386143558103908, 0.00036059107387633843, 0.00034306046056311115, 0.00032562159681245043, 0.0003286067070932877, 0.00032416179864669826, 0.0003148980465862563, 0.0002932084429774432, 0.000279465510275551, 0.00027539503677137506, 0.00025876908241294897, 0.000236557047955932, 0.00023132569041422676, 0.0002179396730913027, 0.00020696817469503017, 0.00019835442690749707, 0.00019010455293429, 0.0001741334792260179, 0.00016403994050788394, 0.0001611067347378223, 0.00015370027440434, 0.00014695982447392926, 0.00014145740041484557, 0.00013558944882489758, 0.000129837549197426, 0.0001243259349092193, 0.00011787651102871634, 0.00011279531253475714, 0.00010662394865141443, 0.00010228231780612096, 9.358963391582938e-05, 8.724586489483595e-05, 8.510122827085133e-05, 7.744691129844142e-05, 7.612218245310819e-05, 7.293643270169313e-05, 7.104429489730195e-05, 6.696315641325561e-05, 6.822688930157791e-05, 6.901100567701327e-05, 6.609065052839721e-05, 5.981977988362238e-05, 6.096281871942975e-05, 5.8894791870150034e-05, 5.4582240731053805e-05, 5.3145951765308e-05, 5.227695199327138e-05, 5.0361530116252715e-05, 5.065481212789951e-05, 4.896647669729028e-05, 4.716677241673443e-05, 4.378006660419264e-05, 4.075923440340245e-05, 3.7109963384134484e-05, 3.389010016087579e-05, 3.484767156078243e-05, 3.232308980375072e-05, 2.929446444406876e-05, 2.764925252840535e-05, 2.7670984624073915e-05, 2.683446967663554e-05, 2.523001232453679e-05, 2.3580992680690637e-05, 2.2858810899761676e-05, 2.130336723934835e-05, 2.0234687001518797e-05, 1.8785659026914526e-05, 1.8833662566273823e-05, 1.7055598095266054e-05, 1.5543891301144967e-05, 1.4499163576762876e-05, 1.3944247562356605e-05, 1.390013270476918e-05, 1.2776802600762465e-05, 1.1638257065339919e-05, 1.11866697965321e-05, 1.0295717922356325e-05, 1.0067811878188658e-05, 1.0067981041942473e-05, 9.22274334125064e-06, 8.525801854333865e-06, 8.817214649525222e-06, 8.478273287323524e-06, 7.650245139452645e-06, 7.288285494398421e-06, 6.824595351437972e-06, 6.442582545529756e-06, 5.8221925823677685e-06, 5.348630929288655e-06, 5.292581176502375e-06, 5.448034521863138e-06, 5.5565073050714385e-06, 6.4587426935350625e-06, 6.215994229820845e-06, 5.99756788103596e-06, 5.915777016810142e-06, 5.399062293656126e-06, 4.900730033575471e-06, 4.416682656687973e-06, 4.198001736406819e-06, 3.843523751393754e-06, 3.895579706993464e-06, 3.7659636570536775e-06, 3.6064481047007075e-06, 3.6178003624794776e-06, 3.7129731870693242e-06, 3.342182219328318e-06, 3.485224254649694e-06, 3.1680486544540983e-06, 3.3552494306349503e-06, 3.190341266473488e-06, 3.0486861034376406e-06, 2.8109230684134273e-06, 2.5302877799703478e-06, 2.652025027152135e-06, 2.54337657060749e-06, 2.3265776891424567e-06, 2.2541253353814743e-06, 2.1965200112036207e-06, 2.364743056313301e-06, 2.4191564571452754e-06, 2.256033400670616e-06, 2.105400131061962e-06, 1.909108632828e-06, 1.8929373731321082e-06, 1.7927875248184452e-06, 1.7993264154031736e-06, 1.769906064746788e-06, 1.6914292729561286e-06, 1.695841773182088e-06, 1.6831657407669623e-06, 1.7840624130388175e-06, 1.6451523992523292e-06, 1.5019073288823183e-06, 1.4141755051592372e-06, 1.3171981265737056e-06, 1.2532426165489008e-06, 1.366454716525213e-06, 1.4048554115721445e-06, 1.264927089760916e-06, 1.2532821707563594e-06, 1.1862633523863485e-06, 1.0677102274306098e-06, 1.2258061641997394e-06, 1.1334233952766972e-06, 1.1662191335074814e-06, 1.0543248793886627e-06, 9.604550034255128e-07, 8.737752187832901e-07, 1.014506064335989e-06, 9.130560084808512e-07, 8.395592669720363e-07], "duration": 226283.121527, "accuracy_train": [0.41352941176470587, 0.43727058823529413, 0.5240941176470588, 0.5663529411764706, 0.5906117647058824, 0.6063764705882353, 0.6219764705882352, 0.6452941176470588, 0.6585882352941177, 0.6538823529411765, 0.6746588235294118, 0.7070352941176471, 0.7157882352941176, 0.7494588235294117, 0.6536941176470589, 0.7564705882352941, 0.7543294117647059, 0.76, 0.7776941176470589, 0.7675764705882353, 0.8091764705882353, 0.8144705882352942, 0.8044705882352942, 0.7706352941176471, 0.8066588235294118, 0.8143294117647059, 0.8046117647058824, 0.8282823529411765, 0.8155764705882353, 0.8138352941176471, 0.812, 0.8262823529411765, 0.8312, 0.8451529411764706, 0.8547529411764706, 0.8326823529411764, 0.8391764705882353, 0.8521411764705883, 0.8439764705882353, 0.8343764705882353, 0.8622588235294117, 0.8768235294117647, 0.8651764705882353, 0.8539294117647059, 0.8624941176470589, 0.8711294117647059, 0.8543058823529411, 0.8783764705882353, 0.8649882352941176, 0.884235294117647, 0.8838117647058824, 0.8780470588235294, 0.8774588235294117, 0.8925176470588235, 0.8732470588235294, 0.8764470588235294, 0.8923058823529412, 0.8890588235294118, 0.9008705882352941, 0.8873176470588235, 0.8989411764705882, 0.8961411764705882, 0.894, 0.9004, 0.9053882352941176, 0.9125176470588235, 0.8980705882352941, 0.9075529411764706, 0.9145176470588235, 0.9133411764705882, 0.9142588235294118, 0.9174588235294118, 0.9149411764705883, 0.9192705882352942, 0.9212235294117647, 0.9157411764705883, 0.9233882352941176, 0.9180235294117647, 0.9201882352941176, 0.9259058823529411, 0.9282588235294118, 0.9336470588235294, 0.9325176470588236, 0.9310588235294117, 0.9326823529411765, 0.931364705882353, 0.9402823529411765, 0.934635294117647, 0.9381411764705883, 0.9389176470588235, 0.9464705882352941, 0.9463529411764706, 0.9464, 0.9424, 0.9461647058823529, 0.9506117647058824, 0.9469647058823529, 0.943435294117647, 0.952, 0.9495294117647058, 0.9511294117647059, 0.9528470588235294, 0.953764705882353, 0.9493647058823529, 0.9531529411764705, 0.9572705882352941, 0.9560941176470589, 0.9570352941176471, 0.9583294117647059, 0.9588235294117647, 0.9595058823529412, 0.9602352941176471, 0.9601882352941177, 0.9614823529411765, 0.9612470588235295, 0.9628470588235294, 0.959435294117647, 0.9615058823529412, 0.9648470588235294, 0.9602352941176471, 0.9659058823529412, 0.9653176470588235, 0.9667529411764706, 0.9655764705882353, 0.969764705882353, 0.9704941176470588, 0.9688705882352942, 0.9648235294117647, 0.9719764705882353, 0.9706588235294118, 0.9688235294117648, 0.9717411764705882, 0.9727529411764706, 0.9724941176470588, 0.9747294117647058, 0.9739294117647059, 0.9742823529411765, 0.9728470588235294, 0.9732705882352941, 0.9719529411764706, 0.9723294117647059, 0.9771764705882353, 0.9741882352941177, 0.9727529411764706, 0.9751764705882353, 0.9773411764705883, 0.976964705882353, 0.9762588235294117, 0.9762588235294117, 0.9777176470588236, 0.9767294117647058, 0.9776, 0.9770352941176471, 0.9793882352941177, 0.9763058823529411, 0.9768, 0.9778588235294118, 0.9788705882352942, 0.9799058823529412, 0.9781411764705882, 0.977835294117647, 0.9795294117647059, 0.9785882352941176, 0.9801411764705882, 0.9808, 0.9791294117647059, 0.9795058823529412, 0.9816470588235294, 0.9808941176470588, 0.9791529411764706, 0.9808470588235294, 0.9806588235294118, 0.9809411764705882, 0.9798117647058824, 0.9804470588235294, 0.981764705882353, 0.9824470588235295, 0.9826588235294118, 0.9842588235294117, 0.9827529411764706, 0.982964705882353, 0.9834588235294117, 0.9822117647058823, 0.9820705882352941, 0.9817176470588236, 0.9830588235294118, 0.9824941176470589, 0.9839294117647058, 0.9836470588235294, 0.9836705882352941, 0.9843058823529411, 0.9847294117647059, 0.9827764705882353, 0.9850117647058824, 0.9835294117647059, 0.9853647058823529, 0.9846117647058823, 0.9847764705882353, 0.9843764705882353, 0.9836705882352941, 0.9856470588235294, 0.9851294117647059, 0.9845882352941177, 0.9853411764705883, 0.9855058823529412, 0.9863529411764705, 0.9862823529411765, 0.9856, 0.9856705882352941, 0.9852470588235294, 0.9862823529411765, 0.9860235294117647, 0.9865647058823529, 0.9865647058823529, 0.9864470588235295, 0.9868941176470588, 0.986964705882353, 0.9875058823529411, 0.9866117647058823, 0.9855294117647059, 0.9868, 0.9867529411764706, 0.9869882352941176, 0.987835294117647, 0.987764705882353, 0.9865882352941177, 0.9876470588235294, 0.9874352941176471, 0.9866823529411765, 0.9884235294117647, 0.9874588235294117, 0.9882117647058823, 0.9872941176470589, 0.9867294117647059, 0.9867294117647059, 0.9886117647058823, 0.9871764705882353, 0.9876235294117647], "end": "2016-02-06 15:40:13.454000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0, 211.0, 212.0, 213.0, 214.0, 215.0, 216.0, 217.0, 218.0, 219.0, 220.0, 221.0, 222.0, 223.0, 224.0, 225.0, 226.0, 227.0, 228.0, 229.0, 230.0, 231.0, 232.0, 233.0, 234.0, 235.0, 236.0, 237.0], "moving_var_accuracy_valid": [0.015207822399999998, 0.027223897264, 0.041347782303839994, 0.054158243107710405, 0.06481976918044542, 0.0721574035515048, 0.07690152618919088, 0.08093269303417565, 0.08237270791188069, 0.08192079668028591, 0.08061293971419528, 0.07989169621022395, 0.07816039420801471, 0.0769111907639694, 0.07132985842487546, 0.06908276702408711, 0.06596280124297686, 0.06268823716535696, 0.05955439404126407, 0.05597569338787083, 0.05331106428623507, 0.05045014370098678, 0.04726195221767496, 0.04331873706439167, 0.04036948368323785, 0.03742056154548193, 0.03448999851363884, 0.031907597245222456, 0.029261324364278658, 0.026710033189153472, 0.024365364553862574, 0.022350648317689716, 0.02053519560784333, 0.01895152469709279, 0.017546923550180313, 0.015956418358955532, 0.014580177748014565, 0.013320715652999776, 0.0121001974137347, 0.010934077476381933, 0.010033410845417734, 0.009331953821749632, 0.008576109717715165, 0.007767857759060604, 0.007080242936495524, 0.006490130737478225, 0.005856272024332227, 0.0053542497708217425, 0.004849176263144873, 0.004475736232077893, 0.004076360437237138, 0.003693806651310126, 0.0033543184174686636, 0.0030778790739683813, 0.002778342949467273, 0.0025082667316668594, 0.0023062554222842593, 0.0020930030987763113, 0.0019331420725663018, 0.0017511724792857047, 0.0016021812220722328, 0.001470695091441281, 0.0013307235480993953, 0.0012136334438389638, 0.0011277678948266347, 0.001044608671974053, 0.0009402620366985424, 0.000865809236380101, 0.0008073370304488902, 0.0007417477921512158, 0.0006812810014121933, 0.0006305600462132476, 0.0005718095352972247, 0.0005375484653820027, 0.0004984527491728817, 0.00045401706148494894, 0.000432559414750929, 0.0003928527034570162, 0.00035485689796916134, 0.00033367466754076133, 0.0003064764250250853, 0.00029882476911940515, 0.00028074750703399354, 0.00026103806650253444, 0.0002389464937301726, 0.00022336209492368337, 0.00021174807502794972, 0.00019405906078682586, 0.0001808051674578322, 0.000167707781039297, 0.00016255871589490294, 0.00015109087544805372, 0.00015403868953742876, 0.00014730435426015985, 0.0001411586223337895, 0.00013204679407864992, 0.00012170928757649784, 0.00011108575936185497, 0.00010612535658336039, 0.00010104282419858383, 9.576287423980641e-05, 9.110075659494546e-05, 8.792045825785061e-05, 7.957946177185507e-05, 7.348321719100853e-05, 7.164798855988736e-05, 7.002073010133018e-05, 6.733623456369148e-05, 6.380703441107742e-05, 6.07063463561833e-05, 5.546376285044059e-05, 5.052418991394792e-05, 4.60818541548459e-05, 4.165257271962501e-05, 3.7577719930503514e-05, 3.622432704143983e-05, 3.359300505977606e-05, 3.297601040049824e-05, 3.1343520373816934e-05, 2.8669730595308457e-05, 2.8862518918722065e-05, 2.5976927741657613e-05, 2.476424870011577e-05, 2.3975691093182918e-05, 2.9925954067722478e-05, 2.8374163639013008e-05, 2.76112166210534e-05, 2.5145735486090702e-05, 2.598242114203806e-05, 2.4036364532902692e-05, 2.3726022057769168e-05, 2.4971555763587676e-05, 2.5543644342978613e-05, 2.882987954789118e-05, 2.8427705382471225e-05, 2.8073611274027137e-05, 2.5298577663377694e-05, 2.2960722757102702e-05, 2.0688478472305934e-05, 1.868778980910926e-05, 1.865494321597862e-05, 1.981520214098054e-05, 1.7871503495481085e-05, 1.6581335969014073e-05, 1.5243023071629374e-05, 1.6314124796689472e-05, 1.6146626450398264e-05, 1.4699188264950742e-05, 1.3819151479901491e-05, 1.286134206485661e-05, 1.1686689985101656e-05, 1.0729160819675058e-05, 9.674080231151581e-06, 9.873492652188836e-06, 8.888865635259291e-06, 8.277663439105177e-06, 7.638480481796532e-06, 6.886916985657746e-06, 6.80623866909247e-06, 6.731245971108444e-06, 6.058537565653411e-06, 5.839119230535178e-06, 5.8960469882350675e-06, 5.46606477986864e-06, 6.456004608835086e-06, 5.817524546518154e-06, 7.096420293576024e-06, 7.2106071718291156e-06, 6.736441347614145e-06, 6.075521803696888e-06, 5.645651258931595e-06, 5.892219276603742e-06, 5.355637137788075e-06, 4.862711652973482e-06, 4.376640958238867e-06, 3.968816967244031e-06, 3.6772559015133913e-06, 3.309676195773835e-06, 3.601434045358974e-06, 3.294290327338681e-06, 3.1648446152601834e-06, 3.261134860584925e-06, 2.9680624682837122e-06, 2.80345746084277e-06, 2.6301947186623094e-06, 2.376107737668501e-06, 2.162503159835488e-06, 3.532455551532498e-06, 3.3968314466337104e-06, 3.0607302321272427e-06, 2.776012850066999e-06, 2.5431531743749415e-06, 2.351153037575676e-06, 2.125126099673958e-06, 2.011335362818577e-06, 1.910830143119595e-06, 1.7575782736990194e-06, 1.5908553424222528e-06, 1.6302818156544595e-06, 1.597568985979415e-06, 1.4977846489176862e-06, 1.3483902004336858e-06, 1.3275497235567379e-06, 1.3744630989990706e-06, 1.2570350007157927e-06, 1.2056765269040644e-06, 1.3499255754900057e-06, 1.2619599344437916e-06, 1.137000955220308e-06, 1.0357674958830677e-06, 9.357323432058847e-07, 9.360644686403893e-07, 9.020936970437757e-07, 8.154584241585597e-07, 7.850765899339769e-07, 8.637215830234052e-07, 1.3003931880825945e-06, 1.2427708959634269e-06, 1.4811310668881447e-06, 1.3434167254282043e-06, 1.213729443915731e-06, 1.1115662411644099e-06, 1.265903215248214e-06, 1.1578811425864002e-06, 1.1009666280845368e-06, 1.1686376538258919e-06, 1.1414517079846312e-06, 1.035493837869384e-06, 1.0352651277486946e-06, 1.3533184567958775e-06, 1.3365722253007365e-06, 1.7069104367972031e-06, 1.5377352278538428e-06, 1.4220024432429453e-06, 1.2839577655886412e-06, 1.407548603660267e-06, 1.3188760964231456e-06, 1.3853643254330295e-06], "accuracy_test": 0.7563, "start": "2016-02-04 00:48:50.332000", "learning_rate_per_epoch": [0.005456923972815275, 0.005334137473255396, 0.005214113742113113, 0.0050967903807759285, 0.0049821073189377785, 0.004870004486292601, 0.004760424140840769, 0.0046533094719052315, 0.004548605065792799, 0.004446256440132856, 0.004346210975199938, 0.004248416516929865, 0.004152822773903608, 0.004059379920363426, 0.0039680395275354385, 0.0038787543307989836, 0.003791478229686618, 0.003706165822222829, 0.003622773103415966, 0.0035412567667663097, 0.0034615746699273586, 0.003383685601875186, 0.003307549050077796, 0.003233125666156411, 0.003160376800224185, 0.003089264966547489, 0.0030197531450539827, 0.002951805479824543, 0.0028853865806013346, 0.0028204622212797403, 0.002756998874247074, 0.002694963477551937, 0.002634323900565505, 0.0025750487111508846, 0.0025171074084937572, 0.002460469724610448, 0.0024051065556705, 0.0023509890306741, 0.0022980892099440098, 0.0022463796194642782, 0.0021958337165415287, 0.0021464251913130283, 0.0020981281995773315, 0.002050918061286211, 0.0020047700963914394, 0.001959660556167364, 0.0019155660411342978, 0.0018724637338891625, 0.0018303312826901674, 0.001789146801456809, 0.0017488891026005149, 0.0017095372313633561, 0.001671070815064013, 0.0016334699466824532, 0.0015967150684446096, 0.0015607872046530247, 0.0015256678452715278, 0.001491338713094592, 0.0014577819965779781, 0.0014249803498387337, 0.001392916776239872, 0.0013615746283903718, 0.0013309377245604992, 0.0013009902322664857, 0.0012717165518552065, 0.001243101549334824, 0.0012151304399594665, 0.0011877886718139052, 0.0011610621586441994, 0.0011349370470270514, 0.0011093997163698077, 0.0010844370117411017, 0.0010600360110402107, 0.0010361840249970555, 0.0010128687135875225, 0.0009900780860334635, 0.0009678002679720521, 0.0009460237342864275, 0.0009247371926903725, 0.0009039295837283134, 0.0008835901971906424, 0.000863708439283073, 0.0008442740654572845, 0.0008252770057879388, 0.0008067073649726808, 0.0007885555969551206, 0.0007708122720941901, 0.0007534681353718042, 0.0007365142810158432, 0.0007199419196695089, 0.0007037424365989864, 0.000687907449901104, 0.0006724288105033338, 0.0006572984275408089, 0.0006425085011869669, 0.0006280513480305672, 0.0006139195174910128, 0.0006001056754030287, 0.0005866026622243226, 0.0005734034930355847, 0.0005605012993328273, 0.0005478893872350454, 0.0005355612956918776, 0.0005235105636529624, 0.0005117310211062431, 0.0005002164980396628, 0.0004889611154794693, 0.0004779589653480798, 0.0004672043723985553, 0.0004566917777992785, 0.0004464157100301236, 0.00043637087219394743, 0.00042655205470509827, 0.0004169541643932462, 0.0004075722536072135, 0.0003984014329034835, 0.00038943695835769176, 0.00038067420246079564, 0.0003721086250152439, 0.0003637357731349766, 0.00035555133945308626, 0.00034755104570649564, 0.0003397307591512799, 0.0003320864634588361, 0.00032461417140439153, 0.0003173100121784955, 0.0003101702022831887, 0.00030319104553200305, 0.0002963689330499619, 0.0002897003141697496, 0.00028318175463937223, 0.0002768098493106663, 0.00027058133855462074, 0.00026449296274222434, 0.0002585415786597878, 0.00025272410130128264, 0.0002470375329721719, 0.0002414789196336642, 0.0002360453800065443, 0.00023073410557117313, 0.00022554234601557255, 0.00022046739468351007, 0.0002155066467821598, 0.00021065751207061112, 0.00020591748761944473, 0.00020128412870690227, 0.00019675501971505582, 0.00019232781778555363, 0.00018800023826770484, 0.00018377002561464906, 0.00017963499703910202, 0.00017559301340952516, 0.0001716419792501256, 0.0001677798427408561, 0.00016400461026933044, 0.00016031433187890798, 0.00015670708671677858, 0.000153181012137793, 0.0001497342746006325, 0.00014636509877163917, 0.0001430717238690704, 0.00013985246187075973, 0.00013670562475454062, 0.00013362959725782275, 0.00013062279322184622, 0.00012768364103976637, 0.00012481062731239945, 0.00012200225319247693, 0.00011925707076443359, 0.00011657366121653467, 0.00011395062756491825, 0.00011138661648146808, 0.00010888029646594077, 0.000106430372397881, 0.00010403557098470628, 0.0001016946553136222, 9.940641757566482e-05, 9.716966451378539e-05, 9.498323925072327e-05, 9.284601401304826e-05, 9.075687557924539e-05, 8.871474710758775e-05, 8.671856630826369e-05, 8.476730727124959e-05, 8.285995136247948e-05, 8.099551632767543e-05, 7.917302718851715e-05, 7.739154534647241e-05, 7.565015403088182e-05, 7.394794374704361e-05, 7.228403410408646e-05, 7.06575665390119e-05, 6.906769704073668e-05, 6.751359615009278e-05, 6.599447078770027e-05, 6.450952059822157e-05, 6.305798888206482e-05, 6.163911893963814e-05, 6.0252172261243686e-05, 5.889643216505647e-05, 5.757120015914552e-05, 5.6275785027537495e-05, 5.500951738213189e-05, 5.3771742386743426e-05, 5.256181975710206e-05, 5.137912376085296e-05, 5.0223039579577744e-05, 4.909296694677323e-05, 4.798832378583029e-05, 4.69085352960974e-05, 4.5853041228838265e-05, 4.4821299525210634e-05, 4.3812771764351055e-05, 4.2826937715290114e-05, 4.186328442301601e-05, 4.092131712241098e-05, 4.000054468633607e-05, 3.9100490539567545e-05, 3.82206890208181e-05, 3.736068174475804e-05, 3.65200248779729e-05, 3.569828550098464e-05, 3.489503433229402e-05, 3.410985664231703e-05, 3.334234861540608e-05, 3.25921100738924e-05, 3.1858751754043624e-05, 3.1141895306063816e-05, 3.0441169656114653e-05, 2.9756211006315425e-05, 2.908666465373244e-05, 2.8432183171389624e-05, 2.7792428227257915e-05, 2.7167068765265867e-05, 2.655578100529965e-05, 2.5958246624213643e-05, 2.5374158212798648e-05, 2.480321199982427e-05], "accuracy_train_first": 0.41352941176470587, "accuracy_train_last": 0.9876235294117647, "batch_size_eval": 1024, "accuracy_train_std": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "accuracy_test_std": 0, "error_valid": [0.5889333333333333, 0.5710666666666666, 0.4874666666666667, 0.4429333333333333, 0.41080000000000005, 0.3993333333333333, 0.3874666666666666, 0.3546666666666667, 0.354, 0.3528, 0.3409333333333333, 0.3042666666666667, 0.2976, 0.26480000000000004, 0.3548, 0.25960000000000005, 0.26413333333333333, 0.2566666666666667, 0.24293333333333333, 0.24839999999999995, 0.2141333333333333, 0.2109333333333333, 0.21640000000000004, 0.25239999999999996, 0.21240000000000003, 0.21399999999999997, 0.21799999999999997, 0.20533333333333337, 0.21586666666666665, 0.22133333333333338, 0.21919999999999995, 0.2049333333333333, 0.1982666666666667, 0.18746666666666667, 0.17866666666666664, 0.20240000000000002, 0.19146666666666667, 0.1889333333333333, 0.19599999999999995, 0.2056, 0.17920000000000003, 0.16293333333333337, 0.17066666666666663, 0.18720000000000003, 0.17679999999999996, 0.16893333333333338, 0.18853333333333333, 0.1697333333333333, 0.17879999999999996, 0.16013333333333335, 0.16866666666666663, 0.17279999999999995, 0.16959999999999997, 0.1604, 0.1738666666666666, 0.17320000000000002, 0.15826666666666667, 0.16533333333333333, 0.15439999999999998, 0.16426666666666667, 0.15733333333333333, 0.15480000000000005, 0.16200000000000003, 0.15666666666666662, 0.14880000000000004, 0.1485333333333333, 0.1637333333333333, 0.15000000000000002, 0.14559999999999995, 0.1485333333333333, 0.1478666666666667, 0.14506666666666668, 0.15066666666666662, 0.14093333333333335, 0.1425333333333333, 0.14626666666666666, 0.13693333333333335, 0.14533333333333331, 0.1472, 0.138, 0.14106666666666667, 0.13253333333333328, 0.13546666666666662, 0.13613333333333333, 0.13813333333333333, 0.13453333333333328, 0.13226666666666664, 0.1358666666666667, 0.13319999999999999, 0.13319999999999999, 0.1285333333333334, 0.13146666666666662, 0.12386666666666668, 0.12680000000000002, 0.12586666666666668, 0.12719999999999998, 0.12826666666666664, 0.12919999999999998, 0.1246666666666667, 0.12426666666666664, 0.124, 0.12319999999999998, 0.12173333333333336, 0.12680000000000002, 0.12426666666666664, 0.12053333333333338, 0.11973333333333336, 0.11986666666666668, 0.12013333333333331, 0.11946666666666672, 0.12186666666666668, 0.122, 0.12173333333333336, 0.1226666666666667, 0.12293333333333334, 0.1186666666666667, 0.12, 0.11746666666666672, 0.11813333333333331, 0.12426666666666664, 0.11639999999999995, 0.12173333333333336, 0.11773333333333336, 0.11693333333333333, 0.11119999999999997, 0.11586666666666667, 0.1146666666666667, 0.12080000000000002, 0.11306666666666665, 0.11586666666666667, 0.11346666666666672, 0.11146666666666671, 0.11133333333333328, 0.10853333333333337, 0.11053333333333337, 0.10999999999999999, 0.11413333333333331, 0.11613333333333331, 0.11533333333333329, 0.11399999999999999, 0.11026666666666662, 0.10853333333333337, 0.11439999999999995, 0.11146666666666671, 0.11546666666666672, 0.10840000000000005, 0.10919999999999996, 0.11146666666666671, 0.1101333333333333, 0.11026666666666662, 0.11333333333333329, 0.11080000000000001, 0.11173333333333335, 0.10853333333333337, 0.11160000000000003, 0.10999999999999999, 0.1101333333333333, 0.11106666666666665, 0.10880000000000001, 0.11373333333333335, 0.11146666666666671, 0.10933333333333328, 0.11386666666666667, 0.1101333333333333, 0.10719999999999996, 0.11119999999999997, 0.10640000000000005, 0.10746666666666671, 0.10853333333333337, 0.11040000000000005, 0.11146666666666671, 0.10719999999999996, 0.11066666666666669, 0.11066666666666669, 0.10999999999999999, 0.10946666666666671, 0.11106666666666665, 0.1101333333333333, 0.10746666666666671, 0.10906666666666665, 0.10826666666666662, 0.10746666666666671, 0.10999999999999999, 0.11066666666666669, 0.11066666666666669, 0.10999999999999999, 0.10919999999999996, 0.10546666666666671, 0.11080000000000001, 0.10960000000000003, 0.10893333333333333, 0.10866666666666669, 0.1101333333333333, 0.10906666666666665, 0.11040000000000005, 0.10840000000000005, 0.10999999999999999, 0.10973333333333335, 0.11093333333333333, 0.11080000000000001, 0.11053333333333337, 0.10973333333333335, 0.10866666666666669, 0.10826666666666662, 0.10906666666666665, 0.11040000000000005, 0.10786666666666667, 0.1101333333333333, 0.10960000000000003, 0.10986666666666667, 0.10933333333333328, 0.11053333333333337, 0.10880000000000001, 0.10933333333333328, 0.11026666666666662, 0.10826666666666662, 0.11186666666666667, 0.10880000000000001, 0.10760000000000003, 0.10906666666666665, 0.10960000000000003, 0.10893333333333333, 0.11106666666666665, 0.10906666666666665, 0.10866666666666669, 0.11080000000000001, 0.11053333333333337, 0.10933333333333328, 0.10853333333333337, 0.10733333333333328, 0.1081333333333333, 0.1068, 0.10880000000000001, 0.10826666666666662, 0.10906666666666665, 0.10719999999999996, 0.10946666666666671, 0.11026666666666662], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.02250107924608673, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 16, "valid_ratio": 0.15, "learning_rate": 0.005582536986037197, "optimization": "rmsprop", "nb_data_augmentation": 2, "learning_rate_decay_method": "exp", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 2.9325489018497037e-05, "rotation_range": [0, 0], "momentum": 0.7961239983398196}, "accuracy_valid_max": 0.8945333333333333, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        y_pred = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            y_pred.extend((nnet.predict(X[mini_batch]) == y[mini_batch]).tolist())\n        return np.mean(y_pred)\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            acc = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = acc\n            status[\"accuracy_train_std\"] = 0\n            acc = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = acc\n            status[\"accuracy_valid_std\"] = 0\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n\n    # rescaling to [-1, 1]\n    X_min = X_train.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X_train.max(axis=(0, 2, 3))[None, :, None, None]\n    def preprocess(a):\n        return (a / 255.) * 2 - 1\n        # return 2 * ((a - X_min) / (X_max - X_min)) - 1\n    X_train = preprocess(X_train)\n    X_valid = preprocess(X_valid)\n\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = preprocess(X_test)\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    acc = evaluate(X_test, y_test, batch_size_eval)\n    light.set(\"accuracy_test\", acc)\n    light.set(\"accuracy_test_std\", 0)\n    print(\"Test accuracy : {}+-{}\".format(acc, 0))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8897333333333334, "accuracy_valid_std": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "accuracy_valid": [0.4110666666666667, 0.42893333333333333, 0.5125333333333333, 0.5570666666666667, 0.5892, 0.6006666666666667, 0.6125333333333334, 0.6453333333333333, 0.646, 0.6472, 0.6590666666666667, 0.6957333333333333, 0.7024, 0.7352, 0.6452, 0.7404, 0.7358666666666667, 0.7433333333333333, 0.7570666666666667, 0.7516, 0.7858666666666667, 0.7890666666666667, 0.7836, 0.7476, 0.7876, 0.786, 0.782, 0.7946666666666666, 0.7841333333333333, 0.7786666666666666, 0.7808, 0.7950666666666667, 0.8017333333333333, 0.8125333333333333, 0.8213333333333334, 0.7976, 0.8085333333333333, 0.8110666666666667, 0.804, 0.7944, 0.8208, 0.8370666666666666, 0.8293333333333334, 0.8128, 0.8232, 0.8310666666666666, 0.8114666666666667, 0.8302666666666667, 0.8212, 0.8398666666666667, 0.8313333333333334, 0.8272, 0.8304, 0.8396, 0.8261333333333334, 0.8268, 0.8417333333333333, 0.8346666666666667, 0.8456, 0.8357333333333333, 0.8426666666666667, 0.8452, 0.838, 0.8433333333333334, 0.8512, 0.8514666666666667, 0.8362666666666667, 0.85, 0.8544, 0.8514666666666667, 0.8521333333333333, 0.8549333333333333, 0.8493333333333334, 0.8590666666666666, 0.8574666666666667, 0.8537333333333333, 0.8630666666666666, 0.8546666666666667, 0.8528, 0.862, 0.8589333333333333, 0.8674666666666667, 0.8645333333333334, 0.8638666666666667, 0.8618666666666667, 0.8654666666666667, 0.8677333333333334, 0.8641333333333333, 0.8668, 0.8668, 0.8714666666666666, 0.8685333333333334, 0.8761333333333333, 0.8732, 0.8741333333333333, 0.8728, 0.8717333333333334, 0.8708, 0.8753333333333333, 0.8757333333333334, 0.876, 0.8768, 0.8782666666666666, 0.8732, 0.8757333333333334, 0.8794666666666666, 0.8802666666666666, 0.8801333333333333, 0.8798666666666667, 0.8805333333333333, 0.8781333333333333, 0.878, 0.8782666666666666, 0.8773333333333333, 0.8770666666666667, 0.8813333333333333, 0.88, 0.8825333333333333, 0.8818666666666667, 0.8757333333333334, 0.8836, 0.8782666666666666, 0.8822666666666666, 0.8830666666666667, 0.8888, 0.8841333333333333, 0.8853333333333333, 0.8792, 0.8869333333333334, 0.8841333333333333, 0.8865333333333333, 0.8885333333333333, 0.8886666666666667, 0.8914666666666666, 0.8894666666666666, 0.89, 0.8858666666666667, 0.8838666666666667, 0.8846666666666667, 0.886, 0.8897333333333334, 0.8914666666666666, 0.8856, 0.8885333333333333, 0.8845333333333333, 0.8916, 0.8908, 0.8885333333333333, 0.8898666666666667, 0.8897333333333334, 0.8866666666666667, 0.8892, 0.8882666666666666, 0.8914666666666666, 0.8884, 0.89, 0.8898666666666667, 0.8889333333333334, 0.8912, 0.8862666666666666, 0.8885333333333333, 0.8906666666666667, 0.8861333333333333, 0.8898666666666667, 0.8928, 0.8888, 0.8936, 0.8925333333333333, 0.8914666666666666, 0.8896, 0.8885333333333333, 0.8928, 0.8893333333333333, 0.8893333333333333, 0.89, 0.8905333333333333, 0.8889333333333334, 0.8898666666666667, 0.8925333333333333, 0.8909333333333334, 0.8917333333333334, 0.8925333333333333, 0.89, 0.8893333333333333, 0.8893333333333333, 0.89, 0.8908, 0.8945333333333333, 0.8892, 0.8904, 0.8910666666666667, 0.8913333333333333, 0.8898666666666667, 0.8909333333333334, 0.8896, 0.8916, 0.89, 0.8902666666666667, 0.8890666666666667, 0.8892, 0.8894666666666666, 0.8902666666666667, 0.8913333333333333, 0.8917333333333334, 0.8909333333333334, 0.8896, 0.8921333333333333, 0.8898666666666667, 0.8904, 0.8901333333333333, 0.8906666666666667, 0.8894666666666666, 0.8912, 0.8906666666666667, 0.8897333333333334, 0.8917333333333334, 0.8881333333333333, 0.8912, 0.8924, 0.8909333333333334, 0.8904, 0.8910666666666667, 0.8889333333333334, 0.8909333333333334, 0.8913333333333333, 0.8892, 0.8894666666666666, 0.8906666666666667, 0.8914666666666666, 0.8926666666666667, 0.8918666666666667, 0.8932, 0.8912, 0.8917333333333334, 0.8909333333333334, 0.8928, 0.8905333333333333, 0.8897333333333334], "seed": 831243688, "model": "residualv3", "loss_std": [0.3165910243988037, 0.26686516404151917, 0.2714148759841919, 0.26423323154449463, 0.2643430531024933, 0.2615427076816559, 0.25976940989494324, 0.25741738080978394, 0.254208505153656, 0.25165438652038574, 0.24865490198135376, 0.2502996325492859, 0.25001853704452515, 0.24635517597198486, 0.24448728561401367, 0.24485692381858826, 0.24362216889858246, 0.2427578568458557, 0.23712417483329773, 0.2408786416053772, 0.23573870956897736, 0.23709790408611298, 0.23993323743343353, 0.23836208879947662, 0.23428528010845184, 0.2294967919588089, 0.22983619570732117, 0.22967156767845154, 0.22695590555667877, 0.22640711069107056, 0.2295122891664505, 0.22616145014762878, 0.2236902266740799, 0.2223636358976364, 0.22067254781723022, 0.21849052608013153, 0.21751847863197327, 0.21746385097503662, 0.2181302011013031, 0.2152853012084961, 0.21750599145889282, 0.21354013681411743, 0.20897924900054932, 0.208303302526474, 0.20929685235023499, 0.20597529411315918, 0.20737554132938385, 0.20373572409152985, 0.2032967209815979, 0.20246367156505585, 0.2010326385498047, 0.2001381814479828, 0.1981687694787979, 0.1955031305551529, 0.19800841808319092, 0.19420292973518372, 0.19352956116199493, 0.19474069774150848, 0.19165191054344177, 0.18716849386692047, 0.18619617819786072, 0.18578198552131653, 0.1854800432920456, 0.18511445820331573, 0.1827334463596344, 0.1799621880054474, 0.17746004462242126, 0.1764606237411499, 0.17720700800418854, 0.17366017401218414, 0.17396079003810883, 0.17453789710998535, 0.17137180268764496, 0.16876845061779022, 0.17039285600185394, 0.16757015883922577, 0.16786238551139832, 0.1644705832004547, 0.16410018503665924, 0.1627679467201233, 0.15744563937187195, 0.15752939879894257, 0.15644492208957672, 0.1577921211719513, 0.15587438642978668, 0.15186575055122375, 0.14900392293930054, 0.14813949167728424, 0.14730782806873322, 0.14851723611354828, 0.14482232928276062, 0.14344872534275055, 0.1438867598772049, 0.14014555513858795, 0.1401372104883194, 0.1373414397239685, 0.13994580507278442, 0.1359848827123642, 0.13470734655857086, 0.13448725640773773, 0.1315300017595291, 0.13138447701931, 0.12866978347301483, 0.12817585468292236, 0.12995438277721405, 0.12779732048511505, 0.12675096094608307, 0.1220589280128479, 0.12350162118673325, 0.12097416818141937, 0.12093132734298706, 0.11954672634601593, 0.11786974221467972, 0.11874140053987503, 0.11854653805494308, 0.113417848944664, 0.11208613216876984, 0.11256955564022064, 0.10997532308101654, 0.10996589809656143, 0.11011335998773575, 0.11105111986398697, 0.10720553249120712, 0.10694126039743423, 0.1059843897819519, 0.10365575551986694, 0.10260380804538727, 0.10444452613592148, 0.10109841823577881, 0.10370109230279922, 0.10072172433137894, 0.09831641614437103, 0.10160912573337555, 0.09870351850986481, 0.0963139608502388, 0.09838300198316574, 0.09583896398544312, 0.09785208106040955, 0.09424889832735062, 0.09464241564273834, 0.09388169646263123, 0.09171497821807861, 0.09351935982704163, 0.08980677276849747, 0.09069573134183884, 0.08682166039943695, 0.08865947276353836, 0.08701248466968536, 0.08701814711093903, 0.08492641150951385, 0.08550078421831131, 0.08891478180885315, 0.08555015921592712, 0.0842699334025383, 0.08127027750015259, 0.08044112473726273, 0.08202211558818817, 0.0811934694647789, 0.07956365495920181, 0.08094418793916702, 0.07847633212804794, 0.08139478415250778, 0.07785153388977051, 0.07912234961986542, 0.07610428333282471, 0.07490567117929459, 0.07600965350866318, 0.07858626544475555, 0.07503446191549301, 0.07439398020505905, 0.07360077649354935, 0.07204148918390274, 0.07397647947072983, 0.07386957108974457, 0.07288780063390732, 0.07030027359724045, 0.07094009220600128, 0.07251948118209839, 0.07087844610214233, 0.06970676779747009, 0.06964688748121262, 0.0689229816198349, 0.0692940428853035, 0.06831037253141403, 0.06631128489971161, 0.06789091229438782, 0.06812052428722382, 0.06608685106039047, 0.06659407913684845, 0.06667501479387283, 0.0653403177857399, 0.067848801612854, 0.0665062889456749, 0.06591302901506424, 0.06424040347337723, 0.06418193876743317, 0.0645691454410553, 0.06343791633844376, 0.0632179006934166, 0.06116625294089317, 0.06155477091670036, 0.06192734092473984, 0.06102235987782478, 0.062190767377614975, 0.06199922412633896, 0.06252884864807129, 0.05955301970243454, 0.0603368803858757, 0.06087331473827362, 0.059723205864429474, 0.059257637709379196, 0.06040303036570549, 0.05868086218833923, 0.05781141668558121, 0.05903475359082222, 0.05904899537563324, 0.06044241413474083, 0.05763182044029236, 0.0585218220949173, 0.05690586194396019, 0.056717172265052795, 0.055173326283693314, 0.0574188269674778, 0.05521227419376373, 0.05331166088581085, 0.056418173015117645, 0.05518007650971413, 0.055835310369729996, 0.056477099657058716, 0.054880984127521515, 0.05377132073044777, 0.05668359622359276, 0.055929314345121384, 0.05297328159213066, 0.054576996713876724, 0.053828462958335876, 0.05546245723962784, 0.05207880958914757]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:42 2016", "state": "available"}], "summary": "89aa5e00e7a449728dfa804c4fb4c3b5"}