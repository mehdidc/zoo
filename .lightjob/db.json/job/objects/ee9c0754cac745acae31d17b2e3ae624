{"content": {"hp_model": {"f0": 32, "f1": 16, "f2": 64, "f3": 64, "nonlin": "rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.015002501277178377, 0.021993491264280872, 0.015336665294021213, 0.013133294565651947, 0.010792055414651194, 0.012007427387990429, 0.013688679858921048, 0.011664980467668864, 0.012123867018610531, 0.013076238892228087, 0.020192458434581975, 0.015870556153312468, 0.00825907240978467, 0.011697274842643225, 0.007720432320779287, 0.012941395699100793, 0.01038742398760054, 0.01227252641624788, 0.012910493892529863, 0.013845229755919706, 0.012777154415913967, 0.01186262621193584, 0.014062012428802631, 0.013084258761221036, 0.01319062852247935, 0.016352532320268186, 0.013131698455647423, 0.011253410590436635, 0.017076629946741743, 0.012785469914328542, 0.016719528475762135, 0.012991036951399693, 0.0144336967218389, 0.012755064722495305, 0.010612409624141228, 0.014624961484130686, 0.012593949753777508, 0.010994286461168025, 0.014082043921104527, 0.01623252783884961, 0.014273419023844398, 0.015343897657310928, 0.013763177609757316, 0.01002476288981092, 0.01445669026724174, 0.012955064703277593, 0.01275673372993226, 0.01476147230059554, 0.013519885717550816, 0.016255114805150604, 0.014256764429163906, 0.014087121948398559, 0.014712929538513022, 0.015909926978168277, 0.012358168055511412, 0.016413170058255395, 0.015618067607398759, 0.01019096449356436, 0.013932832476181049, 0.015559170345916461, 0.014062723680462364, 0.01693200761470704, 0.01457721823082173, 0.012511414788860923, 0.01427791005216053, 0.011638900629161664, 0.01339983148923952, 0.014139971385152198, 0.014170145281714057, 0.01451009580149222, 0.015772182105778295, 0.016773678655276694, 0.013955494458544989, 0.014310835128030622, 0.012701019313307261, 0.014100615348178499, 0.011374810186462962, 0.01408099347135194, 0.011153635687728565, 0.013795020834504545, 0.015504000290984582, 0.015433705187420188, 0.014963554052886878, 0.013902697403744444, 0.013381767090154656, 0.014095342381654575, 0.013138382359077785, 0.012610748868984476, 0.013280277979319315, 0.011343436876165843, 0.013820401332693381, 0.014350056498935724, 0.012430805475760922, 0.013699401817939827, 0.012010035644804741, 0.015900980787737076, 0.012182801434418564, 0.012214171931540611, 0.012466304510240804, 0.009778652508243946, 0.011965286779880437, 0.010367974015965519, 0.012170357458548288, 0.012367438923381947, 0.010683989203783861, 0.012102355418633179, 0.014343872070521731, 0.01037001720951035, 0.010448062767936812, 0.010660323429696418, 0.00944529880595036, 0.011846100945359319, 0.009137473572161478, 0.008997405052114157, 0.010372873084014888, 0.009802583597739898, 0.012803286039221146, 0.01046541961905532, 0.011597683692432924, 0.011545861753655845, 0.009058858195564165, 0.00826321813578291, 0.009526919900502485, 0.011031929866676582, 0.011921242525381858, 0.010229011213855737, 0.00976574183790218, 0.011062721738021149, 0.012104876301553035, 0.010656866985039438, 0.010052677380155103, 0.011892253820785915, 0.010246307527859433, 0.009754361243360071, 0.010464334574090739, 0.011366958136058432, 0.009955796964556616, 0.009816277011855085, 0.01089824886466989, 0.010718401500147579, 0.011547346632819373, 0.010343642219741078, 0.01151284488622716, 0.010196125648451838, 0.009190314690944699, 0.010808406022059461, 0.009922873079248313, 0.010982926222972126, 0.01094526190656608, 0.00987817935619973, 0.008808744164138462, 0.009765315677363814, 0.011005914841133526, 0.0097283843664589, 0.010217350529079681, 0.009530688097698975, 0.010792927745014483, 0.011656287238596698, 0.009791588569562524, 0.010408863198202034, 0.011492856608954891, 0.01048695349024357, 0.009557742333422858, 0.010609983871447725, 0.010116881301905554, 0.010192550305615411, 0.00988757007133631, 0.009658514014558472, 0.012200055886358413, 0.011689826099717594, 0.010433828622528823, 0.010894635512856243, 0.01144792004870638, 0.011178719910600661, 0.011232991968465715, 0.01115443465526676, 0.009834764768460324, 0.009995795858659593, 0.010483494768122456, 0.011214660988018764, 0.011309392540711964, 0.010778454275227613, 0.010981038043499968, 0.01111778216147811, 0.012022895189533303, 0.010349645914586305, 0.011193381254919662, 0.010885004377202528, 0.010430176839948034, 0.010573911295110927, 0.009857806286910903, 0.010580157639417793, 0.010657313207843064, 0.010229472928519926, 0.010465429229866286, 0.011018142292789104, 0.0100170388225352, 0.009991098016698103, 0.010400943041682757, 0.00970599179176845, 0.011073250042353374, 0.009864793773675046], "moving_avg_accuracy_train": [0.03255511143410852, 0.07456137960271317, 0.12052391334440751, 0.16569559714147283, 0.20387197256252304, 0.25044255420877165, 0.2918609291404231, 0.3298862005443948, 0.37541122280492395, 0.414412737625299, 0.44924431160409434, 0.47896061791599936, 0.5168382322675795, 0.5513160245470046, 0.5808119801164828, 0.609078481613417, 0.6364037042058129, 0.6600783672962136, 0.6834335515441209, 0.7027260283387675, 0.7223534116621444, 0.7413849918114522, 0.7577882198589891, 0.7728720316863054, 0.7874517463725106, 0.8002526551031997, 0.8129242413774403, 0.8250053594254659, 0.8348251093067934, 0.843500340076234, 0.8535537089329055, 0.8632808285515658, 0.8710587458059976, 0.8777938404195194, 0.8865081319728054, 0.8934837138648106, 0.9000524172176245, 0.9058782557780234, 0.9100752656157342, 0.91559865307917, 0.9206138435748245, 0.9244695700054557, 0.9278816311216045, 0.9318592581130339, 0.936224950605302, 0.9398541296519146, 0.9428111820510181, 0.9461305102744877, 0.9464140099518654, 0.9497916182197834, 0.9528314296120908, 0.9555370689794623, 0.9577861325053348, 0.9604636204452776, 0.9626454950078926, 0.9648486724416272, 0.9665525503236642, 0.9683301449936788, 0.969948581387168, 0.9717399955698798, 0.9732569372331299, 0.9745617308610073, 0.9760011120903828, 0.9773105060896778, 0.9786819480402338, 0.9797186081469247, 0.9807422830465179, 0.9818054245335328, 0.982650644728989, 0.9836089805537092, 0.9844505564566717, 0.9852056856693471, 0.98601314909646, 0.9866747620141949, 0.9874236734615849, 0.9881093195082835, 0.9888194069026932, 0.9893306023731381, 0.9897767274036815, 0.9902944973716467, 0.9907976927237677, 0.9911622128859148, 0.9915460846032758, 0.9920380535239006, 0.9924645495107962, 0.9928483958990022, 0.9931310786305305, 0.9934691984460489, 0.9937572302383487, 0.9941141151014186, 0.9943423055258005, 0.9946034804791728, 0.9949036421038746, 0.9951482109292014, 0.9953520468303289, 0.995533173992534, 0.9957403662658997, 0.9958826614845478, 0.9960804816456169, 0.9962724706834362, 0.9964359602222355, 0.9965993768488215, 0.996799930235368, 0.9969362504558787, 0.9970984661841004, 0.9971723807264047, 0.9973179588740023, 0.9974722306949354, 0.9975808483992513, 0.9976414019521833, 0.9977400779772031, 0.9978521378878161, 0.9979529918073679, 0.9980530609302024, 0.9981338225455155, 0.998222784040964, 0.9983028493868675, 0.9983702579005618, 0.9984588273486008, 0.9985199386613598, 0.9985772639916524, 0.9986381573841537, 0.9986813356933574, 0.9987434476597359, 0.99878307238781, 0.9988419861311718, 0.9988764073097213, 0.9989120366680349, 0.9989464282393266, 0.9989750555046796, 0.9990124457875449, 0.9990507473397428, 0.9990921941831494, 0.9991155454493583, 0.9991226106960891, 0.9991336197157659, 0.9991551535775227, 0.9991861597971514, 0.9992140653948173, 0.9992345301350974, 0.9992575986989687, 0.9992737101088337, 0.9993068115681885, 0.9993063759470839, 0.9993129593345185, 0.9993188843832095, 0.9993404929686981, 0.9993576155468283, 0.9993707007183359, 0.9993801522238832, 0.9993886585788758, 0.9993986394471787, 0.9994122725262703, 0.9994268674462623, 0.9994539537671122, 0.9994667057118296, 0.9994781824620753, 0.9994978121325345, 0.9994992027942811, 0.999500454389853, 0.9995085562722963, 0.9995181731153048, 0.9995152025299647, 0.9995102038543492, 0.9995173307903429, 0.9995283953303562, 0.9995290528211301, 0.9995273194140172, 0.9995304096452344, 0.9995355160021395, 0.9995354614257351, 0.9995330871581617, 0.9995286251685359, 0.9995315848243014, 0.9995412239609188, 0.999547574035065, 0.9995579393994156, 0.9995695933761407, 0.9995754316575742, 0.9995853364084835, 0.9996012261307304, 0.9996062262855144, 0.9996177018712487, 0.9996187293031714, 0.99962895458714, 0.9996265315986641, 0.9996359766530833, 0.9996468023508703, 0.9996588706276881, 0.9996697320768241, 0.9996748570834273, 0.9996771444405608, 0.9996745527643618, 0.9996768705534018, 0.9996836068611569, 0.9996873443893268, 0.999695358462299, 0.9997002459791643, 0.9997069698931527, 0.999708371118123, 0.999711957369406, 0.9997105346979416], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 640752000, "moving_var_accuracy_train": [0.009538517524385011, 0.02446540486102107, 0.04103185494652649, 0.05529299860543234, 0.0628806195074898, 0.07611192923056999, 0.08394007234532266, 0.08855935649890179, 0.09835616971540666, 0.10221061616842142, 0.1029087014641419, 0.10056536106513443, 0.10342124797932431, 0.10377758662556058, 0.10122993051761423, 0.09829789342773845, 0.09518811419248042, 0.09071370982522818, 0.08655152052398861, 0.08124616541943754, 0.07658865646259873, 0.07218960020315464, 0.06739223319625469, 0.0627007022898062, 0.05834374478380591, 0.053984139684408225, 0.05003084760431715, 0.04634134356349841, 0.04257505659673506, 0.038994887597189, 0.036005030865784736, 0.03325607948388781, 0.030474935506850126, 0.02783569545124291, 0.025735575801599693, 0.023599946906028357, 0.021628282989060936, 0.019770918244541316, 0.017952360444287756, 0.01643169468150014, 0.01501489443471945, 0.013647204628018321, 0.0123872636147595, 0.011290930901630082, 0.010333371249900483, 0.009418572589881786, 0.008555412760913, 0.007799032943517798, 0.007019852997769678, 0.006420541836496278, 0.005861651732553865, 0.005341370918774916, 0.004852758407588116, 0.004432003041846151, 0.004031647927124417, 0.00367216905165263, 0.0033310809450194205, 0.0030264114358152555, 0.002747344319471665, 0.0025014923704906864, 0.002272053141528954, 0.0020601702050782058, 0.001872799549481691, 0.001700950208342031, 0.0015477828647215324, 0.0014026765558406188, 0.0012718400929570722, 0.0011548285120540746, 0.001045775235457931, 0.0009494633798886163, 0.0008608912919037778, 0.000779934143863922, 0.0007078087041526541, 0.0006409674186136126, 0.000581918491956538, 0.0005279576372730656, 0.0004796998905150556, 0.0004340817887445812, 0.0003924648577560187, 0.00035563114363795686, 0.00032234687933572716, 0.0002913080659396598, 0.0002635034768042003, 0.00023933142989352688, 0.00021703537634571704, 0.0001966578811587951, 0.00017771127878325459, 0.00016096907599174478, 0.00014561882921294963, 0.00013220324754105046, 0.00011945156061496198, 0.00010812031575988717, 9.811915719239089e-05, 8.884556666604752e-05, 8.033495167073884e-05, 7.259671994366157e-05, 6.572340569257722e-05, 5.9333296486570554e-05, 5.375216218304188e-05, 4.870868408052259e-05, 4.407837513614142e-05, 3.991088256712994e-05, 3.628178925811408e-05, 3.282085915498374e-05, 2.9775598721827693e-05, 2.6847209085721344e-05, 2.435322515067081e-05, 2.2132100788209878e-05, 2.0025070960606767e-05, 1.8055564459500295e-05, 1.6337640634773572e-05, 1.4816893383395612e-05, 1.3426747662856561e-05, 1.2174197360674972e-05, 1.1015479571179298e-05, 9.985158943113184e-06, 9.044337185333842e-06, 8.180798636266606e-06, 7.433319696773433e-06, 6.723599060020206e-06, 6.080814895456578e-06, 5.5061054531639185e-06, 4.97227420531872e-06, 4.509767852093546e-06, 4.0729221385586555e-06, 3.6968673871149603e-06, 3.337844006198083e-06, 3.015484666142814e-06, 2.7245812211117724e-06, 2.4594987818949437e-06, 2.2261312029802298e-06, 2.0167211627891205e-06, 1.830509613965608e-06, 1.6523661872710646e-06, 1.4875788279462662e-06, 1.3399117317798374e-06, 1.2100939234212813e-06, 1.097737001980111e-06, 9.949718032118742e-07, 8.992438732433038e-07, 8.141089136706932e-07, 7.350342200541775e-07, 6.7139215755147e-07, 6.042546496880437e-07, 5.442192536302496e-07, 4.901132840851425e-07, 4.45304334377981e-07, 4.0341254507660233e-07, 3.646122859894031e-07, 3.2895503600446995e-07, 2.9671075508136927e-07, 2.6793623916195044e-07, 2.428153628554301e-07, 2.2045093177604768e-07, 2.0500885759312502e-07, 1.8597148068048077e-07, 1.6855977477823235e-07, 1.5517171296141447e-07, 1.3967194712611286e-07, 1.2571885083678176e-07, 1.1373773024522436e-07, 1.0319631024574674e-07, 9.295609861653198e-08, 8.388536957606139e-08, 7.595397156837333e-08, 6.946039082289496e-08, 6.251824238766535e-08, 5.629346045087393e-08, 5.075006016657575e-08, 4.590972807750118e-08, 4.1318782077006344e-08, 3.72376381879016e-08, 3.369305853188914e-08, 3.040258873895013e-08, 2.8198546457629194e-08, 2.5741602786827862e-08, 2.4134409511238902e-08, 2.2943305121697935e-08, 2.095574438040382e-08, 1.9743106757530328e-08, 2.0041145539524763e-08, 1.8262044916357058e-08, 1.7621042036222335e-08, 1.586843837980308e-08, 1.5222602431963667e-08, 1.3753180047156639e-08, 1.3180743519293212e-08, 1.2917430760522187e-08, 1.2936477432614629e-08, 1.2704569385349773e-08, 1.1670503680970207e-08, 1.0550541336776922e-08, 9.555938272780368e-09, 8.64869375980808e-09, 8.192224963364097e-09, 7.498724518423342e-09, 7.326880356997719e-09, 6.809182711278555e-09, 6.535163614053575e-09, 5.899318135408436e-09, 5.4251371062462555e-09, 4.900839342482519e-09], "duration": 114266.67833, "accuracy_train": [0.32555111434108525, 0.4526177931201551, 0.5341867170196567, 0.5722407513150609, 0.5474593513519749, 0.6695777890250092, 0.6646263035252861, 0.6721136431801403, 0.7851364231496861, 0.7654263710086747, 0.7627284774132521, 0.7464073747231451, 0.8577367614318014, 0.8616161550618309, 0.8462755802417867, 0.863476995085825, 0.8823307075373754, 0.8731503351098191, 0.8936302097752861, 0.8763583194905868, 0.898999861572536, 0.9126692131552234, 0.9054172722868217, 0.9086263381321521, 0.9186691785483574, 0.9154608336794019, 0.9269685178456073, 0.9337354218576966, 0.9232028582387413, 0.9215774170011997, 0.9440340286429494, 0.9508249051195091, 0.9410600010958842, 0.9384096919412146, 0.9649367559523809, 0.9562639508928571, 0.9591707473929494, 0.9583108028216132, 0.9478483541551311, 0.9653091402500923, 0.9657505580357143, 0.959171107881137, 0.9585901811669435, 0.9676579010358989, 0.9755161830357143, 0.9725167410714286, 0.9694246536429494, 0.9760044642857143, 0.9489655070482651, 0.9801900926310447, 0.9801897321428571, 0.9798878232858066, 0.9780277042381875, 0.9845610119047619, 0.9822823660714286, 0.9846772693452381, 0.9818874512619971, 0.9843284970238095, 0.9845145089285714, 0.9878627232142857, 0.9869094122023809, 0.9863048735119048, 0.9889555431547619, 0.9890950520833334, 0.9910249255952381, 0.9890485491071429, 0.9899553571428571, 0.9913736979166666, 0.9902576264880952, 0.9922340029761905, 0.9920247395833334, 0.9920018485834257, 0.9932803199404762, 0.9926292782738095, 0.9941638764880952, 0.9942801339285714, 0.9952101934523809, 0.9939313616071429, 0.9937918526785714, 0.9949544270833334, 0.9953264508928571, 0.9944428943452381, 0.9950009300595238, 0.9964657738095238, 0.9963030133928571, 0.9963030133928571, 0.9956752232142857, 0.9965122767857143, 0.9963495163690477, 0.9973260788690477, 0.9963960193452381, 0.9969540550595238, 0.9976050967261905, 0.9973493303571429, 0.9971865699404762, 0.9971633184523809, 0.9976050967261905, 0.9971633184523809, 0.9978608630952381, 0.9980003720238095, 0.9979073660714286, 0.9980701264880952, 0.9986049107142857, 0.9981631324404762, 0.9985584077380952, 0.9978376116071429, 0.9986281622023809, 0.9988606770833334, 0.9985584077380952, 0.9981863839285714, 0.9986281622023809, 0.9988606770833334, 0.9988606770833334, 0.9989536830357143, 0.9988606770833334, 0.9990234375, 0.9990234375, 0.9989769345238095, 0.9992559523809523, 0.9990699404761905, 0.9990931919642857, 0.9991861979166666, 0.9990699404761905, 0.9993024553571429, 0.9991396949404762, 0.9993722098214286, 0.9991861979166666, 0.9992327008928571, 0.9992559523809523, 0.9992327008928571, 0.9993489583333334, 0.9993954613095238, 0.9994652157738095, 0.9993257068452381, 0.9991861979166666, 0.9992327008928571, 0.9993489583333334, 0.9994652157738095, 0.9994652157738095, 0.9994187127976191, 0.9994652157738095, 0.9994187127976191, 0.9996047247023809, 0.9993024553571429, 0.9993722098214286, 0.9993722098214286, 0.9995349702380952, 0.99951171875, 0.9994884672619048, 0.9994652157738095, 0.9994652157738095, 0.9994884672619048, 0.9995349702380952, 0.9995582217261905, 0.9996977306547619, 0.9995814732142857, 0.9995814732142857, 0.9996744791666666, 0.99951171875, 0.99951171875, 0.9995814732142857, 0.9996047247023809, 0.9994884672619048, 0.9994652157738095, 0.9995814732142857, 0.9996279761904762, 0.9995349702380952, 0.99951171875, 0.9995582217261905, 0.9995814732142857, 0.9995349702380952, 0.99951171875, 0.9994884672619048, 0.9995582217261905, 0.9996279761904762, 0.9996047247023809, 0.9996512276785714, 0.9996744791666666, 0.9996279761904762, 0.9996744791666666, 0.9997442336309523, 0.9996512276785714, 0.9997209821428571, 0.9996279761904762, 0.9997209821428571, 0.9996047247023809, 0.9997209821428571, 0.9997442336309523, 0.9997674851190477, 0.9997674851190477, 0.9997209821428571, 0.9996977306547619, 0.9996512276785714, 0.9996977306547619, 0.9997442336309523, 0.9997209821428571, 0.9997674851190477, 0.9997442336309523, 0.9997674851190477, 0.9997209821428571, 0.9997442336309523, 0.9996977306547619], "end": "2016-01-25 15:42:45.953000", "learning_rate_per_epoch": [0.0004186807491350919, 0.00040753575740382075, 0.00039668745012022555, 0.00038612791104242206, 0.0003758494567591697, 0.0003658446075860411, 0.00035610608756542206, 0.00034662679536268115, 0.0003373998333700001, 0.0003284184786025435, 0.0003196762118022889, 0.00031116665923036635, 0.00030288362177088857, 0.000294821074930951, 0.0002869731397368014, 0.0002793341118376702, 0.0002718984324019402, 0.0002646606881171465, 0.00025761561118997633, 0.00025075807934626937, 0.0002440830721752718, 0.00023758575844112784, 0.000231261394219473, 0.00022510538110509515, 0.00021911323710810393, 0.00021328059665393084, 0.0002076032105833292, 0.00020207696070428938, 0.0001966978161362931, 0.00019146186241414398, 0.00018636528693605214, 0.00018140437896363437, 0.0001765755150699988, 0.00017187520279549062, 0.00016730000788811594, 0.00016284659795928746, 0.0001585117424838245, 0.00015429226914420724, 0.00015018512203823775, 0.00014618730347137898, 0.00014229590306058526, 0.00013850808318238705, 0.0001348210935248062, 0.00013123225653544068, 0.0001277389528695494, 0.00012433863594196737, 0.00012102883192710578, 0.00011780713248299435, 0.00011467118747532368, 0.00011161871952936053, 0.00010864750947803259, 0.00010575538908597082, 0.00010294025560142472, 0.0001002000572043471, 9.753280028235167e-05, 9.493654943071306e-05, 9.240940562449396e-05, 8.99495353223756e-05, 8.755514136282727e-05, 8.52244847919792e-05, 8.295587031170726e-05, 8.074764627963305e-05, 7.85981974331662e-05, 7.65059667173773e-05, 7.446943345712498e-05, 7.248710608109832e-05, 7.055755122564733e-05, 6.867935735499486e-05, 6.685116386506706e-05, 6.507163197966293e-05, 6.333947385428473e-05, 6.165342347230762e-05, 6.0012251196894795e-05, 5.841476740897633e-05, 5.685980795533396e-05, 5.5346241424558684e-05, 5.387296187109314e-05, 5.243890336714685e-05, 5.1043018174823374e-05, 4.9684287660056725e-05, 4.836172593059018e-05, 4.7074372560018674e-05, 4.582128531183116e-05, 4.460155469132587e-05, 4.341429303167388e-05, 4.225863449391909e-05, 4.113373870495707e-05, 4.003878711955622e-05, 3.897298302035779e-05, 3.793554787989706e-05, 3.692572863656096e-05, 3.5942790418630466e-05, 3.498602018225938e-05, 3.405471579753794e-05, 3.3148204238386825e-05, 3.226582339266315e-05, 3.140692933811806e-05, 3.057089998037554e-05, 2.9757124138996005e-05, 2.8965010642423294e-05, 2.8193982871016487e-05, 2.744347875704989e-05, 2.6712952603702433e-05, 2.6001873266068287e-05, 2.530972233216744e-05, 2.4635995941935107e-05, 2.3980202968232334e-05, 2.334186683583539e-05, 2.2720521883456968e-05, 2.2115717001724988e-05, 2.1527011995203793e-05, 2.0953977582394145e-05, 2.0396197214722633e-05, 1.9853265257552266e-05, 1.932478517119307e-05, 1.88103731488809e-05, 1.830965447879862e-05, 1.7822263544076122e-05, 1.7347847460769117e-05, 1.688606062089093e-05, 1.6436564692412503e-05, 1.59990340762306e-05, 1.5573150449199602e-05, 1.5158604583120905e-05, 1.4755093616258819e-05, 1.436232378182467e-05, 1.3980008588987403e-05, 1.3607870641862974e-05, 1.3245638911030255e-05, 1.2893049643025734e-05, 1.2549845450848807e-05, 1.2215777132951189e-05, 1.1890601854247507e-05, 1.1574082236620598e-05, 1.1265988177910913e-05, 1.0966095942421816e-05, 1.067418634193018e-05, 1.0390047464170493e-05, 1.011347194435075e-05, 9.844258784141857e-06, 9.582211532688234e-06, 9.32714010559721e-06, 9.07885805645492e-06, 8.837185305310413e-06, 8.601945410191547e-06, 8.372967386094388e-06, 8.15008479548851e-06, 7.933134838822298e-06, 7.721960173512343e-06, 7.516407094954047e-06, 7.316325536521617e-06, 7.1215699790627696e-06, 6.931998996151378e-06, 6.747473889845423e-06, 6.567860964423744e-06, 6.393029252649285e-06, 6.222851425263798e-06, 6.057203336240491e-06, 5.89596493227873e-06, 5.7390184338146355e-06, 5.586249699263135e-06, 5.437547770270612e-06, 5.292803962220205e-06, 5.151913228473859e-06, 5.0147727961302735e-06, 4.881283075519605e-06, 4.7513467507087626e-06, 4.624869234248763e-06, 4.501758667174727e-06, 4.381925009511178e-06, 4.2652814045140985e-06, 4.1517428144288715e-06, 4.041226475237636e-06, 3.933651896659285e-06, 3.828940862149466e-06, 3.7270172015269054e-06, 3.6278065635997336e-06, 3.531236870912835e-06, 3.4372378650004975e-06, 3.345741106386413e-06, 3.2566797472100006e-06, 3.169989213347435e-06, 3.0856062949169427e-06, 3.003469601026154e-06, 2.9235193323984277e-06, 2.84569728137285e-06, 2.7699468319042353e-06, 2.6962127321894513e-06, 2.624441549414769e-06, 2.5545807602611603e-06, 2.486579660399002e-06, 2.4203886823670473e-06, 2.355959622946102e-06, 2.2932456431590253e-06, 2.2322010408970527e-06, 2.172781478293473e-06, 2.114943526976276e-06, 2.0586451228155056e-06, 2.0038453385495814e-06, 1.950504383785301e-06, 1.8985833776241634e-06, 1.8480444623492076e-06], "accuracy_valid": [0.31822848032756024, 0.4471494375941265, 0.5310779249811747, 0.5611086925828314, 0.5295013060052711, 0.6488095938441265, 0.6355436394013554, 0.6510892201618976, 0.7442818147590362, 0.725330031061747, 0.7187161732868976, 0.7046089631965362, 0.7983721997364458, 0.7985751600150602, 0.788463914250753, 0.7969882459525602, 0.8076598385730422, 0.801403367375753, 0.8141398602221386, 0.7997355633471386, 0.8172019131212349, 0.8206404720444277, 0.8164694912462349, 0.8169783626694277, 0.8233363140060241, 0.8206610622176205, 0.8289824336408133, 0.8279852809676205, 0.8232951336596386, 0.8230612881212349, 0.8324915874435241, 0.8383509624435241, 0.8341608621987951, 0.8288294780685241, 0.8489313700112951, 0.8373846950301205, 0.8404673381024097, 0.8390642648719879, 0.8298163356551205, 0.8454619258283133, 0.8437426463667168, 0.8324915874435241, 0.8422880977033133, 0.840843844126506, 0.850243258189006, 0.8514036615210843, 0.8448824595256024, 0.8495726068335843, 0.8292971691453314, 0.8512301157756024, 0.8491755106362951, 0.8492872858621988, 0.8457678369728916, 0.8514742564006024, 0.8508741999246988, 0.8506300592996988, 0.851830172251506, 0.8527258447853916, 0.8515963267131024, 0.851586031626506, 0.8562761789344879, 0.849877047251506, 0.8568865304969879, 0.8567850503576807, 0.8589823159826807, 0.8555437570594879, 0.8554010965737951, 0.8569174157567772, 0.8580969385353916, 0.8552996164344879, 0.8609251458960843, 0.8581881235881024, 0.8573439264871988, 0.8595926675451807, 0.8606604150978916, 0.8613825418862951, 0.8604677499058735, 0.8592058664344879, 0.8610986916415663, 0.8580969385353916, 0.8579542780496988, 0.8595720773719879, 0.8596838525978916, 0.8619120034826807, 0.8599485833960843, 0.8610575112951807, 0.8601824289344879, 0.8593176416603916, 0.8604368646460843, 0.8619120034826807, 0.8600500635353916, 0.8617796380835843, 0.8630312264683735, 0.8632444818335843, 0.8604162744728916, 0.8600397684487951, 0.8633768472326807, 0.8617899331701807, 0.8615149072853916, 0.8603353845067772, 0.8614031320594879, 0.8641298592808735, 0.8621561441076807, 0.8620237787085843, 0.8621767342808735, 0.8593073465737951, 0.8612707666603916, 0.8620134836219879, 0.8626444253576807, 0.8610883965549698, 0.8593485269201807, 0.8648313958960843, 0.8644960702183735, 0.8640180840549698, 0.8645975503576807, 0.8611795816076807, 0.8626135400978916, 0.8644857751317772, 0.8632444818335843, 0.8638651284826807, 0.8644960702183735, 0.8646181405308735, 0.8625120599585843, 0.8642210443335843, 0.8644754800451807, 0.8640989740210843, 0.8627562005835843, 0.8639974938817772, 0.8627459054969879, 0.8636415780308735, 0.8634989175451807, 0.8630209313817772, 0.8648416909826807, 0.8644754800451807, 0.8638548333960843, 0.8643431146460843, 0.8652181970067772, 0.8632444818335843, 0.8637430581701807, 0.8622679193335843, 0.8642313394201807, 0.8630106362951807, 0.8643431146460843, 0.8620134836219879, 0.8621458490210843, 0.8634886224585843, 0.8634886224585843, 0.8643431146460843, 0.8626238351844879, 0.8642313394201807, 0.8636312829442772, 0.8663271249058735, 0.8632547769201807, 0.8653299722326807, 0.8640989740210843, 0.8634886224585843, 0.8627459054969879, 0.8623796945594879, 0.8638651284826807, 0.8629900461219879, 0.8644548898719879, 0.8642313394201807, 0.8650961266942772, 0.8632444818335843, 0.8645975503576807, 0.8643534097326807, 0.8643637048192772, 0.8642313394201807, 0.8638548333960843, 0.8632444818335843, 0.8647093255835843, 0.8630003412085843, 0.8641092691076807, 0.8639769037085843, 0.8653196771460843, 0.8640989740210843, 0.8641092691076807, 0.8644754800451807, 0.8639871987951807, 0.8645872552710843, 0.8642210443335843, 0.8642313394201807, 0.8651976068335843, 0.8649534662085843, 0.8644651849585843, 0.8642313394201807, 0.8645872552710843, 0.8653299722326807, 0.8645975503576807, 0.8653299722326807, 0.8648313958960843, 0.8655741128576807, 0.8647196206701807, 0.8650858316076807, 0.8647196206701807, 0.8649637612951807, 0.8634989175451807, 0.8641092691076807, 0.8643534097326807, 0.8638651284826807, 0.8645975503576807, 0.8637430581701807], "accuracy_test": 0.8643056441326531, "start": "2016-01-24 07:58:19.275000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0], "accuracy_train_last": 0.9996977306547619, "batch_size_eval": 1024, "accuracy_train_std": [0.01279044096482461, 0.015991018441056905, 0.015910663815195066, 0.018111399618324, 0.017861691429031518, 0.01612025527209111, 0.016879515425725754, 0.016361713755330406, 0.019004779090938874, 0.01456289527588068, 0.02078839648969174, 0.017802938600645572, 0.018758794953735638, 0.01896130279279888, 0.017360210282701037, 0.018977267002556173, 0.019592618209675096, 0.016145642256110468, 0.017577486797646796, 0.01733414691315115, 0.01842320213142082, 0.01733266199452448, 0.017203587706357788, 0.016129523363377776, 0.01645820503291843, 0.015936830460848893, 0.01531037777085681, 0.014863518446143142, 0.017523583696232237, 0.014152463575984616, 0.013807959210793365, 0.012837517750943812, 0.011607502605812003, 0.013417459513407047, 0.010454692333017379, 0.012351793634478877, 0.010439577787209496, 0.011158335952802531, 0.011350386871116728, 0.010074085734982365, 0.010377485845549945, 0.011222704017153796, 0.010487099610490068, 0.009674767458413384, 0.008455649088854027, 0.009045157611619729, 0.0094967224274853, 0.008384861872346504, 0.011584709228426408, 0.007018091978490394, 0.0070815956087681665, 0.007654975627567489, 0.007457303864698145, 0.0063726047010047235, 0.0063380673502235205, 0.0066387375836883256, 0.006998294682640319, 0.005843668420871121, 0.006703690723623864, 0.00531966405271697, 0.0051236172174556616, 0.005660245144274474, 0.005195914273266587, 0.005340001773869086, 0.0045418435787048265, 0.0052913868731195865, 0.004491811994435733, 0.004319836147594072, 0.004695476685317492, 0.0035655988312328075, 0.0038530879923037564, 0.003606010603330178, 0.0034110020208871147, 0.003470396569654553, 0.0033592566805433783, 0.0031361077567126795, 0.0026685578004842954, 0.0032145129734010465, 0.0038642965916474725, 0.0033282131652729824, 0.003221568984027759, 0.003084395564939888, 0.003183756227768952, 0.0024664119431131175, 0.0024160298943201133, 0.0024533350160531493, 0.002705178491861791, 0.0025137428853682256, 0.002110256690955482, 0.0023543782207170257, 0.0021887296502025124, 0.0024128950998098643, 0.001932113319432871, 0.001909455465741054, 0.0023557555871886165, 0.002118311379079311, 0.0016941705014831539, 0.0016888969305556992, 0.0017859806891435566, 0.0015940417132628917, 0.002016857183984509, 0.0016297618032807904, 0.0012987574445469624, 0.0014168152656791409, 0.0013517896069334668, 0.0017251601377929263, 0.001348786738492669, 0.001063739698074579, 0.001282841724714818, 0.0014066669886769136, 0.0011487741189579264, 0.0011259579625003025, 0.0011259579625003025, 0.0012855781200501381, 0.0013631401792317571, 0.0012425957232038121, 0.0008786478621508177, 0.001020949516373705, 0.0009242283721331748, 0.0010645017803440845, 0.0009139343381246999, 0.0009268569302452915, 0.0009029244716772507, 0.0008599907928249131, 0.0009338302468388767, 0.0007010236900893479, 0.0008501909086115245, 0.0008923847220235997, 0.0008737115991469465, 0.0008124723404788015, 0.0008161239199583447, 0.0008240348375497279, 0.0007743129559778047, 0.0008386667037235601, 0.0008764916678278815, 0.0009419005348088306, 0.0008434875905513884, 0.0006103238815293289, 0.0007743129559778047, 0.0007701123122060637, 0.000713256215131044, 0.0007990532123065282, 0.0006414208623574089, 0.000859990792824913, 0.0007630598531167197, 0.0007922584187779401, 0.0006818674804043519, 0.0006120929399687585, 0.0006477115084256009, 0.0006464582763419756, 0.0006464582763419756, 0.0007143922756574227, 0.0007143922756574227, 0.0006464582763419756, 0.0005428114550748116, 0.0005695428159373089, 0.0006786885937186768, 0.0005502306345888779, 0.0006120929399687585, 0.0006481287139493282, 0.0006081053213644913, 0.0005662107357647843, 0.0006477115084256009, 0.000713256215131044, 0.0006081053213644913, 0.0005618975992184975, 0.0006116511550393372, 0.0006822637998103241, 0.0006103238815293289, 0.0006081053213644913, 0.0006818674804043519, 0.0006120929399687585, 0.0006818674804043519, 0.000713256215131044, 0.0006376166853051566, 0.0006049856692363137, 0.0005565805991250848, 0.0006273594476949424, 0.0006009508920814772, 0.000550230634588878, 0.0005245774819883965, 0.0005959824041097608, 0.0005342785199533136, 0.0005618975992184976, 0.000489939255620012, 0.0006414208623574089, 0.000489939255620012, 0.0005245774819883965, 0.0005136421603974731, 0.0004673491267262319, 0.0005342785199533136, 0.0004992306211305778, 0.0005141681637717629, 0.0004992306211305778, 0.0004793417068472913, 0.0005342785199533136, 0.0004673491267262319, 0.0004793417068472913, 0.0004673491267262319, 0.000489939255620012, 0.0004793417068472913, 0.0004992306211305778], "accuracy_test_std": 0.005773138313934193, "error_valid": [0.6817715196724398, 0.5528505624058735, 0.4689220750188253, 0.43889130741716864, 0.4704986939947289, 0.3511904061558735, 0.3644563605986446, 0.34891077983810237, 0.2557181852409638, 0.274669968938253, 0.28128382671310237, 0.2953910368034638, 0.2016278002635542, 0.20142483998493976, 0.21153608574924698, 0.20301175404743976, 0.19234016142695776, 0.19859663262424698, 0.18586013977786142, 0.20026443665286142, 0.1827980868787651, 0.1793595279555723, 0.1835305087537651, 0.1830216373305723, 0.17666368599397586, 0.17933893778237953, 0.17101756635918675, 0.17201471903237953, 0.17670486634036142, 0.1769387118787651, 0.16750841255647586, 0.16164903755647586, 0.16583913780120485, 0.17117052193147586, 0.15106862998870485, 0.16261530496987953, 0.1595326618975903, 0.16093573512801207, 0.17018366434487953, 0.15453807417168675, 0.1562573536332832, 0.16750841255647586, 0.15771190229668675, 0.15915615587349397, 0.14975674181099397, 0.14859633847891573, 0.15511754047439763, 0.15042739316641573, 0.17070283085466864, 0.14876988422439763, 0.15082448936370485, 0.15071271413780118, 0.1542321630271084, 0.14852574359939763, 0.14912580007530118, 0.14936994070030118, 0.14816982774849397, 0.1472741552146084, 0.14840367328689763, 0.14841396837349397, 0.14372382106551207, 0.15012295274849397, 0.14311346950301207, 0.1432149496423193, 0.1410176840173193, 0.14445624294051207, 0.14459890342620485, 0.14308258424322284, 0.1419030614646084, 0.14470038356551207, 0.13907485410391573, 0.14181187641189763, 0.14265607351280118, 0.1404073324548193, 0.1393395849021084, 0.13861745811370485, 0.1395322500941265, 0.14079413356551207, 0.13890130835843373, 0.1419030614646084, 0.14204572195030118, 0.14042792262801207, 0.1403161474021084, 0.1380879965173193, 0.14005141660391573, 0.1389424887048193, 0.13981757106551207, 0.1406823583396084, 0.13956313535391573, 0.1380879965173193, 0.1399499364646084, 0.13822036191641573, 0.1369687735316265, 0.13675551816641573, 0.1395837255271084, 0.13996023155120485, 0.1366231527673193, 0.1382100668298193, 0.1384850927146084, 0.13966461549322284, 0.13859686794051207, 0.1358701407191265, 0.1378438558923193, 0.13797622129141573, 0.1378232657191265, 0.14069265342620485, 0.1387292333396084, 0.13798651637801207, 0.1373555746423193, 0.13891160344503017, 0.1406514730798193, 0.13516860410391573, 0.1355039297816265, 0.13598191594503017, 0.1354024496423193, 0.1388204183923193, 0.1373864599021084, 0.13551422486822284, 0.13675551816641573, 0.1361348715173193, 0.1355039297816265, 0.1353818594691265, 0.13748794004141573, 0.13577895566641573, 0.1355245199548193, 0.13590102597891573, 0.13724379941641573, 0.13600250611822284, 0.13725409450301207, 0.1363584219691265, 0.1365010824548193, 0.13697906861822284, 0.1351583090173193, 0.1355245199548193, 0.13614516660391573, 0.13565688535391573, 0.13478180299322284, 0.13675551816641573, 0.1362569418298193, 0.13773208066641573, 0.1357686605798193, 0.1369893637048193, 0.13565688535391573, 0.13798651637801207, 0.13785415097891573, 0.13651137754141573, 0.13651137754141573, 0.13565688535391573, 0.13737616481551207, 0.1357686605798193, 0.13636871705572284, 0.1336728750941265, 0.1367452230798193, 0.1346700277673193, 0.13590102597891573, 0.13651137754141573, 0.13725409450301207, 0.13762030544051207, 0.1361348715173193, 0.13700995387801207, 0.13554511012801207, 0.1357686605798193, 0.13490387330572284, 0.13675551816641573, 0.1354024496423193, 0.1356465902673193, 0.13563629518072284, 0.1357686605798193, 0.13614516660391573, 0.13675551816641573, 0.13529067441641573, 0.13699965879141573, 0.1358907308923193, 0.13602309629141573, 0.13468032285391573, 0.13590102597891573, 0.1358907308923193, 0.1355245199548193, 0.1360128012048193, 0.13541274472891573, 0.13577895566641573, 0.1357686605798193, 0.13480239316641573, 0.13504653379141573, 0.13553481504141573, 0.1357686605798193, 0.13541274472891573, 0.1346700277673193, 0.1354024496423193, 0.1346700277673193, 0.13516860410391573, 0.1344258871423193, 0.1352803793298193, 0.1349141683923193, 0.1352803793298193, 0.1350362387048193, 0.1365010824548193, 0.1358907308923193, 0.1356465902673193, 0.1361348715173193, 0.1354024496423193, 0.1362569418298193], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-3, 3], "momentum": 0.6710828889257737, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.00043013050474831376, "patience_threshold": 1, "do_flip": true, "batch_size": 128, "optimization": "rmsprop", "nb_data_augmentation": 3, "learning_rate_decay_method": "exp", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 7.431555964895843e-08, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.02661929536563722}, "accuracy_valid_max": 0.8663271249058735, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-3, 3)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8637430581701807, "loss_train": [1.5477298498153687, 1.1316434144973755, 0.9445737600326538, 0.827510416507721, 0.7458699345588684, 0.6793400645256042, 0.6303951144218445, 0.5852833390235901, 0.5490877628326416, 0.5164459347724915, 0.4868270754814148, 0.4614468514919281, 0.4362132251262665, 0.4130673408508301, 0.3940640985965729, 0.371488094329834, 0.35658586025238037, 0.33822423219680786, 0.32285603880882263, 0.3068583607673645, 0.2924232482910156, 0.2799617052078247, 0.2654382884502411, 0.25548824667930603, 0.24416470527648926, 0.23327884078025818, 0.2225845456123352, 0.21182158589363098, 0.20309986174106598, 0.194878488779068, 0.18662646412849426, 0.17819339036941528, 0.17189353704452515, 0.16446442902088165, 0.1579437255859375, 0.14981776475906372, 0.14448970556259155, 0.13808265328407288, 0.1354086697101593, 0.12780386209487915, 0.12441480159759521, 0.11888177692890167, 0.1144019216299057, 0.109126515686512, 0.10504530370235443, 0.10133197158575058, 0.09647667407989502, 0.09294518828392029, 0.089349165558815, 0.08649511635303497, 0.0841536670923233, 0.08071243762969971, 0.07892236113548279, 0.07503300160169601, 0.07187319546937943, 0.0695435181260109, 0.06728918850421906, 0.06559306383132935, 0.06319812685251236, 0.0617753267288208, 0.05837588757276535, 0.05821877717971802, 0.05573388561606407, 0.05412270873785019, 0.05266008526086807, 0.051060765981674194, 0.04919368401169777, 0.04831945151090622, 0.04680978134274483, 0.045347653329372406, 0.04297409579157829, 0.0429331474006176, 0.04126012697815895, 0.04087282717227936, 0.039274170994758606, 0.038090165704488754, 0.03721338137984276, 0.03685355931520462, 0.035536397248506546, 0.03535468876361847, 0.03430968523025513, 0.03322891891002655, 0.03231652453541756, 0.031122155487537384, 0.03164953365921974, 0.031112389639019966, 0.03004550002515316, 0.0296900924295187, 0.02847885712981224, 0.0281080175191164, 0.028373850509524345, 0.027181049808859825, 0.02681143954396248, 0.026039913296699524, 0.025158878415822983, 0.02568068727850914, 0.02547762170433998, 0.02421102672815323, 0.023743798956274986, 0.023602839559316635, 0.02350568026304245, 0.023288054391741753, 0.022908523678779602, 0.022565383464097977, 0.022172529250383377, 0.02206280268728733, 0.02146475948393345, 0.02116858772933483, 0.021231912076473236, 0.020839743316173553, 0.0203595869243145, 0.02084125392138958, 0.020383622497320175, 0.019786788150668144, 0.019943885505199432, 0.019405946135520935, 0.01931428723037243, 0.01893303170800209, 0.01879604160785675, 0.018640713766217232, 0.01863887347280979, 0.018084749579429626, 0.0181427039206028, 0.01841716282069683, 0.017511075362563133, 0.017706051468849182, 0.017401354387402534, 0.01774335466325283, 0.017232999205589294, 0.01721026748418808, 0.01690327562391758, 0.016580676659941673, 0.016325542703270912, 0.016522256657481194, 0.016743365675210953, 0.016671374440193176, 0.016706179827451706, 0.016556184738874435, 0.016377346590161324, 0.016356879845261574, 0.015920301899313927, 0.0159700196236372, 0.016037816181778908, 0.015472079627215862, 0.01581995189189911, 0.015412195585668087, 0.015544295310974121, 0.015285998582839966, 0.015276232734322548, 0.015113321132957935, 0.015395566821098328, 0.015251385979354382, 0.014992796815931797, 0.015216563828289509, 0.014936720952391624, 0.01493610069155693, 0.014905308373272419, 0.014708662405610085, 0.01476091705262661, 0.015106325969099998, 0.014822149649262428, 0.014421164989471436, 0.014871303923428059, 0.014057417400181293, 0.01436795387417078, 0.014502570964396, 0.014490225352346897, 0.014264585450291634, 0.01434895396232605, 0.014318583533167839, 0.013785010203719139, 0.014221387915313244, 0.014387328177690506, 0.014340688474476337, 0.013730617240071297, 0.013989935629069805, 0.01395987905561924, 0.014283830299973488, 0.013632073067128658, 0.013986129313707352, 0.014064961113035679, 0.013793956488370895, 0.013641157187521458, 0.01371124666184187, 0.01376237254589796, 0.013751085847616196, 0.013283835723996162, 0.013721279799938202, 0.0139424754306674, 0.013773703016340733, 0.013705282472074032, 0.013840606436133385, 0.014099144376814365, 0.013369000516831875, 0.013541951775550842, 0.013935813680291176, 0.013380764052271843, 0.01350097730755806, 0.013320366851985455, 0.01329532265663147, 0.013254200108349323, 0.01325614470988512], "accuracy_train_first": 0.32555111434108525, "model": "residualv3", "loss_std": [0.3366536796092987, 0.12018553167581558, 0.10154891014099121, 0.09463914483785629, 0.08949116617441177, 0.08554942160844803, 0.08346584439277649, 0.08395233750343323, 0.0796622559428215, 0.07758650928735733, 0.07729195803403854, 0.07633023709058762, 0.07183200865983963, 0.0719248428940773, 0.06848055124282837, 0.06902311742305756, 0.06607935577630997, 0.06252910196781158, 0.06111584231257439, 0.05751876160502434, 0.056742165237665176, 0.0552908331155777, 0.05321522802114487, 0.05388661101460457, 0.052397310733795166, 0.04972723871469498, 0.048032958060503006, 0.044776447117328644, 0.04413580521941185, 0.04210881516337395, 0.04273362085223198, 0.0398985929787159, 0.0410013385117054, 0.037042126059532166, 0.036003999412059784, 0.03480418398976326, 0.03581061586737633, 0.03179715946316719, 0.031151507049798965, 0.030720947310328484, 0.02955888956785202, 0.030223630368709564, 0.02785033918917179, 0.02707655355334282, 0.026688147336244583, 0.026045897975564003, 0.024933762848377228, 0.023312194272875786, 0.023095756769180298, 0.022387078031897545, 0.021058622747659683, 0.021026238799095154, 0.020535573363304138, 0.02032485045492649, 0.01882774755358696, 0.018315482884645462, 0.018134258687496185, 0.01861058734357357, 0.017816629260778427, 0.016077883541584015, 0.015863317996263504, 0.017374664545059204, 0.015296301804482937, 0.015394811518490314, 0.01435072347521782, 0.014324948191642761, 0.01361835841089487, 0.01345454528927803, 0.013880809769034386, 0.01318325288593769, 0.012004045769572258, 0.012365794740617275, 0.011468850076198578, 0.010983656160533428, 0.010958691127598286, 0.010425535030663013, 0.010782734490931034, 0.010855522006750107, 0.010767282918095589, 0.0109904445707798, 0.010665633715689182, 0.010313465259969234, 0.009947065263986588, 0.008657391183078289, 0.009663608856499195, 0.009453409351408482, 0.00921717006713152, 0.008854223415255547, 0.008267124183475971, 0.00848726648837328, 0.009351580403745174, 0.00823109783232212, 0.008439541794359684, 0.008299345150589943, 0.007676522247493267, 0.008164770901203156, 0.008466225117444992, 0.007416532374918461, 0.007526118773967028, 0.00717356288805604, 0.007202641572803259, 0.007316304836422205, 0.007285218220204115, 0.00703701376914978, 0.0069059040397405624, 0.0065393769182264805, 0.006708277389407158, 0.006545679643750191, 0.007085205987095833, 0.006499276030808687, 0.005968895275145769, 0.0065726847387850285, 0.006861152593046427, 0.006037860177457333, 0.006660125683993101, 0.0061201113276183605, 0.0062393918633461, 0.005617499351501465, 0.005719288717955351, 0.005755532067269087, 0.005939633585512638, 0.005595740862190723, 0.005957537330687046, 0.005934805143624544, 0.005654928274452686, 0.00554147083312273, 0.005710500292479992, 0.00563194090500474, 0.005444990936666727, 0.00546888867393136, 0.0053050401620566845, 0.0049288468435406685, 0.00494757667183876, 0.005198902916163206, 0.005143590271472931, 0.005438712891191244, 0.005216384772211313, 0.004673228599131107, 0.005445238668471575, 0.005239192396402359, 0.004873319063335657, 0.004854972008615732, 0.005018885247409344, 0.004777153022587299, 0.004872895311564207, 0.004536065272986889, 0.005177977494895458, 0.004989072680473328, 0.004745182581245899, 0.0051221055909991264, 0.005095766391605139, 0.005128331016749144, 0.005041602533310652, 0.004799955524504185, 0.004595327191054821, 0.004614066798239946, 0.004554553423076868, 0.004307062365114689, 0.0042990283109247684, 0.005108675453811884, 0.004616071004420519, 0.004525928292423487, 0.0048776124604046345, 0.0046914382837712765, 0.004620660096406937, 0.004682747181504965, 0.004389713052660227, 0.0046582333743572235, 0.004374874755740166, 0.004370681010186672, 0.004336897283792496, 0.004684371408075094, 0.00456710159778595, 0.004740734584629536, 0.004265741445124149, 0.0043927994556725025, 0.004404876846820116, 0.004369223024696112, 0.003856242634356022, 0.004212267696857452, 0.004951430018991232, 0.004352813586592674, 0.00409137224778533, 0.004194876179099083, 0.0042869956232607365, 0.004539744928479195, 0.0038845620583742857, 0.004267160315066576, 0.00428410479798913, 0.004444182850420475, 0.0041068061254918575, 0.003916387911885977, 0.004910212941467762, 0.0038241322617977858, 0.0040660579688847065, 0.004503865260630846, 0.004367341753095388, 0.0042307437397539616, 0.0038847296964377165, 0.004167488310486078, 0.00423504738137126, 0.0040690409950912]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:13 2016", "state": "available"}], "summary": "cf3b7a41b3647848f8f49758291bfa40"}