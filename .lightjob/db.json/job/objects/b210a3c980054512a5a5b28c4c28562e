{"content": {"hp_model": {"f0": 32, "f1": 32, "f2": 64, "f3": 16, "nonlin": "very_leaky_rectify", "nbg1": 7, "nbg3": 7, "nbg2": 2, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.01529336674996807, 0.018824631035607286, 0.008467470742899958, 0.01041362453111923, 0.011537723794969604, 0.01060213857415365, 0.012327821035723725, 0.011544659791786333, 0.011668946511222087, 0.009891638230758865, 0.007104377502411509, 0.005546520107208702, 0.009378005348192886, 0.009913726575401582, 0.011255033508794794, 0.011351322156263721, 0.012429311281947584, 0.011908962517785088, 0.012495074517886664, 0.013455146240203126, 0.013448931788255339, 0.012780741493513449, 0.01432840022696363, 0.01180177116326035, 0.012325362949912823, 0.011993830529689437, 0.012455387991271206, 0.014410269468954809, 0.014445921262606048, 0.012764108154150728, 0.014619211097341454, 0.01349119519430434, 0.012473986584022484, 0.01357792938641461, 0.013101216464489513, 0.014377253832171321, 0.012001782585847387, 0.01324928530034671, 0.013009189664382926, 0.012885557218947323, 0.015137308558201449, 0.014259584929187366, 0.01199885219898947, 0.014435668073278072, 0.013711235508978532, 0.012173388176445097, 0.013822399278628968, 0.014001444373505138, 0.012908047310980218, 0.013051524929160197, 0.013302691633647305, 0.012924399091648034, 0.013518246047892557, 0.013730463353965373, 0.01336430627217926, 0.0134386460999406, 0.01273081034507761, 0.013364799587331755, 0.013605082040925132, 0.012327524856039149, 0.012959240482222242, 0.012727763911673646, 0.012390893990212702, 0.012234204752620983, 0.013020225970708234, 0.011972659898560122, 0.012720420536226594, 0.013047508990628655, 0.011419453313362998, 0.012718179408033247, 0.011504116697787089, 0.0121773989479057, 0.014302586105745797, 0.013374135477552429, 0.013010590329903962, 0.012385380996264189, 0.012375649395045535, 0.012619530975728306, 0.012709384625507823, 0.011796016945800925, 0.013600380527006739, 0.013204842768708534, 0.012741889236037451, 0.012180518036786377, 0.012104616749559247, 0.012662293302662365, 0.012348776547943889, 0.011403783091725723, 0.013444275901966628, 0.013975234114198315, 0.013061547201612516, 0.012410721467237288, 0.012470300861568594, 0.012967411526974379, 0.013263663694205206, 0.012828706708871256, 0.012407893761842596, 0.014292509239172716, 0.012592478010198901, 0.013239386848576363, 0.013922042092203744, 0.012653061028175245, 0.013428134831949516, 0.011828497409321808, 0.013378447469447304, 0.013756414246404548, 0.013334503966599994, 0.013451854536582267, 0.013510485959038926, 0.012666738756737553, 0.012137061484502729, 0.014133327497781453, 0.01352323675787475, 0.013044724888986203, 0.014128809112949612, 0.014632299050334148, 0.012673737253222083, 0.012827750069382816, 0.013358400209205812, 0.012882001258993397, 0.013300208152539638, 0.012099573635275486, 0.013911625057340777, 0.013556662751192338, 0.013013880373861642, 0.012550033813346084, 0.011581228909726448, 0.012577277307538947, 0.012784201429573422, 0.012345773993120271, 0.01353495982427092, 0.013882819554013721, 0.01344908240785429, 0.012969432719752254, 0.01363900201336332, 0.011465588721552596, 0.012488657630095143, 0.014163493251480424, 0.013144441915012106, 0.013537429719273571, 0.011767324695253087, 0.01328284758628069, 0.012485516857330457], "moving_avg_accuracy_train": [0.02778298683208748, 0.06047978995074288, 0.09462348612985647, 0.12854709852070065, 0.1616982876973312, 0.193810318152635, 0.22461522614449592, 0.25376946565975284, 0.28134033914961, 0.30733025405109415, 0.3315907110195543, 0.3542085171958842, 0.3752200544747731, 0.39490685748406024, 0.4131665956697627, 0.43008152164519137, 0.44577699628022927, 0.4602771282148217, 0.47373185889763075, 0.48621085122169194, 0.4976697728478616, 0.5082593868732914, 0.517966750805702, 0.5268475375710621, 0.5349495276539338, 0.542459810618976, 0.5494677841124955, 0.555823788381663, 0.5617231565846097, 0.5669977467839376, 0.5720866387895139, 0.5766060074813912, 0.5807247007243467, 0.5845175191001402, 0.5880264227883636, 0.5912006761006124, 0.5941691472733124, 0.5968779737096946, 0.5992994972654971, 0.6016718558169097, 0.6037651618834284, 0.6057025797170955, 0.6073346846733575, 0.6089825434946892, 0.6103889586208109, 0.6116593104343019, 0.6128002658688156, 0.6139504307444202, 0.614948376751512, 0.6158488172578854, 0.6166778149040976, 0.6175169187380695, 0.6182232840636442, 0.6188497483102422, 0.6194345285202848, 0.6199258453307239, 0.6204913354446615, 0.6209955902007674, 0.6214075307538727, 0.6217109200338287, 0.6219583577000656, 0.6223507874627741, 0.622692348505164, 0.6228905074980862, 0.6231012945285932, 0.623339903078687, 0.6235639153201908, 0.6238097762625626, 0.6239983548320889, 0.6241332704101572, 0.6243151122506476, 0.6244741196094699, 0.6246289240740951, 0.6247542611505819, 0.6248601251218101, 0.6249669923911444, 0.6249748172787835, 0.6250493250419535, 0.6250606505550155, 0.6251381286369725, 0.6251869686202667, 0.6252426224469168, 0.6253205766277973, 0.6253767844977326, 0.6254715133592366, 0.6255404932929235, 0.6256002861332509, 0.6255610937371646, 0.6255677093080771, 0.6256224553980797, 0.6255763597290728, 0.6255907132472138, 0.625626882901636, 0.6256780728299111, 0.6256915195843877, 0.6257268371026932, 0.6258493397215583, 0.6257759413714032, 0.6257052325586446, 0.6256392694783524, 0.625640356575137, 0.6256994276336624, 0.6256829092196871, 0.6257284244185194, 0.6258065544296021, 0.6258653177931665, 0.6258995675810795, 0.6258257967425913, 0.6257943162689137, 0.6258310519604516, 0.6257571211887789, 0.6258463684645116, 0.625847672001966, 0.6258930230130558, 0.6259455367647219, 0.6259623198649662, 0.6260054345873567, 0.6259234742946879, 0.6259379214907729, 0.6260044384386871, 0.6259249029096948, 0.6259206781514404, 0.6259168758690113, 0.6257948712255395, 0.6257361842714056, 0.6256811129615131, 0.625717471142106, 0.6255991669784768, 0.6255135835216775, 0.6254922898843492, 0.6255941414953052, 0.6255160360332517, 0.625622488475746, 0.6256624560537436, 0.6256589353929982, 0.6256696095447284, 0.6257699331336757, 0.625792795048252, 0.6258204183154368, 0.6258033905285129, 0.6257647779833673, 0.6257812520641833, 0.6258053432833371], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 531041758, "moving_var_accuracy_train": [0.00694704921580752, 0.01587407270184779, 0.024778793330607563, 0.03265821729634494, 0.03928340766113317, 0.044635709394681075, 0.048712619662696105, 0.05149108483184322, 0.05318335393360226, 0.053944299629519535, 0.053846997617434146, 0.05306638426176098, 0.05173310812498411, 0.050047929227024016, 0.048043898651815235, 0.0458145412734218, 0.04345021846225168, 0.0409974810511118, 0.038527000945722936, 0.03607582809596645, 0.03365000724988209, 0.03129426585176209, 0.029012935497232287, 0.026821457309655204, 0.024730091768416236, 0.02276472174350964, 0.020930254801531527, 0.019200818433805465, 0.01759395949717036, 0.016084955263390936, 0.01470953113365161, 0.013422400260644536, 0.012232832940846178, 0.011139018887847369, 0.010135928644901678, 0.009213018737224419, 0.008371023253430332, 0.007599960594049293, 0.006892738521626116, 0.006254117435331651, 0.005668143064391602, 0.005135111048712344, 0.004645573843135405, 0.00420545540707723, 0.0038027118979323624, 0.003436964851709487, 0.0031049843802704556, 0.0028063918554130807, 0.002534715735969406, 0.0022885413003221264, 0.002065872304166742, 0.0018656219309477456, 0.0016835503056115398, 0.0015187273921207754, 0.0013699323639552148, 0.0012351116574336749, 0.0011144785033109577, 0.001005319108711362, 0.0009063144530138596, 0.0008165114132092038, 0.0007354113004763385, 0.0006632561804966395, 0.0005979805379580824, 0.0005385358870405575, 0.0004850821788865713, 0.000437086367359515, 0.0003938293639826553, 0.0003549904560112455, 0.0003198114673020823, 0.0002879941404907237, 0.00025949232453622765, 0.00023377064214404164, 0.0002106092577300485, 0.00018968971640172385, 0.00017082160938518943, 0.00015384223396596524, 0.00013845856162916778, 0.00012466266812720438, 0.00011219755571969899, 0.0001010318258263826, 9.0950111339458e-05, 8.188297634129926e-05, 7.374937039602008e-05, 6.640286727820205e-05, 5.9843342565198365e-05, 5.3901832389941565e-05, 4.854382580473721e-05, 4.370326761946233e-05, 3.933333474952259e-05, 3.5426975483905426e-05, 3.190340123182566e-05, 2.8714915319990328e-05, 2.5855197983100497e-05, 2.3293261863601575e-05, 2.0965563014095e-05, 1.888023265657878e-05, 1.712727141558005e-05, 1.546303013427146e-05, 1.3961724746659937e-05, 1.2604712423648778e-05, 1.134425181729867e-05, 1.0241231145166635e-05, 9.219563752652312e-06, 8.316252077309796e-06, 7.539565557264894e-06, 6.816687197614957e-06, 6.145575909602201e-06, 5.579997548143203e-06, 5.030916975335658e-06, 4.539970877097026e-06, 4.135165620388415e-06, 3.793334744380889e-06, 3.414016562831854e-06, 3.091125334410511e-06, 2.806832047995797e-06, 2.5286838952805125e-06, 2.2925454193336892e-06, 2.1237482835696085e-06, 1.913251948485095e-06, 1.761747292874989e-06, 1.6425056669363756e-06, 1.4784157374835155e-06, 1.3307042799001936e-06, 1.3316000491683594e-06, 1.2294374715211246e-06, 1.1337893669284974e-06, 1.0323076858998657e-06, 1.0550397934979322e-06, 1.015456566847648e-06, 9.179916810789157e-07, 9.195562688600049e-07, 8.82504810797467e-07, 8.962434323347818e-07, 8.209957547202268e-07, 7.390077347169586e-07, 6.661323988816734e-07, 6.901025614867891e-07, 6.257963095809651e-07, 5.700840826325157e-07, 5.156851841169941e-07, 4.775350234888026e-07, 4.322240791885408e-07, 3.9422515283254905e-07], "duration": 196595.852782, "accuracy_train": [0.27782986832087486, 0.35475101801864156, 0.4019167517418789, 0.4338596100382983, 0.46005899028700625, 0.48281859225036916, 0.501859398071244, 0.5161576212970653, 0.5294782005583242, 0.5412394881644518, 0.5499348237356958, 0.5577687727828534, 0.5643238899847729, 0.5720880845676449, 0.5775042393410853, 0.5823158554240495, 0.5870362679955703, 0.5907783156261536, 0.5948244350429125, 0.5985217821382429, 0.6008000674833887, 0.6035659131021595, 0.6053330261973976, 0.6067746184593024, 0.6078674383997785, 0.6100523573043558, 0.6125395455541712, 0.6130278268041712, 0.6148174704111296, 0.6144690585778885, 0.617886666839701, 0.6172803257082872, 0.617792939910945, 0.6186528844822813, 0.6196065559823736, 0.6197689559108527, 0.6208853878276117, 0.6212574116371355, 0.6210932092677187, 0.6230230827796235, 0.6226049164820967, 0.6231393402200996, 0.6220236292797158, 0.623813272886674, 0.6230466947559062, 0.6230924767557217, 0.6230688647794389, 0.6243019146248615, 0.6239298908153378, 0.6239527818152455, 0.6241387937200074, 0.6250688532438169, 0.6245805719938169, 0.6244879265296235, 0.6246975504106681, 0.6243476966246769, 0.6255807464700996, 0.6255338830057217, 0.6251149957318198, 0.624441423553433, 0.6241852966961978, 0.6258826553271503, 0.625766397886674, 0.6246739384343853, 0.6249983778031561, 0.6254873800295312, 0.6255800254937246, 0.6260225247439092, 0.6256955619578258, 0.6253475106127722, 0.6259516888150609, 0.6259051858388704, 0.6260221642557217, 0.6258822948389626, 0.6258129008628645, 0.6259287978151532, 0.6250452412675341, 0.6257198949104835, 0.6251625801725729, 0.6258354313745847, 0.6256265284699151, 0.6257435068867663, 0.6260221642557217, 0.6258826553271503, 0.6263240731127722, 0.6261613126961055, 0.6261384216961978, 0.6252083621723883, 0.6256272494462901, 0.6261151702081026, 0.6251614987080103, 0.6257198949104835, 0.625952409791436, 0.6261387821843853, 0.6258125403746769, 0.6260446947674418, 0.6269518632913437, 0.6251153562200074, 0.6250688532438169, 0.6250456017557217, 0.6256501404461978, 0.6262310671603912, 0.6255342434939092, 0.6261380612080103, 0.6265097245293466, 0.6263941880652455, 0.626207815672296, 0.6251618591961978, 0.625510992005814, 0.626161673184293, 0.6250917442437246, 0.6266495939461055, 0.625859403839055, 0.6263011821128645, 0.6264181605297158, 0.626113367767165, 0.6263934670888704, 0.6251858316606681, 0.6260679462555372, 0.6266030909699151, 0.6252090831487633, 0.6258826553271503, 0.6258826553271503, 0.624696829434293, 0.6252080016842008, 0.6251854711724806, 0.6260446947674418, 0.624534429505814, 0.6247433324104835, 0.6253006471483943, 0.6265108059939092, 0.6248130868747692, 0.6265805604581949, 0.6260221642557217, 0.6256272494462901, 0.625765676910299, 0.6266728454342008, 0.6259985522794389, 0.6260690277200996, 0.6256501404461978, 0.625417265077058, 0.6259295187915282, 0.6260221642557217], "end": "2016-01-27 01:23:49.989000", "learning_rate_per_epoch": [0.0053303237073123455, 0.004871814977377653, 0.004452746827155352, 0.004069726448506117, 0.0037196530029177666, 0.003399692475795746, 0.0031072546262294054, 0.0028399720322340727, 0.0025956807658076286, 0.0023724031634628773, 0.0021683317609131336, 0.001981814159080386, 0.0018113406840711832, 0.0016555311158299446, 0.0015131242107599974, 0.0013829669915139675, 0.0012640056665986776, 0.0011552772484719753, 0.0010559015208855271, 0.0009650740539655089, 0.00088205945212394, 0.0008061856497079134, 0.0007368384394794703, 0.0006734564085491002, 0.0006155264563858509, 0.0005625795456580818, 0.0005141870933584869, 0.00046995727461762726, 0.0004295320832170546, 0.0003925842174794525, 0.0003588145482353866, 0.0003279497323092073, 0.00029973985510878265, 0.00027395656798034906, 0.00025039113825187087, 0.00022885277576278895, 0.00020916711946483701, 0.000191174796782434, 0.00017473015759605914, 0.00015970006643328816, 0.00014596284017898142, 0.00013340727309696376, 0.00012193172733532265, 0.00011144329619128257, 0.00010185706923948601, 9.309543384006247e-05, 8.50874712341465e-05, 7.776833808748052e-05, 7.107879355316982e-05, 6.496467540273443e-05, 5.937648529652506e-05, 5.426898496807553e-05, 4.9600828788243234e-05, 4.53342217952013e-05, 4.1434621380176395e-05, 3.787046080105938e-05, 3.461288724793121e-05, 3.1635525374440476e-05, 2.8914271752000786e-05, 2.6427098418935202e-05, 2.415386916254647e-05, 2.2076179448049515e-05, 2.017721089941915e-05, 1.844158941821661e-05, 1.68552651302889e-05, 1.5405394151457585e-05, 1.4080239452596288e-05, 1.2869072634202894e-05, 1.176208934339229e-05, 1.0750327419373207e-05, 9.825595952861477e-06, 8.980408892966807e-06, 8.20792411104776e-06, 7.5018874667875934e-06, 6.856583240733016e-06, 6.266787750064395e-06, 5.727725692850072e-06, 5.235032858763589e-06, 4.784721113537671e-06, 4.3731447476602625e-06, 3.996971827291418e-06, 3.653156682048575e-06, 3.338916258144309e-06, 3.051706471524085e-06, 2.7892019716091454e-06, 2.5492779514024733e-06, 2.329991957594757e-06, 2.1295686565281358e-06, 1.9463855096546467e-06, 1.7789595858630491e-06, 1.6259353969871881e-06, 1.486074211243249e-06, 1.3582437077275245e-06, 1.2414091088430723e-06, 1.134624426413211e-06, 1.0370252994107432e-06, 9.478215474700846e-07, 8.662909749546088e-07, 7.917735729279229e-07, 7.236660621856572e-07, 6.614171184082807e-07, 6.045227678441734e-07, 5.525224082703062e-07, 5.04995057326596e-07, 4.61555941910774e-07, 4.2185340021205775e-07, 3.855660395402083e-07, 3.5240006468484353e-07, 3.2208700417868386e-07, 2.9438143656079774e-07, 2.6905905770036043e-07, 2.4591489022895985e-07, 2.2476156402717606e-07, 2.0542782408483617e-07, 1.8775715204810695e-07, 1.7160648724257044e-07, 1.568450898048468e-07, 1.4335344644678116e-07, 1.3102234674988722e-07, 1.1975194524893595e-07, 1.0945101536208313e-07, 1.0003616068843257e-07, 9.143116130871931e-08, 8.356635561312942e-08, 7.637807186711143e-08, 6.98081166206066e-08, 6.380329864441592e-08, 5.831500970998604e-08, 5.3298816027336215e-08, 4.8714110079117745e-08, 4.45237766655282e-08, 4.0693890923648723e-08, 3.7193448321204414e-08, 3.3994108861179484e-08, 3.106997326085548e-08, 2.8397368012633706e-08, 2.5954657090210276e-08, 2.3722066089248983e-08, 2.168152057890893e-08, 1.9816500440583695e-08, 1.8111906641138376e-08, 1.655394044064451e-08, 1.5129987929185518e-08], "accuracy_valid": [0.2675266495670181, 0.34577813205948793, 0.3908647284450301, 0.42636806993599397, 0.4546898531626506, 0.47833178416792166, 0.4969379471009036, 0.5151073042168675, 0.5268569394766567, 0.535168015813253, 0.5468264660203314, 0.5515769131212349, 0.5546389660203314, 0.5607836619917168, 0.5686373423381024, 0.5694506541792168, 0.5717493999435241, 0.5758086055158133, 0.5786162227033133, 0.5835299204631024, 0.5843535273908133, 0.584862398814006, 0.5858183711408133, 0.5867037485881024, 0.586205172251506, 0.5896334360881024, 0.5902540827371988, 0.590721773814006, 0.5926954889871988, 0.5913527155496988, 0.5928278543862951, 0.5925940088478916, 0.5923395731362951, 0.5949339349585843, 0.5960222726844879, 0.5918307017131024, 0.5950251200112951, 0.5935499811746988, 0.5945574289344879, 0.5967443994728916, 0.5957575418862951, 0.5962458231362951, 0.5962767083960843, 0.5974665262612951, 0.5947809793862951, 0.5960119775978916, 0.5967238092996988, 0.5978430322853916, 0.5961237528237951, 0.5988401849585843, 0.5985651590737951, 0.5982092432228916, 0.5975783014871988, 0.5963678934487951, 0.5966120340737951, 0.5975783014871988, 0.5982195383094879, 0.5972120905496988, 0.5967238092996988, 0.5971003153237951, 0.5971003153237951, 0.5999285226844879, 0.5989416650978916, 0.5982195383094879, 0.5974665262612951, 0.5985651590737951, 0.5975885965737951, 0.5987990046121988, 0.5980768778237951, 0.5990431452371988, 0.6000608880835843, 0.5989416650978916, 0.5979342173381024, 0.5998064523719879, 0.5985857492469879, 0.5992975809487951, 0.5975988916603916, 0.5980768778237951, 0.5979651025978916, 0.5980871729103916, 0.5979548075112951, 0.5986975244728916, 0.5989416650978916, 0.5973444559487951, 0.6001726633094879, 0.5983313135353916, 0.5984430887612951, 0.5978533273719879, 0.5994093561746988, 0.5972120905496988, 0.5978327371987951, 0.5993181711219879, 0.6006609445594879, 0.5973444559487951, 0.5977209619728916, 0.5969576548381024, 0.5986872293862951, 0.5975680064006024, 0.5977312570594879, 0.5977106668862951, 0.5980768778237951, 0.5990534403237951, 0.5989210749246988, 0.5988298898719879, 0.5983107233621988, 0.5978121470256024, 0.5995417215737951, 0.5989416650978916, 0.5967135142131024, 0.5981989481362951, 0.5985754541603916, 0.5981886530496988, 0.5986666392131024, 0.5988195947853916, 0.5969576548381024, 0.5969679499246988, 0.5994299463478916, 0.5972120905496988, 0.5999285226844879, 0.5987990046121988, 0.5980768778237951, 0.5974665262612951, 0.5986872293862951, 0.5983210184487951, 0.5986975244728916, 0.5982092432228916, 0.6000402979103916, 0.5983313135353916, 0.5979651025978916, 0.5973444559487951, 0.5989313700112951, 0.5959913874246988, 0.5979445124246988, 0.5990534403237951, 0.5984533838478916, 0.5984636789344879, 0.5991961008094879, 0.5992975809487951, 0.5975885965737951, 0.5979445124246988, 0.5995417215737951, 0.5969679499246988, 0.5983210184487951], "accuracy_test": 0.5985610650510205, "start": "2016-01-24 18:47:14.136000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0], "accuracy_train_last": 0.6260221642557217, "batch_size_eval": 1024, "accuracy_train_std": [0.014713072165387857, 0.015132683880487238, 0.015791421294650217, 0.016049738742404257, 0.01553117984890636, 0.016393257867886056, 0.016113978814041186, 0.01529695044087271, 0.015454188823312065, 0.015624612185947268, 0.016072441063190154, 0.015946725630984322, 0.015607463114056884, 0.014648317635080284, 0.0140321455602854, 0.013760226310706253, 0.014237698120023116, 0.013263727892356744, 0.013534412352669788, 0.014495425109351676, 0.014365277908406787, 0.013573734231580894, 0.013244072225822924, 0.01392578150538352, 0.013566259557236025, 0.013126674892848045, 0.013054719821468034, 0.013292922074482906, 0.013441141508690326, 0.013662772318669145, 0.013491597638385268, 0.01319304686850794, 0.0132038937199728, 0.012875276325956714, 0.01301372654921683, 0.012696880107659904, 0.012274286441367988, 0.012418411983730908, 0.012962289577930794, 0.012474054068971471, 0.012763409910835771, 0.012969046459312822, 0.01207926257246736, 0.012894597545398047, 0.012500611741704299, 0.012805163121873814, 0.012487936924584475, 0.012750993249222381, 0.012155704126358238, 0.012264389542761085, 0.012350763133147835, 0.012486482414772668, 0.013018391431884319, 0.01207406453048761, 0.01255770121772727, 0.012439202242980694, 0.012600168355428338, 0.012517046506328521, 0.012585430548811064, 0.012247533374980782, 0.012377788759286308, 0.012213356856075807, 0.012220273886798083, 0.01196531406010827, 0.01209702003596535, 0.012558783077643146, 0.012735872378770925, 0.012587373166335993, 0.012067021075699177, 0.012100938522703317, 0.012206437244430072, 0.012632255455004812, 0.012250758588126773, 0.011873666428698422, 0.012319065314541617, 0.012106872680307593, 0.012344513296971206, 0.012507727036646158, 0.01212989062797993, 0.012145035461889591, 0.012568545466002954, 0.012248826772527019, 0.011958120596713516, 0.012570665571783627, 0.011831590024318406, 0.012403871090098597, 0.011579908271275717, 0.012400990613058792, 0.01202055649955849, 0.012183147505929857, 0.012584919717050029, 0.012493195392763263, 0.012190433036363411, 0.012200750420980382, 0.012281827173858518, 0.012035800687620172, 0.011883701815855113, 0.012625390395268563, 0.012668818509463078, 0.012507483942199383, 0.012443303896268132, 0.012550957837611194, 0.012832551500905114, 0.01233167680593188, 0.012269066372805858, 0.012212155105724706, 0.011963243282285494, 0.012440074714287977, 0.012223632263378902, 0.012057004357741109, 0.011913674239601636, 0.012493752178366173, 0.011922451925428061, 0.012160953364275104, 0.012301450166404083, 0.012289928222219677, 0.012138698238645503, 0.012166005929455225, 0.012216347643891323, 0.011906230974863397, 0.012001203061923564, 0.012348329412108052, 0.012991710001928577, 0.011781136318897496, 0.012064681418885368, 0.012411646388172893, 0.012122272851065296, 0.01191123676142418, 0.012045256732652825, 0.01249317556691467, 0.01198347917154319, 0.011912914444890428, 0.012297230979263956, 0.011899111028235583, 0.012144588947195974, 0.012120629866042358, 0.012433265072382306, 0.011430699258595247, 0.0119732363681578, 0.012147827864483419, 0.01232096699138021, 0.0120641847152274, 0.012357792829641973], "accuracy_test_std": 0.013762101779132573, "error_valid": [0.7324733504329819, 0.6542218679405121, 0.6091352715549698, 0.573631930064006, 0.5453101468373494, 0.5216682158320783, 0.5030620528990963, 0.48489269578313254, 0.4731430605233433, 0.464831984186747, 0.45317353397966864, 0.4484230868787651, 0.44536103397966864, 0.4392163380082832, 0.43136265766189763, 0.4305493458207832, 0.42825060005647586, 0.42419139448418675, 0.42138377729668675, 0.41647007953689763, 0.41564647260918675, 0.41513760118599397, 0.41418162885918675, 0.41329625141189763, 0.41379482774849397, 0.41036656391189763, 0.4097459172628012, 0.40927822618599397, 0.4073045110128012, 0.4086472844503012, 0.40717214561370485, 0.4074059911521084, 0.40766042686370485, 0.40506606504141573, 0.40397772731551207, 0.40816929828689763, 0.40497487998870485, 0.4064500188253012, 0.40544257106551207, 0.4032556005271084, 0.40424245811370485, 0.40375417686370485, 0.40372329160391573, 0.40253347373870485, 0.40521902061370485, 0.4039880224021084, 0.4032761907003012, 0.4021569677146084, 0.40387624717620485, 0.40115981504141573, 0.40143484092620485, 0.4017907567771084, 0.4024216985128012, 0.40363210655120485, 0.40338796592620485, 0.4024216985128012, 0.40178046169051207, 0.4027879094503012, 0.4032761907003012, 0.40289968467620485, 0.40289968467620485, 0.40007147731551207, 0.4010583349021084, 0.40178046169051207, 0.40253347373870485, 0.40143484092620485, 0.40241140342620485, 0.4012009953878012, 0.40192312217620485, 0.4009568547628012, 0.39993911191641573, 0.4010583349021084, 0.40206578266189763, 0.40019354762801207, 0.40141425075301207, 0.40070241905120485, 0.4024011083396084, 0.40192312217620485, 0.4020348974021084, 0.4019128270896084, 0.40204519248870485, 0.4013024755271084, 0.4010583349021084, 0.40265554405120485, 0.39982733669051207, 0.4016686864646084, 0.40155691123870485, 0.40214667262801207, 0.4005906438253012, 0.4027879094503012, 0.40216726280120485, 0.40068182887801207, 0.39933905544051207, 0.40265554405120485, 0.4022790380271084, 0.40304234516189763, 0.40131277061370485, 0.40243199359939763, 0.40226874294051207, 0.40228933311370485, 0.40192312217620485, 0.40094655967620485, 0.4010789250753012, 0.40117011012801207, 0.4016892766378012, 0.40218785297439763, 0.40045827842620485, 0.4010583349021084, 0.40328648578689763, 0.40180105186370485, 0.4014245458396084, 0.4018113469503012, 0.40133336078689763, 0.4011804052146084, 0.40304234516189763, 0.4030320500753012, 0.4005700536521084, 0.4027879094503012, 0.40007147731551207, 0.4012009953878012, 0.40192312217620485, 0.40253347373870485, 0.40131277061370485, 0.40167898155120485, 0.4013024755271084, 0.4017907567771084, 0.3999597020896084, 0.4016686864646084, 0.4020348974021084, 0.40265554405120485, 0.40106862998870485, 0.4040086125753012, 0.4020554875753012, 0.40094655967620485, 0.4015466161521084, 0.40153632106551207, 0.40080389919051207, 0.40070241905120485, 0.40241140342620485, 0.4020554875753012, 0.40045827842620485, 0.4030320500753012, 0.40167898155120485], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-3, 3], "momentum": 0.5327889759943379, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.005831984559643978, "patience_threshold": 1, "do_flip": true, "batch_size": 16, "optimization": "adadelta", "nb_data_augmentation": 3, "learning_rate_decay_method": "exp", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 6.653573168494634e-07, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.08601889267870533}, "accuracy_valid_max": 0.6006609445594879, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-3, 3)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.5983210184487951, "loss_train": [2.1861188411712646, 1.9647939205169678, 1.8438348770141602, 1.7626891136169434, 1.6966279745101929, 1.6411336660385132, 1.5934852361679077, 1.555501103401184, 1.5229151248931885, 1.4943469762802124, 1.4699987173080444, 1.4505951404571533, 1.4323222637176514, 1.4159431457519531, 1.4021179676055908, 1.3895562887191772, 1.3791574239730835, 1.3681461811065674, 1.3586944341659546, 1.3500325679779053, 1.344008445739746, 1.3370463848114014, 1.3316012620925903, 1.3271414041519165, 1.3213285207748413, 1.3184140920639038, 1.314275860786438, 1.3098505735397339, 1.3060617446899414, 1.3040956258773804, 1.3016713857650757, 1.299758791923523, 1.2970964908599854, 1.2949784994125366, 1.2935540676116943, 1.29104745388031, 1.2892929315567017, 1.289599895477295, 1.2875169515609741, 1.2863490581512451, 1.284706950187683, 1.2840038537979126, 1.2838836908340454, 1.2831658124923706, 1.2824089527130127, 1.28177011013031, 1.2820326089859009, 1.2814061641693115, 1.2823703289031982, 1.2802363634109497, 1.278857707977295, 1.2784078121185303, 1.278533935546875, 1.2769840955734253, 1.2782678604125977, 1.27846360206604, 1.2785232067108154, 1.2768422365188599, 1.2773640155792236, 1.2767404317855835, 1.2763776779174805, 1.2760803699493408, 1.276911735534668, 1.275813102722168, 1.276384711265564, 1.275591254234314, 1.2753218412399292, 1.2750645875930786, 1.275000810623169, 1.2756068706512451, 1.2755872011184692, 1.2753158807754517, 1.276157259941101, 1.2755423784255981, 1.2751092910766602, 1.2753105163574219, 1.275011420249939, 1.275396466255188, 1.2754029035568237, 1.2749826908111572, 1.274767279624939, 1.274502158164978, 1.2740405797958374, 1.2745110988616943, 1.274966835975647, 1.2763502597808838, 1.2753841876983643, 1.2747117280960083, 1.2764073610305786, 1.275477409362793, 1.2758427858352661, 1.274314045906067, 1.2748267650604248, 1.2758046388626099, 1.2754170894622803, 1.2759820222854614, 1.2756690979003906, 1.275223970413208, 1.2750604152679443, 1.2758575677871704, 1.2750749588012695, 1.2760088443756104, 1.2753827571868896, 1.2744508981704712, 1.2747414112091064, 1.2755622863769531, 1.275282621383667, 1.275250792503357, 1.2749940156936646, 1.2766724824905396, 1.275031328201294, 1.2736108303070068, 1.2757980823516846, 1.2742023468017578, 1.2752841711044312, 1.2759336233139038, 1.2761859893798828, 1.2757130861282349, 1.2747882604599, 1.2749707698822021, 1.2750959396362305, 1.2754545211791992, 1.274908423423767, 1.2751810550689697, 1.2740023136138916, 1.2759151458740234, 1.2754714488983154, 1.2745356559753418, 1.2752997875213623, 1.2748289108276367, 1.2751524448394775, 1.2740486860275269, 1.2749073505401611, 1.2748616933822632, 1.2753442525863647, 1.275227427482605, 1.2751338481903076, 1.275449514389038, 1.275695562362671, 1.2760142087936401, 1.2750214338302612, 1.276562213897705, 1.275821328163147], "accuracy_train_first": 0.27782986832087486, "model": "residualv5", "loss_std": [0.15864263474941254, 0.13860772550106049, 0.14033815264701843, 0.14936141669750214, 0.15711674094200134, 0.16635040938854218, 0.1741669476032257, 0.18074047565460205, 0.18646888434886932, 0.19044657051563263, 0.1937824785709381, 0.19610536098480225, 0.1993391513824463, 0.20209775865077972, 0.202887624502182, 0.2043369561433792, 0.2057143747806549, 0.206814706325531, 0.2085670679807663, 0.20860734581947327, 0.21069854497909546, 0.2099820375442505, 0.21123428642749786, 0.21103741228580475, 0.21220584213733673, 0.21279285848140717, 0.2133128046989441, 0.21316754817962646, 0.21300743520259857, 0.21435123682022095, 0.2136324942111969, 0.21498967707157135, 0.21505756676197052, 0.21624352037906647, 0.21640527248382568, 0.21444539725780487, 0.21582551300525665, 0.21618881821632385, 0.21553663909435272, 0.21578152477741241, 0.2161124348640442, 0.21626880764961243, 0.2155529409646988, 0.21723878383636475, 0.2158174365758896, 0.21590714156627655, 0.21754169464111328, 0.21764607727527618, 0.2165987491607666, 0.21607349812984467, 0.21564063429832458, 0.21804940700531006, 0.21569055318832397, 0.21606120467185974, 0.21629633009433746, 0.2167067676782608, 0.2159138023853302, 0.21650858223438263, 0.21683496236801147, 0.21742182970046997, 0.2169712334871292, 0.21555647253990173, 0.21754270792007446, 0.2172277569770813, 0.21641948819160461, 0.21748149394989014, 0.21728092432022095, 0.216637521982193, 0.2172427922487259, 0.217577263712883, 0.21677352488040924, 0.21683606505393982, 0.21724997460842133, 0.21771347522735596, 0.21620996296405792, 0.21632371842861176, 0.21757137775421143, 0.21596446633338928, 0.21761876344680786, 0.21556217968463898, 0.21734724938869476, 0.21800504624843597, 0.21643805503845215, 0.21857556700706482, 0.21611729264259338, 0.21762318909168243, 0.21663880348205566, 0.21658672392368317, 0.21629376709461212, 0.217305988073349, 0.2171589583158493, 0.2151154726743698, 0.21748772263526917, 0.21571289002895355, 0.21632903814315796, 0.2167109102010727, 0.21738548576831818, 0.21707583963871002, 0.21590876579284668, 0.21632738411426544, 0.21566295623779297, 0.21691526472568512, 0.21528594195842743, 0.21634961664676666, 0.2164837121963501, 0.21736419200897217, 0.2167114019393921, 0.21715553104877472, 0.2175498753786087, 0.2172350287437439, 0.21830815076828003, 0.2170809656381607, 0.2183421552181244, 0.21793042123317719, 0.21747326850891113, 0.216374933719635, 0.21674200892448425, 0.21688367426395416, 0.2173040807247162, 0.21784274280071259, 0.21685175597667694, 0.2176455706357956, 0.2172614187002182, 0.21596021950244904, 0.217126727104187, 0.2176329642534256, 0.21723325550556183, 0.21710388362407684, 0.21569658815860748, 0.2165604531764984, 0.21602608263492584, 0.21665218472480774, 0.21623043715953827, 0.21627001464366913, 0.21694375574588776, 0.21743835508823395, 0.21634186804294586, 0.21740609407424927, 0.2170933485031128, 0.21679556369781494, 0.21615011990070343, 0.21674907207489014, 0.21699611842632294]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:16 2016", "state": "available"}], "summary": "dba69a56c46748d9f1fc47b506c44c88"}