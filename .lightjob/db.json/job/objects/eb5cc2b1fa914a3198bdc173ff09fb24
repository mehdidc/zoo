{"content": {"hp_model": {"f0": 32, "f1": 32, "f2": 32, "f3": 64, "nonlin": "very_leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.6823736429214478, 1.2231383323669434, 0.9350255131721497, 0.8049808740615845, 0.7254546880722046, 0.6683622002601624, 0.6244891881942749, 0.5867476463317871, 0.5587698221206665, 0.5370864868164062, 0.514004111289978, 0.4937323331832886, 0.47737622261047363, 0.4623103141784668, 0.4511445462703705, 0.4380108714103699, 0.42740949988365173, 0.4161352515220642, 0.40605342388153076, 0.3977135717868805, 0.38454777002334595, 0.3798544406890869, 0.37261977791786194, 0.36466678977012634, 0.3563182055950165, 0.35119709372520447, 0.3452098071575165, 0.3374922573566437, 0.333804726600647, 0.3270949721336365, 0.32111260294914246, 0.31855443120002747, 0.3122979402542114, 0.3084166944026947, 0.3048083484172821, 0.29844626784324646, 0.29540225863456726, 0.2894270718097687, 0.2858991026878357, 0.2816469669342041, 0.27799364924430847, 0.27552539110183716, 0.2701564431190491, 0.268472820520401, 0.2652529180049896, 0.2645452320575714, 0.2585753798484802, 0.257232666015625, 0.2542892396450043, 0.25112003087997437, 0.2501109540462494, 0.2463282197713852, 0.24208131432533264, 0.23879295587539673, 0.2382848709821701, 0.237434521317482, 0.23328077793121338, 0.2311989665031433, 0.23052023351192474, 0.22573429346084595, 0.22519193589687347, 0.2230069786310196, 0.22082693874835968, 0.2182311713695526, 0.21640552580356598, 0.21529413759708405, 0.21344387531280518, 0.21151117980480194, 0.21033117175102234, 0.20798861980438232, 0.20632556080818176, 0.204178586602211, 0.20232106745243073, 0.2015099972486496, 0.20144818723201752, 0.19786480069160461, 0.19843539595603943, 0.19627182185649872, 0.19503924250602722, 0.1914430409669876, 0.19061279296875, 0.19103080034255981, 0.19148439168930054, 0.1870693862438202, 0.18618348240852356, 0.18573273718357086, 0.18413592875003815, 0.18335843086242676, 0.18179284036159515, 0.18139730393886566, 0.17991779744625092, 0.17903180420398712, 0.1770048439502716, 0.1755354106426239, 0.17413313686847687, 0.17288726568222046, 0.17304104566574097, 0.17060323059558868, 0.1721430867910385, 0.16991117596626282, 0.1683131754398346, 0.1685185432434082, 0.1670176386833191, 0.16569846868515015, 0.16521486639976501, 0.16407538950443268, 0.16250282526016235, 0.1630401611328125, 0.1625562608242035, 0.1602548360824585, 0.16184940934181213, 0.15849360823631287, 0.1576220691204071, 0.15837201476097107, 0.1569622904062271, 0.15737281739711761, 0.15578855574131012, 0.1548292636871338, 0.15566161274909973, 0.15377642214298248, 0.1513991802930832, 0.15202070772647858, 0.1506366729736328, 0.14948667585849762, 0.14999182522296906, 0.14816264808177948, 0.14833585917949677, 0.1478932946920395, 0.14617514610290527, 0.14437612891197205, 0.14509375393390656, 0.14520137012004852, 0.1442251354455948, 0.14303793013095856, 0.14299730956554413, 0.14295469224452972, 0.14292049407958984, 0.1408175379037857, 0.13981293141841888, 0.1401999592781067, 0.13906727731227875], "moving_avg_accuracy_train": [0.04483845803340716, 0.09652525075558321, 0.15560165514546415, 0.21453824812113548, 0.27102415040504446, 0.324204563581825, 0.3738062188480888, 0.41951008530076606, 0.46180120082476844, 0.5008860900284673, 0.5365601082058531, 0.5695176208833298, 0.5994956025311541, 0.6274078101986275, 0.6528285610516883, 0.6761028005076121, 0.6972564100667161, 0.7166318052472908, 0.7345833385526188, 0.7511373550226614, 0.766189501764766, 0.780087387107623, 0.7926511793411667, 0.8041980105810995, 0.814913462528019, 0.8248456516838089, 0.83399610231923, 0.8422455308816038, 0.850202367518665, 0.8575030654694101, 0.8642550191834049, 0.8706060369390678, 0.8764359212596498, 0.8820059767838786, 0.886944730140236, 0.8914800726716167, 0.8957573015963155, 0.8998090234773355, 0.9035927929988342, 0.907044616446736, 0.910439648099866, 0.9135091274805401, 0.9162087717588335, 0.9189476243033268, 0.921526559933856, 0.9240080372691895, 0.9262947371471524, 0.9284597959801947, 0.9304687307013428, 0.9323860539444238, 0.9341163672584532, 0.9358968995756126, 0.9374388887431898, 0.9391056968511521, 0.9404708934197282, 0.9417018233826188, 0.9430026837492295, 0.9442455376922745, 0.9455316251017569, 0.946730884351225, 0.9479427151090704, 0.949017014651827, 0.9499793781379638, 0.950868648617126, 0.9517456859102676, 0.9526072072336466, 0.9535986710686982, 0.9543376368452724, 0.9551212886334749, 0.9558706088749631, 0.9566333166982457, 0.9573245842809128, 0.9579465809100381, 0.9587156773179268, 0.9593753120016935, 0.9598991927039788, 0.9606497752908161, 0.9613485511070649, 0.9619541618047747, 0.9626084573779424, 0.9633229535271451, 0.9638333944816473, 0.9642996586406716, 0.9647496314647734, 0.9652406375124176, 0.9655475761778887, 0.9659635101494778, 0.9663935461488803, 0.966680597149533, 0.9671225577084354, 0.9675623190852939, 0.9679579601291917, 0.9682536552972896, 0.9686522423330829, 0.9690017421676964, 0.9693325680605152, 0.9697163779188233, 0.9700246044103481, 0.9703415357824824, 0.9705755125971375, 0.9709698866327356, 0.9713038608766695, 0.9715811862081148, 0.9718005881207106, 0.9721701108539514, 0.9725049343650403, 0.9727155586726299, 0.9730702421637556, 0.9733778315617212, 0.9737058873913371, 0.9740127273332203, 0.974207503072582, 0.9745292495641886, 0.9748932261685394, 0.9750743567862739, 0.9752745406743685, 0.975443080429606, 0.9755878268117101, 0.975848306888937, 0.9760361999334319, 0.9763029599234774, 0.9764640249038132, 0.976725204777773, 0.976978867854813, 0.9771698901455591, 0.9773442435024963, 0.9775407251023205, 0.9776617189219149, 0.9777030758976174, 0.9779309954269495, 0.9781455317450427, 0.9783082433015278, 0.9784872357856976, 0.9787739591548023, 0.9790041084012822, 0.9791298264659619, 0.979263971161097, 0.9795008867295572, 0.9796373447816383, 0.9798159245511212, 0.97989533823296], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.04432652484939758, 0.09558146649096383, 0.15390323795180721, 0.21161155873493975, 0.26731616669804215, 0.31946050305911144, 0.3676752325913027, 0.41201254861116343, 0.45290299061563444, 0.49016413757251975, 0.5238996298770148, 0.5552188689318585, 0.5836960643052389, 0.6096307159225313, 0.633143830324254, 0.6546016905617834, 0.6738496410933309, 0.691418996214043, 0.7076443958678647, 0.7222940246639848, 0.7355061931689719, 0.7475303926585506, 0.7580906769996232, 0.7678055409638175, 0.776754379045523, 0.7849202556176273, 0.7923936738623405, 0.7988634026263324, 0.8049648612153558, 0.8106158948603864, 0.8155563702745737, 0.8202500272983211, 0.8244478455398746, 0.8283947213774534, 0.831716005546184, 0.8348516456730415, 0.8374428177021228, 0.8399203273946364, 0.8424949715188775, 0.8447999441994445, 0.8467492607734759, 0.8484782021189445, 0.8500139533020049, 0.8517277782278285, 0.8531338843000005, 0.8544349713500456, 0.855668014359996, 0.8568876163502012, 0.8580117312212051, 0.8585798634454701, 0.8591786906833779, 0.8598529420499046, 0.8605187444187092, 0.8615350646304528, 0.8622432627984316, 0.8627554823111336, 0.8633385501850654, 0.8636181411379444, 0.8640061093566048, 0.8643155711336703, 0.8648291088527581, 0.8653533574648468, 0.8657000223772476, 0.8661961557758181, 0.8663995647181911, 0.8668786900423058, 0.8671959215181204, 0.8674845183723324, 0.867729989492554, 0.8681482850180726, 0.8685491650535395, 0.8688265668840289, 0.8691229976391501, 0.8692310939125092, 0.8695104565186227, 0.8695411267929651, 0.869874935329783, 0.8700787362715787, 0.870442174061966, 0.8706747003492935, 0.870906329053069, 0.8709184528778073, 0.8709812809710508, 0.8711853401386295, 0.8712550120735617, 0.871255652150091, 0.8713782985314674, 0.8714551477069351, 0.8714368037287867, 0.8714569152422033, 0.8718585811163262, 0.8716656164534887, 0.8716893197742543, 0.8715275472941935, 0.871625063178479, 0.8714910418945166, 0.8717651657735891, 0.8718124467301157, 0.8719770699034897, 0.8717854929018456, 0.8718348591801851, 0.8719870930946214, 0.8721017485724333, 0.872143903346214, 0.872254055321457, 0.8723399555592661, 0.8723307870458847, 0.8721526664550011, 0.8722141435030252, 0.8719723860616082, 0.8719145252792426, 0.8716294874727039, 0.8717503419069094, 0.8719964767674232, 0.8722057911106358, 0.8721978320108674, 0.8723870108297356, 0.8725084436417169, 0.8726533247575904, 0.8726473814008072, 0.8727427771556813, 0.8726465573749776, 0.872777627117525, 0.8728477912694774, 0.8728499038499844, 0.8729006332974408, 0.8729076096890822, 0.8725822395804903, 0.8723769147188267, 0.8723752278120795, 0.8724378332782361, 0.8722479785554577, 0.8722846288362072, 0.8723043775489719, 0.87250628636787, 0.8727500689697878, 0.8728840240927638, 0.8729770811149633, 0.8729153775686025, 0.872983973706697, 0.873033503199732], "moving_var_accuracy_train": [0.01809438586932254, 0.04032866815953702, 0.06770599534431401, 0.09219709373410223, 0.11169329877213782, 0.12597737600580186, 0.13552255625160115, 0.14076989130495846, 0.14278974824484325, 0.14225943049694767, 0.139487207603537, 0.13531426562015764, 0.12987095351123745, 0.12389568019196315, 0.11732204333816737, 0.11046505100461576, 0.10344582268056536, 0.09647989385813972, 0.0897322224044365, 0.08322531931560684, 0.07694189147795877, 0.07098606328319171, 0.06530809683246142, 0.059977250954366665, 0.05501291405277068, 0.05039945808033111, 0.046113088993779534, 0.04211425773885295, 0.03847263320838756, 0.03510507160266094, 0.032004864352998194, 0.029167396756493078, 0.026556545041566078, 0.0241801202042964, 0.02198162974630115, 0.019968590758563585, 0.018136383868175746, 0.016470493533168398, 0.014952296385977958, 0.01356430251341952, 0.012311608421409362, 0.011165242912283881, 0.010114311334119391, 0.009170392020051741, 0.008313210998924287, 0.0075373094669238215, 0.006830639487218321, 0.006189762856251296, 0.005607108939050676, 0.0050794832009117335, 0.004598480738302926, 0.004167165322464678, 0.003771848365554538, 0.003419667772418004, 0.0030944748502138735, 0.0027986640623543627, 0.002534027795359695, 0.0022945271891374065, 0.0020799606576471287, 0.0018849085966093286, 0.0017096345410193368, 0.001549058162485508, 0.0014024876375520047, 0.0012693560916627895, 0.0011493432322185605, 0.0010410888799124336, 0.0009458269967471297, 0.0008561589308429486, 0.0007760700288850312, 0.0007035163534152642, 0.0006384002270870059, 0.0005788608622159233, 0.0005244566942541214, 0.0004773346083903559, 0.0004335172087955704, 0.00039263554682805724, 0.0003584423601222213, 0.00032699271288236633, 0.00029759432044875584, 0.00027168781267748304, 0.00024911357413476536, 0.0002265471664335867, 0.00020584907018414357, 0.00018708644304760155, 0.00017054758119224923, 0.0001543407251722747, 0.00014046366227354434, 0.00012808167469322923, 0.00011601509171668814, 0.00010617154476564696, 9.729490090627113e-05, 8.897419733619222e-05, 8.086369829450055e-05, 7.420717309097354e-05, 6.788580699142971e-05, 6.208223823452082e-05, 5.7199804477078634e-05, 5.2334856160070205e-05, 4.800537999584936e-05, 4.369754834442956e-05, 4.0727571429572624e-05, 3.765866344711634e-05, 3.458498115755579e-05, 3.1559717835056e-05, 2.963266950498669e-05, 2.7678363606688502e-05, 2.5309790636548562e-05, 2.391101498278787e-05, 2.2371414624176267e-05, 2.1102858807864063e-05, 1.9839929676492068e-05, 1.819737500663764e-05, 1.7309324749724873e-05, 1.6770702991385075e-05, 1.538890739837444e-05, 1.4210678960011272e-05, 1.3045261905870016e-05, 1.1929299351472783e-05, 1.1347018252014849e-05, 1.0530050592339819e-05, 1.011749356370762e-05, 9.339221558352336e-06, 9.019233741571877e-06, 8.696414977295193e-06, 8.15517911962263e-06, 7.613253045338016e-06, 7.199372912429525e-06, 6.6111911606070935e-06, 5.9654656394997506e-06, 5.836444882208128e-06, 5.6670328800160826e-06, 5.338605047538496e-06, 5.0930893272883244e-06, 5.323673008076034e-06, 5.268023788165891e-06, 4.883466695430903e-06, 4.557073218983781e-06, 4.606526776295061e-06, 4.313461298465719e-06, 4.1691317752362445e-06, 3.808977393481352e-06], "duration": 98973.954251, "accuracy_train": [0.44838458033407164, 0.5617063852551679, 0.6872892946543927, 0.7449675849021778, 0.7793972709602253, 0.8028282821728497, 0.8202211162444629, 0.8308448833748615, 0.84242124054079, 0.8526500928617571, 0.8576262718023256, 0.8661352349806202, 0.8692974373615725, 0.8786176792058878, 0.8816153187292359, 0.8855709556109265, 0.8876388960986527, 0.8910103618724622, 0.8961471383005721, 0.9001235032530455, 0.9016588224437062, 0.905168355193337, 0.9057253094430602, 0.9081194917404946, 0.9113525300502953, 0.9142353540859173, 0.9163501580380213, 0.9164903879429678, 0.921813897252215, 0.9232093470261166, 0.9250226026093578, 0.9277651967400333, 0.9289048801448875, 0.932136476501938, 0.931393510347453, 0.9322981554540422, 0.9342523619186047, 0.9362745204065154, 0.937646718692322, 0.9381110274778516, 0.9409949329780363, 0.9411344419066077, 0.9405055702634736, 0.9435972972037652, 0.9447369806086194, 0.9463413332871908, 0.9468750360488187, 0.9479453254775747, 0.9485491431916758, 0.9496419631321521, 0.9496891870847176, 0.951921690430048, 0.9513167912513842, 0.9541069698228128, 0.952757662536914, 0.9527801930486341, 0.9547104270487264, 0.9554312231796788, 0.9571064117870985, 0.9575242175964378, 0.9588491919296788, 0.9586857105366371, 0.9586406495131967, 0.9588720829295865, 0.9596390215485419, 0.9603608991440569, 0.9625218455841639, 0.9609883288344407, 0.9621741547272978, 0.9626144910483574, 0.9634976871077889, 0.9635459925249169, 0.9635445505721669, 0.9656375449889257, 0.9653120241555924, 0.9646141190245479, 0.9674050185723514, 0.9676375334533037, 0.9674046580841639, 0.9684971175364526, 0.9697534188699704, 0.9684273630721669, 0.96849603607189, 0.9687993868816908, 0.9696596919412146, 0.9683100241671282, 0.9697069158937799, 0.9702638701435032, 0.969264056155408, 0.9711002027385567, 0.971520171477021, 0.971518729524271, 0.9709149118101699, 0.9722395256552234, 0.9721472406792175, 0.9723100010958842, 0.9731706666435955, 0.9727986428340717, 0.9731939181316908, 0.972681303929033, 0.9745192529531194, 0.9743096290720746, 0.9740771141911223, 0.9737752053340717, 0.9754958154531194, 0.9755183459648394, 0.9746111774409376, 0.9762623935838871, 0.976146136143411, 0.9766583898578812, 0.9767742868101699, 0.9759604847268365, 0.977424967988649, 0.9781690156076966, 0.9767045323458842, 0.9770761956672205, 0.9769599382267442, 0.976890544250646, 0.9781926275839794, 0.9777272373338871, 0.9787037998338871, 0.9779136097268365, 0.979075823643411, 0.9792618355481728, 0.9788890907622739, 0.9789134237149317, 0.9793090595007383, 0.9787506632982651, 0.9780752886789406, 0.9799822711909376, 0.9800763586078812, 0.979772647309893, 0.9800981681432264, 0.9813544694767442, 0.9810754516196014, 0.9802612890480805, 0.9804712734173128, 0.9816331268456996, 0.9808654672503692, 0.9814231424764673, 0.9806100613695091], "end": "2016-02-05 01:00:55.176000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0], "moving_var_accuracy_valid": [0.017683567247018354, 0.03955883190643979, 0.0662158099527734, 0.08956648154597639, 0.10853686352628514, 0.1221544635044836, 0.13086095844879644, 0.13546704093052175, 0.13696859106335865, 0.13576726960990626, 0.13243329361815542, 0.12801801687110992, 0.12251477109100203, 0.11631674937249477, 0.10966087337506182, 0.10283874393131576, 0.09588922193516831, 0.08907843989587072, 0.08253996825161976, 0.0762174760412348, 0.07016678100654887, 0.06445133526618065, 0.05900987818784133, 0.05395829760564241, 0.049283203172191425, 0.04495501671668987, 0.04096218286736459, 0.03724268109314474, 0.03385346315605238, 0.030755524471762562, 0.027899646700450004, 0.025307955776714165, 0.022935755300944816, 0.020782380230745707, 0.01880342056443628, 0.017011568659039085, 0.01537083934549382, 0.01388899789943293, 0.012559757240788044, 0.011351597608232684, 0.01025063636336156, 0.009252475870610044, 0.00834845506881548, 0.007540044324821301, 0.006803834100914961, 0.006138686138429622, 0.005538501080166145, 0.004998037833280143, 0.004509606758141039, 0.00406155105034517, 0.0036586232918583957, 0.003296852496819924, 0.0029711568822866855, 0.002683337355013203, 0.0024195175213180395, 0.00217992708864897, 0.0019649940930945753, 0.0017691982236935039, 0.001593633075372368, 0.0014351316671583115, 0.0012939919893428134, 0.001167066319874024, 0.0010514412769400308, 0.000948512484388621, 0.0008540336127302946, 0.0007706963011431365, 0.0006945323933120506, 0.0006258287472791954, 0.0005637881771890414, 0.000508984099790158, 0.00045953203303666516, 0.0004142713957130286, 0.0003736350968749613, 0.0003363767504262923, 0.00030344146657491406, 0.0002731057859089769, 0.0002467980605713521, 0.00022249206792910857, 0.00020143164438353223, 0.0001817750962138638, 0.00016408045330019175, 0.00014767373085430914, 0.0001329418840925836, 0.00012002245697818126, 0.00010806389888701795, 9.725751268559783e-05, 8.766714063082065e-05, 7.895357872966921e-05, 7.106124937051105e-05, 6.395876469020707e-05, 5.901490749110169e-05, 5.3448534991927204e-05, 4.810873811947234e-05, 4.353339732527082e-05, 3.926564172193553e-05, 3.550073289073626e-05, 3.262695471136207e-05, 2.9384378639876524e-05, 2.6689847878794313e-05, 2.435117881894556e-05, 2.1937994201984872e-05, 1.995277126412779e-05, 1.807580704504561e-05, 1.628421956511362e-05, 1.476499872745181e-05, 1.3354908512407598e-05, 1.2020174215905467e-05, 1.1103699298385613e-05, 1.0027344215450735e-05, 9.55062973823002e-06, 8.625697595630693e-06, 8.494346796475274e-06, 7.776364265231973e-06, 7.5439691647504725e-06, 7.183884696745716e-06, 6.466066352493255e-06, 6.141557346818792e-06, 5.660114962567992e-06, 5.283018305941935e-06, 4.7550343867564066e-06, 4.361434098512755e-06, 4.008614904449438e-06, 3.7623669107075162e-06, 3.4304372936094443e-06, 3.08743373121609e-06, 2.801851649647542e-06, 2.522104515045803e-06, 3.222685431627468e-06, 3.279841577818891e-06, 2.9518830309263636e-06, 2.691969727367786e-06, 2.747176096482464e-06, 2.484547674545314e-06, 2.2396030119935905e-06, 2.3825472511332506e-06, 2.6791621390003224e-06, 2.572741699844025e-06, 2.3934040142854126e-06, 2.188329561558355e-06, 2.0118454768559244e-06, 1.8327394652931763e-06], "accuracy_test": 0.7971839126275511, "start": "2016-02-03 21:31:21.222000", "learning_rate_per_epoch": [0.0030811105389147997, 0.0015405552694573998, 0.001027036807499826, 0.0007702776347286999, 0.0006162220961414278, 0.000513518403749913, 0.00044015864841639996, 0.00038513881736434996, 0.0003423456219024956, 0.0003081110480707139, 0.0002801009686663747, 0.0002567592018749565, 0.00023700849851593375, 0.00022007932420819998, 0.00020540737023111433, 0.00019256940868217498, 0.00018124179041478783, 0.0001711728109512478, 0.0001621637202333659, 0.00015405552403535694, 0.00014671955432277173, 0.00014005048433318734, 0.0001339613227173686, 0.00012837960093747824, 0.00012324441922828555, 0.00011850424925796688, 0.00011411520245019346, 0.00011003966210409999, 0.00010624519200064242, 0.00010270368511555716, 9.939066512743011e-05, 9.628470434108749e-05, 9.336698713013902e-05, 9.062089520739391e-05, 8.803173113847151e-05, 8.55864054756239e-05, 8.327325485879555e-05, 8.108186011668295e-05, 7.900283526396379e-05, 7.702776201767847e-05, 7.514903700212017e-05, 7.335977716138586e-05, 7.165373244788498e-05, 7.002524216659367e-05, 6.846912583569065e-05, 6.69806613586843e-05, 6.555554136866704e-05, 6.418980046873912e-05, 6.287980795605108e-05, 6.162220961414278e-05, 6.041393135092221e-05, 5.925212462898344e-05, 5.813416282762773e-05, 5.705760122509673e-05, 5.602019155048765e-05, 5.5019831052049994e-05, 5.405456977314316e-05, 5.312259600032121e-05, 5.2222214435460046e-05, 5.135184255777858e-05, 5.051001062383875e-05, 4.9695332563715056e-05, 4.8906516894930974e-05, 4.8142352170543745e-05, 4.740169970318675e-05, 4.668349356506951e-05, 4.5986726036062464e-05, 4.531044760369696e-05, 4.4653777877101675e-05, 4.401586556923576e-05, 4.339592487667687e-05, 4.279320273781195e-05, 4.220699338475242e-05, 4.163662742939778e-05, 4.108147550141439e-05, 4.054093005834147e-05, 4.0014423575485125e-05, 3.9501417631981894e-05, 3.900139927281998e-05, 3.8513881008839235e-05, 3.8038400816731155e-05, 3.7574518501060084e-05, 3.7121815694263205e-05, 3.667988858069293e-05, 3.624835881055333e-05, 3.582686622394249e-05, 3.541506521287374e-05, 3.5012621083296835e-05, 3.461922096903436e-05, 3.4234562917845324e-05, 3.385835952940397e-05, 3.349033067934215e-05, 3.313022170914337e-05, 3.277777068433352e-05, 3.243274113629013e-05, 3.209490023436956e-05, 3.176402606186457e-05, 3.143990397802554e-05, 3.112233025603928e-05, 3.081110480707139e-05, 3.050604573218152e-05, 3.0206965675461106e-05, 2.9913695470895618e-05, 2.962606231449172e-05, 2.9343909773160703e-05, 2.9067081413813867e-05, 2.8795426260330714e-05, 2.8528800612548366e-05, 2.826706986525096e-05, 2.8010095775243826e-05, 2.775775283225812e-05, 2.7509915526024997e-05, 2.7266465622233227e-05, 2.702728488657158e-05, 2.6792265998665243e-05, 2.6561298000160605e-05, 2.6334279027651064e-05, 2.6111107217730023e-05, 2.5891686163959093e-05, 2.567592127888929e-05, 2.5463723432039842e-05, 2.5255005311919376e-05, 2.5049679607036524e-05, 2.4847666281857528e-05, 2.4648885300848633e-05, 2.4453258447465487e-05, 2.4260712962131947e-05, 2.4071176085271873e-05, 2.3884578695287928e-05, 2.3700849851593375e-05, 2.3519927708548494e-05, 2.3341746782534756e-05, 2.316624522791244e-05, 2.2993363018031232e-05, 2.2823041945230216e-05, 2.265522380184848e-05, 2.248985765618272e-05, 2.2326888938550837e-05, 2.216626307927072e-05, 2.200793278461788e-05, 2.1851847122889012e-05], "accuracy_train_first": 0.44838458033407164, "accuracy_train_last": 0.9806100613695091, "batch_size_eval": 1024, "accuracy_train_std": [0.019614950819454652, 0.023273648986725407, 0.021766882432432785, 0.02010991814905753, 0.017045535193065325, 0.01737737888085773, 0.016014118706126946, 0.01679657045114759, 0.015958757088528102, 0.016155942533793193, 0.01596311438500395, 0.01518227163568132, 0.0152275985152029, 0.01613562795788522, 0.016497560593516328, 0.014475047184087547, 0.01375485326720945, 0.013932942881616533, 0.014347662296031207, 0.013381431876207361, 0.013369748652504856, 0.012779443879460483, 0.013177816521365017, 0.012835580163715164, 0.0121270168673254, 0.012637639018593043, 0.011665468770664487, 0.012091660229736255, 0.011248941503544715, 0.010918293680332224, 0.010333459329123901, 0.011312460882736736, 0.010807770799336898, 0.010178348916906, 0.009773551939056584, 0.010941167733024235, 0.01004973276457037, 0.010526817701526726, 0.00975635702715544, 0.01003540942870121, 0.009780783035342031, 0.009760096062427455, 0.010298704173448824, 0.0095047277922206, 0.009343049121585949, 0.009387866753850874, 0.009631718520089903, 0.008990679280494055, 0.009976989517861532, 0.008877276737252613, 0.008933253272724324, 0.008505778155329798, 0.009352826169338146, 0.009063736369084321, 0.00843438172824331, 0.00847157921991925, 0.008940527845216914, 0.008762839365941806, 0.007975474301664173, 0.008130940817524244, 0.007979189629117967, 0.0088661228572098, 0.008473267817785457, 0.008388936143375084, 0.008198212738960377, 0.007233035540636464, 0.008318694939722251, 0.007204462291527893, 0.007445878192355825, 0.008460117664426376, 0.008246566645675099, 0.007561911259885926, 0.007321335525645757, 0.007536290694810253, 0.007260207574884914, 0.007911640714959915, 0.008187272872094416, 0.007380952187155609, 0.007933940532930056, 0.007270806024915325, 0.006575894558899535, 0.007518353530795368, 0.007238581022195705, 0.007665618649749297, 0.006103356360731232, 0.007427947911103464, 0.006846588167848092, 0.007071032037598608, 0.007270982383207555, 0.0072817202898270705, 0.006017990107523659, 0.00738951360058846, 0.007017073749008192, 0.0073283233837575425, 0.007241817195791023, 0.007001299383789032, 0.0065844091214092414, 0.006495458975225992, 0.006448377937749774, 0.006901670738145259, 0.006859378505830396, 0.0068020717300697415, 0.006597847706884262, 0.0063679262255633435, 0.006267360460274322, 0.006123203097916173, 0.006250302418852337, 0.005740506625619966, 0.0062250391915668795, 0.005776625613659576, 0.006302255358283306, 0.006603147898033671, 0.006039671424460756, 0.005313882545784357, 0.0062116881737738485, 0.006415474028224812, 0.005875405798104309, 0.0060226563951036385, 0.005652572661988512, 0.005691288922068688, 0.005682932253523899, 0.005284335193546564, 0.005395449029901636, 0.004941394785777839, 0.00578448721776158, 0.00493319230062985, 0.0047646099450848875, 0.004730020374029069, 0.006198710865913823, 0.005602094594019038, 0.00489958688411439, 0.005319378988210593, 0.005498406865341195, 0.0053678111993354285, 0.005099557015786874, 0.004694714289642153, 0.005205383533470721, 0.00499282657095122, 0.005609035890108559, 0.005194195739199554, 0.005311956645036147], "accuracy_test_std": 0.006964734452180116, "error_valid": [0.5567347515060241, 0.44312405873493976, 0.32120081890060237, 0.26901355421686746, 0.2313423616340362, 0.2112404696912651, 0.19839220161897586, 0.1889516072100903, 0.1790830313441265, 0.17448553981551207, 0.17248093938253017, 0.16290797957454817, 0.16000917733433728, 0.15695741952183728, 0.15523814006024095, 0.15227756730045183, 0.15291880412274095, 0.15045680769954817, 0.14632700724774095, 0.14585931617093373, 0.1455842902861446, 0.14425181193524095, 0.14686676393072284, 0.14476068335843373, 0.1427060782191265, 0.14158685523343373, 0.14034556193524095, 0.14290903849774095, 0.14012201148343373, 0.13852480233433728, 0.13997935099774095, 0.13750705948795183, 0.1377717902861446, 0.13608339608433728, 0.13839243693524095, 0.13692759318524095, 0.1392366340361446, 0.13778208537274095, 0.13433323136295183, 0.13445530167545183, 0.13570689006024095, 0.13596132577183728, 0.13616428605045183, 0.13284779743975905, 0.13421116105045183, 0.13385524519954817, 0.13323459855045183, 0.13213596573795183, 0.13187123493975905, 0.1363069465361446, 0.13543186417545183, 0.1340787956513554, 0.13348903426204817, 0.1293180534638554, 0.13138295368975905, 0.13263454207454817, 0.13141383894954817, 0.1338655402861446, 0.13250217667545183, 0.13289927287274095, 0.13054905167545183, 0.1299284050263554, 0.1311799934111446, 0.12933864363704817, 0.13176975480045183, 0.12880918204066272, 0.12994899519954817, 0.12991810993975905, 0.13006077042545183, 0.12808705525225905, 0.12784291462725905, 0.12867681664156627, 0.12820912556475905, 0.12979603962725905, 0.1279752800263554, 0.13018284073795183, 0.1271207878388554, 0.12808705525225905, 0.12628688582454817, 0.12723256306475905, 0.12700901261295183, 0.12897243269954817, 0.12845326618975905, 0.12697812735316272, 0.12811794051204817, 0.1287385871611446, 0.1275178840361446, 0.1278532097138554, 0.12872829207454817, 0.12836208113704817, 0.12452642601656627, 0.13007106551204817, 0.1280973503388554, 0.1299284050263554, 0.12749729386295183, 0.1297151496611446, 0.12576771931475905, 0.1277620246611446, 0.1265413215361446, 0.12993870011295183, 0.12772084431475905, 0.12664280167545183, 0.12686635212725905, 0.12747670368975905, 0.1267545769013554, 0.12688694230045183, 0.12775172957454817, 0.12945041886295183, 0.12723256306475905, 0.1302034309111446, 0.12860622176204817, 0.1309358527861446, 0.12716196818524095, 0.12578830948795183, 0.12591037980045183, 0.12787379988704817, 0.12591037980045183, 0.12639866105045183, 0.12604274519954817, 0.12740610881024095, 0.12639866105045183, 0.1282194206513554, 0.12604274519954817, 0.12652073136295183, 0.12713108292545183, 0.12664280167545183, 0.1270296027861446, 0.13034609139683728, 0.1294710090361446, 0.1276399543486446, 0.1269987175263554, 0.12946071394954817, 0.12738551863704817, 0.1275178840361446, 0.12567653426204817, 0.12505588761295183, 0.12591037980045183, 0.12618540568524095, 0.1276399543486446, 0.12639866105045183, 0.12652073136295183], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.05063972588109721, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 32, "valid_ratio": 0.15, "learning_rate": 0.0030811105564658554, "optimization": "rmsprop", "nb_data_augmentation": 2, "learning_rate_decay_method": "lin", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 1.262938948946669e-07, "rotation_range": [0, 0], "momentum": 0.552340799120736}, "accuracy_valid_max": 0.8754735739834337, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n\n    # rescaling to [-1, 1]\n    X_min = X_train.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X_train.max(axis=(0, 2, 3))[None, :, None, None]\n    def preprocess(a):\n        return (a / 255.) * 2 - 1\n        # return 2 * ((a - X_min) / (X_max - X_min)) - 1\n    X_train = preprocess(X_train)\n    X_valid = preprocess(X_valid)\n\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = preprocess(X_test)\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8734792686370482, "accuracy_valid_std": [0.012127540311068039, 0.015156436394413575, 0.012451428632806079, 0.013083543336811814, 0.007102575600095849, 0.00715994630268559, 0.007416168004475537, 0.009323198678696519, 0.014153703547580086, 0.00806509689259857, 0.012956312202533574, 0.015249796055278524, 0.011815505650047364, 0.010599450896225819, 0.011458361818894548, 0.013895454016377947, 0.010697014566379134, 0.011342158443824692, 0.008822718333554708, 0.009653067214522274, 0.009820659387051461, 0.008668665529712866, 0.008990582727130221, 0.007881007049231486, 0.008001391968602556, 0.0076641150810025085, 0.006404261283356002, 0.008665775938747388, 0.006786706681995981, 0.0071269780969544635, 0.009278713537029717, 0.009096834218394996, 0.006722219038579981, 0.007389563014623561, 0.008781589035298304, 0.008541649704366295, 0.010848126078476794, 0.008651759663326862, 0.009553689574548763, 0.008209907489048571, 0.010482751961559601, 0.008094571552099295, 0.0090550268231768, 0.009422817025369531, 0.008921278909799152, 0.008182044451818945, 0.007568819356077059, 0.009116706062779267, 0.00875780072115389, 0.007970702404961645, 0.009125101110918937, 0.008435950541167556, 0.0089093877466706, 0.009036462464160537, 0.009027841115653816, 0.007966903221237295, 0.0071620502877579794, 0.006138361329984453, 0.008743133270949643, 0.006319616265414882, 0.008046102250568376, 0.008291097443432508, 0.007332548721457716, 0.006734676247770319, 0.006738740345299526, 0.007864347713029202, 0.008314555538660643, 0.00876566906148654, 0.006123349725966647, 0.007147230245825584, 0.008766375636933436, 0.008998399439682066, 0.007744047771066977, 0.008495970114167757, 0.006497500424255005, 0.006982975472782551, 0.008384194091359761, 0.00917731347846205, 0.008253875105369849, 0.007960972070043875, 0.006064531337261597, 0.00813398151966226, 0.008551354705420738, 0.008602009399504536, 0.009635534804579385, 0.007659611638042748, 0.008598624245708918, 0.007820998569777013, 0.007879165605822039, 0.0088670913913444, 0.009510583762753941, 0.007426525963140858, 0.0078195752220114, 0.008573836270802255, 0.007292108374922337, 0.008781624504583316, 0.0088196145898875, 0.00894641127278098, 0.007838974735421339, 0.0077031978718087235, 0.00906519022927966, 0.009165776080169693, 0.01031902058959295, 0.009618030594522839, 0.007521591038025783, 0.008593682136691051, 0.009752552365665086, 0.007027108571595864, 0.008383985632976954, 0.008388475629494468, 0.008003090693350593, 0.0078112595296398486, 0.008273794733142832, 0.007744803563439309, 0.00694343730565493, 0.009219980131820942, 0.007488600318344688, 0.007603466073058434, 0.009779392172715182, 0.00830718701891127, 0.00810435126549326, 0.009144423322787008, 0.00834559082064851, 0.005556316550567661, 0.008165263401747941, 0.00898184786357401, 0.008929337610112106, 0.008525953381476433, 0.007840806720777293, 0.006765306651527859, 0.009473745664067768, 0.00818844906780801, 0.008531946340077977, 0.008010067239151566, 0.008261473667983521, 0.007981211198063607, 0.010186405182802453, 0.007418577932762129, 0.008357610338994329, 0.008293367890237508, 0.008307805141333186], "accuracy_valid": [0.4432652484939759, 0.5568759412650602, 0.6787991810993976, 0.7309864457831325, 0.7686576383659638, 0.7887595303087349, 0.8016077983810241, 0.8110483927899097, 0.8209169686558735, 0.8255144601844879, 0.8275190606174698, 0.8370920204254518, 0.8399908226656627, 0.8430425804781627, 0.844761859939759, 0.8477224326995482, 0.847081195877259, 0.8495431923004518, 0.853672992752259, 0.8541406838290663, 0.8544157097138554, 0.855748188064759, 0.8531332360692772, 0.8552393166415663, 0.8572939217808735, 0.8584131447665663, 0.859654438064759, 0.857090961502259, 0.8598779885165663, 0.8614751976656627, 0.860020649002259, 0.8624929405120482, 0.8622282097138554, 0.8639166039156627, 0.861607563064759, 0.863072406814759, 0.8607633659638554, 0.862217914627259, 0.8656667686370482, 0.8655446983245482, 0.864293109939759, 0.8640386742281627, 0.8638357139495482, 0.867152202560241, 0.8657888389495482, 0.8661447548004518, 0.8667654014495482, 0.8678640342620482, 0.868128765060241, 0.8636930534638554, 0.8645681358245482, 0.8659212043486446, 0.8665109657379518, 0.8706819465361446, 0.868617046310241, 0.8673654579254518, 0.8685861610504518, 0.8661344597138554, 0.8674978233245482, 0.867100727127259, 0.8694509483245482, 0.8700715949736446, 0.8688200065888554, 0.8706613563629518, 0.8682302451995482, 0.8711908179593373, 0.8700510048004518, 0.870081890060241, 0.8699392295745482, 0.871912944747741, 0.872157085372741, 0.8713231833584337, 0.871790874435241, 0.870203960372741, 0.8720247199736446, 0.8698171592620482, 0.8728792121611446, 0.871912944747741, 0.8737131141754518, 0.872767436935241, 0.8729909873870482, 0.8710275673004518, 0.871546733810241, 0.8730218726468373, 0.8718820594879518, 0.8712614128388554, 0.8724821159638554, 0.8721467902861446, 0.8712717079254518, 0.8716379188629518, 0.8754735739834337, 0.8699289344879518, 0.8719026496611446, 0.8700715949736446, 0.8725027061370482, 0.8702848503388554, 0.874232280685241, 0.8722379753388554, 0.8734586784638554, 0.8700612998870482, 0.872279155685241, 0.8733571983245482, 0.873133647872741, 0.872523296310241, 0.8732454230986446, 0.8731130576995482, 0.8722482704254518, 0.8705495811370482, 0.872767436935241, 0.8697965690888554, 0.8713937782379518, 0.8690641472138554, 0.872838031814759, 0.8742116905120482, 0.8740896201995482, 0.8721262001129518, 0.8740896201995482, 0.8736013389495482, 0.8739572548004518, 0.872593891189759, 0.8736013389495482, 0.8717805793486446, 0.8739572548004518, 0.8734792686370482, 0.8728689170745482, 0.8733571983245482, 0.8729703972138554, 0.8696539086031627, 0.8705289909638554, 0.8723600456513554, 0.8730012824736446, 0.8705392860504518, 0.8726144813629518, 0.8724821159638554, 0.8743234657379518, 0.8749441123870482, 0.8740896201995482, 0.873814594314759, 0.8723600456513554, 0.8736013389495482, 0.8734792686370482], "seed": 287601004, "model": "residualv3", "loss_std": [0.35067471861839294, 0.2070487141609192, 0.18123289942741394, 0.17500391602516174, 0.17158938944339752, 0.16511347889900208, 0.1604604870080948, 0.15576674044132233, 0.154781311750412, 0.15159764885902405, 0.15064656734466553, 0.14740385115146637, 0.14596213400363922, 0.1427847295999527, 0.13928525149822235, 0.13902932405471802, 0.1389245092868805, 0.13430005311965942, 0.13331599533557892, 0.13264818489551544, 0.1314190775156021, 0.1286247819662094, 0.12825381755828857, 0.12577711045742035, 0.1249895989894867, 0.1241958737373352, 0.12409121543169022, 0.12305456399917603, 0.12121142446994781, 0.11988389492034912, 0.1196729838848114, 0.11774176359176636, 0.11625345051288605, 0.11729400604963303, 0.11419110745191574, 0.11344639956951141, 0.1121988594532013, 0.11108190566301346, 0.11083921045064926, 0.11221621930599213, 0.10927721858024597, 0.1089915931224823, 0.1067153811454773, 0.10783792287111282, 0.10620361566543579, 0.1065150797367096, 0.1037256270647049, 0.10406751185655594, 0.1031060516834259, 0.10533226281404495, 0.10113007575273514, 0.10234576463699341, 0.10025205463171005, 0.10014747828245163, 0.09974253177642822, 0.09964610636234283, 0.09770891070365906, 0.0970490425825119, 0.09658535569906235, 0.09741462767124176, 0.09674125909805298, 0.09640853852033615, 0.09494400769472122, 0.09288877248764038, 0.09207062423229218, 0.0934005007147789, 0.09408064931631088, 0.09127505868673325, 0.09177080541849136, 0.08943458646535873, 0.09202569723129272, 0.08900056034326553, 0.09022875130176544, 0.09010905772447586, 0.08651300519704819, 0.08839281648397446, 0.08682209998369217, 0.08717398345470428, 0.08782818913459778, 0.08579392731189728, 0.08572010695934296, 0.08566295355558395, 0.08481746166944504, 0.08339298516511917, 0.08449673652648926, 0.08508496731519699, 0.0840088278055191, 0.08401194214820862, 0.08213546872138977, 0.08422621339559555, 0.0851491391658783, 0.0827731341123581, 0.08183526992797852, 0.08186599612236023, 0.08131594210863113, 0.0802474319934845, 0.08009400963783264, 0.0790480524301529, 0.08026555180549622, 0.0805976614356041, 0.07965604960918427, 0.0800786167383194, 0.07840748131275177, 0.07828548550605774, 0.07816068083047867, 0.07978001981973648, 0.07652969658374786, 0.07881151884794235, 0.07712709158658981, 0.07397664338350296, 0.07589712738990784, 0.07431931793689728, 0.07502996176481247, 0.0736670270562172, 0.07665150612592697, 0.0748082771897316, 0.07518366724252701, 0.0745207816362381, 0.07426587492227554, 0.07353095710277557, 0.0724729523062706, 0.0736168622970581, 0.07347618043422699, 0.07170809060335159, 0.07528120279312134, 0.07139864563941956, 0.07280409336090088, 0.07164280116558075, 0.07117120176553726, 0.0692601352930069, 0.07079079002141953, 0.07046342641115189, 0.07007060199975967, 0.07322502881288528, 0.06905315071344376, 0.07040733844041824, 0.0696948692202568, 0.07006321102380753, 0.06873214989900589, 0.06989645957946777, 0.06948614865541458]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:39 2016", "state": "available"}], "summary": "d4f43660d55abb591cd5319cf83252f3"}