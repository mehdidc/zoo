{"content": {"hp_model": {"f0": 32, "f1": 64, "f2": 16, "f3": 64, "nonlin": "rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.08801801585362701, 0.08789107998447587, 0.08115230971577941, 0.07960667616448551, 0.07545488710687853, 0.07130622435596597, 0.06853759466817227, 0.07394791024113263, 0.06817963905827772, 0.06380375177558366, 0.06654568507887694, 0.06554859402633834, 0.06348597802917404, 0.061855335730267545, 0.06355434647308791, 0.05959130299691547, 0.06253209440587823, 0.06135042594574648, 0.061897417724196976, 0.06085524854502412, 0.058053638826970226, 0.05934357840155617, 0.06050372241047772, 0.0580739113123374, 0.05859168588238558, 0.05606239158083236, 0.05984753892736284, 0.056909361887973964, 0.057987704303190855, 0.06311463272741306, 0.058746720368900884, 0.05974866894707454, 0.06359754771291731, 0.05859229466196139, 0.06152967370331948, 0.059389842708809516, 0.0596644333902343, 0.06152286168793554, 0.06289553711123586, 0.06178191194295268, 0.061112605325107226, 0.06007062610710823, 0.05779376984379937, 0.06265803058724768, 0.05886500064406252, 0.05836859889556677, 0.05853885015721155, 0.06024139618669714, 0.059248683837687756, 0.05815370386251416, 0.05750218455133603, 0.05931005931815412, 0.060791912623046986, 0.05922203792744915, 0.06179879703793032, 0.06458720486547127, 0.06052862551978417, 0.06437866221558691, 0.0610074542257599, 0.05850944249140082, 0.059146099071031986, 0.0624373327526655, 0.060342710039527805, 0.061349117764586444, 0.05970089031019317, 0.058301949904862614, 0.05583670538291202, 0.057774633855395954, 0.060929057242308904, 0.06106837630201324, 0.05873594200363107, 0.0606015076961187, 0.06033073873102906, 0.05630111272625376, 0.06007953236427114, 0.060769758803543954, 0.060565740132472706, 0.06125427298251451, 0.05730038235858456, 0.05932494232503436, 0.056509166474206764, 0.05722438671000296, 0.058423572864800266, 0.06020763638714334, 0.05705583972359674, 0.061584866697650335, 0.05966323770421591, 0.05950684467942139, 0.06207738444950822, 0.060375803521747894, 0.05987748083515266, 0.06309244645889094, 0.05769308052861595, 0.06309301181099207, 0.060649164968481606, 0.057160615809382205, 0.06190591711888092, 0.05853336591041696, 0.05765473536159354, 0.060191193842113404, 0.05919191512007239, 0.06169770622066113, 0.05821853136936768, 0.057123318195577893, 0.05936641464357104, 0.06308283469796339, 0.061043839469496514, 0.05935695068829276, 0.0605551382786201, 0.06008621119081014, 0.06231552991584052, 0.05830256170978319, 0.05848932096198088, 0.05987271496643805, 0.05772506687050417, 0.059549987297281906, 0.060744367344115265, 0.05593770730579205, 0.060547922066506124, 0.0619836540405239, 0.06128876574906675, 0.059248683837687756, 0.061152427842037484, 0.057388554190011355, 0.05987271496643805, 0.05733632058343653, 0.06034610886705821, 0.0598296560180918, 0.05969954598318391, 0.060772106610654686, 0.056562163940065774, 0.05699328842212931, 0.06127174010125247, 0.05787009642714035, 0.0582509946107903, 0.05737006022660547, 0.058276707354936876, 0.06027217810606246, 0.061300986374913186, 0.05795770926868476, 0.05986064966544794, 0.05670592178221724, 0.06005815512823115, 0.05674412236011287, 0.060501953752604073, 0.06155807313034877], "moving_avg_accuracy_train": [0.04777390813253011, 0.09728868599397587, 0.15019394766566263, 0.20342097609186746, 0.25584808028990963, 0.3046067398814006, 0.35214399511012795, 0.3964147236111633, 0.4384515268524566, 0.4776588967575724, 0.5143268361480802, 0.5474456397320673, 0.5785326834395834, 0.6065298480775528, 0.6316143444444964, 0.6549998791265528, 0.6765104333825722, 0.6957240361286523, 0.713237475889281, 0.7295996281497504, 0.7440690704552573, 0.7588917379579243, 0.7721780159693608, 0.7839638853061597, 0.7948323687634955, 0.8059929571883508, 0.8145714664393953, 0.8240593499159377, 0.8324407831472355, 0.8402405677843191, 0.8474368611564895, 0.8543370944685513, 0.8601496199614552, 0.8660562505857916, 0.8712686789910679, 0.875879857025696, 0.8795286936122831, 0.8831538551245488, 0.8877107399434192, 0.8909836230273905, 0.8939692215680249, 0.8977834251642345, 0.900849115027329, 0.9033446816872467, 0.9052659552353894, 0.907592804741971, 0.9100728879725931, 0.9115425381813579, 0.9142559424957521, 0.9165756419208757, 0.9177268126685472, 0.9185346095643431, 0.920092293186222, 0.9228119795302504, 0.9236642529627676, 0.924626611552033, 0.9263845829269501, 0.9267101683089539, 0.9281397727129983, 0.9301229528212165, 0.9324490423282514, 0.9343260319207275, 0.9356035190901005, 0.9361649668798857, 0.9376562450412947, 0.9384454021636712, 0.9400451390557378, 0.9414237200296821, 0.9421349813098465, 0.9436269613415125, 0.9450450445748311, 0.9461354196354202, 0.946700247400794, 0.9480792625703531, 0.948576776825366, 0.9500034553175282, 0.9505791640026428, 0.9513302649216555, 0.9526604349656346, 0.953702279270276, 0.9544940430601159, 0.9545995145071163, 0.9554427445323083, 0.9561851794164269, 0.9564862774386396, 0.9567054960803177, 0.9571075180084305, 0.9577870147015635, 0.9578055647374312, 0.9582458290468207, 0.9591950601481626, 0.958512752928527, 0.9586681606176021, 0.959208065188372, 0.9590445064104987, 0.9597091446248706, 0.9596696119394919, 0.9600976055648197, 0.9610687373276149, 0.9613803500406365, 0.9619478873257294, 0.9627857604907468, 0.9634151287187807, 0.9639862664493122, 0.9643426285092003, 0.9633832338811718, 0.9639810927219702, 0.9643026747148333, 0.964759173056603, 0.9651158988232318, 0.9647639474951255, 0.965499055004649, 0.9662665440824973, 0.9666866705477416, 0.9670741970170638, 0.9676535807792128, 0.9679867731530987, 0.968114865416102, 0.9681642598985882, 0.9681075289388499, 0.9681600102317118, 0.9680660536362515, 0.9682497532425058, 0.9690433773158456, 0.9687340132288392, 0.9692627203396902, 0.9693291064382513, 0.9696171107040648, 0.9695327527963089, 0.9692332802275214, 0.9693967368433234, 0.9699038816830874, 0.9700896983340558, 0.970193397928361, 0.9707714790692599, 0.9708658296563097, 0.9703318634075462, 0.9706937260125746, 0.9707111380498714, 0.9712303856906674, 0.9715023960673839, 0.9718578040510069, 0.9722200281639785, 0.9727931119439661, 0.9732688835808948, 0.9731393785059379], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 1234423, "moving_var_accuracy_train": [0.02054111668429884, 0.04055242405588384, 0.061687882063242286, 0.0810171428526742, 0.0976528398587381, 0.10928421783929433, 0.11869391176749544, 0.12446359720885738, 0.1279210729286965, 0.12896392632971607, 0.12816837370905806, 0.12522323269566477, 0.12139854800435548, 0.11631326425381, 0.11034502545027783, 0.10423247199654101, 0.09797356029649731, 0.09149866704120518, 0.08510928548732721, 0.07900783717794758, 0.07299133630584435, 0.06766960592231151, 0.06249137198065499, 0.05749239522680635, 0.052806271098087165, 0.04864667259417957, 0.044444322723493905, 0.04081006984692453, 0.037361298669328406, 0.03417269856585954, 0.031221508453958696, 0.028527876586410823, 0.02597915800122066, 0.023695236769089734, 0.02157023778110194, 0.019604580668795082, 0.01776394867783612, 0.016105829973962617, 0.01468213376983842, 0.01331032626598669, 0.012059517827200569, 0.010984499386140552, 0.009970635536556628, 0.0090296226594878, 0.00815988202196016, 0.007392621877400652, 0.006708717005137908, 0.006057284150249218, 0.005517818801984661, 0.005014465970592463, 0.004524946120345868, 0.004078324330734999, 0.0036923293020543284, 0.003389666616137947, 0.003057237284558123, 0.0027598487625913075, 0.002511678056527431, 0.002261464303443459, 0.002053711791867681, 0.0018837376427556056, 0.0017440601100326893, 0.0016013619084017917, 0.0014559134787728272, 0.0013131591434814359, 0.0012018584241255514, 0.001087277502387175, 0.0010015821752630083, 0.0009185283272521982, 0.0008312285280049272, 0.0007681397149384439, 0.0007094243839541745, 0.0006491822055135507, 0.0005871352586030284, 0.0005455368782835941, 0.0004932108743607039, 0.0004622084906046172, 0.00041897060595520345, 0.0003821509186745605, 0.00035985999792019697, 0.0003336429541242, 0.00030592066780189435, 0.00027542871905689625, 0.00025428517902967404, 0.000233817547141112, 0.00021125173259782458, 0.0001905590706537753, 0.0001729577582645496, 0.00015981742424190092, 0.00014383877875218709, 0.0001311993948360676, 0.00012618881250625594, 0.0001177598195333313, 0.0001062012015284109, 9.820455388541399e-05, 8.862486176124737e-05, 8.373807118915292e-05, 7.537832956915683e-05, 6.948910350213325e-05, 7.102806525830628e-05, 6.479918107872607e-05, 6.121815010058984e-05, 6.14146180564377e-05, 5.8838095548919025e-05, 5.5890070759157894e-05, 5.1444008942791044e-05, 5.458355051912078e-05, 5.2342112208896866e-05, 4.8038635791212114e-05, 4.511028883643717e-05, 4.1744539405986015e-05, 3.868491310158962e-05, 3.967986924645248e-05, 4.101323768335585e-05, 3.850047013620785e-05, 3.6002014002415274e-05, 3.542298249675156e-05, 3.287983866921822e-05, 2.9739523452868362e-05, 2.678752944168223e-05, 2.4137742113649503e-05, 2.1748756477188754e-05, 1.965333140594437e-05, 1.7991708173392013e-05, 2.1861089884112722e-05, 2.053633614066431e-05, 2.099848340817773e-05, 1.8938299094099433e-05, 1.7790987298830053e-05, 1.6075934878355585e-05, 1.5275495765625786e-05, 1.3988408776308417e-05, 1.4904330895171347e-05, 1.3724648255648129e-05, 1.2448965882815002e-05, 1.4211669543699566e-05, 1.2870620888819593e-05, 1.4149638393304745e-05, 1.3913175458236516e-05, 1.2524586523798275e-05, 1.3698690883668285e-05, 1.2994728600673988e-05, 1.2832089254013678e-05, 1.2729737100774863e-05, 1.4412588560662238e-05, 1.5008557559148261e-05, 1.3658645883189883e-05], "duration": 128358.217933, "accuracy_train": [0.4777390813253012, 0.5429216867469879, 0.6263413027108434, 0.6824642319277109, 0.7276920180722891, 0.7434346762048193, 0.7799792921686747, 0.7948512801204819, 0.8167827560240963, 0.8305252259036144, 0.8443382906626506, 0.8455148719879518, 0.8583160768072289, 0.8585043298192772, 0.8573748117469879, 0.8654696912650602, 0.870105421686747, 0.8686464608433735, 0.8708584337349398, 0.8768589984939759, 0.8742940512048193, 0.8922957454819277, 0.8917545180722891, 0.8900367093373494, 0.8926487198795181, 0.9064382530120482, 0.8917780496987951, 0.9094503012048193, 0.9078736822289156, 0.9104386295180723, 0.9122035015060241, 0.9164391942771084, 0.9124623493975904, 0.9192159262048193, 0.9181805346385542, 0.9173804593373494, 0.9123682228915663, 0.9157803087349398, 0.928722703313253, 0.9204395707831325, 0.9208396084337349, 0.9321112575301205, 0.9284403237951807, 0.925804781626506, 0.9225574171686747, 0.9285344503012049, 0.9323936370481928, 0.924769390060241, 0.9386765813253012, 0.9374529367469879, 0.9280873493975904, 0.925804781626506, 0.9341114457831325, 0.947289156626506, 0.9313347138554217, 0.9332878388554217, 0.9422063253012049, 0.9296404367469879, 0.9410062123493976, 0.9479715737951807, 0.9533838478915663, 0.9512189382530121, 0.9471009036144579, 0.9412179969879518, 0.9510777484939759, 0.9455478162650602, 0.9544427710843374, 0.9538309487951807, 0.9485363328313253, 0.957054781626506, 0.9578077936746988, 0.9559487951807228, 0.9517836972891566, 0.9604903990963856, 0.9530544051204819, 0.9628435617469879, 0.9557605421686747, 0.9580901731927711, 0.9646319653614458, 0.9630788780120482, 0.9616199171686747, 0.9555487575301205, 0.9630318147590361, 0.962867093373494, 0.9591961596385542, 0.9586784638554217, 0.9607257153614458, 0.963902484939759, 0.957972515060241, 0.9622082078313253, 0.967738140060241, 0.9523719879518072, 0.9600668298192772, 0.9640672063253012, 0.9575724774096386, 0.9656908885542169, 0.9593138177710844, 0.9639495481927711, 0.9698089231927711, 0.9641848644578314, 0.9670557228915663, 0.9703266189759037, 0.9690794427710844, 0.9691265060240963, 0.9675498870481928, 0.9547486822289156, 0.9693618222891566, 0.9671969126506024, 0.9688676581325302, 0.9683264307228916, 0.9615963855421686, 0.9721150225903614, 0.9731739457831325, 0.9704678087349398, 0.9705619352409639, 0.9728680346385542, 0.9709855045180723, 0.9692676957831325, 0.9686088102409639, 0.9675969503012049, 0.9686323418674698, 0.9672204442771084, 0.9699030496987951, 0.9761859939759037, 0.9659497364457831, 0.9740210843373494, 0.9699265813253012, 0.9722091490963856, 0.968773531626506, 0.9665380271084337, 0.9708678463855421, 0.9744681852409639, 0.9717620481927711, 0.9711266942771084, 0.9759742093373494, 0.971714984939759, 0.9655261671686747, 0.9739504894578314, 0.9708678463855421, 0.9759036144578314, 0.9739504894578314, 0.9750564759036144, 0.9754800451807228, 0.9779508659638554, 0.977550828313253, 0.9719738328313253], "end": "2016-01-20 23:51:31.362000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0], "accuracy_valid": [0.46274038461538464, 0.5311164529914529, 0.6125801282051282, 0.6614583333333334, 0.6965811965811965, 0.7127403846153846, 0.7486645299145299, 0.7646901709401709, 0.7908653846153846, 0.7907318376068376, 0.8043536324786325, 0.8052884615384616, 0.8171741452991453, 0.8134348290598291, 0.811698717948718, 0.8242521367521367, 0.8225160256410257, 0.8161057692307693, 0.8209134615384616, 0.8213141025641025, 0.8187767094017094, 0.8325320512820513, 0.8384081196581197, 0.8323985042735043, 0.8338675213675214, 0.8472222222222222, 0.8311965811965812, 0.8457532051282052, 0.8448183760683761, 0.8489583333333334, 0.8498931623931624, 0.8476228632478633, 0.8456196581196581, 0.8461538461538461, 0.8456196581196581, 0.8490918803418803, 0.8436164529914529, 0.8433493589743589, 0.8505608974358975, 0.8461538461538461, 0.8458867521367521, 0.8544337606837606, 0.8497596153846154, 0.8502938034188035, 0.8464209401709402, 0.8497596153846154, 0.8513621794871795, 0.8520299145299145, 0.8556356837606838, 0.860176282051282, 0.8464209401709402, 0.8478899572649573, 0.8509615384615384, 0.8635149572649573, 0.8513621794871795, 0.8500267094017094, 0.8577724358974359, 0.8465544871794872, 0.8560363247863247, 0.8603098290598291, 0.8659188034188035, 0.8636485042735043, 0.859375, 0.8549679487179487, 0.8625801282051282, 0.859107905982906, 0.8656517094017094, 0.8660523504273504, 0.8613782051282052, 0.8628472222222222, 0.8657852564102564, 0.8645833333333334, 0.8605769230769231, 0.8717948717948718, 0.8632478632478633, 0.8708600427350427, 0.8649839743589743, 0.8656517094017094, 0.8684561965811965, 0.8683226495726496, 0.8696581196581197, 0.8633814102564102, 0.8681891025641025, 0.8651175213675214, 0.8665865384615384, 0.8620459401709402, 0.8641826923076923, 0.8664529914529915, 0.8629807692307693, 0.8655181623931624, 0.8692574786324786, 0.8560363247863247, 0.8639155982905983, 0.8637820512820513, 0.8557692307692307, 0.8728632478632479, 0.8584401709401709, 0.8641826923076923, 0.8697916666666666, 0.8636485042735043, 0.8677884615384616, 0.8671207264957265, 0.8685897435897436, 0.8681891025641025, 0.8664529914529915, 0.8573717948717948, 0.8708600427350427, 0.8652510683760684, 0.8657852564102564, 0.8668536324786325, 0.859107905982906, 0.8697916666666666, 0.8691239316239316, 0.8649839743589743, 0.8680555555555556, 0.8691239316239316, 0.8709935897435898, 0.8703258547008547, 0.8680555555555556, 0.8675213675213675, 0.8676549145299145, 0.8631143162393162, 0.8676549145299145, 0.8696581196581197, 0.8649839743589743, 0.8691239316239316, 0.8675213675213675, 0.8691239316239316, 0.8645833333333334, 0.8641826923076923, 0.8675213675213675, 0.8644497863247863, 0.8667200854700855, 0.8693910256410257, 0.8704594017094017, 0.8657852564102564, 0.8611111111111112, 0.8693910256410257, 0.859909188034188, 0.8708600427350427, 0.8669871794871795, 0.8697916666666666, 0.8723290598290598, 0.8677884615384616, 0.8715277777777778, 0.8645833333333334], "accuracy_test": 0.8598758012820513, "start": "2016-01-19 12:12:13.144000", "learning_rate_per_epoch": [0.001051941653713584, 0.00074383505852893, 0.0006073387921787798, 0.000525970826856792, 0.0004704425809904933, 0.00042945335735566914, 0.00039759656647220254, 0.000371917529264465, 0.00035064719850197434, 0.00033265314414165914, 0.0003171723219566047, 0.0003036693960893899, 0.0002917561214417219, 0.00028114323504269123, 0.0002716101589612663, 0.000262985413428396, 0.00025513331638649106, 0.0002479450195096433, 0.00024133195984177291, 0.00023522129049524665, 0.00022955247550271451, 0.00022427471412811428, 0.0002193449909100309, 0.00021472667867783457, 0.0002103883307427168, 0.00020630272047128528, 0.00020244625920895487, 0.00019879828323610127, 0.00019534066086634994, 0.00019205738499294966, 0.00018893429660238326, 0.0001859587646322325, 0.0001831195259001106, 0.00018040649592876434, 0.0001778105943230912, 0.00017532359925098717, 0.00017293813289143145, 0.0001706474577076733, 0.00016844547644723207, 0.00016632657207082957, 0.00016428568051196635, 0.00016231811605393887, 0.00016041960043366998, 0.00015858616097830236, 0.00015681420336477458, 0.00015510033699683845, 0.00015344146231655031, 0.00015183469804469496, 0.0001502773811807856, 0.00014876700879540294, 0.00014730129623785615, 0.00014587806072086096, 0.00014449529408011585, 0.00014315111911855638, 0.0001418437750544399, 0.00014057161752134562, 0.00013933307491242886, 0.00013812670658808202, 0.0001369511301163584, 0.00013580507948063314, 0.00013468731776811182, 0.00013359672448132187, 0.00013253219367470592, 0.000131492706714198, 0.000130477303173393, 0.0001294850662816316, 0.00012851512292400002, 0.00012756665819324553, 0.00012663888628594577, 0.0001257310650544241, 0.00012484249600674957, 0.00012397250975482166, 0.0001231204514624551, 0.00012228572450112551, 0.0001214677540701814, 0.00012066597992088646, 0.00011987987090833485, 0.00011910893226740882, 0.00011835267650894821, 0.00011761064524762332, 0.00011688240192597732, 0.00011616751726251096, 0.00011546559107955545, 0.00011477623775135726, 0.00011409908620407805, 0.00011343377991579473, 0.00011277997691649944, 0.00011213735706405714, 0.0001115055856644176, 0.00011088438623119146, 0.00011027344589820132, 0.00010967249545501545, 0.00010908126569120213, 0.00010849949467228726, 0.00010792693501571193, 0.00010736333933891729, 0.00010680848936317489, 0.00010626215225784108, 0.00010572410974418744, 0.0001051941653713584, 0.0001046721008606255, 0.00010415774158900604, 0.0001036508911056444, 0.00010315136023564264, 0.00010265898890793324, 0.00010217360249953344, 0.00010169503366341814, 0.00010122312960447744, 0.0001007577302516438, 0.00010029869736172259, 9.984587813960388e-05, 9.939914161805063e-05, 9.895834227791056e-05, 9.852335642790422e-05, 9.809406037675217e-05, 9.767033043317497e-05, 9.725203562993556e-05, 9.683907410362735e-05, 9.643132943892851e-05, 9.602869249647483e-05, 9.56310541369021e-05, 9.523831977276132e-05, 9.485038026468828e-05, 9.446714830119163e-05, 9.408852201886475e-05, 9.371440683025867e-05, 9.334472269983962e-05, 9.297938231611624e-05, 9.261829836759716e-05, 9.226138354279101e-05, 9.190856508212164e-05, 9.15597629500553e-05, 9.121490438701585e-05, 9.087391663342714e-05, 9.053671965375543e-05, 9.020324796438217e-05, 8.987343608168885e-05, 8.954721852205694e-05, 8.922452252591029e-05, 8.89052971615456e-05, 8.858946239342913e-05, 8.82769818417728e-05, 8.796777547104284e-05, 8.766179962549359e-05, 8.735899609746411e-05, 8.705930667929351e-05], "accuracy_train_last": 0.9719738328313253, "error_valid": [0.5372596153846154, 0.46888354700854706, 0.3874198717948718, 0.33854166666666663, 0.30341880341880345, 0.2872596153846154, 0.2513354700854701, 0.2353098290598291, 0.20913461538461542, 0.20926816239316237, 0.19564636752136755, 0.19471153846153844, 0.18282585470085466, 0.1865651709401709, 0.18830128205128205, 0.1757478632478633, 0.17748397435897434, 0.18389423076923073, 0.17908653846153844, 0.17868589743589747, 0.18122329059829057, 0.16746794871794868, 0.16159188034188032, 0.16760149572649574, 0.1661324786324786, 0.1527777777777778, 0.16880341880341876, 0.15424679487179482, 0.15518162393162394, 0.15104166666666663, 0.15010683760683763, 0.1523771367521367, 0.1543803418803419, 0.15384615384615385, 0.1543803418803419, 0.15090811965811968, 0.15638354700854706, 0.15665064102564108, 0.14943910256410253, 0.15384615384615385, 0.15411324786324787, 0.14556623931623935, 0.15024038461538458, 0.14970619658119655, 0.15357905982905984, 0.15024038461538458, 0.14863782051282048, 0.1479700854700855, 0.14436431623931623, 0.13982371794871795, 0.15357905982905984, 0.1521100427350427, 0.14903846153846156, 0.1364850427350427, 0.14863782051282048, 0.14997329059829057, 0.1422275641025641, 0.15344551282051277, 0.14396367521367526, 0.1396901709401709, 0.13408119658119655, 0.13635149572649574, 0.140625, 0.14503205128205132, 0.1374198717948718, 0.14089209401709402, 0.13434829059829057, 0.1339476495726496, 0.13862179487179482, 0.1371527777777778, 0.1342147435897436, 0.13541666666666663, 0.13942307692307687, 0.1282051282051282, 0.1367521367521367, 0.1291399572649573, 0.13501602564102566, 0.13434829059829057, 0.13154380341880345, 0.1316773504273504, 0.13034188034188032, 0.13661858974358976, 0.13181089743589747, 0.1348824786324786, 0.13341346153846156, 0.13795405982905984, 0.1358173076923077, 0.13354700854700852, 0.13701923076923073, 0.13448183760683763, 0.1307425213675214, 0.14396367521367526, 0.13608440170940173, 0.13621794871794868, 0.14423076923076927, 0.12713675213675213, 0.1415598290598291, 0.1358173076923077, 0.13020833333333337, 0.13635149572649574, 0.13221153846153844, 0.13287927350427353, 0.1314102564102564, 0.13181089743589747, 0.13354700854700852, 0.14262820512820518, 0.1291399572649573, 0.13474893162393164, 0.1342147435897436, 0.13314636752136755, 0.14089209401709402, 0.13020833333333337, 0.13087606837606836, 0.13501602564102566, 0.13194444444444442, 0.13087606837606836, 0.12900641025641024, 0.12967414529914534, 0.13194444444444442, 0.13247863247863245, 0.1323450854700855, 0.13688568376068377, 0.1323450854700855, 0.13034188034188032, 0.13501602564102566, 0.13087606837606836, 0.13247863247863245, 0.13087606837606836, 0.13541666666666663, 0.1358173076923077, 0.13247863247863245, 0.1355502136752137, 0.1332799145299145, 0.13060897435897434, 0.12954059829059827, 0.1342147435897436, 0.13888888888888884, 0.13060897435897434, 0.14009081196581197, 0.1291399572649573, 0.13301282051282048, 0.13020833333333337, 0.12767094017094016, 0.13221153846153844, 0.1284722222222222, 0.13541666666666663], "accuracy_train_std": [0.09112523847077073, 0.09169090945575553, 0.08879736822224812, 0.08560534050545354, 0.08319073985929577, 0.08008821329873146, 0.07536446361618718, 0.07506444270994934, 0.07111354085444022, 0.06939222504022555, 0.06613872053670058, 0.06753728900175447, 0.06426624199481146, 0.06498591302172899, 0.06416366057839891, 0.06216665149751689, 0.06047538279888419, 0.06141462394382235, 0.06206713512364215, 0.06148161065530963, 0.0617502700960175, 0.0571544746036156, 0.055390709702114224, 0.05626836061067628, 0.0564568986283232, 0.05309752877071085, 0.05718012382576185, 0.0535396994067008, 0.052143487935761006, 0.051666942859210856, 0.052504574823754084, 0.050425717035880994, 0.05104677505559697, 0.049754556805757244, 0.04999822778727038, 0.05034333428361714, 0.051716412322089705, 0.04967395932094137, 0.04767016778692586, 0.04895604885937175, 0.04877052075512618, 0.04622344055212514, 0.04615875347058681, 0.047676068358022336, 0.048078234275238996, 0.04774306063875317, 0.04661970369263784, 0.048544319647458235, 0.04452668768473699, 0.04424403875122721, 0.048079391762494414, 0.04784543329620613, 0.04565416942637464, 0.04153230084458779, 0.047438801886918625, 0.04637719432746971, 0.04299552080610439, 0.04734326995201125, 0.044146579386996426, 0.04149862880964138, 0.039520181010546766, 0.0411710604310878, 0.041540832892182744, 0.044054196452258755, 0.04080543488975662, 0.04195717381136777, 0.04009037841825245, 0.0395147231501871, 0.04158255469545695, 0.03750665931972357, 0.0379082086990891, 0.03851419232694103, 0.03899234535965127, 0.03618168745157879, 0.03878850842829486, 0.035663070837716804, 0.03905830377374262, 0.038277841610106986, 0.0345995179989671, 0.034972897747091135, 0.035666238187591105, 0.037731574439402185, 0.035028270567509116, 0.03499047624277356, 0.03672282030543622, 0.03658422551491013, 0.03635616495052723, 0.03421595596971477, 0.03658407415497735, 0.03543806700460778, 0.03289629647950864, 0.03997640364775562, 0.03600076614347008, 0.03497201106934713, 0.03725191746853202, 0.03405685730040796, 0.03702235367510422, 0.03361555237150884, 0.032011100022785836, 0.0342333003960431, 0.03287940040532327, 0.030961817496882225, 0.032539432389512644, 0.03173808442554427, 0.03241704261216883, 0.03825804663957806, 0.03187315160468771, 0.03357239925853227, 0.032142987304293456, 0.03169111698163057, 0.03445544434582472, 0.03081876851971504, 0.030262367014534105, 0.031025492473536437, 0.030925168295926033, 0.02874897438676672, 0.030658512625895737, 0.03189788139456395, 0.032085646714543524, 0.03203081400847819, 0.032619884926730305, 0.033364751285477535, 0.03168457268445205, 0.02873449600931957, 0.03352435959568834, 0.02935992672278979, 0.03129840788849861, 0.029140088280302005, 0.03266514404751775, 0.03264653399598604, 0.030582822961126126, 0.0285255245076857, 0.030628018750025184, 0.030261552747499032, 0.028544774715024097, 0.030871736228154336, 0.03234693192038177, 0.02857300556424344, 0.030268624245833157, 0.02923685038903189, 0.02867576596362846, 0.02823246056045438, 0.02865247794900889, 0.027170123847669467, 0.02701803759506963, 0.0295428207722263], "accuracy_test_std": 0.06128424604627612, "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-3, 3], "momentum": 0.7795681119394973, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.0010519416220870189, "patience_threshold": 1, "do_flip": true, "batch_size": 32, "optimization": "adam", "nb_data_augmentation": 4, "learning_rate_decay_method": "sqrt", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 4.0755907626623075e-05, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.02828144613148308}, "accuracy_valid_max": 0.8728632478632479, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = 1234423\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-10, -4], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128, 256],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-3, 3)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_optimizer.learning_rate = learning_rate\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8645833333333334, "loss_train": [4.001457214355469, 2.850757598876953, 2.5962471961975098, 2.3947031497955322, 2.211409568786621, 2.04351806640625, 1.8967970609664917, 1.7568020820617676, 1.6336652040481567, 1.524156928062439, 1.4276924133300781, 1.3442524671554565, 1.2764527797698975, 1.2177960872650146, 1.1658389568328857, 1.1198856830596924, 1.079065203666687, 1.0443021059036255, 1.0129404067993164, 0.9834524393081665, 0.9580743312835693, 0.9323840737342834, 0.9116480350494385, 0.8917340040206909, 0.8717621564865112, 0.854278564453125, 0.836422860622406, 0.8223512172698975, 0.8126512765884399, 0.7964450716972351, 0.7830743789672852, 0.7726567387580872, 0.7627214193344116, 0.7537209987640381, 0.743929386138916, 0.732281506061554, 0.7247691750526428, 0.7174288034439087, 0.7075742483139038, 0.6998580098152161, 0.6951304078102112, 0.6873300075531006, 0.6798082590103149, 0.6741872429847717, 0.6690859198570251, 0.6597501039505005, 0.6538311243057251, 0.6496263146400452, 0.644822895526886, 0.6396691203117371, 0.6356689929962158, 0.6305012106895447, 0.6253840923309326, 0.6205745935440063, 0.6146722435951233, 0.6133370995521545, 0.6071760058403015, 0.6050127744674683, 0.5998786091804504, 0.5969809889793396, 0.5927960276603699, 0.5887912511825562, 0.5844400525093079, 0.5806236267089844, 0.5765931010246277, 0.5736541748046875, 0.5734974145889282, 0.5703334212303162, 0.5646815896034241, 0.5620220303535461, 0.5603614449501038, 0.556031346321106, 0.5549319386482239, 0.5522630214691162, 0.5486512184143066, 0.5453720688819885, 0.5463841557502747, 0.5419428944587708, 0.5392382740974426, 0.5368399024009705, 0.5325174331665039, 0.533411979675293, 0.531019926071167, 0.5257405638694763, 0.5245016813278198, 0.5224311947822571, 0.520919144153595, 0.518137514591217, 0.5174168348312378, 0.5145432949066162, 0.5131675004959106, 0.5094847679138184, 0.510466992855072, 0.5083234906196594, 0.5048333406448364, 0.5033197999000549, 0.5030786991119385, 0.5008829832077026, 0.4970671534538269, 0.4969113767147064, 0.49524733424186707, 0.4935927391052246, 0.49319005012512207, 0.4897233247756958, 0.4873845875263214, 0.48733076453208923, 0.48491084575653076, 0.4840044677257538, 0.48230549693107605, 0.4804192781448364, 0.47912102937698364, 0.4803110957145691, 0.47780144214630127, 0.47479504346847534, 0.4734093248844147, 0.47379666566848755, 0.4703538119792938, 0.46991682052612305, 0.46885934472084045, 0.4686834216117859, 0.46536004543304443, 0.4652767479419708, 0.4642012417316437, 0.46143242716789246, 0.4596966803073883, 0.4603726267814636, 0.4588596522808075, 0.4585384726524353, 0.4574410021305084, 0.4542575180530548, 0.4536295533180237, 0.45290300250053406, 0.4511653184890747, 0.4527583420276642, 0.45015114545822144, 0.4496105909347534, 0.44703447818756104, 0.4469107985496521, 0.44772428274154663, 0.4457099437713623, 0.445904403924942, 0.44276294112205505, 0.44229060411453247, 0.4414750337600708, 0.4407463073730469, 0.43972137570381165], "accuracy_train_first": 0.4777390813253012, "model": "residualv2", "loss_std": [13.402900695800781, 0.2038719803094864, 0.19862070679664612, 0.19441133737564087, 0.19147618114948273, 0.1875816136598587, 0.18403786420822144, 0.17965181171894073, 0.172042116522789, 0.16716913878917694, 0.1619771271944046, 0.15851996839046478, 0.15152554214000702, 0.14955466985702515, 0.1464407742023468, 0.14370103180408478, 0.13823074102401733, 0.13611003756523132, 0.13568724691867828, 0.1321050077676773, 0.1279037594795227, 0.12651754915714264, 0.12464047223329544, 0.12200204282999039, 0.11932752281427383, 0.11756112426519394, 0.11370357125997543, 0.1124657541513443, 0.11149416118860245, 0.10907439142465591, 0.10821864753961563, 0.10631469637155533, 0.10378710180521011, 0.10345599055290222, 0.10158363729715347, 0.09873364120721817, 0.09767674654722214, 0.095039002597332, 0.09464999288320541, 0.09332053363323212, 0.08923432976007462, 0.08958593755960464, 0.08831952512264252, 0.08711519092321396, 0.08768100291490555, 0.08434122055768967, 0.08314768224954605, 0.08205049484968185, 0.0785040408372879, 0.08140451461076736, 0.07901746779680252, 0.07946117222309113, 0.07686633616685867, 0.07540575414896011, 0.07695204764604568, 0.07420839369297028, 0.07464728504419327, 0.07126219570636749, 0.07215919345617294, 0.07036879658699036, 0.07043711841106415, 0.06917820125818253, 0.06808564811944962, 0.06534779816865921, 0.06578771024942398, 0.06387381255626678, 0.06394445896148682, 0.06485120952129364, 0.06292297691106796, 0.06230660527944565, 0.06121039390563965, 0.05862519145011902, 0.061412204056978226, 0.05798657238483429, 0.05797000974416733, 0.05741322040557861, 0.05872591957449913, 0.056156598031520844, 0.059034258127212524, 0.05514862388372421, 0.05390975624322891, 0.057204194366931915, 0.05375067517161369, 0.05215475708246231, 0.05164751783013344, 0.05257824808359146, 0.05327798053622246, 0.05018758773803711, 0.051098745316267014, 0.05099645256996155, 0.05081762373447418, 0.049880996346473694, 0.049288149923086166, 0.05000828579068184, 0.04759538546204567, 0.04872272163629532, 0.047348663210868835, 0.048051733523607254, 0.04679151996970177, 0.04688281565904617, 0.0467989407479763, 0.04579830914735794, 0.045741163194179535, 0.04631967097520828, 0.04443446919322014, 0.043076060712337494, 0.04513401910662651, 0.04377611726522446, 0.0451645702123642, 0.043468065559864044, 0.04268568009138107, 0.044423457235097885, 0.04153599590063095, 0.04132145270705223, 0.043091002851724625, 0.04402879625558853, 0.04003921523690224, 0.04131300747394562, 0.04222733899950981, 0.04230793938040733, 0.03976968303322792, 0.042129404842853546, 0.04090072959661484, 0.03947298973798752, 0.03839731216430664, 0.04075514152646065, 0.039218682795763016, 0.03811013698577881, 0.039941344410181046, 0.03840675204992294, 0.039049435406923294, 0.03809235990047455, 0.03920494765043259, 0.04027640074491501, 0.038135722279548645, 0.03820608928799629, 0.03830690681934357, 0.03766472265124321, 0.03881388530135155, 0.037271421402692795, 0.036884650588035583, 0.03789854049682617, 0.03748985752463341, 0.03616837412118912, 0.03771362081170082, 0.036091774702072144]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:04 2016", "state": "available"}], "summary": "a18175a916215339519beae2d65e544e"}