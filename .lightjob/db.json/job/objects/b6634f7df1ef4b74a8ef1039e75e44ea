{"content": {"hp_model": {"f0": 64, "f1": 16, "f2": 64, "f3": 16, "nonlin": "rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.6603642702102661, 1.2855819463729858, 1.1258951425552368, 1.0345834493637085, 0.9704274535179138, 0.9228252172470093, 0.8845958113670349, 0.8547159433364868, 0.8292368054389954, 0.8127297163009644, 0.7947233319282532, 0.7783402800559998, 0.7682784795761108, 0.7547772526741028, 0.7435885071754456, 0.7346068024635315, 0.7267524003982544, 0.7187202572822571, 0.7110142111778259, 0.7063169479370117, 0.7001746892929077, 0.6930804252624512, 0.6894255876541138, 0.6836163997650146, 0.6786630749702454, 0.6707983016967773, 0.6699506640434265, 0.6681200265884399, 0.6618677377700806, 0.6582481265068054, 0.6567574143409729, 0.6538439989089966, 0.6492851376533508, 0.6463792324066162, 0.6447106003761292, 0.6418354511260986, 0.6379183530807495, 0.6382580399513245, 0.6352929472923279, 0.6305974721908569, 0.6285000443458557, 0.6285521984100342, 0.6240182518959045, 0.6217522025108337, 0.6230642199516296, 0.6204618215560913, 0.6201406121253967, 0.616694986820221, 0.6169755458831787, 0.6121289134025574, 0.6116393208503723, 0.6091748476028442, 0.6089211702346802, 0.606160581111908, 0.6043693423271179, 0.6057860255241394, 0.6016104221343994, 0.6000336408615112, 0.6010275483131409, 0.5972156524658203, 0.5995457768440247, 0.5962114334106445, 0.5962517857551575, 0.5947427153587341, 0.5921761393547058, 0.5935863256454468, 0.5923365354537964, 0.5895499587059021, 0.5916311740875244, 0.5917710065841675, 0.5890921950340271, 0.585159420967102, 0.588330090045929, 0.5852837562561035, 0.5850244164466858, 0.5867787003517151, 0.5827527642250061, 0.5824716687202454, 0.5838044285774231, 0.5821650624275208, 0.5788523554801941, 0.5795877575874329, 0.5777148008346558, 0.5788606405258179, 0.5779921412467957, 0.5772358179092407, 0.5747034549713135, 0.5790559649467468, 0.5755812525749207, 0.5743066072463989, 0.574883759021759, 0.5752092003822327, 0.5745782852172852, 0.5705532431602478, 0.569527804851532, 0.572955846786499, 0.5698227882385254, 0.5704469084739685, 0.5688626170158386, 0.5706930160522461, 0.569482684135437, 0.5670343637466431, 0.5661363005638123, 0.5674440264701843, 0.5666255950927734, 0.563990592956543, 0.5643964409828186, 0.5647196173667908, 0.5631573796272278, 0.5641387104988098, 0.5634927153587341, 0.5647684931755066, 0.5629942417144775, 0.5632577538490295, 0.5647871494293213, 0.5621469020843506, 0.5607262253761292, 0.5605034232139587, 0.5615355968475342, 0.5560678839683533, 0.560615062713623, 0.5594164133071899, 0.5586975812911987, 0.5581170320510864, 0.5586506128311157, 0.5591455101966858, 0.5584123730659485, 0.556202232837677, 0.5585492849349976, 0.5559475421905518, 0.5576258897781372, 0.5535202622413635, 0.5555455088615417, 0.5549882650375366, 0.556229829788208, 0.5548539757728577, 0.5513174533843994, 0.5538637638092041, 0.553767740726471, 0.5517634153366089, 0.5513616800308228, 0.5511738061904907, 0.5541325211524963, 0.5526242256164551, 0.5523055791854858, 0.5490717887878418, 0.5463808178901672, 0.5516937971115112, 0.5488632321357727, 0.5501896739006042, 0.5484786629676819, 0.5518489480018616, 0.5490978360176086, 0.5510618686676025, 0.549332320690155, 0.547035813331604, 0.5490006804466248, 0.5474591851234436, 0.5461045503616333, 0.5494625568389893, 0.546858012676239, 0.5475953817367554, 0.5489643216133118, 0.5482239127159119, 0.5458478331565857, 0.5506323575973511, 0.5448366403579712, 0.5443575978279114, 0.5430199503898621, 0.5458614826202393, 0.5473697781562805, 0.5437182188034058, 0.5461676716804504, 0.5446498990058899, 0.5444477796554565, 0.5443697571754456, 0.5465174913406372, 0.5431855916976929, 0.5444812774658203, 0.5428014993667603, 0.5441374182701111, 0.5407279133796692, 0.543563187122345, 0.544582724571228, 0.5403666496276855, 0.5410996079444885, 0.5412901639938354, 0.5421538352966309, 0.5401357412338257, 0.5417383313179016, 0.5432397127151489, 0.5410057902336121, 0.5406711101531982, 0.5398174524307251, 0.5385396480560303, 0.5383099317550659, 0.5408299565315247, 0.5397062301635742], "moving_avg_accuracy_train": [0.05213279231266149, 0.11255431836009595, 0.1715600169573643, 0.22906557092677182, 0.28285943661464097, 0.3322129874622208, 0.37882100476897024, 0.4218566423296332, 0.46085567566150004, 0.4963640318506564, 0.5302581490374992, 0.5596117976984872, 0.5869601049683671, 0.6129571171505632, 0.6369328854846487, 0.6582645751626972, 0.6771774270087383, 0.6937780696380139, 0.7092673470745909, 0.7244326897019417, 0.738328108035642, 0.7502179282479047, 0.7621857922960378, 0.7741053132071686, 0.7842331017807854, 0.793057323700575, 0.801610421272378, 0.8088387092715706, 0.8163951357327487, 0.8232772276585049, 0.8286364540667038, 0.8343826616673701, 0.8385498923687117, 0.8425048688998822, 0.847096605702908, 0.8506830111971244, 0.8538943919537962, 0.8565498306538578, 0.8601300935279331, 0.861994443209839, 0.8631889432640858, 0.8652357973688326, 0.8682752374559157, 0.8706873215568542, 0.8715633777513441, 0.8723495031775754, 0.8730801594028227, 0.8744120792091259, 0.8750992382478847, 0.8756921407946816, 0.8766698925582367, 0.8776683796370846, 0.8778392824794854, 0.8790507854042482, 0.8803878561520755, 0.8819027537167775, 0.8816248227583979, 0.8827717748910114, 0.883527230955574, 0.8842003101625266, 0.8843853376607166, 0.8845750417995452, 0.886043100413749, 0.8851883022713681, 0.8853327674253683, 0.8855462751281986, 0.8855619370441348, 0.8873614226148228, 0.8883321710129473, 0.8869483053213056, 0.8888018890737358, 0.8887129508296605, 0.8905390597991585, 0.8914106084669448, 0.8917649218358279, 0.8925442914785647, 0.8926689791058098, 0.8929880280679217, 0.8937984027136028, 0.8943837248638002, 0.8947548379751962, 0.8950327478134739, 0.8959269328881619, 0.8961830003851523, 0.8962158955812716, 0.8963316759590063, 0.8979261905394161, 0.8974037307058972, 0.8978587097890081, 0.8961872909257403, 0.8961057526784966, 0.8965997406143198, 0.8959096210886943, 0.8962488360227835, 0.8964425223206066, 0.8965102436315969, 0.8971475773745189, 0.8975931503633497, 0.8990149424294972, 0.9001992241878395, 0.9003141640048897, 0.8999131246462059, 0.9008121314876023, 0.9006380962354368, 0.9011113274941375, 0.9022183960817023, 0.9030056746617473, 0.9035699580111409, 0.9040427195005399, 0.9037941279791497, 0.9047701363467939, 0.9047092558562636, 0.9045197129302903, 0.9053138447599541, 0.9054613712924028, 0.9057893495251502, 0.9047523999108597, 0.9044166003555895, 0.9051490359272656, 0.905471117413212, 0.9062816077909845, 0.9060508708191028, 0.9069454362265797, 0.9057695183075947, 0.9058389614507647, 0.9064174630712383, 0.9063219500951406, 0.9052667618512689, 0.9057421563102911, 0.9058537910853159, 0.9061218893900365, 0.9064512451284968, 0.9073035200538624, 0.9077425052116359, 0.9074795386917182, 0.9087891288309443, 0.9077311830943984, 0.9070278949517636, 0.907168885737595, 0.9082236199662995, 0.9078430037995422, 0.9046083606674192, 0.9046287619114931, 0.9037683970252532, 0.9044191685549631, 0.9053303116673977, 0.904734613234139, 0.9057816225929418, 0.9061612089551409, 0.9061310652133274, 0.9064527442159427, 0.9065165716884975, 0.9065251882887969, 0.9070721975087821, 0.9075856844877858, 0.9080941815498045, 0.9086193663675538, 0.9095916152582071, 0.9094763782086691, 0.9099493378176656, 0.9101632687288208, 0.910111593826223, 0.9108719490996084, 0.9114541064933113, 0.9115408480738159, 0.9111654754295941, 0.9116437312331187, 0.9112629368634982, 0.9114851609939165, 0.9110853103672546, 0.9111090943568303, 0.9113165118522104, 0.9103314207445087, 0.9102607136379888, 0.9094018763492637, 0.9100820326489073, 0.9109988038614529, 0.9111893125718378, 0.9110004444433454, 0.9119487876074457, 0.9120747051218488, 0.9111578457669175, 0.9113511235748695, 0.9108390826055792, 0.9104618789927234, 0.9100039933959612, 0.9107287859802761, 0.9100907138145112, 0.9096512353986378, 0.9091208822422181, 0.9095457221395356, 0.9095583072887695, 0.9087932144647929, 0.9094229182005764, 0.9099570273818107, 0.9106470611354163, 0.9117400246243571, 0.9124887435881857], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.051440576760165654, 0.11140441159167919, 0.16911133253717991, 0.2253975296204348, 0.2781008877747317, 0.32620632634100855, 0.3709884198458384, 0.4122506294896431, 0.4500062353095041, 0.4843016043425597, 0.5160931118299905, 0.5435416825652897, 0.5693450585482185, 0.5939689640732913, 0.6164763939554952, 0.6360352210509096, 0.6532169493223247, 0.668637853693556, 0.6829346481434775, 0.6964488828773225, 0.7091955726204638, 0.7199788520959928, 0.7301323752542399, 0.7406978863169184, 0.7495689921223501, 0.7571592738212596, 0.764753540339736, 0.7713736541144973, 0.7779513736522193, 0.7837430738717113, 0.7879515389807751, 0.7928877950977579, 0.7962895923481478, 0.7995038713004565, 0.803069138584944, 0.8063105292727598, 0.8088462743970651, 0.8110287297416207, 0.8134090081228802, 0.8144677743380772, 0.8155417047355947, 0.8167554712443394, 0.8193575620152368, 0.8209496672594963, 0.8216704361321912, 0.8222825070238666, 0.8223765631268264, 0.8233636514959961, 0.8238022828166525, 0.8242876477672915, 0.8246949146170683, 0.8256769538876657, 0.8252281638076341, 0.8260592219091749, 0.8264511112769923, 0.8275546176662358, 0.8276973992636936, 0.8287942291321284, 0.8298628542704969, 0.8302263252911128, 0.8302503324457365, 0.8302241402685574, 0.8316482035572137, 0.8311455749371851, 0.8312537031079996, 0.8313946990434347, 0.8312826023036244, 0.8323881523142258, 0.8329792857838575, 0.8319051253248242, 0.8333808166703538, 0.8331667349091919, 0.8342822726852155, 0.8354165630654138, 0.8356530858816135, 0.8363145280464641, 0.8361325469568027, 0.8368620833473424, 0.8373964487136624, 0.8378479630102178, 0.8378525233545273, 0.8384802157668155, 0.8392994275768056, 0.8394019525807968, 0.8395318756867984, 0.8396966050985403, 0.8406739101854483, 0.8402880715483042, 0.8403862998791666, 0.8393831320523945, 0.8393816893761459, 0.8399175003425223, 0.83953611951949, 0.839653655440282, 0.8402222754478351, 0.8397705604218769, 0.8400833493062706, 0.8403007356199959, 0.8417476775418066, 0.8427782820579572, 0.8427729442041947, 0.8425351770333988, 0.8431675897705709, 0.8429931618364356, 0.8429408924336655, 0.8431023990110821, 0.8436700005538744, 0.8446457386385472, 0.8445035127603852, 0.8443552134421779, 0.84543435818418, 0.8453435767332319, 0.8452404008181919, 0.8454171266908155, 0.8453045367626978, 0.8455755938168497, 0.8449437274415653, 0.8442468003393515, 0.8445127382459284, 0.8444011669815765, 0.8452081911907984, 0.8452519487377577, 0.8459300671169939, 0.8446849049083066, 0.8449142389186055, 0.8454653778561878, 0.8457387348841986, 0.8450009871487908, 0.8447866153258545, 0.8451613811746697, 0.845582060640034, 0.8458406608636811, 0.8465993329173732, 0.8468110640175787, 0.8464024479678539, 0.8472514256861288, 0.8465811059243683, 0.8457855941647628, 0.8465812464387986, 0.8472729194229307, 0.8466075097754419, 0.8445998320283193, 0.844801052232867, 0.8441092741462519, 0.8446585488682984, 0.8454398348888481, 0.8447594797414543, 0.8463100107300348, 0.8470513094028896, 0.8470287074065013, 0.8471935300958211, 0.846822998151751, 0.8472270888203861, 0.8475520903110885, 0.8482606602238802, 0.8480052008468236, 0.8478830916714033, 0.848712252383781, 0.8482030847508246, 0.8484437231883927, 0.8485891146120232, 0.8483639039697215, 0.8486139040565596, 0.8491633464351656, 0.8484930290638628, 0.8482784564850367, 0.8492979552924819, 0.848450485158565, 0.8488749325952687, 0.8484390641945521, 0.8486815482589071, 0.8485273959273687, 0.8480917191170113, 0.8477910891857319, 0.8472306420235592, 0.8480578354925136, 0.8485459619583224, 0.8487034845501408, 0.8480507683428676, 0.8488865459419394, 0.848619385135396, 0.8479568418590552, 0.847151856797773, 0.8462585308224385, 0.845727210166776, 0.8457159772901587, 0.8461035521891548, 0.8454615410496821, 0.8455124666698645, 0.8455603587453479, 0.8463582385334637, 0.8463947956100872, 0.8455426136770906, 0.8456067575710532, 0.8461345313150773, 0.8468760233548799, 0.8474517399199942, 0.848073718130555], "moving_var_accuracy_train": [0.02446045230883588, 0.0548712543670595, 0.08071918113291865, 0.10240926165558277, 0.11821235536082504, 0.12831307665612438, 0.1350325344859083, 0.13819787593959218, 0.13806640975301354, 0.1356073590110162, 0.1323859237287935, 0.12690206156332864, 0.12094322460174571, 0.11493150392318244, 0.1086118907357518, 0.10184607052286171, 0.09488072715512823, 0.08787288646095973, 0.08124485725442902, 0.07519026008203061, 0.06940897792984435, 0.06374039055897929, 0.05865541943195273, 0.05406855229751542, 0.04958484598029084, 0.04532716341466902, 0.04145284637585715, 0.03777779506486489, 0.034513911786147135, 0.03148878931100345, 0.02859840214915214, 0.02603573205034652, 0.023588451150775708, 0.021370382589957125, 0.01942310075277577, 0.017596551416818706, 0.015929712972415737, 0.014400203867382222, 0.013075548020871337, 0.011799275416412005, 0.010632189348187167, 0.00960667691890352, 0.008729152991399881, 0.007908601039649897, 0.007124648205788045, 0.006417745323881149, 0.0057807755181684675, 0.0052186640596854255, 0.004701047341617815, 0.0042341064083260185, 0.0038192997540936325, 0.0034463425667039032, 0.0031019711800673792, 0.0028049837160910203, 0.0025405751681441776, 0.0023071718830136203, 0.00207714990527089, 0.001881274407494361, 0.0016982833915342842, 0.0015325323729503454, 0.0013795872522310887, 0.0012419524159505784, 0.0011371539392081634, 0.0010300146640653063, 0.0009272010292852582, 0.0008348911962092439, 0.0007514042842488167, 0.0007054071906959649, 0.0006433476436985215, 0.0005962486376011966, 0.0005675457283865348, 0.0005108623456492142, 0.0004897881768006206, 0.0004476457328434384, 0.000404011001229419, 0.00036907665446665543, 0.0003323089120594819, 0.00029999415101555584, 0.00027590509951126463, 0.0002513980077357445, 0.0002274977314352196, 0.00020544306319560156, 0.0001920948594061952, 0.00017347550853270995, 0.00015613769652478852, 0.00014064457273512538, 0.00014946240618586812, 0.00013697284406604705, 0.00012513861335405833, 0.0001377675211670371, 0.0001240506054222055, 0.00011384176160663552, 0.00010674397008281688, 9.710517401811757e-05, 8.773228605398579e-05, 7.90003330322474e-05, 7.475604842782401e-05, 6.906726118042265e-05, 8.03539691766199e-05, 8.49412818072381e-05, 7.656605408040632e-05, 7.035694177728786e-05, 7.059516730745681e-05, 6.380824499767808e-05, 5.944295091581323e-05, 6.452906354238748e-05, 6.365442525152748e-05, 6.01547240120019e-05, 5.615078244353074e-05, 5.109188389974191e-05, 5.455602651317238e-05, 4.913378176899999e-05, 4.454374227917886e-05, 4.576517631722824e-05, 4.138453538549235e-05, 3.821420933734887e-05, 4.40701689268105e-05, 4.067800410600622e-05, 4.143836049531533e-05, 3.8228152798088656e-05, 4.031738939043702e-05, 3.676480640313183e-05, 4.0290551177108006e-05, 4.870654262910804e-05, 4.387928951739723e-05, 4.250333768967231e-05, 3.833510847813228e-05, 4.452239770036543e-05, 4.210415695535031e-05, 3.800590216676877e-05, 3.485220225903811e-05, 3.23432588552452e-05, 3.5646285905381974e-05, 3.381602903355349e-05, 3.105678864557696e-05, 4.3386346775843915e-05, 4.912095473154021e-05, 4.866038716252177e-05, 4.3973254061473784e-05, 4.958810729413432e-05, 4.593311456229412e-05, 0.0001355060488357734, 0.00012195918984903392, 0.00011642532050139948, 0.0001085943207061872, 0.00010520652457760281, 9.787958173032526e-05, 9.795768093407846e-05, 8.945868509797778e-05, 8.052099439471465e-05, 7.340019138175479e-05, 6.609683775985397e-05, 5.948782219607505e-05, 5.623201175720697e-05, 5.298183047994305e-05, 5.001077079068409e-05, 4.7492065546765314e-05, 5.125027014047721e-05, 4.624475932470541e-05, 4.3633500517913994e-05, 3.968204837885181e-05, 3.57378762009931e-05, 3.736734985677948e-05, 3.668077995048777e-05, 3.308041887153494e-05, 3.1040518582651635e-05, 2.99950242468305e-05, 2.830056098955934e-05, 2.5914956967865136e-05, 2.47623859838556e-05, 2.2291238488911298e-05, 2.0449312796527917e-05, 2.7138021931131068e-05, 2.446921519222969e-05, 2.8660707069547996e-05, 2.9958149690096903e-05, 3.45265598264581e-05, 3.140054596240455e-05, 2.858153189580602e-05, 3.381757151828745e-05, 3.057851135035976e-05, 3.508633990584887e-05, 3.191391271468467e-05, 3.1082195031301064e-05, 2.9254518618134326e-05, 2.8215999733821267e-05, 3.0122318372940188e-05, 3.077431133415995e-05, 2.9435151702911516e-05, 2.902310676733947e-05, 2.7745196535780217e-05, 2.4972102356033362e-05, 2.77431953841347e-05, 2.8537616999458925e-05, 2.8251308856822436e-05, 2.9711497201174967e-05, 3.7491470174476966e-05, 3.878754393819843e-05], "duration": 178285.177916, "accuracy_train": [0.521327923126615, 0.6563480527870063, 0.7026113043327796, 0.7466155566514396, 0.7670042278054633, 0.7763949450904393, 0.7982931605297158, 0.8091773803755999, 0.8118469756483019, 0.8159392375530639, 0.8353052037190846, 0.8237946356473791, 0.8330948703972868, 0.8469302267903286, 0.8527148004914176, 0.8502497822651348, 0.8473930936231081, 0.8431838533014949, 0.8486708440037836, 0.8609207733480989, 0.8633868730389442, 0.8572263101582688, 0.8698965687292359, 0.8813810014073459, 0.875383198943337, 0.8724753209786821, 0.8785882994186047, 0.8738933012643041, 0.8844029738833518, 0.8852160549903102, 0.8768694917404946, 0.8860985300733666, 0.8760549686807864, 0.8780996576804172, 0.8884222369301403, 0.8829606606450721, 0.8827968187638427, 0.8804487789544113, 0.8923524593946106, 0.8787735903469915, 0.873939443752307, 0.8836574843115541, 0.895630198239664, 0.8923960784653008, 0.8794478835017534, 0.8794246320136582, 0.879656065430048, 0.8863993574658545, 0.8812836695967147, 0.8810282637158545, 0.8854696584302326, 0.8866547633467147, 0.8793774080610927, 0.8899543117271133, 0.8924214928825213, 0.8955368317990956, 0.8791234441329827, 0.893094344084533, 0.8903263355366371, 0.8902580230251015, 0.886050585144426, 0.8862823790490033, 0.8992556279415835, 0.8774951189899409, 0.8866329538113695, 0.8874678444536729, 0.88570289428756, 0.9035567927510151, 0.8970689065960686, 0.8744935140965301, 0.9054841428456073, 0.8879125066329827, 0.90697404052464, 0.899254546477021, 0.8949537421557769, 0.8995586182631967, 0.8937911677510151, 0.8958594687269288, 0.9010917745247323, 0.8996516242155776, 0.8980948559777593, 0.8975339363579733, 0.9039745985603543, 0.8984876078580657, 0.8965119523463455, 0.8973736993586194, 0.9122768217631044, 0.8927015922042267, 0.9019535215370063, 0.8811445211563308, 0.8953719084533037, 0.9010456320367294, 0.8896985453580657, 0.8993017704295865, 0.8981856990010151, 0.8971197354305095, 0.9028835810608158, 0.9016033072628276, 0.9118110710248246, 0.9108577600129198, 0.9013486223583426, 0.896303770418051, 0.9089031930601699, 0.8990717789659468, 0.9053704088224437, 0.912182013369786, 0.9100911818821521, 0.9086485081556847, 0.9082975729051311, 0.9015568042866371, 0.9135542116555924, 0.9041613314414912, 0.9028138265965301, 0.9124610312269288, 0.9067891100844407, 0.9087411536198781, 0.8954198533822444, 0.901394404358158, 0.9117409560723514, 0.9083698507867294, 0.9135760211909376, 0.9039742380721669, 0.9149965248938722, 0.8951862570367294, 0.9064639497392949, 0.9116239776555003, 0.9054623333102622, 0.8957700676564231, 0.9100207064414912, 0.9068585040605389, 0.9085347741325213, 0.90941544677464, 0.9149739943821521, 0.9116933716315985, 0.9051128400124585, 0.9205754400839794, 0.8982096714654854, 0.900698301668051, 0.9084378028100776, 0.91771622802464, 0.9044174582987264, 0.8754965724783131, 0.904812373108158, 0.8960251130490956, 0.9102761123223514, 0.9135305996793098, 0.8993733273348099, 0.9152047068221669, 0.9095774862149317, 0.9058597715370063, 0.9093478552394795, 0.9070910189414912, 0.9066027376914912, 0.911995280488649, 0.9122070672988187, 0.9126706551079733, 0.9133460297272978, 0.9183418552740864, 0.9084392447628276, 0.9142059742986341, 0.9120886469292175, 0.9096465197028424, 0.9177151465600776, 0.9166935230366371, 0.9123215222983574, 0.9077871216315985, 0.9159480334648394, 0.907835787536914, 0.9134851781676817, 0.9074866547272978, 0.9113231502630121, 0.9131832693106312, 0.9014656007751938, 0.9096243496793098, 0.9016723407507383, 0.9162034393456996, 0.9192497447743633, 0.9129038909653008, 0.909300631286914, 0.9204838760843485, 0.9132079627514765, 0.902906111572536, 0.9130906238464378, 0.9062307138819674, 0.907067046477021, 0.9058830230251015, 0.9172519192391103, 0.9043480643226283, 0.9056959296557769, 0.9043477038344407, 0.9133692812153931, 0.9096715736318751, 0.9019073790490033, 0.9150902518226283, 0.9147640100129198, 0.9168573649178663, 0.9215766960248246, 0.9192272142626431], "end": "2016-02-04 07:11:50.402000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0], "moving_var_accuracy_valid": [0.02381519643676646, 0.053794630182399154, 0.0783859656892517, 0.09906059295918163, 0.11415332930992429, 0.12356519535581637, 0.12925759890831265, 0.131654968519685, 0.13131884360513899, 0.1287725102786463, 0.12499155878569197, 0.11927321922581925, 0.11333822521228484, 0.10746143320082573, 0.10127454947986413, 0.09459002398801254, 0.08778792766674598, 0.08114937352471135, 0.07487402115612997, 0.06903032990449014, 0.06358959980871225, 0.05827715187406729, 0.053377282979386105, 0.04904422489758772, 0.04484807107172941, 0.04088177535097569, 0.037312653771460993, 0.03397582155183196, 0.030967636945501125, 0.02817276737384318, 0.02551489124362673, 0.023182701739336103, 0.020968581586197334, 0.018964707730226894, 0.017182637134492737, 0.015558932943362996, 0.014060909679045643, 0.01269768671311989, 0.01147890956835853, 0.01034110748460866, 0.009317376674636203, 0.008398898069412337, 0.007619946149891007, 0.006880764726881093, 0.006197363824103597, 0.005580999118681165, 0.005022978825767585, 0.004529450034227779, 0.004078236607724149, 0.0036725331591695126, 0.003306772639834906, 0.0029847749860123745, 0.0026881102002345504, 0.002425515098324325, 0.0021843457839813675, 0.0019768707427431425, 0.0017793671477299817, 0.001612257754799601, 0.0014613096164968185, 0.0013163676554925853, 0.001184736077034585, 0.0010662686436024348, 0.000977893385493079, 0.0008823777667108177, 0.0007942452153516494, 0.0007149996125007671, 0.0006436127423623755, 0.0005902516355596051, 0.0005343714210139132, 0.0004913186651382784, 0.00046178578314989, 0.0004160196838390605, 0.00038561753622277516, 0.000358635314599992, 0.00032327527052323964, 0.00029488529510789715, 0.0002656948196500569, 0.00024391534779114523, 0.00022209373011453136, 0.00020171914354302385, 0.00018154741635938347, 0.0001669386546034432, 0.0001562847610497446, 0.00014075088733276055, 0.00012682771872074255, 0.00011438916886050359, 0.00011154637907051981, 0.00010173158424868772, 9.164526506867497e-05, 9.153784975984295e-05, 8.238408351569148e-05, 7.672951568932503e-05, 7.036562610998404e-05, 6.345339573307338e-05, 6.001801457667347e-05, 5.585263130109397e-05, 5.114790014678648e-05, 4.645842141666373e-05, 6.0655347600835175e-05, 6.414912385914303e-05, 5.773446790737385e-05, 5.246982016421139e-05, 5.082235097902863e-05, 4.6013941818986516e-05, 4.143713645128133e-05, 3.752818217709257e-05, 3.6674907561806104e-05, 4.157600009455392e-05, 3.760045388886895e-05, 3.4038342690008506e-05, 4.111548878872449e-05, 3.7078111356378254e-05, 3.34661076457396e-05, 3.040058518765674e-05, 2.747461509611299e-05, 2.5388400925951263e-05, 2.6442856879291137e-05, 2.816993766356264e-05, 2.598945062859689e-05, 2.3502538888998877e-05, 2.7013877668530555e-05, 2.4329722407920576e-05, 2.6035350993450898e-05, 3.7385676227594166e-05, 3.412045539935254e-05, 3.344219701609055e-05, 3.077049389734723e-05, 3.2591889997506716e-05, 2.9746298503977385e-05, 2.8035713626522918e-05, 2.6824883177083806e-05, 2.474426154040888e-05, 2.7450084951848832e-05, 2.5108546985811833e-05, 2.4100395972064433e-05, 2.8177224870002452e-05, 2.940345963006164e-05, 3.2158664304091336e-05, 3.464036074428672e-05, 3.548202832266302e-05, 3.591875548113779e-05, 6.860380935964204e-05, 6.210783456014136e-05, 6.020406339421378e-05, 5.6898981537305646e-05, 5.6702753996732184e-05, 5.5198426736326616e-05, 7.131590118162913e-05, 6.913002456485488e-05, 6.222161976053605e-05, 5.62439564547138e-05, 5.1855206103429466e-05, 4.813928890938846e-05, 4.427599373907841e-05, 4.436703625699401e-05, 4.0517668071230395e-05, 3.660009712060382e-05, 3.912765479110033e-05, 3.754815441804431e-05, 3.431450069495703e-05, 3.107329862004936e-05, 2.8422447258698004e-05, 2.614270292359967e-05, 2.6245414977914628e-05, 2.7664801884554565e-05, 2.5312694220355634e-05, 3.2135825163758235e-05, 3.538609329831199e-05, 3.346888460720015e-05, 3.1831827511169614e-05, 2.917783145324787e-05, 2.64739147797913e-05, 2.5534851849560583e-05, 2.379477186483431e-05, 2.4242203872636573e-05, 2.7976224801098927e-05, 2.732300934059665e-05, 2.481402870893555e-05, 2.6166971863174884e-05, 2.983699243284729e-05, 2.7495667258538516e-05, 2.869677286990437e-05, 3.1659104122898905e-05, 3.567547539447561e-05, 3.464864260723096e-05, 3.118491394416178e-05, 2.9418351270732715e-05, 3.018612087252286e-05, 2.719084955438923e-05, 2.4492407456997306e-05, 2.7772676117851238e-05, 2.500743628472743e-05, 2.9042619078588287e-05, 2.6175387122923652e-05, 2.606475453456318e-05, 2.8406573086921135e-05, 2.8548961848353445e-05, 2.9175777713230026e-05], "accuracy_test": 0.20383848852040817, "start": "2016-02-02 05:40:25.224000", "learning_rate_per_epoch": [0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388, 0.0014037447981536388], "accuracy_train_first": 0.521327923126615, "accuracy_train_last": 0.9192272142626431, "batch_size_eval": 1024, "accuracy_train_std": [0.018175436395289922, 0.017839150358147274, 0.015797866349716826, 0.017049926111060428, 0.018732781446302117, 0.017031256526097157, 0.018632338887580438, 0.018748918691249884, 0.020085172062119443, 0.01828614355390169, 0.018777443648941964, 0.020260657157115895, 0.018066624898017546, 0.01783982848303636, 0.01929506176702573, 0.018904083389678646, 0.01782581078087713, 0.018591538109122712, 0.018297224546026284, 0.01984256084016331, 0.019384078682668823, 0.01868039692117583, 0.01829755327051064, 0.017592010793455903, 0.018723536348481663, 0.018233992591186115, 0.020633125639852767, 0.019076917956307816, 0.017695014161163876, 0.01819152867206855, 0.020021240084808458, 0.019594131816948034, 0.019961123609165896, 0.019311764922996057, 0.01945932596185989, 0.018183074453260256, 0.02031194434409074, 0.019220640357337376, 0.01800834928931173, 0.019963580529670708, 0.02052943596763412, 0.019194996987605847, 0.01947821718001335, 0.019516471026377342, 0.02006730058851584, 0.021739116743566534, 0.01951318950181367, 0.019754813353963854, 0.020625274965807593, 0.020793805118395226, 0.018476210667339656, 0.020382622846632368, 0.020290608804261596, 0.020471098136990076, 0.01953709165403513, 0.018169008236988426, 0.01825465021140696, 0.019748699517298414, 0.020598196011772336, 0.01984299502382927, 0.020586623551755643, 0.019177334247762842, 0.01873573153101896, 0.0205362992306033, 0.02025766411753515, 0.01914466738972951, 0.019741578218977342, 0.01810616013310857, 0.02083325889970644, 0.021772119579720056, 0.02015719860490157, 0.019449391871524755, 0.020067790534740837, 0.01981266969433999, 0.019086441895318915, 0.018836271010278256, 0.02088045910875804, 0.020070304216479883, 0.018727534181060126, 0.01939950440471276, 0.018175608417304062, 0.01926852210104457, 0.01963315754076876, 0.019165070887477594, 0.02017263966195716, 0.019642507372469562, 0.01753518923328361, 0.018765104746468808, 0.017840711749008406, 0.020192532603049002, 0.019489461528365774, 0.018515743928743687, 0.021128829861651138, 0.018944702238426207, 0.01913082706977737, 0.017630364612899314, 0.018173528664722233, 0.018862170771824505, 0.018130268875292496, 0.01744862297544098, 0.0195900215556223, 0.01854242527392317, 0.018889301621718792, 0.018254464316478706, 0.020404134645288376, 0.01855660846428648, 0.017066549024473566, 0.018566452504579677, 0.01837161360607129, 0.019074574604365832, 0.01702173146863266, 0.01890092834527138, 0.018677617962139785, 0.018399801604688692, 0.017327695824637165, 0.019379221517368525, 0.0190914971340613, 0.019065278420700296, 0.018567992235177817, 0.019077435027713585, 0.019001294883551876, 0.01898521277865249, 0.016929399339902663, 0.02035470437074261, 0.018801012284852037, 0.01830891649336705, 0.019468682907652992, 0.016975981770539297, 0.017427048955245928, 0.018909227731219655, 0.016780356525010847, 0.019460496520291942, 0.016485889107755607, 0.018116852104506396, 0.018203626161131754, 0.0172139281291374, 0.01844741846370599, 0.01815534322034427, 0.018529484746217576, 0.01735493391330111, 0.01782437032608627, 0.019193950938035394, 0.01952790638213744, 0.017863535855259363, 0.01854778229392942, 0.01818411277269831, 0.016868324952870183, 0.017554482624283524, 0.0190350317666046, 0.018612030146682647, 0.01731857648485223, 0.0184147236121304, 0.018027584666648402, 0.01835347892629894, 0.016612103540637347, 0.018293558137871584, 0.017940118653513006, 0.0183151979771457, 0.018262672670713423, 0.01747943873775511, 0.018136019672106943, 0.01901598771457642, 0.019207446293218952, 0.01622754867003247, 0.018710303218134043, 0.020290619842459296, 0.01835324875209303, 0.017138276947682513, 0.01633466615704797, 0.0185366122826532, 0.016991819572854935, 0.018082300414409544, 0.017645586848746477, 0.017831680523434874, 0.018267325413476965, 0.018029754558523247, 0.018960298002800614, 0.017850779884928496, 0.016599787209363653, 0.01666612010036288, 0.017092623630558232, 0.01674888807552719, 0.01800152177546003, 0.018990283722427313, 0.018637344297290644, 0.01835390483354545, 0.018688541068137134, 0.017949423481979355, 0.017697689152246735, 0.018177268367489275, 0.016759932744385746, 0.017887364979430213, 0.016570914809737082, 0.016521022555668806, 0.017184162437709413, 0.016908532147718862, 0.015989071552780686, 0.017086279739819302], "accuracy_test_std": 0.009884877805366695, "error_valid": [0.4855942323983433, 0.3489210749246988, 0.31152637895331325, 0.2680266966302711, 0.24756888883659633, 0.2408447265625, 0.22597273861069278, 0.21638948371611444, 0.21019331231174698, 0.20704007435993976, 0.19778332078313254, 0.2094211808170181, 0.19842455760542166, 0.1844158862010542, 0.18095673710466864, 0.18793533509036142, 0.19214749623493976, 0.19257400696536142, 0.18839420180722888, 0.1819230045180723, 0.1760842196912651, 0.18297163262424698, 0.1784859163215362, 0.16421251411897586, 0.1705910556287651, 0.1745281908885542, 0.16689806099397586, 0.16904532191265065, 0.1628491505082832, 0.16413162415286142, 0.17417227503765065, 0.16268589984939763, 0.17309423239834332, 0.1715676181287651, 0.16484345585466864, 0.16451695453689763, 0.16833201948418675, 0.16932917215737953, 0.1651684864457832, 0.17600332972515065, 0.17479292168674698, 0.17232063017695776, 0.15722362104668675, 0.16472138554216864, 0.1718426440135542, 0.1722088549510542, 0.1767769319465362, 0.16775255318147586, 0.17225003529743976, 0.17134406767695776, 0.17163968373493976, 0.16548469267695776, 0.17881094691265065, 0.16646125517695776, 0.17002188441265065, 0.1625138248305723, 0.17101756635918675, 0.16133430205195776, 0.16051951948418675, 0.16650243552334332, 0.16953360316265065, 0.1700115893260542, 0.15553522684487953, 0.1733780826430723, 0.16777314335466864, 0.16733633753765065, 0.16972626835466864, 0.15766189759036142, 0.16170051298945776, 0.17776231880647586, 0.15333796121987953, 0.1687600009412651, 0.1556778873305723, 0.15437482351280118, 0.1622182087725903, 0.15773249246987953, 0.16550528285015065, 0.15657208913780118, 0.15779426298945776, 0.1580884083207832, 0.16210643354668675, 0.1558705525225903, 0.1533276661332832, 0.1596753223832832, 0.15929881635918675, 0.1588208301957832, 0.15053034403237953, 0.16318447618599397, 0.1587296451430723, 0.1696453783885542, 0.1606312947100903, 0.1552602009600903, 0.16389630788780118, 0.1592885212725903, 0.15466014448418675, 0.16429487481174698, 0.15710155073418675, 0.15774278755647586, 0.14522984516189763, 0.14794627729668675, 0.15727509647966864, 0.1596047275037651, 0.15114069559487953, 0.1585766895707832, 0.1575295321912651, 0.15544404179216864, 0.15122158556099397, 0.14657261859939763, 0.1567765201430723, 0.15697948042168675, 0.14485333913780118, 0.15547345632530118, 0.15568818241716864, 0.1529923404555723, 0.15570877259036142, 0.1519848926957832, 0.16074306993599397, 0.1620255435805723, 0.15309382059487953, 0.1566029743975903, 0.14752859092620485, 0.1543542333396084, 0.14796686746987953, 0.16652155496987953, 0.15302175498870485, 0.1495743717055723, 0.15180105186370485, 0.16163874246987953, 0.1571427310805723, 0.15146572618599397, 0.15063182417168675, 0.15183193712349397, 0.14657261859939763, 0.1512833560805723, 0.15727509647966864, 0.14510777484939763, 0.15945177193147586, 0.16137401167168675, 0.14625788309487953, 0.14650202371987953, 0.15938117705195776, 0.1734692676957832, 0.15338796592620485, 0.1621167286332832, 0.1503979786332832, 0.14752859092620485, 0.1613637165850903, 0.13973521037274095, 0.14627700254141573, 0.15317471056099397, 0.15132306570030118, 0.15651178934487953, 0.14913609516189763, 0.1495228962725903, 0.14536221056099397, 0.15429393354668675, 0.15321589090737953, 0.1438253012048193, 0.1563794239457832, 0.14939053087349397, 0.15010236257530118, 0.15366299181099397, 0.14913609516189763, 0.14589167215737953, 0.15753982727786142, 0.15365269672439763, 0.14152655544051207, 0.15917674604668675, 0.14730504047439763, 0.15548375141189763, 0.14913609516189763, 0.15285997505647586, 0.15582937217620485, 0.1549145801957832, 0.15781338243599397, 0.14449742328689763, 0.14706089984939763, 0.14987881212349397, 0.1578236775225903, 0.14359145566641573, 0.15378506212349397, 0.15800604762801207, 0.1600930087537651, 0.1617814029555723, 0.15905467573418675, 0.15438511859939763, 0.15040827371987953, 0.1603165592055723, 0.15402920274849397, 0.15400861257530118, 0.14646084337349397, 0.15327619070030118, 0.16212702371987953, 0.1538159473832832, 0.14911550498870485, 0.14645054828689763, 0.14736681099397586, 0.14632847797439763], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.0026774712832381975, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 16, "valid_ratio": 0.15, "learning_rate": 0.001403744783288199, "optimization": "adam", "nb_data_augmentation": 2, "learning_rate_decay_method": "none", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 3.800696960081472e-05, "rotation_range": [0, 0], "momentum": 0.8583089739025772}, "accuracy_valid_max": 0.860264789627259, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8536715220256024, "accuracy_valid_std": [0.021999864533376837, 0.02018140957340947, 0.021279570897105214, 0.02462763923599167, 0.018819216871007773, 0.02105810190799998, 0.0199067637260805, 0.017134700455417196, 0.01395392806501898, 0.014149419907953218, 0.01511441449208759, 0.016651057146559928, 0.017607111036891342, 0.016470855229560374, 0.01530503418692864, 0.015066419046622596, 0.01727423949229493, 0.016442138614096016, 0.022236297594352465, 0.0145358682623831, 0.014724490749457544, 0.017125956044387207, 0.020381740242533506, 0.015267648770274846, 0.01597409722762963, 0.016381072606428047, 0.015701619953907626, 0.020866500630574825, 0.012944988730290208, 0.020750284653516575, 0.01793856703986218, 0.013094721145479328, 0.022830723062877336, 0.018084780690027388, 0.019155635549312374, 0.014532308291978066, 0.012569860041897907, 0.013038673818794816, 0.01829846875721514, 0.016281420440552346, 0.01961309632871349, 0.019612847027129347, 0.015712449298826724, 0.015419859144127893, 0.019482475042604273, 0.017332647304870826, 0.023196711820597894, 0.014353970217574244, 0.020520817462134508, 0.017353211891376015, 0.021776991548284764, 0.017926978609531977, 0.01944593606318258, 0.017607264783230173, 0.01940527111953636, 0.013850621132547909, 0.01348993133763532, 0.021054017461604384, 0.016050370317514023, 0.02265864774529726, 0.020755318116589018, 0.018243426466781086, 0.017026359286498507, 0.01476363884587567, 0.018183860675517256, 0.019120617424975856, 0.012573248208721261, 0.017405350517607665, 0.02187224552861324, 0.013897441405939234, 0.017266739841041084, 0.018438576756635242, 0.016686270109861866, 0.015596000368816728, 0.0179076544973641, 0.012523633267067685, 0.019853018710954573, 0.01259648035211061, 0.021523939254966852, 0.015206117372488392, 0.014620861999317808, 0.013073193534869835, 0.017066727772350944, 0.015342421343230982, 0.014858382670251857, 0.014748090226324182, 0.01907014434237252, 0.012888617290353377, 0.02063478180458861, 0.01788184540174676, 0.015116487668528085, 0.017011794916739414, 0.011987544192003286, 0.014605415815724946, 0.013464247912929281, 0.02480714634910529, 0.018118647533465072, 0.01684942453580176, 0.01607672014915455, 0.01684581149668722, 0.017889910768995067, 0.021354642675454043, 0.015545193942096148, 0.01592202755334214, 0.02045266901508693, 0.02603306363581043, 0.012783101184651641, 0.016077228249046116, 0.01808972910385885, 0.017613613033899096, 0.01689466101995259, 0.017913478691530014, 0.020864903986770635, 0.017616658760371245, 0.024684928568446514, 0.0188334114345288, 0.016266150054504082, 0.01882443385826287, 0.019811690360911724, 0.015277219851384286, 0.017810205906885264, 0.01598288556389826, 0.015217512557834103, 0.012290139603948216, 0.015846721423422642, 0.021673773693401184, 0.01796689519921573, 0.020648363677246023, 0.019450865690383224, 0.0158779997913726, 0.019537850321091954, 0.01718766958377322, 0.019391173963146137, 0.022101059579129615, 0.02312197285081902, 0.016495066982941415, 0.01739491475215201, 0.01926915600058682, 0.02311117875522518, 0.02388340835360776, 0.026862202923626653, 0.017348649748996473, 0.019118704144168216, 0.019942844296360496, 0.02064609406938847, 0.018907569233347125, 0.014956590944085928, 0.015352034008901935, 0.017773527555735653, 0.017406161698878226, 0.01743947235758615, 0.01990470111999213, 0.01800144140407019, 0.01719163963527792, 0.02043341720198293, 0.017910153193417343, 0.019573603035624547, 0.016033437596003605, 0.015222614006564612, 0.015360171190602828, 0.01346039761639252, 0.01687478663243585, 0.018329561412626618, 0.01872436667393201, 0.021448455487494102, 0.016954270256118873, 0.014487510041556594, 0.01626454819150292, 0.017265697297012272, 0.018522364387080215, 0.01645857069992787, 0.02007257951324076, 0.01952964343084636, 0.01689007081242311, 0.016559737881824955, 0.01766003245377783, 0.018300914093932336, 0.0161552403236318, 0.017619164656946667, 0.015602490348086346, 0.015810438245937516, 0.011463075282824725, 0.019524832623814183, 0.018521692657834953, 0.017012779023439707, 0.012613818847620925, 0.01761149182100476, 0.01752269135105844, 0.016371977734762285, 0.0116818617566438, 0.018580395748965726, 0.014885439192138155, 0.020380638675741584, 0.017274795887247955, 0.012978406610880322, 0.012757697729173723, 0.021587585380825856, 0.01670223784481899], "accuracy_valid": [0.5144057676016567, 0.6510789250753012, 0.6884736210466867, 0.7319733033697289, 0.7524311111634037, 0.7591552734375, 0.7740272613893072, 0.7836105162838856, 0.789806687688253, 0.7929599256400602, 0.8022166792168675, 0.7905788191829819, 0.8015754423945783, 0.8155841137989458, 0.8190432628953314, 0.8120646649096386, 0.8078525037650602, 0.8074259930346386, 0.8116057981927711, 0.8180769954819277, 0.8239157803087349, 0.817028367375753, 0.8215140836784638, 0.8357874858810241, 0.8294089443712349, 0.8254718091114458, 0.8331019390060241, 0.8309546780873494, 0.8371508494917168, 0.8358683758471386, 0.8258277249623494, 0.8373141001506024, 0.8269057676016567, 0.8284323818712349, 0.8351565441453314, 0.8354830454631024, 0.8316679805158133, 0.8306708278426205, 0.8348315135542168, 0.8239966702748494, 0.825207078313253, 0.8276793698230422, 0.8427763789533133, 0.8352786144578314, 0.8281573559864458, 0.8277911450489458, 0.8232230680534638, 0.8322474468185241, 0.8277499647025602, 0.8286559323230422, 0.8283603162650602, 0.8345153073230422, 0.8211890530873494, 0.8335387448230422, 0.8299781155873494, 0.8374861751694277, 0.8289824336408133, 0.8386656979480422, 0.8394804805158133, 0.8334975644766567, 0.8304663968373494, 0.8299884106739458, 0.8444647731551205, 0.8266219173569277, 0.8322268566453314, 0.8326636624623494, 0.8302737316453314, 0.8423381024096386, 0.8382994870105422, 0.8222376811935241, 0.8466620387801205, 0.8312399990587349, 0.8443221126694277, 0.8456251764871988, 0.8377817912274097, 0.8422675075301205, 0.8344947171498494, 0.8434279108621988, 0.8422057370105422, 0.8419115916792168, 0.8378935664533133, 0.8441294474774097, 0.8466723338667168, 0.8403246776167168, 0.8407011836408133, 0.8411791698042168, 0.8494696559676205, 0.836815523814006, 0.8412703548569277, 0.8303546216114458, 0.8393687052899097, 0.8447397990399097, 0.8361036921121988, 0.8407114787274097, 0.8453398555158133, 0.835705125188253, 0.8428984492658133, 0.8422572124435241, 0.8547701548381024, 0.8520537227033133, 0.8427249035203314, 0.8403952724962349, 0.8488593044051205, 0.8414233104292168, 0.8424704678087349, 0.8445559582078314, 0.848778414439006, 0.8534273814006024, 0.8432234798569277, 0.8430205195783133, 0.8551466608621988, 0.8445265436746988, 0.8443118175828314, 0.8470076595444277, 0.8442912274096386, 0.8480151073042168, 0.839256930064006, 0.8379744564194277, 0.8469061794051205, 0.8433970256024097, 0.8524714090737951, 0.8456457666603916, 0.8520331325301205, 0.8334784450301205, 0.8469782450112951, 0.8504256282944277, 0.8481989481362951, 0.8383612575301205, 0.8428572689194277, 0.848534273814006, 0.8493681758283133, 0.848168062876506, 0.8534273814006024, 0.8487166439194277, 0.8427249035203314, 0.8548922251506024, 0.8405482280685241, 0.8386259883283133, 0.8537421169051205, 0.8534979762801205, 0.8406188229480422, 0.8265307323042168, 0.8466120340737951, 0.8378832713667168, 0.8496020213667168, 0.8524714090737951, 0.8386362834149097, 0.860264789627259, 0.8537229974585843, 0.846825289439006, 0.8486769342996988, 0.8434882106551205, 0.8508639048381024, 0.8504771037274097, 0.854637789439006, 0.8457060664533133, 0.8467841090926205, 0.8561746987951807, 0.8436205760542168, 0.850609469126506, 0.8498976374246988, 0.846337008189006, 0.8508639048381024, 0.8541083278426205, 0.8424601727221386, 0.8463473032756024, 0.8584734445594879, 0.8408232539533133, 0.8526949595256024, 0.8445162485881024, 0.8508639048381024, 0.8471400249435241, 0.8441706278237951, 0.8450854198042168, 0.842186617564006, 0.8555025767131024, 0.8529391001506024, 0.850121187876506, 0.8421763224774097, 0.8564085443335843, 0.846214937876506, 0.8419939523719879, 0.8399069912462349, 0.8382185970444277, 0.8409453242658133, 0.8456148814006024, 0.8495917262801205, 0.8396834407944277, 0.845970797251506, 0.8459913874246988, 0.853539156626506, 0.8467238092996988, 0.8378729762801205, 0.8461840526167168, 0.8508844950112951, 0.8535494517131024, 0.8526331890060241, 0.8536715220256024], "seed": 46073259, "model": "residualv3", "loss_std": [0.32579338550567627, 0.25956833362579346, 0.2605850398540497, 0.2573097050189972, 0.2562221884727478, 0.25297293066978455, 0.24676859378814697, 0.24535296857357025, 0.2398761659860611, 0.23998741805553436, 0.23575067520141602, 0.2330321967601776, 0.23356319963932037, 0.23025868833065033, 0.22683948278427124, 0.2239830046892166, 0.22126182913780212, 0.21720276772975922, 0.21718789637088776, 0.2156127542257309, 0.2143203318119049, 0.21585366129875183, 0.21047735214233398, 0.211703822016716, 0.2103288620710373, 0.20860689878463745, 0.20816828310489655, 0.2070980668067932, 0.20533610880374908, 0.20333632826805115, 0.20688459277153015, 0.2038743495941162, 0.20511315762996674, 0.204054057598114, 0.19657929241657257, 0.20109675824642181, 0.19916653633117676, 0.1999075710773468, 0.19738291203975677, 0.19682101905345917, 0.19338390231132507, 0.1970183402299881, 0.19094403088092804, 0.19336427748203278, 0.19527551531791687, 0.19508366286754608, 0.19220033288002014, 0.19210025668144226, 0.19209089875221252, 0.18838346004486084, 0.19345246255397797, 0.18788042664527893, 0.1919897198677063, 0.18866638839244843, 0.18550388514995575, 0.18947480618953705, 0.18802686035633087, 0.1858701854944229, 0.18688762187957764, 0.18493786454200745, 0.18751050531864166, 0.18523862957954407, 0.1866239458322525, 0.18511103093624115, 0.18489255011081696, 0.181684210896492, 0.18393844366073608, 0.18144230544567108, 0.18328604102134705, 0.18050628900527954, 0.18179161846637726, 0.17985890805721283, 0.1808156818151474, 0.18131893873214722, 0.17997071146965027, 0.17809559404850006, 0.17734982073307037, 0.1795405149459839, 0.1792798638343811, 0.17850975692272186, 0.17610056698322296, 0.17561186850070953, 0.17970451712608337, 0.1756657063961029, 0.1816130131483078, 0.17729994654655457, 0.17584259808063507, 0.17902998626232147, 0.1757304072380066, 0.17367659509181976, 0.1757984608411789, 0.17451554536819458, 0.17218990623950958, 0.174132838845253, 0.17526279389858246, 0.17415234446525574, 0.17229105532169342, 0.17380498349666595, 0.17247021198272705, 0.17581355571746826, 0.17204394936561584, 0.16898740828037262, 0.17113374173641205, 0.17007669806480408, 0.16866712272167206, 0.16883766651153564, 0.17135821282863617, 0.17067097127437592, 0.1698305457830429, 0.16986140608787537, 0.17113007605075836, 0.1690162867307663, 0.1683155745267868, 0.16843560338020325, 0.16969938576221466, 0.16921140253543854, 0.1696418821811676, 0.169450581073761, 0.1675318330526352, 0.16681958734989166, 0.16780595481395721, 0.17057079076766968, 0.1711105853319168, 0.16946108639240265, 0.16879375278949738, 0.17143456637859344, 0.1700742095708847, 0.167510524392128, 0.16885636746883392, 0.16454556584358215, 0.16966769099235535, 0.1666119396686554, 0.16751804947853088, 0.16954469680786133, 0.1677865982055664, 0.16924229264259338, 0.16581349074840546, 0.16787375509738922, 0.16996309161186218, 0.16231735050678253, 0.16849106550216675, 0.165554940700531, 0.1697525829076767, 0.16704779863357544, 0.1664699763059616, 0.16665717959403992, 0.1657356172800064, 0.1705399602651596, 0.1658765822649002, 0.16574615240097046, 0.16440615057945251, 0.16531258821487427, 0.1683351993560791, 0.16704359650611877, 0.16502337157726288, 0.16449174284934998, 0.16769517958164215, 0.16294816136360168, 0.1653335988521576, 0.1646125465631485, 0.16629411280155182, 0.16201135516166687, 0.16407866775989532, 0.16483832895755768, 0.16399051249027252, 0.16595469415187836, 0.16330811381340027, 0.16349616646766663, 0.16182485222816467, 0.16227971017360687, 0.16382573544979095, 0.161491259932518, 0.1630077213048935, 0.16214990615844727, 0.1609651893377304, 0.1664685755968094, 0.16398242115974426, 0.16168710589408875, 0.1616072803735733, 0.16088418662548065, 0.16308379173278809, 0.1617649644613266, 0.16331732273101807, 0.16267189383506775, 0.16138851642608643, 0.16017946600914001, 0.15976570546627045, 0.1631685346364975, 0.16248933970928192, 0.1622467339038849, 0.1637183278799057, 0.16227354109287262, 0.16293352842330933, 0.16025938093662262, 0.15949931740760803, 0.161100834608078, 0.1628868579864502, 0.16358867287635803]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:36 2016", "state": "available"}], "summary": "2ce2d2009b4e9fe1600fbd71904e7e65"}