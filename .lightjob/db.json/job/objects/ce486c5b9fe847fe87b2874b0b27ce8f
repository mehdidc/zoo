{"content": {"hp_model": {"f0": 32, "f1": 32, "f2": 32, "f3": 64, "nonlin": "very_leaky_rectify", "nbg1": 2, "nbg3": 2, "nbg2": 2, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.025079936394153395, 0.021501996779510826, 0.02833726483402434, 0.02567336417180918, 0.024458773899206334, 0.024454322667909947, 0.024267381340703147, 0.026960525321121146, 0.019760746657098927, 0.022170043196678667, 0.018580551832457305, 0.016593603011244067, 0.017368244525543697, 0.016679758803021163, 0.014948474567893407, 0.015503765700364394, 0.01687657054665787, 0.015954781917926764, 0.017030678129955408, 0.018129807498266422, 0.01684536444058565, 0.01988066242186971, 0.018547321635838904, 0.01889908495115394, 0.017778131413681487, 0.016416620038158258, 0.018621519028863637, 0.019084421361224884, 0.01912620656948354, 0.019053974563315725, 0.018478718361683747, 0.021603856666854316, 0.01911671795126326, 0.019855092427465817, 0.020033397028640307, 0.020363956022590793, 0.02190742299451721, 0.021515493522639463, 0.01693881028811247, 0.024888946593038995, 0.020769757831246103, 0.023924779997143385, 0.01794775549994706, 0.022178225534361457, 0.020010742495747715, 0.025265174312720864, 0.021225131840978382, 0.019573471753494304, 0.021825277647894867, 0.02112058656950564, 0.02048919733144059, 0.02048831179076675, 0.018338763389135246, 0.02078634884096716, 0.023413489732796066, 0.019828574457661802, 0.022200302782419298, 0.020922939481624568, 0.01703919879496876, 0.01936284787291639, 0.019201946719585215, 0.016760059973161007, 0.02033364055831216, 0.020857800764310985, 0.016365701938181557, 0.020939409120638824, 0.020065070403403756, 0.01834865431450197, 0.021380991291502664, 0.0212311147240129, 0.018649753382350743, 0.022858261695306657, 0.01995535749911932, 0.01895085548325237, 0.02196284198550839, 0.01898911302796246, 0.02080728699670736, 0.018353597778318388, 0.017705523481056917, 0.020026150366428665, 0.021638262411169788], "moving_avg_accuracy_train": [0.016467432228915658, 0.032902390813253, 0.06100098832831324, 0.09251854762801202, 0.1270025738893072, 0.16519651830760537, 0.20658819930817013, 0.24974479729903987, 0.2941674372980515, 0.33652450757427044, 0.37674018558190364, 0.41463563238515905, 0.44986869941772745, 0.482750334747039, 0.5129932794349856, 0.5407861013408847, 0.5665785378935432, 0.5904247315439479, 0.6127146190823242, 0.6330014214813207, 0.6524926008693331, 0.670037015481195, 0.6865776475174128, 0.7021372208680812, 0.7166373542029598, 0.7303557723971217, 0.7429494308501806, 0.7546084599037167, 0.7652380694856342, 0.7756659756394804, 0.786051185304448, 0.7946401556294248, 0.8031303004580486, 0.8109361521893521, 0.8171989940487301, 0.8242662746137366, 0.8316433933873026, 0.8366802965184519, 0.8428418978907031, 0.8489168007221147, 0.8557514007703851, 0.8601470814764791, 0.8649503326661806, 0.8715722986465505, 0.8762284159204496, 0.879661203093465, 0.8845038177238775, 0.8884056573370319, 0.8940986947659794, 0.8979376016448031, 0.90270332943213, 0.905342917422652, 0.9091704479695434, 0.9131846908231915, 0.917204606530029, 0.9206036865396767, 0.9239122937893236, 0.9276454055248491, 0.9298050931350148, 0.9329936350263326, 0.9362774793550246, 0.9375810190701246, 0.9394930978860038, 0.9416822481877648, 0.9439748667424822, 0.9470877339838967, 0.9496257602843021, 0.9515523032317755, 0.9531497084507666, 0.954963879171955, 0.9564907405017474, 0.9581472952166329, 0.9602947268395479, 0.9616014740351111, 0.9633352460593109, 0.9654133366642232, 0.9673189156484033, 0.9684880029992257, 0.9667846281511103, 0.9681695124745535, 0.9692958970704716], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 1234, "moving_var_accuracy_train": [0.002440586917925372, 0.004627498999152789, 0.011270529740057655, 0.019083685663942158, 0.027877649702255203, 0.03821888124408214, 0.049816434424146544, 0.06159721853304176, 0.0731978351800737, 0.08202514428352728, 0.08837833667369724, 0.09246508700209277, 0.09439089941457664, 0.09468262695048747, 0.09344608558602256, 0.0910534455728575, 0.08793534906547798, 0.0842595827234436, 0.08030517622936041, 0.07597864777060774, 0.07179993765896812, 0.06739020224972753, 0.06311351459817278, 0.05898106604404897, 0.0549752442402074, 0.05117147479593586, 0.047481729415433, 0.04395695310013051, 0.04057815518889339, 0.03749901071078487, 0.03471978285777442, 0.03191173827318698, 0.02936930747876735, 0.02698075862215047, 0.024635691453335624, 0.02262164039926273, 0.020849273291930173, 0.018992679501110384, 0.017435099534234097, 0.016023729580510522, 0.014841762442837837, 0.0135314842783834, 0.012385976848467378, 0.011542033064627226, 0.010582944610779223, 0.009630706399678264, 0.008878694007838608, 0.008127843778355777, 0.007606755477026764, 0.0069787147835426114, 0.00648525275727445, 0.005899434304384374, 0.005441340784732419, 0.005042234017451758, 0.00468344811631729, 0.004319087008893441, 0.003985700245395834, 0.0037125553299255125, 0.00338327805209449, 0.0031364514414192377, 0.0029198589994530602, 0.00264316604160734, 0.0024117538460298087, 0.0022137098728201333, 0.0020396437840750348, 0.0019228888878315708, 0.001788574196562365, 0.0016431208864622637, 0.001501774128718978, 0.0013812176544976355, 0.0012640776387316123, 0.0011623674365691353, 0.0010876338560880808, 0.0009942387645772853, 0.0009218685770066355, 0.0008685478643659941, 0.0008143741593139342, 0.0007452376304872154, 0.0006968272402972214, 0.0006444056575713674, 0.0005913837721355258], "duration": 1636.078102, "accuracy_train": [0.16467432228915663, 0.18081701807228914, 0.31388836596385544, 0.3761765813253012, 0.43735881024096385, 0.5089420180722891, 0.579113328313253, 0.6381541792168675, 0.6939711972891566, 0.717738140060241, 0.7386812876506024, 0.7556946536144579, 0.7669663027108434, 0.7786850527108434, 0.785179781626506, 0.7909214984939759, 0.7987104668674698, 0.8050404743975904, 0.8133236069277109, 0.8155826430722891, 0.8279132153614458, 0.8279367469879518, 0.8354433358433735, 0.8421733810240963, 0.8471385542168675, 0.8538215361445783, 0.8562923569277109, 0.8595397213855421, 0.8609045557228916, 0.8695171310240963, 0.8795180722891566, 0.8719408885542169, 0.8795416039156626, 0.8811888177710844, 0.8735645707831325, 0.8878717996987951, 0.8980374623493976, 0.8820124246987951, 0.8982963102409639, 0.9035909262048193, 0.9172628012048193, 0.8997082078313253, 0.908179593373494, 0.9311699924698795, 0.9181334713855421, 0.9105562876506024, 0.9280873493975904, 0.9235222138554217, 0.945336031626506, 0.9324877635542169, 0.9455948795180723, 0.9290992093373494, 0.9436182228915663, 0.9493128765060241, 0.9533838478915663, 0.951195406626506, 0.9536897590361446, 0.9612434111445783, 0.949242281626506, 0.9616905120481928, 0.965832078313253, 0.9493128765060241, 0.9567018072289156, 0.9613846009036144, 0.9646084337349398, 0.9751035391566265, 0.9724679969879518, 0.9688911897590361, 0.9675263554216867, 0.9712914156626506, 0.9702324924698795, 0.9730562876506024, 0.9796216114457831, 0.9733621987951807, 0.9789391942771084, 0.9841161521084337, 0.9844691265060241, 0.9790097891566265, 0.9514542545180723, 0.9806334713855421, 0.9794333584337349], "end": "2016-01-18 01:51:26.558000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0], "accuracy_valid": [0.1635237068965517, 0.17928340517241378, 0.32098599137931033, 0.3824084051724138, 0.43763469827586204, 0.5013469827586207, 0.560479525862069, 0.6099137931034483, 0.6594827586206896, 0.6790140086206896, 0.6920797413793104, 0.7001616379310345, 0.7079741379310345, 0.7117456896551724, 0.7152478448275862, 0.7172683189655172, 0.7235991379310345, 0.7254849137931034, 0.728582974137931, 0.7261584051724138, 0.7314116379310345, 0.7283135775862069, 0.7293911637931034, 0.7319504310344828, 0.7308728448275862, 0.7331627155172413, 0.7327586206896551, 0.7331627155172413, 0.732354525862069, 0.734375, 0.7381465517241379, 0.7288523706896551, 0.7318157327586207, 0.732354525862069, 0.7234644396551724, 0.7297952586206896, 0.7272359913793104, 0.7215786637931034, 0.7314116379310345, 0.7275053879310345, 0.7346443965517241, 0.7229256465517241, 0.7214439655172413, 0.7367995689655172, 0.7281788793103449, 0.7244073275862069, 0.7292564655172413, 0.7272359913793104, 0.7314116379310345, 0.7293911637931034, 0.7307381465517241, 0.7215786637931034, 0.7322198275862069, 0.7283135775862069, 0.7301993534482759, 0.728582974137931, 0.7300646551724138, 0.734375, 0.7254849137931034, 0.7330280172413793, 0.7296605603448276, 0.7229256465517241, 0.7254849137931034, 0.7250808189655172, 0.7288523706896551, 0.7334321120689655, 0.7326239224137931, 0.7268318965517241, 0.7280441810344828, 0.7261584051724138, 0.7288523706896551, 0.7281788793103449, 0.7367995689655172, 0.7330280172413793, 0.7351831896551724, 0.7334321120689655, 0.7358566810344828, 0.7331627155172413, 0.7159213362068966, 0.7331627155172413, 0.7346443965517241], "accuracy_test": 0.7318709935897436, "start": "2016-01-18 01:24:10.480000", "learning_rate_per_epoch": [0.0016297476831823587, 0.0011524056317284703, 0.0009409352205693722, 0.0008148738415911794, 0.000728845305275172, 0.0006653416785411537, 0.0006159867043606937, 0.0005762028158642352, 0.0005432491889223456, 0.0005153714446350932, 0.0004913873854093254, 0.0004704676102846861, 0.00045201065950095654, 0.00043556836317293346, 0.00042079901322722435, 0.0004074369207955897, 0.0003952718689106405, 0.00038413520087487996, 0.00037388975033536553, 0.000364422652637586, 0.00035564007703214884, 0.0003474633558653295, 0.00033982587046921253, 0.00033267083927057683, 0.0003259495133534074, 0.00031961980857886374, 0.0003136450832244009, 0.00030799335218034685, 0.0003026365302503109, 0.00029754985007457435, 0.0002927113091573119, 0.0002881014079321176, 0.00028370265499688685, 0.00027949942159466445, 0.00027547762147150934, 0.0002716245944611728, 0.00026792887365445495, 0.0002643799816723913, 0.0002609684888739139, 0.0002576857223175466, 0.00025452382396906614, 0.0002514755178708583, 0.000248534168349579, 0.0002456936927046627, 0.0002429484302410856, 0.00024029317137319595, 0.00023772312852088362, 0.00023523380514234304, 0.0002328210830455646, 0.00023048112052492797, 0.00022821030870545655, 0.00022600532975047827, 0.000223863054998219, 0.0002217805595137179, 0.0002197551220888272, 0.00021778418158646673, 0.00021586535149253905, 0.0002139963471563533, 0.00021217507310211658, 0.00021039950661361217, 0.00020866779959760606, 0.00020697816216852516, 0.00020532890630420297, 0.00020371846039779484, 0.00020214531105011702, 0.00020060806127730757, 0.00019910535775125027, 0.00019763593445532024, 0.00019619855447672307, 0.0001947920973179862, 0.00019341545703355223, 0.00019206760043743998, 0.0001907475379994139, 0.00018945430929306895, 0.0001881870412034914, 0.00018694487516768277, 0.00018572698172647506, 0.00018453257507644594, 0.00018336092762183398, 0.000182211326318793, 0.00018108307267539203], "accuracy_train_last": 0.9794333584337349, "error_valid": [0.8364762931034483, 0.8207165948275862, 0.6790140086206897, 0.6175915948275862, 0.5623653017241379, 0.49865301724137934, 0.43952047413793105, 0.3900862068965517, 0.3405172413793104, 0.3209859913793104, 0.3079202586206896, 0.2998383620689655, 0.2920258620689655, 0.2882543103448276, 0.2847521551724138, 0.28273168103448276, 0.2764008620689655, 0.2745150862068966, 0.27141702586206895, 0.2738415948275862, 0.2685883620689655, 0.27168642241379315, 0.2706088362068966, 0.26804956896551724, 0.2691271551724138, 0.2668372844827587, 0.26724137931034486, 0.2668372844827587, 0.26764547413793105, 0.265625, 0.2618534482758621, 0.27114762931034486, 0.26818426724137934, 0.26764547413793105, 0.2765355603448276, 0.2702047413793104, 0.2727640086206896, 0.2784213362068966, 0.2685883620689655, 0.2724946120689655, 0.2653556034482759, 0.2770743534482759, 0.2785560344827587, 0.26320043103448276, 0.27182112068965514, 0.27559267241379315, 0.2707435344827587, 0.2727640086206896, 0.2685883620689655, 0.2706088362068966, 0.2692618534482759, 0.2784213362068966, 0.26778017241379315, 0.27168642241379315, 0.2698006465517241, 0.27141702586206895, 0.2699353448275862, 0.265625, 0.2745150862068966, 0.26697198275862066, 0.2703394396551724, 0.2770743534482759, 0.2745150862068966, 0.27491918103448276, 0.27114762931034486, 0.2665678879310345, 0.26737607758620685, 0.2731681034482759, 0.27195581896551724, 0.2738415948275862, 0.27114762931034486, 0.27182112068965514, 0.26320043103448276, 0.26697198275862066, 0.2648168103448276, 0.2665678879310345, 0.26414331896551724, 0.2668372844827587, 0.2840786637931034, 0.2668372844827587, 0.2653556034482759], "accuracy_train_std": [0.023837897707321296, 0.024783528697959155, 0.030668101760622235, 0.030140403685759168, 0.03180365267617578, 0.03166220352521224, 0.030655125915415048, 0.0303707617136805, 0.030089258763358082, 0.027577831429369558, 0.028955654463391538, 0.029258736501185937, 0.029509344562317852, 0.028860530228834426, 0.028088296087990728, 0.02815760445681786, 0.02733256917895014, 0.027161012290063048, 0.026834113620383546, 0.027590759325853937, 0.026449223688813507, 0.026087225248335066, 0.026070886429654577, 0.025907508598006862, 0.025339136890544504, 0.025880039610807724, 0.026210270964214723, 0.02529851228325082, 0.02508843316605669, 0.025282616497787035, 0.02568653541215152, 0.025687009672738063, 0.02516719281111497, 0.02528327354637714, 0.02586245656609533, 0.025518885356811356, 0.025852991233721614, 0.025310011888609586, 0.02595328253202055, 0.025572036256218598, 0.024134227040013407, 0.025403172827247248, 0.023219789976229914, 0.02413139328074008, 0.024740067042550412, 0.0245920358618759, 0.023010321294676582, 0.022309223012728963, 0.020729867130437937, 0.023678815367942467, 0.020707991767587655, 0.019702830068133694, 0.02199863514909984, 0.020292722933512038, 0.018362706886967874, 0.02028726470172551, 0.01891568101125749, 0.018018358644392753, 0.01946759670817116, 0.01669392694312019, 0.017593475282771553, 0.01977888851436082, 0.017184984277282003, 0.01662494007940508, 0.016364617887182714, 0.013689825825416621, 0.013207176475150701, 0.015428996823946753, 0.014870573008220025, 0.015677079703856837, 0.015089571782879766, 0.01369793341515783, 0.012405341432134015, 0.013397082178715318, 0.012171038589482373, 0.010398236747022019, 0.01025417457183458, 0.012671644315037024, 0.0172964498155907, 0.011191978999556583, 0.011570096587755398], "accuracy_test_std": 0.024233400944326498, "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-3, 3], "momentum": 0.6527170588611453, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.0016297476250825603, "patience_threshold": 1, "do_flip": true, "batch_size": 256, "optimization": "rmsprop", "nb_data_augmentation": 0, "learning_rate_decay_method": "sqrt", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 7.322237056612505e-10, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.015615356992064122}, "accuracy_valid_max": 0.7381465517241379, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = 1234\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-10, -6], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128, 256],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-3, 3)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_optimizer.learning_rate = learning_rate\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.7346443965517241, "loss_train": [1.9566515684127808, 1.655273675918579, 1.3999003171920776, 1.2245714664459229, 1.0987640619277954, 1.005704641342163, 0.9319570660591125, 0.869857907295227, 0.8166126608848572, 0.7708613872528076, 0.7301498651504517, 0.6947131752967834, 0.6626068949699402, 0.6327151656150818, 0.6043155789375305, 0.5785971879959106, 0.5541035532951355, 0.531681478023529, 0.5102002620697021, 0.48903632164001465, 0.46947866678237915, 0.4493778347969055, 0.43160924315452576, 0.41566991806030273, 0.3970402777194977, 0.3807070851325989, 0.364534467458725, 0.34966030716896057, 0.3357524871826172, 0.3226041793823242, 0.3071545362472534, 0.2949677109718323, 0.2838664650917053, 0.2701566219329834, 0.2588922083377838, 0.2456827163696289, 0.23552338778972626, 0.22560012340545654, 0.21858039498329163, 0.20415444672107697, 0.195539191365242, 0.18647481501102448, 0.18119211494922638, 0.16874873638153076, 0.16404344141483307, 0.15362462401390076, 0.1447790116071701, 0.1367698311805725, 0.13415123522281647, 0.12389853596687317, 0.12108807265758514, 0.11270356178283691, 0.1090734452009201, 0.101072758436203, 0.09879714250564575, 0.0932009369134903, 0.0888892412185669, 0.08718005567789078, 0.08062773942947388, 0.0790463387966156, 0.07330399751663208, 0.07142200320959091, 0.06507028639316559, 0.062293149530887604, 0.06337525695562363, 0.061503827571868896, 0.055250383913517, 0.05247288942337036, 0.05080932378768921, 0.04906186833977699, 0.05017903074622154, 0.045936036854982376, 0.045683354139328, 0.04312095418572426, 0.03890737518668175, 0.03888192027807236, 0.03726232424378395, 0.03585593402385712, 0.034831199795007706, 0.033724069595336914, 0.035467181354761124], "accuracy_train_first": 0.16467432228915663, "model": "residual", "loss_std": [0.19050121307373047, 0.20997753739356995, 0.19235645234584808, 0.1495465338230133, 0.11777881532907486, 0.10905781388282776, 0.10608523339033127, 0.1022580936551094, 0.0984632819890976, 0.09404687583446503, 0.09033644944429398, 0.08967101573944092, 0.0862145870923996, 0.0843811109662056, 0.08067333698272705, 0.08095227181911469, 0.0796443372964859, 0.07701976597309113, 0.07513967156410217, 0.07375498861074448, 0.07280562818050385, 0.07114747166633606, 0.06834021955728531, 0.06937826424837112, 0.06510534882545471, 0.06433308124542236, 0.06314244866371155, 0.06074390560388565, 0.057983916252851486, 0.05867046117782593, 0.055982790887355804, 0.054379694163799286, 0.05652214214205742, 0.05472161993384361, 0.05387425795197487, 0.05086778476834297, 0.05557449907064438, 0.05404689908027649, 0.05556930601596832, 0.050458576530218124, 0.052540503442287445, 0.0486777238547802, 0.04888269677758217, 0.0467507541179657, 0.051716968417167664, 0.04633316397666931, 0.04637249559164047, 0.04318685084581375, 0.04337158799171448, 0.039666369557380676, 0.04027632996439934, 0.039660606533288956, 0.04290859028697014, 0.03556913882493973, 0.0367082878947258, 0.03709927573800087, 0.03256627172231674, 0.035466913133859634, 0.03630030155181885, 0.03514732047915459, 0.032408591359853745, 0.040074147284030914, 0.028443343937397003, 0.03223637118935585, 0.03492480516433716, 0.033393677324056625, 0.03137846291065216, 0.03057110123336315, 0.026425180956721306, 0.027844717726111412, 0.0274941585958004, 0.03139664977788925, 0.02829170599579811, 0.028681613504886627, 0.021057887002825737, 0.026578113436698914, 0.025564230978488922, 0.026669364422559738, 0.019851038232445717, 0.02624749019742012, 0.03377445042133331]}, "state": "available", "life": [{"dt": "Sun May 15 22:04:59 2016", "state": "available"}], "summary": "629bf65e35682c87b485437ff52a59b4"}