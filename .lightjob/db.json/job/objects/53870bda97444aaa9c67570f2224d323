{"content": {"hp_model": {"f0": 16, "f1": 16, "f2": 32, "f3": 64, "nonlin": "leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.4416897296905518, 1.094644546508789, 0.9556958079338074, 0.867394745349884, 0.8121190071105957, 0.7693532705307007, 0.7369610667228699, 0.7083673477172852, 0.6860347390174866, 0.6636831760406494, 0.649155855178833, 0.6308107376098633, 0.6158826947212219, 0.6006391048431396, 0.5841268301010132, 0.574202299118042, 0.5713162422180176, 0.5600036978721619, 0.5413047671318054, 0.5345360040664673, 0.5249032378196716, 0.5193150639533997, 0.5146194100379944, 0.5064514875411987, 0.5084777474403381, 0.5031172633171082, 0.499198317527771, 0.49592262506484985, 0.4863872826099396, 0.48693811893463135, 0.48468366265296936, 0.48174983263015747, 0.4788852632045746, 0.47586533427238464, 0.4717404842376709, 0.469698429107666, 0.4718475639820099, 0.46972522139549255, 0.4656548798084259, 0.4678789973258972, 0.461836040019989, 0.46006080508232117, 0.46168872714042664, 0.45274263620376587, 0.4549795389175415, 0.4557114243507385, 0.4844071567058563, 0.46383634209632874, 0.45063403248786926, 0.44529685378074646, 0.4459018409252167, 0.44026365876197815, 0.4440608620643616, 0.4408150315284729, 0.44185400009155273, 0.43892768025398254, 0.43268245458602905, 0.44015416502952576, 0.43550172448158264, 0.4349871873855591, 0.43347999453544617, 0.4285854697227478, 0.43054941296577454, 0.43372565507888794, 0.42012810707092285, 0.4307716190814972, 0.42698934674263, 0.426819771528244, 0.42102864384651184, 0.4227040112018585, 0.42418965697288513, 0.416483074426651, 0.4155069589614868, 0.4281379282474518, 0.4122535288333893, 0.42415139079093933, 0.41443485021591187, 0.4165780544281006, 0.41913485527038574, 0.4130893647670746, 0.41210946440696716, 0.4115096628665924, 0.4147633910179138, 0.4077744483947754, 0.41438764333724976, 0.41034963726997375, 0.40812593698501587, 0.4026455283164978, 0.4093172550201416, 0.40525105595588684, 0.40351811051368713, 0.4053260385990143, 0.4017089903354645, 0.3986031711101532, 0.3424486517906189, 0.3006691038608551, 0.28838422894477844, 0.2795341908931732, 0.2705558240413666, 0.26016148924827576, 0.24749042093753815, 0.23188509047031403, 0.21289649605751038, 0.19043076038360596, 0.16497471928596497, 0.13779997825622559, 0.13657398521900177, 0.12094786018133163, 0.11475428193807602, 0.11222244799137115, 0.10973157733678818, 0.10652468353509903, 0.10231579095125198, 0.09684664011001587, 0.08989471197128296, 0.08135747909545898, 0.07144588232040405, 0.11723612248897552, 0.07251904904842377, 0.06773275136947632, 0.06642969697713852, 0.06546365469694138, 0.06483480334281921, 0.06474123895168304, 0.06472887098789215, 0.06472746282815933, 0.06472741812467575, 0.06472741812467575, 0.06472741812467575, 0.06472741812467575, 0.06472741812467575, 0.06472741812467575, 0.06472741812467575, 0.06472741812467575, 0.06472741812467575, 0.06472741812467575, 0.06472741812467575, 0.06472741812467575, 0.06472741812467575, 0.06472741812467575, 0.06472741812467575, 0.06472741812467575, 0.06472741812467575, 0.06472741812467575, 0.06472741812467575], "moving_avg_accuracy_train": [0.057610230077980794, 0.11790261123223512, 0.1757233400960917, 0.23181582993753458, 0.28469089189463936, 0.3317156534976726, 0.374981697015338, 0.416557097906043, 0.4551257631731168, 0.4870477438302422, 0.5198224922762139, 0.5488010414001521, 0.5760697064092694, 0.5980192163365872, 0.621207407232921, 0.640917286808863, 0.6605297436842243, 0.679485002866005, 0.6975234074855046, 0.7156575461227976, 0.7305391479785411, 0.7446277009963017, 0.7588931502003813, 0.7710413410923493, 0.783446423945093, 0.7917775433342216, 0.8010114636270453, 0.8071794303480433, 0.815322204050273, 0.8226272326012719, 0.8303758502995242, 0.8380681853565504, 0.8446400452423608, 0.8485993771372369, 0.8535183736473966, 0.8598985234089028, 0.865436081147839, 0.8701918743342733, 0.8733958687426233, 0.8743543125934902, 0.8755003278723009, 0.8793870243613259, 0.880188275119407, 0.8849291144587029, 0.8875241960164779, 0.885861306540107, 0.8895372969409135, 0.8905952326445704, 0.8900130289053072, 0.8900700443518013, 0.894552839542867, 0.8979897559219597, 0.8999925219202953, 0.8998398856093566, 0.9013647059377713, 0.904524975521412, 0.904388701812248, 0.9043776265680387, 0.9071481763362533, 0.909371917816923, 0.910578156354306, 0.9112805180213854, 0.9089878226122886, 0.910127911071344, 0.9119561309749609, 0.9128667458155878, 0.9153814768495697, 0.9182468762241827, 0.919911880081554, 0.9217079305031697, 0.9213689978314519, 0.9231984089852299, 0.9212254874988516, 0.9201166351371631, 0.9211762805127972, 0.9212557053984869, 0.9193626533444724, 0.9210696114088809, 0.920076183859724, 0.9219746748833585, 0.9237365428855173, 0.9235854460732023, 0.924677173562366, 0.9261501905061479, 0.9241167146045272, 0.9250437080977012, 0.9258360414165302, 0.9267446701963888, 0.9283971645208806, 0.9288706805807896, 0.927548152885852, 0.9273671006901978, 0.9272482954926712, 0.9281248006148696, 0.9345056020629341, 0.9407063776816684, 0.9463986828813864, 0.9515960902253998, 0.9563202237623837, 0.9605998817802022, 0.9644724642867057, 0.967967089137797, 0.9711215520990174, 0.9739605687641157, 0.9765273095067518, 0.9788257504310767, 0.9802130786617785, 0.9819569668146576, 0.9836892265689153, 0.9852738009358333, 0.9866999178660595, 0.9879927236985011, 0.9891655495429367, 0.9902210928029287, 0.9911710817369215, 0.9920190963310864, 0.9925823466682159, 0.9928590822394895, 0.993394173606026, 0.9939757011858996, 0.9945223274958811, 0.9950328923653405, 0.9954993761942826, 0.9959192116403306, 0.9962970635417737, 0.9966371302530725, 0.9969431902932415, 0.9972186443293936, 0.9974665529619304, 0.9976896707312136, 0.9978904767235685, 0.9980712021166878, 0.9982338549704952, 0.9983802425389219, 0.9985119913505058, 0.9986305652809314, 0.9987372818183144, 0.9988333267019592, 0.9989197670972395, 0.9989975634529917, 0.9990675801731688, 0.9991305952213281, 0.9991873087646714, 0.9992383509536805, 0.9992842889237886], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.056780344032379505, 0.11659670910203311, 0.17252707607774848, 0.22632120228962724, 0.2767959627082549, 0.32116932436288126, 0.36151421186258714, 0.3994655003431206, 0.43426054363523925, 0.46341234954091715, 0.4925471888055754, 0.5178977268903793, 0.5421192258504377, 0.5607363636475475, 0.5800014356318139, 0.5958660381623826, 0.6112214877158733, 0.6251614036091956, 0.6392526204113032, 0.6530740874740886, 0.6632458414714086, 0.6743973727233642, 0.6848407009160127, 0.6932091581210982, 0.7010844784253287, 0.7065499081242115, 0.7125825761502693, 0.7167445051410406, 0.721908315875054, 0.7262851318308468, 0.7305690045193736, 0.7351855909838518, 0.7384879385590359, 0.7406351207949998, 0.7422643209707709, 0.7456451930905613, 0.7490583069705111, 0.7514749007369389, 0.7536163025589528, 0.7536717703828768, 0.7532660603061855, 0.7554726338689404, 0.754416793204185, 0.7567971441925014, 0.757985252627167, 0.7552427208697213, 0.7576044557010474, 0.7575033368911083, 0.7568609545385938, 0.7558564467635748, 0.7584966939358619, 0.7605952431981493, 0.7607320079408344, 0.7602478331727299, 0.7612830966833937, 0.7638221910060181, 0.7624685040025548, 0.7617190533575704, 0.763284758620458, 0.7638677861940296, 0.7638848161683767, 0.7642837086573372, 0.7635176060050823, 0.7640121956493031, 0.7651684226675807, 0.7656485330551901, 0.766152845082879, 0.767673885169847, 0.7691557730553472, 0.7694231953262433, 0.7683486045210286, 0.7695035842891818, 0.7683162388497213, 0.767370580702701, 0.7670760113372502, 0.7667010356270945, 0.7648223830244754, 0.7650369220657779, 0.7638322285884922, 0.7651874987642815, 0.7655720162541937, 0.7653799431114551, 0.766205112392177, 0.7673607447900076, 0.7655065709604797, 0.7660199316543415, 0.7666091741346152, 0.7671771429692711, 0.7687565036911693, 0.768960166669191, 0.7681873439499376, 0.768339089129492, 0.7681907800377025, 0.7685019025233901, 0.7729578940293342, 0.7767210572337051, 0.7800956970863888, 0.7831450799850542, 0.786051304526082, 0.7885183632206876, 0.7907631301083327, 0.7927356216908729, 0.7946105793824783, 0.7964079045861733, 0.7980366747920891, 0.7993540245850941, 0.8001957835064792, 0.8011354424958163, 0.80200452014006, 0.8028386066708583, 0.8035750185000073, 0.8043741255073109, 0.805044493688884, 0.8056722391147999, 0.8061883818731241, 0.8065684906455256, 0.8065454071118465, 0.8066355247214451, 0.8069373866412433, 0.807443054980131, 0.8077496130928108, 0.8081130236302918, 0.8084899507476844, 0.8088535992158377, 0.8091686758059256, 0.8094522447370046, 0.8097074567749758, 0.8099371476091499, 0.8101438693599066, 0.8103299189355876, 0.8104973635537005, 0.8106480637100021, 0.8107836938506735, 0.8109057609772778, 0.8110156213912217, 0.8111144957637713, 0.8112034826990658, 0.8112835709408309, 0.8113556503584195, 0.8114205218342492, 0.811478906162496, 0.811531452057918, 0.8115787433637979, 0.8116213055390897, 0.8116596114968524], "moving_var_accuracy_train": [0.02987044748674095, 0.0595999437653158, 0.08372907956591279, 0.10367347835883287, 0.11846808011565861, 0.12652322593849177, 0.13071845803969523, 0.13320323786873123, 0.13327079154621038, 0.12911482803325433, 0.12587100245119942, 0.12084170899003599, 0.11544975891344753, 0.10824081189654758, 0.10225596048029623, 0.09552667860834979, 0.089435846929706, 0.0837259788925717, 0.07828183737426558, 0.07341327649388811, 0.0680651075086351, 0.06304498269297983, 0.05857201179262945, 0.05404301749089579, 0.050023690467057125, 0.04564598937283464, 0.04184877799131917, 0.038006294513429295, 0.03480240793417787, 0.03180243811993826, 0.029162563994047454, 0.026778855762308696, 0.024489674267306347, 0.022181793622059767, 0.020181382999856456, 0.018529601498684038, 0.016952622260224107, 0.015460918153690908, 0.014007216559840463, 0.012614762435393799, 0.011365106351027832, 0.010364553402305242, 0.009333876087070647, 0.008602768497132724, 0.007803101682042986, 0.007047678326534314, 0.006464526642722282, 0.005828147030027701, 0.005248382977771041, 0.004723573936844187, 0.004432075617685146, 0.004095179603688518, 0.0037217612881164687, 0.0033497948398955747, 0.003035741049211537, 0.0028220526788619465, 0.002540014545690036, 0.0022860141950703412, 0.0021264962897266926, 0.0019583518963096794, 0.0017756118093603225, 0.0016024904356267355, 0.0014895494622141048, 0.0013522927312429357, 0.0012471449502624678, 0.0011298934297279497, 0.0010738189363146055, 0.0010403316648674376, 0.0009612486389862465, 0.0008941559491404964, 0.000805774232430067, 0.0007553175157131678, 0.000714817536864569, 0.0006544017652183099, 0.0005990672235954017, 0.0005392172760480629, 0.0005175483631561309, 0.0004920168793433584, 0.0004516972760678359, 0.0004389659619624379, 0.0004230069754794733, 0.00038091175015175213, 0.0003535473953319383, 0.00033772066604876043, 0.00034116381762613326, 0.00031478128829100384, 0.0002889532882550426, 0.0002674884157658243, 0.0002653162116215399, 0.00024080254759031137, 0.00023246400836617216, 0.00020951262660751587, 0.0001886883960213983, 0.00017673390748241822, 0.0005254921608107796, 0.0008189895091947065, 0.0010287116046558634, 0.0011689578320867218, 0.0012529189879553477, 0.0012924663439051146, 0.0012981917669417024, 0.0012782842158963184, 0.0012400115234700827, 0.0011885505117454251, 0.0011289888829300531, 0.0010636354707805478, 0.0009745940402798142, 0.000904504949259598, 0.0008410609690396286, 0.0007795527554543078, 0.0007199017653969771, 0.0006629537111408363, 0.0006090380241791373, 0.0005581617659246552, 0.0005104679001045696, 0.000465893268861364, 0.0004221592004557155, 0.00038063252359781734, 0.0003451461761729123, 0.0003136751274910037, 0.00028499681764677893, 0.0002588432242554369, 0.00023491736629387385, 0.00021301198588031078, 0.0001929957358270974, 0.00017473697055759, 0.00015810632823552484, 0.00014297856974626437, 0.00012923384098241446, 0.00011675849073490198, 0.00010544554908050223, 9.519494918191527e-05, 8.591355782138899e-05, 7.751506592095894e-05, 6.991977907304721e-05, 6.305433915853167e-05, 5.6851401016837746e-05, 5.1249282492222956e-05, 4.6191601720426536e-05, 4.162691200509884e-05, 3.750834187452807e-05, 3.379324575372594e-05, 3.04428690123404e-05, 2.7422029856635874e-05, 2.469881954485119e-05], "duration": 31292.75049, "accuracy_train": [0.5761023007798081, 0.6605340416205242, 0.6961098998708011, 0.7366482385105205, 0.7605664495085824, 0.7549385079249723, 0.7643760886743264, 0.7907357059223883, 0.802243750576781, 0.7743455697443706, 0.8147952282899593, 0.8096079835155963, 0.8214876914913253, 0.7955648056824474, 0.8299011252999261, 0.8183062029923404, 0.8370418555624769, 0.8500823355020304, 0.8598690490610004, 0.8788647938584349, 0.8644735646802326, 0.8714246781561462, 0.8872821930370985, 0.8803750591200628, 0.895092169619786, 0.8667576178363787, 0.8841167462624585, 0.8626911308370249, 0.8886071673703396, 0.8883724895602622, 0.9001134095837948, 0.907299200869786, 0.9037867842146549, 0.8842333641911223, 0.8977893422388336, 0.9173198712624585, 0.9152741007982651, 0.9129940130121816, 0.902231818417774, 0.882980307251292, 0.8858144653815985, 0.9143672927625508, 0.8873995319421374, 0.9275966685123662, 0.9108799300364526, 0.8708953012527685, 0.9226212105481728, 0.9001166539774824, 0.884773195251938, 0.8905831833702473, 0.9348979962624585, 0.9289220033337948, 0.9180174159053157, 0.8984661588109081, 0.9150880888935032, 0.9329674017741787, 0.9031622384297711, 0.904277949370155, 0.9320831242501846, 0.9293855911429494, 0.921434303190753, 0.9176017730251015, 0.8883535639304172, 0.9203887072028424, 0.9284101101075121, 0.9210622793812293, 0.938014056155408, 0.9440354705956996, 0.9348969147978959, 0.9378723842977114, 0.9183186037859912, 0.9396631093692323, 0.903469194121447, 0.9101369638819674, 0.9307130888935032, 0.9219705293696937, 0.9023251848583426, 0.9364322339885567, 0.9111353359173128, 0.9390610940960686, 0.9395933549049464, 0.9222255747623662, 0.9345027209648394, 0.9394073430001846, 0.9058154314899409, 0.933386649536268, 0.9329670412859912, 0.9349223292151162, 0.9432696134413067, 0.9331323251199704, 0.9156454036314139, 0.9257376309293098, 0.9261790487149317, 0.9360133467146549, 0.991932815095515, 0.9965133582502769, 0.9976294296788483, 0.9983727563215209, 0.9988374255952381, 0.9991168039405685, 0.9993257068452381, 0.9994187127976191, 0.99951171875, 0.99951171875, 0.9996279761904762, 0.99951171875, 0.9926990327380952, 0.9976519601905685, 0.9992795643572352, 0.9995349702380952, 0.9995349702380952, 0.9996279761904762, 0.9997209821428571, 0.9997209821428571, 0.9997209821428571, 0.9996512276785714, 0.9976515997023809, 0.9953497023809523, 0.9982099959048542, 0.9992094494047619, 0.9994419642857143, 0.9996279761904762, 0.9996977306547619, 0.9996977306547619, 0.9996977306547619, 0.9996977306547619, 0.9996977306547619, 0.9996977306547619, 0.9996977306547619, 0.9996977306547619, 0.9996977306547619, 0.9996977306547619, 0.9996977306547619, 0.9996977306547619, 0.9996977306547619, 0.9996977306547619, 0.9996977306547619, 0.9996977306547619, 0.9996977306547619, 0.9996977306547619, 0.9996977306547619, 0.9996977306547619, 0.9996977306547619, 0.9996977306547619, 0.9996977306547619], "end": "2016-02-01 22:34:53.347000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0], "moving_var_accuracy_valid": [0.02901606721591838, 0.058316438265641236, 0.08063864798942083, 0.09861905532457463, 0.1116864627459779, 0.11823877349217046, 0.12106428566922871, 0.12192055977829987, 0.12062475913977358, 0.11621073331385706, 0.11222920971326862, 0.1067901367726436, 0.10139125220222825, 0.09437150735981462, 0.08827464361086382, 0.08171234977085376, 0.07566322327267656, 0.06984579224142501, 0.06464827453595849, 0.059902743648271584, 0.054843650497882396, 0.05047849529146422, 0.04641221369597211, 0.04240127201031502, 0.03871933083833155, 0.035116236050639266, 0.03193215019718892, 0.028894830053788026, 0.026245331520079632, 0.02379320702926961, 0.0215790504132462, 0.01961296120717762, 0.017749814582025777, 0.016016326647813148, 0.014438582621946427, 0.013097597026365168, 0.01189268144094621, 0.01075597262573903, 0.00972164577903505, 0.008749508891246963, 0.007876039408119226, 0.007132256169297945, 0.0064290637479523126, 0.005837152010605293, 0.005266141224417474, 0.004807220425941106, 0.004376698506068484, 0.003939120680585147, 0.0035489225083080306, 0.0032031115803078905, 0.0029455385684540314, 0.002690619892664847, 0.0024217262447519374, 0.0021816634471313657, 0.0019731430372468355, 0.0018338517333348074, 0.0016669587765314347, 0.0015053179853016991, 0.001376849083503633, 0.0012422234655171734, 0.0011180037291456926, 0.0010076353931908655, 0.0009121540733359055, 0.000823140236247848, 0.0007528579608832176, 0.00067964671865351, 0.0006139710223796034, 0.0005733959866571193, 0.0005358203133381385, 0.00048288191404306525, 0.0004449864312266264, 0.000412493592487552, 0.0003839323359722622, 0.0003535875263542692, 0.0003190097137184012, 0.0002883742033954223, 0.00029130080346782697, 0.00026258496612323106, 0.0002493880468788386, 0.00024098005743541134, 0.00021821273499230575, 0.0001967234903225287, 0.0001831792803669006, 0.00017688072848044939, 0.0001901343009433589, 0.00017349272366704211, 0.00015926831160536972, 0.00014624477781909672, 0.00015406972264606002, 0.00013903605785900376, 0.0001305077466716518, 0.00011766421140014869, 0.00010609575054050085, 9.635735029635456e-05, 0.00026542435797613474, 0.0003663344979030938, 0.00043219479533067486, 0.00047266394036166095, 0.0005014128160713405, 0.000506048941887871, 0.0005007948531178948, 0.0004857318751948338, 0.00046879788478314564, 0.0004509914972953646, 0.0004297683790189394, 0.00040241023541121707, 0.00036854623460567915, 0.0003396382422912879, 0.00031247208162767635, 0.0002874861765326399, 0.00026361828031837254, 0.0002430036003686291, 0.00022274778182155728, 0.00020401958251722567, 0.00018601525438823803, 0.000168714073059123, 0.00015184746139895473, 0.00013673580591109697, 0.00012388231088760593, 0.00011379538401942653, 0.00010326164650553133, 9.412408682374804e-05, 8.59903446078056e-05, 7.858147202253683e-05, 7.161678413887601e-05, 6.517880777304865e-05, 5.9247125654672604e-05, 5.3797234002937693e-05, 4.880211554276713e-05, 4.423343398999022e-05, 4.006242989220604e-05, 3.6260581736969463e-05, 3.280008337879959e-05, 2.9654178491496568e-05, 2.6797384437314232e-05, 2.4205631267506336e-05, 2.1856336212633755e-05, 1.9728429729591604e-05, 1.7802345738591636e-05, 1.6059985940119423e-05, 1.4484665914170913e-05, 1.3061048962885203e-05, 1.1775072275103105e-05, 1.0613868896482998e-05, 9.565688124435765e-06], "accuracy_test": 0.0973812181122449, "start": "2016-02-01 13:53:20.597000", "learning_rate_per_epoch": [0.0031808288767933846, 0.0031808288767933846, 0.0031808288767933846, 0.0031808288767933846, 0.0031808288767933846, 0.0031808288767933846, 0.0031808288767933846, 0.0031808288767933846, 0.0031808288767933846, 0.0031808288767933846, 0.0031808288767933846, 0.0031808288767933846, 0.0031808288767933846, 0.0031808288767933846, 0.0031808288767933846, 0.0031808288767933846, 0.0031808288767933846, 0.0031808288767933846, 0.0031808288767933846, 0.0031808288767933846, 0.0031808288767933846, 0.0031808288767933846, 0.0031808288767933846, 0.0031808288767933846, 0.0031808288767933846, 0.0031808288767933846, 0.0031808288767933846, 0.0031808288767933846, 0.0031808288767933846, 0.0031808288767933846, 0.0031808288767933846, 0.0031808288767933846, 0.0031808288767933846, 0.0031808288767933846, 0.0031808288767933846, 0.0031808288767933846, 0.0031808288767933846, 0.0031808288767933846, 0.0031808288767933846, 0.0031808288767933846, 0.0031808288767933846, 0.0031808288767933846, 0.0031808288767933846, 0.0031808288767933846, 0.0031808288767933846, 0.0031808288767933846, 0.0031808288767933846, 0.0031808288767933846, 0.0031808288767933846, 0.0031808288767933846, 0.0031808288767933846, 0.0031808288767933846, 0.0031808288767933846, 0.0031808288767933846, 0.0031808288767933846, 0.0031808288767933846, 0.0031808288767933846, 0.0031808288767933846, 0.0031808288767933846, 0.0031808288767933846, 0.0031808288767933846, 0.0031808288767933846, 0.0031808288767933846, 0.0031808288767933846, 0.0031808288767933846, 0.0031808288767933846, 0.0031808288767933846, 0.0031808288767933846, 0.0031808288767933846, 0.0031808288767933846, 0.0031808288767933846, 0.0031808288767933846, 0.0031808288767933846, 0.0031808288767933846, 0.0031808288767933846, 0.0031808288767933846, 0.0031808288767933846, 0.0031808288767933846, 0.0031808288767933846, 0.0031808288767933846, 0.0031808288767933846, 0.0031808288767933846, 0.0031808288767933846, 0.0031808288767933846, 0.0031808288767933846, 0.0031808288767933846, 0.0031808288767933846, 0.0031808288767933846, 0.0031808288767933846, 0.0031808288767933846, 0.0031808288767933846, 0.0031808288767933846, 0.0031808288767933846, 0.00031808289350010455, 0.00031808289350010455, 0.00031808289350010455, 0.00031808289350010455, 0.00031808289350010455, 0.00031808289350010455, 0.00031808289350010455, 0.00031808289350010455, 0.00031808289350010455, 0.00031808289350010455, 0.00031808289350010455, 0.00031808289350010455, 0.00031808289350010455, 0.00031808289350010455, 0.00031808289350010455, 0.00031808289350010455, 0.00031808289350010455, 0.00031808289350010455, 0.00031808289350010455, 0.00031808289350010455, 0.00031808289350010455, 0.00031808289350010455, 0.00031808289350010455, 0.00031808289350010455, 0.00031808289350010455, 0.00031808289350010455, 0.00031808289350010455, 0.00031808289350010455, 3.1808289350010455e-05, 3.1808290259505156e-06, 3.180828969107097e-07, 3.180829111215644e-08, 3.180829155624565e-09, 3.180829211135716e-10, 3.180829211135716e-11, 3.18082929787189e-12, 3.1808293520819986e-13, 3.180829284319363e-14, 3.1808293690226575e-15, 3.180829474901776e-16, 3.180829408727327e-17, 3.1808293673682963e-18, 3.180829470765873e-19, 3.180829341518902e-20, 3.180829341518902e-21, 3.180829291031804e-22, 3.1808291648140594e-23, 3.1808291648140594e-24, 3.1808291648140594e-25, 3.1808292264438176e-26, 3.1808293034810154e-27, 3.1808293034810154e-28], "accuracy_train_first": 0.5761023007798081, "accuracy_train_last": 0.9996977306547619, "batch_size_eval": 1024, "accuracy_train_std": [0.018599634827762328, 0.019220429086848685, 0.01790239144800485, 0.021092434555240992, 0.020924247210899416, 0.023377817814857222, 0.022807248567482036, 0.0247362926387588, 0.02479010721162672, 0.023617595624939605, 0.028015252531214518, 0.029120390798625842, 0.031276579887656177, 0.030316651645128742, 0.03200771145910885, 0.0285977555305538, 0.03063161486266368, 0.029914778992884212, 0.03015992163638817, 0.02902811286435596, 0.02741777066649276, 0.03159852649395574, 0.028105452563285976, 0.027301702007899713, 0.02827657422070687, 0.02714821631763787, 0.029694561318582113, 0.02840815773015775, 0.027967821567526183, 0.028451251649546733, 0.02827894481222504, 0.027024831185502234, 0.02698334666085643, 0.029366202062511032, 0.027209451426469865, 0.024271749074391287, 0.025299882127563032, 0.02497375152502244, 0.02431656588991593, 0.02590050136312444, 0.02573841860300176, 0.025224181212822116, 0.024220455360132735, 0.020056129298771704, 0.022247017913568902, 0.02678772515119069, 0.021418820414753403, 0.02131612873517083, 0.0237825656335994, 0.0241356099265777, 0.02019513908186748, 0.021797474360336466, 0.022800720168792154, 0.025158586465903847, 0.02162645315497175, 0.020920616888976923, 0.02222483968222787, 0.021591147760008138, 0.020002494082658865, 0.021107543830067032, 0.02319370578365979, 0.01984061755585962, 0.023546492423209733, 0.02138350737188194, 0.020468513241064946, 0.022127275448672445, 0.017210806171779103, 0.017560236062712172, 0.018208173550154638, 0.019586867231797155, 0.02134912636157764, 0.01933416183698626, 0.021879752430355025, 0.020129469043229535, 0.018008358987873195, 0.02014697219663356, 0.02154498376760324, 0.02067551563484214, 0.021423371965756638, 0.01687055719723621, 0.01842619532319612, 0.018007204007803694, 0.019280288718699995, 0.01760878090158677, 0.01933348762980169, 0.018783066881730048, 0.01816853236830944, 0.017155311872090098, 0.015800455483523483, 0.01737121621860898, 0.02151276144662346, 0.01967419747275125, 0.018441449900276633, 0.017254544030751526, 0.004640400898557024, 0.0025036789867155405, 0.0018204794452373228, 0.001290316343774303, 0.0011718380922948431, 0.0008732732406980762, 0.0008111404177135606, 0.0007400404791127022, 0.0006822637998103241, 0.0006822637998103241, 0.0005618975992184976, 0.0006481287139493282, 0.005302970718061172, 0.0017581472444918582, 0.0007695387069092537, 0.0005733271952166087, 0.0005322508901720424, 0.0005199190795897948, 0.000489939255620012, 0.000489939255620012, 0.0005342785199533136, 0.0005565805991250848, 0.0017320407892916381, 0.0031233248902914775, 0.0014439890731137491, 0.0011121894292319009, 0.0009077018831818794, 0.0006376166853051566, 0.0005428114550748116, 0.0005428114550748116, 0.0005428114550748116, 0.0005428114550748116, 0.0005428114550748116, 0.0005428114550748116, 0.0005428114550748116, 0.0005428114550748116, 0.0005428114550748116, 0.0005428114550748116, 0.0005428114550748116, 0.0005428114550748116, 0.0005428114550748116, 0.0005428114550748116, 0.0005428114550748116, 0.0005428114550748116, 0.0005428114550748116, 0.0005428114550748116, 0.0005428114550748116, 0.0005428114550748116, 0.0005428114550748116, 0.0005428114550748116, 0.0005428114550748116], "accuracy_test_std": 0.008305620184512284, "error_valid": [0.43219655967620485, 0.34505600527108427, 0.32409962114081325, 0.2895316618034638, 0.26893119352409633, 0.2794704207454819, 0.27538180064006024, 0.25897290333207834, 0.2525840667356928, 0.2742213973079819, 0.2452392578125, 0.25394743034638556, 0.2398872835090362, 0.2717093961784638, 0.24661291650978923, 0.2613525390625, 0.2505794663027108, 0.24937935335090367, 0.23392642836972888, 0.22253270896084332, 0.24520837255271077, 0.2252388460090362, 0.22116934535015065, 0.23147472703313254, 0.22803763883659633, 0.24426122458584332, 0.23312341161521077, 0.2457981339420181, 0.23161738751882532, 0.2343235245670181, 0.23087614128388556, 0.22326513083584332, 0.23179093326430722, 0.24004023908132532, 0.24307287744728923, 0.22392695783132532, 0.22022366810993976, 0.22677575536521077, 0.22711108104292166, 0.24582901920180722, 0.2503853303840362, 0.2246682040662651, 0.25508577277861444, 0.22177969691265065, 0.23132177146084332, 0.2694400649472892, 0.2211399308170181, 0.24340673239834332, 0.2489204866340362, 0.25318412321159633, 0.2177410815135542, 0.2205178134412651, 0.238037109375, 0.24410973974021077, 0.22939953172063254, 0.21332596009036142, 0.24971467902861444, 0.24502600244728923, 0.2226238940135542, 0.23088496564382532, 0.2359619140625, 0.2321262589420181, 0.24337731786521077, 0.23153649755271077, 0.22442553416792166, 0.23003047345632532, 0.22930834666792166, 0.21863675404743976, 0.21750723597515065, 0.22817000423569278, 0.24132271272590367, 0.22010159779743976, 0.24236987010542166, 0.2411403426204819, 0.23557511295180722, 0.23667374576430722, 0.25208549039909633, 0.2330322265625, 0.24701001270707834, 0.22261506965361444, 0.23096732633659633, 0.23634871517319278, 0.22636836408132532, 0.2222385636295181, 0.2511809935052711, 0.22935982210090367, 0.22808764354292166, 0.22771113751882532, 0.21702924981174698, 0.22920686652861444, 0.23876806052334332, 0.2302952042545181, 0.23314400178840367, 0.22869799510542166, 0.18693818241716864, 0.18941047392695776, 0.18953254423945776, 0.18941047392695776, 0.18779267460466864, 0.18927810852786142, 0.18903396790286142, 0.1895119540662651, 0.1885148013930723, 0.1874161685805723, 0.18730439335466864, 0.18878982727786142, 0.1922283862010542, 0.19040762660015065, 0.19017378106174698, 0.18965461455195776, 0.18979727503765065, 0.18843391142695776, 0.18892219267695776, 0.18867805205195776, 0.18916633330195776, 0.19001053040286142, 0.1936623446912651, 0.19255341679216864, 0.1903458560805723, 0.18800592996987953, 0.1894913638930723, 0.18861628153237953, 0.1881177051957832, 0.1878735645707832, 0.1879956348832832, 0.1879956348832832, 0.1879956348832832, 0.1879956348832832, 0.1879956348832832, 0.1879956348832832, 0.1879956348832832, 0.1879956348832832, 0.1879956348832832, 0.1879956348832832, 0.1879956348832832, 0.1879956348832832, 0.1879956348832832, 0.1879956348832832, 0.1879956348832832, 0.1879956348832832, 0.1879956348832832, 0.1879956348832832, 0.1879956348832832, 0.1879956348832832, 0.1879956348832832], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.015251424718082452, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 64, "valid_ratio": 0.15, "learning_rate": 0.0031808289721306214, "optimization": "adam", "nb_data_augmentation": 0, "learning_rate_decay_method": "discrete", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 1.8855569937430317e-05, "rotation_range": [0, 0], "momentum": 0.8480883513557724}, "accuracy_valid_max": 0.8130618175828314, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8120043651167168, "accuracy_valid_std": [0.01802527078376171, 0.010802914886470599, 0.011600072214556112, 0.013500766277615288, 0.011896034361211581, 0.010542096040905554, 0.014079308302389704, 0.01693597367419003, 0.018615745706237745, 0.021276495349686595, 0.01543646554917342, 0.01192794825798223, 0.01856572073153015, 0.013393424301371347, 0.017042279353599546, 0.012167293719676387, 0.014789367217768782, 0.014119190076559419, 0.010163773744618358, 0.013961296356301504, 0.016439271529023494, 0.014067815908364778, 0.01440598546875635, 0.012117412514553642, 0.01805421961664385, 0.015918362391987368, 0.011001949150694398, 0.013968265812262149, 0.009571194276090171, 0.016912134844786358, 0.01918427120352984, 0.010102272703365812, 0.012963112484170976, 0.013948189242629363, 0.012280495015026196, 0.0070381841972505105, 0.010332073642293337, 0.00859771746371628, 0.010034502196251032, 0.009618739434135097, 0.018542052621838938, 0.013243532185107073, 0.015789078888489343, 0.015386706012469113, 0.017101369752684368, 0.007989242243418963, 0.012167546868204928, 0.018593930292222664, 0.012981019393921607, 0.01015733011820061, 0.01701627945172958, 0.017553978999211736, 0.014944517522268448, 0.013547381036314154, 0.012513785164482913, 0.012298208915657725, 0.015431260774323692, 0.019798219978543856, 0.015127279549194857, 0.01313864972351318, 0.010253179615250832, 0.012811752987894292, 0.015779473865483267, 0.014954222226351415, 0.01202439626335831, 0.010747279469511258, 0.016384496898715874, 0.011711670266268173, 0.016918267745473885, 0.014400043167922042, 0.018256398646414963, 0.013090089148277532, 0.013345797356292865, 0.016670986569410774, 0.015443020962848077, 0.012512427854392546, 0.015567784703011959, 0.01440171040734973, 0.013743528104043582, 0.015414940842627855, 0.016178996427920415, 0.013393385945086385, 0.010250942429800708, 0.0125382338004223, 0.014624576864217198, 0.013981496822778508, 0.012399225961049547, 0.007546581379068137, 0.008363111485719546, 0.00932450118471662, 0.013392685903401324, 0.012773517307845914, 0.01282192818043325, 0.012505763022941976, 0.01049796857880868, 0.012728240432291843, 0.012400732850185127, 0.011672998670404281, 0.010816368881427249, 0.011389920173460855, 0.010714628679844759, 0.008748184799594362, 0.00817570320994305, 0.008992148224901903, 0.00983746171631662, 0.00904851652006699, 0.014109463762148267, 0.011562694620462229, 0.01315377370838819, 0.011212925646470452, 0.01201594407339915, 0.010963646760198904, 0.011159513257520052, 0.012128646700905256, 0.011962132184070327, 0.011112594074411796, 0.00886596919704708, 0.01581049078115356, 0.010985854312413463, 0.012161620050848407, 0.011534634915300771, 0.011664769075595436, 0.01185508011284671, 0.011925411740354126, 0.011845725067919297, 0.011845725067919297, 0.011845725067919297, 0.011845725067919297, 0.011845725067919297, 0.011845725067919297, 0.011845725067919297, 0.011845725067919297, 0.011845725067919297, 0.011845725067919297, 0.011845725067919297, 0.011845725067919297, 0.011845725067919297, 0.011845725067919297, 0.011845725067919297, 0.011845725067919297, 0.011845725067919297, 0.011845725067919297, 0.011845725067919297, 0.011845725067919297, 0.011845725067919297], "accuracy_valid": [0.5678034403237951, 0.6549439947289157, 0.6759003788591867, 0.7104683381965362, 0.7310688064759037, 0.7205295792545181, 0.7246181993599398, 0.7410270966679217, 0.7474159332643072, 0.7257786026920181, 0.7547607421875, 0.7460525696536144, 0.7601127164909638, 0.7282906038215362, 0.7533870834902108, 0.7386474609375, 0.7494205336972892, 0.7506206466490963, 0.7660735716302711, 0.7774672910391567, 0.7547916274472892, 0.7747611539909638, 0.7788306546498494, 0.7685252729668675, 0.7719623611634037, 0.7557387754141567, 0.7668765883847892, 0.7542018660579819, 0.7683826124811747, 0.7656764754329819, 0.7691238587161144, 0.7767348691641567, 0.7682090667356928, 0.7599597609186747, 0.7569271225527108, 0.7760730421686747, 0.7797763318900602, 0.7732242446347892, 0.7728889189570783, 0.7541709807981928, 0.7496146696159638, 0.7753317959337349, 0.7449142272213856, 0.7782203030873494, 0.7686782285391567, 0.7305599350527108, 0.7788600691829819, 0.7565932676016567, 0.7510795133659638, 0.7468158767884037, 0.7822589184864458, 0.7794821865587349, 0.761962890625, 0.7558902602597892, 0.7706004682793675, 0.7866740399096386, 0.7502853209713856, 0.7549739975527108, 0.7773761059864458, 0.7691150343561747, 0.7640380859375, 0.7678737410579819, 0.7566226821347892, 0.7684635024472892, 0.7755744658320783, 0.7699695265436747, 0.7706916533320783, 0.7813632459525602, 0.7824927640248494, 0.7718299957643072, 0.7586772872740963, 0.7798984022025602, 0.7576301298945783, 0.7588596573795181, 0.7644248870481928, 0.7633262542356928, 0.7479145096009037, 0.7669677734375, 0.7529899872929217, 0.7773849303463856, 0.7690326736634037, 0.7636512848268072, 0.7736316359186747, 0.7777614363704819, 0.7488190064947289, 0.7706401778990963, 0.7719123564570783, 0.7722888624811747, 0.782970750188253, 0.7707931334713856, 0.7612319394766567, 0.7697047957454819, 0.7668559982115963, 0.7713020048945783, 0.8130618175828314, 0.8105895260730422, 0.8104674557605422, 0.8105895260730422, 0.8122073253953314, 0.8107218914721386, 0.8109660320971386, 0.8104880459337349, 0.8114851986069277, 0.8125838314194277, 0.8126956066453314, 0.8112101727221386, 0.8077716137989458, 0.8095923733998494, 0.809826218938253, 0.8103453854480422, 0.8102027249623494, 0.8115660885730422, 0.8110778073230422, 0.8113219479480422, 0.8108336666980422, 0.8099894695971386, 0.8063376553087349, 0.8074465832078314, 0.8096541439194277, 0.8119940700301205, 0.8105086361069277, 0.8113837184676205, 0.8118822948042168, 0.8121264354292168, 0.8120043651167168, 0.8120043651167168, 0.8120043651167168, 0.8120043651167168, 0.8120043651167168, 0.8120043651167168, 0.8120043651167168, 0.8120043651167168, 0.8120043651167168, 0.8120043651167168, 0.8120043651167168, 0.8120043651167168, 0.8120043651167168, 0.8120043651167168, 0.8120043651167168, 0.8120043651167168, 0.8120043651167168, 0.8120043651167168, 0.8120043651167168, 0.8120043651167168, 0.8120043651167168], "seed": 5009491, "model": "residualv3", "loss_std": [0.3012307584285736, 0.15592071413993835, 0.14486418664455414, 0.13980543613433838, 0.13569217920303345, 0.12986081838607788, 0.12726928293704987, 0.12365935742855072, 0.12299588322639465, 0.12162254750728607, 0.11548198014497757, 0.10692095756530762, 0.10629618167877197, 0.10152460634708405, 0.09800853580236435, 0.09544997662305832, 0.10623645782470703, 0.10115199536085129, 0.08909410983324051, 0.08688975870609283, 0.08635406941175461, 0.08335458487272263, 0.08429869264364243, 0.08329564332962036, 0.0795239582657814, 0.07862557470798492, 0.0819317102432251, 0.0808163657784462, 0.07550138235092163, 0.07489001005887985, 0.0776667520403862, 0.0715666264295578, 0.07457619160413742, 0.07294769585132599, 0.06609833240509033, 0.06998200714588165, 0.07047215104103088, 0.06783157587051392, 0.06862376630306244, 0.07579854875802994, 0.0683126226067543, 0.06364056468009949, 0.06806997209787369, 0.06374341994524002, 0.06955405324697495, 0.08471444994211197, 0.11866707354784012, 0.09041967242956161, 0.06687865406274796, 0.06329676508903503, 0.06491619348526001, 0.06059814244508743, 0.06389009207487106, 0.0645490437746048, 0.06345673650503159, 0.06560414284467697, 0.060552340000867844, 0.06436267495155334, 0.06082810088992119, 0.06341075152158737, 0.0604567714035511, 0.062162965536117554, 0.06069399416446686, 0.06167316064238548, 0.059815164655447006, 0.06389858573675156, 0.0606226772069931, 0.06351273506879807, 0.06242916360497475, 0.06240864098072052, 0.061668071895837784, 0.05733760818839073, 0.06162974238395691, 0.06134830042719841, 0.058321233838796616, 0.06413059681653976, 0.05778452754020691, 0.06293632090091705, 0.0635383352637291, 0.05658579617738724, 0.05892064422369003, 0.06034865975379944, 0.06354007124900818, 0.057852089405059814, 0.060766056180000305, 0.06180838495492935, 0.057793498039245605, 0.05779255926609039, 0.06244070455431938, 0.06219528242945671, 0.057214491069316864, 0.05912803113460541, 0.05825266242027283, 0.05766216292977333, 0.05061144754290581, 0.013377608731389046, 0.006114968564361334, 0.0041260127909481525, 0.0036278902553021908, 0.0037988608237355947, 0.004349397961050272, 0.0051594446413218975, 0.006110069341957569, 0.007045221049338579, 0.007749973330646753, 0.007970265112817287, 0.03214186057448387, 0.008884617127478123, 0.0016998330829665065, 0.0007424894720315933, 0.000850317592266947, 0.0010869214311242104, 0.0014068253803998232, 0.001802563900128007, 0.002253221580758691, 0.0027046524919569492, 0.003009709995239973, 0.0655512884259224, 0.007302670273929834, 0.001140440464951098, 0.0004003175417892635, 0.00035704008769243956, 0.00018008932238444686, 0.0001601662952452898, 0.00015797004743944854, 0.000157681162818335, 0.00015766729484312236, 0.00015766698925290257, 0.00015766684373375028, 0.0001576671056682244, 0.0001576669019414112, 0.00015766693104524165, 0.0001576669019414112, 0.00015766703290864825, 0.00015766684373375028, 0.0001576666400069371, 0.00015766693104524165, 0.00015766703290864825, 0.00015766703290864825, 0.00015766677097417414, 0.00015766672731842846, 0.0001576669601490721, 0.00015766688738949597, 0.00015766701835673302, 0.0001576670038048178]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:29 2016", "state": "available"}], "summary": "ba345f61a19681488d399a9763d7b484"}