{"content": {"hp_model": {"f0": 16, "f1": 64, "f2": 32, "f3": 16, "nonlin": "very_leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.6969624757766724, 1.2990273237228394, 1.1302379369735718, 1.0313266515731812, 0.9572704434394836, 0.8983656764030457, 0.8535405993461609, 0.8104274868965149, 0.7773856520652771, 0.7492936849594116, 0.7226312756538391, 0.6960191130638123, 0.6777029633522034, 0.6578255891799927, 0.640175461769104, 0.6266192197799683, 0.6145214438438416, 0.6017965078353882, 0.5895198583602905, 0.5794294476509094, 0.5715418457984924, 0.562454879283905, 0.556894838809967, 0.5479909181594849, 0.5433302521705627, 0.537405788898468, 0.5329182744026184, 0.5290349125862122, 0.5234862565994263, 0.5198642015457153, 0.5149132609367371, 0.5128925442695618, 0.5105815529823303, 0.5095337629318237, 0.5085922479629517, 0.5051846504211426, 0.501581609249115, 0.4985184371471405, 0.4995723366737366, 0.49801895022392273, 0.4960840940475464, 0.49714308977127075, 0.4956997036933899, 0.49489104747772217, 0.49159151315689087, 0.4908553957939148, 0.49111273884773254, 0.49048471450805664, 0.4896602928638458, 0.4891727566719055, 0.488317608833313, 0.48733848333358765, 0.4874471426010132, 0.48571380972862244, 0.4863889217376709, 0.4859296679496765, 0.485284686088562, 0.48548418283462524, 0.48523151874542236, 0.4857746660709381, 0.4849923253059387, 0.48567771911621094, 0.48357653617858887, 0.48406606912612915, 0.4829166829586029, 0.48504313826560974, 0.48416006565093994, 0.4826999008655548, 0.4850122630596161, 0.48438867926597595, 0.48225387930870056, 0.4834354519844055, 0.48464128375053406, 0.48406755924224854, 0.482024610042572, 0.4829232394695282, 0.4844872057437897, 0.4829730689525604, 0.48253241181373596, 0.4822893440723419, 0.48421576619148254, 0.4840007424354553, 0.4830356240272522, 0.48281329870224, 0.48326313495635986, 0.48342809081077576, 0.484403520822525, 0.48078012466430664, 0.4846915304660797, 0.4820668697357178, 0.4823620319366455, 0.48393791913986206, 0.4831373989582062, 0.4834403991699219, 0.48220089077949524, 0.4837363660335541, 0.48089373111724854, 0.48479020595550537, 0.4835647940635681, 0.48248055577278137, 0.48225292563438416, 0.4841678738594055, 0.48458626866340637, 0.4844735562801361, 0.48476916551589966, 0.4824937582015991, 0.4845687747001648, 0.4798666536808014, 0.48279011249542236, 0.4835965633392334, 0.4823528826236725, 0.4824991822242737, 0.48364073038101196, 0.4833031892776489, 0.4839709997177124, 0.4830681383609772, 0.48318809270858765, 0.48255419731140137, 0.48431089520454407, 0.48401033878326416, 0.48422932624816895, 0.48214319348335266, 0.4830384850502014, 0.483045369386673, 0.48316890001296997], "moving_avg_accuracy_train": [0.04941195364410298, 0.1026600946065199, 0.15722328944750136, 0.21058432195416318, 0.2609592917290311, 0.3080679511381693, 0.35257440223490055, 0.3933091516743396, 0.4314072960364829, 0.46568178321601106, 0.4980188094786661, 0.5282800572221986, 0.5560686016568632, 0.5815453942170444, 0.605437155177169, 0.6278929429067297, 0.6485980480715754, 0.6675118047723544, 0.6849756035886775, 0.701276454630465, 0.7161613145026436, 0.7300063700102051, 0.7425645041193729, 0.7542457880247575, 0.7649356548491274, 0.7747518474910604, 0.7837886367175911, 0.7920356072154979, 0.7996066901874236, 0.8066462042966805, 0.8130724838474019, 0.8190188237620804, 0.8245053522674246, 0.8295013566424725, 0.8341139098740354, 0.8382722192776895, 0.8419891932017108, 0.8455552867725973, 0.8488508375411663, 0.8518445908233176, 0.8545787125999283, 0.8570253631595646, 0.8594109993703707, 0.8614533681660302, 0.8634519713987997, 0.8651577083559112, 0.866785805472055, 0.868262862815907, 0.8695385918074798, 0.8706680746117816, 0.8717730368880527, 0.8727999829223925, 0.8736917543676025, 0.8745640310349396, 0.8753305148938857, 0.8759785337371845, 0.8765826049378016, 0.8771402559600329, 0.8776816694098029, 0.8781828203098156, 0.8786293139686643, 0.8789776077413715, 0.8792794463927603, 0.8796440710825726, 0.8799537042105648, 0.8801788595543201, 0.8804140514470331, 0.880635024745713, 0.8808130104240579, 0.8809754505857403, 0.8811658606074542, 0.8813371575293593, 0.8814774459638541, 0.8815711174227474, 0.8817368740417222, 0.881839552022609, 0.8818668580387405, 0.8819355752318211, 0.8819974207055935, 0.8820251437974557, 0.8820966336051408, 0.8821935265153908, 0.8822784049858063, 0.8822989920377518, 0.882354722765455, 0.8823002487239593, 0.8823465171389849, 0.8823882308101455, 0.8823560186499043, 0.8823711694842493, 0.8824172852208557, 0.8824634757302393, 0.8825097335351223, 0.882546715261898, 0.8825335679374431, 0.882586767414463, 0.8826416584390281, 0.8826328955920798, 0.8826413211203119, 0.8826372423028543, 0.882673026799267, 0.8827029437460475, 0.8827206044517306, 0.8827737014677979, 0.8827447588715441, 0.8827721889575347, 0.8827921896884885, 0.8827056307475558, 0.8827648754316596, 0.8828088950521149, 0.8827624822045722, 0.8827509375763076, 0.8827986400822889, 0.8828299826424432, 0.8829046578739537, 0.8829276877549322, 0.8828763710835364, 0.8828557987650038, 0.8828372476295057, 0.8827647840849475, 0.8827808389567222, 0.8828023358853856, 0.8827542177568877, 0.8827992670960014, 0.882788694276213], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.04847691547439758, 0.10074709972703311, 0.15386451371893822, 0.2057566003270896, 0.2540862490293204, 0.2990641016772167, 0.34079237477380225, 0.3789022845102774, 0.41400892635292436, 0.4452142790113066, 0.4748768919610795, 0.5023341975880137, 0.5277965786104624, 0.550509172961389, 0.5719597205090001, 0.5920220492393501, 0.6104952431764843, 0.627417174995884, 0.6427354513780728, 0.6569094365960637, 0.6699020749206441, 0.6817572293449954, 0.6924604008946826, 0.7023262183918108, 0.7110314671757471, 0.7191907804544073, 0.7266419666691323, 0.7334334834811347, 0.7396720369590755, 0.7452836465632433, 0.750396159871904, 0.7552537695059486, 0.7596287067025677, 0.7637014570319344, 0.767612102462024, 0.771023879085174, 0.7737109125339607, 0.7765107491325978, 0.7790529571165519, 0.7815149312655895, 0.7836442292723137, 0.7856216326346155, 0.7872781958395274, 0.7887802802465386, 0.7901698068152583, 0.7911599150362174, 0.7923348626798096, 0.7934167296215424, 0.7942795170791924, 0.795130297487237, 0.7958939408371579, 0.7964713565708367, 0.7969177885436476, 0.7975525404215871, 0.7980627819554826, 0.7986084780633983, 0.7989490021508837, 0.7992575328469399, 0.7994497612546405, 0.7998557299239807, 0.8002099242037964, 0.8003923626945614, 0.8005809713987498, 0.8008016063748387, 0.8009360541710898, 0.8010469091737851, 0.8010835845026415, 0.8012651356909316, 0.801403088189233, 0.8016879958612735, 0.8018966141497696, 0.8019836258334372, 0.8021362080448977, 0.8022247039102122, 0.8025128992289049, 0.8025494599272494, 0.8024735307831691, 0.8024896142635871, 0.8023942261147133, 0.8023439683658173, 0.8023618305653801, 0.802453207749806, 0.80242352491722, 0.802456816015483, 0.8024511864188293, 0.8025325985092506, 0.8025580707742894, 0.802493487576755, 0.802496397855224, 0.8026221169270058, 0.8026864359666095, 0.8027544711161835, 0.80275466759455, 0.8027436669024897, 0.8027327367709757, 0.8027605502550227, 0.8027601388195054, 0.8028238922097687, 0.8027571409311864, 0.8028069280617124, 0.8028029083541857, 0.8027748765549116, 0.8027984760605651, 0.8029163423569936, 0.8028149024925293, 0.8029077415919209, 0.8028315758664638, 0.8027141985885523, 0.802794753033161, 0.8027176791323298, 0.8026483126215818, 0.8026713319806585, 0.8026686648499872, 0.8027039150347928, 0.802685782567458, 0.8026949069180165, 0.8027895975609287, 0.8027862813948208, 0.802721232180414, 0.8026515103648576, 0.8026009677621068, 0.8027915310480196, 0.8027036018231122, 0.8027221217706956, 0.8028507120220898], "moving_var_accuracy_train": [0.02197387046634284, 0.045294764063289375, 0.06755956773825458, 0.0864302090760223, 0.10062592638678977, 0.11053636587004648, 0.1173101469860734, 0.12051301059451099, 0.12152492696960833, 0.11994509851542424, 0.11736173807148662, 0.11386725229929698, 0.10943035588554323, 0.10432892292937943, 0.09903337681242311, 0.0936684007541766, 0.0881598730977448, 0.08256345752079666, 0.07705197019059006, 0.07173823287370995, 0.06655844106706851, 0.061627767018428765, 0.05688435090732047, 0.0524239873596922, 0.048210047898227866, 0.044256261850256925, 0.04056560770095391, 0.03712115963239863, 0.03392493534546881, 0.030978434640971775, 0.028252264796650776, 0.025745268942413777, 0.02344166000353199, 0.02132213454061827, 0.019381401912382464, 0.017598885555012878, 0.015963340055878284, 0.014481459260497319, 0.013131059228261528, 0.011898616333864903, 0.010776033497482443, 0.00975230503838291, 0.008828295875717406, 0.007983007720823018, 0.007220656682679044, 0.006524776861512845, 0.005896155477337924, 0.0053261752151773775, 0.004808205053799094, 0.004338866131066106, 0.003915967992647334, 0.003533862756799617, 0.0031876337879140825, 0.0028757182083821026, 0.0025934338650981156, 0.0023378698343797365, 0.002107366969080498, 0.0018994290441358085, 0.001712124296434555, 0.0015431722368123516, 0.0013906492224176447, 0.0012526760771448399, 0.0011282284285736063, 0.0010166021461960315, 0.0009158047856419808, 0.0008246805614371766, 0.0007427103423310405, 0.0006688787708865015, 0.0006022760039131144, 0.0005422858847769497, 0.0004883836000865765, 0.000439809323797006, 0.0003960055190209826, 0.00035648393619878516, 0.0003210828198895122, 0.000289069422810392, 0.00026016919109600554, 0.0002341947704600288, 0.0002108097171776612, 0.0001897356625882967, 0.00017080809346289286, 0.00015381177824111406, 0.00013849543960966332, 0.00012464971008906722, 0.00011221269230625346, 0.00010101812986639997, 9.093558377582084e-05, 8.185768567149399e-05, 7.368125571375124e-05, 6.63151960724083e-05, 5.9702816415632275e-05, 5.375173684248311e-05, 4.839582121884818e-05, 4.356854793000116e-05, 3.921324880626394e-05, 3.531739558483417e-05, 3.181277324755106e-05, 2.8632187010175693e-05, 2.5769607214892022e-05, 2.3192796224169487e-05, 2.088504137340408e-05, 1.880459244940572e-05, 1.6926940309192198e-05, 1.5259619916310177e-05, 1.3741196989580338e-05, 1.2373848977179398e-05, 1.1140064342609638e-05, 1.0093489960647104e-05, 9.115730357933386e-06, 8.2215968650053e-06, 7.418824550257897e-06, 6.678141601208004e-06, 6.030807202779254e-06, 5.436567687194544e-06, 4.9430984302855236e-06, 4.4535619660179574e-06, 4.031906376284448e-06, 3.632524721264295e-06, 3.27236955079231e-06, 2.9923912833224953e-06, 2.695471985159573e-06, 2.4300838481212186e-06, 2.207913651920399e-06, 2.005387273319635e-06, 1.8058546066521754e-06], "duration": 114797.601028, "accuracy_train": [0.4941195364410299, 0.5818933632682723, 0.6482920430163345, 0.6908336145141196, 0.7143340197028424, 0.7320458858204134, 0.7531324621054817, 0.7599218966292912, 0.7742905952957733, 0.7741521678317644, 0.7890520458425618, 0.8006312869139904, 0.8061655015688446, 0.8108365272586747, 0.8204630038182908, 0.8299950324727758, 0.8349439945551864, 0.8377356150793651, 0.842149792935585, 0.8479841140065523, 0.8501250533522517, 0.8546118695782576, 0.8555877111018827, 0.859377343173219, 0.861144456268457, 0.863097581268457, 0.8651197397563677, 0.8662583416966593, 0.8677464369347545, 0.8700018312799926, 0.8709089998038945, 0.872535882994186, 0.8738841088155224, 0.8744653960179033, 0.8756268889581026, 0.8756970039105758, 0.8754419585179033, 0.8776501289105758, 0.8785107944582872, 0.8787883703626799, 0.8791858085894242, 0.8790452181962901, 0.8808817252676264, 0.8798346873269656, 0.8814394004937246, 0.8805093409699151, 0.8814386795173496, 0.8815563789105758, 0.8810201527316353, 0.8808334198504982, 0.8817176973744923, 0.8820424972314507, 0.8817176973744923, 0.8824145210409744, 0.8822288696244001, 0.8818107033268733, 0.8820192457433554, 0.8821591151601144, 0.8825543904577334, 0.8826931784099299, 0.8826477568983019, 0.8821122516957364, 0.8819959942552602, 0.8829256932908823, 0.8827404023624953, 0.8822052576481173, 0.8825307784814507, 0.8826237844338316, 0.882414881529162, 0.8824374120408823, 0.8828795508028792, 0.8828788298265043, 0.8827400418743078, 0.8824141605527871, 0.8832286836124953, 0.8827636538505905, 0.8821126121839239, 0.8825540299695459, 0.8825540299695459, 0.8822746516242157, 0.8827400418743078, 0.8830655627076411, 0.8830423112195459, 0.8824842755052602, 0.8828562993147839, 0.8818099823504982, 0.8827629328742157, 0.8827636538505905, 0.8820661092077334, 0.8825075269933554, 0.8828323268503139, 0.8828791903146919, 0.8829260537790697, 0.8828795508028792, 0.8824152420173496, 0.8830655627076411, 0.8831356776601144, 0.8825540299695459, 0.8827171508744001, 0.8826005329457364, 0.8829950872669805, 0.8829721962670728, 0.8828795508028792, 0.883251574612403, 0.8824842755052602, 0.8830190597314507, 0.8829721962670728, 0.881926600279162, 0.8832980775885935, 0.8832050716362125, 0.8823447665766887, 0.8826470359219268, 0.8832279626361205, 0.8831120656838316, 0.883576734957549, 0.8831349566837394, 0.8824145210409744, 0.8826706478982096, 0.8826702874100221, 0.8821126121839239, 0.8829253328026948, 0.8829958082433554, 0.8823211546004062, 0.8832047111480252, 0.8826935388981173], "end": "2016-02-02 17:40:38.377000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0], "moving_var_accuracy_valid": [0.02115010200520699, 0.04362464126092651, 0.0646553141575208, 0.08242488061469966, 0.09520418704635905, 0.10389083340106597, 0.10917298904156841, 0.11132697711851215, 0.11128656611987195, 0.1089218758186908, 0.10594852369989406, 0.10213880402052251, 0.09775991924446144, 0.09262668479936301, 0.08750515023025762, 0.0823771085139935, 0.07721072771074493, 0.07206682092817417, 0.06697198515724684, 0.062082903354160585, 0.05739389087444472, 0.05291940396482719, 0.04865848449934277, 0.04466864524338825, 0.04088381292656146, 0.03739460117251921, 0.03415482263932585, 0.031154462680862673, 0.028389292358250547, 0.02583377458357178, 0.02348563725619569, 0.021349440872786982, 0.01938675746477753, 0.01759736737550799, 0.01597526896707612, 0.014482504047904967, 0.013099234981908564, 0.011859863248529317, 0.010732042316579505, 0.009713389935316317, 0.008782856131797643, 0.00793976163513306, 0.007170483286486568, 0.0064737412759299845, 0.005843744205103586, 0.005268192613196125, 0.004753797869563156, 0.0042889520073233715, 0.0038667564263647376, 0.0034865952294526772, 0.003143184067000315, 0.0028318663406657818, 0.002550473420156334, 0.00229905226765963, 0.0020714901586998753, 0.001867021201009636, 0.0016813626907960927, 0.0015140831424301639, 0.0013630073940336914, 0.0012281899496746948, 0.0011065000369979133, 0.0009961495875243354, 0.0008968547879615626, 0.0008076074272994701, 0.000727009370458774, 0.0006544190328974997, 0.0005889892353254703, 0.0005303869592986491, 0.0004775195413948726, 0.0004304981386896735, 0.00038784001913336164, 0.0003491241565178775, 0.0003144212728473774, 0.0002830496292262395, 0.0002554921751790627, 0.00022995498782312732, 0.0002070113761551016, 0.00018631256664467265, 0.00016776320007071547, 0.0001510096126355608, 0.00013591152289556372, 0.00012239551871450968, 0.00011016389647801168, 9.915748150522248e-05, 8.924201858592658e-05, 8.037746808353482e-05, 7.234556080175719e-05, 6.514854362621534e-05, 5.863376549108071e-05, 5.29126365070598e-05, 4.7658605306053636e-05, 4.293440380964626e-05, 3.8640963776115375e-05, 3.477795653553612e-05, 3.130123609195673e-05, 2.817807479181456e-05, 2.536026883614577e-05, 2.2860822405461797e-05, 2.0614841763646955e-05, 1.857566641257632e-05, 1.6718245193756088e-05, 1.5053492710315313e-05, 1.3553155869287552e-05, 1.2322872456862569e-05, 1.1183195626099373e-05, 1.0142447948872115e-05, 9.180414113594641e-06, 8.386369530564277e-06, 7.606133744423689e-06, 6.8989838456853065e-06, 6.252390876437006e-06, 5.631920806824048e-06, 5.0687927484158026e-06, 4.573096653333619e-06, 4.118746065345051e-06, 3.7076207427685625e-06, 3.4175555291878525e-06, 3.0758989488879663e-06, 2.8063916566536195e-06, 2.5695026750686713e-06, 2.335543399797253e-06, 2.4288183532585288e-06, 2.2555204552679856e-06, 2.0330553058676006e-06, 1.978568850063695e-06], "accuracy_test": 0.8008529974489796, "start": "2016-02-01 09:47:20.776000", "learning_rate_per_epoch": [0.0001264000020455569, 0.00011459145753178746, 0.0001038860937114805, 9.418084664503112e-05, 8.538227848475799e-05, 7.740568980807438e-05, 7.017429015832022e-05, 6.361846317304298e-05, 5.767509719589725e-05, 5.2286974096205086e-05, 4.7402219934156165e-05, 4.2973806557711214e-05, 3.895910776918754e-05, 3.531947004375979e-05, 3.2019852369558066e-05, 2.9028493372607045e-05, 2.6316592993680388e-05, 2.3858043277869e-05, 2.162917735404335e-05, 1.9608536604209803e-05, 1.777666693669744e-05, 1.6115935068228282e-05, 1.4610352081945166e-05, 1.3245423360785935e-05, 1.2008009434794076e-05, 1.0886196832871065e-05, 9.86918621492805e-06, 8.947186870500445e-06, 8.111322131298948e-06, 7.3535461524443235e-06, 6.666562967438949e-06, 6.0437591855588835e-06, 5.479138962982688e-06, 4.967266704625217e-06, 4.503214768192265e-06, 4.082515260961372e-06, 3.7011184303992195e-06, 3.3553524190210737e-06, 3.041888476218446e-06, 2.7577091259445297e-06, 2.500078380762716e-06, 2.26651604862127e-06, 2.0547736312437337e-06, 1.8628124962560833e-06, 1.6887847777979914e-06, 1.5310150729419547e-06, 1.3879845255360124e-06, 1.2583161606016802e-06, 1.1407616966607748e-06, 1.0341893812437775e-06, 9.375733043270884e-07, 8.499832802044693e-07, 7.70576093600539e-07, 6.985873142184573e-07, 6.333238502520544e-07, 5.741574682360806e-07, 5.205185402701318e-07, 4.718906438938575e-07, 4.2780567355293897e-07, 3.87839207860452e-07, 3.516064737141278e-07, 3.187586798958364e-07, 2.8897960646645515e-07, 2.6198253522125015e-07, 2.3750759226004448e-07, 2.1531914740080538e-07, 1.9520359728630865e-07, 1.7696727638849552e-07, 1.604346380190691e-07, 1.454465063943644e-07, 1.31858598706458e-07, 1.1954010403769644e-07, 1.0837242570005401e-07, 9.824805147218285e-08, 8.906951620701875e-08, 8.074846391536994e-08, 7.320478090377947e-08, 6.636584259922529e-08, 6.016581011181188e-08, 5.454499785173539e-08, 4.9449294436954006e-08, 4.4829640444277175e-08, 4.064156300387367e-08, 3.6844745920916466e-08, 3.340263532436438e-08, 3.028209505373525e-08, 2.7453079809447445e-08, 2.488835804115297e-08, 2.2563236612427318e-08, 2.0455333427094047e-08, 1.8544355384619848e-08, 1.6811904757219054e-08, 1.5241303330526534e-08, 1.3817429866946895e-08, 1.2526577108928905e-08, 1.1356318552202538e-08, 1.0295388541692319e-08, 9.333572137393276e-09, 8.461610967458455e-09, 7.671109969464851e-09, 6.954459230712473e-09, 6.3047593812370906e-09, 5.715755868607175e-09, 5.1817785617913614e-09, 4.697686240007215e-09, 4.258819075175779e-09, 3.860951558465331e-09, 3.500253642485518e-09, 3.173252771659918e-09, 2.8768010196245086e-09, 2.6080444470721886e-09, 2.3643955682217666e-09, 2.1435089259114193e-09, 1.94325799895978e-09, 1.7617148850845865e-09, 1.5971318712004745e-09, 1.4479245580290012e-09, 1.3126565390209066e-09, 1.1900255225683054e-09, 1.0788508975068112e-09, 9.780624088406853e-10, 8.866898326687078e-10, 8.03853483777317e-10, 7.287558334567734e-10, 6.606739599845923e-10], "accuracy_train_first": 0.4941195364410299, "accuracy_train_last": 0.8826935388981173, "batch_size_eval": 1024, "accuracy_train_std": [0.017768194147366634, 0.01641314874920672, 0.016530650619768054, 0.01740523142142114, 0.018316107337249827, 0.01768126557044931, 0.0164620475769082, 0.01590406224706095, 0.015741637695960285, 0.014557711832659196, 0.015278200109989105, 0.01457058333820732, 0.014202972994178433, 0.014054260568848804, 0.014747693050146182, 0.01423189641711958, 0.01288378582582455, 0.012546633586785502, 0.011648269163868753, 0.01276869968128801, 0.012084476127392302, 0.011630234455238187, 0.011687272745975077, 0.011955285982039453, 0.010683609525775045, 0.011296815440494304, 0.01135719781450046, 0.010392164875843627, 0.010238366291655091, 0.010492411426219973, 0.009637098508193631, 0.009831038556767315, 0.009602839121711094, 0.009782349201212357, 0.009541354467982167, 0.0098794354533878, 0.009658525329099459, 0.00980803508200614, 0.009611128321872931, 0.009322750756154057, 0.010029717165815204, 0.009951263535691712, 0.009490948562277628, 0.009795045815838238, 0.009539892483932399, 0.009662187552639694, 0.009474567648274862, 0.009425755167925634, 0.009939642066104936, 0.009265576944455003, 0.008811908895013208, 0.009767106003119743, 0.009371353392956621, 0.009374624830779747, 0.009377762207798422, 0.009616014807259784, 0.009682347230848168, 0.009441949426834174, 0.009806309901057492, 0.009381414060343833, 0.009581718363710837, 0.009773516398076502, 0.009412834271941076, 0.009345114164743143, 0.009303341012234958, 0.009688762446897946, 0.009752700565932103, 0.009698917891876782, 0.009426110385305268, 0.00912053860517001, 0.00974372966003297, 0.009145572578875959, 0.009184675808767199, 0.009295874089740863, 0.00914282450366227, 0.008991975686458459, 0.009522443749731747, 0.008920412244742059, 0.009201087850736858, 0.009217683532986647, 0.008944194524623012, 0.009509481721983862, 0.009366930816884307, 0.00924441640853355, 0.009023993515562215, 0.009471869246912751, 0.009295727340459267, 0.009713048252922747, 0.009282197938198017, 0.00890110450445354, 0.009347158684735781, 0.009049306954280453, 0.009377540049692462, 0.009687639247171725, 0.009015490104878067, 0.009099497133880264, 0.00910428376306548, 0.009291948178738399, 0.009257867948065733, 0.00905050813698769, 0.00916011704285309, 0.009310353682748176, 0.009428697171431834, 0.008976809089241028, 0.009450891231131346, 0.009477121585929065, 0.009385653447742939, 0.008963574775328621, 0.008933839787147334, 0.009365159541054092, 0.00902707489554164, 0.00924016460745034, 0.009296345880874508, 0.009373441856938017, 0.009052410273409322, 0.009214185825777871, 0.00917880960163088, 0.009161112319802583, 0.009426658751044412, 0.009385541561023664, 0.009214376373646582, 0.009054896203254928, 0.009781902678882968, 0.009422292665936402, 0.009481021136779759], "accuracy_test_std": 0.010888652131173827, "error_valid": [0.5152308452560241, 0.428821241999247, 0.36807876035391573, 0.32721462019954817, 0.31094691265060237, 0.2961352244917168, 0.2836531673569277, 0.2781085278614458, 0.270031297063253, 0.273937547063253, 0.2581595914909638, 0.25055005176957834, 0.2430419921875, 0.24507747788027112, 0.2349853515625, 0.2274169921875, 0.22324601138930722, 0.2202854386295181, 0.21940006118222888, 0.2155246964420181, 0.21316418015813254, 0.21154638083584332, 0.21121105515813254, 0.2088814241340362, 0.21062129376882532, 0.20737540003765065, 0.20629735739834332, 0.20544286521084332, 0.20418098173945776, 0.20421186699924698, 0.20359122035015065, 0.20102774378765065, 0.20099685852786142, 0.1996437900037651, 0.19719208866716864, 0.19827013130647586, 0.20210578642695776, 0.19829072147966864, 0.19806717102786142, 0.1963273013930723, 0.19719208866716864, 0.19658173710466864, 0.1978127353162651, 0.19770096009036142, 0.1973244540662651, 0.19992911097515065, 0.19709060852786142, 0.19684646790286142, 0.19795539580195776, 0.19721267884036142, 0.1972332690135542, 0.1983319018260542, 0.1990643237010542, 0.19673469267695776, 0.19734504423945776, 0.19648025696536142, 0.19798628106174698, 0.1979656908885542, 0.1988201830760542, 0.19649055205195776, 0.19660232727786142, 0.1979656908885542, 0.1977215502635542, 0.19721267884036142, 0.19785391566265065, 0.19795539580195776, 0.19858633753765065, 0.19710090361445776, 0.1973553393260542, 0.19574783509036142, 0.1962258212537651, 0.1972332690135542, 0.19649055205195776, 0.19697883330195776, 0.19489334290286142, 0.19712149378765065, 0.1982098315135542, 0.19736563441265065, 0.19846426722515065, 0.19810835137424698, 0.1974774096385542, 0.19672439759036142, 0.1978436205760542, 0.19724356410015065, 0.1975994799510542, 0.19673469267695776, 0.19721267884036142, 0.1980877612010542, 0.1974774096385542, 0.19624641142695776, 0.19673469267695776, 0.19663321253765065, 0.19724356410015065, 0.1973553393260542, 0.19736563441265065, 0.1969891283885542, 0.19724356410015065, 0.19660232727786142, 0.1978436205760542, 0.1967449877635542, 0.1972332690135542, 0.1974774096385542, 0.1969891283885542, 0.19602286097515065, 0.19809805628765065, 0.1962567065135542, 0.19785391566265065, 0.19834219691265065, 0.19648025696536142, 0.19797598597515065, 0.19797598597515065, 0.19712149378765065, 0.1973553393260542, 0.19697883330195776, 0.1974774096385542, 0.19722297392695776, 0.19635818665286142, 0.19724356410015065, 0.19786421074924698, 0.19797598597515065, 0.19785391566265065, 0.1954933993787651, 0.1980877612010542, 0.1971111987010542, 0.19599197571536142], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.09342202901908092, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 32, "valid_ratio": 0.15, "learning_rate": 0.00013942540107611228, "optimization": "adam", "nb_data_augmentation": 2, "learning_rate_decay_method": "exp", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 5.316852593892481e-08, "rotation_range": [0, 0], "momentum": 0.9121796075985023}, "accuracy_valid_max": 0.8051066570971386, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8040080242846386, "accuracy_valid_std": [0.02115817772154841, 0.014828586983206143, 0.015854923138317257, 0.022082176249811587, 0.015314343841011439, 0.01586967295369503, 0.018659937872215007, 0.016292453671311574, 0.016955973882587898, 0.015712478208217126, 0.015900319115949683, 0.014117725424179153, 0.013977450569132292, 0.014940825890036329, 0.017536962846508564, 0.01686824451436844, 0.017043355616542306, 0.014693386946584255, 0.017270650636680904, 0.016867192471988052, 0.0168660344955102, 0.01570041727320335, 0.016834070429586574, 0.017292263160990987, 0.015931278754113078, 0.013127764794623818, 0.013979319846974125, 0.015268716675502137, 0.012966909432389003, 0.012446660812034516, 0.013795142509509679, 0.013655858175925444, 0.013481630632937986, 0.015073034801221933, 0.014790718563474137, 0.015067855722429644, 0.013147859280006549, 0.01376471353860953, 0.012362601005720933, 0.013136945099684149, 0.012261289631264322, 0.01372521507611148, 0.013254806150604167, 0.012318163103984803, 0.013471057206045712, 0.012470528432542876, 0.012822363414541984, 0.013246645418587923, 0.014154417527032361, 0.014069066826483326, 0.011839287468032806, 0.01218402077580751, 0.012609051360443338, 0.011071519985572558, 0.012427574109131053, 0.012065977376449991, 0.013623632765046335, 0.012778228427419625, 0.011923249035482362, 0.011878332105006164, 0.01236114870123877, 0.012722130728329497, 0.01279858448129272, 0.012650299889366466, 0.012863934150981707, 0.012810461018484987, 0.01260307095105052, 0.011736033059836542, 0.011444763462163299, 0.011204583713740288, 0.012771307853624318, 0.012225671856208133, 0.01191840794373252, 0.013548982696697787, 0.012247221093984654, 0.011964796234100001, 0.012383252597251319, 0.012987605969198302, 0.013273728346857513, 0.012771859300365361, 0.011877651124858663, 0.011325577735214013, 0.012079543432668685, 0.013610933665918387, 0.012476968079761465, 0.013479850236857977, 0.013734639302984724, 0.012812891541900502, 0.012971442258339447, 0.012927535636942062, 0.012752759547642819, 0.012107513141557892, 0.013093098856654979, 0.012161839080545302, 0.011611307875460606, 0.012597220862331438, 0.011596307943022112, 0.012225389037029254, 0.012255892622145367, 0.012456223367903436, 0.012206154807205931, 0.012455733314857397, 0.012425716841717031, 0.012840501227399551, 0.013386584406146622, 0.012300861989408107, 0.012854663871984048, 0.013117120744035086, 0.011916859123211937, 0.012731992154084423, 0.013495583902010264, 0.013719236872863189, 0.012727019524934782, 0.013192353427321205, 0.012407787857444237, 0.012102151219498399, 0.01233596766165395, 0.013496594893887503, 0.011946960134240975, 0.013064711748038055, 0.01255440116972697, 0.012548722396469108, 0.013412897371008568, 0.013222550073191208, 0.01288565491613505], "accuracy_valid": [0.4847691547439759, 0.571178758000753, 0.6319212396460843, 0.6727853798004518, 0.6890530873493976, 0.7038647755082832, 0.7163468326430723, 0.7218914721385542, 0.729968702936747, 0.726062452936747, 0.7418404085090362, 0.7494499482304217, 0.7569580078125, 0.7549225221197289, 0.7650146484375, 0.7725830078125, 0.7767539886106928, 0.7797145613704819, 0.7805999388177711, 0.7844753035579819, 0.7868358198418675, 0.7884536191641567, 0.7887889448418675, 0.7911185758659638, 0.7893787062311747, 0.7926245999623494, 0.7937026426016567, 0.7945571347891567, 0.7958190182605422, 0.795788133000753, 0.7964087796498494, 0.7989722562123494, 0.7990031414721386, 0.8003562099962349, 0.8028079113328314, 0.8017298686935241, 0.7978942135730422, 0.8017092785203314, 0.8019328289721386, 0.8036726986069277, 0.8028079113328314, 0.8034182628953314, 0.8021872646837349, 0.8022990399096386, 0.8026755459337349, 0.8000708890248494, 0.8029093914721386, 0.8031535320971386, 0.8020446041980422, 0.8027873211596386, 0.8027667309864458, 0.8016680981739458, 0.8009356762989458, 0.8032653073230422, 0.8026549557605422, 0.8035197430346386, 0.802013718938253, 0.8020343091114458, 0.8011798169239458, 0.8035094479480422, 0.8033976727221386, 0.8020343091114458, 0.8022784497364458, 0.8027873211596386, 0.8021460843373494, 0.8020446041980422, 0.8014136624623494, 0.8028990963855422, 0.8026446606739458, 0.8042521649096386, 0.8037741787462349, 0.8027667309864458, 0.8035094479480422, 0.8030211666980422, 0.8051066570971386, 0.8028785062123494, 0.8017901684864458, 0.8026343655873494, 0.8015357327748494, 0.801891648625753, 0.8025225903614458, 0.8032756024096386, 0.8021563794239458, 0.8027564358998494, 0.8024005200489458, 0.8032653073230422, 0.8027873211596386, 0.8019122387989458, 0.8025225903614458, 0.8037535885730422, 0.8032653073230422, 0.8033667874623494, 0.8027564358998494, 0.8026446606739458, 0.8026343655873494, 0.8030108716114458, 0.8027564358998494, 0.8033976727221386, 0.8021563794239458, 0.8032550122364458, 0.8027667309864458, 0.8025225903614458, 0.8030108716114458, 0.8039771390248494, 0.8019019437123494, 0.8037432934864458, 0.8021460843373494, 0.8016578030873494, 0.8035197430346386, 0.8020240140248494, 0.8020240140248494, 0.8028785062123494, 0.8026446606739458, 0.8030211666980422, 0.8025225903614458, 0.8027770260730422, 0.8036418133471386, 0.8027564358998494, 0.802135789250753, 0.8020240140248494, 0.8021460843373494, 0.8045066006212349, 0.8019122387989458, 0.8028888012989458, 0.8040080242846386], "seed": 9592957, "model": "residualv3", "loss_std": [0.3225725591182709, 0.17738617956638336, 0.17342118918895721, 0.17092572152614594, 0.169667586684227, 0.16662248969078064, 0.16695208847522736, 0.1628078818321228, 0.16119560599327087, 0.1604466289281845, 0.1590004563331604, 0.15394432842731476, 0.15556316077709198, 0.15329280495643616, 0.1513861119747162, 0.1515490859746933, 0.15047597885131836, 0.14836740493774414, 0.14674511551856995, 0.14588169753551483, 0.14559292793273926, 0.144701287150383, 0.14304789900779724, 0.1420881152153015, 0.14229626953601837, 0.14101849496364594, 0.14042238891124725, 0.13905303180217743, 0.13927549123764038, 0.13770587742328644, 0.1370849907398224, 0.13688218593597412, 0.13945049047470093, 0.13756941258907318, 0.13779954612255096, 0.1367594301700592, 0.13631759583950043, 0.13596117496490479, 0.13595488667488098, 0.13553817570209503, 0.13569697737693787, 0.13766971230506897, 0.13567861914634705, 0.13551653921604156, 0.13488973677158356, 0.13401660323143005, 0.13611692190170288, 0.13424082100391388, 0.13737016916275024, 0.13407927751541138, 0.13475649058818817, 0.13428707420825958, 0.1328999251127243, 0.13402436673641205, 0.13522347807884216, 0.13197311758995056, 0.1343620866537094, 0.1337139755487442, 0.13297255337238312, 0.13521075248718262, 0.13291336596012115, 0.13429240882396698, 0.13446512818336487, 0.13347303867340088, 0.13447383046150208, 0.13327163457870483, 0.1353241503238678, 0.13429836928844452, 0.13338567316532135, 0.13313744962215424, 0.13321280479431152, 0.13406257331371307, 0.13448823988437653, 0.13559293746948242, 0.13378164172172546, 0.13496649265289307, 0.1344672590494156, 0.13359935581684113, 0.1340191811323166, 0.13240087032318115, 0.1337561011314392, 0.13443458080291748, 0.13220882415771484, 0.1352650374174118, 0.1337425410747528, 0.13319283723831177, 0.1332920789718628, 0.13260988891124725, 0.13687211275100708, 0.13288514316082, 0.13193060457706451, 0.13466766476631165, 0.13185040652751923, 0.13266262412071228, 0.13352850079536438, 0.13315336406230927, 0.13345931470394135, 0.1338735669851303, 0.13451988995075226, 0.13308803737163544, 0.13507243990898132, 0.13309167325496674, 0.13418057560920715, 0.13227590918540955, 0.13367857038974762, 0.13421767950057983, 0.13304917514324188, 0.13055947422981262, 0.13272510468959808, 0.13373194634914398, 0.13424034416675568, 0.13364426791667938, 0.1342017501592636, 0.1325770765542984, 0.1320759505033493, 0.13432851433753967, 0.1341724991798401, 0.13360244035720825, 0.13352838158607483, 0.13357920944690704, 0.1326838880777359, 0.1327800154685974, 0.13295462727546692, 0.1327207386493683, 0.1320049911737442]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:30 2016", "state": "available"}], "summary": "cf028e9b84dcacb3897c97390a98768c"}