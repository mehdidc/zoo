{"content": {"hp_model": {"f0": 32, "f1": 64, "f2": 32, "f3": 32, "nonlin": "rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.463278889656067, 1.109770655632019, 0.9412664175033569, 0.8198372721672058, 0.7241616249084473, 0.6433541178703308, 0.5736865401268005, 0.5110762715339661, 0.4567940831184387, 0.4115090072154999, 0.37538692355155945, 0.3356538414955139, 0.30421993136405945, 0.2811216711997986, 0.25098916888237, 0.23766832053661346, 0.2018449604511261, 0.19281485676765442, 0.17421774566173553, 0.16163000464439392, 0.14902953803539276, 0.1267520636320114, 0.1253531575202942, 0.1118980273604393, 0.11426697671413422, 0.09451211243867874, 0.09177562594413757, 0.0817253440618515, 0.07634777575731277, 0.07256574183702469, 0.07503468543291092, 0.0687374547123909, 0.06351892650127411, 0.06152695044875145, 0.05558756738901138, 0.04237130656838417, 0.04035016521811485, 0.05125023424625397, 0.05112176761031151, 0.04982469603419304, 0.04234478622674942, 0.03226206451654434, 0.028998306021094322, 0.027368875220417976, 0.028206294402480125, 0.040411461144685745, 0.04242686927318573, 0.04816434159874916, 0.04007416218519211, 0.03441822901368141, 0.02662799321115017, 0.02032661624252796, 0.014267791993916035, 0.011751965619623661, 0.011084120720624924, 0.010973669588565826, 0.01093305740505457, 0.010905450209975243, 0.010884324088692665, 0.010867097415030003, 0.010852483101189137, 0.010839710012078285, 0.010828310623764992, 0.01081794872879982, 0.010808413848280907, 0.010799545794725418, 0.01079122070223093, 0.010783357545733452, 0.010775869712233543, 0.010768706910312176, 0.01076182909309864, 0.010755195282399654, 0.01074877567589283, 0.010742543265223503, 0.010736474767327309, 0.010730552487075329, 0.010724763385951519, 0.010719094425439835, 0.01071353629231453, 0.010708073154091835, 0.010702702216804028, 0.010697414167225361, 0.010692200623452663, 0.010687056928873062, 0.01068197749555111, 0.010676956735551357, 0.010671990923583508, 0.010668733157217503, 0.010668476112186909, 0.01066847238689661, 0.01066847238689661, 0.01066847238689661, 0.01066847238689661, 0.01066847238689661, 0.01066847238689661, 0.01066847238689661, 0.01066847238689661, 0.01066847238689661, 0.01066847238689661, 0.01066847238689661, 0.01066847238689661, 0.01066847238689661, 0.01066847238689661, 0.01066847238689661, 0.01066847238689661, 0.01066847238689661, 0.01066847238689661, 0.01066847238689661, 0.01066847238689661, 0.01066847238689661, 0.01066847238689661, 0.01066847238689661], "moving_avg_accuracy_train": [0.055434683866279054, 0.11476852692990955, 0.1750238668414313, 0.23249173988181748, 0.2857864466787502, 0.3354276104041937, 0.3813368784796843, 0.42386608154510635, 0.46183355209814464, 0.496734011833882, 0.5275888231770257, 0.5556413168571545, 0.5817119882872106, 0.6064564972265774, 0.6322115207516587, 0.6535219467207434, 0.6760770496273419, 0.6968371298540227, 0.7159069604675038, 0.7353599714505614, 0.7526469363936836, 0.7694397507138944, 0.7845997865782746, 0.7992597646907037, 0.8129164856538038, 0.8245009416242299, 0.8343780366344721, 0.8456459412293766, 0.855780260162456, 0.8653568042712566, 0.873450174289406, 0.8810809428689094, 0.8881602591809479, 0.8958660990343554, 0.9017318585476789, 0.9083756160703105, 0.9123674642431797, 0.9178453906974609, 0.9226686397587133, 0.9277884527162121, 0.9318382847124942, 0.9358132685912817, 0.9400603608905146, 0.9443523519217104, 0.9475549097343382, 0.9500560315562163, 0.9523301845375456, 0.9546372667897711, 0.9564044320739262, 0.9586761494308562, 0.9614483945318366, 0.9642317335751, 0.9669529054556852, 0.9698065360410691, 0.972456183776248, 0.9748780691188612, 0.9770740419688799, 0.9790597181291348, 0.9808607775662214, 0.9824863813572182, 0.983965700810782, 0.9852994134677991, 0.9864997548591145, 0.9875800621112982, 0.9885523386382636, 0.9894320378101515, 0.9902237670648506, 0.9909363233940798, 0.9915892498344338, 0.9921792087795618, 0.9927124969789866, 0.9931947815072784, 0.993628837582741, 0.994026463497086, 0.9943866519688059, 0.9947108215933539, 0.995002574255447, 0.9952698019489499, 0.9955126320219121, 0.9957288539387685, 0.9959234536639392, 0.9960962682677833, 0.9962587768576716, 0.9964073597373806, 0.9965434094779283, 0.9966681793932307, 0.9967804723170028, 0.9968838610972073, 0.9969769109993912, 0.9970606559113568, 0.9971360263321258, 0.9972038597108179, 0.9972649097516408, 0.9973198547883815, 0.9973693053214481, 0.997413810801208, 0.9974538657329919, 0.9974899151715975, 0.9975223596663424, 0.9975515597116129, 0.9975778397523564, 0.9976014917890255, 0.9976227786220276, 0.9976419367717296, 0.9976591791064614, 0.99767469720772, 0.9976886634988527, 0.9977012331608721, 0.9977125458566897, 0.9977227272829254, 0.9977318905665377, 0.9977401375217886], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.054494393589984924, 0.11192381341773341, 0.16905698036285766, 0.22285734877988514, 0.27211702405814664, 0.3174405308484916, 0.35889601419060024, 0.3961815351359981, 0.4285279488757868, 0.4567312533857985, 0.4810491272000801, 0.5029199180757046, 0.5220084267858299, 0.5407141106038583, 0.5599204786285629, 0.5750495362815198, 0.5911560696168467, 0.6052286744868639, 0.6182064012117467, 0.6315454820054818, 0.6432506264818915, 0.6538187890784313, 0.663906954410046, 0.6737818192570685, 0.6827821494266176, 0.6895446736123445, 0.6960968715843179, 0.7032226418807054, 0.7092677122653005, 0.7152189119066168, 0.7204996903789822, 0.7246470399122437, 0.7286726232421789, 0.7336060656901748, 0.7367795740966845, 0.7413305970616395, 0.7428446570843913, 0.7461817616413888, 0.7486509878209999, 0.751385104259156, 0.7537858034059061, 0.7559168710321829, 0.7587819798626995, 0.7615447127875741, 0.7636486364165727, 0.7649806442451714, 0.7659586952197507, 0.7672651576819624, 0.7679659291878624, 0.7691103483643322, 0.7714302999736821, 0.7737369534759373, 0.7758433856697592, 0.7787330917187472, 0.7812605849753362, 0.7835363584149261, 0.7856323531268974, 0.7875930200638311, 0.7894064484320715, 0.7910751550572378, 0.7925769910198874, 0.7939540869574317, 0.7951812662699718, 0.7962613135887577, 0.7971468774482555, 0.7979815355242131, 0.798732727792575, 0.7993599727091006, 0.7999244931339737, 0.8004325615163596, 0.8009020300917566, 0.801324551809614, 0.8016926143244358, 0.8020360776190254, 0.802357401615406, 0.8026343861808986, 0.8028714652585919, 0.8031469010934253, 0.8034070003760255, 0.8036655037928657, 0.8038981568680219, 0.8041319586981626, 0.8043301733140391, 0.804496359437078, 0.804633719916563, 0.8047573443480994, 0.8048808133677322, 0.8049919354854017, 0.8050919453913042, 0.8051819543066165, 0.8052629623303976, 0.8053358695518006, 0.8054014860510632, 0.8054605409003996, 0.8055136902648025, 0.805561524692765, 0.8056045756779312, 0.8056433215645808, 0.8056781928625655, 0.8057095770307516, 0.8057378227821193, 0.8057632439583501, 0.8057861230169578, 0.8058067141697048, 0.8058252462071771, 0.8058419250409021, 0.8058569359912546, 0.8058704458465719, 0.8058826047163574, 0.8058935476991644, 0.8059033963836907, 0.8059122601997644], "moving_var_accuracy_train": [0.0276570375781887, 0.056575878214665615, 0.08359464428387631, 0.10495818774156221, 0.1200253009205452, 0.1302009770526373, 0.1361498274044189, 0.1388134426843962, 0.13790585779771902, 0.13507765082583958, 0.13013806019044455, 0.12420673578646306, 0.11790318138714222, 0.11162347975228162, 0.10643102290805087, 0.09987512891210037, 0.09446621002503561, 0.08889841740169603, 0.08328150161816818, 0.07835912818311402, 0.07321276777730522, 0.0684294785147124, 0.06365497084992482, 0.059223708389244435, 0.054979891797495815, 0.050689699198922925, 0.046498742331602796, 0.042991559164081365, 0.03961674302980956, 0.03648046050043882, 0.03342193819465105, 0.030603802037211386, 0.027994472308503306, 0.025729444788270266, 0.023466164521456532, 0.021516803695486586, 0.019508536992455087, 0.0178277523973562, 0.016254350741182432, 0.014864828029542155, 0.013525955479370931, 0.012315564402963422, 0.011246348099650907, 0.010287503972792609, 0.009351060964402358, 0.008472255361272996, 0.007671575771188096, 0.006952321850736092, 0.006285195523936188, 0.0057031222692905635, 0.005201978128460698, 0.004751503101682417, 0.004342995779147365, 0.003981985068893173, 0.0036469722600886985, 0.0033350647915947147, 0.0030449589832574142, 0.0027759492732523138, 0.0025275486817903483, 0.0022985771027790453, 0.0020884148669123737, 0.001895582485284523, 0.001718991611857414, 0.0015575960245037606, 0.001410344316857376, 0.0012762747208688212, 0.0011542887656966572, 0.0010434295178279132, 0.0009429233824737412, 0.0008517635082387966, 0.0007691467241477284, 0.0006943254370290225, 0.0006265885354159345, 0.0005653526391841689, 0.0005099849968821917, 0.00045993227070328875, 0.000414705120175506, 0.0003738773039195294, 0.00033702027152658973, 0.0003037390116298923, 0.0002737059319442319, 0.0002466041227355251, 0.00022218139183806008, 0.00020016194450353771, 0.0001803123358403117, 0.00016242121004216164, 0.0001462925763445092, 0.00013175952186890783, 0.00011866149424068518, 0.00010685846370913787, 9.622374364116628e-05, 8.664278158243284e-05, 7.801204739154995e-05, 7.023801326595687e-05, 6.323622013634634e-05, 5.6930424762269695e-05, 5.1251821864084697e-05, 4.6138335735890225e-05, 4.153397596945454e-05, 3.73882521563033e-05, 3.3655642705546276e-05, 3.0295113204539032e-05, 2.7269680047418513e-05, 2.45460153549767e-05, 2.2094089502442064e-05, 1.988684785539792e-05, 1.7899918585450176e-05, 1.6111348694534722e-05, 1.4501365618861195e-05, 1.3052162009936831e-05, 1.1747701500842171e-05, 1.0573543461196162e-05], "duration": 67286.959422, "accuracy_train": [0.5543468386627907, 0.648773114502584, 0.7173219260451273, 0.7497025972452934, 0.7654388078511444, 0.7821980839331857, 0.7945202911590993, 0.8066289091339055, 0.803540787075489, 0.8108381494555187, 0.8052821252653194, 0.8081137599783131, 0.816348031157715, 0.8291570776808784, 0.8640067324773901, 0.8453157804425065, 0.8790729757867294, 0.8836778518941492, 0.8875354359888336, 0.9104370702980805, 0.9082296208817828, 0.9205750795957919, 0.9210401093576966, 0.9311995677025655, 0.9358269743217055, 0.9287610453580657, 0.9232718917266519, 0.947057082583518, 0.9469891305601699, 0.9515457012504615, 0.9462905044527501, 0.9497578600844407, 0.9518741059892949, 0.9652186577150241, 0.9545236941675894, 0.9681694337739941, 0.9482940977990033, 0.9671467287859912, 0.9660778813099853, 0.9738667693337025, 0.968286772679033, 0.9715881235003692, 0.9782841915836102, 0.9829802712024732, 0.9763779300479882, 0.9725661279531194, 0.9727975613695091, 0.9754010070598007, 0.9723089196313216, 0.9791216056432264, 0.9863986004406607, 0.9892817849644703, 0.9914434523809523, 0.9954892113095238, 0.9963030133928571, 0.9966750372023809, 0.9968377976190477, 0.9969308035714286, 0.9970703125, 0.9971168154761905, 0.9972795758928571, 0.9973028273809523, 0.9973028273809523, 0.9973028273809523, 0.9973028273809523, 0.9973493303571429, 0.9973493303571429, 0.9973493303571429, 0.9974655877976191, 0.9974888392857143, 0.9975120907738095, 0.9975353422619048, 0.9975353422619048, 0.9976050967261905, 0.9976283482142857, 0.9976283482142857, 0.9976283482142857, 0.9976748511904762, 0.9976981026785714, 0.9976748511904762, 0.9976748511904762, 0.9976515997023809, 0.9977213541666666, 0.9977446056547619, 0.9977678571428571, 0.9977911086309523, 0.9977911086309523, 0.9978143601190477, 0.9978143601190477, 0.9978143601190477, 0.9978143601190477, 0.9978143601190477, 0.9978143601190477, 0.9978143601190477, 0.9978143601190477, 0.9978143601190477, 0.9978143601190477, 0.9978143601190477, 0.9978143601190477, 0.9978143601190477, 0.9978143601190477, 0.9978143601190477, 0.9978143601190477, 0.9978143601190477, 0.9978143601190477, 0.9978143601190477, 0.9978143601190477, 0.9978143601190477, 0.9978143601190477, 0.9978143601190477, 0.9978143601190477, 0.9978143601190477], "end": "2016-02-02 04:28:28.189000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0], "moving_var_accuracy_valid": [0.026726750394661717, 0.05373731971096167, 0.07774137662648044, 0.09601755574010339, 0.10825444064277093, 0.11591697898846386, 0.11979229498176812, 0.1203249561331186, 0.11770907485623708, 0.11309700483817338, 0.10710953523598193, 0.10070356515411145, 0.09391254912168907, 0.08767041767342063, 0.08222333706038211, 0.07606099882354236, 0.07078968268592611, 0.06549305828778217, 0.06045954497751561, 0.056014970167560144, 0.05164656681572674, 0.047487084680156194, 0.043654315929962516, 0.04016650093868891, 0.0368789043332681, 0.033602599500204146, 0.03062872123455912, 0.028022838531955113, 0.025549440562351984, 0.02331324750065402, 0.021232902342056402, 0.019264416681210154, 0.017483822903405413, 0.01595449030255407, 0.01444968167275435, 0.013191119795726848, 0.011892639215926619, 0.010803601695752967, 0.009778115227512362, 0.00886758223903768, 0.00803269422267277, 0.007270297843455381, 0.006617147696606184, 0.006024127165873243, 0.005461552901015847, 0.004931365814613297, 0.004446838486531846, 0.004017516235365175, 0.0036201843381599916, 0.0032699531616072384, 0.002991397424674042, 0.002740143535621837, 0.002506062691344179, 0.002330610031655778, 0.0021550430279491327, 0.001986151027889304, 0.0018270746695938764, 0.0016789651361727553, 0.0015406653245761304, 0.001411660028326384, 0.0012907936268221176, 0.0011787818031307174, 0.00107445734440378, 0.0009775101298607528, 0.0008868171270179139, 0.0008044053012499734, 0.0007290433795413952, 0.000659679967255021, 0.0005965801203204088, 0.0005392453096189887, 0.000487304385346659, 0.0004401806682305442, 0.00039738183154084186, 0.0003587053516993305, 0.0003237640585252474, 0.00029207813671841245, 0.00026337618144829027, 0.0002377213473954555, 0.00021455807738719239, 0.00019370368579713575, 0.0001748204642978389, 0.00015783038753004877, 0.00014240095008256754, 0.00012840941552172698, 0.00011573828508147336, 0.00010430200357398047, 9.400900460586425e-05, 8.471923727059611e-05, 7.633733137504433e-05, 6.877651268106125e-05, 6.195792211220744e-05, 5.580996906738106e-05, 5.02677218854224e-05, 4.527233697395152e-05, 4.077052697098417e-05, 3.6714067466372266e-05, 3.305934120564912e-05, 2.9766918278674613e-05, 2.6801170517615385e-05, 2.4129918159968518e-05, 2.172410674620455e-05, 1.9557512197392734e-05, 1.7606472039558456e-05, 1.584964079574566e-05, 1.4267767643886963e-05, 1.284349453094812e-05, 1.1561173035527692e-05, 1.0406698377691176e-05, 9.367359082952224e-06, 8.431700914511437e-06, 7.589403792342386e-06, 6.831170518226644e-06], "accuracy_test": 0.797476881377551, "start": "2016-02-01 09:47:01.230000", "learning_rate_per_epoch": [0.009102641604840755, 0.009102641604840755, 0.009102641604840755, 0.009102641604840755, 0.009102641604840755, 0.009102641604840755, 0.009102641604840755, 0.009102641604840755, 0.009102641604840755, 0.009102641604840755, 0.009102641604840755, 0.009102641604840755, 0.009102641604840755, 0.009102641604840755, 0.009102641604840755, 0.009102641604840755, 0.009102641604840755, 0.009102641604840755, 0.009102641604840755, 0.009102641604840755, 0.009102641604840755, 0.009102641604840755, 0.009102641604840755, 0.009102641604840755, 0.009102641604840755, 0.009102641604840755, 0.009102641604840755, 0.009102641604840755, 0.009102641604840755, 0.009102641604840755, 0.009102641604840755, 0.009102641604840755, 0.009102641604840755, 0.009102641604840755, 0.009102641604840755, 0.009102641604840755, 0.009102641604840755, 0.009102641604840755, 0.009102641604840755, 0.009102641604840755, 0.009102641604840755, 0.009102641604840755, 0.009102641604840755, 0.009102641604840755, 0.009102641604840755, 0.009102641604840755, 0.009102641604840755, 0.009102641604840755, 0.009102641604840755, 0.009102641604840755, 0.009102641604840755, 0.009102641604840755, 0.009102641604840755, 0.009102641604840755, 0.009102641604840755, 0.009102641604840755, 0.009102641604840755, 0.009102641604840755, 0.009102641604840755, 0.009102641604840755, 0.009102641604840755, 0.009102641604840755, 0.009102641604840755, 0.009102641604840755, 0.009102641604840755, 0.009102641604840755, 0.009102641604840755, 0.009102641604840755, 0.009102641604840755, 0.009102641604840755, 0.009102641604840755, 0.009102641604840755, 0.009102641604840755, 0.009102641604840755, 0.009102641604840755, 0.009102641604840755, 0.009102641604840755, 0.009102641604840755, 0.009102641604840755, 0.009102641604840755, 0.009102641604840755, 0.009102641604840755, 0.009102641604840755, 0.009102641604840755, 0.009102641604840755, 0.009102641604840755, 0.0009102641488425434, 9.102641342906281e-05, 9.102641342906281e-06, 9.102641342906281e-07, 9.102641485014829e-08, 9.102641307379145e-09, 9.102641418401447e-10, 9.102641557179325e-11, 9.102641383706978e-12, 9.102641600547412e-13, 9.102641600547412e-14, 9.102641261734233e-15, 9.102641685250707e-16, 9.102641552901809e-17, 9.102641222029564e-18, 9.102640808439257e-19, 9.102640549945316e-20, 9.102640388386603e-21, 9.10263998448982e-22, 9.102640110707564e-23, 9.102639952935383e-24, 9.102640347365836e-25, 9.102640100846803e-26, 9.102639792698012e-27, 9.102640177884e-28, 9.102640057513379e-29], "accuracy_train_first": 0.5543468386627907, "accuracy_train_last": 0.9978143601190477, "batch_size_eval": 1024, "accuracy_train_std": [0.018180636016290372, 0.020043563576387608, 0.0231172313787811, 0.023239719386047363, 0.02344947211646876, 0.02698400099529563, 0.027836918894676998, 0.030035563285248066, 0.02776213226333806, 0.029913289336935985, 0.031178825891915275, 0.03432220680807694, 0.030227700886293424, 0.03042192551519669, 0.03175663198048278, 0.03184034362857697, 0.02741531890956432, 0.026400754332863458, 0.026883334025764278, 0.023696316240927155, 0.02193526593757544, 0.021918719455131557, 0.019337277312166865, 0.020010470634455324, 0.017624649310831603, 0.016266050390534973, 0.018206891507133786, 0.01583874968195821, 0.015033480436784591, 0.014425112107340574, 0.015039902860579268, 0.013363345341396642, 0.012761316064282073, 0.009365387508451032, 0.012397187421935732, 0.009390939555141274, 0.01040136002880734, 0.009823831831075523, 0.009892636055463803, 0.007449404277871534, 0.008757962525450762, 0.00846028484657997, 0.006639003489322112, 0.006333805347928444, 0.006173543704986912, 0.007280896232880653, 0.00851069659498466, 0.007071475400886549, 0.007223815047612697, 0.00574108684239813, 0.004583958781038557, 0.003380930273940591, 0.00284961192333585, 0.002119842135424383, 0.0018024028065739858, 0.0017188810991700353, 0.0017548327998617194, 0.0017774846404033102, 0.0017956412366357721, 0.0018076443197151053, 0.0017251601377929265, 0.001728760258132543, 0.001728760258132543, 0.0017805235973843607, 0.0017418453125305098, 0.0017480418646961032, 0.0017480418646961032, 0.0017480418646961032, 0.0016922547407857852, 0.0016663386223185377, 0.0016534734651749467, 0.0016811967524670032, 0.0016811967524670032, 0.0016941705014831539, 0.001638858834329064, 0.001638858834329064, 0.001638858834329064, 0.0016222808354271212, 0.001620446901416102, 0.00167733340744712, 0.00167733340744712, 0.0016787832037002137, 0.0016598370812476513, 0.001671036421952283, 0.001626939702802818, 0.0015955671934800746, 0.0015955671934800746, 0.0016062049887621414, 0.0016062049887621414, 0.0016062049887621414, 0.0016062049887621414, 0.0016062049887621414, 0.0016062049887621414, 0.0016062049887621414, 0.0016062049887621414, 0.0016062049887621414, 0.0016062049887621414, 0.0016062049887621414, 0.0016062049887621414, 0.0016062049887621414, 0.0016062049887621414, 0.0016062049887621414, 0.0016062049887621414, 0.0016062049887621414, 0.0016062049887621414, 0.0016062049887621414, 0.0016062049887621414, 0.0016062049887621414, 0.0016062049887621414, 0.0016062049887621414, 0.0016062049887621414, 0.0016062049887621414], "accuracy_test_std": 0.007986587704747522, "error_valid": [0.45505606410015065, 0.3712114081325302, 0.31674451713102414, 0.29293933546686746, 0.2845458984375, 0.27464790803840367, 0.26800463573042166, 0.26824877635542166, 0.28035432746611444, 0.28943900602409633, 0.30009000847138556, 0.3002429640436747, 0.30619499482304224, 0.29093473503388556, 0.26722220914909633, 0.28878894484186746, 0.2638851303652108, 0.2681178816829819, 0.2649940582643072, 0.24840279085090367, 0.25140307323042166, 0.2510677475527108, 0.24529955760542166, 0.23734439711972888, 0.23621487904743976, 0.24959260871611444, 0.24493334666792166, 0.23264542545180722, 0.23632665427334332, 0.2312202913215362, 0.23197330336972888, 0.23802681428840367, 0.23509712678840367, 0.22199295227786142, 0.23465885024472888, 0.2177101962537651, 0.24352880271084332, 0.22378429734563254, 0.2291259765625, 0.22400784779743976, 0.22460790427334332, 0.22490352033132532, 0.21543204066265065, 0.2135906908885542, 0.21741605092243976, 0.22303128529743976, 0.2252388460090362, 0.22097668015813254, 0.2257271272590362, 0.22058987904743976, 0.20769013554216864, 0.2055031650037651, 0.20519872458584332, 0.19525955384036142, 0.19599197571536142, 0.1959816806287651, 0.19550369446536142, 0.1947609775037651, 0.1942726962537651, 0.1939064853162651, 0.1939064853162651, 0.19365204960466864, 0.19377411991716864, 0.19401826054216864, 0.1948830478162651, 0.19450654179216864, 0.19450654179216864, 0.19499482304216864, 0.19499482304216864, 0.19499482304216864, 0.19487275272966864, 0.19487275272966864, 0.19499482304216864, 0.19487275272966864, 0.19475068241716864, 0.19487275272966864, 0.19499482304216864, 0.1943741763930723, 0.1942521060805723, 0.1940079654555723, 0.1940079654555723, 0.1937638248305723, 0.1938858951430723, 0.1940079654555723, 0.1941300357680723, 0.1941300357680723, 0.1940079654555723, 0.1940079654555723, 0.1940079654555723, 0.1940079654555723, 0.1940079654555723, 0.1940079654555723, 0.1940079654555723, 0.1940079654555723, 0.1940079654555723, 0.1940079654555723, 0.1940079654555723, 0.1940079654555723, 0.1940079654555723, 0.1940079654555723, 0.1940079654555723, 0.1940079654555723, 0.1940079654555723, 0.1940079654555723, 0.1940079654555723, 0.1940079654555723, 0.1940079654555723, 0.1940079654555723, 0.1940079654555723, 0.1940079654555723, 0.1940079654555723, 0.1940079654555723], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.041898021450695794, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 16, "valid_ratio": 0.15, "learning_rate": 0.009102642043475849, "optimization": "nesterov_momentum", "nb_data_augmentation": 0, "learning_rate_decay_method": "discrete", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 1.5665836405302597e-06, "rotation_range": [0, 0], "momentum": 0.6020846914666564}, "accuracy_valid_max": 0.8063479503953314, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8059920345444277, "accuracy_valid_std": [0.018135373799792943, 0.017858572510446362, 0.014466093411882921, 0.016288998277590314, 0.017860267570131663, 0.014051438469450865, 0.01664875749426023, 0.019594979108994316, 0.020338748594211038, 0.01752915068523674, 0.01764958636136952, 0.013541842497041996, 0.014875226266748835, 0.013814209019035356, 0.013962685398093793, 0.014106230900787354, 0.012339713841668567, 0.01513358312807836, 0.011215123783992116, 0.011980879488469941, 0.014252288971640333, 0.01326496924760085, 0.012193027570119428, 0.008599056785647317, 0.011667807561163882, 0.011418986864838407, 0.011420611254579818, 0.012007952038869767, 0.014503491638493973, 0.013209974069902092, 0.010961145604861241, 0.011272562410573998, 0.010227364963259758, 0.01223783573532692, 0.013361317101799396, 0.014147649168035967, 0.01734122818053978, 0.013021572210388275, 0.012078794196812959, 0.010746699311326161, 0.010743658830651131, 0.008739546372144368, 0.013726955533095057, 0.01384602408333979, 0.01223526622824903, 0.0118550719935279, 0.011565848289395844, 0.009780226992933745, 0.007556361699764013, 0.013620856134156638, 0.014267460397007293, 0.010438707558959757, 0.012161346502739657, 0.013569891724814038, 0.012904144286852907, 0.012106400710966898, 0.011659659361100413, 0.01172221136303215, 0.011770325955791605, 0.011858546280390235, 0.01227349584293011, 0.012092452215847739, 0.011895036081407281, 0.012200785265624072, 0.011993668307550028, 0.012006773186034157, 0.012135157348349017, 0.012128290815903045, 0.012332973213149008, 0.011979948223118275, 0.011790502818073067, 0.012139217724859524, 0.012216431775564918, 0.012040615481798595, 0.011870983669887581, 0.011911212350227315, 0.011880024142971221, 0.011743916922362442, 0.011738603404828884, 0.011341709127594397, 0.011341709127594397, 0.011087424920649, 0.011258385946954979, 0.011193592177113484, 0.011276072689971148, 0.01112708206144321, 0.011289033308456941, 0.011289033308456941, 0.011289033308456941, 0.011289033308456941, 0.011289033308456941, 0.011289033308456941, 0.011289033308456941, 0.011289033308456941, 0.011289033308456941, 0.011289033308456941, 0.011289033308456941, 0.011289033308456941, 0.011289033308456941, 0.011289033308456941, 0.011289033308456941, 0.011289033308456941, 0.011289033308456941, 0.011289033308456941, 0.011289033308456941, 0.011289033308456941, 0.011289033308456941, 0.011289033308456941, 0.011289033308456941, 0.011289033308456941, 0.011289033308456941, 0.011289033308456941], "accuracy_valid": [0.5449439358998494, 0.6287885918674698, 0.6832554828689759, 0.7070606645331325, 0.7154541015625, 0.7253520919615963, 0.7319953642695783, 0.7317512236445783, 0.7196456725338856, 0.7105609939759037, 0.6999099915286144, 0.6997570359563253, 0.6938050051769578, 0.7090652649661144, 0.7327777908509037, 0.7112110551581325, 0.7361148696347892, 0.7318821183170181, 0.7350059417356928, 0.7515972091490963, 0.7485969267695783, 0.7489322524472892, 0.7547004423945783, 0.7626556028802711, 0.7637851209525602, 0.7504073912838856, 0.7550666533320783, 0.7673545745481928, 0.7636733457266567, 0.7687797086784638, 0.7680266966302711, 0.7619731857115963, 0.7649028732115963, 0.7780070477221386, 0.7653411497552711, 0.7822898037462349, 0.7564711972891567, 0.7762157026543675, 0.7708740234375, 0.7759921522025602, 0.7753920957266567, 0.7750964796686747, 0.7845679593373494, 0.7864093091114458, 0.7825839490775602, 0.7769687147025602, 0.7747611539909638, 0.7790233198418675, 0.7742728727409638, 0.7794101209525602, 0.7923098644578314, 0.7944968349962349, 0.7948012754141567, 0.8047404461596386, 0.8040080242846386, 0.8040183193712349, 0.8044963055346386, 0.8052390224962349, 0.8057273037462349, 0.8060935146837349, 0.8060935146837349, 0.8063479503953314, 0.8062258800828314, 0.8059817394578314, 0.8051169521837349, 0.8054934582078314, 0.8054934582078314, 0.8050051769578314, 0.8050051769578314, 0.8050051769578314, 0.8051272472703314, 0.8051272472703314, 0.8050051769578314, 0.8051272472703314, 0.8052493175828314, 0.8051272472703314, 0.8050051769578314, 0.8056258236069277, 0.8057478939194277, 0.8059920345444277, 0.8059920345444277, 0.8062361751694277, 0.8061141048569277, 0.8059920345444277, 0.8058699642319277, 0.8058699642319277, 0.8059920345444277, 0.8059920345444277, 0.8059920345444277, 0.8059920345444277, 0.8059920345444277, 0.8059920345444277, 0.8059920345444277, 0.8059920345444277, 0.8059920345444277, 0.8059920345444277, 0.8059920345444277, 0.8059920345444277, 0.8059920345444277, 0.8059920345444277, 0.8059920345444277, 0.8059920345444277, 0.8059920345444277, 0.8059920345444277, 0.8059920345444277, 0.8059920345444277, 0.8059920345444277, 0.8059920345444277, 0.8059920345444277, 0.8059920345444277, 0.8059920345444277, 0.8059920345444277], "seed": 822850653, "model": "residualv3", "loss_std": [0.3321309983730316, 0.2814313769340515, 0.27348580956459045, 0.2665083110332489, 0.258821040391922, 0.24913078546524048, 0.23861050605773926, 0.2283109724521637, 0.21273097395896912, 0.20493444800376892, 0.19352224469184875, 0.1837901920080185, 0.17809347808361053, 0.16358457505702972, 0.15533779561519623, 0.1530214250087738, 0.13586512207984924, 0.13732706010341644, 0.1310827136039734, 0.1271727830171585, 0.11963985860347748, 0.10453315824270248, 0.10968819260597229, 0.1009177714586258, 0.10599618405103683, 0.09065025299787521, 0.09126780927181244, 0.08253271132707596, 0.0800827294588089, 0.07796508073806763, 0.0822276771068573, 0.07965481281280518, 0.07419030368328094, 0.07038463652133942, 0.0668393075466156, 0.0521325059235096, 0.05061640590429306, 0.062341876327991486, 0.06608323752880096, 0.06752388924360275, 0.055471133440732956, 0.04177974537014961, 0.037028294056653976, 0.03641491383314133, 0.03925471007823944, 0.0602433942258358, 0.060566093772649765, 0.06632479280233383, 0.05891901254653931, 0.049945078790187836, 0.035646386444568634, 0.02417055144906044, 0.01133988332003355, 0.0028768451884388924, 0.0005035365466028452, 0.0002176702837459743, 0.00017296368605457246, 0.00014740298502147198, 0.0001298596034757793, 0.00011676883150357753, 0.00010654570360202342, 9.82756755547598e-05, 9.14172051125206e-05, 8.559843990951777e-05, 8.059374522417784e-05, 7.623403507750481e-05, 7.238939724629745e-05, 6.898050924064592e-05, 6.592429417651147e-05, 6.316821236396208e-05, 6.0673464759020135e-05, 5.839835648657754e-05, 5.631473322864622e-05, 5.439544838736765e-05, 5.261912156129256e-05, 5.096838503959589e-05, 4.94316773256287e-05, 4.799795715371147e-05, 4.6660908992635086e-05, 4.540407462627627e-05, 4.422541314852424e-05, 4.31169246439822e-05, 4.2070492781931534e-05, 4.108237772015855e-05, 4.01472752855625e-05, 3.9261005440494046e-05, 3.841943544102833e-05, 3.74254013877362e-05, 3.732499681063928e-05, 3.7319983675843105e-05, 3.7320009141694754e-05, 3.7319969123927876e-05, 3.7320034607546404e-05, 3.731996548594907e-05, 3.731999095180072e-05, 3.731996548594907e-05, 3.7319972761906683e-05, 3.73199800378643e-05, 3.731997639988549e-05, 3.732000550371595e-05, 3.7320020055631176e-05, 3.7319947296055034e-05, 3.73199800378643e-05, 3.73199800378643e-05, 3.732000186573714e-05, 3.7319983675843105e-05, 3.732000186573714e-05, 3.732002733158879e-05, 3.7319958209991455e-05, 3.732000550371595e-05, 3.7319921830203384e-05, 3.731998731382191e-05]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:30 2016", "state": "available"}], "summary": "1841dd2d936dc0800dc154b9c40fb0c0"}