{"content": {"hp_model": {"f0": 64, "f1": 32, "f2": 32, "f3": 64, "nonlin": "rectify", "nbg1": 8, "nbg3": 8, "nbg2": 2, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.019133804431286646, 0.025213071922204462, 0.02603831011848575, 0.023654398519301623, 0.01925497514212147, 0.016368820531034268, 0.020130277542649008, 0.017869542404458306, 0.017539716108057928, 0.017439911112074765, 0.01622883230588808, 0.017725444107069534, 0.01795039706566906, 0.017956970685968444, 0.016989057133728253, 0.017908960897024515, 0.014863257312922361, 0.015575111604248423, 0.01386668035769578, 0.01514941523333901, 0.013489738361595482, 0.014872393734183913, 0.013277387595251532, 0.013841299160295743, 0.014273011670417588, 0.014292929884175468, 0.012853742988827709, 0.01722009970147779, 0.013653750313375798, 0.014502071168664895, 0.014262224698169323, 0.014443065264892935, 0.013319683283379244, 0.014745387519085914, 0.014708159564380359, 0.013597085803575316, 0.01398682763219402, 0.012881863318910168, 0.013421486279688085, 0.013440340542723752, 0.01237588193989737, 0.012447576275187012, 0.012775031773654414, 0.013937040366622043, 0.012936159786017705, 0.014554139784691967, 0.013664196381491184, 0.011511826811738339, 0.01125519878578602, 0.012920349666432986, 0.010364395866728793, 0.0125529433991826, 0.013661778616012213, 0.01371676100116088, 0.013310654363625187, 0.013782986091763419, 0.011367135010744754, 0.012447377913173498, 0.012227184389858831, 0.014436309373164282, 0.013903783959752863, 0.012782946863604622, 0.012110377316180471, 0.012956692088277983, 0.012909046257235445, 0.014031802702646744, 0.012306416273431164, 0.011637199647170696, 0.009863026641525652, 0.012978089361530424, 0.014351558933657917, 0.009768126243967607, 0.012383238186607068, 0.01128887283800493, 0.015527972367885055, 0.01348595998075799, 0.012098708359088425, 0.013144886875156983, 0.013916434283070387, 0.010665674020484671, 0.013919317296009473, 0.013676023288035102, 0.011769615845745778, 0.012057346506147185, 0.011752660454592175, 0.012145056273118047, 0.011709238745124563, 0.012017733998258303, 0.011365806052196118, 0.013264356113370875, 0.013389372705413688, 0.012379314111842033, 0.013938245214329369, 0.013798746488073738, 0.014807511968186679, 0.013041444367880437, 0.014136880988472066, 0.011438589123430279, 0.012194042164301975, 0.01462690173404766, 0.013675839501731742, 0.015046393681202055, 0.01541727195621132, 0.01529409435160114, 0.013986746209710552, 0.013288577169487639, 0.01286956173784429, 0.014374558143349585, 0.014084158561304074, 0.013211593713458106, 0.014999643460833288, 0.013945042617359422, 0.015068356211884855, 0.016152605607864988, 0.014107235542864029, 0.013646691241745636, 0.016306213537162828, 0.015951442195697458, 0.015139253512069996, 0.013139094632873662, 0.01456188974280489, 0.012269843328470044, 0.01302216621266426, 0.01233417244767054, 0.013409598119243431, 0.014465463301475766, 0.014565401941303291, 0.012803267370908127, 0.01510284985853503, 0.014835830619383808, 0.01283032500140115, 0.013350882259534297, 0.013604191284742221, 0.013420672385658663, 0.011838285042472637, 0.01349862946978831, 0.011288465376512246, 0.01605080108979698, 0.013343719701091734, 0.013184155186797988, 0.014865141140886603, 0.013176434975477104, 0.01509358086617933, 0.013110453058778136, 0.013446329252666282, 0.016236505493675783, 0.013414721884824328, 0.013826863702392167, 0.012318386110611044, 0.014498008623610035, 0.015192192546374375, 0.015979263059026354, 0.013670499130532357, 0.012050636941962541, 0.012937721837859635, 0.012310269580804546, 0.014282178022834218, 0.01362630144306855, 0.0126645873606991, 0.014720761351696364, 0.014824664109420682, 0.013497416148836889, 0.01485820157151725, 0.014344172458336423, 0.014001621491383322, 0.013676498321483009, 0.012448142845027687, 0.01347946190726957, 0.014028468297070862, 0.012868112859625361, 0.01342507649438368, 0.012693876364596368, 0.013988738024868249, 0.014791328091131116, 0.015571170379190095, 0.014495388813224102, 0.013444275901966648, 0.013496596897204595, 0.013644609546754982, 0.013765388061066575, 0.014657897818100422, 0.012801832025574243, 0.013995288270138826, 0.013619026676686097, 0.013649166749556893, 0.014598785431914323, 0.013549801095689056, 0.014333905641236508, 0.013976238499773766, 0.01417057085648384, 0.012552219663196421, 0.013624099781744049, 0.01344435304804907, 0.014016534612796647, 0.014216390390898899, 0.014149725952516696, 0.013903291099509213, 0.012873545900209572, 0.014286853959908185, 0.012409408923821325, 0.013523738990429296, 0.013489795364776142, 0.013450280790829395, 0.013207378165467028, 0.013341933871532836, 0.01414898483021154, 0.014729807885863984, 0.01334839129679785, 0.014091877190845746, 0.014508555889170906, 0.014886816325435542, 0.014234961138219, 0.014522036929484717, 0.015215271606431017, 0.014976812425388876, 0.015026536750829666, 0.01491730896473204, 0.01480167182672881, 0.014002675269403827, 0.012964621486518202, 0.013042730372768667, 0.014478931579655293, 0.01453437587618688, 0.01396808037378936, 0.013293700312274151, 0.014455086231906675, 0.014175870847657033, 0.0135647771383363, 0.013828992962651152, 0.011847769379507704, 0.013605712490933567, 0.013996160549407678, 0.01191851320576644, 0.012597432060746691, 0.012486023324959346, 0.012955111453274801, 0.01338385815207614, 0.013799984883620723, 0.012955111453274801, 0.013568662298805169, 0.013464340848868623, 0.015057968060054614, 0.014182303016965552, 0.014331739785656643, 0.01339691038862063, 0.012435098162904697, 0.01398111894556786, 0.014018072716757394, 0.013886099833256857], "moving_avg_accuracy_train": [0.053565444467515676, 0.11354186854005166, 0.17433403916920448, 0.23496359523521132, 0.2913178746409537, 0.3440384629382094, 0.39249116830090375, 0.4372891153666236, 0.4783116436197822, 0.5162059761547216, 0.5508572854064052, 0.5826758321114734, 0.6116612604186447, 0.6383270358510327, 0.6627540250723155, 0.6849684690547941, 0.705405572061644, 0.7240314075511237, 0.7411875735916462, 0.7568861784971549, 0.7714123431144475, 0.7847184061509633, 0.7968496839028844, 0.8080118303093383, 0.818148478927537, 0.8276224880565163, 0.8363559624190079, 0.8441627190690876, 0.8514816246088843, 0.8583430071542251, 0.864592584109299, 0.8703963119248365, 0.8758359418469246, 0.8807454875720235, 0.8853757033150981, 0.8896056765017223, 0.8934660586946656, 0.8970961876385526, 0.900530678353518, 0.9037100396029298, 0.9065226005535818, 0.9092027149329781, 0.9117706028446728, 0.9141839724639984, 0.9164607089154576, 0.9184771475408, 0.9205035308452749, 0.9223180112728829, 0.9241160931743876, 0.9257458484345144, 0.927289466225799, 0.92874618760225, 0.9300780910827042, 0.9314256497877412, 0.9326152011341793, 0.9337392757685926, 0.9349229679026506, 0.9359697256816454, 0.9369699364029788, 0.9379166650771882, 0.9388406202529781, 0.9396839138016929, 0.9405382090967266, 0.941355902987257, 0.9422149882780017, 0.9430044771301571, 0.9437009580577836, 0.9443743659664755, 0.9449431586057084, 0.9455365603358077, 0.9462194314167064, 0.9468293650918962, 0.9473620293579004, 0.9479622628377619, 0.9485419644505804, 0.9491172103735548, 0.949625595060175, 0.950097128219809, 0.9506400185551278, 0.9510589014414478, 0.9515289019915167, 0.9518984240639597, 0.9522984592934534, 0.9527328597130836, 0.9532122478431503, 0.9536250599209153, 0.9540221674278087, 0.9544097911185365, 0.9547679530354296, 0.9551182005463478, 0.9555311156049928, 0.9559073534065736, 0.9562203547422728, 0.9565695213086971, 0.9568837351696602, 0.9571153743707174, 0.9573866286695261, 0.9576447444801298, 0.9579653322667976, 0.9582887024581227, 0.9585705071327146, 0.9588938858041332, 0.9591267978881717, 0.9593619593518924, 0.9596689357704314, 0.9598987476197449, 0.9602101739317368, 0.9604997582077676, 0.9607534446585855, 0.9609886658131127, 0.9612166408938538, 0.9614288299617681, 0.9617035054800339, 0.9619344374048064, 0.9621539379299681, 0.9624141953228331, 0.9626809790597451, 0.962925734720585, 0.9631414005665405, 0.9633146095374335, 0.9635797075076473, 0.9637323372725249, 0.9638743543585337, 0.9640858750930845, 0.9643180964327517, 0.9645061692991664, 0.964703336664654, 0.9648505243102502, 0.9650342185627339, 0.9652622863590076, 0.9653791917208919, 0.965526295273978, 0.9657283347895851, 0.9659427584857835, 0.9661496546564003, 0.9663080315218788, 0.9664412340567526, 0.966589054172672, 0.9667523192115234, 0.9669294486321945, 0.9671679201703225, 0.9673523176201139, 0.9674392202654022, 0.9676197752425995, 0.9677520477875532, 0.967938486344669, 0.9680458632258444, 0.9682005950903216, 0.9683398177195324, 0.9685256080036884, 0.9686486774808666, 0.9687874138936787, 0.9689168188163726, 0.9690519204860919, 0.9691270450614676, 0.9692410880578587, 0.9693623279450869, 0.9694691186947827, 0.9695675915671372, 0.9696561811034374, 0.9697893901087267, 0.9698930382206392, 0.9700118260606276, 0.9701350832559215, 0.9702274135412098, 0.9703732898158264, 0.970502217265353, 0.9706368531604032, 0.9707347739778531, 0.970846190250472, 0.9709348391517815, 0.9710495003951027, 0.9711293719283591, 0.971257131977356, 0.971330227294063, 0.9714680926921946, 0.9715944606505037, 0.971708191812982, 0.9718059356104121, 0.9719310713602328, 0.9720042020541283, 0.9721327626476726, 0.972185724212824, 0.9722682668536032, 0.9723727821648284, 0.9725086625746836, 0.9726239794971248, 0.9726998629416075, 0.9728146970666512, 0.9729087471839526, 0.9729933922895238, 0.973097474670252, 0.9731911127640888, 0.9732660864533037, 0.9733312736736064, 0.9734154827599648, 0.9734703806472205, 0.9734988624064649, 0.9735268211385943, 0.9735938366760823, 0.9736797272967262, 0.9737686545993534, 0.9738556646181463, 0.9739664896695746, 0.9740429807277647, 0.9741258096218117, 0.9741747429407306, 0.974230444720624, 0.9743246820522715, 0.9744048453531352, 0.9745188810513027, 0.9745633844594153, 0.9746173884195738, 0.9746985801158685, 0.9747088736246766, 0.9747808447028236, 0.9748549192683938, 0.9749285618238357, 0.9750181276606473, 0.9750824608721111, 0.9751264459183903, 0.9751823085017081, 0.9752511860171704, 0.9752643476560865, 0.9753435863977684, 0.9753870355283866, 0.9754586557804574, 0.9754975373704166, 0.9756208504073228, 0.9757178812476813, 0.9756843373147276, 0.9757633937202981, 0.9758066426995972, 0.9758316519369279, 0.9759076026243259, 0.9759503816060794, 0.9760446862610861, 0.9760946832184492, 0.9761606789169992, 0.9762153526504378, 0.976301725342666, 0.9763423304823566, 0.9763811281592503, 0.9764625490446449, 0.9764986615093665, 0.9766009532407204, 0.9766906546013105, 0.9766993062127465, 0.9767699077297147], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 493778363, "moving_var_accuracy_train": [0.02582331156902256, 0.05561552341287837, 0.0833151631598268, 0.10806713446269371, 0.12584266428248872, 0.13827354172791845, 0.14557516946780327, 0.14907935707275033, 0.14931705178400606, 0.14730917055002263, 0.14338467259072268, 0.1381579845614539, 0.13190358159265997, 0.12511279564808891, 0.11797161630503003, 0.11061578836758318, 0.10331328614463818, 0.09610425325930484, 0.08914283423226417, 0.0824465665728511, 0.07610099504196416, 0.07008435735955332, 0.06440043272264627, 0.05908173106197365, 0.05409832276265575, 0.04949630212717404, 0.04523313408441932, 0.04125832972051945, 0.037614594153171706, 0.0342768418717561, 0.031200672593637017, 0.0283837546432849, 0.025811685342159902, 0.02344744956098545, 0.02129565468533364, 0.019327123275236315, 0.01752853390379303, 0.015894281038756945, 0.014411014473121897, 0.01306088806739807, 0.011825993752568455, 0.010708041495091433, 0.009696583780525545, 0.008779344578748346, 0.007948061880698137, 0.007189849915196278, 0.006507820987346539, 0.005886669941611437, 0.005327100834170959, 0.004818295670625061, 0.004357910906532689, 0.003941218150396907, 0.0035630620372884315, 0.0032230990637312795, 0.002913524449010467, 0.002633543898163005, 0.002382799651960783, 0.0021543810033956804, 0.0019479466964397458, 0.0017612186834389026, 0.001592780053596832, 0.0014399023443208842, 0.001302480493948847, 0.0011782500542414588, 0.0010670672966482761, 0.0009659702008125494, 0.000873738951874223, 0.0007904463605902003, 0.0007143134501291894, 0.0006460512356358325, 0.0005856429282903998, 0.000530426807254535, 0.00047993770751158144, 0.00043518645883354316, 0.0003946922985893281, 0.0003582012395774839, 0.0003247072105260444, 0.00029423758115915014, 0.00026746639228887863, 0.00024229891891205677, 0.00022005713167443723, 0.0001992803375651967, 0.00018079255747220142, 0.0001644116352461561, 0.00015003878853478, 0.00013656863398524052, 0.0001243310199349958, 0.00011325018707201726, 0.00010307968799322853, 9.387577906404458e-05, 8.602269076854262e-05, 7.869441564173372e-05, 7.170670260290585e-05, 6.563328796259177e-05, 5.9958532320124494e-05, 5.444558956331e-05, 4.966324065857862e-05, 4.529653053787313e-05, 4.169186624473033e-05, 3.846379414599564e-05, 3.5332139602992936e-05, 3.274008952884895e-05, 2.9954312925984538e-05, 2.7456589859559295e-05, 2.555904156745506e-05, 2.347845878547333e-05, 2.200349003713397e-05, 2.0557872509739293e-05, 1.9081296596722723e-05, 1.767112786088442e-05, 1.637176881174639e-05, 1.513980973545304e-05, 1.4304848524918954e-05, 1.33543296573394e-05, 1.2452521016521687e-05, 1.1816874109738166e-05, 1.1275748759290707e-05, 1.0687321884979838e-05, 1.0037195510487307e-05, 9.303488087818925e-06, 9.005631683340443e-06, 8.31473112114565e-06, 7.664777683497128e-06, 7.3009691054517745e-06, 7.05621295027799e-06, 6.668934282983358e-06, 6.3519155848048066e-06, 5.911701853469834e-06, 5.624223873682617e-06, 5.5299357635885376e-06, 5.099943959965593e-06, 4.78470466194416e-06, 4.673613888550134e-06, 4.620050193117468e-06, 4.5432994025492495e-06, 4.314718545963361e-06, 4.042932929038191e-06, 3.835296716168243e-06, 3.6916663007514308e-06, 3.60487315568219e-06, 3.756203910587758e-06, 3.686605294934808e-06, 3.385913393264354e-06, 3.3407229520544594e-06, 3.1641148921859997e-06, 3.1605374231823903e-06, 2.948251832362777e-06, 2.8689041980878356e-06, 2.7564602426381967e-06, 2.7914764855553847e-06, 2.6486437029161494e-06, 2.5570094627845207e-06, 2.4520192226624975e-06, 2.3710894508448058e-06, 2.1847738221886985e-06, 2.083348685202518e-06, 2.0073058089781394e-06, 1.909213606065683e-06, 1.8055644047668515e-06, 1.6956409177673113e-06, 1.6857785778021636e-06, 1.61388709994916e-06, 1.5794933483164522e-06, 1.5582750392101067e-06, 1.479171469521786e-06, 1.5227733100336651e-06, 1.5200965642032679e-06, 1.5312283259065383e-06, 1.464401871726394e-06, 1.4296839567923615e-06, 1.3574432104433702e-06, 1.3400236958789167e-06, 1.263436482713619e-06, 1.28399650551941e-06, 1.2036831828879415e-06, 1.2543766766168368e-06, 1.2726587569401608e-06, 1.261805877114002e-06, 1.2216099388271397e-06, 1.2403795478932062e-06, 1.1644744786106328e-06, 1.1967774666618522e-06, 1.102344066445372e-06, 1.0534292477221452e-06, 1.0463973754742999e-06, 1.1079290099688603e-06, 1.1168180423837668e-06, 1.0569609124645545e-06, 1.0699467076891948e-06, 1.0425608579997311e-06, 1.0027879172741114e-06, 1.0000074033493859e-06, 9.789194965708564e-07, 9.316170335843053e-07, 8.766996934430219e-07, 8.52850256126637e-07, 7.946892327401743e-07, 7.225212049530378e-07, 6.573043007783056e-07, 6.319936110836285e-07, 6.351890384066005e-07, 6.42842720938756e-07, 6.466951391780689e-07, 6.925653534769639e-07, 6.759667559767745e-07, 6.701157115806704e-07, 6.246543677263411e-07, 5.901131255032831e-07, 6.110278850372366e-07, 6.077604897817236e-07, 6.640217049124298e-07, 6.154445144239417e-07, 5.801479123967644e-07, 5.814619450820553e-07, 5.242693574860962e-07, 5.184609465442101e-07, 5.159982232697307e-07, 5.13207434690881e-07, 5.3408504333582e-07, 5.179253978775412e-07, 4.835450167553615e-07, 4.632761690143281e-07, 4.596455613391906e-07, 4.1524006385587636e-07, 4.3022506112021095e-07, 4.041929975714339e-07, 4.099388423745909e-07, 3.825509604768303e-07, 4.811508100686224e-07, 5.177705848878966e-07, 4.761202853411939e-07, 4.847574941625203e-07, 4.531160126399892e-07, 4.1343356894280536e-07, 4.2400677429461303e-07, 3.9807646838391046e-07, 4.383091331488825e-07, 4.169754815441537e-07, 4.1447682343373175e-07, 3.999320952432949e-07, 4.2708106338381453e-07, 3.9921195336905117e-07, 3.7283809562315607e-07, 3.95218531266872e-07, 3.6743366911457034e-07, 4.2486268693345176e-07, 4.547934250657069e-07, 4.0998773598308256e-07, 4.1385013016873644e-07], "duration": 169958.960272, "accuracy_train": [0.5356544446751569, 0.6533296851928756, 0.7214635748315799, 0.7806295998292728, 0.7985063892926356, 0.8185237576135106, 0.8285655165651532, 0.8404706389581026, 0.8475143978982096, 0.8572549689691769, 0.8627190686715578, 0.8690427524570875, 0.8725301151831857, 0.8783190147425249, 0.8825969280638611, 0.8848984648971022, 0.8893394991232927, 0.8916639269564415, 0.8955930679563492, 0.8981736226467331, 0.9021478246700813, 0.9044729734796051, 0.9060311836701735, 0.9084711479674235, 0.9093783164913253, 0.9128885702173312, 0.9149572316814323, 0.9144235289198044, 0.9173517744670543, 0.9200954500622923, 0.9208387767049648, 0.9226298622646733, 0.924792611145718, 0.9249313990979143, 0.9270476450027685, 0.92767543518134, 0.9282094984311554, 0.9297673481335363, 0.9314410947882059, 0.9323242908476375, 0.9318356491094499, 0.9333237443475452, 0.9348815940499261, 0.9359042990379292, 0.9369513369785898, 0.9366250951688816, 0.9387409805855482, 0.9386483351213547, 0.9402988302879292, 0.9404136457756552, 0.9411820263473607, 0.9418566799903102, 0.9420652224067922, 0.943553678133075, 0.9433211632521227, 0.9438559474783131, 0.9455761971091732, 0.9453905456925988, 0.9459718328949798, 0.9464372231450721, 0.9471562168350868, 0.9472735557401256, 0.9482268667520304, 0.9487151480020304, 0.9499467558947029, 0.950109876799557, 0.9499692864064231, 0.9504350371447029, 0.950062292358804, 0.9508771759067, 0.9523652711447952, 0.9523187681686047, 0.952156007751938, 0.9533643641565154, 0.9537592789659468, 0.9542944236803249, 0.9542010572397563, 0.9543409266565154, 0.9555260315729974, 0.9548288474183279, 0.9557589069421374, 0.9552241227159468, 0.9558987763588963, 0.9566424634897563, 0.9575267410137505, 0.9573403686208011, 0.9575961349898486, 0.9578984043350868, 0.9579914102874677, 0.9582704281446106, 0.9592473511327981, 0.9592934936208011, 0.9590373667635659, 0.9597120204065154, 0.9597116599183279, 0.9592001271802326, 0.959827917358804, 0.9599677867755629, 0.960850622346807, 0.961199034180048, 0.9611067492040422, 0.9618042938468992, 0.9612230066445183, 0.9614784125253784, 0.9624317235372831, 0.9619670542635659, 0.963013010739664, 0.9631060166920451, 0.9630366227159468, 0.9631056562038575, 0.9632684166205242, 0.9633385315729974, 0.964175585144426, 0.9640128247277593, 0.9641294426564231, 0.9647565118586194, 0.9650820326919527, 0.9651285356681433, 0.9650823931801403, 0.9648734902754706, 0.9659655892395718, 0.9651060051564231, 0.9651525081326136, 0.9659895617040422, 0.9664080884897563, 0.9661988250968992, 0.9664778429540422, 0.9661752131206165, 0.9666874668350868, 0.9673148965254706, 0.9664313399778516, 0.9668502272517534, 0.967546690430048, 0.9678725717515688, 0.9680117201919527, 0.967733423311185, 0.9676400568706165, 0.9679194352159468, 0.968221704561185, 0.9685236134182356, 0.9693141640134736, 0.9690118946682356, 0.9682213440729974, 0.9692447700373754, 0.9689425006921374, 0.9696164333587117, 0.9690122551564231, 0.9695931818706165, 0.969592821382429, 0.9701977205610927, 0.9697563027754706, 0.9700360416089886, 0.9700814631206165, 0.9702678355135659, 0.9698031662398486, 0.9702674750253784, 0.9704534869301403, 0.9704302354420451, 0.9704538474183279, 0.9704534869301403, 0.9709882711563308, 0.9708258712278516, 0.9710809166205242, 0.9712443980135659, 0.971058386108804, 0.9716861762873754, 0.9716625643110927, 0.9718485762158545, 0.9716160613349022, 0.9718489367040422, 0.9717326792635659, 0.9720814515849945, 0.971848215727667, 0.9724069724183279, 0.971988085144426, 0.9727088812753784, 0.9727317722752861, 0.9727317722752861, 0.9726856297872831, 0.9730572931086194, 0.9726623782991879, 0.9732898079895718, 0.9726623782991879, 0.9730111506206165, 0.9733134199658545, 0.9737315862633813, 0.9736618317990956, 0.9733828139419527, 0.9738482041920451, 0.973755198239664, 0.973755198239664, 0.974034216096807, 0.9740338556086194, 0.9739408496562385, 0.9739179586563308, 0.9741733645371908, 0.9739644616325213, 0.973755198239664, 0.9737784497277593, 0.9741969765134736, 0.9744527428825213, 0.9745690003229974, 0.9746387547872831, 0.974963915132429, 0.9747314002514765, 0.9748712696682356, 0.9746151428110004, 0.974731760739664, 0.9751728180370985, 0.9751263150609081, 0.9755452023348099, 0.974963915132429, 0.9751034240610004, 0.9754293053825213, 0.9748015152039498, 0.9754285844061462, 0.9755215903585271, 0.9755913448228128, 0.9758242201919527, 0.9756614597752861, 0.9755223113349022, 0.9756850717515688, 0.9758710836563308, 0.9753828024063308, 0.9760567350729051, 0.9757780777039498, 0.9761032380490956, 0.975847471680048, 0.9767306677394795, 0.9765911588109081, 0.9753824419181433, 0.9764749013704319, 0.976195883513289, 0.9760567350729051, 0.9765911588109081, 0.9763353924418604, 0.9768934281561462, 0.9765446558347176, 0.9767546402039498, 0.9767074162513842, 0.9770790795727206, 0.9767077767395718, 0.976730307251292, 0.9771953370131967, 0.9768236736918604, 0.9775215788229051, 0.9774979668466224, 0.9767771707156699, 0.977405321382429], "end": "2016-01-27 16:42:29.704000", "learning_rate_per_epoch": [0.0002929499023593962, 0.0001464749511796981, 9.764996502781287e-05, 7.323747558984905e-05, 5.858997974428348e-05, 4.8824982513906434e-05, 4.1849983972497284e-05, 3.661873779492453e-05, 3.254998955526389e-05, 2.929498987214174e-05, 2.6631809305399656e-05, 2.4412491256953217e-05, 2.2534608433488756e-05, 2.0924991986248642e-05, 1.9529992641764693e-05, 1.8309368897462264e-05, 1.7232347090612166e-05, 1.6274994777631946e-05, 1.541841629659757e-05, 1.464749493607087e-05, 1.3949995263828896e-05, 1.3315904652699828e-05, 1.2736952157865744e-05, 1.2206245628476609e-05, 1.1717996130755637e-05, 1.1267304216744378e-05, 1.0849996215256397e-05, 1.0462495993124321e-05, 1.0101720363309141e-05, 9.764996320882346e-06, 9.4499964689021e-06, 9.154684448731132e-06, 8.877269465301652e-06, 8.616173545306083e-06, 8.369996976398397e-06, 8.137497388815973e-06, 7.917565199022647e-06, 7.709208148298785e-06, 7.5115358413313515e-06, 7.323747468035435e-06, 7.145119525375776e-06, 6.974997631914448e-06, 6.8127883423585445e-06, 6.657952326349914e-06, 6.509997547254898e-06, 6.368476078932872e-06, 6.232976375031285e-06, 6.103122814238304e-06, 5.978569333819905e-06, 5.858998065377818e-06, 5.744115696870722e-06, 5.633652108372189e-06, 5.527356734091882e-06, 5.424998107628198e-06, 5.326361588231521e-06, 5.2312479965621606e-06, 5.139471795700956e-06, 5.050860181654571e-06, 4.965252628608141e-06, 4.882498160441173e-06, 4.802457169716945e-06, 4.72499823445105e-06, 4.649998572858749e-06, 4.577342224365566e-06, 4.506921413849341e-06, 4.438634732650826e-06, 4.372386683826335e-06, 4.308086772653041e-06, 4.245650870871032e-06, 4.184998488199199e-06, 4.126055046071997e-06, 4.0687486944079865e-06, 4.013012130599236e-06, 3.9587825995113235e-06, 3.905998710251879e-06, 3.8546040741493925e-06, 3.804544121521758e-06, 3.7557679206656758e-06, 3.708226586240926e-06, 3.6618737340177177e-06, 3.6166654808766907e-06, 3.572559762687888e-06, 3.529516789058107e-06, 3.487498815957224e-06, 3.4464694635971682e-06, 3.4063941711792722e-06, 3.3672401968942722e-06, 3.328976163174957e-06, 3.2915718293224927e-06, 3.254998773627449e-06, 3.2192297112487722e-06, 3.184238039466436e-06, 3.149998974549817e-06, 3.1164881875156425e-06, 3.083683168370044e-06, 3.051561407119152e-06, 3.0201019853848265e-06, 2.9892846669099526e-06, 2.9590898975584423e-06, 2.929499032688909e-06, 2.9004941097809933e-06, 2.872057848435361e-06, 2.84417387774738e-06, 2.8168260541860946e-06, 2.789998916341574e-06, 2.763678367045941e-06, 2.7378496270102914e-06, 2.712499053814099e-06, 2.6876136871578638e-06, 2.6631807941157604e-06, 2.6391883238829905e-06, 2.6156239982810803e-06, 2.5924769033736084e-06, 2.569735897850478e-06, 2.547390522522619e-06, 2.5254300908272853e-06, 2.503845280443784e-06, 2.4826263143040705e-06, 2.4617638700874522e-06, 2.4412490802205866e-06, 2.4210735318774823e-06, 2.4012285848584725e-06, 2.381706508458592e-06, 2.362499117225525e-06, 2.343599135201657e-06, 2.3249992864293745e-06, 2.306692067577387e-06, 2.288671112182783e-06, 2.270929371661623e-06, 2.2534607069246704e-06, 2.2362587515090127e-06, 2.219317366325413e-06, 2.202630867031985e-06, 2.1861933419131674e-06, 2.1699993340007495e-06, 2.1540433863265207e-06, 2.138320496669621e-06, 2.122825435435516e-06, 2.107553200403345e-06, 2.0924992440995993e-06, 2.077658791677095e-06, 2.0630275230359985e-06, 2.0486006633291254e-06, 2.0343743472039932e-06, 2.020344027187093e-06, 2.006506065299618e-06, 1.99285636881541e-06, 1.9793912997556617e-06, 1.9661067653942155e-06, 1.9529993551259395e-06, 1.940065658345702e-06, 1.9273020370746963e-06, 1.9147053080814658e-06, 1.902272060760879e-06, 1.889999339255155e-06, 1.8778839603328379e-06, 1.8659229681361467e-06, 1.854113293120463e-06, 1.8424522068016813e-06, 1.8309368670088588e-06, 1.8195645452578901e-06, 1.8083327404383454e-06, 1.7972386103792815e-06, 1.786279881343944e-06, 1.7754539385350654e-06, 1.7647583945290535e-06, 1.7541909755891538e-06, 1.743749407978612e-06, 1.7334313042738358e-06, 1.7232347317985841e-06, 1.7131573031292646e-06, 1.7031970855896361e-06, 1.6933520328166196e-06, 1.6836200984471361e-06, 1.673999463491782e-06, 1.6644880815874785e-06, 1.6550841337448219e-06, 1.6457859146612464e-06, 1.6365916053473484e-06, 1.6274993868137244e-06, 1.6185076674446464e-06, 1.6096148556243861e-06, 1.6008191323635401e-06, 1.592119019733218e-06, 1.5835129261176917e-06, 1.5749994872749085e-06, 1.5665769979023025e-06, 1.5582440937578212e-06, 1.549999410599412e-06, 1.541841584185022e-06, 1.5337691365857609e-06, 1.525780703559576e-06, 1.5178751482380903e-06, 1.5100509926924133e-06, 1.5023072137410054e-06, 1.4946423334549763e-06, 1.4870553286527866e-06, 1.4795449487792212e-06, 1.472110056965903e-06, 1.4647495163444546e-06, 1.457462190046499e-06, 1.4502470548904967e-06, 1.4431029740080703e-06, 1.4360289242176805e-06, 1.4290238823377877e-06, 1.42208693887369e-06, 1.4152168432701728e-06, 1.4084130270930473e-06, 1.4016741261002608e-06, 1.394999458170787e-06, 1.3883881138099241e-06, 1.3818391835229704e-06, 1.3753516441283864e-06, 1.3689248135051457e-06, 1.3625576684717089e-06, 1.3562495269070496e-06, 1.349999479316466e-06, 1.3438068435789319e-06, 1.3376707101997454e-06, 1.3315903970578802e-06, 1.3255651083454723e-06, 1.3195941619414953e-06, 1.3136766483512474e-06, 1.3078119991405401e-06, 1.301999532188347e-06, 1.2962384516868042e-06, 1.2905281892017229e-06, 1.284867948925239e-06, 1.2792571624231641e-06, 1.2736952612613095e-06, 1.2681813359449734e-06, 1.2627150454136427e-06, 1.257295707546291e-06, 1.251922640221892e-06, 1.246595275006257e-06, 1.2413131571520353e-06, 1.2360754908513627e-06, 1.2308819350437261e-06, 1.2257318076080992e-06, 1.2206245401102933e-06, 1.2155597914897953e-06, 1.2105367659387412e-06, 1.2055551223966177e-06, 1.2006142924292362e-06, 1.195713821289246e-06, 1.190853254229296e-06, 1.1860320228151977e-06, 1.1812495586127625e-06, 1.1765056342483149e-06], "accuracy_valid": [0.5268069347703314, 0.6269604786332832, 0.6997276214231928, 0.7359427946159638, 0.755627000188253, 0.7623614575489458, 0.7698489269578314, 0.7788821300828314, 0.7803160885730422, 0.7877020778426205, 0.7888007106551205, 0.7918524684676205, 0.7939585490399097, 0.794212984751506, 0.7973971079631024, 0.7992487528237951, 0.7983427852033133, 0.7985766307417168, 0.7994414180158133, 0.8011606974774097, 0.8020048945783133, 0.803002047251506, 0.8005194606551205, 0.7997870387801205, 0.8011504023908133, 0.803368258189006, 0.8027270213667168, 0.8036329889871988, 0.8026152461408133, 0.804588961314006, 0.8034800334149097, 0.8018828242658133, 0.8051684276167168, 0.8033373729292168, 0.806175875376506, 0.8042021602033133, 0.803978609751506, 0.8043139354292168, 0.805199312876506, 0.806786226939006, 0.8056670039533133, 0.8045683711408133, 0.8065317912274097, 0.8048228068524097, 0.8038359492658133, 0.805687594126506, 0.8061655802899097, 0.8055243434676205, 0.8062773555158133, 0.8049345820783133, 0.8069891872176205, 0.8059008494917168, 0.8064097209149097, 0.8075289439006024, 0.8061655802899097, 0.8070406626506024, 0.8063891307417168, 0.8079966349774097, 0.8066538615399097, 0.807030367564006, 0.8071627329631024, 0.8078951548381024, 0.8063994258283133, 0.807030367564006, 0.8063891307417168, 0.8071627329631024, 0.8066332713667168, 0.8071421427899097, 0.8067450465926205, 0.805687594126506, 0.8072950983621988, 0.8058905544051205, 0.8056772990399097, 0.8065317912274097, 0.8074274637612951, 0.8071627329631024, 0.8076304240399097, 0.8072848032756024, 0.8075392389871988, 0.8068877070783133, 0.8081495905496988, 0.8079054499246988, 0.8071421427899097, 0.807884859751506, 0.8060332148908133, 0.808373141001506, 0.8066538615399097, 0.807884859751506, 0.8082407756024097, 0.806908297251506, 0.8072642131024097, 0.807884859751506, 0.8087599421121988, 0.8080275202371988, 0.8088923075112951, 0.8067965220256024, 0.8081598856362951, 0.8068980021649097, 0.806786226939006, 0.8080481104103916, 0.8086378717996988, 0.8073053934487951, 0.8075495340737951, 0.8086584619728916, 0.8082716608621988, 0.8074171686746988, 0.8076510142131024, 0.8065626764871988, 0.8071730280496988, 0.8061861704631024, 0.8092585184487951, 0.8069185923381024, 0.8099909403237951, 0.8074274637612951, 0.8072848032756024, 0.8088820124246988, 0.8079260400978916, 0.8079260400978916, 0.8074274637612951, 0.8077730845256024, 0.8087702371987951, 0.8082613657756024, 0.8090040827371988, 0.8077730845256024, 0.8077833796121988, 0.8074171686746988, 0.8090246729103916, 0.8081392954631024, 0.8089026025978916, 0.8098791650978916, 0.8086378717996988, 0.8080275202371988, 0.8090040827371988, 0.8086378717996988, 0.8098482798381024, 0.8090040827371988, 0.807762789439006, 0.8095232492469879, 0.8076613092996988, 0.8095026590737951, 0.8090246729103916, 0.8077833796121988, 0.8104895166603916, 0.8093702936746988, 0.8101130106362951, 0.8101438958960843, 0.8090143778237951, 0.8084040262612951, 0.8086378717996988, 0.8102556711219879, 0.8096453195594879, 0.8094011789344879, 0.8077936746987951, 0.8088717173381024, 0.8095026590737951, 0.8104792215737951, 0.8096350244728916, 0.8112116434487951, 0.8105909967996988, 0.8103674463478916, 0.8090143778237951, 0.8082819559487951, 0.8094011789344879, 0.8114660791603916, 0.8091261530496988, 0.8104895166603916, 0.8093702936746988, 0.8088717173381024, 0.8093805887612951, 0.8082716608621988, 0.8097467996987951, 0.8087599421121988, 0.8110998682228916, 0.8108557275978916, 0.8104998117469879, 0.8101233057228916, 0.8088820124246988, 0.8106115869728916, 0.8084040262612951, 0.8110998682228916, 0.8102453760353916, 0.8108351374246988, 0.8104895166603916, 0.8097467996987951, 0.8107336572853916, 0.8103674463478916, 0.8095026590737951, 0.8109777979103916, 0.8100012354103916, 0.8081598856362951, 0.8087599421121988, 0.8104895166603916, 0.8106012918862951, 0.8106115869728916, 0.8092688135353916, 0.8110998682228916, 0.8109777979103916, 0.8088820124246988, 0.8123308664344879, 0.8099806452371988, 0.8121985010353916, 0.8090040827371988, 0.8091364481362951, 0.8112219385353916, 0.8104792215737951, 0.8092585184487951, 0.8106115869728916, 0.8099909403237951, 0.8103674463478916, 0.8109675028237951, 0.8104895166603916, 0.8103674463478916, 0.8119646554969879, 0.8114660791603916, 0.8107439523719879, 0.8095129541603916, 0.8114763742469879, 0.8097570947853916, 0.8107336572853916, 0.8103571512612951, 0.8107233621987951, 0.8101233057228916, 0.8108660226844879, 0.8099909403237951, 0.8102350809487951, 0.8107336572853916, 0.8117102197853916, 0.8109777979103916, 0.8106012918862951, 0.8098585749246988, 0.8093805887612951, 0.8112219385353916, 0.8102247858621988, 0.8104792215737951, 0.8099806452371988, 0.8097467996987951, 0.8118219950112951, 0.8110998682228916, 0.8097467996987951, 0.8098688700112951, 0.8113440088478916, 0.8106115869728916, 0.8112219385353916, 0.8107233621987951, 0.8112116434487951, 0.8103468561746988, 0.8117102197853916, 0.8098688700112951, 0.8110998682228916], "accuracy_test": 0.800015943877551, "start": "2016-01-25 17:29:50.743000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0, 211.0, 212.0, 213.0, 214.0, 215.0, 216.0, 217.0, 218.0, 219.0, 220.0, 221.0, 222.0, 223.0, 224.0, 225.0, 226.0, 227.0, 228.0, 229.0, 230.0, 231.0, 232.0, 233.0, 234.0, 235.0, 236.0, 237.0, 238.0, 239.0, 240.0, 241.0, 242.0, 243.0, 244.0, 245.0, 246.0, 247.0, 248.0], "accuracy_train_last": 0.977405321382429, "batch_size_eval": 1024, "accuracy_train_std": [0.017205907358065636, 0.019143786285467094, 0.022974463412840353, 0.021646057436375703, 0.02078824889052495, 0.017265750751181183, 0.019276430005930045, 0.01871462670998041, 0.017463340256452912, 0.017285816197948365, 0.0168753360943426, 0.014813222439788174, 0.015031233489854223, 0.014823361254248843, 0.014640976404195462, 0.014494021761868772, 0.01338508156321041, 0.012258238868944536, 0.012074827578574853, 0.011952236652487649, 0.012610490270243848, 0.011722138641565828, 0.011527106233118716, 0.011982709185378557, 0.012280302934128299, 0.011506677454178355, 0.011181297531238303, 0.011345252865921012, 0.011351410311695714, 0.011050962724788662, 0.01061539193576398, 0.010170778643665362, 0.009797709544584723, 0.010268378747341265, 0.010135828880720775, 0.010612556798127145, 0.00986725694132808, 0.009104778140341567, 0.009659036565686608, 0.009699236973825659, 0.009489659000349858, 0.009255771951279718, 0.00905885168660996, 0.009348547515317659, 0.009134383024999445, 0.008892229039748907, 0.009022635853183511, 0.008664073219167878, 0.008880378215134004, 0.009171011176831199, 0.00809217575426276, 0.008584038992787623, 0.008786358259928191, 0.008941488151476704, 0.008476884198338978, 0.008314892044225187, 0.007916784452359566, 0.008349181977540933, 0.008264613125251296, 0.007810146891323997, 0.008595739307995993, 0.008130021097780871, 0.008158334306283248, 0.007695961111619193, 0.00816661230255215, 0.007907321080651942, 0.00782730887062858, 0.008159922568519877, 0.008066563196068926, 0.008196161161068803, 0.007887120257769104, 0.007286744448206704, 0.007265328754521645, 0.008410777347042892, 0.008148325534880067, 0.008145110850946158, 0.008181313582325296, 0.00800325420534374, 0.008245972562603524, 0.007901974740413086, 0.007769143887117041, 0.007833862951442398, 0.008075663620203719, 0.007389629332923007, 0.007462825990122699, 0.007586022535491067, 0.007512565637421792, 0.00798427880621865, 0.007671753280659953, 0.007502625007780132, 0.0069631205190167945, 0.007225486460581541, 0.007542330021514444, 0.007416068266059659, 0.007215890278003726, 0.0072304210442563155, 0.007406821703770989, 0.007122010520014423, 0.007343414599437597, 0.007038896505822505, 0.007745435172949782, 0.006971155571869082, 0.007110632870758478, 0.006838173758308248, 0.006774046774467765, 0.00652602637379765, 0.006763336751614738, 0.006824526129706585, 0.006964693160827085, 0.0072361111864864104, 0.007020702344306738, 0.006759683511447815, 0.0073859044219401595, 0.006662194638354361, 0.006826974874630543, 0.006646829673007224, 0.0066942043959149315, 0.006408133668903785, 0.0065207825598012185, 0.006214876805991846, 0.0061072262233171255, 0.006463832290621164, 0.006398213730061236, 0.006115841713279757, 0.006608447520431648, 0.006163829640782047, 0.006351195081588238, 0.006234958424393698, 0.005921443591213719, 0.006025861248118544, 0.00611550558387587, 0.006317365674374787, 0.00601671607162933, 0.006074586437254929, 0.0060838263459272466, 0.00608434106219718, 0.005964533407847303, 0.0063350606381248125, 0.006340594381318658, 0.006458049664733507, 0.006141294992281021, 0.006316268800741456, 0.005862681550029112, 0.005658415983561999, 0.006154619291300601, 0.005628016098902318, 0.005499175335479438, 0.00573740032321566, 0.005918710137542541, 0.005735435048306394, 0.006300528010936278, 0.00590049570833264, 0.005777305746988912, 0.005685079018478561, 0.00600616051944115, 0.005731258612478219, 0.005820915587026712, 0.005860695000132331, 0.005724102768943178, 0.005541132003775916, 0.0054547398593938045, 0.005546966501145015, 0.00556682146501777, 0.005583200109825211, 0.0051370094075602986, 0.005470927419037959, 0.005375152550497222, 0.005583914482095759, 0.005395885021871773, 0.005149676734084012, 0.005340456521541089, 0.005250441982203015, 0.005754428034311293, 0.005403497644510342, 0.005340877330693111, 0.005133945245144535, 0.005031284936640918, 0.005182458127664343, 0.00544951014007784, 0.005202503703561926, 0.004921962243759763, 0.005099808771517635, 0.005058468563664006, 0.004865015039957893, 0.005228393291493594, 0.0047773945094741685, 0.004576873423451532, 0.004788496736540788, 0.005054946625001846, 0.004972027001365965, 0.005190981850664363, 0.005374862498634447, 0.004744836484128223, 0.0050258126352796546, 0.00522168301848751, 0.004863504712281315, 0.005109008138746566, 0.005044567524108328, 0.0049964579868391765, 0.004879003319452195, 0.0049377764609392786, 0.004960735698056882, 0.004930971577808889, 0.004601625038412215, 0.0048017570724060476, 0.00491486814985282, 0.004518303189209859, 0.005002696733439171, 0.005198614438291714, 0.004883800329332827, 0.004953257622227179, 0.005225487198027934, 0.005068402077474006, 0.004894992285967422, 0.005047957228054269, 0.004448806254043971, 0.004911897543996771, 0.0051756654611101885, 0.004717457123592739, 0.0047920732620759165, 0.005128108911480723, 0.0049126979572810476, 0.005073795798829484, 0.004809031514273765, 0.0047956012400205885, 0.004811843372859031, 0.004343347339104332, 0.004675122372094195, 0.004762796607887051, 0.004915781585385152, 0.004726297652117767, 0.004889609857440483, 0.005018899816973779, 0.004786122138842152, 0.004850683015577043, 0.004719631663690169, 0.004804147752762696, 0.004813140775972331, 0.00513922249216444, 0.004846260392693729, 0.004384336194589954, 0.004815295527900615, 0.004864340381491986, 0.004802334705317606, 0.0046328782187259904, 0.005003582162415134, 0.004675480748213541, 0.004600847458012207, 0.00482388334592194], "accuracy_test_std": 0.012248723087878562, "error_valid": [0.47319306522966864, 0.3730395213667168, 0.3002723785768072, 0.2640572053840362, 0.24437299981174698, 0.2376385424510542, 0.23015107304216864, 0.22111786991716864, 0.21968391142695776, 0.21229792215737953, 0.21119928934487953, 0.20814753153237953, 0.2060414509600903, 0.20578701524849397, 0.20260289203689763, 0.20075124717620485, 0.20165721479668675, 0.2014233692582832, 0.20055858198418675, 0.1988393025225903, 0.19799510542168675, 0.19699795274849397, 0.19948053934487953, 0.20021296121987953, 0.19884959760918675, 0.19663174181099397, 0.1972729786332832, 0.19636701101280118, 0.19738475385918675, 0.19541103868599397, 0.1965199665850903, 0.19811717573418675, 0.1948315723832832, 0.1966626270707832, 0.19382412462349397, 0.19579783979668675, 0.19602139024849397, 0.1956860645707832, 0.19480068712349397, 0.19321377306099397, 0.19433299604668675, 0.19543162885918675, 0.1934682087725903, 0.1951771931475903, 0.19616405073418675, 0.19431240587349397, 0.1938344197100903, 0.19447565653237953, 0.19372264448418675, 0.19506541792168675, 0.19301081278237953, 0.1940991505082832, 0.1935902790850903, 0.19247105609939763, 0.1938344197100903, 0.19295933734939763, 0.1936108692582832, 0.1920033650225903, 0.1933461384600903, 0.19296963243599397, 0.19283726703689763, 0.19210484516189763, 0.19360057417168675, 0.19296963243599397, 0.1936108692582832, 0.19283726703689763, 0.1933667286332832, 0.1928578572100903, 0.19325495340737953, 0.19431240587349397, 0.19270490163780118, 0.19410944559487953, 0.1943227009600903, 0.1934682087725903, 0.19257253623870485, 0.19283726703689763, 0.1923695759600903, 0.19271519672439763, 0.19246076101280118, 0.19311229292168675, 0.19185040945030118, 0.19209455007530118, 0.1928578572100903, 0.19211514024849397, 0.19396678510918675, 0.19162685899849397, 0.1933461384600903, 0.19211514024849397, 0.1917592243975903, 0.19309170274849397, 0.1927357868975903, 0.19211514024849397, 0.19124005788780118, 0.19197247976280118, 0.19110769248870485, 0.19320347797439763, 0.19184011436370485, 0.1931019978350903, 0.19321377306099397, 0.1919518895896084, 0.19136212820030118, 0.19269460655120485, 0.19245046592620485, 0.1913415380271084, 0.19172833913780118, 0.19258283132530118, 0.19234898578689763, 0.19343732351280118, 0.19282697195030118, 0.19381382953689763, 0.19074148155120485, 0.19308140766189763, 0.19000905967620485, 0.19257253623870485, 0.19271519672439763, 0.19111798757530118, 0.1920739599021084, 0.1920739599021084, 0.19257253623870485, 0.19222691547439763, 0.19122976280120485, 0.19173863422439763, 0.19099591726280118, 0.19222691547439763, 0.19221662038780118, 0.19258283132530118, 0.1909753270896084, 0.19186070453689763, 0.1910973974021084, 0.1901208349021084, 0.19136212820030118, 0.19197247976280118, 0.19099591726280118, 0.19136212820030118, 0.19015172016189763, 0.19099591726280118, 0.19223721056099397, 0.19047675075301207, 0.19233869070030118, 0.19049734092620485, 0.1909753270896084, 0.19221662038780118, 0.1895104833396084, 0.19062970632530118, 0.18988698936370485, 0.18985610410391573, 0.19098562217620485, 0.19159597373870485, 0.19136212820030118, 0.18974432887801207, 0.19035468044051207, 0.19059882106551207, 0.19220632530120485, 0.19112828266189763, 0.19049734092620485, 0.18952077842620485, 0.1903649755271084, 0.18878835655120485, 0.18940900320030118, 0.1896325536521084, 0.19098562217620485, 0.19171804405120485, 0.19059882106551207, 0.1885339208396084, 0.19087384695030118, 0.1895104833396084, 0.19062970632530118, 0.19112828266189763, 0.19061941123870485, 0.19172833913780118, 0.19025320030120485, 0.19124005788780118, 0.1889001317771084, 0.1891442724021084, 0.18950018825301207, 0.1898766942771084, 0.19111798757530118, 0.1893884130271084, 0.19159597373870485, 0.1889001317771084, 0.1897546239646084, 0.18916486257530118, 0.1895104833396084, 0.19025320030120485, 0.1892663427146084, 0.1896325536521084, 0.19049734092620485, 0.1890222020896084, 0.1899987645896084, 0.19184011436370485, 0.19124005788780118, 0.1895104833396084, 0.18939870811370485, 0.1893884130271084, 0.1907311864646084, 0.1889001317771084, 0.1890222020896084, 0.19111798757530118, 0.18766913356551207, 0.19001935476280118, 0.1878014989646084, 0.19099591726280118, 0.19086355186370485, 0.1887780614646084, 0.18952077842620485, 0.19074148155120485, 0.1893884130271084, 0.19000905967620485, 0.1896325536521084, 0.18903249717620485, 0.1895104833396084, 0.1896325536521084, 0.18803534450301207, 0.1885339208396084, 0.18925604762801207, 0.1904870458396084, 0.18852362575301207, 0.1902429052146084, 0.1892663427146084, 0.18964284873870485, 0.18927663780120485, 0.1898766942771084, 0.18913397731551207, 0.19000905967620485, 0.18976491905120485, 0.1892663427146084, 0.1882897802146084, 0.1890222020896084, 0.18939870811370485, 0.19014142507530118, 0.19061941123870485, 0.1887780614646084, 0.18977521413780118, 0.18952077842620485, 0.19001935476280118, 0.19025320030120485, 0.18817800498870485, 0.1889001317771084, 0.19025320030120485, 0.19013112998870485, 0.1886559911521084, 0.1893884130271084, 0.1887780614646084, 0.18927663780120485, 0.18878835655120485, 0.18965314382530118, 0.1882897802146084, 0.19013112998870485, 0.1889001317771084], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "momentum": 0.6219169065500622, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.00029294989792773106, "patience_threshold": 1, "do_flip": true, "batch_size": 64, "l2_decay": 4.4601359701026697e-07, "optimization": "rmsprop", "nb_data_augmentation": 1, "learning_rate_decay_method": "lin", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.03273781296245478}, "accuracy_valid_max": 0.8123308664344879, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        #nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        nb_data_augmentation=make_constant_param(1),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8110998682228916, "loss_train": [1.5148186683654785, 1.1242468357086182, 0.8899714350700378, 0.7819060683250427, 0.7198225855827332, 0.6725554466247559, 0.6371915936470032, 0.6134281158447266, 0.5917009711265564, 0.5732913017272949, 0.5575436949729919, 0.5431097149848938, 0.5312774181365967, 0.5194652676582336, 0.5081515908241272, 0.5004071593284607, 0.49100303649902344, 0.4821820557117462, 0.47811126708984375, 0.47227951884269714, 0.46902185678482056, 0.45922112464904785, 0.45781219005584717, 0.45167630910873413, 0.44714489579200745, 0.44144299626350403, 0.43976712226867676, 0.43604835867881775, 0.43048518896102905, 0.426843523979187, 0.42222514748573303, 0.4218192994594574, 0.4186398684978485, 0.4143168330192566, 0.4129595458507538, 0.40874022245407104, 0.40455254912376404, 0.40424245595932007, 0.40206918120384216, 0.39887097477912903, 0.3953823447227478, 0.397007018327713, 0.39222651720046997, 0.3900860548019409, 0.390260249376297, 0.3850582242012024, 0.3859427273273468, 0.38269296288490295, 0.379824697971344, 0.37707823514938354, 0.3805736005306244, 0.37540024518966675, 0.3755323588848114, 0.3746340870857239, 0.3724752962589264, 0.3714078664779663, 0.3710170090198517, 0.36706042289733887, 0.3682440221309662, 0.3675522208213806, 0.3622995615005493, 0.36383968591690063, 0.36069008708000183, 0.3585278391838074, 0.35874783992767334, 0.35618191957473755, 0.3531622290611267, 0.35307687520980835, 0.35441702604293823, 0.3535059690475464, 0.3506125509738922, 0.3515707850456238, 0.3503672182559967, 0.3497737944126129, 0.3464919328689575, 0.34766483306884766, 0.3483780026435852, 0.34372472763061523, 0.3450726866722107, 0.3412898778915405, 0.3433534502983093, 0.34291496872901917, 0.34241005778312683, 0.33941131830215454, 0.338800311088562, 0.3371638357639313, 0.3370282053947449, 0.33691585063934326, 0.33612683415412903, 0.3368801474571228, 0.3350396752357483, 0.33289554715156555, 0.3332306146621704, 0.33450213074684143, 0.3307829797267914, 0.33118557929992676, 0.330363929271698, 0.33170050382614136, 0.32833218574523926, 0.33061569929122925, 0.33016785979270935, 0.32727620005607605, 0.32702746987342834, 0.3253123462200165, 0.3255622088909149, 0.32441750168800354, 0.3246724307537079, 0.3226149082183838, 0.32344287633895874, 0.3221226632595062, 0.32189586758613586, 0.3198670446872711, 0.3203727602958679, 0.31998610496520996, 0.3212376534938812, 0.32070600986480713, 0.31947383284568787, 0.31859874725341797, 0.31883034110069275, 0.31657058000564575, 0.31799694895744324, 0.3190169334411621, 0.3173956573009491, 0.31583455204963684, 0.31643322110176086, 0.3105424642562866, 0.3153991401195526, 0.3151603937149048, 0.3142325282096863, 0.31444063782691956, 0.3121439814567566, 0.31000930070877075, 0.31017982959747314, 0.3132380545139313, 0.3101232647895813, 0.3113210201263428, 0.3109075725078583, 0.31288692355155945, 0.3120037019252777, 0.3095054626464844, 0.30969110131263733, 0.30891841650009155, 0.3086923360824585, 0.3068373501300812, 0.3055891692638397, 0.30737465620040894, 0.3054578900337219, 0.3057645559310913, 0.3051410913467407, 0.3065062165260315, 0.30665650963783264, 0.30295464396476746, 0.3058907687664032, 0.3025013208389282, 0.3018152117729187, 0.30339163541793823, 0.3023194670677185, 0.3044976592063904, 0.30261483788490295, 0.301567405462265, 0.3015594184398651, 0.30161601305007935, 0.30164778232574463, 0.2997579276561737, 0.29993313550949097, 0.3025483787059784, 0.30043554306030273, 0.2973153293132782, 0.29631680250167847, 0.29849421977996826, 0.297174334526062, 0.2947714328765869, 0.29863888025283813, 0.29651662707328796, 0.29537448287010193, 0.29722562432289124, 0.2959357500076294, 0.2953070402145386, 0.2958024740219116, 0.29793187975883484, 0.2952108383178711, 0.29418647289276123, 0.2937646508216858, 0.29416269063949585, 0.29641449451446533, 0.29332235455513, 0.2932077646255493, 0.29377442598342896, 0.29036998748779297, 0.2945432662963867, 0.29470527172088623, 0.2908628582954407, 0.29005149006843567, 0.2923685908317566, 0.29274603724479675, 0.2896822690963745, 0.2913776934146881, 0.29352518916130066, 0.28741350769996643, 0.29122304916381836, 0.2903313934803009, 0.2888719141483307, 0.2892984449863434, 0.29192444682121277, 0.28954046964645386, 0.2888369560241699, 0.2894119918346405, 0.2894081473350525, 0.28857022523880005, 0.28871938586235046, 0.2871871888637543, 0.289305180311203, 0.286027729511261, 0.28708913922309875, 0.28795623779296875, 0.2870274484157562, 0.2885233759880066, 0.28416651487350464, 0.2878659963607788, 0.28389567136764526, 0.28880101442337036, 0.2850084602832794, 0.28474393486976624, 0.2847803831100464, 0.2847346365451813, 0.28562942147254944, 0.28683730959892273, 0.28462129831314087, 0.2836032509803772, 0.2850753664970398, 0.2840503752231598, 0.2840662896633148, 0.28374141454696655, 0.28314951062202454, 0.2867010235786438, 0.2844041883945465, 0.2828710079193115, 0.28099849820137024, 0.28194376826286316, 0.28588932752609253, 0.28089430928230286, 0.2805788516998291, 0.2817520797252655, 0.2815670073032379, 0.2846306264400482, 0.2807384729385376, 0.2815624177455902, 0.28030961751937866, 0.2817525863647461], "accuracy_train_first": 0.5356544446751569, "model": "residualv5", "loss_std": [0.2522216737270355, 0.14945197105407715, 0.12957458198070526, 0.1207367330789566, 0.11662125587463379, 0.11394374072551727, 0.1091463565826416, 0.10807859152555466, 0.106293685734272, 0.10631602257490158, 0.1050943061709404, 0.10323845595121384, 0.10134977847337723, 0.10040955990552902, 0.09851539134979248, 0.09664687514305115, 0.09668970853090286, 0.0954943373799324, 0.09414491057395935, 0.09566207230091095, 0.09357229620218277, 0.09341973066329956, 0.0931786596775055, 0.09105006605386734, 0.09216693043708801, 0.08786929398775101, 0.09211423248052597, 0.0890219658613205, 0.0897730141878128, 0.09021139144897461, 0.08889074623584747, 0.08835555613040924, 0.08827903121709824, 0.0876438245177269, 0.08591551333665848, 0.08604498952627182, 0.08561044931411743, 0.08553308993577957, 0.08455315977334976, 0.08530666679143906, 0.08590611070394516, 0.0864163190126419, 0.08376599103212357, 0.08019548654556274, 0.08516397327184677, 0.0825815200805664, 0.08209272474050522, 0.08212543278932571, 0.08177294582128525, 0.0827213004231453, 0.08226227015256882, 0.08201461285352707, 0.08190609514713287, 0.08069498091936111, 0.080367811024189, 0.07947025448083878, 0.08115648478269577, 0.07932981848716736, 0.0799555629491806, 0.08109647780656815, 0.07921144366264343, 0.08124268800020218, 0.07894477993249893, 0.07636608183383942, 0.07967724651098251, 0.07846327871084213, 0.07549262046813965, 0.07842599600553513, 0.07845824211835861, 0.0762183740735054, 0.07798459380865097, 0.07721398025751114, 0.07547450810670853, 0.07635168731212616, 0.07773913443088531, 0.07439825683832169, 0.08061075955629349, 0.07582995295524597, 0.07640396058559418, 0.07568299025297165, 0.07624002546072006, 0.07539007812738419, 0.0757957175374031, 0.07647091150283813, 0.0754818245768547, 0.07693071663379669, 0.07419001311063766, 0.075792096555233, 0.07531409710645676, 0.07702216506004333, 0.0760764330625534, 0.07327118515968323, 0.07157661765813828, 0.07222817093133926, 0.07430250942707062, 0.07367048412561417, 0.07607033103704453, 0.0734608918428421, 0.07452690601348877, 0.07470786571502686, 0.07439224421977997, 0.07077666372060776, 0.07333141565322876, 0.07229999452829361, 0.07579629868268967, 0.07300041615962982, 0.0736413300037384, 0.07580548524856567, 0.07290498912334442, 0.07228429615497589, 0.07475009560585022, 0.07201538234949112, 0.07116837054491043, 0.07064861804246902, 0.07419600337743759, 0.07472362369298935, 0.07334521412849426, 0.07256050407886505, 0.07581868767738342, 0.07326247543096542, 0.06999185681343079, 0.07132641226053238, 0.07245642691850662, 0.07130466401576996, 0.0712234154343605, 0.06913948804140091, 0.0753803700208664, 0.07076343148946762, 0.07302112877368927, 0.0734926164150238, 0.07221458107233047, 0.07154113054275513, 0.06965672224760056, 0.0714435800909996, 0.07382435351610184, 0.07295133173465729, 0.07196551561355591, 0.07106652855873108, 0.07037688791751862, 0.07107169926166534, 0.070219986140728, 0.06974715739488602, 0.06953615695238113, 0.07034798711538315, 0.06899042427539825, 0.07102951407432556, 0.07090312242507935, 0.06812320649623871, 0.06996528804302216, 0.07189207524061203, 0.07419492304325104, 0.06897833943367004, 0.06999243050813675, 0.07036291062831879, 0.0701695904135704, 0.06895224004983902, 0.06974299252033234, 0.07132061570882797, 0.06954638659954071, 0.07051186263561249, 0.07065635919570923, 0.06922587752342224, 0.06922594457864761, 0.0705382376909256, 0.0692739188671112, 0.07051976770162582, 0.07095256447792053, 0.0704718753695488, 0.06896454095840454, 0.06865048408508301, 0.06906262785196304, 0.07030646502971649, 0.06962788850069046, 0.06801194697618484, 0.0709826648235321, 0.06874088943004608, 0.06810872256755829, 0.06825606524944305, 0.06937599927186966, 0.0690392404794693, 0.07135643810033798, 0.07115701586008072, 0.06798111647367477, 0.06860107183456421, 0.07102099806070328, 0.06893453001976013, 0.06493271887302399, 0.0679108202457428, 0.06860212236642838, 0.0715867429971695, 0.06806326657533646, 0.0683060809969902, 0.06942638754844666, 0.06897681206464767, 0.06772619485855103, 0.07009376585483551, 0.06911401450634003, 0.07117293030023575, 0.06664247065782547, 0.06774357706308365, 0.06713826954364777, 0.07037016749382019, 0.06796220690011978, 0.06830832362174988, 0.06961580365896225, 0.07099784165620804, 0.06759417057037354, 0.0679749920964241, 0.06882493197917938, 0.06844254583120346, 0.06805501133203506, 0.06911491602659225, 0.06842954456806183, 0.06734616309404373, 0.06986173242330551, 0.06859280914068222, 0.07042503356933594, 0.06691096723079681, 0.06959734857082367, 0.06612339615821838, 0.06746767461299896, 0.06813470274209976, 0.06908650696277618, 0.06729159504175186, 0.06783710420131683, 0.06688761711120605, 0.06732914596796036, 0.06678524613380432, 0.0682111606001854, 0.06870446354150772, 0.06839916110038757, 0.06812238693237305, 0.06784329563379288, 0.06587294489145279, 0.0695222020149231, 0.06747874617576599, 0.06852889060974121, 0.06876744329929352, 0.0639190673828125, 0.06711708754301071, 0.06389667093753815, 0.06502912938594818, 0.06758926063776016, 0.06639837473630905, 0.0684783011674881, 0.06681302934885025, 0.06632742285728455, 0.06644657999277115, 0.06672494858503342]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:18 2016", "state": "available"}], "summary": "f5b12185ddcad7759a6af999da20e6dd"}