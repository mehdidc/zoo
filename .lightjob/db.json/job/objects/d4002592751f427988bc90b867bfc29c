{"content": {"hp_model": {"f0": 16, "f1": 32, "f2": 32, "f3": 32, "nonlin": "rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [2.03535532951355, 1.730421781539917, 1.6564157009124756, 1.6081594228744507, 1.5697827339172363, 1.5374126434326172, 1.5096653699874878, 1.4824769496917725, 1.460170030593872, 1.4384185075759888, 1.4180783033370972, 1.3998757600784302, 1.3842447996139526, 1.3708293437957764, 1.3571943044662476, 1.3456685543060303, 1.334258794784546, 1.3260830640792847, 1.3165146112442017, 1.3068276643753052, 1.29982590675354, 1.2939635515213013, 1.2878659963607788, 1.281233310699463, 1.2757093906402588, 1.2721853256225586, 1.2670793533325195, 1.2632886171340942, 1.2584772109985352, 1.253834843635559, 1.2521309852600098, 1.2490713596343994, 1.246059775352478, 1.2436965703964233, 1.2415955066680908, 1.240525484085083, 1.2382266521453857, 1.2353243827819824, 1.2336441278457642, 1.2316864728927612, 1.2287393808364868, 1.2288587093353271, 1.22892165184021, 1.2263811826705933, 1.2260102033615112, 1.2265450954437256, 1.2259448766708374, 1.223807692527771, 1.222743272781372, 1.2222743034362793, 1.2201673984527588, 1.2209219932556152, 1.219626545906067, 1.2191940546035767, 1.2192013263702393, 1.217910885810852, 1.2172046899795532, 1.2172529697418213, 1.2153127193450928, 1.2169773578643799, 1.215977668762207, 1.2156583070755005, 1.2159347534179688, 1.2157981395721436, 1.2157514095306396, 1.2153658866882324, 1.2145028114318848, 1.2137959003448486, 1.2136788368225098, 1.2142168283462524, 1.2145252227783203, 1.2149103879928589, 1.2130122184753418, 1.2128057479858398, 1.2135292291641235, 1.212458610534668, 1.2133915424346924, 1.2127183675765991, 1.21246337890625, 1.2134050130844116, 1.2132014036178589, 1.2119392156600952, 1.213536024093628, 1.2138721942901611, 1.2134947776794434, 1.213427186012268, 1.2120188474655151, 1.2122437953948975, 1.2130485773086548, 1.213862419128418, 1.212188482284546, 1.2121849060058594, 1.2129204273223877, 1.2123265266418457, 1.2106983661651611, 1.213423252105713, 1.2121903896331787, 1.211783766746521, 1.212278962135315, 1.2116450071334839, 1.2115187644958496, 1.2123080492019653, 1.2120000123977661, 1.2130571603775024, 1.2124563455581665, 1.2119020223617554, 1.2112513780593872, 1.2115832567214966, 1.212709665298462, 1.211220145225525, 1.2122695446014404, 1.2128431797027588, 1.2121859788894653, 1.2131321430206299, 1.2135783433914185, 1.212717056274414, 1.210811734199524, 1.212156057357788, 1.2127842903137207, 1.2114174365997314, 1.212432861328125, 1.2113021612167358, 1.2104893922805786, 1.2126022577285767, 1.2129040956497192, 1.211470603942871, 1.212844729423523, 1.212936520576477, 1.211492657661438, 1.2120332717895508, 1.2123932838439941, 1.2144180536270142, 1.2116512060165405, 1.2121822834014893, 1.2118679285049438, 1.2137280702590942, 1.213598370552063, 1.2123825550079346, 1.2122849225997925, 1.2132545709609985, 1.212989091873169, 1.212525725364685, 1.2134208679199219, 1.2136183977127075, 1.2127869129180908, 1.2118510007858276, 1.212258219718933, 1.2126041650772095, 1.213423490524292, 1.2118566036224365, 1.2129966020584106, 1.2112038135528564, 1.2126275300979614, 1.2121959924697876, 1.2135300636291504, 1.2125483751296997, 1.2112983465194702, 1.2117327451705933, 1.2116341590881348, 1.211127758026123, 1.212178349494934, 1.2129162549972534, 1.2134482860565186, 1.2117284536361694, 1.2120856046676636, 1.2122039794921875, 1.212433099746704, 1.211504578590393, 1.2128167152404785, 1.2117093801498413, 1.213420033454895, 1.213506817817688, 1.212423324584961, 1.2117704153060913], "moving_avg_accuracy_train": [0.03558506872346806, 0.07186807682435861, 0.10667543932695411, 0.1399317227982823, 0.1714967772914792, 0.20109089988648224, 0.22904822335759778, 0.25535832984705026, 0.2800299758386741, 0.3032132367334889, 0.32507784133164236, 0.34523249678065787, 0.36430160201330636, 0.3820822502572046, 0.3986797833814011, 0.4140128745396157, 0.4282102930772562, 0.4414504580813154, 0.4536270232516353, 0.4647208265846943, 0.4751492448606103, 0.4848416328053355, 0.49392496372815187, 0.5022232665432289, 0.509910230967256, 0.5170424126393565, 0.5236612668442285, 0.5296833758440987, 0.5352427828725533, 0.5403299185064866, 0.5450431631591602, 0.5494524940608522, 0.5535626538521183, 0.5574686638106681, 0.5608887416721724, 0.564015675921345, 0.5668555294313239, 0.5695439310724477, 0.5721192414708786, 0.5744254671830561, 0.5765150572656917, 0.5784561782579303, 0.5802148128949925, 0.5818439788980828, 0.5833708263651867, 0.584770565722485, 0.5860442099392731, 0.5873067111260398, 0.5884453954893956, 0.5894795120116538, 0.5904381186674006, 0.5912102199028201, 0.5920375723992028, 0.5927775753971471, 0.5935156577083922, 0.5941868351373039, 0.594728151854286, 0.5952734295709892, 0.5958222721874413, 0.5962092736970103, 0.5966344131127927, 0.5970588912655685, 0.5973805758804753, 0.5977467498469682, 0.5980506216334508, 0.5983496828781899, 0.5986373670912938, 0.5989289791616956, 0.5991588779417238, 0.5993773404901594, 0.599648397594475, 0.599885301444293, 0.6000242543424993, 0.6002213194663427, 0.6004196044170875, 0.6006026390727395, 0.6007836102556741, 0.6008931140441525, 0.600987017156164, 0.6010714939081556, 0.6011777859682906, 0.6012176452509835, 0.6013580421553795, 0.6014030552098213, 0.6015132853742858, 0.6016055891735128, 0.6016398344678171, 0.6017149051577095, 0.6017010525214606, 0.6017304738762266, 0.6017638203954884, 0.6017450762354616, 0.6018048643045142, 0.6018307717809472, 0.6018611000049843, 0.601879166909017, 0.6019325574059613, 0.6019643688603633, 0.6020928363728597, 0.6021062226841248, 0.6021252818595109, 0.6021842156982922, 0.6021513337936992, 0.6021821218509756, 0.6021841823679821, 0.6022418764535353, 0.602128859760332, 0.6021782073114307, 0.6021947183217051, 0.6022072530821426, 0.6022161371200894, 0.6022334693982981, 0.6022281060605816, 0.602218736905474, 0.6022103767635146, 0.602207394786914, 0.602188543112763, 0.6021970450964756, 0.6022023356841888, 0.602200121766702, 0.6021400005207258, 0.6021696328053088, 0.6021498349340618, 0.6020715629808919, 0.6020731617873155, 0.6021142003404961, 0.6021277754038072, 0.602165569597692, 0.6021671043864925, 0.6021451981594989, 0.6020696789837761, 0.602117933117283, 0.6021474830422195, 0.6021717888746717, 0.6021378245036313, 0.6021398086530282, 0.6021625207267711, 0.6022735702990737, 0.6022828341105746, 0.6022261034230775, 0.6022029475900444, 0.6022146594236479, 0.6022088879834057, 0.6021758279502921, 0.6021949380943087, 0.6021980781846102, 0.6021824472706804, 0.6021126119255337, 0.6020381704196728, 0.602075588467914, 0.6020953498672927, 0.6020666321505432, 0.602043147403097, 0.6020452626184906, 0.6020449132611727, 0.6020561164371779, 0.6020639822932294, 0.6020244864898477, 0.6020377683918043, 0.6020195312178601, 0.6019775050755869, 0.6019886178189972, 0.6019822711487622, 0.6020811187443419, 0.6021003631648966, 0.6021874015588627, 0.6022122216419946, 0.6021950682358701, 0.6022563600810723, 0.6022535742656099, 0.6022416582899994, 0.6021449394548164, 0.6021647772507522, 0.6021965821599516], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.03518507624246987, 0.07084246164344879, 0.10507857504235692, 0.1378352306687688, 0.16933346941476843, 0.1990593668727645, 0.22685233125853022, 0.25294330648169827, 0.27721261123450736, 0.29977828888176444, 0.320910417401344, 0.340489826997806, 0.3586361979783718, 0.375431799048381, 0.39115010257727784, 0.40566381619944464, 0.4189916246209158, 0.4314027207713995, 0.4427802268380849, 0.45352458861399025, 0.4636074942574858, 0.47282962322029143, 0.48132588129547615, 0.48911487990350383, 0.4964953076228673, 0.5032740289313637, 0.5094582683104413, 0.5151614496213399, 0.5203828505458776, 0.524821645687073, 0.5291360031439681, 0.5331399656590141, 0.5368422176812151, 0.540259693719946, 0.5433598362173038, 0.5463086558711758, 0.5488893513721607, 0.551248598416797, 0.5533708912483101, 0.5553074278764911, 0.5570869319356041, 0.5587861418388057, 0.5602889576718679, 0.5618032718538528, 0.5631295335238893, 0.5643821451658527, 0.5655369982320988, 0.566586514005651, 0.567592113358098, 0.56852259634646, 0.569346794496076, 0.5700265081658207, 0.5707369362272506, 0.5713885285137876, 0.5718152406567613, 0.5724078306253472, 0.5729055700119842, 0.5732660272238881, 0.5737013315045114, 0.574042218214753, 0.5744243174587897, 0.5748058573808325, 0.5751115927082613, 0.5754101390567876, 0.5756066180916208, 0.57580992230279, 0.5759918665841827, 0.5761912080225264, 0.5764092954281052, 0.5766411656782163, 0.5768376418720663, 0.5770256479691217, 0.5772558886127216, 0.5773308868655308, 0.5775947273017188, 0.5778077696317878, 0.5780361288226, 0.5781419368270118, 0.5781384782723227, 0.5782717019341718, 0.5783783666899263, 0.578426566353765, 0.5785075966536295, 0.5786415590797576, 0.5787377112007728, 0.5788476326635268, 0.5788600832525959, 0.5788723182914176, 0.5788324426840379, 0.5786724253075769, 0.5786647460298312, 0.5785601784298602, 0.5786481435499766, 0.5787059866215603, 0.5787570158773259, 0.578754114082515, 0.5788145966407545, 0.5788690309431701, 0.5788905192268652, 0.5788142614495101, 0.5788056350974808, 0.5788589065369045, 0.5787816919939068, 0.5788607422975282, 0.5789339465881067, 0.5790120374808774, 0.5789571604458921, 0.5789443922081553, 0.5789105457490115, 0.578828167284803, 0.5788405053944251, 0.5788048405854043, 0.5787706832399663, 0.578800976785322, 0.5788669210872115, 0.5788774428339121, 0.5789479475621926, 0.5790734664825546, 0.5790358311012419, 0.5790416688777893, 0.5791089875415917, 0.5790942731341946, 0.5791197102786065, 0.5792779105609869, 0.5792483628689695, 0.579145439232675, 0.5791148726249196, 0.5790639781240993, 0.5789408128512225, 0.5789907145292027, 0.5791098977355444, 0.5791083288486616, 0.5792157506230575, 0.5792778681435831, 0.5790977222836675, 0.5790597203395629, 0.5790377256211185, 0.579129852673088, 0.5791527613722702, 0.5790604273943052, 0.5789508537343175, 0.5788940060773768, 0.5789607954639916, 0.578852066491764, 0.5788274526042593, 0.5788541282305051, 0.578842544709036, 0.5788687406334636, 0.5788201042866082, 0.5789716440744384, 0.578950367985895, 0.5789515155340675, 0.5788925426798325, 0.578876088204771, 0.5788246580834656, 0.5788638201930407, 0.5788014098416583, 0.5788042166643449, 0.5787457076485129, 0.5787927648015833, 0.5787343714633677, 0.578769325695043, 0.57875401539587, 0.5788368628679547, 0.5788005328029213, 0.5788299004093009, 0.5787911780641539, 0.5787980965905698, 0.578863299403275, 0.5788141776707788, 0.5788299737591226, 0.5787210904174723, 0.5786332434239179, 0.5786936060167671], "moving_var_accuracy_train": [0.011396674044485505, 0.022105116731680557, 0.030798577417996333, 0.03767254318912474, 0.04287246285663866, 0.04646752540048743, 0.04885528028145597, 0.050199747584687336, 0.050657983869642695, 0.050429357754131855, 0.04968897038682103, 0.04837596457455572, 0.04681104508646455, 0.044975303645577266, 0.04295707623329874, 0.04077730177016395, 0.03851367183134399, 0.036240022372222014, 0.033950438789123245, 0.03166304716174426, 0.029475509615206952, 0.027373440110325754, 0.025378658205173713, 0.02346054885115278, 0.021646298764543822, 0.01993948102672406, 0.018339816002919822, 0.016832226573884706, 0.015427166975068515, 0.01411736081818397, 0.012905556812769179, 0.0117899809224978, 0.010763023551835695, 0.009824033420818732, 0.008946902471945623, 0.008140211684938897, 0.007398773428068259, 0.006723943615717407, 0.006111239266980057, 0.005547983433601631, 0.005032482570662513, 0.004563145869954843, 0.0041346664450394345, 0.0037450874373261174, 0.0033915600622837195, 0.003070037488470675, 0.002777633265942226, 0.0025142151225672885, 0.0022744630290247184, 0.002056641298956715, 0.0018592475095450215, 0.0016786880214501448, 0.0015169798286845683, 0.0013702102857488096, 0.001238092146657484, 0.0011183372442614592, 0.0010091407339280715, 0.0009109026106302616, 0.0008225234035259424, 0.0007416189946890256, 0.0006690837869257904, 0.0006037970435528664, 0.0005443486681207893, 0.0004911205516723446, 0.00044283953906869115, 0.00039936052381476606, 0.00036016933129151215, 0.0003249177365587972, 0.000292901643944444, 0.00026404101251562034, 0.00023829815884825764, 0.0002149734538699588, 0.00019364987965424243, 0.00017463440365613717, 0.0001575248155857504, 0.00014207384919370181, 0.00012816121939580644, 0.0001154530171734458, 0.00010398707560611018, 9.365259494014256e-05, 8.438901746455794e-05, 7.596441457985328e-05, 6.854537473874358e-05, 6.17090728405008e-05, 5.564752175887162e-05, 5.015944950515012e-05, 4.5154059216273014e-05, 4.068937377097392e-05, 3.662216345365595e-05, 3.296773765333681e-05, 2.968097180112503e-05, 2.6716036712828547e-05, 2.4076604560355016e-05, 2.1674984880335672e-05, 1.9515764602861282e-05, 1.7567125859767117e-05, 1.5836068180266102e-05, 1.4261569079920038e-05, 1.2983947287831013e-05, 1.16871652990115e-05, 1.0521718038607884e-05, 9.500805010928591e-06, 8.560455486682688e-06, 7.712941078252132e-06, 6.941685181999927e-06, 6.277474131370331e-06, 5.7646816747167655e-06, 5.210130134439926e-06, 4.691570642138472e-06, 4.223827659897645e-06, 3.8021552290800233e-06, 3.4246433769832024e-06, 3.0824379278080313e-06, 2.774984164634106e-06, 2.498114775932922e-06, 2.24838332799965e-06, 2.026743465764337e-06, 1.82471967273135e-06, 1.642499618323363e-06, 1.478293769366772e-06, 1.362995470389693e-06, 1.2345985739572038e-06, 1.1146663179147022e-06, 1.0583381740005701e-06, 9.525273622383327e-07, 8.724320916389074e-07, 7.868474235701024e-07, 7.210182910356315e-07, 6.489376621220243e-07, 5.88362840939699e-07, 5.808548699624921e-07, 5.43725535570713e-07, 4.972117645874286e-07, 4.528075495493894e-07, 4.179090010960126e-07, 3.761535326258754e-07, 3.431807240066339e-07, 4.1985071918324425e-07, 3.7863801109663634e-07, 3.6973954812202333e-07, 3.375913267409366e-07, 3.050666974840528e-07, 2.748598134378752e-07, 2.5721052419931056e-07, 2.3477625021838028e-07, 2.1138736670045698e-07, 1.9244755926294702e-07, 2.1709558222243074e-07, 2.45259864153672e-07, 2.3333487074599494e-07, 2.1351599982006108e-07, 1.9958676513584034e-07, 1.8459188888583008e-07, 1.661729672226956e-07, 1.4955676895524596e-07, 1.3573069243316974e-07, 1.2271446841265786e-07, 1.2448228793424002e-07, 1.1362173941706101e-07, 1.0525291609659046e-07, 1.1062339419629728e-07, 1.0067249237159694e-07, 9.096776514207121e-08, 1.6980861299479473e-07, 1.561608811977027e-07, 2.0872593129592674e-07, 1.9339766690636488e-07, 1.7670605429078493e-07, 1.9284566145632584e-07, 1.7363094222080532e-07, 1.575457622714654e-07, 2.2598198375679742e-07, 2.0692562870945006e-07, 1.9533703608115425e-07], "duration": 181032.396399, "accuracy_train": [0.35585068723468066, 0.3984151497323736, 0.41994170185031376, 0.43923827404023624, 0.45558226773025107, 0.46743800324150975, 0.4806641345976375, 0.49214928825212256, 0.502074789763289, 0.5118625847868218, 0.521859282715024, 0.5266243958217978, 0.5359235491071429, 0.5421080844522886, 0.5480575814991694, 0.5520106949635475, 0.5559870599160207, 0.5606119431178479, 0.5632161097845146, 0.564565056582226, 0.5690050093438538, 0.5720731243078626, 0.5756749420334994, 0.5769079918789222, 0.5790929107834994, 0.5812320476882613, 0.5832309546880767, 0.583882356842931, 0.5852774461286453, 0.5861141392118863, 0.5874623650332226, 0.5891364721760797, 0.5905540919735143, 0.5926227534376154, 0.5916694424257106, 0.5921580841638981, 0.5924142110211333, 0.5937395458425618, 0.5952970350567552, 0.5951814985926541, 0.595321368009413, 0.5959262671880767, 0.596042524628553, 0.5965064729258952, 0.5971124535691215, 0.597368219938169, 0.5975070078903654, 0.5986692218069398, 0.5986935547595976, 0.5987865607119786, 0.5990655785691215, 0.5981591310215947, 0.5994837448666482, 0.5994376023786453, 0.6001583985095976, 0.6002274319975083, 0.5996000023071244, 0.6001809290213178, 0.6007618557355112, 0.5996922872831303, 0.6004606678548358, 0.6008791946405501, 0.6002757374146365, 0.6010423155454042, 0.600785467711794, 0.6010412340808416, 0.6012265250092286, 0.6015534877953119, 0.6012279669619786, 0.6013435034260797, 0.6020879115333149, 0.6020174360926541, 0.6012748304263565, 0.6019949055809339, 0.602204168973791, 0.6022499509736065, 0.6024123509020857, 0.6018786481404577, 0.6018321451642672, 0.6018317846760797, 0.6021344145095053, 0.6015763787952196, 0.6026216142949428, 0.6018081726997969, 0.6025053568544666, 0.602436323366556, 0.601948042116556, 0.6023905413667405, 0.6015763787952196, 0.6019952660691215, 0.6020639390688446, 0.6015763787952196, 0.6023429569259874, 0.6020639390688446, 0.6021340540213178, 0.6020417690453119, 0.6024130718784606, 0.6022506719499815, 0.6032490439853267, 0.6022266994855112, 0.6022968144379844, 0.6027146202473238, 0.6018553966523624, 0.6024592143664635, 0.602202727021041, 0.6027611232235143, 0.6011117095215024, 0.6026223352713178, 0.6023433174141749, 0.6023200659260797, 0.6022960934616095, 0.6023894599021778, 0.6021798360211333, 0.6021344145095053, 0.6021351354858804, 0.6021805569975083, 0.6020188780454042, 0.6022735629498892, 0.6022499509736065, 0.6021801965093208, 0.6015989093069398, 0.602436323366556, 0.6019716540928387, 0.6013671154023624, 0.6020875510451273, 0.6024835473191215, 0.6022499509736065, 0.6025057173426541, 0.6021809174856958, 0.601948042116556, 0.6013900064022701, 0.6025522203188446, 0.6024134323666482, 0.6023905413667405, 0.6018321451642672, 0.6021576659976006, 0.6023669293904577, 0.6032730164497969, 0.6023662084140826, 0.6017155272356035, 0.6019945450927464, 0.6023200659260797, 0.6021569450212255, 0.6018782876522701, 0.6023669293904577, 0.6022263389973238, 0.6020417690453119, 0.6014840938192137, 0.6013681968669251, 0.6024123509020857, 0.6022732024617017, 0.6018081726997969, 0.6018317846760797, 0.6020642995570321, 0.6020417690453119, 0.6021569450212255, 0.6021347749976929, 0.601669024259413, 0.602157305509413, 0.6018553966523624, 0.6015992697951273, 0.6020886325096899, 0.6019251511166482, 0.6029707471045589, 0.6022735629498892, 0.6029707471045589, 0.6024356023901809, 0.6020406875807494, 0.6028079866878922, 0.6022285019264488, 0.6021344145095053, 0.601274469938169, 0.6023433174141749, 0.6024828263427464], "end": "2016-02-05 23:56:46.835000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0], "moving_var_accuracy_valid": [0.011141906311695765, 0.02147075788323169, 0.02987268524087459, 0.03654240310723321, 0.04181741419340982, 0.0455883335912185, 0.047981540056232626, 0.049310036943473086, 0.049680025627788275, 0.049294911334128715, 0.0483845219026281, 0.04699624923367954, 0.04526024132819047, 0.04327304713309747, 0.0411693280122265, 0.038948226158960285, 0.03665207783893921, 0.03437318782395426, 0.03210089784023599, 0.02992977984595622, 0.027851786737301307, 0.025832037027030744, 0.02389851093584894, 0.022054676336106763, 0.020339445122382824, 0.01871906017334891, 0.017191357506293616, 0.015764958249249107, 0.014433829672857064, 0.013167772826320855, 0.012018518666081567, 0.010960952241870449, 0.009988217048006428, 0.009094507625483484, 0.008271554814470452, 0.007522659169182965, 0.006830333155683898, 0.0061973942596741365, 0.005618191975470949, 0.005090124344934435, 0.004609611622708586, 0.004174636289093974, 0.0037774987590374984, 0.0034203872101095938, 0.003094179219255304, 0.0027988826206600155, 0.002530997529035575, 0.0022878111263624308, 0.002068131084244963, 0.0018691101631451443, 0.0016883128701391048, 0.0015236396791807362, 0.001375818083536867, 0.001242057427754051, 0.0011194904342552963, 0.0010107018566675849, 0.0009118613714739142, 0.0008218445989410453, 0.000741365547397501, 0.0006682748264007253, 0.0006027613422512938, 0.0005437953624351756, 0.0004902570930056, 0.0004420335530050054, 0.000398177633804666, 0.0003587318638447118, 0.0003231566109540241, 0.0002911985829399904, 0.0002625067836942397, 0.00023673997964079524, 0.00021341340772946393, 0.00019239018358928765, 0.00017362826201604644, 0.00015631605845576165, 0.0001413109585920958, 0.00012758834604249765, 0.00011529884271850345, 0.00010386971645083134, 9.348285246015304e-05, 8.429430411082599e-05, 7.59672700308249e-05, 6.839145189608989e-05, 6.161139989194603e-05, 5.561177328727837e-05, 5.013380303193205e-05, 4.522916728050476e-05, 4.070764570696777e-05, 3.663822840184572e-05, 3.298871613823626e-05, 2.9920294571337847e-05, 2.6928795855964315e-05, 2.4334325717041284e-05, 2.1970533906550977e-05, 1.9803592904267976e-05, 1.784666947833715e-05, 1.6062078314221556e-05, 1.4488793741460118e-05, 1.306658220682928e-05, 1.1764079703171776e-05, 1.0640008970318994e-05, 9.5766777988311e-06, 8.644550635272344e-06, 7.833754342598178e-06, 7.1066194628620326e-06, 6.444187330007753e-06, 5.854652284810438e-06, 5.296290457048419e-06, 4.768128662397707e-06, 4.301626041327123e-06, 3.932539339482382e-06, 3.5406554660755696e-06, 3.198037726890443e-06, 2.888734472427745e-06, 2.6081203151969785e-06, 2.386446142242496e-06, 2.1487978924009173e-06, 1.9786563535499537e-06, 1.922585712514689e-06, 1.743074938602186e-06, 1.5690741614571338e-06, 1.4529529677766814e-06, 1.3096062950644497e-06, 1.184469100400474e-06, 1.2912681544673296e-06, 1.169998933952573e-06, 1.1483385147301342e-06, 1.0419135208442035e-06, 9.61034420683487e-07, 1.0014581386004645e-06, 9.237239219275422e-07, 9.591932597999524e-07, 8.632960864744152e-07, 8.80821416356219e-07, 8.274665519268042e-07, 1.0367926743364962e-06, 9.461107367044827e-07, 8.558535717890052e-07, 8.466547579513735e-07, 7.667125586402065e-07, 7.667713741577091e-07, 7.981517194099476e-07, 7.474214523658713e-07, 7.12826706608629e-07, 7.479419405625497e-07, 6.786003376291496e-07, 6.171446051866614e-07, 5.566377463946391e-07, 5.071500098647873e-07, 4.777244569972506e-07, 6.366307769579818e-07, 5.770417467554771e-07, 5.193494238812042e-07, 4.987146593227652e-07, 4.512799411364555e-07, 4.299574634202224e-07, 4.007647545155635e-07, 3.957437467010665e-07, 3.5624027631330517e-07, 3.5142599308471927e-07, 3.362127746720626e-07, 3.3327953473646374e-07, 3.1094776607089635e-07, 2.8196263681070074e-07, 3.155397058070291e-07, 2.958645978543208e-07, 2.740402448090639e-07, 2.601310004512764e-07, 2.3454869447606086e-07, 2.493564860903172e-07, 2.4613733891211466e-07, 2.2376925268358964e-07, 3.0809256621547607e-07, 3.4673715808294994e-07, 3.4485622581393767e-07], "accuracy_test": 0.5867904974489796, "start": "2016-02-03 21:39:34.439000", "learning_rate_per_epoch": [0.0002739981864579022, 0.0002538655826356262, 0.00023521226830780506, 0.00021792954066768289, 0.00020191670046187937, 0.00018708044080995023, 0.00017333430878352374, 0.00016059819608926773, 0.00014879790251143277, 0.00013786465569864959, 0.0001277347473660484, 0.00011834916222142056, 0.00010965320689138025, 0.00010159620433114469, 9.413120278622955e-05, 8.721471385797486e-05, 8.080642874119803e-05, 7.486899994546548e-05, 6.936783756827936e-05, 6.427088374039158e-05, 5.9548441640799865e-05, 5.517299359780736e-05, 5.111904101795517e-05, 4.736295886687003e-05, 4.388286470202729e-05, 4.0658476791577414e-05, 3.767100861296058e-05, 3.490304879960604e-05, 3.233847382944077e-05, 2.9962335247546434e-05, 2.7760788725572638e-05, 2.5721004931256175e-05, 2.3831098587834276e-05, 2.2080057533457875e-05, 2.045767723757308e-05, 1.8954506231239066e-05, 1.7561782442498952e-05, 1.6271393178612925e-05, 1.5075817827892024e-05, 1.3968089660920668e-05, 1.2941753993800376e-05, 1.1990830898866989e-05, 1.1109778824902605e-05, 1.0293463674315717e-05, 9.537129699310753e-06, 8.836368579068221e-06, 8.187097591871861e-06, 7.585533239762299e-06, 7.02816987541155e-06, 6.511760147986934e-06, 6.033294539520284e-06, 5.589985448750667e-06, 5.179249455977697e-06, 4.798692771146307e-06, 4.446098500920925e-06, 4.1194116420228966e-06, 3.816729076788761e-06, 3.5362866128707537e-06, 3.2764503430371406e-06, 3.0357061859831447e-06, 2.8126512461312814e-06, 2.6059856281790417e-06, 2.4145051611412782e-06, 2.237094122392591e-06, 2.0727188712044153e-06, 1.9204214822821086e-06, 1.7793144024835783e-06, 1.6485754485984216e-06, 1.527442805127066e-06, 1.4152107041809359e-06, 1.3112251053826185e-06, 1.2148800578870578e-06, 1.1256141760895844e-06, 1.0429072290207841e-06, 9.66277411862393e-07, 8.952781058724213e-07, 8.294956046484003e-07, 7.685466130169516e-07, 7.120759732970328e-07, 6.597546757802775e-07, 6.112778123679163e-07, 5.663628712682112e-07, 5.247481453807268e-07, 4.86191140680603e-07, 4.5046721197650186e-07, 4.173681702468457e-07, 3.8670114577143977e-07, 3.5828745126309514e-07, 3.3196150184267026e-07, 3.0756990554436925e-07, 2.849705538210401e-07, 2.640317404711823e-07, 2.446314510962111e-07, 2.266566241360124e-07, 2.1000253980218986e-07, 1.9457215216789336e-07, 1.8027554915533983e-07, 1.6702941252333403e-07, 1.547565631199177e-07, 1.4338549192416394e-07, 1.3284993372053577e-07, 1.2308849761666352e-07, 1.1404430466654958e-07, 1.0566465391548263e-07, 9.790071686666124e-08, 9.070725326409956e-08, 8.404234108638775e-08, 7.786714917301651e-08, 7.214569563984696e-08, 6.684463471628987e-08, 6.193307910962176e-08, 5.7382411711159875e-08, 5.316611506600566e-08, 4.9259622159070204e-08, 4.564016720109976e-08, 4.228665773098328e-08, 3.917955382348737e-08, 3.6300750849704855e-08, 3.3633476448358124e-08, 3.1162183944388744e-08, 2.8872475965613376e-08, 2.6751008519454444e-08, 2.4785419938666564e-08, 2.296425805070612e-08, 2.12769091234577e-08, 1.9713542798172057e-08, 1.826504814061991e-08, 1.692298390310043e-08, 1.5679530562806576e-08, 1.4527443248368854e-08, 1.3460008219112751e-08, 1.2471005561565107e-08, 1.1554671885960488e-08, 1.0705667463639656e-08, 9.919045140804883e-09, 9.190221916810515e-09, 8.514950522453546e-09, 7.889296327334705e-09, 7.309613359041123e-09, 6.77252387504268e-09, 6.274897934588353e-09, 5.81383652331624e-09, 5.38665245741754e-09, 4.990856616871042e-09, 4.624142846409995e-09, 4.284374188756601e-09, 3.969570450124138e-09, 3.67789776412053e-09, 3.4076563792950765e-09, 3.1572715553096486e-09, 2.9252842370652843e-09, 2.7103426170072e-09, 2.511194363563618e-09, 2.3266790716292007e-09, 2.155721379182296e-09, 1.99732519412521e-09, 1.8505674770352698e-09, 1.7145930231166062e-09, 1.5886095772188469e-09, 1.4718830598781096e-09, 1.3637332374472066e-09, 1.2635300583596631e-09, 1.1706894342822238e-09, 1.0846704645572913e-09, 1.0049719945115498e-09, 9.311295068314962e-10, 8.627127900950882e-10, 7.993231077030316e-10, 7.40591143966185e-10, 6.861746171260563e-10, 6.357564474868127e-10, 5.890428700361383e-10, 5.457616580883951e-10, 5.056606244835393e-10], "accuracy_train_first": 0.35585068723468066, "accuracy_train_last": 0.6024828263427464, "batch_size_eval": 1024, "accuracy_train_std": [0.01866931351807962, 0.016303964682938557, 0.016137541675647203, 0.01783832517909965, 0.01860294487496842, 0.018610489809687054, 0.01750997116544641, 0.017062226619744516, 0.01644042624540918, 0.015841288187381244, 0.0152784881569389, 0.016531488655950125, 0.015790568152306752, 0.0157550161752682, 0.01606763056014841, 0.01703186735863523, 0.01652393338399519, 0.015968091504095363, 0.015614659881957246, 0.016207148713895496, 0.01663456108163184, 0.01667229821077322, 0.01632335152065596, 0.017222820610338214, 0.01681809432609591, 0.017698362909653535, 0.017347045659255465, 0.01706930289984354, 0.01700988453064144, 0.01762057979616199, 0.016893225825338417, 0.01642276576745785, 0.015946881139826282, 0.016337508536712727, 0.01581801267044914, 0.015465860193627256, 0.016040041972610924, 0.016793554495030324, 0.015013813851848691, 0.015808962915271937, 0.016236881432773062, 0.016009055740441362, 0.015890981763877474, 0.015791774276642648, 0.016047186573818678, 0.016063005169772045, 0.016223668952432248, 0.015688232240010046, 0.01610765948516604, 0.015818277196781368, 0.015874728571280073, 0.016092975425578148, 0.015774530878353305, 0.015843031596932344, 0.015612777590116691, 0.015451494764395638, 0.01537188367385318, 0.015428285350011128, 0.015538754524330067, 0.015164687464269203, 0.015681243235458282, 0.01567610461874219, 0.01646499784461954, 0.016475178698629675, 0.015857560839257573, 0.015493709048670685, 0.015666068661119067, 0.01590014722048147, 0.0156330502470807, 0.01556014163761791, 0.01609807419252029, 0.015370612765409906, 0.016349468195626816, 0.015917876438592857, 0.01566560240274345, 0.0150755035324116, 0.015296578848975714, 0.015491960565648721, 0.015919325890002087, 0.01555862289902307, 0.01545159900132384, 0.015406268493729086, 0.014857882007209387, 0.014883869474282786, 0.014640231832125924, 0.01528889462791824, 0.015162191029706618, 0.015500848395708389, 0.015280477694514606, 0.01582912448716042, 0.015372120498427979, 0.01582074055682549, 0.015182056817912064, 0.015344029471659503, 0.015454514911363541, 0.015635818889510412, 0.015510630115203476, 0.015382780926877681, 0.015145999470511621, 0.014852737387233604, 0.01525460857480406, 0.015024250028264992, 0.015620135926117634, 0.015671035740322102, 0.015001552821796488, 0.015209892118046197, 0.015768256333922523, 0.015364569180932857, 0.015339882506998347, 0.01572477961643704, 0.01536848375609378, 0.015245270555618433, 0.015068522729383806, 0.015574550040477475, 0.015768479817171523, 0.015399735076811458, 0.016256480158114834, 0.01543107701600589, 0.015382649240916632, 0.015109361813270052, 0.01520206027477855, 0.015297803022646857, 0.01550297075746508, 0.01521892857640569, 0.015407608363279972, 0.01605409273829825, 0.0153323792523453, 0.01503223808749071, 0.015681748457501678, 0.01558607024917446, 0.015239296754618012, 0.015544665558222929, 0.015799283260876616, 0.015773837731246812, 0.0158793378651908, 0.01533788368855739, 0.015423583711557776, 0.015428309379626482, 0.015314126946423121, 0.015287109611947076, 0.015526462670751279, 0.015458269745154974, 0.015748255813678436, 0.015384563611376849, 0.015603613082161768, 0.015226949678185076, 0.0158793463322489, 0.0156964382915981, 0.01608318743512081, 0.015162392583005828, 0.014781619770537359, 0.015202359249611077, 0.015225222308622175, 0.015584946112657271, 0.015794754484987475, 0.015420428725191266, 0.01579597127668634, 0.015551100553513046, 0.015492525866106978, 0.015526822287206293, 0.015193224020073084, 0.01637753548863405, 0.015527543352026876, 0.015430161444671306, 0.015508869404191394, 0.01509542788741903, 0.015375964202981061, 0.015183369221062365, 0.015400718154591535, 0.015882250546091395, 0.015483894829041053, 0.015648493272248917, 0.01538717691243154, 0.015269817351652485], "accuracy_test_std": 0.010105359270223307, "error_valid": [0.6481492375753012, 0.608241069747741, 0.5867964043674698, 0.5673548686935241, 0.5471823818712349, 0.5334075560052711, 0.5230109892695783, 0.5122379165097892, 0.5043636459902108, 0.49713061229292166, 0.48890042592243976, 0.4832954866340362, 0.4780464631965362, 0.4734077913215362, 0.46738516566265065, 0.4637127612010542, 0.4610580995858433, 0.456897413874247, 0.454822218561747, 0.4497761554028614, 0.4456463549510542, 0.44417121611445776, 0.4422077960278614, 0.440784132624247, 0.4370808429028614, 0.43571747929216864, 0.4348835772778614, 0.4335099185805723, 0.4326245411332832, 0.43522919804216864, 0.43203477974397586, 0.4308243717055723, 0.42983751411897586, 0.42898302193147586, 0.42873888130647586, 0.42715196724397586, 0.42788438911897586, 0.42751817818147586, 0.4275284732680723, 0.4272637424698795, 0.4268975315323795, 0.4259209690323795, 0.4261856998305723, 0.4245679005082832, 0.4249341114457832, 0.42434435005647586, 0.42406932417168675, 0.4239678440323795, 0.4233574924698795, 0.4231030567582832, 0.4232354221573795, 0.42385606880647586, 0.4228692112198795, 0.4227471409073795, 0.42434435005647586, 0.4222588596573795, 0.4226147755082832, 0.42348985786897586, 0.4223809299698795, 0.4228898013930723, 0.4221367893448795, 0.4217602833207832, 0.4221367893448795, 0.42190294380647586, 0.4226250705948795, 0.42236033979668675, 0.4223706348832832, 0.4220147190323795, 0.42162791792168675, 0.4212720020707832, 0.4213940723832832, 0.4212822971573795, 0.4206719455948795, 0.42199412885918675, 0.4200307087725903, 0.4202748493975903, 0.4199086384600903, 0.4209057911332832, 0.4218926487198795, 0.42052928510918675, 0.4206616505082832, 0.42113963667168675, 0.4207631306475903, 0.4201527790850903, 0.4203969197100903, 0.42016307417168675, 0.4210278614457832, 0.42101756635918675, 0.4215264377823795, 0.4227677310805723, 0.4214043674698795, 0.4223809299698795, 0.42056017036897586, 0.42077342573418675, 0.4207837208207832, 0.4212720020707832, 0.4206410603350903, 0.4206410603350903, 0.4209160862198795, 0.42187205854668675, 0.4212720020707832, 0.4206616505082832, 0.4219132388930723, 0.4204278049698795, 0.42040721479668675, 0.42028514448418675, 0.42153673286897586, 0.42117052193147586, 0.4213940723832832, 0.4219132388930723, 0.42104845161897586, 0.4215161426957832, 0.42153673286897586, 0.42092638130647586, 0.4205395801957832, 0.4210278614457832, 0.4204175098832832, 0.41979686323418675, 0.4213028873305723, 0.4209057911332832, 0.42028514448418675, 0.4210381565323795, 0.42065135542168675, 0.4192982868975903, 0.42101756635918675, 0.42178087349397586, 0.4211602268448795, 0.4213940723832832, 0.42216767460466864, 0.42056017036897586, 0.4198174534073795, 0.4209057911332832, 0.4198174534073795, 0.42016307417168675, 0.4225235904555723, 0.4212822971573795, 0.4211602268448795, 0.42004100385918675, 0.4206410603350903, 0.4217705784073795, 0.4220353092055723, 0.4216176228350903, 0.42043810005647586, 0.4221264942582832, 0.4213940723832832, 0.4209057911332832, 0.42126170698418675, 0.42089549604668675, 0.4216176228350903, 0.4196644978350903, 0.42124111681099397, 0.4210381565323795, 0.4216382130082832, 0.4212720020707832, 0.4216382130082832, 0.4207837208207832, 0.4217602833207832, 0.42117052193147586, 0.42178087349397586, 0.4207837208207832, 0.4217911685805723, 0.4209160862198795, 0.42138377729668675, 0.4204175098832832, 0.4215264377823795, 0.4209057911332832, 0.42155732304216864, 0.42113963667168675, 0.4205498752823795, 0.42162791792168675, 0.4210278614457832, 0.4222588596573795, 0.4221573795180723, 0.4207631306475903], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.07347714748112423, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 32, "valid_ratio": 0.15, "learning_rate": 0.0002957274028821407, "optimization": "nesterov_momentum", "nb_data_augmentation": 4, "learning_rate_decay_method": "exp", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 8.930332071522352e-08, "rotation_range": [0, 0], "momentum": 0.7897165110094908}, "accuracy_valid_max": 0.5807017131024097, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n\n    # rescaling to [-1, 1]\n    X_min = X_train.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X_train.max(axis=(0, 2, 3))[None, :, None, None]\n    def preprocess(a):\n        return (a / 255.) * 2 - 1\n        # return 2 * ((a - X_min) / (X_max - X_min)) - 1\n    X_train = preprocess(X_train)\n    X_valid = preprocess(X_valid)\n\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = preprocess(X_test)\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.5792368693524097, "accuracy_valid_std": [0.013212594430147698, 0.012574631292862652, 0.013436500781487682, 0.010972089174333347, 0.008291648417796822, 0.013118376493628955, 0.012220001838429606, 0.012286290194346966, 0.014700782688668774, 0.013708474491759682, 0.013933601861366045, 0.01334781829421882, 0.011216442151178884, 0.012363965666495077, 0.01168504314726587, 0.012580428269617487, 0.012103101766204235, 0.011900887371375934, 0.011411078017183503, 0.01182297089591244, 0.012024187592210158, 0.011926983480012917, 0.011641483594511424, 0.01408379203924986, 0.012924725691990817, 0.013795480010914046, 0.013712589624142754, 0.013596674334553778, 0.012399662774525474, 0.01385324460046127, 0.013523003309355066, 0.013311916636200415, 0.013047567760073016, 0.013161394634974294, 0.012370528630395247, 0.014094031179245556, 0.013465341978071525, 0.01320067610671784, 0.013243410668670737, 0.012284412006582836, 0.013727917893959168, 0.012045663886345767, 0.012267900030436063, 0.013337570990541558, 0.01254220516325398, 0.013424487993793643, 0.01240714770494645, 0.013609602494705952, 0.013385740596079084, 0.01234510250307294, 0.014062862727188569, 0.012760862451049586, 0.013368561972976893, 0.013698487272321, 0.01348650466018607, 0.013350964562661233, 0.013311809393183306, 0.0135406489427679, 0.01357274149251606, 0.01364890851386869, 0.013579644439233192, 0.012927828527505646, 0.012586465666497751, 0.014222792677263195, 0.012566823215821737, 0.012380291163148156, 0.0123672458347892, 0.013002990309810647, 0.013580888399966766, 0.01287729123434091, 0.013296803751777086, 0.013503178912914443, 0.013911087460579003, 0.012976676520254669, 0.013164718381689202, 0.012500773768758341, 0.012558067634875228, 0.013061902167721807, 0.0139373644216882, 0.012795950675109868, 0.013570082285837505, 0.012672639792450284, 0.012779499959297242, 0.013284840570387042, 0.012530073155943724, 0.013798274409174765, 0.012854266751842786, 0.01266025895453459, 0.01325335817822524, 0.013335039503800201, 0.0129630751907013, 0.01396238330952984, 0.01428344863242367, 0.012874950402184412, 0.013793828880135004, 0.013079366891186578, 0.013516611940340215, 0.012336175267105589, 0.013861415150230538, 0.012476094042701776, 0.01311577342708671, 0.013339724516936631, 0.01415747453202658, 0.01436875707189532, 0.014091119517682629, 0.013343164248761905, 0.012849128324306971, 0.013482031093391278, 0.012849999207676889, 0.013306730040585782, 0.013470674082482892, 0.012463170600305244, 0.01357983812870147, 0.01391117974366937, 0.013448262785251719, 0.013210999141340213, 0.012550197116820627, 0.013122343710955308, 0.013912286782053912, 0.012766514548140604, 0.012683607313147365, 0.01312294436021751, 0.012924017503544914, 0.012847761658258742, 0.012669671478573341, 0.014007143849854109, 0.012831697907392534, 0.012578089713599253, 0.01469774545623555, 0.013616910650553735, 0.01363248383036532, 0.01268219722666272, 0.013544756535063903, 0.012681816623114113, 0.014430534249911508, 0.013280639151661272, 0.013439736455242962, 0.013489718398399315, 0.012219664368202954, 0.013453895476445153, 0.014468951068161161, 0.012462203431419301, 0.01402311136169534, 0.013068058347252822, 0.01349259792454765, 0.013633491016670515, 0.01320869026470516, 0.012684336256839405, 0.011753313433698877, 0.012781276963649214, 0.011988605134424865, 0.013086557773721792, 0.013141106455794343, 0.012328680056929964, 0.012451734980676553, 0.013310022288631245, 0.01304715263821609, 0.013770728939076566, 0.014024154733484256, 0.01367230247862534, 0.014043143124428031, 0.012787830170707351, 0.013164080860540668, 0.013324211358109869, 0.013135907333412852, 0.013563359785422753, 0.015140696562942884, 0.012887180709081807, 0.013583352332289049, 0.013261125928242124, 0.01281711743767391, 0.012320056512579908, 0.01354356267861182, 0.01208922998020824], "accuracy_valid": [0.3518507624246988, 0.39175893025225905, 0.4132035956325301, 0.4326451313064759, 0.4528176181287651, 0.4665924439947289, 0.47698901073042166, 0.48776208349021083, 0.49563635400978917, 0.5028693877070783, 0.5110995740775602, 0.5167045133659638, 0.5219535368034638, 0.5265922086784638, 0.5326148343373494, 0.5362872387989458, 0.5389419004141567, 0.543102586125753, 0.545177781438253, 0.5502238445971386, 0.5543536450489458, 0.5558287838855422, 0.5577922039721386, 0.559215867375753, 0.5629191570971386, 0.5642825207078314, 0.5651164227221386, 0.5664900814194277, 0.5673754588667168, 0.5647708019578314, 0.5679652202560241, 0.5691756282944277, 0.5701624858810241, 0.5710169780685241, 0.5712611186935241, 0.5728480327560241, 0.5721156108810241, 0.5724818218185241, 0.5724715267319277, 0.5727362575301205, 0.5731024684676205, 0.5740790309676205, 0.5738143001694277, 0.5754320994917168, 0.5750658885542168, 0.5756556499435241, 0.5759306758283133, 0.5760321559676205, 0.5766425075301205, 0.5768969432417168, 0.5767645778426205, 0.5761439311935241, 0.5771307887801205, 0.5772528590926205, 0.5756556499435241, 0.5777411403426205, 0.5773852244917168, 0.5765101421310241, 0.5776190700301205, 0.5771101986069277, 0.5778632106551205, 0.5782397166792168, 0.5778632106551205, 0.5780970561935241, 0.5773749294051205, 0.5776396602033133, 0.5776293651167168, 0.5779852809676205, 0.5783720820783133, 0.5787279979292168, 0.5786059276167168, 0.5787177028426205, 0.5793280544051205, 0.5780058711408133, 0.5799692912274097, 0.5797251506024097, 0.5800913615399097, 0.5790942088667168, 0.5781073512801205, 0.5794707148908133, 0.5793383494917168, 0.5788603633283133, 0.5792368693524097, 0.5798472209149097, 0.5796030802899097, 0.5798369258283133, 0.5789721385542168, 0.5789824336408133, 0.5784735622176205, 0.5772322689194277, 0.5785956325301205, 0.5776190700301205, 0.5794398296310241, 0.5792265742658133, 0.5792162791792168, 0.5787279979292168, 0.5793589396649097, 0.5793589396649097, 0.5790839137801205, 0.5781279414533133, 0.5787279979292168, 0.5793383494917168, 0.5780867611069277, 0.5795721950301205, 0.5795927852033133, 0.5797148555158133, 0.5784632671310241, 0.5788294780685241, 0.5786059276167168, 0.5780867611069277, 0.5789515483810241, 0.5784838573042168, 0.5784632671310241, 0.5790736186935241, 0.5794604198042168, 0.5789721385542168, 0.5795824901167168, 0.5802031367658133, 0.5786971126694277, 0.5790942088667168, 0.5797148555158133, 0.5789618434676205, 0.5793486445783133, 0.5807017131024097, 0.5789824336408133, 0.5782191265060241, 0.5788397731551205, 0.5786059276167168, 0.5778323253953314, 0.5794398296310241, 0.5801825465926205, 0.5790942088667168, 0.5801825465926205, 0.5798369258283133, 0.5774764095444277, 0.5787177028426205, 0.5788397731551205, 0.5799589961408133, 0.5793589396649097, 0.5782294215926205, 0.5779646907944277, 0.5783823771649097, 0.5795618999435241, 0.5778735057417168, 0.5786059276167168, 0.5790942088667168, 0.5787382930158133, 0.5791045039533133, 0.5783823771649097, 0.5803355021649097, 0.578758883189006, 0.5789618434676205, 0.5783617869917168, 0.5787279979292168, 0.5783617869917168, 0.5792162791792168, 0.5782397166792168, 0.5788294780685241, 0.5782191265060241, 0.5792162791792168, 0.5782088314194277, 0.5790839137801205, 0.5786162227033133, 0.5795824901167168, 0.5784735622176205, 0.5790942088667168, 0.5784426769578314, 0.5788603633283133, 0.5794501247176205, 0.5783720820783133, 0.5789721385542168, 0.5777411403426205, 0.5778426204819277, 0.5792368693524097], "seed": 255820489, "model": "residualv3", "loss_std": [0.4220810532569885, 0.14221832156181335, 0.1455475091934204, 0.1479063630104065, 0.14878417551517487, 0.1511942595243454, 0.15205466747283936, 0.1540164351463318, 0.15513424575328827, 0.1569509357213974, 0.15643838047981262, 0.15819008648395538, 0.15877555310726166, 0.16035695374011993, 0.15946020185947418, 0.16161276400089264, 0.16182170808315277, 0.16154836118221283, 0.1616884469985962, 0.16204750537872314, 0.16225889325141907, 0.1636175811290741, 0.16257034242153168, 0.1623746007680893, 0.16267666220664978, 0.16313162446022034, 0.16354785859584808, 0.16346251964569092, 0.16291172802448273, 0.162709042429924, 0.16442231833934784, 0.16333599388599396, 0.1642945110797882, 0.16393843293190002, 0.1635075807571411, 0.16260147094726562, 0.1651485115289688, 0.1643524467945099, 0.16492235660552979, 0.1651167869567871, 0.16411198675632477, 0.16446612775325775, 0.1636650413274765, 0.16291216015815735, 0.16323824226856232, 0.16374816000461578, 0.16512015461921692, 0.16388025879859924, 0.16538217663764954, 0.1631001979112625, 0.1624961942434311, 0.16409094631671906, 0.16388127207756042, 0.16440528631210327, 0.16431887447834015, 0.16482827067375183, 0.16487738490104675, 0.16405102610588074, 0.1649782657623291, 0.16384010016918182, 0.16407112777233124, 0.1639329195022583, 0.16544775664806366, 0.16386178135871887, 0.1641128659248352, 0.16400139033794403, 0.16451005637645721, 0.16466118395328522, 0.16245143115520477, 0.16379797458648682, 0.16477881371974945, 0.16402551531791687, 0.16418974101543427, 0.1638983190059662, 0.1636156290769577, 0.16350743174552917, 0.16477394104003906, 0.16308178007602692, 0.16392655670642853, 0.16396379470825195, 0.1638498157262802, 0.16444042325019836, 0.1648234724998474, 0.1635715663433075, 0.16392017900943756, 0.16443896293640137, 0.16299551725387573, 0.164613738656044, 0.16362957656383514, 0.1630968153476715, 0.163960799574852, 0.1637411266565323, 0.1644834727048874, 0.16444632411003113, 0.16348089277744293, 0.1639113873243332, 0.162555530667305, 0.16428381204605103, 0.16333897411823273, 0.16468583047389984, 0.1648121178150177, 0.16420312225818634, 0.16367259621620178, 0.1625286340713501, 0.16509146988391876, 0.16332456469535828, 0.1633596271276474, 0.16455470025539398, 0.1646965742111206, 0.1626976728439331, 0.16236628592014313, 0.16405357420444489, 0.1635425090789795, 0.163070410490036, 0.16638197004795074, 0.1650574803352356, 0.16254054009914398, 0.16372567415237427, 0.16457344591617584, 0.16330263018608093, 0.1639915555715561, 0.1632319539785385, 0.16394655406475067, 0.1629582792520523, 0.16411511600017548, 0.163900688290596, 0.1641133725643158, 0.16432100534439087, 0.16304415464401245, 0.16324399411678314, 0.16316089034080505, 0.1638341099023819, 0.16384686529636383, 0.1646016389131546, 0.16412290930747986, 0.16403809189796448, 0.16427434980869293, 0.164157435297966, 0.16435988247394562, 0.1650051325559616, 0.16351386904716492, 0.1646604686975479, 0.16548892855644226, 0.16405357420444489, 0.16421157121658325, 0.1633535921573639, 0.16385596990585327, 0.16603712737560272, 0.1629628837108612, 0.1642231047153473, 0.1643153876066208, 0.16364836692810059, 0.16381055116653442, 0.1644948273897171, 0.16405878961086273, 0.16335847973823547, 0.16316856443881989, 0.16294462978839874, 0.16337890923023224, 0.1629253476858139, 0.16324388980865479, 0.1637205332517624, 0.16365179419517517, 0.16401445865631104, 0.16354165971279144, 0.16327829658985138, 0.16415952146053314, 0.16596835851669312, 0.1649208515882492, 0.1635410338640213, 0.16422678530216217, 0.1642197072505951, 0.16374163329601288, 0.16321329772472382]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:40 2016", "state": "available"}], "summary": "50f5d3e2bc76fb4b63c6916f5606c5c5"}