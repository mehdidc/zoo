{"content": {"hp_model": {"f0": 32, "f1": 64, "f2": 64, "f3": 64, "nonlin": "very_leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.447299599647522, 0.9911528825759888, 0.8129781484603882, 0.7039411067962646, 0.6296564340591431, 0.5713794827461243, 0.5232686400413513, 0.48322218656539917, 0.4507640302181244, 0.41775810718536377, 0.39070773124694824, 0.36685818433761597, 0.34410446882247925, 0.3213937282562256, 0.3038109540939331, 0.2873009443283081, 0.27202075719833374, 0.2560000419616699, 0.24590350687503815, 0.23342591524124146, 0.22045300900936127, 0.2107425183057785, 0.20299869775772095, 0.19366668164730072, 0.18548783659934998, 0.17829568684101105, 0.1707489937543869, 0.16437844932079315, 0.1574094295501709, 0.1534223109483719, 0.14717857539653778, 0.14117850363254547, 0.13761630654335022, 0.1324618011713028, 0.12843367457389832, 0.1238548755645752, 0.1204531341791153, 0.11751869320869446, 0.11312327533960342, 0.11076324433088303, 0.1070956215262413, 0.10518601536750793, 0.10207106173038483, 0.10025616735219955, 0.09398627281188965, 0.09399626404047012, 0.09222611784934998, 0.08970660716295242, 0.08739899098873138, 0.08416242897510529, 0.08313175290822983, 0.0803881362080574, 0.07819484174251556, 0.07606454193592072, 0.07636294513940811, 0.07276821881532669, 0.07098989933729172, 0.07154083997011185, 0.06856782734394073, 0.06706418842077255, 0.06663031131029129, 0.06471695005893707, 0.06469382345676422, 0.06257882714271545, 0.06256631016731262, 0.06092173606157303, 0.05931636318564415, 0.05864902585744858, 0.05803292617201805, 0.05589050427079201, 0.05752875283360481, 0.05332858860492706, 0.05608624964952469, 0.052652470767498016, 0.05288564786314964, 0.051533978432416916, 0.05116403475403786, 0.04989965632557869, 0.05057127773761749, 0.050290659070014954, 0.04895753413438797, 0.045446980744600296, 0.04754065349698067, 0.04704545810818672, 0.04354281723499298, 0.04690142720937729, 0.04222417622804642, 0.04338179528713226, 0.023484978824853897, 0.014618250541388988, 0.012385797686874866, 0.010505853220820427, 0.010076550766825676, 0.008828273974359035, 0.008425327949225903, 0.007752187550067902, 0.007145230192691088, 0.007346180267632008, 0.0068141561932861805, 0.007028831634670496, 0.006024282891303301, 0.005553992930799723, 0.005727654788643122, 0.005945339798927307, 0.005880534648895264, 0.005654968321323395, 0.005778343882411718, 0.005725948140025139, 0.006007753312587738, 0.005916106514632702, 0.005622225813567638, 0.0061361840926110744, 0.005989610217511654, 0.005874342285096645, 0.005747056566178799, 0.005920752417296171, 0.005843609571456909, 0.005730374716222286, 0.005672304891049862, 0.0056362696923315525, 0.005758101586252451, 0.005661523900926113, 0.005870286840945482, 0.005642946343868971, 0.0061799525283277035, 0.005654558073729277, 0.005648404825478792, 0.005896930117160082, 0.005653021857142448, 0.005432367790490389, 0.005644645541906357, 0.005875169299542904, 0.0057929083704948425, 0.005787603557109833, 0.0056721423752605915, 0.006039580330252647, 0.005396225024014711, 0.0052939485758543015, 0.0058492813259363174, 0.005728604272007942, 0.006052853539586067, 0.005593521520495415, 0.005311910063028336], "moving_avg_accuracy_train": [0.06429988147148392, 0.13089225512758396, 0.19434927159756826, 0.2555709448323297, 0.31293020699592217, 0.36591074492036096, 0.41508146848572613, 0.45954191374332737, 0.5018300216203327, 0.5401405068787037, 0.5750406153016676, 0.6075433165298988, 0.6366934771365066, 0.6647909937812446, 0.690806494290071, 0.7147250380885003, 0.7368144135189914, 0.7560205943004902, 0.774959265709773, 0.7915971689364609, 0.8054366813190699, 0.8190500944729233, 0.8320229624423439, 0.8432173820065259, 0.8541710495713771, 0.8642293131773623, 0.87340509145611, 0.8804681654188877, 0.8881942283412847, 0.8955407072178889, 0.9016828220984902, 0.9072736487041635, 0.9127749645623371, 0.9176958858513506, 0.9224921606210051, 0.9271249921029707, 0.9308666770069687, 0.9342482524598802, 0.9375799527710627, 0.941006202285623, 0.9438946224951837, 0.9464244822683213, 0.9490314190486413, 0.9511498336164146, 0.9523357547917333, 0.9544654966113787, 0.9562126004836018, 0.9580732042745274, 0.9593688205280454, 0.9609045377681072, 0.9624773815353627, 0.9640534122425684, 0.9651810881314068, 0.9665123608647316, 0.9676359934163629, 0.9687426298628403, 0.9694781499491846, 0.9697472760245411, 0.9710170610708966, 0.9719855535495396, 0.9729966696600711, 0.9740670733785878, 0.9749025535407291, 0.9754848219211985, 0.9759367838505256, 0.9767411139845299, 0.9774696253539341, 0.9782369287780738, 0.9785183838157795, 0.9790552894068391, 0.9792967250114208, 0.9796743441769454, 0.9801955630330604, 0.9806577566547728, 0.9809249213905045, 0.9813234437228918, 0.9809730876304107, 0.9813342773042929, 0.9818685393060064, 0.9820354800182628, 0.9824229278866838, 0.9824623501277774, 0.982832651573333, 0.9832356773386187, 0.9835589811440703, 0.9840079565415679, 0.9844957758052775, 0.9851091632545116, 0.9863750326433461, 0.9875492283742588, 0.9886199554249375, 0.9895952355145958, 0.9904776378929074, 0.9912997018191021, 0.9920372342038678, 0.9927126390942046, 0.9933321292395553, 0.9938431673941804, 0.9943333286678668, 0.9948023395510801, 0.9952314247924007, 0.9956176015095891, 0.9959558599598207, 0.9962649428626482, 0.996547767772812, 0.9967999850431498, 0.9970269805864539, 0.997233601724237, 0.9974149104506229, 0.9975827386019892, 0.9977337839382189, 0.9978673995920161, 0.9979830033828144, 0.998087046794533, 0.9981853361626988, 0.9982784468916669, 0.9983622465477382, 0.9984283656429643, 0.9984948482750965, 0.9985500323463963, 0.9985996980105663, 0.9986397468107001, 0.9986850913260586, 0.9987235762410719, 0.9987558875157743, 0.9987896179606254, 0.9988176502121819, 0.9988428792385828, 0.9988562847671055, 0.9988753251892044, 0.9988878112714744, 0.9989060241919461, 0.9989270661179895, 0.9989413535538096, 0.9989495619484287, 0.9989569495035857, 0.9989659234520367, 0.9989763251544521, 0.9989787112401974, 0.9989831838661777, 0.9989895343783695, 0.9989975749881516, 0.9990024863881459], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.06329610433923191, 0.12782213325960087, 0.18856389866105042, 0.24729451698395138, 0.30172986903179116, 0.3521093164753741, 0.3988516863150354, 0.44056066772118246, 0.4798108238876937, 0.5157626410659424, 0.5476411432902969, 0.5775830895316738, 0.603749591148913, 0.6291499108837958, 0.6522258071730517, 0.6738303690103399, 0.693354776339351, 0.7102665337393015, 0.7267913557983533, 0.7413849929500692, 0.7530491280206496, 0.764937568710678, 0.7760532339028632, 0.7853319701887818, 0.7944967329778855, 0.8028039956270095, 0.8106529200006791, 0.8163903564927046, 0.8226710662312354, 0.8288506663569823, 0.8338812270744468, 0.8384066727028454, 0.8424725142804524, 0.8461998663445005, 0.8499237826656227, 0.853976255679181, 0.8567475165946966, 0.8596475719758896, 0.8619545050550326, 0.8645546176613516, 0.8670320848767675, 0.8689067719557323, 0.8709082846133217, 0.8722151877031943, 0.8719936218696219, 0.8737942537997531, 0.8748410920681211, 0.8767914296327699, 0.8782618536875653, 0.8794815491075888, 0.880869155209631, 0.8820335809913789, 0.8828994882348615, 0.8839972170751554, 0.8851337164237393, 0.8863976179364859, 0.8871037651695692, 0.8874179439048714, 0.8884463631815529, 0.8889863160011988, 0.8900765951220879, 0.8907220794351803, 0.891734379445352, 0.8920533349025488, 0.8923048032289355, 0.8931375053232107, 0.8935480817863716, 0.894720323212403, 0.8950764511886026, 0.8955180071710225, 0.89602630034511, 0.8959160637123309, 0.8964069062774682, 0.8973278273308207, 0.8973193715932206, 0.8978825504448172, 0.8974991805659981, 0.8980626155308591, 0.8984335177108304, 0.898552603580937, 0.8988683299039427, 0.8984555062320726, 0.898628280863007, 0.8991327814664654, 0.8994402005619122, 0.8997495278795915, 0.9003169202535299, 0.9007766862477552, 0.9027062065348772, 0.9044224787654257, 0.9059427097104192, 0.9073088585435942, 0.9088079766896113, 0.9099862845835267, 0.911033525148141, 0.9117929361875438, 0.9124875836455967, 0.9130873227866846, 0.9136392950449137, 0.9142479923758892, 0.9148680326526074, 0.9153517972054943, 0.9158126288742522, 0.9162894420410439, 0.9166941598286563, 0.9169953116639383, 0.9172795848556017, 0.9175099871569391, 0.9177183787368024, 0.9178438664937697, 0.9179934265687903, 0.9181524446988089, 0.9183576256807352, 0.918493460439469, 0.9186157117223293, 0.9187989800644035, 0.9188652358136107, 0.9189625165903068, 0.9189880046244236, 0.9190974225825385, 0.9192081057760918, 0.9192456559853802, 0.9192916582049898, 0.9193829178362981, 0.9194772585357256, 0.9194878934690506, 0.9194730508465431, 0.9195339641824459, 0.9195521650910086, 0.9195308953063053, 0.919573817164982, 0.9196368609002911, 0.9196681566909095, 0.9196719088399662, 0.9196641082515268, 0.9196682652445217, 0.9196353854444671, 0.9197055088917372, 0.9197431764231206, 0.9197160420451158, 0.9197394197212518, 0.9197848736922742, 0.919790190681104], "moving_var_accuracy_train": [0.03721027281522195, 0.07340014359608255, 0.10230126568992104, 0.12580397858390407, 0.14283434532907918, 0.15381334738503732, 0.16019185315000758, 0.16196324856754424, 0.161861480321145, 0.15888457181561721, 0.15395827274546722, 0.14807027575510545, 0.14091083495011417, 0.13392498542951473, 0.126623743287086, 0.1191102395953138, 0.11159068019796507, 0.1037515086000731, 0.09660441721280488, 0.08943535390555007, 0.08221560744189055, 0.0756619718569791, 0.06961043240144932, 0.06377722442571486, 0.05847934748123486, 0.05354193073401876, 0.048945491823803526, 0.04449992576565619, 0.040587161623618105, 0.037014182228215835, 0.03365229418225278, 0.030568380843239845, 0.027783923044458387, 0.025223469937006558, 0.022908161208300123, 0.020810513235332735, 0.018855463765086712, 0.017072832861471644, 0.015465451617996284, 0.014024559127820885, 0.012697189956801781, 0.011485072675367261, 0.0103977304822198, 0.00939834655652641, 0.008471169582180393, 0.0076648748259274735, 0.00692585869079776, 0.006264429439919242, 0.005653094089214741, 0.005109010527266075, 0.004620374012185214, 0.00418069146607719, 0.003774067195661878, 0.0034126110599101375, 0.0030827129049188933, 0.0027854634124490534, 0.0025117859793808905, 0.002261259241042733, 0.00204964450351399, 0.001853121852293285, 0.0016770108691647428, 0.0015196216593337981, 0.0013739417373124022, 0.0012395988917832117, 0.001117477428874941, 0.001011552208667653, 0.0009151735471390476, 0.0008289549833274104, 0.0007467724374389189, 0.0006746896022184261, 0.0006077452623570219, 0.000548254102228863, 0.0004958737138697051, 0.00044820894897829933, 0.0004040304470446365, 0.0003650567827848759, 0.0003296558490302362, 0.00029786438595188444, 0.00027064687033497096, 0.00024383300611415226, 0.00022080074815943314, 0.00019873466036132535, 0.0001800953027704179, 0.00016354764040073358, 0.00014813360451623525, 0.00013513445423263607, 0.00012376271751578786, 0.00011477264323011101, 0.00011771720669339514, 0.00011835410655449828, 0.00011683680365254217, 0.00011371366456684375, 0.00010935000372540829, 0.00010449710524162286, 9.894298088466394e-05, 9.315422868921455e-05, 8.729271818197251e-05, 8.091388632311919e-05, 7.49848203588044e-05, 6.946607920007681e-05, 6.417649857894122e-05, 5.910104083313352e-05, 5.42207057621977e-05, 4.965842535336011e-05, 4.541249218630629e-05, 4.144376493078575e-05, 3.776313122782634e-05, 3.437104875625306e-05, 3.1229799569000584e-05, 2.8360316207619726e-05, 2.57296168292283e-05, 2.3317333432762243e-05, 2.110587821750865e-05, 1.909271567945612e-05, 1.7270391310560302e-05, 1.5621378650145213e-05, 1.412244222634987e-05, 1.2749543616496645e-05, 1.1514368718223932e-05, 1.039033938192861e-05, 9.373505547512678e-06, 8.45059015029086e-06, 7.624036260919697e-06, 6.874962432979872e-06, 6.1968623559378915e-06, 5.5874158065328075e-06, 5.035746490025472e-06, 4.537900374981138e-06, 4.085727711237773e-06, 3.6804177791773604e-06, 3.313779121513712e-06, 2.985386603611285e-06, 2.6908328071147326e-06, 2.4235867038040783e-06, 2.181834433103667e-06, 1.9641421735340983e-06, 1.768452741937891e-06, 1.5925812264623437e-06, 1.4333743444627631e-06, 1.2902169494649202e-06, 1.1615582155643067e-06, 1.0459842566588882e-06, 9.416029276421413e-07], "duration": 225878.728074, "accuracy_train": [0.6429988147148394, 0.7302236180324843, 0.7654624198274271, 0.8065660039451827, 0.829163566468254, 0.8427355862403102, 0.8576179805740125, 0.8596859210617387, 0.8824229925133813, 0.8849348742040422, 0.8891415911083426, 0.9000676275839794, 0.8990449225959765, 0.9176686435838871, 0.9249459988695091, 0.9299919322743633, 0.935618792393411, 0.9288762213339794, 0.9454073083933187, 0.9413382979766519, 0.9299922927625508, 0.9415708128576044, 0.9487787741671282, 0.9439671580841639, 0.9527540576550388, 0.9547536856312293, 0.9559870959648394, 0.9440358310838871, 0.9577287946428571, 0.9616590171073275, 0.9569618560239018, 0.9575910881552234, 0.9622868072858989, 0.9619841774524732, 0.9656586335478959, 0.9688204754406607, 0.9645418411429494, 0.9646824315360835, 0.9675652555717055, 0.9718424479166666, 0.9698904043812293, 0.9691932202265596, 0.9724938500715209, 0.970215564726375, 0.9630090453696014, 0.9736331729881875, 0.9719365353336102, 0.9748186383928571, 0.9710293668097084, 0.9747259929286637, 0.9766329754406607, 0.9782376886074198, 0.9753301711309523, 0.9784938154646549, 0.9777486863810447, 0.978702357881137, 0.9760978307262828, 0.9721694107027501, 0.9824451264880952, 0.9807019858573275, 0.9820967146548542, 0.9837007068452381, 0.982421875, 0.9807252373454227, 0.9800044412144703, 0.9839800851905685, 0.9840262276785714, 0.9851426595953304, 0.9810514791551311, 0.983887439726375, 0.9814696454526578, 0.9830729166666666, 0.9848865327380952, 0.9848174992501846, 0.9833294040120893, 0.984910144714378, 0.9778198827980805, 0.9845849843692323, 0.9866768973214286, 0.9835379464285714, 0.9859099587024732, 0.9828171502976191, 0.9861653645833334, 0.9868629092261905, 0.9864687153931341, 0.9880487351190477, 0.9888861491786637, 0.9906296502976191, 0.9977678571428571, 0.9981169899524732, 0.9982564988810447, 0.9983727563215209, 0.9984192592977114, 0.9986982771548542, 0.9986750256667589, 0.9987912831072352, 0.9989075405477114, 0.9984425107858066, 0.9987447801310447, 0.9990234375, 0.9990931919642857, 0.9990931919642857, 0.9990001860119048, 0.9990466889880952, 0.9990931919642857, 0.9990699404761905, 0.9990699404761905, 0.9990931919642857, 0.9990466889880952, 0.9990931919642857, 0.9990931919642857, 0.9990699404761905, 0.9990234375, 0.9990234375, 0.9990699404761905, 0.9991164434523809, 0.9991164434523809, 0.9990234375, 0.9990931919642857, 0.9990466889880952, 0.9990466889880952, 0.9990001860119048, 0.9990931919642857, 0.9990699404761905, 0.9990466889880952, 0.9990931919642857, 0.9990699404761905, 0.9990699404761905, 0.9989769345238095, 0.9990466889880952, 0.9990001860119048, 0.9990699404761905, 0.9991164434523809, 0.9990699404761905, 0.9990234375, 0.9990234375, 0.9990466889880952, 0.9990699404761905, 0.9990001860119048, 0.9990234375, 0.9990466889880952, 0.9990699404761905, 0.9990466889880952], "end": "2016-02-04 20:58:56.605000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0], "moving_var_accuracy_valid": [0.036057571420706404, 0.0699242899527264, 0.09613791953421635, 0.11756769733810717, 0.13247979557744674, 0.14207461454218867, 0.147530795332019, 0.14843446796826198, 0.1474561940032955, 0.1443433730287299, 0.1390551858624705, 0.133218348578717, 0.12605868598281017, 0.11925940356823764, 0.11212593611738635, 0.1051141563352786, 0.09803356303569306, 0.09080427457731675, 0.08418147481633499, 0.07768009554254497, 0.07113655441079317, 0.06529491416807677, 0.059877444865251914, 0.05466455490229931, 0.04995403530489425, 0.045579727288898614, 0.04157620508442101, 0.037714848173479136, 0.034298389189508297, 0.031212237389984653, 0.02831877252117505, 0.025671212192277883, 0.02325287058265788, 0.021052621905076365, 0.019072167689469212, 0.01731275375825285, 0.015650597365984346, 0.014161230520311789, 0.012793004930365408, 0.011574549707418719, 0.010472335330908, 0.009456731862613539, 0.008547113152618602, 0.00770777379853361, 0.006937438241447709, 0.00627287489543321, 0.005655450239130968, 0.005124139564762591, 0.004631184930394617, 0.004181455349613794, 0.0037806388709022407, 0.0034147779704228063, 0.0030800483315693663, 0.0027828885758737483, 0.0025162243952103576, 0.002278978978994629, 0.002055568876328287, 0.0018509003631949027, 0.001675329142753264, 0.0015104201699049297, 0.0013700765299674582, 0.0012368187269567478, 0.0011223596160564162, 0.0010110392477038549, 0.0009105044498060509, 0.0008256945398257375, 0.0007446422431320788, 0.0006825453684670107, 0.0006154322758391969, 0.0005556437934257742, 0.0005024046716406124, 0.00045227357351340934, 0.0004092145539758244, 0.0003759259588568142, 0.000338334006466618, 0.00030735513958992733, 0.00027794237780680634, 0.00025300527066277734, 0.00022894285944046692, 0.00020617620649655193, 0.000186455733846245, 0.0001693439709181279, 0.0001526782334841655, 0.00013970109786575735, 0.00012658154658138978, 0.00011478454242841463, 0.00010620349513960461, 9.74856085506582e-05, 0.00012124448454133306, 0.0001356303494113645, 0.00014286723360527258, 0.00014537777395421251, 0.0001510661935002506, 0.00014845525958599514, 0.00014348014882895665, 0.0001343224800869621, 0.0001252330478970795, 0.00011594692644354741, 0.00010709429416388374, 9.97194767141256e-05, 9.32075785454894e-05, 8.599307397460872e-05, 7.93050590195187e-05, 7.34207102817997e-05, 6.755280764210916e-05, 6.161375872894143e-05, 5.6179684083533755e-05, 5.103948265933458e-05, 4.632637784842251e-05, 4.183546465791853e-05, 3.7853232136488226e-05, 3.429548981391078e-05, 3.1244833950618036e-05, 2.828641029067843e-05, 2.5592277647059564e-05, 2.3335335449213592e-05, 2.1041310323019347e-05, 1.9022351236348893e-05, 1.7125962871662305e-05, 1.552111719051832e-05, 1.4079262395483077e-05, 1.2684026319893197e-05, 1.1434669525784937e-05, 1.03661574559651e-05, 9.409643218484676e-06, 8.469696812897659e-06, 7.624709862593986e-06, 6.895632786751993e-06, 6.209050965729344e-06, 5.592217502828321e-06, 5.049576326115854e-06, 4.5803893065597035e-06, 4.131165214497655e-06, 3.7181754006507783e-06, 3.346905503205707e-06, 3.0123704782019733e-06, 2.720863161646441e-06, 2.4930325261952522e-06, 2.2564988598604797e-06, 2.0374754441018145e-06, 1.838646541365312e-06, 1.6733764585641293e-06, 1.506293246039663e-06], "accuracy_test": 0.908575813137755, "start": "2016-02-02 06:14:17.877000", "learning_rate_per_epoch": [0.0004967956338077784, 0.0004967956338077784, 0.0004967956338077784, 0.0004967956338077784, 0.0004967956338077784, 0.0004967956338077784, 0.0004967956338077784, 0.0004967956338077784, 0.0004967956338077784, 0.0004967956338077784, 0.0004967956338077784, 0.0004967956338077784, 0.0004967956338077784, 0.0004967956338077784, 0.0004967956338077784, 0.0004967956338077784, 0.0004967956338077784, 0.0004967956338077784, 0.0004967956338077784, 0.0004967956338077784, 0.0004967956338077784, 0.0004967956338077784, 0.0004967956338077784, 0.0004967956338077784, 0.0004967956338077784, 0.0004967956338077784, 0.0004967956338077784, 0.0004967956338077784, 0.0004967956338077784, 0.0004967956338077784, 0.0004967956338077784, 0.0004967956338077784, 0.0004967956338077784, 0.0004967956338077784, 0.0004967956338077784, 0.0004967956338077784, 0.0004967956338077784, 0.0004967956338077784, 0.0004967956338077784, 0.0004967956338077784, 0.0004967956338077784, 0.0004967956338077784, 0.0004967956338077784, 0.0004967956338077784, 0.0004967956338077784, 0.0004967956338077784, 0.0004967956338077784, 0.0004967956338077784, 0.0004967956338077784, 0.0004967956338077784, 0.0004967956338077784, 0.0004967956338077784, 0.0004967956338077784, 0.0004967956338077784, 0.0004967956338077784, 0.0004967956338077784, 0.0004967956338077784, 0.0004967956338077784, 0.0004967956338077784, 0.0004967956338077784, 0.0004967956338077784, 0.0004967956338077784, 0.0004967956338077784, 0.0004967956338077784, 0.0004967956338077784, 0.0004967956338077784, 0.0004967956338077784, 0.0004967956338077784, 0.0004967956338077784, 0.0004967956338077784, 0.0004967956338077784, 0.0004967956338077784, 0.0004967956338077784, 0.0004967956338077784, 0.0004967956338077784, 0.0004967956338077784, 0.0004967956338077784, 0.0004967956338077784, 0.0004967956338077784, 0.0004967956338077784, 0.0004967956338077784, 0.0004967956338077784, 0.0004967956338077784, 0.0004967956338077784, 0.0004967956338077784, 0.0004967956338077784, 0.0004967956338077784, 4.967956192558631e-05, 4.967956192558631e-05, 4.967956192558631e-05, 4.967956192558631e-05, 4.967956192558631e-05, 4.967956192558631e-05, 4.967956192558631e-05, 4.967956192558631e-05, 4.967956192558631e-05, 4.967956192558631e-05, 4.967956192558631e-05, 4.967956192558631e-05, 4.967956101609161e-06, 4.967956215295999e-07, 4.967956357404546e-08, 4.967956268586704e-09, 4.967956046542099e-10, 4.967955907764221e-11, 4.9679557342918734e-12, 4.967955842712091e-13, 4.967955978237362e-14, 4.967955808830773e-15, 4.967955702951654e-16, 4.9679557691261034e-17, 4.967955769126103e-18, 4.967955769126103e-19, 4.9679556398791327e-20, 4.967955478320419e-21, 4.967955680268811e-22, 4.967955680268811e-23, 4.967955680268811e-24, 4.967955581661198e-25, 4.9679557049207143e-26, 4.967955550846319e-27, 4.967955550846319e-28, 4.967955550846319e-29, 4.96795547561468e-30, 4.967955569654228e-31, 4.967955334555358e-32, 4.967955408023755e-33, 4.967955499859251e-34, 4.967955385064881e-35, 4.9679552415719183e-36, 4.967955062205715e-37, 4.967955062205715e-38, 4.967954781946022e-39, 4.967953380647558e-40, 4.967883315724341e-41, 4.9676030560314765e-42, 4.9605965637098524e-43, 4.90454462513686e-44, 5.605193857299268e-45, 0.0, 0.0, 0.0, 0.0], "accuracy_train_first": 0.6429988147148394, "accuracy_train_last": 0.9990466889880952, "batch_size_eval": 1024, "accuracy_train_std": [0.021893949532716412, 0.01834545201918069, 0.022683031964651813, 0.021176836341598622, 0.021490156338222523, 0.022818075883059757, 0.0209701952556257, 0.021672901016961114, 0.02099151803127338, 0.01958789206798148, 0.019813917131012188, 0.019533876054423884, 0.018675201219759695, 0.017615827913716787, 0.016792977336000964, 0.014914262336799771, 0.015470799183620357, 0.014963300448398628, 0.013692405539227367, 0.01420342977335125, 0.01492295296883727, 0.01431252791072317, 0.01225204905190437, 0.012423137134813397, 0.011422272407361257, 0.011657640166686016, 0.011031006195799864, 0.012185541274199175, 0.011448802298607386, 0.010989099116279016, 0.010409496734624985, 0.011556291820609875, 0.009858252833251023, 0.00952780908885058, 0.009014234658631494, 0.009712605629138296, 0.009936242400531944, 0.008158452719446837, 0.00860051814418462, 0.009187986673445147, 0.007742644707123487, 0.007846329100441873, 0.007601319624244568, 0.008945451029873189, 0.009077840352535222, 0.006827254910002313, 0.007933969444242718, 0.006750824416359003, 0.0083177593612224, 0.007602067210201925, 0.007507204702537614, 0.007248787680806821, 0.007248835540016685, 0.005566690424997628, 0.006090650842118573, 0.006738216542579395, 0.00730664024161157, 0.007200004473078133, 0.006666043894737268, 0.006125745360345953, 0.006219991720061347, 0.005892176659924366, 0.006050035415179451, 0.006018230399380696, 0.006345865468833066, 0.0058990894710139975, 0.005862557375227246, 0.0057881832079158925, 0.005064373125270407, 0.0046083485994881865, 0.00627151671010071, 0.00560047217906421, 0.005446608354557782, 0.005094735685496087, 0.004653266231379354, 0.005546468339169782, 0.007030068760439303, 0.005745505146149742, 0.005597044200099601, 0.006587520412619322, 0.004999242750210398, 0.005898229325482211, 0.004382458339139407, 0.0051064947587016836, 0.004159273762362164, 0.0039051426293186414, 0.004160147186265884, 0.003635643601911865, 0.0016408369386511671, 0.0015716555817990546, 0.0014060386111327463, 0.0012725970313988138, 0.0012953215431819676, 0.001100250402099698, 0.0013779665683414407, 0.0009949402792026256, 0.0010038242806223662, 0.0012795678016656593, 0.001049948132073554, 0.0010655170421679315, 0.0009384503381050302, 0.0008887423202016552, 0.0010105707687918866, 0.0009162974560657567, 0.0009139343381246999, 0.0009754546578963464, 0.001020949516373705, 0.0009139343381246999, 0.0009878462512206102, 0.0009623419869128943, 0.0010084285919922545, 0.0009754546578963464, 0.001022008043891179, 0.001022008043891179, 0.0009754546578963464, 0.000899324758454879, 0.0009242283721331749, 0.001022008043891179, 0.0009384503381050302, 0.0010105707687918866, 0.0009407518751927042, 0.0010105707687918866, 0.0008628150723251243, 0.0009277314612751116, 0.0009878462512206102, 0.0009623419869128943, 0.0009754546578963464, 0.0009754546578963464, 0.0009984612422611555, 0.0009407518751927042, 0.0009407518751927042, 0.0009518921824060046, 0.0008993247584548789, 0.001020949516373705, 0.0010439892262204078, 0.000953027414988802, 0.0009645865193773968, 0.0009518921824060046, 0.0009645865193773968, 0.0009765625, 0.0009645865193773968, 0.0009754546578963464, 0.0009645865193773968], "accuracy_test_std": 0.008842520927367281, "error_valid": [0.3670389566076807, 0.29144360645707834, 0.26476021272590367, 0.22412991810993976, 0.20835196253765065, 0.19447565653237953, 0.18046698512801207, 0.18405849962349397, 0.16693777061370485, 0.1606710043298193, 0.16545233669051207, 0.15293939429593373, 0.16075189429593373, 0.14224721150225905, 0.1400911262236446, 0.13172857445406627, 0.13092555769954817, 0.1375276496611446, 0.12448524567018071, 0.12727227268448793, 0.1419736563441265, 0.12806646507906627, 0.12390577936746983, 0.13115940323795183, 0.12302040192018071, 0.12243064053087349, 0.11870676063629515, 0.13197271507906627, 0.12080254612198793, 0.11553293251129515, 0.12084372646837349, 0.12086431664156627, 0.12093491152108427, 0.12025396507906627, 0.11656097044427716, 0.10955148719879515, 0.11831113516566272, 0.11425192959337349, 0.11728309723268071, 0.11204436888177716, 0.11067071018448793, 0.11422104433358427, 0.11107810146837349, 0.11602268448795183, 0.13000047063253017, 0.11000005882906627, 0.11573736351656627, 0.1056555322853916, 0.10850432981927716, 0.10954119211219882, 0.10664238987198793, 0.1074865869728916, 0.10930734657379515, 0.10612322336219882, 0.10463778943900603, 0.10222726844879515, 0.10654090973268071, 0.1097544474774097, 0.10229786332831325, 0.10615410862198793, 0.1001108927899097, 0.10346856174698793, 0.09915492046310237, 0.10507606598268071, 0.10543198183358427, 0.09936817582831325, 0.10275673004518071, 0.09472950395331325, 0.10171839702560237, 0.10050798898719882, 0.09939906108810237, 0.10507606598268071, 0.09917551063629515, 0.09438388318900603, 0.10275673004518071, 0.09704883989081325, 0.10595114834337349, 0.0968664697853916, 0.0982283626694277, 0.10037562358810237, 0.09829013318900603, 0.10525990681475905, 0.09981674745858427, 0.0963267131024097, 0.09779302757906627, 0.09746652626129515, 0.09457654838102414, 0.09508541980421681, 0.07992811088102414, 0.08013107115963858, 0.08037521178463858, 0.08039580195783136, 0.07769995999623491, 0.07940894437123491, 0.07954130977033136, 0.08137236445783136, 0.0812605892319277, 0.08151502494352414, 0.08139295463102414, 0.08027373164533136, 0.0795516048569277, 0.08029432181852414, 0.0800398861069277, 0.07941923945783136, 0.07966338008283136, 0.08029432181852414, 0.0801619564194277, 0.08041639213102414, 0.0804060970444277, 0.08102674369352414, 0.08066053275602414, 0.08041639213102414, 0.0797957454819277, 0.0802840267319277, 0.0802840267319277, 0.0795516048569277, 0.08053846244352414, 0.0801619564194277, 0.08078260306852414, 0.0799178157944277, 0.0797957454819277, 0.08041639213102414, 0.08029432181852414, 0.0797957454819277, 0.0796736751694277, 0.08041639213102414, 0.08066053275602414, 0.0799178157944277, 0.0802840267319277, 0.08066053275602414, 0.0800398861069277, 0.0797957454819277, 0.08005018119352414, 0.08029432181852414, 0.0804060970444277, 0.08029432181852414, 0.08066053275602414, 0.07966338008283136, 0.0799178157944277, 0.0805281673569277, 0.08005018119352414, 0.07980604056852414, 0.0801619564194277], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.09631947406525822, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 16, "valid_ratio": 0.15, "learning_rate": 0.0004967956482188723, "optimization": "adam", "nb_data_augmentation": 2, "learning_rate_decay_method": "discrete", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 1.6778335618243966e-08, "rotation_range": [0, 0], "momentum": 0.828101716949704}, "accuracy_valid_max": 0.9223000400037651, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.9198380435805723, "accuracy_valid_std": [0.010129153945490809, 0.01158282158270707, 0.01113419615920489, 0.010454922754753402, 0.011145435981578614, 0.009933112216774552, 0.012335968100013748, 0.010235589764627614, 0.009751822199471226, 0.010638969077965209, 0.011620561074010941, 0.009931524182163986, 0.010826966528618545, 0.014571448409167548, 0.011293795607945294, 0.01503342393840021, 0.013552929840749154, 0.011160616186739332, 0.01121507412040879, 0.016291740385862986, 0.011284104914688006, 0.010544847055422728, 0.012068197578721228, 0.012992911714724444, 0.01059947589485171, 0.010286127128055345, 0.016805665944597394, 0.01431837347844454, 0.011995370313417839, 0.014752218504018162, 0.008876921596924806, 0.009077063754963734, 0.014368829329718015, 0.007983726237919413, 0.009132952018746253, 0.014844197366484417, 0.007837042961857167, 0.009218495912475798, 0.011138687448667287, 0.010370085103920887, 0.009065495047282468, 0.007985471924788861, 0.007437729207624322, 0.007722889724845084, 0.005610112616108446, 0.007178058449493112, 0.012746518033880447, 0.0076273885153197146, 0.0090200882855213, 0.011294525675182566, 0.011512709611607197, 0.00933601897717466, 0.012268664251810236, 0.010981628177546764, 0.014437155006379255, 0.009995572536870405, 0.0057152534385765084, 0.014286768948271907, 0.012760960170620044, 0.009907787190011105, 0.010360772222240393, 0.009055220908592525, 0.007947249322944156, 0.009381837967969861, 0.008019572266138858, 0.014143996531885967, 0.008506125592845162, 0.012519878194926854, 0.012905941084904367, 0.01247169397486869, 0.008149877932455541, 0.006959985598144688, 0.009759698543278137, 0.011322956232210218, 0.0067565739648049765, 0.012403942174511962, 0.005415853812246715, 0.009540662541596547, 0.01660931451271178, 0.009319787568267424, 0.010101379701921893, 0.008102066962077282, 0.005583696410641267, 0.010376381707993088, 0.01075455983009814, 0.010597444283596408, 0.012851698973036048, 0.010329519654930562, 0.008887837136774487, 0.012863873617780401, 0.013440705943325307, 0.010800846742462006, 0.01190469996490616, 0.011307354288140536, 0.010563307130880122, 0.011725847503060426, 0.009548983193052132, 0.010452616381857797, 0.009018896389904222, 0.011177942029863781, 0.009110749345413836, 0.008676054642826177, 0.00993874853916114, 0.010333844187036482, 0.010540567742680463, 0.008730841796855095, 0.00984809716074512, 0.008720132201094763, 0.009535408499634098, 0.009381398979508935, 0.009692045371021682, 0.008855782935211625, 0.008883226017611207, 0.009693782421345087, 0.01026710813370805, 0.009304946658712348, 0.009655494118865904, 0.009884344865734945, 0.009690090928816878, 0.009451822671935723, 0.009212609237208735, 0.008774644147825942, 0.008959950576115835, 0.009814084814916708, 0.009656717198012272, 0.00859621651615731, 0.008583132542976952, 0.0096267757805581, 0.009693782421345087, 0.008666064604243344, 0.009509683078637426, 0.009814084814916708, 0.008664956191131244, 0.008946636001723605, 0.00993940784781698, 0.008825901004170784, 0.009605561587967246, 0.010841626721750666, 0.009286415362958832, 0.010059833546570699, 0.008356809023424509, 0.009077400104922164, 0.009287420612199374], "accuracy_valid": [0.6329610433923193, 0.7085563935429217, 0.7352397872740963, 0.7758700818900602, 0.7916480374623494, 0.8055243434676205, 0.8195330148719879, 0.815941500376506, 0.8330622293862951, 0.8393289956701807, 0.8345476633094879, 0.8470606057040663, 0.8392481057040663, 0.857752788497741, 0.8599088737763554, 0.8682714255459337, 0.8690744423004518, 0.8624723503388554, 0.8755147543298193, 0.8727277273155121, 0.8580263436558735, 0.8719335349209337, 0.8760942206325302, 0.8688405967620482, 0.8769795980798193, 0.8775693594691265, 0.8812932393637049, 0.8680272849209337, 0.8791974538780121, 0.8844670674887049, 0.8791562735316265, 0.8791356833584337, 0.8790650884789157, 0.8797460349209337, 0.8834390295557228, 0.8904485128012049, 0.8816888648343373, 0.8857480704066265, 0.8827169027673193, 0.8879556311182228, 0.8893292898155121, 0.8857789556664157, 0.8889218985316265, 0.8839773155120482, 0.8699995293674698, 0.8899999411709337, 0.8842626364834337, 0.8943444677146084, 0.8914956701807228, 0.8904588078878012, 0.8933576101280121, 0.8925134130271084, 0.8906926534262049, 0.8938767766378012, 0.895362210560994, 0.8977727315512049, 0.8934590902673193, 0.8902455525225903, 0.8977021366716867, 0.8938458913780121, 0.8998891072100903, 0.8965314382530121, 0.9008450795368976, 0.8949239340173193, 0.8945680181664157, 0.9006318241716867, 0.8972432699548193, 0.9052704960466867, 0.8982816029743976, 0.8994920110128012, 0.9006009389118976, 0.8949239340173193, 0.9008244893637049, 0.905616116810994, 0.8972432699548193, 0.9029511601091867, 0.8940488516566265, 0.9031335302146084, 0.9017716373305723, 0.8996243764118976, 0.901709866810994, 0.894740093185241, 0.9001832525414157, 0.9036732868975903, 0.9022069724209337, 0.9025334737387049, 0.9054234516189759, 0.9049145801957832, 0.9200718891189759, 0.9198689288403614, 0.9196247882153614, 0.9196041980421686, 0.9223000400037651, 0.9205910556287651, 0.9204586902296686, 0.9186276355421686, 0.9187394107680723, 0.9184849750564759, 0.9186070453689759, 0.9197262683546686, 0.9204483951430723, 0.9197056781814759, 0.9199601138930723, 0.9205807605421686, 0.9203366199171686, 0.9197056781814759, 0.9198380435805723, 0.9195836078689759, 0.9195939029555723, 0.9189732563064759, 0.9193394672439759, 0.9195836078689759, 0.9202042545180723, 0.9197159732680723, 0.9197159732680723, 0.9204483951430723, 0.9194615375564759, 0.9198380435805723, 0.9192173969314759, 0.9200821842055723, 0.9202042545180723, 0.9195836078689759, 0.9197056781814759, 0.9202042545180723, 0.9203263248305723, 0.9195836078689759, 0.9193394672439759, 0.9200821842055723, 0.9197159732680723, 0.9193394672439759, 0.9199601138930723, 0.9202042545180723, 0.9199498188064759, 0.9197056781814759, 0.9195939029555723, 0.9197056781814759, 0.9193394672439759, 0.9203366199171686, 0.9200821842055723, 0.9194718326430723, 0.9199498188064759, 0.9201939594314759, 0.9198380435805723], "seed": 423058430, "model": "residualv3", "loss_std": [0.3832026422023773, 0.27280929684638977, 0.2580924928188324, 0.24792709946632385, 0.23903894424438477, 0.23009243607521057, 0.221711203455925, 0.21606867015361786, 0.2101738154888153, 0.20111510157585144, 0.19427096843719482, 0.18894299864768982, 0.18524588644504547, 0.17533563077449799, 0.17068377137184143, 0.16595107316970825, 0.15462911128997803, 0.1490042805671692, 0.1469545066356659, 0.14210021495819092, 0.13576582074165344, 0.1303023397922516, 0.13178566098213196, 0.12611068785190582, 0.12106633931398392, 0.11869180202484131, 0.11650025844573975, 0.11011671274900436, 0.1119222491979599, 0.10435081273317337, 0.10896315425634384, 0.09986238181591034, 0.09912027418613434, 0.09780146926641464, 0.0956973284482956, 0.09312859177589417, 0.09092594683170319, 0.09119464457035065, 0.08596234023571014, 0.0848478451371193, 0.08344397693872452, 0.08335855603218079, 0.08240905404090881, 0.08383847028017044, 0.07576917111873627, 0.07811538875102997, 0.07581540197134018, 0.07535212486982346, 0.07696007937192917, 0.07097884267568588, 0.07306579500436783, 0.07057886570692062, 0.06920404732227325, 0.06679318845272064, 0.06980638206005096, 0.0662870779633522, 0.06496114283800125, 0.06538848578929901, 0.0627070963382721, 0.06366060674190521, 0.06436198949813843, 0.06271547824144363, 0.06546657532453537, 0.060160327702760696, 0.06256686896085739, 0.06083936244249344, 0.06289352476596832, 0.059045907109975815, 0.06070190295577049, 0.05541707202792168, 0.06051598861813545, 0.05581517517566681, 0.05725036934018135, 0.05375870317220688, 0.055065419524908066, 0.053653817623853683, 0.05675654113292694, 0.05436943843960762, 0.055786196142435074, 0.05474833771586418, 0.05501653999090195, 0.051864251494407654, 0.05458156391978264, 0.052398376166820526, 0.04793154448270798, 0.055128805339336395, 0.04792702570557594, 0.05028386414051056, 0.03418223559856415, 0.02024967037141323, 0.01916792429983616, 0.016220835968852043, 0.016298256814479828, 0.013826088979840279, 0.013164196163415909, 0.013121475465595722, 0.011213087476789951, 0.013291039504110813, 0.011282361112535, 0.011645497754216194, 0.010310063138604164, 0.010216405615210533, 0.0103690130636096, 0.010328443720936775, 0.01047810260206461, 0.009290766902267933, 0.010438460856676102, 0.01075983140617609, 0.010315719060599804, 0.010411196388304234, 0.010127752088010311, 0.011052924208343029, 0.0111626498401165, 0.010211044922471046, 0.009515197016298771, 0.010997699573636055, 0.010093132965266705, 0.009565416723489761, 0.01052801962941885, 0.009335441514849663, 0.011077743954956532, 0.009872259572148323, 0.009958891198039055, 0.009490075521171093, 0.011816296726465225, 0.00960866454988718, 0.009714244864881039, 0.010646732524037361, 0.009614553302526474, 0.00915227085351944, 0.009517079219222069, 0.009674595668911934, 0.00949779525399208, 0.009420479647815228, 0.009808299131691456, 0.010317837819457054, 0.008423218503594398, 0.008863922208547592, 0.009758069179952145, 0.009873230941593647, 0.010850131511688232, 0.008960096165537834, 0.008297129534184933]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:38 2016", "state": "available"}], "summary": "cb6b7bfb3ccfc62eb71c8cb00354b5dd"}