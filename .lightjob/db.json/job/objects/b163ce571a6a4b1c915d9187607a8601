{"content": {"hp_model": {"f0": 64, "f1": 32, "f2": 32, "f3": 64, "nonlin": "rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.9950326681137085, 1.7574282884597778, 1.636587142944336, 1.542497158050537, 1.4655487537384033, 1.4039709568023682, 1.3514471054077148, 1.3068852424621582, 1.268028974533081, 1.2302801609039307, 1.1980003118515015, 1.1697957515716553, 1.144008755683899, 1.118615984916687, 1.0987507104873657, 1.075714111328125, 1.0587010383605957, 1.0405527353286743, 1.0260798931121826, 1.009546160697937, 0.9936087727546692, 0.9820034503936768, 0.9698399305343628, 0.9567186832427979, 0.9472306966781616, 0.9342788457870483, 0.925683856010437, 0.9176076650619507, 0.9067919254302979, 0.8978896737098694, 0.8899786472320557, 0.8821560740470886, 0.8780183792114258, 0.8691655993461609, 0.8629356026649475, 0.8580679893493652, 0.8495000600814819, 0.8434379696846008, 0.8385134339332581, 0.8322596549987793, 0.8261072039604187, 0.8230637311935425, 0.8173078298568726, 0.813510000705719, 0.8075572848320007, 0.8064637184143066, 0.7995359897613525, 0.7967606782913208, 0.7925164699554443, 0.7885133624076843, 0.7857417464256287, 0.784280002117157, 0.780815839767456, 0.7786422967910767, 0.7738426327705383, 0.7706738114356995, 0.7700720429420471, 0.7654328942298889, 0.7633001208305359, 0.7607499957084656, 0.760650098323822, 0.756519615650177, 0.7551107406616211, 0.7525733709335327, 0.7505190372467041, 0.7481057643890381, 0.7465446591377258, 0.7455448508262634, 0.7419280409812927, 0.7416015863418579, 0.7398836612701416, 0.7407090663909912, 0.7386350035667419, 0.736833930015564, 0.7354785203933716, 0.7332485318183899, 0.7322015166282654, 0.7329471111297607, 0.7299935221672058, 0.7290868759155273, 0.7296063899993896, 0.7280032634735107, 0.7265273332595825, 0.7247523665428162, 0.7243530750274658, 0.7232469916343689, 0.7225663065910339, 0.7195004820823669, 0.7208797931671143, 0.7212131023406982, 0.7178284525871277, 0.7194727659225464, 0.718131422996521, 0.7177683711051941, 0.7181282043457031, 0.7197563052177429, 0.7151191234588623, 0.7156864404678345, 0.7152154445648193, 0.7167349457740784, 0.7133066654205322, 0.7135665416717529, 0.7141559720039368, 0.712866485118866, 0.7116617560386658, 0.7126129269599915, 0.71110600233078, 0.7119646668434143, 0.7115501165390015, 0.7099652290344238, 0.7094875574111938, 0.7094787359237671, 0.712543785572052, 0.7091345191001892, 0.7092989683151245, 0.7091721892356873, 0.7071645855903625, 0.709423840045929, 0.7094681859016418, 0.7081322073936462, 0.706250786781311, 0.7073648571968079, 0.7065470814704895, 0.7079492211341858, 0.7054482698440552, 0.7074997425079346, 0.7045915126800537, 0.7062097191810608, 0.7073842883110046, 0.7049395442008972, 0.7080722451210022, 0.7052550315856934, 0.7061015963554382, 0.707251787185669, 0.7053478360176086, 0.707024097442627, 0.7056658864021301, 0.7059489488601685, 0.705879271030426, 0.705757737159729, 0.7046108841896057, 0.7061175107955933, 0.7047793865203857, 0.7048909664154053, 0.7044308185577393, 0.7030015587806702, 0.7032642364501953, 0.7049614191055298, 0.7029520273208618, 0.7046501040458679, 0.7054890394210815, 0.7049703598022461, 0.7032232284545898, 0.703728973865509, 0.7036100625991821, 0.7029401063919067, 0.7040778398513794, 0.7038418650627136, 0.7012353539466858, 0.7021834254264832, 0.7032366394996643, 0.7029239535331726, 0.7022033929824829, 0.7043572068214417, 0.7044899463653564, 0.704037606716156, 0.702725350856781, 0.7043845057487488, 0.7037114500999451, 0.7044270634651184, 0.7017771005630493, 0.704300045967102, 0.7026140689849854, 0.7025622129440308, 0.7015500664710999, 0.7037086486816406, 0.7043757438659668], "moving_avg_accuracy_train": [0.04992235294117646, 0.10082894117647057, 0.15021192941176467, 0.19781897176470586, 0.24300648635294114, 0.2860281906588235, 0.3269006657105882, 0.3651753050218823, 0.40136836275498816, 0.43529505589125406, 0.46677496206683455, 0.4959209952719158, 0.5230512486859007, 0.5481767120526048, 0.571582570259109, 0.5933678426449627, 0.6135345877922311, 0.632305834895361, 0.6496376043470014, 0.6655914909711248, 0.6806652830504829, 0.6945516959219052, 0.7075624086826559, 0.7194485207555668, 0.7304119039741277, 0.7407424782825973, 0.7503341128072788, 0.7590607015265509, 0.7672722784327194, 0.7750768152953298, 0.7822373690599145, 0.7888489262715701, 0.7951475630561777, 0.8009433949858541, 0.8062984672519746, 0.8112639146444242, 0.815944582003511, 0.8202865943913953, 0.8242814643640205, 0.8281192002805596, 0.8317896331936802, 0.8351636110507827, 0.8382919558280575, 0.841399230833487, 0.8443840136324912, 0.8470279652104185, 0.8497651686893766, 0.8519980635851448, 0.854275904285454, 0.8565589020922026, 0.8585618354123942, 0.8605527106946842, 0.8624456749193334, 0.864245813309753, 0.8658894672728954, 0.8673781676044293, 0.8690191743733982, 0.8705101981125289, 0.8719321194777466, 0.8732589075299719, 0.8746718403063866, 0.8759317150992773, 0.8771667788834672, 0.8784006892304146, 0.8795347379544319, 0.8806565582766358, 0.8816756083313252, 0.8826421651452515, 0.8835755956895499, 0.8844839184735361, 0.8852802325085354, 0.8861545621988584, 0.8869579295083843, 0.8877209600869576, 0.8885065111370853, 0.8891688011998474, 0.8897695681386861, 0.8904114348542292, 0.8909444090158651, 0.8915346739966316, 0.8921176771852037, 0.8926259094666834, 0.8931162596964857, 0.8935669866680135, 0.8939561703541533, 0.8943676121422673, 0.8948484979868642, 0.8951895305411189, 0.8955999892517129, 0.8959835197383063, 0.8962839912938874, 0.8966179451056752, 0.8969702682421664, 0.8972708884767733, 0.8975555643349783, 0.8978141255485392, 0.8980397718172146, 0.8983322652237284, 0.8985719798778262, 0.8988136054194554, 0.8990804801716276, 0.8993089027427, 0.8995615418801947, 0.8996783288686457, 0.8998987312758988, 0.9001206228541913, 0.9002803252746544, 0.9004640574530713, 0.900636475237176, 0.9008292983016938, 0.9009581331774067, 0.9010952610361367, 0.9011974996384053, 0.9013412790863295, 0.9014447982365201, 0.9014885537069857, 0.9015844042186401, 0.9016989049732467, 0.9017972497700397, 0.9018504659695064, 0.9019477723137322, 0.9020565244941237, 0.9020673426329465, 0.9021688436637695, 0.9022484298856278, 0.9023200574853003, 0.9023680517367704, 0.9024300700925051, 0.9025305924950193, 0.9025975332455174, 0.9026507210974363, 0.9026891783994573, 0.9027261429124527, 0.902766469797678, 0.9028145287002631, 0.9028483699478839, 0.9028882388354484, 0.9029170620107271, 0.9029382969861249, 0.9029456437581007, 0.9030651970293494, 0.9030645596793556, 0.9030569272408319, 0.9030853521638075, 0.9031297581238973, 0.9031367823115075, 0.9031878099627096, 0.9031349113193798, 0.9031461260697948, 0.9031821016981094, 0.903261538587122, 0.9032577376695862, 0.9032331403732158, 0.9031921792770707, 0.9032423731140695, 0.903289900508545, 0.9032597339871022, 0.9033572900001567, 0.9033462668824941, 0.9033151696060093, 0.9033177702924672, 0.90331305208675, 0.9033582174663103, 0.9033918074843852, 0.9034338032065349, 0.903473952297646, 0.9035053805972932, 0.9035383719493286, 0.9035280641661605, 0.9035046695142503, 0.9035142025628253, 0.903583958777131, 0.9035290923111826, 0.9035220654330055, 0.9035110353602932, 0.9035434612360286, 0.9035985268771316], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.049373333333333325, 0.099436, 0.1475857333333333, 0.19361382666666663, 0.23703911066666664, 0.2787485329333333, 0.3181803463066666, 0.35500231167599994, 0.3895287471750666, 0.42190920579089325, 0.4516649518784706, 0.47940512335729024, 0.5051046110215612, 0.5286741499194051, 0.5504334015941312, 0.5703767281013847, 0.5890190552912462, 0.606410483095455, 0.6222761014525761, 0.6368084913073184, 0.6500343088432533, 0.6625642112922613, 0.6744411234963684, 0.6852370111467315, 0.6949666433653917, 0.7041766456955192, 0.7125989811259673, 0.7202190830133706, 0.7274238413787002, 0.7341347905741635, 0.7403746448500805, 0.7458171803650725, 0.7509021289952319, 0.755585249429042, 0.7599200578194711, 0.7637147187041907, 0.7670899135004383, 0.7704475888170611, 0.7737228299353549, 0.7765905469418194, 0.7791448255809709, 0.7817503430228738, 0.7840686420539198, 0.7863284445151945, 0.788455600063675, 0.7904100400573075, 0.7921690360515767, 0.7935654657797524, 0.7950355858684439, 0.7965720272815996, 0.798008157886773, 0.7992740087647624, 0.8006666078882861, 0.8018532804327908, 0.8030279523895117, 0.8038584904838939, 0.8050059747688378, 0.8060120439586207, 0.8069175062294253, 0.8077324222731493, 0.8087591800458344, 0.8096432620412509, 0.8105189358371259, 0.8115203755867466, 0.8123150046947386, 0.8130968375585981, 0.8138271538027383, 0.8145511050891311, 0.8153226612468847, 0.8160170617888629, 0.8164953556099765, 0.8171524867156456, 0.8178505713774143, 0.8183855142396729, 0.8187869628157056, 0.8193082665341351, 0.8197107732140549, 0.8201530292259828, 0.8204843929700512, 0.820915953673046, 0.8213976916390747, 0.8216312558085006, 0.8218681302276505, 0.8221213172048855, 0.8223625188177301, 0.8225396002692905, 0.8228456402423614, 0.823147742884792, 0.8232329685963127, 0.8235230050700147, 0.8237307045630132, 0.8239976341067119, 0.8242512040293741, 0.8243860836264367, 0.8245741419304597, 0.8248767277374137, 0.8249223882970057, 0.8250568161339717, 0.8251111345205746, 0.8253733544018504, 0.8255293522949987, 0.8258964170654989, 0.8261201086922823, 0.826308097823054, 0.8264906213740819, 0.8266015592366737, 0.8267947366463396, 0.827048596315039, 0.8272104033502018, 0.8272493630151816, 0.8271777600469968, 0.8271933173756305, 0.8273539856380674, 0.8276052537409273, 0.8277380617001678, 0.8278709221968178, 0.8280704966438027, 0.8281701136460892, 0.8282064356148136, 0.8283324587199988, 0.8283258795146655, 0.828333291563199, 0.8285532957402124, 0.8286446328328578, 0.8287935028829053, 0.8287408192612814, 0.8289600706684866, 0.8290107302683046, 0.8291229905748074, 0.8291706915173267, 0.8292136223655939, 0.8292789267957013, 0.8292310341161311, 0.8292412640378513, 0.8292638043007328, 0.8292307572039928, 0.8292943481502602, 0.8293915800019008, 0.8293457553350441, 0.829264513134873, 0.8293247284880524, 0.8291789223059138, 0.8291010300753225, 0.8291242604011236, 0.8291051676943445, 0.8291679842582435, 0.8291978524990857, 0.8292114005825105, 0.8292102605242595, 0.8291959011385003, 0.829262977691317, 0.8293500132555186, 0.8293883452633001, 0.8294361774036367, 0.8294525596632731, 0.8294139703636125, 0.8293792399939179, 0.8293746493278594, 0.8293838510617401, 0.8294587992888993, 0.8294329193600094, 0.8294362940906751, 0.8294393313482742, 0.8292953982134468, 0.8293791917254354, 0.8293212725528918, 0.8292291452976026, 0.8293328974345091, 0.8293196076910582, 0.8293209802552857, 0.829348882229757, 0.8292673273401147, 0.8292739279394366, 0.8292265351454928, 0.8291705482976102, 0.8292268268011825, 0.8292508107877309], "moving_var_accuracy_train": [0.022430171908650515, 0.043510481249605534, 0.06110754886806976, 0.07539466831561539, 0.08623240475601165, 0.09426696765285547, 0.09987530383928409, 0.10307230558504263, 0.1045545118791858, 0.10445824525572837, 0.10293128116556567, 0.10028357431333439, 0.09687967273476429, 0.09287330564581214, 0.08851648286667776, 0.08393621741634259, 0.0792028741632222, 0.07445382420716082, 0.06971195387736807, 0.06503149697536972, 0.06057332014669822, 0.05625148029394883, 0.05214984008343879, 0.04820637301698309, 0.044467497659658003, 0.040981234783577514, 0.037711106380915314, 0.03462537589890173, 0.031769708266584824, 0.029140934600684947, 0.02668830291255602, 0.02441288682016738, 0.02232865356625036, 0.020398113219438827, 0.018616393088273297, 0.016976654789710627, 0.015476167133077368, 0.014098228063958489, 0.01283203613244628, 0.011681386471887501, 0.010634496524626216, 0.00967350041138556, 0.00879422923965652, 0.008001702737325173, 0.00728171281880774, 0.006616455856444787, 0.0060222408167672934, 0.0054648891116304945, 0.004965097224771305, 0.00451549621316475, 0.004100052268814475, 0.0037257193014397266, 0.0033853971932979706, 0.0030760219579901355, 0.002792734147346104, 0.0025334067907054783, 0.002304302240577144, 0.002093880382635293, 0.0019026890876915267, 0.0017282634777421254, 0.0015734045412439124, 0.001430349647563375, 0.0013010431257661952, 0.0011846416258883103, 0.001077752061875488, 0.0009813031832057255, 0.0008925190320108145, 0.0008116752174806592, 0.0007383493289618557, 0.0006719398485848457, 0.0006104529081073934, 0.0005562876889630756, 0.0005064675113729024, 0.0004610607012101539, 0.00042050844516035014, 0.0003824052537894153, 0.00034741301664368933, 0.0003163796509040198, 0.0002872982389263613, 0.00026170412976139794, 0.00023859275124622566, 0.00021705817658904494, 0.00019751634906094452, 0.00017959310738061455, 0.00016299697211656953, 0.00014822083400997115, 0.00013548001136877614, 0.00012297873905945227, 0.00011219715233142934, 0.0001023012978056057, 9.28837164264653e-05, 8.459907111948676e-05, 7.725634834010175e-05, 7.034406623518748e-05, 6.403902270987166e-05, 5.8236805549307715e-05, 5.287137114148148e-05, 4.8354205563019906e-05, 4.403595304522072e-05, 4.0157803862006815e-05, 3.678302267592844e-05, 3.357431224711412e-05, 3.0791319826549336e-05, 2.783494064993775e-05, 2.5488641575050523e-05, 2.3382900270199617e-05, 2.1274154011095984e-05, 1.945055623045882e-05, 1.7773051637892876e-05, 1.6330373081993513e-05, 1.484672160059398e-05, 1.353128588729326e-05, 1.2272231884708635e-05, 1.1231061463046192e-05, 1.020440124684719e-05, 9.201191992923494e-06, 8.363758678890786e-06, 7.645376616250997e-06, 6.967884246132166e-06, 6.29658349648999e-06, 5.752141868480354e-06, 5.283371012291436e-06, 4.7560872002106235e-06, 4.373200613512736e-06, 3.992886252548628e-06, 3.639772244607372e-06, 3.296526053714099e-06, 3.0014899363750424e-06, 2.792283723402581e-06, 2.553384927757524e-06, 2.3235069633074393e-06, 2.1044669436853614e-06, 1.906317626305746e-06, 1.7303221827229521e-06, 1.5780768875098474e-06, 1.4305762691236218e-06, 1.301824395971974e-06, 1.1791189352730877e-06, 1.0652653593671153e-06, 9.592245989565763e-07, 9.91939001057424e-07, 8.927487568868126e-07, 8.039981682585075e-07, 7.308701376481939e-07, 6.755301275068833e-07, 6.084211676604539e-07, 5.710134415793177e-07, 5.390964956165769e-07, 4.863187816967467e-07, 4.493351160207672e-07, 4.611935784426713e-07, 4.1520424336542415e-07, 3.7912906192747076e-07, 3.563164583114062e-07, 3.43359603934238e-07, 3.2935332257139424e-07, 3.0460816145784817e-07, 3.5980192645983114e-07, 3.249153159209072e-07, 3.0112714977170983e-07, 2.710753069250091e-07, 2.4416812941922555e-07, 2.3811052007473212e-07, 2.2445407189568117e-07, 2.1788143081599664e-07, 2.106008333879119e-07, 1.9843039221752624e-07, 1.8838321677787677e-07, 1.7050114864466387e-07, 1.583768214221798e-07, 1.4335705041615475e-07, 1.7281471028290965e-07, 1.8262620102561925e-07, 1.6480797407529756e-07, 1.4942213920411848e-07, 1.439428620385677e-07, 1.5683859930549513e-07], "duration": 216989.904355, "accuracy_train": [0.49922352941176473, 0.5589882352941177, 0.5946588235294118, 0.6262823529411765, 0.6496941176470589, 0.6732235294117647, 0.6947529411764706, 0.7096470588235294, 0.7271058823529412, 0.7406352941176471, 0.7500941176470588, 0.758235294117647, 0.7672235294117647, 0.7743058823529412, 0.782235294117647, 0.789435294117647, 0.7950352941176471, 0.8012470588235294, 0.8056235294117647, 0.8091764705882353, 0.8163294117647059, 0.8195294117647058, 0.8246588235294118, 0.8264235294117647, 0.8290823529411765, 0.8337176470588236, 0.8366588235294118, 0.8376, 0.8411764705882353, 0.8453176470588235, 0.8466823529411764, 0.8483529411764706, 0.851835294117647, 0.8531058823529412, 0.8544941176470588, 0.8559529411764706, 0.8580705882352941, 0.8593647058823529, 0.8602352941176471, 0.8626588235294118, 0.8648235294117647, 0.8655294117647059, 0.8664470588235295, 0.8693647058823529, 0.8712470588235294, 0.8708235294117647, 0.8744, 0.8720941176470588, 0.8747764705882353, 0.8771058823529412, 0.8765882352941177, 0.8784705882352941, 0.8794823529411765, 0.8804470588235294, 0.8806823529411765, 0.8807764705882353, 0.8837882352941177, 0.8839294117647059, 0.8847294117647059, 0.8852, 0.8873882352941177, 0.8872705882352941, 0.8882823529411765, 0.8895058823529411, 0.8897411764705883, 0.8907529411764706, 0.8908470588235294, 0.8913411764705882, 0.8919764705882353, 0.8926588235294117, 0.8924470588235294, 0.8940235294117647, 0.8941882352941176, 0.8945882352941177, 0.8955764705882353, 0.8951294117647058, 0.8951764705882352, 0.8961882352941176, 0.8957411764705883, 0.8968470588235294, 0.897364705882353, 0.8972, 0.8975294117647059, 0.8976235294117647, 0.8974588235294118, 0.8980705882352941, 0.8991764705882352, 0.8982588235294118, 0.8992941176470588, 0.8994352941176471, 0.8989882352941176, 0.8996235294117647, 0.9001411764705882, 0.8999764705882353, 0.9001176470588236, 0.9001411764705882, 0.9000705882352941, 0.9009647058823529, 0.9007294117647059, 0.9009882352941176, 0.9014823529411765, 0.901364705882353, 0.9018352941176471, 0.9007294117647059, 0.9018823529411765, 0.9021176470588236, 0.9017176470588235, 0.9021176470588236, 0.9021882352941176, 0.9025647058823529, 0.9021176470588236, 0.9023294117647059, 0.9021176470588236, 0.9026352941176471, 0.9023764705882353, 0.9018823529411765, 0.9024470588235294, 0.9027294117647059, 0.9026823529411765, 0.9023294117647059, 0.9028235294117647, 0.903035294117647, 0.902164705882353, 0.9030823529411764, 0.9029647058823529, 0.9029647058823529, 0.9028, 0.9029882352941176, 0.9034352941176471, 0.9032, 0.9031294117647058, 0.903035294117647, 0.9030588235294118, 0.9031294117647058, 0.9032470588235294, 0.9031529411764706, 0.9032470588235294, 0.9031764705882352, 0.9031294117647058, 0.9030117647058824, 0.9041411764705882, 0.9030588235294118, 0.9029882352941176, 0.9033411764705882, 0.9035294117647059, 0.9032, 0.9036470588235294, 0.9026588235294117, 0.9032470588235294, 0.9035058823529412, 0.9039764705882353, 0.9032235294117648, 0.9030117647058824, 0.9028235294117647, 0.9036941176470589, 0.9037176470588235, 0.9029882352941176, 0.904235294117647, 0.9032470588235294, 0.903035294117647, 0.9033411764705882, 0.9032705882352942, 0.9037647058823529, 0.9036941176470589, 0.9038117647058823, 0.9038352941176471, 0.9037882352941177, 0.9038352941176471, 0.9034352941176471, 0.9032941176470588, 0.9036, 0.9042117647058824, 0.903035294117647, 0.9034588235294118, 0.9034117647058824, 0.9038352941176471, 0.9040941176470588], "end": "2016-02-06 13:05:20.226000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0], "moving_var_accuracy_valid": [0.0219395344, 0.042302016304000004, 0.058937386054239996, 0.0721109158319344, 0.08187162186306686, 0.08934154282913215, 0.09440119969940346, 0.09716379393240657, 0.09817608727360681, 0.0977949254477875, 0.09598407273006412, 0.09331131948012655, 0.08992436052796805, 0.08593163294988403, 0.08159965495589229, 0.07701931590987736, 0.07244521158637428, 0.06792284627735788, 0.06339602226230648, 0.05895713323008781, 0.0546357201525236, 0.050585134235706146, 0.04679617020367217, 0.04316551389473832, 0.039700954193258, 0.03649427606022079, 0.033483270061125545, 0.030657536629982652, 0.02805895985490947, 0.025658395421355333, 0.023442977911681918, 0.02136527085600126, 0.019461454093543374, 0.017712693237167166, 0.016110538987486062, 0.014629080149807641, 0.013268699594040428, 0.01204329548642311, 0.010935510777227467, 0.009915973906967213, 0.008983095570568317, 0.008145884503772028, 0.007379666646970963, 0.006687660346749715, 0.0060596174286216276, 0.005488034206957856, 0.004967077388432769, 0.004487919793461088, 0.004058579091791545, 0.003673967052556927, 0.003325132587337275, 0.0030070407346113053, 0.002723790652019727, 0.002464085312368686, 0.002230095468984977, 0.0020132940638224587, 0.0018238151390979527, 0.001650543202119831, 0.0014928676392225036, 0.0013495576687251245, 0.0012240899855665347, 0.0011087153957814587, 0.0010047450975743505, 0.0009132965219659997, 0.0008276497885428134, 0.0007503861733316285, 0.000680147812346561, 0.0006168499802975334, 0.000560522672408887, 0.0005088101341822948, 0.00045998800557790526, 0.0004178755966304547, 0.00038047393672238075, 0.00034500201784307523, 0.00031195226469155594, 0.00028320285632403553, 0.0002563406753380527, 0.00023246692122502434, 0.00021020844648046914, 0.00019086380359574715, 0.00017386606644739392, 0.00015697042979381085, 0.00014177837222845835, 0.00012817746781458488, 0.00011588332499547677, 0.00010457721306030949, 9.496243594033396e-05, 8.62875864053721e-05, 7.772419856197295e-05, 7.070886911047345e-05, 6.402623391395271e-05, 5.8264872954250115e-05, 5.301706500993506e-05, 4.787909106027544e-05, 4.340947528565595e-05, 3.9892551292220543e-05, 3.592206014331875e-05, 3.2492491719149346e-05, 2.9269796931342613e-05, 2.696165063343535e-05, 2.4484503654092158e-05, 2.324868220036378e-05, 2.137415547536466e-05, 1.955479914742313e-05, 1.789915285279945e-05, 1.6220002451727464e-05, 1.4933859811002e-05, 1.40204764124312e-05, 1.2854062420841554e-05, 1.1582316878215459e-05, 1.047022805586981e-05, 9.42538352455077e-06, 8.715173787086273e-06, 8.411877344011082e-06, 7.729431195948857e-06, 7.115355280484432e-06, 6.7622893914398835e-06, 6.175372376596629e-06, 5.5697087076451254e-06, 5.155674244245554e-06, 4.640496393306347e-06, 4.176941200146872e-06, 4.1948636212624145e-06, 3.850459439572448e-06, 3.6648741218256385e-06, 3.3233667855297275e-06, 3.423670723029846e-06, 3.1044012062103456e-06, 2.9073824733343728e-06, 2.637122645255965e-06, 2.389997900326946e-06, 2.189380127618967e-06, 1.99108549366473e-06, 1.7929188059838755e-06, 1.6181994964424103e-06, 1.4662085422246216e-06, 1.3559819640267963e-06, 1.3054700643853488e-06, 1.193822158779605e-06, 1.133842598699377e-06, 1.053091337656076e-06, 1.1391171886389169e-06, 1.0798102660535399e-06, 9.766860717796056e-07, 8.822982476709946e-07, 8.295817092044519e-07, 7.546525445831434e-07, 6.808392452051879e-07, 6.127670182800112e-07, 5.533460440864675e-07, 5.385048151176846e-07, 5.528310385289825e-07, 5.107720200611137e-07, 4.802860408976729e-07, 4.3467284268503265e-07, 4.0460776485122736e-07, 3.750027755782114e-07, 3.3769216595413245e-07, 3.0468499651642536e-07, 3.2477162765371524e-07, 2.983224013624731e-07, 2.685926604898218e-07, 2.418164188443527e-07, 4.0408550267116133e-07, 4.268691262665765e-07, 4.1437388857307157e-07, 4.493233802198176e-07, 5.012715954116443e-07, 4.5273399139939756e-07, 4.0747754765248583e-07, 3.7373647450188087e-07, 3.962236272729006e-07, 3.5699337574828133e-07, 3.4150873043352336e-07, 3.3556860161264965e-07, 3.305171711304339e-07, 3.02642538514183e-07], "accuracy_test": 0.8277, "start": "2016-02-04 00:48:50.322000", "learning_rate_per_epoch": [0.0003333203203510493, 0.00031988913542591035, 0.0003069991653319448, 0.0002946286113001406, 0.0002827565185725689, 0.0002713628055062145, 0.0002604282053653151, 0.0002499342372175306, 0.00023986311862245202, 0.0002301978092873469, 0.0002209219674114138, 0.00021201989147812128, 0.00020347653480712324, 0.00019527743279468268, 0.00018740871746558696, 0.00017985707381740212, 0.00017260971071664244, 0.00016565439000260085, 0.00015897932462394238, 0.00015257323684636503, 0.0001464252854930237, 0.00014052506594453007, 0.0001348625955870375, 0.00012942829926032573, 0.00012421298015397042, 0.00011920780525542796, 0.00011440431262599304, 0.00010979438229696825, 0.00010537020716583356, 0.0001011243075481616, 9.704949479782954e-05, 9.313887858297676e-05, 8.938583778217435e-05, 8.578402776038274e-05, 8.232735126512125e-05, 7.900996570242569e-05, 7.58262540330179e-05, 7.27708320482634e-05, 6.98385265422985e-05, 6.702437531203032e-05, 6.432361988117918e-05, 6.17316909483634e-05, 5.9244208387099206e-05, 5.685695941792801e-05, 5.4565902246395126e-05, 5.236716606304981e-05, 5.0257025577593595e-05, 4.8231915570795536e-05, 4.6288405428640544e-05, 4.4423210056265816e-05, 4.2633171688066795e-05, 4.091526352567598e-05, 3.926657882402651e-05, 3.768432725337334e-05, 3.616583489929326e-05, 3.470852971076965e-05, 3.3309945138171315e-05, 3.1967716495273635e-05, 3.067957368330099e-05, 2.9443337552947924e-05, 2.8256916266400367e-05, 2.7118301659356803e-05, 2.6025567422038876e-05, 2.4976865461212583e-05, 2.3970420443220064e-05, 2.30045297939796e-05, 2.2077560061006807e-05, 2.1187943275435828e-05, 2.0334173314040527e-05, 1.951480589923449e-05, 1.872845496109221e-05, 1.7973790818359703e-05, 1.724953472148627e-05, 1.6554462490603328e-05, 1.5887399058556184e-05, 1.5247214832925238e-05, 1.4632826605520677e-05, 1.4043195733393077e-05, 1.3477323591359891e-05, 1.2934253390994854e-05, 1.2413066542649176e-05, 1.1912880836462136e-05, 1.1432850442361087e-05, 1.0972163181577343e-05, 1.0530038707656786e-05, 1.0105730325449258e-05, 9.698518624645658e-06, 9.307716027251445e-06, 8.932660421123728e-06, 8.572717888455372e-06, 8.22727906779619e-06, 7.895760063547641e-06, 7.5775997174787335e-06, 7.272259608726017e-06, 6.979223144298885e-06, 6.697994649584871e-06, 6.428098458854947e-06, 6.1690775510214735e-06, 5.920494004385546e-06, 5.681927177647594e-06, 5.452973255160032e-06, 5.233245246927254e-06, 5.022371169616235e-06, 4.819994501303881e-06, 4.6257723624876235e-06, 4.439376425580122e-06, 4.260491550667211e-06, 4.088814876013203e-06, 3.924055818060879e-06, 3.7659356166841462e-06, 3.614186880440684e-06, 3.4685529044509167e-06, 3.3287872156506637e-06, 3.194653345417464e-06, 3.065924602196901e-06, 2.9423829346342245e-06, 2.823819386321702e-06, 2.7100334136775928e-06, 2.6008324311987963e-06, 2.496031584087177e-06, 2.3954537482495653e-06, 2.2989286208030535e-06, 2.206293174822349e-06, 2.117390295097721e-06, 2.032069915003376e-06, 1.950187424881733e-06, 1.8716044678512844e-06, 1.796188030311896e-06, 1.7238104419448064e-06, 1.654349375712627e-06, 1.5876871657383163e-06, 1.5237111483656918e-06, 1.4623130937252427e-06, 1.4033890920472913e-06, 1.346839439975156e-06, 1.292568413191475e-06, 1.240484266418207e-06, 1.1904988923561177e-06, 1.1425275943111046e-06, 1.0964893135678722e-06, 1.052306174642581e-06, 1.0099033715960104e-06, 9.692092817203957e-07, 9.301548971052398e-07, 8.926742225412454e-07, 8.567038207729638e-07, 8.22182869342214e-07, 7.89052933214407e-07, 7.57257964778546e-07, 7.267441901603888e-07, 6.974599955356098e-07, 6.693557565995434e-07, 6.423840090974409e-07, 6.16499107763957e-07, 5.916572263231501e-07, 5.67816357488482e-07, 5.449361424325616e-07, 5.229778707871446e-07, 5.01904423799715e-07, 4.81680160646647e-07, 4.622708047463675e-07, 4.436435574461939e-07, 4.257668990703678e-07, 4.086105889200553e-07, 3.9214558000821853e-07, 3.763440474813251e-07, 3.611792180890916e-07, 3.466254838713212e-07, 3.326581747842283e-07, 3.192536723872763e-07, 3.0638932457804913e-07, 2.9404333190541365e-07, 2.8219483283464797e-07, 2.708237616388942e-07, 2.5991090524257743e-07, 2.494377611128584e-07, 2.393866509464715e-07], "accuracy_train_first": 0.49922352941176473, "accuracy_train_last": 0.9040941176470588, "batch_size_eval": 1024, "accuracy_train_std": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "accuracy_test_std": 0, "error_valid": [0.5062666666666666, 0.44999999999999996, 0.4190666666666667, 0.39213333333333333, 0.3721333333333333, 0.34586666666666666, 0.3269333333333333, 0.3136, 0.2997333333333333, 0.2866666666666666, 0.2805333333333333, 0.27093333333333336, 0.26359999999999995, 0.2592, 0.25373333333333337, 0.2501333333333333, 0.24319999999999997, 0.23706666666666665, 0.23493333333333333, 0.23240000000000005, 0.23093333333333332, 0.22466666666666668, 0.21866666666666668, 0.21760000000000002, 0.2174666666666667, 0.2129333333333333, 0.2116, 0.21120000000000005, 0.20773333333333333, 0.2054666666666667, 0.20346666666666668, 0.20520000000000005, 0.20333333333333337, 0.2022666666666667, 0.20106666666666662, 0.2021333333333334, 0.20253333333333334, 0.19933333333333336, 0.19679999999999997, 0.1976, 0.19786666666666664, 0.19479999999999997, 0.19506666666666672, 0.19333333333333336, 0.19240000000000002, 0.19199999999999995, 0.19199999999999995, 0.19386666666666663, 0.1917333333333333, 0.1896, 0.18906666666666672, 0.18933333333333335, 0.18679999999999997, 0.18746666666666667, 0.1864, 0.18866666666666665, 0.18466666666666665, 0.18493333333333328, 0.18493333333333328, 0.18493333333333328, 0.18200000000000005, 0.1824, 0.18159999999999998, 0.17946666666666666, 0.18053333333333332, 0.17986666666666662, 0.17959999999999998, 0.1789333333333334, 0.1777333333333333, 0.1777333333333333, 0.17920000000000003, 0.1769333333333334, 0.17586666666666662, 0.17679999999999996, 0.17759999999999998, 0.17600000000000005, 0.17666666666666664, 0.17586666666666662, 0.17653333333333332, 0.17520000000000002, 0.17426666666666668, 0.17626666666666668, 0.17600000000000005, 0.17559999999999998, 0.17546666666666666, 0.17586666666666662, 0.1744, 0.17413333333333336, 0.17600000000000005, 0.1738666666666666, 0.1744, 0.17359999999999998, 0.17346666666666666, 0.1744, 0.1737333333333333, 0.1724, 0.17466666666666664, 0.1737333333333333, 0.1744, 0.17226666666666668, 0.1730666666666667, 0.17079999999999995, 0.1718666666666666, 0.17200000000000004, 0.1718666666666666, 0.1724, 0.17146666666666666, 0.17066666666666663, 0.17133333333333334, 0.1724, 0.17346666666666666, 0.17266666666666663, 0.17120000000000002, 0.17013333333333336, 0.1710666666666667, 0.17093333333333338, 0.17013333333333336, 0.17093333333333338, 0.17146666666666666, 0.17053333333333331, 0.1717333333333333, 0.17159999999999997, 0.16946666666666665, 0.17053333333333331, 0.16986666666666672, 0.1717333333333333, 0.1690666666666667, 0.17053333333333331, 0.16986666666666672, 0.1704, 0.1704, 0.17013333333333336, 0.17120000000000002, 0.17066666666666663, 0.17053333333333331, 0.1710666666666667, 0.17013333333333336, 0.1697333333333333, 0.1710666666666667, 0.17146666666666666, 0.17013333333333336, 0.17213333333333336, 0.17159999999999997, 0.17066666666666663, 0.1710666666666667, 0.17026666666666668, 0.17053333333333331, 0.17066666666666663, 0.17079999999999995, 0.17093333333333338, 0.17013333333333336, 0.16986666666666672, 0.17026666666666668, 0.17013333333333336, 0.1704, 0.17093333333333338, 0.17093333333333338, 0.17066666666666663, 0.17053333333333331, 0.16986666666666672, 0.17079999999999995, 0.17053333333333331, 0.17053333333333331, 0.17200000000000004, 0.16986666666666672, 0.17120000000000002, 0.17159999999999997, 0.1697333333333333, 0.17079999999999995, 0.17066666666666663, 0.1704, 0.17146666666666666, 0.17066666666666663, 0.17120000000000002, 0.17133333333333334, 0.17026666666666668, 0.17053333333333331], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.040295087943819856, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 16, "valid_ratio": 0.15, "learning_rate": 0.0003473154263282606, "optimization": "nesterov_momentum", "nb_data_augmentation": 3, "learning_rate_decay_method": "exp", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 7.214713177214144e-05, "rotation_range": [0, 0], "momentum": 0.909365511099554}, "accuracy_valid_max": 0.8309333333333333, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        y_pred = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            y_pred.extend((nnet.predict(X[mini_batch]) == y[mini_batch]).tolist())\n        return np.mean(y_pred)\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            acc = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = acc\n            status[\"accuracy_train_std\"] = 0\n            acc = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = acc\n            status[\"accuracy_valid_std\"] = 0\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n\n    # rescaling to [-1, 1]\n    X_min = X_train.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X_train.max(axis=(0, 2, 3))[None, :, None, None]\n    def preprocess(a):\n        return (a / 255.) * 2 - 1\n        # return 2 * ((a - X_min) / (X_max - X_min)) - 1\n    X_train = preprocess(X_train)\n    X_valid = preprocess(X_valid)\n\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = preprocess(X_test)\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    acc = evaluate(X_test, y_test, batch_size_eval)\n    light.set(\"accuracy_test\", acc)\n    light.set(\"accuracy_test_std\", 0)\n    print(\"Test accuracy : {}+-{}\".format(acc, 0))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8294666666666667, "accuracy_valid_std": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "accuracy_valid": [0.49373333333333336, 0.55, 0.5809333333333333, 0.6078666666666667, 0.6278666666666667, 0.6541333333333333, 0.6730666666666667, 0.6864, 0.7002666666666667, 0.7133333333333334, 0.7194666666666667, 0.7290666666666666, 0.7364, 0.7408, 0.7462666666666666, 0.7498666666666667, 0.7568, 0.7629333333333334, 0.7650666666666667, 0.7676, 0.7690666666666667, 0.7753333333333333, 0.7813333333333333, 0.7824, 0.7825333333333333, 0.7870666666666667, 0.7884, 0.7888, 0.7922666666666667, 0.7945333333333333, 0.7965333333333333, 0.7948, 0.7966666666666666, 0.7977333333333333, 0.7989333333333334, 0.7978666666666666, 0.7974666666666667, 0.8006666666666666, 0.8032, 0.8024, 0.8021333333333334, 0.8052, 0.8049333333333333, 0.8066666666666666, 0.8076, 0.808, 0.808, 0.8061333333333334, 0.8082666666666667, 0.8104, 0.8109333333333333, 0.8106666666666666, 0.8132, 0.8125333333333333, 0.8136, 0.8113333333333334, 0.8153333333333334, 0.8150666666666667, 0.8150666666666667, 0.8150666666666667, 0.818, 0.8176, 0.8184, 0.8205333333333333, 0.8194666666666667, 0.8201333333333334, 0.8204, 0.8210666666666666, 0.8222666666666667, 0.8222666666666667, 0.8208, 0.8230666666666666, 0.8241333333333334, 0.8232, 0.8224, 0.824, 0.8233333333333334, 0.8241333333333334, 0.8234666666666667, 0.8248, 0.8257333333333333, 0.8237333333333333, 0.824, 0.8244, 0.8245333333333333, 0.8241333333333334, 0.8256, 0.8258666666666666, 0.824, 0.8261333333333334, 0.8256, 0.8264, 0.8265333333333333, 0.8256, 0.8262666666666667, 0.8276, 0.8253333333333334, 0.8262666666666667, 0.8256, 0.8277333333333333, 0.8269333333333333, 0.8292, 0.8281333333333334, 0.828, 0.8281333333333334, 0.8276, 0.8285333333333333, 0.8293333333333334, 0.8286666666666667, 0.8276, 0.8265333333333333, 0.8273333333333334, 0.8288, 0.8298666666666666, 0.8289333333333333, 0.8290666666666666, 0.8298666666666666, 0.8290666666666666, 0.8285333333333333, 0.8294666666666667, 0.8282666666666667, 0.8284, 0.8305333333333333, 0.8294666666666667, 0.8301333333333333, 0.8282666666666667, 0.8309333333333333, 0.8294666666666667, 0.8301333333333333, 0.8296, 0.8296, 0.8298666666666666, 0.8288, 0.8293333333333334, 0.8294666666666667, 0.8289333333333333, 0.8298666666666666, 0.8302666666666667, 0.8289333333333333, 0.8285333333333333, 0.8298666666666666, 0.8278666666666666, 0.8284, 0.8293333333333334, 0.8289333333333333, 0.8297333333333333, 0.8294666666666667, 0.8293333333333334, 0.8292, 0.8290666666666666, 0.8298666666666666, 0.8301333333333333, 0.8297333333333333, 0.8298666666666666, 0.8296, 0.8290666666666666, 0.8290666666666666, 0.8293333333333334, 0.8294666666666667, 0.8301333333333333, 0.8292, 0.8294666666666667, 0.8294666666666667, 0.828, 0.8301333333333333, 0.8288, 0.8284, 0.8302666666666667, 0.8292, 0.8293333333333334, 0.8296, 0.8285333333333333, 0.8293333333333334, 0.8288, 0.8286666666666667, 0.8297333333333333, 0.8294666666666667], "seed": 421652851, "model": "residualv3", "loss_std": [0.27676212787628174, 0.24251586198806763, 0.24873533844947815, 0.25006601214408875, 0.25288447737693787, 0.25384312868118286, 0.2527431547641754, 0.2538299262523651, 0.24987556040287018, 0.24933964014053345, 0.24714568257331848, 0.24800170958042145, 0.24421460926532745, 0.24382635951042175, 0.24340228736400604, 0.2402416467666626, 0.24037976562976837, 0.23713034391403198, 0.23577874898910522, 0.23581644892692566, 0.23390674591064453, 0.23231804370880127, 0.22915756702423096, 0.22864580154418945, 0.22908151149749756, 0.22645264863967896, 0.2261575311422348, 0.22162321209907532, 0.2204674780368805, 0.22013944387435913, 0.21970897912979126, 0.21735872328281403, 0.2180163711309433, 0.2160571813583374, 0.21542254090309143, 0.2140161693096161, 0.21319566667079926, 0.21134693920612335, 0.21035638451576233, 0.2103084772825241, 0.20695006847381592, 0.20616410672664642, 0.20524610579013824, 0.20495420694351196, 0.20393720269203186, 0.20351679623126984, 0.20206935703754425, 0.20209570229053497, 0.20039469003677368, 0.20144905149936676, 0.19898775219917297, 0.199528768658638, 0.19888637959957123, 0.19782429933547974, 0.19665776193141937, 0.196883887052536, 0.1963879019021988, 0.19420671463012695, 0.19180399179458618, 0.1929379105567932, 0.19300593435764313, 0.19229502975940704, 0.1900976151227951, 0.19185416400432587, 0.1917160153388977, 0.19060373306274414, 0.18927960097789764, 0.18814896047115326, 0.18674780428409576, 0.18843959271907806, 0.18857428431510925, 0.18998859822750092, 0.18710297346115112, 0.1883533000946045, 0.18549954891204834, 0.18495629727840424, 0.18613290786743164, 0.18818938732147217, 0.18529142439365387, 0.18458802998065948, 0.1872740238904953, 0.18517257273197174, 0.18360599875450134, 0.18230652809143066, 0.18361583352088928, 0.1835004687309265, 0.18415628373622894, 0.18230387568473816, 0.18201695382595062, 0.1836058795452118, 0.18304339051246643, 0.1819687932729721, 0.18174298107624054, 0.1794872283935547, 0.1812765747308731, 0.18014074862003326, 0.18062402307987213, 0.18054991960525513, 0.1808047741651535, 0.1816989630460739, 0.18196164071559906, 0.18232859671115875, 0.18095006048679352, 0.18199172616004944, 0.1802167147397995, 0.1810348629951477, 0.18005695939064026, 0.18120408058166504, 0.1793837696313858, 0.17993280291557312, 0.1783544272184372, 0.18118035793304443, 0.18175344169139862, 0.17943298816680908, 0.18007183074951172, 0.18026164174079895, 0.17868372797966003, 0.18060994148254395, 0.18090911209583282, 0.1796303391456604, 0.1788521111011505, 0.17884160578250885, 0.17784036695957184, 0.17824696004390717, 0.17948222160339355, 0.1793658435344696, 0.17780807614326477, 0.18016204237937927, 0.1792752891778946, 0.17853829264640808, 0.1791447252035141, 0.17970216274261475, 0.17976422607898712, 0.1806817352771759, 0.17821116745471954, 0.18017764389514923, 0.18081381916999817, 0.1798066347837448, 0.1789514422416687, 0.17734405398368835, 0.1796238273382187, 0.1787036508321762, 0.1793363094329834, 0.178611621260643, 0.17861969769001007, 0.17930656671524048, 0.17629975080490112, 0.1777874082326889, 0.1788046509027481, 0.17815633118152618, 0.17878372967243195, 0.18004262447357178, 0.1788240671157837, 0.17604446411132812, 0.1784197837114334, 0.1758614182472229, 0.17651210725307465, 0.17707379162311554, 0.17646494507789612, 0.17740701138973236, 0.17981868982315063, 0.17960099875926971, 0.17757974565029144, 0.17748858034610748, 0.17780348658561707, 0.17864331603050232, 0.17706085741519928, 0.17815707623958588, 0.1785515397787094, 0.17725397646427155, 0.17746075987815857, 0.17938430607318878, 0.1786927580833435, 0.17625920474529266, 0.1763169914484024, 0.1776697188615799, 0.1801898181438446]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:42 2016", "state": "available"}], "summary": "f98db52d6f6d9fe98fb2ebfe5814540d"}