{"content": {"hp_model": {"f0": 16, "f1": 16, "f2": 32, "f3": 16, "nonlin": "leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.8832241296768188, 1.6558476686477661, 1.5592920780181885, 1.4855778217315674, 1.4261178970336914, 1.3774628639221191, 1.3380179405212402, 1.30255126953125, 1.2734322547912598, 1.2477612495422363, 1.225909948348999, 1.2080203294754028, 1.1906733512878418, 1.175301432609558, 1.1596204042434692, 1.1471902132034302, 1.1343673467636108, 1.1232956647872925, 1.1164220571517944, 1.1061623096466064, 1.0989545583724976, 1.0916930437088013, 1.0862525701522827, 1.077370524406433, 1.0748071670532227, 1.0663516521453857, 1.0634633302688599, 1.0605665445327759, 1.0572760105133057, 1.0539884567260742, 1.051865577697754, 1.0485763549804688, 1.0444374084472656, 1.0415468215942383, 1.038547158241272, 1.038050889968872, 1.0374833345413208, 1.0366350412368774, 1.0345056056976318, 1.0319480895996094, 1.0320920944213867, 1.0307682752609253, 1.0282856225967407, 1.0277090072631836, 1.0262556076049805, 1.0250425338745117, 1.0242081880569458, 1.0240602493286133, 1.0232763290405273, 1.022826075553894, 1.0241820812225342, 1.0231105089187622, 1.0219662189483643, 1.0220065116882324, 1.0202919244766235, 1.0225600004196167, 1.019891619682312, 1.0208678245544434, 1.019812822341919, 1.0196669101715088, 1.0205926895141602, 1.0199472904205322, 1.020042061805725, 1.0188571214675903, 1.0182961225509644, 1.0204894542694092, 1.0171691179275513, 1.0185885429382324, 1.018243432044983, 1.0189738273620605, 1.017967939376831, 1.019018292427063, 1.0177990198135376, 1.0205754041671753, 1.0191024541854858, 1.018506646156311, 1.01836097240448, 1.0189794301986694, 1.0172557830810547, 1.016930103302002, 1.0204317569732666, 1.0170801877975464, 1.0178327560424805, 1.01823890209198, 1.0202183723449707, 1.0156172513961792, 1.0171265602111816, 1.0181063413619995, 1.0182905197143555, 1.0184508562088013, 1.015894889831543, 1.016732096672058, 1.0168118476867676, 1.0190881490707397, 1.0188143253326416, 1.019146203994751, 1.0188714265823364, 1.016599416732788, 1.016255259513855, 1.0156291723251343, 1.0175410509109497, 1.016740083694458, 1.0175013542175293, 1.0172728300094604, 1.0181019306182861, 1.018635869026184, 1.0178898572921753, 1.0167731046676636, 1.018744945526123, 1.0168278217315674, 1.0174784660339355, 1.0178817510604858, 1.018864631652832, 1.0187654495239258, 1.0151976346969604, 1.0170871019363403, 1.0184738636016846, 1.0167818069458008, 1.0172486305236816, 1.0166194438934326, 1.0170674324035645, 1.018268346786499, 1.017159104347229, 1.0141109228134155, 1.0171040296554565, 1.0170038938522339, 1.0184438228607178, 1.0176284313201904, 1.0152796506881714, 1.017232894897461, 1.0192022323608398, 1.017411708831787, 1.017512559890747, 1.0168529748916626, 1.0164722204208374], "moving_avg_accuracy_train": [0.041000971155177174, 0.08293503534226188, 0.12437174244849342, 0.16459410585591427, 0.20291222719515672, 0.2394207669860231, 0.2735663689453665, 0.3058179138349291, 0.33583456528666134, 0.36370509820866886, 0.38939765868011467, 0.4136737682792959, 0.4357965984292641, 0.4563372608916164, 0.4752957181207918, 0.4928953308555934, 0.5092114936275922, 0.5243446496473543, 0.5383272853770633, 0.5513463522147262, 0.5632285979340989, 0.5741456891719735, 0.5841152305122512, 0.5933831476661294, 0.6018428556939056, 0.6098051128986014, 0.6170109603031395, 0.623705342164806, 0.6298766981176684, 0.6355448147180924, 0.6407345113620547, 0.6454865825011353, 0.649851766132251, 0.6539524563633412, 0.657759262914161, 0.6612807199110893, 0.6643802406952203, 0.667202433581909, 0.6698911806549196, 0.6723366296575339, 0.6746306478587241, 0.6767183715326154, 0.6786184294224971, 0.6803841769483628, 0.682017527549023, 0.6834991688336649, 0.6848883774636336, 0.6861154497913289, 0.6872500778695972, 0.6883292997626393, 0.6892658303806907, 0.6901877269476419, 0.6909801954281268, 0.6916771770677154, 0.6923160862873927, 0.6928702142946354, 0.6933734356034977, 0.6939078231362635, 0.6944236130990767, 0.6948413210894182, 0.6952010903855151, 0.6956202138531928, 0.695967198039579, 0.6962817368584985, 0.696557918446735, 0.6968087349273199, 0.697001917676513, 0.6972082981853013, 0.6974010160896393, 0.6975210919273809, 0.6976570619670625, 0.697774784705157, 0.6978621700277845, 0.6979523344157407, 0.6980683235482253, 0.6981658104186704, 0.69820937077469, 0.6982858135248788, 0.6983731771417062, 0.6984494792480413, 0.6985041642020671, 0.6985231537261664, 0.6986099987621415, 0.6986556793088232, 0.698624640090104, 0.6986781210504088, 0.6986867624337401, 0.6987618968965769, 0.6987784006881391, 0.6988118192422026, 0.69883495654325, 0.6988930545927825, 0.6989801479718671, 0.6988934824963858, 0.6990060376243774, 0.6989748758550645, 0.6989840326436352, 0.6990318733807482, 0.6991004345834173, 0.6990365816301051, 0.6990697947756955, 0.6990438830352984, 0.6990670654451314, 0.6990623529770763, 0.6990279569189405, 0.6990736222308762, 0.6991007701187613, 0.699074085992867, 0.6990988984045623, 0.6990723293524505, 0.6990718128889201, 0.6990898771645815, 0.6990526565900577, 0.6990680222468049, 0.6990353123128682, 0.6990384254556585, 0.6990458415329701, 0.6990828150347118, 0.6990997430469752, 0.6992032978639554, 0.6992267787837707, 0.699224696172328, 0.6991669822017821, 0.6991476277604429, 0.6991604356977615, 0.6991277850139673, 0.6991425411771146, 0.6991256308382422, 0.699087123996343, 0.6991686531814725, 0.6991724191790782, 0.6991734113304764, 0.6991767015131818, 0.6991633505871313, 0.6991374199096474], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.04093914721385541, 0.0826714573136295, 0.12294873341020329, 0.16163865863846008, 0.19846566250352976, 0.23346852325807135, 0.2661288539612552, 0.29712433170518987, 0.32564076125116187, 0.3519322244709252, 0.37633108127835074, 0.39907130240503375, 0.419370721016187, 0.4385242515595231, 0.4560655458124564, 0.47222303961223483, 0.4874026381830144, 0.5015912382577852, 0.5145196697313289, 0.5265469125661779, 0.5376511633276324, 0.5478932476725801, 0.5573318796541925, 0.5660331384602341, 0.5739619276356716, 0.5813949246782039, 0.5882911120390732, 0.5945444497715364, 0.6003362926803015, 0.6055479217895304, 0.6104957651527461, 0.6151695802507998, 0.6194746995977078, 0.623483584353675, 0.6270417230003859, 0.6305787851694737, 0.6336847808995143, 0.6364710585512797, 0.6392727066965282, 0.6416832972373423, 0.6438538582327346, 0.6460759178160875, 0.6480381208386957, 0.6496952697864526, 0.6512975966293435, 0.6525921769042857, 0.6540635044416433, 0.6550692869041055, 0.6560853839102311, 0.6570120782469941, 0.6578196300702617, 0.6585851068222717, 0.6593350710553306, 0.6599357671689241, 0.6604896302110678, 0.6611966559889068, 0.6616366371803022, 0.6621567495823775, 0.6626105846956758, 0.6631797867212137, 0.6634193958220592, 0.6637469663113894, 0.6640183951979463, 0.6641863504823685, 0.6643375102383485, 0.6645844468086402, 0.6646602053469026, 0.6647782456649985, 0.6649333100762849, 0.6649985963502829, 0.6652303114517004, 0.665339139775657, 0.6652630983037389, 0.6654317421160607, 0.6656445567034004, 0.6656896054570062, 0.6657392678405224, 0.6658958862842562, 0.6660867005172764, 0.6661485700457445, 0.6662897018401157, 0.6663058276651402, 0.6664546182514124, 0.666601766318967, 0.6667942052273563, 0.6668585664723166, 0.6669409056552807, 0.6670028038886984, 0.6670829263612743, 0.6670309072567734, 0.6668742267814725, 0.6668196930811114, 0.6669303336656961, 0.6669434314644126, 0.6670040476082575, 0.6669110882540583, 0.6669393471338483, 0.6668783013982496, 0.66694748956603, 0.6669354872208728, 0.6670711694852314, 0.6670701837019943, 0.6669706107384215, 0.6670173314322752, 0.6670349659942434, 0.6670020089750148, 0.6668349817879802, 0.6668199641720587, 0.6668949860624582, 0.6670458959652485, 0.6670219939628502, 0.6670259257318514, 0.6671138840340427, 0.6670618576882438, 0.6670607735760459, 0.667171720173637, 0.6671484722903094, 0.6670543070078147, 0.6669339666684789, 0.6669965588005767, 0.6669409694208955, 0.6669275600729325, 0.6668422494722659, 0.666975048480235, 0.6669592607349977, 0.666993879889284, 0.6668653162132322, 0.6668859452658548, 0.6669665760781247, 0.6668885413995291, 0.666992297152272, 0.666974784539831, 0.6670699159785436, 0.6670812625772254, 0.6670538239136294], "moving_var_accuracy_train": [0.015129716721009042, 0.029442936702127005, 0.04195164929418288, 0.05231703102747226, 0.06029983373144521, 0.06626571165725216, 0.0701324396900202, 0.07248065505088945, 0.07334158382517353, 0.07299832489086655, 0.07163946137398998, 0.0697794807120341, 0.06720630916542976, 0.06428294757841738, 0.061089460725169895, 0.05776823196838782, 0.05438736327997027, 0.05100973865203937, 0.04766839170438305, 0.044427017445856525, 0.04125500557129083, 0.03820215094402648, 0.035276461640443395, 0.03252186407173941, 0.029913777603802448, 0.027492977701565776, 0.025210998062766603, 0.023093230993078227, 0.02112667860244277, 0.01930315865436251, 0.017615239350233437, 0.016056955036208038, 0.014622752985787479, 0.013311818630550955, 0.012111062752534141, 0.011011562411711666, 0.00999686943236184, 0.009068865443332746, 0.008227043146403082, 0.007458160819182262, 0.00675970741283056, 0.006122963982794242, 0.005543159564378933, 0.0050169043868669525, 0.0045392244558423505, 0.004105059358325311, 0.0037119225280509947, 0.003354281633722456, 0.0030304398982341657, 0.002737878387460542, 0.002471984355101416, 0.002232434959112683, 0.0020148435198344746, 0.0018177312185043396, 0.001639631941572804, 0.00147843226805122, 0.0013328681264173367, 0.0012021514440921823, 0.0010843306532546146, 0.0009774679076159094, 0.0008808860223720453, 0.0007943784004652646, 0.0007160241426491567, 0.0006453121404017067, 0.0005814674127886637, 0.0005238868516721943, 0.0004718340426762471, 0.00042503397463829206, 0.00038286483889033503, 0.0003447081188625854, 0.0003104036976415461, 0.0002794880556649715, 0.0002516079758499708, 0.00022652034481667328, 0.00020398939164469678, 0.0001836759856894097, 0.00016532546466201773, 0.0001488455096423238, 0.00013402965029199785, 0.00012067908336567862, 0.0001086380890268819, 9.777752554242336e-05, 8.806765153064268e-05, 7.92796667886847e-05, 7.136037100770449e-05, 6.42500758249702e-05, 5.782574030402607e-05, 5.209397296117541e-05, 4.688702704128126e-05, 4.220837553495441e-05, 3.7992355993756776e-05, 3.422349884461643e-05, 3.0869416270278174e-05, 2.785007278501393e-05, 2.5179083418047268e-05, 2.2669914579042974e-05, 2.0403677742131033e-05, 1.83839085930656e-05, 1.6587823480361804e-05, 1.4965735929145838e-05, 1.3479090353591365e-05, 1.2137224082845921e-05, 1.0928338491692325e-05, 9.835704508719622e-06, 8.862781857185214e-06, 7.995271557894426e-06, 7.202377472454567e-06, 6.4885481083817554e-06, 5.845234199510771e-06, 5.26706401033078e-06, 4.740360009908907e-06, 4.269260871414532e-06, 3.8548031247840525e-06, 3.4714477429711255e-06, 3.133932426677285e-06, 2.820626408931853e-06, 2.539058751862892e-06, 2.297456235156025e-06, 2.070289630033133e-06, 1.9597730681080873e-06, 1.7687579436556066e-06, 1.5919211847238404e-06, 1.4627071878169325e-06, 1.3198078186311976e-06, 1.1893034260932851e-06, 1.0799676878540432e-06, 9.739306182261288e-07, 8.79111192450534e-07, 8.045450650628974e-07, 7.839136308074667e-07, 7.05649912368422e-07, 6.350937804111523e-07, 5.716818300901517e-07, 5.161178721188052e-07, 4.705576852198901e-07], "duration": 80657.106504, "accuracy_train": [0.41000971155177185, 0.4603416130260244, 0.49730210640457734, 0.5265953765227022, 0.5477753192483389, 0.5679976251038206, 0.5808767865794573, 0.596081817840993, 0.6059844283522517, 0.6145398945067369, 0.6206307029231266, 0.632158754671927, 0.6349020697789775, 0.641203223052787, 0.6459218331833703, 0.6512918454688077, 0.6560569585755813, 0.6605430538252123, 0.6641710069444444, 0.6685179537536914, 0.6701688094084532, 0.6723995103128461, 0.6738411025747508, 0.6767944020510336, 0.6779802279438907, 0.6814654277408637, 0.681863586943983, 0.6839547789198044, 0.6854189016934293, 0.6865578641219084, 0.687441781157715, 0.6882552227528608, 0.6891384188122923, 0.6908586684431525, 0.6920205218715394, 0.692973832883444, 0.6922759277523994, 0.6926021695621078, 0.6940899043120156, 0.6943456706810631, 0.6952768116694352, 0.6955078845976376, 0.6957189504314323, 0.6962759046811554, 0.696717682954965, 0.6968339403954411, 0.6973912551333518, 0.697159100740587, 0.6974617305740126, 0.6980422968000185, 0.6976946059431525, 0.6984847960502031, 0.6981124117524917, 0.6979500118240126, 0.6980662692644888, 0.6978573663598191, 0.6979024273832595, 0.6987173109311554, 0.6990657227643965, 0.6986006930024917, 0.6984390140503876, 0.6993923250622923, 0.6990900557170543, 0.6991125862287745, 0.6990435527408637, 0.699066083252584, 0.6987405624192506, 0.6990657227643965, 0.6991354772286822, 0.6986017744670543, 0.6988807923241971, 0.6988342893480066, 0.6986486379314323, 0.6987638139073459, 0.699112225740587, 0.6990431922526762, 0.6986014139788667, 0.698973798276578, 0.6991594496931525, 0.6991361982050572, 0.6989963287882983, 0.6986940594430602, 0.6993916040859174, 0.699066804228959, 0.6983452871216317, 0.6991594496931525, 0.6987645348837209, 0.6994381070621078, 0.6989269348122, 0.6991125862287745, 0.6990431922526762, 0.6994159370385751, 0.6997639883836286, 0.6981134932170543, 0.7000190337763013, 0.6986944199312477, 0.6990664437407714, 0.6994624400147655, 0.6997174854074382, 0.6984619050502953, 0.6993687130860096, 0.6988106773717239, 0.6992757071336286, 0.699019940764581, 0.698718392395718, 0.6994846100382983, 0.6993451011097269, 0.6988339288598191, 0.6993222101098191, 0.698833207883444, 0.6990671647171466, 0.6992524556455334, 0.6987176714193429, 0.6992063131575305, 0.6987409229074382, 0.6990664437407714, 0.6991125862287745, 0.6994155765503876, 0.6992520951573459, 0.7001352912167774, 0.6994381070621078, 0.6992059526693429, 0.6986475564668697, 0.6989734377883905, 0.6992757071336286, 0.6988339288598191, 0.6992753466454411, 0.6989734377883905, 0.6987405624192506, 0.6999024158476376, 0.6992063131575305, 0.6991823406930602, 0.6992063131575305, 0.6990431922526762, 0.6989040438122923], "end": "2016-01-30 14:07:50.314000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0], "moving_var_accuracy_valid": [0.01508412397137953, 0.02924998293061494, 0.04092531536539015, 0.05030497665636405, 0.05748053291382835, 0.06275928197146197, 0.06608362858908777, 0.06812174249535176, 0.06862824903227004, 0.06798659347316854, 0.066545672047435, 0.06454516375470545, 0.061799244942792424, 0.05892104003898377, 0.05579820907169728, 0.052567969617536466, 0.04938495457071293, 0.046258306462377725, 0.04313677487943501, 0.04012498852336736, 0.037222229135789986, 0.03444410884777218, 0.031801487926153824, 0.02930274627682589, 0.026938262929240103, 0.024741681641624727, 0.022695530078508188, 0.0207779151658236, 0.019002032647759546, 0.017346279084733046, 0.015831981561781994, 0.014445384333740948, 0.013167652373686946, 0.011995527549197884, 0.01090991794994105, 0.00993152343403888, 0.009025195975910266, 0.008192546466693786, 0.007443934910992379, 0.0067518399406923, 0.006119057961935535, 0.005551590104869723, 0.0050310832607001425, 0.004552690218345596, 0.004120528258314074, 0.003723558875277091, 0.0033706862302490623, 0.0030427219924803245, 0.0027477418713650085, 0.0024806965457726056, 0.0022384961507207085, 0.0020199201275694473, 0.0018229901319703123, 0.0016439386411612575, 0.0014823056554702063, 0.0013385740589779453, 0.0012064589041191867, 0.0010882476659044, 0.0009812765961045233, 0.0008860648550069583, 0.0007979750821971344, 0.0007191432958067414, 0.0006478920289901847, 0.0005833567068892544, 0.0005252266796467804, 0.00047325281070982904, 0.00042597918384392315, 0.0003835066671097964, 0.00034537240514364486, 0.0003108735253074333, 0.00028026939977071424, 0.0002523490522304994, 0.00022716618775651273, 0.00020470553559977163, 0.00018464259247705561, 0.00016619659774116293, 0.00014959913513807528, 0.00013485998565652643, 0.00012170167773458142, 0.00010956596050809911, 9.878862810773145e-05, 8.891210567705281e-05, 8.022014285641634e-05, 7.239300155484e-05, 6.548699600051513e-05, 5.897557772913898e-05, 5.3139037625685866e-05, 4.785961638481937e-05, 4.3131431241842574e-05, 3.884264200275602e-05, 3.517931674454485e-05, 3.168815039036604e-05, 2.8629507401944493e-05, 2.5768100632731014e-05, 2.3224359421509526e-05, 2.0979696453156837e-05, 1.8888913886423994e-05, 1.7033561734294653e-05, 1.5373288583912548e-05, 1.3837256232124753e-05, 1.2619217700665304e-05, 1.1357304676516087e-05, 1.0310807184536738e-05, 9.299371875190542e-06, 8.3722334876538e-06, 7.5447856249363065e-06, 7.041389793320947e-06, 6.339280573080547e-06, 5.756007072124594e-06, 5.385370553753788e-06, 4.8519752498462645e-06, 4.366916854128941e-06, 3.999855135035548e-06, 3.6242302874466234e-06, 3.261817836395279e-06, 3.0464183804093057e-06, 2.7466407190812797e-06, 2.5517805510190804e-06, 2.426938671360028e-06, 2.2195047792290595e-06, 2.025365913506182e-06, 1.8244476176707102e-06, 1.7075039431787343e-06, 1.6954737375192672e-06, 1.5281696398644589e-06, 1.3861390484695474e-06, 1.39628271282239e-06, 1.2604844618490884e-06, 1.192947966649978e-06, 1.1284578695566593e-06, 1.1124993886461305e-06, 1.0040096741321145e-06, 9.850586224028077e-07, 8.877114678773351e-07, 8.057162434290301e-07], "accuracy_test": 0.675308912627551, "start": "2016-01-29 15:43:33.207000", "learning_rate_per_epoch": [0.0011051935143768787, 0.001009350991807878, 0.0009218198829330504, 0.0008418795187026262, 0.0007688715704716742, 0.0007021949277259409, 0.0006413004593923688, 0.0005856867646798491, 0.0005348959239199758, 0.0004885096568614244, 0.00044614600483328104, 0.0004074561584275216, 0.00037212148890830576, 0.0003398510452825576, 0.00031037910957820714, 0.00028346298495307565, 0.00025888101663440466, 0.00023643081658519804, 0.0002159275027224794, 0.000197202229173854, 0.00018010081839747727, 0.00016448245150968432, 0.00015021850413177162, 0.00013719152775593102, 0.00012529426021501422, 0.00011442872346378863, 0.00010450544505147263, 9.544271597405896e-05, 8.716590673429891e-05, 7.960686343722045e-05, 7.270334026543424e-05, 6.639849743805826e-05, 6.06404100835789e-05, 5.5381664424203336e-05, 5.0578957598190755e-05, 4.619274113792926e-05, 4.2186900827800855e-05, 3.852844747598283e-05, 3.518725497997366e-05, 3.213580930605531e-05, 2.9348986572586e-05, 2.6803838409250602e-05, 2.4479404601152055e-05, 2.235654574178625e-05, 2.0417783161974512e-05, 1.864714977273252e-05, 1.703006637399085e-05, 1.5553216144326143e-05, 1.4204438230080996e-05, 1.2972626791452058e-05, 1.1847638234030455e-05, 1.0820209354278632e-05, 9.881879122985993e-06, 9.024921382660978e-06, 8.242278454417828e-06, 7.527506568294484e-06, 6.874719929328421e-06, 6.2785429690848105e-06, 5.734066689910833e-06, 5.2368072829267476e-06, 4.782670657732524e-06, 4.367916517367121e-06, 3.989130163972732e-06, 3.6431920307222754e-06, 3.3272538075834746e-06, 3.0387138849619078e-06, 2.775196207949193e-06, 2.5345307221869007e-06, 2.3147358660935424e-06, 2.114001517838915e-06, 1.930674898176221e-06, 1.7632464732741937e-06, 1.6103374491649447e-06, 1.4706887441207073e-06, 1.3431504157779273e-06, 1.2266722251297324e-06, 1.1202949963262654e-06, 1.023142885969719e-06, 9.344157660962082e-07, 8.533830850865343e-07, 7.7937755804669e-07, 7.117898235264875e-07, 6.500632707684417e-07, 5.936896627645183e-07, 5.422048161562998e-07, 4.951847358825034e-07, 4.522422329955589e-07, 4.1302371300844243e-07, 3.772062200368964e-07, 3.4449482200216153e-07, 3.1462016636396584e-07, 2.873362632271892e-07, 2.624184105570748e-07, 2.3966143203324464e-07, 2.188779575362787e-07, 1.9989683153198712e-07, 1.8256174882935738e-07, 1.667299613927753e-07, 1.522711130519383e-07, 1.3906613105518772e-07, 1.2700628815309756e-07, 1.1599227889291797e-07, 1.0593340959985653e-07, 9.674684520177834e-08, 8.835694131903438e-08, 8.069461188142668e-08, 7.369676069401976e-08, 6.730576274094346e-08, 6.14689952271874e-08, 5.613839348939109e-08, 5.127006375005294e-08, 4.682391363530769e-08, 4.276333598340898e-08, 3.905489265321194e-08, 3.5668044517933595e-08, 3.257490632790905e-08, 2.9750005126061296e-08, 2.7170079519578394e-08, 2.4813886057017953e-08, 2.2662021592623205e-08, 2.0696766966921132e-08, 1.8901939569104798e-08, 1.7262760110270392e-08, 1.5765730054795313e-08, 1.4398523262570961e-08, 1.3149881183949219e-08, 1.2009521377365218e-08, 1.0968054020565887e-08, 1.0016902862730603e-08, 9.14823505837603e-09, 8.354898994866744e-09, 7.63036034356901e-09, 6.9686540982161205e-09, 6.364331284203217e-09, 5.81241543784472e-09], "accuracy_train_first": 0.41000971155177185, "accuracy_train_last": 0.6989040438122923, "batch_size_eval": 1024, "accuracy_train_std": [0.017046495665563818, 0.015679073041643333, 0.015730150284011254, 0.014544879566287795, 0.014816494491663603, 0.014383545328286222, 0.014000689482190284, 0.012780654652785545, 0.013070077549382265, 0.013259529438202784, 0.013223895613760145, 0.0137394392952847, 0.01334863871461025, 0.01469952441416982, 0.014246833971610893, 0.014018645577453115, 0.01440318760644045, 0.01392083076284057, 0.014865847394360377, 0.014728099222998628, 0.01573085696997407, 0.015326510477188693, 0.014308907486943155, 0.014960115322383685, 0.014730107089603761, 0.015119958563234498, 0.015083813915162819, 0.014703778141703577, 0.014941200618495888, 0.01526485650987752, 0.014694169118095594, 0.014731161082476745, 0.015298772991544312, 0.01472018279768622, 0.015593419305675532, 0.015258842308668138, 0.015526088097340816, 0.015366864467390978, 0.016153056322345156, 0.014807760400226938, 0.015407272759472634, 0.01572474012104361, 0.015146981948773404, 0.015287338513890634, 0.015053029077645783, 0.015589533967857245, 0.015287165623215661, 0.015605133132981173, 0.015210238680143158, 0.015710656942606326, 0.015736861237346238, 0.015394410603606964, 0.016022908082647717, 0.01583060312977955, 0.0165565171980693, 0.014853310803904354, 0.015991555764273812, 0.016538281322935066, 0.01635245467745055, 0.016478473535696717, 0.016030985278276087, 0.015699262881245183, 0.016265839252206, 0.01562879186934873, 0.015559046522453863, 0.015922040186292764, 0.016167545658677313, 0.016362171781792185, 0.015671575553780133, 0.015913710294904293, 0.016217396488621066, 0.015727309391719034, 0.016046067077618176, 0.01577080461227831, 0.016106587002289754, 0.015988046441720773, 0.015563132405144172, 0.01627357589338373, 0.015388996670852896, 0.016478355703361956, 0.01580312064000897, 0.01569285694179045, 0.015894714344826578, 0.01578746631423808, 0.014890665031620032, 0.01606187117826572, 0.015576742932930885, 0.01580531857418334, 0.01542466591495321, 0.0157301636890225, 0.016095621199157154, 0.015790449346795396, 0.015703794130184794, 0.01581349009149528, 0.015790009961603197, 0.01593986371452769, 0.0157298504294809, 0.015848650543460262, 0.01547815428895054, 0.01589341446483368, 0.015524269851854688, 0.0155079210595403, 0.015847691069183528, 0.016349397177719608, 0.01587555515821841, 0.015605080215797925, 0.016024620018588648, 0.015883914546357007, 0.01565863152172498, 0.015631505107725114, 0.015849082757140837, 0.01540529592967688, 0.015786741059381913, 0.01601948630951138, 0.015369876766911296, 0.015168401517225993, 0.01535765993563176, 0.015547108334834067, 0.015925589001840466, 0.016334417921196, 0.016150649196367484, 0.01588857025834074, 0.015294046460497085, 0.015852341689487796, 0.015932001849195035, 0.016212173569855663, 0.015973216062357843, 0.016308438192783964, 0.015799605988433336, 0.01578302178257908, 0.015669816665335803, 0.016022951229764767, 0.015740660545124855, 0.015933986715211977, 0.01590757546000056], "accuracy_test_std": 0.014550431838818292, "error_valid": [0.5906085278614458, 0.5417377517884037, 0.5145557817206325, 0.4901520143072289, 0.4700913027108433, 0.4515057299510542, 0.4399281697100903, 0.42391636859939763, 0.4177113728350903, 0.41144460655120485, 0.4040792074548193, 0.3962667074548193, 0.39793451148343373, 0.38909397355045183, 0.3860628059111446, 0.38235951618975905, 0.3759809746799698, 0.37071136106927716, 0.36912444700677716, 0.3652079019201807, 0.36241057981927716, 0.3599279932228916, 0.35772043251129515, 0.3556555322853916, 0.3546789697853916, 0.35170810193900603, 0.34964320171310237, 0.34917551063629515, 0.34753712114081325, 0.3475474162274097, 0.34497364457831325, 0.3427660838667168, 0.3417792262801205, 0.3404364528426205, 0.3409350291792168, 0.3375876553087349, 0.3383612575301205, 0.33845244258283136, 0.3355124599962349, 0.33662138789533136, 0.3366110928087349, 0.3339255459337349, 0.33430205195783136, 0.3353903896837349, 0.3342814617846386, 0.3357566006212349, 0.3326945477221386, 0.3358786709337349, 0.3347697430346386, 0.3346476727221386, 0.33491240352033136, 0.3345256024096386, 0.3339152508471386, 0.3346579678087349, 0.3345256024096386, 0.33244011201054224, 0.3344035320971386, 0.3331622387989458, 0.3333048992846386, 0.3316973950489458, 0.33442412227033136, 0.3333048992846386, 0.33353874482304224, 0.33430205195783136, 0.33430205195783136, 0.3331931240587349, 0.3346579678087349, 0.3341593914721386, 0.3336711102221386, 0.3344138271837349, 0.33268425263554224, 0.3336814053087349, 0.33542127494352414, 0.33305046357304224, 0.33244011201054224, 0.33390495576054224, 0.33381377070783136, 0.3326945477221386, 0.33219597138554224, 0.33329460419804224, 0.33244011201054224, 0.3335490399096386, 0.3322062664721386, 0.33207390107304224, 0.3314738445971386, 0.33256218232304224, 0.33231804169804224, 0.33244011201054224, 0.33219597138554224, 0.3334372646837349, 0.3345358974962349, 0.3336711102221386, 0.33207390107304224, 0.3329386883471386, 0.3324504070971386, 0.3339255459337349, 0.33280632294804224, 0.3336711102221386, 0.3324298169239458, 0.33317253388554224, 0.33170769013554224, 0.3329386883471386, 0.3339255459337349, 0.33256218232304224, 0.33280632294804224, 0.33329460419804224, 0.33466826289533136, 0.3333151943712349, 0.3324298169239458, 0.3315959149096386, 0.3331931240587349, 0.3329386883471386, 0.3320944912462349, 0.3334063794239458, 0.3329489834337349, 0.33182976044804224, 0.3330607586596386, 0.3337931805346386, 0.33414909638554224, 0.33244011201054224, 0.3335593349962349, 0.3331931240587349, 0.3339255459337349, 0.33182976044804224, 0.3331828289721386, 0.3326945477221386, 0.3342917568712349, 0.33292839326054224, 0.3323077466114458, 0.33381377070783136, 0.33207390107304224, 0.3331828289721386, 0.33207390107304224, 0.3328166180346386, 0.3331931240587349], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.0867201647180806, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 16, "valid_ratio": 0.15, "learning_rate": 0.001210136853508103, "optimization": "nesterov_momentum", "nb_data_augmentation": 1, "learning_rate_decay_method": "exp", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 5.9040104159505776e-06, "rotation_range": [0, 0], "momentum": 0.5810607881286608}, "accuracy_valid_max": 0.6685261554028614, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.6668068759412651, "accuracy_valid_std": [0.022018123374717362, 0.025327688043155883, 0.02473004298256772, 0.022235005744199402, 0.022635479953818188, 0.017713289163019064, 0.01897829228012495, 0.021215757245725598, 0.018885624248058498, 0.019138507607865028, 0.019223387388331205, 0.017636974670162158, 0.019984671733029816, 0.017743714792783385, 0.021190391154009603, 0.01711462107557588, 0.02003351960530874, 0.0212579822013154, 0.01920044994671464, 0.020300350698010946, 0.01952384534822268, 0.019262404320361807, 0.01855702628128448, 0.018458143779887044, 0.018641490889568898, 0.019669937022528797, 0.01795898527100076, 0.01633747509894936, 0.018947667304249613, 0.01813859510200585, 0.018028028898363573, 0.01953398034387776, 0.018385382107728533, 0.018945733360902346, 0.017805682560841345, 0.02050849068531546, 0.01748471107918064, 0.019102848403475252, 0.019453458822180532, 0.01855884688435658, 0.02063318924475459, 0.01983744028409129, 0.019173298847774103, 0.019392096662079777, 0.020203145695753863, 0.019971243671576124, 0.019371103711531095, 0.019861246028441682, 0.019681792966800556, 0.020102733762282093, 0.018792179280825962, 0.01993670012745469, 0.020634300181419304, 0.0181629526000051, 0.020461946767404388, 0.019873892469327813, 0.02010930570440732, 0.02078551832537771, 0.019312808101487226, 0.02008489796344295, 0.018237176531893582, 0.019343646234884944, 0.020521180058858838, 0.018177516067004646, 0.018092060381225013, 0.0177599375456071, 0.01911588908306817, 0.01895335610889464, 0.020004254530986997, 0.020436774788947745, 0.020123726207773043, 0.019787075544842086, 0.016566869256219058, 0.019368678237815348, 0.019565588525475668, 0.020141864186712526, 0.018677583724402442, 0.019371103711531095, 0.021099278151219834, 0.020067930174578762, 0.02107900016026444, 0.01992134952012015, 0.019855619891493943, 0.020518415307829142, 0.01897656799912589, 0.02043589624162879, 0.019331788787342305, 0.020552122481123074, 0.019293093409234935, 0.018173763047397457, 0.01920378651365824, 0.02021175028564, 0.020518415307829142, 0.018492009479805456, 0.0202620695223103, 0.01834508099245175, 0.019997561100820963, 0.019497257209566233, 0.020442584549478813, 0.020220203055411428, 0.020179034332057647, 0.019125801225344026, 0.01841642204636209, 0.020248369333538137, 0.020322782088868976, 0.02007386958388617, 0.01793961720281022, 0.018823832661442972, 0.021142051218199724, 0.019151705618377205, 0.01790699836276586, 0.01941653794395758, 0.01888640901538756, 0.022191464193138586, 0.018765999102639555, 0.019523080830367932, 0.01866211354054236, 0.019025393475357526, 0.021364854891745614, 0.019338840888777884, 0.019624510131468785, 0.01945142376554162, 0.01893977097211209, 0.02071407298211759, 0.019936902741270215, 0.019371103711531095, 0.020035469464653528, 0.020617708425046535, 0.02135995904081076, 0.018600836293910124, 0.019976757931837025, 0.019641730589204487, 0.01934619666085949, 0.01894649217749556, 0.01940847640730633], "accuracy_valid": [0.4093914721385542, 0.4582622482115964, 0.48544421827936746, 0.5098479856927711, 0.5299086972891567, 0.5484942700489458, 0.5600718302899097, 0.5760836314006024, 0.5822886271649097, 0.5885553934487951, 0.5959207925451807, 0.6037332925451807, 0.6020654885165663, 0.6109060264495482, 0.6139371940888554, 0.617640483810241, 0.6240190253200302, 0.6292886389307228, 0.6308755529932228, 0.6347920980798193, 0.6375894201807228, 0.6400720067771084, 0.6422795674887049, 0.6443444677146084, 0.6453210302146084, 0.648291898060994, 0.6503567982868976, 0.6508244893637049, 0.6524628788591867, 0.6524525837725903, 0.6550263554216867, 0.6572339161332832, 0.6582207737198795, 0.6595635471573795, 0.6590649708207832, 0.6624123446912651, 0.6616387424698795, 0.6615475574171686, 0.6644875400037651, 0.6633786121046686, 0.6633889071912651, 0.6660744540662651, 0.6656979480421686, 0.6646096103162651, 0.6657185382153614, 0.6642433993787651, 0.6673054522778614, 0.6641213290662651, 0.6652302569653614, 0.6653523272778614, 0.6650875964796686, 0.6654743975903614, 0.6660847491528614, 0.6653420321912651, 0.6654743975903614, 0.6675598879894578, 0.6655964679028614, 0.6668377612010542, 0.6666951007153614, 0.6683026049510542, 0.6655758777296686, 0.6666951007153614, 0.6664612551769578, 0.6656979480421686, 0.6656979480421686, 0.6668068759412651, 0.6653420321912651, 0.6658406085278614, 0.6663288897778614, 0.6655861728162651, 0.6673157473644578, 0.6663185946912651, 0.6645787250564759, 0.6669495364269578, 0.6675598879894578, 0.6660950442394578, 0.6661862292921686, 0.6673054522778614, 0.6678040286144578, 0.6667053958019578, 0.6675598879894578, 0.6664509600903614, 0.6677937335278614, 0.6679260989269578, 0.6685261554028614, 0.6674378176769578, 0.6676819583019578, 0.6675598879894578, 0.6678040286144578, 0.6665627353162651, 0.6654641025037651, 0.6663288897778614, 0.6679260989269578, 0.6670613116528614, 0.6675495929028614, 0.6660744540662651, 0.6671936770519578, 0.6663288897778614, 0.6675701830760542, 0.6668274661144578, 0.6682923098644578, 0.6670613116528614, 0.6660744540662651, 0.6674378176769578, 0.6671936770519578, 0.6667053958019578, 0.6653317371046686, 0.6666848056287651, 0.6675701830760542, 0.6684040850903614, 0.6668068759412651, 0.6670613116528614, 0.6679055087537651, 0.6665936205760542, 0.6670510165662651, 0.6681702395519578, 0.6669392413403614, 0.6662068194653614, 0.6658509036144578, 0.6675598879894578, 0.6664406650037651, 0.6668068759412651, 0.6660744540662651, 0.6681702395519578, 0.6668171710278614, 0.6673054522778614, 0.6657082431287651, 0.6670716067394578, 0.6676922533885542, 0.6661862292921686, 0.6679260989269578, 0.6668171710278614, 0.6679260989269578, 0.6671833819653614, 0.6668068759412651], "seed": 769237292, "model": "residualv3", "loss_std": [0.2744869291782379, 0.21123261749744415, 0.22125621140003204, 0.2304604947566986, 0.23679248988628387, 0.24190212786197662, 0.245225727558136, 0.2456061840057373, 0.2503345310688019, 0.25025102496147156, 0.25133296847343445, 0.2503993809223175, 0.25078123807907104, 0.25212767720222473, 0.25204998254776, 0.2533740997314453, 0.2503790557384491, 0.2483564019203186, 0.252042680978775, 0.25110509991645813, 0.25166815519332886, 0.24990138411521912, 0.2501559555530548, 0.24962730705738068, 0.24986718595027924, 0.25001227855682373, 0.24616894125938416, 0.24845652282238007, 0.24831421673297882, 0.24861477315425873, 0.2492760717868805, 0.24816393852233887, 0.24831189215183258, 0.24860946834087372, 0.24955005943775177, 0.24922609329223633, 0.2491854578256607, 0.249693363904953, 0.24835826456546783, 0.24827875196933746, 0.24982678890228271, 0.2468939870595932, 0.24915917217731476, 0.24854522943496704, 0.24934184551239014, 0.24925018846988678, 0.248928502202034, 0.24711695313453674, 0.24813787639141083, 0.24893753230571747, 0.2486562728881836, 0.24764414131641388, 0.2494104653596878, 0.24810199439525604, 0.2487775832414627, 0.24797524511814117, 0.2489459216594696, 0.24908111989498138, 0.24703139066696167, 0.2489383965730667, 0.2491941750049591, 0.24845315515995026, 0.24773076176643372, 0.2485596239566803, 0.24933482706546783, 0.24752020835876465, 0.24694737792015076, 0.2492939978837967, 0.2463904619216919, 0.24701495468616486, 0.2516109347343445, 0.24816246330738068, 0.24855683743953705, 0.24898624420166016, 0.24750056862831116, 0.2517429292201996, 0.2501601278781891, 0.2484818994998932, 0.2496400624513626, 0.24844229221343994, 0.24971860647201538, 0.24817943572998047, 0.2467392235994339, 0.24764424562454224, 0.2481110692024231, 0.24604323506355286, 0.24846495687961578, 0.24831265211105347, 0.24793623387813568, 0.249797984957695, 0.2474311739206314, 0.24829059839248657, 0.2477092742919922, 0.24861904978752136, 0.24692247807979584, 0.24647697806358337, 0.24837668240070343, 0.24830031394958496, 0.24906007945537567, 0.24770832061767578, 0.24813446402549744, 0.2481159269809723, 0.24734199047088623, 0.24598291516304016, 0.24957668781280518, 0.24862682819366455, 0.24711649119853973, 0.24655143916606903, 0.24781537055969238, 0.24713730812072754, 0.24730977416038513, 0.24731402099132538, 0.25011253356933594, 0.24730496108531952, 0.24627937376499176, 0.24778816103935242, 0.24690666794776917, 0.2479735016822815, 0.2486739605665207, 0.2470676600933075, 0.2490091621875763, 0.24861082434654236, 0.24753114581108093, 0.2469855546951294, 0.2476654350757599, 0.24863752722740173, 0.24598564207553864, 0.2475593388080597, 0.24648824334144592, 0.2487819641828537, 0.24748852849006653, 0.24772034585475922, 0.24620682001113892, 0.24743959307670593, 0.24733512103557587]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:23 2016", "state": "available"}], "summary": "85eac231878bd9bbf799d42a9381c965"}