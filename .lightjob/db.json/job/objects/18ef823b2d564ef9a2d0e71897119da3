{"content": {"hp_model": {"f0": 32, "f1": 32, "f2": 16, "f3": 16, "nonlin": "leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.005251468584635131, 0.012308502514683864, 0.00919439211287775, 0.015408736011079886, 0.012729727405211385, 0.01069627900439636, 0.01724089218964242, 0.007632395038707481, 0.01321208880142152, 0.01057487730237949, 0.012175656922472983, 0.007971637909982898, 0.00918408859169317, 0.00922341971165427, 0.009103184389074327, 0.010081681690097787, 0.010800428580017741, 0.0082339737502968, 0.008845640207518124, 0.010408068824801057, 0.010047768579058714, 0.013141346523897483, 0.008699931845122246, 0.012229118150641772, 0.009459226801872826, 0.011973443506012978, 0.010489205089863129, 0.012661510216414259, 0.012856335680548277, 0.010312439572636872, 0.01440716378588021, 0.01191125738612466, 0.01170988352422493, 0.01509125944987918, 0.012551418076889467, 0.01139622337477955, 0.013344943430207337, 0.013150583140675626, 0.013189348082215393, 0.011203858983278029, 0.014213390951591836, 0.011184698594037625, 0.014237905899601326, 0.012169742941767049, 0.010206603524276402, 0.011896297554909601, 0.011795203946411587, 0.00977784376798811, 0.01031438786365066, 0.009913147056299471, 0.00798431535508928, 0.008070996281616638, 0.007722329541953249, 0.006558506943614823, 0.008965873465123281, 0.009588054996518807, 0.009741936952109953, 0.010469698037567214, 0.010406417747311129, 0.009589403069705658, 0.0088743349097647, 0.009730252756519326, 0.008963517930877286, 0.008635527709480163, 0.010185806351002205, 0.009406541427422352, 0.011407155337083436, 0.010730972288007585, 0.010323414892554646, 0.009129122006829086, 0.009354483802187959, 0.010685698706062643, 0.01044650332000656, 0.009307431218512192, 0.009401539598652068, 0.008833232785059899, 0.00961154426496915, 0.010698300540865176, 0.01012584344672571, 0.008618817474269631, 0.007534595102210364, 0.008869032939947757, 0.010048126790855977, 0.01102643303098911, 0.00952064140730788, 0.00854686106499043, 0.007757193517033234, 0.009960514930734417, 0.009897602604805428, 0.008205290254818039, 0.009088757081540791, 0.010194481407462111, 0.009820456311009213, 0.009814714260799582, 0.007843251092409045, 0.00909440474286534, 0.00988484959491006, 0.008778890363221417, 0.009262252563977915, 0.009124735228464896, 0.009448933359334326, 0.010177792313426071, 0.008987910346559018, 0.009128925346080972, 0.008234295678685275, 0.009934130520339828, 0.0082862623251489, 0.008845611719562652, 0.008198515322483906, 0.008790521169494895, 0.00847122097749211, 0.009263493005498804, 0.008728385910423114, 0.008666397562272615, 0.009884897188896005, 0.00853290193809514, 0.00937287117309804, 0.010091103808741581, 0.01052235916648914, 0.00923858451594677], "moving_avg_accuracy_train": [0.04557735067137319, 0.08510523010681983, 0.13125951436473327, 0.17424021480049137, 0.22179842485767923, 0.2689477246695765, 0.3128815270918769, 0.35341028171126987, 0.39113898349123183, 0.42763784296326035, 0.45906175260957915, 0.48921476374120154, 0.517891505978654, 0.5445701075494855, 0.5696432617250926, 0.5916420525641651, 0.6121801813966652, 0.6328264171041286, 0.6524543822539501, 0.6698056557995038, 0.6860425446250075, 0.7010949093024107, 0.7151349330108739, 0.7283613979508349, 0.7405512457491898, 0.7517080485748339, 0.7621304594738565, 0.7719780923401475, 0.7809966387435912, 0.7896130933054631, 0.7974842319492615, 0.8046960678155665, 0.8114543285012485, 0.8176180712290583, 0.8233978103697644, 0.8288182477309515, 0.8336733898679246, 0.8383893568173632, 0.8428291116694955, 0.846820024445883, 0.8505885933029743, 0.8540269524458219, 0.8573378587089455, 0.8604523887814342, 0.8633274373133129, 0.8659105109384784, 0.8684421072987187, 0.8709228680181824, 0.873113736035947, 0.8752297125269444, 0.8771340553200233, 0.8788131226504702, 0.8805614844752625, 0.8821349019711194, 0.8835673258566948, 0.8849168891251228, 0.8861314960667079, 0.8872898185784388, 0.8883973769568445, 0.8893778313581054, 0.8903857983549545, 0.891279089856899, 0.89207840191103, 0.8927117883026144, 0.8934235980347838, 0.8940479147032507, 0.89459595695847, 0.8951100852786346, 0.895635543735821, 0.896092180305622, 0.8964776486791757, 0.8968454605058409, 0.8971718769010394, 0.8974307744245751, 0.897700948527891, 0.8979930054435127, 0.8982186182378011, 0.8984309703478989, 0.8986220511981681, 0.898812697251524, 0.8990260232316595, 0.8992343287042669, 0.8993985160926995, 0.8995393092958602, 0.8996497471370383, 0.8997793681286224, 0.8999309042531909, 0.9000300843843502, 0.900112371055965, 0.9001330227354368, 0.9002306282576665, 0.9002650669026917, 0.9003122656272435, 0.9003593947769591, 0.9004297127974175, 0.9004882766205735, 0.9005340807126229, 0.9005381380633336, 0.9005719805646784, 0.9006093782134984, 0.900554752540312, 0.900649712611816, 0.9006770479559315, 0.9006969994680165, 0.9006986437384076, 0.9007931295341404, 0.9008409643693476, 0.9008212006543582, 0.9008057745084961, 0.9008104561188776, 0.9008310898051625, 0.9008193971394765, 0.900834414328445, 0.9008293286080404, 0.9008294378061141, 0.9008596909212668, 0.9008613781368183, 0.9008395730450819, 0.9008385857018141, 0.9008354079928822], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 23266975, "moving_var_accuracy_train": [0.018695654047991905, 0.03088816791716157, 0.04697131272368789, 0.05890024694085451, 0.07336627234136149, 0.08603715336199491, 0.09480504898333096, 0.10010776364398857, 0.10290808172160146, 0.1046067742342713, 0.10303325568798427, 0.10091276684191952, 0.09822269006590714, 0.094806151095293, 0.09098350352858753, 0.08624067436116012, 0.08141293954850745, 0.07710804903364994, 0.07286455727358843, 0.06828770178910326, 0.06383166063877883, 0.05948765771633479, 0.0553129923363092, 0.05135614747595043, 0.04755786423247894, 0.043922346052843717, 0.04050775128809195, 0.03732975901690604, 0.03432879072829505, 0.031564101258416785, 0.028965284544524184, 0.026536851279134466, 0.024294232938681756, 0.022206735164545233, 0.020286710108902213, 0.018522469368690966, 0.01688237407835378, 0.015394299768932182, 0.014032272600362251, 0.012772391803424621, 0.011622971624157903, 0.010567075284098948, 0.009609026658237773, 0.008735426670565918, 0.007936277140055249, 0.007202699850226958, 0.006540110686384905, 0.00594148718147152, 0.005390537587365743, 0.0048917800372232515, 0.00443524072676289, 0.004017090057988168, 0.003642891973822874, 0.0033008835599870044, 0.002989261747680006, 0.002706727462051415, 0.002449332146049195, 0.002216474330814917, 0.0020058670677876142, 0.00181393197850542, 0.0016416827578555116, 0.0014846962094369777, 0.0013419766863321928, 0.0012113896225883715, 0.0010948107181828332, 0.0009888375880872814, 0.000892656982100107, 0.0008057702352564522, 0.000727678171042865, 0.0006567870065504956, 0.0005924455786985375, 0.0005344185906871976, 0.0004819356605859671, 0.000434345345876607, 0.000391567757703869, 0.0003531786571111445, 0.00031831890159654986, 0.00028689285220486133, 0.0002585321740064315, 0.00023300606986473017, 0.00021011503464246417, 0.00018949405170748146, 0.00017078726402341635, 0.0001538869421555813, 0.00013860801659089988, 0.00012489842934494308, 0.00011261525518389205, 0.00010144225995125384, 9.135897382305745e-05, 8.222691486753676e-05, 7.40899649225106e-05, 6.669164261270006e-05, 6.0042527827823894e-05, 5.405826545581779e-05, 4.869694052624668e-05, 4.3858113966065845e-05, 3.9491184703095434e-05, 3.5542214391639004e-05, 3.199830078655052e-05, 2.88110579651309e-05, 2.595680784615733e-05, 2.3442283798162085e-05, 2.1104780407687115e-05, 1.8997884932428684e-05, 1.7098120771811884e-05, 1.5468656784988183e-05, 1.394238464962308e-05, 1.2551661624532407e-05, 1.129863715586462e-05, 1.0168970697560033e-05, 9.155905368891402e-06, 8.241545297879877e-06, 7.419420411772546e-06, 6.677711151563585e-06, 6.009940143725201e-06, 5.417183388140664e-06, 4.87549066959345e-06, 4.392220760864824e-06, 3.9530074583989e-06, 3.5577975930655118e-06], "duration": 69281.665371, "accuracy_train": [0.455773506713732, 0.44085614502583975, 0.5466480726859542, 0.5610665187223145, 0.6498223153723699, 0.6932914229766519, 0.7082857488925802, 0.7181690732858066, 0.7306972995108896, 0.7561275782115172, 0.7418769394264488, 0.7605918639258029, 0.7759821861157253, 0.7846775216869692, 0.7953016493055556, 0.7896311701158176, 0.7970233408891657, 0.8186425384712993, 0.829106068602344, 0.8259671177094868, 0.8321745440545404, 0.8365661913990403, 0.8414951463870433, 0.8473995824104835, 0.8502598759343853, 0.8521192740056294, 0.8559321575650609, 0.8606067881367663, 0.8621635563745846, 0.8671611843623109, 0.8683244797434477, 0.8696025906123109, 0.8722786746723883, 0.8730917557793466, 0.8754154626361205, 0.8776021839816353, 0.8773696691006828, 0.8808330593623109, 0.8827869053386858, 0.8827382394333703, 0.8845057130167959, 0.8849721847314507, 0.887136015077058, 0.8884831594338316, 0.8892028741002216, 0.8891581735649685, 0.8912264745408823, 0.8932497144933554, 0.8928315481958287, 0.894273500945921, 0.8942731404577334, 0.8939247286244923, 0.8962967408983942, 0.8962956594338316, 0.8964591408268733, 0.8970629585409744, 0.8970629585409744, 0.8977147211840162, 0.8983654023624953, 0.8982019209694537, 0.8994575013265966, 0.8993187133744001, 0.8992722103982096, 0.8984122658268733, 0.8998298856243078, 0.8996667647194537, 0.8995283372554448, 0.8997372401601144, 0.9003646698504982, 0.9002019094338316, 0.8999468640411591, 0.9001557669458287, 0.9001096244578257, 0.8997608521363971, 0.9001325154577334, 0.9006215176841085, 0.9002491333863971, 0.900342139338778, 0.9003417788505905, 0.9005285117317275, 0.9009459570528792, 0.9011090779577334, 0.9008762025885935, 0.9008064481243078, 0.9006436877076411, 0.9009459570528792, 0.9012947293743078, 0.9009227055647839, 0.9008529511004982, 0.9003188878506828, 0.9011090779577334, 0.900575014707918, 0.9007370541482096, 0.9007835571244001, 0.901062574981543, 0.9010153510289776, 0.9009463175410668, 0.9005746542197305, 0.900876563076781, 0.9009459570528792, 0.9000631214816353, 0.9015043532553525, 0.9009230660529715, 0.900876563076781, 0.9007134421719268, 0.9016435016957364, 0.9012714778862125, 0.9006433272194537, 0.9006669391957364, 0.9008525906123109, 0.9010167929817275, 0.9007141631483019, 0.900969569029162, 0.9007835571244001, 0.900830420588778, 0.9011319689576411, 0.900876563076781, 0.9006433272194537, 0.900829699612403, 0.9008068086124953], "end": "2016-01-22 04:58:48.834000", "learning_rate_per_epoch": [0.0025505414232611656, 0.002356079174205661, 0.002176443347707391, 0.0020105037838220596, 0.0018572158878669143, 0.0017156152753159404, 0.0015848107868805528, 0.0014639792498201132, 0.00135236035566777, 0.0012492516543716192, 0.0011540043633431196, 0.0010660190600901842, 0.000984742073342204, 0.0009096618741750717, 0.0008403060492128134, 0.0007762381574138999, 0.0007170550525188446, 0.0006623842637054622, 0.0006118817836977541, 0.0005652297986671329, 0.0005221347091719508, 0.0004823253257200122, 0.00044555115164257586, 0.00041158078238368034, 0.0003802004212047905, 0.0003512125986162573, 0.0003244349209126085, 0.00029969887691549957, 0.00027684879023581743, 0.0002557408588472754, 0.00023624228197149932, 0.00021823034330736846, 0.00020159169798716903, 0.00018622164498083293, 0.00017202345770783722, 0.00015890778740867972, 0.00014679209562018514, 0.00013560014485847205, 0.0001252615184057504, 0.00011571114009711891, 0.00010688891779864207, 9.873932867776603e-05, 9.121109906118363e-05, 8.425684791291133e-05, 7.78328103478998e-05, 7.189856114564463e-05, 6.641676009166986e-05, 6.135291187092662e-05, 5.66751514270436e-05, 5.235404023551382e-05, 4.8362384404754266e-05, 4.4675067329080775e-05, 4.126888234168291e-05, 3.8122398109408095e-05, 3.521581311360933e-05, 3.2530835596844554e-05, 3.0050570785533637e-05, 2.775940993160475e-05, 2.564293572504539e-05, 2.368782770645339e-05, 2.188178405049257e-05, 2.0213439711369574e-05, 1.8672295482247137e-05, 1.7248654330614954e-05, 1.593355591467116e-05, 1.4718725651619025e-05, 1.3596518328995444e-05, 1.2559871720441151e-05, 1.1602262929955032e-05, 1.0717665645643137e-05, 9.900512850435916e-06, 9.145663170784246e-06, 8.448365406366065e-06, 7.80423215474002e-06, 7.209210252767662e-06, 6.659554856014438e-06, 6.151807156129507e-06, 5.6827716434781905e-06, 5.249497007753234e-06, 4.8492570385860745e-06, 4.479532890400151e-06, 4.137997621000977e-06, 3.822502094408264e-06, 3.5310611110617174e-06, 3.261840674895211e-06, 3.013146624653018e-06, 2.783413719953387e-06, 2.5711965463415254e-06, 2.3751595108478796e-06, 2.19406911128317e-06, 2.0267855234124e-06, 1.8722563481787802e-06, 1.7295089946856024e-06, 1.597645223228028e-06, 1.4758351198906894e-06, 1.363312321700505e-06, 1.2593685596584692e-06, 1.1633499070740072e-06, 1.074652004717791e-06, 9.92716763903445e-07, 9.170285579784831e-07, 8.471110959362704e-07, 7.825243528714054e-07, 7.228619551824522e-07, 6.677484520878352e-07, 6.168369850456656e-07, 5.698071845472441e-07, 5.263630669105623e-07, 4.862313289777376e-07, 4.491593585953524e-07, 4.14913870372402e-07, 3.8327937090798514e-07, 3.5405682297096064e-07, 3.2706228125789494e-07, 3.0212589763323194e-07, 2.790907558392064e-07, 2.5781190515772323e-07, 2.381554224939464e-07, 2.1999761656843475e-07, 2.032242321092781e-07], "accuracy_valid": [0.4466935123305723, 0.4358086643448795, 0.5403861539909638, 0.5566729809864458, 0.6366628623870482, 0.6822083254894578, 0.706787109375, 0.7063885424510542, 0.7185455689947289, 0.7360428040286144, 0.7243431734751506, 0.7398269837161144, 0.7469982468938253, 0.7571918533509037, 0.7667442229856928, 0.7580669357115963, 0.7643837067018072, 0.7918715879141567, 0.797375047063253, 0.7949027555534638, 0.7992869917168675, 0.8013724821159638, 0.804699265813253, 0.8083304899284638, 0.8124205807605422, 0.8120337796498494, 0.814098679875753, 0.8172930981739458, 0.816906297063253, 0.8206095867846386, 0.8188491269766567, 0.8221759106739458, 0.8234274990587349, 0.8228980374623494, 0.8244952466114458, 0.8251261883471386, 0.8263263012989458, 0.8290221432605422, 0.8311076336596386, 0.8321047863328314, 0.8340373211596386, 0.8357874858810241, 0.8329386883471386, 0.8358992611069277, 0.8355639354292168, 0.8357771907944277, 0.8372523296310241, 0.8395922557417168, 0.8384833278426205, 0.8383715526167168, 0.8392466349774097, 0.842308687876506, 0.842796969126506, 0.841332125376506, 0.8411997599774097, 0.8419321818524097, 0.8405688182417168, 0.8406908885542168, 0.8413218302899097, 0.8409556193524097, 0.841820406626506, 0.8422983927899097, 0.843163180064006, 0.841698336314006, 0.8418101115399097, 0.842796969126506, 0.8430308146649097, 0.843163180064006, 0.8432749552899097, 0.8448824595256024, 0.843285250376506, 0.8433970256024097, 0.8435190959149097, 0.843895601939006, 0.843529391001506, 0.8441500376506024, 0.8429087443524097, 0.8427866740399097, 0.8427866740399097, 0.8432955454631024, 0.8429293345256024, 0.842552828501506, 0.8429087443524097, 0.8419218867658133, 0.842186617564006, 0.8430514048381024, 0.8419527720256024, 0.842796969126506, 0.8420542521649097, 0.8430514048381024, 0.8434176157756024, 0.8436617564006024, 0.8426646037274097, 0.8424204631024097, 0.842064547251506, 0.8436617564006024, 0.843041109751506, 0.8435396860881024, 0.843041109751506, 0.842308687876506, 0.8429087443524097, 0.8429087443524097, 0.843041109751506, 0.8431734751506024, 0.8430514048381024, 0.8433970256024097, 0.843163180064006, 0.842186617564006, 0.842552828501506, 0.8422983927899097, 0.843163180064006, 0.842552828501506, 0.8431734751506024, 0.842064547251506, 0.8425425334149097, 0.8430514048381024, 0.8434176157756024, 0.8422983927899097, 0.843651461314006, 0.843163180064006], "accuracy_test": 0.8361863057324841, "start": "2016-01-21 09:44:07.169000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0], "accuracy_train_last": 0.9008068086124953, "batch_size_eval": 1024, "accuracy_train_std": [0.019099773899289508, 0.01693582530592173, 0.019838851617377785, 0.018638254049409428, 0.01795551197774461, 0.018053672839282612, 0.017088210739191527, 0.01655435502165481, 0.01651338509221566, 0.018106778406990563, 0.017033860756161718, 0.01596401978989289, 0.017709105482091064, 0.017234468660006025, 0.016575390277854603, 0.014235319124569847, 0.015849167951967374, 0.016020346339766324, 0.0164314777030547, 0.014529278076832707, 0.015520462261467019, 0.016254851115357842, 0.015703590728030296, 0.015394816536753675, 0.014918772880214958, 0.016026018348426173, 0.013508555030819637, 0.014708222607293577, 0.014994712009324182, 0.014414360639801598, 0.015356644616864291, 0.015399988721241938, 0.013693837780994585, 0.014209684518629265, 0.01279147329389204, 0.014040522476160082, 0.013298771959970843, 0.012938104155612766, 0.01343394620382453, 0.013743274534390506, 0.012491042268441211, 0.01311406211403801, 0.012010487809858077, 0.011456377136016072, 0.01087697192214653, 0.011018810485210065, 0.011115974366566169, 0.010961729074016673, 0.011333805658480441, 0.01102551848014018, 0.011273842052485225, 0.010855042050542676, 0.010072679613695299, 0.010481214443198342, 0.010344689180724852, 0.01043107833931938, 0.010407105823977759, 0.011005524650863494, 0.010981545840279522, 0.010869755224389112, 0.010949495749698685, 0.011187450333560322, 0.010675973069565617, 0.010994149500004232, 0.011118938791381464, 0.011324910985118782, 0.011057602753181767, 0.011140467598140944, 0.011039848752222102, 0.01094634364924123, 0.010766355441128528, 0.011157139944834044, 0.010838029500015969, 0.010659536506341268, 0.010640985692294845, 0.011324625827030801, 0.011322274268114666, 0.010970647372653413, 0.010978988778186241, 0.010786000802735937, 0.011831548392832963, 0.011182242588684072, 0.010762447515475821, 0.010753761267688334, 0.010889929566210245, 0.011035166479707387, 0.011249733774942872, 0.01107840922612335, 0.011273510975268616, 0.010775258096895736, 0.011449126021204388, 0.011010333204454348, 0.010795265190048284, 0.01129330519628832, 0.011497462544665151, 0.011596234316158856, 0.011469073151151804, 0.01117429613664046, 0.011413702816481837, 0.011063936103957834, 0.011011325236273895, 0.011539114958709552, 0.011413617629184343, 0.011208946408516378, 0.010980495300944094, 0.011575891867447565, 0.011539506075985773, 0.011252765137127108, 0.01122916377255173, 0.011315560970573655, 0.011474099218853005, 0.01125819015939704, 0.011277247424405121, 0.01103088650879961, 0.011405559551001266, 0.011118574681308663, 0.010773036868137215, 0.010833303210166179, 0.010933971541952452, 0.010888105860575628], "accuracy_test_std": 0.04556643155963063, "error_valid": [0.5533064876694277, 0.5641913356551205, 0.4596138460090362, 0.4433270190135542, 0.36333713761295183, 0.31779167451054224, 0.293212890625, 0.2936114575489458, 0.2814544310052711, 0.26395719597138556, 0.27565682652484935, 0.26017301628388556, 0.2530017531061747, 0.24280814664909633, 0.23325577701430722, 0.24193306428840367, 0.23561629329819278, 0.20812841208584332, 0.20262495293674698, 0.2050972444465362, 0.20071300828313254, 0.1986275178840362, 0.19530073418674698, 0.1916695100715362, 0.18757941923945776, 0.18796622035015065, 0.18590132012424698, 0.1827069018260542, 0.18309370293674698, 0.17939041321536142, 0.18115087302334332, 0.1778240893260542, 0.1765725009412651, 0.17710196253765065, 0.1755047533885542, 0.17487381165286142, 0.1736736987010542, 0.17097785673945776, 0.16889236634036142, 0.16789521366716864, 0.16596267884036142, 0.16421251411897586, 0.16706131165286142, 0.1641007388930723, 0.1644360645707832, 0.1642228092055723, 0.16274767036897586, 0.1604077442582832, 0.16151667215737953, 0.1616284473832832, 0.1607533650225903, 0.15769131212349397, 0.15720303087349397, 0.15866787462349397, 0.1588002400225903, 0.1580678181475903, 0.1594311817582832, 0.1593091114457832, 0.1586781697100903, 0.1590443806475903, 0.15817959337349397, 0.1577016072100903, 0.15683681993599397, 0.15830166368599397, 0.1581898884600903, 0.15720303087349397, 0.1569691853350903, 0.15683681993599397, 0.1567250447100903, 0.15511754047439763, 0.15671474962349397, 0.1566029743975903, 0.1564809040850903, 0.15610439806099397, 0.15647060899849397, 0.15584996234939763, 0.1570912556475903, 0.1572133259600903, 0.1572133259600903, 0.15670445453689763, 0.15707066547439763, 0.15744717149849397, 0.1570912556475903, 0.15807811323418675, 0.15781338243599397, 0.15694859516189763, 0.15804722797439763, 0.15720303087349397, 0.1579457478350903, 0.15694859516189763, 0.15658238422439763, 0.15633824359939763, 0.1573353962725903, 0.1575795368975903, 0.15793545274849397, 0.15633824359939763, 0.15695889024849397, 0.15646031391189763, 0.15695889024849397, 0.15769131212349397, 0.1570912556475903, 0.1570912556475903, 0.15695889024849397, 0.15682652484939763, 0.15694859516189763, 0.1566029743975903, 0.15683681993599397, 0.15781338243599397, 0.15744717149849397, 0.1577016072100903, 0.15683681993599397, 0.15744717149849397, 0.15682652484939763, 0.15793545274849397, 0.1574574665850903, 0.15694859516189763, 0.15658238422439763, 0.1577016072100903, 0.15634853868599397, 0.15683681993599397], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-3, 3], "momentum": 0.7793417287436722, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.002761053849971295, "patience_threshold": 1, "do_flip": true, "batch_size": 64, "optimization": "adam", "nb_data_augmentation": 3, "learning_rate_decay_method": "exp", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.00011528945064681189, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.07624350337232716}, "accuracy_valid_max": 0.8448824595256024, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import os\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-6, -3], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128, 256],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-3, 3)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.843163180064006, "loss_train": [2.464749813079834, 1.804128885269165, 1.5849543809890747, 1.4435124397277832, 1.3344347476959229, 1.24275803565979, 1.1728672981262207, 1.11786687374115, 1.073885202407837, 1.0381580591201782, 1.00424063205719, 0.9787262678146362, 0.9525014162063599, 0.9310804009437561, 0.9119390249252319, 0.8915410041809082, 0.8754857182502747, 0.8602566719055176, 0.8449386954307556, 0.8306313753128052, 0.816281795501709, 0.8050803542137146, 0.7935161590576172, 0.7828859090805054, 0.7712879776954651, 0.7632879018783569, 0.7528031468391418, 0.7427670359611511, 0.7343992590904236, 0.7274137139320374, 0.7198289632797241, 0.7133134007453918, 0.7072901129722595, 0.6997774243354797, 0.6954067349433899, 0.6875225305557251, 0.6836654543876648, 0.6778882741928101, 0.6752304434776306, 0.6689432263374329, 0.6681355834007263, 0.6628196835517883, 0.6603564023971558, 0.6553173065185547, 0.6541693210601807, 0.6503275632858276, 0.6488110423088074, 0.6450900435447693, 0.6440576910972595, 0.6402812600135803, 0.6388571858406067, 0.6355701684951782, 0.6353641748428345, 0.6353242993354797, 0.6326721906661987, 0.6310345530509949, 0.6300868391990662, 0.6276741027832031, 0.6260852217674255, 0.6264175772666931, 0.6254444718360901, 0.6251762509346008, 0.6245323419570923, 0.6225183010101318, 0.6215135455131531, 0.6209307312965393, 0.6200875043869019, 0.6209889650344849, 0.6211580038070679, 0.618882417678833, 0.6170874238014221, 0.6178049445152283, 0.6174135208129883, 0.6156170964241028, 0.6181108355522156, 0.6175107359886169, 0.6150048971176147, 0.6163404583930969, 0.6145698428153992, 0.6147622466087341, 0.6137956380844116, 0.6137269735336304, 0.6132907867431641, 0.6150118112564087, 0.6144464612007141, 0.6138439774513245, 0.6139599084854126, 0.6115568280220032, 0.6125458478927612, 0.6126976609230042, 0.6137115955352783, 0.6127411723136902, 0.6119483709335327, 0.6142330169677734, 0.6120242476463318, 0.6132456660270691, 0.6137586236000061, 0.611386775970459, 0.6118420958518982, 0.6119644641876221, 0.6130104064941406, 0.611887514591217, 0.612220048904419, 0.6132982969284058, 0.6108793020248413, 0.6131369471549988, 0.6124825477600098, 0.611998438835144, 0.6124594211578369, 0.6122170686721802, 0.6114268898963928, 0.6105148792266846, 0.6117361187934875, 0.6129603981971741, 0.6116783022880554, 0.6115072965621948, 0.6118408441543579, 0.6129486560821533, 0.6117069125175476, 0.611373245716095], "accuracy_train_first": 0.455773506713732, "model": "residualv3", "loss_std": [0.4792715013027191, 0.16123631596565247, 0.1412244588136673, 0.13869166374206543, 0.13250184059143066, 0.12776748836040497, 0.12453446537256241, 0.12204089760780334, 0.12040609866380692, 0.11651694029569626, 0.11482121050357819, 0.11272244155406952, 0.10993331670761108, 0.10987144708633423, 0.10925047099590302, 0.10800227522850037, 0.10624784231185913, 0.10460671037435532, 0.10461944341659546, 0.10306113958358765, 0.10235534608364105, 0.09992092102766037, 0.09951110184192657, 0.09884956479072571, 0.09877616912126541, 0.09789154678583145, 0.098674476146698, 0.09696406871080399, 0.09723125398159027, 0.09478607028722763, 0.0946861132979393, 0.09291107207536697, 0.09330248832702637, 0.09357154369354248, 0.0936644971370697, 0.09254159778356552, 0.09342829138040543, 0.0926087498664856, 0.09020089358091354, 0.08871480077505112, 0.09106340259313583, 0.08917129039764404, 0.08921453356742859, 0.0905071347951889, 0.0884612426161766, 0.08729156851768494, 0.08944092690944672, 0.08816199004650116, 0.08861338347196579, 0.08812259137630463, 0.08807002753019333, 0.08670245110988617, 0.08681388199329376, 0.08730746805667877, 0.08569934219121933, 0.0862160325050354, 0.08644025027751923, 0.08533892780542374, 0.08658350259065628, 0.0868360623717308, 0.08601206541061401, 0.08670748770236969, 0.08612248301506042, 0.08475323021411896, 0.08563930541276932, 0.08619966357946396, 0.08402324467897415, 0.08700917661190033, 0.08532573282718658, 0.08506423234939575, 0.08502202481031418, 0.08458959311246872, 0.08597635477781296, 0.08506588637828827, 0.08593501895666122, 0.08610494434833527, 0.08455098420381546, 0.08395227789878845, 0.08492957800626755, 0.08409855514764786, 0.0845944806933403, 0.0844830870628357, 0.0849291980266571, 0.08495358377695084, 0.08431677520275116, 0.08544686436653137, 0.08346375077962875, 0.08471106737852097, 0.08463495969772339, 0.08658545464277267, 0.08537357300519943, 0.08418742567300797, 0.08403335511684418, 0.08490235358476639, 0.08396131545305252, 0.08613387495279312, 0.0847957655787468, 0.08541170507669449, 0.08469720929861069, 0.08435487002134323, 0.08540352433919907, 0.08508388698101044, 0.08505214750766754, 0.08472676575183868, 0.08439900726079941, 0.08353973925113678, 0.0840674340724945, 0.08295824378728867, 0.08432533591985703, 0.08378856629133224, 0.08567831665277481, 0.08370279520750046, 0.08393064141273499, 0.08538342267274857, 0.0825488269329071, 0.08418057858943939, 0.0835736021399498, 0.08346625417470932, 0.08510859310626984, 0.08421774953603745]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:07 2016", "state": "available"}], "summary": "6f4fca1d5ea822e7cbbd78772a5e8d64"}