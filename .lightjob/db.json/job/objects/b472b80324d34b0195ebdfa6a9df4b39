{"content": {"hp_model": {"f0": 16, "f1": 32, "f2": 32, "f3": 64, "nonlin": "leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.017783360136781657, 0.005236387547563666, 0.010235873358965782, 0.010412382026448673, 0.017362476783128963, 0.008679031904005043, 0.00850766836765626, 0.008187784025321002, 0.006945233467790059, 0.010272149223970212, 0.014154388262529365, 0.01045071786952001, 0.01308154663700065, 0.012614909080044864, 0.012808527210768618, 0.011116721388779128, 0.012888013104878752, 0.008980566593026795, 0.012849035399662168, 0.011729949986574223, 0.010181626288947735, 0.011462258860339197, 0.013019807983477774, 0.011399114086802881, 0.009192006188749538, 0.008688111925659476, 0.00933202000907023, 0.013104127962208407, 0.01273649208215111, 0.009139291886183426, 0.00958119066675025, 0.012111743160652853, 0.008756630926671572, 0.01128845742449576, 0.010292618841500208, 0.007820749516446616, 0.012340016740255573, 0.010384964531941026, 0.011201439561478973, 0.010604027621093647, 0.012838366145058448, 0.009645940706324583, 0.010227747444876381, 0.01211808573311753, 0.014523212905448361, 0.010848649272244519, 0.012867050469383341, 0.015194623601862281, 0.011621543845138076, 0.009975308146881322, 0.010173430256205769, 0.013957331962571425, 0.013887666167404769, 0.01345295973909134, 0.013258292711921416, 0.013802315440009415, 0.01174524886262743, 0.011198926617190567, 0.014144792967249005, 0.010743351997070612, 0.010413504888202686, 0.014270489472571826, 0.00981863284360653, 0.011099994026677655, 0.01125361971479588, 0.012866226131962494, 0.016317863834191378, 0.014435059486303942, 0.012105629283555684, 0.012844086528077009, 0.016872098706884674, 0.01261179648961026, 0.013978681101998517, 0.012615660767685721, 0.013702462780507237, 0.013074516122369357, 0.012183583421248043, 0.01254159800082718, 0.010569946212243709, 0.010587764367641203, 0.011834445660840751, 0.009139998095463052, 0.012041218355473456, 0.01425822511490091, 0.010179569612387163, 0.012289636592089476, 0.011236688532682471, 0.009301225144263887, 0.006187959321653939, 0.010122042941367943, 0.009180870942075054, 0.007333652347750236, 0.010595342565869009, 0.012660357792320278, 0.009686099910520262, 0.012587252390405531, 0.011898879908090543, 0.010635972945976195, 0.008062501398259892, 0.00915911967249199, 0.012419705451246604, 0.013183591124874066, 0.011169944134058933, 0.011019803785583747, 0.014478809675250268, 0.013548158899814531, 0.009715457657462918, 0.010981747145990156, 0.009176391515421743, 0.008887334805975906, 0.00545324005674538, 0.011616728961793847, 0.010459156867331546, 0.011105298633068153, 0.009432987052351531, 0.011080932011507602, 0.010349272536100044, 0.009350789752297608, 0.008563719412493861, 0.006028893726437139, 0.006788951829287468, 0.010600466303173475, 0.010489383774090635, 0.011516023664098787, 0.012167967111577001, 0.007478680190119616, 0.011866824405522511, 0.009465770432341104, 0.009348347721373499, 0.007821093570262626, 0.00997965439044456, 0.01185185667106324, 0.010549757174653005, 0.010014278675272338, 0.0105104148744992, 0.012274582028006934, 0.006151423380069959, 0.007714534301290371, 0.010432405238709244, 0.010502075868180508, 0.0069460556258163775, 0.012389971196520105, 0.00902118447461094, 0.012474822449121724, 0.010229507606499188, 0.010193386514122674, 0.013007480047885652, 0.015870804203623558, 0.010462657953741003, 0.012597955405464083, 0.010000679558804505, 0.012039609427521487, 0.010188806951504004, 0.010479349234700418, 0.014537841828179411, 0.011551667647276293, 0.011055069377514156, 0.01305446275324267, 0.014650855361134028, 0.009665871596024611, 0.011204415276971223, 0.007957089407988166, 0.007107819562936818, 0.012696725900045096, 0.01281868471495863, 0.013438343578869654, 0.011191999775708044, 0.011380870215824996, 0.009369956822562945, 0.010703815537468603, 0.01262348532989835, 0.014081906523014057, 0.012830660824002107, 0.014292609728976672, 0.011216547829844227, 0.0152254294030803, 0.011949725313301391, 0.010605402068625142, 0.011006150974248457, 0.012223752679772295, 0.011690883809297711, 0.013253157106041273, 0.01304713440172943, 0.016167174081926007, 0.013168766577766187, 0.013467382881617352, 0.013401455545257884, 0.012568296844901723, 0.013906119907675691, 0.013015346928160072, 0.011447135518686244], "moving_avg_accuracy_train": [0.024620387914128825, 0.05198849429851882, 0.09972468561825164, 0.15571079480075095, 0.2107430391659601, 0.2633731226789968, 0.31461989788571876, 0.36188811369077, 0.4056987147259473, 0.4471762431241536, 0.48438532723735656, 0.5189963335213267, 0.550527239142293, 0.5804235566178754, 0.6076185968470993, 0.6330423611818614, 0.6558377546260138, 0.6774348750198169, 0.6970071420051922, 0.7152009641015077, 0.731970462992898, 0.7470792519879973, 0.7607864440776342, 0.7734160659524012, 0.7854263232252858, 0.7965843991899481, 0.8068475566950488, 0.815765817013916, 0.8238992081461346, 0.8322235802555705, 0.839822471999301, 0.8466406563758291, 0.8529118448968379, 0.8587813819026322, 0.8643497782673246, 0.8694310173621961, 0.8739995903964176, 0.8785343390152753, 0.8825042579711213, 0.8864071398694227, 0.8898919399386358, 0.8931422643902318, 0.8962091381323178, 0.8988602587990602, 0.9018158207110054, 0.9041665816400895, 0.9065030835155323, 0.9088175237450976, 0.9108236818945358, 0.9130222464754771, 0.9149777752078667, 0.9166097236872184, 0.9184738607627103, 0.9201190320473197, 0.9211787621713068, 0.9225138436876571, 0.9241640626261542, 0.9257492050207925, 0.9265062984652802, 0.9281152366962253, 0.9294681302469793, 0.9304695316521909, 0.932014642844207, 0.9331124183622964, 0.9343631941928718, 0.9352679672546662, 0.936512271344768, 0.9374252788794495, 0.9383887836892251, 0.9393117055406329, 0.9400935791795375, 0.9410389006866484, 0.9419432405633047, 0.9428571638999237, 0.9437517384671572, 0.9446219957931529, 0.9452912590460635, 0.945503115168958, 0.9463470443485831, 0.9471274348518938, 0.9477763078822544, 0.9482929003429216, 0.948904317932522, 0.9495638397083912, 0.9502853285400161, 0.9511717255205937, 0.9519322804221612, 0.9522797053538284, 0.9529434131649296, 0.9535895783199205, 0.954194486593964, 0.9546063705584602, 0.9550165215586313, 0.9553321790361661, 0.955802318719528, 0.956213854739325, 0.9566166450452005, 0.9570396822871736, 0.9575204332525777, 0.9579182679381173, 0.958164675963427, 0.9583050629778724, 0.958840565383712, 0.9591226628978048, 0.9592044175509461, 0.95948264588283, 0.9598469476243736, 0.9601910952334295, 0.9607171029696842, 0.9611067324775331, 0.961615473104826, 0.9622012949515508, 0.9624495167564604, 0.9625009274666116, 0.9628051804771487, 0.9631929044294799, 0.9635140262985014, 0.9637239448722781, 0.9640222256803622, 0.9644348015850096, 0.9645853389087438, 0.9648508866381629, 0.9649736221541639, 0.9651607779804603, 0.9652990994360595, 0.9655932885603754, 0.9656488674770399, 0.9658732386139335, 0.9658821492371286, 0.9660529292146708, 0.9664483745730118, 0.9667716872633666, 0.966881451272818, 0.9671777683836776, 0.9675095219012991, 0.9679243935564534, 0.9680861895044257, 0.9683550026956868, 0.9685760803261735, 0.9687121990781169, 0.9689904548762853, 0.9692734732267889, 0.9694375449874896, 0.9696712400780725, 0.9696537371250825, 0.9699030514316772, 0.9701970806254419, 0.9702222526212679, 0.9703356603187203, 0.9704516060416472, 0.9704630233375379, 0.9707244510240868, 0.9710154313669531, 0.9713633441814852, 0.9716439496800495, 0.9717476851049479, 0.9718736711683272, 0.9721311097051029, 0.9721442764489245, 0.9722654445612303, 0.9723907358551535, 0.9726824984291988, 0.9728149124613251, 0.9729735405223632, 0.9731977941320871, 0.9733925387879536, 0.9737026676091859, 0.9737446544185423, 0.973724313826725, 0.9740105296904995, 0.9742681600167153, 0.9745209896984142, 0.9746113886809998, 0.9748554360843561, 0.9747472688140526, 0.9749104070350837, 0.9750617735851744, 0.9750817820885986, 0.9751346669738233, 0.9753240974479065, 0.9754201801126766, 0.9755740477776271, 0.9754288965701394, 0.9754587318000764, 0.9755994797498675, 0.9757332004487457, 0.975972095618203, 0.9761382731457146, 0.9763529370871415, 0.9764717298725212, 0.976520478610306], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 608640739, "moving_var_accuracy_train": [0.005455471509379628, 0.01165104358204726, 0.030994634879269754, 0.05610517118389538, 0.07775158534435468, 0.09490575802522216, 0.10905126994449457, 0.11825470097858118, 0.1237035497482944, 0.12681666303168104, 0.12659564019340366, 0.12471737197796448, 0.12119341686367258, 0.11711818336471265, 0.11206249694586334, 0.10667355738782275, 0.1006828713095038, 0.0948125046622933, 0.08877891691058526, 0.0828801616817781, 0.07712309035121538, 0.07146526086017975, 0.06600971880900164, 0.06084431306639782, 0.056058098277605946, 0.051572812382943915, 0.047363522762420084, 0.04334298879021381, 0.0396040583729793, 0.03626730907482858, 0.033160266568942144, 0.030262628655778907, 0.027590316039395314, 0.02514134761741727, 0.02290627619834443, 0.02084801949516324, 0.01895106428176806, 0.01724103335891734, 0.015658772331669478, 0.014229987482511322, 0.012916283217961682, 0.011719736377531301, 0.010632414170727246, 0.009632428720761178, 0.008747803964623133, 0.007922758260672192, 0.007179615603730506, 0.0065098637455435265, 0.005895099405674189, 0.005349092641055901, 0.00484860021055912, 0.004387709492056535, 0.003980213606176893, 0.0036065515425605333, 0.0032560036397256517, 0.0029464452596507885, 0.0026763097365904804, 0.0024312928506329454, 0.0021933222799228277, 0.0019972881920095192, 0.0018140322614456131, 0.0016416542782702903, 0.0014989751678045006, 0.0013599236508170997, 0.0012380112473405552, 0.001121577651246639, 0.0010233545201397728, 0.0009285213129512632, 0.0008440242553222835, 0.0007672878924843103, 0.0006960610407208043, 0.0006344976314149809, 0.0005784083437860794, 0.0005280848121944219, 0.0004824787038820495, 0.0004410469638148857, 0.00040097348714866575, 0.00036128008558507045, 0.0003315620251685668, 0.0003038869066906289, 0.0002772875419073308, 0.0002519605976503608, 0.00023012902110517978, 0.00021103083975027357, 0.00019461267098268113, 0.00018222270034900639, 0.00016920642413878999, 0.00015337211847320684, 0.00014199947915253565, 0.00013155729590500284, 0.00012169479249455777, 0.00011105214884698375, 0.00010146094854875654, 9.221161048199367e-05, 8.497973133063974e-05, 7.800601525788831e-05, 7.16655740066654e-05, 6.61096611788649e-05, 6.157878847761115e-05, 5.684536156301525e-05, 5.170727764114709e-05, 4.671392650145647e-05, 4.462339929125045e-05, 4.087727042924147e-05, 3.684969779610957e-05, 3.386142705846515e-05, 3.166972618264329e-05, 2.9568691755748426e-05, 2.9101979827571714e-05, 2.755808222529346e-05, 2.7131627235488864e-05, 2.750714963684118e-05, 2.531096125304991e-05, 2.2803652677809258e-05, 2.1356416459816174e-05, 2.0573743582737048e-05, 1.9444442517337378e-05, 1.7896590534151502e-05, 1.6907674444978235e-05, 1.674887689434122e-05, 1.5277942577440544e-05, 1.4384788689093197e-05, 1.308188588217614e-05, 1.208894302380882e-05, 1.1052244147139754e-05, 1.072594490021704e-05, 9.68115155399379e-06, 9.166118062232986e-06, 8.250220848861208e-06, 7.68769097053899e-06, 8.32631515638586e-06, 8.434463502447295e-06, 7.699450392140321e-06, 7.719739824619842e-06, 7.938309410246237e-06, 8.693544881476025e-06, 8.059791752350514e-06, 7.904157363278855e-06, 7.553619495265484e-06, 6.965012377414545e-06, 6.965347742601983e-06, 6.989707448837942e-06, 6.533012587888663e-06, 6.37123188736278e-06, 5.736865878896823e-06, 5.722597902262544e-06, 5.928416613109768e-06, 5.341277616163599e-06, 4.922901607120325e-06, 4.551602142393433e-06, 4.097615119963184e-06, 4.302953525615755e-06, 4.634684212465647e-06, 5.260605729859797e-06, 5.443200169294486e-06, 4.995729497774592e-06, 4.639008941489532e-06, 4.7715794492951415e-06, 4.295981772651394e-06, 3.998518998343971e-06, 3.7399482735059227e-06, 4.132082042677623e-06, 3.876675121545267e-06, 3.7154733651289164e-06, 3.7965331618834942e-06, 3.758209174592546e-06, 4.248007228963871e-06, 3.839072535506877e-06, 3.458888939035496e-06, 3.850275731217751e-06, 4.062608622970646e-06, 4.231653392204435e-06, 3.88203583745673e-06, 4.029864469475516e-06, 3.7321794478120553e-06, 3.5984882154814786e-06, 3.4448458863107063e-06, 3.103964359563137e-06, 2.818739223373859e-06, 2.8598204416387828e-06, 2.6569253036987775e-06, 2.604310098184778e-06, 2.533498945682546e-06, 2.2881603196228192e-06, 2.2376341559943467e-06, 2.174801768171051e-06, 2.4709597092642277e-06, 2.472398474186526e-06, 2.6398840965089442e-06, 2.5029012195822548e-06, 2.2739990525445275e-06], "duration": 144635.14433, "accuracy_train": [0.2462038791412883, 0.2983014517580288, 0.5293504074958472, 0.6595857774432448, 0.7060332384528424, 0.7370438742963271, 0.7758408747462164, 0.7873020559362311, 0.7999941240425433, 0.8204739987080103, 0.8192670842561831, 0.830495390077058, 0.8343053897309893, 0.8494904138981173, 0.8523739589101144, 0.8618562401947213, 0.860996295623385, 0.8718089585640458, 0.8731575448735696, 0.8789453629683462, 0.8828959530154117, 0.8830583529438908, 0.884151172884367, 0.8870826628253045, 0.8935186386812477, 0.8970070828719084, 0.899215974240956, 0.8960301598837209, 0.8970997283361019, 0.9071429292404946, 0.9082124976928755, 0.908004315764581, 0.9093525415859173, 0.9116072149547803, 0.914465345549557, 0.9151621692160392, 0.9151167477044113, 0.9193470765849945, 0.9182335285737356, 0.9215330769541344, 0.9212551405615541, 0.9223951844545959, 0.9238110018110927, 0.9227203447997416, 0.9284158779185124, 0.9253234300018457, 0.9275316003945183, 0.929647485811185, 0.9288791052394795, 0.9328093277039498, 0.9325775337993725, 0.9312972600013842, 0.9352510944421374, 0.934925573608804, 0.9307163332871908, 0.9345295773348099, 0.9390160330726283, 0.940015486572536, 0.9333201394656699, 0.9425956807747323, 0.9416441722037652, 0.9394821442990956, 0.9459206435723514, 0.9429923980251015, 0.945620176668051, 0.9434109248108158, 0.9477110081556847, 0.9456423466915835, 0.9470603269772055, 0.9476180022033037, 0.9471304419296788, 0.949546794250646, 0.9500822994532114, 0.9510824739294942, 0.9518029095722591, 0.9524543117271133, 0.9513146283222591, 0.9474098202750092, 0.9539424069652085, 0.9541509493816908, 0.9536161651555003, 0.9529422324889257, 0.9544070762389257, 0.9554995356912146, 0.95677872802464, 0.9591492983457919, 0.958777274536268, 0.9554065297388336, 0.9589167834648394, 0.9594050647148394, 0.9596386610603543, 0.9583133262389257, 0.9587078805601699, 0.9581730963339794, 0.960033575869786, 0.9599176789174971, 0.9602417577980805, 0.9608470174649317, 0.9618471919412146, 0.9614987801079733, 0.9603823481912146, 0.9595685461078812, 0.963660087036268, 0.96166154052464, 0.9599402094292175, 0.961986700869786, 0.9631256632982651, 0.9632884237149317, 0.9654511725959765, 0.9646133980481728, 0.9661941387504615, 0.9674736915720746, 0.964683513000646, 0.9629636238579733, 0.9655434575719823, 0.9666824200004615, 0.9664041231196937, 0.965613212036268, 0.9667067529531194, 0.9681479847268365, 0.9659401748223514, 0.9672408162029347, 0.9660782417981728, 0.9668451804171282, 0.9665439925364526, 0.9682409906792175, 0.966149077727021, 0.9678925788459765, 0.9659623448458842, 0.9675899490125508, 0.9700073827980805, 0.9696815014765596, 0.9678693273578812, 0.9698446223814139, 0.970495303559893, 0.9716582384528424, 0.9695423530361758, 0.9707743214170359, 0.9705657790005537, 0.9699372678456073, 0.9714947570598007, 0.9718206383813216, 0.9709141908337948, 0.9717744958933187, 0.9694962105481728, 0.97214688019103, 0.9728433433693245, 0.9704488005837025, 0.9713563295957919, 0.9714951175479882, 0.9705657790005537, 0.9730773002030271, 0.9736342544527501, 0.9744945595122739, 0.9741693991671282, 0.972681303929033, 0.9730075457387413, 0.9744480565360835, 0.9722627771433187, 0.9733559575719823, 0.9735183575004615, 0.9753083615956073, 0.9740066387504615, 0.9744011930717055, 0.9752160766196014, 0.975145240690753, 0.9764938270002769, 0.9741225357027501, 0.9735412485003692, 0.9765864724644703, 0.9765868329526578, 0.9767964568337025, 0.975424979524271, 0.9770518627145626, 0.9737737633813216, 0.9763786510243633, 0.9764240725359912, 0.9752618586194168, 0.9756106309408453, 0.9770289717146549, 0.9762849240956073, 0.9769588567621816, 0.9741225357027501, 0.9757272488695091, 0.9768662112979882, 0.976936686738649, 0.9781221521433187, 0.9776338708933187, 0.9782849125599853, 0.9775408649409376, 0.9769592172503692], "end": "2016-01-25 04:51:24.453000", "learning_rate_per_epoch": [0.0048260558396577835, 0.0024130279198288918, 0.0016086852410808206, 0.0012065139599144459, 0.0009652111330069602, 0.0008043426205404103, 0.0006894365069456398, 0.0006032569799572229, 0.0005362284136936069, 0.0004826055665034801, 0.00043873232789337635, 0.00040217131027020514, 0.00037123504444025457, 0.0003447182534728199, 0.0003217370540369302, 0.00030162848997861147, 0.00028388563077896833, 0.0002681142068468034, 0.0002540029236115515, 0.00024130278325174004, 0.00022981216898187995, 0.00021936616394668818, 0.0002098285040119663, 0.00020108565513510257, 0.00019304222951177508, 0.00018561752222012728, 0.00017874280456453562, 0.00017235912673640996, 0.00016641571710351855, 0.0001608685270184651, 0.00015567921218462288, 0.00015081424498930573, 0.00014624411414843053, 0.00014194281538948417, 0.00013788729847874492, 0.0001340571034234017, 0.00013043393846601248, 0.00012700146180577576, 0.0001237450196640566, 0.00012065139162587002, 0.00011770867422455922, 0.00011490608449093997, 0.00011223385081393644, 0.00010968308197334409, 0.00010724567982833833, 0.00010491425200598314, 0.00010268203914165497, 0.00010054282756755129, 9.849093476077542e-05, 9.652111475588754e-05, 9.462854359298944e-05, 9.280876111006364e-05, 9.105765639105812e-05, 8.937140228226781e-05, 8.774646994424984e-05, 8.617956336820498e-05, 8.466764120385051e-05, 8.320785855175927e-05, 8.179755241144449e-05, 8.043426350923255e-05, 7.911566353868693e-05, 7.783960609231144e-05, 7.660406117793173e-05, 7.540712249465287e-05, 7.424700743285939e-05, 7.312205707421526e-05, 7.203068525996059e-05, 7.097140769474208e-05, 6.994283467065543e-05, 6.894364923937246e-05, 6.797261448809877e-05, 6.702855171170086e-05, 6.611035496462137e-05, 6.521696923300624e-05, 6.434741226257756e-05, 6.350073090288788e-05, 6.267604476306587e-05, 6.18725098320283e-05, 6.108931120252237e-05, 6.032569581293501e-05, 5.9580932429526e-05, 5.885433711227961e-05, 5.814524774905294e-05, 5.745304224546999e-05, 5.677712397300638e-05, 5.611692540696822e-05, 5.5471904488513246e-05, 5.4841540986672044e-05, 5.4225343774305657e-05, 5.362283991416916e-05, 5.303357829689048e-05, 5.245712600299157e-05, 5.189307194086723e-05, 5.1341019570827484e-05, 5.080058690509759e-05, 5.027141378377564e-05, 4.9753150960896164e-05, 4.924546738038771e-05, 4.874803562415764e-05, 4.826055737794377e-05, 4.7782730689505115e-05, 4.731427179649472e-05, 4.685490785050206e-05, 4.640438055503182e-05, 4.596243525156751e-05, 4.552882819552906e-05, 4.5103322918294e-05, 4.4685701141133904e-05, 4.4275740947341546e-05, 4.387323497212492e-05, 4.3477979488670826e-05, 4.308978168410249e-05, 4.270845602150075e-05, 4.2333820601925254e-05, 4.196570080239326e-05, 4.1603929275879636e-05, 4.1248338675359264e-05, 4.089877620572224e-05, 4.0555089071858674e-05, 4.0217131754616275e-05, 3.988475873484276e-05, 3.9557831769343466e-05, 3.923622352886014e-05, 3.891980304615572e-05, 3.860844662995078e-05, 3.830203058896586e-05, 3.800043850787915e-05, 3.7703561247326434e-05, 3.7411286029964685e-05, 3.7123503716429695e-05, 3.684011971927248e-05, 3.656102853710763e-05, 3.628613194450736e-05, 3.6015342629980296e-05, 3.574855873011984e-05, 3.548570384737104e-05, 3.522668339428492e-05, 3.4971417335327715e-05, 3.471982563496567e-05, 3.447182461968623e-05, 3.4227345167892054e-05, 3.3986307244049385e-05, 3.374864172656089e-05, 3.351427585585043e-05, 3.328314414829947e-05, 3.305517748231068e-05, 3.283031037426554e-05, 3.260848461650312e-05, 3.23896347254049e-05, 3.217370613128878e-05, 3.196063335053623e-05, 3.175036545144394e-05, 3.1542847864329815e-05, 3.133802238153294e-05, 3.1135841709328815e-05, 3.093625491601415e-05, 3.0739207431906834e-05, 3.0544655601261184e-05, 3.0352552130352706e-05, 3.0162847906467505e-05, 2.99755010928493e-05, 2.9790466214763e-05, 2.960770325444173e-05, 2.9427168556139804e-05, 2.9248822102090344e-05, 2.907262387452647e-05, 2.8898537493660115e-05, 2.8726521122734994e-05, 2.8556542019941844e-05, 2.838856198650319e-05, 2.822254828060977e-05, 2.805846270348411e-05, 2.789627615129575e-05, 2.7735952244256623e-05, 2.757746187853627e-05, 2.7420770493336022e-05, 2.726585080381483e-05, 2.7112671887152828e-05, 2.6961204639519565e-05, 2.681141995708458e-05, 2.6663290555006824e-05, 2.651678914844524e-05, 2.6371888452558778e-05, 2.6228563001495786e-05, 2.6086787329404615e-05, 2.5946535970433615e-05, 2.5807783458731137e-05, 2.5670509785413742e-05, 2.5534685846650973e-05, 2.5400293452548794e-05, 2.5267307137255557e-05], "accuracy_valid": [0.2504897519766566, 0.3041609798569277, 0.5326354245105422, 0.6558293721762049, 0.6912297628012049, 0.7248726350715362, 0.7608848479856928, 0.7693282897213856, 0.7795116010918675, 0.7926951948418675, 0.7894801863704819, 0.805919968938253, 0.8028270307793675, 0.8154620434864458, 0.8171607327748494, 0.8262968867658133, 0.8244143566453314, 0.834129976939006, 0.8346991481551205, 0.8401011271649097, 0.8415865610881024, 0.8405688182417168, 0.8432749552899097, 0.8397246211408133, 0.8478224421121988, 0.8467135142131024, 0.8469782450112951, 0.8430205195783133, 0.8443632930158133, 0.8527155496987951, 0.8514845514871988, 0.8469576548381024, 0.8532038309487951, 0.849877047251506, 0.8527155496987951, 0.8575380624058735, 0.8527964396649097, 0.8551775461219879, 0.8549025202371988, 0.8539259577371988, 0.855736422251506, 0.8570086008094879, 0.8572630365210843, 0.8552584360881024, 0.8596529673381024, 0.8555334619728916, 0.8579439829631024, 0.8557261271649097, 0.855858492564006, 0.8605383447853916, 0.8604265695594879, 0.855370211314006, 0.858910250376506, 0.8580763483621988, 0.8536509318524097, 0.857201266001506, 0.8606604150978916, 0.8653093820594879, 0.8546274943524097, 0.8604265695594879, 0.8630003412085843, 0.8564482539533133, 0.8646990304969879, 0.8621355539344879, 0.8618914133094879, 0.8615046121987951, 0.8596220820783133, 0.862572359751506, 0.8594191217996988, 0.8613722467996988, 0.8609648555158133, 0.8626032450112951, 0.8608736704631024, 0.8632135965737951, 0.8645460749246988, 0.8644342996987951, 0.8619928934487951, 0.8607824854103916, 0.8676698983433735, 0.8653402673192772, 0.8629900461219879, 0.8622576242469879, 0.8639769037085843, 0.8635798075112951, 0.8655535226844879, 0.8645666650978916, 0.8659197336219879, 0.8634989175451807, 0.8676698983433735, 0.8652181970067772, 0.8662050545933735, 0.8659506188817772, 0.8636003976844879, 0.8631018213478916, 0.8645872552710843, 0.8654108621987951, 0.8651770166603916, 0.8649328760353916, 0.8664389001317772, 0.8636003976844879, 0.8624708796121988, 0.8627047251506024, 0.8661535791603916, 0.8652990869728916, 0.8617281626506024, 0.861473726939006, 0.8650858316076807, 0.8638239481362951, 0.8649637612951807, 0.8638548333960843, 0.8657270684299698, 0.8665300851844879, 0.8615149072853916, 0.8628473856362951, 0.8666624505835843, 0.8639563135353916, 0.8644445947853916, 0.8659300287085843, 0.8656961831701807, 0.8636312829442772, 0.8639974938817772, 0.8661535791603916, 0.8645666650978916, 0.8645563700112951, 0.8639563135353916, 0.8678022637424698, 0.8619825983621988, 0.8655638177710843, 0.8651873117469879, 0.8658182534826807, 0.8657976633094879, 0.8652887918862951, 0.8639666086219879, 0.8655535226844879, 0.8652990869728916, 0.8657770731362951, 0.8669271813817772, 0.8669168862951807, 0.8645666650978916, 0.8648108057228916, 0.8679140389683735, 0.8635695124246988, 0.8648211008094879, 0.8636812876506024, 0.8632238916603916, 0.8662756494728916, 0.8639254282756024, 0.8593676463667168, 0.8637121729103916, 0.8647902155496988, 0.8645666650978916, 0.8651564264871988, 0.8662859445594879, 0.8648108057228916, 0.8674654673381024, 0.8660315088478916, 0.8643122293862951, 0.8664991999246988, 0.864403414439006, 0.8643225244728916, 0.8666418604103916, 0.8658182534826807, 0.8650858316076807, 0.8655226374246988, 0.8658888483621988, 0.8646578501506024, 0.8648005106362951, 0.8649225809487951, 0.8645769601844879, 0.8666418604103916, 0.8668757059487951, 0.862206148814006, 0.8652784967996988, 0.8651461314006024, 0.8635798075112951, 0.864403414439006, 0.8656550028237951, 0.8640783838478916, 0.8656652979103916, 0.8662653543862951, 0.8649225809487951, 0.8662653543862951, 0.8644342996987951, 0.8605883494917168, 0.8631930064006024, 0.8636812876506024, 0.8656447077371988, 0.8654108621987951, 0.8663874246987951, 0.8645460749246988, 0.8646887354103916], "accuracy_test": 0.7945033482142857, "start": "2016-01-23 12:40:49.309000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0], "accuracy_train_last": 0.9769592172503692, "batch_size_eval": 1024, "accuracy_train_std": [0.016489374083948228, 0.016724157315483896, 0.017842011833614255, 0.013886635833915164, 0.013197289603089206, 0.01487169652798762, 0.014770604640752782, 0.015456132047168178, 0.014939693846506253, 0.015725686656791103, 0.014701682532135931, 0.01421304135017167, 0.016927662511502777, 0.013233752026284257, 0.013632418286212157, 0.01460461528069317, 0.01395838319393405, 0.012632295215335874, 0.012854091192819288, 0.011655133958872053, 0.013346540184645905, 0.012938415373620964, 0.012357409176772263, 0.011764452822767404, 0.013538361912924516, 0.012466542793646597, 0.012772801941012789, 0.01372485304393138, 0.01422561486471333, 0.01281682021760408, 0.012706056478616922, 0.01201328155552933, 0.01167798572558894, 0.011517142870822596, 0.012487852869674443, 0.013317929478061588, 0.013159186279302966, 0.012564250324148228, 0.011855924639174125, 0.013069583706510431, 0.01332770004712258, 0.012021030375869964, 0.012727689469896565, 0.011684698862282216, 0.011243832545967036, 0.012335347671392905, 0.01159258680894693, 0.011073280847860169, 0.01257473413076057, 0.011466956796507922, 0.011497991479313416, 0.011811838099141415, 0.011943271311469083, 0.010258690370831753, 0.011902631429137174, 0.011391919830954967, 0.011700462386472694, 0.011551154633772927, 0.01180411262828689, 0.011227052760100411, 0.010637590433332463, 0.010680614602832391, 0.01101241535764292, 0.010787684554268128, 0.009982908771350964, 0.009941854523912882, 0.01059408031377579, 0.01052992877835714, 0.010492056706588725, 0.010459555351198562, 0.011075630760544611, 0.011117709736942755, 0.010415576289085054, 0.010236701624993017, 0.00964679927725345, 0.009853464613165792, 0.00958734712086751, 0.010459895668124898, 0.008915112477517273, 0.009899103936098316, 0.01010201077229385, 0.009813082748638109, 0.009435695586860382, 0.009843943160616084, 0.009315162867656407, 0.009387915259645977, 0.009663600638570507, 0.009571531374376429, 0.009646771210448855, 0.009200288728550174, 0.008960312829701826, 0.009525371423954015, 0.008872273790966383, 0.009625226846748419, 0.00822450037136007, 0.00929916916743831, 0.008944429102280237, 0.00847778636678362, 0.007934380902573021, 0.008614604515422999, 0.008148796766853124, 0.008506051339726406, 0.009048455674173445, 0.00846274339941938, 0.008660042867127555, 0.008176742568833854, 0.008333934809546632, 0.009288047935081005, 0.008292882185434421, 0.008417094896432118, 0.007958959461189984, 0.00865784743582139, 0.008262610559345364, 0.00887437690386131, 0.008554598450822317, 0.008137300026638301, 0.008090298345211442, 0.008166892812352724, 0.00807282685672876, 0.007173007551983398, 0.007206302911391634, 0.007385842673576173, 0.008123626366801376, 0.008029820380538512, 0.00786271319945917, 0.007977170169663466, 0.007422002732995536, 0.007693309457612728, 0.007473341606672822, 0.00783956010286712, 0.007128283229273034, 0.007672857892031643, 0.007223274328244858, 0.00777411045691758, 0.007482677077379246, 0.0069085145388379145, 0.007310669824516044, 0.007488256815053305, 0.0071865577470595805, 0.007210805506337937, 0.007383095996474231, 0.007031982293227559, 0.0065025264583532605, 0.006793547961179618, 0.006801315845310632, 0.006694227030937099, 0.00655878683423895, 0.0068837414418994325, 0.0072181471391525856, 0.006946498141235575, 0.006810190671357498, 0.0064483467187411835, 0.007119632020587567, 0.006383604708996565, 0.006600981505079045, 0.00680892746492149, 0.005655508302684556, 0.0066474777768926315, 0.006412568675043083, 0.006574203417688677, 0.006476143412711254, 0.00662460171630297, 0.006344055226802592, 0.0070274931367451405, 0.006054542530321603, 0.007373218406681958, 0.006318918336950835, 0.006770517521568164, 0.006491773443120522, 0.006329235739819473, 0.0068137303146998075, 0.006131266464197014, 0.006509586455445499, 0.006488517657053809, 0.006557018182293092, 0.006137142548389471, 0.006394580800560878, 0.006588113427578757, 0.0062731459933348215, 0.00624652107884544, 0.006503312570657266, 0.006403319368853915, 0.0059911974413822235, 0.006237511931230101, 0.006032983557412042, 0.0060875906317826795, 0.0057932155364295, 0.005219131869260358, 0.00596217921669293, 0.0058555399040320175, 0.006310986642880192], "accuracy_test_std": 0.007426017154414782, "error_valid": [0.7495102480233433, 0.6958390201430723, 0.46736457548945776, 0.34417062782379515, 0.30877023719879515, 0.2751273649284638, 0.23911515201430722, 0.23067171027861444, 0.22048839890813254, 0.20730480515813254, 0.2105198136295181, 0.19408003106174698, 0.19717296922063254, 0.1845379565135542, 0.18283926722515065, 0.17370311323418675, 0.17558564335466864, 0.16587002306099397, 0.16530085184487953, 0.1598988728350903, 0.15841343891189763, 0.1594311817582832, 0.1567250447100903, 0.16027537885918675, 0.15217755788780118, 0.15328648578689763, 0.15302175498870485, 0.15697948042168675, 0.15563670698418675, 0.14728445030120485, 0.14851544851280118, 0.15304234516189763, 0.14679616905120485, 0.15012295274849397, 0.14728445030120485, 0.1424619375941265, 0.1472035603350903, 0.14482245387801207, 0.14509747976280118, 0.14607404226280118, 0.14426357774849397, 0.14299139919051207, 0.14273696347891573, 0.14474156391189763, 0.14034703266189763, 0.1444665380271084, 0.14205601703689763, 0.1442738728350903, 0.14414150743599397, 0.1394616552146084, 0.13957343044051207, 0.14462978868599397, 0.14108974962349397, 0.14192365163780118, 0.1463490681475903, 0.14279873399849397, 0.1393395849021084, 0.13469061794051207, 0.1453725056475903, 0.13957343044051207, 0.13699965879141573, 0.14355174604668675, 0.13530096950301207, 0.13786444606551207, 0.13810858669051207, 0.13849538780120485, 0.14037791792168675, 0.13742764024849397, 0.14058087820030118, 0.13862775320030118, 0.13903514448418675, 0.13739675498870485, 0.13912632953689763, 0.13678640342620485, 0.13545392507530118, 0.13556570030120485, 0.13800710655120485, 0.1392175145896084, 0.1323301016566265, 0.13465973268072284, 0.13700995387801207, 0.13774237575301207, 0.13602309629141573, 0.13642019248870485, 0.13444647731551207, 0.1354333349021084, 0.13408026637801207, 0.1365010824548193, 0.1323301016566265, 0.13478180299322284, 0.1337949454066265, 0.13404938111822284, 0.13639960231551207, 0.1368981786521084, 0.13541274472891573, 0.13458913780120485, 0.1348229833396084, 0.1350671239646084, 0.13356109986822284, 0.13639960231551207, 0.13752912038780118, 0.13729527484939763, 0.1338464208396084, 0.1347009130271084, 0.13827183734939763, 0.13852627306099397, 0.1349141683923193, 0.13617605186370485, 0.1350362387048193, 0.13614516660391573, 0.13427293157003017, 0.13346991481551207, 0.1384850927146084, 0.13715261436370485, 0.13333754941641573, 0.1360436864646084, 0.1355554052146084, 0.13406997129141573, 0.1343038168298193, 0.13636871705572284, 0.13600250611822284, 0.1338464208396084, 0.1354333349021084, 0.13544362998870485, 0.1360436864646084, 0.13219773625753017, 0.13801740163780118, 0.13443618222891573, 0.13481268825301207, 0.1341817465173193, 0.13420233669051207, 0.13471120811370485, 0.13603339137801207, 0.13444647731551207, 0.1347009130271084, 0.13422292686370485, 0.13307281861822284, 0.1330831137048193, 0.1354333349021084, 0.1351891942771084, 0.1320859610316265, 0.13643048757530118, 0.13517889919051207, 0.13631871234939763, 0.1367761083396084, 0.1337243505271084, 0.13607457172439763, 0.1406323536332832, 0.1362878270896084, 0.13520978445030118, 0.1354333349021084, 0.13484357351280118, 0.13371405544051207, 0.1351891942771084, 0.13253453266189763, 0.1339684911521084, 0.13568777061370485, 0.13350080007530118, 0.13559658556099397, 0.1356774755271084, 0.1333581395896084, 0.1341817465173193, 0.1349141683923193, 0.13447736257530118, 0.13411115163780118, 0.13534214984939763, 0.13519948936370485, 0.13507741905120485, 0.13542303981551207, 0.1333581395896084, 0.13312429405120485, 0.13779385118599397, 0.13472150320030118, 0.13485386859939763, 0.13642019248870485, 0.13559658556099397, 0.13434499717620485, 0.1359216161521084, 0.1343347020896084, 0.13373464561370485, 0.13507741905120485, 0.13373464561370485, 0.13556570030120485, 0.1394116505082832, 0.13680699359939763, 0.13631871234939763, 0.13435529226280118, 0.13458913780120485, 0.13361257530120485, 0.13545392507530118, 0.1353112645896084], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-3, 3], "momentum": 0.6651022557408289, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.00482605567092568, "patience_threshold": 1, "do_flip": true, "batch_size": 64, "optimization": "rmsprop", "nb_data_augmentation": 4, "learning_rate_decay_method": "lin", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 4.233546147610005e-05, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.03612441052748483}, "accuracy_valid_max": 0.8679140389683735, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import os\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-3, 3)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8646887354103916, "loss_train": [2.508826732635498, 1.737819790840149, 1.370963454246521, 1.194654941558838, 1.08367121219635, 1.0043857097625732, 0.946125328540802, 0.9020364880561829, 0.8661229014396667, 0.8353742957115173, 0.8091383576393127, 0.7877467274665833, 0.767257809638977, 0.7501017451286316, 0.7346453070640564, 0.7202653884887695, 0.7071415781974792, 0.6955705881118774, 0.6828113794326782, 0.67423415184021, 0.6634472012519836, 0.6547811031341553, 0.6476829648017883, 0.6385860443115234, 0.6314608454704285, 0.6233814358711243, 0.616227388381958, 0.6107343435287476, 0.6044378280639648, 0.5964480638504028, 0.5903313755989075, 0.5852678418159485, 0.5807409882545471, 0.5771586894989014, 0.5707747340202332, 0.565721869468689, 0.5616616606712341, 0.5571248531341553, 0.5527467727661133, 0.5471397638320923, 0.5444373488426208, 0.5420924425125122, 0.5365082025527954, 0.5338125228881836, 0.5304556488990784, 0.5262485146522522, 0.5238443613052368, 0.5200433731079102, 0.516796350479126, 0.5133917927742004, 0.5123831033706665, 0.5061987042427063, 0.5054947733879089, 0.501950204372406, 0.4988211393356323, 0.4963296353816986, 0.49324262142181396, 0.4930354654788971, 0.4890512526035309, 0.48617976903915405, 0.48448261618614197, 0.4827415645122528, 0.4803612232208252, 0.47659072279930115, 0.47425660490989685, 0.4736084043979645, 0.4712129831314087, 0.46993646025657654, 0.46839746832847595, 0.464173287153244, 0.46401652693748474, 0.45959174633026123, 0.4591469168663025, 0.4573574364185333, 0.45584937930107117, 0.4539947211742401, 0.45156970620155334, 0.45158541202545166, 0.44701680541038513, 0.44689372181892395, 0.4456103444099426, 0.44230809807777405, 0.44122952222824097, 0.4410323202610016, 0.4382799565792084, 0.4368790090084076, 0.43630972504615784, 0.43372347950935364, 0.43348416686058044, 0.4322119653224945, 0.43041032552719116, 0.4285747706890106, 0.42586439847946167, 0.426064670085907, 0.4235213100910187, 0.42333167791366577, 0.4223548471927643, 0.4206518232822418, 0.4199395775794983, 0.4169865846633911, 0.41653382778167725, 0.4143787920475006, 0.41470202803611755, 0.4135640263557434, 0.41278010606765747, 0.41086888313293457, 0.4099908471107483, 0.4091559946537018, 0.4087461531162262, 0.4067649841308594, 0.40464794635772705, 0.4048782289028168, 0.40426769852638245, 0.4028659760951996, 0.4016205966472626, 0.3996849060058594, 0.4000943601131439, 0.39829084277153015, 0.39644014835357666, 0.39711326360702515, 0.39607617259025574, 0.3951869308948517, 0.39469650387763977, 0.3937150239944458, 0.39203202724456787, 0.39039596915245056, 0.39085692167282104, 0.3895762860774994, 0.3891337215900421, 0.3879048526287079, 0.3868738114833832, 0.3859019875526428, 0.3846159875392914, 0.3833180069923401, 0.38372910022735596, 0.383371502161026, 0.3812030851840973, 0.3813437521457672, 0.37897825241088867, 0.37922152876853943, 0.3780849874019623, 0.37821030616760254, 0.37808752059936523, 0.3768348693847656, 0.3747519254684448, 0.3764176070690155, 0.37525904178619385, 0.37291404604911804, 0.37421128153800964, 0.3725469410419464, 0.3711712658405304, 0.3719852864742279, 0.3707936406135559, 0.36974963545799255, 0.36851540207862854, 0.36809131503105164, 0.36769795417785645, 0.3665387034416199, 0.36803558468818665, 0.36615660786628723, 0.3646721839904785, 0.36466339230537415, 0.3645440340042114, 0.36348235607147217, 0.36352548003196716, 0.3619955778121948, 0.36007845401763916, 0.3625735938549042, 0.3611083924770355, 0.35871315002441406, 0.3579874336719513, 0.3580162227153778, 0.35819414258003235, 0.35801056027412415, 0.356626957654953, 0.35507383942604065, 0.35586225986480713, 0.35369670391082764, 0.35375550389289856, 0.3541359007358551, 0.35192209482192993, 0.35297107696533203, 0.3525576591491699, 0.3517298996448517, 0.35166308283805847, 0.3504054546356201, 0.3495102822780609, 0.3494037389755249, 0.3496418297290802, 0.34813791513442993, 0.3493809998035431], "accuracy_train_first": 0.2462038791412883, "model": "residualv3", "loss_std": [0.5903902053833008, 0.17766547203063965, 0.15910030901432037, 0.14312882721424103, 0.1376638263463974, 0.13233835995197296, 0.13031400740146637, 0.12964434921741486, 0.1253734678030014, 0.12156590819358826, 0.12022705376148224, 0.11815287917852402, 0.1161772832274437, 0.11369949579238892, 0.11262912303209305, 0.1099500060081482, 0.10837321728467941, 0.10829158127307892, 0.10649964958429337, 0.10475757718086243, 0.10508335381746292, 0.10368798673152924, 0.10291408747434616, 0.10283179581165314, 0.10101448744535446, 0.09976289421319962, 0.0978206992149353, 0.09780912101268768, 0.09607774019241333, 0.09521900862455368, 0.09497274458408356, 0.09455668181180954, 0.09370686858892441, 0.09406526386737823, 0.09180376678705215, 0.09257230907678604, 0.08979104459285736, 0.09042952954769135, 0.08975056558847427, 0.08728904277086258, 0.08749321103096008, 0.08841830492019653, 0.08526510745286942, 0.08479750156402588, 0.08605794608592987, 0.08312060683965683, 0.08374711126089096, 0.0837041363120079, 0.08363453298807144, 0.08201131969690323, 0.08311177045106888, 0.08180412650108337, 0.08239138871431351, 0.08039982616901398, 0.07860018312931061, 0.07861624658107758, 0.07834403216838837, 0.07900922745466232, 0.07803554832935333, 0.07815785706043243, 0.0765497162938118, 0.07712528109550476, 0.07516027241945267, 0.07338815182447433, 0.07350679486989975, 0.0757535770535469, 0.07388434559106827, 0.07344522327184677, 0.07391484081745148, 0.07252279669046402, 0.07205843925476074, 0.07174563407897949, 0.07188809663057327, 0.07127761095762253, 0.07097494602203369, 0.0702642872929573, 0.0710003450512886, 0.07086358964443207, 0.06865721940994263, 0.06915398687124252, 0.06847919523715973, 0.06768623739480972, 0.06761484593153, 0.06792539358139038, 0.06615867465734482, 0.06604481488466263, 0.06614898890256882, 0.06552886217832565, 0.0659569799900055, 0.0661308765411377, 0.0657317265868187, 0.06400096416473389, 0.06376265734434128, 0.06460530310869217, 0.06416518241167068, 0.0643109679222107, 0.06366562098264694, 0.06167943403124809, 0.06293466687202454, 0.06116503104567528, 0.0610622763633728, 0.06125771999359131, 0.06116838753223419, 0.06107889860868454, 0.05994456633925438, 0.05988713353872299, 0.06035573035478592, 0.05982351303100586, 0.06101170927286148, 0.058920491486787796, 0.05836385861039162, 0.05987263098359108, 0.05749499052762985, 0.05772749334573746, 0.05935591459274292, 0.057517219334840775, 0.05708858743309975, 0.05698693171143532, 0.056180331856012344, 0.05844077467918396, 0.05668994411826134, 0.05734879523515701, 0.05713207274675369, 0.05700021982192993, 0.055231764912605286, 0.0558575838804245, 0.05592522397637367, 0.05377538129687309, 0.05482545867562294, 0.054940398782491684, 0.05403895676136017, 0.0545070506632328, 0.05344365909695625, 0.05258234590291977, 0.052248552441596985, 0.053286753594875336, 0.052474260330200195, 0.05230705812573433, 0.05281447619199753, 0.05331254377961159, 0.05285504087805748, 0.05171851068735123, 0.051703162491321564, 0.05095682665705681, 0.051175639033317566, 0.051861874759197235, 0.05221550911664963, 0.05057889595627785, 0.05155569314956665, 0.05136476829648018, 0.04876148700714111, 0.05113008990883827, 0.05024045705795288, 0.05081706494092941, 0.04918278008699417, 0.04840533062815666, 0.049653246998786926, 0.0488613024353981, 0.04911617934703827, 0.04809071496129036, 0.04798850417137146, 0.0473036989569664, 0.04892974719405174, 0.04664725065231323, 0.04757995158433914, 0.04825102537870407, 0.04572831839323044, 0.04634474590420723, 0.04717082530260086, 0.04686424881219864, 0.04707903787493706, 0.045996811240911484, 0.04774252325296402, 0.04638853669166565, 0.04717295616865158, 0.044035639613866806, 0.04472745954990387, 0.04561536759138107, 0.044120099395513535, 0.046435099095106125, 0.04449976608157158, 0.04465266689658165, 0.04443436488509178, 0.04446842893958092, 0.04395688697695732, 0.04425213485956192, 0.043811194598674774, 0.04394545778632164, 0.044461630284786224, 0.04346002638339996, 0.04454921558499336]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:12 2016", "state": "available"}], "summary": "bf973025b80400bdb60d746c5da40eaa"}