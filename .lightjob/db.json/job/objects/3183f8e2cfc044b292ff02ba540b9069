{"content": {"hp_model": {"f0": 16, "f1": 16, "f2": 64, "f3": 16, "nonlin": "very_leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.009780112429163733, 0.021229052350891682, 0.027386803839565755, 0.020029084372087207, 0.015291721545387197, 0.018585296581876135, 0.01437723457474485, 0.017515715054592193, 0.012742054494322669, 0.012952897989736447, 0.006841171479363933, 0.006555606645354825, 0.006852202303110978, 0.009491664209796481, 0.008702159256816295, 0.009617762404747515, 0.011255969790099497, 0.012454092399792834, 0.012589788458348954, 0.011229318843285136, 0.009664935139721935, 0.012174229222613298, 0.012743520420574526, 0.015107352253083298, 0.013750402887296615, 0.013047272915496502, 0.0165079902588829, 0.013823704560009269, 0.014574983310953782, 0.014892718428699193, 0.013785375067557264, 0.01582806386597347, 0.012783672173247813, 0.013410659464962968, 0.017214149791286375, 0.012353431214988954, 0.013910100996172181, 0.011865988914214282, 0.012408183142091156, 0.011688414561039286, 0.010142272635999264, 0.013591763823486156, 0.012744368818794994, 0.01279461729684678, 0.011497550569921484, 0.01235933290563933, 0.012754761505222102, 0.010881920744690266, 0.01475723690087394, 0.010578241121386589, 0.0134361330127135, 0.009713383890573184, 0.011325601322070692, 0.009866614738757109, 0.011700046712712823, 0.011455350766552219, 0.008650289597526803, 0.010736305298516394, 0.011091255908301777, 0.007879494069498826, 0.007154915739050737, 0.006806111100096243, 0.010614615558433776, 0.01016897422509159, 0.007965391349937519, 0.007893562467011436, 0.008124099691447074, 0.007433603660900853, 0.008272699259429937, 0.009455904564942165, 0.007570868854314934, 0.007907390147246262, 0.008884599111560584, 0.007711709609902678, 0.0077937212361422386, 0.009512513464251393, 0.009878934777993535, 0.008421795382542768, 0.007471377111055123, 0.008221239405223327, 0.006744689589947691, 0.008298065407572415, 0.008803725910007294, 0.007476037344467861, 0.0059280027481503576, 0.008295717633259394, 0.0056151517471349196, 0.00855215587654211, 0.007273309279390422, 0.0077816066791458055, 0.0062893169482838435, 0.008305936963997836, 0.007839442566962092, 0.00850449580853136, 0.0081860022113126, 0.009520451584451639, 0.00977352197919359, 0.008013468478344703, 0.006070461110091276, 0.007660227658211483, 0.007478258340513164, 0.007838068378955877, 0.007587923390450515, 0.004290352418726729, 0.008414117563973278, 0.00875463628405567, 0.008472601740704966, 0.007416448435104402, 0.008538205406147841, 0.009244450088062649, 0.00755643727037702, 0.006500784680284622, 0.008434118570533595, 0.007066163732771765, 0.009530855248558345, 0.007591199625927362, 0.005812902501016752, 0.006354590196671573, 0.007766919448876069, 0.009165390935436087, 0.007606887033457331, 0.006476599119391393, 0.009805533045593937, 0.008686048257588543, 0.007496385814915768, 0.0083446531727647, 0.008933929166470794, 0.007593599724069129, 0.008890043259448843, 0.007808913172918572, 0.007202866495932881, 0.008372246195339498, 0.00791517363104694, 0.007912236038661095, 0.006686302536934349, 0.010527866065925378, 0.009165390935436087, 0.007131727956308908, 0.007371906419167937, 0.007859478350013516, 0.005681451720087322, 0.009164558405840132, 0.0051593787063655165, 0.0075335619714510926, 0.005907748980601696, 0.006959460512475193, 0.007440849046835325, 0.007747018425858011, 0.006895408881232928, 0.008214199819661799, 0.005660430643385857, 0.005882273444605405, 0.008966650625475431, 0.006957099701545895, 0.0069217984121526365, 0.0075679312345318436, 0.0059487386319450036, 0.0069134026364729975, 0.007541743465849533, 0.005778067832553745, 0.007087017751439227, 0.0065979984423015685, 0.006661554069455115, 0.008256942771211381, 0.006776265139118271, 0.00736202443287306, 0.006660193256057041, 0.005989268281343926, 0.007448240576951683, 0.007596658966553007, 0.007039745254575335, 0.007469602205152747, 0.008718660142628462, 0.0063736182538746446, 0.007787514034520355, 0.00796685448632737, 0.007689786551696, 0.009010400710286597, 0.00969780295398648, 0.007567417748414986, 0.008353526347492016, 0.007193226499552256, 0.005778067832553745, 0.0074985803033912725, 0.008318323877181075, 0.008685154837042904, 0.008260497013589581, 0.00597722420813606, 0.006695895113144382, 0.0067344604119921915, 0.006191573896091628, 0.006048105836268304, 0.007370643447579539, 0.007242773171318117, 0.006260767120713397, 0.008050888293338564, 0.006113135279245186, 0.006258702127661524, 0.006677704777493969, 0.004934316646041758, 0.006998035649687305, 0.007606548219978118, 0.004785038277608093, 0.005669550159858346, 0.006641975975431836, 0.006176927995340693, 0.007856039764633047, 0.00709193055870504, 0.005686951210112318, 0.005607328951002641, 0.006531172196076496, 0.007121915742268863, 0.0058008293325570795, 0.006364551891534761, 0.006500436131300779, 0.005306145143000642, 0.006959279621465967, 0.006620462608461853, 0.0068555748840189315, 0.00734598490972589, 0.004710516614454065, 0.007481618593925789, 0.006986738490828687, 0.006126032556728293, 0.004852959403104151, 0.005260474702380359], "moving_avg_accuracy_train": [0.04874638430347913, 0.10516951776774175, 0.15806335742115396, 0.21145882152446122, 0.26296708051611806, 0.3107356265942589, 0.3554777585811508, 0.3968127043800125, 0.4352787481608189, 0.4707352050861545, 0.5035503189617675, 0.5333815765462571, 0.5610131753746509, 0.5863511141356355, 0.6097550392669224, 0.6312138471826997, 0.6508592705866612, 0.6686330494561512, 0.6851084310934541, 0.7003082623277318, 0.7141787005433252, 0.7267619681897125, 0.7381823122702891, 0.7487372425535038, 0.7582808936845968, 0.767179460543066, 0.775202157657364, 0.7826828575316238, 0.7896154502160768, 0.7959128763035038, 0.8017432481012172, 0.8072626972275112, 0.8121672782280436, 0.8167372581963983, 0.8208664801607655, 0.8247547327941444, 0.828402969687995, 0.8318723587484037, 0.8349761356146579, 0.8377928223312006, 0.8402674225558602, 0.842694633702129, 0.845118325670602, 0.8473322726231984, 0.8493829536007734, 0.8513610999627337, 0.853213547350412, 0.854920169382628, 0.8565096797318791, 0.8580077044104999, 0.8594674256176595, 0.8608765779029313, 0.8622099191263424, 0.8634657658476598, 0.8645773546087318, 0.8656475750068009, 0.8666874672269588, 0.8676930886405678, 0.8686400366402062, 0.8695015904351188, 0.8702652910088549, 0.8709992326478642, 0.8717875912098588, 0.872613407404949, 0.8733820744221599, 0.8740506953471919, 0.8746942708094735, 0.8754246954957835, 0.8760309244396529, 0.8766439237557928, 0.8772583661093571, 0.8778556141525835, 0.8784512661117253, 0.8790221580094583, 0.8796312197209709, 0.8801725800589975, 0.8807015849441553, 0.8812543111050555, 0.8817495115986935, 0.88223933382153, 0.8827848055185114, 0.8832734409458037, 0.8835481272648908, 0.8840579786210891, 0.8845262535833619, 0.8848941865779696, 0.8853670161552225, 0.8857973572676442, 0.8861706773271478, 0.8866065025842356, 0.8870221409989848, 0.8874612476412883, 0.887819241238409, 0.8881181479389036, 0.8884941208145869, 0.888899925718178, 0.8892582107338003, 0.8895480791157082, 0.8900252355475298, 0.8902826153242644, 0.890551459504278, 0.8908979788650814, 0.8911843057017182, 0.8915094291701677, 0.8917462006715249, 0.8920407112798984, 0.8923452623083777, 0.8925821919018755, 0.8928187160729375, 0.8931221965328272, 0.8933628129122133, 0.8936258345810326, 0.8939043707127227, 0.8941806298681484, 0.8944944033235169, 0.8947094061666913, 0.8949284493136341, 0.8952534713304066, 0.8955436299478735, 0.8958048087524124, 0.8960840475038785, 0.8963957802004269, 0.8966763396273204, 0.8969242288627242, 0.8971961572995876, 0.8974315922975266, 0.8976085705147101, 0.8978074144887559, 0.8980584536784924, 0.898244897468312, 0.8985405440148547, 0.898862393429353, 0.8990800503869437, 0.8993270228249474, 0.8995586346632075, 0.8997461229295372, 0.8999078869228053, 0.9001138923369756, 0.9003225847466427, 0.9005359845522479, 0.9006699517058733, 0.900876480552451, 0.9011205212834278, 0.9013355076436879, 0.9015499217072076, 0.901782313747681, 0.9019845632353161, 0.9021805026182259, 0.9024219522295114, 0.9026299562844303, 0.9027683678576761, 0.9029509948961979, 0.903161862207058, 0.9033307524963651, 0.9034966686007798, 0.9037087721126102, 0.9038717274387246, 0.9040161341810555, 0.9041739659860489, 0.904348602742695, 0.9045267742605997, 0.9047242589100288, 0.9049368002290203, 0.9051025828768455, 0.9051959476396408, 0.9054055700106897, 0.9055476911196243, 0.9056662995224275, 0.9057823837290072, 0.9059518915839582, 0.9061277001415091, 0.9062394609159333, 0.90630985472721, 0.9065010562930642, 0.9066498862142377, 0.9067838691921125, 0.9068602399960002, 0.9070011614790505, 0.9070861020864057, 0.9072577715877597, 0.9074494765199306, 0.9075755440315126, 0.9076703315038228, 0.9078091907491586, 0.9079480789139991, 0.9081544224218701, 0.9082517759241922, 0.9084347251774725, 0.9085877537613771, 0.9087393943309297, 0.9088341263114118, 0.9089122654521421, 0.9090872584240467, 0.9093028087213615, 0.9094386031710692, 0.9095935505032332, 0.9097678082366862, 0.9098850045205757, 0.9099836859737416, 0.9101443986505923, 0.9102520179228992, 0.9103279489286897, 0.9104148159267399, 0.9105743764333184, 0.9106854288059056, 0.9107947125852911, 0.9110092533295766, 0.911183810906595, 0.9112966628008929, 0.9113843146617229, 0.9115236912543361, 0.9116444077924315, 0.9117484023790983, 0.9118652489951936, 0.9120168418282323, 0.9121254456898904, 0.9122464046046592, 0.9123691464231708, 0.9124494952717637, 0.9125333989307262, 0.9126275134142685, 0.9126495095292371, 0.9127644929386243, 0.9128029098892247, 0.912914287153117, 0.9129192316382482], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 451176326, "moving_var_accuracy_train": [0.021385889843962294, 0.04789943076890001, 0.06828931215153797, 0.08712006121765235, 0.10228596179505153, 0.11259387156532158, 0.11935120978138156, 0.12279328850099688, 0.12383068836821945, 0.12276206257068112, 0.12017734160163807, 0.11616874280312375, 0.11142341580713105, 0.10605917449231657, 0.10038295044704287, 0.09448897933683463, 0.08851356534963943, 0.08250537375238927, 0.07669778017800336, 0.07110731598615774, 0.06572808589417531, 0.06058032492670334, 0.055696110764301844, 0.05112915866742337, 0.04683597429288914, 0.04286503729281202, 0.03915780658442072, 0.03574567376145742, 0.032603653957268455, 0.029700206739481066, 0.027036125183229114, 0.024606691532825926, 0.02236251661266038, 0.020314227403594814, 0.018436258929514445, 0.01672869961343181, 0.015175616343991501, 0.013766384653664704, 0.012476447065817681, 0.011300205875768247, 0.010225298104638389, 0.009255790479711699, 0.008383079976562891, 0.007588886028886804, 0.006867845058244213, 0.006216278119683823, 0.005625534359632486, 0.0050891939525168454, 0.004603013445618546, 0.004162908802496507, 0.0037657949962705416, 0.0034070868881112652, 0.003082378388662568, 0.0027883349086833048, 0.002520622083978648, 0.0022788682208847747, 0.0020607137812622012, 0.0018637438729835623, 0.001685439880311377, 0.0015235763667539935, 0.001376467877175519, 0.001243669122423212, 0.001124895793181324, 0.0010185439653558502, 0.0009220072096703962, 0.0008338299741758733, 0.0007541746811391441, 0.0006835588950265684, 0.0006185106273153768, 0.0005600414780381312, 0.0005074351848870009, 0.00045990201342454, 0.0004171050233899512, 0.0003783277790810305, 0.00034383360668880384, 0.0003120878851602177, 0.00028339771216088366, 0.0002578074968252854, 0.00023423375890285116, 0.00021296971530242596, 0.00019435059812205298, 0.0001770644195370953, 0.00016003705074842795, 0.00014637288132233984, 0.00013370912615272998, 0.00012155658573414589, 0.00011141303744285738, 0.00010193847495593427, 9.299893826179055e-05, 8.540853732805185e-05, 7.842248122158369e-05, 7.231556488926023e-05, 6.623744314054895e-05, 6.041780576689939e-05, 5.56482256194554e-05, 5.156550163551736e-05, 4.756426484374023e-05, 4.356405146883527e-05, 4.125675066580996e-05, 3.772727474447674e-05, 3.460504200817353e-05, 3.222521881405984e-05, 2.974054444906047e-05, 2.7717837431783683e-05, 2.545060038329967e-05, 2.3686168830970654e-05, 2.215231390840405e-05, 2.0442303208038814e-05, 1.8901566038703936e-05, 1.7840312940647407e-05, 1.657734782484277e-05, 1.5542236626774823e-05, 1.4686254354008904e-05, 1.3904501007217137e-05, 1.340013493814116e-05, 1.2476157447484445e-05, 1.1660360804739933e-05, 1.1445078526747338e-05, 1.1058298883685413e-05, 1.0566398306780563e-05, 1.0211526998985301e-05, 1.0064969765962183e-05, 9.766895117535006e-06, 9.343247263043284e-06, 9.074428209713874e-06, 8.665852133033317e-06, 8.081158523946988e-06, 7.628893005681203e-06, 7.433189778164916e-06, 7.002722381209187e-06, 7.0891120674321124e-06, 7.312484271204909e-06, 7.0076068047732515e-06, 6.855804590497263e-06, 6.653020524047792e-06, 6.304085121744812e-06, 5.909184915232847e-06, 5.700210499716456e-06, 5.522162146418966e-06, 5.3798012250681364e-06, 5.00334588681562e-06, 4.886898778352422e-06, 4.934211805898245e-06, 4.856762841189147e-06, 4.78484707278566e-06, 4.792416909785628e-06, 4.681318916044673e-06, 4.558717200416203e-06, 4.6275267134841886e-06, 4.554165223900227e-06, 4.271168573985574e-06, 4.144225433380201e-06, 4.129988095146506e-06, 3.973704654032084e-06, 3.824087571966561e-06, 3.846569912347298e-06, 3.7009028658940006e-06, 3.5184923443803335e-06, 3.3908410179495123e-06, 3.3262388871014836e-06, 3.2793208065236425e-06, 3.3023904067126563e-06, 3.3787156765495186e-06, 3.2881990857738972e-06, 3.03783198758297e-06, 3.129522634821936e-06, 2.9983560577833036e-06, 2.8251320309446325e-06, 2.6638987150053203e-06, 2.6561050595152443e-06, 2.6686723937372885e-06, 2.5142193906626472e-06, 2.3073950495909675e-06, 2.4056778936974514e-06, 2.364463213256203e-06, 2.289579837172549e-06, 2.1131143506333523e-06, 2.0805326950361856e-06, 1.9374135865333985e-06, 2.008905987135883e-06, 2.138772417590055e-06, 2.0679323331194893e-06, 1.942001083970092e-06, 1.921337985709874e-06, 1.9028134881337696e-06, 2.095730928485153e-06, 1.9714571753659384e-06, 2.075545321311737e-06, 2.0787505166073484e-06, 2.0778292259547004e-06, 1.9508136364938722e-06, 1.8106838006712012e-06, 1.9052182825482942e-06, 2.132853830345879e-06, 2.085529640454059e-06, 2.0930547581114186e-06, 2.157041101313783e-06, 2.0649517117999985e-06, 1.9460988034101485e-06, 1.983946003573638e-06, 1.8897885731631869e-06, 1.7526993746101562e-06, 1.6453423153013976e-06, 1.7099440811075584e-06, 1.6499433381122615e-06, 1.5924355042319196e-06, 1.8474415324362596e-06, 1.936930508443262e-06, 1.857857408018865e-06, 1.7412173055794111e-06, 1.7419280861377644e-06, 1.6988876206515525e-06, 1.6263327250903443e-06, 1.5865776378177068e-06, 1.6347433572944395e-06, 1.5774222104685204e-06, 1.5513595209800745e-06, 1.531813554985768e-06, 1.4367356367189576e-06, 1.3564204889326032e-06, 1.3004962641513742e-06, 1.1748010993995995e-06, 1.1763116493681689e-06, 1.0719632432723046e-06, 1.0764109731541315e-06, 9.689899072376392e-07], "duration": 87550.958533, "accuracy_train": [0.4874638430347914, 0.6129777189461055, 0.6341079143018641, 0.6920179984542267, 0.7265414114410299, 0.7406525412975268, 0.7581569464631783, 0.7688272165697674, 0.7814731421880768, 0.789843317414175, 0.798886343842285, 0.801862894806663, 0.8096975648301956, 0.814392562984496, 0.8203903654485051, 0.8243431184246955, 0.8276680812223146, 0.8285970592815615, 0.8333868658291805, 0.8371067434362311, 0.8390126444836655, 0.8400113770071982, 0.840965408995478, 0.8437316151024363, 0.8441737538644334, 0.8472665622692875, 0.8474064316860466, 0.850009156399963, 0.8520087843761536, 0.852589711090347, 0.8542165942806386, 0.8569377393641565, 0.856308507232835, 0.8578670779115909, 0.8580294778400701, 0.8597490064945552, 0.8612371017326504, 0.863096860292082, 0.862910127410945, 0.8631430027800849, 0.8625388245777963, 0.8645395340185493, 0.8669315533868586, 0.867257795196567, 0.8678390823989479, 0.8691644172203765, 0.8698855738395165, 0.8702797676725729, 0.8708152728751385, 0.8714899265180879, 0.8726049164820967, 0.8735589484703765, 0.8742099901370433, 0.8747683863395165, 0.8745816534583795, 0.8752795585894242, 0.8760464972083795, 0.8767436813630491, 0.8771625686369509, 0.8772555745893319, 0.8771385961724806, 0.8776047073989479, 0.878882818267811, 0.8800457531607604, 0.880300077577058, 0.8800682836724806, 0.8804864499700074, 0.8819985176725729, 0.8814869849344776, 0.8821609176010521, 0.882788347291436, 0.8832308465416205, 0.8838121337440015, 0.884160185089055, 0.8851127751245846, 0.8850448231012367, 0.8854626289105758, 0.8862288465531561, 0.886206316041436, 0.886647733827058, 0.8876940507913437, 0.887671159791436, 0.886020304136674, 0.8886466408268733, 0.8887407282438169, 0.8882055835294389, 0.8896224823504982, 0.8896704272794389, 0.8895305578626799, 0.8905289298980252, 0.8907628867317275, 0.8914132074220191, 0.8910411836124953, 0.8908083082433554, 0.8918778766957364, 0.8925521698504982, 0.8924827758744001, 0.8921568945528792, 0.8943196434339239, 0.8925990333148762, 0.8929710571244001, 0.8940166531123109, 0.8937612472314507, 0.8944355403862125, 0.8938771441837394, 0.8946913067552602, 0.8950862215646919, 0.8947145582433554, 0.8949474336124953, 0.8958535206718347, 0.8955283603266887, 0.8959930296004062, 0.8964111958979328, 0.8966669622669805, 0.8973183644218347, 0.8966444317552602, 0.8968998376361205, 0.8981786694813585, 0.8981550575050757, 0.8981554179932633, 0.8985971962670728, 0.8992013744693614, 0.8992013744693614, 0.8991552319813585, 0.8996435132313585, 0.8995505072789776, 0.8992013744693614, 0.899597010255168, 0.9003178063861205, 0.8999228915766887, 0.9012013629337394, 0.9017590381598376, 0.9010389630052602, 0.9015497747669805, 0.901643141207549, 0.9014335173265043, 0.9013637628622186, 0.9019679410645073, 0.9022008164336471, 0.9024565828026948, 0.9018756560885014, 0.9027352401716501, 0.9033168878622186, 0.9032703848860282, 0.9034796482788853, 0.9038738421119417, 0.9038048086240311, 0.903943957064415, 0.9045949987310816, 0.9045019927787007, 0.9040140720168882, 0.9045946382428941, 0.9050596680047989, 0.9048507651001293, 0.9049899135405132, 0.9056177037190846, 0.9053383253737541, 0.9053157948620341, 0.9055944522309893, 0.9059203335525102, 0.9061303179217424, 0.9065016207548912, 0.9068496720999446, 0.9065946267072721, 0.9060362305047989, 0.9072921713501293, 0.906826781100037, 0.906733775147656, 0.9068271415882245, 0.9074774622785161, 0.9077099771594684, 0.9072453078857512, 0.9069433990287007, 0.9082218703857512, 0.9079893555047989, 0.9079897159929864, 0.9075475772309893, 0.9082694548265043, 0.9078505675526025, 0.9088027970999446, 0.9091748209094684, 0.9087101516357512, 0.9085234187546143, 0.9090589239571798, 0.9091980723975637, 0.9100115139927095, 0.9091279574450905, 0.9100812684569952, 0.909965011016519, 0.9101041594569029, 0.9096867141357512, 0.9096155177187154, 0.9106621951711886, 0.9112427613971945, 0.9106607532184385, 0.9109880764927095, 0.911336127837763, 0.9109397710755813, 0.9108718190522334, 0.9115908127422481, 0.9112205913736618, 0.9110113279808048, 0.9111966189091916, 0.9120104209925249, 0.9116849001591916, 0.91177826659976, 0.912940120028147, 0.91275482909976, 0.9123123298495754, 0.9121731814091916, 0.9127780805878553, 0.9127308566352897, 0.9126843536590993, 0.9129168685400517, 0.9133811773255813, 0.9131028804448136, 0.9133350348375784, 0.9134738227897747, 0.9131726349090993, 0.9132885318613879, 0.9134745437661499, 0.9128474745639534, 0.9137993436231081, 0.913148662444629, 0.913916682528147, 0.9129637320044297], "end": "2016-01-25 04:21:51.237000", "learning_rate_per_epoch": [0.0006639750208705664, 0.0003319875104352832, 0.00022132499725557864, 0.0001659937552176416, 0.00013279500126373023, 0.00011066249862778932, 9.485357441008091e-05, 8.29968776088208e-05, 7.377500151051208e-05, 6.639750063186511e-05, 6.036136255715974e-05, 5.533124931389466e-05, 5.107499964651652e-05, 4.7426787205040455e-05, 4.426500163390301e-05, 4.14984388044104e-05, 3.905735502485186e-05, 3.688750075525604e-05, 3.494605334708467e-05, 3.319875031593256e-05, 3.161785571137443e-05, 3.018068127857987e-05, 2.886847869376652e-05, 2.766562465694733e-05, 2.6559000616543926e-05, 2.553749982325826e-05, 2.4591667170170695e-05, 2.3713393602520227e-05, 2.2895690563018434e-05, 2.2132500816951506e-05, 2.141854929504916e-05, 2.07492194022052e-05, 2.0120454792049713e-05, 1.952867751242593e-05, 1.8970715245814063e-05, 1.844375037762802e-05, 1.7945270883501507e-05, 1.7473026673542336e-05, 1.7025000488501973e-05, 1.659937515796628e-05, 1.6194511772482656e-05, 1.5808927855687216e-05, 1.5441279174410738e-05, 1.5090340639289934e-05, 1.4754999938304536e-05, 1.443423934688326e-05, 1.412712754245149e-05, 1.3832812328473665e-05, 1.3550510630011559e-05, 1.3279500308271963e-05, 1.3019117432122584e-05, 1.276874991162913e-05, 1.2527830222097691e-05, 1.2295833585085347e-05, 1.2072272511431947e-05, 1.1856696801260114e-05, 1.1648684449028224e-05, 1.1447845281509217e-05, 1.125381368183298e-05, 1.1066250408475753e-05, 1.0884836228797212e-05, 1.070927464752458e-05, 1.0539285540289711e-05, 1.03746097011026e-05, 1.0214999747404363e-05, 1.0060227396024857e-05, 9.910074368235655e-06, 9.764338756212965e-06, 9.622825928090606e-06, 9.485357622907031e-06, 9.351761036668904e-06, 9.22187518881401e-06, 9.095548193727154e-06, 8.972635441750754e-06, 8.853000508679543e-06, 8.736513336771168e-06, 8.623052053735591e-06, 8.512500244250987e-06, 8.404746949963737e-06, 8.29968757898314e-06, 8.197222086891998e-06, 8.097255886241328e-06, 7.999698937055655e-06, 7.904463927843608e-06, 7.811470823071431e-06, 7.720639587205369e-06, 7.631896551174577e-06, 7.545170319644967e-06, 7.460393135261256e-06, 7.377499969152268e-06, 7.296428520930931e-06, 7.21711967344163e-06, 7.139516128518153e-06, 7.063563771225745e-06, 6.9892107603664044e-06, 6.916406164236832e-06, 6.8451031438598875e-06, 6.775255315005779e-06, 6.706818112434121e-06, 6.6397501541359816e-06, 6.574010058102431e-06, 6.509558716061292e-06, 6.446359293477144e-06, 6.384374955814565e-06, 6.323571597022237e-06, 6.2639151110488456e-06, 6.205374120327178e-06, 6.147916792542674e-06, 6.091514023864875e-06, 6.036136255715974e-06, 5.9817566580022685e-06, 5.928348400630057e-06, 5.875885108252987e-06, 5.824342224514112e-06, 5.773695647803834e-06, 5.7239226407546084e-06, 5.675000011251541e-06, 5.62690684091649e-06, 5.579621756623965e-06, 5.5331252042378765e-06, 5.487396720127435e-06, 5.442418114398606e-06, 5.398170742410002e-06, 5.35463732376229e-06, 5.311800123308785e-06, 5.269642770144856e-06, 5.228149802860571e-06, 5.1873048505513e-06, 5.147092906554462e-06, 5.1074998737021815e-06, 5.06851165482658e-06, 5.030113698012428e-06, 4.992293270333903e-06, 4.9550371841178276e-06, 4.918333161185728e-06, 4.882169378106482e-06, 4.8465331019542646e-06, 4.811412964045303e-06, 4.776798505190527e-06, 4.742678811453516e-06, 4.7090425141504966e-06, 4.675880518334452e-06, 4.643181910068961e-06, 4.610937594407005e-06, 4.5791380216542166e-06, 4.547774096863577e-06, 4.516836725088069e-06, 4.486317720875377e-06, 4.456207989278482e-06, 4.426500254339771e-06, 4.397185421112226e-06, 4.368256668385584e-06, 4.339705810707528e-06, 4.311526026867796e-06, 4.283709586161422e-06, 4.256250122125493e-06, 4.229140358802397e-06, 4.202373474981869e-06, 4.175943558948347e-06, 4.14984378949157e-06, 4.124068254895974e-06, 4.098611043445999e-06, 4.073466243426083e-06, 4.048627943120664e-06, 4.024091140308883e-06, 3.999849468527827e-06, 3.975898380303988e-06, 3.952231963921804e-06, 3.928846126655117e-06, 3.905735411535716e-06, 3.882894816342741e-06, 3.8603197936026845e-06, 3.838005795842037e-06, 3.815948275587289e-06, 3.7941429127386073e-06, 3.7725851598224835e-06, 3.751271151486435e-06, 3.730196567630628e-06, 3.7093575429025805e-06, 3.688749984576134e-06, 3.6683702546724817e-06, 3.6482142604654655e-06, 3.6282788187236292e-06, 3.608559836720815e-06, 3.5890541312255664e-06, 3.5697580642590765e-06, 3.550668452589889e-06, 3.5317818856128724e-06, 3.5130951800965704e-06, 3.4946053801832022e-06, 3.4763088478939608e-06, 3.458203082118416e-06, 3.4402848996251123e-06, 3.4225515719299437e-06, 3.4050001431751298e-06, 3.3876276575028896e-06, 3.3704316138027934e-06, 3.3534090562170604e-06, 3.336557711008936e-06, 3.3198750770679908e-06, 3.3033581985364435e-06, 3.2870050290512154e-06, 3.270812840128201e-06, 3.254779358030646e-06, 3.2389025363954715e-06, 3.223179646738572e-06, 3.2076086426968686e-06, 3.1921874779072823e-06, 3.1769138786330586e-06, 3.1617857985111186e-06, 3.146800963804708e-06, 3.1319575555244228e-06, 3.1172535273071844e-06, 3.102687060163589e-06, 3.0882558803568827e-06, 3.073958396271337e-06, 3.0597925615438726e-06, 3.0457570119324373e-06, 3.031849246326601e-06, 3.018068127857987e-06, 3.004411837537191e-06, 2.9908783290011343e-06, 2.9774664653814398e-06, 2.9641742003150284e-06, 2.950999942186172e-06, 2.9379425541264936e-06], "accuracy_valid": [0.48695759600903615, 0.6230336384600903, 0.6330948795180723, 0.6816494493599398, 0.7028896837349398, 0.7180278732115963, 0.7296348479856928, 0.7393401731927711, 0.7510692182793675, 0.7556064100150602, 0.7644969526543675, 0.7660838667168675, 0.7712417051016567, 0.7772643307605422, 0.7793498211596386, 0.7833987316453314, 0.7837752376694277, 0.7867255153426205, 0.7878344432417168, 0.7902655544051205, 0.7922083843185241, 0.7935820430158133, 0.7946909709149097, 0.7967970514871988, 0.794823336314006, 0.797508883189006, 0.7991369775978916, 0.8002150202371988, 0.8015680887612951, 0.8027990869728916, 0.8029108621987951, 0.8055052240210843, 0.8032667780496988, 0.8030329325112951, 0.8025755365210843, 0.8042433405496988, 0.8054846338478916, 0.8073053934487951, 0.8064509012612951, 0.8055861139871988, 0.805687594126506, 0.8083025461219879, 0.8079260400978916, 0.8076818994728916, 0.8076716043862951, 0.8092688135353916, 0.8103777414344879, 0.8092585184487951, 0.8111307534826807, 0.8099909403237951, 0.8114866693335843, 0.8105909967996988, 0.8114660791603916, 0.8118219950112951, 0.8102453760353916, 0.8107336572853916, 0.8122999811746988, 0.8124426416603916, 0.8125647119728916, 0.8119234751506024, 0.8092173381024097, 0.811546969126506, 0.8131750635353916, 0.8124323465737951, 0.8122896860881024, 0.8121676157756024, 0.8130324030496988, 0.8124117564006024, 0.8114351939006024, 0.8118116999246988, 0.8119234751506024, 0.8118014048381024, 0.8121779108621988, 0.8120455454631024, 0.8124117564006024, 0.8109572077371988, 0.8121882059487951, 0.8132765436746988, 0.8129000376506024, 0.8132765436746988, 0.812767672251506, 0.8129103327371988, 0.8121779108621988, 0.8136324595256024, 0.8121470256024097, 0.8135206842996988, 0.8117808146649097, 0.8143751764871988, 0.8139986704631024, 0.8127779673381024, 0.813500094126506, 0.8141310358621988, 0.8143751764871988, 0.8141310358621988, 0.8131441782756024, 0.8136530496987951, 0.8132868387612951, 0.8121676157756024, 0.812401461314006, 0.8122896860881024, 0.8142428110881024, 0.8146193171121988, 0.8133883189006024, 0.8139677852033133, 0.8147413874246988, 0.8140089655496988, 0.8148634577371988, 0.8137545298381024, 0.8143751764871988, 0.8141413309487951, 0.8133883189006024, 0.8139780802899097, 0.8141310358621988, 0.8136324595256024, 0.8151178934487951, 0.8137545298381024, 0.814110445689006, 0.813378023814006, 0.8135103892131024, 0.8145075418862951, 0.8137545298381024, 0.815575289439006, 0.8156061746987951, 0.8138868952371988, 0.8137545298381024, 0.8131441782756024, 0.8143751764871988, 0.8138766001506024, 0.8132765436746988, 0.8152296686746988, 0.8149752329631024, 0.8151075983621988, 0.813866305064006, 0.8148531626506024, 0.814232516001506, 0.8153723291603916, 0.8145075418862951, 0.8159517954631024, 0.8153414439006024, 0.8148634577371988, 0.815819430064006, 0.8156061746987951, 0.8143442912274097, 0.8163283014871988, 0.815819430064006, 0.8146090220256024, 0.8166945124246988, 0.8158400202371988, 0.8147310923381024, 0.8174372293862951, 0.815819430064006, 0.816185641001506, 0.8169489481362951, 0.8161959360881024, 0.8170504282756024, 0.8166945124246988, 0.815941500376506, 0.8154635142131024, 0.8164503717996988, 0.816673922251506, 0.8163180064006024, 0.8165621470256024, 0.816185641001506, 0.8165724421121988, 0.8165621470256024, 0.8159517954631024, 0.8169283579631024, 0.8181490610881024, 0.8173048639871988, 0.8182917215737951, 0.8176710749246988, 0.8179152155496988, 0.8165827371987951, 0.8172945689006024, 0.8168165827371988, 0.8190241434487951, 0.8179152155496988, 0.8187902979103916, 0.8167151025978916, 0.8164400767131024, 0.8173048639871988, 0.8168062876506024, 0.816673922251506, 0.8184034967996988, 0.8166945124246988, 0.8175592996987951, 0.8174269342996988, 0.8170298381024097, 0.8169283579631024, 0.815697359751506, 0.816307711314006, 0.815941500376506, 0.8176710749246988, 0.8168062876506024, 0.8165415568524097, 0.8176710749246988, 0.816551851939006, 0.8175181193524097, 0.816673922251506, 0.8173857539533133, 0.818260836314006, 0.8181593561746988, 0.8159312052899097, 0.8163988963667168, 0.8159312052899097, 0.816673922251506, 0.8176710749246988, 0.8170504282756024, 0.8171519084149097, 0.8167856974774097, 0.8174166392131024, 0.818138766001506, 0.817406344126506, 0.8161650508283133, 0.816918062876506, 0.8160429805158133, 0.817406344126506, 0.817650484751506, 0.816185641001506, 0.816063570689006, 0.8170195430158133, 0.8179049204631024, 0.816551851939006, 0.8165415568524097, 0.8167651073042168, 0.8170195430158133], "accuracy_test": 0.8082569355867347, "start": "2016-01-24 04:02:40.278000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0, 211.0, 212.0, 213.0, 214.0, 215.0, 216.0, 217.0, 218.0, 219.0, 220.0, 221.0, 222.0, 223.0, 224.0, 225.0], "accuracy_train_last": 0.9129637320044297, "batch_size_eval": 1024, "accuracy_train_std": [0.014985334038252628, 0.014246790424618743, 0.01657387207853718, 0.01641727540066337, 0.01581797399217006, 0.01628668305283277, 0.01614942569760007, 0.014705153217763474, 0.014781026300113183, 0.014435773419368313, 0.013278587262345013, 0.01246982158415344, 0.012064537444075938, 0.012169018610507861, 0.011960901355616211, 0.012658884370996633, 0.012958212015105266, 0.011574202445662147, 0.011274542149179799, 0.01233019573344026, 0.011684967428862285, 0.011891450670498545, 0.012371981749230123, 0.012001262319241462, 0.012200510902758695, 0.011700258322720104, 0.011467813761261788, 0.011850146166448082, 0.01169729877953673, 0.011336539328146504, 0.011546258422521041, 0.011457084535770675, 0.010674459609918783, 0.011489809842386987, 0.011142702350843527, 0.010812451569652, 0.01066598421112856, 0.011119145008192199, 0.010976871958981197, 0.010961580169261046, 0.010045456789842727, 0.009988150103435511, 0.011052627857749716, 0.010948430500039321, 0.011453723801042476, 0.010667007224320317, 0.010748145596357898, 0.010426554340632437, 0.011192120974261196, 0.01026443224116981, 0.01088951254053519, 0.01084987897544933, 0.01063850851552063, 0.0105859548466461, 0.010472766307610636, 0.010937436818680816, 0.010854427555676241, 0.010438895714799109, 0.010755941096203458, 0.010416948157030769, 0.009986844112533685, 0.010436705755028848, 0.010913123536835038, 0.010811441594682367, 0.010697831480523978, 0.009952840184639218, 0.011256171876468232, 0.010341477188697665, 0.010938797856315782, 0.009899032280333883, 0.010979695180090477, 0.010619309709695745, 0.010521246394477816, 0.010068144716101983, 0.010094712031171885, 0.009656190821064569, 0.01022111434827986, 0.01026978644364353, 0.010432254957382798, 0.009871224388584376, 0.0102717766494497, 0.010543339283777141, 0.011257287304028852, 0.010397929666974138, 0.010609779025808203, 0.0103975979703883, 0.009576602395191387, 0.010323809815191732, 0.010154584376669901, 0.010010584875764951, 0.009779733445364364, 0.009818095084441055, 0.010473535695651369, 0.010298393084462069, 0.009852566149940475, 0.009636518135135167, 0.009964285058456828, 0.00959544912980794, 0.009763821687289178, 0.01013081887764901, 0.01038233408098768, 0.00978089601090307, 0.009582623947829206, 0.00948342837493511, 0.010211150934760164, 0.010032903130850665, 0.009987759108014653, 0.009582187683062052, 0.009951943844176536, 0.009512159186768008, 0.009812469347360894, 0.009629471956428059, 0.009846287926998636, 0.009699643632340278, 0.009538256644641786, 0.009522543390264981, 0.009762989615809195, 0.009393379642154809, 0.009185697909290122, 0.009609000273368093, 0.009187870325216868, 0.009408940140316022, 0.009946242804152069, 0.009499460012390633, 0.009412571876922297, 0.00957224121801765, 0.009154517794220779, 0.009464109118751469, 0.009360985267724895, 0.009233720260198737, 0.009163994194597083, 0.009382137483003093, 0.00986056403202542, 0.0094907727685199, 0.009524672687721787, 0.009423875737789992, 0.009111731797335812, 0.009376653701619964, 0.009463054889557955, 0.009353652348339635, 0.009112839546939193, 0.00933353490616475, 0.00921406367505895, 0.009089875424627516, 0.009248355673020962, 0.00876072006269991, 0.008870444813808682, 0.008572007004557777, 0.009030674918954582, 0.009050844457095279, 0.009110556416690351, 0.00900228960826128, 0.00900768548443916, 0.008893099932244696, 0.008908159542500344, 0.008861289096042127, 0.00880781522593859, 0.008854447245613385, 0.009332645687694298, 0.009104348765222309, 0.009173625675762572, 0.008985098767450584, 0.008590869088155288, 0.008759315546553039, 0.009160643841541017, 0.00873517097947447, 0.008911060639535737, 0.009050554242018106, 0.00849153795952474, 0.008404468655576432, 0.008501816091628752, 0.008739997775270408, 0.008724322458143953, 0.00856732746867981, 0.008784557542280032, 0.008589083281265382, 0.008604470665924136, 0.008493777020414593, 0.008255914566230023, 0.008769842077924481, 0.008459147248788015, 0.008732306117499157, 0.008624477658532484, 0.0085378056199121, 0.007850739051772719, 0.008503166805801373, 0.008538283136398248, 0.008770894929819164, 0.008063630191205692, 0.008601489986577027, 0.008474166092756545, 0.008529094013208728, 0.00850171423253355, 0.007518695498829011, 0.008146946626072219, 0.007885869760934736, 0.007964041689240694, 0.008208273230721824, 0.008153325720524615, 0.008172792217819604, 0.007938363836201898, 0.008206553145181568, 0.008418168156701282, 0.008061666090457059, 0.008290090261881246, 0.008393398853280332, 0.00771202913113339, 0.007649602737426364, 0.007271929348656774, 0.007972808119925958, 0.007923219061463416, 0.00798738675950717, 0.007945273891286384, 0.007446409584062482, 0.007989605407659993, 0.007866419438485614, 0.007445664878487502, 0.007817154187533802, 0.007942088614841695, 0.007959949962055719, 0.007753536723216784, 0.007613238018032311, 0.007356568435150657, 0.00733106408813712, 0.007421768496288729, 0.0077445975837853835], "accuracy_test_std": 0.009956148280702015, "error_valid": [0.5130424039909638, 0.3769663615399097, 0.3669051204819277, 0.31835055064006024, 0.29711031626506024, 0.28197212678840367, 0.2703651520143072, 0.2606598268072289, 0.24893078172063254, 0.24439358998493976, 0.23550304734563254, 0.23391613328313254, 0.22875829489834332, 0.22273566923945776, 0.22065017884036142, 0.21660126835466864, 0.2162247623305723, 0.21327448465737953, 0.2121655567582832, 0.20973444559487953, 0.20779161568147586, 0.20641795698418675, 0.2053090290850903, 0.20320294851280118, 0.20517666368599397, 0.20249111681099397, 0.2008630224021084, 0.19978497976280118, 0.19843191123870485, 0.1972009130271084, 0.19708913780120485, 0.19449477597891573, 0.19673322195030118, 0.19696706748870485, 0.19742446347891573, 0.19575665945030118, 0.1945153661521084, 0.19269460655120485, 0.19354909873870485, 0.19441388601280118, 0.19431240587349397, 0.19169745387801207, 0.1920739599021084, 0.1923181005271084, 0.19232839561370485, 0.1907311864646084, 0.18962225856551207, 0.19074148155120485, 0.1888692465173193, 0.19000905967620485, 0.18851333066641573, 0.18940900320030118, 0.1885339208396084, 0.18817800498870485, 0.1897546239646084, 0.1892663427146084, 0.18770001882530118, 0.1875573583396084, 0.1874352880271084, 0.18807652484939763, 0.1907826618975903, 0.18845303087349397, 0.1868249364646084, 0.18756765342620485, 0.18771031391189763, 0.18783238422439763, 0.18696759695030118, 0.18758824359939763, 0.18856480609939763, 0.18818830007530118, 0.18807652484939763, 0.18819859516189763, 0.18782208913780118, 0.18795445453689763, 0.18758824359939763, 0.18904279226280118, 0.18781179405120485, 0.18672345632530118, 0.18709996234939763, 0.18672345632530118, 0.18723232774849397, 0.18708966726280118, 0.18782208913780118, 0.18636754047439763, 0.1878529743975903, 0.18647931570030118, 0.1882191853350903, 0.18562482351280118, 0.18600132953689763, 0.18722203266189763, 0.18649990587349397, 0.18586896413780118, 0.18562482351280118, 0.18586896413780118, 0.18685582172439763, 0.18634695030120485, 0.18671316123870485, 0.18783238422439763, 0.18759853868599397, 0.18771031391189763, 0.18575718891189763, 0.18538068288780118, 0.18661168109939763, 0.18603221479668675, 0.18525861257530118, 0.18599103445030118, 0.18513654226280118, 0.18624547016189763, 0.18562482351280118, 0.18585866905120485, 0.18661168109939763, 0.1860219197100903, 0.18586896413780118, 0.18636754047439763, 0.18488210655120485, 0.18624547016189763, 0.18588955431099397, 0.18662197618599397, 0.18648961078689763, 0.18549245811370485, 0.18624547016189763, 0.18442471056099397, 0.18439382530120485, 0.18611310476280118, 0.18624547016189763, 0.18685582172439763, 0.18562482351280118, 0.18612339984939763, 0.18672345632530118, 0.18477033132530118, 0.18502476703689763, 0.18489240163780118, 0.18613369493599397, 0.18514683734939763, 0.18576748399849397, 0.1846276708396084, 0.18549245811370485, 0.18404820453689763, 0.18465855609939763, 0.18513654226280118, 0.18418056993599397, 0.18439382530120485, 0.1856557087725903, 0.18367169851280118, 0.18418056993599397, 0.18539097797439763, 0.18330548757530118, 0.18415997976280118, 0.18526890766189763, 0.18256277061370485, 0.18418056993599397, 0.18381435899849397, 0.18305105186370485, 0.18380406391189763, 0.18294957172439763, 0.18330548757530118, 0.18405849962349397, 0.18453648578689763, 0.18354962820030118, 0.18332607774849397, 0.18368199359939763, 0.18343785297439763, 0.18381435899849397, 0.18342755788780118, 0.18343785297439763, 0.18404820453689763, 0.18307164203689763, 0.18185093891189763, 0.18269513601280118, 0.18170827842620485, 0.18232892507530118, 0.18208478445030118, 0.18341726280120485, 0.18270543109939763, 0.18318341726280118, 0.18097585655120485, 0.18208478445030118, 0.1812097020896084, 0.1832848974021084, 0.18355992328689763, 0.18269513601280118, 0.18319371234939763, 0.18332607774849397, 0.18159650320030118, 0.18330548757530118, 0.18244070030120485, 0.18257306570030118, 0.1829701618975903, 0.18307164203689763, 0.18430264024849397, 0.18369228868599397, 0.18405849962349397, 0.18232892507530118, 0.18319371234939763, 0.1834584431475903, 0.18232892507530118, 0.18344814806099397, 0.1824818806475903, 0.18332607774849397, 0.18261424604668675, 0.18173916368599397, 0.18184064382530118, 0.1840687947100903, 0.1836011036332832, 0.1840687947100903, 0.18332607774849397, 0.18232892507530118, 0.18294957172439763, 0.1828480915850903, 0.1832143025225903, 0.18258336078689763, 0.18186123399849397, 0.18259365587349397, 0.18383494917168675, 0.18308193712349397, 0.18395701948418675, 0.18259365587349397, 0.18234951524849397, 0.18381435899849397, 0.18393642931099397, 0.18298045698418675, 0.18209507953689763, 0.18344814806099397, 0.1834584431475903, 0.1832348926957832, 0.18298045698418675], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-3, 3], "momentum": 0.8299727674414832, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.0006639750068546349, "patience_threshold": 1, "do_flip": true, "batch_size": 128, "optimization": "adam", "nb_data_augmentation": 2, "learning_rate_decay_method": "lin", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 7.32118675582115e-09, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.08876079894492844}, "accuracy_valid_max": 0.8190241434487951, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-3, 3)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8170195430158133, "loss_train": [1.625075340270996, 1.1916815042495728, 0.9845694899559021, 0.8840711712837219, 0.8176579475402832, 0.7720222473144531, 0.735226035118103, 0.708556592464447, 0.6845017671585083, 0.6661242842674255, 0.64967280626297, 0.6361047029495239, 0.6228338479995728, 0.6109963059425354, 0.602145791053772, 0.595284104347229, 0.5868037939071655, 0.5783131718635559, 0.5712497234344482, 0.5661373734474182, 0.5609967708587646, 0.5547477006912231, 0.550183892250061, 0.5436239838600159, 0.5405871868133545, 0.536415696144104, 0.5326765179634094, 0.5283108949661255, 0.5243130922317505, 0.5220358967781067, 0.5167140364646912, 0.5145296454429626, 0.5136557221412659, 0.5101333260536194, 0.5068841576576233, 0.5044453740119934, 0.5054056644439697, 0.49957045912742615, 0.4972189962863922, 0.4943675398826599, 0.49147936701774597, 0.4904625713825226, 0.4905416965484619, 0.487436980009079, 0.4856550991535187, 0.4836861193180084, 0.4799041748046875, 0.48054173588752747, 0.47865328192710876, 0.4767429828643799, 0.47449395060539246, 0.47486019134521484, 0.47154074907302856, 0.4718395173549652, 0.4691033661365509, 0.4661079943180084, 0.46692049503326416, 0.4662848711013794, 0.4636388123035431, 0.4616098999977112, 0.4589304029941559, 0.4597617983818054, 0.45684516429901123, 0.45733973383903503, 0.45616498589515686, 0.4556705057621002, 0.45355892181396484, 0.45457619428634644, 0.4510098099708557, 0.4496197998523712, 0.44966840744018555, 0.4480571448802948, 0.44745203852653503, 0.44642287492752075, 0.4449748694896698, 0.444211483001709, 0.4424046576023102, 0.442442923784256, 0.4412635266780853, 0.4404112994670868, 0.4396666884422302, 0.43940097093582153, 0.43817487359046936, 0.4381178617477417, 0.4363572299480438, 0.4358195960521698, 0.4352874755859375, 0.4334675967693329, 0.4347946047782898, 0.4328690767288208, 0.43143993616104126, 0.43083855509757996, 0.43118152022361755, 0.4287356734275818, 0.42767131328582764, 0.42772242426872253, 0.42707160115242004, 0.42698901891708374, 0.42773112654685974, 0.42745596170425415, 0.42406409978866577, 0.4247720241546631, 0.42167073488235474, 0.42306679487228394, 0.4224003851413727, 0.42190396785736084, 0.4215497076511383, 0.4205070734024048, 0.41982364654541016, 0.4186963140964508, 0.41783782839775085, 0.41902559995651245, 0.4177883565425873, 0.41622158885002136, 0.4170794188976288, 0.41713258624076843, 0.4156704843044281, 0.41399112343788147, 0.4151836335659027, 0.4126765727996826, 0.4141305088996887, 0.41146424412727356, 0.4141932427883148, 0.4109756052494049, 0.41294965147972107, 0.40982675552368164, 0.409586638212204, 0.4106380343437195, 0.4119153618812561, 0.40928173065185547, 0.4100240170955658, 0.4074328541755676, 0.40758103132247925, 0.4058789312839508, 0.4062061011791229, 0.405364990234375, 0.4060567319393158, 0.4052163362503052, 0.40441325306892395, 0.4063418209552765, 0.4043729305267334, 0.4027058482170105, 0.40342944860458374, 0.4027998745441437, 0.40302228927612305, 0.4015319049358368, 0.40064337849617004, 0.40070995688438416, 0.4029507040977478, 0.40077173709869385, 0.39982160925865173, 0.4007152318954468, 0.39799991250038147, 0.39870908856391907, 0.397199809551239, 0.3958949148654938, 0.39926615357398987, 0.39746108651161194, 0.3956582546234131, 0.39673933386802673, 0.3966207504272461, 0.39518842101097107, 0.396664559841156, 0.39577165246009827, 0.3937186896800995, 0.39455580711364746, 0.39410680532455444, 0.39499661326408386, 0.3913438618183136, 0.3926283121109009, 0.39295101165771484, 0.39157405495643616, 0.3925779461860657, 0.38948383927345276, 0.3911426067352295, 0.39062920212745667, 0.3924797773361206, 0.38821476697921753, 0.3910214900970459, 0.3891298174858093, 0.38953524827957153, 0.38853856921195984, 0.38812321424484253, 0.38744083046913147, 0.3881846070289612, 0.3882767856121063, 0.3878172039985657, 0.3855340778827667, 0.3886745274066925, 0.3875493109226227, 0.3859633207321167, 0.38493722677230835, 0.3852287828922272, 0.38775235414505005, 0.385379821062088, 0.38352668285369873, 0.3840079605579376, 0.3860723674297333, 0.3841802179813385, 0.38365453481674194, 0.38279926776885986, 0.3815597891807556, 0.3832767605781555, 0.38096511363983154, 0.38298606872558594, 0.38210529088974, 0.38239586353302, 0.38301438093185425, 0.38099515438079834, 0.3820669651031494, 0.38178351521492004, 0.37979936599731445, 0.3802136480808258, 0.37886351346969604, 0.38091740012168884, 0.3776213824748993, 0.37820568680763245, 0.37820714712142944, 0.38010162115097046, 0.3797738254070282, 0.37732455134391785, 0.3766379952430725, 0.3793802261352539, 0.38036009669303894, 0.3775179386138916, 0.3769986629486084], "accuracy_train_first": 0.4874638430347914, "model": "residualv3", "loss_std": [0.3445146679878235, 0.1272532343864441, 0.09468405693769455, 0.08646414428949356, 0.0810110941529274, 0.07579834759235382, 0.07399319857358932, 0.07381144911050797, 0.07437615841627121, 0.07194645702838898, 0.07031639665365219, 0.07303514331579208, 0.07092402130365372, 0.0705583244562149, 0.07256104052066803, 0.07131747901439667, 0.06970272958278656, 0.06842684000730515, 0.06995869427919388, 0.0683613270521164, 0.07030603289604187, 0.0693121924996376, 0.06808672100305557, 0.06846882402896881, 0.06784994155168533, 0.06725245714187622, 0.06870205700397491, 0.06698349118232727, 0.06700287759304047, 0.06900078058242798, 0.06699883937835693, 0.0659366324543953, 0.06661992520093918, 0.0685674175620079, 0.06649807095527649, 0.06680714339017868, 0.06661774963140488, 0.06476494669914246, 0.06611325591802597, 0.06677927076816559, 0.06716030091047287, 0.06749068200588226, 0.06839237362146378, 0.0658043846487999, 0.06678961962461472, 0.06609054654836655, 0.06351442635059357, 0.06587374210357666, 0.06456992775201797, 0.06666313111782074, 0.06362728774547577, 0.06677231192588806, 0.06388181447982788, 0.06655164808034897, 0.06466282159090042, 0.06698168814182281, 0.06382942944765091, 0.06785557419061661, 0.06299443542957306, 0.06490384042263031, 0.06419338285923004, 0.06437607854604721, 0.06532268971204758, 0.06306083500385284, 0.06339482218027115, 0.06658616662025452, 0.06596190482378006, 0.06700537353754044, 0.06473104655742645, 0.06461676955223083, 0.06382878869771957, 0.06388036161661148, 0.06467920541763306, 0.06531178951263428, 0.0646883025765419, 0.0638519749045372, 0.06517785042524338, 0.06483641266822815, 0.064629927277565, 0.06373193114995956, 0.06604556739330292, 0.06479830294847488, 0.06393928825855255, 0.06305297464132309, 0.06326271593570709, 0.062250036746263504, 0.06313668191432953, 0.06549704819917679, 0.06552301347255707, 0.06436870247125626, 0.0636114552617073, 0.06282775849103928, 0.06193244084715843, 0.06063154339790344, 0.06323913484811783, 0.06312008202075958, 0.06425189226865768, 0.06378142535686493, 0.06304656714200974, 0.060124561190605164, 0.0629531517624855, 0.06242913380265236, 0.0627264603972435, 0.06229941174387932, 0.0634523555636406, 0.06485375761985779, 0.06269118189811707, 0.06311218440532684, 0.062243226915597916, 0.06359169632196426, 0.06384209543466568, 0.06256765127182007, 0.06329084187746048, 0.06166420876979828, 0.0633634477853775, 0.06257860362529755, 0.062352415174245834, 0.06203377991914749, 0.06341809779405594, 0.06066706404089928, 0.062114179134368896, 0.06132037937641144, 0.06178133934736252, 0.06305696815252304, 0.06298654526472092, 0.06194501370191574, 0.061469823122024536, 0.06140672042965889, 0.06134960055351257, 0.06131376326084137, 0.06245914101600647, 0.059871695935726166, 0.062388453632593155, 0.060644593089818954, 0.06061054393649101, 0.06212113797664642, 0.06306125223636627, 0.06077682599425316, 0.06282462179660797, 0.06251346319913864, 0.06194144859910011, 0.060203004628419876, 0.06174065172672272, 0.06043723225593567, 0.06271850317716599, 0.06114918738603592, 0.061273615807294846, 0.06267590075731277, 0.06233413144946098, 0.061000335961580276, 0.06088139861822128, 0.0629708468914032, 0.06126054376363754, 0.061929620802402496, 0.060382939875125885, 0.06044559180736542, 0.06110234931111336, 0.062270600348711014, 0.06239979341626167, 0.05976548790931702, 0.06013847887516022, 0.061782356351614, 0.05996794253587723, 0.061857569962739944, 0.062932588160038, 0.06067034229636192, 0.060425177216529846, 0.06081227958202362, 0.059106770902872086, 0.06084715574979782, 0.06080346927046776, 0.060405176132917404, 0.06026598811149597, 0.060339491814374924, 0.060855891555547714, 0.06256687641143799, 0.06022372096776962, 0.059787482023239136, 0.06024917960166931, 0.06081897020339966, 0.05982625484466553, 0.0604582354426384, 0.06003424525260925, 0.06005411222577095, 0.06104768067598343, 0.062036629766225815, 0.06115984916687012, 0.05975968763232231, 0.060358766466379166, 0.060416679829359055, 0.06123792380094528, 0.05871836096048355, 0.05980151519179344, 0.05874202400445938, 0.061587583273649216, 0.0612509548664093, 0.057770729064941406, 0.0607481524348259, 0.05839218944311142, 0.06077937036752701, 0.05966167524456978, 0.05873538926243782, 0.06003496050834656, 0.06188427656888962, 0.06130010262131691, 0.05971945449709892, 0.0588274784386158, 0.05852198600769043, 0.05978674069046974, 0.05956505984067917, 0.059782713651657104, 0.05970492959022522, 0.05997689068317413, 0.060104724019765854, 0.05891401320695877, 0.05805816873908043, 0.05720259249210358, 0.0592813566327095, 0.0602075420320034, 0.06120619550347328, 0.05936993286013603, 0.060656316578388214, 0.058839332312345505, 0.05939071998000145, 0.06216464936733246, 0.05789678171277046]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:12 2016", "state": "available"}], "summary": "b1e5d4069109a55a0915d409099b71ee"}