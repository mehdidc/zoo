{"content": {"hp_model": {"f0": 64, "f1": 64, "f2": 64, "f3": 64, "nonlin": "very_leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.8955456018447876, 1.603342890739441, 1.4872585535049438, 1.4001014232635498, 1.3354064226150513, 1.284709095954895, 1.241213083267212, 1.2027758359909058, 1.1682369709014893, 1.1366660594940186, 1.1075392961502075, 1.080459713935852, 1.0554533004760742, 1.0323632955551147, 1.0109809637069702, 0.9909065365791321, 0.9720587730407715, 0.9542282223701477, 0.9372192025184631, 0.9209976196289062, 0.9054893851280212, 0.8906136155128479, 0.8763906359672546, 0.8627036809921265, 0.8495781421661377, 0.8370318412780762, 0.8249727487564087, 0.813310980796814, 0.8020912408828735, 0.7912868857383728, 0.7808600068092346, 0.7708123922348022, 0.7611287832260132, 0.7517553567886353, 0.7426552176475525, 0.7339227795600891, 0.7254815697669983, 0.7173140645027161, 0.7093960642814636, 0.7017933130264282, 0.6944223046302795, 0.6873081922531128, 0.6804299354553223, 0.6737845540046692, 0.667342483997345, 0.6611316204071045, 0.6551389098167419, 0.6493353247642517, 0.6437610983848572, 0.638373076915741, 0.6331654787063599, 0.6281574964523315, 0.6233043074607849, 0.6186086535453796, 0.6140825748443604, 0.609706461429596, 0.6055115461349487, 0.6014493107795715, 0.5975373983383179, 0.5937623977661133, 0.5901282429695129, 0.5866338014602661, 0.5832512974739075, 0.5800025463104248, 0.5768623352050781, 0.5738382339477539, 0.5709340572357178, 0.5681373476982117, 0.5654401779174805, 0.562839686870575, 0.5603317618370056, 0.5579313635826111, 0.5556161403656006, 0.5533850789070129, 0.5512369871139526, 0.5491734147071838, 0.5471896529197693, 0.5452901124954224, 0.5434549450874329, 0.5416855812072754, 0.5399872660636902, 0.5383570194244385, 0.5367832779884338, 0.5352665781974792, 0.5338146686553955, 0.5324121713638306, 0.5310602188110352, 0.5297640562057495, 0.5285131931304932, 0.5273131728172302, 0.5261573195457458, 0.5250465273857117, 0.5239811539649963, 0.5229477882385254, 0.5219573974609375, 0.521007776260376, 0.5200864672660828, 0.5192049741744995, 0.5183506608009338, 0.5175331830978394, 0.5167471766471863, 0.5159918069839478, 0.5152651071548462, 0.5145623087882996, 0.5138913989067078, 0.5132431387901306, 0.5126187801361084, 0.512017011642456, 0.5114392638206482, 0.5108816027641296, 0.5103428959846497, 0.5098264217376709, 0.5093270540237427, 0.5088465809822083, 0.5083863735198975, 0.5079414248466492, 0.5075131058692932, 0.5071002840995789, 0.5067033767700195, 0.5063215494155884, 0.5059513449668884, 0.5055962204933167, 0.5052556991577148, 0.504924476146698, 0.5046066641807556, 0.5043001770973206, 0.5040038824081421, 0.5037202835083008, 0.5034456253051758, 0.5031803250312805, 0.5029277801513672, 0.5026838779449463, 0.5024478435516357, 0.5022205710411072, 0.5020009279251099, 0.5017896294593811, 0.5015864372253418, 0.5013906359672546, 0.5012018084526062, 0.5010194778442383, 0.5008445382118225, 0.5006757974624634, 0.5005125403404236, 0.5003564357757568, 0.5002062320709229, 0.5000615119934082, 0.4999212920665741, 0.4997863471508026, 0.49965739250183105], "moving_avg_accuracy_train": [0.041449411764705874, 0.08453270588235293, 0.1259759058823529, 0.16496655058823526, 0.2015922484705882, 0.2360730236235294, 0.2684916036141176, 0.29938832560564704, 0.3284377283392, 0.35591630844645644, 0.3816752658371049, 0.40592891572398265, 0.4286324947398197, 0.45001395114819065, 0.4700949089745481, 0.4890265945476815, 0.5067780527399721, 0.5235237768777397, 0.5392608109546716, 0.5541747298592045, 0.5682113745203429, 0.5815643547153674, 0.5940667427732425, 0.6059330096723887, 0.6171373557639734, 0.627663620187576, 0.6375725522864655, 0.6469470617637013, 0.6557817673520371, 0.6641330023815393, 0.6720726433198558, 0.6795030260466938, 0.686510370500848, 0.6931252158037043, 0.6993703412821575, 0.7052921306833535, 0.7108735058503122, 0.7160873317358691, 0.7210150691505176, 0.7257017975295834, 0.7301269118942721, 0.7343683383519037, 0.7384162103990662, 0.7422475305356302, 0.745836895129126, 0.7492508526750369, 0.7524881203487097, 0.7555593083138387, 0.7585516127765723, 0.7613999809106798, 0.7641188063490236, 0.7667728080670624, 0.7692767037309444, 0.771664327475497, 0.7739543653161826, 0.7761353993727995, 0.7782065653178725, 0.7802164970213793, 0.7821642590839473, 0.7840748919990819, 0.7858815204462325, 0.7876298389898446, 0.7893303845026249, 0.7909926401700096, 0.7925827879177145, 0.794129215008296, 0.7956151170368782, 0.7970488994508375, 0.7984734212704596, 0.7998472556140019, 0.8011660594643665, 0.8024541594002828, 0.8036722728720193, 0.8048415161730527, 0.805990305732218, 0.8071136281001726, 0.8082140299960378, 0.809277332878787, 0.8102954819438495, 0.8112518161024057, 0.812176046256871, 0.813073735748831, 0.8139240092327715, 0.8147292553683179, 0.8154751533608979, 0.8162005792012788, 0.8168887565752685, 0.8175810573883299, 0.8182300104730262, 0.8188470094257236, 0.8194470143655042, 0.8200411364583655, 0.8206252581066467, 0.821202732295982, 0.8217271649487368, 0.8222273896303337, 0.8227152389025945, 0.8231943032476292, 0.8236419317463958, 0.8240730326894032, 0.8244798470675218, 0.8248600976548872, 0.8252070290658691, 0.825545149688694, 0.8258612229551188, 0.8261739241890187, 0.826462414123058, 0.826750290357811, 0.8270164377926181, 0.827298323425121, 0.8275684910826089, 0.8278351713861127, 0.828094007188678, 0.8283410770580455, 0.8285704987640056, 0.8287769782993697, 0.8289816334106093, 0.8291893524224895, 0.8294045348272994, 0.8296193754622164, 0.8298597908571712, 0.8300926353008659, 0.8303233717707793, 0.8305475051819367, 0.8307515781931547, 0.8309517144914863, 0.8311459548070436, 0.8313372416792804, 0.831507046923117, 0.8316716363484524, 0.8318268256547836, 0.8319970842657758, 0.8321667876039041, 0.8323265794317489, 0.8324798038415152, 0.8326341763985402, 0.8327919352292744, 0.832948035823994, 0.8331144087121829, 0.8332594384291999, 0.8333970239980446, 0.8335232039511813, 0.8336273541442984, 0.8337352069651627, 0.8338416862686464, 0.8339398705829583, 0.8340376482305448, 0.8341303539957257, 0.8342114362432119], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.04113333333333332, 0.08308666666666664, 0.12380466666666665, 0.1621575333333333, 0.19764844666666662, 0.23067026866666662, 0.26156324179999996, 0.2910202509533333, 0.3188915591913333, 0.3449090699388666, 0.36913816294498, 0.391824346650482, 0.4127219119854338, 0.4325030541202237, 0.450959415374868, 0.46813014050404783, 0.4840237931203097, 0.4988214138082787, 0.5125126057607842, 0.5252480118513724, 0.5373098773329018, 0.548512222932945, 0.5589276673063172, 0.5686749005756855, 0.5776340771847837, 0.5860706694663054, 0.5941169358530082, 0.6015985756010407, 0.6085587180409366, 0.6150361795701763, 0.621105894946492, 0.6265953054518428, 0.6316557749066586, 0.6364101974159927, 0.64075584434106, 0.6446535932402874, 0.648201567249592, 0.6513947438579661, 0.6543886028055028, 0.6570697425249525, 0.6596561016057906, 0.6620104914452116, 0.6641294423006905, 0.6660498314039548, 0.6678048482635593, 0.6693310301038701, 0.6707045937601497, 0.6719941343841347, 0.6732213876123879, 0.6743525821844825, 0.6753839906327009, 0.6762455915694309, 0.6771010324124878, 0.6778842625045723, 0.6785758362541151, 0.6792382526287036, 0.6798744273658333, 0.68048698462925, 0.6811849528329916, 0.6818797908830257, 0.6825984784613899, 0.6832452972819176, 0.6838407675537259, 0.6844166907983533, 0.6849083550518513, 0.6853375195466662, 0.6857237675919996, 0.6861380574994663, 0.6865509184161864, 0.6870024932412344, 0.6873822439171109, 0.6877373528587332, 0.6880569509061932, 0.6883045891489072, 0.6886341302340165, 0.6890107172106149, 0.68941631215622, 0.6897413476072647, 0.6900205461798715, 0.6902318248952177, 0.6904619757390292, 0.6907224448317929, 0.6909568670152803, 0.6912345136470855, 0.6914577289490437, 0.6916719560541393, 0.6919180937820587, 0.6921929510705195, 0.6924936559634676, 0.6928442903671208, 0.6932131946637421, 0.6935852085307012, 0.6938933543442977, 0.6942373522432013, 0.6945602836855478, 0.694824255316993, 0.695075163118627, 0.695314313473431, 0.6955562154594213, 0.6957739272468125, 0.6959698678554647, 0.6961062144032515, 0.6962022596295929, 0.6962220336666336, 0.6962131636333035, 0.6961785139366399, 0.6961473292096426, 0.6960792629553451, 0.6960313366598105, 0.6959882029938295, 0.6959360493611132, 0.6959024444250018, 0.6959121999825016, 0.6958676466509182, 0.695867548652493, 0.6959207937872438, 0.6959687144085194, 0.6959985096343341, 0.6960253253375673, 0.6960494594704772, 0.6960711801900962, 0.6961173955044199, 0.6962123226206445, 0.6963244236919134, 0.6963986479893887, 0.6964787831904499, 0.6965242382047382, 0.696578481050931, 0.6966406329458379, 0.6966965696512541, 0.696720246019462, 0.6967148880841825, 0.6967100659424309, 0.6966657260148544, 0.696599153413369, 0.6965525714053654, 0.6964839809314955, 0.6964355828383461, 0.6963920245545114, 0.6963661554323937, 0.6963828732224877, 0.6963979192335723, 0.6963981273102151, 0.6963849812458602, 0.6963998164546076, 0.6963998348091469, 0.6963998513282321, 0.6963865328620755, 0.6963612129092013], "moving_var_accuracy_train": [0.015462483620761242, 0.03062176734693425, 0.04301744004840082, 0.0523981294147839, 0.05923129218163005, 0.06400847765979623, 0.06706630885127211, 0.06895114483454162, 0.06965084054367285, 0.06948140777170377, 0.06850498196721258, 0.06694863956600852, 0.06489284811056287, 0.0625180634027942, 0.05989546086753028, 0.05713159324853715, 0.05425446233525723, 0.05135278959381539, 0.0484463988082805, 0.04560358372127129, 0.04281647188923174, 0.04013954342110702, 0.037532376443343576, 0.035046413410105205, 0.03267160841115476, 0.030401667754479715, 0.02824518339709536, 0.02621159790883488, 0.024292906323444528, 0.022491303829761933, 0.020809514530850285, 0.01922545836497088, 0.017744838415166275, 0.01636416017907614, 0.015078758491343141, 0.013886490949617884, 0.012778207593445192, 0.011745042657384814, 0.010789081755895862, 0.00990786238639855, 0.009093310882023866, 0.008345887079380956, 0.007658765784434663, 0.007025000331890761, 0.006438452142367057, 0.005899502883257891, 0.005403871712851163, 0.004948374301220427, 0.004534121845077654, 0.004153728469816479, 0.0038048837287124986, 0.0034877888819154256, 0.0031954354351843464, 0.0029271986159758756, 0.002681677214184236, 0.0024563216787709227, 0.0022492970662421016, 0.00206072578869275, 0.0018887972032948855, 0.0017327721461929616, 0.0015888700886881508, 0.0014574926393887753, 0.0013377700708192307, 0.0012288609088710763, 0.0011287319467197482, 0.0010373816827661325, 0.0009535146580364205, 0.0008766647803279874, 0.0008072616640264048, 0.0007435222848552302, 0.0006848232487313335, 0.0006312737368623683, 0.0005815005670463633, 0.0005356546794148297, 0.000493966668534572, 0.00045592667996224043, 0.00042123197095782764, 0.0003892842910462089, 0.0003596855096097764, 0.0003319481338541918, 0.0003064411328745794, 0.00028304963740289946, 0.0002612513586400391, 0.0002409620148253466, 0.00022187308768082547, 0.00020442196276177264, 0.00018824205936823852, 0.0001737313771733038, 0.0001601485004112065, 0.00014755983973875278, 0.00013604390911472734, 0.00012561634775428815, 0.00011612549587877496, 0.00010751423424503403, 9.923807728600901e-05, 9.156629214611673e-05, 8.455163514351308e-05, 7.816199544931326e-05, 7.214913736055341e-05, 6.660685583205551e-05, 6.143565169304546e-05, 5.6593401106466995e-05, 5.201731363115311e-05, 4.7844512268253e-05, 4.3959181829163214e-05, 4.0443302201389515e-05, 3.7148009959628585e-05, 3.417906350248595e-05, 3.139866726572715e-05, 2.8973936127458417e-05, 2.6733457583085053e-05, 2.470017728326854e-05, 2.2833123309147784e-05, 2.1099202661376466e-05, 1.946299126772984e-05, 1.7900396327674602e-05, 1.6487310125915213e-05, 1.5226903804392256e-05, 1.4120944630010886e-05, 1.3124258652714147e-05, 1.2332028846624323e-05, 1.1586774776597114e-05, 1.0907251165870343e-05, 1.0268648123256653e-05, 9.616595456099593e-06, 9.015426751678467e-06, 8.453447778200632e-06, 7.937419007791875e-06, 7.403181494522574e-06, 6.906670455460296e-06, 6.4327568971103805e-06, 6.050373158952364e-06, 5.7045288498041325e-06, 5.363876819037824e-06, 5.038788614868075e-06, 4.749387730643056e-06, 4.498439595650381e-06, 4.267902197131722e-06, 4.0902314187371995e-06, 3.870510846225781e-06, 3.6538278603921114e-06, 3.4317374995151636e-06, 3.1861891141007416e-06, 2.9722602814061025e-06, 2.777074831898916e-06, 2.586128784900986e-06, 2.413560121718881e-06, 2.249553339626829e-06, 2.0837669833809425e-06], "duration": 78047.246673, "accuracy_train": [0.41449411764705885, 0.4722823529411765, 0.4989647058823529, 0.5158823529411765, 0.5312235294117648, 0.5464, 0.5602588235294118, 0.5774588235294118, 0.5898823529411765, 0.6032235294117647, 0.6135058823529411, 0.6242117647058824, 0.632964705882353, 0.6424470588235294, 0.6508235294117647, 0.6594117647058824, 0.6665411764705882, 0.674235294117647, 0.6808941176470589, 0.6884, 0.6945411764705882, 0.7017411764705882, 0.7065882352941176, 0.7127294117647058, 0.7179764705882353, 0.7224, 0.7267529411764706, 0.7313176470588235, 0.7352941176470589, 0.7392941176470588, 0.7435294117647059, 0.7463764705882353, 0.7495764705882353, 0.7526588235294117, 0.7555764705882353, 0.7585882352941177, 0.7611058823529412, 0.7630117647058824, 0.765364705882353, 0.7678823529411765, 0.7699529411764706, 0.7725411764705883, 0.7748470588235294, 0.7767294117647059, 0.7781411764705882, 0.7799764705882353, 0.7816235294117647, 0.7832, 0.7854823529411765, 0.787035294117647, 0.7885882352941177, 0.7906588235294117, 0.7918117647058823, 0.7931529411764706, 0.794564705882353, 0.7957647058823529, 0.7968470588235295, 0.7983058823529412, 0.7996941176470588, 0.8012705882352941, 0.8021411764705882, 0.803364705882353, 0.804635294117647, 0.8059529411764705, 0.8068941176470589, 0.8080470588235295, 0.8089882352941177, 0.8099529411764705, 0.8112941176470588, 0.8122117647058823, 0.8130352941176471, 0.8140470588235295, 0.814635294117647, 0.815364705882353, 0.8163294117647059, 0.8172235294117647, 0.8181176470588235, 0.8188470588235294, 0.8194588235294118, 0.8198588235294118, 0.8204941176470588, 0.8211529411764706, 0.8215764705882352, 0.8219764705882353, 0.8221882352941177, 0.8227294117647059, 0.8230823529411765, 0.8238117647058824, 0.8240705882352941, 0.8244, 0.8248470588235294, 0.8253882352941176, 0.8258823529411765, 0.8264, 0.8264470588235294, 0.8267294117647059, 0.8271058823529411, 0.8275058823529412, 0.8276705882352942, 0.8279529411764706, 0.8281411764705883, 0.8282823529411765, 0.8283294117647059, 0.8285882352941176, 0.8287058823529412, 0.8289882352941177, 0.8290588235294117, 0.8293411764705882, 0.8294117647058824, 0.829835294117647, 0.83, 0.8302352941176471, 0.8304235294117647, 0.830564705882353, 0.830635294117647, 0.830635294117647, 0.8308235294117647, 0.8310588235294117, 0.8313411764705883, 0.8315529411764706, 0.8320235294117647, 0.8321882352941177, 0.8324, 0.832564705882353, 0.8325882352941176, 0.8327529411764706, 0.8328941176470588, 0.8330588235294117, 0.8330352941176471, 0.8331529411764705, 0.8332235294117647, 0.8335294117647059, 0.8336941176470588, 0.833764705882353, 0.8338588235294118, 0.8340235294117647, 0.8342117647058823, 0.8343529411764706, 0.8346117647058824, 0.834564705882353, 0.834635294117647, 0.8346588235294118, 0.834564705882353, 0.8347058823529412, 0.8348, 0.8348235294117647, 0.8349176470588235, 0.8349647058823529, 0.8349411764705882], "end": "2016-02-08 03:28:25.855000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0], "moving_var_accuracy_valid": [0.015227559999999996, 0.029545543599999995, 0.041512588956, 0.05059981149435999, 0.056876274708031596, 0.061002613791025596, 0.06349173451307473, 0.06495199955610334, 0.06544808800657147, 0.06499547699539643, 0.0637793698269468, 0.06203339922433019, 0.05976043343425438, 0.057306032348239805, 0.05464116455027324, 0.051830552308402555, 0.04892097081893997, 0.04599959993927106, 0.0430866785790671, 0.04023772583579013, 0.03752335064226171, 0.0349004485005208, 0.032386736983922006, 0.030003140293197124, 0.02772522787349451, 0.025593289890066734, 0.023616642525952112, 0.02175875267323096, 0.02001886965096063, 0.018394600256629586, 0.016886713233711974, 0.01546924455960708, 0.014152795263574477, 0.0129409565377924, 0.011816822708789289, 0.01077187245624321, 0.009807978286755196, 0.008918947849750086, 0.00810772178735479, 0.00736164620037621, 0.006685684859993891, 0.00606700473763822, 0.005500713838425809, 0.004983833503354654, 0.004513170910616654, 0.004082816898642237, 0.003691515302838685, 0.003337330007742986, 0.003017152361345008, 0.002726953535649933, 0.002463832412568447, 0.002224130376879167, 0.00200830335051498, 0.0018129940598578028, 0.0016359991221315323, 0.0014763483689982856, 0.0013323559967639148, 0.0012024974346962042, 0.0010866321277474933, 0.0009823141142147216, 0.0008887313093109031, 0.0008036235496591122, 0.0007264524582946675, 0.0006567924007185202, 0.000593288764290178, 0.0005356175273336475, 0.0004833984625729975, 0.00043660334146255667, 0.00039447709454529516, 0.0003568646634943201, 0.00032247609232734643, 0.0002913634043383924, 0.0002631463501120154, 0.0002373836373941042, 0.00021462264959566877, 0.00019443674439459323, 0.00017647363529423818, 0.00015977710416473662, 0.00014450096033477436, 0.00013045261256132196, 0.00011788407600335434, 0.00010670626573758546, 9.653022300482585e-05, 8.757098957371875e-05, 7.926231625560114e-05, 7.174912390305991e-05, 6.511946554270187e-05, 5.928743774961187e-05, 5.417250486843679e-05, 4.986175474682048e-05, 4.610039269272901e-05, 4.273590227834503e-05, 3.9316896632444264e-05, 3.645021795925046e-05, 3.374375861142958e-05, 3.0996511950157315e-05, 2.8463453279429004e-05, 2.6131843981311864e-05, 2.4045308720614842e-05, 2.2067363649875007e-05, 2.0206161783958022e-05, 1.8352859035402802e-05, 1.6600595301389462e-05, 1.4944054884118503e-05, 1.3450357493128135e-05, 1.2116127157125294e-05, 1.0913266826193846e-05, 9.863637278341407e-06, 8.897945918740214e-06, 8.024895945134878e-06, 7.246886363270931e-06, 6.5323613525232494e-06, 5.879981755390114e-06, 5.309848574047805e-06, 4.778863803076247e-06, 4.3264928221402e-06, 3.9145110134171565e-06, 3.5310497114076e-06, 3.1844164777258898e-06, 2.8712169372951313e-06, 2.588341350512502e-06, 2.3487299129636124e-06, 2.194957338219853e-06, 2.0885614560145573e-06, 1.9292885274344516e-06, 1.7941545287329818e-06, 1.6333345007752654e-06, 1.4964816279656431e-06, 1.381599187533749e-06, 1.2715995038957556e-06, 1.1494846872098317e-06, 1.0347945857229858e-06, 9.315244046103384e-07, 8.560662267466421e-07, 8.1034680548884e-07, 7.488410761667492e-07, 7.162988465013253e-07, 6.657503406358172e-07, 6.162512233877835e-07, 5.606490043613193e-07, 5.070994644758197e-07, 4.584269600742484e-07, 4.1258465372982693e-07, 3.7288155942903837e-07, 3.375741542533205e-07, 3.0381674185999045e-07, 2.7343507012991305e-07, 2.4768799698378763e-07, 2.286890974073799e-07], "accuracy_test": 0.6857, "start": "2016-02-07 05:47:38.608000", "learning_rate_per_epoch": [0.0001558645599288866, 0.00014980930427554995, 0.00014398929488379508, 0.00013839539315085858, 0.0001330188097199425, 0.00012785110448021442, 0.00012288415746297687, 0.00011811017611762509, 0.00011352166620781645, 0.00010911141725955531, 0.00010487250256119296, 0.00010079826461151242, 9.688230784377083e-05, 9.31184840737842e-05, 8.950088522396982e-05, 8.6023828771431e-05, 8.26818504719995e-05, 7.94697116361931e-05, 7.638235547346994e-05, 7.34149434720166e-05, 7.056281174300238e-05, 6.782148557249457e-05, 6.518665759358555e-05, 6.265419506235048e-05, 6.0220114391995594e-05, 5.788059934275225e-05, 5.5631971918046474e-05, 5.3470703278435394e-05, 5.1393395551713184e-05, 4.9396792746847495e-05, 4.747775528812781e-05, 4.5633270929101855e-05, 4.3860443838639185e-05, 4.2156490962952375e-05, 4.051873474963941e-05, 3.894460678566247e-05, 3.743162960745394e-05, 3.597743125283159e-05, 3.457972707110457e-05, 3.32363233610522e-05, 3.194511009496637e-05, 3.070406091865152e-05, 2.9511225875467062e-05, 2.8364731406327337e-05, 2.726277853071224e-05, 2.6203635570709594e-05, 2.518563997000456e-05, 2.4207192836911418e-05, 2.3266757125384174e-05, 2.2362857635016553e-05, 2.1494073735084385e-05, 2.065904118353501e-05, 1.985644848900847e-05, 1.9085036910837516e-05, 1.8343595002079383e-05, 1.7630956790526398e-05, 1.694600359769538e-05, 1.6287660400848836e-05, 1.565489401400555e-05, 1.5046710359456483e-05, 1.4462154467764776e-05, 1.3900307749281637e-05, 1.3360288903641049e-05, 1.2841249372286256e-05, 1.2342374247964472e-05, 1.1862880455737468e-05, 1.1402014024497475e-05, 1.0959051905956585e-05, 1.0533299246162642e-05, 1.0124086657015141e-05, 9.730772035254631e-06, 9.352736924483906e-06, 8.989388334157411e-06, 8.640156011097133e-06, 8.304490620503202e-06, 7.981865564943291e-06, 7.671774255868513e-06, 7.373730113613419e-06, 7.0872647484065965e-06, 6.811928415118018e-06, 6.547289103764342e-06, 6.292930720519507e-06, 6.048453997209435e-06, 5.813475127069978e-06, 5.587624855252216e-06, 5.3705489335698076e-06, 5.161906301509589e-06, 4.961369086231571e-06, 4.768622602568939e-06, 4.583364443533355e-06, 4.4053035708202515e-06, 4.234159860061482e-06, 4.069665010320023e-06, 3.911560725100571e-06, 3.759598939723219e-06, 3.6135406844550744e-06, 3.4731567666312912e-06, 3.3382266337866895e-06, 3.208538373655756e-06, 3.083888486798969e-06, 2.964081204481772e-06, 2.848928261300898e-06, 2.738249122558045e-06, 2.6318696200178238e-06, 2.529623088776134e-06, 2.4313487756444374e-06, 2.336892293897108e-06, 2.2461053958977573e-06, 2.1588455183518818e-06, 2.0749755549331894e-06, 1.9943638562835986e-06, 1.916884002639563e-06, 1.8424141217110446e-06, 1.770837343428866e-06, 1.7020413451973582e-06, 1.6359180108338478e-06, 1.572363544255495e-06, 1.5112781284187804e-06, 1.4525658116326667e-06, 1.3961345075585996e-06, 1.341895540463156e-06, 1.289763645218045e-06, 1.2396570809869445e-06, 1.1914970627913135e-06, 1.1452081025709049e-06, 1.1007174407495768e-06, 1.0579551599221304e-06, 1.01685418485431e-06, 9.773499414222897e-07, 9.393804702995112e-07, 9.028860858961707e-07, 8.678094900460565e-07, 8.340956014762924e-07, 8.016914421205001e-07, 7.705461939622182e-07, 7.406109148178075e-07, 7.1183859517987e-07, 6.841841013738303e-07, 6.576039481842599e-07, 6.32056412541715e-07, 6.075013629924797e-07, 5.839002596985665e-07, 5.61216097594297e-07, 5.394131790126266e-07, 5.184572842154012e-07, 4.983155008631002e-07, 4.789562240148371e-07, 4.603490424415213e-07, 4.4246473862585844e-07], "accuracy_train_first": 0.41449411764705885, "accuracy_train_last": 0.8349411764705882, "batch_size_eval": 1024, "accuracy_train_std": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "accuracy_test_std": 0, "error_valid": [0.5886666666666667, 0.5393333333333333, 0.5097333333333334, 0.4926666666666667, 0.4829333333333333, 0.4721333333333333, 0.46040000000000003, 0.44386666666666663, 0.4302666666666667, 0.4209333333333334, 0.41279999999999994, 0.404, 0.3992, 0.3894666666666666, 0.38293333333333335, 0.3773333333333333, 0.37293333333333334, 0.368, 0.3642666666666666, 0.3601333333333333, 0.3541333333333333, 0.3506666666666667, 0.3473333333333334, 0.3436, 0.34173333333333333, 0.33799999999999997, 0.3334666666666667, 0.3310666666666666, 0.3288, 0.32666666666666666, 0.3242666666666667, 0.32399999999999995, 0.3228, 0.3208, 0.3201333333333334, 0.3202666666666667, 0.31986666666666663, 0.31986666666666663, 0.31866666666666665, 0.3188, 0.3170666666666667, 0.31679999999999997, 0.31679999999999997, 0.31666666666666665, 0.3164, 0.3169333333333333, 0.3169333333333333, 0.3164, 0.3157333333333333, 0.3154666666666667, 0.31533333333333335, 0.31599999999999995, 0.31520000000000004, 0.3150666666666667, 0.31520000000000004, 0.31479999999999997, 0.3144, 0.31399999999999995, 0.31253333333333333, 0.3118666666666666, 0.3109333333333333, 0.3109333333333333, 0.31079999999999997, 0.3104, 0.31066666666666665, 0.31079999999999997, 0.31079999999999997, 0.31013333333333337, 0.3097333333333333, 0.3089333333333333, 0.30920000000000003, 0.3090666666666667, 0.3090666666666667, 0.30946666666666667, 0.3084, 0.3076, 0.3069333333333333, 0.30733333333333335, 0.30746666666666667, 0.3078666666666666, 0.30746666666666667, 0.3069333333333333, 0.3069333333333333, 0.3062666666666667, 0.3065333333333333, 0.3064, 0.3058666666666666, 0.30533333333333335, 0.30479999999999996, 0.30400000000000005, 0.30346666666666666, 0.3030666666666667, 0.30333333333333334, 0.30266666666666664, 0.3025333333333333, 0.30279999999999996, 0.30266666666666664, 0.3025333333333333, 0.3022666666666667, 0.3022666666666667, 0.3022666666666667, 0.30266666666666664, 0.3029333333333334, 0.3036, 0.3038666666666666, 0.30413333333333337, 0.30413333333333337, 0.3045333333333333, 0.3044, 0.3044, 0.3045333333333333, 0.3044, 0.30400000000000005, 0.3045333333333333, 0.30413333333333337, 0.3036, 0.3036, 0.3037333333333333, 0.3037333333333333, 0.3037333333333333, 0.3037333333333333, 0.30346666666666666, 0.3029333333333334, 0.30266666666666664, 0.3029333333333334, 0.30279999999999996, 0.3030666666666667, 0.3029333333333334, 0.30279999999999996, 0.30279999999999996, 0.3030666666666667, 0.30333333333333334, 0.30333333333333334, 0.3037333333333333, 0.30400000000000005, 0.3038666666666666, 0.30413333333333337, 0.30400000000000005, 0.30400000000000005, 0.3038666666666666, 0.30346666666666666, 0.30346666666666666, 0.3036, 0.3037333333333333, 0.30346666666666666, 0.3036, 0.3036, 0.3037333333333333, 0.3038666666666666], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.03884942510625085, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 32, "valid_ratio": 0.15, "learning_rate": 0.00016216455857310516, "optimization": "nesterov_momentum", "nb_data_augmentation": 0, "learning_rate_decay_method": "exp", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 9.455946565746785e-06, "rotation_range": [0, 0], "momentum": 0.8486012116317911}, "accuracy_valid_max": 0.6977333333333333, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        y_pred = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            y_pred.extend((nnet.predict(X[mini_batch]) == y[mini_batch]).tolist())\n        return np.mean(y_pred)\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            acc = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = acc\n            status[\"accuracy_train_std\"] = 0\n            acc = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = acc\n            status[\"accuracy_valid_std\"] = 0\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n\n    # rescaling to [-1, 1]\n    X_min = X_train.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X_train.max(axis=(0, 2, 3))[None, :, None, None]\n    def preprocess(a):\n        return (a / 255.) * 2 - 1\n        # return 2 * ((a - X_min) / (X_max - X_min)) - 1\n    X_train = preprocess(X_train)\n    X_valid = preprocess(X_valid)\n\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = preprocess(X_test)\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    acc = evaluate(X_test, y_test, batch_size_eval)\n    light.set(\"accuracy_test\", acc)\n    light.set(\"accuracy_test_std\", 0)\n    print(\"Test accuracy : {}+-{}\".format(acc, 0))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.6961333333333334, "accuracy_valid_std": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "accuracy_valid": [0.41133333333333333, 0.46066666666666667, 0.4902666666666667, 0.5073333333333333, 0.5170666666666667, 0.5278666666666667, 0.5396, 0.5561333333333334, 0.5697333333333333, 0.5790666666666666, 0.5872, 0.596, 0.6008, 0.6105333333333334, 0.6170666666666667, 0.6226666666666667, 0.6270666666666667, 0.632, 0.6357333333333334, 0.6398666666666667, 0.6458666666666667, 0.6493333333333333, 0.6526666666666666, 0.6564, 0.6582666666666667, 0.662, 0.6665333333333333, 0.6689333333333334, 0.6712, 0.6733333333333333, 0.6757333333333333, 0.676, 0.6772, 0.6792, 0.6798666666666666, 0.6797333333333333, 0.6801333333333334, 0.6801333333333334, 0.6813333333333333, 0.6812, 0.6829333333333333, 0.6832, 0.6832, 0.6833333333333333, 0.6836, 0.6830666666666667, 0.6830666666666667, 0.6836, 0.6842666666666667, 0.6845333333333333, 0.6846666666666666, 0.684, 0.6848, 0.6849333333333333, 0.6848, 0.6852, 0.6856, 0.686, 0.6874666666666667, 0.6881333333333334, 0.6890666666666667, 0.6890666666666667, 0.6892, 0.6896, 0.6893333333333334, 0.6892, 0.6892, 0.6898666666666666, 0.6902666666666667, 0.6910666666666667, 0.6908, 0.6909333333333333, 0.6909333333333333, 0.6905333333333333, 0.6916, 0.6924, 0.6930666666666667, 0.6926666666666667, 0.6925333333333333, 0.6921333333333334, 0.6925333333333333, 0.6930666666666667, 0.6930666666666667, 0.6937333333333333, 0.6934666666666667, 0.6936, 0.6941333333333334, 0.6946666666666667, 0.6952, 0.696, 0.6965333333333333, 0.6969333333333333, 0.6966666666666667, 0.6973333333333334, 0.6974666666666667, 0.6972, 0.6973333333333334, 0.6974666666666667, 0.6977333333333333, 0.6977333333333333, 0.6977333333333333, 0.6973333333333334, 0.6970666666666666, 0.6964, 0.6961333333333334, 0.6958666666666666, 0.6958666666666666, 0.6954666666666667, 0.6956, 0.6956, 0.6954666666666667, 0.6956, 0.696, 0.6954666666666667, 0.6958666666666666, 0.6964, 0.6964, 0.6962666666666667, 0.6962666666666667, 0.6962666666666667, 0.6962666666666667, 0.6965333333333333, 0.6970666666666666, 0.6973333333333334, 0.6970666666666666, 0.6972, 0.6969333333333333, 0.6970666666666666, 0.6972, 0.6972, 0.6969333333333333, 0.6966666666666667, 0.6966666666666667, 0.6962666666666667, 0.696, 0.6961333333333334, 0.6958666666666666, 0.696, 0.696, 0.6961333333333334, 0.6965333333333333, 0.6965333333333333, 0.6964, 0.6962666666666667, 0.6965333333333333, 0.6964, 0.6964, 0.6962666666666667, 0.6961333333333334], "seed": 446775380, "model": "residualv3", "loss_std": [0.2567315101623535, 0.15708209574222565, 0.16470849514007568, 0.1680130660533905, 0.17022483050823212, 0.17134301364421844, 0.1716068834066391, 0.17127394676208496, 0.17063938081264496, 0.1699678897857666, 0.1692456752061844, 0.1684855818748474, 0.1678004264831543, 0.1672622412443161, 0.16665560007095337, 0.16600270569324493, 0.1653335988521576, 0.16470752656459808, 0.1639336496591568, 0.16317591071128845, 0.1624719202518463, 0.16171695291996002, 0.16097861528396606, 0.16025760769844055, 0.1595316380262375, 0.15884904563426971, 0.15818800032138824, 0.1574213206768036, 0.1567070186138153, 0.15596027672290802, 0.1552264541387558, 0.1545349508523941, 0.1538449078798294, 0.1531600058078766, 0.15247392654418945, 0.15179456770420074, 0.1511683166027069, 0.15046337246894836, 0.1498057097196579, 0.1491343230009079, 0.14851219952106476, 0.14784619212150574, 0.14716802537441254, 0.1464964747428894, 0.14585988223552704, 0.14519625902175903, 0.14456209540367126, 0.14394789934158325, 0.14332102239131927, 0.14274023473262787, 0.1421377956867218, 0.14156939089298248, 0.1409749537706375, 0.1403908133506775, 0.13983187079429626, 0.13927796483039856, 0.13873714208602905, 0.13820867240428925, 0.13769717514514923, 0.13720421493053436, 0.13672038912773132, 0.13625378906726837, 0.13580730557441711, 0.13536003232002258, 0.13492971658706665, 0.1345154047012329, 0.1341070830821991, 0.13370439410209656, 0.13332167267799377, 0.13294672966003418, 0.13256749510765076, 0.13221846520900726, 0.131875142455101, 0.1315428465604782, 0.13122113049030304, 0.13091294467449188, 0.13061052560806274, 0.13032343983650208, 0.130035400390625, 0.12975730001926422, 0.1294921338558197, 0.12924012541770935, 0.12898816168308258, 0.12874874472618103, 0.12851758301258087, 0.1282898485660553, 0.128069207072258, 0.12785811722278595, 0.12765765190124512, 0.12747126817703247, 0.12729056179523468, 0.12710972130298615, 0.1269364356994629, 0.12676902115345, 0.12661072611808777, 0.126461461186409, 0.12631253898143768, 0.12616993486881256, 0.12603220343589783, 0.1259039044380188, 0.12578006088733673, 0.12565651535987854, 0.12554070353507996, 0.1254291981458664, 0.12532128393650055, 0.125213623046875, 0.1251128911972046, 0.12501704692840576, 0.12492213398218155, 0.12483204156160355, 0.12474493682384491, 0.12466186285018921, 0.12458097189664841, 0.12450137734413147, 0.1244274452328682, 0.1243538111448288, 0.12428532540798187, 0.12421911954879761, 0.12415662407875061, 0.1240958422422409, 0.12403713166713715, 0.12398003041744232, 0.12392483651638031, 0.12387113273143768, 0.1238197460770607, 0.12376925349235535, 0.12371956557035446, 0.12367163598537445, 0.12362485378980637, 0.12358058989048004, 0.12353724986314774, 0.12349463254213333, 0.12345383316278458, 0.12341473251581192, 0.12337547540664673, 0.12333884090185165, 0.1233026385307312, 0.12326740473508835, 0.12323213368654251, 0.12319815903902054, 0.12316589057445526, 0.12313371896743774, 0.12310265749692917, 0.12307284772396088, 0.12304431200027466, 0.12301616370677948, 0.12298820912837982, 0.12296196818351746, 0.12293682992458344]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:46 2016", "state": "available"}], "summary": "07e94e350d5f6d1fed01f41ff6ee6ac1"}