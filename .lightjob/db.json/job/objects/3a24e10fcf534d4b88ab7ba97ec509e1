{"content": {"hp_model": {"f0": 16, "f1": 32, "f2": 64, "f3": 32, "nonlin": "rectify", "nbg1": 2, "nbg3": 2, "nbg2": 2, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.12283501103898492, 0.12275810718665256, 0.1173433723936185, 0.11875288056742854, 0.11837991321396481, 0.11394338457823941, 0.1142887777543297, 0.11940178062038634, 0.11635309464985072, 0.11532029704674888, 0.11694936554694767, 0.11868595465829436, 0.11689101966003343, 0.1141329332524527, 0.11131637844129151, 0.1098951509090843, 0.1116544841179471, 0.11584084914273522, 0.11451326977531953, 0.12086675679041824, 0.1165665735394689, 0.11722377275456859, 0.11266390055726584, 0.11406282757356417, 0.11562448789899486, 0.10926357253645942, 0.11421220941017234, 0.1091712284472463, 0.11105170533477506, 0.10663341865969111, 0.11408072932829745, 0.11602276345772919, 0.10864022468697385, 0.11455531305234476, 0.10799255539603522, 0.11511248613236866, 0.11152790455717258, 0.11255374839213625, 0.11000148008417084, 0.11497831281911546, 0.11169983881827346, 0.10928617718630922, 0.10943971609508374, 0.11138172789526199, 0.1076036737744141, 0.11221596819818244, 0.11193756983800161, 0.1186976750509269, 0.11580882113145498, 0.11028880876272455, 0.11427465432521133, 0.1140984719637775, 0.11535764006357344, 0.11115500281074048, 0.11933783399881968, 0.1077143347341274, 0.10722444323660411, 0.11305959129885751, 0.1138017211189172, 0.11197795223439237, 0.11307094850741954, 0.11434767158632884, 0.11530939338864829, 0.11396967747220237, 0.11161877828790297, 0.10759770677806808, 0.1091320137036092, 0.11493859664769941, 0.11071836341496367, 0.11151447102473817, 0.1118132248340517, 0.11286958320406698, 0.11913074352433504, 0.11422696510112233, 0.11136443324230559, 0.1126826576594866, 0.1092097758807745, 0.1144973048587576, 0.10864088134060813, 0.11375383362636302, 0.11389759228843177, 0.11255945265934678, 0.11435827705282119, 0.11072673939410987, 0.11632948686864233, 0.11243761954970742, 0.11130388079460705, 0.10983922802771569, 0.11294066643951801, 0.11020946146350283, 0.11658003685085615, 0.10595286711140495, 0.11305832931637025, 0.11341176493591955, 0.11243960227647572, 0.10721887098624841, 0.1120885111358789, 0.11068356410135828, 0.11280889009106532, 0.1117232276183041, 0.10892393986136138, 0.11041293219756604, 0.11425405127620067, 0.1086664877372668, 0.11420510413827964, 0.10941820264860269, 0.10795514279080885, 0.10925312548656523, 0.10894784273927177, 0.1148190550473443, 0.11501886810848298, 0.11284714320029603, 0.115018868108483, 0.1108433734921074, 0.11012592736895215, 0.1097365610091314, 0.10573507971388016, 0.11232819813381448, 0.11453281403297295, 0.11167939954322084, 0.1150111923919186, 0.11039378944504417, 0.1097580933008412, 0.12077848513728702, 0.11267149874707758, 0.11296553500471905, 0.10961102082144568, 0.11334923800848527, 0.11430906249937688, 0.11568809755020433, 0.11129739107839527, 0.10979675956738562, 0.10939839675974307, 0.10648002056842586, 0.1131823306603178, 0.11031403263860079, 0.11098262623833838, 0.11066688555234883, 0.11175227719345328, 0.10801633415540166, 0.10654306353433694, 0.10817101599281861, 0.11059982369493837, 0.11185173873425822, 0.10720514704523111, 0.10790433008381284, 0.11051973145770866, 0.11050302818095631, 0.1119913301715561, 0.1129162662397384, 0.10852730331688364, 0.11336151016305743, 0.11241104763694258, 0.11234820188356122, 0.1143876708889828, 0.11280375180529718, 0.10448122568813246, 0.11517297185148044, 0.10899268741459046, 0.10841977514396309, 0.10629167407095531], "moving_avg_accuracy_train": [0.04620905496987951, 0.09678393260542166, 0.148744281814759, 0.1996784003200301, 0.24847703242658128, 0.294937216985128, 0.33892099905167544, 0.3804501152609657, 0.4197206157830619, 0.45678893447584007, 0.49129405834753315, 0.5234452436272377, 0.5532943374874054, 0.5807679910880624, 0.6065532025214249, 0.6303034733837402, 0.652057576346571, 0.671676272778179, 0.6896837208015659, 0.7055256838117707, 0.7208776711534852, 0.7346732812971729, 0.748242380125287, 0.7610052091308306, 0.7728070790310005, 0.7834381745917559, 0.7947310288193272, 0.8052499251843825, 0.8148345900454623, 0.8235902123662173, 0.8298371775753787, 0.8377984899383227, 0.8444953916975024, 0.8514873999675112, 0.858467330904495, 0.8638574201032022, 0.8700309777916772, 0.8762460652534733, 0.8820796665594512, 0.8869063384577229, 0.891087974943276, 0.8951291209730448, 0.9000439197191137, 0.904217803349612, 0.9060752963580243, 0.9096719271137881, 0.9121652953963852, 0.9150540934169876, 0.9175551788042046, 0.9194931850201696, 0.9220092279639358, 0.9232594535109157, 0.9249729471658482, 0.9259668045576971, 0.9280260917224092, 0.9302041866164333, 0.9314044004849105, 0.9328069762496725, 0.9343987372090425, 0.9365913936086202, 0.9390001194586015, 0.9388830517898498, 0.9384717797433949, 0.9398006183353205, 0.94129542472468, 0.9430713792401638, 0.9441285108944607, 0.9435386078471832, 0.9448290429961999, 0.9471528969797125, 0.9485054724925845, 0.9462754071710369, 0.9468874184117645, 0.9482124190404675, 0.9491743096665413, 0.949955297374586, 0.9503052119142358, 0.9518061289758243, 0.9521874513192057, 0.9530106866089719, 0.9549799492733759, 0.9565875642857974, 0.9572084577066152, 0.9584473257913754, 0.9599082219772981, 0.9606818011349899, 0.9610391669552258, 0.9620785108018719, 0.9634469021915643, 0.964130167544697, 0.9636673578685406, 0.964723908979277, 0.9658183479006264, 0.9663656546768288, 0.965561638154929, 0.9655345594297975, 0.9652325153844081, 0.9660643090266903, 0.9668152764673947, 0.9673758421941493, 0.9670496849325657, 0.9673538467103935, 0.969364226346583, 0.9688062863926477, 0.9682994341088046, 0.9691892760894903, 0.969792468209457, 0.96987882756321, 0.9709707640839974, 0.9719252690008989, 0.9725913640887609, 0.9727672803907281, 0.973278579460089, 0.9739646522369717, 0.974855084603636, 0.9747457997878507, 0.9752027898391861, 0.9747951802829783, 0.9740118218932347, 0.9740315734388509, 0.9747976555527972, 0.9737528485818548, 0.9741373528802959, 0.9747422546404592, 0.9747172008631602, 0.9740922428250369, 0.9746216480606056, 0.9748345585557499, 0.9747061478808978, 0.9745741061349767, 0.9741681827202743, 0.9744899751410179, 0.9754220017233016, 0.9764302533582003, 0.9743703417271996, 0.9750696027352025, 0.9752541899014413, 0.9737519260317791, 0.9743624261996855, 0.9752577912604399, 0.9762424601765646, 0.9762274009058961, 0.9772115885261499, 0.9777632082879927, 0.9773466389652175, 0.9778565157313462, 0.9785342489473683, 0.9789112457393785, 0.979017579749778, 0.9785038112326315, 0.977359002398525, 0.9766557640562629, 0.9773100295181065, 0.9771882133132839, 0.9761420199940036, 0.9769347268801455, 0.9776975794933358, 0.976040396845207, 0.9756055024920116, 0.9765153965199188, 0.9768354306631076], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 1234, "moving_var_accuracy_train": [0.01921749085088412, 0.04031610599644628, 0.06058339640640821, 0.07787361661694837, 0.09151801341448831, 0.10179315081596746, 0.10902499349826858, 0.11364450158656436, 0.11615960132921155, 0.11691018345267469, 0.1159345972680152, 0.11364442597522267, 0.11029869901615837, 0.1060620438940626, 0.10143973366262518, 0.09637243859066277, 0.09099436369305348, 0.08535896657082842, 0.07974148357258042, 0.07402604534347267, 0.06874459244718685, 0.06358300293559772, 0.05888178662910208, 0.05445961620421458, 0.05026721178205796, 0.04625767233924938, 0.042779662114771036, 0.03949752052994288, 0.036374560681441556, 0.033427052913310724, 0.03043556879089991, 0.027962454362673212, 0.025569845364954838, 0.0234528544452902, 0.021546043923726752, 0.01965291708548425, 0.01803064070773218, 0.01657522244637894, 0.01522397833951502, 0.013911251360085614, 0.012677500977352827, 0.011556728630722789, 0.010618452988079764, 0.009713399430320261, 0.00877311200977494, 0.008012222583937204, 0.0072669522940774335, 0.00661536345070422, 0.006010125958661147, 0.005442916175633106, 0.004955598806923676, 0.004474106501496199, 0.0040531203958960255, 0.0036566981289444175, 0.0033291942886907126, 0.003038971736128008, 0.002748039182485971, 0.0024909402332204526, 0.002264649536464381, 0.0020814542615974234, 0.0019255264774209967, 0.0017330971732304997, 0.0015613097581732064, 0.0014210710903864025, 0.0012990739966227925, 0.0011975527269301193, 0.0010878552002477554, 0.0009822015506696642, 0.0008989684014670554, 0.0008576742373505339, 0.0007883719582776721, 0.0007542934844952281, 0.0006822351558746981, 0.0006298122802818, 0.0005751581544423767, 0.0005231318151991907, 0.0004719205953447969, 0.0004450033040422235, 0.0004018116342040583, 0.0003677299178645001, 0.00036585888505079046, 0.0003525328307991767, 0.00032074912547939305, 0.00030248736011438974, 0.0002914465830973422, 0.0002676877472065456, 0.00024206836545114725, 0.00022758364959008236, 0.00022167773958953093, 0.00020371162951570218, 0.00018526820173122834, 0.00017678808380448854, 0.00016988944439711996, 0.00015559640232290183, 0.00014585474519800145, 0.00013127586999439402, 0.00011896935844315128, 0.00011329934856890402, 0.00010704498258499754, 9.916858973260399e-05, 9.020913779289704e-05, 8.202085349742949e-05, 0.00011019340468213767, 0.00010197573714370102, 9.409025656806337e-05, 9.180759966657455e-05, 8.590140630222555e-05, 7.737838711382887e-05, 8.037147669131041e-05, 8.0534045749681e-05, 7.647378516937685e-05, 6.910492556011977e-05, 6.454727364907197e-05, 6.232880898078013e-05, 6.323175627913214e-05, 5.701606918986995e-05, 5.319402143405888e-05, 4.936992924346002e-05, 4.9955789620149396e-05, 4.4963721770122554e-05, 4.574928584088523e-05, 5.0998951715564655e-05, 4.72296485436848e-05, 4.579983894435288e-05, 4.1225504275730086e-05, 4.0618106792890804e-05, 3.9078725244628615e-05, 3.5578830630648915e-05, 3.216935128032777e-05, 2.9109331356288052e-05, 2.7681362588092832e-05, 2.5845179587716155e-05, 3.107872357969631e-05, 3.711999345521073e-05, 7.159711745747942e-05, 6.883809932755094e-05, 6.226094119225645e-05, 7.634601767986281e-05, 7.206581000700062e-05, 7.20743363344771e-05, 7.359305857046887e-05, 6.623579374811959e-05, 6.832984182005452e-05, 6.423541689294911e-05, 5.937364520975048e-05, 5.5776049538517666e-05, 5.4332345393561605e-05, 5.017825008487895e-05, 4.5262187372299826e-05, 4.311159143796783e-05, 5.059571769400615e-05, 4.9987043418853895e-05, 4.8840908728021176e-05, 4.4090370545035715e-05, 4.953201764229076e-05, 5.023427374409134e-05, 5.044834335474316e-05, 7.01197979826004e-05, 6.481001607031158e-05, 6.578017874147244e-05, 6.012395754258475e-05], "duration": 11470.295498, "accuracy_train": [0.4620905496987952, 0.5519578313253012, 0.6163874246987951, 0.6580854668674698, 0.6876647213855421, 0.7130788780120482, 0.7347750376506024, 0.7542121611445783, 0.7731551204819277, 0.7904038027108434, 0.8018401731927711, 0.8128059111445783, 0.8219361822289156, 0.8280308734939759, 0.8386201054216867, 0.8440559111445783, 0.8478445030120482, 0.8482445406626506, 0.8517507530120482, 0.8481033509036144, 0.8590455572289156, 0.8588337725903614, 0.8703642695783133, 0.8758706701807228, 0.8790239081325302, 0.8791180346385542, 0.8963667168674698, 0.8999199924698795, 0.9010965737951807, 0.9023908132530121, 0.8860598644578314, 0.9094503012048193, 0.9047675075301205, 0.9144154743975904, 0.9212867093373494, 0.9123682228915663, 0.9255929969879518, 0.9321818524096386, 0.934582078313253, 0.9303463855421686, 0.928722703313253, 0.9314994352409639, 0.9442771084337349, 0.9417827560240963, 0.9227927334337349, 0.9420416039156626, 0.934605609939759, 0.9410532756024096, 0.9400649472891566, 0.9369352409638554, 0.9446536144578314, 0.9345114834337349, 0.940394390060241, 0.9349115210843374, 0.9465596762048193, 0.9498070406626506, 0.9422063253012049, 0.9454301581325302, 0.9487245858433735, 0.9563253012048193, 0.9606786521084337, 0.9378294427710844, 0.9347703313253012, 0.9517601656626506, 0.9547486822289156, 0.9590549698795181, 0.9536426957831325, 0.9382294804216867, 0.9564429593373494, 0.9680675828313253, 0.9606786521084337, 0.9262048192771084, 0.9523955195783133, 0.9601374246987951, 0.9578313253012049, 0.9569841867469879, 0.9534544427710844, 0.9653143825301205, 0.9556193524096386, 0.9604198042168675, 0.9727033132530121, 0.9710560993975904, 0.9627964984939759, 0.9695971385542169, 0.9730562876506024, 0.9676440135542169, 0.9642554593373494, 0.9714326054216867, 0.9757624246987951, 0.9702795557228916, 0.9595020707831325, 0.9742328689759037, 0.9756682981927711, 0.9712914156626506, 0.9583254894578314, 0.9652908509036144, 0.9625141189759037, 0.9735504518072289, 0.9735739834337349, 0.9724209337349398, 0.9641142695783133, 0.9700913027108434, 0.9874576430722891, 0.9637848268072289, 0.9637377635542169, 0.9771978539156626, 0.9752211972891566, 0.9706560617469879, 0.9807981927710844, 0.9805158132530121, 0.9785862198795181, 0.9743505271084337, 0.9778802710843374, 0.9801393072289156, 0.9828689759036144, 0.9737622364457831, 0.9793157003012049, 0.9711266942771084, 0.9669615963855421, 0.9742093373493976, 0.9816923945783133, 0.9643495858433735, 0.9775978915662651, 0.9801863704819277, 0.9744917168674698, 0.9684676204819277, 0.9793862951807228, 0.9767507530120482, 0.9735504518072289, 0.9733857304216867, 0.9705148719879518, 0.9773861069277109, 0.9838102409638554, 0.9855045180722891, 0.9558311370481928, 0.9813629518072289, 0.9769154743975904, 0.9602315512048193, 0.9798569277108434, 0.9833160768072289, 0.9851044804216867, 0.9760918674698795, 0.9860692771084337, 0.9827277861445783, 0.973597515060241, 0.982445406626506, 0.9846338478915663, 0.9823042168674698, 0.9799745858433735, 0.9738798945783133, 0.9670557228915663, 0.9703266189759037, 0.9831984186746988, 0.9760918674698795, 0.9667262801204819, 0.9840690888554217, 0.9845632530120482, 0.9611257530120482, 0.971691453313253, 0.9847044427710844, 0.9797157379518072], "end": "2016-01-17 11:02:50.840000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0], "accuracy_valid": [0.45259081196581197, 0.5507478632478633, 0.6025641025641025, 0.6328792735042735, 0.656784188034188, 0.6813568376068376, 0.6971153846153846, 0.7123397435897436, 0.7216880341880342, 0.7286324786324786, 0.7366452991452992, 0.7398504273504274, 0.7429220085470085, 0.7433226495726496, 0.7446581196581197, 0.7449252136752137, 0.7402510683760684, 0.734909188034188, 0.7355769230769231, 0.7272970085470085, 0.7299679487179487, 0.7290331196581197, 0.734642094017094, 0.734642094017094, 0.734909188034188, 0.7325053418803419, 0.7383814102564102, 0.7377136752136753, 0.7406517094017094, 0.7401175213675214, 0.7243589743589743, 0.7323717948717948, 0.7319711538461539, 0.7347756410256411, 0.7414529914529915, 0.7301014957264957, 0.7405181623931624, 0.7427884615384616, 0.7373130341880342, 0.7441239316239316, 0.7407852564102564, 0.7403846153846154, 0.7493322649572649, 0.7462606837606838, 0.7322382478632479, 0.7455929487179487, 0.7350427350427351, 0.7358440170940171, 0.7383814102564102, 0.7286324786324786, 0.7426549145299145, 0.733840811965812, 0.7397168803418803, 0.7389155982905983, 0.7342414529914529, 0.7367788461538461, 0.7359775641025641, 0.7359775641025641, 0.7402510683760684, 0.7437232905982906, 0.7447916666666666, 0.7260950854700855, 0.7258279914529915, 0.7385149572649573, 0.7433226495726496, 0.7442574786324786, 0.7361111111111112, 0.7323717948717948, 0.7439903846153846, 0.7565438034188035, 0.7489316239316239, 0.7262286324786325, 0.7362446581196581, 0.7451923076923077, 0.7510683760683761, 0.7414529914529915, 0.7403846153846154, 0.7482638888888888, 0.7381143162393162, 0.7419871794871795, 0.7525373931623932, 0.750801282051282, 0.7383814102564102, 0.7509348290598291, 0.7524038461538461, 0.7482638888888888, 0.7342414529914529, 0.7536057692307693, 0.7569444444444444, 0.7461271367521367, 0.7401175213675214, 0.7470619658119658, 0.7589476495726496, 0.7498664529914529, 0.7379807692307693, 0.7425213675213675, 0.7345085470085471, 0.7552083333333334, 0.7433226495726496, 0.7478632478632479, 0.7382478632478633, 0.7446581196581197, 0.7586805555555556, 0.734107905982906, 0.7366452991452992, 0.7477297008547008, 0.7437232905982906, 0.7471955128205128, 0.7540064102564102, 0.7561431623931624, 0.7608173076923077, 0.7516025641025641, 0.7504006410256411, 0.750267094017094, 0.7501335470085471, 0.7485309829059829, 0.7568108974358975, 0.7483974358974359, 0.7402510683760684, 0.7437232905982906, 0.7532051282051282, 0.7381143162393162, 0.7475961538461539, 0.7483974358974359, 0.7458600427350427, 0.7370459401709402, 0.7536057692307693, 0.7425213675213675, 0.7438568376068376, 0.7438568376068376, 0.7381143162393162, 0.7462606837606838, 0.7558760683760684, 0.7477297008547008, 0.7323717948717948, 0.750534188034188, 0.7483974358974359, 0.7267628205128205, 0.7443910256410257, 0.7510683760683761, 0.749732905982906, 0.7434561965811965, 0.7545405982905983, 0.7509348290598291, 0.7458600427350427, 0.7470619658119658, 0.7554754273504274, 0.7470619658119658, 0.7517361111111112, 0.7421207264957265, 0.7374465811965812, 0.7390491452991453, 0.750801282051282, 0.7443910256410257, 0.7321047008547008, 0.7479967948717948, 0.7558760683760684, 0.7339743589743589, 0.7414529914529915, 0.7537393162393162, 0.7518696581196581], "accuracy_test": 0.7516, "start": "2016-01-17 07:51:40.545000", "learning_rate_per_epoch": [0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756, 0.00022058225295040756], "accuracy_train_last": 0.9797157379518072, "error_valid": [0.547409188034188, 0.4492521367521367, 0.39743589743589747, 0.36712072649572647, 0.34321581196581197, 0.31864316239316237, 0.3028846153846154, 0.2876602564102564, 0.2783119658119658, 0.2713675213675214, 0.2633547008547008, 0.2601495726495726, 0.2570779914529915, 0.2566773504273504, 0.2553418803418803, 0.2550747863247863, 0.25974893162393164, 0.26509081196581197, 0.26442307692307687, 0.2727029914529915, 0.2700320512820513, 0.2709668803418803, 0.265357905982906, 0.265357905982906, 0.26509081196581197, 0.2674946581196581, 0.26161858974358976, 0.26228632478632474, 0.25934829059829057, 0.2598824786324786, 0.27564102564102566, 0.2676282051282052, 0.26802884615384615, 0.2652243589743589, 0.2585470085470085, 0.26989850427350426, 0.25948183760683763, 0.25721153846153844, 0.2626869658119658, 0.25587606837606836, 0.2592147435897436, 0.2596153846153846, 0.2506677350427351, 0.2537393162393162, 0.26776175213675213, 0.2544070512820513, 0.2649572649572649, 0.26415598290598286, 0.26161858974358976, 0.2713675213675214, 0.2573450854700855, 0.26615918803418803, 0.2602831196581197, 0.2610844017094017, 0.26575854700854706, 0.26322115384615385, 0.2640224358974359, 0.2640224358974359, 0.25974893162393164, 0.25627670940170943, 0.25520833333333337, 0.2739049145299145, 0.2741720085470085, 0.2614850427350427, 0.2566773504273504, 0.2557425213675214, 0.26388888888888884, 0.2676282051282052, 0.2560096153846154, 0.24345619658119655, 0.25106837606837606, 0.27377136752136755, 0.2637553418803419, 0.2548076923076923, 0.24893162393162394, 0.2585470085470085, 0.2596153846153846, 0.25173611111111116, 0.2618856837606838, 0.2580128205128205, 0.2474626068376068, 0.24919871794871795, 0.26161858974358976, 0.2490651709401709, 0.24759615384615385, 0.25173611111111116, 0.26575854700854706, 0.24639423076923073, 0.24305555555555558, 0.2538728632478633, 0.2598824786324786, 0.2529380341880342, 0.2410523504273504, 0.25013354700854706, 0.2620192307692307, 0.25747863247863245, 0.26549145299145294, 0.24479166666666663, 0.2566773504273504, 0.25213675213675213, 0.2617521367521367, 0.2553418803418803, 0.24131944444444442, 0.265892094017094, 0.2633547008547008, 0.2522702991452992, 0.25627670940170943, 0.2528044871794872, 0.24599358974358976, 0.24385683760683763, 0.2391826923076923, 0.2483974358974359, 0.24959935897435892, 0.24973290598290598, 0.24986645299145294, 0.25146901709401714, 0.24318910256410253, 0.2516025641025641, 0.25974893162393164, 0.25627670940170943, 0.2467948717948718, 0.2618856837606838, 0.25240384615384615, 0.2516025641025641, 0.2541399572649573, 0.26295405982905984, 0.24639423076923073, 0.25747863247863245, 0.25614316239316237, 0.25614316239316237, 0.2618856837606838, 0.2537393162393162, 0.24412393162393164, 0.2522702991452992, 0.2676282051282052, 0.24946581196581197, 0.2516025641025641, 0.2732371794871795, 0.25560897435897434, 0.24893162393162394, 0.250267094017094, 0.25654380341880345, 0.24545940170940173, 0.2490651709401709, 0.2541399572649573, 0.2529380341880342, 0.2445245726495726, 0.2529380341880342, 0.24826388888888884, 0.25787927350427353, 0.26255341880341876, 0.26095085470085466, 0.24919871794871795, 0.25560897435897434, 0.2678952991452992, 0.2520032051282052, 0.24412393162393164, 0.2660256410256411, 0.2585470085470085, 0.24626068376068377, 0.2481303418803419], "accuracy_train_std": [0.12526941744731393, 0.125940280175161, 0.12208072664333798, 0.11984443684754886, 0.11683230275491341, 0.11479505740761248, 0.11269560778120988, 0.11070443373253076, 0.10645470639874649, 0.10385026667467366, 0.10153525450774194, 0.10005569833861662, 0.09866074094872353, 0.0974573952488719, 0.09498207500987571, 0.09425017466506744, 0.09249124282819214, 0.09236126832097039, 0.08992638701885168, 0.09139159953424643, 0.09045188698017213, 0.08874786579098287, 0.08711785380448497, 0.08628749037917606, 0.08570177991008483, 0.08406861540245295, 0.07914975825034787, 0.07784924647262403, 0.07604480581884722, 0.07505129605208465, 0.08256250822339552, 0.07516058971892935, 0.07585661704896215, 0.07040828990181797, 0.07009005857150556, 0.0722015340059042, 0.06782216466786452, 0.06623321469927233, 0.06410554088052162, 0.06661443954738944, 0.06540474518828053, 0.06470277779004015, 0.058043932481906715, 0.060800604729530604, 0.07046322673170738, 0.05967070673922657, 0.06395731153040479, 0.06039878211385304, 0.061198404770533565, 0.0635012877011244, 0.05897940128631623, 0.0635839658069484, 0.06120774183505423, 0.06232850479786039, 0.059178400867737437, 0.055309429951352115, 0.059842550170377755, 0.058592063069960856, 0.05559215655726002, 0.05287631353490716, 0.050056803124015434, 0.062074111413873874, 0.06182492315225139, 0.05316337637814554, 0.05287174217151759, 0.050704665394519434, 0.052873276476147334, 0.06119974389441582, 0.05149506537191681, 0.04448744077407887, 0.049406189552574024, 0.06626078162528605, 0.05528447534446327, 0.05089970774462566, 0.051863961061400975, 0.051933867875127014, 0.053013678501626665, 0.046937307948796726, 0.05117641339655255, 0.049184116757615255, 0.040827473709553495, 0.04264250180026145, 0.04762051830980924, 0.0437175823883757, 0.04189453704914085, 0.044346466156256564, 0.04712946561261411, 0.042273989038602376, 0.0392327756983922, 0.042608433518271135, 0.050381069466829465, 0.04046902737165216, 0.038797222832484934, 0.04231748079490584, 0.0511956858988627, 0.045953928882062414, 0.04874471210330447, 0.0408086983896044, 0.040443897592909485, 0.04188481451288452, 0.04788980852945195, 0.04268374352437649, 0.028390407584046155, 0.047455578382563396, 0.04635311730947056, 0.03883547596510813, 0.03943770727034214, 0.042524136317866876, 0.034157418354753256, 0.034851602008969855, 0.03710683075151437, 0.04113788688877886, 0.037089068380049765, 0.03535286812637741, 0.03223362164437425, 0.04045736778140065, 0.035690201564116855, 0.04253502108369371, 0.04585719338120194, 0.04043585303918636, 0.03365437248021539, 0.046730984227562194, 0.03747275389814757, 0.03521259255118822, 0.040650774112036145, 0.044093320376991506, 0.03558666644607491, 0.039080867361281545, 0.04098849998255567, 0.040863745038578266, 0.04331828590831842, 0.03752212849789659, 0.03280629451617015, 0.02988186734716035, 0.05281347895993692, 0.03416882923053743, 0.03747067766252436, 0.0497762599309575, 0.03605966667123474, 0.032125755367803134, 0.030297469455016022, 0.03841422529410545, 0.029805868197311413, 0.03283703365310254, 0.04094705865872393, 0.032911148090881845, 0.03078662579700502, 0.03319184091009659, 0.03552991185607081, 0.03911154540958764, 0.04553894447872572, 0.043018947685342306, 0.03167210070173521, 0.03917246189877975, 0.04581532142007897, 0.03130727926558277, 0.030390473855550534, 0.05009750093903474, 0.04112794509170369, 0.03150606228463773, 0.036184235537223365], "accuracy_test_std": 0.10780742089485305, "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1.0, 1.2], "translation_range": [-3, 3], "momentum": 0.6958358778483277, "shear_range": [1, 1.1], "patience_check_each": 1, "learning_rate": 0.00022058225592469543, "patience_threshold": 1, "do_flip": true, "batch_size": 16, "optimization": "adam", "nb_data_augmentation": 0, "learning_rate_decay_method": "none", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 1.4185674623303677e-07, "valid_ratio": 0.15, "rotation_range": [-90, 90], "learning_rate_decay": 0.9497008523533242}, "accuracy_valid_max": 0.7608173076923077, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = 1234\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.98, interval=[0.8, 1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-10, -6], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128, 256, 512],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1.0, 1.2)),\n        rotation_range=make_constant_param((-90, 90)),\n        shear_range=make_constant_param((1, 1.1)),\n        translation_range=make_constant_param((-3, 3)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    instantiate = instantiate_random\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n    )\n    batch_optimizer.learning_rate = learning_rate\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.7518696581196581, "loss_train": [1.6403037309646606, 1.2991201877593994, 1.125307321548462, 1.0108658075332642, 0.9282442331314087, 0.8598125576972961, 0.7982609868049622, 0.7427576780319214, 0.6933912634849548, 0.6491430997848511, 0.6093565821647644, 0.5717868208885193, 0.5349932909011841, 0.5002486705780029, 0.4669210910797119, 0.43665367364883423, 0.40766072273254395, 0.3805795907974243, 0.3537081182003021, 0.33378922939300537, 0.3129914402961731, 0.2937532067298889, 0.2740623354911804, 0.2638810873031616, 0.24235029518604279, 0.23171421885490417, 0.220251202583313, 0.20741242170333862, 0.19849629700183868, 0.19038709998130798, 0.18017299473285675, 0.1796010285615921, 0.16664958000183105, 0.15986019372940063, 0.1570444405078888, 0.15190251171588898, 0.14326129853725433, 0.1382395178079605, 0.1442139595746994, 0.13388264179229736, 0.12749874591827393, 0.12909172475337982, 0.11834154278039932, 0.12190958112478256, 0.11675658077001572, 0.11643900722265244, 0.1126832664012909, 0.11377313733100891, 0.11203300207853317, 0.10475247353315353, 0.10269010812044144, 0.10479193180799484, 0.09930824488401413, 0.10069417953491211, 0.10117237269878387, 0.0943215936422348, 0.09535423666238785, 0.09614526480436325, 0.09111015498638153, 0.09302959591150284, 0.08966787904500961, 0.08885201811790466, 0.0862039253115654, 0.08455046266317368, 0.08656623959541321, 0.08589708060026169, 0.08357123285531998, 0.08284854888916016, 0.08236803114414215, 0.0815887525677681, 0.08136312663555145, 0.07917775958776474, 0.07815780490636826, 0.07917440682649612, 0.07451970875263214, 0.07771274447441101, 0.0762103870511055, 0.07729875296354294, 0.06977084279060364, 0.07445809245109558, 0.07430272549390793, 0.06956400722265244, 0.07697103172540665, 0.06812654435634613, 0.06946635991334915, 0.07212520390748978, 0.06462794542312622, 0.0713069960474968, 0.07350204885005951, 0.06810030341148376, 0.06437528133392334, 0.06541059166193008, 0.06736617535352707, 0.06980500370264053, 0.07004838436841965, 0.06259559094905853, 0.05898557975888252, 0.06864667683839798, 0.06359341740608215, 0.06523340940475464, 0.05772547423839569, 0.06787674129009247, 0.0648072212934494, 0.06195366010069847, 0.06067228689789772, 0.06123775243759155, 0.060248877853155136, 0.05592979118227959, 0.06826949864625931, 0.05945908650755882, 0.05759353190660477, 0.05957085266709328, 0.0624956451356411, 0.05711213871836662, 0.056669361889362335, 0.06300465017557144, 0.056592874228954315, 0.059301283210515976, 0.060270775109529495, 0.05548820272088051, 0.05446837469935417, 0.059408999979496, 0.054336875677108765, 0.053919918835163116, 0.05710199475288391, 0.05864514783024788, 0.05449385195970535, 0.05831223353743553, 0.055657465010881424, 0.05285998433828354, 0.05993228033185005, 0.05413975566625595, 0.04966813325881958, 0.052855778485536575, 0.05375578999519348, 0.05496089532971382, 0.05314256623387337, 0.05198053643107414, 0.051360223442316055, 0.05218455195426941, 0.05315345525741577, 0.04866475611925125, 0.05287042260169983, 0.05184490978717804, 0.05220256373286247, 0.04889005422592163, 0.05371642857789993, 0.04889405146241188, 0.05274005979299545, 0.0502593033015728, 0.04807477071881294, 0.05314670130610466, 0.0507325604557991, 0.051048021763563156, 0.05114477872848511, 0.04521697759628296, 0.05321500822901726, 0.049459058791399, 0.04724471643567085, 0.0462406724691391, 0.04893101379275322], "accuracy_train_first": 0.4620905496987952, "model": "residual", "loss_std": [0.29405486583709717, 0.2661014795303345, 0.2752772271633148, 0.27676841616630554, 0.2734221816062927, 0.2689007520675659, 0.2643861472606659, 0.2595961093902588, 0.2555055320262909, 0.2522713243961334, 0.24869577586650848, 0.24414150416851044, 0.23862619698047638, 0.23328687250614166, 0.22749438881874084, 0.22157834470272064, 0.21474747359752655, 0.20844203233718872, 0.20129035413265228, 0.19854415953159332, 0.189576655626297, 0.184087336063385, 0.1790875643491745, 0.17666089534759521, 0.16953939199447632, 0.16654163599014282, 0.16090728342533112, 0.15697883069515228, 0.15806180238723755, 0.14770662784576416, 0.14915014803409576, 0.15256696939468384, 0.14308729767799377, 0.14163050055503845, 0.14543502032756805, 0.13939224183559418, 0.13664156198501587, 0.13610103726387024, 0.14542770385742188, 0.1321355700492859, 0.13639985024929047, 0.13038071990013123, 0.12413185834884644, 0.13733722269535065, 0.12985897064208984, 0.12969446182250977, 0.12671028077602386, 0.1317763328552246, 0.13010002672672272, 0.12374584376811981, 0.1217852234840393, 0.1227562353014946, 0.12280208617448807, 0.12112565338611603, 0.12489432096481323, 0.12367410957813263, 0.12382159382104874, 0.12136495858430862, 0.11878756433725357, 0.12243381142616272, 0.12101052701473236, 0.11640224605798721, 0.11784589290618896, 0.12249410152435303, 0.11444315314292908, 0.1261204332113266, 0.11379280686378479, 0.11728416383266449, 0.1126377061009407, 0.1123046875, 0.11830877512693405, 0.11683928221464157, 0.11554368585348129, 0.11331918835639954, 0.11576070636510849, 0.12160813808441162, 0.11339364945888519, 0.11759225279092789, 0.10461630672216415, 0.10867435485124588, 0.11410903930664062, 0.11050133407115936, 0.11846849322319031, 0.10864932835102081, 0.10724174231290817, 0.10938918590545654, 0.10377568751573563, 0.11200545728206635, 0.11993315070867538, 0.1138659119606018, 0.10152697563171387, 0.10860299319028854, 0.10833272337913513, 0.11317294090986252, 0.11519527435302734, 0.10425680875778198, 0.09840340912342072, 0.1130940169095993, 0.10955177247524261, 0.11005303263664246, 0.09930501878261566, 0.11313989013433456, 0.11352548003196716, 0.10547932237386703, 0.10592294484376907, 0.10492870956659317, 0.10383549332618713, 0.09448286890983582, 0.11597418785095215, 0.10551687330007553, 0.10440073162317276, 0.10605793446302414, 0.10824805498123169, 0.10214164108037949, 0.10299887508153915, 0.11257200688123703, 0.1018688753247261, 0.1086246594786644, 0.11078911274671555, 0.0992518737912178, 0.10284695029258728, 0.1087087094783783, 0.10044465959072113, 0.10102622956037521, 0.10516761988401413, 0.10572012513875961, 0.09985049068927765, 0.11085448414087296, 0.10551352798938751, 0.1094817966222763, 0.11526993662118912, 0.10243071615695953, 0.09525135159492493, 0.09916935116052628, 0.09367574751377106, 0.10789473354816437, 0.10166499763727188, 0.09649976342916489, 0.1031014621257782, 0.10088583081960678, 0.10741009563207626, 0.09715208411216736, 0.09923270344734192, 0.1053655743598938, 0.10487506538629532, 0.09860672801733017, 0.1030815914273262, 0.09029491990804672, 0.10521096736192703, 0.10123233497142792, 0.09431860595941544, 0.10352632403373718, 0.09778019785881042, 0.0963868722319603, 0.09958372265100479, 0.08809974044561386, 0.1062542200088501, 0.09659325331449509, 0.09694615751504898, 0.0923246443271637, 0.09955934435129166]}, "state": "available", "life": [{"dt": "Sun May 15 22:04:58 2016", "state": "available"}], "summary": "7ca40365c2df95ce81c047b3ff6d3770"}