{"content": {"hp_model": {"f0": 64, "f1": 32, "f2": 16, "f3": 32, "nonlin": "leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.010927808144494806, 0.01462048407706055, 0.016426533781677355, 0.01711757311891518, 0.01845588007539926, 0.020375057440602456, 0.015465051583315743, 0.014894838146372087, 0.01292040826091513, 0.012618767519638127, 0.010392478834859747, 0.013148661848183377, 0.011351453637583322, 0.013778679036022163, 0.014869914871037676, 0.015274664297706896, 0.014715678775965484, 0.0194313950397507, 0.0178995685459141, 0.01801880995272449, 0.01833749933844175, 0.015917869128584845, 0.01681561219063732, 0.016669172001586043, 0.015590216196126985, 0.015331594811211388, 0.01376767883358258, 0.014334917338221514, 0.014636157322390682, 0.016708796997637998, 0.01593444737992747, 0.016913990294631365, 0.016506192630161207, 0.013716837876272507, 0.01644042956687832, 0.015105916971008385, 0.014579681209616073, 0.0155368081209776, 0.01448956677612785, 0.015358609404702493, 0.014160196500926798, 0.015523478476855795, 0.01563800568681946, 0.016615773528848722, 0.015347017369715343, 0.015567359739616251, 0.016235737258948654, 0.01381806599983013, 0.013639242119570185, 0.01556756155872894, 0.014866705662216559, 0.0170902606617483, 0.01595665406307179, 0.015837184480487293, 0.01585412856855128, 0.01599009706640138, 0.016580666020430897, 0.017856766998951744, 0.01726984273452595, 0.017803262208152426, 0.017447261941398028, 0.016044650387642752, 0.01763411638535302, 0.016175304444274858, 0.016284638886938506, 0.01684679964995908, 0.017694864637940202, 0.016863624990444734, 0.017842073078530727, 0.016605158081607268, 0.016128611008453954, 0.01637579993172114, 0.01662863414933147, 0.014568541669608621, 0.015893875082370806, 0.01592935772243108, 0.015026872217431621, 0.01707504907568262, 0.015534950256094419, 0.015661996528590294, 0.016628750699749967, 0.015875188875104695, 0.014930537052951054, 0.015250152708748566, 0.015810726571828727, 0.01614962986345194, 0.014308075223673142, 0.014813572412126323, 0.014125380614542927, 0.015498282758908842, 0.01523946941585045, 0.014481645404927116, 0.013789589602294655, 0.01448897650160544, 0.014481019332067973, 0.013938857957421145, 0.013879380161536682, 0.014404549752470882, 0.01532161885832273, 0.014805675739506122, 0.014398140777178596, 0.014449448326318927, 0.014781501221665393, 0.014431220743785306, 0.014702583026852147, 0.014443106973830434, 0.015199787378763782, 0.014207415598546292, 0.014802796801459635, 0.014697316308261888, 0.015521433462325476, 0.014201749584925791, 0.015719446779360647, 0.014438683133546185, 0.01520529366423324, 0.015352611670756524, 0.014804067729350273, 0.014764931236936035, 0.014128627618763323, 0.014935459038046578, 0.013984741582500982, 0.014734518517519585, 0.01513061740572255, 0.013897307941584704, 0.014230683566521924, 0.015363553271759518, 0.014132130021980325, 0.014096979295164529, 0.01525615549029241, 0.014458514341891951, 0.01480603951187335, 0.015531746365216184, 0.015464506375534778, 0.01514472010618844, 0.013113105760005172, 0.013335674691327053, 0.014700042936109032, 0.01252254303367323, 0.01486297170926169, 0.014514549438979299, 0.013778059954351265, 0.013929525390352885, 0.013607328268253794, 0.015121727906023145, 0.014108532026468017, 0.0142145231482397, 0.01594703617093657, 0.014263134413400942, 0.014462736793182487, 0.01392182667309765, 0.013731724526125807, 0.013578297775056968, 0.014045182306764223, 0.01349303212492977, 0.014033880523960157, 0.014692087231552072, 0.014754356134588056, 0.013765142219726561, 0.013643220780920296, 0.014594396331137394, 0.014461796258895434, 0.014182961873453416, 0.013432736410817705, 0.014412645492037015, 0.01282251793596651, 0.0144015319768938, 0.014452497030134917, 0.013920227511224601, 0.01507858486292732, 0.013631291309980526, 0.015166043073922206, 0.013522996191471183, 0.013893933163194427, 0.014147050437346988, 0.014219100853331418, 0.014410826718716125, 0.014181698428203115, 0.014113144493784016, 0.013745850922946886, 0.013830454175677093, 0.01378918983844378, 0.013911812958641136, 0.01333010141359453, 0.013964152680959456, 0.013738509726822144, 0.013961639369209628, 0.013577603204776682, 0.013977505660696703, 0.014564733133098478, 0.014676382706606435, 0.014449117791893628, 0.012685129102878298, 0.01406306883374456, 0.014432116434288851, 0.014354278606639695, 0.014375828712936155, 0.013735314278726452, 0.014157981995295022, 0.013953102365124561, 0.013016012673318854, 0.013584563152647017, 0.013822974202861445, 0.014232994130097024, 0.014173851688266324, 0.013574111973065555, 0.013091216052496763, 0.015103077505718886, 0.014373934554140794, 0.013575784966960604, 0.013802185287164988, 0.013388417594928419, 0.013158945388930065, 0.013232557236345827, 0.013877131364683323, 0.014115819539920103, 0.013749664153004772, 0.014091935135201867, 0.014384237236362939, 0.013364004819819652, 0.014989238569763549, 0.014533869872108144, 0.014643757597178605, 0.013747568313281349, 0.01372817574636387, 0.013537061496765326, 0.014805189819180005, 0.014565421024199901], "moving_avg_accuracy_train": [0.03188554067460317, 0.06656616255998522, 0.10143345964839423, 0.1349922588766611, 0.1668316864069998, 0.19679597762233528, 0.22497262870653956, 0.25141945988581177, 0.2761073814971291, 0.29922383814369713, 0.3203609291124577, 0.34065130079262024, 0.3591591731762135, 0.3763254298619144, 0.39269782051705737, 0.4078191631019795, 0.4216235036843157, 0.4344334570084367, 0.44648997143817626, 0.4578383441725425, 0.4686329505429405, 0.47889675929770886, 0.4884293008317161, 0.49735739658256994, 0.5055462146774045, 0.5133183296091657, 0.5205875645584559, 0.527418194465198, 0.5337239796467698, 0.5398314116470245, 0.5456512600829588, 0.551165816383633, 0.5560474286994502, 0.5607499803800774, 0.565126472167651, 0.5694836613180878, 0.5735957216582244, 0.5774360488440999, 0.5809830601637782, 0.5843055426360032, 0.5874841139145771, 0.5903727658998267, 0.5933073020174852, 0.5959810087043487, 0.5984617855332494, 0.600915337767346, 0.6032212991744892, 0.6054779179016047, 0.6076064228595524, 0.6095988072324195, 0.6114687912251705, 0.6133215126817417, 0.6150492716664284, 0.6167089585467125, 0.6181584989115871, 0.6196397605006794, 0.6210915145689669, 0.622595622732779, 0.6238773846623896, 0.625186647222821, 0.6263323953950573, 0.6273985180798501, 0.628511524366411, 0.629622439920726, 0.6306686587493436, 0.6316522165201273, 0.6325560557531276, 0.6334694564128185, 0.634384414812465, 0.6352287676626138, 0.6359491937468047, 0.6367022449678238, 0.6373729795714936, 0.6380091927981298, 0.6386468528199503, 0.6393253785360173, 0.6398872235554776, 0.6404790227254005, 0.6411464645604648, 0.6416656738572329, 0.6421864766957622, 0.6426692222409331, 0.6431547383589403, 0.6435777880211083, 0.6440003493468122, 0.6444643238482698, 0.6448238082281623, 0.6451451271677123, 0.6455366207609263, 0.645867930509077, 0.6462521758371838, 0.6465770342443754, 0.6468857189013332, 0.6471844253830621, 0.6474951138951897, 0.6477701553561229, 0.6479851405876295, 0.6481715797519193, 0.6483928534223992, 0.6486012282234317, 0.6488027885348556, 0.6490445745865472, 0.649190174517612, 0.6493421047460372, 0.6494858534468672, 0.6496501766073945, 0.64981191019827, 0.6499366522372285, 0.6500767858091867, 0.6502215072144253, 0.6503494313303305, 0.6504575875882166, 0.6505967808988855, 0.6507383309201542, 0.6508354990047722, 0.6509322508761666, 0.6511215980592218, 0.6512246533061328, 0.6512407091664572, 0.6512621348871775, 0.6513674124929596, 0.6514133702619822, 0.651512824925522, 0.6515860220322223, 0.6516309730889668, 0.651783036182894, 0.6518362597079232, 0.6519188939173174, 0.6519584595712667, 0.6519848401622206, 0.6519853312059837, 0.6520113137334566, 0.6520765146379348, 0.6520515261436411, 0.6519778832249673, 0.6519534212279136, 0.6519617044627265, 0.6520505035335726, 0.6520839557699626, 0.6520953894945998, 0.6520104568920391, 0.6519943993211444, 0.6519822726561488, 0.6520154643873963, 0.652054781736032, 0.6520994318962235, 0.6521582542796909, 0.6521530296557546, 0.6522157568096882, 0.6522512488601241, 0.6522251350829157, 0.6522224148274388, 0.6522200386951471, 0.6522201892760754, 0.6522273002453395, 0.6522499761593438, 0.6522146169593378, 0.6522525481436181, 0.6523099016487469, 0.6522870789926393, 0.6522340946652653, 0.652274728376572, 0.652241580301281, 0.6522419018704054, 0.6522398661338079, 0.6522357448708793, 0.6522925256521099, 0.6522365994123419, 0.6523304250227412, 0.6524195904673571, 0.652467287284178, 0.6525101783704982, 0.6524999522231862, 0.6524651720537008, 0.6524431704964019, 0.6524558851293475, 0.652513831275189, 0.6525334307231131, 0.6525464920262631, 0.6525605723479079, 0.652494081431408, 0.6524738752827762, 0.6525183966692273, 0.6525351783801192, 0.6525550403639973, 0.652544906217317, 0.652463777969847, 0.6524489273161809, 0.652428550232634, 0.652433462345537, 0.6524611347352451, 0.6523976842312205, 0.6523871178026074, 0.6524473264323226, 0.652413086446667, 0.6523985825500622, 0.6524157559776418, 0.6523777336398444, 0.6523528501798836, 0.6523188293218712, 0.6523904449996417, 0.6524479597120252, 0.6524579063234177, 0.6524552325296235, 0.6524597655128184, 0.6524685675929504, 0.6525438827317266, 0.6525790782244731, 0.6525782020846116, 0.6526146519885074, 0.6525637875936896, 0.6525295632847636, 0.6525685158710159, 0.6525756714129287, 0.6525937371446979, 0.6526192608497094, 0.6525818504128097, 0.6526016594422189, 0.652640413907973, 0.6525891903235618, 0.6525338966488099, 0.6525305271712675, 0.652534578234364, 0.6525683790280372, 0.6525220698316289, 0.6524548149179565, 0.6524920138432889], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 942875815, "moving_var_accuracy_train": [0.009150189337005961, 0.01905988021251693, 0.028095447847527613, 0.03542164011356299, 0.04100321841134389, 0.04498362530254636, 0.04763057576918063, 0.049162432107086894, 0.04973163015775714, 0.04956780225301535, 0.04863201155930876, 0.04747410304965014, 0.0458095648061915, 0.04388073164296669, 0.04190515506055155, 0.03977253456863162, 0.037510319481986666, 0.03523614167128344, 0.033020763365905766, 0.030877757102778146, 0.02883869313272687, 0.026902935750844313, 0.025030466308638043, 0.02324481772140197, 0.021523846625374383, 0.019915113897449495, 0.018399178498436378, 0.016979178192898656, 0.01563912671441396, 0.01441092057371218, 0.013274664238696185, 0.012220890795566318, 0.01121327296522713, 0.01029097159948514, 0.009434257562836923, 0.008661697682187391, 0.00794770927613697, 0.00728567136457445, 0.006670335831834334, 0.006102652256655082, 0.005583316869346349, 0.0051000839750386905, 0.004667579097567404, 0.004265159554837071, 0.0038940318824266587, 0.0035588079612729577, 0.0032507842872467672, 0.002971536811238206, 0.002715157930318456, 0.00247936849668982, 0.002262903208219144, 0.0020675060785579824, 0.0018876218306846737, 0.0017236506924814897, 0.0015701961286579504, 0.0014329237388500376, 0.0013085996738381388, 0.0011981007787703398, 0.001093076923691098, 0.0009991967473913146, 0.0009110917225198259, 0.0008302121084791136, 0.0007583399445765198, 0.0006936131504382383, 0.0006341029999306022, 0.0005793991729337615, 0.0005288115838723786, 0.00048343913237125545, 0.00044262955899188375, 0.0004047829887126857, 0.00036897581352645975, 0.00033718200744711833, 0.0003075127708794475, 0.00028040439921922386, 0.0002560234520281542, 0.0002345646811516167, 0.00021394924146948602, 0.00019570635364023033, 0.00018014502570495256, 0.00016455672777911226, 0.00015054217537078163, 0.00013758534718614473, 0.0001259483455751323, 0.00011496425016756308, 0.00010507484781663224, 9.650481407699425e-05, 8.801739384377489e-05, 8.014486720761894e-05, 7.350978558860552e-05, 6.71467023727217e-05, 6.176083238499595e-05, 5.653454600900362e-05, 5.1738667365073677e-05, 4.736783068860856e-05, 4.3499793783859875e-05, 3.983064465256447e-05, 3.626354803520143e-05, 3.295002928951114e-05, 3.009568469578888e-05, 2.747689674555802e-05, 2.50948461032739e-05, 2.3111505946080116e-05, 2.0991149410806592e-05, 1.9099779618509972e-05, 1.7375774857571863e-05, 1.5881216281586075e-05, 1.4528514443184933e-05, 1.3215708185418212e-05, 1.2070874128784301e-05, 1.1052285282113933e-05, 1.0094337968773675e-05, 9.19018415697564e-06, 8.44553874089287e-06, 7.781312543494105e-06, 7.08815601915972e-06, 6.463588738808469e-06, 6.139901066505998e-06, 5.621494415098336e-06, 5.061665089445294e-06, 4.55963013407629e-06, 4.203417489181491e-06, 3.802084789065161e-06, 3.5108973810567705e-06, 3.2080279908147446e-06, 2.9054105692553346e-06, 2.8229781731423407e-06, 2.5661750483768765e-06, 2.3710132565988515e-06, 2.148000899690905e-06, 1.9394642299334595e-06, 1.7455199770559099e-06, 1.5770438049552289e-06, 1.4575998459627654e-06, 1.3174596849900697e-06, 1.2345232317282534e-06, 1.1164564122541517e-06, 1.0054282788394321e-06, 9.758529258038521e-07, 8.883391022988209e-07, 8.006817626006504e-07, 7.855355091402699e-07, 7.093025684735486e-07, 6.39695815661429e-07, 5.856414533040975e-07, 5.409899931074184e-07, 5.048337250428588e-07, 4.854910077095935e-07, 4.371875771961091e-07, 4.2888108204196314e-07, 3.97330144634974e-07, 3.6373449441229306e-07, 3.274276430797989e-07, 2.9473569281382713e-07, 2.65262327603988e-07, 2.3919118779845077e-07, 2.198998427019604e-07, 2.0916231565733823e-07, 2.0119505675977803e-07, 2.1068037203880772e-07, 1.943001975211882e-07, 2.0013622829447433e-07, 1.9498249191597606e-07, 1.8537339678386685e-07, 1.6683698776579604e-07, 1.5019058700066882e-07, 1.3532439157374117e-07, 1.508084664708209e-07, 1.6387731847505207e-07, 2.2671879312868206e-07, 2.756012024377393e-07, 2.685159592076298e-07, 2.5822117085833433e-07, 2.3334022057210437e-07, 2.2089314021983792e-07, 2.0316044291001712e-07, 1.8429935563749501e-07, 1.9608922243475527e-07, 1.7993754542163402e-07, 1.6347916963928547e-07, 1.4891555179390375e-07, 1.7381337440749077e-07, 1.6010663294949792e-07, 1.6193535431826772e-07, 1.4827645127059565e-07, 1.3699929177569394e-07, 1.2422367095856033e-07, 1.7103743670064958e-07, 1.559185702593872e-07, 1.4406374303832264e-07, 1.29874528413044e-07, 1.2377892594111041e-07, 1.476347314958947e-07, 1.3387610306900733e-07, 1.5311420459181626e-07, 1.4835417369193986e-07, 1.3541202347321788e-07, 1.2452516065938183e-07, 1.2508392813766494e-07, 1.181482145404914e-07, 1.1675016210551667e-07, 1.5123439361761096e-07, 1.6588243352092255e-07, 1.5018460587257648e-07, 1.3523048784461085e-07, 1.2189237048995732e-07, 1.1040042297280895e-07, 1.5041171183544535e-07, 1.4651904503891806e-07, 1.318740491245383e-07, 1.3064400365819815e-07, 1.4086428323408778e-07, 1.3731958480383396e-07, 1.3724336210514844e-07, 1.2397984191522723e-07, 1.1451919370289579e-07, 1.0893040999026399e-07, 1.1063323609247248e-07, 1.0310149129848194e-07, 1.0630851971161302e-07, 1.1929236813981885e-07, 1.3487964553394615e-07, 1.21493861390736e-07, 1.0949217526157549e-07, 1.0882540061188301e-07, 1.1724373559860363e-07, 1.462283727563571e-07, 1.4405937589371803e-07], "duration": 139079.736722, "accuracy_train": [0.31885540674603174, 0.37869175952842377, 0.4152391334440753, 0.4370214519310632, 0.453386534180048, 0.4664745985603544, 0.478562488464378, 0.48944094049926173, 0.4982986759989848, 0.5072719479628092, 0.5105947478313031, 0.5232646459140826, 0.525730024628553, 0.5308217400332226, 0.5400493364133444, 0.5439112463662791, 0.5458625689253415, 0.549723036925526, 0.5549986013058325, 0.5599736987818383, 0.5657844078765227, 0.5712710380906239, 0.5742221746377815, 0.5777102583402547, 0.5792455775309154, 0.5832673639950167, 0.5860106791020672, 0.5888938636258767, 0.5904760462809154, 0.5947982996493171, 0.5980298960063676, 0.600796823089701, 0.5999819395418051, 0.6030729455057217, 0.604514898255814, 0.6086983636720192, 0.6106042647194536, 0.6119989935169804, 0.6129061620408822, 0.6142078848860281, 0.6160912554217424, 0.6163706337670727, 0.6197181270764119, 0.6200443688861204, 0.6207887769933554, 0.6229973078742156, 0.6239749518387782, 0.6257874864456442, 0.6267629674810815, 0.6275302665882244, 0.6282986471599299, 0.6299960057908822, 0.6305991025286084, 0.631646140469269, 0.6312043621954595, 0.6329711148025101, 0.6341573011835548, 0.6361325962070875, 0.6354132420288852, 0.6369700102667036, 0.6366441289451827, 0.6369936222429863, 0.6385285809454595, 0.6396206799095607, 0.6400846282069029, 0.6405042364571798, 0.6406906088501292, 0.641690062350037, 0.6426190404092839, 0.6428279433139534, 0.642433028504522, 0.6434797059569952, 0.643409591004522, 0.6437351118378553, 0.6443857930163345, 0.6454321099806202, 0.6449438287306202, 0.6458052152547066, 0.6471534410760429, 0.646338557528147, 0.6468737022425249, 0.6470139321474714, 0.647524383421004, 0.6473852349806202, 0.647803401278147, 0.6486400943613879, 0.6480591676471945, 0.6480369976236618, 0.6490600630998523, 0.6488497182424326, 0.649710383790144, 0.6495007599090993, 0.6496638808139534, 0.6498727837186231, 0.6502913105043374, 0.650245528504522, 0.6499200076711886, 0.6498495322305279, 0.6503843164567183, 0.6504766014327242, 0.6506168313376707, 0.6512206490517718, 0.6505005738971945, 0.6507094768018641, 0.6507795917543374, 0.6511290850521411, 0.6512675125161499, 0.6510593305878553, 0.6513379879568106, 0.6515239998615725, 0.6515007483734773, 0.6514309939091916, 0.6518495206949059, 0.6520122811115725, 0.6517100117663345, 0.6518030177187154, 0.6528257227067183, 0.6521521505283315, 0.6513852119093761, 0.6514549663736618, 0.6523149109449982, 0.6518269901831857, 0.6524079168973791, 0.6522447959925249, 0.6520355325996677, 0.6531516040282392, 0.6523152714331857, 0.6526626018018641, 0.6523145504568106, 0.6522222654808048, 0.6519897505998523, 0.6522451564807125, 0.6526633227782392, 0.6518266296949982, 0.6513150969569029, 0.6517332632544297, 0.6520362535760429, 0.6528496951711886, 0.6523850258974714, 0.6521982930163345, 0.6512460634689923, 0.6518498811830934, 0.6518731326711886, 0.6523141899686231, 0.6524086378737541, 0.6525012833379475, 0.652687655730897, 0.6521060080403286, 0.6527803011950905, 0.6525706773140458, 0.65199011108804, 0.652197932528147, 0.652198653504522, 0.6522215445044297, 0.6522912989687154, 0.652454059385382, 0.6518963841592839, 0.6525939288021411, 0.6528260831949059, 0.6520816750876707, 0.6517572357189, 0.6526404317783315, 0.6519432476236618, 0.6522447959925249, 0.6522215445044297, 0.652198653504522, 0.6528035526831857, 0.6517332632544297, 0.6531748555163345, 0.6532220794689, 0.6528965586355666, 0.6528961981473791, 0.6524079168973791, 0.6521521505283315, 0.6522451564807125, 0.6525703168258582, 0.653035346587763, 0.6527098257544297, 0.6526640437546143, 0.6526872952427095, 0.6518956631829088, 0.6522920199450905, 0.6529190891472868, 0.652686213778147, 0.6527337982189, 0.6524536988971945, 0.6517336237426172, 0.6523152714331857, 0.6522451564807125, 0.6524776713616648, 0.6527101862426172, 0.6518266296949982, 0.6522920199450905, 0.65298920409976, 0.6521049265757659, 0.6522680474806202, 0.6525703168258582, 0.6520355325996677, 0.6521288990402363, 0.65201264159976, 0.6530349860995754, 0.6529655921234773, 0.6525474258259505, 0.6524311683854743, 0.6525005623615725, 0.6525477863141381, 0.6532217189807125, 0.6528958376591916, 0.6525703168258582, 0.6529427011235696, 0.6521060080403286, 0.6522215445044297, 0.6529190891472868, 0.652640071290144, 0.6527563287306202, 0.6528489741948136, 0.6522451564807125, 0.6527799407069029, 0.65298920409976, 0.6521281780638611, 0.6520362535760429, 0.652500201873385, 0.6525710378022334, 0.6528725861710963, 0.6521052870639534, 0.6518495206949059, 0.6528268041712809], "end": "2016-01-25 01:20:15.452000", "learning_rate_per_epoch": [0.0001981394598260522, 0.00019084758241660893, 0.00018382405687589198, 0.00017705901700537652, 0.00017054293130058795, 0.00016426666115876287, 0.00015822135901544243, 0.0001523985411040485, 0.00014679001469630748, 0.00014138789265416563, 0.00013618457887787372, 0.0001311727537540719, 0.00012634537415578961, 0.00012169564433861524, 0.00011721703049261123, 0.00011290323891444132, 0.00010874820145545527, 0.00010474607552168891, 0.00010089123679790646, 9.717826469568536e-05, 9.360193507745862e-05, 9.015722025651485e-05, 8.683927444508299e-05, 8.364344103028998e-05, 8.056521619437262e-05, 7.76002780185081e-05, 7.474445010302588e-05, 7.199372339528054e-05, 6.934422708582133e-05, 6.679223588434979e-05, 6.433416274376214e-05, 6.196655158419162e-05, 5.968607365502976e-05, 5.74895238969475e-05, 5.5373810027958825e-05, 5.3335956181399524e-05, 5.137309926794842e-05, 4.9482481699669734e-05, 4.766144047607668e-05, 4.5907418098067865e-05, 4.421794437803328e-05, 4.25906473537907e-05, 4.1023238736670464e-05, 3.9513513911515474e-05, 3.805934829870239e-05, 3.665869735414162e-05, 3.530959293129854e-05, 3.401013964321464e-05, 3.275850758654997e-05, 3.155293597956188e-05, 3.0391733162105083e-05, 2.927326386270579e-05, 2.8195956474519335e-05, 2.7158295779372565e-05, 2.6158822947763838e-05, 2.519613190088421e-05, 2.426887112960685e-05, 2.337573459954001e-05, 2.2515467207995243e-05, 2.1686859327019192e-05, 2.0888744984404184e-05, 2.0120003682677634e-05, 1.9379553123144433e-05, 1.866635102487635e-05, 1.797939694370143e-05, 1.7317723177256994e-05, 1.6680400221957825e-05, 1.606653313501738e-05, 1.547525607747957e-05, 1.4905739590176381e-05, 1.4357182408275548e-05, 1.382881328026997e-05, 1.3319889148988295e-05, 1.2829694242100231e-05, 1.2357539162621833e-05, 1.1902759979420807e-05, 1.1464717317721806e-05, 1.1042795449611731e-05, 1.0636401384545024e-05, 1.0244963050354272e-05, 9.867930202744901e-06, 9.50477351580048e-06, 9.15498094400391e-06, 8.818061360216234e-06, 8.49354182719253e-06, 8.180964869097807e-06, 7.879891199991107e-06, 7.589897450088756e-06, 7.31057616576436e-06, 7.041534445306752e-06, 6.782393938919995e-06, 6.532789939228678e-06, 6.292371836025268e-06, 6.0608017520280555e-06, 5.8377536333864555e-06, 5.622914159175707e-06, 5.415980922407471e-06, 5.21666333952453e-06, 5.024681286158739e-06, 4.8397641876363195e-06, 4.661652383219916e-06, 4.490095307119191e-06, 4.324851943238173e-06, 4.165689915680559e-06, 4.0123854887497146e-06, 3.864722657453967e-06, 3.722494057001313e-06, 3.5854998259310378e-06, 3.453547151366365e-06, 3.326450723761809e-06, 3.2040316000347957e-06, 3.0861176583130145e-06, 2.9725431431870675e-06, 2.8631484383367933e-06, 2.757779611783917e-06, 2.6562884158920497e-06, 2.5585322873666883e-06, 2.464373892507865e-06, 2.373680672462797e-06, 2.2863250705995597e-06, 2.2021843051334145e-06, 2.1211401417531306e-06, 2.0430784388736356e-06, 1.967889602383366e-06, 1.8954677898364025e-06, 1.8257112515129847e-06, 1.7585218756721588e-06, 1.693805188551778e-06, 1.631470240681665e-06, 1.5714292658230988e-06, 1.51359790834249e-06, 1.4578948821508675e-06, 1.4042418570170412e-06, 1.352563344880764e-06, 1.3027866998527315e-06, 1.2548418908409076e-06, 1.208661501550523e-06, 1.164180616797239e-06, 1.1213367088203086e-06, 1.0800695235957392e-06, 1.0403210808362928e-06, 1.0020354466178105e-06, 9.6515884706605e-07, 9.296393272961723e-07, 8.954269787864177e-07, 8.624737120044301e-07, 8.307331995638378e-07, 8.001607625374163e-07, 7.707134273005067e-07, 7.423498118441785e-07, 7.150300689318101e-07, 6.887157155688328e-07, 6.633698035329871e-07, 6.389566351572284e-07, 6.154419338599837e-07, 5.927926167714759e-07, 5.709767947337241e-07, 5.499638291439624e-07, 5.297242182678019e-07, 5.102294267089746e-07, 4.914521127830085e-07, 4.7336581587842375e-07, 4.5594512698698964e-07, 4.3916554659517715e-07, 4.2300348468415905e-07, 4.0743623230810044e-07, 3.924418763290305e-07, 3.7799932783855184e-07, 3.6408829373613116e-07, 3.5068921988568036e-07, 3.3778323427213763e-07, 3.253522322665958e-07, 3.133787060960458e-07, 3.0184583010850474e-07, 2.907373755078879e-07, 2.80037738775718e-07, 2.6973185640599695e-07, 2.598052617486246e-07, 2.5024397132256126e-07, 2.4103457008095575e-07, 2.3216408351345308e-07, 2.2362004870046803e-07, 2.1539044325891155e-07, 2.0746369955304544e-07, 1.99828676272773e-07, 1.9247463001192955e-07, 1.853912294791371e-07, 1.7856851286524034e-07, 1.7199688784330647e-07, 1.656671031469159e-07, 1.5957026278101694e-07, 1.536977976002163e-07, 1.4804145109792444e-07, 1.4259326519550086e-07, 1.3734558024225407e-07, 1.3229102080458688e-07, 1.2742248145514168e-07, 1.2273311256194575e-07, 1.1821632028841123e-07, 1.1386575238248042e-07, 1.096752981766258e-07, 1.056390530607132e-07, 1.017513540091386e-07, 9.800672984283665e-08, 9.439990833470802e-08, 9.092583042047409e-08, 8.757960046068547e-08, 8.43565217678588e-08, 8.125205397391255e-08, 7.826183434644918e-08, 7.538166357790033e-08, 7.260748446924481e-08, 6.993540324629066e-08, 6.736166113796571e-08, 6.488263437631758e-08, 6.249484130194105e-08, 6.019492104769597e-08, 5.797964419684831e-08, 5.584589146678809e-08, 5.379066436717039e-08, 5.181107454177436e-08, 4.990433666307581e-08, 4.806777198496093e-08, 4.6298794131871546e-08, 4.459491975694618e-08, 4.295375077845165e-08, 4.1372977932496724e-08], "accuracy_valid": [0.3195021296121988, 0.37835178605045183, 0.41306093514683734, 0.43524831748870485, 0.44871723221009036, 0.4620537815323795, 0.4740681475903614, 0.4823998141001506, 0.49407885448042166, 0.5020649002259037, 0.5068359375, 0.5203460325677711, 0.5209666792168675, 0.530407273625753, 0.5356871823230422, 0.5414450771837349, 0.5428893307605422, 0.5477118434676205, 0.5513842479292168, 0.5577421992658133, 0.5621470256024097, 0.5677828501506024, 0.5693285838667168, 0.5723906367658133, 0.5710272731551205, 0.5761748164533133, 0.577782320689006, 0.5802134318524097, 0.5826857233621988, 0.5863684229103916, 0.5895216608621988, 0.5909865046121988, 0.5900511224585843, 0.5928484445594879, 0.5924719385353916, 0.5977312570594879, 0.6000505929969879, 0.6019022378576807, 0.6032450112951807, 0.6027361398719879, 0.6068056405308735, 0.6078939782567772, 0.6096235528049698, 0.6101015389683735, 0.6097456231174698, 0.6112207619540663, 0.6120649590549698, 0.6123090996799698, 0.6131635918674698, 0.6135400978915663, 0.6163580101656627, 0.6185552757906627, 0.619053852127259, 0.6179449242281627, 0.6200201195406627, 0.621861469314759, 0.6231027626129518, 0.621861469314759, 0.6230924675263554, 0.6238248894013554, 0.622960102127259, 0.6237131141754518, 0.6243234657379518, 0.6255544639495482, 0.6253103233245482, 0.6245676063629518, 0.6246793815888554, 0.6271516730986446, 0.6259103798004518, 0.6283723762236446, 0.6279752800263554, 0.6284841514495482, 0.6284841514495482, 0.6287179969879518, 0.6299592902861446, 0.628748882247741, 0.6313329489834337, 0.628870952560241, 0.6294607139495482, 0.6304681617093373, 0.6298578101468373, 0.6314550192959337, 0.629481304122741, 0.6309564429593373, 0.6296136695218373, 0.6309358527861446, 0.6319330054593373, 0.6307123023343373, 0.6293489387236446, 0.630824077560241, 0.6305696418486446, 0.6313020637236446, 0.6305593467620482, 0.6311799934111446, 0.6299592902861446, 0.6309358527861446, 0.6324006965361446, 0.631068218185241, 0.6313020637236446, 0.631556499435241, 0.6332757788968373, 0.632533061935241, 0.631556499435241, 0.6332757788968373, 0.6328889777861446, 0.6336213996611446, 0.6330110480986446, 0.6341302710843373, 0.6332448936370482, 0.634608257247741, 0.634242046310241, 0.6331228233245482, 0.6343847067959337, 0.6340082007718373, 0.633631694747741, 0.6337640601468373, 0.6346185523343373, 0.634119975997741, 0.6331331184111446, 0.6349847632718373, 0.634486186935241, 0.6337640601468373, 0.6346185523343373, 0.6332448936370482, 0.6339876105986446, 0.6357171851468373, 0.6343744117093373, 0.6344964820218373, 0.6346185523343373, 0.6347406226468373, 0.634242046310241, 0.6347406226468373, 0.6338861304593373, 0.634608257247741, 0.6346185523343373, 0.6352289038968373, 0.6333772590361446, 0.6343538215361446, 0.6337434699736446, 0.6341302710843373, 0.6346185523343373, 0.6351068335843373, 0.6349847632718373, 0.6358495505459337, 0.6353509742093373, 0.634242046310241, 0.6357274802334337, 0.6348626929593373, 0.633753765060241, 0.6347200324736446, 0.6353509742093373, 0.6349950583584337, 0.6339876105986446, 0.6341302710843373, 0.6356054099209337, 0.634119975997741, 0.6333669639495482, 0.635340679122741, 0.6354730445218373, 0.634119975997741, 0.6334993293486446, 0.6342523413968373, 0.6347406226468373, 0.6331331184111446, 0.6349847632718373, 0.6352289038968373, 0.6363275367093373, 0.6342317512236446, 0.6341096809111446, 0.6333772590361446, 0.633631694747741, 0.6357171851468373, 0.6343744117093373, 0.634364116622741, 0.634608257247741, 0.6338655402861446, 0.6364701971950302, 0.6359716208584337, 0.6351171286709337, 0.634486186935241, 0.6344964820218373, 0.635218608810241, 0.634486186935241, 0.6353509742093373, 0.635096538497741, 0.6352391989834337, 0.6346185523343373, 0.634364116622741, 0.6363481268825302, 0.634974468185241, 0.6343744117093373, 0.6352289038968373, 0.6351171286709337, 0.6349847632718373, 0.6356157050075302, 0.6347406226468373, 0.634486186935241, 0.6342317512236446, 0.6344758918486446, 0.6352391989834337, 0.6357171851468373, 0.6359716208584337, 0.6343744117093373, 0.6360936911709337, 0.6351068335843373, 0.6359613257718373, 0.635340679122741, 0.6344758918486446, 0.6357171851468373, 0.634608257247741, 0.6351068335843373, 0.6348626929593373, 0.6359716208584337, 0.6354730445218373, 0.6342317512236446, 0.6346288474209337, 0.6352289038968373, 0.6351068335843373, 0.634852397872741, 0.6354833396084337, 0.6348729880459337, 0.6359716208584337, 0.6362157614834337, 0.634242046310241, 0.6349847632718373, 0.6354833396084337, 0.6351068335843373], "accuracy_test": 0.6337859424920128, "start": "2016-01-23 10:42:15.715000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0, 211.0, 212.0, 213.0, 214.0, 215.0, 216.0, 217.0, 218.0, 219.0, 220.0, 221.0, 222.0, 223.0, 224.0, 225.0, 226.0], "accuracy_train_last": 0.6528268041712809, "batch_size_eval": 1024, "accuracy_train_std": [0.01542629883589486, 0.015652430195397686, 0.015056239515879736, 0.014125607094005085, 0.013466033857672999, 0.013280899923102248, 0.014995064661479434, 0.016156443577160628, 0.014659323464093045, 0.013171347590114844, 0.014150649891546912, 0.013988814199583333, 0.011934864111134931, 0.013158750984645267, 0.012019771103661722, 0.01209182215836, 0.01186536025103256, 0.01242012600489317, 0.012299551506260388, 0.012388718842404965, 0.013539908379744593, 0.013493607994680298, 0.013716430397724742, 0.013189398296998599, 0.013403214457708826, 0.012353770595465456, 0.013142600419942, 0.012519070455152421, 0.013258469579063393, 0.013145381373142792, 0.013607372288326635, 0.01355619008432396, 0.013327874024150562, 0.013917364257650569, 0.0123617074931458, 0.012987950810097609, 0.013533132683700972, 0.01319132461239984, 0.01363043942434146, 0.014139756175502281, 0.013792699600778162, 0.014178217953065403, 0.014623263973897526, 0.013875458462895852, 0.013634037980258015, 0.012647737929386048, 0.013034071721260657, 0.012883705725249425, 0.013111446699706653, 0.01282421711666426, 0.01297512105754301, 0.012456336556285629, 0.013269593373033468, 0.01236907137450684, 0.012937063076943867, 0.012685091108759414, 0.011709201069502429, 0.011623060750129735, 0.012220416517291628, 0.012187222082867901, 0.0118176547587975, 0.01195354544052651, 0.011612516007587188, 0.012307646778632414, 0.012714068531028643, 0.012514185268837656, 0.011981592119649798, 0.012130612596390939, 0.012560214343335218, 0.012720901363958292, 0.01255220909551369, 0.011362935527354042, 0.012396101927747852, 0.012274142777283744, 0.012364114388603962, 0.01201785474123001, 0.011832438615413863, 0.01226468610665257, 0.011853788437846251, 0.011952881592873296, 0.012017938435278189, 0.011925752097969132, 0.011134917893201002, 0.01155454538543976, 0.011997737875995396, 0.012369012070898744, 0.011130243232247018, 0.011400273851279445, 0.011849520283772395, 0.011567237640292615, 0.011247243201242103, 0.011500000828573035, 0.011765377136693404, 0.011919874042753915, 0.011659264624325165, 0.011043124179289434, 0.01147191346571678, 0.01145745054602959, 0.011375798614804365, 0.011116060244934911, 0.011579318768150658, 0.011260140797542615, 0.010855372895885196, 0.010709686419543965, 0.011087038222249657, 0.011239703383705824, 0.01097835078345726, 0.011409455110155934, 0.010963914723904077, 0.01113137726024386, 0.011189664141798152, 0.010957553022479569, 0.011089314474426801, 0.01143103501140519, 0.010991695010859406, 0.011276000246032343, 0.010950534583306259, 0.011088545805784656, 0.01097858730817495, 0.011317388124877273, 0.011497298463002174, 0.01121766342744509, 0.011320018713265064, 0.010838017060023929, 0.011416778754094954, 0.011564434418014668, 0.011220420832966038, 0.011210173157291987, 0.011922816472859511, 0.011718702488163825, 0.010960936061726469, 0.011611875263002603, 0.011411493139309814, 0.01139140771595042, 0.01132661732987819, 0.011479030049182288, 0.01155920185085777, 0.01172973356162044, 0.01115238151239154, 0.011446822864794648, 0.011079735495772835, 0.011409368421844321, 0.011791053990884058, 0.011465316574938947, 0.011872056661971102, 0.011115654355370836, 0.01120795666790584, 0.011630363019509973, 0.011236237559963003, 0.011448580726861484, 0.0115803171832049, 0.011239400592191329, 0.011667960131508491, 0.011372466196837414, 0.011731711486857813, 0.011368593901423422, 0.011252991870878451, 0.01135749148712294, 0.011560994011721065, 0.011266552026069162, 0.011047979447340988, 0.011625117580515668, 0.01152684273276414, 0.011068128285826697, 0.01113641140870551, 0.011354321497271237, 0.011484168200039651, 0.01074545677790161, 0.011655691984511626, 0.010917911309456742, 0.0111895954474341, 0.01152648626124424, 0.01153265023210513, 0.01161071304649433, 0.011126557099607717, 0.01110721316866896, 0.011625804269949, 0.01154462555623932, 0.011477041051111813, 0.011188845407320616, 0.011124766101819034, 0.01130802577490371, 0.011256146776316034, 0.011344739701019738, 0.011073076165150506, 0.01152211994456423, 0.011091844840185707, 0.011373180576821736, 0.011615785520097373, 0.011364453354152836, 0.011295186649363964, 0.011297334013154263, 0.01155237004411935, 0.011278655941361288, 0.011599721082325199, 0.011290715439678314, 0.011343875559460117, 0.011155229220897811, 0.011271924400150173, 0.011300296912760122, 0.011850956677994476, 0.011293955419230239, 0.010934706280113043, 0.011464368544756725, 0.01094470594856494, 0.01146485196548645, 0.011232660653964312, 0.011108068891284259, 0.01166169009196199, 0.011928858794501092, 0.011496836769430812, 0.010771581129583908, 0.011426327081712692, 0.011279030430559804, 0.011225929472240562, 0.011604878533077354, 0.011240258199936848, 0.011403757258472254, 0.011432623743936787, 0.01165972525267964, 0.011815653672286916, 0.01118762164714509, 0.012058081042973224, 0.011525516825220305, 0.011777702132285262, 0.01207729601800344, 0.011557127076942621], "accuracy_test_std": 0.09325379174377277, "error_valid": [0.6804978703878012, 0.6216482139495482, 0.5869390648531627, 0.5647516825112951, 0.5512827677899097, 0.5379462184676205, 0.5259318524096386, 0.5176001858998494, 0.5059211455195783, 0.49793509977409633, 0.4931640625, 0.4796539674322289, 0.47903332078313254, 0.469592726374247, 0.46431281767695776, 0.4585549228162651, 0.45711066923945776, 0.4522881565323795, 0.4486157520707832, 0.44225780073418675, 0.4378529743975903, 0.43221714984939763, 0.4306714161332832, 0.42760936323418675, 0.4289727268448795, 0.42382518354668675, 0.42221767931099397, 0.4197865681475903, 0.4173142766378012, 0.4136315770896084, 0.4104783391378012, 0.4090134953878012, 0.40994887754141573, 0.40715155544051207, 0.4075280614646084, 0.40226874294051207, 0.39994940700301207, 0.3980977621423193, 0.3967549887048193, 0.39726386012801207, 0.3931943594691265, 0.39210602174322284, 0.3903764471950302, 0.3898984610316265, 0.3902543768825302, 0.38877923804593373, 0.3879350409450302, 0.3876909003200302, 0.3868364081325302, 0.38645990210843373, 0.3836419898343373, 0.3814447242093373, 0.38094614787274095, 0.3820550757718373, 0.3799798804593373, 0.37813853068524095, 0.37689723738704817, 0.37813853068524095, 0.3769075324736446, 0.3761751105986446, 0.37703989787274095, 0.37628688582454817, 0.37567653426204817, 0.37444553605045183, 0.37468967667545183, 0.37543239363704817, 0.3753206184111446, 0.3728483269013554, 0.37408962019954817, 0.3716276237763554, 0.3720247199736446, 0.37151584855045183, 0.37151584855045183, 0.37128200301204817, 0.3700407097138554, 0.37125111775225905, 0.36866705101656627, 0.37112904743975905, 0.37053928605045183, 0.3695318382906627, 0.3701421898531627, 0.36854498070406627, 0.37051869587725905, 0.3690435570406627, 0.3703863304781627, 0.3690641472138554, 0.3680669945406627, 0.3692876976656627, 0.3706510612763554, 0.36917592243975905, 0.3694303581513554, 0.3686979362763554, 0.36944065323795183, 0.3688200065888554, 0.3700407097138554, 0.3690641472138554, 0.3675993034638554, 0.36893178181475905, 0.3686979362763554, 0.36844350056475905, 0.3667242211031627, 0.36746693806475905, 0.36844350056475905, 0.3667242211031627, 0.3671110222138554, 0.3663786003388554, 0.3669889519013554, 0.3658697289156627, 0.36675510636295183, 0.36539174275225905, 0.36575795368975905, 0.36687717667545183, 0.36561529320406627, 0.3659917992281627, 0.36636830525225905, 0.3662359398531627, 0.3653814476656627, 0.36588002400225905, 0.3668668815888554, 0.3650152367281627, 0.36551381306475905, 0.3662359398531627, 0.3653814476656627, 0.36675510636295183, 0.3660123894013554, 0.3642828148531627, 0.3656255882906627, 0.3655035179781627, 0.3653814476656627, 0.3652593773531627, 0.36575795368975905, 0.3652593773531627, 0.3661138695406627, 0.36539174275225905, 0.3653814476656627, 0.3647710961031627, 0.3666227409638554, 0.3656461784638554, 0.3662565300263554, 0.3658697289156627, 0.3653814476656627, 0.3648931664156627, 0.3650152367281627, 0.36415044945406627, 0.3646490257906627, 0.36575795368975905, 0.36427251976656627, 0.3651373070406627, 0.36624623493975905, 0.3652799675263554, 0.3646490257906627, 0.36500494164156627, 0.3660123894013554, 0.3658697289156627, 0.36439459007906627, 0.36588002400225905, 0.36663303605045183, 0.36465932087725905, 0.3645269554781627, 0.36588002400225905, 0.3665006706513554, 0.3657476586031627, 0.3652593773531627, 0.3668668815888554, 0.3650152367281627, 0.3647710961031627, 0.3636724632906627, 0.3657682487763554, 0.3658903190888554, 0.3666227409638554, 0.36636830525225905, 0.3642828148531627, 0.3656255882906627, 0.36563588337725905, 0.36539174275225905, 0.3661344597138554, 0.3635298028049698, 0.36402837914156627, 0.36488287132906627, 0.36551381306475905, 0.3655035179781627, 0.36478139118975905, 0.36551381306475905, 0.3646490257906627, 0.36490346150225905, 0.36476080101656627, 0.3653814476656627, 0.36563588337725905, 0.3636518731174698, 0.36502553181475905, 0.3656255882906627, 0.3647710961031627, 0.36488287132906627, 0.3650152367281627, 0.3643842949924698, 0.3652593773531627, 0.36551381306475905, 0.3657682487763554, 0.3655241081513554, 0.36476080101656627, 0.3642828148531627, 0.36402837914156627, 0.3656255882906627, 0.36390630882906627, 0.3648931664156627, 0.3640386742281627, 0.36465932087725905, 0.3655241081513554, 0.3642828148531627, 0.36539174275225905, 0.3648931664156627, 0.3651373070406627, 0.36402837914156627, 0.3645269554781627, 0.3657682487763554, 0.36537115257906627, 0.3647710961031627, 0.3648931664156627, 0.36514760212725905, 0.36451666039156627, 0.36512701195406627, 0.36402837914156627, 0.36378423851656627, 0.36575795368975905, 0.3650152367281627, 0.36451666039156627, 0.3648931664156627], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-3, 3], "momentum": 0.782688554848471, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.00020570994481141207, "patience_threshold": 1, "do_flip": true, "batch_size": 32, "optimization": "nesterov_momentum", "nb_data_augmentation": 2, "learning_rate_decay_method": "exp", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 1.9198945473843877e-10, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.03680173839330299}, "accuracy_valid_max": 0.6364701971950302, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import os\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-3, 3)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.6351068335843373, "loss_train": [2.1059651374816895, 1.80792236328125, 1.692091703414917, 1.6276721954345703, 1.58266019821167, 1.545799732208252, 1.5160030126571655, 1.487646460533142, 1.4606750011444092, 1.4370571374893188, 1.4137786626815796, 1.394202470779419, 1.3758286237716675, 1.360158920288086, 1.3441435098648071, 1.3297595977783203, 1.3184387683868408, 1.3071362972259521, 1.294057011604309, 1.2871932983398438, 1.275141716003418, 1.266628384590149, 1.2580150365829468, 1.2498773336410522, 1.2432949542999268, 1.2357869148254395, 1.228454828262329, 1.2212209701538086, 1.2154735326766968, 1.2087512016296387, 1.2047816514968872, 1.198880672454834, 1.192348599433899, 1.1887813806533813, 1.1854666471481323, 1.179603934288025, 1.174757719039917, 1.1708991527557373, 1.1672449111938477, 1.1623561382293701, 1.1610230207443237, 1.1560999155044556, 1.1525800228118896, 1.1489391326904297, 1.1464776992797852, 1.1433444023132324, 1.1398087739944458, 1.1384642124176025, 1.1349520683288574, 1.1330301761627197, 1.1299666166305542, 1.1276072263717651, 1.1254812479019165, 1.1227527856826782, 1.1207127571105957, 1.1204427480697632, 1.1185460090637207, 1.1150811910629272, 1.113888144493103, 1.1117546558380127, 1.1097478866577148, 1.1089465618133545, 1.107214331626892, 1.105120301246643, 1.104393482208252, 1.104505181312561, 1.1035667657852173, 1.1014366149902344, 1.0994065999984741, 1.0990358591079712, 1.097848892211914, 1.0963042974472046, 1.0960755348205566, 1.0953515768051147, 1.093241572380066, 1.0923436880111694, 1.0922104120254517, 1.0908474922180176, 1.0899401903152466, 1.0913442373275757, 1.0882905721664429, 1.0855411291122437, 1.0871421098709106, 1.0856090784072876, 1.0856525897979736, 1.0851837396621704, 1.0840201377868652, 1.085154414176941, 1.0834821462631226, 1.0841974020004272, 1.0829670429229736, 1.0820471048355103, 1.0806846618652344, 1.0810370445251465, 1.079917073249817, 1.0814101696014404, 1.081296443939209, 1.0794860124588013, 1.0795501470565796, 1.0800288915634155, 1.07973051071167, 1.0782839059829712, 1.0785741806030273, 1.077215552330017, 1.078089952468872, 1.076645016670227, 1.0766414403915405, 1.0737926959991455, 1.076909065246582, 1.0762053728103638, 1.0749980211257935, 1.0752370357513428, 1.0753881931304932, 1.075195550918579, 1.0746111869812012, 1.0752469301223755, 1.0737556219100952, 1.0729665756225586, 1.073881983757019, 1.0752642154693604, 1.0727312564849854, 1.0730171203613281, 1.0735505819320679, 1.0731450319290161, 1.0728607177734375, 1.072542667388916, 1.0736349821090698, 1.0737825632095337, 1.0721838474273682, 1.0729310512542725, 1.0727733373641968, 1.07470703125, 1.0714558362960815, 1.0724314451217651, 1.0717960596084595, 1.0721265077590942, 1.0712486505508423, 1.0715967416763306, 1.0708218812942505, 1.0718239545822144, 1.071259617805481, 1.0709837675094604, 1.0716450214385986, 1.0708881616592407, 1.0701566934585571, 1.0720949172973633, 1.0715736150741577, 1.0693970918655396, 1.0725891590118408, 1.0722545385360718, 1.071784496307373, 1.0701757669448853, 1.0700894594192505, 1.0708909034729004, 1.0710384845733643, 1.0715121030807495, 1.0705537796020508, 1.070315957069397, 1.0699217319488525, 1.069399356842041, 1.0707148313522339, 1.0695936679840088, 1.071079969406128, 1.0696967840194702, 1.0686705112457275, 1.0709779262542725, 1.0702006816864014, 1.069615364074707, 1.0699002742767334, 1.070380449295044, 1.0692843198776245, 1.0705451965332031, 1.0702860355377197, 1.0704412460327148, 1.069586157798767, 1.069574236869812, 1.0688729286193848, 1.0693905353546143, 1.0698227882385254, 1.0687404870986938, 1.0700531005859375, 1.0703190565109253, 1.0700652599334717, 1.0694246292114258, 1.0695993900299072, 1.0710843801498413, 1.0697226524353027, 1.0701051950454712, 1.0694202184677124, 1.0703431367874146, 1.0699368715286255, 1.0691641569137573, 1.0701920986175537, 1.0697441101074219, 1.0699673891067505, 1.0685111284255981, 1.0672693252563477, 1.0697903633117676, 1.0689374208450317, 1.0692616701126099, 1.0709896087646484, 1.0693002939224243, 1.070286750793457, 1.0698522329330444, 1.070339560508728, 1.068734049797058, 1.070511817932129, 1.0691436529159546, 1.0701175928115845, 1.0698035955429077, 1.0694025754928589, 1.0690544843673706, 1.0692005157470703, 1.0684163570404053, 1.0685080289840698, 1.0693464279174805, 1.0695749521255493, 1.069596290588379, 1.069434642791748, 1.0677961111068726, 1.0688271522521973, 1.0689245462417603, 1.070668339729309, 1.0691936016082764, 1.0684702396392822, 1.069445252418518, 1.070685625076294], "accuracy_train_first": 0.31885540674603174, "model": "residualv3", "loss_std": [0.27351659536361694, 0.12796659767627716, 0.1359780877828598, 0.14366558194160461, 0.14965948462486267, 0.15348203480243683, 0.15698345005512238, 0.15847180783748627, 0.16024932265281677, 0.16204409301280975, 0.16282415390014648, 0.16488702595233917, 0.16606445610523224, 0.16764627397060394, 0.16935667395591736, 0.16943427920341492, 0.16946865618228912, 0.17036186158657074, 0.1704450249671936, 0.17241479456424713, 0.17226573824882507, 0.17096543312072754, 0.1719898134469986, 0.1721714586019516, 0.17527766525745392, 0.17237821221351624, 0.17213891446590424, 0.1755855679512024, 0.17342153191566467, 0.1734681874513626, 0.17418840527534485, 0.17392891645431519, 0.17493687570095062, 0.1767033338546753, 0.17663098871707916, 0.17552852630615234, 0.17645379900932312, 0.17791011929512024, 0.17656280100345612, 0.17821715772151947, 0.17577779293060303, 0.17472733557224274, 0.1742742955684662, 0.17631815373897552, 0.17594879865646362, 0.17688709497451782, 0.17692831158638, 0.1767330914735794, 0.175805002450943, 0.17687036097049713, 0.17839352786540985, 0.17647038400173187, 0.17602437734603882, 0.1762428879737854, 0.17785508930683136, 0.17640259861946106, 0.1757984757423401, 0.1768406480550766, 0.1763581484556198, 0.17634916305541992, 0.17562906444072723, 0.17529088258743286, 0.17605428397655487, 0.17709466814994812, 0.17760683596134186, 0.17699550092220306, 0.17580503225326538, 0.17519327998161316, 0.178212970495224, 0.17750334739685059, 0.17547370493412018, 0.17698213458061218, 0.17684206366539001, 0.17763592302799225, 0.17697441577911377, 0.17787738144397736, 0.1763620674610138, 0.1753261238336563, 0.17585915327072144, 0.17797669768333435, 0.17739273607730865, 0.17678362131118774, 0.17734922468662262, 0.17495299875736237, 0.1760532259941101, 0.1761385053396225, 0.17611129581928253, 0.17671115696430206, 0.1763094812631607, 0.1765831857919693, 0.1753750890493393, 0.17576490342617035, 0.1765931248664856, 0.175721675157547, 0.1756776124238968, 0.1783815622329712, 0.17905476689338684, 0.17781195044517517, 0.17788857221603394, 0.17746104300022125, 0.17634473741054535, 0.1756378412246704, 0.1762489676475525, 0.17770981788635254, 0.17632347345352173, 0.1766427457332611, 0.17628663778305054, 0.17434561252593994, 0.17556515336036682, 0.17661191523075104, 0.17672057449817657, 0.1769210249185562, 0.1745476871728897, 0.17559750378131866, 0.17747405171394348, 0.174618199467659, 0.17524270713329315, 0.1781703233718872, 0.17781789600849152, 0.17753461003303528, 0.17551353573799133, 0.17732590436935425, 0.17693671584129333, 0.17607083916664124, 0.1766798347234726, 0.17587591707706451, 0.1758122593164444, 0.1744929999113083, 0.176114022731781, 0.17760051786899567, 0.1766403168439865, 0.1760488897562027, 0.1757633090019226, 0.1779814213514328, 0.17731647193431854, 0.1761579066514969, 0.1775207668542862, 0.1759992092847824, 0.17532312870025635, 0.17717058956623077, 0.17765606939792633, 0.17539970576763153, 0.17546319961547852, 0.17644938826560974, 0.17576782405376434, 0.17718829214572906, 0.1755049228668213, 0.1750660538673401, 0.1746809333562851, 0.17680299282073975, 0.17618535459041595, 0.1747562736272812, 0.17643365263938904, 0.1766296625137329, 0.1760568469762802, 0.17803257703781128, 0.1771022081375122, 0.17506879568099976, 0.17570680379867554, 0.1752977818250656, 0.17681048810482025, 0.17689518630504608, 0.17592166364192963, 0.17682893574237823, 0.17571796476840973, 0.1775839626789093, 0.1788049042224884, 0.17770352959632874, 0.17718133330345154, 0.17609834671020508, 0.17599768936634064, 0.1782943308353424, 0.1764029711484909, 0.1773233413696289, 0.17917309701442719, 0.17624631524085999, 0.17732010781764984, 0.17682099342346191, 0.17479954659938812, 0.17657537758350372, 0.17610037326812744, 0.17594891786575317, 0.1754700094461441, 0.17453646659851074, 0.1752796471118927, 0.17518356442451477, 0.1747763454914093, 0.1751914769411087, 0.17543219029903412, 0.1776103377342224, 0.17386175692081451, 0.17587067186832428, 0.17559297382831573, 0.1759900599718094, 0.17604732513427734, 0.17676833271980286, 0.17593002319335938, 0.17663197219371796, 0.17597325146198273, 0.17612382769584656, 0.17636743187904358, 0.17504149675369263, 0.176333948969841, 0.17492690682411194, 0.17760686576366425, 0.17505992949008942, 0.17697645723819733, 0.17619159817695618, 0.17536687850952148, 0.1759079545736313, 0.17624615132808685, 0.17673996090888977, 0.17460845410823822, 0.17460837960243225, 0.17525166273117065, 0.17557545006275177, 0.17532172799110413, 0.17462287843227386, 0.17660892009735107, 0.17554214596748352, 0.17642073333263397, 0.17726780474185944, 0.17527075111865997, 0.17630831897258759, 0.17701396346092224, 0.17813262343406677, 0.17631959915161133]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:12 2016", "state": "available"}], "summary": "1e6c0676fdf0574b61648e5b044109b3"}