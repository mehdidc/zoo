{"content": {"hp_model": {"f0": 64, "f1": 64, "f2": 64, "f3": 32, "nonlin": "rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.012237175379683789, 0.0050272930946039275, 0.00994733745317282, 0.01102955254883862, 0.013656366618463284, 0.011666374539544798, 0.011532291359964334, 0.010862684337597513, 0.008617096297692889, 0.006825232651143458, 0.01260877207987187, 0.01191118547388486, 0.011532291359964334, 0.009485622797885361, 0.008781498883554088, 0.007082593861001477, 0.004722071583672455, 0.011532291359964334, 0.008302981185360877, 0.005392330586914734, 0.007524116690718114, 0.00612501398579851, 0.00612501398579851, 0.00612501398579851, 0.007185505070343625, 0.008929272568610954, 0.010222799141033502, 0.0056772607419089734, 0.01326077352201813, 0.009448436936240432, 0.01039330592979333, 0.00632969160987774, 0.008694451874599614, 0.010906483584192522, 0.013009734768888684, 0.012301616953893204, 0.010765162064326546, 0.011291207434322464, 0.013152275224729986, 0.006563047299444022, 0.017200232631903324, 0.006952190554669636, 0.007093509519030648, 0.013681154389065418, 0.013072819268284627, 0.01571639170455031, 0.005066585665392765, 0.010710465167547914, 0.011660453336502305, 0.01276215944154053, 0.012166353610784037, 0.012465087533726943, 0.011894217393075801, 0.007197266685531101, 0.012294693581964682, 0.006680953544794155, 0.004248478752978927, 0.014666471963459927, 0.009800038617661261, 0.01618130308890456, 0.008261540170545052, 0.004969538147348702, 0.018645737230982617, 0.012273267438411898, 0.009561362987668999, 0.006251275341791707, 0.016960162119011857, 0.01068781503696479, 0.005442963441208312, 0.011870020366520424, 0.011687376801143106, 0.008163085574298044, 0.009217782106984434, 0.018501072369767467, 0.010976719901243142, 0.007752926468161471, 0.006224750280751502, 0.010622143749572733, 0.006867635092131853, 0.009898101413205117, 0.010877938186228979, 0.011796549440574948, 0.011406447649331535, 0.010678812467641704, 0.011532291359964334, 0.011339955269901092, 0.012162112261874216, 0.012451151723283697, 0.005947740613179011, 0.011561987566597284, 0.013148326580195473, 0.010964046170042431, 0.007005993489744526, 0.010888281921638232, 0.013821503748165006, 0.01108715881578186, 0.015777436737786196, 0.013069189799206397, 0.011814749922603831, 0.011532291359964334, 0.011041236156855289, 0.010002995637568757, 0.0034445498405019283, 0.00486180502468796, 0.009610562903007797, 0.00689325740041113, 0.005417677119262874, 0.009189368375404969, 0.009746570134728252, 0.010281094259062975, 0.011752123026306667, 0.01067450773117996, 0.012438561295524566, 0.01720737008229036, 0.015905755258506817, 0.014766573464475467, 0.015568197844958439, 0.012661963691436526, 0.010175567250610203, 0.012301616953893204, 0.01287329050447596, 0.012516457262611155, 0.011388954355405753, 0.012353521126478792, 0.01296442869973132, 0.010741721847055088, 0.012162672833511532, 0.00920409260399317, 0.011123390200374906, 0.008296057633056915, 0.011532291359964334, 0.011458583720183253, 0.010368614063130395, 0.009933183641876411, 0.014303604857112297, 0.017060788209860102, 0.007972641810650062, 0.011532291359964334, 0.011532291359964334, 0.009704768347558217, 0.005124648004285856, 0.011376995393388092, 0.011532291359964334, 0.011532291359964334, 0.011532291359964334, 0.011532291359964334, 0.00966250702491241, 0.012152716707195883, 0.009746570134728252, 0.009746570134728252, 0.010520714203375692, 0.00982139060222573, 0.008481728318224328, 0.007357424280490342, 0.006961521933832714, 0.003391006774586151, 0.010059376409689449, 0.005686275857023755, 0.008081341868369635, 0.01251835394102466, 0.014789939361021999, 0.007889442652583607, 0.0071406316175495746, 0.004973835778845233, 0.004224408040356246, 0.012952165539895056, 0.012692329719711676, 0.01565661853352636, 0.00906834635897445, 0.00906834635897445, 0.010002716252992174, 0.010941850399708, 0.010923255396165135, 0.00906834635897445, 0.012712899882739449, 0.006247196566882533, 0.009670233331339286, 0.009746570134728252, 0.007690715164618003, 0.010465553239538648, 0.011366829022459765, 0.012301616953893204, 0.012301616953893204, 0.012301616953893204, 0.012301616953893204], "moving_avg_accuracy_train": [0.010095886252999259, 0.025359383291228308, 0.03507256320799879, 0.041624495442708326, 0.04949943140738914, 0.05726563698388906, 0.06158631165759318, 0.06545202786401917, 0.07088346832697126, 0.07661608215203529, 0.07890874240536258, 0.08156025508689167, 0.08342589131339076, 0.08524240003873311, 0.09052712869257483, 0.0915034134931217, 0.09379521955752308, 0.09443735933695903, 0.09498066024803979, 0.09582054829415682, 0.09612318771308, 0.09643974101749182, 0.09672463899146247, 0.09698104716803604, 0.09756523714599988, 0.10314022197352798, 0.10345723074355245, 0.10687586591370625, 0.10638643607413518, 0.10669024917930012, 0.1079816275178762, 0.11221713796768511, 0.11104520398473167, 0.11394245935107024, 0.11560154475940176, 0.11407217236979418, 0.11338412546204141, 0.11211616672720677, 0.11489473612406896, 0.11339229935643874, 0.11760096099748368, 0.11597681941277777, 0.11628159225186174, 0.11819014298076379, 0.11695526013686793, 0.12399228649490501, 0.12182884956399148, 0.1253677376615329, 0.12463093188402856, 0.12930791647455703, 0.12929323634195922, 0.1264551486631435, 0.12387761826411413, 0.12151652895869755, 0.12155307180502972, 0.11949168621690402, 0.11848039610781051, 0.11944848661307782, 0.12644055107700075, 0.13121497877545607, 0.13286226737931506, 0.12945604854328463, 0.13334349367857412, 0.13066760842916447, 0.12789229868097857, 0.12508076901360052, 0.12768105846629177, 0.12537882592392616, 0.12556428378811346, 0.12923451019427812, 0.1302666203540216, 0.12756643084076782, 0.12671703702994686, 0.13351466948074509, 0.1315078739404779, 0.13873773683828133, 0.13578320442775516, 0.13270330937257654, 0.12970604463248572, 0.13360133063273844, 0.13311356384276654, 0.14016236399688042, 0.13618867962284759, 0.13710622745629003, 0.13341726644584928, 0.1317523830494648, 0.1382838984186139, 0.13682746317254432, 0.13827161230217988, 0.14275146516272416, 0.1432960781575721, 0.13945069249482336, 0.14139441047656104, 0.14884709434815557, 0.15482216005522817, 0.1563825097739117, 0.16233434695751167, 0.15624119263505323, 0.15068284083647962, 0.14563621848801991, 0.1493330356561522, 0.15332752486462337, 0.1505855072549606, 0.1454814773397266, 0.14625321240383443, 0.1451717207592428, 0.14106694356496305, 0.14359369688865314, 0.13921896679811746, 0.13564203568447403, 0.14365350074951685, 0.1401353123347719, 0.14256674753263468, 0.14996894208654174, 0.1487689942545487, 0.14881688257364903, 0.1493922608941265, 0.1486567740765854, 0.14709282239797816, 0.14241432224451295, 0.13821529785044187, 0.13444547649101599, 0.13313324891505024, 0.12985535640749685, 0.12734270556625804, 0.13010052301489247, 0.13007136195555902, 0.12715113585713306, 0.12448093549470325, 0.12261493664115393, 0.1203751047122268, 0.11836623142262095, 0.12349776924266044, 0.12532880454153947, 0.135717421295765, 0.13766051397608145, 0.13409305191609788, 0.13070540845967638, 0.12765885449770653, 0.12769504012467084, 0.12529139196883904, 0.12973960618303856, 0.12678963244873248, 0.12413233093904749, 0.12174075958033101, 0.11958834535748616, 0.11763504071053416, 0.11748331176983327, 0.11571496989356053, 0.11412346220491507, 0.11279337578393445, 0.11153820533363254, 0.11297762506997212, 0.1117809398458745, 0.1106411441263295, 0.11061717872509375, 0.10982140740804765, 0.11077445377503173, 0.11121846321249718, 0.11479764961780707, 0.11950926567185324, 0.11837527383035082, 0.11924441432522272, 0.11753712737198246, 0.11600060516288499, 0.11707029853410111, 0.1210455502536884, 0.12685262587500654, 0.12423109399417809, 0.12187171530143248, 0.12022246063982929, 0.12381771642154588, 0.12616999822392802, 0.12361672910820742, 0.12357382076110945, 0.12365585864508526, 0.12140047041462067, 0.11924041267386919, 0.12314971555446122, 0.12129360580803135, 0.1195716653718849, 0.11764528092102901, 0.1159115349152587, 0.11435116351006543, 0.11294682924539148], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 951459146, "moving_var_accuracy_train": [0.000917342273101495, 0.0029223771223155885, 0.0034792521869440114, 0.0035176773123236434, 0.0037240411291216886, 0.0038944625577176455, 0.0036730303686697747, 0.003440221187900418, 0.003361703978633715, 0.0033212993321761817, 0.0030364760182932443, 0.002796103091966706, 0.0025478181695366676, 0.0023227336881262053, 0.0023418155318162054, 0.0021162121667405945, 0.0019518623253979774, 0.0017603871843251862, 0.0015870050488124982, 0.0014346532513013408, 0.0012920122417321822, 0.0011637128715097703, 0.0010480720860589466, 0.000943856583830176, 0.0008525424268203389, 0.001047012286582822, 0.0009432155089669917, 0.0009540775559098048, 0.0008608256744295874, 0.0007755738286124583, 0.0007130253678713041, 0.0008031787700181379, 0.0007352217563599344, 0.0007372463786439399, 0.0006882948203087929, 0.0006405161574327597, 0.0005807252186148969, 0.0005371221709325982, 0.0005528939848779506, 0.0005179204325566999, 0.0006255438845802579, 0.0005867300191067715, 0.0005288929955470842, 0.0005087867889555087, 0.00047163253080329296, 0.0008701469373963422, 0.0008252563778430733, 0.000855444300761049, 0.0007747858154688182, 0.0008941748976623044, 0.0008047593474527117, 0.0007967760877612502, 0.0007768914456064099, 0.0007493749854191419, 0.0006744495052937902, 0.0006452483496508027, 0.0005899278838484756, 0.0005393698885011265, 0.0009254335888598988, 0.0010380466686039063, 0.0009586640394431484, 0.0009672185763291904, 0.0010065067858152437, 0.0009702993640457924, 0.0009425905254265953, 0.0009194737645188574, 0.0008883799352069684, 0.0008472444137984166, 0.0007628295239930752, 0.000807781628446343, 0.0007365907280383199, 0.0007285508659018584, 0.0006621890079244213, 0.0010118403695572851, 0.0009469013876654836, 0.0013226495065882492, 0.0012689479118130709, 0.001227424902589987, 0.0011855347756307127, 0.001203540575281525, 0.001085327765725968, 0.001423965241667096, 0.0014236802250402797, 0.0012888892487761463, 0.0012824762239274994, 0.00117917513204676, 0.0014452038559989652, 0.0013197743030330129, 0.001206566973107356, 0.0012665320106657618, 0.0011425482394266007, 0.0011613763335414013, 0.0010792410565200346, 0.0014711994228773604, 0.0016453921724241386, 0.0015027651763830868, 0.0016713079514855274, 0.0018383159227126204, 0.0019325418028917172, 0.0019685031967543016, 0.0018946509916502512, 0.0018487893888145589, 0.00173157839507841, 0.0017928806479509986, 0.0016189527582384602, 0.001467584100010508, 0.0014724684523415702, 0.0013826819483364243, 0.001416658123788126, 0.0013901422371350664, 0.0018287801658171733, 0.0017573009967302672, 0.0016347777911498952, 0.0019644323699599293, 0.0017809480061594797, 0.0016028738451634881, 0.0014455660025522188, 0.0013058778700259877, 0.0011973035867005543, 0.0012745685012042656, 0.001305797903841875, 0.0013031220911955397, 0.0011883073529761104, 0.0011661778312981705, 0.0011063807764181558, 0.0010641927124962732, 0.0009577810945530789, 0.0009387524692911242, 0.0009090469521416942, 0.0008494798224205512, 0.0008096834638070691, 0.0007650352644695885, 0.0009255258616090907, 0.0008631474878398507, 0.0017481429616514439, 0.0016073091479649937, 0.0015611193031132937, 0.0015082925264924849, 0.0014409966932319843, 0.0012969088085051752, 0.00121921564776796, 0.0012753735702498241, 0.001226157318522703, 0.0011670928484907998, 0.0011018600857162172, 0.0010333700600249382, 0.0009643716454166834, 0.000868141675918031, 0.0008094708052486457, 0.0007513197952309396, 0.0006921099846933389, 0.0006370780619578046, 0.0005920176183582995, 0.0005457043562526317, 0.0005028261291680065, 0.0004525486853153134, 0.0004129930846850817, 0.0003798684526151676, 0.00034365590677867635, 0.00042458549401640537, 0.0005819208771814748, 0.0005353022269326736, 0.0004885706510378417, 0.0004659470446003969, 0.0004406004446318052, 0.000406838595344438, 0.0005083783719167307, 0.0007610396801704235, 0.0007467875767731808, 0.0007222088294379004, 0.0006744683149434893, 0.0007233542606728395, 0.0007008179017059191, 0.0006894087601309611, 0.0006204844542541211, 0.000558496580758374, 0.0005484279073136006, 0.0005355777615726637, 0.0006195638265252444, 0.000588613734389848, 0.0005564380707415887, 0.0005341928771399241, 0.0005078264663386521, 0.00047895665000409043, 0.00044881037754611726], "duration": 71105.594399, "accuracy_train": [0.10095886252999262, 0.16273085663528977, 0.12249118245893319, 0.10059188555509413, 0.12037385508951644, 0.12716148717238834, 0.10047238372093023, 0.10024347372185308, 0.11976643249354006, 0.12820960657761166, 0.09954268468530823, 0.10542386922065339, 0.10021661735188261, 0.10159097856681433, 0.13808968657715023, 0.10028997669804356, 0.11442147413713548, 0.10021661735188261, 0.09987036844776671, 0.10337954070921003, 0.0988469424833887, 0.09928872075719822, 0.09928872075719822, 0.09928872075719822, 0.10282294694767441, 0.15331508542128092, 0.10631030967377261, 0.13764358244509042, 0.10198156751799556, 0.10942456712578442, 0.1196040325650609, 0.15033673201596529, 0.1004977981381506, 0.14001775764811739, 0.1305333134343854, 0.10030782086332596, 0.10719170329226653, 0.10070453811369509, 0.13990186069582872, 0.09987036844776671, 0.15547891576688816, 0.1013595451504245, 0.11902454780361757, 0.13536709954088225, 0.10584131454180509, 0.18732552371723882, 0.10235791718576966, 0.1572177305394057, 0.11799967988648948, 0.1714007777893134, 0.1291611151485788, 0.10091235955380215, 0.10067984467284977, 0.10026672520994831, 0.1218819574220192, 0.10093921592377261, 0.10937878512596899, 0.12816130116048358, 0.18936913125230714, 0.17418482806155408, 0.14768786481404578, 0.09880007901901071, 0.1683304998961794, 0.10658464118447766, 0.10291451094730528, 0.09977700200719822, 0.1510836635405131, 0.10465873304263566, 0.12723340456579918, 0.16226654784976008, 0.1395556117917128, 0.10326472522148393, 0.11907249273255814, 0.19469336153792913, 0.11344671407807308, 0.20380650291851238, 0.10919241273301956, 0.10498425387596899, 0.10273066197166851, 0.16865890463501293, 0.12872366273301955, 0.2036015653839055, 0.10042552025655224, 0.14536415795727206, 0.10021661735188261, 0.11676843248200444, 0.19706753674095606, 0.12371954595791805, 0.15126895446889996, 0.18307014090762275, 0.1481975951112034, 0.10484222153008489, 0.15888787231220008, 0.2159212491925065, 0.2085977514188815, 0.1704256572420635, 0.21590088160991142, 0.10140280373292729, 0.10065767464931709, 0.10021661735188261, 0.18260439016934293, 0.18927792774086377, 0.12590734876799556, 0.09954520810262088, 0.15319882798080472, 0.13543829595791804, 0.10412394881644518, 0.16633447680186414, 0.09984639598329642, 0.10344965566168328, 0.2157566863349022, 0.10847161660206718, 0.16444966431339977, 0.21658869307170542, 0.1379694637666113, 0.14924787744555187, 0.15457066577842377, 0.14203739271871538, 0.1330172572905131, 0.10030782086332596, 0.10042407830380215, 0.1005170842561831, 0.12132320073135842, 0.10035432383951644, 0.10472884799510888, 0.15492088005260243, 0.1298089124215578, 0.10086910097129936, 0.10044913223283498, 0.10582094695921003, 0.10021661735188261, 0.10028637181616833, 0.16968160962301587, 0.14180812223145073, 0.22921497208379477, 0.1551483480989295, 0.10198589337624583, 0.10021661735188261, 0.10023986883997785, 0.12802071076734958, 0.10365855856635289, 0.16977353411083426, 0.10023986883997785, 0.10021661735188261, 0.10021661735188261, 0.10021661735188261, 0.10005529888796603, 0.11611775130352528, 0.09979989300710594, 0.09979989300710594, 0.10082259799510888, 0.10024167128091548, 0.12593240269702843, 0.10101077282899593, 0.1003829826504245, 0.11040149011397193, 0.1026594655546327, 0.11935187107788851, 0.11521454814968622, 0.14701032726559615, 0.16191381015826872, 0.10816934725682908, 0.12706667877906977, 0.10217154479282023, 0.10217190528100775, 0.12669753887504615, 0.15682281572997417, 0.17911630646686968, 0.10063730706672203, 0.10063730706672203, 0.10537916868540051, 0.1561750184569952, 0.1473405344453673, 0.10063730706672203, 0.12318764563722776, 0.12439419960086748, 0.10110197634043928, 0.09979989300710594, 0.1583334414797896, 0.10458861809016241, 0.104074201446567, 0.10030782086332596, 0.10030782086332596, 0.10030782086332596, 0.10030782086332596], "end": "2016-01-30 06:32:45.756000", "learning_rate_per_epoch": [0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838, 0.005840875208377838], "accuracy_valid": [0.09906373541039157, 0.16033567865210843, 0.11667421639683735, 0.09964467243975904, 0.12136436370481928, 0.1235910438629518, 0.09818865304969879, 0.09988881306475904, 0.12148496329066265, 0.13414056617093373, 0.09658114881400602, 0.10370240728539157, 0.09818865304969879, 0.09787391754518072, 0.14171922063253012, 0.09710031532379518, 0.11524908226656627, 0.09818865304969879, 0.09941965126129518, 0.1063070641942771, 0.10445541933358433, 0.10361122223268072, 0.10361122223268072, 0.10361122223268072, 0.1070394860692771, 0.14779185099774095, 0.10847491528614459, 0.13658197242093373, 0.10004029791039157, 0.11239146037274096, 0.11829054499246988, 0.15253347373870482, 0.10003000282379518, 0.14130300498870482, 0.12822089137801204, 0.09881959478539157, 0.10370240728539157, 0.09868722938629518, 0.14415180252259036, 0.10145513695406627, 0.15618675875376506, 0.09881959478539157, 0.12155702889683735, 0.1311594032379518, 0.10422157379518072, 0.1927990281438253, 0.10597173851656627, 0.15245111304593373, 0.11720220726656627, 0.16697895096009036, 0.12442494587725904, 0.09942994634789157, 0.09956231174698796, 0.09697824501129518, 0.11999952936746988, 0.10495399567018072, 0.10962355280496988, 0.12719285344503012, 0.18313341255647592, 0.1772019719503012, 0.1503465032003012, 0.10324501129518072, 0.1662759436182229, 0.10579819277108433, 0.1051584266754518, 0.10385536285768072, 0.14669468891189757, 0.1061452842620482, 0.12251300122364459, 0.15619558311370482, 0.1336008094879518, 0.09810776308358433, 0.11393719408885541, 0.19070324265813254, 0.11446665568524096, 0.2031029391001506, 0.11308270190135541, 0.1060232139495482, 0.09795480751129518, 0.16563617752259036, 0.12462937688253012, 0.20314411944653615, 0.09831072336219879, 0.14555487575301204, 0.09818865304969879, 0.11451666039156627, 0.18968402908509036, 0.1194906579442771, 0.15106862998870482, 0.1764695500753012, 0.13979551016566266, 0.10384506777108433, 0.1589620199548193, 0.2134686205760542, 0.2089005435805723, 0.17592096903237953, 0.20955354621611444, 0.09864604903990964, 0.09843279367469879, 0.09818865304969879, 0.1817185735128012, 0.18761912885918675, 0.12922686841114459, 0.10290968561746988, 0.15233933782003012, 0.13974550545933734, 0.10878965079066265, 0.16966449783509036, 0.10282879565135541, 0.1066335655120482, 0.2094505953501506, 0.10346856174698796, 0.15836196347891568, 0.20756806522966867, 0.1361348715173193, 0.14493422910391568, 0.1486154579254518, 0.1372629188629518, 0.13045786662274095, 0.09881959478539157, 0.09918580572289157, 0.09918580572289157, 0.11926710749246988, 0.09906373541039157, 0.10049769390060241, 0.15069212396460843, 0.1317697548004518, 0.09577813205948796, 0.09819894813629518, 0.10251258942018072, 0.09818865304969879, 0.09831072336219879, 0.16167845208960843, 0.14164715502635541, 0.22369164156626506, 0.15278643872364459, 0.10272584478539157, 0.09818865304969879, 0.09818865304969879, 0.13053875658885541, 0.10769101797816265, 0.16888060052710843, 0.09818865304969879, 0.09818865304969879, 0.09818865304969879, 0.09818865304969879, 0.10307293627635541, 0.11583884365587349, 0.10282879565135541, 0.10282879565135541, 0.1035715126129518, 0.10295086596385541, 0.1177419639495482, 0.09746652626129518, 0.09697824501129518, 0.10902349632906627, 0.1047922157379518, 0.12361163403614459, 0.11355039297816265, 0.15099656438253012, 0.16510671592620482, 0.11439459007906627, 0.12937982398343373, 0.10570700771837349, 0.10571730280496988, 0.1267148672816265, 0.1572427404932229, 0.1717691076807229, 0.09602227268448796, 0.09602227268448796, 0.1017904626317771, 0.15497487998870482, 0.1416677451995482, 0.09602227268448796, 0.12065106127635541, 0.11940976797816265, 0.10404949877635541, 0.10282879565135541, 0.15863698936370482, 0.1083425498870482, 0.10236992893448796, 0.09881959478539157, 0.09881959478539157, 0.09881959478539157, 0.09881959478539157], "accuracy_test": 0.10025709502551021, "start": "2016-01-29 10:47:40.161000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0], "accuracy_train_last": 0.10030782086332596, "batch_size_eval": 1024, "accuracy_train_std": [0.010654559953339209, 0.01157613675448441, 0.009853807756964324, 0.010899537160121474, 0.010685069725906556, 0.009668481852421543, 0.009794381057452653, 0.010744810539114824, 0.009960003253524922, 0.011826692566004353, 0.009720404960731007, 0.009999354934035686, 0.009772983806395252, 0.009557357347875843, 0.011291235426137544, 0.010599393418454952, 0.011551194520960806, 0.009772983806395252, 0.010488392299436093, 0.01080550896767861, 0.010241192528303885, 0.010585789311474773, 0.010585789311474773, 0.010585789311474773, 0.010630443229152738, 0.012848252919369335, 0.010145277094795957, 0.011014311962864988, 0.010985933413950408, 0.008337826559942635, 0.010704553899502672, 0.011635561452019045, 0.010510194934645063, 0.012317677997950279, 0.009088126959957485, 0.010563294810593671, 0.010127104361216811, 0.009847601988270666, 0.009578500223068649, 0.008712127349257208, 0.011333037695587994, 0.010652283093092388, 0.008559337841742289, 0.012079636501952613, 0.011053349707558587, 0.012157082957431451, 0.01065402415192791, 0.010279877169623133, 0.010767082420989059, 0.01303931690732565, 0.011592182268843084, 0.010591967600002646, 0.01063080991470442, 0.010554708397880898, 0.010833979969040548, 0.010614112696983077, 0.011250566299619449, 0.010680163639294065, 0.012707059668890906, 0.014767856015060917, 0.011142623663698066, 0.010604523865976162, 0.010512440641889593, 0.010993776465229137, 0.009944182353699826, 0.010686437667858808, 0.00996503136432206, 0.00966019744491938, 0.011354199348419398, 0.012280132819956852, 0.01037812780879405, 0.009371921607119095, 0.010315593820188897, 0.01058912735044655, 0.010358130166307688, 0.010498023177192754, 0.01120331230314517, 0.00957112280276463, 0.010778689564431402, 0.012470675137450125, 0.011308322160966056, 0.013720258775253659, 0.009888429221130828, 0.013272775693338745, 0.009772983806395252, 0.011730132547846772, 0.0129659134477044, 0.010714188352341341, 0.009206371031460768, 0.012388370424113494, 0.011353392944177613, 0.009579142037076178, 0.01183532351468767, 0.015020032640238762, 0.010731708974465091, 0.01091303143674331, 0.013267013345794063, 0.01028843655279063, 0.009983125058351161, 0.009772983806395252, 0.012360995571385862, 0.01373378724680102, 0.010052749289448772, 0.010704574001566375, 0.010818436029462525, 0.010607345855160379, 0.01103008776652838, 0.011492255136990683, 0.009653315106078865, 0.00985952855320527, 0.010961284401722733, 0.009853811164405805, 0.00906758365659497, 0.011536846718280959, 0.009953176377183967, 0.009985859337626584, 0.012161091789702986, 0.011478456652216336, 0.011738692497560667, 0.010563294810593671, 0.010583967061199092, 0.010681076921529385, 0.010685137366131643, 0.010566565870427036, 0.010158632997094139, 0.012128943188233227, 0.010269254774516982, 0.009130198805334269, 0.00970909379345801, 0.009278999845836241, 0.009772983806395252, 0.00979741263540823, 0.011757022215487046, 0.011812927216873024, 0.01253179142063386, 0.011369402119276298, 0.010460418295155835, 0.009772983806395252, 0.009791243246113582, 0.01002297401667825, 0.011001179067394432, 0.011486744298882268, 0.009768025065370415, 0.009772983806395252, 0.009772983806395252, 0.009772983806395252, 0.009598344353080464, 0.010006178117916938, 0.009663748062914599, 0.009663748062914599, 0.009656051894502483, 0.009704769870849644, 0.011423518047901541, 0.010527285362528574, 0.010547901042262595, 0.010721805797239695, 0.00947339133152645, 0.012216039522693454, 0.011438664867871547, 0.014200148460517373, 0.0136354202769529, 0.011319704444631356, 0.012472852009610205, 0.010669347164787913, 0.010868725700403793, 0.01139064794765655, 0.012833843940244317, 0.010875282337860767, 0.009243740029823868, 0.009243740029823868, 0.009760930387572682, 0.01092261698798277, 0.009454196889146768, 0.009243740029823868, 0.009435197138456284, 0.009996663357688016, 0.009719372689964563, 0.009663748062914599, 0.011880363584624687, 0.010184694712631854, 0.010309826816918418, 0.010563294810593671, 0.010563294810593671, 0.010563294810593671, 0.010563294810593671], "accuracy_test_std": 0.007787554219822601, "error_valid": [0.9009362645896084, 0.8396643213478916, 0.8833257836031626, 0.900355327560241, 0.8786356362951807, 0.8764089561370482, 0.9018113469503012, 0.900111186935241, 0.8785150367093374, 0.8658594338290663, 0.903418851185994, 0.8962975927146084, 0.9018113469503012, 0.9021260824548193, 0.8582807793674698, 0.9028996846762049, 0.8847509177334337, 0.9018113469503012, 0.9005803487387049, 0.8936929358057228, 0.8955445806664156, 0.8963887777673193, 0.8963887777673193, 0.8963887777673193, 0.8929605139307228, 0.852208149002259, 0.8915250847138554, 0.8634180275790663, 0.8999597020896084, 0.887608539627259, 0.8817094550075302, 0.8474665262612951, 0.8999699971762049, 0.8586969950112951, 0.8717791086219879, 0.9011804052146084, 0.8962975927146084, 0.9013127706137049, 0.8558481974774097, 0.8985448630459337, 0.8438132412462349, 0.9011804052146084, 0.8784429711031626, 0.8688405967620482, 0.8957784262048193, 0.8072009718561747, 0.8940282614834337, 0.8475488869540663, 0.8827977927334337, 0.8330210490399097, 0.875575054122741, 0.9005700536521084, 0.9004376882530121, 0.9030217549887049, 0.8800004706325302, 0.8950460043298193, 0.8903764471950302, 0.8728071465549698, 0.8168665874435241, 0.8227980280496988, 0.8496534967996988, 0.8967549887048193, 0.8337240563817772, 0.8942018072289156, 0.8948415733245482, 0.8961446371423193, 0.8533053110881024, 0.8938547157379518, 0.8774869987763554, 0.8438044168862951, 0.8663991905120482, 0.9018922369164156, 0.8860628059111446, 0.8092967573418675, 0.885533344314759, 0.7968970608998494, 0.8869172980986446, 0.8939767860504518, 0.9020451924887049, 0.8343638224774097, 0.8753706231174698, 0.7968558805534638, 0.9016892766378012, 0.8544451242469879, 0.9018113469503012, 0.8854833396084337, 0.8103159709149097, 0.8805093420557228, 0.8489313700112951, 0.8235304499246988, 0.8602044898343373, 0.8961549322289156, 0.8410379800451807, 0.7865313794239458, 0.7910994564194277, 0.8240790309676205, 0.7904464537838856, 0.9013539509600903, 0.9015672063253012, 0.9018113469503012, 0.8182814264871988, 0.8123808711408133, 0.8707731315888554, 0.8970903143825302, 0.8476606621799698, 0.8602544945406627, 0.8912103492093374, 0.8303355021649097, 0.8971712043486446, 0.8933664344879518, 0.7905494046498494, 0.8965314382530121, 0.8416380365210843, 0.7924319347703314, 0.8638651284826807, 0.8550657708960843, 0.8513845420745482, 0.8627370811370482, 0.869542133377259, 0.9011804052146084, 0.9008141942771084, 0.9008141942771084, 0.8807328925075302, 0.9009362645896084, 0.8995023060993976, 0.8493078760353916, 0.8682302451995482, 0.9042218679405121, 0.9018010518637049, 0.8974874105798193, 0.9018113469503012, 0.9016892766378012, 0.8383215479103916, 0.8583528449736446, 0.7763083584337349, 0.8472135612763554, 0.8972741552146084, 0.9018113469503012, 0.9018113469503012, 0.8694612434111446, 0.8923089820218374, 0.8311193994728916, 0.9018113469503012, 0.9018113469503012, 0.9018113469503012, 0.9018113469503012, 0.8969270637236446, 0.8841611563441265, 0.8971712043486446, 0.8971712043486446, 0.8964284873870482, 0.8970491340361446, 0.8822580360504518, 0.9025334737387049, 0.9030217549887049, 0.8909765036709337, 0.8952077842620482, 0.8763883659638554, 0.8864496070218374, 0.8490034356174698, 0.8348932840737951, 0.8856054099209337, 0.8706201760165663, 0.8942929922816265, 0.8942826971950302, 0.8732851327183735, 0.8427572595067772, 0.8282308923192772, 0.9039777273155121, 0.9039777273155121, 0.8982095373682228, 0.8450251200112951, 0.8583322548004518, 0.9039777273155121, 0.8793489387236446, 0.8805902320218374, 0.8959505012236446, 0.8971712043486446, 0.8413630106362951, 0.8916574501129518, 0.8976300710655121, 0.9011804052146084, 0.9011804052146084, 0.9011804052146084, 0.9011804052146084], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "momentum": 0.7302204421594276, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.005840875294964832, "patience_threshold": 1, "do_flip": true, "batch_size": 128, "l2_decay": 7.98439917149294e-07, "optimization": "santa_sss", "nb_data_augmentation": 0, "learning_rate_decay_method": "none", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.037077265931297634}, "accuracy_valid_max": 0.22369164156626506, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop', 'santa_sss'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.09881959478539157, "loss_train": [187331.328125, 378714.1875, 232897.25, 149894.109375, 241605.09375, 291595.96875, 261058.0, 241035.921875, 260688.34375, 287050.0, 293574.375, 297941.4375, 307843.9375, 324598.4375, 343954.84375, 354272.34375, 375381.03125, 391032.875, 382282.75, 418406.6875, 465844.75, 495575.625, 514468.65625, 551782.3125, 589391.125, 612538.0625, 648699.5, 678967.9375, 719801.5625, 779131.875, 812602.4375, 861677.375, 881021.0, 922709.5, 989356.0, 998719.8125, 1040054.5625, 1099304.0, 1179712.375, 1197063.75, 1278319.875, 1390689.875, 1425840.875, 1513530.375, 1551726.875, 1643363.125, 1746122.75, 1807312.75, 1857445.25, 1816463.0, 1877746.5, 1967898.75, 1931291.25, 2055306.0, 2122494.75, 2183057.5, 2187954.25, 2292316.25, 2250872.0, 2309422.5, 2483545.75, 2500813.0, 2626824.5, 2873745.0, 2744712.75, 2666311.5, 2629892.0, 2557706.5, 2558565.75, 2700644.25, 2671006.25, 2764916.75, 2721555.5, 2825402.5, 2912886.75, 2883785.25, 2923670.25, 2984743.25, 2949187.75, 3113996.75, 3166870.0, 3253953.75, 3219041.0, 3209789.5, 3335403.25, 3279326.5, 3490760.0, 3594832.0, 3801487.75, 3694378.25, 3883216.25, 3861315.75, 3823172.5, 3823773.5, 3878497.75, 3836470.5, 3813750.75, 3872628.0, 3861108.0, 3758170.25, 3841978.75, 3730327.5, 3649669.0, 3639089.5, 3738046.75, 3745882.0, 3541390.5, 3618350.25, 3548689.75, 3581852.75, 3553448.25, 3589354.5, 3623141.0, 3613310.5, 3687462.0, 3804125.75, 3916584.25, 3807763.25, 3879570.0, 3904176.75, 3985092.75, 3957820.0, 4011817.5, 3895410.5, 3829867.75, 3923676.75, 4038148.5, 4065980.25, 4174141.5, 4147574.75, 4193096.0, 4217186.5, 4303787.5, 4213052.5, 4097910.0, 4080634.5, 4124397.25, 4165409.0, 4099893.0, 4125323.25, 4163048.25, 4263954.5, 4405185.5, 4504921.0, 4511248.0, 4549217.0, 4478067.5, 4487122.0, 4366836.0, 4406021.0, 4356386.5, 4418014.0, 4512562.5, 4465980.5, 4421231.5, 4503145.0, 4757715.5, 4869725.5, 4872349.5, 4791493.0, 4813146.5, 4886706.5, 4804731.5, 4830173.5, 4877672.0, 4826094.0, 4852723.5, 4945706.5, 5100001.0, 5208279.5, 5270532.5, 5247745.0, 5195477.0, 5313543.5, 5503259.5, 5436594.0, 5717664.5, 5674756.0, 5415810.0, 5492173.5, 5533008.5, 5607237.0, 5787710.5, 5661705.0, 5577683.0], "accuracy_train_first": 0.10095886252999262, "model": "residualv3", "loss_std": [116021.0859375, 18543.939453125, 53278.375, 13314.8408203125, 30787.791015625, 4436.3359375, 11838.626953125, 4603.55517578125, 9269.025390625, 7079.19287109375, 7932.90283203125, 7122.7705078125, 11490.015625, 11933.58984375, 12914.89453125, 15028.8818359375, 17413.890625, 21526.537109375, 20993.294921875, 21562.3359375, 28880.44921875, 28482.16015625, 34788.04296875, 30340.60546875, 41691.9921875, 30523.244140625, 38667.58203125, 52532.828125, 49672.16796875, 75421.28125, 65219.94921875, 70816.28125, 80538.2421875, 91005.640625, 77640.65625, 122743.34375, 122611.265625, 101887.9375, 78422.0859375, 97004.5625, 114684.6640625, 167127.296875, 103660.296875, 138710.875, 167214.609375, 160479.890625, 181512.796875, 195019.6875, 189765.953125, 194555.59375, 224293.5625, 217476.0625, 144115.828125, 178370.65625, 256819.515625, 258317.90625, 256885.90625, 202322.53125, 156204.203125, 198213.4375, 190247.03125, 313998.03125, 340795.75, 331142.53125, 226026.4375, 249862.0, 228431.40625, 230424.6875, 268697.40625, 252960.4375, 193065.5625, 263212.03125, 252648.5625, 287273.28125, 315522.46875, 204216.15625, 205469.78125, 221039.1875, 293527.59375, 269932.21875, 311985.46875, 227667.28125, 260434.0, 252775.234375, 263581.0, 306600.15625, 261467.671875, 307407.1875, 391567.59375, 292903.0, 390384.59375, 341433.25, 327654.59375, 279864.375, 299402.84375, 281425.6875, 300800.96875, 369164.03125, 290420.75, 326679.71875, 332597.125, 277593.28125, 256061.734375, 309292.65625, 270820.0, 276956.6875, 206952.953125, 266248.6875, 166142.53125, 220423.28125, 214350.859375, 191628.8125, 216790.25, 208777.546875, 196270.875, 267980.15625, 269256.46875, 239398.078125, 247566.125, 244044.25, 216310.15625, 203725.59375, 258420.03125, 268490.0, 225259.921875, 252777.453125, 274870.875, 240643.265625, 242425.421875, 226690.84375, 241958.75, 216679.015625, 235196.40625, 284847.8125, 248956.9375, 251015.515625, 293847.625, 256126.0, 216034.125, 219246.59375, 178112.21875, 241912.453125, 214452.796875, 234069.53125, 222046.734375, 284994.1875, 212574.046875, 309549.53125, 270820.90625, 236462.96875, 198734.359375, 223718.4375, 298731.625, 223792.15625, 224232.875, 288128.46875, 274326.9375, 322859.1875, 389540.9375, 330886.3125, 307216.84375, 296425.40625, 304932.15625, 295529.40625, 301822.53125, 263670.53125, 293318.84375, 307236.1875, 281783.375, 265732.09375, 421775.5625, 326856.40625, 286385.46875, 334742.34375, 388109.75, 286583.96875, 435747.1875, 344224.3125, 291102.34375, 349150.09375, 331931.15625, 399780.0625, 486795.28125, 443263.0625, 259079.265625]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:22 2016", "state": "available"}], "summary": "9b08bcd732bb462f8137e8d1ed936b29"}