{"content": {"hp_model": {"f0": 32, "f1": 64, "f2": 64, "f3": 16, "nonlin": "very_leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.029265506021282178, 0.028681561691904157, 0.03153666083332305, 0.030404835265232816, 0.029910600642184263, 0.029174850539973655, 0.028371179083336938, 0.029485387975268288, 0.028189295253851485, 0.02887354237062565, 0.030420644629240136, 0.029649535303363696, 0.03037886620803493, 0.03126538200874206, 0.030465641419672573, 0.02841718643099077, 0.031011334552046365, 0.029739963968696964, 0.02920064768028234, 0.02799553878025364, 0.031155801471498865, 0.02890745510390871, 0.02847076770550949, 0.03187770660094837, 0.031776234003590656, 0.024021654107177694, 0.02658030808243453, 0.026925171214148145, 0.026421471108092277, 0.03170993103635297, 0.026816799606202143, 0.026363725200137383, 0.029235732496925106, 0.025822752082589998, 0.02755888539611425, 0.03039170426256244, 0.027508804648640892, 0.02767090783799029, 0.028435058074194573, 0.028601744482730597, 0.027628911605634503, 0.028362544397316145, 0.027142621727613028, 0.027689261160379206, 0.028821654199880326, 0.03451773086816304, 0.03338452678196454, 0.029710971246914384, 0.031558802823952445, 0.031020401723544318, 0.037016301804563945, 0.03141676667481304, 0.031055475493904406, 0.03239066448386597, 0.02906643979486938, 0.033636023197815564, 0.029938187999586297, 0.03322135753952718, 0.03651987985659171, 0.027245369347992265, 0.025524523603386712, 0.037361960824462984, 0.02904021104959584, 0.02989937652580712, 0.02872201882098631, 0.03202449738138131, 0.030365723975607717, 0.030641711392243677, 0.03202223108288891, 0.02639708199979346, 0.03089615551924706, 0.03426767554553084, 0.032464240622848285, 0.030768164533624808, 0.030958633983192898, 0.0307938052891887, 0.028325417280763664, 0.03343448901286555, 0.028257438233482864, 0.029903017241379313, 0.030089315603028927, 0.03032745964462256, 0.0307044121529112, 0.033282196821425525, 0.03327674492419147, 0.032924017171438655, 0.027749152547021746, 0.03148858509133392, 0.03755159883987677, 0.028398025761000943, 0.03235591655542878, 0.029780506359925817, 0.02969386753869966, 0.032303162918576094, 0.035277578738945804, 0.030892925500651088, 0.027746537046516667, 0.024210118182910925, 0.02904395946339464, 0.03370795758538158, 0.026234023771292855, 0.024959921263649636, 0.026202190551266873, 0.027907259368400675, 0.025626678858753176, 0.0271439586050077, 0.031011334552046365, 0.027745229203804407, 0.025993975417359514, 0.028022745335537405, 0.027283633699405604, 0.029165831702348464, 0.03321671499614698, 0.033055736315038727, 0.02985686870820776, 0.03128742603174809, 0.028027600869913693, 0.028048308372457694, 0.033141250949408604, 0.030004777924740923, 0.029471539506847725, 0.03249021816472725, 0.02518137343962716, 0.025037942187297168, 0.028542370775447686, 0.027100811077498472, 0.028159350307410038, 0.031387010607432195, 0.03079409988640659, 0.03224244580412136, 0.027080719036374407, 0.02945306475347639, 0.03228968009316753, 0.029093580353215808, 0.030579772061149035, 0.032648428073753476, 0.03448512628253561, 0.030657102693708794], "moving_avg_accuracy_train": [0.01199407003012048, 0.023214655496987945, 0.03347790380271083, 0.04240185664533132, 0.05082639236634035, 0.05795902044898343, 0.06442309581372364, 0.07017487508777297, 0.07520087402477879, 0.07943483406205995, 0.08343365110766118, 0.08736202921978664, 0.0899751297616634, 0.09280931859272597, 0.09555775420333289, 0.0982031271263731, 0.10110636486554302, 0.10386046858983208, 0.10680979447181274, 0.10896296412101701, 0.11144675054024061, 0.11252204913079487, 0.11355100009120936, 0.11600896484112456, 0.11675040645942174, 0.11778244412673257, 0.11891365001526413, 0.11926108395952086, 0.11945141005152057, 0.12022040684757333, 0.120503053662816, 0.11994324151942595, 0.12072659056025443, 0.1219516536427832, 0.12119521192308318, 0.12149333003800378, 0.12122276209444437, 0.1201132870898192, 0.1208984568748132, 0.1196849289584162, 0.12058352943606855, 0.1194626764924617, 0.11947282827092637, 0.11951490914865301, 0.12028461552294432, 0.12140327369956555, 0.12384784843804272, 0.12565969386532277, 0.12652322372577846, 0.12620853313030905, 0.1286879245461938, 0.13042285950121296, 0.13239139809928444, 0.13129928389176562, 0.13310723200861316, 0.1340190238679928, 0.13645861244504895, 0.13699761565837537, 0.138099247164827, 0.1349397366049708, 0.13215971249266648, 0.13395927211689382, 0.13694841644134903, 0.13867149648396113, 0.14138237770905898, 0.14322446749839404, 0.14603304484494017, 0.14732770722791605, 0.14777048243885937, 0.14802543720702163, 0.14833725719113874, 0.14784370466479596, 0.1490278959453043, 0.1476157878266775, 0.1484909748572628, 0.1486762335462353, 0.14584033082414188, 0.14304328945859518, 0.14053301171755495, 0.13926444322652234, 0.13768269016893037, 0.138233415880953, 0.13893614733502638, 0.14067929841477675, 0.14084094312149184, 0.1423394918816318, 0.14062436799467345, 0.14496601628556754, 0.14362594703652887, 0.14379648486299645, 0.14613841017187754, 0.14811201267878618, 0.14554196351934132, 0.14517969111319032, 0.1435688191404255, 0.14661828135288896, 0.1456612724947085, 0.1438351678355991, 0.14540844698577415, 0.14253458270888347, 0.14097879010064573, 0.1392232491929908, 0.13735617653272789, 0.13910907544572015, 0.13847706473849755, 0.13691051413814176, 0.13863503124842397, 0.13612083158743699, 0.13731014150098245, 0.1388888035557035, 0.13927420783868735, 0.1414494790728909, 0.14405434291258976, 0.1460810434104874, 0.14630492325618566, 0.14719353861129, 0.14228216667787186, 0.14646038826309674, 0.14849121314160635, 0.14667861291178305, 0.150214817885665, 0.15030534663926717, 0.1448686561018465, 0.14217584169648112, 0.13996644653285711, 0.13890984329523406, 0.138238926736795, 0.13844458978600707, 0.1384108424037919, 0.13883933647666571, 0.13852844499767383, 0.13844395516658115, 0.14045987591498327, 0.14118469028131625, 0.14167465498812437, 0.1440616887362999, 0.1472312917000193, 0.14607649837339087], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 1234, "moving_var_accuracy_train": [0.0012947194429869088, 0.0022983613426617146, 0.0030165336004598572, 0.0034316126494496994, 0.003727206603535745, 0.003812355393469948, 0.003807178287012325, 0.003724207141667543, 0.0035791324153338425, 0.003382556932176103, 0.0031882160788362117, 0.0030082838622790273, 0.002768910126028734, 0.0025643127503969395, 0.0023758665601081156, 0.0022012618852148926, 0.0020569948010246675, 0.0019195611068394873, 0.001805891704578627, 0.001667027789965054, 0.001555847765755425, 0.0014106693927095142, 0.001279131114149004, 0.0012055923191405353, 0.0010899807082865702, 0.0009905685531786488, 0.0009030283387210203, 0.0008138118979595143, 0.0007327567243552259, 0.0006648032565707581, 0.0005990419339131837, 0.0005419582472448478, 0.0004932851439982652, 0.0004574636456040128, 0.00041686711772133584, 0.00037598027564319657, 0.00033904111118761467, 0.0003162154131418452, 0.0002901422961490681, 0.0002743819165690345, 0.00025421107027806446, 0.0002400967651409874, 0.00021608801615434262, 0.00019449515174134054, 0.00018037766769082877, 0.00017360246596683973, 0.00021002573023816266, 0.00021856821188554756, 0.0002034225450760802, 0.00018397156210636423, 0.00022090084203419503, 0.000225900751514102, 0.0002381869742715667, 0.00022510269782478987, 0.00023201051558122382, 0.00021629174357658043, 0.0002482269010466477, 0.0002260189311177689, 0.00021433936579005381, 0.00028274799201163356, 0.0003240299993954104, 0.0003207727330262122, 0.0003691103138553953, 0.00035892032596908835, 0.0003891681865214724, 0.00038079102099707773, 0.0004137048793011592, 0.0003874197475440779, 0.0003504422217765034, 0.0003159830170031311, 0.00028525980062527116, 0.0002589261674290782, 0.00024565433158565855, 0.0002390353424753186, 0.00022202537927432857, 0.00020013172838345407, 0.00025249965378769997, 0.0002976606520141451, 0.00032460803584718947, 0.00030663062641043715, 0.00029848504838620804, 0.0002713662328365327, 0.00024867409302177615, 0.0002511538649011126, 0.00022627363951188242, 0.00022385711103934763, 0.0002279462494639493, 0.00037480081345396744, 0.0003534828024385426, 0.000318396270546995, 0.00033591817086369504, 0.0003373823154748098, 0.000363090458064999, 0.000327962583924825, 0.00031852050214609535, 0.0003703614299986695, 0.0003415680805905254, 0.0003374231965656625, 0.0003259577424684754, 0.00036769383115952276, 0.00035270886380219527, 0.000345175292328025, 0.00034203140596353435, 0.0003354821567597063, 0.000305528878890132, 0.0002970627180523943, 0.0002941220796200598, 0.00032160067107581694, 0.0003021707266023528, 0.00029438321888926186, 0.00026628172515241646, 0.0002822397971183559, 0.0003150836580168566, 0.0003205429263887783, 0.0002889397334176893, 0.00026715249531986503, 0.000457531414203187, 0.0005688960933200188, 0.0005491247311725789, 0.0005237819343937198, 0.0005839464515101138, 0.0005256255654561612, 0.0007390814449076625, 0.0007304345452125857, 0.0007013239335927338, 0.0006412392338492602, 0.0005811664717198237, 0.0005234305001561423, 0.0004710977001127855, 0.0004256403946358989, 0.0003839462367776968, 0.00034561585988394976, 0.00034762970207009847, 0.00031759493465387305, 0.000287996029913744, 0.00031047779795672926, 0.0003698474646896259, 0.00034486464686569345], "duration": 77857.476254, "accuracy_train": [0.11994070030120482, 0.12419992469879518, 0.12584713855421686, 0.12271743222891567, 0.1266472138554217, 0.12215267319277108, 0.12259977409638555, 0.12194088855421686, 0.12043486445783133, 0.11754047439759036, 0.11942300451807229, 0.12271743222891567, 0.11349303463855422, 0.11831701807228916, 0.12029367469879518, 0.12201148343373494, 0.12723550451807228, 0.12864740210843373, 0.13335372740963855, 0.12834149096385541, 0.13380082831325302, 0.12219973644578314, 0.12281155873493976, 0.13813064759036145, 0.12342338102409639, 0.12707078313253012, 0.1290945030120482, 0.12238798945783133, 0.12116434487951808, 0.1271413780120482, 0.123046875, 0.11490493222891567, 0.12777673192771086, 0.13297722138554216, 0.11438723644578314, 0.12417639307228916, 0.11878765060240964, 0.11012801204819277, 0.12796498493975902, 0.10876317771084337, 0.12867093373493976, 0.109375, 0.11956419427710843, 0.11989363704819277, 0.12721197289156627, 0.13147119728915663, 0.14584902108433734, 0.14196630271084337, 0.13429499246987953, 0.12337631777108433, 0.15100244728915663, 0.14603727409638553, 0.15010824548192772, 0.12147025602409639, 0.14937876506024098, 0.14222515060240964, 0.15841490963855423, 0.14184864457831325, 0.14801393072289157, 0.10650414156626506, 0.10713949548192771, 0.15015530873493976, 0.16385071536144577, 0.15417921686746988, 0.16578030873493976, 0.15980327560240964, 0.17131024096385541, 0.1589796686746988, 0.1517554593373494, 0.15032003012048192, 0.15114363704819278, 0.14340173192771086, 0.15968561746987953, 0.13490681475903615, 0.15636765813253012, 0.15034356174698796, 0.1203172063253012, 0.1178699171686747, 0.11794051204819277, 0.1278473268072289, 0.12344691265060241, 0.14318994728915663, 0.14526073042168675, 0.15636765813253012, 0.14229574548192772, 0.15582643072289157, 0.1251882530120482, 0.18404085090361447, 0.1315653237951807, 0.14533132530120482, 0.16721573795180722, 0.16587443524096385, 0.12241152108433735, 0.14191923945783133, 0.12907097138554216, 0.17406344126506024, 0.13704819277108435, 0.12740022590361447, 0.1595679593373494, 0.11666980421686747, 0.12697665662650603, 0.12342338102409639, 0.12055252259036145, 0.1548851656626506, 0.13278896837349397, 0.12281155873493976, 0.15415568524096385, 0.11349303463855422, 0.14801393072289157, 0.15309676204819278, 0.14274284638554216, 0.1610269201807229, 0.16749811746987953, 0.16432134789156627, 0.14831984186746988, 0.1551910768072289, 0.09807981927710843, 0.18406438253012047, 0.16676863704819278, 0.1303652108433735, 0.1820406626506024, 0.15112010542168675, 0.09593844126506024, 0.11794051204819277, 0.12008189006024096, 0.1294004141566265, 0.13220067771084337, 0.14029555722891565, 0.13810711596385541, 0.14269578313253012, 0.13573042168674698, 0.13768354668674698, 0.1586031626506024, 0.14770801957831325, 0.1460843373493976, 0.16554499246987953, 0.17575771837349397, 0.13568335843373494], "end": "2016-01-18 16:08:59.310000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0], "accuracy_valid": [0.12028556034482758, 0.12446120689655173, 0.12230603448275862, 0.11341594827586207, 0.12594288793103448, 0.12540409482758622, 0.12607758620689655, 0.12405711206896551, 0.12297952586206896, 0.12163254310344827, 0.12217133620689655, 0.1258081896551724, 0.11732219827586207, 0.12351831896551724, 0.125, 0.12419181034482758, 0.1297144396551724, 0.12904094827586207, 0.1341594827586207, 0.1302532327586207, 0.13523706896551724, 0.12324892241379311, 0.12365301724137931, 0.13577586206896552, 0.12338362068965517, 0.12931034482758622, 0.1320043103448276, 0.12648168103448276, 0.12419181034482758, 0.12661637931034483, 0.1258081896551724, 0.11314655172413793, 0.1283674568965517, 0.12984913793103448, 0.11664870689655173, 0.1255387931034483, 0.11988146551724138, 0.10816271551724138, 0.12850215517241378, 0.11018318965517242, 0.12782866379310345, 0.11355064655172414, 0.11893857758620689, 0.12432650862068965, 0.1320043103448276, 0.1361799568965517, 0.14574353448275862, 0.14102909482758622, 0.12917564655172414, 0.11961206896551724, 0.15301724137931033, 0.14816810344827586, 0.15247844827586207, 0.12486530172413793, 0.15584590517241378, 0.14641702586206898, 0.1635237068965517, 0.1458782327586207, 0.1570581896551724, 0.11220366379310345, 0.10668103448275862, 0.16015625, 0.16689116379310345, 0.15463362068965517, 0.17214439655172414, 0.15692349137931033, 0.1685075431034483, 0.1606950431034483, 0.16042564655172414, 0.14938038793103448, 0.14574353448275862, 0.1453394396551724, 0.16244612068965517, 0.13941271551724138, 0.1509967672413793, 0.14574353448275862, 0.12082435344827586, 0.12365301724137931, 0.11974676724137931, 0.1322737068965517, 0.12850215517241378, 0.14614762931034483, 0.1437230603448276, 0.15813577586206898, 0.1392780172413793, 0.1580010775862069, 0.1255387931034483, 0.1841325431034483, 0.1328125, 0.14493534482758622, 0.17147090517241378, 0.1744342672413793, 0.1280980603448276, 0.14722521551724138, 0.13092672413793102, 0.18116918103448276, 0.1384698275862069, 0.13294719827586207, 0.15746228448275862, 0.11772629310344827, 0.125, 0.11947737068965517, 0.11961206896551724, 0.1544989224137931, 0.13739224137931033, 0.12594288793103448, 0.1531519396551724, 0.11557112068965517, 0.1466864224137931, 0.1528825431034483, 0.1398168103448276, 0.15975215517241378, 0.16810344827586207, 0.16056034482758622, 0.14574353448275862, 0.1528825431034483, 0.09752155172413793, 0.1818426724137931, 0.1646012931034483, 0.13442887931034483, 0.1804956896551724, 0.15422952586206898, 0.09307650862068965, 0.11664870689655173, 0.12405711206896551, 0.13308189655172414, 0.13900862068965517, 0.14938038793103448, 0.14641702586206898, 0.1453394396551724, 0.13954741379310345, 0.14493534482758622, 0.1666217672413793, 0.14493534482758622, 0.15032327586206898, 0.16258081896551724, 0.1779364224137931, 0.13483297413793102], "accuracy_test": 0.1842948717948718, "start": "2016-01-17 18:31:21.834000", "learning_rate_per_epoch": [0.005991949699819088, 0.002995974849909544, 0.001997316488996148, 0.001497987424954772, 0.0011983899166807532, 0.000998658244498074, 0.0008559927809983492, 0.000748993712477386, 0.0006657721824012697, 0.0005991949583403766, 0.0005447227158583701, 0.000499329122249037, 0.00046091919648461044, 0.0004279963904991746, 0.0003994633152615279, 0.000374496856238693, 0.0003524676139932126, 0.00033288609120063484, 0.0003153657598886639, 0.0002995974791701883, 0.00028533092699944973, 0.00027236135792918503, 0.00026051956228911877, 0.0002496645611245185, 0.00023967798915691674, 0.00023045959824230522, 0.00022192405594978482, 0.0002139981952495873, 0.00020661894814111292, 0.00019973165763076395, 0.00019328869529999793, 0.0001872484281193465, 0.00018157422891817987, 0.0001762338069966063, 0.00017119856784120202, 0.00016644304560031742, 0.00016194458294194192, 0.00015768287994433194, 0.0001536397321615368, 0.00014979873958509415, 0.00014614511746913195, 0.00014266546349972486, 0.0001393476704834029, 0.00013618067896459252, 0.0001331544335698709, 0.00013025978114455938, 0.00012748829612974077, 0.00012483228056225926, 0.0001222846913151443, 0.00011983899457845837, 0.0001174892095150426, 0.00011522979912115261, 0.00011305565567454323, 0.00011096202797489241, 0.0001089445358957164, 0.00010699909762479365, 0.00010512192238820717, 0.00010330947407055646, 0.00010155847121495754, 9.986582881538197e-05, 9.822868014452979e-05, 9.664434764999896e-05, 9.511031385045499e-05, 9.362421405967325e-05, 9.218384366249666e-05, 9.078711445908993e-05, 8.94320837687701e-05, 8.811690349830315e-05, 8.683984924573451e-05, 8.559928392060101e-05, 8.439365774393082e-05, 8.322152280015871e-05, 8.208150393329561e-05, 8.097229147097096e-05, 7.989266305230558e-05, 7.884143997216597e-05, 7.781753083691001e-05, 7.68198660807684e-05, 7.584746344946325e-05, 7.489936979254708e-05, 7.397469016723335e-05, 7.307255873456597e-05, 7.219216786324978e-05, 7.133273174986243e-05, 7.049352279864252e-05, 6.967383524170145e-05, 6.887298513902351e-05, 6.809033948229626e-05, 6.732527981512249e-05, 6.657721678493544e-05, 6.584559741895646e-05, 6.512989057227969e-05, 6.442956509999931e-05, 6.374414806487039e-05, 6.307315197773278e-05, 6.241614028112963e-05, 6.177267641760409e-05, 6.114234565757215e-05, 6.0524744185386226e-05, 5.9919497289229184e-05, 5.93262338952627e-05, 5.87446047575213e-05, 5.817426790599711e-05, 5.7614899560576305e-05, 5.706618685508147e-05, 5.6527827837271616e-05, 5.599952783086337e-05, 5.5481013987446204e-05, 5.497201345860958e-05, 5.44722679478582e-05, 5.398153007263318e-05, 5.3499548812396824e-05, 5.302610225044191e-05, 5.2560961194103584e-05, 5.210391100263223e-05, 5.165473703527823e-05, 5.121324647916481e-05, 5.077923560747877e-05, 5.0352518883300945e-05, 4.993291440769099e-05, 4.9520243919687346e-05, 4.9114340072264895e-05, 4.8715039156377316e-05, 4.832217382499948e-05, 4.793559855897911e-05, 4.755515692522749e-05, 4.718070704257116e-05, 4.6812107029836625e-05, 4.6449222281808034e-05, 4.609192183124833e-05, 4.574007471092045e-05, 4.539355722954497e-05, 4.5052252971800044e-05, 4.471604188438505e-05, 4.438481118995696e-05, 4.405845174915157e-05, 4.373685806058347e-05, 4.3419924622867256e-05], "accuracy_train_last": 0.13568335843373494, "error_valid": [0.8797144396551724, 0.8755387931034483, 0.8776939655172413, 0.8865840517241379, 0.8740571120689655, 0.8745959051724138, 0.8739224137931034, 0.8759428879310345, 0.877020474137931, 0.8783674568965517, 0.8778286637931034, 0.8741918103448276, 0.8826778017241379, 0.8764816810344828, 0.875, 0.8758081896551724, 0.8702855603448276, 0.8709590517241379, 0.8658405172413793, 0.8697467672413793, 0.8647629310344828, 0.8767510775862069, 0.8763469827586207, 0.8642241379310345, 0.8766163793103449, 0.8706896551724138, 0.8679956896551724, 0.8735183189655172, 0.8758081896551724, 0.8733836206896551, 0.8741918103448276, 0.8868534482758621, 0.8716325431034483, 0.8701508620689655, 0.8833512931034483, 0.8744612068965517, 0.8801185344827587, 0.8918372844827587, 0.8714978448275862, 0.8898168103448276, 0.8721713362068966, 0.8864493534482758, 0.8810614224137931, 0.8756734913793104, 0.8679956896551724, 0.8638200431034483, 0.8542564655172413, 0.8589709051724138, 0.8708243534482758, 0.8803879310344828, 0.8469827586206897, 0.8518318965517242, 0.8475215517241379, 0.8751346982758621, 0.8441540948275862, 0.853582974137931, 0.8364762931034483, 0.8541217672413793, 0.8429418103448276, 0.8877963362068966, 0.8933189655172413, 0.83984375, 0.8331088362068966, 0.8453663793103449, 0.8278556034482758, 0.8430765086206897, 0.8314924568965517, 0.8393049568965517, 0.8395743534482758, 0.8506196120689655, 0.8542564655172413, 0.8546605603448276, 0.8375538793103449, 0.8605872844827587, 0.8490032327586207, 0.8542564655172413, 0.8791756465517242, 0.8763469827586207, 0.8802532327586207, 0.8677262931034483, 0.8714978448275862, 0.8538523706896551, 0.8562769396551724, 0.841864224137931, 0.8607219827586207, 0.8419989224137931, 0.8744612068965517, 0.8158674568965517, 0.8671875, 0.8550646551724138, 0.8285290948275862, 0.8255657327586207, 0.8719019396551724, 0.8527747844827587, 0.869073275862069, 0.8188308189655172, 0.8615301724137931, 0.8670528017241379, 0.8425377155172413, 0.8822737068965517, 0.875, 0.8805226293103449, 0.8803879310344828, 0.8455010775862069, 0.8626077586206897, 0.8740571120689655, 0.8468480603448276, 0.8844288793103449, 0.8533135775862069, 0.8471174568965517, 0.8601831896551724, 0.8402478448275862, 0.8318965517241379, 0.8394396551724138, 0.8542564655172413, 0.8471174568965517, 0.9024784482758621, 0.8181573275862069, 0.8353987068965517, 0.8655711206896551, 0.8195043103448276, 0.845770474137931, 0.9069234913793104, 0.8833512931034483, 0.8759428879310345, 0.8669181034482758, 0.8609913793103449, 0.8506196120689655, 0.853582974137931, 0.8546605603448276, 0.8604525862068966, 0.8550646551724138, 0.8333782327586207, 0.8550646551724138, 0.849676724137931, 0.8374191810344828, 0.8220635775862069, 0.865167025862069], "accuracy_train_std": [0.02722505458500342, 0.030241024149295033, 0.028551146544990337, 0.028552959880473325, 0.028708931523182116, 0.02850863106644204, 0.0286172830351683, 0.028354076925139644, 0.02973872550455454, 0.029564143241552345, 0.029531459994306255, 0.0306280458691741, 0.02912697353499481, 0.029535331773327293, 0.03095896478348335, 0.032716942000513075, 0.03130236184368845, 0.03250898247234531, 0.0318138104468221, 0.03067184811256046, 0.032145166486080186, 0.031097692141672184, 0.029979800107644, 0.029780479651828583, 0.030067148444489763, 0.029948282355853405, 0.031158112828106696, 0.0303846699901741, 0.029578599294484496, 0.029835680483854682, 0.02855807926502441, 0.028385062879406255, 0.030227471117811645, 0.03149860056560929, 0.02840347251597823, 0.030249510016043316, 0.028421996865998226, 0.02858595798533983, 0.029470278501987607, 0.026354203056486364, 0.03102178882955282, 0.02639615035524702, 0.029539109315469592, 0.02871509337318557, 0.031059358079458116, 0.031117984798427152, 0.031101262116738502, 0.0324993060794931, 0.030022989620316964, 0.029434321280669785, 0.03331495788103963, 0.032863397113903736, 0.0313317134117173, 0.03034937648055141, 0.031052832213785492, 0.03219484276955293, 0.03175336440455406, 0.031676121638957395, 0.030442504240064505, 0.027771267523566533, 0.027062669805763672, 0.030496579413888444, 0.0312639864928845, 0.028987525589221994, 0.034920006094485, 0.03322408171349386, 0.03400599304046038, 0.03259605116608684, 0.03127092869719323, 0.032869159196414095, 0.031511185150903176, 0.03440317356973192, 0.033382712141352536, 0.029894419385441216, 0.03131148853116846, 0.0314471553722667, 0.02885736425476657, 0.02972582830254704, 0.028962910974638836, 0.027955126610859606, 0.02958452386006928, 0.03093429886592369, 0.030966645942547485, 0.031399435329315134, 0.03203956036435416, 0.0320946196742438, 0.03182074580978271, 0.03678386148449413, 0.030192415677569005, 0.03163448896130573, 0.03449884117222652, 0.03234600749733487, 0.029154726107238765, 0.032353812906184845, 0.030248118753385476, 0.03327557539804681, 0.0318109296948658, 0.030356746739639777, 0.03247073709864808, 0.027017545708135426, 0.030706239004218722, 0.02926149003797691, 0.028042146732269953, 0.032142823644361045, 0.03125881425708257, 0.029671608168187646, 0.031117771260429927, 0.028314042734311127, 0.02986329362798862, 0.03245600812115141, 0.030268624245833153, 0.03291988763447363, 0.03235354762166873, 0.03280733255824917, 0.031081662264676027, 0.03223715171045646, 0.02733374419039282, 0.03437038744019205, 0.03288717182544355, 0.03098835570319072, 0.03464763316553592, 0.032248426437108665, 0.025528539640587322, 0.029146405960004694, 0.02860690000743445, 0.02796423684421816, 0.030989963890391294, 0.029015211155172645, 0.02976923747294746, 0.029359323186528685, 0.0308106551075764, 0.031167343939056485, 0.031102223536240033, 0.03028836629601197, 0.03140223039483392, 0.03155164673619677, 0.0317026383008527, 0.030297661359649446], "accuracy_test_std": 0.03357804440179547, "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-3, 3], "momentum": 0.9615710147442615, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.005991949657640235, "patience_threshold": 1, "do_flip": true, "batch_size": 128, "optimization": "adam", "nb_data_augmentation": 2, "learning_rate_decay_method": "lin", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 2.243001877807718e-08, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.03357398146232402}, "accuracy_valid_max": 0.1841325431034483, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = 1234\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-10, -6], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128, 256],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-3, 3)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_optimizer.learning_rate = learning_rate\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.13483297413793102, "loss_train": [3.826373032427913e+19, 5.373687846089523e+16, 1.5573670600638464e+16, 1.0669979737260032e+16, 8633248263438336.0, 7254264187453440.0, 6219939962683392.0, 5408688286203904.0, 4766182116687872.0, 4232717650624512.0, 3811289822396416.0, 3442730088792064.0, 3100201078226944.0, 2851902576394240.0, 2588470488858624.0, 2258878993530880.0, 1959457093320704.0, 1791509779185664.0, 1638507743281152.0, 1501616129703936.0, 1389926310477824.0, 1277841085825024.0, 1175691328487424.0, 1082949697011712.0, 998256163160064.0, 912398424735744.0, 856451979411456.0, 795077970493440.0, 748559850799104.0, 668704262062080.0, 620639551488000.0, 624197797675008.0, 539865779273728.0, 496647201095680.0, 459673404702720.0, 457155815669760.0, 360437312913408.0, 369462951804928.0, 322531475062784.0, 290883270344704.0, 242209949483008.0, 240192606699520.0, 194285328662528.0, 183585558495232.0, 145490591088640.0, 182111008980992.0, 122581000650752.0, 105246042882048.0, 108074891214848.0, 86845245358080.0, 81014600761344.0, 164288689864704.0, 58828427100160.0, 48017008754688.0, 49549993312256.0, 42223026569216.0, 37893124915200.0, 36758288859136.0, 32018903597056.0, 365791325192192.0, 23063880007680.0, 21892895342592.0, 19113451716608.0, 16519674200064.0, 16041283420160.0, 16841263022080.0, 13968880959488.0, 14839954014208.0, 13568841875456.0, 11661014466560.0, 11549181739008.0, 25199460220928.0, 8968953921536.0, 7585650442240.0, 7238969196544.0, 6648981094400.0, 210476801720320.0, 11889578868736.0, 7627386388480.0, 6084986142720.0, 5216359415808.0, 4860821372928.0, 4274456625152.0, 4154708197376.0, 4078526791680.0, 3454164271104.0, 3309781909504.0, 3077101060096.0, 3015050526720.0, 2895164735488.0, 2844655091712.0, 2450301124608.0, 2160314155008.0, 27602962087936.0, 2137232506880.0, 1761402683392.0, 1449017212928.0, 1401559580672.0, 1541924323328.0, 1958477299712.0, 1452812926976.0, 1064343961600.0, 3266370076672.0, 959407652864.0, 815527952384.0, 892662185984.0, 936222785536.0, 1629011705856.0, 756872773632.0, 667265335296.0, 7565892648960.0, 643746299904.0, 509978345472.0, 509776527360.0, 430078230528.0, 447242600448.0, 481282260992.0, 593238622208.0, 415811895296.0, 409678872576.0, 357173329920.0, 370138120192.0, 976051961856.0, 29932669370368.0, 3083629232128.0, 2058476716032.0, 1603851517952.0, 1257919873024.0, 1078146105344.0, 898754478080.0, 808426799104.0, 726248914944.0, 654231666688.0, 574607982592.0, 520123613184.0, 489129410560.0, 451978788864.0, 416590659584.0], "accuracy_train_first": 0.11994070030120482, "model": "residualv2", "loss_std": [Infinity, 3.383336006110413e+16, 2651626640769024.0, 1302854706921472.0, 969112394137600.0, 804761511133184.0, 709517725663232.0, 611447314841600.0, 534271685033984.0, 482984306147328.0, 441802381328384.0, 402007328292864.0, 355778212921344.0, 346120442085376.0, 306896720363520.0, 285238441803776.0, 237980849537024.0, 223898155089920.0, 192755565330432.0, 178578935250944.0, 182278361710592.0, 165085188194304.0, 162392528912384.0, 162206083710976.0, 152069826674688.0, 132028578136064.0, 152192048693248.0, 145988169760768.0, 158690552315904.0, 119149623771136.0, 132457571549184.0, 155784805613568.0, 143817332228096.0, 114989629177856.0, 102627606003712.0, 145178836860928.0, 79351215292416.0, 120194492006400.0, 91172173250560.0, 118984066203648.0, 53408908181504.0, 71984465575936.0, 54711780638720.0, 57911518691328.0, 32637355819008.0, 108704221364224.0, 31838609342464.0, 24710542786560.0, 56916306821120.0, 26194514804736.0, 26891975131136.0, 239316819247104.0, 13998264156160.0, 9496991629312.0, 16893729570816.0, 9039494774784.0, 8367177203712.0, 10400310493184.0, 7699343343616.0, 816944320086016.0, 4958028038144.0, 5756231352320.0, 4972887932928.0, 2968240259072.0, 3353268977664.0, 4657992171520.0, 3023320121344.0, 4290471264256.0, 4435820937216.0, 2736550576128.0, 3129468518400.0, 33031349862400.0, 2625573748736.0, 1739113627648.0, 1652920418304.0, 2041643794432.0, 764500957462528.0, 2951312834560.0, 1554111791104.0, 1357879836672.0, 1040331833344.0, 1308475129856.0, 880515547136.0, 855427579904.0, 1132317507584.0, 797858463744.0, 813961838592.0, 738085896192.0, 950108749824.0, 768409141248.0, 857311936512.0, 633581469696.0, 567023435776.0, 92560957636608.0, 367407693824.0, 332963414016.0, 218261258240.0, 284934340608.0, 563668647936.0, 1758117101568.0, 1197323845632.0, 276912373760.0, 7419345764352.0, 220901720064.0, 177034067968.0, 311881531392.0, 385907195904.0, 2286800076800.0, 348272623616.0, 181278130176.0, 18092612648960.0, 135391584256.0, 95118245888.0, 117903081472.0, 85443084288.0, 124757901312.0, 170969939968.0, 641718026240.0, 146135564288.0, 124025331712.0, 97222926336.0, 121071091712.0, 4924351971328.0, 74401097711616.0, 721189535744.0, 396577898496.0, 282099318784.0, 202439213056.0, 164241653760.0, 125943627776.0, 127630893056.0, 118219300864.0, 131711696896.0, 99602432000.0, 85901656064.0, 87020945408.0, 79571214336.0, 91099004928.0]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:00 2016", "state": "available"}], "summary": "2f1cb20a9b2e7d49f5f900d35aba1dcc"}