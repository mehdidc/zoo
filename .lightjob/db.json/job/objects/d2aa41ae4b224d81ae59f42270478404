{"content": {"hp_model": {"f0": 16, "f1": 32, "f2": 64, "f3": 64, "nonlin": "leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.6909935474395752, 1.279963731765747, 1.0384982824325562, 0.9085497856140137, 0.8111194372177124, 0.7308881282806396, 0.6622244119644165, 0.6022447347640991, 0.549638032913208, 0.5043116807937622, 0.46542900800704956, 0.43206071853637695, 0.4038359522819519, 0.38000738620758057, 0.36003178358078003, 0.34369829297065735, 0.3304973840713501, 0.3202536106109619, 0.31232941150665283, 0.30613499879837036, 0.3012367784976959, 0.29727303981781006, 0.2938913106918335, 0.29091206192970276, 0.2881893515586853, 0.2856002748012543, 0.2830319106578827, 0.2804016172885895, 0.2776370644569397, 0.2746782898902893, 0.271478533744812, 0.2680039405822754, 0.2642343044281006, 0.26016339659690857, 0.25579577684402466, 0.25114521384239197, 0.24623170495033264, 0.2410806119441986, 0.23572130501270294, 0.23018652200698853, 0.22451122105121613, 0.21873100101947784, 0.21288162469863892, 0.20699839293956757, 0.2011147439479828, 0.19526132941246033, 0.18946586549282074, 0.18375176191329956, 0.17813917994499207, 0.17264404892921448, 0.16729244589805603, 0.17332683503627777, 0.16577774286270142, 0.16475221514701843, 0.16449905931949615, 0.1642850786447525, 0.16403290629386902], "moving_avg_accuracy_train": [0.06013458465992985, 0.12465222623085084, 0.18832531715750733, 0.24913720710334758, 0.3074502053448862, 0.3622518975907132, 0.4132655882130095, 0.46058433641228813, 0.5044567286927555, 0.5451808976701022, 0.5831787667151537, 0.6184044204342345, 0.650895553983742, 0.6808560451604416, 0.7085272243111103, 0.734054353330027, 0.757626260593462, 0.7793897122496013, 0.7995046275198885, 0.8179428366428997, 0.8347790403298001, 0.8500920589158678, 0.8640388612088048, 0.8767304922010195, 0.8882459660463938, 0.8987028984596116, 0.9081931926910314, 0.9167925862195473, 0.9245715679249735, 0.9316028783943809, 0.9379566344537523, 0.9437098921393294, 0.9489110755444441, 0.9536200423947616, 0.957879038899333, 0.9617377123903521, 0.9652174939786979, 0.9683655734498758, 0.9712104707179835, 0.973787154300947, 0.9761108198232332, 0.9782090942397195, 0.9800905657681285, 0.9817908655901252, 0.9833234605787317, 0.9847144218125252, 0.985973262369368, 0.9871155194657646, 0.988141225703712, 0.9890690116154837, 0.989934245870602, 0.9905107408514173, 0.9911759986115137, 0.9918189084229815, 0.9924114781461595, 0.9929494411946388, 0.9934452336823179], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.05827371987951806, 0.12100588878953311, 0.18090016413309484, 0.23689859133800822, 0.2890804318935899, 0.336560901740752, 0.3798477885527461, 0.41853331396140214, 0.45299731243160224, 0.48363546357737275, 0.5113806980460662, 0.5363249359880711, 0.5586882714084658, 0.578668788911821, 0.5970104061143889, 0.6133825547442904, 0.6280320392924517, 0.641350852729547, 0.6534852987065923, 0.6643055553099542, 0.6740671708068202, 0.6828668908025689, 0.6908720880174927, 0.6980900020508338, 0.7045372965558407, 0.7104629614315067, 0.7157584092171965, 0.7206107909517268, 0.7249301358964637, 0.7288043098068173, 0.7321801735362259, 0.7352072733701034, 0.737870628064343, 0.7402310261954087, 0.7422821423258679, 0.7440782892096215, 0.7456459932799997, 0.7468361708721805, 0.7477964379152335, 0.7486606782539813, 0.7493520158314446, 0.7499386280660711, 0.7505276142334851, 0.7508735668067481, 0.7511360959976847, 0.7513734017781873, 0.7515381488556397, 0.7516508296402565, 0.7517166507613212, 0.7516294053952794, 0.751486760883613, 0.7515139837033842, 0.7519077837046573, 0.7522235235947337, 0.7526674104107121, 0.7530424944825926, 0.7532335857722853], "moving_var_accuracy_train": [0.032545514450000444, 0.06675369766986491, 0.09656669047626684, 0.12019279505770503, 0.1387771674271937, 0.15192847994153133, 0.16015720162554448, 0.1642930568433106, 0.16518683239868046, 0.16359427060887188, 0.16022938601566858, 0.15537406753353103, 0.14933772461416533, 0.14248263143669068, 0.1351256156933172, 0.12747776296752125, 0.11973069997909298, 0.11202046043308565, 0.1044599027367546, 0.09707362046405416, 0.08991737820892996, 0.08303603723199224, 0.07648305315657748, 0.07028444531610266, 0.06444945602544387, 0.05898863734235092, 0.053900364769506104, 0.04917587441408002, 0.04480289998003222, 0.04076756392428379, 0.03705413947641341, 0.033646625294741946, 0.030525433544590513, 0.027672459509307964, 0.025068465019410735, 0.022695622767462304, 0.020535040409839386, 0.018570730008067107, 0.016786497971455174, 0.015167601858890083, 0.013699436466136233, 0.012369117619264534, 0.011164065273348005, 0.010073677921375342, 0.009087449755829724, 0.008196117738632, 0.007390768080696769, 0.006663434034095506, 0.006006559290265032, 0.005413650441521267, 0.004879023070215212, 0.0043941118813598384, 0.00395868380421017, 0.0035665354210202855, 0.0032130421288097026, 0.002894342554102494, 0.002607120590409795], "duration": 16111.499408, "accuracy_train": [0.6013458465992987, 0.70531100036914, 0.761383135497416, 0.7964442166159099, 0.8322671895187338, 0.8554671278031561, 0.8723888038136766, 0.8864530702057956, 0.8993082592169619, 0.9116984184662238, 0.9251595881206165, 0.9354353039059615, 0.9433157559293098, 0.9505004657507383, 0.9575678366671282, 0.9637985145002769, 0.969773425964378, 0.9752607771548542, 0.9805388649524732, 0.98388671875, 0.9863048735119048, 0.9879092261904762, 0.9895600818452381, 0.9909551711309523, 0.9918852306547619, 0.9928152901785714, 0.9936058407738095, 0.9941871279761905, 0.9945824032738095, 0.9948846726190477, 0.9951404389880952, 0.9954892113095238, 0.9957217261904762, 0.9960007440476191, 0.9962100074404762, 0.9964657738095238, 0.9965355282738095, 0.9966982886904762, 0.9968145461309523, 0.9969773065476191, 0.9970238095238095, 0.9970935639880952, 0.9970238095238095, 0.9970935639880952, 0.9971168154761905, 0.9972330729166666, 0.9973028273809523, 0.9973958333333334, 0.9973725818452381, 0.9974190848214286, 0.9977213541666666, 0.995699195678756, 0.9971633184523809, 0.9976050967261905, 0.9977446056547619, 0.9977911086309523, 0.9979073660714286], "end": "2016-02-01 14:16:03.017000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0], "moving_var_accuracy_valid": [0.030562437857368854, 0.0629241192170239, 0.08891762526569502, 0.1082482773839414, 0.12192994999946062, 0.13002651015167999, 0.13388765026538146, 0.1339680141241373, 0.1312611174267092, 0.12658327243471798, 0.12085312751275115, 0.11436774982004222, 0.10743204377816372, 0.10028182911766431, 0.0932813805003477, 0.0863656677071492, 0.07966056751417563, 0.0732910278851074, 0.06728712810912514, 0.061612116874875986, 0.05630850742136628, 0.05137457232726187, 0.0468138637365841, 0.04260136190986001, 0.03871533417678265, 0.03515982229707269, 0.03189621597262412, 0.028918504851840145, 0.026194565033420755, 0.02371019154146766, 0.021441740490596724, 0.019380036442175397, 0.017505873922003815, 0.01580542984383768, 0.014262750555879578, 0.01286551079294378, 0.011601078978119928, 0.0104537197846163, 0.009416646821300435, 0.00848170434143846, 0.007637835436108731, 0.006877148917722181, 0.006192556168298608, 0.005574377700115272, 0.00501756022428859, 0.004516311028160871, 0.004064924199740545, 0.003658546052399488, 0.0032927304389393434, 0.0029635259008304713, 0.0026673564378578013, 0.002400627463809268, 0.002161960423397365, 0.001946661606161298, 0.0017537687650937635, 0.001579658081133194, 0.0014220209159488417], "accuracy_test": 0.11874402104591837, "start": "2016-02-01 09:47:31.518000", "learning_rate_per_epoch": [0.0005062579875811934, 0.0002531289937905967, 0.00016875265282578766, 0.00012656449689529836, 0.00010125159315066412, 8.437632641289383e-05, 7.232256757561117e-05, 6.328224844764918e-05, 5.625088306260295e-05, 5.062579657533206e-05, 4.6023451432120055e-05, 4.2188163206446916e-05, 3.894291876349598e-05, 3.616128378780559e-05, 3.375052983756177e-05, 3.164112422382459e-05, 2.9779879696434364e-05, 2.8125441531301476e-05, 2.6645155230653472e-05, 2.531289828766603e-05, 2.4107521312544122e-05, 2.3011725716060027e-05, 2.201121606049128e-05, 2.1094081603223458e-05, 2.0250317902537063e-05, 1.947145938174799e-05, 1.8750295566860586e-05, 1.8080641893902794e-05, 1.7457170542911626e-05, 1.6875264918780886e-05, 1.6330901416949928e-05, 1.5820562111912295e-05, 1.5341151083703153e-05, 1.4889939848217182e-05, 1.4464512787526473e-05, 1.4062720765650738e-05, 1.368264747725334e-05, 1.3322577615326736e-05, 1.2980973224330228e-05, 1.2656449143833015e-05, 1.234775481862016e-05, 1.2053760656272061e-05, 1.1773440746765118e-05, 1.1505862858030014e-05, 1.125017661252059e-05, 1.100560803024564e-05, 1.0771445886348374e-05, 1.0547040801611729e-05, 1.0331795238016639e-05, 1.0125158951268531e-05, 9.92662626231322e-06, 9.735729690873995e-06, 9.552037226967514e-06, 9.375147783430293e-06, 9.204690286424011e-06, 9.040320946951397e-06, 8.881718713382725e-06], "accuracy_train_first": 0.6013458465992987, "accuracy_train_last": 0.9979073660714286, "batch_size_eval": 1024, "accuracy_train_std": [0.01979323225416731, 0.02060004429060329, 0.021323669671202163, 0.022393619534093384, 0.023449575276411262, 0.023780830965372004, 0.02538983298805174, 0.024700425521272273, 0.023926101201121663, 0.02298847725573458, 0.021947679812646045, 0.02006038747368649, 0.018328348982791474, 0.016344904034789182, 0.01417385761124372, 0.012361539094504052, 0.011090392297103199, 0.009828269454575189, 0.008663304730464567, 0.007410515910047012, 0.006468971837429912, 0.006031061323241621, 0.005147619007423655, 0.004634984553261947, 0.00433856817990144, 0.003928400671744724, 0.003476933334614873, 0.0030133785043382467, 0.0029773709816966467, 0.0027605695832693135, 0.002657290088788992, 0.002547921787909733, 0.0023533446668303564, 0.0021920617093364354, 0.002111793285836968, 0.0020434870296428416, 0.002035136222296683, 0.0019291730387547936, 0.0019067637959050325, 0.0018058489433545016, 0.0018572103373991685, 0.0018143611583797809, 0.0019054874673128607, 0.0018018028056230816, 0.0017437065903322798, 0.0016768498634768222, 0.0016342339463174833, 0.0016734611435497194, 0.0015874143590679207, 0.001620446901416102, 0.0014712883202712404, 0.0019305868483787268, 0.0016202800772120859, 0.0014327520929133088, 0.001195813883056528, 0.001225289446266313, 0.0012162106427289826], "accuracy_test_std": 0.011485352919522575, "error_valid": [0.4172628012048193, 0.31440459102033136, 0.28005135777484935, 0.2591155638177711, 0.24128300310617468, 0.23611486963478923, 0.23057023013930722, 0.23329695736069278, 0.23682670133659633, 0.24062117611069278, 0.23891219173569278, 0.23917692253388556, 0.2400417098079819, 0.2415065535579819, 0.2379150390625, 0.23926810758659633, 0.24012259977409633, 0.23877982633659633, 0.2373046875, 0.23831213525978923, 0.23807828972138556, 0.23793562923569278, 0.23708113704819278, 0.23694877164909633, 0.23743705289909633, 0.2362060546875, 0.23658256071159633, 0.2357177734375, 0.23619575960090367, 0.236328125, 0.23743705289909633, 0.237548828125, 0.2381591796875, 0.238525390625, 0.2392578125, 0.23975638883659633, 0.24024467008659633, 0.24245223079819278, 0.24356115869728923, 0.24356115869728923, 0.24442594597138556, 0.24478186182228923, 0.24417151025978923, 0.24601286003388556, 0.24650114128388556, 0.24649084619728923, 0.24697912744728923, 0.24733504329819278, 0.24769095914909633, 0.24915580289909633, 0.24979703972138556, 0.24824101091867468, 0.24454801628388556, 0.24493481739457834, 0.2433376082454819, 0.2435817488704819, 0.2450465926204819], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.043652629646221366, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 64, "valid_ratio": 0.15, "learning_rate": 0.0005062579604926147, "optimization": "adam", "nb_data_augmentation": 0, "learning_rate_decay_method": "lin", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 5.953279643786704e-05, "rotation_range": [0, 0], "momentum": 0.5606809390412955}, "accuracy_valid_max": 0.7694297698606928, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.7549534073795181, "accuracy_valid_std": [0.012362579397380563, 0.01189896716453085, 0.012600855642604275, 0.010448545234516207, 0.012731537520589833, 0.010815848925869646, 0.008681382281571927, 0.011742636684468464, 0.014197105266039848, 0.014055254358507322, 0.01587387752523073, 0.015248832644971858, 0.01640272837661489, 0.015608533623620047, 0.014906079785156874, 0.0136035527344693, 0.012615694030200688, 0.012883835394278616, 0.01060818407343771, 0.011255293338508207, 0.011738879525638098, 0.009717279561786292, 0.01051800695291172, 0.009732875211747375, 0.00965241344575449, 0.009295796564206994, 0.010508440558484227, 0.010895758405516537, 0.009062878289488489, 0.009923047574658972, 0.010661757975611547, 0.009799136836127674, 0.010165606370469668, 0.009565204113359245, 0.009923047574658972, 0.009432067950669842, 0.009900189797999243, 0.009367898470081572, 0.01035707515380357, 0.010448748938238862, 0.011067815649301614, 0.010603716849081152, 0.01081764146719386, 0.010873740030673634, 0.01107288358044263, 0.010609979488283381, 0.010636613643591421, 0.01009264883872241, 0.010679793999543264, 0.010274433265541316, 0.010993729273964746, 0.010908409653291117, 0.010685810342058682, 0.011996495834562164, 0.011024128695144329, 0.011316483107557614, 0.010775447244803591], "accuracy_valid": [0.5827371987951807, 0.6855954089796686, 0.7199486422251506, 0.7408844361822289, 0.7587169968938253, 0.7638851303652108, 0.7694297698606928, 0.7667030426393072, 0.7631732986634037, 0.7593788238893072, 0.7610878082643072, 0.7608230774661144, 0.7599582901920181, 0.7584934464420181, 0.7620849609375, 0.7607318924134037, 0.7598774002259037, 0.7612201736634037, 0.7626953125, 0.7616878647402108, 0.7619217102786144, 0.7620643707643072, 0.7629188629518072, 0.7630512283509037, 0.7625629471009037, 0.7637939453125, 0.7634174392884037, 0.7642822265625, 0.7638042403990963, 0.763671875, 0.7625629471009037, 0.762451171875, 0.7618408203125, 0.761474609375, 0.7607421875, 0.7602436111634037, 0.7597553299134037, 0.7575477692018072, 0.7564388413027108, 0.7564388413027108, 0.7555740540286144, 0.7552181381777108, 0.7558284897402108, 0.7539871399661144, 0.7534988587161144, 0.7535091538027108, 0.7530208725527108, 0.7526649567018072, 0.7523090408509037, 0.7508441971009037, 0.7502029602786144, 0.7517589890813253, 0.7554519837161144, 0.7550651826054217, 0.7566623917545181, 0.7564182511295181, 0.7549534073795181], "seed": 721203618, "model": "residualv3", "loss_std": [0.3464621901512146, 0.15357685089111328, 0.13277634978294373, 0.1262224316596985, 0.11910278350114822, 0.11094147711992264, 0.10120073705911636, 0.09094954282045364, 0.08046310395002365, 0.0701654702425003, 0.060921862721443176, 0.05243131145834923, 0.044569212943315506, 0.036988165229558945, 0.029777253046631813, 0.023145508021116257, 0.01718973182141781, 0.01239760685712099, 0.00880295317620039, 0.006220867391675711, 0.0045289634726941586, 0.003429585602134466, 0.0026626046746969223, 0.00211174413561821, 0.0017188378842547536, 0.0014474288327619433, 0.0012590737314894795, 0.0011429650476202369, 0.0010867270175367594, 0.001074890955351293, 0.0010983768152073026, 0.0011458765948191285, 0.0012081029126420617, 0.0012774333590641618, 0.0013487726682797074, 0.0014184295432642102, 0.001484028296545148, 0.0015433990629389882, 0.001594895264133811, 0.0016372583340853453, 0.0016700823325663805, 0.0016930955462157726, 0.0017062050756067038, 0.0017096555093303323, 0.0017040790989995003, 0.0016902056522667408, 0.0016690504271537066, 0.0016417704755440354, 0.0016093214508146048, 0.001572699286043644, 0.0015259400242939591, 0.04215789586305618, 0.002917645499110222, 0.0003588392282836139, 0.00016710814088582993, 0.00014455853670369834, 0.00013972570013720542]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:28 2016", "state": "available"}], "summary": "7e2f92c145267b591bbad48b9c1f9352"}