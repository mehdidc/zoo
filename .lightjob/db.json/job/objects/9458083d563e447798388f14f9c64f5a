{"content": {"hp_model": {"f0": 64, "f1": 64, "f2": 16, "f3": 64, "nonlin": "rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.018576762314168903, 0.011846999643068777, 0.015427005885213254, 0.01480756462885088, 0.012058459067351392, 0.010125039366137114, 0.015003809783435702, 0.018222002895902718, 0.01972375672444995, 0.011932644723518017, 0.01636295342144102, 0.017622867078980867, 0.013681328617892645, 0.014931504196102055, 0.014074681365471962, 0.012758811692977044, 0.012981260672994233, 0.016924049765866596, 0.015266513348406233, 0.005947484035841982, 0.009133587198173834, 0.012734253272844172, 0.011623393760774389, 0.01479944025571013, 0.012519354350766353, 0.012468288620415909, 0.015247990958663518, 0.01169562664467441, 0.00844524888885322, 0.008017410575706539, 0.014402854607694554, 0.008087965298910664, 0.01114533069605529, 0.012988582559613694, 0.009734500430333387, 0.01715000165334676, 0.010667721328957575, 0.009389598147800104, 0.012791554860966739, 0.008917188308915303, 0.01572897424882925, 0.015954556586591742, 0.009924606344117874, 0.01308861245947801, 0.010818026672823562, 0.013588054246082337, 0.00765418547071775, 0.01346032425830437, 0.009813643010597667, 0.014764525138143927, 0.014414565476187408, 0.01529265735985411, 0.011932817019918081, 0.010688476810363908, 0.010573699058291289, 0.009102580831502788, 0.010499318797249812, 0.011278805390906269, 0.01783968609585705, 0.017212500118810103, 0.009615451952151362, 0.010964150335789227, 0.013164250431035248, 0.013409067092258012, 0.011137285395881737, 0.012903276638821053, 0.01480798415455462, 0.011776284487518407, 0.004597776631851829, 0.00893025565003899, 0.009452864448022036, 0.012727763911673646, 0.01118552425287003, 0.009734921718436164, 0.009985384449846941, 0.008357184843782185, 0.009148300009895223, 0.01206426668524995, 0.015635223078790207, 0.012646220165504138, 0.014332930771309224, 0.009778204789408974, 0.007084863701544724, 0.009198317228975817, 0.008340544682953527, 0.011575329648185327, 0.012162183135064158, 0.011679144651195579, 0.010496418503066478, 0.012665791401923854, 0.011142268538507341, 0.013666735350430172, 0.010677071576085282, 0.00678982747602742, 0.010332679383601545, 0.008884402394394053, 0.010605165476860506, 0.01259653324100874, 0.014802672888188937, 0.01612613705968665, 0.008736659056303695, 0.011930869681046672, 0.006442025689967521, 0.013346473845449307, 0.011395861224982819, 0.009033582771387829, 0.010314929427165515, 0.010999959427683188, 0.012807282290010054, 0.012182233710182424, 0.012373707505523272, 0.011526427665917091, 0.014407130755877203, 0.01422780503932552, 0.011276226053317387, 0.01049147579251368, 0.010535850433021349, 0.0072937151751633275, 0.009653107660458633, 0.011309032296351204, 0.010392338551032132, 0.012564463190521076, 0.011531812219057054, 0.009694376838816096, 0.01041870196053723, 0.014721134275214452, 0.01068416503530635, 0.011605611681542616, 0.011924831671473315, 0.00951091717620049, 0.009136481536658572, 0.010743868716684937, 0.010792099508860052, 0.013787267170842261, 0.011668619519359784, 0.009549018416867115, 0.01084958463755135, 0.012891584772487152, 0.01330727490406641, 0.012148833814144357, 0.01207281296934282, 0.011354434901215585, 0.010294658711088466, 0.010035748055482142, 0.009225067271940637, 0.006076665063558704, 0.014097618203781387, 0.008303319845723932, 0.008923840839812971, 0.008131100632379028, 0.012221808867007118, 0.009583321591664667, 0.010604668209033073, 0.014292507725766658, 0.008316009133343348, 0.008568018659255792, 0.007211313249275461, 0.013233751118113927, 0.013087350465808157, 0.012773082018101345, 0.009717568043165467, 0.010780005331658538, 0.011492474448140107, 0.007547325420205157, 0.008581251206697782, 0.009426697969146011, 0.012443189938241157, 0.011970849772520492, 0.01271945192616832, 0.00922513480015788, 0.011484354314911882, 0.009007446167411927, 0.0075460470916460815, 0.004681359681410211, 0.007858006506404872, 0.00888765850107106, 0.008046964209081051, 0.009629818496350117, 0.010852376297746073, 0.00632301019333767, 0.00918918489118421, 0.008567096644946216, 0.007964018252649956, 0.010077822462679489, 0.010372360091875907, 0.013367245177551123, 0.00900782305730126, 0.008777947128968533, 0.006234112953065488, 0.0085550853612676, 0.009453290507653351, 0.01449576335664227, 0.01064299908944667, 0.011581624577183574, 0.009713939699516547, 0.01340252923658285, 0.006270791972297796, 0.010979933328164597, 0.008938948371078625, 0.00872567487396137, 0.011541953959467135, 0.00711887550764981, 0.007315393650853002, 0.01157651656090539, 0.0062017353605286684, 0.009244536193116392, 0.011328593016859172, 0.00976811915794065, 0.007696649142313952, 0.008872977170015653, 0.006531453036299243, 0.011234914622439961, 0.011017217213271977, 0.009579059267797556, 0.011821625023944115, 0.007468567179551378, 0.007229799648415371, 0.011683556236383143, 0.01213741720144358, 0.009411880277801809, 0.005773370788372263, 0.01381942154269648, 0.006690589999303409, 0.007783034191364452, 0.009740533265862673, 0.012162849963086007, 0.010768372536903093, 0.005407650322827863, 0.007247656802466378, 0.011842914967362549, 0.0065077710176181585, 0.011906249367948644, 0.009650412985622407, 0.009558564604648807, 0.012982419349929015, 0.013945659404718872, 0.01178076398567004, 0.006747693900602414, 0.010864983097239952, 0.00822317759182519, 0.011969052203794515, 0.013261119403796338, 0.010181654012999171, 0.00895678752453677, 0.012991996718173667, 0.011414257089845267, 0.01321105808400897, 0.00971683524809445, 0.011964725728354688, 0.012659569118770109, 0.011905462337099264, 0.01230437134174086, 0.011586203418956366, 0.00889699908409737], "moving_avg_accuracy_train": [0.041590044902408634, 0.08928016457006735, 0.14466884635849248, 0.20060460179393336, 0.25520813057494973, 0.30887256735018825, 0.3588492457907416, 0.40574846881571874, 0.45019627501185083, 0.49076663689789346, 0.5300042439260811, 0.5657484590741245, 0.599031782694213, 0.6298818841463217, 0.6574006899235039, 0.6836344956312272, 0.7058685047705777, 0.7270298814126137, 0.7475630714332664, 0.7658340395471841, 0.782926375025836, 0.7985280769935367, 0.8124161489430387, 0.8256011884011251, 0.8372887595527069, 0.8465776059177111, 0.8558375444307758, 0.8650152657686598, 0.8737426059322885, 0.8806859700391334, 0.8879136690911908, 0.8941558924713852, 0.8996901881564171, 0.9049221342955558, 0.9099843444886468, 0.9136405371219619, 0.9164360340396107, 0.9206352087594961, 0.9245539028383268, 0.9270856359164359, 0.9293061390641334, 0.9321254054756418, 0.934785869986448, 0.9372686076521166, 0.9388218750476653, 0.9414078585988697, 0.9435423285414005, 0.945300555024193, 0.9474315858312974, 0.9483544580136715, 0.948343555401673, 0.9499866715127147, 0.9509401005281559, 0.9535140022682067, 0.9542845061688316, 0.9561309811471866, 0.9568349915134573, 0.9582938844752068, 0.9595232188324573, 0.960641281546849, 0.9606267976624207, 0.962113483148578, 0.9631561701384914, 0.9641620177448895, 0.9645348936109044, 0.9654425405950982, 0.9659709602332166, 0.9668441383539518, 0.9685739730304613, 0.969626302996472, 0.9697828854194623, 0.9698400321453732, 0.971081976437988, 0.9723159476929988, 0.9729499023653748, 0.973590216034799, 0.9738688432408429, 0.9743870358881964, 0.9743674892184429, 0.9740267736287783, 0.9743292104397193, 0.9746735192314802, 0.9753019064821509, 0.9751605737208405, 0.975947157717804, 0.9768155546317472, 0.9775622346221531, 0.9774367205718518, 0.9778840827408571, 0.9783820758429711, 0.9787256739872638, 0.9793859376897279, 0.9782713348422206, 0.9790886396020462, 0.9800753299573177, 0.9806006280627765, 0.9806502192743559, 0.9810343230909679, 0.9814660830806898, 0.9816105264464395, 0.9816196177375192, 0.9810976659709193, 0.9814788777964464, 0.9816173553441826, 0.9818815301145355, 0.9808358413138146, 0.9814897013788617, 0.9820060958243089, 0.9818291458026015, 0.9823208964009127, 0.9820543736501256, 0.9823468901541607, 0.9824660678792392, 0.9822595048401618, 0.9821664598620979, 0.9828267670008881, 0.9826375043258084, 0.9828298911325225, 0.9831913402633178, 0.9834375894215098, 0.9831430306281683, 0.9827872469105895, 0.9831180832314353, 0.9834972521773486, 0.983245591282242, 0.983528268017113, 0.9835897257761252, 0.9837845466878077, 0.9844132534773603, 0.9843838514927196, 0.9843481251601235, 0.984497297019111, 0.984371171074352, 0.9849482269204974, 0.9852094856641712, 0.9856677967703731, 0.9859407678373834, 0.9862748335012733, 0.986324340478527, 0.9857550574723409, 0.9858797935405831, 0.986357140413915, 0.9867216123844282, 0.9870426617114616, 0.9872944037248392, 0.9873233338880696, 0.9875075172028432, 0.9872338290611395, 0.9877780242800256, 0.9874842608770323, 0.9865665430476809, 0.9866403574929128, 0.98645807166864, 0.9865962477232137, 0.9867879994389875, 0.9872372686915174, 0.9876346355723656, 0.9880829465687005, 0.986693880928534, 0.9863922383713949, 0.9860393798616363, 0.9865635110719012, 0.9868678905444915, 0.9869279183793466, 0.9871190551128405, 0.9873073542146517, 0.9873489402217579, 0.9873863676281536, 0.9876827941093858, 0.9877566266401231, 0.987957934548739, 0.9880646708557699, 0.988432775942812, 0.9884781493152159, 0.9888188574491705, 0.989065040900682, 0.9892168515427567, 0.9894929900491953, 0.9896415693549993, 0.9898171073599755, 0.9899146376954066, 0.990141923925866, 0.9902186343975743, 0.9889786871399875, 0.9890299232617215, 0.9892759264712636, 0.9895461935336702, 0.9897963732874461, 0.9898122716729871, 0.9897638012021169, 0.9898968890878576, 0.9898190305362146, 0.9900070493575931, 0.9897251874277861, 0.9896505481492932, 0.9899181942272209, 0.9897544998044988, 0.9893236508645621, 0.9892171216781152, 0.9890909824269702, 0.9891285917735589, 0.9894926113164411, 0.9899318360478923, 0.9904108436633411, 0.9901095286422451, 0.9893803268566013, 0.9890867684638075, 0.9893806016245789, 0.9894683041109305, 0.9897937021224565, 0.9894262541197438, 0.989416385404198, 0.989507521007835, 0.9899173529844324, 0.9899560306324178, 0.9898931842656046, 0.989869174618806, 0.9894197746045537, 0.9896802711024317, 0.9900565520279029, 0.9902999819060927, 0.9905678608726448, 0.9907368723294464, 0.9909214626262637, 0.9909062322862564, 0.9910715614385832, 0.990904137437582, 0.9910859521164429, 0.9911938178048079, 0.9913280993052888, 0.9912420144116738, 0.9912041376348197, 0.9914977863713378, 0.9915575292366033, 0.9913625068927234, 0.9909474964558505, 0.9909436136257416, 0.9910377753286437, 0.990969061039827, 0.9910513774060823, 0.9909975789511883, 0.9910073251108407, 0.9900744113866705, 0.9896904821527654, 0.9897913744136794, 0.9895706075080257, 0.9897997446738898, 0.9902454584505485, 0.9901327429626364, 0.9896964775949442, 0.9902152970973546, 0.9902707193590569, 0.990697237452913, 0.9908764906421456], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 476033673, "moving_var_accuracy_train": [0.0155675865148593, 0.03447995548861386, 0.05864311457208725, 0.08093808174007962, 0.09967818176412524, 0.11562920955734485, 0.12654530409316458, 0.13368660776696706, 0.1380984142711106, 0.13910216121607974, 0.13904825334215817, 0.1366422682568889, 0.13294805811199586, 0.12821881113724493, 0.12221249206614089, 0.11618515591672204, 0.10901580078672833, 0.10214445546053046, 0.09572451694629547, 0.0891565197340441, 0.08287019914967272, 0.07677389717330603, 0.07083241433824633, 0.06531378029402334, 0.0600117961394306, 0.05478716052662143, 0.05008016262535093, 0.0458302214834185, 0.041932697532061816, 0.038173320524937646, 0.034826145174727895, 0.031694218831809305, 0.028800452807192654, 0.026166766870099018, 0.02378072393144044, 0.021522961239443562, 0.01944099834264846, 0.017655596123336732, 0.016028241980554227, 0.014483104833907925, 0.013079170058577543, 0.011842787420611326, 0.01072221132126952, 0.009705466065991335, 0.008756633215810876, 0.007941155692573683, 0.007188043780736425, 0.006497061645945915, 0.005888227112058781, 0.005307069638437903, 0.004776363744396648, 0.004323025844946264, 0.0038989045024390074, 0.0035686387837020345, 0.0032171179916797347, 0.002926091421122979, 0.0026379429543730297, 0.002393303977000307, 0.0021675749459575235, 0.0019620680294615894, 0.0017658631145616036, 0.0016091689067182012, 0.001458036781476792, 0.0013213386679947854, 0.0011904561288984144, 0.0010788249234398178, 0.0009734554769213793, 0.000882971889504017, 0.0008216056526261107, 0.0007494116725797766, 0.0006746911678185045, 0.000607251442771195, 0.000560408129127701, 0.0005180714817386655, 0.0004698814203044454, 0.0004265832926312634, 0.00038462366144766764, 0.000348578007880842, 0.0003137236457434439, 0.00028339606518646343, 0.00025587967088932606, 0.0002313586406971476, 0.0002117766114586826, 0.00019077872485759073, 0.00017726928183034427, 0.00016632937244862316, 0.00015471421427641443, 0.00013938457684018038, 0.00012724731534847643, 0.00011675455798140685, 0.00010614163934611914, 9.945100882263156e-05, 0.00010068696350941096, 9.663015079237143e-05, 9.572915642780772e-05, 8.863968368141346e-05, 7.979784890766542e-05, 7.314588569432218e-05, 6.750904732341235e-05, 6.094591756425322e-05, 5.4852069671989345e-05, 5.181876552470051e-05, 4.794479107552593e-05, 4.332289624901692e-05, 3.961870140773418e-05, 4.5498016878536855e-05, 4.479601205265385e-05, 4.271637985698664e-05, 3.872654366292812e-05, 3.703025715509087e-05, 3.3966540829765845e-05, 3.133997989298528e-05, 2.8333811875080908e-05, 2.588444528958915e-05, 2.337391707211626e-05, 2.4960575022740434e-05, 2.2786900762070888e-05, 2.0841324836442625e-05, 1.9933001620172926e-05, 1.8485449289348208e-05, 1.7417788305026364e-05, 1.681524795777103e-05, 1.6118797202711085e-05, 1.580083928834509e-05, 1.4790754214643202e-05, 1.4030834021115025e-05, 1.2661744124288756e-05, 1.1737166400518779e-05, 1.4120899805532349e-05, 1.2716590115286475e-05, 1.145641844132471e-05, 1.051104678881674e-05, 9.60311189540725e-06, 1.1639741752002395e-05, 1.1090072757116152e-05, 1.1871507112016854e-05, 1.1354975231637887e-05, 1.122387651858612e-05, 1.012354733389862e-05, 1.2027940870698653e-05, 1.0965178564113244e-05, 1.1919401045019356e-05, 1.1923019296125854e-05, 1.1658371400010443e-05, 1.1062900631704318e-05, 9.964143157634708e-06, 9.273040282840498e-06, 9.01988304473929e-06, 1.0783230666591239e-05, 1.0481580032376132e-05, 1.7013276157924146e-05, 1.5360985693055788e-05, 1.4123940219327604e-05, 1.2883379795913088e-05, 1.1925960300841757e-05, 1.2549950022175856e-05, 1.2716058961913956e-05, 1.3253297810635385e-05, 2.9293498203791432e-05, 2.7183042473909095e-05, 2.558532037769928e-05, 2.5499210070093503e-05, 2.3783110833092967e-05, 2.143722981840011e-05, 1.962230609457661e-05, 1.797918445080504e-05, 1.6196830569607918e-05, 1.4589754809392664e-05, 1.3921597257435072e-05, 1.2578498715047255e-05, 1.168537271018454e-05, 1.061936919231337e-05, 1.0776944469038191e-05, 9.717778708444117e-06, 9.790739130485091e-06, 9.357121843619696e-06, 8.628827898681796e-06, 8.452217381456971e-06, 7.805677934330034e-06, 7.3024324616164414e-06, 6.657798712418377e-06, 6.456950116184288e-06, 5.864215572793307e-06, 1.9115016829889135e-05, 1.7227141408433296e-05, 1.6049085479535293e-05, 1.5101575496779214e-05, 1.4154727129895336e-05, 1.2741529244871126e-05, 1.148852079930137e-05, 1.0499080187349584e-05, 9.503729755189983e-06, 8.871516474404179e-06, 8.699380154234114e-06, 7.879581335856173e-06, 7.736333009541876e-06, 7.203862484860559e-06, 8.15415351777625e-06, 7.440874374084129e-06, 6.839986932790166e-06, 6.1687184060685995e-06, 6.744438613863254e-06, 7.806260034941728e-06, 9.090668692369663e-06, 8.998718500575538e-06, 1.2884463848193627e-05, 1.237160623319115e-05, 1.1911486947191913e-05, 1.0789563787483058e-05, 1.0663562201880403e-05, 1.0812368293969712e-05, 9.732007988491458e-06, 8.833558473894917e-06, 9.461862867881424e-06, 8.529140225176374e-06, 7.71177319505332e-06, 6.945784043802506e-06, 8.06884899471214e-06, 7.87268992390109e-06, 8.359706945371575e-06, 8.057059201193841e-06, 7.897185547563382e-06, 7.364550845578815e-06, 6.934757960132711e-06, 6.243369833430088e-06, 5.865036407568783e-06, 5.530809931813091e-06, 5.275238135675228e-06, 4.852429382645682e-06, 4.529470136723544e-06, 4.14321860322935e-06, 3.7418085949300222e-06, 4.143693959564712e-06, 3.761447453159574e-06, 3.72760613935556e-06, 4.904948489841137e-06, 4.4145893281839115e-06, 4.052928232006203e-06, 3.6901302901939125e-06, 3.3821011185559613e-06, 3.069939470441141e-06, 2.7638004120487403e-06, 1.032037252154762e-05, 1.0614950179215811e-05, 9.645068396105155e-06, 9.119203796181615e-06, 8.679817983585515e-06, 9.599783121556715e-06, 8.754147840338172e-06, 9.591680295732926e-06, 1.105507535089152e-05, 9.977212459632198e-06, 1.0616750373149689e-05, 9.844260688484951e-06], "duration": 166327.456449, "accuracy_train": [0.4159004490240864, 0.518491241578996, 0.6431669824543189, 0.7040264007129015, 0.7466398896040974, 0.7918524983273348, 0.8086393517557217, 0.8278414760405132, 0.8502265307770396, 0.8558998938722776, 0.8831427071797711, 0.8874463954065154, 0.8985816952750092, 0.9075327972153008, 0.9050699419181433, 0.9197387470007383, 0.9059745870247323, 0.9174822711909376, 0.93236178161914, 0.9302727525724437, 0.9367573943337025, 0.9389433947028424, 0.9374087964885567, 0.9442665435239018, 0.9424768999169435, 0.9301772232027501, 0.9391769910483574, 0.9476147578096161, 0.9522886674049464, 0.9431762470007383, 0.9529629605597084, 0.9503359028931341, 0.9494988493217055, 0.9520096495478036, 0.9555442362264673, 0.9465462708217978, 0.9415955062984496, 0.9584277812384644, 0.9598221495478036, 0.9498712336194168, 0.949290667393411, 0.9574988031792175, 0.9587300505837025, 0.9596132466431341, 0.9528012816076044, 0.9646817105597084, 0.9627525580241787, 0.9611245933693245, 0.9666108630952381, 0.9566603076550388, 0.9482454318936876, 0.9647747165120893, 0.9595209616671282, 0.9766791179286637, 0.9612190412744556, 0.9727492559523809, 0.963171084809893, 0.9714239211309523, 0.9705872280477114, 0.970703845976375, 0.9604964427025655, 0.9754936525239941, 0.9725403530477114, 0.9732146462024732, 0.9678907764050388, 0.9736113634528424, 0.9707267369762828, 0.9747027414405685, 0.9841424851190477, 0.9790972726905685, 0.971192127226375, 0.9703543526785714, 0.9822594750715209, 0.9834216889880952, 0.9786554944167589, 0.9793530390596161, 0.9763764880952381, 0.979050769714378, 0.9741915691906607, 0.9709603333217978, 0.9770511417381875, 0.9777722983573275, 0.9809573917381875, 0.9738885788690477, 0.9830264136904762, 0.9846311268572352, 0.9842823545358066, 0.97630709411914, 0.9819103422619048, 0.9828640137619971, 0.9818180572858989, 0.9853283110119048, 0.9682399092146549, 0.9864443824404762, 0.9889555431547619, 0.9853283110119048, 0.9810965401785714, 0.9844912574404762, 0.9853519229881875, 0.9829105167381875, 0.9817014393572352, 0.9764001000715209, 0.9849097842261905, 0.9828636532738095, 0.9842591030477114, 0.9714246421073275, 0.9873744419642857, 0.9866536458333334, 0.9802365956072352, 0.9867466517857143, 0.9796556688930418, 0.9849795386904762, 0.9835386674049464, 0.9804004374884644, 0.9813290550595238, 0.98876953125, 0.9809341402500923, 0.9845613723929494, 0.9864443824404762, 0.9856538318452381, 0.9804920014880952, 0.9795851934523809, 0.9860956101190477, 0.9869097726905685, 0.9809806432262828, 0.9860723586309523, 0.9841428456072352, 0.9855379348929494, 0.9900716145833334, 0.9841192336309523, 0.9840265881667589, 0.98583984375, 0.9832360375715209, 0.9901417295358066, 0.9875608143572352, 0.9897925967261905, 0.9883975074404762, 0.9892814244762828, 0.9867699032738095, 0.9806315104166666, 0.9870024181547619, 0.9906532622739018, 0.9900018601190477, 0.9899321056547619, 0.9895600818452381, 0.9875837053571429, 0.9891651670358066, 0.9847706357858066, 0.99267578125, 0.9848403902500923, 0.978307082583518, 0.9873046875, 0.9848174992501846, 0.987839832214378, 0.9885137648809523, 0.9912806919642857, 0.9912109375, 0.9921177455357143, 0.9741922901670359, 0.9836774553571429, 0.9828636532738095, 0.9912806919642857, 0.9896073057978036, 0.9874681688930418, 0.9888392857142857, 0.9890020461309523, 0.9877232142857143, 0.9877232142857143, 0.9903506324404762, 0.9884211194167589, 0.9897697057262828, 0.9890252976190477, 0.9917457217261905, 0.9888865096668512, 0.9918852306547619, 0.9912806919642857, 0.9905831473214286, 0.9919782366071429, 0.9909787831072352, 0.9913969494047619, 0.9907924107142857, 0.9921875, 0.9909090286429494, 0.9778191618217055, 0.9894910483573275, 0.9914899553571429, 0.9919785970953304, 0.9920479910714286, 0.9899553571428571, 0.9893275669642857, 0.9910946800595238, 0.9891183035714286, 0.99169921875, 0.9871884300595238, 0.9889787946428571, 0.9923270089285714, 0.98828125, 0.9854460104051311, 0.9882583590000923, 0.9879557291666666, 0.9894670758928571, 0.9927687872023809, 0.9938848586309523, 0.9947219122023809, 0.9873976934523809, 0.9828175107858066, 0.9864447429286637, 0.9920251000715209, 0.9902576264880952, 0.9927222842261905, 0.9861192220953304, 0.9893275669642857, 0.9903277414405685, 0.9936058407738095, 0.9903041294642857, 0.9893275669642857, 0.9896530877976191, 0.9853751744762828, 0.9920247395833334, 0.9934430803571429, 0.9924908508098007, 0.9929787715716132, 0.9922579754406607, 0.9925827752976191, 0.9907691592261905, 0.9925595238095238, 0.9893973214285714, 0.9927222842261905, 0.9921646090000923, 0.9925366328096161, 0.99046725036914, 0.9908632466431341, 0.994140625, 0.9920952150239941, 0.9896073057978036, 0.9872124025239941, 0.9909086681547619, 0.9918852306547619, 0.9903506324404762, 0.9917922247023809, 0.9905133928571429, 0.9910950405477114, 0.98167818786914, 0.9862351190476191, 0.9906994047619048, 0.9875837053571429, 0.9918619791666666, 0.9942568824404762, 0.9891183035714286, 0.9857700892857143, 0.9948846726190477, 0.990769519714378, 0.9945359002976191, 0.9924897693452381], "end": "2016-01-25 08:54:13.401000", "learning_rate_per_epoch": [0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603, 0.00015698156494181603], "accuracy_valid": [0.41892178087349397, 0.5096950301204819, 0.6259103798004518, 0.6792168674698795, 0.7150967149849398, 0.7502544357115963, 0.7620555464043675, 0.776501023625753, 0.7927878506212349, 0.7936320477221386, 0.8163886012801205, 0.8149752329631024, 0.8188505977033133, 0.8234583843185241, 0.8216273296310241, 0.8287985928087349, 0.8166327419051205, 0.8290236139871988, 0.8303252070783133, 0.8329401590737951, 0.8346079631024097, 0.8374876458960843, 0.8300707713667168, 0.8364287227033133, 0.8352697900978916, 0.8197756847703314, 0.8327460231551205, 0.8405188135353916, 0.8440691476844879, 0.8341402720256024, 0.845982563064759, 0.8397658014871988, 0.8407423639871988, 0.8408850244728916, 0.8435808664344879, 0.8345770778426205, 0.8334078501506024, 0.8460325677710843, 0.8474253459149097, 0.8317900508283133, 0.8399599374058735, 0.8408247246799698, 0.8436014566076807, 0.8417601068335843, 0.8385348032756024, 0.8471723809299698, 0.8509359704442772, 0.8436720514871988, 0.8495726068335843, 0.8436838173004518, 0.8327666133283133, 0.8449030496987951, 0.8436117516942772, 0.859166156814759, 0.8447000894201807, 0.8521463784826807, 0.8486372246799698, 0.8455545816076807, 0.8527582007718373, 0.8484857398343373, 0.8443338784826807, 0.8576807228915663, 0.8519125329442772, 0.861414897872741, 0.8441912179969879, 0.855259906814759, 0.8496034920933735, 0.8595117775790663, 0.8671110222138554, 0.8617193382906627, 0.8460325677710843, 0.8499285226844879, 0.8577616128576807, 0.8638151237763554, 0.859898578689759, 0.8573145119540663, 0.8583219597138554, 0.8570806664156627, 0.8588102409638554, 0.8517389871987951, 0.8562349985881024, 0.8565820900790663, 0.860264789627259, 0.8551275414156627, 0.8639577842620482, 0.8667962867093373, 0.8676610739834337, 0.8590029061558735, 0.860020649002259, 0.8620046592620482, 0.8591352715549698, 0.8637636483433735, 0.8430822900978916, 0.8645166603915663, 0.8637739434299698, 0.8643945900790663, 0.8617899331701807, 0.8633768472326807, 0.8615663827183735, 0.860631000564759, 0.8581690041415663, 0.8588396554969879, 0.8666330360504518, 0.8630312264683735, 0.8619120034826807, 0.8471517907567772, 0.8675787132906627, 0.8657167733433735, 0.8549745858433735, 0.8572424463478916, 0.855748188064759, 0.8651681923004518, 0.8644357704254518, 0.8569380059299698, 0.8578527979103916, 0.8656049981174698, 0.8655241081513554, 0.8638651284826807, 0.8683523155120482, 0.8615457925451807, 0.8570600762424698, 0.8596750282379518, 0.8621767342808735, 0.8653402673192772, 0.8624311699924698, 0.8659520896084337, 0.8589514307228916, 0.8591249764683735, 0.8692171027861446, 0.8592573418674698, 0.8637945336031627, 0.8650961266942772, 0.8624826454254518, 0.8732454230986446, 0.866856586502259, 0.8664594903049698, 0.8665506753576807, 0.8661741693335843, 0.8676198936370482, 0.8569380059299698, 0.8615972679781627, 0.8762868858245482, 0.8655035179781627, 0.8691862175263554, 0.8679346291415663, 0.8643739999058735, 0.8654417474585843, 0.8633474326995482, 0.8729395119540663, 0.8635812782379518, 0.8555349326995482, 0.8669683617281627, 0.864710796310241, 0.8682611304593373, 0.8733983786709337, 0.874110210372741, 0.8707834266754518, 0.8708349021084337, 0.8510374505835843, 0.8632562476468373, 0.8564497246799698, 0.874180805252259, 0.8726659567959337, 0.8722688605986446, 0.869297992752259, 0.8699495246611446, 0.8670492516942772, 0.8673860480986446, 0.8707437170557228, 0.8689935523343373, 0.8740396154932228, 0.8743852362575302, 0.873866069747741, 0.8690641472138554, 0.8745881965361446, 0.8746293768825302, 0.8670198371611446, 0.8722894507718373, 0.8707937217620482, 0.874302875564759, 0.8710172722138554, 0.874424945877259, 0.8726247764495482, 0.8561349891754518, 0.8696744987763554, 0.8702848503388554, 0.872157085372741, 0.8781488257718373, 0.8672536826995482, 0.8675993034638554, 0.872767436935241, 0.8753617987575302, 0.8727571418486446, 0.8656961831701807, 0.8714143684111446, 0.8707113610692772, 0.869053852127259, 0.864832866622741, 0.873011577560241, 0.8689729621611446, 0.8703054405120482, 0.8769384177334337, 0.8764898461031627, 0.8813329489834337, 0.8654020378388554, 0.8571512612951807, 0.8666433311370482, 0.8702142554593373, 0.8719026496611446, 0.8726556617093373, 0.8652799675263554, 0.8780782308923193, 0.8704172157379518, 0.8758397849209337, 0.8718717644013554, 0.8728586219879518, 0.8706613563629518, 0.8630518166415663, 0.8803460913968373, 0.8783120764307228, 0.8802240210843373, 0.8770604880459337, 0.8764192512236446, 0.8735601586031627, 0.8744558311370482, 0.8736013389495482, 0.8627870858433735, 0.8803049110504518, 0.8773855186370482, 0.8801534262048193, 0.8734895637236446, 0.8694921286709337, 0.8736116340361446, 0.8771928534450302, 0.8776002447289157, 0.8639989646084337, 0.8778237951807228, 0.8738763648343373, 0.8739572548004518, 0.8738866599209337, 0.8769590079066265, 0.8707525414156627, 0.8619428887424698, 0.8736013389495482, 0.8750058829066265, 0.867518413497741, 0.8751882530120482, 0.8770604880459337, 0.873988140060241, 0.8683317253388554, 0.8783723762236446, 0.8788503623870482, 0.8806314123682228, 0.8744661262236446], "accuracy_test": 0.879, "start": "2016-01-23 10:42:05.945000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0, 211.0, 212.0, 213.0, 214.0, 215.0, 216.0, 217.0, 218.0, 219.0, 220.0, 221.0, 222.0, 223.0, 224.0, 225.0, 226.0, 227.0, 228.0, 229.0, 230.0, 231.0, 232.0, 233.0, 234.0, 235.0, 236.0, 237.0, 238.0, 239.0, 240.0, 241.0, 242.0, 243.0, 244.0, 245.0, 246.0, 247.0, 248.0, 249.0, 250.0, 251.0, 252.0, 253.0], "accuracy_train_last": 0.9924897693452381, "batch_size_eval": 1024, "accuracy_train_std": [0.02007919289367084, 0.01909568939182524, 0.02149183214899306, 0.02425402875243293, 0.022927935280711628, 0.023047607689853117, 0.022864839319290613, 0.02160463058326114, 0.02130324500234118, 0.021443518801097194, 0.023530867404263415, 0.021968120396564633, 0.020639462148144827, 0.01993274451810505, 0.019169528664098212, 0.0187657550745517, 0.020028460880017012, 0.01909985366134538, 0.01784491660497654, 0.016031693554336233, 0.016080952528762554, 0.016276012799641245, 0.016478014719934576, 0.016441099623409408, 0.015417661597435977, 0.018207799971658587, 0.015037060454048219, 0.013844543365978273, 0.0119375249577072, 0.013780193999312176, 0.01307990919562424, 0.013606379551700432, 0.013717327546999671, 0.012700626456929502, 0.012661422879556116, 0.013143419221293947, 0.013114227727833832, 0.010989378707758277, 0.011642398404093066, 0.012464920744074848, 0.01337826362199591, 0.01091299200203729, 0.01099123292934415, 0.01117212174506609, 0.012140613912862561, 0.010242084472824176, 0.011647700178023754, 0.011119856915911396, 0.008991443944944218, 0.010859367048991942, 0.011487624260332183, 0.009759549331506164, 0.009734519572207414, 0.008029055855222861, 0.010297729484988695, 0.008775642666397595, 0.010663968679967328, 0.009393629967026291, 0.00939744582028161, 0.008518291759689298, 0.010615316207585998, 0.007505549569643937, 0.008875462614651887, 0.008573373434336, 0.009019722822804232, 0.007774071642357408, 0.008752616314580634, 0.008635764875213432, 0.006053072895681443, 0.008087717924721195, 0.007945188899685164, 0.009572640755031623, 0.006379475501565355, 0.006528370294271268, 0.007019095140285273, 0.006581647397369503, 0.007510914490925832, 0.007816870306114608, 0.008346036001575187, 0.00807099303289734, 0.007761714849368428, 0.007450674701958758, 0.006262200337366877, 0.009111409427115158, 0.005985711647955389, 0.00577456225141049, 0.005581634203118199, 0.008246316599293974, 0.005762617587298984, 0.0066556651756170294, 0.00674028985914434, 0.0056682626278829725, 0.008375698200989802, 0.005594532235096973, 0.004843067854741094, 0.0051827874688315555, 0.006417284346258404, 0.005298534087729577, 0.004625874065894898, 0.006531855607135863, 0.006820371662148054, 0.006906196044108935, 0.006005594267978014, 0.005952426365270403, 0.005199991277186978, 0.007907380821366319, 0.004752697444715279, 0.0049137166850965605, 0.006832922121072479, 0.004526342767817643, 0.007082087409191105, 0.0053231183144165466, 0.0060273637351027755, 0.006195203623308102, 0.006169711279286488, 0.004655468200610888, 0.006021858527298861, 0.005260902596177195, 0.005203400436228221, 0.005273989027622606, 0.005534489149868883, 0.007089683369956439, 0.005223466336362748, 0.0056712597115425555, 0.006923431497859993, 0.005176524909941054, 0.005524396626099213, 0.005107537063395847, 0.004479820229130491, 0.0053998045708074345, 0.005694464958697659, 0.005290160666771363, 0.006151095889950223, 0.0037758120434252247, 0.005332808471139005, 0.004268469149000342, 0.0049986373359330125, 0.004130180344436257, 0.005508640178223015, 0.006435287775053449, 0.004894369207075886, 0.003261668354835726, 0.00477375191659555, 0.003983883127937179, 0.004568015069209364, 0.005417546830063504, 0.004361861099094912, 0.005234152978362957, 0.0038016716322985195, 0.006316091261509601, 0.007157549857267405, 0.00516308704136285, 0.005156754268207033, 0.005243229757367703, 0.005398753202105219, 0.003351845361811932, 0.0037460160664334686, 0.0038322656606291096, 0.006457728079972354, 0.005817706323100737, 0.0050752731238669, 0.004112359440181713, 0.0038537433127545043, 0.004727779864981428, 0.004698641936324314, 0.005105859490846184, 0.004980162628564073, 0.0048273583602533405, 0.004601269167366595, 0.0040192702204953955, 0.0042212209653502706, 0.004792740302994075, 0.0033573248703406534, 0.0038487544427466924, 0.003663197973358507, 0.003994724752576252, 0.00397437244596584, 0.003106750346563666, 0.0038941512997244417, 0.003529021367169675, 0.004273469173886773, 0.00362902027811136, 0.004297094501716465, 0.007096900807835954, 0.004372618485429817, 0.003126784872398482, 0.0030632988774666444, 0.0031649366680010335, 0.0042099963905343285, 0.004150830230695268, 0.003947894532555172, 0.004375482797628837, 0.0036305841740159332, 0.004566594629672794, 0.004640987704566462, 0.004189915574129801, 0.0051498765624654495, 0.005606845859951615, 0.003664161878073225, 0.00420665624457034, 0.0036758683387989387, 0.003059051171819731, 0.0028595549377438715, 0.0027461375720038484, 0.0043297742183795946, 0.005164412508205762, 0.005062660224114964, 0.0030587012990502625, 0.004411230749944675, 0.003470396569654553, 0.004179898281267865, 0.0036811592639393867, 0.00387667957700251, 0.002763799064716343, 0.004441521141656807, 0.003625219436435905, 0.0044038711295200615, 0.005490006197182663, 0.0036905466239993594, 0.002978006444677337, 0.00399571890132442, 0.0028218101650242826, 0.0031501494994648303, 0.0035853312118456484, 0.003993641914552068, 0.0029524812461588497, 0.005054511999300581, 0.0036490023439124317, 0.003157827289549442, 0.0034090486817445107, 0.0035908397365886704, 0.003853453133394672, 0.002786688665658228, 0.004126382974261999, 0.003647916307877095, 0.00459578642638373, 0.0033727483705647742, 0.003706333880105779, 0.004027611465593868, 0.003591658803578174, 0.003464393700183998, 0.0038080146312800046, 0.005840236895422398, 0.005210408945472211, 0.004521801725272594, 0.00478551551606694, 0.004355171927167855, 0.003183756227768952, 0.004243762852139075, 0.004966465606332957, 0.0027274699216174067, 0.003988610668909587, 0.002737856552430236, 0.003178657863932356], "accuracy_test_std": 0.082516664983505, "error_valid": [0.581078219126506, 0.4903049698795181, 0.37408962019954817, 0.3207831325301205, 0.28490328501506024, 0.24974556428840367, 0.23794445359563254, 0.22349897637424698, 0.2072121493787651, 0.20636795227786142, 0.18361139871987953, 0.18502476703689763, 0.18114940229668675, 0.17654161568147586, 0.17837267036897586, 0.1712014071912651, 0.18336725809487953, 0.17097638601280118, 0.16967479292168675, 0.16705984092620485, 0.1653920368975903, 0.16251235410391573, 0.1699292286332832, 0.16357127729668675, 0.1647302099021084, 0.18022431522966864, 0.16725397684487953, 0.1594811864646084, 0.15593085231551207, 0.16585972797439763, 0.15401743693524095, 0.16023419851280118, 0.15925763601280118, 0.1591149755271084, 0.15641913356551207, 0.16542292215737953, 0.16659214984939763, 0.15396743222891573, 0.1525746540850903, 0.16820994917168675, 0.1600400625941265, 0.15917527532003017, 0.1563985433923193, 0.15823989316641573, 0.16146519672439763, 0.15282761907003017, 0.14906402955572284, 0.15632794851280118, 0.15042739316641573, 0.15631618269954817, 0.16723338667168675, 0.15509695030120485, 0.15638824830572284, 0.14083384318524095, 0.1552999105798193, 0.1478536215173193, 0.15136277532003017, 0.1544454183923193, 0.14724179922816272, 0.15151426016566272, 0.1556661215173193, 0.14231927710843373, 0.14808746705572284, 0.13858510212725905, 0.15580878200301207, 0.14474009318524095, 0.1503965079066265, 0.14048822242093373, 0.1328889777861446, 0.13828066170933728, 0.15396743222891573, 0.15007147731551207, 0.1422383871423193, 0.1361848762236446, 0.14010142131024095, 0.14268548804593373, 0.1416780402861446, 0.14291933358433728, 0.1411897590361446, 0.14826101280120485, 0.14376500141189763, 0.14341790992093373, 0.13973521037274095, 0.14487245858433728, 0.13604221573795183, 0.13320371329066272, 0.13233892601656627, 0.1409970938441265, 0.13997935099774095, 0.13799534073795183, 0.14086472844503017, 0.1362363516566265, 0.1569177099021084, 0.13548333960843373, 0.13622605657003017, 0.13560540992093373, 0.1382100668298193, 0.1366231527673193, 0.1384336172816265, 0.13936899943524095, 0.14183099585843373, 0.14116034450301207, 0.13336696394954817, 0.1369687735316265, 0.1380879965173193, 0.15284820924322284, 0.13242128670933728, 0.1342832266566265, 0.1450254141566265, 0.1427575536521084, 0.14425181193524095, 0.13483180769954817, 0.13556422957454817, 0.14306199407003017, 0.1421472020896084, 0.13439500188253017, 0.1344758918486446, 0.1361348715173193, 0.13164768448795183, 0.1384542074548193, 0.14293992375753017, 0.14032497176204817, 0.1378232657191265, 0.13465973268072284, 0.13756883000753017, 0.13404791039156627, 0.1410485692771084, 0.1408750235316265, 0.1307828972138554, 0.14074265813253017, 0.13620546639683728, 0.13490387330572284, 0.13751735457454817, 0.1267545769013554, 0.13314341349774095, 0.13354050969503017, 0.1334493246423193, 0.13382583066641573, 0.13238010636295183, 0.14306199407003017, 0.13840273202183728, 0.12371311417545183, 0.13449648202183728, 0.1308137824736446, 0.13206537085843373, 0.1356260000941265, 0.13455825254141573, 0.13665256730045183, 0.12706048804593373, 0.13641872176204817, 0.14446506730045183, 0.13303163827183728, 0.13528920368975905, 0.13173886954066272, 0.12660162132906627, 0.12588978962725905, 0.12921657332454817, 0.12916509789156627, 0.14896254941641573, 0.13674375235316272, 0.14355027532003017, 0.12581919474774095, 0.12733404320406627, 0.1277311394013554, 0.13070200724774095, 0.1300504753388554, 0.13295074830572284, 0.1326139519013554, 0.12925628294427716, 0.13100644766566272, 0.12596038450677716, 0.12561476374246983, 0.12613393025225905, 0.1309358527861446, 0.1254118034638554, 0.12537062311746983, 0.1329801628388554, 0.12771054922816272, 0.12920627823795183, 0.12569712443524095, 0.1289827277861446, 0.12557505412274095, 0.12737522355045183, 0.14386501082454817, 0.1303255012236446, 0.1297151496611446, 0.12784291462725905, 0.12185117422816272, 0.13274631730045183, 0.1324006965361446, 0.12723256306475905, 0.12463820124246983, 0.1272428581513554, 0.1343038168298193, 0.1285856315888554, 0.12928863893072284, 0.13094614787274095, 0.13516713337725905, 0.12698842243975905, 0.1310270378388554, 0.12969455948795183, 0.12306158226656627, 0.12351015389683728, 0.11866705101656627, 0.1345979621611446, 0.1428487387048193, 0.13335666886295183, 0.12978574454066272, 0.1280973503388554, 0.12734433829066272, 0.1347200324736446, 0.12192176910768071, 0.12958278426204817, 0.12416021507906627, 0.1281282355986446, 0.12714137801204817, 0.12933864363704817, 0.13694818335843373, 0.11965390860316272, 0.12168792356927716, 0.11977597891566272, 0.12293951195406627, 0.12358074877635539, 0.12643984139683728, 0.12554416886295183, 0.12639866105045183, 0.1372129141566265, 0.11969508894954817, 0.12261448136295183, 0.11984657379518071, 0.1265104362763554, 0.13050787132906627, 0.1263883659638554, 0.12280714655496983, 0.12239975527108427, 0.13600103539156627, 0.12217620481927716, 0.12612363516566272, 0.12604274519954817, 0.12611334007906627, 0.12304099209337349, 0.12924745858433728, 0.13805711125753017, 0.12639866105045183, 0.12499411709337349, 0.13248158650225905, 0.12481174698795183, 0.12293951195406627, 0.12601185993975905, 0.1316682746611446, 0.12162762377635539, 0.12114963761295183, 0.11936858763177716, 0.1255338737763554], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-3, 3], "momentum": 0.7789266552123345, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.00015698155967445185, "patience_threshold": 1, "do_flip": true, "batch_size": 16, "optimization": "adam", "nb_data_augmentation": 2, "learning_rate_decay_method": "none", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 2.2319076938336893e-10, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.09996113867262267}, "accuracy_valid_max": 0.8813329489834337, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import os\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-3, 3)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8744661262236446, "loss_train": [2.4433531761169434, 1.5076031684875488, 1.2815086841583252, 1.053308129310608, 0.8853564262390137, 0.7679980397224426, 0.684473991394043, 0.6195166110992432, 0.5699918270111084, 0.526440441608429, 0.4895268678665161, 0.45857661962509155, 0.4288892447948456, 0.40635907649993896, 0.3832613229751587, 0.3626049757003784, 0.3462944030761719, 0.33046627044677734, 0.311260461807251, 0.2981259822845459, 0.28705480694770813, 0.2777698338031769, 0.2655886709690094, 0.25502777099609375, 0.2482522428035736, 0.23827891051769257, 0.23125551640987396, 0.2250303477048874, 0.2202606499195099, 0.2135983109474182, 0.2077217698097229, 0.201487198472023, 0.19635604321956635, 0.19238066673278809, 0.1886422038078308, 0.18482249975204468, 0.17813603579998016, 0.17700572311878204, 0.17160573601722717, 0.16565541923046112, 0.16343089938163757, 0.1615498661994934, 0.1572073996067047, 0.15479406714439392, 0.15269555151462555, 0.14917711913585663, 0.14540129899978638, 0.14708447456359863, 0.14357750117778778, 0.139131098985672, 0.13938197493553162, 0.1336509883403778, 0.13532333076000214, 0.1284160166978836, 0.1272132247686386, 0.12682703137397766, 0.12534844875335693, 0.12173502892255783, 0.12296327948570251, 0.11907559633255005, 0.11948864161968231, 0.11739109456539154, 0.11641857773065567, 0.11612360179424286, 0.11225283890962601, 0.11236090213060379, 0.10794642567634583, 0.11034172773361206, 0.11025521904230118, 0.10751982778310776, 0.10642671585083008, 0.10413134843111038, 0.10419955104589462, 0.10347249358892441, 0.1002453938126564, 0.10158388316631317, 0.10108041763305664, 0.0992589220404625, 0.09882715344429016, 0.09761393815279007, 0.09553509950637817, 0.09483359009027481, 0.09532579779624939, 0.09397893399000168, 0.09578805416822433, 0.09162500500679016, 0.0922594889998436, 0.09097255021333694, 0.09161563962697983, 0.08785004913806915, 0.08969561010599136, 0.08575063198804855, 0.08752097934484482, 0.08698621392250061, 0.08680487424135208, 0.08573907613754272, 0.08689381182193756, 0.08294914662837982, 0.08336325734853745, 0.08314229547977448, 0.08319805562496185, 0.08438584208488464, 0.08223485946655273, 0.08021987229585648, 0.08098741620779037, 0.0771983191370964, 0.08293912559747696, 0.07930333912372589, 0.07854809612035751, 0.0793202742934227, 0.07565074414014816, 0.07547949999570847, 0.07559443265199661, 0.07745570689439774, 0.075571708381176, 0.07583501935005188, 0.07626092433929443, 0.0749235451221466, 0.07452847063541412, 0.07380058616399765, 0.07624397426843643, 0.07305019348859787, 0.0743347704410553, 0.07320887595415115, 0.07227656990289688, 0.07109590619802475, 0.07512842863798141, 0.0689811185002327, 0.07252522557973862, 0.06649405509233475, 0.07258252799510956, 0.07128215581178665, 0.06780655682086945, 0.06986392289400101, 0.06941372901201248, 0.06924468278884888, 0.06806646287441254, 0.07071535289287567, 0.06465189158916473, 0.07006095349788666, 0.06762087345123291, 0.06590214371681213, 0.06742830574512482, 0.06723392009735107, 0.06604801118373871, 0.06728015094995499, 0.06377790868282318, 0.06456644088029861, 0.06440674513578415, 0.06499756872653961, 0.06337655335664749, 0.06421791017055511, 0.06344277411699295, 0.06280062347650528, 0.0628044605255127, 0.06185011565685272, 0.06295739859342575, 0.06129373982548714, 0.06180763989686966, 0.0639980360865593, 0.059551090002059937, 0.059451669454574585, 0.060727935284376144, 0.060339346528053284, 0.06187640503048897, 0.06021607294678688, 0.06145685911178589, 0.059148579835891724, 0.061312295496463776, 0.05837491899728775, 0.06035791337490082, 0.058097705245018005, 0.06014982983469963, 0.05818614363670349, 0.0606745220720768, 0.058168478310108185, 0.05958698317408562, 0.05716463178396225, 0.057425353676080704, 0.05782969668507576, 0.05784965679049492, 0.05997220799326897, 0.05608878657221794, 0.05874704569578171, 0.054764263331890106, 0.05620119720697403, 0.05820541828870773, 0.05515453219413757, 0.05502289533615112, 0.055379629135131836, 0.05579037219285965, 0.0547630749642849, 0.05355444550514221, 0.056384652853012085, 0.052292030304670334, 0.05611855909228325, 0.055266473442316055, 0.054938096553087234, 0.0556081086397171, 0.051432374864816666, 0.057315148413181305, 0.05430523306131363, 0.05370093509554863, 0.055121906101703644, 0.052843719720840454, 0.05374400317668915, 0.05475238338112831, 0.05338810756802559, 0.05388772487640381, 0.05180804058909416, 0.05108407512307167, 0.052515916526317596, 0.054893601685762405, 0.049614690244197845, 0.05398828163743019, 0.051206354051828384, 0.051136720925569534, 0.05437260866165161, 0.05036800354719162, 0.051093339920043945, 0.050145767629146576, 0.05169243738055229, 0.048503387719392776, 0.05329141020774841, 0.048255227506160736, 0.050467632710933685, 0.051784299314022064, 0.04753696918487549, 0.0529402457177639, 0.04949083924293518, 0.047963254153728485, 0.05141538754105568, 0.05023466795682907, 0.0491793230175972, 0.04886104166507721, 0.05116095021367073, 0.04747391492128372, 0.05225018411874771, 0.049080152064561844, 0.04854387044906616, 0.05011051893234253, 0.05087679997086525, 0.04690825566649437, 0.049133945256471634, 0.048511918634176254, 0.04830425977706909, 0.04983845725655556, 0.049228787422180176, 0.047481320798397064, 0.048581745475530624, 0.04924461618065834, 0.04812409728765488, 0.04533885419368744, 0.047195352613925934], "accuracy_train_first": 0.4159004490240864, "model": "residualv2", "loss_std": [13.60012149810791, 0.27041056752204895, 0.2796223759651184, 0.2772597670555115, 0.2683091461658478, 0.25523820519447327, 0.24504533410072327, 0.23881766200065613, 0.22887428104877472, 0.2168285995721817, 0.2127489596605301, 0.20474481582641602, 0.19711355865001678, 0.19516746699810028, 0.1839400678873062, 0.18145965039730072, 0.17461465299129486, 0.170269176363945, 0.16345497965812683, 0.159837007522583, 0.15417344868183136, 0.1530037373304367, 0.15117695927619934, 0.1461305171251297, 0.14218670129776, 0.13933241367340088, 0.13850916922092438, 0.13619104027748108, 0.1333310604095459, 0.13086453080177307, 0.1284043937921524, 0.12807007133960724, 0.12261074781417847, 0.12269352376461029, 0.12055732309818268, 0.11871099472045898, 0.12008121609687805, 0.11649883538484573, 0.11369103938341141, 0.11021189391613007, 0.1113348975777626, 0.11433681100606918, 0.10964421182870865, 0.10947046428918839, 0.10798516124486923, 0.10641839355230331, 0.10376782715320587, 0.10718822479248047, 0.10530737042427063, 0.10434319078922272, 0.1011626198887825, 0.1022297590970993, 0.10389579832553864, 0.09568794071674347, 0.09692955762147903, 0.0979001596570015, 0.09857959300279617, 0.09570052474737167, 0.09642906486988068, 0.09634274244308472, 0.09305346012115479, 0.09268765896558762, 0.09412723779678345, 0.09560297429561615, 0.09274189919233322, 0.09168080985546112, 0.08791851252317429, 0.09281400591135025, 0.09311768412590027, 0.08820886164903641, 0.08844860643148422, 0.08705289661884308, 0.08984094858169556, 0.08862553536891937, 0.08791374415159225, 0.08769631385803223, 0.08885223418474197, 0.08657054603099823, 0.08837030827999115, 0.0854993611574173, 0.08315440267324448, 0.08576261252164841, 0.08405856043100357, 0.08592401444911957, 0.08764369785785675, 0.0836469754576683, 0.08247417211532593, 0.08296855539083481, 0.082678884267807, 0.08146893978118896, 0.08158298581838608, 0.07944824546575546, 0.08329261094331741, 0.08092641830444336, 0.08068086206912994, 0.07880999147891998, 0.0815061554312706, 0.07831933349370956, 0.07935254275798798, 0.0801878273487091, 0.07957423478364944, 0.08229934424161911, 0.07924222946166992, 0.07538110762834549, 0.07901398092508316, 0.07558663934469223, 0.07991399616003036, 0.07743094116449356, 0.07841672748327255, 0.07857401669025421, 0.07431221753358841, 0.07834313809871674, 0.07567013800144196, 0.07984317094087601, 0.07517796009778976, 0.0772942379117012, 0.07819999009370804, 0.0774252638220787, 0.07583995163440704, 0.07250058650970459, 0.07735832035541534, 0.07375220209360123, 0.07573442906141281, 0.07224196940660477, 0.07421794533729553, 0.07131942361593246, 0.07867256551980972, 0.07324402779340744, 0.07247547805309296, 0.07098338752985, 0.07643528282642365, 0.07552118599414825, 0.07092598080635071, 0.07308275997638702, 0.0733821913599968, 0.07388098537921906, 0.07279209047555923, 0.07507889717817307, 0.068711057305336, 0.07559859752655029, 0.07326342910528183, 0.06979622691869736, 0.0731997936964035, 0.07371184229850769, 0.07347268611192703, 0.07305219024419785, 0.06898675113916397, 0.07504129409790039, 0.07099033147096634, 0.0693511962890625, 0.06919437646865845, 0.07104828953742981, 0.07052803784608841, 0.06901226192712784, 0.070210762321949, 0.06978283077478409, 0.07009164243936539, 0.06851455569267273, 0.07148940861225128, 0.0714806541800499, 0.06691715866327286, 0.06535790115594864, 0.06930191069841385, 0.06541218608617783, 0.07159962505102158, 0.06735935807228088, 0.06599373370409012, 0.06833362579345703, 0.06870623677968979, 0.0659995973110199, 0.06909804046154022, 0.06812435388565063, 0.06967552751302719, 0.06727451831102371, 0.06828022748231888, 0.06635066121816635, 0.06872264295816422, 0.06371951848268509, 0.06577622890472412, 0.06967400014400482, 0.07029860466718674, 0.07075367867946625, 0.06541790068149567, 0.06891129165887833, 0.064981609582901, 0.0687711164355278, 0.06895409524440765, 0.0657276138663292, 0.06557343155145645, 0.0671004205942154, 0.06583600491285324, 0.06559556722640991, 0.06374480575323105, 0.06914534419775009, 0.06101415306329727, 0.06627785414457321, 0.06592211872339249, 0.06323672086000443, 0.06542487442493439, 0.06172454357147217, 0.07118389755487442, 0.06679009646177292, 0.0658811628818512, 0.0642690435051918, 0.06395522505044937, 0.06876983493566513, 0.06645128130912781, 0.06982073932886124, 0.06780315935611725, 0.06233718618750572, 0.06229199469089508, 0.06539787352085114, 0.0674033984541893, 0.05952022224664688, 0.06607311218976974, 0.062373556196689606, 0.0636335238814354, 0.06490685045719147, 0.05953096225857735, 0.06341052800416946, 0.06547031551599503, 0.06293616443872452, 0.05987810343503952, 0.06724970042705536, 0.05902574583888054, 0.061938099563121796, 0.06466197222471237, 0.05945553258061409, 0.06654826551675797, 0.06533806025981903, 0.058258142322301865, 0.0651310458779335, 0.06113506481051445, 0.06485685706138611, 0.06550920754671097, 0.0679008886218071, 0.05911600589752197, 0.06439030915498734, 0.06442856788635254, 0.06096171215176582, 0.06546293944120407, 0.06350554525852203, 0.060337748378515244, 0.06337627023458481, 0.06211922690272331, 0.06187889724969864, 0.06561025232076645, 0.06238256394863129, 0.06272416561841965, 0.06548158079385757, 0.06387904286384583, 0.06051414832472801, 0.061047881841659546, 0.06136602908372879]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:12 2016", "state": "available"}], "summary": "046826cbe8ce7d547088a73ddf2eab5f"}