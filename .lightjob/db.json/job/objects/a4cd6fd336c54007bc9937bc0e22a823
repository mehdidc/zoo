{"content": {"hp_model": {"f0": 16, "f1": 64, "f2": 32, "f3": 64, "nonlin": "very_leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.021312636019927726, 0.009892350970611167, 0.00922072332482447, 0.008077541433683798, 0.008848937710324259, 0.009021965141877067, 0.005711712915142959, 0.010129915954527802, 0.009503253332397788, 0.009310202511571669, 0.013484159382816463, 0.012305710117400451, 0.014141201618725176, 0.013848613044761461, 0.0074245940872066166, 0.008014419098433609, 0.014049807026920314, 0.009133429828416426, 0.0068892514884774155, 0.013185639548588007, 0.012367310635237265, 0.011622142401373239, 0.011246098885641867, 0.013978917307968507, 0.01449041400346115, 0.010784787830546682, 0.015487920060826246, 0.009436471854011247, 0.010917264104741402, 0.010987692350923494, 0.009589922307045105, 0.009713606017553875, 0.008991877845681534, 0.00896190772842394, 0.007607164698548602, 0.011941928055113564, 0.010540091643189212, 0.011063895313624957, 0.010205867693319763, 0.01010127177840522, 0.010041009673639708, 0.010338660677111916, 0.010333284775410257, 0.010554819428234389, 0.01028084683773435, 0.01057789789643132, 0.01057789789643132, 0.010508379733164824, 0.010459789680546531, 0.010245866733145656, 0.010245866733145656, 0.010756875060339245, 0.010511512859757056, 0.010612945762967426, 0.01091888389497936, 0.010818751258133372, 0.01091888389497936, 0.010973312162303583, 0.010962467780041403, 0.01091888389497936, 0.011015303591331042, 0.011084031507469994, 0.011084031507469994, 0.011084031507469994, 0.011148375035546523, 0.011416508922349546, 0.011416508922349546, 0.011416508922349546, 0.011416508922349546, 0.011416508922349546, 0.011231011784776883, 0.011199147671527519, 0.011834225232536081, 0.011950918481097007, 0.012021861743116704, 0.011437373524209565, 0.011464677200263592, 0.011374664903094094, 0.011261462628296845, 0.011261462628296845, 0.011261462628296845, 0.011261462628296845, 0.01096105926969117, 0.01096105926969117, 0.01096105926969117, 0.010741418180120104, 0.010741418180120104, 0.010741418180120104, 0.010741418180120104, 0.010741418180120104], "moving_avg_accuracy_train": [0.056728421898071237, 0.12337480930174877, 0.18948031500178797, 0.25231382333411195, 0.3109912036033678, 0.3655373715182248, 0.41563796512729195, 0.46083313107188095, 0.5029222906541078, 0.5418186603566926, 0.5778183758259605, 0.611133939988704, 0.6406763136566387, 0.6692940164779442, 0.6962217519194817, 0.7194640555192925, 0.7418235047281144, 0.7623471869530235, 0.7810323064994616, 0.7987209169924648, 0.8148730010730264, 0.8291262446395886, 0.8430189738578002, 0.8559039708518652, 0.8671865370084192, 0.8776708374361765, 0.8871881601271288, 0.8958908261822915, 0.9038116533843373, 0.9112706050459497, 0.9179883839366574, 0.9241016700584955, 0.9301152684527013, 0.9355297240098398, 0.9405167023517499, 0.9453467436856502, 0.9498984300302173, 0.9540205243772325, 0.9577420350335938, 0.9611122849147858, 0.9641571355519064, 0.9669021514229339, 0.9693842914509063, 0.9716414689641767, 0.9736799041725487, 0.9755237964553215, 0.9771879498074361, 0.9786949884195774, 0.980058298616933, 0.9812829526457436, 0.9823921167181017, 0.983395014680843, 0.9843022731449294, 0.985118805762607, 0.9858536851185169, 0.9865150765388357, 0.9871056785195037, 0.9876418705997239, 0.9881244434719221, 0.98856108420571, 0.988954060866119, 0.9893100650092967, 0.9896304687381566, 0.9899188320941306, 0.9901806842633166, 0.9904186763643936, 0.9906398447017916, 0.9908435465030686, 0.9910315284218371, 0.9912007121487286, 0.9913506523541216, 0.9914832733901656, 0.9916049574714147, 0.9917191234421581, 0.9918241979646366, 0.9919187650348672, 0.9920038753980748, 0.9920781495761521, 0.9921449963364217, 0.9922051584206644, 0.9922616294452923, 0.9923147785162669, 0.9923626126801441, 0.9924010131300145, 0.9924378986837074, 0.9924734208308406, 0.9925053907632604, 0.9925388140000573, 0.9925688949131745, 0.9925959677349799], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 463066583, "moving_var_accuracy_train": [0.028963024659410117, 0.06604239077911873, 0.09876759265592813, 0.12442328131446936, 0.1429682677803881, 0.15544900091011127, 0.16249472613892824, 0.16462868074786585, 0.16410928886212267, 0.1613147081602718, 0.1568470529690589, 0.15115168901148995, 0.14389128668776388, 0.13687291425190448, 0.12971154925079892, 0.12160223641535102, 0.11394151749411292, 0.10633835953332309, 0.09884672681217295, 0.09177803660151433, 0.08494824132267267, 0.07828181175991515, 0.0721907019100986, 0.06646584004692231, 0.06096492273392324, 0.05585771545966614, 0.05108715879453297, 0.0466600704832888, 0.04255871896704195, 0.03880357070935019, 0.03532937061743512, 0.03213278396055874, 0.029244975855324023, 0.026584325230613673, 0.02414972228439643, 0.021944713749541454, 0.01993670301180118, 0.018095957666872314, 0.016411008674273777, 0.01487213506520148, 0.013468361597302692, 0.01218934144676216, 0.011025856474152109, 0.009969124479674619, 0.009009608994595731, 0.008139247543890381, 0.007350247446915531, 0.006635663190630339, 0.005988824403815231, 0.005403439960846245, 0.004874168169216311, 0.004395803591207717, 0.00396363129337285, 0.003573268693677148, 0.0032208022533191153, 0.0029026589754850467, 0.0026155323742326616, 0.002356566654331413, 0.0021230058780911058, 0.0019124211864556203, 0.0017225689437106947, 0.0015514526998892625, 0.0013972313568455423, 0.0012582566019866052, 0.0011330480408145117, 0.0010202529988946358, 0.0009186679379063781, 0.0008271745939303321, 0.0007447751693533536, 0.0006705552606190227, 0.0006037020731438598, 0.0005434901608822865, 0.0004892744079347232, 0.0004404642719611328, 0.00039651721066248635, 0.0003569459759731859, 0.0003213165723411953, 0.0002892345649888374, 0.00026035132489418054, 0.00023434876769218627, 0.00021094259171257037, 0.00018987375595502255, 0.00017090697332462478, 0.00015382954734311454, 0.00013845883750544417, 0.0001246243101613323, 0.00011217107783440942, 0.0001009640240657903, 9.087576541121695e-05, 8.179478530921987e-05], "duration": 67675.675345, "accuracy_train": [0.5672842189807125, 0.7231922959348468, 0.7844298663021411, 0.8178153983250278, 0.8390876260266703, 0.856452882751938, 0.8665433076088963, 0.867589624573182, 0.8817247268941492, 0.8918859876799556, 0.9018158150493725, 0.910974017453396, 0.906557676668051, 0.9268533418696937, 0.9385713708933187, 0.9286447879175894, 0.9430585476075121, 0.9470603269772055, 0.9491983824174051, 0.9579184114294942, 0.9602417577980805, 0.957405436738649, 0.9680535368217055, 0.9718689437984496, 0.9687296324174051, 0.9720295412859912, 0.9728440643456996, 0.974214820678756, 0.9750990982027501, 0.9784011700004615, 0.9784483939530271, 0.9791212451550388, 0.9842376540005537, 0.9842598240240864, 0.9853995074289406, 0.988817115690753, 0.9908636071313216, 0.9911193735003692, 0.9912356309408453, 0.991444533845515, 0.9915607912859912, 0.9916072942621816, 0.9917235517026578, 0.9919560665836102, 0.9920258210478959, 0.9921188270002769, 0.9921653299764673, 0.9922583359288483, 0.9923280903931341, 0.9923048389050388, 0.9923745933693245, 0.992421096345515, 0.9924675993217055, 0.9924675993217055, 0.9924675993217055, 0.9924675993217055, 0.992421096345515, 0.9924675993217055, 0.9924675993217055, 0.9924908508098007, 0.9924908508098007, 0.9925141022978959, 0.9925141022978959, 0.9925141022978959, 0.9925373537859912, 0.9925606052740864, 0.9926303597383721, 0.9926768627145626, 0.992723365690753, 0.992723365690753, 0.9927001142026578, 0.9926768627145626, 0.9927001142026578, 0.9927466171788483, 0.9927698686669435, 0.9927698686669435, 0.9927698686669435, 0.9927466171788483, 0.9927466171788483, 0.9927466171788483, 0.9927698686669435, 0.9927931201550388, 0.9927931201550388, 0.9927466171788483, 0.9927698686669435, 0.9927931201550388, 0.9927931201550388, 0.9928396231312293, 0.9928396231312293, 0.9928396231312293], "end": "2016-01-23 13:57:04.419000", "learning_rate_per_epoch": [0.0019716029055416584, 0.0013941337820142508, 0.001138305407948792, 0.0009858014527708292, 0.0008817276102490723, 0.0008049035095609725, 0.000745195837225765, 0.0006970668910071254, 0.0006572009297087789, 0.0006234755855984986, 0.0005944606382399797, 0.000569152703974396, 0.0005468242452479899, 0.0005269329994916916, 0.0005090656923130155, 0.0004929007263854146, 0.00047818393795751035, 0.0004647112509701401, 0.00045231671538203955, 0.00044086380512453616, 0.00043023901525884867, 0.0004203471471555531, 0.00041110761230811477, 0.0004024517547804862, 0.0003943205811083317, 0.0003866631304845214, 0.00037943513598293066, 0.0003725979186128825, 0.00036611745599657297, 0.0003599638002924621, 0.000354110321495682, 0.0003485334455035627, 0.00034321201383136213, 0.00033812710898928344, 0.00033326170523650944, 0.00032860046485438943, 0.00032412950531579554, 0.00031983622466214, 0.00031570912688039243, 0.0003117377927992493, 0.0003079126472584903, 0.00030422493000514805, 0.0003006666083820164, 0.00029723031911998987, 0.0002939091937150806, 0.00029069697484374046, 0.00028758784173987806, 0.000284576351987198, 0.00028165755793452263, 0.00027882674476131797, 0.0002760796342045069, 0.00027341212262399495, 0.0002708204847294837, 0.0002683011698536575, 0.0002658508892636746, 0.0002634664997458458, 0.0002611451782286167, 0.00025888413074426353, 0.0002566808252595365, 0.00025453284615650773, 0.00025243786512874067, 0.0002503938158042729, 0.0002483986027073115, 0.0002464503631927073, 0.000244547234615311, 0.0002426875289529562, 0.00024086963094305247, 0.00023909196897875518, 0.00023735308786854148, 0.0002356516197323799, 0.00023398621124215424, 0.00023235562548507005, 0.00023075865465216339, 0.00022919417824596167, 0.00022766109032090753, 0.00022615835769101977, 0.00022468499082606286, 0.00022324005840346217, 0.00022182265820447356, 0.00022043190256226808, 0.00021906699112150818, 0.00021772710897494107, 0.00021641152852680534, 0.00021511950762942433, 0.00021385036234278232, 0.00021260340872686356, 0.00021137800649739802, 0.00021017357357777655, 0.00020898948423564434, 0.0002078251854982227], "accuracy_valid": [0.5622190912085843, 0.7006203525037651, 0.7521560852786144, 0.7782497176204819, 0.789196336125753, 0.7933673169239458, 0.7929908108998494, 0.7811794051204819, 0.7824721738516567, 0.7887183499623494, 0.7854415709713856, 0.7888713055346386, 0.782470703125, 0.7899478774472892, 0.7987281155873494, 0.7891448606927711, 0.7949836455195783, 0.8063067700489458, 0.8075686535203314, 0.8006194700677711, 0.8037021131400602, 0.8029799863516567, 0.8108233716114458, 0.810802781438253, 0.8115249082266567, 0.8089511365775602, 0.8057773084525602, 0.8110572171498494, 0.8107115963855422, 0.8103247952748494, 0.8123088055346386, 0.8101218349962349, 0.8137633541980422, 0.8155841137989458, 0.8144354762801205, 0.8157061841114458, 0.8190329678087349, 0.8208743175828314, 0.8216273296310241, 0.8217493999435241, 0.8216273296310241, 0.8213831890060241, 0.8212611186935241, 0.8211390483810241, 0.8213831890060241, 0.8208846126694277, 0.8208846126694277, 0.8210066829819277, 0.8211287532944277, 0.8208846126694277, 0.8208846126694277, 0.8206301769578314, 0.8207522472703314, 0.8208743175828314, 0.8207419521837349, 0.8209860928087349, 0.8207419521837349, 0.8208640224962349, 0.8207419521837349, 0.8207419521837349, 0.8209860928087349, 0.8212302334337349, 0.8212302334337349, 0.8212302334337349, 0.8212302334337349, 0.8211081631212349, 0.8211081631212349, 0.8211081631212349, 0.8211081631212349, 0.8211081631212349, 0.8208640224962349, 0.8207419521837349, 0.8203654461596386, 0.8206095867846386, 0.8207316570971386, 0.8211081631212349, 0.8212302334337349, 0.8211081631212349, 0.8209860928087349, 0.8209860928087349, 0.8209860928087349, 0.8209860928087349, 0.8209860928087349, 0.8209860928087349, 0.8209860928087349, 0.8206198818712349, 0.8206198818712349, 0.8206198818712349, 0.8206198818712349, 0.8206198818712349], "accuracy_test": 0.8104, "start": "2016-01-22 19:09:08.744000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0], "accuracy_train_last": 0.9928396231312293, "batch_size_eval": 1024, "accuracy_train_std": [0.02306364437206244, 0.028376588826720155, 0.025226756467656028, 0.025000371609397987, 0.02342398054179787, 0.02537741989544092, 0.02336159041565695, 0.024662842269789043, 0.02553725587466641, 0.021558317355274893, 0.019159963333705305, 0.019996239817830545, 0.02087738250193835, 0.016684003294245967, 0.01744705807727059, 0.015293985958317366, 0.01465558162668927, 0.01177296297883795, 0.0111710987175118, 0.010289611325599088, 0.011508905220222356, 0.010193157080490638, 0.00902746206698862, 0.007079896613835505, 0.008594392174676784, 0.00899810220153493, 0.006930356162867611, 0.008480751860799373, 0.007696359183943406, 0.006076791476121314, 0.00607353901709696, 0.005988428848863265, 0.005211069173737974, 0.004825152909250267, 0.004199734650103679, 0.003960841076110572, 0.0031529761561856343, 0.002842190147335598, 0.002831554244507802, 0.0028585980692826893, 0.0029447267761793243, 0.0030376116588383822, 0.002955570827985749, 0.0030130943839132007, 0.003036394533941972, 0.002964284946079757, 0.002980274456803371, 0.0030473311574517746, 0.0030337039505069683, 0.003119552391003651, 0.003031191414133718, 0.0030204544364448214, 0.003038995856087227, 0.002970989439419338, 0.003038995856087227, 0.0030835003144853755, 0.003072626960996288, 0.0030613289600063195, 0.003031514926671041, 0.002980156331366422, 0.002980156331366422, 0.0029738826272830946, 0.0029738826272830946, 0.0029738826272830946, 0.002944368107230091, 0.0029299102516742647, 0.0029162905117979965, 0.0029321729858124003, 0.0029702593527707118, 0.0029702593527707118, 0.0029781749229728987, 0.0029553134532204943, 0.002970540827717845, 0.002992645378417881, 0.0029614927649769043, 0.0029614927649769043, 0.0029614927649769043, 0.0029467690755893704, 0.0029467690755893704, 0.0029467690755893704, 0.002953815555230986, 0.0029683221663235906, 0.0029683221663235906, 0.003022842803010099, 0.0030297123561791564, 0.003013870411813362, 0.003013870411813362, 0.003049164546105222, 0.003049164546105222, 0.003049164546105222], "accuracy_test_std": 0.09818014055805786, "error_valid": [0.43778090879141573, 0.2993796474962349, 0.24784391472138556, 0.2217502823795181, 0.21080366387424698, 0.2066326830760542, 0.20700918910015065, 0.2188205948795181, 0.21752782614834332, 0.21128165003765065, 0.21455842902861444, 0.21112869446536142, 0.217529296875, 0.21005212255271077, 0.20127188441265065, 0.21085513930722888, 0.20501635448042166, 0.1936932299510542, 0.19243134647966864, 0.19938052993222888, 0.19629788685993976, 0.19702001364834332, 0.1891766283885542, 0.18919721856174698, 0.18847509177334332, 0.19104886342243976, 0.19422269154743976, 0.18894278285015065, 0.18928840361445776, 0.18967520472515065, 0.18769119446536142, 0.1898781650037651, 0.18623664580195776, 0.1844158862010542, 0.18556452371987953, 0.1842938158885542, 0.1809670321912651, 0.17912568241716864, 0.17837267036897586, 0.17825060005647586, 0.17837267036897586, 0.17861681099397586, 0.17873888130647586, 0.17886095161897586, 0.17861681099397586, 0.1791153873305723, 0.1791153873305723, 0.1789933170180723, 0.1788712467055723, 0.1791153873305723, 0.1791153873305723, 0.17936982304216864, 0.17924775272966864, 0.17912568241716864, 0.1792580478162651, 0.1790139071912651, 0.1792580478162651, 0.1791359775037651, 0.1792580478162651, 0.1792580478162651, 0.1790139071912651, 0.1787697665662651, 0.1787697665662651, 0.1787697665662651, 0.1787697665662651, 0.1788918368787651, 0.1788918368787651, 0.1788918368787651, 0.1788918368787651, 0.1788918368787651, 0.1791359775037651, 0.1792580478162651, 0.17963455384036142, 0.17939041321536142, 0.17926834290286142, 0.1788918368787651, 0.1787697665662651, 0.1788918368787651, 0.1790139071912651, 0.1790139071912651, 0.1790139071912651, 0.1790139071912651, 0.1790139071912651, 0.1790139071912651, 0.1790139071912651, 0.1793801181287651, 0.1793801181287651, 0.1793801181287651, 0.1793801181287651, 0.1793801181287651], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-3, 3], "momentum": 0.8380991165732739, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.001971602856213758, "patience_threshold": 1, "do_flip": true, "batch_size": 16, "optimization": "rmsprop", "nb_data_augmentation": 0, "learning_rate_decay_method": "sqrt", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 1.3646894021968786e-08, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.0359995996291523}, "accuracy_valid_max": 0.8217493999435241, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import os\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-3, 3)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8206198818712349, "loss_train": [1.5300343036651611, 1.0267362594604492, 0.769576907157898, 0.6270342469215393, 0.5267282128334045, 0.44250014424324036, 0.36754003167152405, 0.2986564338207245, 0.2394775152206421, 0.1918727308511734, 0.15643873810768127, 0.13028031587600708, 0.10977998375892639, 0.08961305022239685, 0.07548552006483078, 0.06712357699871063, 0.058693550527095795, 0.05138825997710228, 0.04331454634666443, 0.03580491617321968, 0.03538481146097183, 0.027638787403702736, 0.024512879550457, 0.023333026096224785, 0.01984276808798313, 0.0178522951900959, 0.01539914496243, 0.014395782724022865, 0.014063444919884205, 0.008777832612395287, 0.009393165819346905, 0.008026358671486378, 0.006654241122305393, 0.00475517101585865, 0.0036677704192698, 0.002368135843425989, 0.00157079566270113, 0.0013964854879304767, 0.001370763755403459, 0.0013632287736982107, 0.0013585883425548673, 0.001355180749669671, 0.0013525334652513266, 0.0013503571972250938, 0.0013485359959304333, 0.0013469801051542163, 0.0013456278247758746, 0.001344424788840115, 0.0013433448038995266, 0.0013423740165308118, 0.001341489958576858, 0.001340672024525702, 0.0013399220770224929, 0.001339230453595519, 0.0013385905185714364, 0.0013379885349422693, 0.0013374228728935122, 0.0013368907384574413, 0.0013363902689889073, 0.0013359119184315205, 0.001335460226982832, 0.0013350311201065779, 0.001334620639681816, 0.0013342314632609487, 0.0013338560238480568, 0.0013334983959794044, 0.0013331539230421185, 0.0013328207423910499, 0.001332500483840704, 0.0013321922160685062, 0.0013318952405825257, 0.0013316081603989005, 0.0013313287636265159, 0.0013310590293258429, 0.0013307974440976977, 0.0013305424945428967, 0.001330294064246118, 0.0013300536666065454, 0.0013298193225637078, 0.001329590566456318, 0.0013293673982843757, 0.0013291486538946629, 0.0013289368944242597, 0.0013287296751514077, 0.0013285271124914289, 0.0013283283915370703, 0.001328134792856872, 0.0013279450358822942, 0.0013277583057060838, 0.0013275757664814591], "accuracy_train_first": 0.5672842189807125, "model": "residualv4", "loss_std": [0.3577949106693268, 0.28864437341690063, 0.2672223448753357, 0.2558472752571106, 0.24366916716098785, 0.2293417751789093, 0.21484453976154327, 0.1954370141029358, 0.17601986229419708, 0.15706665813922882, 0.14083024859428406, 0.12790581583976746, 0.1106911301612854, 0.09854050725698471, 0.0900401920080185, 0.08231896907091141, 0.076677605509758, 0.07207845896482468, 0.0656297579407692, 0.05738753452897072, 0.059148602187633514, 0.05091915652155876, 0.04646240547299385, 0.04724632576107979, 0.04117681831121445, 0.04046766459941864, 0.036467596888542175, 0.03350844234228134, 0.03525902330875397, 0.02340622805058956, 0.0284155011177063, 0.02308828942477703, 0.021662816405296326, 0.016097627580165863, 0.012186605483293533, 0.005201078951358795, 0.000957269046921283, 0.00012497529678512365, 5.157904524821788e-05, 4.054982855450362e-05, 3.46969100064598e-05, 3.064447810174897e-05, 2.7652198696159758e-05, 2.5298284526797943e-05, 2.3392638468067162e-05, 2.182165735575836e-05, 2.050042530754581e-05, 1.934970532602165e-05, 1.834393697208725e-05, 1.7458927686675452e-05, 1.6665886505506933e-05, 1.5951247405610047e-05, 1.5307377907447517e-05, 1.4723282220074907e-05, 1.419461113982834e-05, 1.3707785001315642e-05, 1.3256107195047662e-05, 1.284231348108733e-05, 1.245762177859433e-05, 1.2097067155991681e-05, 1.1759269000322092e-05, 1.144651287177112e-05, 1.1151038052048534e-05, 1.0878337889153045e-05, 1.061749571817927e-05, 1.0373818440712057e-05, 1.0142967767023947e-05, 9.922230674419552e-06, 9.712432984088082e-06, 9.514272278465796e-06, 9.326245162810665e-06, 9.146991942543536e-06, 8.975282071332913e-06, 8.812635314825457e-06, 8.656892532599159e-06, 8.506968697474804e-06, 8.362550943274982e-06, 8.224864359362982e-06, 8.093215910776053e-06, 7.966009434312582e-06, 7.842872037144843e-06, 7.72391285863705e-06, 7.610902230226202e-06, 7.501046638935804e-06, 7.396089131361805e-06, 7.293299404409481e-06, 7.1954964369069785e-06, 7.100326911313459e-06, 7.00824557497981e-06, 6.919315637787804e-06]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:09 2016", "state": "available"}], "summary": "619e08fd9b4aa6d3754d964405f7ffe7"}