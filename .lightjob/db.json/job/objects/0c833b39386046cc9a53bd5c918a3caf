{"content": {"hp_model": {"f0": 32, "f1": 32, "f2": 16, "f3": 16, "nonlin": "leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.015461275576049337, 0.015285926523305545, 0.020992815729001812, 0.01366803816859591, 0.012822306058954201, 0.015651461125496372, 0.015438537090309758, 0.01474690952131099, 0.01612593639642048, 0.01258486207879783, 0.015835550972082155, 0.015056788884780908, 0.012185375703012395, 0.01462984012475805, 0.015361775270921123, 0.013672602037935544, 0.012609360568970855, 0.014639553669628087, 0.013280693957789665, 0.015267609313868447, 0.014820695172697346, 0.018190803389366777, 0.013516294841061986, 0.015820237352864717, 0.0149646806619066, 0.015029338937283691, 0.015771318838521963, 0.014154236590441223, 0.015598429851364686, 0.015383922029210385, 0.014586708723611997, 0.012266443567196231, 0.013949353431153442, 0.012238910416232629, 0.015020153901357504, 0.013992017511522065, 0.01310873709549992, 0.01246372309818562, 0.015539333160648905, 0.012360913604341326, 0.011612915421874113, 0.013968642798377751, 0.014153490890274832, 0.012955447296956392, 0.013089693221654802, 0.01223819983481503, 0.01280134776824361, 0.011963429704509433, 0.01277435346959872, 0.012791752113225393, 0.011661329893047174, 0.011092038210738174, 0.012623103898561814, 0.011089253926164257, 0.010779149815034247, 0.012310443444575997, 0.010581240313014339, 0.011630757749312394, 0.011318854528940223, 0.012376255523098208, 0.013484038350528285, 0.01287701172427963, 0.01226414575943516, 0.012962180448986043, 0.012199189226401418, 0.011887313492878247, 0.0124930233284499, 0.013753377558644449, 0.014084923596405284, 0.013772400822153455, 0.0124759465867955, 0.012627982129349999, 0.013797646643843541, 0.013552983226579815, 0.015349950652896441, 0.012621626043163418, 0.012204443559986351, 0.01542155575972661, 0.012159501226206928, 0.013959311782184982, 0.014406147703281297, 0.014023851115597298, 0.013697706575959647, 0.01465757058237237, 0.013909963765717882, 0.012995921292500729, 0.014307087935053654, 0.015364336608158492, 0.01459299656785328, 0.01393056140881836, 0.014808128400287971, 0.013670364874584343, 0.014372571788147503, 0.013375236751977417, 0.013842196066569836, 0.014161551752354808, 0.015331952595097379, 0.012950641390424388, 0.014066487916153518, 0.015273752801248517, 0.015062389163119005, 0.014380944763866126, 0.014353280405684355, 0.016292977214243328, 0.01443406357498394, 0.015043646433912457, 0.01365985076204761, 0.015186350571407875, 0.014766187039959716, 0.014511981205641905, 0.016117497962649444, 0.016046759812013776, 0.015242538341642055, 0.01554455852320641, 0.016022606771879788, 0.014872391043548705, 0.013985722782234044, 0.015303845285753313, 0.01838633269370117, 0.014921561479377812, 0.015419318721183566, 0.01566677452138959, 0.01468749722883836, 0.015522533794587083, 0.015444380590374923, 0.014791711446007294, 0.01635502049481092, 0.016383029856897584, 0.016950557443905334, 0.015136037387531037, 0.016033318809184943, 0.015968393390594, 0.015624046365799395, 0.01627249823989246, 0.015309174338813855, 0.016280147724701294, 0.016629531475432544, 0.01563501694510038, 0.0163307357476971, 0.015771190259841927, 0.015222480366893262, 0.014434583867050045, 0.01521721739857848, 0.016265232548275492, 0.015207606918601478, 0.01697454983070048, 0.014868553629602026, 0.017201850787319724, 0.01451122631317419, 0.015347875329197323, 0.01564804036291716, 0.015775384538649184, 0.015768638219043063, 0.013890216222565838, 0.015713459375156256, 0.01707335023618246, 0.015489473905547804, 0.01586912992508513, 0.016114603990566984, 0.016703988930635315, 0.015061269288402476, 0.017956142587493912, 0.016370995472234057, 0.015400584882146394, 0.015606611672245212, 0.01650710788134686, 0.015996099581996878, 0.014510362338288847, 0.015104208314984699, 0.01485759187562962, 0.01456580171162518, 0.015568009441919666, 0.01606964638724715, 0.01651596856336032, 0.014021643460887755, 0.015650676473524767, 0.015539486485979201, 0.01584382966606505, 0.01677706265334463, 0.01656023037810767, 0.016435739611719645, 0.015617635702248346, 0.015017508583649884, 0.015315775337921826, 0.016174961637506268], "moving_avg_accuracy_train": [0.04484132391449796, 0.09454846150851326, 0.14802796757480846, 0.20299190972582706, 0.2540702450544625, 0.3028139284036213, 0.3478226384321684, 0.39012734885297884, 0.42952667071140554, 0.46597417653039974, 0.4992907536055805, 0.52991948685175, 0.5580132997483396, 0.58403709262788, 0.6076629030217919, 0.6296051839751499, 0.6495601750772196, 0.6681333261107029, 0.6851188072051051, 0.7008985635912298, 0.7151841217935224, 0.7281875725017671, 0.7403462631105789, 0.7515936431037386, 0.7618860930583149, 0.7714839392019113, 0.7803149880823385, 0.7882142120961793, 0.7956188436562642, 0.8024500622864389, 0.8088772490083764, 0.8147593733081202, 0.8204136832433657, 0.8255815090981543, 0.8302813083948265, 0.8345484022404213, 0.8386584318657239, 0.8423273036916099, 0.8457849291098705, 0.8490502518077335, 0.8521193226667811, 0.8549278812696582, 0.857620633528905, 0.8601092868265313, 0.8624444058955855, 0.8646785825886958, 0.8668915935101049, 0.8689251199691258, 0.8706343499953305, 0.8723284780379716, 0.874022784944169, 0.8756756164419078, 0.8770956633767591, 0.8787062018978872, 0.8800859681514355, 0.8815182757867349, 0.8827935099121035, 0.8838831279535159, 0.8850869624276826, 0.8862029294889472, 0.8873747466071897, 0.8884712346921796, 0.8895046129936796, 0.8904670974019067, 0.8913890648431021, 0.8923071911949398, 0.8931521781997076, 0.893840478744447, 0.894601855409731, 0.8954149415441917, 0.8962537480080819, 0.897001770476792, 0.8977375534235756, 0.8985368697578053, 0.8990703146514877, 0.8996551188498678, 0.9002627146902871, 0.9009213383336153, 0.901551013603013, 0.9021341415824128, 0.9027774672555207, 0.9033263055244315, 0.9039805870878521, 0.9044973608818355, 0.904960132147611, 0.9055370615546661, 0.9059540635710341, 0.9065501463762139, 0.9069588098139894, 0.9074358889020349, 0.9078977761157904, 0.9084110948093516, 0.9088009659716427, 0.9091634397129337, 0.9095873583789142, 0.9100317362937914, 0.910401449482657, 0.9108805315323609, 0.9112256748711421, 0.9115340508248733, 0.9118534418618027, 0.9121083777605246, 0.9124633420562698, 0.9128269877498214, 0.9130728165680472, 0.913410247847289, 0.913748885328387, 0.9140629236077945, 0.9143757489449663, 0.9146712786900968, 0.914999962380934, 0.9152655868169823, 0.9156231953498928, 0.9158475309747872, 0.9160749015276407, 0.9163887088728002, 0.9165993082120799, 0.9168747339281089, 0.9171342788654013, 0.9174863437517939, 0.9177823479078993, 0.9181650090888701, 0.9184582508779343, 0.9188058377964163, 0.918944243813517, 0.919166429430089, 0.9193618543338409, 0.9196167557579226, 0.9198043143610247, 0.9200405103704741, 0.9201833683635117, 0.9204583888834266, 0.9206106122989783, 0.9208730632622331, 0.9212186232208474, 0.9213623606645894, 0.921624149699644, 0.921845808938336, 0.922173113340045, 0.9223723922492115, 0.9226238589293753, 0.9228455646927223, 0.923089205609478, 0.9232898451952634, 0.9235727994677267, 0.92379723137842, 0.9239201289897013, 0.9241795103148452, 0.924361836282484, 0.9246142492593021, 0.9247764249182279, 0.9249990408243379, 0.9252086596862563, 0.9253647285298308, 0.9255982324902474, 0.9258455523867561, 0.9260076864245663, 0.9261861591419288, 0.9264164669542032, 0.9267074853912118, 0.9269136344619098, 0.92703410050769, 0.9271796862810258, 0.9272688968472753, 0.9273631012009383, 0.9274642332585392, 0.9275528188151142, 0.9277023002803174, 0.9278438090454288, 0.9279293503042765, 0.9281086079360397, 0.928335080020112, 0.9284738367779292, 0.9286847844147358, 0.9288582891485575, 0.9289028362661399, 0.9291289405767259, 0.9293277120609968, 0.929490330355174, 0.9297251506211517, 0.9299411031093319, 0.9300935716213038, 0.930247105372564, 0.9303968393951083, 0.9305525984523214], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 847703218, "moving_var_accuracy_train": [0.01809669897364434, 0.03852422482639324, 0.06041232046560808, 0.08156040285007195, 0.09688532962456552, 0.10858031666009649, 0.11595434079999134, 0.12046610343408988, 0.12239025215681602, 0.12210701306496495, 0.11988626053052659, 0.11634070817985902, 0.11180999826949031, 0.10672413860507621, 0.10107533499549026, 0.09530097473686606, 0.0893546922921326, 0.08352388051673257, 0.07776805157713397, 0.07223225282386958, 0.06684572209984244, 0.06168295746275392, 0.056845165532365814, 0.052299180989523956, 0.04802267362517868, 0.044049474118026136, 0.040346413525179974, 0.03687335183284952, 0.03367947376643002, 0.030731516321546217, 0.028030143251819444, 0.025538523403136223, 0.02327241205041695, 0.021185528661964045, 0.019265768816628642, 0.0175030647439498, 0.01590478936124261, 0.014435456009391376, 0.013099506970249253, 0.011885517264114947, 0.010781738301144149, 0.009774556483861887, 0.008862359068042808, 0.008031863718360604, 0.007277752376124487, 0.006594901047976375, 0.005979487699223222, 0.005418755998036743, 0.004903173603775389, 0.00443868687182162, 0.004020654267670949, 0.003643175508543114, 0.003297006757363428, 0.0029906505905794223, 0.002708719325751357, 0.0024563109396354543, 0.0022253158443424504, 0.0020134696671937477, 0.0018251656574451026, 0.0016538575340370402, 0.0015008301788607939, 0.0013615677360594353, 0.0012350217988795914, 0.0011198570051163545, 0.0010155215202683394, 0.0009215559722229567, 0.0008358264023446983, 0.0007565075808692262, 0.0006860740726202542, 0.0006234166469166981, 0.0005674073487798037, 0.0005157024524250795, 0.000469004596085571, 0.00042785429589651196, 0.0003876299373982229, 0.00035194490721238874, 0.0003200729708388031, 0.00029196973968688157, 0.00026634118422221394, 0.0002427674099632214, 0.0002222154802620171, 0.00020270494324460477, 0.00018628720819823362, 0.0001700619837657415, 0.0001549832005890142, 0.00014248050839663702, 0.00012979747369186825, 0.00012001555871835959, 0.00010951705509489348, 0.00010061378969165713, 9.247246890656904e-05, 8.55966867463468e-05, 7.840501378038827e-05, 7.174699732047809e-05, 6.618966090673074e-05, 6.134794039713242e-05, 5.6443336935609684e-05, 5.286467973518637e-05, 4.8650327080413443e-05, 4.464115593192831e-05, 4.109513604897323e-05, 3.75705532561902e-05, 3.494749479185658e-05, 3.264288902661919e-05, 2.992248639478941e-05, 2.7954976569207262e-05, 2.619155700472597e-05, 2.4459981672652476e-05, 2.2894720729577254e-05, 2.1391289128931848e-05, 2.0224456933639504e-05, 1.883701830950964e-05, 1.8104271243852166e-05, 1.6746782372837982e-05, 1.5537380450298395e-05, 1.4869917854153168e-05, 1.3782094804082886e-05, 1.3086619249125374e-05, 1.2384229494480259e-05, 1.2261353703108533e-05, 1.1823784476681677e-05, 1.1959272243811762e-05, 1.1537261741113124e-05, 1.1470885560099758e-05, 1.0496203034216929e-05, 9.890880764698103e-06, 9.24551072528627e-06, 8.905732276747874e-06, 8.331763115451815e-06, 8.000683797825018e-06, 7.384291073614806e-06, 7.326588543622106e-06, 6.802477403440107e-06, 6.742154236115588e-06, 7.142643977481499e-06, 6.614323654335089e-06, 6.5696927787746445e-06, 6.354918863774536e-06, 6.683580519800091e-06, 6.3726312205670605e-06, 6.304487519603547e-06, 6.116419777155009e-06, 6.039025866298101e-06, 5.797429470125204e-06, 5.938254605859948e-06, 5.797756288110465e-06, 5.353915065027262e-06, 5.424031605025365e-06, 5.180813270801565e-06, 5.236142741516688e-06, 4.949236966497257e-06, 4.900333844726254e-06, 4.8057610657012e-06, 4.544402314543052e-06, 4.580678978861325e-06, 4.673115261856706e-06, 4.4423907516206965e-06, 4.284824274043381e-06, 4.333717042190817e-06, 4.662570914082176e-06, 4.578790776820563e-06, 4.251520312811835e-06, 4.01712523811082e-06, 3.6870394404748885e-06, 3.398205638669003e-06, 3.1504343124733994e-06, 2.9060174887294107e-06, 2.8165181158102355e-06, 2.7150888796596206e-06, 2.5094357543808514e-06, 2.5476918658509164e-06, 2.7545291230422457e-06, 2.652357151297362e-06, 2.787611585436098e-06, 2.7797854608194275e-06, 2.5196669259016005e-06, 2.7278086667019322e-06, 2.810618726665161e-06, 2.767559240408456e-06, 2.9870683321917548e-06, 3.1080807933333024e-06, 3.0064925382864367e-06, 2.91799679944196e-06, 2.82797961706332e-06, 2.763529610492082e-06], "duration": 51665.559406, "accuracy_train": [0.44841323914497966, 0.5419126998546512, 0.6293435221714655, 0.6976673890849945, 0.7137752630121816, 0.7415070785460502, 0.7529010286890919, 0.7708697426402732, 0.7841205674372462, 0.7940017289013473, 0.7991399472822074, 0.8055780860672758, 0.8108576158176449, 0.8182512285437431, 0.8202951965669989, 0.827085712555371, 0.8291550949958472, 0.8352916854120525, 0.837988137054725, 0.8429163710663529, 0.8437541456141565, 0.8452186288759689, 0.8497744785898856, 0.8528200630421743, 0.8545181426495018, 0.8578645544942783, 0.8597944280061831, 0.8593072282207457, 0.8622605276970285, 0.8639310299580103, 0.866721929505814, 0.867698492005814, 0.8713024726605758, 0.8720919417912514, 0.8725795020648762, 0.8729522468507751, 0.8756486984934477, 0.8753471501245846, 0.8769035578742157, 0.8784381560885014, 0.8797409603982096, 0.8802049086955519, 0.8818554038621264, 0.882507166505168, 0.8834604775170728, 0.8847861728266887, 0.8868086918027871, 0.8872268581003139, 0.8860174202311739, 0.8875756304217424, 0.8892715470999446, 0.8905510999215578, 0.8898760857904209, 0.89320104858804, 0.8925038644333703, 0.8944090445044297, 0.8942706170404209, 0.8936896903262275, 0.8959214726951827, 0.8962466330403286, 0.8979211006713732, 0.8983396274570875, 0.8988050177071798, 0.8991294570759505, 0.8996867718138611, 0.9005703283614802, 0.9007570612426172, 0.9000351836471022, 0.9014542453972868, 0.9027327167543374, 0.9038030061830934, 0.9037339726951827, 0.904359599944629, 0.9057307167658729, 0.903871318694629, 0.9049183566352897, 0.9057310772540605, 0.9068489511235696, 0.9072180910275931, 0.9073822933970099, 0.908567398313492, 0.908265849944629, 0.9098691211586378, 0.9091483250276854, 0.9091250735395902, 0.9107294262181617, 0.9097070817183462, 0.9119148916228312, 0.9106367807539681, 0.9117296006944444, 0.9120547610395902, 0.9130309630514026, 0.9123098064322629, 0.9124257033845515, 0.9134026263727391, 0.9140311375276854, 0.9137288681824474, 0.9151922699796974, 0.9143319649201735, 0.9143094344084534, 0.9147279611941677, 0.9144028008490217, 0.9156580207179772, 0.9160997989917867, 0.9152852759320783, 0.9164471293604651, 0.9167966226582688, 0.9168892681224622, 0.9171911769795128, 0.9173310463962717, 0.9179581155984681, 0.9176562067414176, 0.9188416721460871, 0.9178665515988372, 0.9181212365033223, 0.9192129749792359, 0.9184947022655963, 0.9193535653723699, 0.9194701833010337, 0.9206549277293282, 0.9204463853128461, 0.921608959717608, 0.9210974269795128, 0.9219341200627538, 0.9201898979674235, 0.9211660999792359, 0.921120678467608, 0.9219108685746585, 0.9214923417889442, 0.9221662744555187, 0.921469090300849, 0.9229335735626615, 0.9219806230389442, 0.9232351219315246, 0.9243286628483758, 0.9226559976582688, 0.9239802510151348, 0.9238407420865633, 0.9251188529554264, 0.9241659024317092, 0.924887059050849, 0.9248409165628461, 0.9252819738602805, 0.9250956014673312, 0.9261193879198967, 0.9258171185746585, 0.925026207491233, 0.9265139422411407, 0.926002769991233, 0.9268859660506644, 0.9262360058485604, 0.9270025839793282, 0.9270952294435216, 0.9267693481220007, 0.9276997681339978, 0.9280714314553341, 0.9274668927648578, 0.9277924135981912, 0.9284892372646733, 0.9293266513242894, 0.9287689760981912, 0.9281182949197121, 0.9284899582410484, 0.9280717919435216, 0.9282109403839055, 0.9283744217769472, 0.9283500888242894, 0.9290476334671466, 0.9291173879314323, 0.9286992216339055, 0.9297219266219084, 0.9303733287767626, 0.9297226475982835, 0.9305833131459949, 0.9304198317529531, 0.9293037603243817, 0.9311638793720007, 0.9311166554194352, 0.9309538950027685, 0.9318385330149501, 0.9318846755029531, 0.9314657882290514, 0.9316289091339055, 0.9317444455980066, 0.9319544299672389], "end": "2016-01-24 01:03:35.750000", "learning_rate_per_epoch": [0.004827467259019613, 0.0024137336295098066, 0.0016091556753963232, 0.0012068668147549033, 0.0009654934401623905, 0.0008045778376981616, 0.0006896381382830441, 0.0006034334073774517, 0.0005363852251321077, 0.00048274672008119524, 0.0004388606466818601, 0.0004022889188490808, 0.00037134363083168864, 0.00034481906914152205, 0.0003218311467207968, 0.00030171670368872583, 0.0002839686640072614, 0.00026819261256605387, 0.00025407722569070756, 0.00024137336004059762, 0.00022987938427831978, 0.00021943032334093004, 0.0002098898694384843, 0.0002011444594245404, 0.0001930986763909459, 0.00018567181541584432, 0.0001787950750440359, 0.00017240953457076102, 0.0001664643787080422, 0.0001609155733603984, 0.00015572474512737244, 0.00015085835184436291, 0.0001462868822272867, 0.0001419843320036307, 0.00013792763638775796, 0.00013409630628302693, 0.00013047207903582603, 0.00012703861284535378, 0.00012378121027722955, 0.00012068668002029881, 0.00011774309678003192, 0.00011493969213915989, 0.00011226667265873402, 0.00010971516167046502, 0.00010727704648161307, 0.00010494493471924216, 0.0001027120670187287, 0.0001005722297122702, 9.851973300101236e-05, 9.654933819547296e-05, 9.465621405979618e-05, 9.283590770792216e-05, 9.108428639592603e-05, 8.939753752201796e-05, 8.777212497079745e-05, 8.620476728538051e-05, 8.469240128761157e-05, 8.32321893540211e-05, 8.182147576007992e-05, 8.04577866801992e-05, 7.913880108390003e-05, 7.786237256368622e-05, 7.66264638514258e-05, 7.542917592218146e-05, 7.426872616633773e-05, 7.314344111364335e-05, 7.205174915725365e-05, 7.099216600181535e-05, 6.99632873875089e-05, 6.896381819387898e-05, 6.799249240430072e-05, 6.704815314151347e-05, 6.612968718400225e-05, 6.523603951791301e-05, 6.436622788896784e-05, 6.351930642267689e-05, 6.269438017625362e-05, 6.189060513861477e-05, 6.11071809544228e-05, 6.0343340010149404e-05, 5.959835834801197e-05, 5.887154839001596e-05, 5.8162255299976096e-05, 5.7469846069579944e-05, 5.6793731346260756e-05, 5.613333632936701e-05, 5.548812623601407e-05, 5.485758083523251e-05, 5.424120172392577e-05, 5.3638523240806535e-05, 5.3049087000545114e-05, 5.247246735962108e-05, 5.190824958845042e-05, 5.135603350936435e-05, 5.081544441054575e-05, 5.02861148561351e-05, 4.976770287612453e-05, 4.925986650050618e-05, 4.876229286310263e-05, 4.827466909773648e-05, 4.7796704166103154e-05, 4.732810702989809e-05, 4.686861211666837e-05, 4.641795385396108e-05, 4.597587758325972e-05, 4.5542143197963014e-05, 4.511651422944851e-05, 4.469876876100898e-05, 4.4288688513915986e-05, 4.3886062485398725e-05, 4.349069422460161e-05, 4.3102383642690256e-05, 4.2720948840724304e-05, 4.234620064380579e-05, 4.1977975342888385e-05, 4.161609467701055e-05, 4.126040221308358e-05, 4.091073788003996e-05, 4.056694888276979e-05, 4.02288933400996e-05, 3.98964220948983e-05, 3.956940054195002e-05, 3.924770135199651e-05, 3.893118628184311e-05, 3.861973527818918e-05, 3.83132319257129e-05, 3.801155253313482e-05, 3.771458796109073e-05, 3.742222543223761e-05, 3.7134363083168864e-05, 3.6850895412499085e-05, 3.6571720556821674e-05, 3.629674392868765e-05, 3.6025874578626826e-05, 3.575901428121142e-05, 3.5496083000907674e-05, 3.52369861502666e-05, 3.498164369375445e-05, 3.4729979233816266e-05, 3.448190909693949e-05, 3.423735324759036e-05, 3.399624620215036e-05, 3.375851156306453e-05, 3.3524076570756733e-05, 3.329287574160844e-05, 3.3064843592001125e-05, 3.283991100033745e-05, 3.261801975895651e-05, 3.239910802221857e-05, 3.218311394448392e-05, 3.1969979318091646e-05, 3.1759653211338446e-05, 3.15520737785846e-05, 3.134719008812681e-05, 3.1144947570282966e-05, 3.094530256930739e-05, 3.074819687753916e-05, 3.05535904772114e-05, 3.036142879864201e-05, 3.0171670005074702e-05, 2.9984266802784987e-05, 2.9799179174005985e-05, 2.961636164400261e-05, 2.943577419500798e-05, 2.925737680925522e-05, 2.9081127649988048e-05, 2.890698851842899e-05, 2.8734923034789972e-05, 2.8564893000293523e-05, 2.8396865673130378e-05, 2.823080103553366e-05, 2.8066668164683506e-05, 2.7904434318770654e-05, 2.7744063118007034e-05, 2.7585525458562188e-05, 2.7428790417616256e-05, 2.7273825253359973e-05, 2.7120600861962885e-05, 2.6969089958583936e-05, 2.6819261620403267e-05, 2.6671088562579826e-05, 2.6524543500272557e-05, 2.6379600967629813e-05, 2.623623367981054e-05, 2.6094416170963086e-05], "accuracy_valid": [0.44084443241716864, 0.5345370740775602, 0.6134062617658133, 0.6774049322289157, 0.6994702442582832, 0.7234783862010542, 0.7306099397590362, 0.7493793533509037, 0.7597450348268072, 0.7642116316829819, 0.7672427993222892, 0.7746184935052711, 0.7763377729668675, 0.7770187194088856, 0.7829604551016567, 0.7879344526543675, 0.7891654508659638, 0.7933055464043675, 0.7947806852409638, 0.7964793745293675, 0.7979751035391567, 0.7974353468561747, 0.8006606504141567, 0.8008944959525602, 0.7989516660391567, 0.805187547063253, 0.8011386365775602, 0.800670945500753, 0.8037021131400602, 0.8037432934864458, 0.8033359022025602, 0.8064185452748494, 0.807262742375753, 0.8078936841114458, 0.8077304334525602, 0.807262742375753, 0.8105998211596386, 0.8111895825489458, 0.808849656438253, 0.8136412838855422, 0.8114440182605422, 0.8111792874623494, 0.8117999341114458, 0.8120543698230422, 0.8129191570971386, 0.8130515224962349, 0.8145266613328314, 0.8136721691453314, 0.8135295086596386, 0.8136618740587349, 0.8124308758471386, 0.8147605068712349, 0.8159503247364458, 0.8152487881212349, 0.8157473644578314, 0.8163371258471386, 0.8179343349962349, 0.8164694912462349, 0.8159709149096386, 0.8158694347703314, 0.8179343349962349, 0.8212508236069277, 0.8192976986069277, 0.8185446865587349, 0.8182799557605422, 0.8195315441453314, 0.8189211925828314, 0.8180461102221386, 0.8189108974962349, 0.8197653896837349, 0.8192874035203314, 0.8183005459337349, 0.8190432628953314, 0.8208743175828314, 0.8201213055346386, 0.8202742611069277, 0.8229803981551205, 0.8199992352221386, 0.8226244823042168, 0.8232142436935241, 0.8219832454819277, 0.8211287532944277, 0.8213728939194277, 0.8213625988328314, 0.8216170345444277, 0.8238245952560241, 0.8216170345444277, 0.8234583843185241, 0.8221053157944277, 0.8227362575301205, 0.8250555934676205, 0.8240584407944277, 0.8220950207078314, 0.8244349468185241, 0.8246790874435241, 0.8242011012801205, 0.8240687358810241, 0.8262660015060241, 0.8262762965926205, 0.8239260753953314, 0.8246687923569277, 0.8231936535203314, 0.8254218044051205, 0.8256350597703314, 0.8257777202560241, 0.8252791439194277, 0.8248011577560241, 0.8255129894578314, 0.8266425075301205, 0.8251776637801205, 0.8254012142319277, 0.8246584972703314, 0.8271410838667168, 0.8271410838667168, 0.8233363140060241, 0.8250555934676205, 0.8260218608810241, 0.8257880153426205, 0.8245364269578314, 0.8260321559676205, 0.8252791439194277, 0.8267645778426205, 0.8263880718185241, 0.8250247082078314, 0.8249232280685241, 0.8258997905685241, 0.8268763530685241, 0.8269881282944277, 0.8266322124435241, 0.8270190135542168, 0.8268969432417168, 0.8284838573042168, 0.8257674251694277, 0.8277411403426205, 0.8289927287274097, 0.8276190700301205, 0.8256453548569277, 0.8268866481551205, 0.8274969997176205, 0.8274867046310241, 0.8268763530685241, 0.8270190135542168, 0.8273852244917168, 0.8269072383283133, 0.8265101421310241, 0.8266425075301205, 0.8274867046310241, 0.8264998470444277, 0.8268969432417168, 0.8261439311935241, 0.8265101421310241, 0.8263880718185241, 0.8270190135542168, 0.8263983669051205, 0.8277411403426205, 0.8269984233810241, 0.8268866481551205, 0.8273852244917168, 0.8250452983810241, 0.8277308452560241, 0.8275072948042168, 0.8255026943712349, 0.8266322124435241, 0.8255232845444277, 0.8274969997176205, 0.8263777767319277, 0.8252791439194277, 0.8266322124435241, 0.8249026378953314, 0.8256556499435241, 0.8260321559676205, 0.8281176463667168, 0.8264998470444277, 0.8262454113328314, 0.8277411403426205, 0.8271101986069277, 0.8264792568712349, 0.8263777767319277, 0.8254012142319277, 0.8274867046310241, 0.8265101421310241, 0.8273646343185241, 0.8282191265060241, 0.8273440441453314, 0.8278426204819277], "accuracy_test": 0.8062697784810127, "start": "2016-01-23 10:42:30.191000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0], "accuracy_train_last": 0.9319544299672389, "batch_size_eval": 1024, "accuracy_train_std": [0.013523951681442607, 0.014652845062514538, 0.013879951303922174, 0.012558659265576609, 0.013468163854632407, 0.014736944682282196, 0.013495066550626775, 0.013936728200665592, 0.012017365860105284, 0.011617476092958044, 0.012337407621411231, 0.012076698851355337, 0.010427623825411986, 0.011156605955293629, 0.011291283338336495, 0.010800122619777848, 0.01149258047943247, 0.011040965685309809, 0.011270992288599336, 0.011559996642583982, 0.010445589512504582, 0.010886663400446252, 0.011798338723640387, 0.011228523859281456, 0.010914534977146626, 0.01150894439569238, 0.010823373657803633, 0.010886703878000519, 0.011246050866490668, 0.012377755684134772, 0.011032031200654683, 0.011138592720340748, 0.010954382023542689, 0.011657856818890769, 0.011477321931278388, 0.011722697850860286, 0.010781071251711903, 0.01115738467015518, 0.011322546953708112, 0.01128939445191904, 0.009823017243672358, 0.009990128538618382, 0.010518387548663162, 0.010258377832473223, 0.009555441233127442, 0.010155257443592572, 0.009738083457531401, 0.010180667856729905, 0.010415323633404314, 0.010326859497880881, 0.010709124580683599, 0.008988689581313129, 0.0099507406338462, 0.009540022545219157, 0.009632275845824824, 0.00996330405360571, 0.008673494525618665, 0.009771330771685379, 0.009793482695494985, 0.009828681796341549, 0.009713219543883975, 0.008688200259480017, 0.009342222874273103, 0.009043229335610461, 0.008908859254934188, 0.00944864044571638, 0.00817570487478255, 0.009090313272719773, 0.009141939868136002, 0.009016833191633628, 0.009210204373154887, 0.008681636557963364, 0.008705315013250933, 0.009292508601043546, 0.009057112900899519, 0.008860272833994137, 0.00844185544378166, 0.00859673423677759, 0.00843567632845297, 0.008118086764288442, 0.008519163178464748, 0.008145311381455812, 0.007736874444263825, 0.008214993530389582, 0.00923381468656992, 0.008231936765604898, 0.0090600752040783, 0.008211962501135473, 0.00866452768012235, 0.00823200831072932, 0.00823294702808621, 0.008289380450378567, 0.00892877657321903, 0.008652983022526097, 0.008476227263416545, 0.008904038280892555, 0.008863323009815512, 0.00823179960528968, 0.008022273227884864, 0.008266684242385716, 0.008104662437766481, 0.008936292555226943, 0.007850474111421117, 0.008251415731886234, 0.00884140150864836, 0.008394948130702679, 0.007675077463523244, 0.008584500879164172, 0.008813673710245048, 0.008547933795378368, 0.008672658486576688, 0.008399903943088814, 0.008149540670904905, 0.008084124127953084, 0.008281861480792733, 0.008723587774235693, 0.00833716964501372, 0.007985338162027589, 0.008276155649682185, 0.008729923392770105, 0.007817378914744485, 0.0075885832013743455, 0.007535268691807239, 0.0074540410958326905, 0.008862224777485064, 0.007955717683502991, 0.007082575760427068, 0.007558550651295658, 0.007947497953548604, 0.00844544781517515, 0.008072500828975515, 0.007719784670388335, 0.007447431872025066, 0.007529294706932243, 0.007417080517968717, 0.007499904381213495, 0.007458554498652669, 0.007737986377138338, 0.007360819407941857, 0.007128294358947421, 0.007562307873720622, 0.007484201276777528, 0.0070255216393422856, 0.0073416991525034146, 0.007086722762674961, 0.0071916471427040125, 0.007392269950443983, 0.006438847944629102, 0.007063905281937829, 0.007038460365739564, 0.007075336436551305, 0.006583859255593718, 0.006978050008744562, 0.007203328660868553, 0.007255529630650027, 0.006857767915431193, 0.006923430456133539, 0.0070775043929135525, 0.006519962665034472, 0.006983988894377857, 0.00692344740528502, 0.007549331530511722, 0.006824344789197474, 0.007338053998202377, 0.006819586182950455, 0.007216566794378178, 0.007584011823944927, 0.00669465648006845, 0.006793503221396683, 0.007292637652546842, 0.007065898482266397, 0.006641290591628262, 0.006851443425871554, 0.00694657213842567, 0.006328067834713324, 0.006638263416990477, 0.006710541216564073, 0.006297246614232079, 0.006686065337173387, 0.006773588027111625, 0.006529414301701974, 0.006286685456581175, 0.006724068670739262, 0.006549323318571005, 0.006549009323650201], "accuracy_test_std": 0.04136564215983919, "error_valid": [0.5591555675828314, 0.46546292592243976, 0.38659373823418675, 0.32259506777108427, 0.3005297557417168, 0.2765216137989458, 0.2693900602409638, 0.25062064664909633, 0.24025496517319278, 0.2357883683170181, 0.23275720067771077, 0.22538150649472888, 0.22366222703313254, 0.22298128059111444, 0.21703954489834332, 0.21206554734563254, 0.2108345491340362, 0.20669445359563254, 0.2052193147590362, 0.20352062547063254, 0.20202489646084332, 0.20256465314382532, 0.19933934958584332, 0.19910550404743976, 0.20104833396084332, 0.19481245293674698, 0.19886136342243976, 0.19932905449924698, 0.19629788685993976, 0.1962567065135542, 0.19666409779743976, 0.19358145472515065, 0.19273725762424698, 0.1921063158885542, 0.19226956654743976, 0.19273725762424698, 0.18940017884036142, 0.1888104174510542, 0.19115034356174698, 0.18635871611445776, 0.18855598173945776, 0.18882071253765065, 0.1882000658885542, 0.18794563017695776, 0.18708084290286142, 0.1869484775037651, 0.18547333866716864, 0.18632783085466864, 0.18647049134036142, 0.1863381259412651, 0.18756912415286142, 0.1852394931287651, 0.1840496752635542, 0.1847512118787651, 0.18425263554216864, 0.18366287415286142, 0.1820656650037651, 0.1835305087537651, 0.18402908509036142, 0.18413056522966864, 0.1820656650037651, 0.1787491763930723, 0.1807023013930723, 0.1814553134412651, 0.18172004423945776, 0.18046845585466864, 0.18107880741716864, 0.18195388977786142, 0.1810891025037651, 0.1802346103162651, 0.18071259647966864, 0.1816994540662651, 0.18095673710466864, 0.17912568241716864, 0.17987869446536142, 0.1797257388930723, 0.17701960184487953, 0.18000076477786142, 0.1773755176957832, 0.17678575630647586, 0.1780167545180723, 0.1788712467055723, 0.1786271060805723, 0.17863740116716864, 0.1783829654555723, 0.17617540474397586, 0.1783829654555723, 0.17654161568147586, 0.1778946842055723, 0.17726374246987953, 0.17494440653237953, 0.1759415592055723, 0.17790497929216864, 0.17556505318147586, 0.17532091255647586, 0.17579889871987953, 0.17593126411897586, 0.17373399849397586, 0.17372370340737953, 0.17607392460466864, 0.1753312076430723, 0.17680634647966864, 0.17457819559487953, 0.17436494022966864, 0.17422227974397586, 0.1747208560805723, 0.17519884224397586, 0.17448701054216864, 0.17335749246987953, 0.17482233621987953, 0.1745987857680723, 0.17534150272966864, 0.1728589161332832, 0.1728589161332832, 0.17666368599397586, 0.17494440653237953, 0.17397813911897586, 0.17421198465737953, 0.17546357304216864, 0.17396784403237953, 0.1747208560805723, 0.17323542215737953, 0.17361192818147586, 0.17497529179216864, 0.17507677193147586, 0.17410020943147586, 0.17312364693147586, 0.1730118717055723, 0.17336778755647586, 0.1729809864457832, 0.1731030567582832, 0.1715161426957832, 0.1742325748305723, 0.17225885965737953, 0.1710072712725903, 0.17238092996987953, 0.1743546451430723, 0.17311335184487953, 0.17250300028237953, 0.17251329536897586, 0.17312364693147586, 0.1729809864457832, 0.1726147755082832, 0.17309276167168675, 0.17348985786897586, 0.17335749246987953, 0.17251329536897586, 0.1735001529555723, 0.1731030567582832, 0.17385606880647586, 0.17348985786897586, 0.17361192818147586, 0.1729809864457832, 0.17360163309487953, 0.17225885965737953, 0.17300157661897586, 0.17311335184487953, 0.1726147755082832, 0.17495470161897586, 0.17226915474397586, 0.1724927051957832, 0.1744973056287651, 0.17336778755647586, 0.1744767154555723, 0.17250300028237953, 0.1736222232680723, 0.1747208560805723, 0.17336778755647586, 0.17509736210466864, 0.17434435005647586, 0.17396784403237953, 0.1718823536332832, 0.1735001529555723, 0.17375458866716864, 0.17225885965737953, 0.1728898013930723, 0.1735207431287651, 0.1736222232680723, 0.1745987857680723, 0.17251329536897586, 0.17348985786897586, 0.17263536568147586, 0.17178087349397586, 0.17265595585466864, 0.1721573795180723], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-3, 3], "momentum": 0.5125309767199434, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.004827467063207049, "patience_threshold": 1, "do_flip": true, "batch_size": 128, "optimization": "adam", "nb_data_augmentation": 1, "learning_rate_decay_method": "lin", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 1.4710048529239018e-06, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.09957510632252908}, "accuracy_valid_max": 0.8289927287274097, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import os\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-3, 3)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8278426204819277, "loss_train": [1.5620198249816895, 1.2398881912231445, 1.0385679006576538, 0.9367385506629944, 0.8637334108352661, 0.8082906007766724, 0.7654633522033691, 0.7314551472663879, 0.7012272477149963, 0.6758480668067932, 0.6549381017684937, 0.6368411183357239, 0.6194698810577393, 0.6059503555297852, 0.5917574763298035, 0.5804104804992676, 0.5696514248847961, 0.5595741868019104, 0.5504050254821777, 0.5424460172653198, 0.5341822504997253, 0.5285298228263855, 0.5219337344169617, 0.5146658420562744, 0.5061671137809753, 0.5037184953689575, 0.49817806482315063, 0.49398693442344666, 0.48963186144828796, 0.4839208424091339, 0.48015400767326355, 0.47638222575187683, 0.47319552302360535, 0.4707620143890381, 0.464873343706131, 0.4614856243133545, 0.4589550495147705, 0.4571422338485718, 0.4519159495830536, 0.4506344795227051, 0.4446031153202057, 0.44496163725852966, 0.4401848614215851, 0.43968746066093445, 0.4354892373085022, 0.4343050420284271, 0.43440964818000793, 0.43099695444107056, 0.42847132682800293, 0.4259648025035858, 0.4241067171096802, 0.42184072732925415, 0.42178547382354736, 0.4194483160972595, 0.41920316219329834, 0.4137405455112457, 0.413240909576416, 0.41101980209350586, 0.4092283844947815, 0.4086570739746094, 0.4071246087551117, 0.4057271182537079, 0.4041239321231842, 0.40146639943122864, 0.398811399936676, 0.3984659016132355, 0.39788249135017395, 0.39585793018341064, 0.3962109088897705, 0.39312341809272766, 0.3929569721221924, 0.3916165828704834, 0.39095965027809143, 0.3868646025657654, 0.3875473439693451, 0.38498401641845703, 0.38441023230552673, 0.3853687047958374, 0.3829348385334015, 0.38171619176864624, 0.38043639063835144, 0.38063687086105347, 0.3795643448829651, 0.3794862926006317, 0.3772302567958832, 0.3745473325252533, 0.37722456455230713, 0.3738747537136078, 0.3712366223335266, 0.3719225823879242, 0.37065693736076355, 0.370826780796051, 0.37016168236732483, 0.36793094873428345, 0.36989665031433105, 0.3661397397518158, 0.3644055426120758, 0.3630228340625763, 0.3660575747489929, 0.3628484308719635, 0.3639507591724396, 0.3619607985019684, 0.3617204427719116, 0.3607247471809387, 0.36054542660713196, 0.36037665605545044, 0.3591068387031555, 0.3568255603313446, 0.3582993745803833, 0.35562852025032043, 0.35446250438690186, 0.35488927364349365, 0.35564517974853516, 0.3568825125694275, 0.3518649935722351, 0.35382500290870667, 0.3507951498031616, 0.3515315353870392, 0.34871163964271545, 0.34843212366104126, 0.34979110956192017, 0.3481982946395874, 0.3476732075214386, 0.34847503900527954, 0.3474750816822052, 0.3475379943847656, 0.3472651243209839, 0.3447614908218384, 0.34516075253486633, 0.3428952991962433, 0.34343695640563965, 0.34490489959716797, 0.34210076928138733, 0.3396317660808563, 0.33817335963249207, 0.3404877781867981, 0.339444637298584, 0.340798556804657, 0.3411179780960083, 0.33749473094940186, 0.33877304196357727, 0.33928367495536804, 0.3386182487010956, 0.33699387311935425, 0.334746390581131, 0.3376109302043915, 0.3347782492637634, 0.33410242199897766, 0.33449891209602356, 0.33433517813682556, 0.33382073044776917, 0.3307938277721405, 0.33191871643066406, 0.33222246170043945, 0.32939615845680237, 0.32952752709388733, 0.331028014421463, 0.33040934801101685, 0.3293282091617584, 0.33169034123420715, 0.3296923339366913, 0.32741522789001465, 0.32730069756507874, 0.329067200422287, 0.32595115900039673, 0.3279523551464081, 0.32791706919670105, 0.3272539973258972, 0.3253166973590851, 0.32659730315208435, 0.32554373145103455, 0.3230203092098236, 0.3250710368156433, 0.32344862818717957, 0.3230898380279541, 0.3238808214664459, 0.32522478699684143, 0.3239002823829651, 0.32292884588241577, 0.32093045115470886, 0.31760767102241516, 0.3198419213294983, 0.320484459400177, 0.319915235042572, 0.31968066096305847], "accuracy_train_first": 0.44841323914497966, "model": "residualv3", "loss_std": [0.24782794713974, 0.1269124448299408, 0.10735110938549042, 0.09754200279712677, 0.09218724071979523, 0.09214681386947632, 0.08922933042049408, 0.08667204529047012, 0.08718090504407883, 0.08341293781995773, 0.08305957913398743, 0.08333488553762436, 0.0819067433476448, 0.08044900000095367, 0.08166879415512085, 0.07921001315116882, 0.08043874800205231, 0.07905995100736618, 0.0780847892165184, 0.07694200426340103, 0.07652454078197479, 0.07671775668859482, 0.0746673122048378, 0.0761767104268074, 0.07535792887210846, 0.07483142614364624, 0.07545981556177139, 0.07311940938234329, 0.07270534336566925, 0.07665344327688217, 0.07467793673276901, 0.07432027161121368, 0.07198329269886017, 0.07310817390680313, 0.07221072912216187, 0.07382337003946304, 0.07061277329921722, 0.07140974700450897, 0.07425633817911148, 0.07130779325962067, 0.07244372367858887, 0.07025787979364395, 0.07219642400741577, 0.07250085473060608, 0.07169022411108017, 0.07216707617044449, 0.07046370953321457, 0.07212121784687042, 0.07111337780952454, 0.07249654829502106, 0.07043369859457016, 0.06963075697422028, 0.070512555539608, 0.06995072960853577, 0.06990844011306763, 0.0705520510673523, 0.06919365376234055, 0.06779811531305313, 0.06808649003505707, 0.06617189943790436, 0.07036840170621872, 0.06979172676801682, 0.06715082377195358, 0.06753075122833252, 0.06803775578737259, 0.06931721419095993, 0.06733202934265137, 0.06670555472373962, 0.06678586453199387, 0.0657280832529068, 0.06959284096956253, 0.06794946640729904, 0.06836957484483719, 0.06641214340925217, 0.06726665794849396, 0.06774170696735382, 0.06883640587329865, 0.06521280854940414, 0.06906210631132126, 0.06590680778026581, 0.06711422652006149, 0.06802248954772949, 0.06666161864995956, 0.06627016514539719, 0.06628279387950897, 0.06439027190208435, 0.06711246073246002, 0.06639736145734787, 0.0625823363661766, 0.0650310218334198, 0.06461391597986221, 0.06486266106367111, 0.064557284116745, 0.06479056179523468, 0.06567584723234177, 0.06450583040714264, 0.06303349137306213, 0.06530724465847015, 0.06491599977016449, 0.06312084943056107, 0.06640497595071793, 0.06471290439367294, 0.06621679663658142, 0.06480994820594788, 0.06410269439220428, 0.0659264549612999, 0.06336762756109238, 0.065142422914505, 0.0629737600684166, 0.06407817453145981, 0.0637039989233017, 0.0647289827466011, 0.06449311971664429, 0.06319012492895126, 0.06314679235219955, 0.06338106095790863, 0.06439083814620972, 0.06160316243767738, 0.06228778883814812, 0.06365842372179031, 0.06355138123035431, 0.06340039521455765, 0.06356900930404663, 0.0627218559384346, 0.0630275085568428, 0.06225014105439186, 0.06220797449350357, 0.06289707869291306, 0.06107635796070099, 0.0621233694255352, 0.06228033825755119, 0.0635899156332016, 0.06031818315386772, 0.06056056171655655, 0.061641838401556015, 0.06118170917034149, 0.06365502625703812, 0.06264082342386246, 0.0612819530069828, 0.060514070093631744, 0.06072256341576576, 0.062159523367881775, 0.06112324073910713, 0.06094607710838318, 0.061615653336048126, 0.060944728553295135, 0.06394205242395401, 0.06060965359210968, 0.06021839380264282, 0.06064588204026222, 0.0603131465613842, 0.06030594930052757, 0.058771323412656784, 0.06025850027799606, 0.059252526611089706, 0.05966196581721306, 0.06336072832345963, 0.06010542809963226, 0.06071467697620392, 0.06161944940686226, 0.061184246093034744, 0.05990171059966087, 0.058514513075351715, 0.05948154255747795, 0.05952051281929016, 0.059437815099954605, 0.05969362333416939, 0.06029964238405228, 0.0588197261095047, 0.06219102814793587, 0.059589311480522156, 0.05907918140292168, 0.05846939980983734, 0.05973655357956886, 0.0572734996676445, 0.059862300753593445, 0.06152791902422905, 0.05855150893330574, 0.0606379471719265, 0.05927851423621178, 0.058893535286188126, 0.060443226248025894, 0.05916638672351837, 0.05999390780925751, 0.0595276914536953]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:10 2016", "state": "available"}], "summary": "42129f4e80062505e71484aee6e5e84e"}