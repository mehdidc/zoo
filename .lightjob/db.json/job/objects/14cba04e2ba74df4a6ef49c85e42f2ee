{"content": {"hp_model": {"f0": 32, "f1": 64, "f2": 64, "f3": 16, "nonlin": "leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.8552595376968384, 1.3298077583312988, 1.0779820680618286, 0.9223870038986206, 0.796949565410614, 0.6823605895042419, 0.5793363451957703, 0.4954642951488495, 0.420307457447052, 0.3727776110172272, 0.3270646333694458, 0.2822207808494568, 0.24013271927833557, 0.19842791557312012, 0.16357558965682983, 0.1372123658657074, 0.11259698122739792, 0.09344646334648132, 0.06875921785831451, 0.04969777166843414, 0.035662535578012466, 0.024602118879556656, 0.016854120418429375, 0.012087021954357624, 0.009476903825998306, 0.00821620598435402, 0.007436846382915974, 0.006889749318361282, 0.006465416867285967, 0.00612528994679451, 0.00584440166130662, 0.005606845021247864, 0.005404582247138023, 0.0052300491370260715, 0.005077598616480827, 0.004942737054079771, 0.004822948016226292, 0.0047147576697170734, 0.004617190919816494, 0.0045286607928574085, 0.004447782877832651, 0.004373382311314344, 0.004305015318095684, 0.004241832997649908, 0.0041831545531749725, 0.004128595814108849, 0.0040774354711174965, 0.004029782954603434, 0.003985004499554634, 0.003942984156310558, 0.003903435543179512, 0.0038660874124616385, 0.0038307677023112774, 0.003797281300649047, 0.0037655059713870287, 0.0037353269290179014, 0.0037066442891955376, 0.0036793085746467113, 0.0036531747318804264, 0.0036282725632190704, 0.0036044467706233263, 0.003581639379262924, 0.0035597970709204674, 0.00353885511867702, 0.0035187439061701298, 0.0034993942826986313, 0.0034808109048753977, 0.003462893422693014, 0.003445651149377227, 0.003429021453484893, 0.0034129617270082235, 0.0033974701073020697, 0.0033824732527136803, 0.0033679723273962736, 0.003353955689817667, 0.0033403541892766953, 0.0033271838910877705, 0.0033144140616059303, 0.003302023047581315, 0.0032899859361350536, 0.0032782945781946182, 0.0032669478096067905, 0.0032559086102992296, 0.0032451751176267862, 0.0032347405795007944, 0.0032245852053165436, 0.003214694792404771, 0.0032050726003944874], "moving_avg_accuracy_train": [0.04749832012504613, 0.10341192696797709, 0.16102182034999074, 0.21677382012216218, 0.27010948778233423, 0.3208077879774304, 0.3678357453946191, 0.411541612877121, 0.450664872483881, 0.48428358387890375, 0.5193336913686435, 0.5525001378060224, 0.5827189172840026, 0.6116245868962724, 0.6389279301342476, 0.6651216038174821, 0.689976886882347, 0.7138485435764029, 0.7361095622057966, 0.7574904517424151, 0.7781610018408757, 0.7978597501652321, 0.8163395025273542, 0.8332339854199214, 0.8485622529101368, 0.8625065031751401, 0.8751051204898244, 0.8864764281563735, 0.8967292062467439, 0.9059892586114107, 0.9143674835669916, 0.9219404381103479, 0.9287793486874637, 0.9349576196949632, 0.9405250029993225, 0.9455402982708649, 0.9500587143128721, 0.9541345893459168, 0.957807527173276, 0.9611224718131375, 0.9641059219890127, 0.9667956774449195, 0.9692234328016641, 0.9714084126227344, 0.9733818699081261, 0.9751649569114073, 0.9767674100655508, 0.9782212436483275, 0.9795343441704457, 0.9807161346403521, 0.981793696956125, 0.9827750927355494, 0.9836606740858409, 0.9844600224499127, 0.9851887365728155, 0.9858538798786661, 0.9864525088539317, 0.9869982503780993, 0.9874917428986596, 0.9879405364647829, 0.9883467758231034, 0.9887123912455918, 0.9890391199770219, 0.989337826132928, 0.9896113119708626, 0.9898574492250037, 0.9900789727537308, 0.9902806690783946, 0.9904668460682111, 0.9906367305078555, 0.990796601949964, 0.9909404862478617, 0.9910699821159696, 0.9911842032484572, 0.9912870022676961, 0.991379521385011, 0.9914604634417848, 0.9915309861440719, 0.9915967817249397, 0.9916583228965303, 0.9917160350997714, 0.9917679760826883, 0.991817048116123, 0.9918612129462143, 0.991903286442106, 0.9919411525884085, 0.9919752321200808, 0.9920082288473954], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.04629303346197288, 0.10014688147119727, 0.15504668527626125, 0.20580442968161705, 0.25260959359410295, 0.29479527627159025, 0.33202996880632885, 0.36529616902659956, 0.39376766987619866, 0.4176830362658378, 0.44119455309294375, 0.462544200758047, 0.4812379522748627, 0.4985486979453433, 0.5148974120175258, 0.5287954426060594, 0.5409159865890528, 0.5538081922882048, 0.5649198076414625, 0.5747493630218947, 0.5839886468816028, 0.593039512756319, 0.6014182551459732, 0.6088939701057735, 0.6157940415157533, 0.6219552776597352, 0.627512597220569, 0.6324388836205, 0.6368135652415073, 0.6407751927629138, 0.6444016926884297, 0.647689956683894, 0.6506860153735619, 0.6533458471005129, 0.6557264591148593, 0.6579056310215209, 0.659892329308676, 0.6618034575882753, 0.663461408375005, 0.6649413570518118, 0.6662488967984379, 0.6673768544454013, 0.6683920163276684, 0.6692934549904589, 0.6701169568182203, 0.6708703154944555, 0.6715361312718172, 0.6721109514089427, 0.6726282895323556, 0.6730328586871773, 0.6734091779577669, 0.6737112442075475, 0.6739210391674403, 0.6740976476000938, 0.6742942457918917, 0.6744711841645098, 0.674630428699866, 0.674748305210527, 0.674829980007622, 0.6748912802937573, 0.6749220364887792, 0.674925303001799, 0.6749404498947666, 0.6749784961609375, 0.6750005307692414, 0.6750447759792149, 0.675084596668191, 0.6751204352882695, 0.6751526900463402, 0.6752061333911037, 0.6752542324013909, 0.6752842849707398, 0.6753245688230634, 0.6753608242901545, 0.6754178682730367, 0.6754692078576308, 0.6755032064525153, 0.6755338051879114, 0.6755359004786082, 0.6755377862402354, 0.6755883115506998, 0.6756581983926178, 0.6757088895190939, 0.6757545115329225, 0.6757833643141182, 0.6757971247859444, 0.675797302179338, 0.6758096688646422], "moving_var_accuracy_train": [0.020304813732312264, 0.046411315230753744, 0.0716402820470612, 0.09245082314972095, 0.10880798183755581, 0.12106004243784907, 0.12885869720356036, 0.13316465315478648, 0.1336238528196293, 0.13043342734042285, 0.12844667492176134, 0.12550212595313692, 0.12117048505607243, 0.11657327617206843, 0.11162520152259778, 0.10663765823955196, 0.10153395828170783, 0.09650926639340679, 0.09131831630783002, 0.08630076661344079, 0.08151613472445343, 0.07685688742192515, 0.07224470990602083, 0.06758905088528398, 0.06294474785501991, 0.05840025210859533, 0.053988753321912546, 0.049753637732147166, 0.04572434908606586, 0.0419236513056266, 0.03836303805572082, 0.035042881014790396, 0.03195952919424744, 0.029107115568601672, 0.02647536582346044, 0.024054207921061204, 0.021832531880713102, 0.019798793508206763, 0.017940328407938907, 0.016245195288833128, 0.014700784534517187, 0.013295819140778691, 0.012019283191350644, 0.010860322103581937, 0.009809340696139136, 0.008857021219876655, 0.00799442980289001, 0.007214009511378696, 0.0065081266570715095, 0.005869883649797212, 0.005293345549716854, 0.004772679233828018, 0.004302469599397073, 0.003877973259721666, 0.003494955152205761, 0.0031494413775410464, 0.002837722449637189, 0.0025566307089742767, 0.00230315945188749, 0.0020746562476836846, 0.0018686758966615544, 0.001683011378729852, 0.0015156710058323443, 0.0013649069335572959, 0.0012290893907335233, 0.0011067257035910563, 0.0009964947872959677, 0.0008972114412328171, 0.0008078022539533696, 0.0007272817750635321, 0.0006547836274591955, 0.0005894915889339095, 0.0005306933526592317, 0.00047774143559726925, 0.00043006240078275046, 0.000387133198788094, 0.00034847884345827777, 0.00031367572017629075, 0.0002823471096848173, 0.00025414648455854223, 0.0002287618123883144, 0.00020590991194084033, 0.00018534059332694512, 0.00016682408878420355, 0.00015015761151729215, 0.00013515475497088517, 0.00012164973220410766, 0.00010949455803981814], "duration": 60716.742662, "accuracy_train": [0.4749832012504614, 0.6066343885543558, 0.6795108607881137, 0.7185418180717055, 0.7501304967238833, 0.7770924897332964, 0.7910873621493172, 0.8048944202196382, 0.8027742089447213, 0.7868519864341085, 0.8347846587763011, 0.8509981557424326, 0.854687932585825, 0.8717756134067, 0.8846580192760245, 0.9008646669665927, 0.9136744344661315, 0.9286934538229051, 0.9364587298703396, 0.9499184575719823, 0.964195952727021, 0.9751484850844407, 0.9826572737864526, 0.9852843314530271, 0.9865166603220746, 0.9880047555601699, 0.9884926763219823, 0.9888181971553157, 0.9890042090600776, 0.989329729893411, 0.9897715081672205, 0.9900970290005537, 0.9903295438815062, 0.9905620587624585, 0.9906314527385567, 0.9906779557147471, 0.9907244586909376, 0.9908174646433187, 0.9908639676195091, 0.99095697357189, 0.99095697357189, 0.9910034765480805, 0.9910732310123662, 0.9910732310123662, 0.9911429854766519, 0.9912127399409376, 0.9911894884528424, 0.9913057458933187, 0.9913522488695091, 0.9913522488695091, 0.9914917577980805, 0.9916076547503692, 0.9916309062384644, 0.9916541577265596, 0.9917471636789406, 0.9918401696313216, 0.9918401696313216, 0.9919099240956073, 0.9919331755837025, 0.991979678559893, 0.9920029300479882, 0.9920029300479882, 0.991979678559893, 0.9920261815360835, 0.9920726845122739, 0.9920726845122739, 0.9920726845122739, 0.9920959360003692, 0.9921424389765596, 0.9921656904646549, 0.9922354449289406, 0.9922354449289406, 0.9922354449289406, 0.9922121934408453, 0.9922121934408453, 0.9922121934408453, 0.9921889419527501, 0.9921656904646549, 0.9921889419527501, 0.9922121934408453, 0.9922354449289406, 0.9922354449289406, 0.9922586964170359, 0.9922586964170359, 0.9922819479051311, 0.9922819479051311, 0.9922819479051311, 0.9923051993932264], "end": "2016-02-04 04:45:28.846000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0], "moving_var_accuracy_valid": [0.019287404524002072, 0.04346079658020764, 0.06624061304269754, 0.08280368929250265, 0.09423983068312441, 0.10083253402150284, 0.1032270815727612, 0.10286413410934134, 0.0998733579440657, 0.09503352489561527, 0.09050529521765534, 0.08555703279470622, 0.08014643662718766, 0.07482875020548145, 0.06975139925125912, 0.06451465661429154, 0.0593853592308555, 0.05494270401787316, 0.05055964557791465, 0.04637326245091601, 0.04250421550198678, 0.038991057509527034, 0.03572378167486403, 0.03265438033481926, 0.02981744117050273, 0.027177344530849638, 0.024737564284075697, 0.022482222534915438, 0.0204062408349905, 0.018506867185056737, 0.016774543981938964, 0.015194403704679867, 0.013755750643259287, 0.012443847922274617, 0.011250468952112805, 0.010168161168690576, 0.009186867782579183, 0.008301052706031022, 0.0074956866427288785, 0.006765830211229829, 0.006104634131807909, 0.005505621314707207, 0.004964334166061359, 0.004475214074420183, 0.0040337960643211015, 0.0036355244015445214, 0.003275961757234523, 0.002951339345221476, 0.002658614159304756, 0.002394225829183579, 0.0021560777920059744, 0.0019412912089786854, 0.001747558213407585, 0.0015730831069131856, 0.001416122653863031, 0.0012747921531660704, 0.0011475411672478308, 0.0010329121043689384, 0.0009296809308843689, 0.0008367466573216547, 0.0007530805050812793, 0.0006777725506041171, 0.0006099973603990046, 0.0005490106520244302, 0.0004941139565376551, 0.0004447201796313399, 0.0004002624328536407, 0.0003602477492284788, 0.0003242323376303947, 0.0002918348095872509, 0.00026267215026164133, 0.00023641306364779742, 0.00021278636238183994, 0.00019151955627370198, 0.0001723968867901793, 0.00015518091988767785, 0.00013967323103899717, 0.000125714334478568, 0.00011314294054289913, 0.00010182867849348144, 9.166878590711103e-05, 8.254586485245933e-05, 7.431440467994422e-05, 6.690169652526176e-05, 6.0219019219580125e-05, 5.419882145288601e-05, 4.877893959081315e-05, 4.3902422045880544e-05], "accuracy_test": 0.6689213966836735, "start": "2016-02-03 11:53:32.103000", "learning_rate_per_epoch": [0.0005446349387057126, 0.00038511506863869727, 0.00031444511841982603, 0.0002723174693528563, 0.00024356815265491605, 0.00022234627977013588, 0.00020585265883710235, 0.00019255753431934863, 0.00018154497956857085, 0.00017222868336830288, 0.00016421361942775548, 0.00015722255920991302, 0.00015105455531738698, 0.00014555981033481658, 0.0001406241353834048, 0.00013615873467642814, 0.00013209338067099452, 0.000128371684695594, 0.0001249478227691725, 0.00012178407632745802, 0.00011884908599313349, 0.00011611655645538121, 0.00011356423783581704, 0.00011117313988506794, 0.00010892698628595099, 0.00010681169806048274, 0.00010481504432391375, 0.00010292632941855118, 0.00010113616735907272, 9.943627810571343e-05, 9.78193202172406e-05, 9.627876715967432e-05, 9.480877633905038e-05, 9.340412361780182e-05, 9.206010872730985e-05, 9.077248978428543e-05, 8.953743235906586e-05, 8.835145126795396e-05, 8.721138874534518e-05, 8.611434168415144e-05, 8.505769073963165e-05, 8.403899846598506e-05, 8.305605297209695e-05, 8.210680971387774e-05, 8.118938421830535e-05, 8.030203753151e-05, 7.944316894281656e-05, 7.861127960495651e-05, 7.780499436194077e-05, 7.702301081735641e-05, 7.62641429901123e-05, 7.552727765869349e-05, 7.481135980924591e-05, 7.41154290153645e-05, 7.343856123043224e-05, 7.277990516740829e-05, 7.213866047095507e-05, 7.151407044148073e-05, 7.090542931109667e-05, 7.03120676917024e-05, 6.973335985094309e-05, 6.916870916029438e-05, 6.861755537101999e-05, 6.807936733821407e-05, 6.755365029675886e-05, 6.703992403345183e-05, 6.65377447148785e-05, 6.604669033549726e-05, 6.556633888976648e-05, 6.509632657980546e-05, 6.463627505581826e-05, 6.4185842347797e-05, 6.374470103764907e-05, 6.33125237072818e-05, 6.288902659434825e-05, 6.247391138458624e-05, 6.206690886756405e-05, 6.166776438476518e-05, 6.127621600171551e-05, 6.089203816372901e-05, 6.051499440218322e-05, 6.014486643834971e-05, 5.978145054541528e-05, 5.942454299656674e-05, 5.907395461690612e-05, 5.872949623153545e-05, 5.839099321747199e-05, 5.805827822769061e-05], "accuracy_train_first": 0.4749832012504614, "accuracy_train_last": 0.9923051993932264, "batch_size_eval": 1024, "accuracy_train_std": [0.01566877658494877, 0.01791524883955809, 0.020523812364369558, 0.022179914992511325, 0.025843847583146035, 0.02823251307607593, 0.028779838718892087, 0.030138877138950682, 0.03296852918161812, 0.0329409460327818, 0.03342297369165962, 0.028716042880509867, 0.03015094034118091, 0.030066177820176253, 0.0288299541596359, 0.025692249175314232, 0.022298719174212778, 0.01991541662616579, 0.019314344262142576, 0.015948543512866235, 0.011391131332630635, 0.008107708107896213, 0.005507952515626651, 0.00481969475592843, 0.004531758959803671, 0.004165642476012698, 0.003838938303970348, 0.0038603825856889195, 0.0037701545758872424, 0.0036743960120932547, 0.003502298399464497, 0.003061598862941765, 0.0031146566173376436, 0.0032560487850837884, 0.0032600717947294604, 0.0031404404123906043, 0.0031262653034351703, 0.0031175529349357126, 0.0031157913971280875, 0.0030137767852503722, 0.003139247135377569, 0.003164264431940544, 0.0032143166105002793, 0.0032213730512351433, 0.0032200750617245475, 0.003259335741619323, 0.003199500215804876, 0.003194615239707712, 0.0032282582876055547, 0.0032492908087076453, 0.0031978932513100607, 0.0031254236931467358, 0.003118746152830183, 0.0031191687465079334, 0.003045458064389351, 0.0030500261392168346, 0.003072279062514339, 0.003141416825749482, 0.0031107086596126746, 0.003135958977857368, 0.0032054307807381937, 0.0032054307807381937, 0.0031791063346356158, 0.003224338934914277, 0.003268284105444144, 0.0032613291663389048, 0.0032613291663389048, 0.003286337213495785, 0.0033079608430361114, 0.003256311073733069, 0.003259516111308621, 0.003259516111308621, 0.003259516111308621, 0.003338919653265102, 0.003338919653265102, 0.003325290712943445, 0.003328794458016893, 0.003304762207439243, 0.0032945116831321025, 0.0033116056830812296, 0.0033147774859511414, 0.0033147774859511414, 0.003317783311056338, 0.003317783311056338, 0.00334107472851777, 0.00334107472851777, 0.00334107472851777, 0.0033232988045679658], "accuracy_test_std": 0.013866559056840875, "error_valid": [0.5370696653802711, 0.4151684864457832, 0.3508550804781627, 0.3373758706701807, 0.32614393119352414, 0.32553357963102414, 0.33285779838102414, 0.3353080289909638, 0.3499888224774097, 0.3670786662274097, 0.34720179546310237, 0.34530897025602414, 0.35051828407379515, 0.34565459102033136, 0.33796416133283136, 0.3461222820971386, 0.34999911756400603, 0.3301619564194277, 0.3350756541792168, 0.3367846385542168, 0.33285779838102414, 0.3255026943712349, 0.3231730633471386, 0.32382459525602414, 0.3221053157944277, 0.3225935970444277, 0.3224715267319277, 0.3232245387801205, 0.3238143001694277, 0.3235701595444277, 0.3229598079819277, 0.3227156673569277, 0.3223494564194277, 0.3227156673569277, 0.32284803275602414, 0.32248182181852414, 0.3222273861069277, 0.32099638789533136, 0.3216170345444277, 0.3217391048569277, 0.3219832454819277, 0.3224715267319277, 0.3224715267319277, 0.3225935970444277, 0.3224715267319277, 0.3223494564194277, 0.3224715267319277, 0.3227156673569277, 0.3227156673569277, 0.3233260189194277, 0.3232039486069277, 0.3235701595444277, 0.32419080619352414, 0.32431287650602414, 0.3239363704819277, 0.3239363704819277, 0.3239363704819277, 0.32419080619352414, 0.32443494681852414, 0.32455701713102414, 0.32480115775602414, 0.32504529838102414, 0.32492322806852414, 0.32467908744352414, 0.32480115775602414, 0.32455701713102414, 0.32455701713102414, 0.32455701713102414, 0.32455701713102414, 0.32431287650602414, 0.32431287650602414, 0.3244452419051205, 0.32431287650602414, 0.32431287650602414, 0.32406873588102414, 0.32406873588102414, 0.32419080619352414, 0.32419080619352414, 0.3244452419051205, 0.3244452419051205, 0.3239569606551205, 0.3237128200301205, 0.3238348903426205, 0.3238348903426205, 0.3239569606551205, 0.3240790309676205, 0.3242011012801205, 0.3240790309676205], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.030677728474563827, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 32, "valid_ratio": 0.15, "learning_rate": 0.0005446349392944648, "optimization": "nesterov_momentum", "nb_data_augmentation": 0, "learning_rate_decay_method": "sqrt", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 3.95688548759758e-07, "rotation_range": [0, 0], "momentum": 0.9699644154555864}, "accuracy_valid_max": 0.6790036121046686, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.6759209690323795, "accuracy_valid_std": [0.008232647945172927, 0.012112873315099344, 0.015621830569930068, 0.01592208515442234, 0.01822407203828817, 0.01597055378982922, 0.015379000111123241, 0.026982909851264857, 0.021025546759018524, 0.023098302057978932, 0.021893636699537134, 0.022158657182827852, 0.018414996364538968, 0.025435540872544217, 0.023860330094976193, 0.023553714496982726, 0.01813648060741259, 0.01710925688420917, 0.015543998361789645, 0.014439558063353695, 0.015448605491430052, 0.01494124920199655, 0.016946933993259358, 0.019151537559558257, 0.017674164656773152, 0.017109760553785262, 0.01588701127301377, 0.015448690900359774, 0.016646370074574902, 0.01669586461908774, 0.015600466783145406, 0.016098171429792692, 0.015427513440912947, 0.015440489208608571, 0.015603346043481816, 0.01588145853001802, 0.01621506947193267, 0.0156956868685704, 0.015606248542648311, 0.01564516932790656, 0.01575018254167305, 0.016243189926419675, 0.016243189926419675, 0.01616393064098148, 0.016177003794702764, 0.016284590537902013, 0.016526933882928876, 0.01671576210068289, 0.01626026851325734, 0.016569879266612672, 0.016458158354687995, 0.016436813150098456, 0.016072843719152106, 0.016068883501597755, 0.01667393004542665, 0.01667393004542665, 0.016587914884353693, 0.016279189950213466, 0.016189658497877363, 0.016301314261935314, 0.01623578528717405, 0.0164296214403791, 0.01646791538590309, 0.016272704007925087, 0.016461825531919775, 0.016286681967428863, 0.016286681967428863, 0.016425162480508442, 0.016403374859272023, 0.01653686203759409, 0.01653686203759409, 0.016594034915195958, 0.016907559615855324, 0.016907559615855324, 0.01683650028764679, 0.01683650028764679, 0.0171423648749646, 0.0171423648749646, 0.01697753215394472, 0.01697753215394472, 0.016679501870198458, 0.01693634667191782, 0.017020289925524436, 0.016992251023113518, 0.017172509518852023, 0.01729498252564668, 0.017250678529523396, 0.017493729143792335], "accuracy_valid": [0.4629303346197289, 0.5848315135542168, 0.6491449195218373, 0.6626241293298193, 0.6738560688064759, 0.6744664203689759, 0.6671422016189759, 0.6646919710090362, 0.6500111775225903, 0.6329213337725903, 0.6527982045368976, 0.6546910297439759, 0.6494817159262049, 0.6543454089796686, 0.6620358386671686, 0.6538777179028614, 0.650000882435994, 0.6698380435805723, 0.6649243458207832, 0.6632153614457832, 0.6671422016189759, 0.6744973056287651, 0.6768269366528614, 0.6761754047439759, 0.6778946842055723, 0.6774064029555723, 0.6775284732680723, 0.6767754612198795, 0.6761856998305723, 0.6764298404555723, 0.6770401920180723, 0.6772843326430723, 0.6776505435805723, 0.6772843326430723, 0.6771519672439759, 0.6775181781814759, 0.6777726138930723, 0.6790036121046686, 0.6783829654555723, 0.6782608951430723, 0.6780167545180723, 0.6775284732680723, 0.6775284732680723, 0.6774064029555723, 0.6775284732680723, 0.6776505435805723, 0.6775284732680723, 0.6772843326430723, 0.6772843326430723, 0.6766739810805723, 0.6767960513930723, 0.6764298404555723, 0.6758091938064759, 0.6756871234939759, 0.6760636295180723, 0.6760636295180723, 0.6760636295180723, 0.6758091938064759, 0.6755650531814759, 0.6754429828689759, 0.6751988422439759, 0.6749547016189759, 0.6750767719314759, 0.6753209125564759, 0.6751988422439759, 0.6754429828689759, 0.6754429828689759, 0.6754429828689759, 0.6754429828689759, 0.6756871234939759, 0.6756871234939759, 0.6755547580948795, 0.6756871234939759, 0.6756871234939759, 0.6759312641189759, 0.6759312641189759, 0.6758091938064759, 0.6758091938064759, 0.6755547580948795, 0.6755547580948795, 0.6760430393448795, 0.6762871799698795, 0.6761651096573795, 0.6761651096573795, 0.6760430393448795, 0.6759209690323795, 0.6757988987198795, 0.6759209690323795], "seed": 718393590, "model": "residualv4", "loss_std": [0.29458752274513245, 0.1872236132621765, 0.18100565671920776, 0.17501720786094666, 0.16867853701114655, 0.15962901711463928, 0.1485455185174942, 0.13786581158638, 0.12751130759716034, 0.12177091836929321, 0.1132650226354599, 0.10284208506345749, 0.09197207540273666, 0.08336127549409866, 0.07096817344427109, 0.0672970712184906, 0.057923875749111176, 0.049684077501297, 0.03951476141810417, 0.028988128527998924, 0.021982897073030472, 0.014649588614702225, 0.008954589255154133, 0.0065446216613054276, 0.0033644784707576036, 0.0025887093506753445, 0.0021357641089707613, 0.0018793210620060563, 0.0016350292135030031, 0.0014592737425118685, 0.0012865940807387233, 0.0011628648499026895, 0.0010731188813224435, 0.0010031121782958508, 0.0009468988282606006, 0.0009008433553390205, 0.0008615086553618312, 0.000826425151899457, 0.0007952583837322891, 0.0007673995569348335, 0.0007421030895784497, 0.0007189378957264125, 0.0006975997239351273, 0.0006778299575671554, 0.0006594979204237461, 0.0006424318416975439, 0.0006264125695452094, 0.0006115349242463708, 0.0005975650274194777, 0.0005844675470143557, 0.0005721141351386905, 0.0005604368634521961, 0.0005493676871992648, 0.0005388670833781362, 0.0005288944812491536, 0.0005194660043343902, 0.0005104953306727111, 0.0005019039963372052, 0.0004936652258038521, 0.00048580230213701725, 0.00047828638344071805, 0.00047107000136747956, 0.0004641515261027962, 0.000457513116998598, 0.00045113638043403625, 0.0004450215201359242, 0.0004391315160319209, 0.0004334521945565939, 0.0004279749118722975, 0.0004226836608722806, 0.00041756583959795535, 0.00041262066224589944, 0.0004078152705915272, 0.0004031578719150275, 0.000398657051846385, 0.00039428158197551966, 0.00039004458812996745, 0.00038595101796090603, 0.0003819683624897152, 0.000378102355170995, 0.00037434196565300226, 0.00037069086101837456, 0.0003671415615826845, 0.0003636761975940317, 0.0003603080695029348, 0.0003570239059627056, 0.00035381675115786493, 0.0003506946377456188]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:35 2016", "state": "available"}], "summary": "1874d47d2d12ef4ce7be131e4d6b4ae1"}