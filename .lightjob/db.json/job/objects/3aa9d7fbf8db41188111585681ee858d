{"content": {"hp_model": {"f0": 32, "f1": 64, "f2": 64, "f3": 32, "nonlin": "very_leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.06022363023019774, 0.057739586218922744, 0.06507919886126995, 0.04248306455708834, 0.057899982835610084, 0.06258754324419516, 0.057554267627597966, 0.06352347045662014, 0.05758648592574023, 0.059594595044743855, 0.061872200787693496, 0.05699000258198529, 0.056365065021030905, 0.055933402890707265, 0.05798093754847931, 0.06234085361069476, 0.062050947263112305, 0.05639796267553775, 0.060405483578881876, 0.059512239217042476, 0.05723358007098615, 0.0675460121671095, 0.05946786960025472, 0.06226528127830448, 0.062475454490538936, 0.06035054183269276, 0.06519611220469182, 0.0606459301659829, 0.065551042747561, 0.05915966673025295, 0.06013739088513341, 0.060897435897435896, 0.06369926328013441, 0.05725943821870444, 0.06003469079169945, 0.05725320840775612, 0.05787287004244195, 0.05800615507662374, 0.058116736705422965, 0.05604107320612869, 0.060984355142581256, 0.055894807777548995, 0.06625681504800911, 0.05915665196378815, 0.05900662400861328, 0.06096622061866297, 0.06063004772322181, 0.058193099396976616, 0.06028549233408633, 0.05744974381662341, 0.05790090691159244, 0.05611199701001598, 0.05607829553500862, 0.057070685582849674, 0.058055635673874915, 0.06509344778077034, 0.06051138599722964, 0.056272912659580565, 0.0614092652548353, 0.06027750415017712, 0.06062004553184292, 0.05476339302833901, 0.059817731108458456, 0.056184418470287004, 0.061158989503855725, 0.057214568439480314, 0.05739694444313591, 0.05688820415538693, 0.063405441910927, 0.06057089314114849, 0.054983105316972765, 0.060324826097486725, 0.05905466234264883, 0.06163104013791478, 0.0517866302309504, 0.06146703271615186, 0.05909994573137798, 0.06176314531233668, 0.05507612048339683, 0.05349753812975808, 0.0555202314144872, 0.06054968938164425, 0.062048935276930076, 0.060186897301967704, 0.05641630111196942, 0.06247716728086593, 0.05393943671230019, 0.06106969049697009, 0.056622671732476114, 0.05899483504698943, 0.05512564282954125, 0.05920818333945887, 0.062260698188782264, 0.06468612360033654, 0.05749163818798749, 0.06070677436944664, 0.06046126034179646, 0.055207435259546606, 0.06054968938164425, 0.061550539865542944, 0.0657051282051282, 0.06041050264413706, 0.05779083812761075], "moving_avg_accuracy_train": [0.029052146084337344, 0.06240681475903613, 0.09983612575301203, 0.10228897778614456, 0.12729398437499997, 0.172791242564006, 0.21301635400037647, 0.2535746583593749, 0.29690836345717236, 0.32865625075603344, 0.353278389234647, 0.3758265857027486, 0.4036994994216303, 0.4312653552023588, 0.45204601094718316, 0.4669764813886094, 0.4829976773762545, 0.4998922808735688, 0.5182883125753686, 0.5348565069202413, 0.5418165452342413, 0.5419317657108171, 0.5524340898927474, 0.5600930717167257, 0.566797902346258, 0.5764231761176564, 0.5820550490179389, 0.5951315471281933, 0.5992102598852536, 0.6097617489569691, 0.613382241982959, 0.6157959003147835, 0.6188223908555943, 0.629838777523047, 0.6306891429936339, 0.6361349124292103, 0.6408666772103856, 0.6440710486459735, 0.6518707397151111, 0.6568761544484192, 0.6627835126481556, 0.6711757186122558, 0.6722692725040422, 0.6807129966090597, 0.6871004695385152, 0.6910654978858685, 0.6921349646635466, 0.6971378650345413, 0.7005462547359065, 0.7043197542623159, 0.709386649318012, 0.7115254505006685, 0.71735897472771, 0.7128035177669872, 0.7107343858698066, 0.7131855143008983, 0.7117817783828566, 0.715372990604812, 0.7134940123274632, 0.7126241856429096, 0.7094364093978958, 0.7143187285484677, 0.7134229061454281, 0.7185913459525721, 0.7161716880139415, 0.716822497375198, 0.7161892875473167, 0.7177066539733079, 0.7171026866181458, 0.7170932839201867, 0.7215911279679271, 0.7244131898699296, 0.7284284785636597, 0.7254204386892213, 0.729264407621504, 0.7253444615882693, 0.7247061938932978, 0.7265978674256548, 0.7286580543276676, 0.7303098505515273, 0.7336013429060131, 0.7351823795491468, 0.7351981212629068, 0.7293952707330017, 0.7325664364307858, 0.7297964268238517, 0.732329773599298, 0.7316448007574405, 0.7334332574286844, 0.7323084934328039, 0.736548464872656, 0.736552315674547, 0.7308893657335984, 0.7289906588289133, 0.7310586486689136, 0.7303713643743114, 0.7330895931477236, 0.7335146323269272, 0.7316028340038729, 0.7316965039167387, 0.7178971471997636, 0.7171870635038837, 0.7217037675450616], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 1234423, "moving_var_accuracy_train": [0.007596244728951099, 0.016849425557646385, 0.027773062895235637, 0.025049904953580057, 0.02817216764880038, 0.04398495540837402, 0.054148996178152435, 0.0635388810326316, 0.07408528290689431, 0.07574810974767554, 0.07362954610224817, 0.07084238196770032, 0.07075023764355234, 0.07051410152351244, 0.06734921224982532, 0.06262056155326352, 0.058668613885807984, 0.055370601143210366, 0.05287926687025192, 0.050061885757871995, 0.04549167638207592, 0.040942628225692335, 0.03784105472212433, 0.03458488927313216, 0.03153099312975537, 0.029211706873349147, 0.026575998117298664, 0.025457351531016154, 0.023061339457705988, 0.021757210806610194, 0.019699461453710345, 0.017781947027224385, 0.0160861891294445, 0.0155698171933615, 0.014019343566927448, 0.012884316852943823, 0.011797391549148785, 0.010710064360908813, 0.010186574551783793, 0.00939340468647718, 0.008768136145929396, 0.008525184619831351, 0.007683428898876388, 0.007556754299843627, 0.0071682771636800025, 0.006592942495469839, 0.005943942078619872, 0.005574808979856778, 0.005121882165078448, 0.004737847636652907, 0.004495123702536558, 0.004086781566773303, 0.0039843734542633034, 0.003772705801925955, 0.0034339669830047333, 0.003144642559975611, 0.0028479125747264515, 0.0026791925642618963, 0.00244304834213644, 0.002205552894073249, 0.0020764548611603957, 0.002083342737636723, 0.0018822309438731386, 0.0019344227798464544, 0.0017936732027216166, 0.0016181178578717464, 0.0014599146642597003, 0.0013346448056702587, 0.0012044633141981465, 0.0010840177784748922, 0.0011576914103275443, 0.001113598569703394, 0.0011473416023790124, 0.0011140421771170053, 0.0011356228337764949, 0.0011603543425301006, 0.0010479853791310887, 0.0009753926999951587, 0.0009160527606366699, 0.0008490033614594189, 0.000861608322590225, 0.0007979445821335836, 0.0007181523541341942, 0.0009493947871725587, 0.0009449619354005298, 0.0009195223208630365, 0.0008853307017387035, 0.0008010203217115745, 0.0007497054849246685, 0.0006861207828500629, 0.0007793049248619133, 0.0007013745658337988, 0.0009198581275536351, 0.0008603181059873631, 0.0008127755331937243, 0.0007357492171888133, 0.0007286732044513877, 0.0006574318087409711, 0.0006245833833191681, 0.0005622040114604378, 0.0022197838225353197, 0.0020023434099781788, 0.0019857146075406952], "duration": 147448.218131, "accuracy_train": [0.2905214608433735, 0.3625988328313253, 0.4366999246987952, 0.12436464608433735, 0.3523390436746988, 0.5822665662650602, 0.5750423569277109, 0.6185993975903614, 0.6869117093373494, 0.6143872364457831, 0.5748776355421686, 0.5787603539156626, 0.6545557228915663, 0.6793580572289156, 0.6390719126506024, 0.6013507153614458, 0.6271884412650602, 0.6519437123493976, 0.6838525978915663, 0.6839702560240963, 0.604456890060241, 0.54296875, 0.6469550075301205, 0.6290239081325302, 0.6271413780120482, 0.663050640060241, 0.6327419051204819, 0.7128200301204819, 0.6359186746987951, 0.7047251506024096, 0.6459666792168675, 0.6375188253012049, 0.6460608057228916, 0.7289862575301205, 0.6383424322289156, 0.6851468373493976, 0.6834525602409639, 0.6729103915662651, 0.7220679593373494, 0.7019248870481928, 0.7159497364457831, 0.7467055722891566, 0.6821112575301205, 0.7567065135542169, 0.7445877259036144, 0.7267507530120482, 0.7017601656626506, 0.742163968373494, 0.7312217620481928, 0.73828125, 0.7549887048192772, 0.7307746611445783, 0.7698606927710844, 0.6718044051204819, 0.6921121987951807, 0.7352456701807228, 0.6991481551204819, 0.7476939006024096, 0.6965832078313253, 0.7047957454819277, 0.6807464231927711, 0.7582596009036144, 0.7053605045180723, 0.7651073042168675, 0.6943947665662651, 0.722679781626506, 0.7104903990963856, 0.7313629518072289, 0.7116669804216867, 0.7170086596385542, 0.7620717243975904, 0.7498117469879518, 0.7645660768072289, 0.6983480798192772, 0.7638601280120482, 0.6900649472891566, 0.7189617846385542, 0.7436229292168675, 0.7471997364457831, 0.7451760165662651, 0.7632247740963856, 0.7494117093373494, 0.735339796686747, 0.6771696159638554, 0.7611069277108434, 0.7048663403614458, 0.7551298945783133, 0.7254800451807228, 0.7495293674698795, 0.7221856174698795, 0.7747082078313253, 0.7365869728915663, 0.6799228162650602, 0.711902296686747, 0.7496705572289156, 0.7241858057228916, 0.7575536521084337, 0.737339984939759, 0.7143966490963856, 0.7325395331325302, 0.5937029367469879, 0.7107963102409639, 0.7623541039156626], "end": "2016-01-22 15:49:15.140000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0], "accuracy_valid": [0.2875267094017094, 0.36538461538461536, 0.44190705128205127, 0.12980769230769232, 0.36204594017094016, 0.5858707264957265, 0.5722489316239316, 0.6132478632478633, 0.6742788461538461, 0.609375, 0.5771901709401709, 0.5655715811965812, 0.6452991452991453, 0.6681356837606838, 0.6270032051282052, 0.5968215811965812, 0.6192574786324786, 0.6394230769230769, 0.6785523504273504, 0.6756143162393162, 0.5958867521367521, 0.5377938034188035, 0.6422275641025641, 0.6235309829059829, 0.6227297008547008, 0.6573183760683761, 0.6179220085470085, 0.7040598290598291, 0.6326121794871795, 0.6952457264957265, 0.6419604700854701, 0.6225961538461539, 0.6383547008547008, 0.7163461538461539, 0.6278044871794872, 0.6788194444444444, 0.6750801282051282, 0.6688034188034188, 0.7132745726495726, 0.6895032051282052, 0.7019230769230769, 0.7367788461538461, 0.6680021367521367, 0.7415865384615384, 0.7317040598290598, 0.7144764957264957, 0.6925747863247863, 0.7256944444444444, 0.7231570512820513, 0.7247596153846154, 0.7445245726495726, 0.7118055555555556, 0.7520032051282052, 0.6649305555555556, 0.6846955128205128, 0.7214209401709402, 0.6911057692307693, 0.7361111111111112, 0.6903044871794872, 0.6903044871794872, 0.6738782051282052, 0.7371794871794872, 0.6911057692307693, 0.7458600427350427, 0.6828258547008547, 0.7119391025641025, 0.7037927350427351, 0.7183493589743589, 0.6976495726495726, 0.6996527777777778, 0.7493322649572649, 0.7329059829059829, 0.7462606837606838, 0.6887019230769231, 0.7475961538461539, 0.6773504273504274, 0.702590811965812, 0.7298344017094017, 0.7283653846153846, 0.7361111111111112, 0.7423878205128205, 0.7345085470085471, 0.7219551282051282, 0.6614583333333334, 0.7381143162393162, 0.6964476495726496, 0.7407852564102564, 0.7080662393162394, 0.7313034188034188, 0.7092681623931624, 0.7512019230769231, 0.7239583333333334, 0.671073717948718, 0.6953792735042735, 0.7395833333333334, 0.7076655982905983, 0.733840811965812, 0.7254273504273504, 0.7029914529914529, 0.7204861111111112, 0.5837339743589743, 0.6983173076923077, 0.7453258547008547], "accuracy_test": 0.10006009615384616, "start": "2016-01-20 22:51:46.922000", "learning_rate_per_epoch": [0.005376793909817934, 0.0038019674830138683, 0.0031042934861034155, 0.002688396954908967, 0.0024045754689723253, 0.0021950670052319765, 0.0020322371274232864, 0.0019009837415069342, 0.001792264636605978, 0.001700291526503861, 0.0016211643815040588, 0.0015521467430517077, 0.0014912543119862676, 0.0014370086137205362, 0.0013882822822779417, 0.0013441984774544835, 0.001304064062424004, 0.0012673224555328488, 0.0012335211504250765, 0.0012022877344861627, 0.0011733126593753695, 0.001146336318925023, 0.0011211390374228358, 0.0010975335026159883, 0.0010753588285297155, 0.0010544760152697563, 0.0010347644565626979, 0.0010161185637116432, 0.0009984455537050962, 0.0009816638194024563, 0.0009657007176429033, 0.0009504918707534671, 0.0009359797113575041, 0.0009221125510521233, 0.0009088440565392375, 0.000896132318302989, 0.0008839395013637841, 0.0008722311467863619, 0.0008609761134721339, 0.0008501457632519305, 0.0008397141355089843, 0.00082965730689466, 0.000819953391328454, 0.0008105821907520294, 0.0008015251369215548, 0.0007927650003693998, 0.0007842860068194568, 0.0007760733715258539, 0.0007681134156882763, 0.0007603935082443058, 0.0007529017748311162, 0.0007456271559931338, 0.0007385594653896987, 0.0007316889823414385, 0.0007250067428685725, 0.0007185043068602681, 0.0007121737580746412, 0.0007060076459310949, 0.0006999989273026586, 0.0006941411411389709, 0.0006884279428049922, 0.0006828535115346313, 0.0006774123758077621, 0.0006720992387272418, 0.0006669092108495533, 0.0006618375773541629, 0.0006568799144588411, 0.000652032031212002, 0.0006472899112850428, 0.0006426497711800039, 0.0006381080602295697, 0.0006336612277664244, 0.0006293061305768788, 0.0006250396254472435, 0.0006208586855791509, 0.0006167605752125382, 0.0006127425585873425, 0.0006088020163588226, 0.000604936562012881, 0.0006011438672430813, 0.000597421545535326, 0.0005937675596214831, 0.0005901798140257597, 0.0005866563296876848, 0.0005831951857544482, 0.0005797945777885616, 0.0005764528177678585, 0.0005731681594625115, 0.000569939031265676, 0.0005667638615705073, 0.0005636411369778216, 0.0005605695187114179, 0.0005575475515797734, 0.000554573955014348, 0.0005516474484466016, 0.0005487667513079941, 0.0005459306994453073, 0.0005431381869129837, 0.000540388107765466, 0.0005376794142648578, 0.0005350110004656017, 0.0005323819350451231, 0.0005297912284731865], "accuracy_train_last": 0.7623541039156626, "error_valid": [0.7124732905982907, 0.6346153846153846, 0.5580929487179487, 0.8701923076923077, 0.6379540598290598, 0.41412927350427353, 0.42775106837606836, 0.3867521367521367, 0.32572115384615385, 0.390625, 0.4228098290598291, 0.43442841880341876, 0.35470085470085466, 0.3318643162393162, 0.3729967948717948, 0.40317841880341876, 0.3807425213675214, 0.36057692307692313, 0.3214476495726496, 0.3243856837606838, 0.40411324786324787, 0.46220619658119655, 0.3577724358974359, 0.37646901709401714, 0.3772702991452992, 0.34268162393162394, 0.3820779914529915, 0.2959401709401709, 0.3673878205128205, 0.30475427350427353, 0.3580395299145299, 0.37740384615384615, 0.3616452991452992, 0.28365384615384615, 0.3721955128205128, 0.3211805555555556, 0.3249198717948718, 0.33119658119658124, 0.2867254273504274, 0.3104967948717948, 0.29807692307692313, 0.26322115384615385, 0.3319978632478633, 0.25841346153846156, 0.26829594017094016, 0.28552350427350426, 0.3074252136752137, 0.2743055555555556, 0.2768429487179487, 0.2752403846153846, 0.2554754273504274, 0.2881944444444444, 0.24799679487179482, 0.3350694444444444, 0.3153044871794872, 0.27857905982905984, 0.3088942307692307, 0.26388888888888884, 0.3096955128205128, 0.3096955128205128, 0.3261217948717948, 0.2628205128205128, 0.3088942307692307, 0.2541399572649573, 0.31717414529914534, 0.28806089743589747, 0.2962072649572649, 0.2816506410256411, 0.3023504273504274, 0.3003472222222222, 0.2506677350427351, 0.26709401709401714, 0.2537393162393162, 0.31129807692307687, 0.25240384615384615, 0.3226495726495726, 0.29740918803418803, 0.2701655982905983, 0.2716346153846154, 0.26388888888888884, 0.2576121794871795, 0.26549145299145294, 0.2780448717948718, 0.33854166666666663, 0.2618856837606838, 0.3035523504273504, 0.2592147435897436, 0.29193376068376065, 0.26869658119658124, 0.29073183760683763, 0.24879807692307687, 0.27604166666666663, 0.32892628205128205, 0.30462072649572647, 0.26041666666666663, 0.2923344017094017, 0.26615918803418803, 0.2745726495726496, 0.29700854700854706, 0.27951388888888884, 0.41626602564102566, 0.3016826923076923, 0.25467414529914534], "accuracy_train_std": [0.056636846322379676, 0.057674240385956144, 0.0622590985027881, 0.03893003758949666, 0.05865916259568755, 0.05968748705634231, 0.061363547465779315, 0.061385782795826425, 0.060764141294197827, 0.0654502781358725, 0.06335691059337133, 0.06401674652585655, 0.06271785488631353, 0.05812818559924245, 0.06229800684217297, 0.06535247017604037, 0.06407436342591559, 0.06293547916586825, 0.06032311686183966, 0.06019283348244453, 0.06403774500084802, 0.06578599416255387, 0.06306812625176109, 0.06281383680868896, 0.06374226816465096, 0.059650101831417314, 0.06399401059270697, 0.05774950727831058, 0.06328269975279667, 0.05747723933998564, 0.062096885952186404, 0.06402207895739336, 0.06323293957249515, 0.06119214307272441, 0.06373159947472566, 0.06217837686132605, 0.060477214053310906, 0.06195369158837915, 0.058510730389585326, 0.06020800596664626, 0.05893841024260851, 0.05796595762209039, 0.059978377005135135, 0.05612970710000495, 0.0556483314912966, 0.05771620602623228, 0.06040874234998518, 0.05620605220700248, 0.05770045029538127, 0.05804841609521662, 0.05523044688604417, 0.06023445094187555, 0.05572384554295736, 0.0603723724326585, 0.0613005690550711, 0.05831522107091211, 0.06092708016710046, 0.05704864187674483, 0.05880080269854273, 0.05822836669559025, 0.06281380154654741, 0.05447009390964884, 0.058234680829935326, 0.056438691775493335, 0.05830504090955675, 0.056762609782030564, 0.057638282574182104, 0.057131687704869295, 0.056897035009547105, 0.05675857581369721, 0.056482291109352975, 0.055355429271976384, 0.05527582071580527, 0.059673898935142834, 0.05584026212234095, 0.06011029054185178, 0.057840216194513114, 0.055615499564724266, 0.05602151840333478, 0.05639449905467499, 0.054614608685558466, 0.05636955328368286, 0.05648166366679193, 0.05997449932025667, 0.05674797979370245, 0.0610228570425637, 0.059164911089308284, 0.05764261041573436, 0.05596151284863318, 0.05687552748857518, 0.05523475787432875, 0.057871403269349905, 0.06175385693936799, 0.05993375000358143, 0.056277005273075435, 0.05698125415821135, 0.055301980798375124, 0.054701248680019986, 0.057606378131763315, 0.05688502898477156, 0.0630622126520176, 0.05927517227417114, 0.055149131751729544], "accuracy_test_std": 0.037479079290263825, "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-3, 3], "momentum": 0.8121607013633471, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.0053767939554899445, "patience_threshold": 1, "do_flip": true, "batch_size": 64, "optimization": "rmsprop", "nb_data_augmentation": 3, "learning_rate_decay_method": "sqrt", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.00024784696225646973, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.05677396065262527}, "accuracy_valid_max": 0.7520032051282052, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = 1234423\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-6, -3], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128, 256],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-3, 3)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_optimizer.learning_rate = learning_rate\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.7453258547008547, "loss_train": [3.912193775177002, 1.9586721658706665, 1.6000360250473022, 1.4591957330703735, 1.3710858821868896, 1.29976224899292, 1.2452034950256348, 1.2010211944580078, 1.1665112972259521, 1.142249584197998, 1.1233965158462524, 1.1071443557739258, 1.093085527420044, 1.0837981700897217, 1.0669623613357544, 1.057005763053894, 1.0479259490966797, 1.0358469486236572, 1.0285433530807495, 1.0212633609771729, 1.0148966312408447, 1.0070842504501343, 1.0009394884109497, 0.9937601685523987, 0.9938917756080627, 0.9852526187896729, 0.9809620976448059, 0.9768681526184082, 0.971235990524292, 0.9692816138267517, 0.9636051654815674, 0.9617564082145691, 0.9567819833755493, 0.9521244168281555, 0.9506106376647949, 0.9465954303741455, 0.9430503249168396, 0.939584493637085, 0.9356496930122375, 0.9336487054824829, 0.9315552711486816, 0.9303351044654846, 0.9274138808250427, 0.924641489982605, 0.9211238622665405, 0.9220570921897888, 0.9181352257728577, 0.9154769778251648, 0.9142907857894897, 0.9129086136817932, 0.910732626914978, 0.9081777930259705, 0.9065983295440674, 0.9047443866729736, 0.9035884141921997, 0.900876522064209, 0.9005317687988281, 0.8987135887145996, 0.8965311646461487, 0.8957061767578125, 0.8939022421836853, 0.8912691473960876, 0.8881769776344299, 0.8900695443153381, 0.886438250541687, 0.8853563070297241, 0.8840163350105286, 0.8830484747886658, 0.88272625207901, 0.8805909752845764, 0.8794891834259033, 0.8776820302009583, 0.8758480548858643, 0.8753368258476257, 0.8749726414680481, 0.8751725554466248, 0.8722224831581116, 0.8701736330986023, 0.8718270659446716, 0.8680992722511292, 0.8658118844032288, 0.867715060710907, 0.8651707768440247, 0.8641495108604431, 0.8642297387123108, 0.8622681498527527, 0.8623376488685608, 0.8607393503189087, 0.8593118786811829, 0.8597738742828369, 0.8582408428192139, 0.8588751554489136, 0.8571770191192627, 0.8548800349235535, 0.8549556136131287, 0.8538855910301208, 0.8541589379310608, 0.8529671430587769, 0.8516561388969421, 0.8502059578895569, 0.8502280712127686, 0.8496597409248352, 0.8494586944580078], "accuracy_train_first": 0.2905214608433735, "model": "residualv4", "loss_std": [1.8487776517868042, 0.20507733523845673, 0.13862375915050507, 0.13659775257110596, 0.13836869597434998, 0.13534094393253326, 0.13401147723197937, 0.13394713401794434, 0.13236020505428314, 0.13309930264949799, 0.13080735504627228, 0.13134951889514923, 0.13073374330997467, 0.1332879364490509, 0.13297343254089355, 0.12834244966506958, 0.12896373867988586, 0.13091306388378143, 0.12866181135177612, 0.12726783752441406, 0.1282113790512085, 0.12849698960781097, 0.12941716611385345, 0.12844796478748322, 0.12924998998641968, 0.12705551087856293, 0.12486916780471802, 0.12745775282382965, 0.12474916875362396, 0.12389735132455826, 0.12927109003067017, 0.1252802163362503, 0.12413565814495087, 0.12578101456165314, 0.12328474223613739, 0.12369080632925034, 0.12294432520866394, 0.12322961539030075, 0.1224360391497612, 0.12232328206300735, 0.12276586145162582, 0.12206228077411652, 0.12168767303228378, 0.1208517849445343, 0.1217469722032547, 0.12031099945306778, 0.12347868084907532, 0.12050099670886993, 0.12084174156188965, 0.12415246665477753, 0.12308511883020401, 0.12547490000724792, 0.11908712238073349, 0.12139041721820831, 0.12066664546728134, 0.11926940083503723, 0.12194682657718658, 0.11917485296726227, 0.12118896096944809, 0.12072532624006271, 0.1212533712387085, 0.11911939084529877, 0.12109151482582092, 0.12027378380298615, 0.11890774965286255, 0.12046307325363159, 0.12356817722320557, 0.12078267335891724, 0.11887230724096298, 0.12052901089191437, 0.12062900513410568, 0.12165187299251556, 0.11951063573360443, 0.11849743127822876, 0.12113616615533829, 0.12042862176895142, 0.11936462670564651, 0.11817554384469986, 0.11979164928197861, 0.11973302066326141, 0.11652003973722458, 0.11756634712219238, 0.11821336299180984, 0.11811862140893936, 0.12046676874160767, 0.11747102439403534, 0.11821535974740982, 0.1183975487947464, 0.1159423291683197, 0.11879701912403107, 0.1181773766875267, 0.11849845200777054, 0.12169813364744186, 0.11758586764335632, 0.11685814708471298, 0.11799049377441406, 0.11966773122549057, 0.11822071671485901, 0.12038785964250565, 0.11744627356529236, 0.1166318878531456, 0.11880922317504883, 0.11701613664627075]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:07 2016", "state": "available"}], "summary": "cd578537a8ef8a1be637004b2c6ec9b5"}