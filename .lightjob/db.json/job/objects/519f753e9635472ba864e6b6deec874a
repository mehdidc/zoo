{"content": {"hp_model": {"f0": 16, "f1": 16, "f2": 32, "f3": 32, "nonlin": "very_leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.019888353969383828, 0.017540110241475733, 0.013878698087590397, 0.011810551166711125, 0.012118468869976601, 0.018279266328742348, 0.015537031219680782, 0.012838088820268885, 0.012728502901881617, 0.021720980303585814, 0.018114025913015714, 0.021823648908313682, 0.020812574633643447, 0.018853466095127804, 0.019592703618586754, 0.020490857486452792, 0.020495508665585624, 0.01753896579956428, 0.014528844593015143, 0.016381680991952183, 0.014674122574626835, 0.017207851270841356, 0.017842251531694664, 0.023744234268981562, 0.013973285033427188, 0.01849318851429057, 0.017636760780915507, 0.020482118121663864, 0.01808803228833413, 0.015592242478643752, 0.012383653992457843, 0.015618568677195611, 0.016598071306907915, 0.012473800000118943, 0.016888753158193954, 0.017886901654929795, 0.008538281406683142, 0.012496566300849343, 0.012877089245256174, 0.014471613031553157, 0.016440829331840875, 0.011453918641280451, 0.012427968503386564, 0.01411793157246515, 0.012552790814983528, 0.013654535503076793, 0.017798258333227547, 0.012289885812574562, 0.010366722179790197, 0.012211071270052722, 0.010875853183207912, 0.013853780382352096, 0.008714479272528339, 0.010540543527794386, 0.00909206395148274, 0.009642654646916101, 0.01000407385479578, 0.010689305993050733, 0.013906511566186356, 0.01139744791767043, 0.01211024978782714, 0.013646300685596108, 0.016927024374391048, 0.012523706153177933, 0.013315194687165296, 0.017702197269482217, 0.007515534096032311, 0.01121004451954706, 0.009658977470682154, 0.010862944292760976, 0.007046278272331144, 0.016401082021980307, 0.009696080343595103, 0.0066904516274459164, 0.018041966365782768, 0.013790724909222712, 0.00944206685368063, 0.013124916880977479, 0.010658075001052649, 0.009392131823443409, 0.012159703306000573, 0.009306195703886211, 0.016440830647490325, 0.01263959627671078, 0.011944037666597344, 0.009742191843297082, 0.01541361889380659, 0.013238490848187866, 0.019471209657129056, 0.0081987770404012, 0.013523692686647873, 0.014123976025073065, 0.012427756078556555, 0.014308456408759409, 0.011039645389017425, 0.007894914255752823, 0.010497329309767277, 0.012014906698547467, 0.011438916356253466, 0.011557962269690617, 0.011638708091667492, 0.011343523828740813, 0.013288888390024485, 0.009545476810514618, 0.01160404180215683, 0.009716533499431732, 0.008244958043395673, 0.012671280720969862, 0.013143532201093325, 0.012053032611755225, 0.012040723358164293, 0.010057846914321942, 0.010606922962506125, 0.010937777039246172, 0.013042343539778843, 0.009284948868777284, 0.011422911825481342, 0.01419068536967584, 0.014941474244460627, 0.014298907017434373, 0.015635762057125555, 0.012525949438934104, 0.020186413815032354, 0.01248315081743832, 0.011661122144623259, 0.014327982963281206, 0.013250938581185975, 0.010489929809277586, 0.010623958386885101, 0.008428800550199544, 0.01082336345361821, 0.008295045284354086, 0.011133634705628673, 0.008505700406543061, 0.01667898520773812, 0.013495713405516085, 0.012811322880554551, 0.013641611791868418, 0.011886650405604137, 0.015716408288879193, 0.011584499180535045], "moving_avg_accuracy_train": [0.04011022286821705, 0.08950896822512919, 0.14115572116382888, 0.19316345186905912, 0.24580483872118972, 0.29726868912832505, 0.34524344486365605, 0.38878584548618667, 0.42881520523204936, 0.46795667952935016, 0.4991788564626739, 0.5314977351321134, 0.5614774028333723, 0.5888289105716759, 0.613190150045839, 0.6389880983176024, 0.6634337500895243, 0.6745526175478107, 0.6932312497575922, 0.710395333218987, 0.7273819684555969, 0.7440973291958621, 0.7615939335144173, 0.7760810072345425, 0.789802570795649, 0.8030844708197201, 0.8119179392854594, 0.8129431368955348, 0.823876030979837, 0.8276130933488153, 0.8377208410056743, 0.847419883243239, 0.8549147637761964, 0.86393854974746, 0.8723504925763316, 0.8792911618437538, 0.8843102298082802, 0.888585647597801, 0.8947165673975909, 0.9008388978590591, 0.9034172349673024, 0.9079489188337597, 0.9123224758706495, 0.9137897657051424, 0.9174259945263779, 0.9170227925393639, 0.9205681254116733, 0.9239054814693894, 0.927192589832002, 0.9306369795083625, 0.9325630184588275, 0.9345731101737605, 0.9377165397433169, 0.9397783272487749, 0.9425222149465625, 0.9445499716495807, 0.9454984076299252, 0.9479933748276654, 0.946674788717638, 0.949036022009034, 0.9506448047403011, 0.9503375823354386, 0.9518952280447703, 0.9535389967569969, 0.9480973218125173, 0.9482493886980338, 0.950376287885401, 0.9526113316409269, 0.9544694111994717, 0.9568485280402573, 0.958450406819611, 0.9590155166198389, 0.9598821523078919, 0.960845955378367, 0.9616923436560527, 0.9620264459691112, 0.9627572184829882, 0.963635694735926, 0.964672861235017, 0.9657179182270562, 0.9672069522972078, 0.9685587447532106, 0.9674550036469832, 0.9678287160049224, 0.9691485590246774, 0.969899361863904, 0.9707842757144276, 0.9715086906644411, 0.9718187951468158, 0.9728954512714384, 0.9738296726979121, 0.9741472414019581, 0.9719617084416257, 0.9725870534070331, 0.973265977121101, 0.974267633463762, 0.9744344852948036, 0.9707369992976229, 0.9714126995155167, 0.972448548946117, 0.9736156895122381, 0.9744428656872139, 0.9754640169530255, 0.9756344272732269, 0.9763063047459319, 0.9762692894487566, 0.976749761470566, 0.9770729403449657, 0.9779892663616874, 0.978374614798193, 0.9792652969195641, 0.9799762660740455, 0.9800278756642693, 0.9809370266014416, 0.9814018037770301, 0.9803972203100876, 0.9804835725338776, 0.9808402713436127, 0.9809194126985464, 0.9811487500370343, 0.9816132812083493, 0.9818942115315897, 0.9822586199165444, 0.9811753302820789, 0.9810536992241552, 0.9816556186612819, 0.982169408320163, 0.9825760154417273, 0.9822141902737543, 0.9821375187892913, 0.9826356705187048, 0.9827656058835472, 0.983205635249973, 0.9833900370892705, 0.9838094760136953, 0.9838288620801922, 0.9839090885578965, 0.9842162045152298, 0.9843415102530295, 0.9844798260051352, 0.9849716115962975], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 727727660, "moving_var_accuracy_train": [0.01447946980684238, 0.034993647211691584, 0.0555007662925222, 0.07429392614123972, 0.0918045740145568, 0.1064608677016532, 0.11652897562226996, 0.12193954392779958, 0.12416673630999286, 0.1255385577704899, 0.12175812098554287, 0.11898289815303816, 0.11517363261703552, 0.11038921413535831, 0.10469152262028021, 0.10021217757354545, 0.09556926883117753, 0.08712500487005424, 0.0815525260941034, 0.07604872533432057, 0.07104076479084327, 0.06645131787385378, 0.06256136655058915, 0.05819410764028134, 0.054069228635306575, 0.0502499855860207, 0.04592725851363553, 0.04134399193352931, 0.03828534629770351, 0.034582502384279866, 0.032043751210104526, 0.029686018872028656, 0.02722297609265552, 0.02523353690268655, 0.023347030251823733, 0.021445883235559012, 0.01952801430109593, 0.017739725646460897, 0.016304046680137907, 0.01501098838463892, 0.013569719946368726, 0.012397573379721435, 0.011329968052143648, 0.010216347702054937, 0.009313712372212896, 0.008383804281572593, 0.007658548319994636, 0.006992934997098937, 0.006390887229877061, 0.005858572889072926, 0.005306102234514006, 0.004811856229384588, 0.004419600951574983, 0.004015899565876445, 0.0036820698865714374, 0.0033508690731340095, 0.0030238779430999197, 0.0027775139006501253, 0.002515410534551131, 0.002314048285003591, 0.0021059370933910423, 0.0018961928545063837, 0.0017284099104579414, 0.001579886699625802, 0.0016884044654756243, 0.001519772137967098, 0.0014085082255493964, 0.0013126161882964977, 0.0012124267062797868, 0.001142125808130794, 0.0010510073679314094, 0.0009487807729150911, 0.0008606622123658464, 0.0007829562383571772, 0.0007111079725708924, 0.0006410017945141221, 0.0005817078712660529, 0.0005304825688822283, 0.00048711574111553655, 0.000448233464053471, 0.00042336511980677185, 0.0003974746934230479, 0.0003686914239469258, 0.0003330792298905214, 0.00031544917727263525, 0.0002889776036758883, 0.0002671274960139359, 0.0002451377395907698, 0.00022148944874159242, 0.00020977319956361936, 0.00019665080667040106, 0.00017789337493946607, 0.00020309302633181287, 0.00018630323063047492, 0.00017182134425314027, 0.00016366904868696404, 0.00014755269961996527, 0.0002558400539520891, 0.00023436518561703416, 0.0002205855234412078, 0.00021078692500685524, 0.00019586621632619955, 0.00018566434386259822, 0.00016735926657141848, 0.0001546861139592327, 0.0001392298337533342, 0.00012738453065167587, 0.00011558607885023231, 0.00011158435128549675, 0.00010176235691460347, 9.872595299511553e-05, 9.340265194321978e-05, 8.408635869712535e-05, 8.311672166646236e-05, 7.674920990634912e-05, 7.815698039420355e-05, 7.04083927137646e-05, 6.451265981018653e-05, 5.8117764015714504e-05, 5.277934814756559e-05, 4.944351621491884e-05, 4.5209461212070394e-05, 4.18836563300912e-05, 4.825693858634433e-05, 4.3564391755974535e-05, 4.246871565949597e-05, 4.0597662415704196e-05, 3.802586033589547e-05, 3.540153137191433e-05, 3.1914284883490604e-05, 3.0956252704799855e-05, 2.8012576225650263e-05, 2.695395119293777e-05, 2.456459241867119e-05, 2.369149427870733e-05, 2.1325727227004574e-05, 1.9251081093827667e-05, 1.817485488568352e-05, 1.6498683148444803e-05, 1.502099605912539e-05, 1.5695574062287193e-05], "duration": 24477.399516, "accuracy_train": [0.40110222868217055, 0.5340976764373385, 0.6059764976121262, 0.6612330282161315, 0.7195773203903654, 0.7604433427925434, 0.7770162464816353, 0.7806674510889626, 0.7890794429448136, 0.8202299482050572, 0.7801784488625876, 0.822367643157069, 0.8312944121447029, 0.8349924802164084, 0.8324413053133074, 0.8711696327634736, 0.8834446160368217, 0.7746224246723883, 0.8613389396456257, 0.8648720843715393, 0.8802616855850868, 0.8945355758582503, 0.9190633723814139, 0.9064646707156699, 0.9132966428456073, 0.9226215710363603, 0.8914191554771133, 0.8221699153862125, 0.9222720777385567, 0.8612466546696198, 0.9286905699174051, 0.9347112633813216, 0.9223686885728128, 0.9451526234888336, 0.9480579780361758, 0.9417571852505537, 0.929481841489018, 0.9270644077034883, 0.9498948455956996, 0.9559398720122739, 0.9266222689414912, 0.9487340736318751, 0.9516844892026578, 0.9269953742155776, 0.9501520539174971, 0.9133939746562385, 0.9524761212624585, 0.9539416859888336, 0.956776565095515, 0.9616364865956073, 0.9498973690130121, 0.952663935608158, 0.9660074058693245, 0.9583344147978959, 0.9672172042266519, 0.9627997819767442, 0.9540343314530271, 0.9704480796073275, 0.9348075137273901, 0.9702871216315985, 0.9651238493217055, 0.9475725806916758, 0.965914039428756, 0.9683329151670359, 0.8991222473122, 0.9496179906676817, 0.9695183805717055, 0.9727267254406607, 0.971192127226375, 0.9782605796073275, 0.9728673158337948, 0.96410150482189, 0.9676818735003692, 0.9695201830126431, 0.9693098381552234, 0.9650333667866371, 0.9693341711078812, 0.9715419810123662, 0.9740073597268365, 0.975123431155408, 0.9806082589285714, 0.9807248768572352, 0.9575213336909376, 0.971192127226375, 0.9810271462024732, 0.9766565874169435, 0.97874850036914, 0.9780284252145626, 0.9746097354881875, 0.9825853563930418, 0.9822376655361758, 0.9770053597383721, 0.9522919117986341, 0.9782151580956996, 0.9793762905477114, 0.9832825405477114, 0.9759361517741787, 0.9374596253229974, 0.9774940014765596, 0.9817711938215209, 0.9841199546073275, 0.9818874512619971, 0.9846543783453304, 0.9771681201550388, 0.9823532020002769, 0.9759361517741787, 0.9810740096668512, 0.9799815502145626, 0.9862362005121816, 0.9818427507267442, 0.9872814360119048, 0.986374988464378, 0.9804923619762828, 0.9891193850359912, 0.9855847983573275, 0.9713559691076044, 0.9812607425479882, 0.9840505606312293, 0.9816316848929494, 0.9832127860834257, 0.9857940617501846, 0.984422584440753, 0.985538295381137, 0.97142572357189, 0.9799590197028424, 0.9870728935954227, 0.9867935152500923, 0.9862354795358066, 0.9789577637619971, 0.9814474754291252, 0.9871190360834257, 0.9839350241671282, 0.9871658995478036, 0.9850496536429494, 0.987584426333518, 0.9840033366786637, 0.9846311268572352, 0.9869802481312293, 0.9854692618932264, 0.9857246677740864, 0.9893976819167589], "end": "2016-01-24 10:03:46.621000", "learning_rate_per_epoch": [0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665, 0.000838864129036665], "accuracy_valid": [0.4071942065135542, 0.5249655849962349, 0.580956148814006, 0.6314447242093373, 0.6762665897966867, 0.7038250658885542, 0.7102639071912651, 0.7127259036144578, 0.7070488987198795, 0.7233857304216867, 0.6916165639118976, 0.7150952442582832, 0.7174557605421686, 0.7218811770519578, 0.7207016542733433, 0.7409653261483433, 0.7437317629894578, 0.6649434652673193, 0.7230812900037651, 0.7175778308546686, 0.7318100527108433, 0.7321762636483433, 0.7541695100715362, 0.7441185641001506, 0.7467438111822289, 0.7543636459902108, 0.7274861163403614, 0.673670757247741, 0.7415756777108433, 0.7106918886483433, 0.7518001694277108, 0.7599479951054217, 0.742053663874247, 0.7665000823606928, 0.7616878647402108, 0.7603450913027108, 0.7526958419615963, 0.7509765625, 0.7587993575865963, 0.7721359069088856, 0.7437023484563253, 0.7569785979856928, 0.7560829254518072, 0.7502750258847892, 0.7603156767695783, 0.7365104951054217, 0.7609245576054217, 0.771252000188253, 0.7643439970820783, 0.7702239622552711, 0.7599288756588856, 0.7588905426393072, 0.7744964231927711, 0.7529296875, 0.7779040968561747, 0.7636012801204819, 0.7611804640436747, 0.7683620223079819, 0.7506206466490963, 0.7699592314570783, 0.7623393966490963, 0.7516883942018072, 0.7578727997929217, 0.7615452042545181, 0.7225032944277108, 0.7522575654179217, 0.7697459760918675, 0.7747817441641567, 0.7683414321347892, 0.7764804334525602, 0.7676604856927711, 0.7600818312311747, 0.7644351821347892, 0.7703563276543675, 0.7585655120481928, 0.7632747788027108, 0.7644557723079819, 0.7699180511106928, 0.7736213408320783, 0.7673957548945783, 0.7779143919427711, 0.7803455031061747, 0.7558299604668675, 0.7638145354856928, 0.7750964796686747, 0.7724006377070783, 0.7733566100338856, 0.7756862410579819, 0.7638954254518072, 0.7782703077936747, 0.7844767742846386, 0.7733066053275602, 0.7550460631588856, 0.7766216232115963, 0.7723197477409638, 0.7783923781061747, 0.7709975644766567, 0.7469070618411144, 0.7685752776731928, 0.7769981292356928, 0.776745164250753, 0.7725124129329819, 0.7826148343373494, 0.7724624082266567, 0.775524461125753, 0.7673957548945783, 0.7715358504329819, 0.7742522825677711, 0.7779349821159638, 0.7775687711784638, 0.7772731551204819, 0.780407273625753, 0.7805396390248494, 0.7802646131400602, 0.7768363493034638, 0.7679958113704819, 0.7637439406061747, 0.7819441829819277, 0.772716843938253, 0.7715564406061747, 0.7799498776355422, 0.7756862410579819, 0.7820971385542168, 0.772960984563253, 0.7727565535579819, 0.7776805464043675, 0.7781394131212349, 0.7781997129141567, 0.7677207854856928, 0.776623093938253, 0.7803160885730422, 0.7694709502070783, 0.7758289015436747, 0.7790439100150602, 0.7829413356551205, 0.7831751811935241, 0.7799498776355422, 0.7816897472703314, 0.7789218397025602, 0.778820359563253, 0.7809867399284638], "accuracy_test": 0.10810148278061224, "start": "2016-01-24 03:15:49.221000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0], "accuracy_train_last": 0.9893976819167589, "batch_size_eval": 1024, "accuracy_train_std": [0.019077866348772017, 0.015814362849759137, 0.01896340392640931, 0.021032173798300247, 0.024311555413683267, 0.025020353461558623, 0.027763355999448923, 0.027500612585663175, 0.03135167494511873, 0.03160662092912043, 0.030483171191396465, 0.033644708304045666, 0.035178863154167635, 0.035631248120150436, 0.02686219710465586, 0.029739370817795863, 0.029799888094008815, 0.029682909546419108, 0.029681292676892065, 0.02627519239373273, 0.027957933755009524, 0.025604039569695074, 0.022712119984023182, 0.0227750234868098, 0.022568393038058486, 0.021506447688118015, 0.025982355395825323, 0.02005299879438727, 0.019517826298208543, 0.023120172639837316, 0.01923081846434485, 0.018130358276488966, 0.01804067054930565, 0.016810747283853962, 0.017334346737147618, 0.017084538326551022, 0.018162165635480787, 0.015223007005270128, 0.016164554905122846, 0.015766061043728172, 0.01722931337381567, 0.012598900759654308, 0.01587335938591994, 0.015206660234062096, 0.015213513640836306, 0.014109281819686634, 0.012913577288429815, 0.012289601338185973, 0.012646641843508367, 0.012201408286035187, 0.013028246991008543, 0.012196829089938023, 0.010983778699322783, 0.011180057886607048, 0.010674275132957217, 0.012017674213129938, 0.011786348145018439, 0.009373823584316543, 0.012894156151747766, 0.008904386503533817, 0.010476083861254603, 0.010409592067943467, 0.01031466348320753, 0.009522312293459735, 0.01405183658392995, 0.01000479303994008, 0.009978860417523237, 0.008044859141585954, 0.009048846420456216, 0.008222109058877025, 0.007950549129143416, 0.010982347830480669, 0.008818382020894102, 0.00825739346263511, 0.008100191801311347, 0.008277362460224048, 0.008131445541197659, 0.007948095241176305, 0.00721828663773255, 0.0061078921144422475, 0.0063773537949149035, 0.007722858849194442, 0.01127405618593294, 0.0075765414770357364, 0.006092719934902038, 0.005842467543380181, 0.00673594230796685, 0.006725492582388305, 0.007101084620907176, 0.006013693897529537, 0.005885960694044338, 0.007795098986266754, 0.00868031137676483, 0.00695920578913522, 0.006625079315131809, 0.006975473546638507, 0.006619444063753558, 0.012013260838956033, 0.007025588062092425, 0.006270910186573103, 0.00572388062017382, 0.00695598725201201, 0.0052826373064591686, 0.007213359471499085, 0.005276000460740288, 0.0069572810094497, 0.007082057524910382, 0.006459970601944691, 0.0045615435550127245, 0.005992251319906938, 0.0048148549167664895, 0.004615732294589229, 0.006515414677936436, 0.0038288206257167446, 0.00552075091980752, 0.007292896528569427, 0.006536871782552541, 0.005978864192448922, 0.005505739368851962, 0.0050044091349073505, 0.005134553642942966, 0.004870782885696027, 0.004743998469863057, 0.006003148463144877, 0.0051896557577817744, 0.004576002818123269, 0.0052607496427978994, 0.004110174027584316, 0.006040666226658467, 0.005240157723555881, 0.005234765872056909, 0.004735047914946298, 0.005073720498019938, 0.004013448189270267, 0.0039785692616643, 0.006009272248439171, 0.004934955195701844, 0.004575567930835113, 0.004904300670880349, 0.004117664783030366, 0.004265843040682625], "accuracy_test_std": 0.009790681887742398, "error_valid": [0.5928057934864458, 0.4750344150037651, 0.41904385118599397, 0.3685552757906627, 0.32373341020331325, 0.2961749341114458, 0.2897360928087349, 0.28727409638554224, 0.2929511012801205, 0.27661426957831325, 0.30838343608810237, 0.2849047557417168, 0.28254423945783136, 0.27811882294804224, 0.2792983457266567, 0.2590346738516567, 0.25626823701054224, 0.3350565347326807, 0.2769187099962349, 0.28242216914533136, 0.2681899472891567, 0.2678237363516567, 0.2458304899284638, 0.25588143589984935, 0.2532561888177711, 0.24563635400978923, 0.2725138836596386, 0.32632924275225905, 0.2584243222891567, 0.2893081113516567, 0.24819983057228923, 0.24005200489457834, 0.257946336125753, 0.23349991763930722, 0.23831213525978923, 0.23965490869728923, 0.24730415803840367, 0.2490234375, 0.24120064241340367, 0.22786409309111444, 0.2562976515436747, 0.24302140201430722, 0.24391707454819278, 0.24972497411521077, 0.23968432323042166, 0.26348950489457834, 0.23907544239457834, 0.22874799981174698, 0.23565600291792166, 0.22977603774472888, 0.24007112434111444, 0.24110945736069278, 0.22550357680722888, 0.2470703125, 0.22209590314382532, 0.2363987198795181, 0.23881953595632532, 0.2316379776920181, 0.24937935335090367, 0.23004076854292166, 0.23766060335090367, 0.24831160579819278, 0.24212720020707834, 0.2384547957454819, 0.2774967055722892, 0.24774243458207834, 0.23025402390813254, 0.22521825583584332, 0.23165856786521077, 0.22351956654743976, 0.23233951430722888, 0.23991816876882532, 0.23556481786521077, 0.22964367234563254, 0.24143448795180722, 0.23672522119728923, 0.2355442276920181, 0.23008194888930722, 0.22637865916792166, 0.23260424510542166, 0.22208560805722888, 0.21965449689382532, 0.24417003953313254, 0.23618546451430722, 0.22490352033132532, 0.22759936229292166, 0.22664338996611444, 0.2243137589420181, 0.23610457454819278, 0.22172969220632532, 0.21552322571536142, 0.22669339467243976, 0.24495393684111444, 0.22337837678840367, 0.2276802522590362, 0.22160762189382532, 0.22900243552334332, 0.25309293815888556, 0.23142472232680722, 0.22300187076430722, 0.22325483574924698, 0.2274875870670181, 0.21738516566265065, 0.22753759177334332, 0.22447553887424698, 0.23260424510542166, 0.2284641495670181, 0.22574771743222888, 0.2220650178840362, 0.2224312288215362, 0.2227268448795181, 0.21959272637424698, 0.21946036097515065, 0.21973538685993976, 0.2231636506965362, 0.2320041886295181, 0.23625605939382532, 0.2180558170180723, 0.22728315606174698, 0.22844355939382532, 0.22005012236445776, 0.2243137589420181, 0.2179028614457832, 0.22703901543674698, 0.2272434464420181, 0.22231945359563254, 0.2218605868787651, 0.22180028708584332, 0.23227921451430722, 0.22337690606174698, 0.21968391142695776, 0.23052904979292166, 0.22417109845632532, 0.22095608998493976, 0.21705866434487953, 0.21682481880647586, 0.22005012236445776, 0.21831025272966864, 0.22107816029743976, 0.22117964043674698, 0.2190132600715362], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-3, 3], "momentum": 0.7650078790501857, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.0008388641275788434, "patience_threshold": 1, "do_flip": true, "batch_size": 64, "optimization": "rmsprop", "nb_data_augmentation": 0, "learning_rate_decay_method": "none", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 3.287963021258949e-07, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.0009233234993254458}, "accuracy_valid_max": 0.7844767742846386, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-3, 3)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.7809867399284638, "loss_train": [1.4243298768997192, 1.0284273624420166, 0.8530747294425964, 0.745167076587677, 0.6660360097885132, 0.6012682914733887, 0.5429729223251343, 0.4908240735530853, 0.4406682252883911, 0.39385461807250977, 0.35489705204963684, 0.3205265700817108, 0.29066193103790283, 0.26919910311698914, 0.24605242908000946, 0.22810733318328857, 0.21393437683582306, 0.1980125904083252, 0.18499554693698883, 0.17421722412109375, 0.1651083528995514, 0.15502658486366272, 0.14773045480251312, 0.1421644687652588, 0.1357530802488327, 0.13056305050849915, 0.12090522795915604, 0.11940941214561462, 0.11633626371622086, 0.11187230050563812, 0.10931428521871567, 0.10478031635284424, 0.09824046492576599, 0.09912194311618805, 0.09220550209283829, 0.09278624504804611, 0.0925859808921814, 0.08509869128465652, 0.08547123521566391, 0.08270728588104248, 0.08402451872825623, 0.07802800089120865, 0.07692742347717285, 0.0768095925450325, 0.07207879424095154, 0.07414067536592484, 0.07179147005081177, 0.07130452990531921, 0.06756782531738281, 0.06893247365951538, 0.06709831953048706, 0.06628335267305374, 0.06577860563993454, 0.06485629081726074, 0.06347263604402542, 0.06041876599192619, 0.06376852095127106, 0.05996238440275192, 0.058449022471904755, 0.05738312005996704, 0.059716686606407166, 0.058445509523153305, 0.055402662605047226, 0.05418376624584198, 0.05375782772898674, 0.05494929850101471, 0.05323229357600212, 0.05256829038262367, 0.052713919430971146, 0.050009798258543015, 0.05068906024098396, 0.04656842350959778, 0.04793938621878624, 0.04784846305847168, 0.048768118023872375, 0.04761933907866478, 0.04860144481062889, 0.04625256732106209, 0.045196253806352615, 0.04664124920964241, 0.0465485081076622, 0.04537554830312729, 0.04594656080007553, 0.04406158626079559, 0.043753549456596375, 0.045937713235616684, 0.039125796407461166, 0.04245671257376671, 0.04291100054979324, 0.042396314442157745, 0.04305103421211243, 0.041093967854976654, 0.04026858136057854, 0.04005947336554527, 0.04111076891422272, 0.0404224693775177, 0.03895879164338112, 0.03953882306814194, 0.03758351504802704, 0.0397469736635685, 0.0374896265566349, 0.03611823171377182, 0.03828257694840431, 0.036675192415714264, 0.03682347759604454, 0.03713073953986168, 0.03767287731170654, 0.03696626424789429, 0.03502777963876724, 0.036797840148210526, 0.036005113273859024, 0.03463912755250931, 0.03512901812791824, 0.03399514779448509, 0.03544878959655762, 0.03518547862768173, 0.03673350811004639, 0.033910974860191345, 0.036492351442575455, 0.033208608627319336, 0.03580755367875099, 0.03416477143764496, 0.03407790884375572, 0.03224346786737442, 0.03451462835073471, 0.032649412751197815, 0.03515021502971649, 0.03217492997646332, 0.031368449330329895, 0.032364621758461, 0.03265034779906273, 0.03109833039343357, 0.03260894864797592, 0.03339310362935066, 0.03205367922782898, 0.03245335444808006, 0.030405744910240173, 0.03189542889595032, 0.03167172148823738, 0.03043430857360363, 0.029794570058584213], "accuracy_train_first": 0.40110222868217055, "model": "residualv3", "loss_std": [0.293439656496048, 0.14816994965076447, 0.13152146339416504, 0.12525302171707153, 0.12289199233055115, 0.1188841313123703, 0.11658585071563721, 0.11185108125209808, 0.10766240209341049, 0.1052003726363182, 0.10240887105464935, 0.09562882781028748, 0.09129678457975388, 0.08938788622617722, 0.08451099693775177, 0.08168569207191467, 0.07523338496685028, 0.0691113993525505, 0.07139717787504196, 0.06919271498918533, 0.0682031512260437, 0.06373146176338196, 0.06583289802074432, 0.06028087064623833, 0.06585361063480377, 0.06075336039066315, 0.055529482662677765, 0.055140335112810135, 0.058090902864933014, 0.05676902458071709, 0.05605112388730049, 0.053159087896347046, 0.05033956468105316, 0.053685206919908524, 0.04699929058551788, 0.05322221294045448, 0.050289664417505264, 0.048673391342163086, 0.04881541058421135, 0.04709899052977562, 0.04918237403035164, 0.04729350283741951, 0.04454362764954567, 0.04498155787587166, 0.04585752263665199, 0.04756646603345871, 0.04577699676156044, 0.04639491066336632, 0.042239971458911896, 0.045163244009017944, 0.04690326377749443, 0.04529085010290146, 0.04411641135811806, 0.045942433178424835, 0.042990732938051224, 0.03906727582216263, 0.04518963396549225, 0.03991292417049408, 0.03951031342148781, 0.04239288717508316, 0.04027050733566284, 0.04793917015194893, 0.039576008915901184, 0.03663106635212898, 0.04059474542737007, 0.03919249773025513, 0.03846106305718422, 0.03902030363678932, 0.03923248499631882, 0.036056216806173325, 0.037518858909606934, 0.0353252999484539, 0.03491217643022537, 0.03652351349592209, 0.03657195717096329, 0.03612428158521652, 0.03805584833025932, 0.033826183527708054, 0.035501107573509216, 0.03624146431684494, 0.03438638150691986, 0.035476092249155045, 0.036279838532209396, 0.033026259392499924, 0.03286166489124298, 0.040174130350351334, 0.03266821801662445, 0.0337674617767334, 0.03350982815027237, 0.0320979505777359, 0.036756109446287155, 0.035754233598709106, 0.03162195533514023, 0.0328328013420105, 0.03386654332280159, 0.036819133907556534, 0.03287301957607269, 0.03260647505521774, 0.030655181035399437, 0.033612996339797974, 0.033150073140859604, 0.029985954985022545, 0.03127392753958702, 0.02990991622209549, 0.031302616000175476, 0.030884716659784317, 0.03126098960638046, 0.030600393190979958, 0.028303993865847588, 0.030578283593058586, 0.02900904230773449, 0.028347229585051537, 0.02976238913834095, 0.029568303376436234, 0.02922932431101799, 0.030000176280736923, 0.029820753261446953, 0.028180623427033424, 0.03221384063363075, 0.030223840847611427, 0.030940907076001167, 0.02930077351629734, 0.029488401487469673, 0.02716955915093422, 0.02804945409297943, 0.02658660337328911, 0.03110683336853981, 0.02878798358142376, 0.026454128324985504, 0.028843307867646217, 0.02687281370162964, 0.026026006788015366, 0.0276638250797987, 0.028580855578184128, 0.027194010093808174, 0.02670394442975521, 0.02607591822743416, 0.025073856115341187, 0.026806415989995003, 0.02721201442182064, 0.02509460411965847]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:11 2016", "state": "available"}], "summary": "a03b76fc10aaff1354e503ec0aa1116b"}