{"content": {"hp_model": {"f0": 64, "f1": 32, "f2": 32, "f3": 32, "nonlin": "rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.04201109560049522, 0.045814994189275766, 0.0493260535819136, 0.049795666468876114, 0.05534069439048721, 0.06212936390271588, 0.060246724952052924, 0.05976344272624197, 0.05997851734358763, 0.0592921646991188, 0.05911352398365445, 0.0621870360280969, 0.060779002769549416, 0.061102098358165345, 0.06184192689089959, 0.06191283101988927, 0.060017160809579494, 0.06150097117075125, 0.060965343004878894, 0.0630067370147983, 0.06130578666857252, 0.06096066218464819, 0.060391309848879325, 0.05953471119565683, 0.060225407061683604, 0.060915883676774224, 0.060396625387425885, 0.06069267100076982, 0.06081860385279899, 0.06035852035038763, 0.059404255387239334, 0.06096446537846111, 0.06026951490750579, 0.061076407051793284, 0.06098464759071302, 0.06114411539286971, 0.06139038470872115, 0.06106881437014123, 0.06098113812056688, 0.06157110929515266, 0.06112398583448799, 0.06084733516012102, 0.06129007521838592, 0.06071294356304561, 0.06063445993018253, 0.060836196059769546, 0.06065710420686646, 0.06051934331017051, 0.060450345132679696, 0.06055381291637524, 0.060929057242308904, 0.06098464759071302, 0.0614870499683499, 0.0612149537423137, 0.06033871986726475, 0.0612149537423137, 0.06067591899270759, 0.06058031465203782, 0.06061474958586402, 0.060524942273636, 0.06017089369504567, 0.06019252718876569, 0.06026951490750579, 0.06039396767663309, 0.06079191262304698, 0.06043470639864113, 0.06055116210485577, 0.06098464759071302, 0.0605982703496223, 0.060779002769549416, 0.06065445790953706, 0.06096329519025074, 0.0607446609476405, 0.06100570017192072, 0.06091617645362578, 0.0608262277671834, 0.06101008521196345, 0.06101008521196345, 0.06096329519025074, 0.06101008521196345, 0.06101008521196345, 0.06101008521196345, 0.06101008521196345, 0.060988156858912075, 0.060826227767183404, 0.06063269508592709, 0.06069149557211249, 0.06079455293717548, 0.060806286279548274, 0.06088630596297442, 0.06088630596297442, 0.06088630596297442, 0.06088630596297442, 0.06088630596297442, 0.06088630596297442, 0.06082886659184211, 0.06082886659184211, 0.06082886659184211, 0.06082886659184211, 0.06082886659184211, 0.06082886659184211, 0.06082886659184211, 0.06082886659184211, 0.06082886659184211, 0.06082886659184211, 0.06082886659184211, 0.06082886659184211, 0.06082886659184211, 0.06082886659184211, 0.06082886659184211, 0.06082886659184211, 0.06082886659184211, 0.06082886659184211, 0.06082886659184211], "moving_avg_accuracy_train": [0.016055628765060236, 0.034941406249999994, 0.055235386859939756, 0.07614492422816264, 0.09692839867281625, 0.1173772191971009, 0.13714599200630645, 0.15598269175145893, 0.17405112061848171, 0.19123985268313956, 0.20724152630036777, 0.2222689738209334, 0.2362831344207678, 0.24934062670158258, 0.26137710243503876, 0.27260996824575173, 0.28302075229467055, 0.2927175475471312, 0.3016517415875988, 0.3099725425794413, 0.3177059923877622, 0.32478846167308234, 0.331266223186497, 0.33735976276543767, 0.34292160275395417, 0.34804962320145033, 0.3527730870861246, 0.35709950578715066, 0.36110152810000185, 0.36481865315144746, 0.3681875973242545, 0.3713514241882146, 0.37422945948023656, 0.37685734184546593, 0.3792671460645338, 0.3815018584159118, 0.3835154526948025, 0.38536769131086446, 0.3870747098303804, 0.3886980935160171, 0.39016855148369245, 0.39154138007026296, 0.39279104477408006, 0.39393927463402145, 0.3949868004838723, 0.3958966294716296, 0.3967366540244667, 0.3974832634714176, 0.39819286257608305, 0.39884091442088443, 0.39944063321975987, 0.39998508646404896, 0.40047980070921035, 0.400957987806964, 0.40138129670699046, 0.40182816327123116, 0.40220445838989116, 0.40255253664728763, 0.40288698554279984, 0.40319975536201386, 0.4034765418740053, 0.40373741554805054, 0.40397220185469124, 0.40417645004271613, 0.4043673328998903, 0.40454383379664827, 0.4046909187904774, 0.40482564844757424, 0.404949258301612, 0.4050699198208484, 0.40517616202551054, 0.40527413317235705, 0.40536466036716956, 0.4054579006557538, 0.40553946375282907, 0.40561993002814856, 0.4056735243747313, 0.40571940612400514, 0.40575834653570103, 0.4057886865809261, 0.40582540527223104, 0.4058584520944055, 0.40588819423436256, 0.40591731532297454, 0.4059482306280265, 0.4059760544025732, 0.4060010957996653, 0.4060236330570482, 0.40604391658869277, 0.4060645249298235, 0.40608542559949173, 0.4061065893648438, 0.40612563675366065, 0.4061451325662464, 0.406160325634923, 0.40617399939673193, 0.40618630578236, 0.4062020878547264, 0.4062162917198562, 0.406229075198473, 0.4062405803292281, 0.4062509349469077, 0.40626025410281935, 0.40626864134313984, 0.40627854302207883, 0.4062874545331239, 0.40629312173041393, 0.4063005753706255, 0.40630728364681595, 0.4063109679327368, 0.40631663695271614, 0.4063217390706975, 0.40632633097688076, 0.4063304636924457], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 549540011, "moving_var_accuracy_train": [0.002320048935372868, 0.005298097362731499, 0.008474898467427846, 0.011562287397263046, 0.01429363394746087, 0.016627658900224426, 0.018482132415639854, 0.01982731048967712, 0.020782792536213367, 0.021363585872507404, 0.02153170931222734, 0.021410955991854795, 0.021037430668530865, 0.02046817054364962, 0.01972524422202338, 0.018888315268714404, 0.017974943562461963, 0.017023699749729056, 0.016039708183130694, 0.015058858927130254, 0.0140912292478576, 0.013133558663669373, 0.012197855345324521, 0.011312250832193116, 0.010459432325494553, 0.009650158436334546, 0.008885942592329488, 0.008165809422085835, 0.007493374123210281, 0.006868389878722013, 0.0062836989544052275, 0.005745417262790745, 0.005245423320790783, 0.004783032880241056, 0.004356993999585086, 0.003966240053267188, 0.0036061071052203033, 0.0032763734857157514, 0.0029749613471779105, 0.0027011835837772408, 0.0024505254451118175, 0.0022224348255535827, 0.0020142462998459206, 0.0018246875561626777, 0.001652094594201362, 0.001494335233863898, 0.0013512524817218298, 0.0012211440645461342, 0.0011035614360955992, 0.000996985033227993, 0.0009005234936447148, 0.0008131390082971954, 0.0007340277871267662, 0.0006626829745182123, 0.0005980273908899659, 0.0005400218593370961, 0.00048729405555033266, 0.0004396550762547486, 0.00039669627320265775, 0.00035790707052069234, 0.0003228058604276063, 0.0002911377700491345, 0.000262520114532295, 0.00023664355897986845, 0.00021330712946834838, 0.00019225678962052082, 0.00017322581661715624, 0.0001560666036799535, 0.00014059745787609528, 0.00012666874490850565, 0.00011410345707211836, 0.00010277949647543636, 9.257530338489829e-05, 8.339601680914594e-05, 7.511628797747186e-05, 6.766293257289882e-05, 6.092249050147961e-05, 5.484918766557952e-05, 4.9377916099988964e-05, 4.444840915508841e-05, 4.001570260019993e-05, 3.6023961172282435e-05, 3.242952640905721e-05, 2.919420610836899e-05, 2.6283387302310187e-05, 2.3662016033949425e-05, 2.1301458074669388e-05, 1.9175883618935523e-05, 1.726199805194576e-05, 1.5539620580268625e-05, 1.3989590064174997e-05, 1.259466220243238e-05, 1.1338461209375796e-05, 1.0208035868813622e-05, 9.189309745954557e-06, 8.27206151721716e-06, 7.4462183896404734e-06, 6.703838214950034e-06, 6.0352701415166525e-06, 5.433213883294901e-06, 4.89108380726864e-06, 4.402940389507391e-06, 3.963427970558801e-06, 3.567718285704661e-06, 3.2118288463464997e-06, 2.8913606969738174e-06, 2.6025136814025516e-06, 2.342762324033935e-06, 2.1088911003555693e-06, 1.898124155984729e-06, 1.7086009804739907e-06, 1.5379751668976566e-06, 1.3843674206294537e-06, 1.2460843926079746e-06], "duration": 44948.898284, "accuracy_train": [0.1605562876506024, 0.20491340361445784, 0.2378812123493976, 0.2643307605421687, 0.2839796686746988, 0.30141660391566266, 0.3150649472891566, 0.3255129894578313, 0.33666698042168675, 0.34593844126506024, 0.3512565888554217, 0.3575160015060241, 0.3624105798192771, 0.3668580572289157, 0.36970538403614456, 0.3737057605421687, 0.37671780873493976, 0.3799887048192771, 0.3820594879518072, 0.3848597515060241, 0.3873070406626506, 0.38853068524096385, 0.38956607680722893, 0.3922016189759036, 0.3929781626506024, 0.3942018072289157, 0.3952842620481928, 0.39603727409638556, 0.39711972891566266, 0.3982727786144578, 0.39850809487951805, 0.39982586596385544, 0.40013177710843373, 0.4005082831325301, 0.40095538403614456, 0.40161426957831325, 0.4016378012048193, 0.4020378388554217, 0.4024378765060241, 0.403308546686747, 0.40340267319277107, 0.4038968373493976, 0.40403802710843373, 0.40427334337349397, 0.4044145331325301, 0.4040850903614458, 0.404296875, 0.4042027484939759, 0.4045792545180723, 0.4046733810240964, 0.4048381024096386, 0.4048851656626506, 0.40493222891566266, 0.405261671686747, 0.40519107680722893, 0.4058499623493976, 0.4055911144578313, 0.40568524096385544, 0.40589702560240964, 0.40601468373493976, 0.4059676204819277, 0.4060852786144578, 0.4060852786144578, 0.40601468373493976, 0.4060852786144578, 0.4061323418674699, 0.40601468373493976, 0.4060382153614458, 0.40606174698795183, 0.4061558734939759, 0.4061323418674699, 0.4061558734939759, 0.40617940512048195, 0.40629706325301207, 0.40627353162650603, 0.4063441265060241, 0.4061558734939759, 0.4061323418674699, 0.40610881024096385, 0.40606174698795183, 0.4061558734939759, 0.4061558734939759, 0.4061558734939759, 0.40617940512048195, 0.40622646837349397, 0.40622646837349397, 0.40622646837349397, 0.40622646837349397, 0.40622646837349397, 0.40625, 0.40627353162650603, 0.40629706325301207, 0.40629706325301207, 0.40632059487951805, 0.40629706325301207, 0.40629706325301207, 0.40629706325301207, 0.4063441265060241, 0.4063441265060241, 0.4063441265060241, 0.4063441265060241, 0.4063441265060241, 0.4063441265060241, 0.4063441265060241, 0.4063676581325301, 0.4063676581325301, 0.4063441265060241, 0.4063676581325301, 0.4063676581325301, 0.4063441265060241, 0.4063676581325301, 0.4063676581325301, 0.4063676581325301, 0.4063676581325301], "end": "2016-01-21 16:01:51.123000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0], "accuracy_valid": [0.15411324786324787, 0.20058760683760685, 0.234375, 0.26161858974358976, 0.28205128205128205, 0.2998130341880342, 0.31356837606837606, 0.3230502136752137, 0.3333333333333333, 0.3421474358974359, 0.34922542735042733, 0.3544337606837607, 0.3612446581196581, 0.36685363247863245, 0.37086004273504275, 0.374866452991453, 0.3784722222222222, 0.3839476495726496, 0.38488247863247865, 0.3875534188034188, 0.3892895299145299, 0.391025641025641, 0.39276175213675213, 0.39329594017094016, 0.3940972222222222, 0.39463141025641024, 0.3951655982905983, 0.3955662393162393, 0.3973023504273504, 0.39676816239316237, 0.39797008547008544, 0.39890491452991456, 0.3994391025641026, 0.40024038461538464, 0.4009081196581197, 0.4011752136752137, 0.4017094017094017, 0.40197649572649574, 0.4017094017094017, 0.4026442307692308, 0.4031784188034188, 0.40224358974358976, 0.4027777777777778, 0.4031784188034188, 0.40371260683760685, 0.4029113247863248, 0.40384615384615385, 0.40344551282051283, 0.40384615384615385, 0.40384615384615385, 0.4042467948717949, 0.4043803418803419, 0.40411324786324787, 0.40411324786324787, 0.4046474358974359, 0.40411324786324787, 0.4043803418803419, 0.4046474358974359, 0.4046474358974359, 0.4045138888888889, 0.4042467948717949, 0.4043803418803419, 0.4046474358974359, 0.4047809829059829, 0.4042467948717949, 0.4043803418803419, 0.4042467948717949, 0.4043803418803419, 0.40411324786324787, 0.4043803418803419, 0.4042467948717949, 0.4042467948717949, 0.4043803418803419, 0.4045138888888889, 0.4043803418803419, 0.4042467948717949, 0.40411324786324787, 0.40411324786324787, 0.4042467948717949, 0.40411324786324787, 0.40411324786324787, 0.40411324786324787, 0.40411324786324787, 0.40397970085470086, 0.4042467948717949, 0.40411324786324787, 0.40384615384615385, 0.40384615384615385, 0.40371260683760685, 0.40357905982905984, 0.40357905982905984, 0.40357905982905984, 0.40357905982905984, 0.40357905982905984, 0.40357905982905984, 0.40344551282051283, 0.40344551282051283, 0.40344551282051283, 0.40344551282051283, 0.40344551282051283, 0.40344551282051283, 0.40344551282051283, 0.40344551282051283, 0.40344551282051283, 0.40344551282051283, 0.40344551282051283, 0.40344551282051283, 0.40344551282051283, 0.40344551282051283, 0.40344551282051283, 0.40344551282051283, 0.40344551282051283, 0.40344551282051283, 0.40344551282051283], "accuracy_test": 0.4026442307692308, "start": "2016-01-21 03:32:42.225000", "learning_rate_per_epoch": [0.0012953297700732946, 0.001186720677651465, 0.0010872180573642254, 0.000996058457531035, 0.0009125422220677137, 0.0008360286010429263, 0.0007659303955733776, 0.0007017096504569054, 0.0006428736378438771, 0.0005889707827009261, 0.0005395875195972621, 0.0004943449166603386, 0.00045289573608897626, 0.00041492193122394383, 0.00038013208541087806, 0.0003482592583168298, 0.00031905886135064065, 0.00029230682412162423, 0.0002677978773135692, 0.00024534392287023365, 0.00022477265156339854, 0.00020592620421666652, 0.0001886599784484133, 0.0001728414645185694, 0.000158349284902215, 0.0001450722193112597, 0.00013290838978718966, 0.00012176446034573019, 0.00011155490938108414, 0.00010220139665761963, 9.363214485347271e-05, 8.578140113968402e-05, 7.858891331125051e-05, 7.199949322966859e-05, 6.596257298951969e-05, 6.04318302066531e-05, 5.53648205823265e-05, 5.072266503702849e-05, 4.6469736844301224e-05, 4.2573403334245086e-05, 3.900376395904459e-05, 3.5733428376261145e-05, 3.273729817010462e-05, 2.999238313350361e-05, 2.7477621188154444e-05, 2.517371285648551e-05, 2.306297938048374e-05, 2.1129224478499964e-05, 1.9357608834980056e-05, 1.7734537323121913e-05, 1.6247555322479457e-05, 1.4885251403029542e-05, 1.3637171832669992e-05, 1.2493739632191136e-05, 1.1446180906204972e-05, 1.0486456631042529e-05, 9.607201718608849e-06, 8.80166953720618e-06, 8.063678251346573e-06, 7.387565347016789e-06, 6.768142611690564e-06, 6.200656116561731e-06, 5.680751655745553e-06, 5.204439275985351e-06, 4.768064172822051e-06, 4.368277586763725e-06, 4.0020117921812925e-06, 3.6664562230726006e-06, 3.3590360999369295e-06, 3.077391966144205e-06, 2.8193628622830147e-06, 2.5829685910139233e-06, 2.3663951651542448e-06, 2.1679807105101645e-06, 1.9862027329509147e-06, 1.8196661812908133e-06, 1.6670932154738693e-06, 1.527312974758388e-06, 1.3992528238304658e-06, 1.281930053664837e-06, 1.174444491880422e-06, 1.0759712267827126e-06, 9.857545819613733e-07, 9.031023182615172e-07, 8.273801768154954e-07, 7.580071041957126e-07, 6.944507049411186e-07, 6.362233193613065e-07, 5.828781013406115e-07, 5.340057214198168e-07, 4.89231126721279e-07, 4.482107271996938e-07, 4.106297524231195e-07, 3.761998073059658e-07, 3.446566836373677e-07, 3.157583705615252e-07, 2.892830650580436e-07, 2.6502763716962363e-07, 2.428059531212057e-07, 2.2244748265620728e-07, 2.0379599163788953e-07, 1.8670837675927032e-07, 1.7105350025303778e-07, 1.567112235534296e-07, 1.4357151201238594e-07, 1.3153351119399304e-07, 1.2050486475345679e-07, 1.1040093284009345e-07, 1.0114418103057687e-07, 9.266357636761313e-08, 8.489404024203395e-08, 7.777595811830906e-08, 7.125470347091323e-08, 6.528023277496686e-08], "accuracy_train_last": 0.4063676581325301, "error_valid": [0.8458867521367521, 0.7994123931623931, 0.765625, 0.7383814102564102, 0.717948717948718, 0.7001869658119658, 0.6864316239316239, 0.6769497863247863, 0.6666666666666667, 0.6578525641025641, 0.6507745726495726, 0.6455662393162394, 0.6387553418803419, 0.6331463675213675, 0.6291399572649572, 0.6251335470085471, 0.6215277777777778, 0.6160523504273504, 0.6151175213675213, 0.6124465811965811, 0.6107104700854701, 0.608974358974359, 0.6072382478632479, 0.6067040598290598, 0.6059027777777778, 0.6053685897435898, 0.6048344017094017, 0.6044337606837606, 0.6026976495726496, 0.6032318376068376, 0.6020299145299146, 0.6010950854700854, 0.6005608974358974, 0.5997596153846154, 0.5990918803418803, 0.5988247863247863, 0.5982905982905983, 0.5980235042735043, 0.5982905982905983, 0.5973557692307692, 0.5968215811965811, 0.5977564102564102, 0.5972222222222222, 0.5968215811965811, 0.5962873931623931, 0.5970886752136753, 0.5961538461538461, 0.5965544871794872, 0.5961538461538461, 0.5961538461538461, 0.5957532051282051, 0.5956196581196581, 0.5958867521367521, 0.5958867521367521, 0.5953525641025641, 0.5958867521367521, 0.5956196581196581, 0.5953525641025641, 0.5953525641025641, 0.5954861111111112, 0.5957532051282051, 0.5956196581196581, 0.5953525641025641, 0.595219017094017, 0.5957532051282051, 0.5956196581196581, 0.5957532051282051, 0.5956196581196581, 0.5958867521367521, 0.5956196581196581, 0.5957532051282051, 0.5957532051282051, 0.5956196581196581, 0.5954861111111112, 0.5956196581196581, 0.5957532051282051, 0.5958867521367521, 0.5958867521367521, 0.5957532051282051, 0.5958867521367521, 0.5958867521367521, 0.5958867521367521, 0.5958867521367521, 0.5960202991452992, 0.5957532051282051, 0.5958867521367521, 0.5961538461538461, 0.5961538461538461, 0.5962873931623931, 0.5964209401709402, 0.5964209401709402, 0.5964209401709402, 0.5964209401709402, 0.5964209401709402, 0.5964209401709402, 0.5965544871794872, 0.5965544871794872, 0.5965544871794872, 0.5965544871794872, 0.5965544871794872, 0.5965544871794872, 0.5965544871794872, 0.5965544871794872, 0.5965544871794872, 0.5965544871794872, 0.5965544871794872, 0.5965544871794872, 0.5965544871794872, 0.5965544871794872, 0.5965544871794872, 0.5965544871794872, 0.5965544871794872, 0.5965544871794872, 0.5965544871794872], "accuracy_train_std": [0.04496355681077054, 0.050679060484812063, 0.05297156247196673, 0.05652713520131425, 0.0570686141702153, 0.0574192083675705, 0.05899899685528094, 0.05852764932772978, 0.05872241367743268, 0.058695551616890086, 0.06008777213701382, 0.059848208258816406, 0.05970777765440081, 0.0603684696060871, 0.06008474938472413, 0.05985694185379624, 0.059519773358208565, 0.060309387322589524, 0.06001035373111535, 0.05995562434030807, 0.06034213383618006, 0.060403072596401645, 0.06049090550116628, 0.06077073867448933, 0.060675978390978255, 0.060597954488785244, 0.06092397635588682, 0.060467745870105144, 0.060579968451934314, 0.06095706492960444, 0.061623138866602895, 0.06152392689648199, 0.06151028533964688, 0.061903976778784335, 0.062419816111063335, 0.062289486607479856, 0.0628465559530934, 0.062449226016668614, 0.06253668652081609, 0.06233349306343087, 0.06256747082102555, 0.062479229179408596, 0.06231350650315824, 0.06239217182718372, 0.062437720020901956, 0.06277955707064053, 0.06283625952230092, 0.06279814319962637, 0.06273314047866761, 0.062823427376347, 0.06287122175417785, 0.06293071460092757, 0.06290249816723917, 0.06295527706302284, 0.06323094292119515, 0.06308719350528744, 0.06305295699673474, 0.06305970128972357, 0.06306415757192729, 0.06296843409614422, 0.06315481735053041, 0.06307076019341837, 0.06309990180671642, 0.0632073855572377, 0.06305326876055474, 0.06315242804256047, 0.06312589407196477, 0.06297143713776977, 0.0630269566166069, 0.06310878644329522, 0.06311748559393743, 0.06312043768069157, 0.06321069261409058, 0.06314378632268319, 0.06321072765482624, 0.06322520219747586, 0.06345159858282472, 0.06337328281024016, 0.06343412486967928, 0.06344559417319852, 0.06338782506210855, 0.06339942501916798, 0.06348056536734287, 0.06346032054654717, 0.06347194215105371, 0.0634603554494464, 0.06343137943580067, 0.06333276182630276, 0.06349510921118226, 0.06352116641289594, 0.06352984396100657, 0.06357322264491437, 0.06349799141856206, 0.06359344023106252, 0.06359635281030751, 0.06359635281030751, 0.06359635281030751, 0.06356160213488861, 0.06356160213488861, 0.06356160213488861, 0.06356160213488861, 0.06356160213488861, 0.06356160213488861, 0.06356160213488861, 0.0635702393208436, 0.0635702393208436, 0.06356160213488861, 0.0635702393208436, 0.0635702393208436, 0.06356160213488861, 0.0635702393208436, 0.0635702393208436, 0.0635702393208436, 0.0635702393208436], "accuracy_test_std": 0.06292041478346808, "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-3, 3], "momentum": 0.6077586172231769, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.0014138788848813513, "patience_threshold": 1, "do_flip": true, "batch_size": 64, "optimization": "adadelta", "nb_data_augmentation": 3, "learning_rate_decay_method": "exp", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 1.3719084776303552e-05, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.08384669396066002}, "accuracy_valid_max": 0.4047809829059829, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import os\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-6, -3], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128, 256],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-3, 3)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_optimizer.learning_rate = learning_rate\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.40344551282051283, "loss_train": [42.706642150878906, 3.61183762550354, 2.8325631618499756, 2.530238628387451, 2.3774960041046143, 2.2818520069122314, 2.2195136547088623, 2.16922664642334, 2.1364917755126953, 2.1058285236358643, 2.0816917419433594, 2.0662970542907715, 2.046738624572754, 2.0323407649993896, 2.021238088607788, 2.0114195346832275, 1.999718427658081, 1.9920905828475952, 1.9852495193481445, 1.9787533283233643, 1.9726959466934204, 1.966431975364685, 1.963810682296753, 1.9606059789657593, 1.9572715759277344, 1.9548918008804321, 1.9480557441711426, 1.9459083080291748, 1.9448899030685425, 1.9422743320465088, 1.9377353191375732, 1.9381868839263916, 1.9387449026107788, 1.9356639385223389, 1.9330805540084839, 1.9322060346603394, 1.9306375980377197, 1.9290860891342163, 1.9307055473327637, 1.9296423196792603, 1.93135404586792, 1.9273327589035034, 1.9275981187820435, 1.9260485172271729, 1.9237600564956665, 1.9259545803070068, 1.9237635135650635, 1.9228662252426147, 1.9238345623016357, 1.925026774406433, 1.9221683740615845, 1.9228665828704834, 1.9232460260391235, 1.9222891330718994, 1.921775460243225, 1.9232953786849976, 1.9212688207626343, 1.9195866584777832, 1.9219307899475098, 1.9214876890182495, 1.921707272529602, 1.9194989204406738, 1.9202603101730347, 1.917562484741211, 1.9210188388824463, 1.9192065000534058, 1.9207655191421509, 1.9209219217300415, 1.9193238019943237, 1.9201979637145996, 1.9197325706481934, 1.9190789461135864, 1.9189579486846924, 1.9190136194229126, 1.9193472862243652, 1.9199093580245972, 1.9206722974777222, 1.9204784631729126, 1.9203349351882935, 1.921459436416626, 1.9215078353881836, 1.918905258178711, 1.9185022115707397, 1.9195393323898315, 1.919500470161438, 1.9191370010375977, 1.9208483695983887, 1.919352412223816, 1.919881820678711, 1.9195278882980347, 1.9199109077453613, 1.9200905561447144, 1.9206444025039673, 1.9188294410705566, 1.921043872833252, 1.9196736812591553, 1.9185150861740112, 1.9194365739822388, 1.9192143678665161, 1.9198179244995117, 1.918406367301941, 1.9177947044372559, 1.9210518598556519, 1.9182813167572021, 1.9201656579971313, 1.919594407081604, 1.9191040992736816, 1.9187735319137573, 1.9195971488952637, 1.9190077781677246, 1.9199756383895874, 1.9193642139434814, 1.9190831184387207, 1.919176459312439], "accuracy_train_first": 0.1605562876506024, "model": "residualv2", "loss_std": [82.77375793457031, 0.4441118836402893, 0.2091032713651657, 0.1561349332332611, 0.14097042381763458, 0.13676396012306213, 0.1303490549325943, 0.1274777501821518, 0.1276041567325592, 0.12589232623577118, 0.1259179413318634, 0.12558913230895996, 0.12489711493253708, 0.1226562038064003, 0.12294799089431763, 0.12325599789619446, 0.12410196661949158, 0.12196000665426254, 0.12338794767856598, 0.12257601320743561, 0.12462414056062698, 0.12415947765111923, 0.12276984006166458, 0.12431137263774872, 0.12407708168029785, 0.12528933584690094, 0.12314961105585098, 0.1226031482219696, 0.1252104789018631, 0.12348422408103943, 0.1238732784986496, 0.12294276058673859, 0.12286383658647537, 0.12428996711969376, 0.12436727434396744, 0.12431193888187408, 0.1231457069516182, 0.12138866633176804, 0.12378843128681183, 0.12659449875354767, 0.1239575520157814, 0.12323053926229477, 0.12372450530529022, 0.12271147221326828, 0.12313514947891235, 0.12377998232841492, 0.12463732063770294, 0.12307669967412949, 0.1245085746049881, 0.125339537858963, 0.12229836732149124, 0.12268271297216415, 0.12332840263843536, 0.12459494173526764, 0.12376812100410461, 0.12264630943536758, 0.12237424403429031, 0.12455689162015915, 0.12318049371242523, 0.12287665903568268, 0.12372169643640518, 0.12398511916399002, 0.12215530872344971, 0.12400591373443604, 0.12278404086828232, 0.12253796309232712, 0.12297476828098297, 0.12196455895900726, 0.12151980400085449, 0.12452409416437149, 0.12502117455005646, 0.12165174633264542, 0.12210724502801895, 0.12351076304912567, 0.12339518964290619, 0.12207107245922089, 0.12510769069194794, 0.1222134530544281, 0.12264776229858398, 0.12212154269218445, 0.12227002531290054, 0.1226809024810791, 0.12352970987558365, 0.12124428898096085, 0.12154684960842133, 0.12359253317117691, 0.12386076152324677, 0.12111832201480865, 0.12324507534503937, 0.12354408949613571, 0.12241069227457047, 0.12323372066020966, 0.12196427583694458, 0.12524333596229553, 0.11970427632331848, 0.12277276068925858, 0.12615810334682465, 0.12289317697286606, 0.12552492320537567, 0.12244757264852524, 0.12345871329307556, 0.12335821241140366, 0.1239335834980011, 0.12347492575645447, 0.12396922707557678, 0.122463658452034, 0.12222960591316223, 0.12290621548891068, 0.12496115267276764, 0.12383626401424408, 0.12329858541488647, 0.12332374602556229, 0.12361293286085129, 0.12463341653347015]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:06 2016", "state": "available"}], "summary": "d53d36dc8ca4398ac8af757cdc52fda2"}