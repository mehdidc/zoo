{"content": {"hp_model": {"f0": 64, "f1": 16, "f2": 64, "f3": 32, "nonlin": "leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.7600295543670654, 1.3258082866668701, 1.1475212574005127, 1.0383657217025757, 0.9633487462997437, 0.9067733883857727, 0.8599340319633484, 0.8226760029792786, 0.7901375889778137, 0.7587930560112, 0.7336959838867188, 0.7111085653305054, 0.6861759424209595, 0.6658900380134583, 0.6478287577629089, 0.6300969123840332, 0.6129592061042786, 0.5979431867599487, 0.5823489427566528, 0.5678878426551819, 0.5559509992599487, 0.5431268215179443, 0.5308307409286499, 0.5190083384513855, 0.5057975053787231, 0.4964568018913269, 0.4841764271259308, 0.4753841459751129, 0.4666251540184021, 0.45494580268859863, 0.4463815987110138, 0.4387751817703247, 0.4304746389389038, 0.4227243959903717, 0.413921594619751, 0.40615227818489075, 0.4008334279060364, 0.39083901047706604, 0.38435325026512146, 0.3781833052635193, 0.3702133297920227, 0.3660446107387543, 0.3577640950679779, 0.3515198528766632, 0.3484581708908081, 0.34299010038375854, 0.33557483553886414, 0.32818639278411865, 0.3249402344226837, 0.31993135809898376, 0.31389790773391724, 0.30943259596824646, 0.3043980002403259, 0.30166110396385193, 0.29718104004859924, 0.2930354177951813, 0.2864133417606354, 0.2831312119960785, 0.2783859670162201, 0.27383583784103394, 0.27257221937179565, 0.2663438320159912, 0.2631644308567047, 0.26120805740356445, 0.25682440400123596, 0.2539175748825073, 0.25055453181266785, 0.2459486424922943, 0.24459891021251678, 0.23887565732002258, 0.237755686044693, 0.23448090255260468, 0.2318013310432434, 0.22864486277103424, 0.2241963893175125, 0.22552689909934998, 0.22030702233314514, 0.21795178949832916, 0.2161332070827484, 0.21236459910869598, 0.21152207255363464, 0.21055109798908234, 0.2060670405626297, 0.20435810089111328, 0.20146450400352478, 0.20102748274803162, 0.19770275056362152, 0.19563329219818115, 0.19512443244457245, 0.19219118356704712, 0.19088976085186005, 0.18901397287845612, 0.18746331334114075, 0.18551421165466309, 0.1829693466424942, 0.1814771592617035, 0.18046344816684723, 0.17896564304828644, 0.1773069053888321, 0.1747739017009735, 0.1748490035533905, 0.1736437827348709, 0.17135101556777954, 0.17000128328800201, 0.16976241767406464, 0.16675792634487152, 0.16561128199100494, 0.16497378051280975, 0.163892924785614, 0.16315005719661713, 0.16150347888469696, 0.1597612053155899, 0.15847264230251312, 0.1587478518486023, 0.15716344118118286, 0.15588581562042236, 0.15450896322727203, 0.15340884029865265, 0.15222974121570587, 0.15193776786327362, 0.15033277869224548, 0.14999566972255707, 0.14855724573135376, 0.1481029987335205, 0.14755655825138092, 0.14606569707393646, 0.1454572081565857, 0.14479325711727142, 0.1431858390569687, 0.1429058164358139, 0.14217530190944672, 0.14160381257534027, 0.14051556587219238, 0.140382781624794, 0.1395552009344101, 0.13913467526435852, 0.13791832327842712, 0.1374552696943283, 0.13688595592975616, 0.13599561154842377, 0.13557416200637817, 0.13509045541286469, 0.13474339246749878, 0.13350534439086914, 0.1330716609954834, 0.13299661874771118, 0.13168485462665558, 0.1319490671157837, 0.13063880801200867, 0.13037608563899994, 0.13009679317474365, 0.1297912895679474, 0.12895308434963226, 0.12851209938526154, 0.12791629135608673, 0.12758508324623108, 0.1267692893743515, 0.12675470113754272, 0.12606297433376312, 0.1258118748664856, 0.12567871809005737, 0.12504984438419342, 0.12490367889404297, 0.12396727502346039, 0.12397163361310959, 0.12353384494781494, 0.12311072647571564, 0.12319490313529968, 0.12262184917926788, 0.12208184599876404, 0.12166900932788849, 0.12130830436944962, 0.1210605800151825, 0.12078991532325745, 0.12020682543516159, 0.12006793916225433, 0.12000489979982376, 0.11952821910381317, 0.1193084642291069, 0.118949294090271, 0.11837785691022873], "moving_avg_accuracy_train": [0.059211752924280166, 0.1236492922174926, 0.1867713104394495, 0.24613127435097704, 0.3021495672104991, 0.3541441579469981, 0.40199461877882103, 0.4459645885120039, 0.4867232429694506, 0.5239059029263815, 0.5581581257970416, 0.5896127002662946, 0.6182498795636777, 0.6446673350539232, 0.6683825911260963, 0.6913048452909876, 0.7118743480727046, 0.7300638490870011, 0.747104151003467, 0.7636515008186796, 0.7779653698917119, 0.7910848009431, 0.8034200255714736, 0.814870716351351, 0.8252831867520224, 0.835349521460218, 0.8444489665202686, 0.8531636262658977, 0.8602769035548672, 0.867274055161359, 0.8732319838321925, 0.8783638217573435, 0.8841217627578274, 0.8892062534082629, 0.894240349309131, 0.8991617687663684, 0.9037583488457117, 0.9071140930147581, 0.9104736984442716, 0.9144667861891301, 0.9174305579785782, 0.9198002974926436, 0.923111913501731, 0.9248555689873369, 0.9277569528993267, 0.9303845105106029, 0.9327609741536179, 0.9351137051228076, 0.9373218437986497, 0.9389672635854514, 0.9408226345471813, 0.9426877088151007, 0.944294304189589, 0.9455751544611525, 0.9471301344007884, 0.9487713597250137, 0.9505159267275493, 0.9514815704369926, 0.9524597875742641, 0.9534518622383032, 0.9546097243025866, 0.9564214965140315, 0.9563339507293689, 0.9568525425231263, 0.9581562926601178, 0.9598412365703243, 0.961088004876424, 0.9624936563602194, 0.9636030658718534, 0.964550309060877, 0.9656494379000828, 0.9665176740196353, 0.9675200117129561, 0.9683686372143256, 0.9692601752036258, 0.97042764380591, 0.9713528796098889, 0.9720228314168032, 0.9728721096215607, 0.9735342616046796, 0.9741627865216387, 0.9742889697730832, 0.9752023137922219, 0.9759894822261226, 0.9766142645083091, 0.977413661643211, 0.9778378612146319, 0.9785753885967677, 0.9791089909561755, 0.9795054556248621, 0.980145941981442, 0.9805991468154591, 0.9814254858541513, 0.9822552935925641, 0.9828068080571357, 0.9829985765812026, 0.9835059896814342, 0.9840603898192801, 0.9845732647873797, 0.9849836629360411, 0.9853925848484171, 0.9859512407231177, 0.9864749213008152, 0.9868439272731238, 0.9871202651255918, 0.9875457165511555, 0.9879100216436867, 0.9881705029603073, 0.9887094585416759, 0.9889597506327833, 0.9891733156730949, 0.9894771313522325, 0.9898110193325039, 0.9900859058290247, 0.990426345677093, 0.9906396995391549, 0.9909340606114484, 0.9912082861717506, 0.9913388317355464, 0.9916050601691345, 0.9918028130807925, 0.9920505451655705, 0.9922386628585464, 0.9923963069893584, 0.9926033269225747, 0.9928454844827167, 0.9929681672832915, 0.9930900994014001, 0.9933184569458023, 0.9934891375524403, 0.9935962471222239, 0.9937367514647727, 0.9939259843909237, 0.9940545134435257, 0.9942027416742009, 0.9943988900508469, 0.9945823990362568, 0.9946359499802686, 0.9947399133524891, 0.9948939703053539, 0.9949698785938937, 0.9950590142464092, 0.9952182913931968, 0.995331558086057, 0.995451991153651, 0.995565139358561, 0.9956553109501135, 0.995722478440835, 0.995813156117008, 0.9958482270005545, 0.9958751765469461, 0.9959645713541839, 0.9960008128044983, 0.9960357552585907, 0.9961485476267884, 0.9962244841212616, 0.9963137533055733, 0.9963917704226442, 0.9964317228446654, 0.9965119299495031, 0.9965608648557618, 0.9965212369630705, 0.9965901314584393, 0.9966614370995094, 0.996711697332434, 0.9967754606349049, 0.9968445454488138, 0.9968950599884655, 0.9969173436836942, 0.9969117502748578, 0.9969602306783428, 0.9969852258021844, 0.9970449237945943, 0.9970777256484774, 0.9971141867145821, 0.9971586634669426, 0.9972057400881331, 0.9972225324102999, 0.9971888534240687, 0.9972189241078708, 0.9972645528649501], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.059757977221385525, 0.12304869870105418, 0.18434193218185238, 0.24195548463384786, 0.29604568659403235, 0.34537590003176766, 0.3907568611863469, 0.4320035877653779, 0.46974614126293646, 0.5046615873776067, 0.5363509550423309, 0.5652814245324502, 0.5921184811567655, 0.6164019914277606, 0.6377465143764756, 0.6584437838254997, 0.6766949673781907, 0.6926968750078415, 0.7075115719197079, 0.7220054966177973, 0.7340215496950989, 0.7451066111694896, 0.755340543661351, 0.765433077680005, 0.7741379403280437, 0.7826000228483267, 0.7898974847954218, 0.7970836411069187, 0.8021778172353081, 0.8070840765979972, 0.8116381054028059, 0.8154611171516819, 0.8200095790434415, 0.8238286100792329, 0.8278740305566259, 0.8317336060401199, 0.8351695733728549, 0.8373565646424971, 0.8393960399553557, 0.8426589079571846, 0.8447480564592824, 0.8459538148664415, 0.8486198815160926, 0.8495005220824803, 0.8516633746182082, 0.8532447604715229, 0.8545774109774579, 0.8558653341775284, 0.8576256981148208, 0.8584134801011549, 0.8597704860537654, 0.8609795843798648, 0.8618124547257637, 0.8623433409832326, 0.8631894085697738, 0.8642581041962302, 0.8653653851263812, 0.8660832352620864, 0.8664057405197633, 0.867066324223811, 0.8675754003387041, 0.8684475783959481, 0.8682773016839889, 0.8691516493582105, 0.8699537107495732, 0.8707711632344803, 0.871719537545445, 0.8726310210555842, 0.8735866630671192, 0.8743318771256181, 0.8753778992210381, 0.8758167718083469, 0.8765769385657652, 0.8773130052984206, 0.8779051646236238, 0.8788959452245144, 0.8798476534129064, 0.8801273717877302, 0.8807006191722101, 0.8813141980682421, 0.8813882858386016, 0.8815943897189734, 0.8824247968502387, 0.8832098138707871, 0.8836314494358921, 0.8844331670676041, 0.8846898162399852, 0.8852228877503994, 0.8856172028910221, 0.8855824910262422, 0.885939816330621, 0.8857201816949234, 0.8861715421963647, 0.8868483803524813, 0.8872653107189651, 0.8868500324708637, 0.887614771552467, 0.8881869963927023, 0.8889858489936429, 0.8894342026296701, 0.8896382903674561, 0.8905107106247617, 0.8910008610890175, 0.8909750407933688, 0.8911104939335349, 0.8912232832544134, 0.8916432059643636, 0.8916599259364514, 0.8921083970570381, 0.8925974702843162, 0.8924374326402972, 0.8927460884255897, 0.8929373999049433, 0.8928176409950213, 0.893479930453501, 0.8933160665026539, 0.894018080663759, 0.8945369416015246, 0.8946000549056041, 0.8949915942663238, 0.8951791112327637, 0.8953011073948789, 0.8955004711941711, 0.8955648877889859, 0.8960552563613675, 0.8965677712466915, 0.8966841492425043, 0.8968255105324858, 0.8968805230146288, 0.8969920989134672, 0.8974729942084909, 0.8974632583230334, 0.8977637898420101, 0.897699530822041, 0.8976172836415689, 0.8977803423161921, 0.8977256055713951, 0.8973630777232166, 0.8973510969463768, 0.897108380653471, 0.8972714424845847, 0.8973836360561563, 0.8976147695796823, 0.8977007194383557, 0.8980445699813424, 0.8981078358277111, 0.8980813848880124, 0.8977625512749642, 0.8975864938131304, 0.8973781844638203, 0.8971815875441702, 0.8974563104727351, 0.8975204556396935, 0.8973096316024561, 0.8975715501251924, 0.8976374078668147, 0.8976102011068651, 0.8979570735037086, 0.898119685759889, 0.8980320441793821, 0.8982084849045161, 0.8983174239234771, 0.8985671009588403, 0.8986087048219171, 0.8986542372952977, 0.898189580696792, 0.898243493014914, 0.8983906998598834, 0.8984845059092866, 0.8985455467999092, 0.8987459384678098, 0.8989130544290107, 0.8991255234590012, 0.8993421891571522, 0.8993021661657593, 0.8992305538884152, 0.8993502378162153, 0.8995810531723949, 0.899604652015547, 0.8996858966219742, 0.8997712237990086], "moving_var_accuracy_train": [0.03155428515929402, 0.06576862487484328, 0.09505126504707648, 0.1172585863825695, 0.1337751699583692, 0.1447285901552365, 0.15086273055607322, 0.15317668164549905, 0.152810424699583, 0.1499723340428795, 0.1455340335828239, 0.13988514251991757, 0.13327742061092004, 0.12623061614103992, 0.11866927486205466, 0.11153121499984807, 0.10418603350204685, 0.09674515167618401, 0.08968398351320445, 0.08317991823504713, 0.07670590804210163, 0.07058439247790059, 0.06489537312980247, 0.05958590069084954, 0.054603086480368324, 0.050054757682448325, 0.045794481017811396, 0.041898540566369054, 0.03816407493384007, 0.034788308615894145, 0.031628949980725386, 0.028703076827063013, 0.026131154105442186, 0.023750707101467254, 0.02160371548517276, 0.019661327261922167, 0.017885351471562286, 0.01619816549475886, 0.01467993148306112, 0.013355441082398266, 0.01209895246313779, 0.010939598205104718, 0.009944339589919044, 0.008977268640999488, 0.00815530403434232, 0.00740191016191327, 0.0067125473607410925, 0.006091110711787443, 0.005525882528314449, 0.004997660931956191, 0.0045288764514112454, 0.004107295324493816, 0.0037197961303203843, 0.003362581714051823, 0.003048085206160673, 0.0027675192706285126, 0.002518158969802684, 0.0022747352827847027, 0.002055873933415099, 0.001859144449324843, 0.001685295805431518, 0.0015463088918038427, 0.0013917469810031682, 0.0012549927199398245, 0.0011447913277231913, 0.001055863518775746, 0.0009642670477800247, 0.000885623047847088, 0.0008081378482429156, 0.0007353994903509884, 0.0006727322991624538, 0.0006122435748798678, 0.0005600613450549443, 0.0005105366977236228, 0.0004666365878285493, 0.0004322397754815681, 0.00039672034957009274, 0.0003610878334253744, 0.0003314705113045217, 0.00030226946741280467, 0.00027559791281267065, 0.0002481814214479095, 0.00023087105497878658, 0.00021336065677087567, 0.00019553776719499638, 0.00018173531248910011, 0.00016518128872773302, 0.00015355867960956227, 0.00014076539495029577, 0.00012810351355691739, 0.00011898516715791014, 0.00010893520203630747, 0.00010418720769447715, 9.996571486959807e-05, 9.270665722432321e-05, 8.376696800329568e-05, 7.770748369154528e-05, 7.270297093798249e-05, 6.780004044031347e-05, 6.253587616010478e-05, 5.77872427178852e-05, 5.481738592313373e-05, 5.1803819457938265e-05, 4.7848926180539645e-05, 4.3751297040845244e-05, 4.1005247576388784e-05, 3.809918662274729e-05, 3.489992260724832e-05, 3.4024188414719615e-05, 3.118558475108598e-05, 2.847751651396688e-05, 2.6460500564578422e-05, 2.4817781158447975e-05, 2.3016066316328213e-05, 2.175755329607078e-05, 1.9991476800574008e-05, 1.8772165088452317e-05, 1.7571745500914522e-05, 1.5967950248863894e-05, 1.5009053433635073e-05, 1.3860104016894428e-05, 1.3026434287660555e-05, 1.2042285256590006e-05, 1.10617217787463e-05, 1.0341264875611397e-05, 9.834900943455327e-06, 8.986870475121607e-06, 8.2219904004476e-06, 7.869115873171715e-06, 7.3443911111951665e-06, 6.713204139528814e-06, 6.219556958051418e-06, 5.919883165303322e-06, 5.476572305037849e-06, 5.1266595498557405e-06, 4.960261265817861e-06, 4.767315068771803e-06, 4.3163928943356425e-06, 3.982029049773124e-06, 3.7974280473296746e-06, 3.4695438570182435e-06, 3.1940959522605183e-06, 3.1030092424338e-06, 2.9081724115935748e-06, 2.7478922843651372e-06, 2.588325702397631e-06, 2.402671375465939e-06, 2.2030074842076242e-06, 2.0567087043921694e-06, 1.8621075358075543e-06, 1.682433284683203e-06, 1.586112840264704e-06, 1.4393225407262259e-06, 1.3063790625356287e-06, 1.2902402211950406e-06, 1.2131133598115104e-06, 1.1635229092393433e-06, 1.101950653319974e-06, 1.0061213522162864e-06, 9.634078339925751e-07, 8.886186760481779e-07, 8.138901373557779e-07, 7.752191870493481e-07, 7.43457718380128e-07, 6.918467656648848e-07, 6.592539177763453e-07, 6.362829296141848e-07, 5.956201050987369e-07, 5.405271622462955e-07, 4.867560220233636e-07, 4.592335655196987e-07, 4.1893301491044605e-07, 4.0911436609928e-07, 3.778865840528882e-07, 3.520626097209827e-07, 3.346599822537566e-07, 3.211398583928644e-07, 2.9156371130734756e-07, 2.726158071986443e-07, 2.534924406976915e-07, 2.468810478813281e-07], "duration": 169794.71331, "accuracy_train": [0.5921175292428018, 0.7035871458564046, 0.7548694744370616, 0.780370949554725, 0.8063142029461978, 0.822095474575489, 0.8326487662652271, 0.8416943161106497, 0.853551133086471, 0.8585498425387597, 0.8664281316329827, 0.8727038704895718, 0.8759844932401256, 0.8824244344661315, 0.8818198957756552, 0.8976051327750092, 0.896999873108158, 0.8937693582156699, 0.9004668682516611, 0.9125776491555924, 0.9067901915490033, 0.9091596804055924, 0.9144370472268365, 0.9179269333702473, 0.9189954203580657, 0.9259465338339794, 0.9263439720607235, 0.9315955639765596, 0.9242963991555924, 0.930248419619786, 0.9268533418696937, 0.9245503630837025, 0.9359432317621816, 0.9349666692621816, 0.9395472124169435, 0.9434545438815062, 0.9451275695598007, 0.9373157905361758, 0.940710147309893, 0.9504045758928571, 0.9441045040836102, 0.9411279531192323, 0.952916457583518, 0.9405484683577889, 0.9538694081072352, 0.9540325290120893, 0.954149146940753, 0.956288283845515, 0.9571950918812293, 0.9537760416666666, 0.9575209732027501, 0.959473377226375, 0.9587536625599853, 0.9571028069052234, 0.9611249538575121, 0.9635423876430418, 0.9662170297503692, 0.9601723638219823, 0.9612637418097084, 0.9623805342146549, 0.965030482881137, 0.9727274464170359, 0.9555460386674051, 0.9615198686669435, 0.9698900438930418, 0.9750057317621816, 0.9723089196313216, 0.975144519714378, 0.9735877514765596, 0.9730754977620893, 0.9755415974529347, 0.9743317990956073, 0.9765410509528424, 0.9760062667266519, 0.9772840171073275, 0.9809348612264673, 0.9796800018456996, 0.978052397679033, 0.980515613464378, 0.9794936294527501, 0.979819510774271, 0.9754246190360835, 0.9834224099644703, 0.9830739981312293, 0.9822373050479882, 0.9846082358573275, 0.9816556573574198, 0.9852131350359912, 0.9839114121908453, 0.9830736376430418, 0.9859103191906607, 0.9846779903216132, 0.9888625372023809, 0.9897235632382798, 0.9877704382382798, 0.9847244932978036, 0.988072707583518, 0.989049991059893, 0.9891891395002769, 0.9886772462739941, 0.9890728820598007, 0.9909791435954227, 0.9911880465000923, 0.9901649810239018, 0.9896073057978036, 0.9913747793812293, 0.9911887674764673, 0.990514834809893, 0.9935600587739941, 0.9912123794527501, 0.9910954010358989, 0.9922114724644703, 0.9928160111549464, 0.9925598842977114, 0.9934903043097084, 0.9925598842977114, 0.9935833102620893, 0.9936763162144703, 0.9925137418097084, 0.9940011160714286, 0.9935825892857143, 0.9942801339285714, 0.9939317220953304, 0.9938151041666666, 0.9944665063215209, 0.9950249025239941, 0.9940723124884644, 0.994187488464378, 0.9953736748454227, 0.9950252630121816, 0.9945602332502769, 0.9950012905477114, 0.9956290807262828, 0.9952112749169435, 0.9955367957502769, 0.9961642254406607, 0.9962339799049464, 0.995117908476375, 0.9956755837024732, 0.996280482881137, 0.995653053190753, 0.9958612351190477, 0.9966517857142857, 0.9963509583217978, 0.9965358887619971, 0.9965834732027501, 0.9964668552740864, 0.9963269858573275, 0.9966292552025655, 0.9961638649524732, 0.9961177224644703, 0.9967691246193245, 0.9963269858573275, 0.9963502373454227, 0.9971636789405685, 0.9969079125715209, 0.997117175964378, 0.9970939244762828, 0.9967912946428571, 0.9972337938930418, 0.9970012790120893, 0.9961645859288483, 0.9972101819167589, 0.99730318786914, 0.997164039428756, 0.9973493303571429, 0.9974663087739941, 0.9973496908453304, 0.997117896940753, 0.9968614095953304, 0.9973965543097084, 0.9972101819167589, 0.9975822057262828, 0.9973729423334257, 0.9974423363095238, 0.9975589542381875, 0.9976294296788483, 0.9973736633098007, 0.9968857425479882, 0.9974895602620893, 0.9976752116786637], "end": "2016-01-31 14:53:25.459000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0], "moving_var_accuracy_valid": [0.03213914257432469, 0.06497666714564515, 0.09229074466586541, 0.11293556303452838, 0.12797375626391744, 0.13707761025783838, 0.14190473394987543, 0.14302589263625595, 0.141543806473275, 0.13836122122242545, 0.13356304330709357, 0.12773948756065273, 0.12144758727889776, 0.11461002839074173, 0.10724932349084151, 0.10037978380556693, 0.09333975673467641, 0.08631033049129967, 0.0796545746434839, 0.0735797818575205, 0.0675212734557772, 0.061875053401218676, 0.05663014842932858, 0.05188386677265493, 0.0473774517988804, 0.04328416818421307, 0.03943502792361547, 0.03595629271405334, 0.032594219116491484, 0.029551439633248106, 0.026782948275118543, 0.024236192217095085, 0.021998769545612674, 0.01993015757352245, 0.018084430657720398, 0.016410054498163452, 0.014875301891951706, 0.013430818080077957, 0.012125171408036003, 0.011008471035608633, 0.009946904805222128, 0.00896529900472783, 0.00813274030667848, 0.007326446026275142, 0.006635902803469368, 0.005994819554076007, 0.005411321215007127, 0.004885117809029932, 0.004424495958852414, 0.003987631767089106, 0.0036054417767789762, 0.0032580548679606654, 0.0029384924382823015, 0.0026471797564193945, 0.002388904254026416, 0.002160292821701837, 0.0019552981790561396, 0.001764406140506514, 0.0015889016132269258, 0.0014339387893747145, 0.001292877336854034, 0.001170435854240473, 0.0010536532162441466, 0.0009551682493184824, 0.0008654411466662658, 0.000784911089085367, 0.0007145147046801088, 0.0006505404539153996, 0.0005937056734117548, 0.0005393332020074392, 0.0004952473418236593, 0.0004474560899723138, 0.00040791116246683746, 0.0003719961943344518, 0.00033795244889883177, 0.0003129920198008596, 0.00028984455410344383, 0.0002615642800160262, 0.0002383653650887421, 0.00021791714013477063, 0.00019617482710074518, 0.00017693965367620936, 0.000165451872341495, 0.00015445295061030192, 0.0001406076444971246, 0.00013233164049639316, 0.0001196912956259093, 0.00011027965318025517, 0.00010065104773334845, 9.05967871820221e-05, 8.268624082216448e-05, 7.485177109873036e-05, 6.920013070921015e-05, 6.640310664446635e-05, 6.132727435448658e-05, 5.674665112915284e-05, 5.633541878262201e-05, 5.3648848314399865e-05, 5.4027452785226145e-05, 5.0433896353153556e-05, 4.576537296026969e-05, 4.8038889612457156e-05, 4.539722794970261e-05, 4.0863505343738845e-05, 3.694228278799278e-05, 3.3362547387331477e-05, 3.161330838958588e-05, 2.8454493567826823e-05, 2.74191813250475e-05, 2.6829996787304827e-05, 2.4377505536102625e-05, 2.2797170526643525e-05, 2.0846854213171646e-05, 1.8891248560405927e-05, 2.0949769645686584e-05, 1.9096455230602873e-05, 2.162222464907138e-05, 2.1882952238815934e-05, 1.9730506617300783e-05, 1.9137183594505884e-05, 1.753992874938071e-05, 1.5919883446580188e-05, 1.4685608422136268e-05, 1.3254393059110471e-05, 1.4093105784214862e-05, 1.5047838774902299e-05, 1.3664949438596661e-05, 1.2478301623484096e-05, 1.125770881985948e-05, 1.0243980568687745e-05, 1.1300925074802937e-05, 1.0171685654513422e-05, 9.967389834148504e-06, 9.007813845560136e-06, 8.167913849264726e-06, 7.590415646667216e-06, 6.858339083079166e-06, 7.355343141116813e-06, 6.621100678128282e-06, 6.489191399892636e-06, 6.079574706798879e-06, 5.584903813637017e-06, 5.507217783550945e-06, 5.022982409049551e-06, 5.584782931354915e-06, 5.062327744070245e-06, 4.562391839561782e-06, 5.021046510890051e-06, 4.7979079286071755e-06, 4.708652200836131e-06, 4.5856401200957025e-06, 4.80633029539965e-06, 4.362728687856795e-06, 4.3264767911649006e-06, 4.511240925019785e-06, 4.099152011702157e-06, 3.6958986806145054e-06, 4.409192949781381e-06, 4.206258367543806e-06, 3.854761950493269e-06, 3.7494677208164975e-06, 3.481330337404595e-06, 3.69424490155361e-06, 3.3403983442044748e-06, 3.0250173649734e-06, 4.665667419289825e-06, 4.225259519768439e-06, 3.99776226464431e-06, 3.677182212321656e-06, 3.342997904041431e-06, 3.3701094987133543e-06, 3.2844482492350777e-06, 3.362291222657517e-06, 3.448558323189453e-06, 3.118119049430862e-06, 2.852462008885318e-06, 2.696133991159619e-06, 2.906002149878845e-06, 2.620414083474011e-06, 2.417778849788255e-06, 2.2415275090755424e-06], "accuracy_test": 0.4768654336734694, "start": "2016-01-29 15:43:30.745000", "learning_rate_per_epoch": [0.0005772303557023406, 0.0005638807779178023, 0.0005508399335667491, 0.0005381006631068885, 0.0005256560398265719, 0.0005134991952218115, 0.0005016234936192632, 0.0004900224739685655, 0.00047868973342701793, 0.0004676190728787333, 0.00045680446783080697, 0.0004462399519979954, 0.00043591976282186806, 0.0004258382541593164, 0.00041598989628255367, 0.0004063693049829453, 0.0003969712124671787, 0.00038779046735726297, 0.0003788220346905291, 0.0003700610250234604, 0.00036150263622403145, 0.00035314218257553875, 0.00034497506567277014, 0.00033699683262966573, 0.000329203117871657, 0.00032158964313566685, 0.0003141522465739399, 0.0003068868536502123, 0.0002997894771397114, 0.00029285624623298645, 0.0002860833774320781, 0.00027946714544668794, 0.0002730039122980088, 0.00026669015642255545, 0.0002605224144645035, 0.00025449731037952006, 0.00024861155543476343, 0.00024286191910505295, 0.00023724525817669928, 0.00023175848764367402, 0.0002263986098114401, 0.00022116269974503666, 0.00021604787616524845, 0.00021105134510435164, 0.0002061703708022833, 0.00020140227570664138, 0.00019674445502460003, 0.00019219434761907905, 0.00018774947966448963, 0.00018340740643907338, 0.00017916574142873287, 0.000175022185430862, 0.00017097445379476994, 0.00016702033462934196, 0.00016315765969920903, 0.00015938431897666305, 0.0001556982460897416, 0.0001520974183222279, 0.00014857987116556615, 0.00014514366921503097, 0.0001417869352735579, 0.00013850783579982817, 0.00013530456635635346, 0.00013217538071330637, 0.00012911856174468994, 0.00012613243598025292, 0.00012321537360548973, 0.00012036577390972525, 0.00011758207983803004, 0.00011486276343930513, 0.0001122063331422396, 0.00010961134103126824, 0.00010707636101869866, 0.00010460001067258418, 0.00010218092938885093, 9.981779294321314e-05, 9.750930621521547e-05, 9.525421046419069e-05, 9.305126877734438e-05, 9.089927334571257e-05, 8.879704546416178e-05, 8.674343553138897e-05, 8.473732304992154e-05, 8.277760207420215e-05, 8.08632030384615e-05, 7.899307820480317e-05, 7.716620893916115e-05, 7.53815911593847e-05, 7.363824261119589e-05, 7.193521014414728e-05, 7.027156971162185e-05, 6.864639726700261e-05, 6.705881241941825e-05, 6.550794205395505e-05, 6.399294215952978e-05, 6.25129760010168e-05, 6.106724322307855e-05, 5.9654943470377475e-05, 5.827530549140647e-05, 5.692757258657366e-05, 5.561100988416001e-05, 5.432489706436172e-05, 5.3068524721311405e-05, 5.184120891499333e-05, 5.0642276619328186e-05, 4.9471072998130694e-05, 4.832695776713081e-05, 4.72093015559949e-05, 4.6117493184283376e-05, 4.505093602347188e-05, 4.400904435897246e-05, 4.299124702811241e-05, 4.1996991058113053e-05, 4.1025727114174515e-05, 4.007692768936977e-05, 3.915006891475059e-05, 3.82446451112628e-05, 3.7360161513788626e-05, 3.649613427114673e-05, 3.565209044609219e-05, 3.4827564377337694e-05, 3.4022108593489975e-05, 3.3235279261134565e-05, 3.246664709877223e-05, 3.171579010086134e-05, 3.098230081377551e-05, 3.0265773602877744e-05, 2.956581738544628e-05, 2.8882050173706375e-05, 2.8214095436851494e-05, 2.7561589377000928e-05, 2.6924173653242178e-05, 2.6301499019609764e-05, 2.5693225325085223e-05, 2.50990178756183e-05, 2.451855289109517e-05, 2.3951512048370205e-05, 2.3397586119244806e-05, 2.285647133248858e-05, 2.232786937383935e-05, 2.1811492842971347e-05, 2.1307059796527028e-05, 2.081429192912765e-05, 2.0332920030341484e-05, 1.986268034670502e-05, 1.9403316400712356e-05, 1.8954577171825804e-05, 1.851621527748648e-05, 1.808799061109312e-05, 1.7669670342002064e-05, 1.7261023458559066e-05, 1.6861828044056892e-05, 1.6471865819767118e-05, 1.6090922144940123e-05, 1.57187878357945e-05, 1.5355259165517054e-05, 1.5000138773757499e-05, 1.4653231119154952e-05, 1.4314346117316745e-05, 1.3983298231323715e-05, 1.365990647173021e-05, 1.3343994396564085e-05, 1.3035388292337302e-05, 1.2733918993035331e-05, 1.2439421880117152e-05, 1.215173597302055e-05, 1.1870703019667417e-05, 1.1596169315453153e-05, 1.1327984793751966e-05, 1.106600302591687e-05, 1.0810079402290285e-05, 1.0560074770182837e-05, 1.031585179589456e-05, 1.0077277693198994e-05, 9.844220585364383e-06, 9.616553143132478e-06, 9.394151675223839e-06, 9.176893399853725e-06, 8.964659173216205e-06, 8.757333489484154e-06, 8.55480266181985e-06], "accuracy_train_first": 0.5921175292428018, "accuracy_train_last": 0.9976752116786637, "batch_size_eval": 1024, "accuracy_train_std": [0.020150378565934924, 0.016839709485514314, 0.018722294784313823, 0.017225509674966406, 0.019150572513731104, 0.020162464761568435, 0.021218051423814686, 0.01968769722074757, 0.018596048600525685, 0.01892070832499972, 0.021431720195890583, 0.020756808162036802, 0.01909431139306483, 0.018957866735629887, 0.01829422783025973, 0.01856384503414517, 0.018003755567372427, 0.01760379188448307, 0.017580488149612956, 0.017291903992464424, 0.016886949647975102, 0.016843214070802554, 0.01761418954795849, 0.01576773954639565, 0.01553715916959437, 0.016181318033466337, 0.013523464110486135, 0.015018578441658182, 0.015299805544252074, 0.015405512365759839, 0.014843115300019976, 0.01689354567119082, 0.015649442103930076, 0.014705492438026671, 0.013738826160336717, 0.013848580306547113, 0.014432267806657963, 0.014772604087757372, 0.013483833695658127, 0.012760492081255785, 0.012343809134686886, 0.013720006352133591, 0.011362443464529904, 0.013378576966015451, 0.01237378845403587, 0.011677461822320413, 0.0122174587136903, 0.011483117173924803, 0.010720345702074198, 0.011008738648065172, 0.01002595128945574, 0.010212108466374478, 0.00984508118235735, 0.01058037694349557, 0.009964066057322455, 0.009493752606828007, 0.00926377629795061, 0.009604364412918407, 0.01022401903024174, 0.009934214694915216, 0.009803277734004839, 0.007946873302078, 0.009537198884933885, 0.009557905933448746, 0.009354121079457444, 0.00656819133070359, 0.0077894638175165735, 0.007085699870297814, 0.007471198550131657, 0.007515859104780465, 0.00704586495915796, 0.007139509926642524, 0.006388050222311941, 0.006627739922689933, 0.007079580241718824, 0.0057690370030482425, 0.006110083801863141, 0.006039548260881485, 0.006537902926325772, 0.006083003399332937, 0.006405508720726278, 0.007157176062990781, 0.0055443385629575445, 0.004902281009308891, 0.0057808475621602545, 0.005828971152987549, 0.005801469572922803, 0.005029410799918397, 0.00556426681469287, 0.005083669119769355, 0.005200189059670796, 0.005232333415827426, 0.004470397156890924, 0.0043383209247356965, 0.00471966498007025, 0.005115293824828598, 0.004725857649267482, 0.0038989010053666023, 0.004035650789051047, 0.004268964499176107, 0.0039090424859033876, 0.0036163115627476008, 0.0031865701812228264, 0.00406864644704298, 0.0033221439244823605, 0.003456281771484482, 0.002675528301341614, 0.003937479238273104, 0.0027550608780545233, 0.0034890689821431282, 0.0035745616638098603, 0.002903308958589092, 0.004020409125571729, 0.00308845304575914, 0.002784649426609568, 0.003572515873521874, 0.002478082591476216, 0.002939864405258934, 0.0026937217555926926, 0.00271714307254307, 0.003343366716521334, 0.002406950203610032, 0.002271338157542109, 0.0022918938858540938, 0.0025725000318272896, 0.0023426254623999847, 0.0025773633395202934, 0.0024577556682640156, 0.002286102804650774, 0.0023708029331613034, 0.00282009153377172, 0.0023754182567087526, 0.0020022264883917387, 0.0017670839735533662, 0.0021104497616750433, 0.0017232558283924802, 0.0017774340476289794, 0.0023541077113760603, 0.0019318576431934873, 0.002013039568596347, 0.0020454758669628376, 0.0018431846337989598, 0.002143176826972268, 0.0021074310636794037, 0.0018727230278407431, 0.0018204171712123682, 0.001868063735566201, 0.0015189147857666541, 0.0015096668677811451, 0.002146141507688903, 0.001958788132955279, 0.0016556916058686263, 0.0018553783154847043, 0.001976835617484488, 0.0017157626056160578, 0.0017039596071919644, 0.0013302717004149988, 0.001629974287579953, 0.0018118266126095756, 0.0017032410435066433, 0.0013705802252787218, 0.0017481958280746678, 0.0016171282319309924, 0.001504175786235976, 0.0016337685381379336, 0.0015987828718072063, 0.0014766764554976482, 0.001449994581215077, 0.0015920363208007497, 0.0015001429193674358, 0.0016864249260188447, 0.001545327928006932, 0.0013005524044991015, 0.0016299125782963206, 0.0014128029697696531, 0.0014336312126236417, 0.0014913859011226117, 0.0013865684227814768, 0.0016635953766125902, 0.0014625016874788162, 0.0015798194420042978], "accuracy_test_std": 0.014902293806691133, "error_valid": [0.4024202277861446, 0.3073348079819277, 0.2640189664909638, 0.23952254329819278, 0.21714249576430722, 0.21065217902861444, 0.20081448842243976, 0.19677587302334332, 0.1905708772590362, 0.18109939759036142, 0.17844473597515065, 0.17434435005647586, 0.16634800922439763, 0.1650464161332832, 0.1701527790850903, 0.1552807911332832, 0.1590443806475903, 0.16328595632530118, 0.15915615587349397, 0.14754918109939763, 0.15783397260918675, 0.15512783556099397, 0.15255406391189763, 0.1437341161521084, 0.1475182958396084, 0.1412412344691265, 0.14442535768072284, 0.1382409520896084, 0.15197459760918675, 0.14875958913780118, 0.14737563535391573, 0.15013177710843373, 0.13905426393072284, 0.1418001105986446, 0.13571718514683728, 0.13353021460843373, 0.13390672063253017, 0.14296051393072284, 0.14224868222891573, 0.1279752800263554, 0.13644960702183728, 0.1431943594691265, 0.12738551863704817, 0.14257371282003017, 0.12887095256024095, 0.1325227668486446, 0.1334287344691265, 0.13254335702183728, 0.12653102644954817, 0.13449648202183728, 0.12801646037274095, 0.12813853068524095, 0.1306917121611446, 0.13287868269954817, 0.1291959831513554, 0.12612363516566272, 0.12466908650225905, 0.12745611351656627, 0.1306917121611446, 0.12698842243975905, 0.12784291462725905, 0.12370281908885539, 0.1332551887236446, 0.12297922157379515, 0.12282773672816272, 0.12187176440135539, 0.11974509365587349, 0.11916562735316272, 0.11781255882906627, 0.1189611963478916, 0.11520790192018071, 0.12023337490587349, 0.11658156061746983, 0.11606239410768071, 0.11676540144954817, 0.11218702936746983, 0.11158697289156627, 0.11735516283885539, 0.11414015436746983, 0.11316359186746983, 0.11794492422816272, 0.11655067535768071, 0.11010153896837349, 0.10972503294427716, 0.11257383047816272, 0.10835137424698793, 0.11300034120858427, 0.10997946865587349, 0.11083396084337349, 0.11472991575677716, 0.11084425592996983, 0.11625653002635539, 0.10976621329066272, 0.10706007624246983, 0.10898231598268071, 0.11688747176204817, 0.10550257671310237, 0.10666298004518071, 0.1038244775978916, 0.10653061464608427, 0.10852491999246983, 0.10163750705948793, 0.10458778473268071, 0.10925734186746983, 0.10767042780496983, 0.10776161285768071, 0.10457748964608427, 0.10818959431475905, 0.10385536285768071, 0.10300087067018071, 0.10900290615587349, 0.10447600950677716, 0.10534079678087349, 0.10826018919427716, 0.10055946442018071, 0.10815870905496983, 0.09966379188629515, 0.10079330995858427, 0.10483192535768071, 0.10148455148719882, 0.10313323606927716, 0.10360092714608427, 0.10270525461219882, 0.10385536285768071, 0.09953142648719882, 0.0988195947853916, 0.10226844879518071, 0.10190223785768071, 0.10262436464608427, 0.10200371799698793, 0.09819894813629515, 0.10262436464608427, 0.09953142648719882, 0.10287880035768071, 0.10312294098268071, 0.10075212961219882, 0.10276702513177716, 0.1058996729103916, 0.10275673004518071, 0.10507606598268071, 0.1012610010353916, 0.10160662179969882, 0.10030502870858427, 0.10152573183358427, 0.09886077513177716, 0.10132277155496983, 0.10215667356927716, 0.10510695124246983, 0.10399802334337349, 0.10449659967996983, 0.10458778473268071, 0.10007118317018071, 0.10190223785768071, 0.10458778473268071, 0.10007118317018071, 0.10176987245858427, 0.10263465973268071, 0.09892107492469882, 0.10041680393448793, 0.10275673004518071, 0.10020354856927716, 0.10070212490587349, 0.0991858057228916, 0.1010168604103916, 0.10093597044427716, 0.10599232868975905, 0.10127129612198793, 0.1002844385353916, 0.10067123964608427, 0.10090508518448793, 0.09945053652108427, 0.09958290192018071, 0.09896225527108427, 0.09870781955948793, 0.10105804075677716, 0.10141395660768071, 0.09957260683358427, 0.09834160862198793, 0.10018295839608427, 0.09958290192018071, 0.09946083160768071], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.02312697809389688, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 16, "valid_ratio": 0.15, "learning_rate": 0.0005908959822341562, "optimization": "adam", "nb_data_augmentation": 3, "learning_rate_decay_method": "exp", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 7.557551481078167e-05, "rotation_range": [0, 0], "momentum": 0.5607274858085106}, "accuracy_valid_max": 0.9018010518637049, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.9005391683923193, "accuracy_valid_std": [0.018974835410795296, 0.009615234306439588, 0.015303892846400451, 0.011162196400057145, 0.015680521996139882, 0.012190121774988327, 0.010485247479825818, 0.010492813240912025, 0.014722529495493304, 0.010473128683992177, 0.013276379054346815, 0.0097397652218701, 0.008751597743423515, 0.010276730585864643, 0.0068575287479129875, 0.013935856989478142, 0.008310744108654188, 0.009197502613494626, 0.011621814185687912, 0.010235397034333682, 0.01643881909384647, 0.008515610342912807, 0.012910788091152234, 0.012629349285700777, 0.01191781555522458, 0.008500879336411544, 0.009403178033122904, 0.010920473239065696, 0.012958346059957894, 0.009785431262612143, 0.010022525111550233, 0.009279553773499025, 0.008359287391859812, 0.00873991155062507, 0.008097498692594853, 0.005633754711389078, 0.007860470295556895, 0.005890990069565807, 0.00592821291795303, 0.004230699315105554, 0.005629481924666222, 0.003010157126514312, 0.006538605920467534, 0.006272784020041348, 0.003197401694677424, 0.0060167294506343885, 0.005447930272047199, 0.004811557501215779, 0.0072400356979681136, 0.003982182200427172, 0.0066065092164225465, 0.004009056272474572, 0.0039009160816569576, 0.010419210491216123, 0.0056597718078872684, 0.006622872709218869, 0.0075922515517557925, 0.008471621978606002, 0.0047043899785280535, 0.00468711886547904, 0.006335051746132743, 0.005852140199041411, 0.006373300422701975, 0.015082170709597035, 0.00791823265204126, 0.008599261036549039, 0.00909099617636324, 0.005074232684910303, 0.005676375228622264, 0.012864166528570002, 0.007743384087672887, 0.006572190020331497, 0.008352826282027707, 0.008288100879273137, 0.007080569826778059, 0.002356836599210526, 0.0038978862555301055, 0.007317254590244606, 0.005398229617978988, 0.00448214173599741, 0.0037186482277884927, 0.008798906008091356, 0.009069157907568903, 0.007091393738864842, 0.006924764460981381, 0.007764257847802431, 0.007577566108513132, 0.004233187454162482, 0.004790785023345296, 0.008770927346724673, 0.005324509680421414, 0.002742863418840784, 0.007594353399773386, 0.005415031805608624, 0.007275971061397764, 0.009996644089757357, 0.010067429707815522, 0.008684584992189454, 0.007329404621807287, 0.008408181726448385, 0.005291345855468994, 0.007958161463665642, 0.007268169688632027, 0.004819077321013316, 0.0051853923881687615, 0.007072064790041973, 0.006799646992790138, 0.0072991314954394685, 0.0058980428088531945, 0.004950209001155635, 0.004934134939968295, 0.004535308719482534, 0.006684862633142221, 0.0064634827405189105, 0.00519595135082926, 0.00319291166690645, 0.008711295498254905, 0.005737456621618127, 0.005841520219239356, 0.008789521105693232, 0.0053494774464549425, 0.006544121506673564, 0.008479549120657685, 0.004904902946592783, 0.006814221655060622, 0.007254273770148588, 0.006335756608766905, 0.005969687832996554, 0.006085365031831647, 0.008670837610318206, 0.007307279159498127, 0.006372436445910058, 0.00864901310424392, 0.006865345574282778, 0.005298340290572569, 0.009720414111819868, 0.005069450996034951, 0.007538843129788485, 0.006913541708400586, 0.006043216905862584, 0.00667532079239121, 0.008823596599608345, 0.006097051459210634, 0.010108078981089888, 0.00542103680994429, 0.005304375657729214, 0.0064942413195003545, 0.005372426581681431, 0.006938556057324961, 0.004531953687988623, 0.005134377113387361, 0.006498526516501522, 0.002464400335747693, 0.007085455966459194, 0.00608158187854435, 0.00795420523093783, 0.004762576948240149, 0.007740376129021561, 0.006003142122600949, 0.004566810650258881, 0.007652637257043262, 0.006160932596783877, 0.006624931282038511, 0.007042605731224536, 0.006883377223599857, 0.0074109751321379605, 0.007055672223320637, 0.007555399682787504, 0.0054091263055440104, 0.007337973814019222, 0.005805069711981902, 0.005349311662249015, 0.007973986432500399, 0.007217292982481976, 0.008196857625601014, 0.006851000125072273, 0.005096479991571784, 0.005066052838716223, 0.009174869124937044, 0.005144838972014504, 0.005177415532982166], "accuracy_valid": [0.5975797722138554, 0.6926651920180723, 0.7359810335090362, 0.7604774567018072, 0.7828575042356928, 0.7893478209713856, 0.7991855115775602, 0.8032241269766567, 0.8094291227409638, 0.8189006024096386, 0.8215552640248494, 0.8256556499435241, 0.8336519907756024, 0.8349535838667168, 0.8298472209149097, 0.8447192088667168, 0.8409556193524097, 0.8367140436746988, 0.840843844126506, 0.8524508189006024, 0.8421660273908133, 0.844872164439006, 0.8474459360881024, 0.8562658838478916, 0.8524817041603916, 0.8587587655308735, 0.8555746423192772, 0.8617590479103916, 0.8480254023908133, 0.8512404108621988, 0.8526243646460843, 0.8498682228915663, 0.8609457360692772, 0.8581998894013554, 0.8642828148531627, 0.8664697853915663, 0.8660932793674698, 0.8570394860692772, 0.8577513177710843, 0.8720247199736446, 0.8635503929781627, 0.8568056405308735, 0.8726144813629518, 0.8574262871799698, 0.871129047439759, 0.8674772331513554, 0.8665712655308735, 0.8674566429781627, 0.8734689735504518, 0.8655035179781627, 0.871983539627259, 0.871861469314759, 0.8693082878388554, 0.8671213173004518, 0.8708040168486446, 0.8738763648343373, 0.875330913497741, 0.8725438864834337, 0.8693082878388554, 0.873011577560241, 0.872157085372741, 0.8762971809111446, 0.8667448112763554, 0.8770207784262049, 0.8771722632718373, 0.8781282355986446, 0.8802549063441265, 0.8808343726468373, 0.8821874411709337, 0.8810388036521084, 0.8847920980798193, 0.8797666250941265, 0.8834184393825302, 0.8839376058923193, 0.8832345985504518, 0.8878129706325302, 0.8884130271084337, 0.8826448371611446, 0.8858598456325302, 0.8868364081325302, 0.8820550757718373, 0.8834493246423193, 0.8898984610316265, 0.8902749670557228, 0.8874261695218373, 0.8916486257530121, 0.8869996587914157, 0.8900205313441265, 0.8891660391566265, 0.8852700842432228, 0.8891557440700302, 0.8837434699736446, 0.8902337867093373, 0.8929399237575302, 0.8910176840173193, 0.8831125282379518, 0.8944974232868976, 0.8933370199548193, 0.8961755224021084, 0.8934693853539157, 0.8914750800075302, 0.8983624929405121, 0.8954122152673193, 0.8907426581325302, 0.8923295721950302, 0.8922383871423193, 0.8954225103539157, 0.891810405685241, 0.8961446371423193, 0.8969991293298193, 0.8909970938441265, 0.8955239904932228, 0.8946592032191265, 0.8917398108057228, 0.8994405355798193, 0.8918412909450302, 0.9003362081137049, 0.8992066900414157, 0.8951680746423193, 0.8985154485128012, 0.8968667639307228, 0.8963990728539157, 0.8972947453878012, 0.8961446371423193, 0.9004685735128012, 0.9011804052146084, 0.8977315512048193, 0.8980977621423193, 0.8973756353539157, 0.8979962820030121, 0.9018010518637049, 0.8973756353539157, 0.9004685735128012, 0.8971211996423193, 0.8968770590173193, 0.8992478703878012, 0.8972329748682228, 0.8941003270896084, 0.8972432699548193, 0.8949239340173193, 0.8987389989646084, 0.8983933782003012, 0.8996949712914157, 0.8984742681664157, 0.9011392248682228, 0.8986772284450302, 0.8978433264307228, 0.8948930487575302, 0.8960019766566265, 0.8955034003200302, 0.8954122152673193, 0.8999288168298193, 0.8980977621423193, 0.8954122152673193, 0.8999288168298193, 0.8982301275414157, 0.8973653402673193, 0.9010789250753012, 0.8995831960655121, 0.8972432699548193, 0.8997964514307228, 0.8992978750941265, 0.9008141942771084, 0.8989831395896084, 0.8990640295557228, 0.894007671310241, 0.8987287038780121, 0.8997155614646084, 0.8993287603539157, 0.8990949148155121, 0.9005494634789157, 0.9004170980798193, 0.9010377447289157, 0.9012921804405121, 0.8989419592432228, 0.8985860433923193, 0.9004273931664157, 0.9016583913780121, 0.8998170416039157, 0.9004170980798193, 0.9005391683923193], "seed": 296040138, "model": "residualv3", "loss_std": [0.3479837477207184, 0.2550557851791382, 0.2511745095252991, 0.24614529311656952, 0.2390771359205246, 0.2351270169019699, 0.2291201502084732, 0.22306907176971436, 0.21870671212673187, 0.21064281463623047, 0.2085818499326706, 0.20413616299629211, 0.20078982412815094, 0.19757162034511566, 0.19239899516105652, 0.18721064925193787, 0.18311145901679993, 0.1812175065279007, 0.1758701503276825, 0.17334240674972534, 0.1687096804380417, 0.16429778933525085, 0.16225506365299225, 0.15585432946681976, 0.15364651381969452, 0.14791323244571686, 0.14598269760608673, 0.14265277981758118, 0.13866795599460602, 0.13721778988838196, 0.13484831154346466, 0.12971438467502594, 0.1265920251607895, 0.12336788326501846, 0.11873643845319748, 0.11912702769041061, 0.11385056376457214, 0.11363730579614639, 0.11032991111278534, 0.10870742797851562, 0.10275491327047348, 0.10289306193590164, 0.0994921326637268, 0.0982242152094841, 0.09526530653238297, 0.09388530254364014, 0.09212949872016907, 0.08804084360599518, 0.0860147550702095, 0.08430363982915878, 0.0830523744225502, 0.08008294552564621, 0.07997498661279678, 0.07810169458389282, 0.07451435923576355, 0.07386919111013412, 0.07080354541540146, 0.06950676441192627, 0.06742201000452042, 0.06548880785703659, 0.06607594341039658, 0.06430919468402863, 0.06317317485809326, 0.06329146027565002, 0.059193819761276245, 0.0580679215490818, 0.056445084512233734, 0.055423554033041, 0.054234784096479416, 0.051865365356206894, 0.05223768576979637, 0.05185488238930702, 0.05156143382191658, 0.048390716314315796, 0.0459432527422905, 0.04802204295992851, 0.04572170972824097, 0.04504406824707985, 0.04320182278752327, 0.04132304713129997, 0.04072181135416031, 0.042310673743486404, 0.03912343084812164, 0.03964172676205635, 0.03673209995031357, 0.037553127855062485, 0.03492582589387894, 0.034809891134500504, 0.03627479448914528, 0.03408413380384445, 0.034242887049913406, 0.033380936831235886, 0.0329333133995533, 0.031230034306645393, 0.030028648674488068, 0.02933216467499733, 0.029306307435035706, 0.029251834377646446, 0.02882099524140358, 0.026807041838765144, 0.02782764472067356, 0.027684371918439865, 0.02613844908773899, 0.026333848014473915, 0.026474587619304657, 0.02346757799386978, 0.022860106080770493, 0.023753032088279724, 0.02313886396586895, 0.02396966516971588, 0.022056221961975098, 0.021740738302469254, 0.020328158512711525, 0.02256866730749607, 0.02083897963166237, 0.02084142342209816, 0.019837670028209686, 0.019332166761159897, 0.018542416393756866, 0.018836304545402527, 0.018054677173495293, 0.017996778711676598, 0.01735846884548664, 0.018082574009895325, 0.017877934500575066, 0.016985692083835602, 0.016556205227971077, 0.016323810443282127, 0.014933984726667404, 0.01614275760948658, 0.015506977215409279, 0.015394371002912521, 0.014866629615426064, 0.01480045448988676, 0.014312232844531536, 0.014703040011227131, 0.014100274071097374, 0.014029242098331451, 0.01374074351042509, 0.013126843608915806, 0.012501439079642296, 0.013760938309133053, 0.012935088016092777, 0.012055695056915283, 0.01260535791516304, 0.012984760105609894, 0.01157459244132042, 0.012096287682652473, 0.011159868910908699, 0.011111567728221416, 0.011516782455146313, 0.011897014454007149, 0.011298459023237228, 0.010712521150708199, 0.010486835613846779, 0.010796157643198967, 0.00971722137182951, 0.010259111411869526, 0.009992395527660847, 0.009146296419203281, 0.010300981812179089, 0.010075325146317482, 0.010339565575122833, 0.00867778155952692, 0.00985229853540659, 0.008913543075323105, 0.009097513742744923, 0.009965989738702774, 0.008809840306639671, 0.00923013873398304, 0.00884176604449749, 0.008194434456527233, 0.007948205806314945, 0.008440501056611538, 0.008254996500909328, 0.008296581916511059, 0.008419638499617577, 0.0077236914075911045, 0.00853088777512312, 0.00798997562378645, 0.006730667315423489]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:25 2016", "state": "available"}], "summary": "8ad61d9dbd3cb2f3693f44898a4ba3f1"}