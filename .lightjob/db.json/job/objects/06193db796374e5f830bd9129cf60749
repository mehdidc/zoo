{"content": {"hp_model": {"f0": 64, "f1": 64, "f2": 16, "f3": 16, "nonlin": "rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.012531255254229866, 0.015194328140332662, 0.016852181281618805, 0.01283980422905716, 0.015249569676348523, 0.01304406492070719, 0.009867314160474908, 0.010293836718012168, 0.008993082582311308, 0.009827814146115598, 0.013311862121108027, 0.00635587401684433, 0.007564420598061361, 0.008145101988885977, 0.007484214382797525, 0.008353088214629267, 0.006130477045658596, 0.00951248128871334, 0.004943134609758994, 0.008741146065127029, 0.005853490430491359, 0.00791689030922184, 0.006453434442828919, 0.006631481542233706, 0.004607097204525341, 0.008668891470416216, 0.008807614904097922, 0.007790856539887585, 0.0074227676352009485, 0.007949172818012806, 0.006685218067950151, 0.008529507489248914, 0.004188850276175399, 0.008214307388847718, 0.005821396209418789, 0.0065841520136589995, 0.00710205344260037, 0.008393976905529458, 0.010052629181908154, 0.0067209097725980434, 0.008421155189549662, 0.006373357609725654, 0.006960974590977945, 0.00611240722183289, 0.011733832869173578, 0.00919126502743699, 0.008811949923702928, 0.010292482660689033, 0.01021117499948052, 0.0089959451838989, 0.009489802979507413, 0.010484286933959527, 0.01164142608796011, 0.011500976287424192, 0.010717793744382562, 0.008600319441349159, 0.009302283669734596, 0.008474712407969204, 0.007229096383350206, 0.008379978896701024, 0.009825140858398005, 0.009256588486618374, 0.00894132332047113, 0.009013158217780839, 0.008587060483122595, 0.009094735338074965, 0.008702116379551792, 0.008692913260775578, 0.007409624821256621, 0.00896253391241847, 0.008546152032845372, 0.008511126421452896, 0.008759612285128933, 0.008433958151583357, 0.00915046922081995, 0.008861980842500686, 0.009396568715813612, 0.009310580155899651, 0.008469840627638305, 0.008938073332545397, 0.011201914296310981, 0.00866793289737316, 0.00795308368669284, 0.007728511151044195, 0.007272333911107867, 0.007809929681694709, 0.0074766522886739566, 0.009086608607292884, 0.008369880981165466, 0.0077653587535160065, 0.009203144062169029, 0.00818937039591505, 0.009000602855231694, 0.007607064040792602, 0.00907714191603733, 0.008943732465599541, 0.0091872483737686, 0.009680216939566083, 0.009411850745904976, 0.008916559061724777, 0.007840764236808908, 0.008032511207452842, 0.009347221750050283, 0.008907327626588874, 0.007824822983184841, 0.009707665740446559, 0.007990996469917551, 0.008455074903659102, 0.00950170466056757, 0.00861056989668674, 0.007908261070534843, 0.00866597824272515, 0.008279458946490516, 0.007937628125953946, 0.008426999880223334, 0.008103155473264194, 0.00810567283770725, 0.009086213441232055, 0.008290415979414751, 0.008074871586980984, 0.008226570784589543, 0.008134592062767817, 0.008076408092002866, 0.009079196738936065, 0.007686113613541059, 0.00799384356105243, 0.008497267052634669, 0.009412558681544763, 0.008461631127829742, 0.008950595334173703, 0.007173841613456207, 0.008754863959143386, 0.0077090635993881705, 0.008768131897054488, 0.008236535563048101, 0.007806204791365727, 0.008465727483052639, 0.008044923208456664, 0.007532762730318789, 0.008159977322261465, 0.008684117606820065, 0.007786366257820265, 0.00811866425482175, 0.009101681242588283], "moving_avg_accuracy_train": [0.032071300237633805, 0.07799613268272423, 0.1293409872185308, 0.17776929592331114, 0.22558084887758395, 0.2718612277910437, 0.31624758332496994, 0.3575458903244681, 0.39237598789801054, 0.4289953596519987, 0.46243653332942525, 0.49227299272834874, 0.522322020628825, 0.5491221853583474, 0.5743790429922248, 0.5979912660174301, 0.6211181931947569, 0.6417862677187198, 0.6606430848664215, 0.6777120567934468, 0.6936136021003978, 0.7096498928051328, 0.7235131453227922, 0.7370802429409837, 0.7497742257009181, 0.7611314169182017, 0.7714550874149197, 0.7814528035142362, 0.7907183564096287, 0.7998686146570932, 0.8079689524000402, 0.8159683546579598, 0.8232700511400689, 0.8305624101537382, 0.8370929811827074, 0.8430981620003908, 0.8484913431875333, 0.8540147049178294, 0.858720843754904, 0.8628608934118056, 0.8671262644803703, 0.8712301293575455, 0.8749631352767652, 0.8789134644505006, 0.8824710137580345, 0.885814642212196, 0.8891030338245404, 0.8925438199815844, 0.8952522997693709, 0.8981221208664002, 0.9008283008870875, 0.9032011199366679, 0.9053203810396234, 0.9076461346715414, 0.9097695759236102, 0.9118456865183108, 0.9138560201309223, 0.9155700613787197, 0.9173266482410323, 0.9189144797659047, 0.9204923376620994, 0.9218310295603414, 0.9232753786449587, 0.924624156994933, 0.9258403466099008, 0.9269419648074378, 0.9282123308959078, 0.9292347886862543, 0.9301968533761376, 0.9311416585101, 0.9320803748342469, 0.9328810416985981, 0.9336528311991426, 0.9343915835281947, 0.9350797481612556, 0.935708469023886, 0.9363230377787972, 0.9369528795689316, 0.9375360492705378, 0.938112055275793, 0.9385653925626748, 0.9390663660244306, 0.9395079415447727, 0.9398867583226044, 0.9403253136238341, 0.9407038094509117, 0.9410699962833676, 0.9413669763004258, 0.9416040674300732, 0.9419197920431936, 0.9421760424092878, 0.9424182574340013, 0.9426896933300438, 0.9429897892079107, 0.943213408570619, 0.9433588984744466, 0.9435665332497871, 0.943746429101165, 0.9438176906126525, 0.9439376295444197, 0.9440571642782392, 0.9442694132839239, 0.944393008073564, 0.9444856782425826, 0.9445853213875472, 0.9446634105227867, 0.9448150349040166, 0.9448980184245046, 0.9450075447762678, 0.9451154551369116, 0.9451590599900532, 0.9452378679364614, 0.9453645986596572, 0.94545304362481, 0.9454815629172755, 0.9455327348197616, 0.9455834398296182, 0.9455779571134985, 0.9456334765380383, 0.9456276764975143, 0.9456852354788998, 0.9457626151990516, 0.9458368711959884, 0.94591067703966, 0.9459492005132504, 0.9459559698537674, 0.9460411173197565, 0.9460224910355938, 0.9460521582584002, 0.946013790641078, 0.9459839100831071, 0.9459919308618949, 0.9460595313342142, 0.9460692545343107, 0.946101220853674, 0.946148627780396, 0.9461912940144458, 0.946204080939367, 0.9462457800575013, 0.9462229635412307, 0.9462674607456163, 0.9462750282438676, 0.9462771526458559, 0.946265149763607], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 944576385, "moving_var_accuracy_train": [0.009257114690392053, 0.027313215337339526, 0.0483085405893338, 0.06458539628644996, 0.07870035802089806, 0.09010718347016895, 0.09882780214140903, 0.10429497337649134, 0.10478369731168462, 0.10637413306942722, 0.10580152863479877, 0.10323330455649138, 0.10103647080071468, 0.09739706318640906, 0.09339853658560969, 0.08907651661277707, 0.08498255779748774, 0.0803288257584917, 0.07549615915911924, 0.0705686914670175, 0.06578755460865689, 0.06152326272389215, 0.05710064438481846, 0.05304717518637044, 0.049192692452519025, 0.04543429533838062, 0.04185006935726582, 0.03856465136636202, 0.03548084046384168, 0.032686301451415156, 0.03000821055022195, 0.027583303423555833, 0.02530480602485186, 0.023252931922224877, 0.021311473951682085, 0.01950488632639154, 0.017816175323608522, 0.01630912551448097, 0.014877542647817266, 0.01354404848349004, 0.012353384148313978, 0.011269621095853593, 0.010268076985004593, 0.009381715191731924, 0.008557449086238548, 0.007802322838769998, 0.007119412229458234, 0.006514022090918958, 0.005928642646674697, 0.005409901240167818, 0.004934821808890345, 0.00449201206017977, 0.004083232262764298, 0.003723591206095289, 0.0033918131102446485, 0.0030914239160330374, 0.0028186544955356952, 0.002563230482574484, 0.0023346778109606787, 0.0021239009104270187, 0.0019339175392495721, 0.0017566546493103811, 0.0015997644828834594, 0.0014561608619313511, 0.0013238568303542148, 0.0012023932111970964, 0.0010966783600659968, 0.0009964193034567588, 0.0009051074893187643, 0.0008226306510573444, 0.0007482982809865885, 0.00067923805973696, 0.0006166751850616193, 0.0005599194615885771, 0.0005081896504894824, 0.000460928294748495, 0.00041823471806426275, 0.0003799815523832328, 0.0003450441792527537, 0.00031352580759028874, 0.00028402285909235567, 0.00025787934286757225, 0.00023384630904230359, 0.00021175319749857437, 0.00019230885451884767, 0.00017436730088699968, 0.00015813740596467643, 0.0001431174395429959, 0.00012931160542251337, 0.00011727758316223229, 0.0001061408030971192, 9.605473585118008e-05, 8.711235927700547e-05, 7.921164117251867e-05, 7.174052762966975e-05, 6.4756980675745e-05, 5.866929240754678e-05, 5.309362582287884e-05, 4.782996706776619e-05, 4.3176438487170775e-05, 3.8987391611757126e-05, 3.549409921430895e-05, 3.208217034111369e-05, 2.895124314903583e-05, 2.6145477641178377e-05, 2.3585811094442426e-05, 2.1434139561848638e-05, 1.9352701987716966e-05, 1.7525395984520438e-05, 1.5877658199476873e-05, 1.4307004828486708e-05, 1.2932200577391626e-05, 1.1783526605468225e-05, 1.0675576551669287e-05, 9.61533904688694e-06, 8.677372214634755e-06, 7.832773975392322e-06, 7.049767119437544e-06, 6.372532066004846e-06, 5.735581623635085e-06, 5.191840788314825e-06, 4.726545299300192e-06, 4.303516347099957e-06, 3.922190435430854e-06, 3.5433279140449227e-06, 3.189407538379743e-06, 2.9357176032210173e-06, 2.645268289054335e-06, 2.3886627571303253e-06, 2.1630451479480758e-06, 1.9547762628551107e-06, 1.7598776326008731e-06, 1.625018284060774e-06, 1.4633673212357439e-06, 1.3262271992749092e-06, 1.213831229658388e-06, 1.108831774444429e-06, 9.994201460404585e-07, 9.151274795149215e-07, 8.283000722959724e-07, 7.632900758495863e-07, 6.874764715326683e-07, 6.187694421336721e-07, 5.581891205608402e-07], "duration": 105545.470302, "accuracy_train": [0.3207130023763381, 0.49131962468853824, 0.5914446780407899, 0.6136240742663345, 0.6558848254660392, 0.6883846380121816, 0.7157247831303064, 0.729230653319952, 0.705846866059893, 0.7585697054378922, 0.7634070964262644, 0.76080112731866, 0.7927632717331118, 0.7903236679240495, 0.8016907616971208, 0.8105012732442783, 0.8292605377906977, 0.8277989384343853, 0.8303544391957364, 0.831332804136674, 0.8367275098629567, 0.8539765091477483, 0.8482824179817275, 0.8591841215047066, 0.8640200705403286, 0.8633461378737541, 0.864368121885382, 0.8714322484080842, 0.8741083324681617, 0.8822209388842747, 0.8808719920865633, 0.8879629749792359, 0.8889853194790514, 0.8961936412767626, 0.8958681204434293, 0.8971447893595422, 0.8970299738718162, 0.9037249604904946, 0.9010760932885751, 0.9001213403239202, 0.905514604097453, 0.9081649132521227, 0.9085601885497416, 0.9144664270141197, 0.9144889575258398, 0.9159072982996493, 0.9186985583356404, 0.9235108953949798, 0.9196286178594499, 0.923950510739664, 0.9251839210732743, 0.9245564913828904, 0.9243937309662238, 0.928577917358804, 0.9288805471922297, 0.9305306818706165, 0.931949022644426, 0.9309964326088963, 0.9331359300018457, 0.9332049634897563, 0.9346930587278516, 0.9338792566445183, 0.9362745204065154, 0.9367631621447029, 0.9367860531446106, 0.9368565285852714, 0.9396456256921374, 0.9384369087993725, 0.9388554355850868, 0.9396449047157622, 0.9405288217515688, 0.9400870434777593, 0.9405989367040422, 0.941040354489664, 0.941273229858804, 0.94136695678756, 0.9418541565729974, 0.9426214556801403, 0.9427845765849945, 0.9432961093230897, 0.9426454281446106, 0.9435751271802326, 0.9434821212278516, 0.9432961093230897, 0.9442723113349022, 0.9441102718946106, 0.9443656777754706, 0.9440397964539498, 0.9437378875968992, 0.9447613135612772, 0.9444822957041344, 0.9445981926564231, 0.945132616394426, 0.9456906521087117, 0.9452259828349945, 0.9446683076088963, 0.9454352462278516, 0.9453654917635659, 0.9444590442160392, 0.9450170799303249, 0.9451329768826136, 0.9461796543350868, 0.9455053611803249, 0.9453197097637505, 0.9454821096922297, 0.9453662127399409, 0.9461796543350868, 0.9456448701088963, 0.9459932819421374, 0.9460866483827058, 0.9455515036683279, 0.9459471394541344, 0.9465051751684201, 0.946249048311185, 0.9457382365494648, 0.9459932819421374, 0.9460397849183279, 0.9455286126684201, 0.9461331513588963, 0.9455754761327981, 0.9462032663113695, 0.9464590326804172, 0.9465051751684201, 0.9465749296327058, 0.9462959117755629, 0.9460168939184201, 0.9468074445136582, 0.9458548544781286, 0.9463191632636582, 0.9456684820851791, 0.9457149850613695, 0.9460641178709857, 0.9466679355850868, 0.9461567633351791, 0.9463889177279439, 0.9465752901208934, 0.9465752901208934, 0.9463191632636582, 0.9466210721207088, 0.9460176148947952, 0.9466679355850868, 0.9463431357281286, 0.9462962722637505, 0.9461571238233666], "end": "2016-01-24 16:01:11.437000", "learning_rate_per_epoch": [0.009021553210914135, 0.008485608734190464, 0.007981503382325172, 0.007507345639169216, 0.007061356212943792, 0.006641861516982317, 0.00624728761613369, 0.00587615417316556, 0.0055270688608288765, 0.005198721773922443, 0.004889880772680044, 0.004599387291818857, 0.004326151218265295, 0.004069147165864706, 0.0038274109829217196, 0.00360003556124866, 0.00338616780936718, 0.0031850053928792477, 0.0029957934748381376, 0.002817822154611349, 0.002650423441082239, 0.0024929693900048733, 0.0023448693100363016, 0.0022055674344301224, 0.0020745410583913326, 0.0019512985600158572, 0.0018353775376453996, 0.0017263430636376143, 0.0016237860545516014, 0.0015273216413334012, 0.0014365878887474537, 0.001351244398392737, 0.001270970911718905, 0.0011954662622883916, 0.00112444709520787, 0.0010576469358056784, 0.0009948151418939233, 0.000935716088861227, 0.0008801278891041875, 0.0008278420427814126, 0.0007786623318679631, 0.0007324042380787432, 0.00068889424437657, 0.0006479690200649202, 0.000609475071541965, 0.0005732679273933172, 0.0005392117309384048, 0.0005071787163615227, 0.000477048713946715, 0.0004487086262088269, 0.00042205213685519993, 0.00039697924512438476, 0.00037339585833251476, 0.00035121350083500147, 0.0003303489356767386, 0.0003107238735537976, 0.0002922646817751229, 0.00027490209322422743, 0.0002585709444247186, 0.00024321000091731548, 0.000228761593461968, 0.00021517153072636575, 0.00020238880824763328, 0.00019036547746509314, 0.00017905641288962215, 0.00016841919568832964, 0.00015841389540582895, 0.00014900298265274614, 0.00014015115448273718, 0.0001318251743214205, 0.00012399382831063122, 0.00011662771430565044, 0.0001096991982194595, 0.00010318229033146054, 9.705252887215465e-05, 9.128692181548104e-05, 8.58638304634951e-05, 8.07629112387076e-05, 7.596502109663561e-05, 7.145215931814164e-05, 6.720739474985749e-05, 6.32148003205657e-05, 5.9459391195559874e-05, 5.592708112089895e-05, 5.2604616939788684e-05, 4.9479527660878375e-05, 4.654009171645157e-05, 4.377527875476517e-05, 4.117471689824015e-05, 3.872864544973709e-05, 3.6427889426704496e-05, 3.426381590543315e-05, 3.222830127924681e-05, 3.031371124961879e-05, 2.8512860808405094e-05, 2.6818994228960946e-05, 2.522575596231036e-05, 2.3727166990283877e-05, 2.231760481663514e-05, 2.0991779820178635e-05, 1.974471888388507e-05, 1.857174174801912e-05, 1.746844827721361e-05, 1.6430698451586068e-05, 1.54545978148235e-05, 1.4536484741256572e-05, 1.3672914064954966e-05, 1.2860646165790968e-05, 1.2096632417524233e-05, 1.1378007002349477e-05, 1.070207235898124e-05, 1.0066293725685682e-05, 9.46828458836535e-06, 8.90580122359097e-06, 8.37673360365443e-06, 7.879096301621757e-06, 7.411022124870215e-06, 6.970754839130677e-06, 6.556642347277375e-06, 6.167131232359679e-06, 5.800759936391842e-06, 5.456153758132132e-06, 5.132019396114629e-06, 4.827140855923062e-06, 4.540374447969953e-06, 4.270643785275752e-06, 4.016937054984737e-06, 3.7783024708915036e-06, 3.55384440808848e-06, 3.3427209018555004e-06, 3.144139554933645e-06, 2.957355263788486e-06, 2.7816672627523076e-06, 2.61641639554e-06, 2.4609826141386293e-06, 2.3147827050706837e-06, 2.177268243030994e-06, 2.0479230897763046e-06, 1.926262029883219e-06, 1.8118283833246096e-06, 1.7041928686012398e-06, 1.6029516700655222e-06, 1.507724959992629e-06, 1.4181554206516012e-06], "accuracy_valid": [0.31879912227033136, 0.4865399096385542, 0.5781882412462349, 0.5906408838478916, 0.6285135659826807, 0.6625726538968373, 0.6928681522966867, 0.7019116505082832, 0.6856351185993976, 0.7279038027108433, 0.7307923098644578, 0.7327057252447289, 0.75341796875, 0.7550151778990963, 0.7572021484375, 0.7685649825865963, 0.7845267789909638, 0.7772937452936747, 0.7816985716302711, 0.7835502164909638, 0.7893081113516567, 0.7950660062123494, 0.7922172086784638, 0.7992575771837349, 0.8033873776355422, 0.801647508000753, 0.799938523625753, 0.8056052334337349, 0.8054625729480422, 0.8091246823230422, 0.8081378247364458, 0.8125838314194277, 0.8158900249435241, 0.8145060711596386, 0.8139266048569277, 0.8126853115587349, 0.8137839443712349, 0.8165812664721386, 0.8136412838855422, 0.8135603939194277, 0.8183108410203314, 0.8182299510542168, 0.8173445736069277, 0.8188094173569277, 0.8178019695971386, 0.8207625423569277, 0.8195315441453314, 0.8206301769578314, 0.8207625423569277, 0.8205286968185241, 0.8216376247176205, 0.8197756847703314, 0.8208743175828314, 0.8204875164721386, 0.8217596950301205, 0.8239466655685241, 0.8234480892319277, 0.8246893825301205, 0.8248114528426205, 0.8241908061935241, 0.8224818218185241, 0.8243437617658133, 0.8255541698042168, 0.8240893260542168, 0.8248217479292168, 0.8272631541792168, 0.8243231715926205, 0.8240687358810241, 0.8248217479292168, 0.8256865352033133, 0.8257983104292168, 0.8265410273908133, 0.8265410273908133, 0.8260630412274097, 0.8248114528426205, 0.8267748729292168, 0.8257983104292168, 0.8274969997176205, 0.8266630977033133, 0.8267542827560241, 0.8268866481551205, 0.8280161662274097, 0.8280058711408133, 0.829369234751506, 0.829125094126506, 0.8291045039533133, 0.8300913615399097, 0.8283617869917168, 0.8291045039533133, 0.828636812876506, 0.8273852244917168, 0.8293589396649097, 0.8284941523908133, 0.8302340220256024, 0.8288603633283133, 0.8284838573042168, 0.8289927287274097, 0.8289721385542168, 0.8281176463667168, 0.8294810099774097, 0.8281279414533133, 0.828880953501506, 0.8277514354292168, 0.8287382930158133, 0.8275175898908133, 0.8292265742658133, 0.8291045039533133, 0.8286162227033133, 0.8276190700301205, 0.8279955760542168, 0.8272734492658133, 0.8282397166792168, 0.8296030802899097, 0.8292265742658133, 0.8281279414533133, 0.8283823771649097, 0.8286059276167168, 0.8278735057417168, 0.8278735057417168, 0.8271410838667168, 0.8281279414533133, 0.8281176463667168, 0.8288603633283133, 0.8281073512801205, 0.8284941523908133, 0.8289927287274097, 0.8286162227033133, 0.8283720820783133, 0.8283617869917168, 0.8270087184676205, 0.8281279414533133, 0.8271410838667168, 0.8292265742658133, 0.8277514354292168, 0.8281176463667168, 0.8288603633283133, 0.8283720820783133, 0.8277514354292168, 0.8292368693524097, 0.8278735057417168, 0.8283617869917168, 0.8282500117658133, 0.8278632106551205, 0.8277514354292168], "accuracy_test": 0.8220925632911392, "start": "2016-01-23 10:42:05.966000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0], "accuracy_train_last": 0.9461571238233666, "batch_size_eval": 1024, "accuracy_train_std": [0.015178383526057012, 0.019578495949992253, 0.020533233001618592, 0.01697410198699054, 0.019306971487897142, 0.015189454524500469, 0.014899703736753783, 0.013861352610282379, 0.011994230454204188, 0.012555266451429162, 0.012524386367678003, 0.014531310274422641, 0.01461715308079766, 0.013192981607060192, 0.014797088795559507, 0.01638170588944857, 0.015346237645307043, 0.014104279525972352, 0.014913025417534477, 0.014023369392499924, 0.012806180107028909, 0.013540563166958033, 0.013607428188514315, 0.014208132770268809, 0.014374276559976114, 0.01356058954478859, 0.01316594346339586, 0.015455685254518578, 0.01403711945885775, 0.01369218942276305, 0.015042083041467228, 0.014081158338856807, 0.013626033118567546, 0.013218595040773615, 0.01335615767071433, 0.01483059589164139, 0.013322710757669174, 0.013539902699115437, 0.012040802329051642, 0.013857595183082024, 0.013122777541740852, 0.012829015819334202, 0.01321734687021358, 0.01207538803414194, 0.012321471035830808, 0.012281904589465412, 0.010889198539054455, 0.011401049173565849, 0.011181502523896741, 0.012556478535850327, 0.01062391356529235, 0.011758581634926596, 0.011573947027987981, 0.010402797126461054, 0.010361779343886025, 0.010109269301596302, 0.010141861431580124, 0.01077665381750018, 0.009427543657777185, 0.010115434791961005, 0.009510856290711214, 0.009779970456398413, 0.00932831421392452, 0.009538691771591955, 0.0094055120446626, 0.009580410303384424, 0.009524265242446346, 0.009348351531935402, 0.009613885432654928, 0.009806607506555615, 0.009728198855936206, 0.010411722542647397, 0.009479832517473195, 0.009381245320187378, 0.009808191695022244, 0.008986335054330651, 0.009190588865160238, 0.009604724105738583, 0.008803010552316588, 0.00860787513361818, 0.00923308672920141, 0.00913280125546814, 0.008905445100289861, 0.00906276041895549, 0.008959237337895755, 0.008623499815291124, 0.008633660266183141, 0.00893425866790226, 0.008919907976966365, 0.008493864159069038, 0.008758442755584524, 0.008640245862215276, 0.009141417539263081, 0.008584350018719256, 0.00848797994382549, 0.00823475173738677, 0.008757457452679333, 0.00861306534085636, 0.008375822192782378, 0.008280716822298695, 0.008419366581269514, 0.008187963072509153, 0.008480141255812137, 0.00831924747472275, 0.008289728752277802, 0.008173136065524887, 0.008132310743143875, 0.008427460242313065, 0.008653454324061576, 0.008397055661079431, 0.008325670861538759, 0.008373572262902494, 0.008071311489946666, 0.008201868239397898, 0.008406161381601027, 0.008386952096561586, 0.008295918765536436, 0.008337811504042057, 0.00808113264074625, 0.008419571069440785, 0.008290618634559512, 0.007992267343588906, 0.00817751585445198, 0.008154517290898257, 0.008510940631859669, 0.008085076991666223, 0.007996728899579074, 0.008175047738718523, 0.00797226426685214, 0.008359302355963145, 0.008349037412222787, 0.00828687708978403, 0.008206253024629278, 0.008474214035727765, 0.008250244376211578, 0.008266916029567776, 0.007875879946175474, 0.008057256941484976, 0.008400475764202777, 0.008037878178094225, 0.008255908419585659, 0.008087326783136117, 0.007791248568334316, 0.008170236378307522], "accuracy_test_std": 0.032246506932246746, "error_valid": [0.6812008777296686, 0.5134600903614458, 0.4218117587537651, 0.4093591161521084, 0.3714864340173193, 0.3374273461031627, 0.30713184770331325, 0.2980883494917168, 0.31436488140060237, 0.2720961972891567, 0.26920769013554224, 0.2672942747552711, 0.24658203125, 0.24498482210090367, 0.2427978515625, 0.23143501741340367, 0.2154732210090362, 0.22270625470632532, 0.21830142836972888, 0.2164497835090362, 0.21069188864834332, 0.20493399378765065, 0.2077827913215362, 0.2007424228162651, 0.19661262236445776, 0.19835249199924698, 0.20006147637424698, 0.1943947665662651, 0.19453742705195776, 0.19087531767695776, 0.1918621752635542, 0.1874161685805723, 0.18410997505647586, 0.18549392884036142, 0.1860733951430723, 0.1873146884412651, 0.1862160556287651, 0.18341873352786142, 0.18635871611445776, 0.1864396060805723, 0.18168915897966864, 0.1817700489457832, 0.1826554263930723, 0.1811905826430723, 0.18219803040286142, 0.1792374576430723, 0.18046845585466864, 0.17936982304216864, 0.1792374576430723, 0.17947130318147586, 0.17836237528237953, 0.18022431522966864, 0.17912568241716864, 0.17951248352786142, 0.17824030496987953, 0.17605333443147586, 0.1765519107680723, 0.17531061746987953, 0.17518854715737953, 0.17580919380647586, 0.17751817818147586, 0.17565623823418675, 0.1744458301957832, 0.1759106739457832, 0.1751782520707832, 0.1727368458207832, 0.17567682840737953, 0.17593126411897586, 0.1751782520707832, 0.17431346479668675, 0.1742016895707832, 0.17345897260918675, 0.17345897260918675, 0.1739369587725903, 0.17518854715737953, 0.1732251270707832, 0.1742016895707832, 0.17250300028237953, 0.17333690229668675, 0.17324571724397586, 0.17311335184487953, 0.1719838337725903, 0.17199412885918675, 0.17063076524849397, 0.17087490587349397, 0.17089549604668675, 0.1699086384600903, 0.1716382130082832, 0.17089549604668675, 0.17136318712349397, 0.1726147755082832, 0.1706410603350903, 0.17150584760918675, 0.16976597797439763, 0.17113963667168675, 0.1715161426957832, 0.1710072712725903, 0.1710278614457832, 0.1718823536332832, 0.1705189900225903, 0.17187205854668675, 0.17111904649849397, 0.1722485645707832, 0.17126170698418675, 0.17248241010918675, 0.17077342573418675, 0.17089549604668675, 0.17138377729668675, 0.17238092996987953, 0.1720044239457832, 0.17272655073418675, 0.1717602833207832, 0.1703969197100903, 0.17077342573418675, 0.17187205854668675, 0.1716176228350903, 0.1713940723832832, 0.1721264942582832, 0.1721264942582832, 0.1728589161332832, 0.17187205854668675, 0.1718823536332832, 0.17113963667168675, 0.17189264871987953, 0.17150584760918675, 0.1710072712725903, 0.17138377729668675, 0.17162791792168675, 0.1716382130082832, 0.17299128153237953, 0.17187205854668675, 0.1728589161332832, 0.17077342573418675, 0.1722485645707832, 0.1718823536332832, 0.17113963667168675, 0.17162791792168675, 0.1722485645707832, 0.1707631306475903, 0.1721264942582832, 0.1716382130082832, 0.17174998823418675, 0.17213678934487953, 0.1722485645707832], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-3, 3], "momentum": 0.8389241234856384, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.00959134718371819, "patience_threshold": 1, "do_flip": true, "batch_size": 128, "optimization": "nesterov_momentum", "nb_data_augmentation": 2, "learning_rate_decay_method": "exp", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 3.0112198265842246e-08, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.05940709905803807}, "accuracy_valid_max": 0.8302340220256024, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import os\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-3, 3)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8277514354292168, "loss_train": [1.6150470972061157, 1.298658013343811, 1.1403372287750244, 1.0337291955947876, 0.9555742740631104, 0.8951177597045898, 0.8456671833992004, 0.8014059066772461, 0.7636467814445496, 0.7291586995124817, 0.697763979434967, 0.6704434752464294, 0.6486190557479858, 0.6251131892204285, 0.6050304174423218, 0.5850210189819336, 0.5669984817504883, 0.5519340634346008, 0.5367721319198608, 0.5217346549034119, 0.508914053440094, 0.4976803958415985, 0.48607391119003296, 0.4743211567401886, 0.46359673142433167, 0.45496228337287903, 0.4450099468231201, 0.43654438853263855, 0.4294469356536865, 0.4207841455936432, 0.41398653388023376, 0.4036175608634949, 0.3993454575538635, 0.39452019333839417, 0.3858182430267334, 0.38317161798477173, 0.3757439851760864, 0.37159934639930725, 0.3644435405731201, 0.3633864223957062, 0.3567749559879303, 0.35318613052368164, 0.350327730178833, 0.34468820691108704, 0.3432120382785797, 0.33872532844543457, 0.3363524377346039, 0.33360496163368225, 0.3307979702949524, 0.32514750957489014, 0.32401755452156067, 0.32098081707954407, 0.3205964267253876, 0.3170830011367798, 0.31405946612358093, 0.31394317746162415, 0.31191879510879517, 0.31039977073669434, 0.30637073516845703, 0.30664539337158203, 0.3041823208332062, 0.30170097947120667, 0.3022688329219818, 0.2997235953807831, 0.2986554801464081, 0.2980159521102905, 0.2982429265975952, 0.2950887084007263, 0.29539066553115845, 0.29356104135513306, 0.2926463186740875, 0.2924254238605499, 0.29278069734573364, 0.29150956869125366, 0.29002436995506287, 0.29007843136787415, 0.28877586126327515, 0.28841331601142883, 0.28939786553382874, 0.28746098279953003, 0.2871725261211395, 0.2854791581630707, 0.28534141182899475, 0.28609445691108704, 0.2849569618701935, 0.2858625650405884, 0.2832161784172058, 0.285169392824173, 0.28359001874923706, 0.2843427062034607, 0.28250592947006226, 0.28254300355911255, 0.28214889764785767, 0.28162893652915955, 0.28192800283432007, 0.2809150516986847, 0.28316107392311096, 0.2807437479496002, 0.2815895974636078, 0.27993887662887573, 0.2813933789730072, 0.2797810435295105, 0.2815496623516083, 0.27976325154304504, 0.28081417083740234, 0.28189224004745483, 0.2797303795814514, 0.2798702120780945, 0.27935290336608887, 0.2806425094604492, 0.27923718094825745, 0.28047850728034973, 0.278134286403656, 0.27935755252838135, 0.27873024344444275, 0.2777547240257263, 0.2796710133552551, 0.28110799193382263, 0.27892225980758667, 0.27748459577560425, 0.2795000374317169, 0.27683237195014954, 0.2784653902053833, 0.27907469868659973, 0.2791844606399536, 0.2771911025047302, 0.27790728211402893, 0.27786538004875183, 0.2777690291404724, 0.27922752499580383, 0.27881667017936707, 0.2785506248474121, 0.2799091041088104, 0.27773305773735046, 0.27884143590927124, 0.277660071849823, 0.2770937979221344, 0.27642908692359924, 0.27844464778900146, 0.2778943181037903, 0.2756897807121277, 0.27961111068725586, 0.2783236503601074, 0.2781658172607422], "accuracy_train_first": 0.3207130023763381, "model": "residualv3", "loss_std": [0.24018366634845734, 0.11708982288837433, 0.10410561412572861, 0.10048175603151321, 0.09834098815917969, 0.09708751738071442, 0.09460971504449844, 0.09121589362621307, 0.08791990578174591, 0.08685117959976196, 0.08725975453853607, 0.08716965466737747, 0.08676808327436447, 0.08169887959957123, 0.08162514120340347, 0.08174961060285568, 0.08046358823776245, 0.08009538799524307, 0.08001089096069336, 0.07801471650600433, 0.07614551484584808, 0.07684556394815445, 0.07703088223934174, 0.07458247989416122, 0.07497934997081757, 0.07321622967720032, 0.0727759525179863, 0.07215975224971771, 0.07169346511363983, 0.07172892987728119, 0.06949734687805176, 0.06976138800382614, 0.0665326863527298, 0.06678616255521774, 0.06644046306610107, 0.06706094741821289, 0.06541840732097626, 0.06352034211158752, 0.06353103369474411, 0.06423329561948776, 0.06490195542573929, 0.06214866787195206, 0.06191081553697586, 0.060075342655181885, 0.06122489646077156, 0.06006474420428276, 0.0598621666431427, 0.05976567417383194, 0.05971749499440193, 0.060404498130083084, 0.05753157660365105, 0.05935875326395035, 0.05764961615204811, 0.05961214751005173, 0.05895301327109337, 0.057542264461517334, 0.05826982855796814, 0.05729708448052406, 0.05584212392568588, 0.05723864212632179, 0.05731220170855522, 0.05807735025882721, 0.0568612664937973, 0.055589206516742706, 0.05642281472682953, 0.05693536251783371, 0.05539611726999283, 0.056817926466464996, 0.05482492968440056, 0.05562996119260788, 0.05602129548788071, 0.0552767813205719, 0.05684299394488335, 0.054849669337272644, 0.05479196086525917, 0.0538652203977108, 0.054909221827983856, 0.05481412634253502, 0.05399494618177414, 0.054454006254673004, 0.05427314341068268, 0.05502793565392494, 0.05377392843365669, 0.05445333570241928, 0.053515829145908356, 0.054553985595703125, 0.05229847505688667, 0.05296832323074341, 0.05160515382885933, 0.05327148735523224, 0.05393187701702118, 0.05324508622288704, 0.05270015820860863, 0.05349346995353699, 0.056542087346315384, 0.05353018641471863, 0.055247124284505844, 0.05384184047579765, 0.053261470049619675, 0.05358356982469559, 0.05285685881972313, 0.052899837493896484, 0.053019557148218155, 0.0530155673623085, 0.054077986627817154, 0.05356467515230179, 0.05352269113063812, 0.055104535073041916, 0.05166452378034592, 0.053153201937675476, 0.052979446947574615, 0.05309451371431351, 0.05316188186407089, 0.05334543064236641, 0.053537990897893906, 0.05149240419268608, 0.053498852998018265, 0.05350296571850777, 0.05250570923089981, 0.05392275005578995, 0.054079215973615646, 0.05283728241920471, 0.05199152231216431, 0.052395544946193695, 0.05377154052257538, 0.053866416215896606, 0.054241422563791275, 0.05328100919723511, 0.052477724850177765, 0.052009422332048416, 0.053389232605695724, 0.052192483097314835, 0.052209850400686264, 0.0519588366150856, 0.05184421315789223, 0.052948202937841415, 0.05277149751782417, 0.053290292620658875, 0.052296433597803116, 0.05477188527584076, 0.05299464985728264, 0.05217226222157478, 0.0538475438952446, 0.0529998317360878]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:11 2016", "state": "available"}], "summary": "a93f2796c6e647ae250f47f56bcf72cc"}