{"content": {"hp_model": {"f0": 16, "f1": 64, "f2": 64, "f3": 16, "nonlin": "very_leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.5274804830551147, 1.1878265142440796, 1.0130585432052612, 0.8857372403144836, 0.7935327291488647, 0.7229564189910889, 0.6665832996368408, 0.6190438866615295, 0.5795556902885437, 0.5443105101585388, 0.5111070871353149, 0.4836491048336029, 0.4586467742919922, 0.435457319021225, 0.41212525963783264, 0.3927886486053467, 0.3723032474517822, 0.3545142114162445, 0.3392118215560913, 0.32309556007385254, 0.3084588944911957, 0.2932620346546173, 0.2785540521144867, 0.2697218358516693, 0.25678735971450806, 0.24557732045650482, 0.23564167320728302, 0.2246982455253601, 0.2129780650138855, 0.20661672949790955, 0.19712118804454803, 0.18885605037212372, 0.1816326230764389, 0.1752035766839981, 0.1648067981004715, 0.15994760394096375, 0.1534162312746048, 0.14725594222545624, 0.1413070261478424, 0.13366803526878357, 0.1308605968952179, 0.12488309293985367, 0.12082065641880035, 0.11661040037870407, 0.11239387840032578, 0.11003261059522629, 0.10298225283622742, 0.10077233612537384, 0.09686313569545746, 0.09497908502817154, 0.09024480730295181, 0.08640320599079132, 0.08485788851976395, 0.08135182410478592, 0.08184954524040222, 0.07737663388252258, 0.07578834146261215, 0.07321124523878098, 0.07116565108299255, 0.07032924890518188, 0.06749431043863297, 0.06537728011608124, 0.06446809321641922, 0.06263467669487, 0.06152579188346863, 0.06054047867655754, 0.05851571261882782, 0.05810651183128357, 0.05722235515713692, 0.05627979710698128, 0.055588629096746445, 0.05355174094438553, 0.05366510897874832, 0.05214296653866768, 0.052461910992860794, 0.05172144994139671, 0.04977836087346077, 0.0499681755900383, 0.04932837560772896, 0.04782479256391525, 0.0466751791536808, 0.04591795429587364, 0.04543234407901764, 0.045988358557224274, 0.0450320839881897, 0.04480830579996109, 0.04432150721549988, 0.044088974595069885, 0.042910464107990265, 0.043048251420259476, 0.04311458021402359, 0.04198795184493065, 0.042645033448934555, 0.04125804454088211, 0.04165203869342804, 0.04155794531106949, 0.041584257036447525, 0.04183186963200569, 0.04036137834191322, 0.039827898144721985, 0.04026671126484871, 0.040227293968200684, 0.039011575281620026, 0.03919266536831856, 0.03970800340175629, 0.039067331701517105, 0.038662075996398926, 0.038636889308691025, 0.038508135825395584, 0.038593851029872894, 0.03895438089966774], "moving_avg_accuracy_train": [0.05043321667935584, 0.10216049402743169, 0.15538674272246444, 0.21014865431562035, 0.26436994469570463, 0.3140986248293087, 0.36225555287678574, 0.4081074441503382, 0.4509761738262974, 0.49073666468817667, 0.5276510566876591, 0.5618618011942347, 0.5927862577834675, 0.6216483817340336, 0.647863495226374, 0.672126632080167, 0.6943096500794685, 0.7155717993145542, 0.7348961427773402, 0.7526366079223635, 0.7688844416564745, 0.7845513756861944, 0.7990469997570175, 0.8121697913314727, 0.8243731457508355, 0.8358513493270532, 0.8464467634611161, 0.8563336615543733, 0.8655062373978286, 0.8739476396593377, 0.8818190169124885, 0.8890777507474947, 0.8957386025299804, 0.9020309160841989, 0.9081496914008437, 0.9136822739691849, 0.9188032521139792, 0.9235913491467211, 0.928081817839238, 0.932276807630388, 0.9360871756745659, 0.9397629005904979, 0.943308110046952, 0.9466337292863506, 0.9496104384625051, 0.9522709836770243, 0.9548212593403297, 0.9575000127956009, 0.9599597550791638, 0.9619597536391691, 0.9640363729538698, 0.9662190812311111, 0.9679859891782749, 0.9697040895152462, 0.9712224059351686, 0.9728284531381173, 0.9742063942076573, 0.9755535340643002, 0.9769030716174033, 0.9780596708902329, 0.9791888577440853, 0.980181874424457, 0.98118716053083, 0.9820640522896702, 0.982957886569055, 0.9836693675169391, 0.9843701181902637, 0.9850798488557796, 0.9857697597285534, 0.9863743674235644, 0.9870069781502925, 0.9876087717412249, 0.9880574160695017, 0.9884681353625608, 0.988984267101314, 0.989320902481668, 0.9896052731335105, 0.9899658744654161, 0.9903299071450742, 0.9906342850686712, 0.9909338018368133, 0.9912219681186174, 0.9914581023329646, 0.991744991838963, 0.9919660260622281, 0.9921927865512433, 0.9923713304032711, 0.9925575965070008, 0.9927415480908429, 0.9929186942115298, 0.9930734754225289, 0.9932011527683804, 0.9933695768510846, 0.993521086427881, 0.9936713959398548, 0.9937486178780306, 0.9938297433664365, 0.9938933836131261, 0.9939994879601468, 0.9940532012915315, 0.9941410347707209, 0.9942108564043909, 0.9942829243722944, 0.9943849879243599, 0.9944651833283525, 0.9945536352336125, 0.9946541682876322, 0.9947376725898214, 0.9947710458808576, 0.9948057321404093, 0.9948346246251965], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.05077213149472891, 0.10292162791792167, 0.1560311676393072, 0.21008416556852405, 0.2633870077183734, 0.31150090248493967, 0.3576488226374246, 0.40138436394295923, 0.4421207451785579, 0.479234118938187, 0.5138396509449707, 0.5455360051746454, 0.5739681562573314, 0.6004421755337067, 0.6239594990665559, 0.6451759773884395, 0.6651141225430444, 0.6838366146562098, 0.7005546392316281, 0.7157249906793237, 0.7296315661125208, 0.7428637278841753, 0.7548662116940258, 0.7653928329474395, 0.7750122469418522, 0.7835120576392333, 0.7915514827582166, 0.7991684718600306, 0.8062170155343439, 0.8126757156657739, 0.818461043195582, 0.823717695606069, 0.8281424774855977, 0.8324198089444927, 0.8364606017228597, 0.8401726164282093, 0.8435043111577528, 0.8466462322633631, 0.8492908557896623, 0.8521104700883317, 0.8544517809484744, 0.856336145634124, 0.8585122661045972, 0.8606650575193633, 0.8624835880061318, 0.8642189512028831, 0.8655589925001399, 0.8669176910946289, 0.8680977216839612, 0.8689400226518602, 0.8700897480316291, 0.8715660130157402, 0.8726138897826903, 0.8737278773104454, 0.8742746878945364, 0.8751228803437876, 0.8756787340168637, 0.876426231473611, 0.877110156707274, 0.8774774307579322, 0.8785099552367323, 0.8790455137416734, 0.8797137103908494, 0.8801512484255597, 0.8806569549553682, 0.8811304749154037, 0.8813541238188182, 0.8816307090367104, 0.8818684582102231, 0.8820061017529055, 0.8822073411634583, 0.8823965456295674, 0.8826044802514751, 0.882779414379942, 0.8830253928402911, 0.8829974852863072, 0.8831606214997698, 0.8832544979322476, 0.8833410457387969, 0.8836020442334412, 0.8834697024324616, 0.8836365040736281, 0.8834874797487201, 0.8836992727659414, 0.8839020935126907, 0.8840836026761053, 0.8842469609231784, 0.8844428114705443, 0.884361699798264, 0.8844871003191905, 0.8844239148072264, 0.8843202787387778, 0.8844742354281531, 0.8845873528774311, 0.884702395121691, 0.8847296024280461, 0.8849392534898348, 0.8851655900478543, 0.8852706071914123, 0.8852409932907952, 0.8854961320076494, 0.885418522054249, 0.8854982459971674, 0.8853960105823151, 0.8854525421012673, 0.8855644556245743, 0.8857760705854603, 0.8859543170190076, 0.8859550178942905, 0.8860797780118645, 0.8861788255777714], "moving_var_accuracy_train": [0.02289158410164171, 0.044683826688080394, 0.06571274597058165, 0.0861312740255531, 0.1039776815973306, 0.11583638808807037, 0.12512455674999226, 0.13153366447524817, 0.13491984988399752, 0.13565593459779607, 0.13435439216823955, 0.1314523283086633, 0.1269139936158511, 0.12171979404470659, 0.11573290421898255, 0.10945791208695729, 0.10294089746627758, 0.0967155186305053, 0.09040483901986425, 0.08419687204993397, 0.07815311375440241, 0.0725468777759865, 0.06718329805321151, 0.06201483717624967, 0.05715365019038596, 0.052624027587381224, 0.04837199003469379, 0.04441454781638248, 0.04073031836317968, 0.03729860197612722, 0.03412636899726727, 0.031187935049527722, 0.02846844406278913, 0.025977938545291655, 0.023717099393142715, 0.021620874682708065, 0.019694806968872408, 0.01793165913073974, 0.016319971999372036, 0.014846356253565509, 0.013492390769897786, 0.012264750275826446, 0.011151391839054985, 0.010135790345078608, 0.009201958488245371, 0.00834546914696734, 0.0075694573859002336, 0.006877093127977357, 0.0062438368040935435, 0.005655453071844402, 0.005128718894663653, 0.004658724944009127, 0.004220950122851974, 0.0038254219294778856, 0.003463627299289151, 0.003140479057923128, 0.0028435196464509423, 0.002575500753946051, 0.0023343419430165635, 0.0021129472456160973, 0.0019131280876127036, 0.0017306900179989038, 0.0015667164176000137, 0.0014169652282505095, 0.001282459162896488, 0.0011587690928596577, 0.001047311647129175, 0.0009471139409744204, 0.0008566863399883225, 0.0007743076601732891, 0.0007004786611401032, 0.0006336901947608783, 0.0005721327108844461, 0.0005164376528352198, 0.00046719141529743294, 0.00042149218418144444, 0.00038007076577196357, 0.0003432339890799154, 0.0003101032682986553, 0.0002799267547521491, 0.0002527414719265188, 0.00022821468298758605, 0.00020589504899349575, 0.00018604629439201466, 0.0001678813701035025, 0.00015155601596755831, 0.00013668731553467467, 0.00012333083953379513, 0.00011130229924719809, 0.00010045449605514787, 9.062466145913825e-05, 8.170890885501779e-05, 7.379331801422887e-05, 6.662058257955495e-05, 6.0161860866107704e-05, 5.419934382911764e-05, 4.88386415500277e-05, 4.399122812401345e-05, 3.969342850372237e-05, 3.575005175106615e-05, 3.2244479056558194e-05, 2.9063906695657356e-05, 2.620426015407125e-05, 2.3677586856606168e-05, 2.136770989633934e-05, 1.9301352562602492e-05, 1.7462179360896998e-05, 1.5778718141164188e-05, 1.4210870316039127e-05, 1.2800611513850433e-05, 1.1528063343559915e-05], "duration": 134103.540057, "accuracy_train": [0.5043321667935585, 0.5677059901601145, 0.6344229809777593, 0.7030058586540237, 0.7523615581164635, 0.761656746031746, 0.795667905304079, 0.8207744656123109, 0.8367947409099299, 0.8485810824450905, 0.8598805846830011, 0.8697585017534146, 0.8711063670865633, 0.8814074972891289, 0.8837995166574382, 0.8904948637643041, 0.893956812073182, 0.9069311424303249, 0.9088152339424143, 0.9123007942275747, 0.9151149452634736, 0.9255537819536729, 0.929507616394426, 0.9302749155015688, 0.9342033355251015, 0.9391551815130121, 0.9418054906676817, 0.9453157443936876, 0.9480594199889257, 0.9499202600129198, 0.9526614121908453, 0.9544063552625508, 0.9556862685723514, 0.9586617380721669, 0.963218669250646, 0.9634755170842562, 0.9648920554171282, 0.966684222441399, 0.96849603607189, 0.9700317157507383, 0.9703804880721669, 0.9728444248338871, 0.9752149951550388, 0.9765643024409376, 0.9764008210478959, 0.9762158906076966, 0.9777737403100776, 0.9816087938930418, 0.9820974356312293, 0.9799597406792175, 0.9827259467861758, 0.9858634557262828, 0.9838881607027501, 0.9851669925479882, 0.9848872537144703, 0.9872828779646549, 0.986607863833518, 0.9876777927740864, 0.9890489095953304, 0.9884690643456996, 0.989351539428756, 0.9891190245478036, 0.9902347354881875, 0.9899560781192323, 0.991002395083518, 0.9900726960478959, 0.9906768742501846, 0.9914674248454227, 0.991978957583518, 0.9918158366786637, 0.9927004746908453, 0.9930249140596161, 0.9920952150239941, 0.9921646090000923, 0.9936294527500923, 0.9923506209048542, 0.9921646090000923, 0.9932112864525655, 0.9936062012619971, 0.9933736863810447, 0.9936294527500923, 0.9938154646548542, 0.9935833102620893, 0.9943269973929494, 0.9939553340716132, 0.9942336309523809, 0.9939782250715209, 0.9942339914405685, 0.9943971123454227, 0.9945130092977114, 0.9944665063215209, 0.9943502488810447, 0.9948853935954227, 0.9948846726190477, 0.9950241815476191, 0.9944436153216132, 0.9945598727620893, 0.9944661458333334, 0.9949544270833334, 0.9945366212739941, 0.9949315360834257, 0.9948392511074198, 0.9949315360834257, 0.9953035598929494, 0.9951869419642857, 0.9953497023809523, 0.9955589657738095, 0.9954892113095238, 0.9950714055001846, 0.995117908476375, 0.9950946569882798], "end": "2016-01-31 04:58:34.277000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0], "moving_var_accuracy_valid": [0.023200284028662393, 0.04535638542052951, 0.06620635576323342, 0.08588125945313338, 0.10286387033908591, 0.11341200513165124, 0.12123747942808727, 0.1263289096448717, 0.12863109348593285, 0.12816460674373695, 0.12612603167861602, 0.12255535835383152, 0.11757530745514655, 0.11212563997944491, 0.10589065653683875, 0.09935284145460148, 0.09299532399899617, 0.08685057699744443, 0.08068095040903857, 0.07468411143555412, 0.06895623585451152, 0.0636364232154215, 0.05856931745233087, 0.05370967350221317, 0.04917150428235498, 0.0449045748911413, 0.04099580860822083, 0.037418394454193114, 0.03412369272013211, 0.031086756714608515, 0.028279311174792068, 0.025700071608394968, 0.023306272699688116, 0.021140305509402584, 0.01917322701496165, 0.017379915792020067, 0.015741825920755872, 0.014256488342785197, 0.012893785810869376, 0.011675959252921782, 0.010557698956524005, 0.009533886533288311, 0.00862311738267759, 0.007802516242289251, 0.007052028096242084, 0.006373928655439625, 0.005752697186000848, 0.005194042024236761, 0.004687170071538924, 0.004224838302669745, 0.003814251288442731, 0.0034524403843282734, 0.0031170787573638697, 0.0028165395955354272, 0.0025375766523157497, 0.0022902938609628753, 0.002064045234619437, 0.0018626694831880876, 0.0016806123183964478, 0.0015137650986113847, 0.0013719835499441402, 0.0012373666011596604, 0.0011176483219014252, 0.0010076064454976456, 0.0009091474527964995, 0.0008202506978898175, 0.000738675797588822, 0.0006654967122747484, 0.0005994557630728273, 0.0005396806984691243, 0.0004860771043254487, 0.0004377915788628641, 0.0003944015522394689, 0.0003552368145592444, 0.00032025768172992176, 0.0002882389230410539, 0.00025965455155423496, 0.00023376841145998436, 0.00021045898501935244, 0.00019002616844527703, 0.0001711811807713282, 0.0001543134677816579, 0.00013908199524822066, 0.00012557750226269216, 0.00011338997833423019, 0.00010234749068843859, 9.235291487157619e-05, 8.346284031655022e-05, 7.517576821531621e-05, 6.779971900962261e-05, 6.105567878896e-05, 5.504677482221524e-05, 4.975542129982416e-05, 4.4895039185822335e-05, 4.052464772891931e-05, 3.647884509369923e-05, 3.3226542693711716e-05, 3.0364942561805725e-05, 2.7427705709594772e-05, 2.469282798662315e-05, 2.2809407071502935e-05, 2.058267610815407e-05, 1.8581611661008878e-05, 1.681751921535815e-05, 1.5164529607537739e-05, 1.3760798377074834e-05, 1.2787746564404177e-05, 1.1794918027614896e-05, 1.0615430645888867e-05, 9.693973363733596e-06, 8.812869810169011e-06], "accuracy_test": 0.8493203922193878, "start": "2016-01-29 15:43:30.737000", "learning_rate_per_epoch": [0.003488607471808791, 0.0033463588915765285, 0.003209910588338971, 0.0030790260061621666, 0.0029534781351685524, 0.002833049511536956, 0.002717531519010663, 0.002606723690405488, 0.00250043417327106, 0.002398478565737605, 0.00230068014934659, 0.0022068696562200785, 0.0021168841049075127, 0.002030567731708288, 0.0019477710593491793, 0.0018683504313230515, 0.0017921681283041835, 0.0017190921353176236, 0.001648995908908546, 0.0015817577950656414, 0.001517261378467083, 0.001455394783988595, 0.0013960507931187749, 0.0013391266111284494, 0.0012845235178247094, 0.0012321468675509095, 0.0011819058563560247, 0.0011337134055793285, 0.001087486045435071, 0.0010431436821818352, 0.0010006093652918935, 0.0009598093456588686, 0.0009206729591824114, 0.0008831323939375579, 0.0008471225155517459, 0.000812580983620137, 0.0007794478442519903, 0.0007476657629013062, 0.0007171795587055385, 0.0006879364373162389, 0.0006598856998607516, 0.000632978742942214, 0.0006071689422242343, 0.0005824115360155702, 0.0005586636252701283, 0.0005358839989639819, 0.0005140332505106926, 0.0004930734867230058, 0.0004729683278128505, 0.00045368296559900045, 0.0004351839597802609, 0.000417439267039299, 0.00040041812462732196, 0.00038409102126024663, 0.00036842963891103864, 0.0003534068528097123, 0.0003389966441318393, 0.00032517401268705726, 0.0003119149769190699, 0.00029919660300947726, 0.0002869968011509627, 0.0002752944710664451, 0.00026406929828226566, 0.00025330184143967927, 0.0002429734158795327, 0.0002330661372980103, 0.00022356283443514258, 0.00021444701997097582, 0.00020570290507748723, 0.0001973153412109241, 0.00018926977645605803, 0.00018155227007810026, 0.00017414944886695594, 0.00016704847803339362, 0.00016023704665713012, 0.00015370335313491523, 0.00014743607607670128, 0.00014142434520181268, 0.00013565774133894593, 0.00013012628187425435, 0.00012482036254368722, 0.00011973079381277785, 0.00011484874994494021, 0.00011016577627742663, 0.00010567375284153968, 0.00010136488708667457, 9.723172115627676e-05, 9.326708095613867e-05, 8.94640979822725e-05, 8.581618749303743e-05, 8.231701940530911e-05, 7.896053284639493e-05, 7.574090705020353e-05, 7.265256135724485e-05, 6.969014066271484e-05, 6.68485154164955e-05, 6.412275979528204e-05, 6.150814442662522e-05, 5.900014002691023e-05, 5.659439921146259e-05, 5.4286752856569365e-05, 5.207320282352157e-05, 4.994991104467772e-05, 4.791319588548504e-05, 4.5959528506500646e-05, 4.408552194945514e-05, 4.228792749927379e-05, 4.0563631046097726e-05, 3.8909642171347514e-05, 3.732309414772317e-05, 3.5801240301225334e-05], "accuracy_train_first": 0.5043321667935585, "accuracy_train_last": 0.9950946569882798, "batch_size_eval": 1024, "accuracy_train_std": [0.01871241577351695, 0.017056860445547956, 0.019074944299568113, 0.01595041236652786, 0.016073392954647638, 0.01754554819777774, 0.016829506733373998, 0.016748205052985604, 0.016440503715279775, 0.017850355106419545, 0.016152154743585008, 0.0164810055162949, 0.017535169815758696, 0.01453731372071005, 0.0164759739833043, 0.015584102872365122, 0.017579284144180456, 0.015926414537147833, 0.016463852432820947, 0.016428564121473782, 0.014656769904707649, 0.01455092606931622, 0.013740358000049122, 0.013894198498277222, 0.01350733252774465, 0.013365563668646946, 0.013028071795712297, 0.012827536287678434, 0.012312431679103923, 0.012244259402725744, 0.01175502454732763, 0.011489894019452555, 0.011348430095334245, 0.010992790810073233, 0.010222837024883538, 0.008880992615871849, 0.009841979974768834, 0.008031491493315822, 0.008663612035127827, 0.008317362636371756, 0.007661214105289817, 0.007650124377659248, 0.007803353545738232, 0.006198665716474364, 0.006728004625692553, 0.007261090650862796, 0.0065745591236207055, 0.0059071445466450335, 0.006024616025517939, 0.005843666374971063, 0.005374631838568548, 0.00540136912493311, 0.005028423215694348, 0.004381262486352234, 0.004945626547656345, 0.0041090930908670555, 0.004927195062755354, 0.004347838599805596, 0.004201319989838472, 0.004301212919825627, 0.003975530154372734, 0.004059009557775917, 0.0037528239218773377, 0.004338531513925205, 0.0035061687347564723, 0.00361718489505461, 0.003371986008417353, 0.003226296298787131, 0.003136798631496234, 0.003066404974649649, 0.00282880216752093, 0.0025273316760361085, 0.0031169750332383775, 0.0032847082547726196, 0.00284561561653859, 0.00329454852940729, 0.002995460867968552, 0.002867364739148297, 0.0028293280086257137, 0.002444752438350646, 0.0027892000283748406, 0.0023608738546635586, 0.002705851614559414, 0.0021128133635035786, 0.0023080829306294792, 0.002074991770584366, 0.002601096097172608, 0.0024191712560728174, 0.0022362041223878888, 0.0023345145998976587, 0.0020958330902380615, 0.002210474361277662, 0.0018802482620306946, 0.002212681780760848, 0.0020082610567047233, 0.0019824457265148632, 0.001896405458434008, 0.002169751329480693, 0.0019752819973630404, 0.0018468758804237372, 0.0018367446601042252, 0.002001965007112253, 0.0020692733507542264, 0.0018854747351231764, 0.0019107291437920162, 0.0020749917705843663, 0.0016941705014831539, 0.0019756925024412204, 0.0021730836423644287, 0.001783351860146397, 0.002104446715125261], "accuracy_test_std": 0.004993596057134571, "error_valid": [0.4922786850527108, 0.4277329042733433, 0.36598297486822284, 0.30343885306852414, 0.2568874129329819, 0.2554740446159638, 0.22701989599021077, 0.20499576430722888, 0.1912518237010542, 0.18674551722515065, 0.17471056099397586, 0.1691968067582832, 0.17014248399849397, 0.16129165097891573, 0.16438458913780118, 0.1638757177146084, 0.15544257106551207, 0.14766095632530118, 0.1489831395896084, 0.14774184629141573, 0.14520925498870485, 0.13804681617093373, 0.1371114340173193, 0.13986757577183728, 0.13841302710843373, 0.13998964608433728, 0.13609369117093373, 0.1322786262236446, 0.13034609139683728, 0.1291959831513554, 0.1294710090361446, 0.12897243269954817, 0.1320344855986446, 0.12908420792545183, 0.12717226327183728, 0.1264192512236446, 0.1265104362763554, 0.1250764777861446, 0.1269075324736446, 0.12251300122364461, 0.12447642131024095, 0.12670457219503017, 0.12190264966114461, 0.11995981974774095, 0.12114963761295183, 0.12016278002635539, 0.12238063582454817, 0.12085402155496983, 0.12128200301204817, 0.12347926863704817, 0.11956272355045183, 0.11514760212725905, 0.11795521931475905, 0.11624623493975905, 0.12080401684864461, 0.11724338761295183, 0.11931858292545183, 0.11684629141566272, 0.11673451618975905, 0.11921710278614461, 0.11219732445406627, 0.11613445971385539, 0.11427251976656627, 0.11591090926204817, 0.11479168627635539, 0.11460784544427716, 0.11663303605045183, 0.11588002400225905, 0.11599179922816272, 0.11675510636295183, 0.11598150414156627, 0.11590061417545183, 0.11552410815135539, 0.11564617846385539, 0.11476080101656627, 0.11725368269954817, 0.11537115257906627, 0.11590061417545183, 0.11588002400225905, 0.11404896931475905, 0.11772137377635539, 0.11486228115587349, 0.11785373917545183, 0.11439459007906627, 0.11427251976656627, 0.11428281485316272, 0.11428281485316272, 0.11379453360316272, 0.11636830525225905, 0.11438429499246983, 0.11614475480045183, 0.11661244587725905, 0.11414015436746983, 0.11439459007906627, 0.11426222467996983, 0.11502553181475905, 0.11317388695406627, 0.11279738092996983, 0.11378423851656627, 0.11502553181475905, 0.11220761954066272, 0.11527996752635539, 0.11378423851656627, 0.11552410815135539, 0.11403867422816272, 0.11342832266566272, 0.11231939476656627, 0.11244146507906627, 0.11403867422816272, 0.11279738092996983, 0.11292974632906627], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.04077515863683258, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 32, "valid_ratio": 0.15, "learning_rate": 0.0036369027023282715, "optimization": "nesterov_momentum", "nb_data_augmentation": 3, "learning_rate_decay_method": "exp", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 4.570303620736554e-07, "rotation_range": [0, 0], "momentum": 0.909282885430319}, "accuracy_valid_max": 0.8878026755459337, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8870702536709337, "accuracy_valid_std": [0.011002593700914741, 0.01826690293679803, 0.01737370134026809, 0.01610163627285072, 0.01412181036812734, 0.01757794755701615, 0.009217520575653499, 0.014821212400055243, 0.009337850864664491, 0.015048520499855837, 0.013586473191839045, 0.010669357818395122, 0.01206389626051083, 0.01512119528106214, 0.011382372248073206, 0.015329345985003736, 0.01414195928533204, 0.017564760326734593, 0.01609129603035904, 0.01431523395738687, 0.013520201852689102, 0.014226244529587, 0.015001545632187783, 0.013867865192561833, 0.013564434056198466, 0.016112185819416862, 0.014299452799582703, 0.01200014890345338, 0.014032389702934547, 0.013631324236399902, 0.013263452340975796, 0.014020578380750269, 0.014257907745556762, 0.010051065087917695, 0.01286300182401549, 0.013045005448300919, 0.01255968454631354, 0.013279793577632185, 0.011229837664184224, 0.011982901644963836, 0.012775613196841026, 0.014747382034668721, 0.009473963592123526, 0.012645248523148897, 0.011728992988639867, 0.008287285534508044, 0.010884529830336769, 0.013059033679448533, 0.01012767791709452, 0.0117946889964179, 0.010816160902203652, 0.011210160581526744, 0.010037353757138299, 0.009268675749715634, 0.010269661108363644, 0.010850821133910388, 0.012480370372704266, 0.01020050037190113, 0.009877063853865102, 0.011497305339544019, 0.009547595769732924, 0.0098269460594599, 0.008508500854008725, 0.009895271038754911, 0.009214363310499437, 0.012396818064554116, 0.011049216023163347, 0.01074631124701763, 0.008658011684796693, 0.010461847196293056, 0.008966099756852112, 0.00891572294813409, 0.00898470838709291, 0.009286667034848156, 0.009259139519812226, 0.010381095013144348, 0.010253288471347835, 0.009523433672035432, 0.005660097413669725, 0.007899393766587577, 0.008155828314588625, 0.008616502245695285, 0.008727677994258928, 0.0065339866759947796, 0.006759564033522592, 0.007437572599659662, 0.008541689588642508, 0.006872821170834103, 0.0072543393681644905, 0.00664523130576153, 0.007960500783557384, 0.005570576557395333, 0.006891960069475584, 0.007767622334768402, 0.007592478329233524, 0.006504177190317163, 0.007278797098144707, 0.007156909378540712, 0.006795800589077159, 0.006261379991241687, 0.007191026809163578, 0.005457588798535321, 0.005408806386837167, 0.006633892041588941, 0.005093584639641891, 0.006378289036266814, 0.005836663927907803, 0.006682647736856575, 0.004779661384750545, 0.0051867771095628, 0.006074110172618488], "accuracy_valid": [0.5077213149472892, 0.5722670957266567, 0.6340170251317772, 0.6965611469314759, 0.7431125870670181, 0.7445259553840362, 0.7729801040097892, 0.7950042356927711, 0.8087481762989458, 0.8132544827748494, 0.8252894390060241, 0.8308031932417168, 0.829857516001506, 0.8387083490210843, 0.8356154108621988, 0.8361242822853916, 0.8445574289344879, 0.8523390436746988, 0.8510168604103916, 0.8522581537085843, 0.8547907450112951, 0.8619531838290663, 0.8628885659826807, 0.8601324242281627, 0.8615869728915663, 0.8600103539156627, 0.8639063088290663, 0.8677213737763554, 0.8696539086031627, 0.8708040168486446, 0.8705289909638554, 0.8710275673004518, 0.8679655144013554, 0.8709157920745482, 0.8728277367281627, 0.8735807487763554, 0.8734895637236446, 0.8749235222138554, 0.8730924675263554, 0.8774869987763554, 0.875523578689759, 0.8732954278049698, 0.8780973503388554, 0.880040180252259, 0.8788503623870482, 0.8798372199736446, 0.8776193641754518, 0.8791459784450302, 0.8787179969879518, 0.8765207313629518, 0.8804372764495482, 0.884852397872741, 0.882044780685241, 0.883753765060241, 0.8791959831513554, 0.8827566123870482, 0.8806814170745482, 0.8831537085843373, 0.883265483810241, 0.8807828972138554, 0.8878026755459337, 0.8838655402861446, 0.8857274802334337, 0.8840890907379518, 0.8852083137236446, 0.8853921545557228, 0.8833669639495482, 0.884119975997741, 0.8840082007718373, 0.8832448936370482, 0.8840184958584337, 0.8840993858245482, 0.8844758918486446, 0.8843538215361446, 0.8852391989834337, 0.8827463173004518, 0.8846288474209337, 0.8840993858245482, 0.884119975997741, 0.885951030685241, 0.8822786262236446, 0.8851377188441265, 0.8821462608245482, 0.8856054099209337, 0.8857274802334337, 0.8857171851468373, 0.8857171851468373, 0.8862054663968373, 0.883631694747741, 0.8856157050075302, 0.8838552451995482, 0.883387554122741, 0.8858598456325302, 0.8856054099209337, 0.8857377753200302, 0.884974468185241, 0.8868261130459337, 0.8872026190700302, 0.8862157614834337, 0.884974468185241, 0.8877923804593373, 0.8847200324736446, 0.8862157614834337, 0.8844758918486446, 0.8859613257718373, 0.8865716773343373, 0.8876806052334337, 0.8875585349209337, 0.8859613257718373, 0.8872026190700302, 0.8870702536709337], "seed": 325033199, "model": "residualv3", "loss_std": [0.2535916864871979, 0.18666638433933258, 0.18333597481250763, 0.18042199313640594, 0.17596043646335602, 0.17061345279216766, 0.1671164333820343, 0.163042351603508, 0.1575174182653427, 0.15331459045410156, 0.14872577786445618, 0.14523789286613464, 0.14139486849308014, 0.13868093490600586, 0.13168588280677795, 0.13063080608844757, 0.1246732696890831, 0.12105531990528107, 0.11862001568078995, 0.11169217526912689, 0.11210684478282928, 0.107179194688797, 0.10445566475391388, 0.10357549041509628, 0.09846921265125275, 0.09618265181779861, 0.09281463921070099, 0.08925485610961914, 0.08696131408214569, 0.08473443984985352, 0.08189372718334198, 0.07961410284042358, 0.07709655910730362, 0.07655682414770126, 0.07122932374477386, 0.06845483183860779, 0.0685972049832344, 0.06675276160240173, 0.06612974405288696, 0.061826255172491074, 0.06118070334196091, 0.05770522356033325, 0.05616002529859543, 0.05585455521941185, 0.05363647639751434, 0.05358084663748741, 0.049009036272764206, 0.05007323622703552, 0.046152595430612564, 0.0473085381090641, 0.04399935528635979, 0.04270629212260246, 0.04240026697516441, 0.03925103694200516, 0.041691526770591736, 0.03858358785510063, 0.03956535831093788, 0.037297315895557404, 0.0363566018640995, 0.03654070943593979, 0.03602968901395798, 0.03522743284702301, 0.032999616116285324, 0.03234894946217537, 0.033643078058958054, 0.032773032784461975, 0.031117191538214684, 0.03189065307378769, 0.030287979170680046, 0.029977716505527496, 0.029779892414808273, 0.029188578948378563, 0.02900533378124237, 0.027946755290031433, 0.028676817193627357, 0.02864685095846653, 0.027920283377170563, 0.02820316143333912, 0.02731717936694622, 0.026071391999721527, 0.02483796328306198, 0.023737994953989983, 0.024338800460100174, 0.024805089458823204, 0.025086844339966774, 0.024310341104865074, 0.02416839450597763, 0.02506021037697792, 0.02427111566066742, 0.02400496043264866, 0.023406468331813812, 0.023206481710076332, 0.023910442367196083, 0.02386307716369629, 0.023197060450911522, 0.02299744263291359, 0.023116271942853928, 0.02286641113460064, 0.022757815197110176, 0.021806318312883377, 0.021778805181384087, 0.022011490538716316, 0.02129599079489708, 0.021625330671668053, 0.022514037787914276, 0.021358300000429153, 0.021191228181123734, 0.02184159867465496, 0.021452702581882477, 0.021601445972919464, 0.021650560200214386]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:25 2016", "state": "available"}], "summary": "9ad41c5e4647f4d166928200de202740"}