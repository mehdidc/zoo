{"content": {"hp_model": {"f0": 32, "f1": 64, "f2": 16, "f3": 16, "nonlin": "leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.01720652583287168, 0.023281549227312754, 0.016079427027169378, 0.013449358954877732, 0.014932941756886847, 0.022704050365039072, 0.01752850975178535, 0.01884843525125035, 0.017226478625423435, 0.016550962383049955, 0.017159456436333573, 0.01370181989064402, 0.022553139349187026, 0.01965567137546441, 0.01459788603939371, 0.019364204440953558, 0.02025662187780957, 0.020816417949091302, 0.018420569732342563, 0.018901488265021915, 0.015873817023931567, 0.014042889284031001, 0.01371205810571441, 0.01038185426043184, 0.011254361325325776, 0.012469385071904192, 0.013483242109048775, 0.009909353707892143, 0.014468537334630917, 0.0122015411926748, 0.012231906812675934, 0.009789571032975506, 0.00991145816273603, 0.010034329423455184, 0.01157065892159866, 0.013226527010496236, 0.008994600630938415, 0.011303447471175858, 0.013375144894973318, 0.00969088255162863, 0.009701966737949938, 0.012976192453606037, 0.011389266206507355, 0.010537369664965298, 0.009830912572576974, 0.009543626301594312, 0.012180534995825342, 0.013193215180358079, 0.01669293278549419, 0.017611855301947046, 0.01345866833213051, 0.011880000473400052, 0.011022215095680006, 0.014318471369669473, 0.009423670809460721, 0.0092904483961998, 0.011539020956591001, 0.009764166568158563, 0.012411670514971234, 0.012616755555591757, 0.010983701668145605, 0.010873089930903052, 0.008335609019638703, 0.012780971406558643, 0.00781361846957861, 0.012150958771246759, 0.009982895605758125, 0.011493528394466894, 0.00938073215731642, 0.007977369839699999, 0.010405464060049002, 0.010188040111643737, 0.012389256097616421, 0.012641852704820024, 0.0159320624163732, 0.00878450036520365, 0.011606485670771733, 0.01194383257177197, 0.014128914517823983, 0.006957471540833727, 0.013933302557569862, 0.010310211364633356, 0.009634607073996037, 0.011787750383636513, 0.013865982761437636, 0.012968082401755452, 0.01363416219489977, 0.011861459996102424, 0.014125586421415868, 0.01267428919057668, 0.012169708371499098, 0.012431133560072149, 0.007606399779943861, 0.007035878699772493, 0.01115467317089059, 0.009462798288618943, 0.012396369983513103, 0.00933413887722785, 0.009790755270418377, 0.01191514363329544, 0.01051165453687703, 0.011070536061243414, 0.01554345362702384, 0.010900275807107111, 0.010968160655059313, 0.013644503729675836, 0.010162573378607327, 0.009497815649726968, 0.011810399155839313, 0.007795836882164925, 0.016422743083066668, 0.010708494708841828, 0.011456338647353894, 0.009360400377508887, 0.00747791818253378, 0.012478167776444539, 0.009060592975051317, 0.008623937683496166, 0.00872206318780807, 0.009603176010234159, 0.008042392611188396, 0.011795591519479363, 0.010365681371567162, 0.006448788570682028, 0.008574241050024885, 0.009926706802761798, 0.00849512086798544, 0.011545427203285321, 0.012239117458870186, 0.012468645036274352, 0.008216017246921769, 0.011219883524166885, 0.009239553766487375, 0.008122996278883514, 0.010216478278254254, 0.008224871010103338, 0.013334630168125586, 0.00662257729201907, 0.011784172349901206, 0.008074897436646123, 0.008197356487117462, 0.012443789560688855, 0.009119488114229374, 0.01008093588254258, 0.011143243508505771, 0.014305715474818629, 0.007799939443661148, 0.005590679877116533, 0.010332227200604087, 0.00814843040532799, 0.01028090196102887, 0.01005507667223211, 0.013481657587380335, 0.009138918406267038, 0.008542025875325257, 0.007271702729488618, 0.012898574215361822, 0.011433069949243601, 0.008579031735501575, 0.01203840501609604, 0.011109872571320057, 0.011878887586173132, 0.01087012062123317, 0.010276203637953802, 0.007788637203021238, 0.006398480960672138, 0.00903388925446705, 0.011553665794573242, 0.01113447551552263, 0.007445184410522947, 0.013161000853457634, 0.010636762093649882, 0.008586659834908533, 0.010062111932977739, 0.009505447240984137, 0.007363889302989512, 0.010433243991980347, 0.006614703789199394, 0.009197636075216304, 0.009614046447676671, 0.010268649747522823, 0.011608240066762554, 0.009067693728198836, 0.008872058566846351, 0.00923140889904925, 0.007421862036703631, 0.00849017659734455, 0.010650219635327196, 0.012446533601120288, 0.012131176028156429, 0.009400270777539823, 0.012299124788474232, 0.007873793082069206, 0.0072404546974864075, 0.008769851055489638, 0.009912035598348787, 0.012873660742626499, 0.00994048023271096, 0.009679131362397982, 0.01326775962876772, 0.01058575349639234, 0.01103459845666329], "moving_avg_accuracy_train": [0.03309146378391472, 0.07156861207721943, 0.10985914554926862, 0.15798925628042518, 0.20730394210956243, 0.25344726095577674, 0.30153898944445523, 0.3436938839604656, 0.3856203064032895, 0.4210804155054855, 0.45597270488309305, 0.4911996617965002, 0.5219365529673485, 0.5505484517842165, 0.5784844761609813, 0.6025506425918101, 0.625909407524674, 0.6462303894143255, 0.6648400715530887, 0.6834949388671412, 0.7011630454556947, 0.7165879742699905, 0.7309399460671056, 0.7454468340796734, 0.7570291052386589, 0.768629350139996, 0.7810295628511718, 0.7923942952888304, 0.802134453476817, 0.811270330555538, 0.8203642983882271, 0.829004454409039, 0.8364086070670645, 0.8415612140260097, 0.8484258005068528, 0.8546226737253628, 0.8604507594005378, 0.864989371514194, 0.8702505035188284, 0.8760875347122575, 0.8806643526292285, 0.8847508285247129, 0.8891006608854198, 0.8925389266017412, 0.8948452123976394, 0.8985670028734531, 0.9022303291468775, 0.9054948788560824, 0.9063266050658506, 0.907221643029642, 0.9099545092672371, 0.9122233906298729, 0.9150117566241023, 0.917574728392709, 0.9199837455808929, 0.9219285386180879, 0.9240370334146862, 0.926432080332769, 0.9282157068959761, 0.9296999909671296, 0.9309731757597669, 0.9327094496266658, 0.9345651730033311, 0.9352170530590334, 0.9369196002448243, 0.9382101132846645, 0.9402318440312257, 0.9414352372686071, 0.9427601066584407, 0.9438618083057195, 0.944755611440633, 0.9461413214644361, 0.9461865027466101, 0.9471503220755851, 0.9470784354014338, 0.947625143385146, 0.9490424816502213, 0.9499019205007031, 0.9511450594768417, 0.9522196706791668, 0.9527706551731734, 0.9533990386511034, 0.9543576060276874, 0.95455761320708, 0.9542216529257316, 0.954174982943928, 0.9548165016126673, 0.9554358292395604, 0.9568904792025091, 0.9570673887965624, 0.9563688798622273, 0.9575093716748233, 0.9581662237919017, 0.9593315943067684, 0.9605827157165769, 0.9616297059746995, 0.9622255139855722, 0.9631291507560811, 0.9638447315507203, 0.9646259380456574, 0.9650685711756155, 0.9657739327330723, 0.9663018373383643, 0.9669233277116708, 0.9676012516369322, 0.9678138187720577, 0.9681585890150993, 0.9685595630374082, 0.9690413113467626, 0.9693448206871234, 0.9693667909243818, 0.9694516683045811, 0.9695163961538942, 0.9701256754373142, 0.9699928302888393, 0.970070979401659, 0.9701831302329493, 0.9709652264358449, 0.9714086963517843, 0.9714917071844906, 0.9714687246351076, 0.9714898569704156, 0.9721575565412311, 0.9728166148752032, 0.9728913312888918, 0.9736769749457169, 0.9740515940059165, 0.9739911146648487, 0.9736786277888492, 0.9744181299278306, 0.9749348723291044, 0.9747722224093168, 0.9747467091707845, 0.9752329187965725, 0.9752426800788292, 0.9753724090197742, 0.9741964904749858, 0.9746726538525057, 0.9750314464279879, 0.9756798084816177, 0.9761889656167985, 0.9764146921575088, 0.9770456373762817, 0.9770184662708518, 0.9776727394354332, 0.9769852588312217, 0.9773964590635942, 0.9776549321298722, 0.9781688648466561, 0.9785569995298569, 0.9789389449257083, 0.9795848930295753, 0.9790804739266454, 0.979191471845904, 0.9796098793113228, 0.9795796170861706, 0.9798638789263723, 0.9801568809146874, 0.9803089755613139, 0.9806737613754298, 0.9807951303640865, 0.980283655868191, 0.980697440578991, 0.9811420345782624, 0.9813677469680737, 0.9819219495403232, 0.9818557206946334, 0.9820054141751886, 0.981949512154126, 0.9821362934161035, 0.9820602919757021, 0.9823755041840936, 0.9827079872478272, 0.9825818279195007, 0.9821427276418547, 0.9826612848252975, 0.9831302753903868, 0.983415219168024, 0.9832345766405258, 0.9832649857169679, 0.9837434327548134, 0.9839646995983796, 0.984331250471875, 0.9846239799258871, 0.9848223322678314, 0.9850310402612863, 0.9849305590030148, 0.984886628846761, 0.9851702873906564, 0.9852744814563619, 0.9852380477821635, 0.9855400789039563, 0.9857467666980846, 0.985763157996161, 0.98578252441323, 0.986081225096907, 0.9861896204443592, 0.9861756412118465, 0.9863536860561473, 0.9863720562898182, 0.9864932211965506, 0.9866325686447711], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 683935214, "moving_var_accuracy_train": [0.009855404778259259, 0.02219428276749799, 0.033170339072915264, 0.050701873196564266, 0.06751913002274644, 0.07993006988776236, 0.09275239204024506, 0.09947046902112419, 0.10534384620869891, 0.1061262356256858, 0.10647085878521359, 0.10699221934710365, 0.10479580572203046, 0.10168399193498763, 0.09853938586330162, 0.09389807057705861, 0.08941895061205193, 0.08419353629548262, 0.07889106508968653, 0.07413399525126203, 0.06953005363995621, 0.06471840413629541, 0.060100375572852534, 0.05598438621384992, 0.05159328863926732, 0.047645050911279574, 0.04426443329769326, 0.04100040425834009, 0.03775419996624912, 0.03472995821960171, 0.032001264656119396, 0.029473008855083197, 0.027019101258825003, 0.024556135359202827, 0.02252462475125931, 0.020617773415309952, 0.018861695317513583, 0.017160916785026254, 0.015693940696255335, 0.01443118502500738, 0.013176591882712604, 0.012009226261640712, 0.010978593009572926, 0.009987128748839917, 0.009036286461507182, 0.008257323334469185, 0.007552370635492317, 0.006893049135177909, 0.0062099701380522575, 0.005596182960856683, 0.005103781685624296, 0.004639733920801313, 0.004245735392981157, 0.0038802812722631185, 0.003544483419353495, 0.003224075057035843, 0.0029416793040978008, 0.0026991376213463853, 0.0024578557726645466, 0.0022318980882330146, 0.0020232972750555407, 0.0018480993700178713, 0.0016942828162724024, 0.0015286790631083643, 0.0014018991590761321, 0.0012766980583224974, 0.0011858148093945673, 0.0010802667260090863, 0.0009880375635092412, 0.0009001575258348697, 0.0008173317296472129, 0.0007528802871131063, 0.0006776106305361256, 0.0006182100967726658, 0.0005564355963406842, 0.0005034820432817086, 0.0004712134687723573, 0.00043073983813457815, 0.00040157440494707525, 0.00037181006757783025, 0.000337361316033768, 0.0003071789765884096, 0.00028473074166862904, 0.0002566176933480433, 0.00023197174780903256, 0.00020879417581294316, 0.00019161867405271898, 0.00017590890703234464, 0.0001773620749614708, 0.00015990754050553653, 0.00014830801903709643, 0.00014518371130477388, 0.00013454843250768983, 0.00013331638518920635, 0.00013407248970901832, 0.00013053093814355054, 0.00012067272900157526, 0.00011595449081855875, 0.0001089675445996113, 0.00010356334242923882, 9.497032497594323e-05, 8.995110681899128e-05, 8.346414558768753e-05, 7.859398358593187e-05, 7.487081286331626e-05, 6.779039465940391e-05, 6.208115387784587e-05, 5.732005998915989e-05, 5.36767868923364e-05, 4.913816948027817e-05, 4.422869675417711e-05, 3.9870664605784905e-05, 3.592130539549662e-05, 3.567016606279127e-05, 3.226197995777193e-05, 2.9090747516505232e-05, 2.6294873045486778e-05, 2.91704559761907e-05, 2.802340047566067e-05, 2.5283077613214095e-05, 2.2759523630077957e-05, 2.0487590447430287e-05, 2.245123585449308e-05, 2.411533325724674e-05, 2.175404281379246e-05, 2.5133762131998974e-05, 2.3883440881181472e-05, 2.152801654932732e-05, 2.025404732344164e-05, 2.3150413313120762e-05, 2.3238576365276473e-05, 2.1152813696410757e-05, 1.9043390654833322e-05, 1.9266649791229394e-05, 1.734084235578812e-05, 1.5758224503277863e-05, 2.662746186874579e-05, 2.6005299740691454e-05, 2.4563358776612523e-05, 2.5890383072234474e-05, 2.563451365976067e-05, 2.3529634534414105e-05, 2.475949790280457e-05, 2.2290192533256673e-05, 2.3913833644954296e-05, 2.5776116510961332e-05, 2.4720275539793802e-05, 2.2849522919735207e-05, 2.2941712164189765e-05, 2.2003377738500966e-05, 2.1115980533360493e-05, 2.2759623056028405e-05, 2.277360843303048e-05, 2.060713243244525e-05, 2.0122002453264586e-05, 1.8118044428378594e-05, 1.7033483129694414e-05, 1.6102786303134684e-05, 1.4700702706613147e-05, 1.4428250647573841e-05, 1.3117999465484465e-05, 1.4160654958500373e-05, 1.4285549544676474e-05, 1.463596900790245e-05, 1.3630886853340997e-05, 1.5032062587798294e-05, 1.3568332669031041e-05, 1.2413172645214266e-05, 1.1199980704322791e-05, 1.0393967792323972e-05, 9.406556983579389e-06, 9.360129912092287e-06, 9.419021809909578e-06, 8.620365214032663e-06, 9.493610177087636e-06, 1.09643631318801e-05, 1.1847496169977112e-05, 1.1393483160706897e-05, 1.0547820349304726e-05, 9.501360721744786e-06, 1.0611428761778385e-05, 9.990917030156615e-06, 1.0201061212882962e-05, 9.952169890811252e-06, 9.311045765723333e-06, 8.771972427938938e-06, 7.985643534519445e-06, 7.204447908723906e-06, 7.208162643574864e-06, 6.585054009171553e-06, 5.9384953217947476e-06, 6.165650976398467e-06, 5.9335644769327626e-06, 5.342626101113141e-06, 4.811739013992648e-06, 5.133563998455352e-06, 4.725953560753284e-06, 4.255116975152752e-06, 4.114904976876226e-06, 3.706451668554744e-06, 3.467934913310678e-06, 3.295900823909794e-06], "duration": 206227.547508, "accuracy_train": [0.33091463783914726, 0.41786294671696195, 0.45447394679771136, 0.5911602528608343, 0.6511361145717978, 0.6687371305717055, 0.7343645458425618, 0.7230879346045589, 0.7629581083887044, 0.7402213974252492, 0.7700033092815615, 0.808242274017165, 0.7985685735049834, 0.8080555411360282, 0.8299086955518641, 0.8191461404692691, 0.8361382919204503, 0.8291192264211886, 0.8323272108019564, 0.8513887446936139, 0.8601760047526762, 0.8554123335986527, 0.8601076922411407, 0.8760088261927832, 0.8612695456695275, 0.8730315542520304, 0.8926314772517534, 0.8946768872277593, 0.889795877168697, 0.8934932242640274, 0.902210008882429, 0.9067658585963455, 0.9030459809892949, 0.8879346766565154, 0.9102070788344407, 0.9103945326919527, 0.9129035304771133, 0.9058368805370985, 0.9176006915605389, 0.9286208154531194, 0.9218557138819674, 0.9215291115840717, 0.9282491521317828, 0.9234833180486341, 0.9156017845607235, 0.9320631171557769, 0.9352002656076966, 0.9348758262389257, 0.9138121409537652, 0.9152769847037652, 0.9345503054055924, 0.9326433228935955, 0.9401070505721669, 0.9406414743101699, 0.9416649002745479, 0.9394316759528424, 0.9430134865840717, 0.947987502595515, 0.9442683459648394, 0.9430585476075121, 0.9424318388935032, 0.948335914428756, 0.9512666833933187, 0.9410839735603543, 0.9522425249169435, 0.9498247306432264, 0.9584274207502769, 0.9522657764050388, 0.9546839311669435, 0.9537771231312293, 0.9527998396548542, 0.9586127116786637, 0.9465931342861758, 0.9558246960363603, 0.9464314553340717, 0.9525455152385567, 0.9617985260358989, 0.9576368701550388, 0.9623333102620893, 0.9618911715000923, 0.9577295156192323, 0.9590544899524732, 0.9629847124169435, 0.9563576778216132, 0.9511980103935955, 0.9537549531076966, 0.9605901696313216, 0.9610097778815985, 0.9699823288690477, 0.9586595751430418, 0.9500822994532114, 0.9677737979881875, 0.9640778928456073, 0.9698199289405685, 0.9718428084048542, 0.9710526182978036, 0.9675877860834257, 0.9712618816906607, 0.9702849587024732, 0.9716567965000923, 0.9690522693452381, 0.9721221867501846, 0.9710529787859912, 0.9725167410714286, 0.9737025669642857, 0.9697269229881875, 0.9712615212024732, 0.9721683292381875, 0.9733770461309523, 0.9720764047503692, 0.9695645230597084, 0.970215564726375, 0.9700989467977114, 0.9756091889880952, 0.9687972239525655, 0.9707743214170359, 0.9711924877145626, 0.9780040922619048, 0.9753999255952381, 0.9722388046788483, 0.9712618816906607, 0.9716800479881875, 0.9781668526785714, 0.9787481398809523, 0.9735637790120893, 0.9807477678571429, 0.9774231655477114, 0.9734468005952381, 0.9708662459048542, 0.9810736491786637, 0.9795855539405685, 0.9733083731312293, 0.9745170900239941, 0.9796088054286637, 0.97533053161914, 0.9765399694882798, 0.96361322357189, 0.9789581242501846, 0.9782605796073275, 0.9815150669642857, 0.9807713798334257, 0.9784462310239018, 0.9827241443452381, 0.9767739263219823, 0.9835611979166666, 0.9707979333933187, 0.9810972611549464, 0.979981189726375, 0.9827942592977114, 0.9820502116786637, 0.9823764534883721, 0.985398425964378, 0.9745407020002769, 0.9801904531192323, 0.9833755465000923, 0.9793072570598007, 0.9824222354881875, 0.9827938988095238, 0.9816778273809523, 0.9839568337024732, 0.9818874512619971, 0.9756803854051311, 0.9844215029761905, 0.9851433805717055, 0.983399158476375, 0.9869097726905685, 0.9812596610834257, 0.9833526555001846, 0.9814463939645626, 0.9838173247739018, 0.9813762790120893, 0.9852124140596161, 0.9857003348214286, 0.9814463939645626, 0.9781908251430418, 0.9873282994762828, 0.9873511904761905, 0.9859797131667589, 0.9816087938930418, 0.9835386674049464, 0.9880494560954227, 0.9859561011904762, 0.9876302083333334, 0.9872585450119971, 0.9866075033453304, 0.9869094122023809, 0.9840262276785714, 0.9844912574404762, 0.9877232142857143, 0.9862122280477114, 0.984910144714378, 0.9882583590000923, 0.9876069568452381, 0.9859106796788483, 0.9859568221668512, 0.98876953125, 0.9871651785714286, 0.9860498281192323, 0.9879560896548542, 0.9865373883928571, 0.9875837053571429, 0.987886695678756], "end": "2016-01-26 17:15:05.469000", "learning_rate_per_epoch": [0.0015147140948101878, 0.0010710646165534854, 0.0008745206287130713, 0.0007573570474050939, 0.0006774007342755795, 0.0006183794466778636, 0.0005725081427954137, 0.0005355323082767427, 0.0005049047176726162, 0.0004789946542587131, 0.00045670350664295256, 0.0004372603143565357, 0.00042010610923171043, 0.00040482438635081053, 0.00039109750650823116, 0.00037867852370254695, 0.0003673721512313932, 0.0003570215485524386, 0.0003474992699921131, 0.0003387003671377897, 0.00033053773222491145, 0.00032293814001604915, 0.00031583974487148225, 0.0003091897233389318, 0.00030294281896203756, 0.00029705988708883524, 0.00029150687623769045, 0.00028625407139770687, 0.0002812753664329648, 0.0002765477111097425, 0.00027205070364288986, 0.00026776615413837135, 0.00026367788086645305, 0.0002597713319119066, 0.00025603341055102646, 0.0002524523588363081, 0.0002490174665581435, 0.00024571907124482095, 0.00024254838353954256, 0.00023949732712935656, 0.00023655861150473356, 0.00023372547002509236, 0.00023099174723029137, 0.00022835175332147628, 0.0002258002496091649, 0.00022333241940941662, 0.00022094376618042588, 0.00021863015717826784, 0.00021638773614540696, 0.00021421292331069708, 0.00021210240083746612, 0.00021005305461585522, 0.00020806198881473392, 0.00020612648222595453, 0.00020424401736818254, 0.00020241219317540526, 0.00020062878320459276, 0.00019889170653186738, 0.00019719898409675807, 0.00019554875325411558, 0.00019393926777411252, 0.00019236888329032809, 0.00019083604274783283, 0.00018933926185127348, 0.00018787717272061855, 0.0001864484220277518, 0.00018505178741179407, 0.0001836860756156966, 0.00018235015159007162, 0.00018104296759702265, 0.0001797634904505685, 0.0001785107742762193, 0.00017728388775140047, 0.00017608194320928305, 0.00017490412574261427, 0.00017374963499605656, 0.0001726176997181028, 0.00017150760686490685, 0.0001704186579445377, 0.00016935018356889486, 0.00016830157255753875, 0.00016727219917811453, 0.00016626148135401309, 0.00016526886611245573, 0.000164293815032579, 0.00016333583334926516, 0.00016239439719356596, 0.00016146907000802457, 0.00016055937157943845, 0.0001596648944541812, 0.00015878518752288073, 0.00015791987243574113, 0.00015706854173913598, 0.00015623083163518459, 0.00015540639287792146, 0.0001545948616694659, 0.00015379591786768287, 0.00015300924133043736, 0.0001522344973636791, 0.00015147140948101878, 0.0001507196866441518, 0.00014997905236668885, 0.00014924921561032534, 0.00014852994354441762, 0.00014782095968257636, 0.00014712204574607313, 0.00014643293980043381, 0.00014575343811884522, 0.0001450833078706637, 0.00014442233077716082, 0.0001437703031115234, 0.00014312703569885343, 0.00014249232481233776, 0.0001418659812770784, 0.00014124781591817737, 0.0001406376832164824, 0.00014003536489326507, 0.00013944074453320354, 0.00013885361840948462, 0.00013827385555487126, 0.0001377012813463807, 0.00013713577936869115, 0.00013657717499881983, 0.00013602535182144493, 0.00013548014976549894, 0.00013494146696757525, 0.00013440914335660636, 0.00013388307706918567, 0.00013336313713807613, 0.00013284922169987112, 0.00013234118523541838, 0.00013183894043322653, 0.00013134237087797374, 0.0001308513747062534, 0.00013036583550274372, 0.0001298856659559533, 0.00012941076420247555, 0.00012894102837890387, 0.00012847637117374688, 0.00012801670527551323, 0.00012756194337271154, 0.0001271119836019352, 0.0001266667532036081, 0.00012622617941815406, 0.0001257901603821665, 0.00012535863788798451, 0.0001249315100722015, 0.00012450873327907175, 0.00012409022019710392, 0.00012367589806672186, 0.00012326569412834942, 0.00012285953562241048, 0.00012245737889315933, 0.00012205914390506223, 0.00012166476517450064, 0.00012127419176977128, 0.00012088735093129799, 0.00012050418445141986, 0.0001201246413984336, 0.00011974866356467828, 0.00011937620001845062, 0.00011900718527613208, 0.00011864156840601936, 0.00011827930575236678, 0.00011792033910751343, 0.000117564617539756, 0.00011721209739334881, 0.00011686273501254618, 0.00011651647218968719, 0.00011617327254498377, 0.00011583308514673263, 0.00011549587361514568, 0.00011516158701851964, 0.00011483018170110881, 0.00011450162855908275, 0.00011417587666073814, 0.00011385288962628692, 0.00011353262379998341, 0.00011321505007799715, 0.00011290012480458245, 0.00011258781887590885, 0.00011227808136027306, 0.00011197089042980224, 0.00011166620970470831, 0.00011136399552924559, 0.00011106422607554123, 0.00011076686496380717, 0.00011047188309021294, 0.00011017924407497048, 0.00010988891153829172, 0.00010960087092826143, 0.00010931507858913392, 0.00010903151269303635, 0.00010875013686018065, 0.00010847092926269397, 0.00010819386807270348, 0.0001079189169104211, 0.00010764604667201638, 0.00010737523552961648, 0.00010710646165534854, 0.00010683969594538212, 0.00010657491657184437], "accuracy_valid": [0.3301619564194277, 0.41294915992093373, 0.4452183734939759, 0.5824004023908133, 0.636387836502259, 0.6461637565888554, 0.7146996187876506, 0.6980259906814759, 0.7340985033885542, 0.7092461643448795, 0.74462890625, 0.7755744658320783, 0.7600700654179217, 0.7736213408320783, 0.7980574642319277, 0.7802440229668675, 0.7976397778614458, 0.7918318782944277, 0.7906714749623494, 0.8041095044239458, 0.8173960490399097, 0.8107218914721386, 0.8140795604292168, 0.827538180064006, 0.8120146602033133, 0.8173651637801205, 0.8317900508283133, 0.8362463525978916, 0.8327371987951807, 0.831078219126506, 0.8417806970067772, 0.8410070947853916, 0.8343844126506024, 0.8275690653237951, 0.8458090173192772, 0.8394407708960843, 0.8396849115210843, 0.8337843561746988, 0.8425337090549698, 0.8452589655496988, 0.8441000329442772, 0.8443132883094879, 0.8464090737951807, 0.8463384789156627, 0.8368669992469879, 0.8495829019201807, 0.8505903496799698, 0.851719867752259, 0.8378950371799698, 0.8350153543862951, 0.8462252329631024, 0.8433470208960843, 0.8520243081701807, 0.8559114387236446, 0.8486166345067772, 0.8495829019201807, 0.8488401849585843, 0.8530214608433735, 0.853184711502259, 0.8482401284826807, 0.845604586314006, 0.8542730492281627, 0.8559305581701807, 0.8456766519201807, 0.8569277108433735, 0.8535406273531627, 0.8568159356174698, 0.8532964867281627, 0.8559614434299698, 0.8516580972326807, 0.8517492822853916, 0.8536112222326807, 0.8477621423192772, 0.8538244775978916, 0.849766742752259, 0.8514551369540663, 0.8570497811558735, 0.8549642907567772, 0.8550863610692772, 0.8565820900790663, 0.854405414627259, 0.8525331795933735, 0.8549642907567772, 0.8512712961219879, 0.852208149002259, 0.8480474632906627, 0.8566129753388554, 0.8494093561746988, 0.8617090432040663, 0.8511801110692772, 0.8438455972326807, 0.8579763389495482, 0.8525125894201807, 0.8615766778049698, 0.8581484139683735, 0.8582704842808735, 0.8564188394201807, 0.8585249199924698, 0.8563379494540663, 0.858799945877259, 0.8557275978915663, 0.858555805252259, 0.8585763954254518, 0.8580469338290663, 0.8582704842808735, 0.8575689476656627, 0.859776508377259, 0.855992328689759, 0.8614751976656627, 0.8547098550451807, 0.8591970420745482, 0.8560732186558735, 0.8565614999058735, 0.8565614999058735, 0.8546789697853916, 0.856968891189759, 0.8567041603915663, 0.8598882836031627, 0.8616178581513554, 0.8583822595067772, 0.8524111092808735, 0.8520757836031627, 0.863194477127259, 0.8623296898531627, 0.8592161615210843, 0.862095844314759, 0.8580160485692772, 0.8586572853915663, 0.8564909050263554, 0.8620546639683735, 0.8623605751129518, 0.8565409097326807, 0.8580263436558735, 0.8629606315888554, 0.8610986916415663, 0.860631000564759, 0.8477621423192772, 0.862217914627259, 0.861607563064759, 0.8635400978915663, 0.8609663262424698, 0.8609354409826807, 0.8640386742281627, 0.8584234398531627, 0.8608957313629518, 0.8518713525978916, 0.8570188958960843, 0.8612310570406627, 0.8651976068335843, 0.8602133141942772, 0.8614546074924698, 0.8682405402861446, 0.8533876717808735, 0.8589014260165663, 0.8594000023531627, 0.8495417215737951, 0.8632856621799698, 0.8621458490210843, 0.8592676369540663, 0.8600103539156627, 0.8592676369540663, 0.8545068947665663, 0.8625532403049698, 0.8663991905120482, 0.8655947030308735, 0.8645269554781627, 0.8643034050263554, 0.8623193947665663, 0.8617605186370482, 0.8617193382906627, 0.8578733880835843, 0.8634695030120482, 0.8651681923004518, 0.8564703148531627, 0.8537847679781627, 0.865635883377259, 0.8635298028049698, 0.8630827019013554, 0.859288227127259, 0.8606515907379518, 0.8659609139683735, 0.8616781579442772, 0.8619531838290663, 0.866002094314759, 0.8611089867281627, 0.8661344597138554, 0.8621973244540663, 0.8629297463290663, 0.8647505059299698, 0.8603147943335843, 0.8590940912085843, 0.8682302451995482, 0.8639166039156627, 0.860142719314759, 0.8655241081513554, 0.8636724632906627, 0.8625738304781627, 0.8664594903049698, 0.8663786003388554, 0.862828266189759, 0.8649137565888554, 0.8617796380835843], "accuracy_test": 0.7689453125, "start": "2016-01-24 07:57:57.921000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0], "accuracy_train_last": 0.987886695678756, "batch_size_eval": 1024, "accuracy_train_std": [0.016038706864972215, 0.01772867247234913, 0.016940909102697663, 0.021624610523770744, 0.02179558655492218, 0.020648090744122848, 0.019522909481320103, 0.021183192306182166, 0.019035516918723766, 0.02082357771418527, 0.01850812513131586, 0.019481352903077086, 0.021098220641005947, 0.02035033602385681, 0.020054494566383377, 0.017879772640297484, 0.01898054159185706, 0.017751370426134797, 0.01756930517247788, 0.018856645320798074, 0.019219091513892596, 0.01738464647540281, 0.017595834534210725, 0.017499666217540753, 0.018996097531384086, 0.018973249220988815, 0.016277236762891893, 0.01652489018523226, 0.01701376076802299, 0.01604789717857151, 0.016802099172611377, 0.01694346710306317, 0.01806493104760167, 0.017935489696411424, 0.015509202225112172, 0.014350365512934438, 0.015861504477441415, 0.017403671147865975, 0.014705069701983308, 0.015033835460999866, 0.01568214911569899, 0.015932684873289315, 0.015233523110565458, 0.01553967664804125, 0.0157312209884037, 0.0141617917936638, 0.014450048768807754, 0.013029477027263744, 0.014828131471002527, 0.013905771188383112, 0.014888805005425353, 0.014790460898255116, 0.013579047031992341, 0.013285453379157586, 0.014771143909625204, 0.0142086190291437, 0.01348976984285434, 0.01281327338610395, 0.013228881807387854, 0.013357028472121208, 0.01381862915992042, 0.012466688583409267, 0.012562090477383167, 0.0125741541935786, 0.012849633948329709, 0.012644494914109416, 0.012043349994709872, 0.01254995970629196, 0.011747662186984496, 0.012445294158602998, 0.01302615415764867, 0.011514643031450132, 0.01249693952301404, 0.01108589544885296, 0.011224701765329406, 0.012389455978158237, 0.010613088503433326, 0.011327988869360478, 0.011604288375094163, 0.010031857890995659, 0.011573505334344439, 0.011895698772637646, 0.010251790195260251, 0.01202807870701551, 0.011616989245145351, 0.01219561350095589, 0.010406845440714522, 0.011676457817360543, 0.009439445542427191, 0.010974009848552903, 0.010325807177340874, 0.010046375318967952, 0.011147412106290347, 0.009456745046301725, 0.009243218302699967, 0.009209463015818355, 0.009638593865201812, 0.009589491915079157, 0.009907781654767958, 0.009530053012446508, 0.008999647585811546, 0.009000848277872835, 0.009048547768479855, 0.009433458545169538, 0.009569251549680111, 0.008803130610561438, 0.009062705010751649, 0.008811409694487325, 0.008799544024444876, 0.009642402051308131, 0.008715797876991177, 0.008953062789678029, 0.008833891246311771, 0.008402027452568033, 0.00949881901186275, 0.007955794560901392, 0.008570212835868772, 0.007757778077061669, 0.009064383213047008, 0.00827046366893771, 0.009938325542679224, 0.00845303277246585, 0.0086872550733055, 0.008472862680310979, 0.00782443261773254, 0.007308812275569638, 0.008180312149947452, 0.009355307821227359, 0.008322904442178744, 0.008457875270209666, 0.008033469221569542, 0.00829916708592266, 0.009548115079927344, 0.006631917853790363, 0.008869865346525582, 0.008039139196358961, 0.008792099608688915, 0.006953662384036647, 0.007328484167557981, 0.007027221355966394, 0.0069992791262366485, 0.00796990838531202, 0.0073743882413933575, 0.007843352756122053, 0.006568000086009502, 0.008651986471953338, 0.00725897305548961, 0.007430442808572829, 0.0062100346260816515, 0.006789965751295532, 0.007063064183692721, 0.005475361510150126, 0.007931706193444325, 0.007771093206547464, 0.006186319418719817, 0.007400384245799657, 0.006682238487530273, 0.0062237593096434556, 0.007811946373524444, 0.006088818514867471, 0.00641246197808546, 0.0069670793885850965, 0.006245611124661033, 0.0055364468181453436, 0.00584153553700154, 0.00542990018483683, 0.00694960481191303, 0.005401057130375164, 0.0056446491437681864, 0.006144510349077841, 0.0066365912062040025, 0.00532662748544098, 0.005685595057420185, 0.0058345429097035086, 0.006548330398770632, 0.005205389965041884, 0.005241444495388903, 0.005993523907992354, 0.006440438710556129, 0.005959167598489227, 0.00528507382050009, 0.006540408437049527, 0.005414951597900178, 0.005730555428068392, 0.0055179688532129584, 0.005581761743490793, 0.00613878917653231, 0.005626908159107205, 0.005285406430188793, 0.005212939699813935, 0.005623714006637164, 0.004815588307384432, 0.005049642964259655, 0.004824502746954967, 0.005782429500891024, 0.0041990024515168025, 0.005477095187624798, 0.005534969252273699, 0.005019600871477597, 0.005736147296845815, 0.005221188827496529, 0.004990081967931443], "accuracy_test_std": 0.010761701026528054, "error_valid": [0.6698380435805723, 0.5870508400790663, 0.5547816265060241, 0.41759959760918675, 0.36361216349774095, 0.3538362434111446, 0.28530038121234935, 0.30197400931852414, 0.2659014966114458, 0.2907538356551205, 0.25537109375, 0.22442553416792166, 0.23992993458207834, 0.22637865916792166, 0.2019425357680723, 0.21975597703313254, 0.2023602221385542, 0.2081681217055723, 0.20932852503765065, 0.1958904955760542, 0.1826039509600903, 0.18927810852786142, 0.1859204395707832, 0.17246181993599397, 0.18798533979668675, 0.18263483621987953, 0.16820994917168675, 0.1637536474021084, 0.1672628012048193, 0.16892178087349397, 0.15821930299322284, 0.1589929052146084, 0.16561558734939763, 0.17243093467620485, 0.15419098268072284, 0.16055922910391573, 0.16031508847891573, 0.16621564382530118, 0.15746629094503017, 0.15474103445030118, 0.15589996705572284, 0.15568671169051207, 0.1535909262048193, 0.15366152108433728, 0.16313300075301207, 0.1504170980798193, 0.14940965032003017, 0.14828013224774095, 0.16210496282003017, 0.16498464561370485, 0.15377476703689763, 0.15665297910391573, 0.1479756918298193, 0.1440885612763554, 0.15138336549322284, 0.1504170980798193, 0.15115981504141573, 0.1469785391566265, 0.14681528849774095, 0.1517598715173193, 0.15439541368599397, 0.14572695077183728, 0.1440694418298193, 0.1543233480798193, 0.1430722891566265, 0.14645937264683728, 0.14318406438253017, 0.14670351327183728, 0.14403855657003017, 0.1483419027673193, 0.1482507177146084, 0.1463887777673193, 0.15223785768072284, 0.1461755224021084, 0.15023325724774095, 0.14854486304593373, 0.1429502188441265, 0.14503570924322284, 0.14491363893072284, 0.14341790992093373, 0.14559458537274095, 0.1474668204066265, 0.14503570924322284, 0.14872870387801207, 0.14779185099774095, 0.15195253670933728, 0.1433870246611446, 0.15059064382530118, 0.13829095679593373, 0.14881988893072284, 0.1561544027673193, 0.14202366105045183, 0.1474874105798193, 0.13842332219503017, 0.1418515860316265, 0.1417295157191265, 0.1435811605798193, 0.14147508000753017, 0.14366205054593373, 0.14120005412274095, 0.14427240210843373, 0.14144419474774095, 0.14142360457454817, 0.14195306617093373, 0.1417295157191265, 0.14243105233433728, 0.14022349162274095, 0.14400767131024095, 0.13852480233433728, 0.1452901449548193, 0.14080295792545183, 0.1439267813441265, 0.1434385000941265, 0.1434385000941265, 0.1453210302146084, 0.14303110881024095, 0.14329583960843373, 0.14011171639683728, 0.1383821418486446, 0.14161774049322284, 0.1475888907191265, 0.14792421639683728, 0.13680552287274095, 0.13767031014683728, 0.14078383847891573, 0.13790415568524095, 0.14198395143072284, 0.14134271460843373, 0.1435090949736446, 0.1379453360316265, 0.13763942488704817, 0.1434590902673193, 0.1419736563441265, 0.1370393684111446, 0.13890130835843373, 0.13936899943524095, 0.15223785768072284, 0.13778208537274095, 0.13839243693524095, 0.13645990210843373, 0.13903367375753017, 0.1390645590173193, 0.13596132577183728, 0.14157656014683728, 0.13910426863704817, 0.1481286474021084, 0.14298110410391573, 0.13876894295933728, 0.13480239316641573, 0.13978668580572284, 0.13854539250753017, 0.1317594597138554, 0.1466123282191265, 0.14109857398343373, 0.14059999764683728, 0.15045827842620485, 0.13671433782003017, 0.13785415097891573, 0.14073236304593373, 0.13998964608433728, 0.14073236304593373, 0.14549310523343373, 0.13744675969503017, 0.13360080948795183, 0.1344052969691265, 0.13547304452183728, 0.1356965949736446, 0.13768060523343373, 0.13823948136295183, 0.13828066170933728, 0.14212661191641573, 0.13653049698795183, 0.13483180769954817, 0.14352968514683728, 0.14621523202183728, 0.13436411662274095, 0.13647019719503017, 0.1369172980986446, 0.14071177287274095, 0.13934840926204817, 0.1340390860316265, 0.13832184205572284, 0.13804681617093373, 0.13399790568524095, 0.13889101327183728, 0.1338655402861446, 0.13780267554593373, 0.13707025367093373, 0.13524949407003017, 0.13968520566641573, 0.14090590879141573, 0.13176975480045183, 0.13608339608433728, 0.13985728068524095, 0.1344758918486446, 0.13632753670933728, 0.13742616952183728, 0.13354050969503017, 0.1336213996611446, 0.13717173381024095, 0.1350862434111446, 0.13822036191641573], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-3, 3], "momentum": 0.6317693979534973, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.0015147141391843608, "patience_threshold": 1, "do_flip": true, "batch_size": 64, "optimization": "rmsprop", "nb_data_augmentation": 3, "learning_rate_decay_method": "sqrt", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 6.798362543028594e-06, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.004365430005554849}, "accuracy_valid_max": 0.8682405402861446, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-3, 3)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8617796380835843, "loss_train": [1.7111014127731323, 1.3409385681152344, 1.1337662935256958, 1.0144020318984985, 0.928333044052124, 0.8684996366500854, 0.8209184408187866, 0.7835167646408081, 0.7515577077865601, 0.724301278591156, 0.6997891664505005, 0.6772456765174866, 0.6576524972915649, 0.6396182179450989, 0.6233212351799011, 0.6086885333061218, 0.5941194891929626, 0.5798326134681702, 0.5661443471908569, 0.5547906756401062, 0.5442816615104675, 0.5335766673088074, 0.5228437781333923, 0.5148088932037354, 0.5037537217140198, 0.49194949865341187, 0.48663902282714844, 0.47696372866630554, 0.4685552716255188, 0.4605844020843506, 0.4537941813468933, 0.44605714082717896, 0.43918007612228394, 0.4320748448371887, 0.4266037940979004, 0.4212511479854584, 0.41467779874801636, 0.4086682200431824, 0.4017696976661682, 0.3958786129951477, 0.3918401896953583, 0.3862139880657196, 0.3799417018890381, 0.37674543261528015, 0.3692406117916107, 0.3661039471626282, 0.3613678216934204, 0.3568003177642822, 0.3525509238243103, 0.34712353348731995, 0.3432867228984833, 0.33856990933418274, 0.33601245284080505, 0.33149051666259766, 0.32812830805778503, 0.3243756890296936, 0.32015499472618103, 0.31735020875930786, 0.3134024441242218, 0.3108510375022888, 0.30655479431152344, 0.30357295274734497, 0.29952067136764526, 0.29966986179351807, 0.29509204626083374, 0.29284343123435974, 0.28777703642845154, 0.28659194707870483, 0.28433769941329956, 0.28192147612571716, 0.27911999821662903, 0.2753492295742035, 0.2729024589061737, 0.27123263478279114, 0.26880189776420593, 0.26662224531173706, 0.26464852690696716, 0.2615267038345337, 0.2585657238960266, 0.25785040855407715, 0.2578042447566986, 0.2550036311149597, 0.25432896614074707, 0.2509244680404663, 0.2485477775335312, 0.24708931148052216, 0.24548766016960144, 0.24298813939094543, 0.24113553762435913, 0.23975522816181183, 0.2367582619190216, 0.23782330751419067, 0.23602421581745148, 0.23315663635730743, 0.23235280811786652, 0.23079505562782288, 0.2311638593673706, 0.2274637520313263, 0.22576114535331726, 0.22482150793075562, 0.22380362451076508, 0.22390665113925934, 0.22094221413135529, 0.22186650335788727, 0.2181013822555542, 0.21980434656143188, 0.21777591109275818, 0.21611757576465607, 0.21466371417045593, 0.21333105862140656, 0.21184919774532318, 0.21172352135181427, 0.21020595729351044, 0.20816531777381897, 0.20848006010055542, 0.2073075771331787, 0.20665773749351501, 0.20527198910713196, 0.20502562820911407, 0.2040681540966034, 0.20116432011127472, 0.2025439739227295, 0.20218072831630707, 0.2001938372850418, 0.19977691769599915, 0.1983414739370346, 0.19922971725463867, 0.19905689358711243, 0.19645287096500397, 0.1958467811346054, 0.19447782635688782, 0.19567880034446716, 0.19563858211040497, 0.1929919421672821, 0.19291143119335175, 0.1922551393508911, 0.19334165751934052, 0.19070905447006226, 0.18997299671173096, 0.1901080310344696, 0.18955758213996887, 0.19063633680343628, 0.18845981359481812, 0.1871592104434967, 0.1864771842956543, 0.186430424451828, 0.18488538265228271, 0.18619249761104584, 0.1846669614315033, 0.18454883992671967, 0.183583602309227, 0.18276090919971466, 0.1824142038822174, 0.18253487348556519, 0.1821148544549942, 0.18157052993774414, 0.18129689991474152, 0.17929093539714813, 0.18022838234901428, 0.17904649674892426, 0.17866836488246918, 0.17847438156604767, 0.17816022038459778, 0.17830674350261688, 0.17718279361724854, 0.17779850959777832, 0.1758965700864792, 0.17590968310832977, 0.17597709596157074, 0.1757107973098755, 0.17381703853607178, 0.1746850460767746, 0.17538827657699585, 0.17358432710170746, 0.1730135828256607, 0.17382481694221497, 0.17239855229854584, 0.17184200882911682, 0.1716431975364685, 0.17108218371868134, 0.1705944538116455, 0.17118902504444122, 0.17117182910442352, 0.17122487723827362, 0.17054685950279236, 0.16865016520023346, 0.16953113675117493, 0.16880092024803162, 0.1677471250295639, 0.1693752557039261, 0.16746389865875244, 0.16713039577007294, 0.1661285161972046, 0.16696837544441223, 0.1664223074913025, 0.1660664677619934, 0.16661512851715088, 0.1654060333967209, 0.16522182524204254, 0.16434146463871002, 0.164081409573555, 0.1642591953277588], "accuracy_train_first": 0.33091463783914726, "model": "residualv3", "loss_std": [0.25989240407943726, 0.14431892335414886, 0.13341714441776276, 0.13030779361724854, 0.12745317816734314, 0.12422480434179306, 0.12249644100666046, 0.11991778016090393, 0.11817494034767151, 0.1173732578754425, 0.11589345335960388, 0.1133727952837944, 0.11210214346647263, 0.11166467517614365, 0.1112995520234108, 0.10853998363018036, 0.10830005258321762, 0.10680926591157913, 0.10285831987857819, 0.10459857434034348, 0.10262829065322876, 0.10077379643917084, 0.10256867110729218, 0.09712033718824387, 0.09645150601863861, 0.09488364309072495, 0.09282013028860092, 0.09181653708219528, 0.09178648889064789, 0.08893381059169769, 0.08829480409622192, 0.0872897207736969, 0.08444461226463318, 0.08481478691101074, 0.08298744261264801, 0.08043473958969116, 0.08045731484889984, 0.08020181208848953, 0.0772993415594101, 0.07628821581602097, 0.07610086351633072, 0.07605010271072388, 0.07287982106208801, 0.07174911350011826, 0.07215390354394913, 0.07373379170894623, 0.07063201814889908, 0.07095781713724136, 0.06992092728614807, 0.0672256276011467, 0.0666947141289711, 0.06591165065765381, 0.06545011699199677, 0.06357139348983765, 0.06304065883159637, 0.06265448778867722, 0.059878330677747726, 0.05874311551451683, 0.059278517961502075, 0.059240687638521194, 0.05777791514992714, 0.05765146389603615, 0.055909931659698486, 0.054810017347335815, 0.05524824932217598, 0.05552782118320465, 0.054134342819452286, 0.05367862060666084, 0.05015934258699417, 0.04933605715632439, 0.050986435264348984, 0.04892025142908096, 0.04861017316579819, 0.04923221096396446, 0.04793088138103485, 0.04824618622660637, 0.044610489159822464, 0.04591472074389458, 0.04346434026956558, 0.04561299458146095, 0.0452544130384922, 0.042586736381053925, 0.04209559038281441, 0.04156867414712906, 0.04070594534277916, 0.0410931296646595, 0.04001912102103233, 0.039742786437273026, 0.039952829480171204, 0.037986669689416885, 0.035599321126937866, 0.03834017366170883, 0.03824453428387642, 0.036789603531360626, 0.035810913890600204, 0.03589096665382385, 0.035582125186920166, 0.03497340902686119, 0.034863341599702835, 0.034116480499506, 0.03266691789031029, 0.034093938767910004, 0.03355437517166138, 0.03167238086462021, 0.03026765026152134, 0.0323699451982975, 0.03210970386862755, 0.03178790956735611, 0.03170080855488777, 0.03000306524336338, 0.028491826727986336, 0.029924245551228523, 0.02799660712480545, 0.02813299186527729, 0.030030135065317154, 0.028333598747849464, 0.027730587869882584, 0.027713019400835037, 0.027298133820295334, 0.026287022978067398, 0.025123268365859985, 0.026328573003411293, 0.02729864791035652, 0.02600870467722416, 0.027047690004110336, 0.025426547974348068, 0.025718430057168007, 0.025729000568389893, 0.024663954973220825, 0.024600084871053696, 0.023437021300196648, 0.02628372609615326, 0.026078613474965096, 0.02398199774324894, 0.02468922734260559, 0.023706117644906044, 0.024659661576151848, 0.023565396666526794, 0.02418442629277706, 0.023511983454227448, 0.022226007655262947, 0.02340950258076191, 0.02359597384929657, 0.02346496656537056, 0.022511979565024376, 0.022730061784386635, 0.021266978234052658, 0.022406399250030518, 0.02012736164033413, 0.022591691464185715, 0.020986663177609444, 0.02152278646826744, 0.021429920569062233, 0.021216290071606636, 0.021209441125392914, 0.02115398459136486, 0.021148717030882835, 0.019272351637482643, 0.020893167704343796, 0.020302345976233482, 0.020857462659478188, 0.02069331333041191, 0.0203149002045393, 0.01931825466454029, 0.02091616578400135, 0.021054649725556374, 0.019252298399806023, 0.01926685869693756, 0.01960354670882225, 0.020323095843195915, 0.01864868775010109, 0.01916230097413063, 0.020680410787463188, 0.01830158196389675, 0.01855493150651455, 0.019898269325494766, 0.018773814663290977, 0.019574282690882683, 0.018672499805688858, 0.017627738416194916, 0.018322959542274475, 0.017511021345853806, 0.018665554001927376, 0.020008286461234093, 0.018709363415837288, 0.017284024506807327, 0.018366597592830658, 0.017653688788414, 0.017893921583890915, 0.01891065202653408, 0.017894543707370758, 0.016629576683044434, 0.016982749104499817, 0.01779841259121895, 0.01731148362159729, 0.017321834340691566, 0.016860879957675934, 0.016346178948879242, 0.016774941235780716, 0.017618246376514435, 0.016748134046792984, 0.016482997685670853]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:16 2016", "state": "available"}], "summary": "7e73c65f843860150322ffd88602ee93"}