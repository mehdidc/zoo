{"content": {"hp_model": {"f0": 64, "f1": 32, "f2": 16, "f3": 32, "nonlin": "rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.542733073234558, 1.175148606300354, 1.0296761989593506, 0.9324362277984619, 0.8565183281898499, 0.7947773933410645, 0.7420080900192261, 0.6959133148193359, 0.6554721593856812, 0.6192167401313782, 0.5865793824195862, 0.5568282008171082, 0.5299032926559448, 0.5053401589393616, 0.4829072058200836, 0.4624486267566681, 0.4437917172908783, 0.42685627937316895, 0.4114384949207306, 0.3974747657775879, 0.38477250933647156, 0.3733137249946594, 0.36291971802711487, 0.3535056710243225, 0.34505149722099304, 0.33735743165016174, 0.3303907811641693, 0.3241228461265564, 0.3184588849544525, 0.31335315108299255, 0.3087448477745056, 0.30460020899772644, 0.3008286952972412, 0.2974347770214081, 0.2943588197231293, 0.29159072041511536, 0.28911659121513367, 0.2868359088897705, 0.2847929894924164, 0.2829616367816925, 0.2812827527523041, 0.2797502875328064, 0.27836528420448303, 0.2771051824092865, 0.27596259117126465, 0.2749195992946625, 0.273978590965271, 0.27311867475509644, 0.27232638001441956, 0.2715992331504822, 0.27094271779060364, 0.27034011483192444, 0.2697961628437042, 0.269289493560791, 0.268824964761734, 0.2683950662612915, 0.2680076062679291, 0.2676528990268707, 0.26732945442199707, 0.26703569293022156, 0.2667676508426666, 0.26652607321739197, 0.2663031816482544, 0.2661020755767822, 0.265919029712677, 0.2657526731491089, 0.2656027674674988, 0.2654666602611542], "moving_avg_accuracy_train": [0.05231040484265411, 0.10788860266126797, 0.16193487672745938, 0.21451453239635243, 0.2654859491041147, 0.31374067183859067, 0.3589112064162211, 0.4008571458348149, 0.4396965167591315, 0.47583748809332355, 0.5092407271023743, 0.5402730850688164, 0.5689556275505376, 0.5955603582328686, 0.6201602717624333, 0.6428953238878233, 0.6639264601613702, 0.6833705576968205, 0.7013863924656213, 0.7180306520920289, 0.7333685586724624, 0.7474052976222612, 0.7603591971639756, 0.7722642085741469, 0.7832645319027787, 0.7934762846925673, 0.8028738364962433, 0.8115153198755042, 0.8194484038382582, 0.8267835279535557, 0.8335246846347136, 0.8397218979322704, 0.845443513077443, 0.8507115492973842, 0.8555690393358075, 0.8600802892989599, 0.864203193283654, 0.8679882476806025, 0.8714435887140373, 0.8746232222060518, 0.8775198056298266, 0.8802011354731285, 0.882639908969005, 0.8848906086867224, 0.886937128723135, 0.888795272797573, 0.8905141054407578, 0.8920866314565289, 0.8935089163659701, 0.8948123324190187, 0.8959830817179529, 0.89705535727747, 0.8980599328107972, 0.8989873022788869, 0.8998242599489772, 0.9005984481913442, 0.9012812667166172, 0.9018888279429345, 0.9024240073025724, 0.9029126441726751, 0.903345441909339, 0.9037349598723364, 0.9040855260390341, 0.9044033607378716, 0.9046894119668254, 0.9049561586681218, 0.9052008809969077, 0.9054281065392435], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.05142719314759035, 0.1047329160391566, 0.15612206442959334, 0.20571717161615205, 0.25341173245717236, 0.2983623219655967, 0.3393671689294286, 0.3771535259728562, 0.41171056371819104, 0.4433754801438117, 0.47205803990427986, 0.49828032326324945, 0.5219302359199818, 0.5434114993197005, 0.5629399488794473, 0.5806885109380387, 0.5968239967229999, 0.6114944773217843, 0.6245870170707806, 0.636331622733808, 0.646888531290623, 0.6564650501965758, 0.6650106750244333, 0.6727760090656647, 0.6796549464215229, 0.685834812519205, 0.6914719932119381, 0.6965831064378075, 0.7011454577386804, 0.7051417106282161, 0.7087505452600481, 0.7120107034599469, 0.7149702894110155, 0.7176827448919771, 0.7201982265210024, 0.722437745924625, 0.724466549927795, 0.7263402721469885, 0.7279889715418529, 0.7294982445683905, 0.7307823185961146, 0.7319746063148164, 0.7330110441678981, 0.7339438382356716, 0.734709081200508, 0.7353866223462705, 0.7359597882837068, 0.736488874167309, 0.736989465525051, 0.7374155836845188, 0.7377502619030398, 0.7380514722997087, 0.7383724192903703, 0.7386612715819658, 0.7389090316131518, 0.7391442226724692, 0.7394057522595144, 0.7396167148253552, 0.7398309951971118, 0.7399994334691928, 0.7401632349453157, 0.7403228633050762, 0.7404543217976107, 0.7405970485033919, 0.7407377095698449, 0.7408643045296526, 0.7409660329622294, 0.7410575885515487], "moving_var_accuracy_train": [0.024627406093221345, 0.04996509013878399, 0.07125757878884645, 0.08901340262229601, 0.10349483025083359, 0.11410201162138056, 0.12105520520550266, 0.12478484118832749, 0.12588282767366457, 0.12505007318710815, 0.1225870532550593, 0.11899541309817002, 0.11450006597729467, 0.10942036463167944, 0.10392472987947007, 0.09818420024782093, 0.09234655845964736, 0.08651455897439554, 0.08078423579870657, 0.07519909462543728, 0.0697964475673245, 0.06459007317369524, 0.059641297476357186, 0.05495273139880824, 0.05054652227893739, 0.046430389106401446, 0.042582176014886286, 0.038996035528344034, 0.03566283636595059, 0.0325807891414369, 0.029731698967892493, 0.027104178145001965, 0.024688392249327, 0.022469322874925812, 0.02043474747269366, 0.01857443511149468, 0.016869976635748282, 0.015311918703264179, 0.013888181267853803, 0.012590353763360285, 0.011406830146802232, 0.010330852899679244, 0.009351296155189029, 0.008461757382644127, 0.00765327584271466, 0.006919022553055518, 0.006253709768647464, 0.0056505943344152074, 0.005103740950246306, 0.004608656895887779, 0.004160127091587596, 0.0037544623563086736, 0.003388098668697243, 0.0030570289290006237, 0.002757630519374268, 0.0024872617743484145, 0.0022427317671596786, 0.0020217807662372277, 0.001822180442136347, 0.0016421112918401258, 0.0014795859875838654, 0.0013329929070169581, 0.0012007996860503606, 0.0010816288875073906, 0.0009742024265069249, 0.0008774225680801052, 0.0007902193124359519, 0.000711662064216165], "duration": 19695.963365, "accuracy_train": [0.5231040484265412, 0.6080923830287929, 0.648351343323182, 0.6877314334163898, 0.7242286994739756, 0.7480331764488741, 0.7654460176148948, 0.7783706006021595, 0.7892508550779809, 0.8011062301010521, 0.8098698781838316, 0.8195643067667959, 0.8270985098860282, 0.8350029343738464, 0.8415594935285161, 0.8475107930163345, 0.8532066866232927, 0.8583674355158729, 0.8635289053848283, 0.8678289887296974, 0.871409717896364, 0.8737359481704503, 0.8769442930394058, 0.8794093112656883, 0.8822674418604651, 0.8853820598006644, 0.8874518027293282, 0.889288670288852, 0.8908461595030455, 0.892799644991233, 0.8941950947651348, 0.8954968176102805, 0.8969380493839978, 0.8981238752768549, 0.8992864496816169, 0.9006815389673312, 0.9013093291459026, 0.9020537372531378, 0.9025416580149501, 0.9032399236341824, 0.9035890564437985, 0.9043331040628461, 0.9045888704318937, 0.9051469061461794, 0.905355809050849, 0.9055185694675157, 0.9059835992294205, 0.9062393655984681, 0.9063094805509413, 0.9065430768964563, 0.9065198254083611, 0.906705837313123, 0.907101112610742, 0.9073336274916944, 0.9073568789797897, 0.9075661423726468, 0.9074266334440754, 0.9073568789797897, 0.9072406215393135, 0.9073103760035992, 0.9072406215393135, 0.9072406215393135, 0.9072406215393135, 0.9072638730274087, 0.9072638730274087, 0.9073568789797897, 0.9074033819559801, 0.9074731364202658], "end": "2016-01-31 08:21:31.066000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0], "moving_var_accuracy_valid": [0.023802805755356083, 0.04699602601675254, 0.06606402456572624, 0.08159469402076987, 0.09390816482305299, 0.1027023478151415, 0.10756469030437281, 0.10965850028145606, 0.10944034997290293, 0.1075203173657865, 0.10417248873892333, 0.09994371316605384, 0.09498320716748805, 0.08963788854597209, 0.08410634277124306, 0.07853081159044778, 0.07302091554505316, 0.06765583099954162, 0.06243297927329888, 0.057431103205589126, 0.0526910277495228, 0.04824731240377515, 0.04407983049668408, 0.04021455116196284, 0.03661897405807896, 0.033300793357138585, 0.030256714276887434, 0.027466154154867608, 0.024906874183914036, 0.022559917099936724, 0.020421138576542246, 0.01847468240228333, 0.01670604650307086, 0.015101658585389569, 0.01364844155728428, 0.012328736425988675, 0.011132907194539316, 0.010051213989677675, 0.009070556477961542, 0.008184001975783095, 0.007380441393182869, 0.006655191203902086, 0.005999339914321581, 0.005407236865845283, 0.00487178355041784, 0.004388736753413866, 0.003952819750799014, 0.0035600571625691566, 0.0032063067716792547, 0.002887310284683783, 0.0025995873418049767, 0.002340445156952032, 0.002107327703994162, 0.0018973458544119858, 0.0017081637342682666, 0.0015378451943508854, 0.0013846762544398974, 0.0012466091758335826, 0.0011223615029497059, 0.00101038069571825, 0.0009095841044586452, 0.0008188550249319395, 0.0007371250544560806, 0.0006635958872233606, 0.0005974143683215657, 0.0005378171680440475, 0.00048412858930559375, 0.0004357911722084547], "accuracy_test": 0.6809729751275511, "start": "2016-01-31 02:53:15.102000", "learning_rate_per_epoch": [0.0002103011211147532, 0.0001895007590064779, 0.00017075770301744342, 0.0001538684737170115, 0.00013864971697330475, 0.00012493621034082025, 0.0001125790731748566, 0.00010144415136892349, 9.141056216321886e-05, 8.236936992034316e-05, 7.422241469612345e-05, 6.688125722575933e-05, 6.0266196669545025e-05, 5.4305415687849745e-05, 4.8934198275674134e-05, 4.409423490869813e-05, 3.9732982259010896e-05, 3.5803092032438144e-05, 3.226189437555149e-05, 2.9070948585285805e-05, 2.6195612008450553e-05, 2.3604668967891484e-05, 2.1269988792482764e-05, 1.9166225683875382e-05, 1.7270540411118418e-05, 1.5562352928100154e-05, 1.402311863785144e-05, 1.2636126484721899e-05, 1.1386317964934278e-05, 1.0260125236527529e-05, 9.245321052731015e-06, 8.330888704222161e-06, 7.506901056331117e-06, 6.7644118644238915e-06, 6.095360276958672e-06, 5.4924830692471005e-06, 4.949235062667867e-06, 4.459718184079975e-06, 4.0186182559409644e-06, 3.621146561272326e-06, 3.2629877750878222e-06, 2.9402535801636986e-06, 2.6494403755350504e-06, 2.3873906229709974e-06, 2.1512596504180692e-06, 1.9384838196856435e-06, 1.7467531279180548e-06, 1.573986082803458e-06, 1.418306965206284e-06, 1.2780257065969636e-06, 1.1516193580973777e-06, 1.037715492202551e-06, 9.350775940220046e-07, 8.425913620158099e-07, 7.592527140332095e-07, 6.84156873376196e-07, 6.164885917314677e-07, 5.555132247536676e-07, 5.005687739867426e-07, 4.5105875301487686e-07, 4.0644565046932257e-07, 3.662451035779668e-07, 3.300206969925057e-07, 2.9737915951955074e-07, 2.6796610086421424e-07, 2.4146223154275503e-07, 2.17579795958045e-07, 1.9605951706580527e-07], "accuracy_train_first": 0.5231040484265412, "accuracy_train_last": 0.9074731364202658, "batch_size_eval": 1024, "accuracy_train_std": [0.017262412695814706, 0.018445776800139826, 0.021139516324234543, 0.021040727924409926, 0.02042053704146823, 0.020447064850127518, 0.019619881280031056, 0.01983769000046835, 0.01984012417178849, 0.020099433445215593, 0.020602808213614, 0.020740780020235775, 0.020848158640626686, 0.020340147374098293, 0.019573073744606144, 0.018961368607927038, 0.01890206449757844, 0.018275446132698967, 0.017990934602043773, 0.01778100021768445, 0.01766763159168824, 0.01751829013733475, 0.01709909893716207, 0.016902240203213768, 0.017222400643893694, 0.016971339339788245, 0.016284096409923208, 0.01585868796277578, 0.0156382023266761, 0.015503471299836912, 0.015393353419708215, 0.015187553265933061, 0.015378277596290577, 0.015016468688204753, 0.015029007423911151, 0.014612253649850498, 0.014384818101078183, 0.014014129764886205, 0.013598041087192142, 0.013258288727061793, 0.013041397851908946, 0.012818375476034946, 0.012669871208834449, 0.012702029870740194, 0.012551680372259177, 0.01229086763366105, 0.012157531710886411, 0.01200919156459144, 0.011831506615669231, 0.011300862149373652, 0.011378531388048143, 0.011248128055691139, 0.011151062861855717, 0.01125664057216889, 0.011174399691247715, 0.011241932221825056, 0.011227092616665185, 0.011241256136588804, 0.011051538485458558, 0.010959415794614463, 0.011078216133949085, 0.011020676338377852, 0.010915092338514358, 0.010934782596601677, 0.01083255419387913, 0.010775086420134028, 0.010791270042387805, 0.010786744455838626], "accuracy_test_std": 0.012784018145273543, "error_valid": [0.48572806852409633, 0.415515577936747, 0.38137560005647586, 0.3479268637048193, 0.3173372199736446, 0.29708237245858427, 0.29158920839608427, 0.28276926063629515, 0.27727609657379515, 0.27164027202560237, 0.26979892225150603, 0.26571912650602414, 0.2652205501694277, 0.26325713008283136, 0.26130400508283136, 0.2595744305346386, 0.25795663121234935, 0.2564711972891567, 0.257580125188253, 0.2579669262989458, 0.25809929169804224, 0.25734627964984935, 0.25807870152484935, 0.257335984563253, 0.258434617375753, 0.2585463926016567, 0.2577933805534638, 0.25741687452936746, 0.2577933805534638, 0.2588920133659638, 0.2587699430534638, 0.2586478727409638, 0.25839343702936746, 0.25790515577936746, 0.2571624388177711, 0.2574065794427711, 0.2572742140436747, 0.2567962278802711, 0.25717273390436746, 0.2569182981927711, 0.25766101515436746, 0.25729480421686746, 0.25766101515436746, 0.25766101515436746, 0.2584037321159638, 0.25851550734186746, 0.25888171827936746, 0.2587493528802711, 0.2585052122552711, 0.2587493528802711, 0.2592376341302711, 0.2592376341302711, 0.2587390577936747, 0.2587390577936747, 0.2588611281061747, 0.2587390577936747, 0.25824048145707834, 0.25848462208207834, 0.25824048145707834, 0.25848462208207834, 0.25836255176957834, 0.25824048145707834, 0.25836255176957834, 0.25811841114457834, 0.25799634083207834, 0.25799634083207834, 0.25811841114457834, 0.25811841114457834], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.09890755121014569, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 32, "valid_ratio": 0.15, "learning_rate": 0.00023338461530831043, "optimization": "rmsprop", "nb_data_augmentation": 0, "learning_rate_decay_method": "exp", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 1.5567285421309148e-05, "rotation_range": [0, 0], "momentum": 0.7225689441176815}, "accuracy_valid_max": 0.7435288027108433, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.7418815888554217, "accuracy_valid_std": [0.019417558858735487, 0.022164538345041195, 0.023309774965563194, 0.02380804098631156, 0.02739234928380263, 0.023179238851886358, 0.025126313446659887, 0.02523440100603431, 0.027307859288896872, 0.028535601043379828, 0.0277173673252283, 0.025141848466355803, 0.023561547755042433, 0.023068206724676115, 0.02405582293620856, 0.022205566821385848, 0.021593707623846867, 0.0199078890148924, 0.020267274824326005, 0.022341241197037228, 0.022450539804332634, 0.021190674669059013, 0.02180953125065358, 0.021368297678173536, 0.021406361716521157, 0.021395916219310813, 0.019715974126003476, 0.019562068363228848, 0.020134769337500438, 0.019167897178592585, 0.018577577302508214, 0.019142760309460008, 0.019094820158774596, 0.01896564923471776, 0.01789569341768152, 0.017916207413950995, 0.017585811410177114, 0.018893335360606573, 0.019347235094815958, 0.01930823011383451, 0.01915291219899447, 0.019211777104618424, 0.019009219299672864, 0.019046808915344972, 0.01961920937445122, 0.018766376010876072, 0.018764964871353342, 0.01870723601650885, 0.019263980347006986, 0.019536385295081424, 0.019506845886198068, 0.02030923745558882, 0.020006117558263856, 0.019772364936195982, 0.0204451517704614, 0.020166359150595715, 0.02002500882777568, 0.019988504338528477, 0.020185100450288478, 0.020436734516626927, 0.020408282795522825, 0.020384908924929963, 0.020408282795522825, 0.020255114918141782, 0.020088169031038473, 0.020088169031038473, 0.020036172112466295, 0.020036172112466295], "accuracy_valid": [0.5142719314759037, 0.584484422063253, 0.6186243999435241, 0.6520731362951807, 0.6826627800263554, 0.7029176275414157, 0.7084107916039157, 0.7172307393637049, 0.7227239034262049, 0.7283597279743976, 0.730201077748494, 0.7342808734939759, 0.7347794498305723, 0.7367428699171686, 0.7386959949171686, 0.7404255694653614, 0.7420433687876506, 0.7435288027108433, 0.742419874811747, 0.7420330737010542, 0.7419007083019578, 0.7426537203501506, 0.7419212984751506, 0.742664015436747, 0.741565382624247, 0.7414536073983433, 0.7422066194465362, 0.7425831254706325, 0.7422066194465362, 0.7411079866340362, 0.7412300569465362, 0.7413521272590362, 0.7416065629706325, 0.7420948442206325, 0.7428375611822289, 0.7425934205572289, 0.7427257859563253, 0.7432037721197289, 0.7428272660956325, 0.7430817018072289, 0.7423389848456325, 0.7427051957831325, 0.7423389848456325, 0.7423389848456325, 0.7415962678840362, 0.7414844926581325, 0.7411182817206325, 0.7412506471197289, 0.7414947877447289, 0.7412506471197289, 0.7407623658697289, 0.7407623658697289, 0.7412609422063253, 0.7412609422063253, 0.7411388718938253, 0.7412609422063253, 0.7417595185429217, 0.7415153779179217, 0.7417595185429217, 0.7415153779179217, 0.7416374482304217, 0.7417595185429217, 0.7416374482304217, 0.7418815888554217, 0.7420036591679217, 0.7420036591679217, 0.7418815888554217, 0.7418815888554217], "seed": 452227441, "model": "residualv3", "loss_std": [0.29965460300445557, 0.1962396502494812, 0.1929808109998703, 0.18915386497974396, 0.18496182560920715, 0.18105639517307281, 0.17686384916305542, 0.17315895855426788, 0.16916543245315552, 0.16551923751831055, 0.16190707683563232, 0.1585218906402588, 0.15494301915168762, 0.15161392092704773, 0.1481943577528, 0.1447208970785141, 0.14145220816135406, 0.13827121257781982, 0.1355300098657608, 0.13288550078868866, 0.1304006576538086, 0.12822669744491577, 0.12604369223117828, 0.12409301847219467, 0.12224207073450089, 0.12044160068035126, 0.11880562454462051, 0.1173078790307045, 0.11596167087554932, 0.11472422629594803, 0.11357592791318893, 0.11253269016742706, 0.1115383431315422, 0.11067976802587509, 0.10988505929708481, 0.10919490456581116, 0.10853448510169983, 0.10791976004838943, 0.10736045241355896, 0.10686453431844711, 0.10640809684991837, 0.10600269585847855, 0.10561855137348175, 0.10527291893959045, 0.10495501011610031, 0.10465975105762482, 0.10438697785139084, 0.10413255542516708, 0.10389479249715805, 0.10367267578840256, 0.10347266495227814, 0.10328659415245056, 0.10311757028102875, 0.1029604971408844, 0.10281333327293396, 0.10267626494169235, 0.10255223512649536, 0.1024358868598938, 0.10232888907194138, 0.10223233699798584, 0.10214468091726303, 0.10206445306539536, 0.10198988020420074, 0.10192254930734634, 0.10186108946800232, 0.10180544853210449, 0.10175509750843048, 0.10170921683311462]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:25 2016", "state": "available"}], "summary": "9c87b6b5b1bf386f0225d546a758b426"}