{"content": {"hp_model": {"f0": 32, "f1": 32, "f2": 64, "f3": 64, "nonlin": "leaky_rectify", "nbg1": 5, "nbg3": 2, "nbg2": 3, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.010351765755169991, 0.012424657013103182, 0.010810158678532075, 0.014679287910849458, 0.009613732247989165, 0.011571287682414031, 0.014579723937142491, 0.01320312826030083, 0.010245547630694839, 0.011015205014893927, 0.008620335313207769, 0.006005507681098586, 0.011650970608959164, 0.009342904038038887, 0.008903003211981229, 0.010981000715864138, 0.01091057467445419, 0.011481663659606083, 0.010613330551481807, 0.007999165334862255, 0.010425468192719018, 0.014799342038102289, 0.01361126747132835, 0.012805768175609447, 0.016660460895926113, 0.014895888925358435, 0.006628299248010002, 0.012896986373664453, 0.012920409181683431, 0.012817027571821309, 0.01292438160243966, 0.013922902492432398, 0.0136878067110573, 0.008146459831590371, 0.013585799339207661, 0.014795934711282472, 0.01610817692875224, 0.019691726500156128, 0.014205039128069745, 0.013923511483209423, 0.005826193945047527, 0.010164540712779793, 0.010280793712905559, 0.013994310909674314, 0.011324855686249849, 0.009537265819414963, 0.014633836066245631, 0.01286549177406168, 0.013689055695940347, 0.008955644688043957, 0.01190742000734525, 0.01125205935734412, 0.011274012201248322, 0.010297271444152876, 0.01508554349598092, 0.009798425483736837, 0.009349542384617566, 0.00871414914453361, 0.008744437086830335, 0.010926139492236727, 0.010486240632156467, 0.011277755160750027, 0.013043200610241104, 0.01864566142022388, 0.009260457342522267, 0.008668332658798149, 0.00997555544881599, 0.01094714677248092, 0.013828459974436488, 0.009836800302436437, 0.010076325927776181, 0.013903526330441254, 0.004527078933346936, 0.014903373871458776, 0.01195885882739006, 0.011185814026396484, 0.011920046024171009, 0.009150864448595109, 0.010515863750044374, 0.013024926323309866, 0.009270970782620194, 0.009173456833586722, 0.0137498618186566, 0.010499247103130866, 0.010278299903374742, 0.00786038829176656, 0.010706902080596319, 0.015318546571848377, 0.008370489176479276, 0.011174905551643695, 0.012534210012205345, 0.008750453322029407, 0.009672996725440437, 0.012224166124839227, 0.013488527287734153, 0.01025269618317491, 0.009234757089829312, 0.007233223142823043, 0.011776853230743737, 0.00847621254214205, 0.011365634009830912, 0.010225720245629044, 0.0096314329199294, 0.01242587977712662, 0.00956466713837934, 0.011872056759566351, 0.012540797029201895, 0.007922754674027232, 0.012147152331546472, 0.014166433019445472, 0.01059969996153531, 0.009737371760691569, 0.010477768741425317, 0.010180049505336748, 0.01115363258483045, 0.009093622563828466, 0.01121669448626937, 0.007810470288597725, 0.007703177654379073, 0.009710073763719563, 0.005812933199978749, 0.012520000773369134, 0.011245685642773515, 0.009146167887517586, 0.009326317244152463, 0.010816602953260649, 0.009299842275516817, 0.007207540234930453, 0.009406132911867262, 0.007823018360675596, 0.011719834444611171, 0.013011949535406105, 0.009803716510415336, 0.00981843369111574, 0.006733183242068578, 0.009302039977707663, 0.0104538356891684, 0.009318601044597932, 0.007929380043952213, 0.008152783226168385, 0.008528637742281379, 0.012713404948267133, 0.010653784320286328, 0.011544971184145011, 0.010692223557203775, 0.007764803723776094, 0.012053297222579746, 0.011154279520416295, 0.009642163710868422, 0.005919206151529761, 0.009118753867943365, 0.013278451933779477, 0.009830067203947475, 0.00873325748809575, 0.012923784444742686, 0.01338753706228277, 0.011908201913638547, 0.012258008902041605, 0.014871300278112426, 0.0097593793916108, 0.009361773838857572, 0.009730979206375933, 0.008722227855606491, 0.008883395489750931, 0.010734653928381233, 0.0093135264405849, 0.012654923815334895, 0.014267944316114056, 0.011304595195064104, 0.007576019938807051, 0.008426016740942401, 0.00839720319807048, 0.01217010268001277, 0.009770921612765753, 0.010261426727297043, 0.009123352517806554, 0.007119545606931376, 0.01035693522583256, 0.013301917628245789, 0.010331946774400821, 0.009652912151522834, 0.009940453359218752, 0.013211073474551391, 0.007410496304911251, 0.011775584288588504, 0.010205826470805516, 0.013495754195657527, 0.0109977658814663, 0.009708287492507135, 0.008488311480730586, 0.01052658279823817, 0.007473417152861393], "moving_avg_accuracy_train": [0.041016886708656324, 0.0845709073055094, 0.1396959480406746, 0.1975119521661014, 0.2583082916428744, 0.3162308726968409, 0.3708460587464167, 0.4201740042052929, 0.467419751509939, 0.5095155660472526, 0.5493613948938009, 0.5853156468080752, 0.6179859713737609, 0.6476240674638211, 0.6745772276067431, 0.6998302714746679, 0.7232206423176958, 0.7434442951978679, 0.7631033789959567, 0.7807756280749509, 0.7966736767996171, 0.8121421338589503, 0.8256289423849693, 0.8381276123191376, 0.8497576315181947, 0.8605315684402033, 0.8694096592890586, 0.8789600438065812, 0.8875530647235421, 0.8957727756988162, 0.9029659024813247, 0.9098396061320018, 0.9151400937700013, 0.9205662246084865, 0.9258496679583613, 0.9302722706934867, 0.9345036931777095, 0.9379725016873195, 0.9418105751793018, 0.945027748241152, 0.9456472077516621, 0.9486457670729337, 0.9510445623144683, 0.9536056927270783, 0.9560897465567606, 0.9582463399439509, 0.9602012248852793, 0.9624186395991323, 0.9641422704308857, 0.9652402062592442, 0.9669491085869005, 0.9684429068055914, 0.9697269434310032, 0.9709499696605312, 0.9723087487361447, 0.9734317406030248, 0.9743586558284366, 0.975523122759897, 0.9764130328791639, 0.9774836331995901, 0.9785262285474975, 0.9793087433415573, 0.9797828529728869, 0.9804513310684554, 0.9809669308485146, 0.9817006879124727, 0.9817961302069582, 0.9824609542767478, 0.9830987874205016, 0.9818476675320413, 0.9825398539038372, 0.9827885808265765, 0.983389037066547, 0.9836249252861105, 0.9841022195503657, 0.9845527467763, 0.9851092838546225, 0.9855427379096364, 0.9859863610305868, 0.9859043520846894, 0.9862932125976583, 0.9867083272748157, 0.9869330849116291, 0.9872725505645231, 0.9875571072640232, 0.9881224891340586, 0.9884220694242334, 0.9886544532556196, 0.9887404018657812, 0.9889200261137269, 0.989081687936878, 0.9894573733098568, 0.9897699135086331, 0.9898117093601507, 0.9901469446741356, 0.9902557051543504, 0.9904163686044007, 0.9905377142213508, 0.9906771161623109, 0.9907049577079938, 0.9906881624205369, 0.9910520098689595, 0.9910981295665874, 0.9911140606575476, 0.9914120667941738, 0.991550063983804, 0.9916300836270903, 0.9918090581512861, 0.9918841407659286, 0.9921795436536214, 0.9923407745561165, 0.9920092989600471, 0.9921619776949947, 0.9922273449921711, 0.9922791640643825, 0.9922491073674773, 0.9923754801128725, 0.9922335213123179, 0.9923916795977528, 0.9925177820617962, 0.9925475689222925, 0.9926696721491108, 0.9926749333568189, 0.992816852223518, 0.9929190025666423, 0.9931620725480733, 0.9933204177111323, 0.9934187144816857, 0.9934374271108981, 0.9935542859248175, 0.9936710485525739, 0.9935553178782874, 0.9936324497809349, 0.9937972356433269, 0.993620058134965, 0.9938534764286113, 0.9938357243583784, 0.9935523553820735, 0.9936437344271996, 0.9938051027249742, 0.9938572921917718, 0.9940669870797374, 0.9940464851348682, 0.9941512662713905, 0.9942920362216324, 0.9944024891840022, 0.9945088362477449, 0.994576646819399, 0.9946470129779446, 0.9947521591503883, 0.994746809306778, 0.9947164178106239, 0.99470308845458, 0.994890982734122, 0.994936854698805, 0.9949734891694006, 0.9950320368298415, 0.9952219135040002, 0.9953532749809813, 0.9951390400793209, 0.9952275356249602, 0.995158408141045, 0.9951194809424351, 0.9951193236958291, 0.9951911896893414, 0.9952349427442168, 0.9953626761483665, 0.9955195249394915, 0.9955420702133995, 0.9957227962277738, 0.9957064492311961, 0.9956987123807047, 0.9958173072509768, 0.9959705095615934, 0.9959456672733004, 0.9959000216769227, 0.9959775592782873, 0.9959217490349824, 0.9958831455600556, 0.9955787572683542, 0.9957441888331855, 0.9957930958427241, 0.9959138420620232, 0.9958411881010681, 0.9958571436957233, 0.9958854906725888, 0.9960062980041394, 0.9960964594608775, 0.9962310471457421, 0.9961336481228438, 0.9962087133700832, 0.9962112039747508], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 448512692, "moving_var_accuracy_train": [0.015141464957436734, 0.030699892853052982, 0.05497883461223027, 0.07956516414828958, 0.10487440177743589, 0.1245821901632722, 0.13896933807201298, 0.1469716200935565, 0.15236390382957107, 0.1530760318606531, 0.15205763936280356, 0.14848624950295916, 0.14324377551770845, 0.1368251486245304, 0.1296808893372874, 0.12245224642491669, 0.11513100681599432, 0.10729887135675413, 0.10004730040310122, 0.09285334585038121, 0.08584274284460984, 0.07941192703431678, 0.07310778036884258, 0.06720295308306787, 0.06169997389389501, 0.056574675955700274, 0.051626592834215, 0.047284822150686336, 0.04322090001193165, 0.03950688284739185, 0.036021864218835846, 0.03284490801384824, 0.02981327373526869, 0.027096932424629072, 0.024638472144848197, 0.02235065966493803, 0.0202767381246041, 0.018357358004430776, 0.016654199477156417, 0.015081931352029825, 0.013577191787593297, 0.01230039483086263, 0.011122143315273648, 0.010068963484659847, 0.0091176018470527, 0.008247699717686487, 0.0074573239221223326, 0.006755843882029006, 0.006106997623023645, 0.005507147028470019, 0.004982715450112238, 0.0045045268031644915, 0.004068912873346631, 0.0036754837244349883, 0.0033245518771784165, 0.0030034466860582865, 0.00271083456396836, 0.00245195495668171, 0.002213886921196901, 0.0020028138944920807, 0.0018123155505781752, 0.0016365949601466593, 0.0014749584836146695, 0.001331484401931496, 0.0012007285499371207, 0.001085501289803584, 0.0009770331439074155, 0.0008833077489106184, 0.0007986384540929941, 0.0007328623174614022, 0.0006638881834749614, 0.0005980561508663231, 0.0005414954650447679, 0.0004878467078094505, 0.00044111232536072476, 0.0003988278658564245, 0.0003617326809467105, 0.0003272503546123117, 0.0002962965324120561, 0.00026672740837571527, 0.00024141558002506135, 0.00021882490377927858, 0.00019739705735910407, 0.000178694483988646, 0.00016155378822685476, 0.00014827531933485263, 0.0001342555225537183, 0.00012131599050415396, 0.0001092508759260369, 9.861617216748374e-05, 8.898976585631628e-05, 8.136104476591707e-05, 7.41040726719851e-05, 6.670938744362336e-05, 6.104989314094421e-05, 5.505136340535861e-05, 4.977854176246157e-05, 4.4933210414992435e-05, 4.061478548378431e-05, 3.65602833004e-05, 3.290679370548681e-05, 3.080757902645015e-05, 2.7745964362388704e-05, 2.4973652123082506e-05, 2.3275555827976076e-05, 2.1119389264291067e-05, 1.906507862766691e-05, 1.7446857687700036e-05, 1.5752908510124063e-05, 1.4962983453627156e-05, 1.3700643743538603e-05, 1.3319464006290665e-05, 1.219731477060861e-05, 1.1016039245409103e-05, 9.938602267071901e-06, 8.952872685624375e-06, 8.201316054070256e-06, 7.562555158157067e-06, 7.03142603160665e-06, 6.471399911386516e-06, 5.832245233771884e-06, 5.3832034923898214e-06, 4.845132265909756e-06, 4.541887721845094e-06, 4.181611183064458e-06, 4.295197207613812e-06, 4.091336202830172e-06, 3.769162878458261e-06, 3.3953980530407954e-06, 3.1787620892525937e-06, 2.983587481492417e-06, 2.805771034080561e-06, 2.5787379043266344e-06, 2.5652535378922147e-06, 2.591255009326801e-06, 2.8224864066731325e-06, 2.5430739899837888e-06, 3.0114483815736536e-06, 2.7854547124095136e-06, 2.741266788908868e-06, 2.49165377401946e-06, 2.6382359109679906e-06, 2.378195287561994e-06, 2.23918753794416e-06, 2.193614394169713e-06, 2.0840516668191248e-06, 1.9774337818373378e-06, 1.8210748663060848e-06, 1.6835299460915503e-06, 1.6146784096983104e-06, 1.453468156168367e-06, 1.3164341278978257e-06, 1.1863897607009571e-06, 1.385489127192323e-06, 1.2658783487680243e-06, 1.1513692738136591e-06, 1.0670828033202074e-06, 1.2848528854943312e-06, 1.3116701356563881e-06, 1.5935724598954218e-06, 1.5046983682879232e-06, 1.3972360127513575e-06, 1.2711503526007178e-06, 1.1440355398791019e-06, 1.0761144751028116e-06, 9.857319958908867e-07, 1.0340011991229618e-06, 1.152014968706925e-06, 1.0413880762165103e-06, 1.2312062990395772e-06, 1.110490687809614e-06, 9.999803487283798e-07, 1.0265650031492314e-06, 1.1351470346387226e-06, 1.0271865847635283e-06, 9.432196105052196e-07, 9.030063660830375e-07, 8.407387787945243e-07, 7.700769554028982e-07, 1.5269393489865826e-06, 1.6205538378705383e-06, 1.480025514321552e-06, 1.4632398081644527e-06, 1.3644232097300535e-06, 1.2302721177642256e-06, 1.1144768658645048e-06, 1.1343788814855399e-06, 1.0941027878672907e-06, 1.1477171133355622e-06, 1.1183245289559258e-06, 1.0572051981483926e-06, 9.515405063380444e-07], "duration": 81806.479726, "accuracy_train": [0.41016886708656336, 0.47655709267718716, 0.6358213146571613, 0.7178559892949428, 0.8054753469338316, 0.8375341021825397, 0.8623827331925988, 0.8641255133351791, 0.8926314772517534, 0.888377896883075, 0.9079738545127353, 0.9089039140365448, 0.9120188924649317, 0.9143669322743633, 0.9171556688930418, 0.9271076662859912, 0.9337339799049464, 0.9254571711194168, 0.940035133178756, 0.9398258697858989, 0.9397561153216132, 0.9513582473929494, 0.94701021911914, 0.9506156417266519, 0.9544278043097084, 0.9574970007382798, 0.949312476928756, 0.9649135044642857, 0.9648902529761905, 0.9697501744762828, 0.9677040435239018, 0.9717029389880952, 0.9628444825119971, 0.9694014021548542, 0.9734006581072352, 0.9700756953096161, 0.9725864955357143, 0.9691917782738095, 0.9763532366071429, 0.9739823057978036, 0.9512223433462532, 0.975632800964378, 0.9726337194882798, 0.9766558664405685, 0.9784462310239018, 0.9776556804286637, 0.9777951893572352, 0.9823753720238095, 0.9796549479166666, 0.9751216287144703, 0.9823292295358066, 0.9818870907738095, 0.9812832730597084, 0.9819572057262828, 0.9845377604166666, 0.9835386674049464, 0.9827008928571429, 0.9860033251430418, 0.9844222239525655, 0.9871190360834257, 0.9879095866786637, 0.9863513764880952, 0.9840498396548542, 0.9864676339285714, 0.9856073288690477, 0.9883045014880952, 0.9826551108573275, 0.9884443709048542, 0.9888392857142857, 0.9705875885358989, 0.98876953125, 0.9850271231312293, 0.9887931432262828, 0.9857479192621816, 0.9883978679286637, 0.9886074918097084, 0.9901181175595238, 0.9894438244047619, 0.98997896911914, 0.9851662715716132, 0.989792957214378, 0.9904443593692323, 0.9889559036429494, 0.9903277414405685, 0.9901181175595238, 0.993210925964378, 0.9911182920358066, 0.9907459077380952, 0.9895139393572352, 0.9905366443452381, 0.9905366443452381, 0.9928385416666666, 0.9925827752976191, 0.9901878720238095, 0.9931640625, 0.9912345494762828, 0.9918623396548542, 0.9916298247739018, 0.9919317336309523, 0.99095553161914, 0.9905370048334257, 0.9943266369047619, 0.9915132068452381, 0.9912574404761905, 0.9940941220238095, 0.9927920386904762, 0.9923502604166666, 0.9934198288690477, 0.9925598842977114, 0.9948381696428571, 0.9937918526785714, 0.9890260185954227, 0.9935360863095238, 0.9928156506667589, 0.9927455357142857, 0.9919785970953304, 0.9935128348214286, 0.9909558921073275, 0.9938151041666666, 0.9936527042381875, 0.9928156506667589, 0.9937686011904762, 0.9927222842261905, 0.9940941220238095, 0.9938383556547619, 0.9953497023809523, 0.9947455241786637, 0.9943033854166666, 0.9936058407738095, 0.9946060152500923, 0.9947219122023809, 0.9925137418097084, 0.9943266369047619, 0.9952803084048542, 0.9920254605597084, 0.9959542410714286, 0.9936759557262828, 0.9910020345953304, 0.9944661458333334, 0.9952574174049464, 0.9943269973929494, 0.9959542410714286, 0.9938619676310447, 0.9950942965000923, 0.9955589657738095, 0.9953965658453304, 0.9954659598214286, 0.9951869419642857, 0.9952803084048542, 0.9956984747023809, 0.9946986607142857, 0.9944428943452381, 0.9945831242501846, 0.99658203125, 0.9953497023809523, 0.9953031994047619, 0.9955589657738095, 0.9969308035714286, 0.9965355282738095, 0.993210925964378, 0.9960239955357143, 0.9945362607858066, 0.9947691361549464, 0.995117908476375, 0.9958379836309523, 0.9956287202380952, 0.9965122767857143, 0.9969311640596161, 0.9957449776785714, 0.9973493303571429, 0.9955593262619971, 0.9956290807262828, 0.9968846610834257, 0.9973493303571429, 0.9957220866786637, 0.9954892113095238, 0.9966753976905685, 0.9954194568452381, 0.9955357142857143, 0.9928392626430418, 0.9972330729166666, 0.9962332589285714, 0.9970005580357143, 0.9951873024524732, 0.9960007440476191, 0.996140613464378, 0.9970935639880952, 0.9969079125715209, 0.9974423363095238, 0.9952570569167589, 0.9968843005952381, 0.9962336194167589], "end": "2016-01-24 14:17:45.341000", "learning_rate_per_epoch": [0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513, 0.0006170545239001513], "accuracy_valid": [0.4056352362575301, 0.47212678840361444, 0.6163374199924698, 0.6929593373493976, 0.7685649825865963, 0.7963675993034638, 0.8154826336596386, 0.8084731504141567, 0.836937594126506, 0.8245364269578314, 0.8427263742469879, 0.835716891001506, 0.8360419215926205, 0.8358492564006024, 0.8342314570783133, 0.8366522731551205, 0.8442721079631024, 0.8352889095444277, 0.8442412227033133, 0.8459913874246988, 0.8448515742658133, 0.8493475856551205, 0.848168062876506, 0.8475371211408133, 0.8532538356551205, 0.8536406367658133, 0.8496534967996988, 0.8562349985881024, 0.8578322077371988, 0.8593176416603916, 0.8620649590549698, 0.8630812311746988, 0.8535597467996988, 0.8638857186558735, 0.8652887918862951, 0.8595514871987951, 0.8657667780496988, 0.8556643566453314, 0.8652990869728916, 0.8666521554969879, 0.8439367822853916, 0.8648416909826807, 0.8596941476844879, 0.8621046686746988, 0.8666830407567772, 0.8664286050451807, 0.8661432840737951, 0.8731527673192772, 0.8668962961219879, 0.8679243340549698, 0.8722776849585843, 0.8692671074924698, 0.8698568688817772, 0.8734586784638554, 0.8714437829442772, 0.8707216561558735, 0.8728689170745482, 0.8735498635165663, 0.8760324501129518, 0.8739675498870482, 0.8761442253388554, 0.8752691429781627, 0.8751779579254518, 0.8749323465737951, 0.8706304711031627, 0.8742822853915663, 0.8734174981174698, 0.8724203454442772, 0.8769472420933735, 0.8572527414344879, 0.8765310264495482, 0.8693479974585843, 0.8799489951995482, 0.8745970208960843, 0.8797048545745482, 0.8788915427334337, 0.8779443947665663, 0.8787694724209337, 0.8785547463290663, 0.8718202889683735, 0.881190288497741, 0.8785650414156627, 0.8709349115210843, 0.8769781273531627, 0.8795827842620482, 0.883997905685241, 0.8830816429781627, 0.8764280755835843, 0.8793489387236446, 0.8783620811370482, 0.8777105492281627, 0.8867952277861446, 0.8871408485504518, 0.8768354668674698, 0.8792665780308735, 0.8772531532379518, 0.8832963690700302, 0.884364116622741, 0.8843538215361446, 0.8789621376129518, 0.8798063347138554, 0.8869584784450302, 0.883143413497741, 0.8816888648343373, 0.8844553016754518, 0.8808637871799698, 0.8791753929781627, 0.8848626929593373, 0.8831948889307228, 0.8819521249058735, 0.8820241905120482, 0.8804372764495482, 0.8817697548004518, 0.8866628623870482, 0.8858186652861446, 0.8794195336031627, 0.8830007530120482, 0.8829095679593373, 0.8821565559111446, 0.8845773719879518, 0.885951030685241, 0.8814844338290663, 0.8808137824736446, 0.8850759483245482, 0.8842111610504518, 0.8878629753388554, 0.8870393684111446, 0.8902543768825302, 0.8846994423004518, 0.8827360222138554, 0.889979350997741, 0.8824712914156627, 0.8880159309111446, 0.887171733810241, 0.8812917686370482, 0.8912309393825302, 0.8854833396084337, 0.8868261130459337, 0.8854730445218373, 0.8889013083584337, 0.8863275367093373, 0.892542827560241, 0.8891454489834337, 0.8895925498870482, 0.8875173545745482, 0.887537944747741, 0.8881277061370482, 0.8893895896084337, 0.886073100997741, 0.8903867422816265, 0.8873952842620482, 0.888026225997741, 0.8837331748870482, 0.8882909567959337, 0.8837228798004518, 0.8878629753388554, 0.8855127541415663, 0.8908029579254518, 0.8869775978915663, 0.8845170721950302, 0.888270366622741, 0.883458149002259, 0.8805696418486446, 0.8878835655120482, 0.8860525108245482, 0.8842111610504518, 0.8874055793486446, 0.8892778143825302, 0.8886571677334337, 0.8921060217432228, 0.8858186652861446, 0.8870393684111446, 0.8877512001129518, 0.8888910132718373, 0.8890130835843373, 0.8852083137236446, 0.888270366622741, 0.8887483527861446, 0.8836625800075302, 0.8777002541415663, 0.8867143378200302, 0.8888601280120482, 0.8883512565888554, 0.8864290168486446, 0.888270366622741, 0.8881791815700302, 0.886998188064759, 0.887904155685241, 0.891322124435241, 0.8828580925263554, 0.8887277626129518, 0.8912206442959337], "accuracy_test": 0.43336854272959185, "start": "2016-01-23 15:34:18.862000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0], "accuracy_train_last": 0.9962336194167589, "batch_size_eval": 1024, "accuracy_train_std": [0.013193634062961038, 0.017038238222619413, 0.018825180375584533, 0.022691296845310138, 0.021397771017307245, 0.020653180018945796, 0.02295916567816504, 0.021934501356261207, 0.019342219090177113, 0.019361835694409198, 0.019861485638139224, 0.019272373134073702, 0.019818701389815927, 0.019860429701174288, 0.020100095582915437, 0.018198850385000003, 0.017652311165356573, 0.017018820677190245, 0.01655340307939449, 0.015769277110658855, 0.014996602440364938, 0.014252427721312275, 0.014213965238347465, 0.013173326135307938, 0.013349172630029703, 0.011257154236560001, 0.01371713287372179, 0.010968912612529136, 0.011116059600498903, 0.00975617440738598, 0.009890098425274121, 0.009481277179539786, 0.011143721844070562, 0.009384106199191961, 0.008553921253321507, 0.008562052665535703, 0.008511855016989199, 0.009321179091575149, 0.008163656741119043, 0.008586570751667494, 0.008507427704313156, 0.008390724059477621, 0.008690945097686897, 0.007775060406687387, 0.00707010233234536, 0.007508762127653741, 0.006893429719877435, 0.006442466667994512, 0.006382141710073679, 0.008024424317028156, 0.0064534073182211426, 0.006404466124849571, 0.006100073137070211, 0.006377076885008264, 0.005816265750392461, 0.006143038217242218, 0.006783539464151255, 0.004798318180552041, 0.005405116920205435, 0.005320811497222202, 0.005626549253198356, 0.0051122086170363994, 0.005874426531461272, 0.005892497790720669, 0.004717531544636352, 0.0051827874688315555, 0.0051816860358981755, 0.0045808273603755705, 0.005101781306511338, 0.007326583023649058, 0.004171876828768325, 0.0049387689888039064, 0.004502412231443115, 0.0043540961654010375, 0.004502263762920145, 0.004351198298720536, 0.003976548321548359, 0.004948580950306186, 0.003716731974800115, 0.005032063971110762, 0.0044514315155495384, 0.003360794025022123, 0.004135702773783922, 0.004469750370605014, 0.003735103878539737, 0.003267016895785568, 0.004229550510830278, 0.004531833731289161, 0.004077994817861632, 0.003978179447231674, 0.0042486647062331346, 0.003172784617450835, 0.0038184154148943822, 0.004356661299201306, 0.0032598553140479694, 0.003236060480138367, 0.0035897961790419797, 0.0033711104077768983, 0.0036667382935727413, 0.003777237498605353, 0.003995944310088522, 0.0024782203486745217, 0.0034918285610114875, 0.0038237211794404234, 0.0031460904530474045, 0.003409337402021759, 0.0038235090902213544, 0.002593036142444408, 0.0034099183420450785, 0.00286135045813154, 0.003258611236868728, 0.0035018695727395107, 0.003065671451608912, 0.0034618948930764584, 0.0031618604288150254, 0.0030780880653516764, 0.003159551283876078, 0.0037715278971831826, 0.0025895937113701367, 0.0030683922374604724, 0.003665778806292224, 0.002556818042018466, 0.003015261725966639, 0.0033352717777234203, 0.002489647048266477, 0.002293308780866435, 0.002618701151843349, 0.0028663530704150274, 0.002962078972154056, 0.0032688482270823954, 0.0026450547843360536, 0.002935733090240619, 0.0024319767233989737, 0.0024293855075657863, 0.0030663672622435, 0.002453665542833504, 0.0021449216495523045, 0.0039100130741931294, 0.002734596433777745, 0.0025536731563412986, 0.0024970161701025825, 0.002240845201480052, 0.0030980887971110917, 0.002147328470029679, 0.0027720025852825147, 0.002560150891210275, 0.002573363106263493, 0.002630708340795123, 0.0025921707259975927, 0.002485735240083069, 0.0029385318597432257, 0.0028068889796934297, 0.0024874822344746485, 0.0019326728657288277, 0.0023997518317411953, 0.002532598363874647, 0.002796468759144464, 0.0020055672110163526, 0.002175350375967552, 0.0027787677705698247, 0.002070427187451985, 0.0027957622571173914, 0.0022132405375115604, 0.002088320733729358, 0.0023926447828565597, 0.0021121772598913253, 0.002143176826972268, 0.0016993478925480634, 0.002608419047511757, 0.0020471875653883128, 0.0018729110009544653, 0.0021445933707198825, 0.0017988839877493043, 0.0020801961724651267, 0.0021309212672428373, 0.002447930094540896, 0.0022051973282588644, 0.0024805098996488546, 0.0024217292720036345, 0.0024830936362586604, 0.0017691005916692017, 0.001994213322395049, 0.001862587879857518, 0.0020818619246723304, 0.002222920089791131, 0.001952924651973753, 0.001923700594482241, 0.0014599507790605786, 0.0016362176419046858, 0.0020946649661091944, 0.0021757231334244022, 0.0019131932047699173], "accuracy_test_std": 0.016817657122719192, "error_valid": [0.5943647637424698, 0.5278732115963856, 0.3836625800075302, 0.30704066265060237, 0.23143501741340367, 0.2036324006965362, 0.18451736634036142, 0.19152684958584332, 0.16306240587349397, 0.17546357304216864, 0.15727362575301207, 0.16428310899849397, 0.16395807840737953, 0.16415074359939763, 0.16576854292168675, 0.16334772684487953, 0.15572789203689763, 0.1647110904555723, 0.15575877729668675, 0.15400861257530118, 0.15514842573418675, 0.15065241434487953, 0.15183193712349397, 0.15246287885918675, 0.14674616434487953, 0.14635936323418675, 0.15034650320030118, 0.14376500141189763, 0.14216779226280118, 0.1406823583396084, 0.13793504094503017, 0.13691876882530118, 0.14644025320030118, 0.1361142813441265, 0.13471120811370485, 0.14044851280120485, 0.13423322195030118, 0.14433564335466864, 0.1347009130271084, 0.13334784450301207, 0.1560632177146084, 0.1351583090173193, 0.14030585231551207, 0.13789533132530118, 0.13331695924322284, 0.1335713949548193, 0.13385671592620485, 0.12684723268072284, 0.13310370387801207, 0.13207566594503017, 0.12772231504141573, 0.13073289250753017, 0.13014313111822284, 0.1265413215361446, 0.12855621705572284, 0.1292783438441265, 0.12713108292545183, 0.12645013648343373, 0.12396754988704817, 0.12603245011295183, 0.12385577466114461, 0.12473085702183728, 0.12482204207454817, 0.12506765342620485, 0.12936952889683728, 0.12571771460843373, 0.12658250188253017, 0.12757965455572284, 0.12305275790662651, 0.14274725856551207, 0.12346897355045183, 0.13065200254141573, 0.12005100480045183, 0.12540297910391573, 0.12029514542545183, 0.12110845726656627, 0.12205560523343373, 0.12123052757906627, 0.12144525367093373, 0.1281797110316265, 0.11880971150225905, 0.12143495858433728, 0.12906508847891573, 0.12302187264683728, 0.12041721573795183, 0.11600209431475905, 0.11691835702183728, 0.12357192441641573, 0.12065106127635539, 0.12163791886295183, 0.12228945077183728, 0.11320477221385539, 0.11285915144954817, 0.12316453313253017, 0.12073342196912651, 0.12274684676204817, 0.11670363092996983, 0.11563588337725905, 0.11564617846385539, 0.12103786238704817, 0.12019366528614461, 0.11304152155496983, 0.11685658650225905, 0.11831113516566272, 0.11554469832454817, 0.11913621282003017, 0.12082460702183728, 0.11513730704066272, 0.11680511106927716, 0.11804787509412651, 0.11797580948795183, 0.11956272355045183, 0.11823024519954817, 0.11333713761295183, 0.11418133471385539, 0.12058046639683728, 0.11699924698795183, 0.11709043204066272, 0.11784344408885539, 0.11542262801204817, 0.11404896931475905, 0.11851556617093373, 0.11918621752635539, 0.11492405167545183, 0.11578883894954817, 0.11213702466114461, 0.11296063158885539, 0.10974562311746983, 0.11530055769954817, 0.11726397778614461, 0.11002064900225905, 0.11752870858433728, 0.11198406908885539, 0.11282826618975905, 0.11870823136295183, 0.10876906061746983, 0.11451666039156627, 0.11317388695406627, 0.11452695547816272, 0.11109869164156627, 0.11367246329066272, 0.10745717243975905, 0.11085455101656627, 0.11040745011295183, 0.11248264542545183, 0.11246205525225905, 0.11187229386295183, 0.11061041039156627, 0.11392689900225905, 0.10961325771837349, 0.11260471573795183, 0.11197377400225905, 0.11626682511295183, 0.11170904320406627, 0.11627712019954817, 0.11213702466114461, 0.11448724585843373, 0.10919704207454817, 0.11302240210843373, 0.11548292780496983, 0.11172963337725905, 0.11654185099774095, 0.11943035815135539, 0.11211643448795183, 0.11394748917545183, 0.11578883894954817, 0.11259442065135539, 0.11072218561746983, 0.11134283226656627, 0.10789397825677716, 0.11418133471385539, 0.11296063158885539, 0.11224879988704817, 0.11110898672816272, 0.11098691641566272, 0.11479168627635539, 0.11172963337725905, 0.11125164721385539, 0.11633741999246983, 0.12229974585843373, 0.11328566217996983, 0.11113987198795183, 0.11164874341114461, 0.11357098315135539, 0.11172963337725905, 0.11182081842996983, 0.11300181193524095, 0.11209584431475905, 0.10867787556475905, 0.11714190747364461, 0.11127223738704817, 0.10877935570406627], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-3, 3], "momentum": 0.778753833124276, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.0006170545380842083, "patience_threshold": 1, "do_flip": true, "batch_size": 64, "optimization": "rmsprop", "nb_data_augmentation": 1, "learning_rate_decay_method": "none", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 2.4810596230326775e-09, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.03369564004549921}, "accuracy_valid_max": 0.892542827560241, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-3, 3)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8912206442959337, "loss_train": [1.3664424419403076, 0.9586042165756226, 0.7769331932067871, 0.6688754558563232, 0.5905740261077881, 0.5319545269012451, 0.4860467314720154, 0.44654136896133423, 0.4121001064777374, 0.3823525607585907, 0.3571031391620636, 0.3330084979534149, 0.3147156536579132, 0.29526129364967346, 0.27814391255378723, 0.26494473218917847, 0.24656888842582703, 0.23853296041488647, 0.2260143756866455, 0.2171483337879181, 0.2078542560338974, 0.1991804987192154, 0.1902395784854889, 0.1826559454202652, 0.17647938430309296, 0.17160779237747192, 0.16529694199562073, 0.16015972197055817, 0.15459872782230377, 0.14883534610271454, 0.14542332291603088, 0.13969364762306213, 0.1376003473997116, 0.13387271761894226, 0.13003745675086975, 0.12275774031877518, 0.1241338774561882, 0.12030546367168427, 0.11676633358001709, 0.11467736959457397, 0.11147676408290863, 0.10881295800209045, 0.10499810427427292, 0.10448961704969406, 0.10111436992883682, 0.09835195541381836, 0.09704989194869995, 0.09405294060707092, 0.09249487519264221, 0.08958747982978821, 0.08884157240390778, 0.08604243397712708, 0.08569210767745972, 0.08357571810483932, 0.08151478320360184, 0.08101671189069748, 0.07965052127838135, 0.07713282108306885, 0.07668399810791016, 0.07446590811014175, 0.07223840802907944, 0.0723886787891388, 0.06962120532989502, 0.06924542784690857, 0.06683889776468277, 0.06550560891628265, 0.064431332051754, 0.06369524449110031, 0.062222130596637726, 0.061078477650880814, 0.06024766340851784, 0.06325756013393402, 0.06007986515760422, 0.05856310576200485, 0.059074677526950836, 0.05670963227748871, 0.05423307046294212, 0.05505776032805443, 0.05373469740152359, 0.05326163023710251, 0.052557844668626785, 0.05170557647943497, 0.05052996054291725, 0.049024906009435654, 0.04736191779375076, 0.05015334486961365, 0.04641370102763176, 0.04665859416127205, 0.04735739901661873, 0.047912441194057465, 0.043288879096508026, 0.044834867119789124, 0.04372962936758995, 0.04296362027525902, 0.04314667358994484, 0.04275820031762123, 0.04203981161117554, 0.04244903475046158, 0.041054047644138336, 0.0405082181096077, 0.039130035787820816, 0.038255784660577774, 0.039300475269556046, 0.03782295063138008, 0.03717381879687309, 0.03733839467167854, 0.0380251482129097, 0.0355633944272995, 0.03617206960916519, 0.035641130059957504, 0.034153491258621216, 0.03536378964781761, 0.033894509077072144, 0.03303839638829231, 0.03440803661942482, 0.03386538475751877, 0.033593837171792984, 0.03249434754252434, 0.03362070396542549, 0.031232312321662903, 0.029435116797685623, 0.031184082850813866, 0.030781451612710953, 0.03201969712972641, 0.02953115478157997, 0.02987223118543625, 0.028865542262792587, 0.02963501214981079, 0.02931286208331585, 0.028213415294885635, 0.028486989438533783, 0.028921015560626984, 0.027914704754948616, 0.027119994163513184, 0.028034135699272156, 0.027053721249103546, 0.027158886194229126, 0.027364710345864296, 0.026842474937438965, 0.02596740797162056, 0.026143884286284447, 0.025186335667967796, 0.02533235400915146, 0.025052132084965706, 0.025403546169400215, 0.023697590455412865, 0.024454394355416298, 0.025579283013939857, 0.024922292679548264, 0.02478589303791523, 0.024556990712881088, 0.023236727342009544, 0.022752031683921814, 0.02263951674103737, 0.022569067776203156, 0.021129433065652847, 0.024110816419124603, 0.022259894758462906, 0.02146715484559536, 0.02307846024632454, 0.02248579077422619, 0.021402882412075996, 0.02075750008225441, 0.022316811606287956, 0.020856093615293503, 0.020298153162002563, 0.021869143471121788, 0.02119932323694229, 0.020901478826999664, 0.018834829330444336, 0.021130457520484924, 0.019877899438142776, 0.018433265388011932, 0.018581321462988853, 0.020529739558696747, 0.019976569339632988, 0.0178123340010643, 0.019804468378424644, 0.018598197028040886, 0.020047714933753014, 0.017733782529830933, 0.019029488787055016, 0.01884055696427822, 0.019120289012789726, 0.019004812464118004, 0.0212433859705925, 0.019131476059556007, 0.019993312656879425, 0.018058231100440025, 0.017948491498827934, 0.018670745193958282, 0.01817581243813038], "accuracy_train_first": 0.41016886708656336, "model": "residualv5", "loss_std": [0.2563422620296478, 0.14204388856887817, 0.129171684384346, 0.12284917384386063, 0.11727628856897354, 0.11466581374406815, 0.11163268983364105, 0.10659103840589523, 0.10419927537441254, 0.0972166657447815, 0.09213986992835999, 0.09253491461277008, 0.08578571677207947, 0.08481395244598389, 0.08023878186941147, 0.07631620764732361, 0.07570432871580124, 0.07362718880176544, 0.07107055187225342, 0.06927802413702011, 0.06677935272455215, 0.06588950008153915, 0.06554892659187317, 0.05991146340966225, 0.06137314438819885, 0.0600905679166317, 0.05867497995495796, 0.05724749714136124, 0.056155405938625336, 0.05616150423884392, 0.054802704602479935, 0.05437067523598671, 0.05350569263100624, 0.04943280667066574, 0.05088082328438759, 0.048512011766433716, 0.05048999562859535, 0.04771875590085983, 0.0494319424033165, 0.04565291106700897, 0.04865262284874916, 0.045183274894952774, 0.04498330503702164, 0.04561389237642288, 0.04620330408215523, 0.045016758143901825, 0.04431112855672836, 0.041034091264009476, 0.03992839530110359, 0.04160977154970169, 0.03967912122607231, 0.03980569541454315, 0.03934631496667862, 0.03886972367763519, 0.03811432793736458, 0.040182966738939285, 0.03757965937256813, 0.03851541131734848, 0.03878865763545036, 0.03740419074892998, 0.036740802228450775, 0.035675324499607086, 0.0357857309281826, 0.035311419516801834, 0.033685628324747086, 0.033871620893478394, 0.03607496991753578, 0.03248923271894455, 0.032752275466918945, 0.033158883452415466, 0.03265737369656563, 0.03475841507315636, 0.03271523863077164, 0.030424270778894424, 0.033553000539541245, 0.032344408333301544, 0.02972419187426567, 0.031041843816637993, 0.030591782182455063, 0.029802339151501656, 0.03168794885277748, 0.03134814649820328, 0.028512991964817047, 0.029295869171619415, 0.02885667234659195, 0.030845029279589653, 0.028700003400444984, 0.02854645997285843, 0.02915637195110321, 0.030067700892686844, 0.027707939967513084, 0.028504468500614166, 0.027930308133363724, 0.027010241523385048, 0.028045134618878365, 0.02822871506214142, 0.026091337203979492, 0.027843989431858063, 0.026634976267814636, 0.02606639452278614, 0.02744433470070362, 0.02726895920932293, 0.025495564565062523, 0.025558341294527054, 0.024319203570485115, 0.02523052506148815, 0.026734162122011185, 0.025196809321641922, 0.024818573147058487, 0.026814054697752, 0.023266810923814774, 0.024745004251599312, 0.02400469221174717, 0.024443164467811584, 0.025313494727015495, 0.025570297613739967, 0.02458910457789898, 0.024554233998060226, 0.02611532434821129, 0.02259867824614048, 0.02116500400006771, 0.02274969592690468, 0.023623842746019363, 0.024495936930179596, 0.023197347298264503, 0.02467423304915428, 0.02251995913684368, 0.021790023893117905, 0.021986298263072968, 0.020490460097789764, 0.022065578028559685, 0.02345528081059456, 0.022186802700161934, 0.021526731550693512, 0.022826412692666054, 0.021839264780282974, 0.022014321759343147, 0.022143693640828133, 0.02253296785056591, 0.02196566015481949, 0.021427500993013382, 0.019590256735682487, 0.021243222057819366, 0.02189687080681324, 0.02163151279091835, 0.02114284783601761, 0.021682847291231155, 0.021457290276885033, 0.020831583067774773, 0.02141503617167473, 0.021283086389303207, 0.02020949125289917, 0.019633712247014046, 0.020744847133755684, 0.020759807899594307, 0.018054304644465446, 0.021300487220287323, 0.019518302753567696, 0.02038480155169964, 0.020732644945383072, 0.020245717838406563, 0.019597608596086502, 0.019771574065089226, 0.01970951445400715, 0.018482785671949387, 0.019262999296188354, 0.019343677908182144, 0.01975168287754059, 0.019350040704011917, 0.017249207943677902, 0.020495036616921425, 0.01877233386039734, 0.017229586839675903, 0.018329499289393425, 0.02051120065152645, 0.019354376941919327, 0.01784921996295452, 0.019201457500457764, 0.019140109419822693, 0.0211112629622221, 0.017078930512070656, 0.016932517290115356, 0.019504722207784653, 0.017663992941379547, 0.017633503302931786, 0.02137167938053608, 0.0192221961915493, 0.019608860835433006, 0.017872057855129242, 0.016912346705794334, 0.018611716106534004, 0.018682416528463364]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:10 2016", "state": "available"}], "summary": "bdbd8df47f1fb94327cbbc73c70c0404"}