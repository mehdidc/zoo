{"content": {"hp_model": {"f0": 64, "f1": 16, "f2": 64, "f3": 16, "nonlin": "very_leaky_rectify", "nbg1": 2, "nbg3": 3, "nbg2": 4, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.015710923082813646, 0.013070856095975731, 0.010139313623919047, 0.011396400364662843, 0.008024265095333459, 0.011120403404169237, 0.010477838930995286, 0.010724138618830642, 0.01167463588928283, 0.011092996048992661, 0.010835128956318796, 0.009183361865861966, 0.010767636124377014, 0.011470184183080273, 0.008627081362195718, 0.010969384081024228, 0.012581629017992434, 0.010247307267812927, 0.010474601463862147, 0.011250402940755508, 0.010227088324014167, 0.009498964189288946, 0.01161004096090435, 0.010152693599174976, 0.010704982693232433, 0.009357172620169843, 0.010131994558686248, 0.009109553165505381, 0.009817592866186206, 0.011033781401868075, 0.009575492629550496, 0.010437812878082502, 0.011504098365543848, 0.01110151711020229, 0.011218898440658329, 0.012209676145095678, 0.011038198128960579, 0.013100126746041461, 0.010298253529244858, 0.011043492461738703, 0.01047431978994741, 0.009737146621578647, 0.010836076766913668, 0.011202263407156672, 0.010040891945778332, 0.011363108550711353, 0.011287262067317632, 0.011091782357207253, 0.012379399379762504, 0.011234235458997485, 0.010125874741345046, 0.011311360819610972, 0.011753928191046835, 0.01065954505626995, 0.009746545389723913, 0.012392314183117774, 0.013270033940624938, 0.0134202329429475, 0.011909542996348152, 0.011526252766482269, 0.009968578660559288, 0.011353430247100385, 0.011549305448305291, 0.009415413324670238, 0.011832635322045, 0.012231143566301561, 0.012210040021379856, 0.012053121623572811, 0.01284277852680584, 0.011206827107967849, 0.010556918866398547, 0.01368558238562062, 0.011453038297677486, 0.0119820637772192, 0.01162520244792821, 0.012618039245070831, 0.011235569969018942, 0.01287411279342362, 0.012233257054991613, 0.012555586490409974, 0.012578069851206417, 0.011091669249344377, 0.012992881082694665, 0.012664681638695673, 0.011594468350582022, 0.012374136829269688, 0.013968169802671262, 0.012091263801658222, 0.011056736276605398, 0.012579647738907062, 0.011779675418174934, 0.013146565460321372, 0.01269322473816084, 0.012009699839083648, 0.012684796930378259, 0.0110141892531148, 0.012914624206210083, 0.012589493630934532, 0.013797474824455591, 0.013832713844681169, 0.01370686787602703, 0.013669067421085132, 0.012469756634413538, 0.013695133626467003, 0.01358313576177233, 0.012945538793819303, 0.012962518529112656, 0.01251261602314666, 0.013097415925857241, 0.012177627995985756, 0.013548356631600061, 0.013547616699183989, 0.012681564785716795, 0.013473984947567724, 0.012249442702968554, 0.012253035364593054, 0.012028692375317352, 0.012830918669777914, 0.012159178176180835, 0.014638589106747089, 0.012909854540289804, 0.012501857509222556, 0.013932708898652238, 0.01152502999141502, 0.012702396914401053, 0.011669836331286947, 0.014877124063547149, 0.010575422502675095, 0.012369790724341507, 0.01323537896515247, 0.011900374086295452, 0.013032175129223026, 0.011481438436498043, 0.013080785094295791, 0.01402778984862604, 0.01286955577122093, 0.013194699588487265, 0.012802847706805268, 0.011881010211815477, 0.014431146475117394, 0.009694813145712725, 0.0106933782232958, 0.01273056635819329, 0.01185753052461677, 0.012548756870638816, 0.013595191574675416, 0.012343969866714422, 0.011108133419248028, 0.01135125965446472, 0.010611646183376015, 0.011858292007590466, 0.011535236762335097, 0.01182733166884711, 0.011749298270960497], "moving_avg_accuracy_train": [0.06160880110280545, 0.12679181374584714, 0.19035289198043553, 0.2509290397748246, 0.3076073836920911, 0.359840849393803, 0.4080180850812684, 0.4521587751023498, 0.4925733157295733, 0.5297485065357227, 0.5634896301231065, 0.5943892446244079, 0.6226267250565315, 0.64847508002873, 0.6720130031120511, 0.6934899944905839, 0.713149133422847, 0.7309425201047871, 0.7471682287578374, 0.7619433554598498, 0.7754175366059098, 0.7878000660064115, 0.799111861327605, 0.8093552200857175, 0.8188299372394289, 0.8274639232300947, 0.8354183055241026, 0.8426841343363102, 0.849314133168506, 0.8553066366567496, 0.8608673726080919, 0.8659509818773676, 0.8705983458816298, 0.8749181572652276, 0.8789313292532673, 0.882654827234179, 0.8860082284681716, 0.8890938630895161, 0.8919963480891654, 0.8946993014412399, 0.8972224960664035, 0.8994608912433548, 0.9016754097002302, 0.9037172683875991, 0.905566675096735, 0.9073170995432723, 0.908913479982079, 0.9103664263210344, 0.9118903168653798, 0.9131896305957392, 0.9144939436816526, 0.9157631205113463, 0.9168030731104516, 0.9178158324579981, 0.9188574881553047, 0.9198135794733566, 0.9208135345393561, 0.921673930520175, 0.9224925728767492, 0.9233200318012376, 0.9240414212475443, 0.9248324337289637, 0.9254583865539263, 0.9261519884785447, 0.9268436234773588, 0.9274800458691486, 0.9281411456277026, 0.9286734284901814, 0.9291756624568702, 0.9297368468744814, 0.9303048721122824, 0.9307950963893802, 0.9313269069447021, 0.9317729843611584, 0.9321209395645313, 0.9325364057951859, 0.9330056204551469, 0.9334372863419874, 0.9338210632448872, 0.9341433551646768, 0.9345193042031648, 0.9348298286497273, 0.9351905727135106, 0.935459582994762, 0.935683091057412, 0.9359702427709307, 0.9362286432642787, 0.9364983700404256, 0.9368202512961191, 0.9371448216583862, 0.937406672001084, 0.9377888216845118, 0.9381188055067399, 0.9383693240193733, 0.938697025130725, 0.9389199846643025, 0.9392833726123702, 0.9395893512310705, 0.9398370104462801, 0.9401086597673314, 0.9403530360098212, 0.9405986233626044, 0.940856926458699, 0.9411242043796896, 0.9414136547312185, 0.9417066039844717, 0.9419284056338282, 0.9422094073265823, 0.9424461049060319, 0.9426451097370419, 0.9429172200373318, 0.9431203387266588, 0.9432961340518058, 0.9434822155813336, 0.9437102509734124, 0.9439898515393695, 0.9441972781725311, 0.9443769866959479, 0.9445620119039372, 0.9447656288256236, 0.9449814361384747, 0.9452570789771928, 0.9455283729713155, 0.9457609839196159, 0.9459865737659342, 0.9462546727454685, 0.9464005586282214, 0.9466342706167743, 0.9468678628945671, 0.9470641811005424, 0.9471711490704531, 0.9473581370957629, 0.9474961993840179, 0.9477040526541339, 0.9478957348460387, 0.9480287573378099, 0.9481345627363655, 0.9482228481974557, 0.9484046116600561, 0.948612268457321, 0.9486992863225063, 0.948849573867812, 0.9490894643550156, 0.9493170636351841, 0.9494544015742219, 0.9495990041562793, 0.9497059310408543, 0.9497184598798291, 0.9498320423825254, 0.9499550848277815, 0.9500426436380542, 0.9501982125268328, 0.950291721550543, 0.9504083236087593], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 485221419, "moving_var_accuracy_train": [0.03416079935992539, 0.06898414565893926, 0.09844582709013655, 0.12162647151559192, 0.1383757363868682, 0.14909317720108858, 0.15507327382735003, 0.15710155108444976, 0.15609141182299024, 0.15292022394395371, 0.14787437233801032, 0.14168001069117062, 0.13468820733244463, 0.12723262369211916, 0.11949566573059434, 0.11169744958559706, 0.10400604031905958, 0.09645487777367101, 0.08917885258794694, 0.08222570665069695, 0.07563711800363881, 0.06945334951246353, 0.06365962498171435, 0.05823800007136952, 0.053222132450518066, 0.04857083063224939, 0.04428319734813749, 0.04033000802827851, 0.03669261918608493, 0.03334654814998593, 0.030290189393872305, 0.027493758203309068, 0.024938764312671165, 0.022612834814912802, 0.02049650127807181, 0.018571631085189312, 0.016815675675195665, 0.01521979837682406, 0.01377363831170036, 0.012462028091941746, 0.01127312388279566, 0.010190905211229886, 0.00921595151806947, 0.008331879048355181, 0.007529473890101838, 0.006804102372778972, 0.006146628010049719, 0.005550964686619699, 0.005016768399478038, 0.004530285505059338, 0.004092568048188167, 0.003697808531794633, 0.0033377611912906425, 0.0030132162056259646, 0.002721660004388948, 0.0024577209994261435, 0.0022209480906896922, 0.002005515812815007, 0.001810995809305303, 0.0016360584228202095, 0.0014771362051373721, 0.0013350538913354869, 0.0012050748546536462, 0.001088897121856791, 0.0009843126404153722, 0.0008895266775207777, 0.0008045074857855406, 0.0007266066626181852, 0.0006562161469720301, 0.0005934288838299542, 0.000536989869483969, 0.00048545376111227575, 0.0004394537872018139, 0.00039729927403488375, 0.0003586590020433839, 0.0003243466115383751, 0.0002938934119586378, 0.0002661810897035296, 0.00024088854313397107, 0.00021773453755462893, 0.00019723312291502668, 0.00017837763951074025, 0.00016171110207566003, 0.00014619129065086462, 0.0001320217642724044, 0.00011956169280435443, 0.00010820646085858171, 9.804058757665885e-05, 8.916899670389445e-05, 8.120021031406428e-05, 7.369727970039618e-05, 6.764189715525347e-05, 6.185771134611817e-05, 5.623677593805534e-05, 5.1579590509680144e-05, 4.6869030041230294e-05, 4.337058424431515e-05, 3.987613205579893e-05, 3.6440534632123535e-05, 3.346062135155944e-05, 3.0652036947444525e-05, 2.812965158332319e-05, 2.5917170830059763e-05, 2.3968391130495168e-05, 2.232558557144813e-05, 2.0865400399137756e-05, 1.922162410413918e-05, 1.8010119255701463e-05, 1.671333902718686e-05, 1.5398431429355905e-05, 1.4524984426135075e-05, 1.3443800801106826e-05, 1.237755668808792e-05, 1.145143803996179e-05, 1.0774295496330857e-05, 1.0400454235049163e-05, 9.747641084847118e-06, 9.063533356860514e-06, 8.465288969497429e-06, 7.99189872972104e-06, 7.611864023268968e-06, 7.534488391770966e-06, 7.443443433817466e-06, 7.1860697698583955e-06, 6.925479801729446e-06, 6.879825387002522e-06, 6.383387065381592e-06, 6.236640001183625e-06, 6.104064171265376e-06, 5.84052529611468e-06, 5.359452085784555e-06, 5.1381875716894544e-06, 4.79591957346444e-06, 4.70515445319969e-06, 4.565317572120605e-06, 4.268040664761652e-06, 3.941989639557206e-06, 3.617939579360816e-06, 3.5534872284528333e-06, 3.586230614660679e-06, 3.2957565329471316e-06, 3.1694579961184275e-06, 3.3704392091638964e-06, 3.4996081792462008e-06, 3.3194027468141553e-06, 3.1756516327715503e-06, 2.9609866972988523e-06, 2.6663007738234633e-06, 2.5157795607099492e-06, 2.400456594650451e-06, 2.2294098424929064e-06, 2.2242839706457717e-06, 2.080551011218353e-06, 1.9948602699189858e-06], "duration": 112795.129218, "accuracy_train": [0.6160880110280547, 0.7134389275332226, 0.7624025960917312, 0.7961143699243264, 0.8177124789474898, 0.82994204070921, 0.841613206268457, 0.849424985292082, 0.8563041813745846, 0.8643252237910668, 0.8671597424095607, 0.8724857751361205, 0.8767640489456442, 0.8811102747785161, 0.8838543108619417, 0.8867829168973791, 0.8900813838132153, 0.8910830002422481, 0.8931996066352897, 0.8949194957779623, 0.8966851669204503, 0.8992428306109265, 0.9009180192183462, 0.9015454489087301, 0.9041023916228312, 0.9051697971460871, 0.9070077461701735, 0.9080765936461794, 0.9089841226582688, 0.9092391680509413, 0.9109139961701735, 0.911703465300849, 0.912424621919989, 0.913796459717608, 0.9150498771456257, 0.9161663090623846, 0.9161888395741048, 0.9168645746816169, 0.9181187130860096, 0.9190258816099114, 0.9199312476928755, 0.9196064478359173, 0.9216060758121077, 0.9220939965739202, 0.922211335478959, 0.9230709195621077, 0.92328090393134, 0.9234429433716316, 0.9256053317644887, 0.9248834541689737, 0.9262327614548725, 0.9271857119785898, 0.9261626465023993, 0.9269306665859173, 0.9282323894310631, 0.928418401335825, 0.9298131301333518, 0.9294174943475452, 0.9298603540859173, 0.9307671621216316, 0.9305339262643041, 0.9319515460617387, 0.9310919619785898, 0.9323944058001107, 0.933068338466685, 0.9332078473952565, 0.934091043454688, 0.9334639742524916, 0.933695768157069, 0.9347875066329827, 0.9354170992524916, 0.9352071148832595, 0.9361132019425988, 0.9357876811092655, 0.9352525363948875, 0.936275601871078, 0.9372285523947952, 0.9373222793235512, 0.9372750553709857, 0.9370439824427832, 0.937902845549557, 0.9376245486687893, 0.93843726928756, 0.9378806755260245, 0.9376946636212625, 0.9385546081925988, 0.9385542477044113, 0.9389259110257475, 0.9397171825973607, 0.9400659549187893, 0.9397633250853636, 0.9412281688353636, 0.9410886599067922, 0.940623990633075, 0.9416463351328904, 0.9409266204665007, 0.9425538641449798, 0.9423431587993725, 0.9420659433831673, 0.9425535036567922, 0.9425524221922297, 0.9428089095376523, 0.9431816543235512, 0.9435297056686047, 0.9440187078949798, 0.9443431472637505, 0.9439246204780363, 0.9447384225613695, 0.944576383121078, 0.9444361532161315, 0.9453662127399409, 0.9449484069306018, 0.9448782919781286, 0.9451569493470838, 0.9457625695021227, 0.9465062566329827, 0.9460641178709857, 0.9459943634067, 0.9462272387758398, 0.9465981811208011, 0.9469237019541344, 0.9477378645256552, 0.9479700189184201, 0.947854482454319, 0.9480168823827981, 0.9486675635612772, 0.9477135315729974, 0.9487376785137505, 0.9489701933947029, 0.948831044954319, 0.9481338607996493, 0.9490410293235512, 0.9487387599783131, 0.9495747320851791, 0.949620874573182, 0.9492259597637505, 0.9490868113233666, 0.9490174173472684, 0.9500404828234589, 0.9504811796327058, 0.9494824471091732, 0.9502021617755629, 0.9512484787398486, 0.9513654571567, 0.9506904430255629, 0.9509004273947952, 0.9506682730020304, 0.9498312194306018, 0.9508542849067922, 0.9510624668350868, 0.9508306729305095, 0.9515983325258398, 0.951133302763935, 0.9514577421327058], "end": "2016-01-24 22:53:39.603000", "learning_rate_per_epoch": [0.00039837969234213233, 0.00019918984617106616, 0.00013279322593007237, 9.959492308553308e-05, 7.967594137880951e-05, 6.639661296503618e-05, 5.691138721886091e-05, 4.979746154276654e-05, 4.426441228133626e-05, 3.9837970689404756e-05, 3.6216337321093306e-05, 3.319830648251809e-05, 3.064459087909199e-05, 2.8455693609430455e-05, 2.6558645913610235e-05, 2.489873077138327e-05, 2.3434100512531586e-05, 2.213220614066813e-05, 2.0967352611478418e-05, 1.9918985344702378e-05, 1.897046240628697e-05, 1.8108168660546653e-05, 1.7320857295999303e-05, 1.6599153241259046e-05, 1.593518754816614e-05, 1.5322295439545996e-05, 1.4754803487448953e-05, 1.4227846804715227e-05, 1.3737230801780242e-05, 1.3279322956805117e-05, 1.2850958228227682e-05, 1.2449365385691635e-05, 1.2072112440364435e-05, 1.1717050256265793e-05, 1.1382277079974301e-05, 1.1066103070334066e-05, 1.0767018466140144e-05, 1.0483676305739209e-05, 1.0214864232693799e-05, 9.959492672351189e-06, 9.716578460938763e-06, 9.485231203143485e-06, 9.264644177164882e-06, 9.054084330273326e-06, 8.852882274368312e-06, 8.660428647999652e-06, 8.476164111925755e-06, 8.299576620629523e-06, 8.130197784339543e-06, 7.96759377408307e-06, 7.811366231180727e-06, 7.661147719772998e-06, 7.51659808884142e-06, 7.3774017437244765e-06, 7.243267191370251e-06, 7.113923402357614e-06, 6.989117537159473e-06, 6.868615400890121e-06, 6.752198260073783e-06, 6.639661478402559e-06, 6.530814516736427e-06, 6.425479114113841e-06, 6.323487468762323e-06, 6.224682692845818e-06, 6.128918357717339e-06, 6.036056220182218e-06, 5.945965767750749e-06, 5.8585251281328965e-06, 5.773618795501534e-06, 5.691138539987151e-06, 5.610981588688446e-06, 5.533051535167033e-06, 5.4572560657106806e-06, 5.383509233070072e-06, 5.311729182722047e-06, 5.2418381528696045e-06, 5.173762474441901e-06, 5.107432116346899e-06, 5.042781140218722e-06, 4.9797463361755945e-06, 4.918267677567201e-06, 4.8582892304693814e-06, 4.7997555157053284e-06, 4.7426156015717424e-06, 4.686819920607377e-06, 4.632322088582441e-06, 4.579077085509198e-06, 4.527042165136663e-06, 4.476176400203258e-06, 4.426441137184156e-06, 4.3777990867965855e-06, 4.330214323999826e-06, 4.283652742742561e-06, 4.238082055962877e-06, 4.1934704313462134e-06, 4.1497883103147615e-06, 4.107007043785416e-06, 4.065098892169772e-06, 4.0240374801214784e-06, 3.983796887041535e-06, 3.944353466067696e-06, 3.905683115590364e-06, 3.867764007736696e-06, 3.830573859886499e-06, 3.794092435782659e-06, 3.75829904442071e-06, 3.7231748137855902e-06, 3.6887008718622383e-06, 3.654859710877645e-06, 3.6216335956851253e-06, 3.5890063827537233e-06, 3.556961701178807e-06, 3.5254840895504458e-06, 3.4945587685797364e-06, 3.46417118635145e-06, 3.4343077004450606e-06, 3.404954668440041e-06, 3.3760991300368914e-06, 3.347728579683462e-06, 3.3198307392012794e-06, 3.292394239906571e-06, 3.2654072583682137e-06, 3.2388593353971373e-06, 3.2127395570569206e-06, 3.1870376915321685e-06, 3.1617437343811616e-06, 3.136847908535856e-06, 3.112341346422909e-06, 3.0882147257216275e-06, 3.0644591788586695e-06, 3.0410662930080434e-06, 3.018028110091109e-06, 2.995335989908199e-06, 2.9729828838753747e-06, 2.950960833913996e-06, 2.9292625640664482e-06, 2.907881025748793e-06, 2.886809397750767e-06, 2.866041086235782e-06, 2.8455692699935753e-06, 2.825388037308585e-06, 2.805490794344223e-06, 2.7858720841322793e-06, 2.7665257675835164e-06, 2.7474461603560485e-06, 2.7286280328553403e-06, 2.710065928113181e-06, 2.691754616535036e-06, 2.6736893232737202e-06, 2.6558645913610235e-06, 2.6382761006971123e-06, 2.6209190764348023e-06, 2.6037889711005846e-06, 2.5868812372209504e-06], "accuracy_valid": [0.6050863610692772, 0.6966832172439759, 0.7384739151920181, 0.7659309111445783, 0.7807631894766567, 0.789806687688253, 0.7949439358998494, 0.8024108151355422, 0.8065920910203314, 0.8107218914721386, 0.8112101727221386, 0.8161444606551205, 0.8144148861069277, 0.8168665874435241, 0.8183314311935241, 0.8195315441453314, 0.8225024119917168, 0.8205286968185241, 0.8213934840926205, 0.8239569606551205, 0.8252291392131024, 0.8266528026167168, 0.8251879588667168, 0.8280367564006024, 0.8291559793862951, 0.8270396037274097, 0.8297560358621988, 0.8290133189006024, 0.8293795298381024, 0.8314856104103916, 0.8297560358621988, 0.8315870905496988, 0.8317400461219879, 0.8318724115210843, 0.8338461266942772, 0.8313841302710843, 0.8303869775978916, 0.8331034097326807, 0.8329710443335843, 0.8314753153237951, 0.8305090479103916, 0.8332048898719879, 0.8327269037085843, 0.8316282708960843, 0.8319738916603916, 0.8343241128576807, 0.8320959619728916, 0.8340799722326807, 0.8356874764683735, 0.8342020425451807, 0.8341814523719879, 0.8332151849585843, 0.8346903237951807, 0.8321268472326807, 0.8319841867469879, 0.8320047769201807, 0.8356977715549698, 0.8351889001317772, 0.8345888436558735, 0.8350565347326807, 0.8337034662085843, 0.8326048333960843, 0.8346903237951807, 0.8348020990210843, 0.8345579583960843, 0.8356771813817772, 0.8367758141942772, 0.8347006188817772, 0.8369184746799698, 0.8356874764683735, 0.8360330972326807, 0.8361860528049698, 0.8357992516942772, 0.8357992516942772, 0.8355448159826807, 0.8369184746799698, 0.8360536874058735, 0.8357992516942772, 0.8354330407567772, 0.8377729668674698, 0.8351786050451807, 0.8365419686558735, 0.8364198983433735, 0.8362875329442772, 0.8371523202183735, 0.8370405449924698, 0.8376714867281627, 0.8377729668674698, 0.8378950371799698, 0.8371729103915663, 0.8365419686558735, 0.8372949807040663, 0.8374067559299698, 0.8355654061558735, 0.8375391213290663, 0.8364096032567772, 0.8386377541415663, 0.8374067559299698, 0.8368066994540663, 0.8366743340549698, 0.8369287697665663, 0.8376611916415663, 0.8380171074924698, 0.8392481057040663, 0.8364507836031627, 0.8372949807040663, 0.8363081231174698, 0.8382612481174698, 0.8381391778049698, 0.8362978280308735, 0.8379053322665663, 0.8364301934299698, 0.8371626153049698, 0.8358095467808735, 0.8377832619540663, 0.8372846856174698, 0.8376406014683735, 0.8376508965549698, 0.8373964608433735, 0.8382715432040663, 0.8372743905308735, 0.8366743340549698, 0.8390039650790663, 0.8376200112951807, 0.8359213220067772, 0.8377729668674698, 0.8380274025790663, 0.8382509530308735, 0.8374067559299698, 0.8371729103915663, 0.8362978280308735, 0.8377832619540663, 0.8375082360692772, 0.8385053887424698, 0.8377832619540663, 0.8382612481174698, 0.8370405449924698, 0.8381391778049698, 0.8364198983433735, 0.8391260353915663, 0.8372538003576807, 0.8367655191076807, 0.8378950371799698, 0.8388715996799698, 0.8386274590549698, 0.8387598244540663, 0.8384950936558735, 0.8361654626317772, 0.8377523766942772, 0.8367758141942772, 0.8361757577183735, 0.8384950936558735, 0.8388715996799698, 0.8382406579442772], "accuracy_test": 0.8295878507653061, "start": "2016-01-23 15:33:44.474000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0], "accuracy_train_last": 0.9514577421327058, "batch_size_eval": 1024, "accuracy_train_std": [0.02021004078229134, 0.020893869235817864, 0.019397292808723345, 0.017112262826689918, 0.017399004395651306, 0.016230283243728792, 0.015844891939551003, 0.015804803628555773, 0.015373916093356107, 0.01620845305289328, 0.015628060851545566, 0.013837899245045745, 0.013325771604936561, 0.012597891240076537, 0.012246169134463036, 0.012236056114183177, 0.012501616437661279, 0.012505145482924253, 0.0111618580901215, 0.012080317629711346, 0.010996156632429114, 0.010716591425981453, 0.010316622558263829, 0.011165118820071339, 0.010350421118936729, 0.01095787147218047, 0.010681377637627727, 0.010599794990814008, 0.010204175569552986, 0.009960843546251616, 0.009158381284163533, 0.009754857926442996, 0.009731884375839576, 0.00928536196661746, 0.00997648046104926, 0.009566302914160036, 0.009649913309806362, 0.009462111017364827, 0.009987017491689032, 0.00949007381958505, 0.009943443250737484, 0.00949492315735247, 0.009003260459873591, 0.00914463267567786, 0.009010821403160459, 0.008916077430464452, 0.008904271165413709, 0.008632339050034073, 0.008714066334939008, 0.009192895763953919, 0.009031934493456356, 0.009267721769098658, 0.009265839485797322, 0.00947575839883573, 0.00861817774873651, 0.009617214643792808, 0.009362726124827006, 0.00845189963955608, 0.008995594503908792, 0.008654653342459191, 0.008994294976785985, 0.008992424871268688, 0.008736481701746878, 0.00837166586626646, 0.00861937312022213, 0.008329458535010121, 0.008409100117258768, 0.008523010073250408, 0.008788604385213724, 0.008290145416124245, 0.008329426932238091, 0.008905159595591251, 0.008305930374601514, 0.008595542473012101, 0.008732441860051784, 0.008719988446921439, 0.008790595073262675, 0.008555476012349583, 0.008403685218014495, 0.008097711784718273, 0.008695930677332714, 0.008445566882356677, 0.00856438673447558, 0.008429934756118345, 0.008030884117470456, 0.008383097694650033, 0.008505218129392915, 0.008545271419062684, 0.008032425849791932, 0.007808136586645783, 0.007848974450923148, 0.008587346090336208, 0.008262518251082514, 0.0077088698511605155, 0.008043472930196367, 0.00759727471621726, 0.0074359195558264465, 0.008262244251483886, 0.007357320101312463, 0.007902180363742793, 0.007962474104409601, 0.008009371745140199, 0.007509127772094666, 0.007774985045348609, 0.0073289297674202026, 0.007833741141423167, 0.007872127579290384, 0.007450249570302941, 0.007094196802928593, 0.007052075227487332, 0.007412647555666903, 0.007333458104802618, 0.007150753211325579, 0.007504074309951041, 0.007168860178282735, 0.006746570352185338, 0.007410274887433383, 0.007418549194907142, 0.00736433687500426, 0.007673045675837408, 0.007521319184178038, 0.007196708936855817, 0.007223013692328111, 0.006838675451451681, 0.0072547902565159765, 0.007362029047015058, 0.007367920135389182, 0.0070683959142638164, 0.007070758870274735, 0.0067875990115563905, 0.00710107607696562, 0.0069299826592132165, 0.006549697441369486, 0.006913484953075022, 0.007032756910929187, 0.007063880922517829, 0.006768928770263797, 0.0069164918889985, 0.006740329414037113, 0.0069351110270185565, 0.007015458906266796, 0.007022304794667514, 0.007348048578057692, 0.006653580096056034, 0.0070129301979384405, 0.006749140475760486, 0.006713670790453608, 0.007101578601090337, 0.0075490469745076024, 0.007554853020049668, 0.007037557684268551, 0.007333048911874646, 0.007498308988178393, 0.0070959876409165965], "accuracy_test_std": 0.005597337162968865, "error_valid": [0.39491363893072284, 0.30331678275602414, 0.2615260848079819, 0.23406908885542166, 0.21923681052334332, 0.21019331231174698, 0.20505606410015065, 0.19758918486445776, 0.19340790897966864, 0.18927810852786142, 0.18878982727786142, 0.18385553934487953, 0.1855851138930723, 0.18313341255647586, 0.18166856880647586, 0.18046845585466864, 0.1774975880082832, 0.17947130318147586, 0.17860651590737953, 0.17604303934487953, 0.17477086078689763, 0.1733471973832832, 0.1748120411332832, 0.17196324359939763, 0.17084402061370485, 0.1729603962725903, 0.17024396413780118, 0.17098668109939763, 0.17062047016189763, 0.1685143895896084, 0.17024396413780118, 0.16841290945030118, 0.16825995387801207, 0.16812758847891573, 0.16615387330572284, 0.16861586972891573, 0.1696130224021084, 0.1668965902673193, 0.16702895566641573, 0.16852468467620485, 0.1694909520896084, 0.16679511012801207, 0.16727309629141573, 0.16837172910391573, 0.1680261083396084, 0.1656758871423193, 0.1679040380271084, 0.1659200277673193, 0.1643125235316265, 0.1657979574548193, 0.16581854762801207, 0.16678481504141573, 0.1653096762048193, 0.1678731527673193, 0.16801581325301207, 0.1679952230798193, 0.16430222844503017, 0.16481109986822284, 0.1654111563441265, 0.1649434652673193, 0.16629653379141573, 0.16739516660391573, 0.1653096762048193, 0.16519790097891573, 0.16544204160391573, 0.16432281861822284, 0.16322418580572284, 0.16529938111822284, 0.16308152532003017, 0.1643125235316265, 0.1639669027673193, 0.16381394719503017, 0.16420074830572284, 0.16420074830572284, 0.1644551840173193, 0.16308152532003017, 0.1639463125941265, 0.16420074830572284, 0.16456695924322284, 0.16222703313253017, 0.1648213949548193, 0.1634580313441265, 0.1635801016566265, 0.16371246705572284, 0.1628476797816265, 0.16295945500753017, 0.16232851327183728, 0.16222703313253017, 0.16210496282003017, 0.16282708960843373, 0.1634580313441265, 0.16270501929593373, 0.16259324407003017, 0.1644345938441265, 0.16246087867093373, 0.16359039674322284, 0.16136224585843373, 0.16259324407003017, 0.16319330054593373, 0.16332566594503017, 0.16307123023343373, 0.16233880835843373, 0.16198289250753017, 0.16075189429593373, 0.16354921639683728, 0.16270501929593373, 0.16369187688253017, 0.16173875188253017, 0.16186082219503017, 0.1637021719691265, 0.16209466773343373, 0.16356980657003017, 0.16283738469503017, 0.1641904532191265, 0.16221673804593373, 0.16271531438253017, 0.1623593985316265, 0.16234910344503017, 0.1626035391566265, 0.16172845679593373, 0.1627256094691265, 0.16332566594503017, 0.16099603492093373, 0.1623799887048193, 0.16407867799322284, 0.16222703313253017, 0.16197259742093373, 0.1617490469691265, 0.16259324407003017, 0.16282708960843373, 0.1637021719691265, 0.16221673804593373, 0.16249176393072284, 0.16149461125753017, 0.16221673804593373, 0.16173875188253017, 0.16295945500753017, 0.16186082219503017, 0.1635801016566265, 0.16087396460843373, 0.1627461996423193, 0.1632344808923193, 0.16210496282003017, 0.16112840032003017, 0.16137254094503017, 0.16124017554593373, 0.1615049063441265, 0.16383453736822284, 0.16224762330572284, 0.16322418580572284, 0.1638242422816265, 0.1615049063441265, 0.16112840032003017, 0.16175934205572284], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-3, 3], "momentum": 0.5874858639152544, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.00039837969852640184, "patience_threshold": 1, "do_flip": true, "batch_size": 16, "optimization": "adam", "nb_data_augmentation": 2, "learning_rate_decay_method": "lin", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 7.2395787382865435e-06, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.003974955342723819}, "accuracy_valid_max": 0.8392481057040663, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-3, 3)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8382406579442772, "loss_train": [1.6887623071670532, 1.229878306388855, 0.9977771043777466, 0.8936997056007385, 0.8330992460250854, 0.7879467010498047, 0.7541207075119019, 0.7323176264762878, 0.7104690074920654, 0.6932522654533386, 0.6777779459953308, 0.6639026403427124, 0.6515359282493591, 0.641846776008606, 0.6332349181175232, 0.6246890425682068, 0.6179133057594299, 0.6095581650733948, 0.6017394065856934, 0.5971262454986572, 0.590370237827301, 0.5864282846450806, 0.5798565149307251, 0.5765271186828613, 0.5709114074707031, 0.5671769976615906, 0.5626941323280334, 0.5582666993141174, 0.5532141327857971, 0.5508815050125122, 0.5483651757240295, 0.5439836382865906, 0.5406320691108704, 0.540668785572052, 0.5356290936470032, 0.5332239866256714, 0.5297081470489502, 0.5277025699615479, 0.5255840420722961, 0.5230302810668945, 0.5213515162467957, 0.5179669260978699, 0.5133944749832153, 0.5142483711242676, 0.5134214758872986, 0.5107226371765137, 0.5063114762306213, 0.5060087442398071, 0.5047639012336731, 0.5034867525100708, 0.49967533349990845, 0.49860867857933044, 0.4976092278957367, 0.4957033395767212, 0.4942072927951813, 0.49403634667396545, 0.49200281500816345, 0.4895850718021393, 0.48848995566368103, 0.48595523834228516, 0.48503538966178894, 0.48445925116539, 0.4837682843208313, 0.48217058181762695, 0.47879108786582947, 0.47888243198394775, 0.478969544172287, 0.47740638256073, 0.4746434688568115, 0.47386518120765686, 0.4722650945186615, 0.47158899903297424, 0.4704415202140808, 0.4708428978919983, 0.46900466084480286, 0.4686957001686096, 0.46731749176979065, 0.46529462933540344, 0.46484121680259705, 0.4637199938297272, 0.4633958041667938, 0.4606265723705292, 0.46106958389282227, 0.460981547832489, 0.46126535534858704, 0.4584200978279114, 0.457597941160202, 0.45736292004585266, 0.4555267095565796, 0.45439788699150085, 0.45127978920936584, 0.4517178237438202, 0.45333772897720337, 0.4501204192638397, 0.45010825991630554, 0.4506707191467285, 0.450257807970047, 0.447406530380249, 0.44763633608818054, 0.4469371438026428, 0.4453485310077667, 0.44528117775917053, 0.4439534544944763, 0.4435127377510071, 0.4440370202064514, 0.4432542622089386, 0.4420381486415863, 0.44220390915870667, 0.43856340646743774, 0.4410650432109833, 0.4395079016685486, 0.4378640651702881, 0.43903273344039917, 0.43604782223701477, 0.43878400325775146, 0.43534013628959656, 0.4345683455467224, 0.43716347217559814, 0.43483173847198486, 0.43128934502601624, 0.4338490664958954, 0.4322660565376282, 0.43137291073799133, 0.4313463270664215, 0.4313186705112457, 0.42975687980651855, 0.43059730529785156, 0.42948880791664124, 0.4283483028411865, 0.42899781465530396, 0.42836904525756836, 0.425598680973053, 0.4262208342552185, 0.4266543984413147, 0.4255125820636749, 0.4253840148448944, 0.4257119596004486, 0.421343058347702, 0.4234743118286133, 0.42283475399017334, 0.4237850308418274, 0.4241451323032379, 0.4232659637928009, 0.4220634996891022, 0.4202212393283844, 0.42328786849975586, 0.42039555311203003, 0.4203939735889435, 0.4171719253063202, 0.41883817315101624, 0.42009127140045166, 0.41784998774528503, 0.41709908843040466, 0.417954683303833], "accuracy_train_first": 0.6160880110280547, "model": "residualv5", "loss_std": [0.31089797616004944, 0.2459610402584076, 0.23356321454048157, 0.22731204330921173, 0.22332200407981873, 0.21934516727924347, 0.21387213468551636, 0.21414488554000854, 0.20943598449230194, 0.20691423118114471, 0.20559266209602356, 0.2022278904914856, 0.2006252110004425, 0.19959673285484314, 0.1961136758327484, 0.19600917398929596, 0.19545921683311462, 0.19434715807437897, 0.19074487686157227, 0.19035080075263977, 0.18802645802497864, 0.1904321014881134, 0.18703661859035492, 0.18731267750263214, 0.18505576252937317, 0.1852433979511261, 0.18330837786197662, 0.18048109114170074, 0.18177591264247894, 0.18017558753490448, 0.1818552017211914, 0.17994529008865356, 0.1764122098684311, 0.1821369081735611, 0.17772452533245087, 0.17666880786418915, 0.1778908520936966, 0.17615728080272675, 0.17464977502822876, 0.17556403577327728, 0.1725882738828659, 0.17290425300598145, 0.16992124915122986, 0.17328867316246033, 0.17281512916088104, 0.17218837141990662, 0.1694256216287613, 0.17110618948936462, 0.16881965100765228, 0.16985571384429932, 0.1678917109966278, 0.1697477251291275, 0.16775982081890106, 0.16760845482349396, 0.1674325168132782, 0.1680058091878891, 0.1676025241613388, 0.16552862524986267, 0.16756699979305267, 0.16346561908721924, 0.1647661030292511, 0.16394652426242828, 0.16446338593959808, 0.1629410684108734, 0.16318151354789734, 0.16256165504455566, 0.1623305380344391, 0.16233289241790771, 0.16283836960792542, 0.161778524518013, 0.16077172756195068, 0.16040225327014923, 0.1607082635164261, 0.1614038199186325, 0.15901954472064972, 0.15906521677970886, 0.15961337089538574, 0.1591869592666626, 0.15803025662899017, 0.15906822681427002, 0.15984684228897095, 0.1567377895116806, 0.15752334892749786, 0.15780167281627655, 0.1576886773109436, 0.158407524228096, 0.15786980092525482, 0.1556721031665802, 0.15434812009334564, 0.1554691642522812, 0.15367083251476288, 0.15336884558200836, 0.15587300062179565, 0.15358325839042664, 0.1522206962108612, 0.15476176142692566, 0.15530720353126526, 0.1541207730770111, 0.1547328680753708, 0.1553519070148468, 0.1513422429561615, 0.15150083601474762, 0.154707133769989, 0.1516621857881546, 0.15318480134010315, 0.15219029784202576, 0.1508454829454422, 0.1523778885602951, 0.151320219039917, 0.152096688747406, 0.1536773145198822, 0.1507166028022766, 0.1519024521112442, 0.14881223440170288, 0.15443375706672668, 0.15111488103866577, 0.14827682077884674, 0.15266984701156616, 0.14907273650169373, 0.14986273646354675, 0.15148907899856567, 0.14954175055027008, 0.14783266186714172, 0.14928194880485535, 0.15030567348003387, 0.14869476854801178, 0.14919690787792206, 0.146525576710701, 0.14804553985595703, 0.14978356659412384, 0.1481855809688568, 0.14710725843906403, 0.1498614400625229, 0.1480865776538849, 0.1453278511762619, 0.14682111144065857, 0.1470872461795807, 0.14500826597213745, 0.14794772863388062, 0.1471889466047287, 0.14601533114910126, 0.14548975229263306, 0.1462145447731018, 0.14600932598114014, 0.14630700647830963, 0.14798785746097565, 0.148159921169281, 0.1459713876247406, 0.1423226296901703, 0.14708727598190308, 0.14538845419883728, 0.1441948562860489, 0.14449366927146912, 0.14604109525680542]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:11 2016", "state": "available"}], "summary": "7bad71e5324aae53d4f752bcfd548a32"}