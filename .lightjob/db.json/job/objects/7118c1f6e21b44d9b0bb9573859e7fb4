{"content": {"hp_model": {"f1": 136, "f2": 144, "f3": 195, "nonlin": "leaky_rectify", "ds2": 3070, "ds1": 3374, "do2": 0.7652579019562766, "do3": 0.7140172757401253, "do1": 0.7960509161471867, "do4": 0.28458587909605826, "do5": 0.4205016937253989}, "accuracy_valid_std": [0.027682707799225135, 0.02952843056487977, 0.024740520417928572, 0.02483019409514136, 0.022762015955429604, 0.02995878614420063, 0.026224339490599047, 0.024824347741563516, 0.025227444428093534, 0.026907645314745415, 0.02504699861485285, 0.027206384300796967, 0.02428457131408823, 0.029483849577768752, 0.028569056582722997, 0.02917733800458778, 0.023847682108144367, 0.02899487941179679, 0.024702723678197078, 0.031133664252572555, 0.025822752082589998, 0.027530231907049032, 0.027263676390247304, 0.02525152619709157, 0.024994425656774143, 0.026856012393494836, 0.024625851038850084, 0.02927263476471956, 0.028465987763615, 0.024320406903647344, 0.022249284902969794, 0.025645433930472656, 0.032703676202616376, 0.029023649764502342, 0.0283151667505727, 0.0255770710159869, 0.02759046850501274, 0.029296487933431963, 0.03215313020752094, 0.028226601415493838, 0.02612486863495951, 0.02558983650349421, 0.026373702272689082, 0.028200556622202007, 0.027771374355858913, 0.026612029891207364, 0.03367914839551649, 0.026226069088710068, 0.0294271806129771, 0.029837416393278124, 0.032733898214958757, 0.032839041856378225, 0.030116136806203734, 0.026683861038374477, 0.035407972597238456, 0.025101991369043074, 0.03136387962018481, 0.030945737971203997, 0.029865375111887034, 0.03177965970486773, 0.03129061532748805, 0.030351380504646357, 0.031446783068443514, 0.028694526825386632, 0.030757548313373087, 0.03021567864829731, 0.03071977203989138, 0.029967566340529453, 0.030517112483442233, 0.036876835818587826, 0.03140088102101645, 0.03250333870221677, 0.03328001616970679, 0.03667851612419496, 0.03414886884013461, 0.03069968448775195, 0.030228886114113753, 0.035456107020435695, 0.032816657909936896, 0.030894100091940845, 0.03118723259424654, 0.038139359528007274, 0.03365867081611638, 0.032259041915986844, 0.03682292174722363, 0.030535834731260248, 0.03198594846896601, 0.03235703803879383, 0.027543080260769476, 0.02619699668477468, 0.030743387650428337, 0.030578882066473103, 0.026359595651718873, 0.03516940771414882, 0.027860409964325303, 0.03671707982198452, 0.030553951690273595, 0.036382749855917364, 0.03133262570027066, 0.027535174291053687, 0.03471166989892439, 0.033799337822726, 0.033998709361787775, 0.03382294895925746, 0.032667039636258874, 0.02796960317121193, 0.025014742791824612, 0.031906436408876644, 0.03125696637571352, 0.0319984252578504, 0.03213845537070418, 0.03372114236244635, 0.033392406226502955, 0.02872328218806431, 0.03036930878484131, 0.03250696685208857, 0.030238487920474896, 0.02792155878489016, 0.03174538605679207, 0.034375253350055465, 0.031627717366353836, 0.03276353857992056, 0.034425095324542236, 0.032221899800736885, 0.032083364951923335, 0.032678423573410595, 0.031247967847867462, 0.033462966638226344, 0.03238478237645657, 0.03252035959130457, 0.032905275263547216, 0.03365759270366203, 0.03480458529931207, 0.034490124146011894, 0.0333769172773811], "moving_avg_accuracy_train": [0.011690512048192768, 0.0218589984939759, 0.02946931475903614, 0.03642213855421686, 0.04312678087349397, 0.04962688516566264, 0.053072046799698784, 0.05624093398719879, 0.059652985166792155, 0.06336389146939005, 0.0663530859067884, 0.06863861692454329, 0.071587443485101, 0.07501087157634993, 0.07811078215967879, 0.0805547867750362, 0.08197078776620728, 0.08371582118838174, 0.08579228123821825, 0.08811054934933618, 0.0891945433601857, 0.09073254384344424, 0.09170023448922031, 0.09325357323909346, 0.0933973424212082, 0.09477861721523197, 0.09617236693949191, 0.09746203913108488, 0.09816152422400048, 0.09840749529557635, 0.09598391444071752, 0.09469689347857348, 0.0964211988596318, 0.09895904885318668, 0.10029008297389211, 0.10029496021867158, 0.10044289266065984, 0.0999242058042324, 0.10306949230212242, 0.10452364999962103, 0.10427224509002037, 0.10394479467740388, 0.1070409966855671, 0.10767443466761281, 0.10885399797795996, 0.10834369230666997, 0.11116237277479815, 0.11022827028647496, 0.11329382804698408, 0.11875190759168326, 0.12232287647106915, 0.1260709163842032, 0.12723688573975878, 0.12674493662361422, 0.1290083194672769, 0.1292804920386215, 0.1310408840998196, 0.1326417090934521, 0.13166340038290208, 0.13339257992292514, 0.1350759122920784, 0.13502841142431635, 0.13660698970959556, 0.13694996167237095, 0.13837403553525435, 0.13929331496365663, 0.13981240214198976, 0.13808407984947754, 0.13866996779826474, 0.13990792207265512, 0.1401702360400884, 0.14125581032764584, 0.14205163366235116, 0.1449963196937064, 0.1478983255255406, 0.14798283408744436, 0.14552453561845896, 0.14804192392408294, 0.14955456135095174, 0.1512171398544108, 0.15274405162198176, 0.15372058772484384, 0.15532189115115463, 0.15554177281917173, 0.15791634177219432, 0.1566672527756978, 0.15648198457644127, 0.15701413250433932, 0.15603410479607407, 0.15504148121405703, 0.15556237074325374, 0.1559888143918199, 0.1521722183442042, 0.15414249650978376, 0.15087997878651624, 0.1532500946126839, 0.15369833439840347, 0.15589250698265952, 0.15619416366391164, 0.15587971717703855, 0.15742276955572024, 0.15872444967846147, 0.15979242263230206, 0.16106892208593934, 0.16068586270867072, 0.15957633140768315, 0.1591095679958305, 0.1599366571299824, 0.15944327379650222, 0.15906982367588815, 0.158849023537215, 0.15863147811120434, 0.1585980554506863, 0.1580902830381478, 0.15796979012589926, 0.15799077045065874, 0.1575413733754724, 0.1572733994415396, 0.1572463607022049, 0.1581115213187314, 0.15774888198806308, 0.1574130939398592, 0.15826628755792146, 0.15831880036839438, 0.15785542560263927, 0.15777959689779703, 0.157687819436933, 0.15835587860769754, 0.15817823502403622, 0.15823014043729525, 0.15835921600199945, 0.15867069651023324, 0.1594804905640292, 0.160046936989554, 0.16064615895324919], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 1234, "moving_var_accuracy_train": [0.0012300126475404626, 0.002037594432169094, 0.0023550872118403487, 0.0025546543191977953, 0.00270375894494299, 0.002813645252730318, 0.002639102975618998, 0.002465569292121013, 0.002323791202178393, 0.0022153495122405006, 0.0020742321114776098, 0.0019138217686279263, 0.0018006997945233898, 0.0017261085541346217, 0.0016399827093428682, 0.0015297428654475768, 0.0013948141081657965, 0.0012827389721497698, 0.001193270251981897, 0.001122312530098944, 0.001020656664229068, 0.0009398800071846927, 0.0008543198331395258, 0.0007906036012722905, 0.000711729267344595, 0.0006577276211195839, 0.0006094377036524974, 0.0005634632225431618, 0.000511520414845746, 0.000460912889273641, 0.00046768529778662166, 0.000435824574620943, 0.00041900117858316866, 0.00043506720403293093, 0.00040750535010397647, 0.0003667550291812286, 0.00033027648232963923, 0.0002996701585919504, 0.0003587385871170384, 0.00034189589988808445, 0.0003082751497564179, 0.0002784126487352805, 0.0003368495857399385, 0.0003067758202598278, 0.0002886205646618992, 0.0002621022150990657, 0.00030739662982182494, 0.0002845098939678666, 0.00034063770401823984, 0.0005746896244627886, 0.0006319870306543924, 0.0006952185563029662, 0.000637932061515522, 0.0005763169807598484, 0.0005647913997567424, 0.0005089789609583992, 0.00048597188674472265, 0.0004604384640123968, 0.00042300840900939964, 0.0004076181250431685, 0.00039235878332420404, 0.000353143211983727, 0.00034025607541014974, 0.00030728913577438477, 0.00029481209949947483, 0.00027293656155687987, 0.0002480679688895806, 0.0002501450535217751, 0.00022821993036640408, 0.00021919071439909668, 0.0001978909205167823, 0.0001887080722693564, 0.00017553727806297385, 0.00023602413266600474, 0.00028821646003140044, 0.0002594590893015758, 0.0002879022626349619, 0.0003161472313030972, 0.0003051251560392663, 0.00029949014595681664, 0.0002905242672746553, 0.00027005444538892695, 0.00026612655481806654, 0.00023994903086762957, 0.00026670132719479566, 0.00025407320436583475, 0.0002289748026801531, 0.0002086259551666334, 0.00019640744843067896, 0.00018563441776779825, 0.00016951290910565932, 0.0001541983058637154, 0.0002698761237934277, 0.0002778264758619217, 0.00034584002532744244, 0.0003618130640597516, 0.0003274400278032937, 0.00033802556498847163, 0.00030504197926972085, 0.00027542767068071026, 0.0002693139994028361, 0.00025763193974000914, 0.00024213384183722337, 0.0002325855153497271, 0.0002106475741933753, 0.00020066235414487797, 0.00018255693147418913, 0.0001704579262492593, 0.0001556029776481374, 0.00014129786481660366, 0.00012760685264608606, 0.00011527210149288088, 0.00010375494501171772, 9.569994591696255e-05, 8.626061820238545e-05, 7.763851794839002e-05, 7.169228573422548e-05, 6.516934742420963e-05, 5.865899252261194e-05, 5.9529619301847325e-05, 5.476022292899071e-05, 5.029898315594091e-05, 5.1820538989466545e-05, 4.666330344789377e-05, 4.392941866495173e-05, 3.9588226730759026e-05, 3.570521197858698e-05, 3.615141828151162e-05, 3.282029163870478e-05, 2.9562510022164617e-05, 2.675620353258155e-05, 2.4953764142409606e-05, 2.836028541423821e-05, 2.8412010849722696e-05, 2.880241242072306e-05], "duration": 64128.553553, "accuracy_train": [0.11690512048192771, 0.1133753765060241, 0.09796216114457831, 0.09899755271084337, 0.10346856174698796, 0.10812782379518072, 0.0840785015060241, 0.0847609186746988, 0.09036144578313253, 0.09676204819277108, 0.09325583584337349, 0.08920839608433735, 0.09812688253012049, 0.10582172439759036, 0.10600997740963855, 0.10255082831325302, 0.09471479668674698, 0.0994211219879518, 0.10448042168674698, 0.10897496234939759, 0.09895048945783133, 0.10457454819277108, 0.10040945030120482, 0.1072336219879518, 0.09469126506024096, 0.10721009036144578, 0.10871611445783133, 0.10906908885542169, 0.10445689006024096, 0.10062123493975904, 0.07417168674698796, 0.08311370481927711, 0.11193994728915663, 0.12179969879518072, 0.11226939006024096, 0.10033885542168675, 0.10177428463855422, 0.09525602409638555, 0.13137707078313254, 0.11761106927710843, 0.10200960090361445, 0.10099774096385543, 0.13490681475903615, 0.1133753765060241, 0.11947006777108433, 0.10375094126506024, 0.1365304969879518, 0.10182134789156627, 0.14088384789156627, 0.16787462349397592, 0.15446159638554216, 0.15980327560240964, 0.13773060993975902, 0.12231739457831325, 0.14937876506024098, 0.1317300451807229, 0.1468844126506024, 0.14704913403614459, 0.1228586219879518, 0.14895519578313254, 0.15022590361445784, 0.13460090361445784, 0.15081419427710843, 0.1400367093373494, 0.15119070030120482, 0.1475668298192771, 0.14448418674698796, 0.12252917921686747, 0.1439429593373494, 0.15104951054216867, 0.14253106174698796, 0.15102597891566266, 0.1492140436746988, 0.1714984939759036, 0.1740163780120482, 0.1487434111445783, 0.12339984939759036, 0.1706984186746988, 0.1631682981927711, 0.16618034638554216, 0.16648625753012047, 0.1625094126506024, 0.1697336219879518, 0.1575207078313253, 0.1792874623493976, 0.1454254518072289, 0.15481457078313254, 0.1618034638554217, 0.14721385542168675, 0.1461078689759036, 0.16025037650602408, 0.15982680722891565, 0.11782285391566265, 0.171875, 0.12151731927710843, 0.17458113704819278, 0.15773249246987953, 0.17564006024096385, 0.1589090737951807, 0.1530496987951807, 0.17131024096385541, 0.17043957078313254, 0.16940417921686746, 0.1725574171686747, 0.15723832831325302, 0.14959054969879518, 0.15490869728915663, 0.1673804593373494, 0.1550028237951807, 0.15570877259036145, 0.15686182228915663, 0.15667356927710843, 0.15829725150602408, 0.1535203313253012, 0.15688535391566266, 0.15817959337349397, 0.15349679969879518, 0.15486163403614459, 0.15700301204819278, 0.16589796686746988, 0.1544851280120482, 0.15439100150602408, 0.16594503012048192, 0.1587914156626506, 0.15368505271084337, 0.15709713855421686, 0.15686182228915663, 0.1643684111445783, 0.15657944277108435, 0.1586972891566265, 0.15952089608433734, 0.16147402108433734, 0.16676863704819278, 0.1651449548192771, 0.16603915662650603], "end": "2016-01-18 18:04:55.771000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0], "accuracy_valid": [0.12055495689655173, 0.11786099137931035, 0.09859913793103449, 0.09684806034482758, 0.1050646551724138, 0.10829741379310345, 0.08432112068965517, 0.08472521551724138, 0.09119073275862069, 0.09927262931034483, 0.09469288793103449, 0.09455818965517242, 0.09617456896551724, 0.10479525862068965, 0.10816271551724138, 0.10169719827586207, 0.09792564655172414, 0.10048491379310345, 0.10466056034482758, 0.11449353448275862, 0.09859913793103449, 0.10492995689655173, 0.1058728448275862, 0.10910560344827586, 0.09859913793103449, 0.10856681034482758, 0.10802801724137931, 0.11072198275862069, 0.11166487068965517, 0.10614224137931035, 0.07570043103448276, 0.08580280172413793, 0.1206896551724138, 0.12877155172413793, 0.11408943965517242, 0.09859913793103449, 0.10237068965517242, 0.10035021551724138, 0.13038793103448276, 0.11516702586206896, 0.10088900862068965, 0.10021551724137931, 0.14075969827586207, 0.10991379310344827, 0.11826508620689655, 0.10169719827586207, 0.1375269396551724, 0.10035021551724138, 0.14951508620689655, 0.171875, 0.1596174568965517, 0.1666217672413793, 0.1375269396551724, 0.12311422413793104, 0.1570581896551724, 0.1353717672413793, 0.1509967672413793, 0.15396012931034483, 0.12284482758620689, 0.1557112068965517, 0.15867456896551724, 0.13564116379310345, 0.1567887931034483, 0.14480064655172414, 0.15638469827586207, 0.1505926724137931, 0.1431842672413793, 0.12001616379310345, 0.14857219827586207, 0.1549030172413793, 0.14830280172413793, 0.14816810344827586, 0.15220905172413793, 0.17429956896551724, 0.17874461206896552, 0.14964978448275862, 0.1206896551724138, 0.17376077586206898, 0.1648706896551724, 0.17200969827586207, 0.1732219827586207, 0.1654094827586207, 0.17120150862068967, 0.15732758620689655, 0.18359375, 0.15301724137931033, 0.1606950431034483, 0.16554418103448276, 0.14951508620689655, 0.14938038793103448, 0.16311961206896552, 0.16338900862068967, 0.11584051724137931, 0.1788793103448276, 0.11839978448275862, 0.18238146551724138, 0.16204202586206898, 0.1827855603448276, 0.16258081896551724, 0.15247844827586207, 0.17955280172413793, 0.1775323275862069, 0.17551185344827586, 0.17699353448275862, 0.15948275862068967, 0.15126616379310345, 0.15436422413793102, 0.17470366379310345, 0.15867456896551724, 0.16419719827586207, 0.16567887931034483, 0.16338900862068967, 0.16527478448275862, 0.15625, 0.15988685344827586, 0.16500538793103448, 0.15732758620689655, 0.15584590517241378, 0.15921336206896552, 0.17079741379310345, 0.16338900862068967, 0.15948275862068967, 0.17281788793103448, 0.16217672413793102, 0.15719288793103448, 0.1615032327586207, 0.1593480603448276, 0.1701239224137931, 0.16217672413793102, 0.16419719827586207, 0.16554418103448276, 0.16796875, 0.17389547413793102, 0.16945043103448276, 0.17376077586206898], "accuracy_test": 0.1851963141025641, "start": "2016-01-18 00:16:07.218000", "learning_rate_per_epoch": [0.0073803081177175045, 0.00690791429951787, 0.0064657568000257015, 0.0060519007965922356, 0.005664534866809845, 0.005301963072270155, 0.004962598439306021, 0.004644955508410931, 0.004347644280642271, 0.004069363232702017, 0.0038088939618319273, 0.0035650967620313168, 0.003336904337629676, 0.0031233178451657295, 0.0029234024696052074, 0.0027362832333892584, 0.002561141038313508, 0.002397209173068404, 0.0022437700536102057, 0.0021001521963626146, 0.0019657269585877657, 0.0018399059772491455, 0.0017221384914591908, 0.001611909014172852, 0.0015087350038811564, 0.0014121648855507374, 0.0013217759551480412, 0.0012371726334095001, 0.0011579844867810607, 0.0010838649468496442, 0.0010144896805286407, 0.000949554902035743, 0.0008887763833627105, 0.0008318881737068295, 0.0007786412024870515, 0.0007288024644367397, 0.0006821537390351295, 0.0006384908920153975, 0.0005976227694191039, 0.0005593704991042614, 0.000523566675838083, 0.0004900545463897288, 0.0004586874565575272, 0.00042932809446938336, 0.00040184793761000037, 0.000376126728951931, 0.00035205186577513814, 0.00032951796310953796, 0.0003084263880737126, 0.00028868482331745327, 0.00027020685956813395, 0.00025291164638474584, 0.00023672344104852527, 0.00022157140483614057, 0.00020738921011798084, 0.00019411477842368186, 0.00018169000395573676, 0.00017006050620693713, 0.00015917538257781416, 0.00014898699009791017, 0.0001394507271470502, 0.00013052485883235931, 0.00012217031326144934, 0.00011435051419539377, 0.00010703124280553311, 0.0001001804557745345, 9.376816888106987e-05, 8.776631875662133e-05, 8.214863191824406e-05, 7.689051562920213e-05, 7.196895603556186e-05, 6.736241630278528e-05, 6.305072747636586e-05, 5.901501572225243e-05, 5.523762229131535e-05, 5.170200893189758e-05, 4.839269968215376e-05, 4.529521174845286e-05, 4.239598638378084e-05, 3.968233068007976e-05, 3.714237027452327e-05, 3.47649838658981e-05, 3.253977047279477e-05, 3.0456985768978484e-05, 2.8507514798548073e-05, 2.6682824682211503e-05, 2.4974928237497807e-05, 2.337634941795841e-05, 2.188009057135787e-05, 2.047960333584342e-05, 1.9168757717125118e-05, 1.7941816622624174e-05, 1.679340857663192e-05, 1.5718507711426355e-05, 1.4712407391925808e-05, 1.3770704754278995e-05, 1.2889277968497481e-05, 1.2064269867551047e-05, 1.1292067938484251e-05, 1.0569292498985305e-05, 9.892780326481443e-06, 9.259570106223691e-06, 8.666889698361047e-06, 8.112145224004053e-06, 7.5929083322989754e-06, 7.106906195986085e-06, 6.652011961705284e-06, 6.226234290807042e-06, 5.827709628647426e-06, 5.454693564388435e-06, 5.1055531002930366e-06, 4.778760285262251e-06, 4.472884484130191e-06, 4.186586920695845e-06, 3.918614766007522e-06, 3.6677945445262594e-06, 3.4330287235206924e-06, 3.2132895739778178e-06, 3.007615532624186e-06, 2.8151059723313665e-06, 2.634918473631842e-06, 2.4662642772455e-06, 2.308405328221852e-06, 2.16065041058755e-06, 2.022352873609634e-06, 1.892907448564074e-06, 1.7717474065648275e-06, 1.6583425122007611e-06, 1.5521964087383822e-06, 1.4528444580719224e-06, 1.359851694360259e-06, 1.2728112324111862e-06, 1.1913419939446612e-06, 1.1150873433507513e-06, 1.0437136097607436e-06], "accuracy_train_last": 0.16603915662650603, "error_valid": [0.8794450431034483, 0.8821390086206896, 0.9014008620689655, 0.9031519396551724, 0.8949353448275862, 0.8917025862068966, 0.9156788793103449, 0.9152747844827587, 0.9088092672413793, 0.9007273706896551, 0.9053071120689655, 0.9054418103448276, 0.9038254310344828, 0.8952047413793104, 0.8918372844827587, 0.8983028017241379, 0.9020743534482758, 0.8995150862068966, 0.8953394396551724, 0.8855064655172413, 0.9014008620689655, 0.8950700431034483, 0.8941271551724138, 0.8908943965517242, 0.9014008620689655, 0.8914331896551724, 0.8919719827586207, 0.8892780172413793, 0.8883351293103449, 0.8938577586206896, 0.9242995689655172, 0.9141971982758621, 0.8793103448275862, 0.8712284482758621, 0.8859105603448276, 0.9014008620689655, 0.8976293103448276, 0.8996497844827587, 0.8696120689655172, 0.884832974137931, 0.8991109913793104, 0.8997844827586207, 0.8592403017241379, 0.8900862068965517, 0.8817349137931034, 0.8983028017241379, 0.8624730603448276, 0.8996497844827587, 0.8504849137931034, 0.828125, 0.8403825431034483, 0.8333782327586207, 0.8624730603448276, 0.876885775862069, 0.8429418103448276, 0.8646282327586207, 0.8490032327586207, 0.8460398706896551, 0.8771551724137931, 0.8442887931034483, 0.8413254310344828, 0.8643588362068966, 0.8432112068965517, 0.8551993534482758, 0.8436153017241379, 0.8494073275862069, 0.8568157327586207, 0.8799838362068966, 0.8514278017241379, 0.8450969827586207, 0.8516971982758621, 0.8518318965517242, 0.8477909482758621, 0.8257004310344828, 0.8212553879310345, 0.8503502155172413, 0.8793103448275862, 0.826239224137931, 0.8351293103448276, 0.8279903017241379, 0.8267780172413793, 0.8345905172413793, 0.8287984913793103, 0.8426724137931034, 0.81640625, 0.8469827586206897, 0.8393049568965517, 0.8344558189655172, 0.8504849137931034, 0.8506196120689655, 0.8368803879310345, 0.8366109913793103, 0.8841594827586207, 0.8211206896551724, 0.8816002155172413, 0.8176185344827587, 0.837957974137931, 0.8172144396551724, 0.8374191810344828, 0.8475215517241379, 0.8204471982758621, 0.8224676724137931, 0.8244881465517242, 0.8230064655172413, 0.8405172413793103, 0.8487338362068966, 0.845635775862069, 0.8252963362068966, 0.8413254310344828, 0.8358028017241379, 0.8343211206896551, 0.8366109913793103, 0.8347252155172413, 0.84375, 0.8401131465517242, 0.8349946120689655, 0.8426724137931034, 0.8441540948275862, 0.8407866379310345, 0.8292025862068966, 0.8366109913793103, 0.8405172413793103, 0.8271821120689655, 0.837823275862069, 0.8428071120689655, 0.8384967672413793, 0.8406519396551724, 0.8298760775862069, 0.837823275862069, 0.8358028017241379, 0.8344558189655172, 0.83203125, 0.826104525862069, 0.8305495689655172, 0.826239224137931], "accuracy_train_std": [0.02888234681910663, 0.02874640291224072, 0.026495179864999992, 0.026601554052876217, 0.027243516372861747, 0.026223345152282328, 0.02354003755929288, 0.023866823852791836, 0.023755150733864576, 0.026970241730287614, 0.025036710646677052, 0.026834092984797563, 0.026290756445338893, 0.02616643859156921, 0.026092860244542115, 0.026139601835925206, 0.024919925807958893, 0.02559725076133339, 0.029293538800342647, 0.028596290525075382, 0.026087511802937682, 0.027815855706303235, 0.027366492506415666, 0.028175081790930635, 0.025377700034027756, 0.027722692978465868, 0.0274331536290506, 0.025641861763837454, 0.027439158001211945, 0.026302590658668093, 0.02496065634080201, 0.024355403952170666, 0.030364470820869058, 0.028538886539277344, 0.02846516698394561, 0.025981473322237856, 0.02701827328771871, 0.024200307279196927, 0.03026920050471333, 0.028132529427634247, 0.026590144448409042, 0.026257415127710577, 0.031732038430470934, 0.028398958969096454, 0.028501404607890225, 0.026378554469110575, 0.03041768348598567, 0.026634090349761195, 0.0320304336779585, 0.03415887734399241, 0.03417407954650205, 0.034563810217503564, 0.03272540345528158, 0.03099260828345652, 0.03238587919531502, 0.030018645795648165, 0.0319449261913199, 0.032480353809166404, 0.029225588562889843, 0.030890671589117794, 0.03180570712284327, 0.030735727463451674, 0.03186250876569567, 0.031743553619304025, 0.03379858418264432, 0.033457198377771606, 0.032729777169582856, 0.02901642298828199, 0.031760923182018806, 0.033988752558671786, 0.03140620654199399, 0.03166363758340329, 0.03266436424913319, 0.03546921056293465, 0.03571411785233974, 0.03265527653010946, 0.028666736980144162, 0.03497135396330143, 0.03497971333172293, 0.03495855772645308, 0.03522428255845996, 0.035119310826318376, 0.035403916649452985, 0.033894855490966366, 0.03541448809665942, 0.03240707392097697, 0.03358037307840099, 0.03438762176893587, 0.031113971818573543, 0.03235833096653644, 0.032577665106464676, 0.03409277120874387, 0.028125275243079676, 0.033311608523526735, 0.029105720814239178, 0.034395438796204894, 0.03267934668264671, 0.03317027955023364, 0.0331693029495566, 0.03186832149949884, 0.032541848774560025, 0.03274888099555623, 0.03382583525903901, 0.03229858681918579, 0.033877141644042576, 0.030858594789496152, 0.03162197971399977, 0.03309322336145131, 0.0331582160984446, 0.03232401898366394, 0.03233129020543314, 0.03235135680186235, 0.0325699048432867, 0.031447833292467475, 0.03117579968924696, 0.03238180958855192, 0.031884120869325044, 0.03145673294143483, 0.03213438110708525, 0.031750957772534984, 0.032416735140527536, 0.03164228613591909, 0.03333015458116288, 0.03153947332991038, 0.03137685406693977, 0.03172901063788652, 0.03171703624683709, 0.033631518820140795, 0.032494202666297434, 0.03205563809578801, 0.033357946034431415, 0.033266196896691316, 0.033204275711618464, 0.033563491408561734, 0.033500772616353744], "accuracy_test_std": 0.03377955692399573, "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-3, 3], "momentum": 0.6388378193320504, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.0078850065792929, "patience_threshold": 1, "do_flip": true, "batch_size": 128, "optimization": "adam", "nb_data_augmentation": 0, "learning_rate_decay_method": "exp", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 1.368892620366548e-08, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.06400735108838271}, "accuracy_valid_max": 0.18359375, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = 1234\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-10, -6], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128, 256],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-3, 3)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_optimizer.learning_rate = learning_rate\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.17376077586206898, "loss_train": [3668775534592.0, 18755997696.0, 3389612544.0, 1360783744.0, 941918720.0, 642822720.0, 446455648.0, 420456640.0, 327874080.0, 283587104.0, 355423680.0, 240343344.0, 210828032.0, 145449472.0, 120290760.0, 110539376.0, 86429976.0, 72210880.0, 70301200.0, 99003576.0, 82053376.0, 65312484.0, 64248012.0, 41323900.0, 80208856.0, 49486488.0, 31159034.0, 30898404.0, 23741194.0, 41949684.0, 96665992.0, 53040600.0, 36677660.0, 28433036.0, 21090594.0, 18331200.0, 17226092.0, 14026063.0, 11920858.0, 11807310.0, 16817844.0, 9759061.0, 10314488.0, 7383435.0, 7611360.5, 6790987.5, 5391756.5, 5550617.0, 9706458.0, 7649992.0, 4995188.0, 4919619.5, 3866257.0, 3351486.0, 3017682.75, 3176847.0, 2960034.75, 2552191.25, 2309314.0, 2287106.0, 2263174.5, 2772221.25, 1931005.125, 2170137.0, 1768044.25, 1669279.125, 1484070.25, 1429713.25, 1319729.5, 1166903.0, 1097852.75, 1161393.125, 1088580.5, 1039514.9375, 1496592.875, 917295.0, 1200654.25, 873787.375, 876266.0625, 801856.6875, 784816.5, 746002.375, 707924.25, 662428.0625, 632883.5625, 641489.875, 717162.875, 634952.625, 644354.5, 616597.625, 586429.25, 571834.625, 628009.3125, 546178.125, 671763.4375, 576532.25, 526854.5625, 502659.28125, 517813.5625, 480563.84375, 477715.59375, 504701.5, 477466.34375, 444938.46875, 459420.9375, 449857.6875, 458338.3125, 422772.5625, 450597.875, 418118.53125, 422567.3125, 416318.40625, 425599.21875, 431705.1875, 410726.5, 431627.46875, 403751.625, 448052.90625, 423033.40625, 448544.28125, 403651.9375, 401538.9375, 387105.40625, 389609.71875, 417403.0, 381847.125, 373359.71875, 376555.53125, 378768.6875, 387493.5625, 376461.65625, 413093.09375, 373698.125, 370955.34375, 366769.84375], "accuracy_train_first": 0.11690512048192771, "model": "vgg", "loss_std": [48422807666688.0, 12417253376.0, 1968549120.0, 379058752.0, 292237792.0, 140539072.0, 87151392.0, 124667896.0, 79328512.0, 212711264.0, 303915232.0, 96443424.0, 75485832.0, 71110040.0, 44928472.0, 31522906.0, 24787604.0, 25768412.0, 98478968.0, 149256704.0, 26200592.0, 30360258.0, 50457468.0, 12311523.0, 98780352.0, 16992172.0, 8158167.0, 15840618.0, 10769919.0, 52041932.0, 368135168.0, 15681729.0, 29772352.0, 9368421.0, 5415114.5, 6894612.5, 12117722.0, 4011179.0, 3938366.25, 12582474.0, 12990280.0, 2174463.75, 6679681.5, 2288180.0, 11442911.0, 2407481.75, 1989168.375, 2542353.5, 8452178.0, 4518827.0, 1294835.25, 2566998.5, 1586559.75, 1978302.0, 1158060.5, 2506328.25, 3639481.75, 677415.125, 554988.8125, 899886.5625, 2064400.5, 1505123.625, 577663.0, 1106006.625, 575483.125, 1345801.5, 1472861.875, 1078098.0, 765026.75, 320119.875, 226908.84375, 419939.03125, 637169.125, 657211.875, 901398.6875, 214897.484375, 3202336.25, 258998.859375, 407518.1875, 219198.59375, 256696.71875, 263312.0625, 187598.0625, 111537.9296875, 129753.4140625, 142545.578125, 468782.84375, 190406.3125, 365866.15625, 266326.75, 142517.09375, 270767.125, 544396.6875, 124483.578125, 1799164.5, 169326.921875, 160221.53125, 102497.7265625, 330744.6875, 93793.2421875, 131395.765625, 429058.5, 250773.0, 102770.8515625, 305893.03125, 334927.34375, 249459.6875, 88942.640625, 268816.09375, 100048.7109375, 224318.6875, 119610.34375, 228273.96875, 264967.0625, 227848.109375, 676351.0625, 245782.09375, 562804.5, 515005.5625, 489414.53125, 137745.484375, 250309.296875, 85420.9375, 128748.5234375, 267646.21875, 89990.3125, 66552.6875, 99254.4296875, 161090.3125, 171285.015625, 151882.71875, 553449.125, 97725.828125, 101265.09375, 79651.4453125]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:00 2016", "state": "available"}], "summary": "d0cdfaf198521859928d6ea535600d6b"}