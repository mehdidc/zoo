{"content": {"hp_model": {"f0": 16, "f1": 16, "f2": 32, "f3": 64, "nonlin": "rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.08769148505970258, 0.08744922843838543, 0.08814830972232737, 0.08313373686740612, 0.08204363491146253, 0.08260290019069426, 0.07757966309268995, 0.07986658233123747, 0.08416894903851728, 0.0865912047775732, 0.08175487832921625, 0.079295553561835, 0.07540677173364868, 0.08035280576883895, 0.0826535156103567, 0.07497555520164031, 0.07607878995897677, 0.07889735286272039, 0.07175042185760883, 0.07930365010848654, 0.07731541406188984, 0.07459482120257895, 0.0781373264437498, 0.07726453328158765, 0.07171312708073241, 0.0776067853940579, 0.08053006423509318, 0.07593178038770598, 0.07626177338273661, 0.07112027101128024, 0.06806182379994205, 0.07322810045528282, 0.07101298607812807, 0.0700377352032441, 0.07130059653148926, 0.07013392519914723, 0.07322810045528282, 0.07193375453500463, 0.07157235103689125, 0.07008470157522419, 0.07423243835740108, 0.0677978323337276, 0.06746609299171498, 0.06600033139359664, 0.07012019184986916, 0.0703532941963168, 0.07120698443786476, 0.07199868386666546, 0.06537695562222197, 0.06608944480300316, 0.06920377118961271, 0.06775099172048435, 0.0695040014173123, 0.06987163661636851, 0.06907414834331986, 0.0663798492428322, 0.07315512045644768, 0.06870562046742353, 0.07125918709745724, 0.06640617446431872, 0.07160710399899031, 0.0673766827352478, 0.06948000507170948, 0.06510139293894103, 0.07344223434263249, 0.07106608413026194, 0.0678303122918856, 0.06609875426821703, 0.06640617446431872, 0.06787197432167157, 0.0662803638411646, 0.0673517960662503, 0.06520924158232468, 0.06698364369953949, 0.07111525544198147, 0.06850024302363106, 0.06585424934120329, 0.07024826536635798, 0.06972944476448537, 0.06628022930040049, 0.06834893625684656, 0.07007044952686647, 0.06808605802673813, 0.07084552373637493, 0.06817872350295644, 0.0732003303345883, 0.06865823024619204, 0.0663782371554053, 0.07361868936838523, 0.07042234000781687, 0.07133873198208135, 0.06856530242070286, 0.06811957873705816, 0.07059990110672343, 0.0714391619688997, 0.06681849445879823, 0.06997149601504866, 0.0665413968060885, 0.06968147110856637, 0.0680429544278578, 0.06804767226141713, 0.06860625816310328, 0.07065293096358924, 0.07122501558046258, 0.06464847782496305, 0.06868225403395779, 0.06688785614411792, 0.06695381637580253, 0.06863017020949955, 0.0682340271445973, 0.06964242816551527, 0.06759748022701088, 0.06632057928849253, 0.07018374959211275, 0.06768567659835557, 0.07255684068045676, 0.06537149940371802, 0.06865576246224911, 0.06986895642970863, 0.06891957301361275, 0.0671444026013227, 0.06464682256261972, 0.06843342779742363, 0.06739295999758238, 0.06945317581532882, 0.06955119999469998, 0.06813790340040649, 0.06603761163025454, 0.06995632861270266, 0.07157720999231648, 0.06645235270016205, 0.06906601462901252, 0.06687292278317979, 0.06987508241958737, 0.06772044894047165, 0.06839028235173028, 0.07001583225384911, 0.06956966028669094, 0.0731955791271349, 0.0677745476483184, 0.06909118730645669, 0.07005606728972562, 0.06818225486288104, 0.06838298012776166, 0.06656538078258374, 0.06915788282488967, 0.06704432260571466, 0.06918405323432572, 0.07424685232994309, 0.06766709768628197, 0.06597762876216412, 0.06602316128340645, 0.06649943755711303, 0.06707583799019369, 0.06884876111353533, 0.0679590271637017, 0.06869939019559079, 0.07115887905380072, 0.06772242410569343, 0.0665546627551445, 0.06776467884283294, 0.06760341632786217, 0.06760499920006036, 0.06983116761692398, 0.07095419299589155, 0.06512111465794507, 0.0693976872505597, 0.06679606990500127, 0.0683088706008998, 0.06908499180609544, 0.06826538517585383, 0.06898591739311741, 0.0684392913921698, 0.06610576923076923, 0.06764139503738754, 0.06942954721869124, 0.06490618181052936, 0.0661359790248386, 0.07002614786836399, 0.06578447561921481, 0.06825023059303631, 0.07026717706513144, 0.0666649759058556, 0.06738621136595863, 0.068740266047972, 0.06590149089900799, 0.0713152279513623, 0.06727549023632948, 0.06329240646962302, 0.06537927437720109, 0.0648704508101617, 0.06757069533408903, 0.06924898528418962, 0.06483236188622443, 0.06452462992030676, 0.06727018800626908, 0.06734054109563542, 0.07213232331142008, 0.06815255958358582, 0.07015896892149623, 0.06786606171102395, 0.06860001886716044, 0.06970808457403113, 0.06959631650836047, 0.06619676138565027, 0.06666778489946726, 0.07345097610712577, 0.06990991377835769, 0.06894971394858937, 0.06690105339306629, 0.0663717884141419, 0.06871431594396905, 0.07363612996363689, 0.06831357006990763, 0.07082084870994458, 0.07011930163256061, 0.07059876431671946, 0.06490989121016744, 0.06530817395602241, 0.06733537642069568, 0.06615862731768642, 0.0650471273801359, 0.06542222467318944, 0.06670629630557591, 0.06409532989806484, 0.067130456175434, 0.06606569298020445, 0.0734629943348027, 0.06665213328463755, 0.06903036991767622, 0.07155739833538503, 0.0665546627551445, 0.05839731398007982, 0.06960669429454848, 0.06420167396114397, 0.06642698549179221, 0.07049005314258676, 0.06955119999469998, 0.06578271338315009], "moving_avg_accuracy_train": [0.04419710090361444, 0.08965502635542166, 0.13913737763554213, 0.1871165239081325, 0.23358512377635537, 0.27898602404932227, 0.3230894924275226, 0.36699006878718, 0.4087078540771367, 0.44751750918147126, 0.4854229495283844, 0.5209685687321725, 0.556550552220401, 0.5885861031730597, 0.6191453204159947, 0.6479452085551181, 0.6738109851393653, 0.6983114754808505, 0.7204936938966209, 0.7410553937840672, 0.7606457316646966, 0.7787100176849739, 0.7948666891092476, 0.8086829192947085, 0.822952993329093, 0.8359043054419668, 0.8480970074278906, 0.8585927471971497, 0.8692907955196034, 0.8783566331363178, 0.8863041023528065, 0.893932163503068, 0.9009809652250503, 0.9073437120760393, 0.9136796533684354, 0.9189113680014713, 0.9242458524362639, 0.9290868921926375, 0.9331920395697593, 0.9373855426910966, 0.9410349778798184, 0.944046502682198, 0.947418113709159, 0.9506431698081226, 0.9532751065923706, 0.9562086087343383, 0.9590770174392178, 0.960938517502525, 0.9623291348787785, 0.9644137100957199, 0.9663251252307262, 0.9676547738522319, 0.9689338183043581, 0.9708144387329584, 0.9723069782933974, 0.9736102601327324, 0.975220902041146, 0.9762563231322121, 0.9774117525659789, 0.9786798958334774, 0.979637678087479, 0.9806408718751166, 0.9816331664647133, 0.9821426660833023, 0.9825541524870202, 0.9831786318166315, 0.9837571353518357, 0.9845013389853268, 0.9848840364120953, 0.9851766945178738, 0.9857883548853635, 0.9865200427402007, 0.9873173984059396, 0.9878067617279962, 0.9883154304347146, 0.9886720462767853, 0.9891247776430827, 0.9891086665956419, 0.9894259625866801, 0.9898503655750001, 0.9902746851921989, 0.9904141970946657, 0.9906997728671268, 0.9910203264539081, 0.9911676349229752, 0.9914531681174247, 0.9916983821791762, 0.9918649520937887, 0.9920901662217592, 0.9924081639068122, 0.9924355139317936, 0.9925542554603011, 0.9928423163600542, 0.9928803738806752, 0.9929499230889932, 0.9932948968945517, 0.9936053733195543, 0.9935741846321773, 0.9934825794219716, 0.993355424642425, 0.9936080787143271, 0.9937436940356655, 0.9940398818610146, 0.9940899599399734, 0.9939750151507953, 0.9940692305031856, 0.994224619199855, 0.9945009524605924, 0.9945378677567019, 0.9946534522159715, 0.9947951288317237, 0.9948520429063826, 0.9947526631639372, 0.9949314819379049, 0.9950830061838735, 0.9951299578245223, 0.9951769206264074, 0.9951768302203932, 0.9953249981019683, 0.9955148250990004, 0.9956268403300642, 0.9955441073512746, 0.9954837666462676, 0.9955659434454962, 0.9957104974443202, 0.9958641276697677, 0.9959412126437548, 0.9959917638191383, 0.9962255128890317, 0.9962499872025381, 0.9963779064039711, 0.9963236059744173, 0.9964229848348068, 0.9962347526163864, 0.9962230055173984, 0.9962642027066224, 0.9963271649660806, 0.9962473475658581, 0.9961919840442119, 0.9962268707301523, 0.9963782800426791, 0.9963945371287727, 0.9964868228737267, 0.9964875193514142, 0.9965563878982004, 0.9966630796806696, 0.9967967528873014, 0.9966111476286917, 0.9966958912995575, 0.9966686214467102, 0.9968770416815572, 0.9968175378146062, 0.9969804752982059, 0.9970682899671804, 0.997180267446366, 0.9973398762438981, 0.9973682191917974, 0.9973796088690031, 0.9974839860845124, 0.9975285091628081, 0.9973426763188165, 0.9973660329339228, 0.9975282436465546, 0.997556575155393, 0.9975632482121428, 0.9975880792644225, 0.997706906880149, 0.9976491303487606, 0.9975877188199086, 0.9976830508535803, 0.9977429648947282, 0.9977380584654965, 0.9977877654201517, 0.9977830852636786, 0.9977976984240577, 0.9978885046358688, 0.9979561112505951, 0.9979910724146922, 0.9975777897214158, 0.9976105792733706, 0.9975553760147082, 0.9975033399192614, 0.9976565262586606, 0.9976743826689392, 0.9978034052454187, 0.9978489306847322, 0.9978193087005964, 0.9978302995172837, 0.9978825481800132, 0.9979978136933372, 0.997934478107136, 0.9979927810494345, 0.9979723056553345, 0.9979374056620901, 0.9980165943127485, 0.998118455212799, 0.9981701262577841, 0.9981813327585117, 0.9981961249344677, 0.9980564823205391, 0.9980461089378828, 0.9979779438272272, 0.9979918964324562, 0.9980044537771624, 0.9980910565922172, 0.9981031105715497, 0.9980810148758404, 0.9981081920027143, 0.9981961868084669, 0.9982847947842467, 0.9983457166612437, 0.9984405501156013, 0.9984529521823544, 0.9984829393436371, 0.9985122809514421, 0.998458680868346, 0.9985186862754872, 0.9985185684009505, 0.998574938217482, 0.99866332165477, 0.9986699187061604, 0.9986170269861468, 0.9986753167574116, 0.9986360042081764, 0.9986900430945876, 0.9987151464658517, 0.9986694977831219, 0.9985319342999904, 0.9985069599964974, 0.9984844831233537, 0.9984948450519823, 0.9985488808781093, 0.9985810409830694, 0.998654695167895, 0.9987186307715874, 0.9987361690498504, 0.9987119497352268, 0.9986619144002583, 0.9987204217554132], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 1234, "moving_var_accuracy_train": [0.017580453554558496, 0.034420215076541215, 0.053014721362770266, 0.06843123551991284, 0.0810220889313386, 0.09147105574856769, 0.09982999348059315, 0.10719233957492481, 0.11213646810292267, 0.11447852525648698, 0.11596207440187878, 0.11573728636291727, 0.11555825566723467, 0.11323891882407472, 0.1103198187681753, 0.10675273890279197, 0.1020988105972685, 0.09729139578030051, 0.09199071352687493, 0.08659669369454014, 0.08139105636958113, 0.0761888165974245, 0.07091927722128953, 0.06554534344799931, 0.06082352421972073, 0.05625080016675431, 0.05196367798553683, 0.04775875516671922, 0.044012913791233295, 0.04035132711734366, 0.03688465480813262, 0.03371987517952852, 0.030795058113018064, 0.028079913229124183, 0.025633219274757972, 0.023316234889295878, 0.021240721918031696, 0.019327570719533636, 0.017546483762471282, 0.015950104602082148, 0.014474959536644053, 0.013109087117697775, 0.011900488254182117, 0.010804048310337067, 0.009785987300429858, 0.008884837483739237, 0.008070403651849365, 0.00729454992903566, 0.006582499286316338, 0.005963358442200482, 0.005399904168345415, 0.004875825440620922, 0.004402966488953464, 0.003994500438826338, 0.003615099463998986, 0.0032688764095737503, 0.0029653362748306164, 0.002678451518869976, 0.0024226215215707096, 0.0021948330555357546, 0.001983605871596904, 0.0017943028644172062, 0.0016237344149483731, 0.001463697282205616, 0.0013188514435290572, 0.0011904760690741568, 0.0010744404592289364, 0.0009719809647389554, 0.0008761009841491571, 0.0007892617246361418, 0.0007137027078189467, 0.0006471507410892976, 0.0005881576514995422, 0.0005314971744983569, 0.00048067615172727287, 0.0004337531102838878, 0.0003922224904657644, 0.00035300257751183473, 0.0003186084104740116, 0.0002883686304950645, 0.0002611521916834156, 0.00023521214465344338, 0.00021242491188445007, 0.00019210721211399023, 0.00017309178896812103, 0.00015651637291750156, 0.00014140590505047745, 0.00012751502437351585, 0.00011522001456710201, 0.00010460811585968338, 9.415403648851342e-05, 8.486552879499249e-05, 7.712578765319194e-05, 6.942624426175517e-05, 6.252715366697863e-05, 5.7345500638974464e-05, 5.247851106941893e-05, 4.723941457045981e-05, 4.259099674424528e-05, 3.847741211147455e-05, 3.52041776207654e-05, 3.184928349712426e-05, 2.9453900198377138e-05, 2.6531080504469238e-05, 2.3996883195055016e-05, 2.1677083669183858e-05, 1.9726686125738972e-05, 1.844125815207286e-05, 1.6609396988647193e-05, 1.5068695194804227e-05, 1.3742476046382901e-05, 1.239738134879318e-05, 1.124653021279066e-05, 1.0409662576821476e-05, 9.575332693186365e-06, 8.63763953290425e-06, 7.793725122461936e-06, 7.01435268377497e-06, 6.510500905571673e-06, 6.183759414234474e-06, 5.678310180723556e-06, 5.172081874665738e-06, 4.687642693325813e-06, 4.2796556609764456e-06, 4.039752822062534e-06, 3.848197755395714e-06, 3.51685681878745e-06, 3.1881699289026424e-06, 3.3611005850966344e-06, 3.0303814547814862e-06, 2.8746132081604946e-06, 2.6136887171919213e-06, 2.441205266503749e-06, 2.51596705231672e-06, 2.265612296096761e-06, 2.0543259420867208e-06, 1.8845715629228338e-06, 1.753451763035094e-06, 1.6056926624930488e-06, 1.4560771239467843e-06, 1.5167924308310396e-06, 1.3674918233822074e-06, 1.3073925695395183e-06, 1.1766576783160901e-06, 1.101677801112589e-06, 1.0939582490192601e-06, 1.14537915965847e-06, 1.3408850519046355e-06, 1.271429954480413e-06, 1.1509797629011985e-06, 1.4268327352543844e-06, 1.3160158533679414e-06, 1.4233518800871086e-06, 1.3504194368624602e-06, 1.3282280957789978e-06, 1.4246800004478085e-06, 1.2894419046636565e-06, 1.1616652369189623e-06, 1.1435501412843858e-06, 1.0470358676643088e-06, 1.2531368940521062e-06, 1.1327329878699127e-06, 1.2562705267156217e-06, 1.1378675435816665e-06, 1.0244815564009817e-06, 9.275826311767361e-07, 9.619043883921e-07, 8.957570977663495e-07, 8.401237708731325e-07, 8.379051635818201e-07, 7.864218781637905e-07, 7.079963477776776e-07, 6.594337450697281e-07, 5.936875053442612e-07, 5.362406549162278e-07, 5.568285023559365e-07, 5.42281541313192e-07, 4.990539341371021e-07, 1.9863718017799257e-06, 1.7974110140585095e-06, 1.6450965105551507e-06, 1.5049566565637012e-06, 1.5656554821139515e-06, 1.4119595963948687e-06, 1.4205850639282428e-06, 1.2971796481576465e-06, 1.1753588408391979e-06, 1.0589101392183708e-06, 9.775884301096848e-07, 9.994048341552851e-07, 9.355669190547858e-07, 8.726033248752128e-07, 7.891161682596727e-07, 7.21166637189764e-07, 7.054875550087592e-07, 7.283195861396779e-07, 6.795166995344447e-07, 6.126953005080219e-07, 5.533950466828551e-07, 6.735560786381483e-07, 6.071689343839447e-07, 5.882703817419509e-07, 5.311954203018668e-07, 4.794950604263104e-07, 4.990459825625022e-07, 4.5044907006598607e-07, 4.097981409792404e-07, 3.754656929073417e-07, 4.076068961716587e-07, 4.3750856690066786e-07, 4.2716098608216373e-07, 4.6538534406242766e-07, 4.2023111099392577e-07, 3.863010684706462e-07, 3.554193311608054e-07, 3.4573411821582126e-07, 3.435665463699881e-07, 3.0921001678264686e-07, 3.068870210464903e-07, 3.465030068233198e-07, 3.122443959244245e-07, 3.061977627459843e-07, 3.061572633783456e-07, 2.894508257867958e-07, 2.867875544092485e-07, 2.637804122077454e-07, 2.5615659110161855e-07, 4.0085433901280715e-07, 3.6638234762617945e-07, 3.342910013004304e-07, 3.0182822725450346e-07, 2.979242390761726e-07, 2.774402663279908e-07, 2.985206901760948e-07, 3.054584739341275e-07, 2.7768094738057236e-07, 2.551920294500461e-07, 2.522046392137009e-07, 2.5779217075731566e-07], "duration": 45974.225412, "accuracy_train": [0.44197100903614456, 0.49877635542168675, 0.5844785391566265, 0.6189288403614458, 0.6518025225903614, 0.6875941265060241, 0.7200207078313253, 0.7620952560240963, 0.784167921686747, 0.7968044051204819, 0.8265719126506024, 0.8408791415662651, 0.8767884036144579, 0.8769060617469879, 0.8941782756024096, 0.9071442018072289, 0.9066029743975904, 0.9188158885542169, 0.9201336596385542, 0.9261106927710844, 0.9369587725903614, 0.9412885918674698, 0.9402767319277109, 0.9330289909638554, 0.9513836596385542, 0.9524661144578314, 0.9578313253012049, 0.9530544051204819, 0.9655732304216867, 0.959949171686747, 0.9578313253012049, 0.9625847138554217, 0.9644201807228916, 0.9646084337349398, 0.970703125, 0.9659967996987951, 0.9722562123493976, 0.97265625, 0.9701383659638554, 0.9751270707831325, 0.9738798945783133, 0.9711502259036144, 0.9777626129518072, 0.9796686746987951, 0.9769625376506024, 0.9826101280120482, 0.9848926957831325, 0.9776920180722891, 0.9748446912650602, 0.9831748870481928, 0.9835278614457831, 0.9796216114457831, 0.980445218373494, 0.9877400225903614, 0.9857398343373494, 0.985339796686747, 0.9897166792168675, 0.9855751129518072, 0.9878106174698795, 0.9900931852409639, 0.988257718373494, 0.9896696159638554, 0.9905638177710844, 0.9867281626506024, 0.9862575301204819, 0.9887989457831325, 0.9889636671686747, 0.991199171686747, 0.9883283132530121, 0.9878106174698795, 0.9912932981927711, 0.9931052334337349, 0.9944935993975904, 0.992211031626506, 0.9928934487951807, 0.9918815888554217, 0.993199359939759, 0.9889636671686747, 0.9922816265060241, 0.9936699924698795, 0.9940935617469879, 0.9916698042168675, 0.9932699548192772, 0.9939053087349398, 0.9924934111445783, 0.9940229668674698, 0.9939053087349398, 0.9933640813253012, 0.994117093373494, 0.9952701430722891, 0.9926816641566265, 0.9936229292168675, 0.9954348644578314, 0.9932228915662651, 0.9935758659638554, 0.9963996611445783, 0.9963996611445783, 0.9932934864457831, 0.9926581325301205, 0.992211031626506, 0.9958819653614458, 0.9949642319277109, 0.9967055722891566, 0.9945406626506024, 0.9929405120481928, 0.9949171686746988, 0.9956231174698795, 0.9969879518072289, 0.9948701054216867, 0.9956937123493976, 0.996070218373494, 0.9953642695783133, 0.9938582454819277, 0.9965408509036144, 0.9964467243975904, 0.9955525225903614, 0.9955995858433735, 0.9951760165662651, 0.9966585090361446, 0.9972232680722891, 0.9966349774096386, 0.9947995105421686, 0.9949407003012049, 0.9963055346385542, 0.9970114834337349, 0.9972467996987951, 0.9966349774096386, 0.9964467243975904, 0.9983292545180723, 0.9964702560240963, 0.9975291792168675, 0.9958349021084337, 0.9973173945783133, 0.9945406626506024, 0.996117281626506, 0.9966349774096386, 0.9968938253012049, 0.9955289909638554, 0.9956937123493976, 0.9965408509036144, 0.9977409638554217, 0.9965408509036144, 0.9973173945783133, 0.9964937876506024, 0.9971762048192772, 0.9976233057228916, 0.9979998117469879, 0.9949407003012049, 0.9974585843373494, 0.9964231927710844, 0.9987528237951807, 0.9962820030120482, 0.9984469126506024, 0.9978586219879518, 0.9981880647590361, 0.9987763554216867, 0.9976233057228916, 0.9974821159638554, 0.9984233810240963, 0.9979292168674698, 0.9956701807228916, 0.9975762424698795, 0.998988140060241, 0.9978115587349398, 0.9976233057228916, 0.9978115587349398, 0.9987763554216867, 0.9971291415662651, 0.997035015060241, 0.9985410391566265, 0.9982821912650602, 0.9976939006024096, 0.9982351280120482, 0.9977409638554217, 0.9979292168674698, 0.9987057605421686, 0.9985645707831325, 0.9983057228915663, 0.9938582454819277, 0.9979056852409639, 0.997058546686747, 0.997035015060241, 0.999035203313253, 0.9978350903614458, 0.9989646084337349, 0.9982586596385542, 0.9975527108433735, 0.9979292168674698, 0.9983527861445783, 0.999035203313253, 0.9973644578313253, 0.9985175075301205, 0.9977880271084337, 0.9976233057228916, 0.9987292921686747, 0.999035203313253, 0.9986351656626506, 0.9982821912650602, 0.9983292545180723, 0.9967996987951807, 0.9979527484939759, 0.9973644578313253, 0.9981174698795181, 0.9981174698795181, 0.9988704819277109, 0.9982115963855421, 0.9978821536144579, 0.9983527861445783, 0.998988140060241, 0.9990822665662651, 0.9988940135542169, 0.9992940512048193, 0.9985645707831325, 0.9987528237951807, 0.9987763554216867, 0.9979762801204819, 0.999058734939759, 0.9985175075301205, 0.9990822665662651, 0.9994587725903614, 0.9987292921686747, 0.9981410015060241, 0.9991999246987951, 0.9982821912650602, 0.9991763930722891, 0.9989410768072289, 0.9982586596385542, 0.9972938629518072, 0.9982821912650602, 0.9982821912650602, 0.9985881024096386, 0.999035203313253, 0.9988704819277109, 0.9993175828313253, 0.9992940512048193, 0.9988940135542169, 0.9984939759036144, 0.9982115963855421, 0.9992469879518072], "end": "2016-01-18 03:07:12.491000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0, 211.0, 212.0, 213.0, 214.0, 215.0, 216.0, 217.0, 218.0, 219.0, 220.0, 221.0, 222.0, 223.0, 224.0, 225.0, 226.0, 227.0, 228.0, 229.0, 230.0, 231.0, 232.0, 233.0, 234.0, 235.0, 236.0, 237.0, 238.0], "accuracy_valid": [0.42761752136752135, 0.48384081196581197, 0.561698717948718, 0.5957532051282052, 0.6208600427350427, 0.6431623931623932, 0.6730769230769231, 0.7001869658119658, 0.7259615384615384, 0.7224893162393162, 0.7437232905982906, 0.7592147435897436, 0.7760416666666666, 0.7755074786324786, 0.7808493589743589, 0.7920673076923077, 0.7853899572649573, 0.7923344017094017, 0.7895299145299145, 0.7912660256410257, 0.796073717948718, 0.8058226495726496, 0.8016826923076923, 0.7964743589743589, 0.8098290598290598, 0.8071581196581197, 0.813034188034188, 0.8092948717948718, 0.8207799145299145, 0.8125, 0.8125, 0.8146367521367521, 0.8185096153846154, 0.8141025641025641, 0.8293269230769231, 0.8205128205128205, 0.8242521367521367, 0.8193108974358975, 0.8219818376068376, 0.8225160256410257, 0.8287927350427351, 0.8259882478632479, 0.8321314102564102, 0.8298611111111112, 0.8269230769230769, 0.8333333333333334, 0.8306623931623932, 0.8238514957264957, 0.8293269230769231, 0.8362713675213675, 0.8326655982905983, 0.8297275641025641, 0.8301282051282052, 0.8348023504273504, 0.8327991452991453, 0.8348023504273504, 0.8393429487179487, 0.8380074786324786, 0.8372061965811965, 0.8425480769230769, 0.8400106837606838, 0.8390758547008547, 0.8406784188034188, 0.8405448717948718, 0.828659188034188, 0.8373397435897436, 0.8381410256410257, 0.8396100427350427, 0.8345352564102564, 0.8348023504273504, 0.8425480769230769, 0.8433493589743589, 0.8416132478632479, 0.8402777777777778, 0.8488247863247863, 0.8393429487179487, 0.8418803418803419, 0.8389423076923077, 0.8392094017094017, 0.8474893162393162, 0.8421474358974359, 0.8452190170940171, 0.8446848290598291, 0.8425480769230769, 0.8382745726495726, 0.844284188034188, 0.8405448717948718, 0.8460202991452992, 0.8400106837606838, 0.8470886752136753, 0.8462873931623932, 0.8452190170940171, 0.8473557692307693, 0.8404113247863247, 0.8444177350427351, 0.8492254273504274, 0.8490918803418803, 0.8433493589743589, 0.8412126068376068, 0.8421474358974359, 0.8469551282051282, 0.8488247863247863, 0.8516292735042735, 0.8424145299145299, 0.8412126068376068, 0.8470886752136753, 0.8417467948717948, 0.8501602564102564, 0.8461538461538461, 0.8480235042735043, 0.8501602564102564, 0.8416132478632479, 0.8408119658119658, 0.8464209401709402, 0.8484241452991453, 0.8412126068376068, 0.843482905982906, 0.8412126068376068, 0.8448183760683761, 0.8501602564102564, 0.8454861111111112, 0.8476228632478633, 0.8477564102564102, 0.8464209401709402, 0.8533653846153846, 0.8472222222222222, 0.8452190170940171, 0.8498931623931624, 0.8529647435897436, 0.8514957264957265, 0.8516292735042735, 0.8438835470085471, 0.8500267094017094, 0.843215811965812, 0.8478899572649573, 0.8502938034188035, 0.8474893162393162, 0.8448183760683761, 0.8460202991452992, 0.8474893162393162, 0.8488247863247863, 0.8541666666666666, 0.8466880341880342, 0.8489583333333334, 0.8496260683760684, 0.8521634615384616, 0.8482905982905983, 0.8416132478632479, 0.8458867521367521, 0.8456196581196581, 0.8530982905982906, 0.8489583333333334, 0.8514957264957265, 0.8510950854700855, 0.8492254273504274, 0.8544337606837606, 0.8444177350427351, 0.8504273504273504, 0.8568376068376068, 0.8557692307692307, 0.8470886752136753, 0.8518963675213675, 0.8556356837606838, 0.8485576923076923, 0.8497596153846154, 0.8501602564102564, 0.8541666666666666, 0.8494925213675214, 0.8510950854700855, 0.8514957264957265, 0.8544337606837606, 0.8481570512820513, 0.8481570512820513, 0.8465544871794872, 0.8470886752136753, 0.8530982905982906, 0.8532318376068376, 0.8510950854700855, 0.8436164529914529, 0.8537660256410257, 0.8501602564102564, 0.8494925213675214, 0.8555021367521367, 0.8510950854700855, 0.8528311965811965, 0.8529647435897436, 0.8529647435897436, 0.8489583333333334, 0.8609775641025641, 0.8604433760683761, 0.8497596153846154, 0.8521634615384616, 0.8482905982905983, 0.8490918803418803, 0.8514957264957265, 0.8543002136752137, 0.8536324786324786, 0.8468215811965812, 0.8537660256410257, 0.8462873931623932, 0.8514957264957265, 0.8480235042735043, 0.8497596153846154, 0.8461538461538461, 0.8541666666666666, 0.8508279914529915, 0.8502938034188035, 0.8518963675213675, 0.8556356837606838, 0.8597756410256411, 0.8548344017094017, 0.8557692307692307, 0.8547008547008547, 0.8534989316239316, 0.8526976495726496, 0.8516292735042735, 0.8584401709401709, 0.8520299145299145, 0.8541666666666666, 0.8564369658119658, 0.8567040598290598, 0.8533653846153846, 0.8520299145299145, 0.8534989316239316, 0.8552350427350427, 0.8560363247863247, 0.8533653846153846, 0.8456196581196581, 0.8522970085470085, 0.8492254273504274, 0.8524305555555556, 0.8557692307692307, 0.8556356837606838, 0.8537660256410257, 0.8575053418803419, 0.8538995726495726, 0.8517628205128205, 0.8472222222222222, 0.8555021367521367], "accuracy_test": 0.8558693910256411, "start": "2016-01-17 14:20:58.266000", "learning_rate_per_epoch": [0.0010000000474974513, 0.0007071067811921239, 0.0005773502634838223, 0.0005000000237487257, 0.00044721359154209495, 0.0004082482773810625, 0.000377964461222291, 0.00035355339059606194, 0.00033333332976326346, 0.0003162277571391314, 0.0003015113470610231, 0.00028867513174191117, 0.00027735010371543467, 0.0002672612317837775, 0.00025819888105615973, 0.0002500000118743628, 0.00024253562150988728, 0.00023570226039737463, 0.00022941573115531355, 0.00022360679577104747, 0.00021821788686793298, 0.00021320072119124234, 0.00020851440785918385, 0.00020412413869053125, 0.00019999999494757503, 0.0001961161324288696, 0.00019245008297730237, 0.0001889822306111455, 0.00018569533131085336, 0.00018257419287692755, 0.00017960529658012092, 0.00017677669529803097, 0.00017407764971721917, 0.00017149858467746526, 0.00016903085634112358, 0.00016666666488163173, 0.0001643989817239344, 0.00016222141857724637, 0.00016012815467547625, 0.0001581138785695657, 0.00015617375902365893, 0.00015430334315169603, 0.00015249857096932828, 0.00015075567353051156, 0.00014907120203133672, 0.00014744195505045354, 0.00014586499310098588, 0.00014433756587095559, 0.0001428571413271129, 0.00014142136205919087, 0.00014002800162415951, 0.00013867505185771734, 0.00013736056280322373, 0.00013608275912702084, 0.0001348399673588574, 0.00013363061589188874, 0.0001324532349826768, 0.00013130642764735967, 0.00013018891331739724, 0.00012909944052807987, 0.00012803687423001975, 0.00012700012302957475, 0.00012598815374076366, 0.0001250000059371814, 0.00012403473374433815, 0.00012309149315115064, 0.00012216944014653563, 0.00012126781075494364, 0.0001203858555527404, 0.00011952286149607971, 0.0001186781664728187, 0.00011785113019868731, 0.00011704114876920357, 0.00011624764010775834, 0.00011547005124157295, 0.00011470786557765678, 0.00011396057379897684, 0.00011322770296828821, 0.00011250878742430359, 0.00011180339788552374, 0.00011111111234640703, 0.00011043152335332707, 0.00010976425983244553, 0.00010910894343396649, 0.00010846523218788207, 0.00010783276957226917, 0.00010721124999690801, 0.00010660036059562117, 0.00010599978850223124, 0.00010540925723034889, 0.00010482848301762715, 0.00010425720392959192, 0.00010369517258368433, 0.00010314212704543024, 0.00010259783448418602, 0.00010206206934526563, 0.00010153461334994063, 0.00010101525549544021, 0.00010050378477899358, 9.999999747378752e-05, 9.95037189568393e-05, 9.901475277729332e-05, 9.853292431216687e-05, 9.80580662144348e-05, 9.759000386111438e-05, 9.712858445709571e-05, 9.667364793131128e-05, 9.622504148865119e-05, 9.578262688592076e-05, 9.534625860396773e-05, 9.49157983995974e-05, 9.449111530557275e-05, 9.407208563061431e-05, 9.365857840748504e-05, 9.32504772208631e-05, 9.284766565542668e-05, 9.245003457181156e-05, 9.205746027873829e-05, 9.16698481887579e-05, 9.128709643846378e-05, 9.09090886125341e-05, 9.053574467543513e-05, 9.016696276376024e-05, 8.980264829006046e-05, 8.944272121880203e-05, 8.908707968657836e-05, 8.87356509338133e-05, 8.838834764901549e-05, 8.804508979665115e-05, 8.770580461714417e-05, 8.737040479900315e-05, 8.703882485860959e-05, 8.671099931234494e-05, 8.638684084871784e-05, 8.606629853602499e-05, 8.574929233873263e-05, 8.543576404917985e-05, 8.512565545970574e-05, 8.481889381073415e-05, 8.451542817056179e-05, 8.421519305557013e-05, 8.391813753405586e-05, 8.362420339835808e-05, 8.333333244081587e-05, 8.304548100568354e-05, 8.276059088530019e-05, 8.247861114796251e-05, 8.21994908619672e-05, 8.19231936475262e-05, 8.164966129697859e-05, 8.137884287862107e-05, 8.111070928862318e-05, 8.084520959528163e-05, 8.058229286689311e-05, 8.032192999962717e-05, 8.006407733773813e-05, 7.980869122548029e-05, 7.955572800710797e-05, 7.930515857879072e-05, 7.905693928478286e-05, 7.881104102125391e-05, 7.856742013245821e-05, 7.832604751456529e-05, 7.808687951182947e-05, 7.78498942963779e-05, 7.761505548842251e-05, 7.738232670817524e-05, 7.715167157584801e-05, 7.69230755395256e-05, 7.669650221941993e-05, 7.647190795978531e-05, 7.624928548466414e-05, 7.602859113831073e-05, 7.580980309285223e-05, 7.55928922444582e-05, 7.537783676525578e-05, 7.51646002754569e-05, 7.495316822314635e-05, 7.474351150449365e-05, 7.453560101566836e-05, 7.432941492879763e-05, 7.412493141600862e-05, 7.392212864942849e-05, 7.372097752522677e-05, 7.352146349148825e-05, 7.332355744438246e-05, 7.312724483199418e-05, 7.293249655049294e-05, 7.273929804796353e-05, 7.25476274965331e-05, 7.23574630683288e-05, 7.216878293547779e-05, 7.198157254606485e-05, 7.179581734817475e-05, 7.161148823797703e-05, 7.142857066355646e-05, 7.124705007299781e-05, 7.106690463842824e-05, 7.088811980793253e-05, 7.071068102959543e-05, 7.053455919958651e-05, 7.035975431790575e-05, 7.018623728072271e-05, 7.001400081207976e-05, 6.984303036006168e-05, 6.967330409679562e-05, 6.950480747036636e-05, 6.933752592885867e-05, 6.917144492035732e-05, 6.900655716890469e-05, 6.884284084662795e-05, 6.868028140161186e-05, 6.851887155789882e-05, 6.835858948761597e-05, 6.819943519076332e-05, 6.804137956351042e-05, 6.788442260585725e-05, 6.77285497658886e-05, 6.757373921573162e-05, 6.74199836794287e-05, 6.726727588102221e-05, 6.711560854455456e-05, 6.696495256619528e-05, 6.681530794594437e-05, 6.666666740784422e-05, 6.651900912402198e-05, 6.637233309447765e-05, 6.62266174913384e-05, 6.608186231460422e-05, 6.593804573640227e-05, 6.579516775673255e-05, 6.565321382367983e-05, 6.55121766612865e-05, 6.537204171763733e-05, 6.523280899273232e-05, 6.509445665869862e-05, 6.495697743957862e-05, 6.482037133537233e-05, 6.468462379416451e-05], "accuracy_train_last": 0.9992469879518072, "error_valid": [0.5723824786324787, 0.516159188034188, 0.43830128205128205, 0.4042467948717948, 0.3791399572649573, 0.3568376068376068, 0.32692307692307687, 0.2998130341880342, 0.27403846153846156, 0.2775106837606838, 0.25627670940170943, 0.2407852564102564, 0.22395833333333337, 0.2244925213675214, 0.21915064102564108, 0.2079326923076923, 0.2146100427350427, 0.20766559829059827, 0.2104700854700855, 0.20873397435897434, 0.20392628205128205, 0.1941773504273504, 0.1983173076923077, 0.20352564102564108, 0.19017094017094016, 0.19284188034188032, 0.18696581196581197, 0.1907051282051282, 0.1792200854700855, 0.1875, 0.1875, 0.18536324786324787, 0.18149038461538458, 0.1858974358974359, 0.17067307692307687, 0.17948717948717952, 0.1757478632478633, 0.18068910256410253, 0.17801816239316237, 0.17748397435897434, 0.1712072649572649, 0.17401175213675213, 0.16786858974358976, 0.17013888888888884, 0.17307692307692313, 0.16666666666666663, 0.1693376068376068, 0.17614850427350426, 0.17067307692307687, 0.16372863247863245, 0.16733440170940173, 0.1702724358974359, 0.16987179487179482, 0.1651976495726496, 0.16720085470085466, 0.1651976495726496, 0.16065705128205132, 0.1619925213675214, 0.16279380341880345, 0.15745192307692313, 0.15998931623931623, 0.16092414529914534, 0.15932158119658124, 0.1594551282051282, 0.17134081196581197, 0.1626602564102564, 0.16185897435897434, 0.1603899572649573, 0.1654647435897436, 0.1651976495726496, 0.15745192307692313, 0.15665064102564108, 0.15838675213675213, 0.1597222222222222, 0.1511752136752137, 0.16065705128205132, 0.1581196581196581, 0.1610576923076923, 0.16079059829059827, 0.15251068376068377, 0.1578525641025641, 0.15478098290598286, 0.1553151709401709, 0.15745192307692313, 0.1617254273504274, 0.15571581196581197, 0.1594551282051282, 0.1539797008547008, 0.15998931623931623, 0.15291132478632474, 0.1537126068376068, 0.15478098290598286, 0.15264423076923073, 0.15958867521367526, 0.1555822649572649, 0.1507745726495726, 0.15090811965811968, 0.15665064102564108, 0.1587873931623932, 0.1578525641025641, 0.1530448717948718, 0.1511752136752137, 0.14837072649572647, 0.15758547008547008, 0.1587873931623932, 0.15291132478632474, 0.15825320512820518, 0.1498397435897436, 0.15384615384615385, 0.15197649572649574, 0.1498397435897436, 0.15838675213675213, 0.15918803418803418, 0.15357905982905984, 0.15157585470085466, 0.1587873931623932, 0.15651709401709402, 0.1587873931623932, 0.15518162393162394, 0.1498397435897436, 0.15451388888888884, 0.1523771367521367, 0.15224358974358976, 0.15357905982905984, 0.14663461538461542, 0.1527777777777778, 0.15478098290598286, 0.15010683760683763, 0.1470352564102564, 0.14850427350427353, 0.14837072649572647, 0.15611645299145294, 0.14997329059829057, 0.15678418803418803, 0.1521100427350427, 0.14970619658119655, 0.15251068376068377, 0.15518162393162394, 0.1539797008547008, 0.15251068376068377, 0.1511752136752137, 0.14583333333333337, 0.15331196581196582, 0.15104166666666663, 0.15037393162393164, 0.14783653846153844, 0.15170940170940173, 0.15838675213675213, 0.15411324786324787, 0.1543803418803419, 0.14690170940170943, 0.15104166666666663, 0.14850427350427353, 0.1489049145299145, 0.1507745726495726, 0.14556623931623935, 0.1555822649572649, 0.1495726495726496, 0.1431623931623932, 0.14423076923076927, 0.15291132478632474, 0.14810363247863245, 0.14436431623931623, 0.1514423076923077, 0.15024038461538458, 0.1498397435897436, 0.14583333333333337, 0.1505074786324786, 0.1489049145299145, 0.14850427350427353, 0.14556623931623935, 0.15184294871794868, 0.15184294871794868, 0.15344551282051277, 0.15291132478632474, 0.14690170940170943, 0.14676816239316237, 0.1489049145299145, 0.15638354700854706, 0.14623397435897434, 0.1498397435897436, 0.1505074786324786, 0.1444978632478633, 0.1489049145299145, 0.14716880341880345, 0.1470352564102564, 0.1470352564102564, 0.15104166666666663, 0.1390224358974359, 0.13955662393162394, 0.15024038461538458, 0.14783653846153844, 0.15170940170940173, 0.15090811965811968, 0.14850427350427353, 0.1456997863247863, 0.1463675213675214, 0.15317841880341876, 0.14623397435897434, 0.1537126068376068, 0.14850427350427353, 0.15197649572649574, 0.15024038461538458, 0.15384615384615385, 0.14583333333333337, 0.14917200854700852, 0.14970619658119655, 0.14810363247863245, 0.14436431623931623, 0.14022435897435892, 0.14516559829059827, 0.14423076923076927, 0.14529914529914534, 0.14650106837606836, 0.1473023504273504, 0.14837072649572647, 0.1415598290598291, 0.1479700854700855, 0.14583333333333337, 0.14356303418803418, 0.14329594017094016, 0.14663461538461542, 0.1479700854700855, 0.14650106837606836, 0.1447649572649573, 0.14396367521367526, 0.14663461538461542, 0.1543803418803419, 0.14770299145299148, 0.1507745726495726, 0.14756944444444442, 0.14423076923076927, 0.14436431623931623, 0.14623397435897434, 0.1424946581196581, 0.1461004273504274, 0.14823717948717952, 0.1527777777777778, 0.1444978632478633], "accuracy_train_std": [0.09030121170686933, 0.09169578897930006, 0.08786726982900564, 0.08628283446261038, 0.0863860047130211, 0.08284849091369599, 0.07920843354992389, 0.0782992091415583, 0.07645245467873157, 0.07423481527384913, 0.06979667656662014, 0.06734116730628249, 0.05963187169247615, 0.06220551083981686, 0.05786177186091769, 0.05520884677580845, 0.05471324810900829, 0.05303740464265331, 0.05068795375812047, 0.05001413469760512, 0.046895402630492425, 0.04343455738638878, 0.04488361519154933, 0.04716289808960812, 0.04038496068164049, 0.040271297124643966, 0.03829132902714743, 0.03922212512338299, 0.03390539933102549, 0.03708939683720446, 0.037377823984333515, 0.035732602449018, 0.03419941232801811, 0.03387717433494577, 0.031224251675889925, 0.03279649479299377, 0.03073132221523301, 0.029422890379036136, 0.030347214319992214, 0.02738336257956032, 0.029609294972762643, 0.030586009479050744, 0.02618695780749722, 0.026093252844877526, 0.027080710660368025, 0.023976672056821396, 0.02175559639517753, 0.027511520866999265, 0.028807084240945382, 0.023233319576450084, 0.02331204022006153, 0.025858206161044853, 0.025045246361610395, 0.01952845719163791, 0.021752745512278526, 0.02190882890686103, 0.01809370923961421, 0.021353119657645997, 0.019852382472377826, 0.01773115474282142, 0.01950768998791662, 0.0185488782928185, 0.017488526874694536, 0.020445828490140563, 0.021269258188749675, 0.018459597356968487, 0.018499016671214553, 0.01699283256260251, 0.01911254871517204, 0.01898131651722727, 0.016470726596111422, 0.014712499069198658, 0.013083161099811242, 0.01573051830805214, 0.01483612600352797, 0.016120091653906862, 0.01505227855094579, 0.018736007518722685, 0.015973797767768392, 0.013298991936271999, 0.013601015488889742, 0.01585011827587603, 0.014813415880822344, 0.014155461611292622, 0.015633768740638945, 0.013543018377377567, 0.013517705927754942, 0.014300957601246961, 0.013310749346976238, 0.01202314139359985, 0.015628950970889323, 0.013925296045644601, 0.011747255696429315, 0.014692179761931748, 0.014775444385203009, 0.01075760859024719, 0.010620013618998194, 0.015288079584363378, 0.014367170921152726, 0.01544748673086352, 0.011178860097528972, 0.012991418652203152, 0.009898510930861708, 0.01343538416283892, 0.015055239662790881, 0.012744325708593985, 0.0115033797185895, 0.009536088209272544, 0.012725454556265765, 0.011369428994511247, 0.011114876761313211, 0.012301235125427955, 0.013819437273100532, 0.010387420759940136, 0.010426369534262977, 0.011635360199188127, 0.011841038699967879, 0.012168354010630971, 0.009956853789498996, 0.009296034951834443, 0.010132025407336452, 0.01289788605723321, 0.012724823585325281, 0.010725647213473717, 0.009734217894896743, 0.009263424355886372, 0.010059185463957053, 0.010284343846437368, 0.007236080372557516, 0.010678173998135247, 0.008519326310639735, 0.01142190827071528, 0.008920566175557509, 0.01299013988954895, 0.01119727170966051, 0.010696903856159661, 0.010178292160318726, 0.011907561061259741, 0.011937355468062077, 0.010387420759940136, 0.008789621633982494, 0.010387420759940136, 0.009323633099582654, 0.01078865410585357, 0.009516557506330852, 0.008716458785305596, 0.007932043231666016, 0.012313652938152227, 0.009043495221827926, 0.010453552628038787, 0.0063529726292490835, 0.01081994319935832, 0.007004523728781914, 0.007987696147366215, 0.007403417698556541, 0.0061816109287854935, 0.008546063447147938, 0.008927360718217836, 0.006839854940114489, 0.007867309372728486, 0.011584827893272167, 0.00887086141201678, 0.005791218860743716, 0.008246920293806264, 0.008459578818478588, 0.008681320614717908, 0.006061483895734917, 0.009346411318478118, 0.009473518369696553, 0.006919258600659764, 0.007524198478478499, 0.008608202194782874, 0.007213700609925326, 0.008620672546577795, 0.00814289486215443, 0.006343552154719618, 0.0065419191366503215, 0.007281279972068417, 0.013872547466193324, 0.008533873397407236, 0.009284919049639717, 0.009395574684092054, 0.005405466105164791, 0.008297559827336006, 0.005723170223479014, 0.007567531081355835, 0.008569131533690544, 0.007960231367419418, 0.007292070999813019, 0.005405466105164791, 0.009097705399239653, 0.006752838841340134, 0.008373424354165254, 0.008546063447147938, 0.006290123050700258, 0.00567102434550747, 0.006386571871807339, 0.007326125130673455, 0.007236080372557516, 0.009780101813147994, 0.007920026776948932, 0.00885189604330478, 0.007533649399007432, 0.007630635617285859, 0.006079727131760219, 0.00735934454419875, 0.008484840177152678, 0.007087513397028682, 0.0056628163696527565, 0.005413655124354382, 0.0057739821971033905, 0.0046435477975337435, 0.006870872285259798, 0.006236147414178204, 0.0061816109287854935, 0.00778566186915471, 0.005341212710508701, 0.006967229415654761, 0.005276071943105206, 0.004076816091297321, 0.006172108338025525, 0.007588101267734592, 0.004935811247915851, 0.007122545620565173, 0.005005935289193275, 0.0056542047580458105, 0.00727016827160191, 0.008954611010293836, 0.007425822249886652, 0.007225052440893328, 0.006602959302445627, 0.005405466105164791, 0.005957546083752507, 0.0045672577471588295, 0.0046435477975337435, 0.00577398219710339, 0.006909167659451268, 0.007748732718756914, 0.004943210092432561], "accuracy_test_std": 0.06285141883799969, "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-3, 3], "rotation_range": [0, 0], "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.001, "patience_threshold": 1, "do_flip": true, "nb_data_augmentation": 1, "optimization": "adam", "batch_size": 32, "learning_rate_decay_method": "sqrt", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0, "valid_ratio": 0.15, "momentum": 0.9, "learning_rate_decay": 0.98}, "accuracy_valid_max": 0.8609775641025641, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = 1234\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.98, interval=[0.8, 1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-10, -6], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128, 256, 512],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-3, 3)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_optimizer.learning_rate = learning_rate\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8555021367521367, "loss_train": [2.898263454437256, 1.4504069089889526, 1.243301510810852, 1.1040476560592651, 0.9857926964759827, 0.8864318132400513, 0.8030593395233154, 0.7283868193626404, 0.6667057275772095, 0.6092514395713806, 0.5638210773468018, 0.5199057459831238, 0.481410950422287, 0.44594261050224304, 0.41678744554519653, 0.3874911367893219, 0.3642217218875885, 0.3505224287509918, 0.32865142822265625, 0.31282472610473633, 0.2971987724304199, 0.28309211134910583, 0.271897554397583, 0.2618879973888397, 0.251578688621521, 0.24100814759731293, 0.2359447181224823, 0.22226707637310028, 0.2175271362066269, 0.21079543232917786, 0.20428648591041565, 0.19813017547130585, 0.19011236727237701, 0.18666626513004303, 0.18230964243412018, 0.17598845064640045, 0.1700056493282318, 0.16566115617752075, 0.16051384806632996, 0.15787769854068756, 0.15411324799060822, 0.15128980576992035, 0.14543680846691132, 0.14432087540626526, 0.13933302462100983, 0.13740912079811096, 0.13192787766456604, 0.13201768696308136, 0.13188892602920532, 0.127559095621109, 0.12387174367904663, 0.12246527522802353, 0.11950851231813431, 0.11858407407999039, 0.11275508254766464, 0.11116629093885422, 0.11181241273880005, 0.10741475969552994, 0.10463235527276993, 0.10380850732326508, 0.1009780764579773, 0.09787216037511826, 0.09799277037382126, 0.09589795023202896, 0.09596577286720276, 0.09024786949157715, 0.0930071622133255, 0.09117201715707779, 0.08658593893051147, 0.088733971118927, 0.08283831179141998, 0.08493620902299881, 0.08249920606613159, 0.08127408474683762, 0.08161312341690063, 0.07903590053319931, 0.07701244205236435, 0.07782608270645142, 0.07600661367177963, 0.0744633674621582, 0.07339499890804291, 0.07193885743618011, 0.06994425505399704, 0.07168354094028473, 0.07012675702571869, 0.06735248118638992, 0.06798486411571503, 0.06772707402706146, 0.06613205373287201, 0.06465649604797363, 0.06439321488142014, 0.06517939269542694, 0.06123032420873642, 0.06110881641507149, 0.06070547178387642, 0.06167253479361534, 0.05818873271346092, 0.058481037616729736, 0.05685121566057205, 0.05808885395526886, 0.055011168122291565, 0.05575079843401909, 0.056257136166095734, 0.05329054594039917, 0.054737892001867294, 0.055558908730745316, 0.051523659378290176, 0.052984386682510376, 0.050037454813718796, 0.04870917275547981, 0.0498770996928215, 0.048785142600536346, 0.047979000955820084, 0.050285086035728455, 0.04821660369634628, 0.04903991147875786, 0.046852998435497284, 0.047623973339796066, 0.04543507471680641, 0.04658852890133858, 0.04673490673303604, 0.04527869448065758, 0.043631624430418015, 0.04459180682897568, 0.043411657214164734, 0.04321708902716637, 0.04215538129210472, 0.04271332919597626, 0.0424429327249527, 0.04145143926143646, 0.03761732950806618, 0.04001108184456825, 0.04029691591858864, 0.040161412209272385, 0.039337124675512314, 0.039046235382556915, 0.04004432633519173, 0.04054981842637062, 0.0381946824491024, 0.04038941487669945, 0.0380590558052063, 0.037586018443107605, 0.03709794208407402, 0.035591624677181244, 0.03727057948708534, 0.03461722284555435, 0.03634219616651535, 0.03391725569963455, 0.03378886729478836, 0.036241330206394196, 0.03487571328878403, 0.035082120448350906, 0.03340645879507065, 0.03152945265173912, 0.03456109017133713, 0.03332860395312309, 0.03543834760785103, 0.031721439212560654, 0.03238007053732872, 0.03152266889810562, 0.032125819474458694, 0.033340297639369965, 0.03271057456731796, 0.029841655865311623, 0.03206023573875427, 0.03189961612224579, 0.030527498573064804, 0.03019721806049347, 0.030848348513245583, 0.031521983444690704, 0.02950967475771904, 0.02998778596520424, 0.030276693403720856, 0.028423728421330452, 0.028313973918557167, 0.02891463227570057, 0.02990475855767727, 0.02872663363814354, 0.02600846253335476, 0.02941230870783329, 0.02892475202679634, 0.02838771417737007, 0.026157740503549576, 0.026237791404128075, 0.027522016316652298, 0.025859950110316277, 0.027582723647356033, 0.02613776922225952, 0.0261190477758646, 0.027204090729355812, 0.02481882832944393, 0.025306569412350655, 0.026228176429867744, 0.024631928652524948, 0.02640766277909279, 0.025242451578378677, 0.025947295129299164, 0.02446688525378704, 0.025262001901865005, 0.024184325709939003, 0.02514847368001938, 0.024973034858703613, 0.024154866114258766, 0.02393977902829647, 0.02589721418917179, 0.023720821365714073, 0.024026039987802505, 0.02220858819782734, 0.023427272215485573, 0.023495541885495186, 0.02281833626329899, 0.023556027561426163, 0.02262481488287449, 0.022712193429470062, 0.022579653188586235, 0.022898467257618904, 0.021443219855427742, 0.021247180178761482, 0.021768398582935333, 0.020588455721735954, 0.02236279472708702, 0.02171841822564602, 0.02251119539141655, 0.021885838359594345, 0.021248135715723038, 0.020830394700169563, 0.021480677649378777, 0.021470407024025917, 0.020999476313591003, 0.021341431885957718, 0.021199561655521393, 0.020045926794409752, 0.02092778868973255, 0.020141689106822014, 0.020825067535042763, 0.02051152102649212, 0.01938798278570175, 0.01911323145031929, 0.02067810483276844], "accuracy_train_first": 0.44197100903614456, "model": "residualv2", "loss_std": [27.891738891601562, 0.19703683257102966, 0.19394657015800476, 0.19364382326602936, 0.19121818244457245, 0.19365522265434265, 0.18551626801490784, 0.17984475195407867, 0.1746646910905838, 0.17127349972724915, 0.16234420239925385, 0.15499934554100037, 0.14775797724723816, 0.14236387610435486, 0.13807399570941925, 0.13101345300674438, 0.12574738264083862, 0.12059694528579712, 0.11792003363370895, 0.11583741009235382, 0.11142215132713318, 0.10978025943040848, 0.10640992224216461, 0.10130522400140762, 0.10103584080934525, 0.09725647419691086, 0.0995926782488823, 0.09315145015716553, 0.09521850943565369, 0.09381294250488281, 0.09001073241233826, 0.08862088620662689, 0.0871419832110405, 0.0874495804309845, 0.08492643386125565, 0.08226721733808517, 0.08140652626752853, 0.07736493647098541, 0.07853193581104279, 0.0756838247179985, 0.07551294565200806, 0.07656645029783249, 0.0749504566192627, 0.07161448150873184, 0.07214092463254929, 0.07071106135845184, 0.0692940503358841, 0.07226569950580597, 0.07211508601903915, 0.06876905262470245, 0.06687874346971512, 0.06731928139925003, 0.06561188399791718, 0.0670555904507637, 0.06437689810991287, 0.06373067945241928, 0.06274279952049255, 0.06272003799676895, 0.061577603220939636, 0.06286021322011948, 0.06082531809806824, 0.05823029577732086, 0.058887433260679245, 0.059912603348493576, 0.061610303819179535, 0.057576779276132584, 0.05901297926902771, 0.05735726282000542, 0.0565207377076149, 0.05704100430011749, 0.05455268919467926, 0.05636116489768028, 0.05521465837955475, 0.054755471646785736, 0.05427416041493416, 0.054854705929756165, 0.05347980186343193, 0.054529882967472076, 0.05041078105568886, 0.052277617156505585, 0.052275341004133224, 0.05177008733153343, 0.05265463516116142, 0.04941249266266823, 0.05060242861509323, 0.04910211265087128, 0.04911137744784355, 0.04933847114443779, 0.046326473355293274, 0.04886920005083084, 0.04900985583662987, 0.04777725040912628, 0.04751439020037651, 0.0452459491789341, 0.04556624963879585, 0.048555146902799606, 0.04530928283929825, 0.0469721183180809, 0.04427526518702507, 0.04627680405974388, 0.04396761581301689, 0.04479210078716278, 0.045776091516017914, 0.04401426389813423, 0.045629169791936874, 0.04686915874481201, 0.04343592748045921, 0.043671928346157074, 0.0421837717294693, 0.040551185607910156, 0.04472148418426514, 0.043308552354574203, 0.04202145338058472, 0.04408390820026398, 0.04245160520076752, 0.04301241412758827, 0.03912702202796936, 0.04111121594905853, 0.040362272411584854, 0.04114793986082077, 0.040733326226472855, 0.04056299477815628, 0.03911713883280754, 0.03926248848438263, 0.03977513685822487, 0.03974887728691101, 0.03904774785041809, 0.04017782211303711, 0.03861992806196213, 0.038838185369968414, 0.03708033263683319, 0.03796914964914322, 0.039353616535663605, 0.03719033673405647, 0.03960724547505379, 0.03761608153581619, 0.03844662755727768, 0.037173278629779816, 0.03738127276301384, 0.03981514275074005, 0.03859797120094299, 0.03810534253716469, 0.03635044023394585, 0.03670519217848778, 0.0358843095600605, 0.03369171544909477, 0.036615826189517975, 0.03500944375991821, 0.03592870011925697, 0.038069259375333786, 0.03633682057261467, 0.03391529992222786, 0.035284314304590225, 0.03292454406619072, 0.036234162747859955, 0.03636212646961212, 0.038785308599472046, 0.034524425864219666, 0.03400888666510582, 0.03442936763167381, 0.03378165140748024, 0.03547162562608719, 0.0373402014374733, 0.03214775770902634, 0.034758660942316055, 0.035550568252801895, 0.032591961324214935, 0.03455996885895729, 0.034057579934597015, 0.032934997230768204, 0.03309077024459839, 0.03260084241628647, 0.034290798008441925, 0.03106990084052086, 0.03083440475165844, 0.03201292082667351, 0.03375320881605148, 0.033838849514722824, 0.03043227642774582, 0.033036623150110245, 0.03271670266985893, 0.03335868567228317, 0.029110155999660492, 0.03132227435708046, 0.033149655908346176, 0.029742611572146416, 0.03303077816963196, 0.03063388727605343, 0.03179797902703285, 0.03428639471530914, 0.030164498835802078, 0.02938825450837612, 0.030441494658589363, 0.02977360412478447, 0.03210851550102234, 0.03084614686667919, 0.031433749943971634, 0.03013635240495205, 0.029778005555272102, 0.030392590910196304, 0.030894070863723755, 0.030550837516784668, 0.02996801771223545, 0.02893800660967827, 0.030539069324731827, 0.029635069891810417, 0.030999576672911644, 0.026688454672694206, 0.02976410835981369, 0.02998143434524536, 0.028383342549204826, 0.03123798593878746, 0.02763761207461357, 0.03017463907599449, 0.02981179766356945, 0.030022932216525078, 0.028330927714705467, 0.027313709259033203, 0.026234164834022522, 0.028583142906427383, 0.029092485085129738, 0.02895538881421089, 0.029507391154766083, 0.027922242879867554, 0.02808152325451374, 0.027777615934610367, 0.027707885950803757, 0.03109937347471714, 0.02820655331015587, 0.028820334002375603, 0.027826867997646332, 0.025238068774342537, 0.026740124449133873, 0.027406975626945496, 0.027231276035308838, 0.027706511318683624, 0.02550770901143551, 0.026975682005286217, 0.02846192754805088]}, "state": "available", "life": [{"dt": "Sun May 15 22:04:59 2016", "state": "available"}], "summary": "d1c4d12eee6452ad3be353de563dc797"}