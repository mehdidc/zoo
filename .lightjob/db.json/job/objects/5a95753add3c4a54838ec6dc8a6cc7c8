{"content": {"hp_model": {"f0": 16, "f1": 16, "f2": 16, "f3": 16, "nonlin": "leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.687749981880188, 1.3654016256332397, 1.2163617610931396, 1.1081359386444092, 1.0212111473083496, 0.9561724662780762, 0.9064884185791016, 0.8726115226745605, 0.8424828052520752, 0.8152554035186768, 0.7954978942871094, 0.7746536135673523, 0.7603910565376282, 0.7450839877128601, 0.7320006489753723, 0.7247365713119507, 0.713044285774231, 0.7014626264572144, 0.6952075958251953, 0.6851764917373657, 0.679064154624939, 0.6731114387512207, 0.6685783267021179, 0.6617369055747986, 0.6567622423171997, 0.6514372229576111, 0.6431626081466675, 0.6409425139427185, 0.6349331140518188, 0.6319122314453125, 0.626326858997345, 0.622503936290741, 0.6230019927024841, 0.6158990859985352, 0.6139304041862488, 0.6105002164840698, 0.607258677482605, 0.6037973761558533, 0.6000785827636719, 0.5988690257072449, 0.5944581627845764, 0.5916083455085754, 0.5906984210014343, 0.5877840518951416, 0.5857097506523132, 0.5854165554046631, 0.5816105008125305, 0.5806081891059875, 0.5807209610939026, 0.5780711770057678, 0.5749210119247437, 0.5734240412712097, 0.5720511674880981, 0.5702103972434998, 0.5691305994987488, 0.5664713978767395, 0.5685608983039856, 0.5619855523109436, 0.5653280019760132, 0.5597394704818726, 0.5598959922790527, 0.5573087930679321, 0.5585155487060547, 0.5552937388420105, 0.5551175475120544, 0.5560391545295715, 0.5533043146133423, 0.5541442036628723, 0.5502840280532837, 0.5507426857948303, 0.552885115146637, 0.5463234782218933, 0.5463476777076721, 0.5463786125183105, 0.5445591807365417, 0.5427802801132202, 0.544774055480957, 0.5435556769371033, 0.538783609867096, 0.5431733727455139, 0.5408854484558105, 0.540117084980011, 0.5397903323173523, 0.5377121567726135, 0.5338679552078247, 0.539862871170044, 0.5351922512054443, 0.5355371832847595, 0.46225497126579285, 0.4293519854545593, 0.4201609194278717, 0.4163045287132263, 0.4114890396595001, 0.40786293148994446, 0.4036673605442047, 0.40329208970069885, 0.39919641613960266, 0.39744582772254944, 0.39329326152801514, 0.3929903209209442, 0.39190974831581116, 0.3894248902797699, 0.3861941695213318, 0.38549908995628357, 0.38223186135292053, 0.3821275532245636, 0.3811410963535309, 0.3795313835144043, 0.37617406249046326, 0.37580087780952454, 0.37459173798561096, 0.37340694665908813, 0.3712295889854431, 0.3615936040878296, 0.3559262156486511, 0.35455507040023804, 0.3581966161727905, 0.3543134927749634, 0.35641908645629883, 0.35665562748908997, 0.3553134500980377, 0.3567635118961334, 0.3558591902256012, 0.35456016659736633, 0.3549111485481262, 0.3541072905063629, 0.3576450049877167, 0.35607534646987915, 0.3566909730434418, 0.35617595911026, 0.3548439145088196, 0.3562036156654358, 0.3549000322818756, 0.3551231920719147, 0.35509687662124634, 0.3562135100364685, 0.35535284876823425, 0.3549763262271881, 0.3569374680519104, 0.3549591898918152, 0.35587337613105774, 0.35583582520484924], "moving_avg_accuracy_train": [0.04924539007705794, 0.10419047691145254, 0.1575888693504291, 0.208874152312027, 0.2590758007343865, 0.3065635074941889, 0.35093884365645334, 0.39075770312501806, 0.42940102311336525, 0.4652099078302218, 0.4968193784432406, 0.5277206816472738, 0.5554552688154645, 0.5807577255071923, 0.603611424884537, 0.6245841139240917, 0.6444778410918063, 0.6633471322987017, 0.6808315823324896, 0.697067133868759, 0.7114609267514936, 0.7240501117387658, 0.7352501698939774, 0.7468436778157553, 0.757642991454907, 0.7668601776361051, 0.7755738835943478, 0.7831742953341194, 0.7904818045177155, 0.7973215208913408, 0.8037469828895083, 0.8088002345474936, 0.8144033460133792, 0.8195040587600018, 0.824155262247466, 0.8287480661837567, 0.8326467857454752, 0.8370345396010218, 0.8410905109650706, 0.8446525655867714, 0.8476978713319887, 0.8507805515241406, 0.8529875913387346, 0.8553550352837187, 0.8574069681652305, 0.8595139802299828, 0.8618962832382692, 0.8642051891694718, 0.8658114876897708, 0.8678195900282134, 0.8695875348471436, 0.8706853210507367, 0.8716759061245113, 0.8727582030397456, 0.8742435867086393, 0.8748783091678048, 0.8761727167096345, 0.8774607721889109, 0.8783480878560018, 0.8785722160051562, 0.878385883829936, 0.8798409945460103, 0.8808274345547721, 0.8816198634126485, 0.8831700308585284, 0.8842837304074117, 0.8854349055740349, 0.885450331043071, 0.8859780358032896, 0.8852905038291806, 0.8862550792941308, 0.8867091765292158, 0.8856600678348582, 0.8864875252563373, 0.8881993906939741, 0.8896052470057136, 0.8902822910862884, 0.8910309594432834, 0.8902865283372257, 0.890646689410849, 0.8898268251140056, 0.890160985043312, 0.8901174267117844, 0.8913593091098198, 0.8916609481335462, 0.8921905868703947, 0.8923532584978162, 0.8927880174636954, 0.8978597169401018, 0.9027381776069721, 0.9072891914261936, 0.9116315335884837, 0.9155884336107261, 0.9192380713831437, 0.9225761156544823, 0.9259151408784397, 0.9287762125002672, 0.9314674704492069, 0.9339314192330053, 0.9361373473943762, 0.9381155991467252, 0.9400704479333724, 0.9418415096830399, 0.9434400434577223, 0.9448834102013742, 0.9463079983063752, 0.9475969949008485, 0.9487316954430635, 0.9497968874655253, 0.9507952320107779, 0.9516727797134007, 0.95241378056958, 0.9532597898961123, 0.9542769286102201, 0.9551830889064977, 0.9558823396838528, 0.9566279588727673, 0.9572082992904002, 0.9577794337912697, 0.9583237178253949, 0.9587996225632505, 0.9591976377951591, 0.9595791390407908, 0.95996899313805, 0.9602268919220209, 0.9605217437966332, 0.9607406075075938, 0.9609422711938961, 0.9611399724555972, 0.9612900739030515, 0.9614693430331414, 0.9616980424680609, 0.9617830002702119, 0.961947781898091, 0.9620775562703436, 0.9621780411148855, 0.962233636291649, 0.9623045261923845, 0.9623916146399604, 0.9624118294737218, 0.9624044822360212, 0.9624536372447003], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.049503629753388546, 0.10385649531720631, 0.1567882756612387, 0.20818782017719312, 0.25824600305028705, 0.30581404558223124, 0.35013292575668886, 0.3892417564396796, 0.4269228789414195, 0.461728031982895, 0.49237922398415374, 0.522014166090633, 0.5485910462624431, 0.5722955123252048, 0.5937333650836482, 0.613461738147949, 0.6319904349092084, 0.6494475119943417, 0.6660948518152389, 0.6810478960482481, 0.6938830772642064, 0.7056321118758883, 0.7157169322677422, 0.7261279550612993, 0.7358609979870219, 0.7445098438302625, 0.7518665589954291, 0.7584398040277386, 0.7643749910760189, 0.7697462210252695, 0.7754643817728931, 0.7793054565379983, 0.7833535088698912, 0.7874860667272545, 0.7912949360522701, 0.7945601090038954, 0.7973003636343793, 0.8004166539840438, 0.8038043113454285, 0.8064361048908555, 0.8084476266495109, 0.8108358447356893, 0.8127766919733402, 0.814755388080976, 0.816451794867758, 0.8178401655974732, 0.8192829527368976, 0.8209456130825602, 0.8220178498258855, 0.8235586523809476, 0.8247246186093438, 0.8253701266749907, 0.8260253556302326, 0.8272814479380678, 0.828232943581008, 0.8284707020278771, 0.8293316572863092, 0.8299488551213079, 0.8306915566761952, 0.8308299381885456, 0.8305262059472512, 0.8316862171296345, 0.8327211086885083, 0.8330614260481967, 0.8338173128108469, 0.8350825188885724, 0.8362803275701218, 0.8362881075954892, 0.8368057459135005, 0.8358322202208703, 0.8360546799100032, 0.8363821114860208, 0.835608611133729, 0.8363600970647838, 0.8376000168575524, 0.838390325789267, 0.8388940842965602, 0.8387462338958952, 0.8379794324189562, 0.838470451667648, 0.8379184519169224, 0.8380452402436789, 0.8378469673958923, 0.8385689006920711, 0.8385085738288128, 0.8386851837369708, 0.8387200033244936, 0.8394390527379026, 0.8430750079215672, 0.8465203250416846, 0.8494868331060402, 0.8524130380202103, 0.8548746944968038, 0.8571787230704668, 0.8591791065992636, 0.8610384279141112, 0.8626497524325645, 0.8641742161953322, 0.865277678894323, 0.8662321152123456, 0.8673128934783851, 0.86836089512264, 0.8689714182327405, 0.8696795804380809, 0.8703179559315469, 0.8708639617785278, 0.8712984499191991, 0.871946866410713, 0.8722720345795061, 0.8726145435650796, 0.8728363229246862, 0.8732455028969013, 0.8735781732868045, 0.8740586230891482, 0.8744432292949171, 0.8746784820901995, 0.8749766883333632, 0.8751585952248009, 0.8753833465833448, 0.8756965155959441, 0.8758176172837141, 0.8758788101863668, 0.8760925752050042, 0.8762849637217778, 0.8763726641681241, 0.8765248367573358, 0.8765519288063763, 0.8767604466279224, 0.8767883917524043, 0.876813542364438, 0.8767995568215183, 0.8769588977790502, 0.8769547907571693, 0.8770731647499764, 0.8770698380622529, 0.8771156721683017, 0.8772311945599052, 0.8772608930161887, 0.877263207564344, 0.8772530836264336, 0.8773914859659739, 0.8773685341879006], "moving_var_accuracy_train": [0.02182597599457438, 0.04681404150026941, 0.067795132185845, 0.08468724120332098, 0.09890036662188859, 0.10930607059944457, 0.11609799767512582, 0.11875807203200898, 0.12032202044630423, 0.11983030442365991, 0.11683970167321162, 0.11374974636325882, 0.10929763765544265, 0.10412980272162925, 0.0984174466265367, 0.09253438513383179, 0.08684279004605977, 0.08136296239730938, 0.0759780200944347, 0.07075255628817294, 0.0655419321213152, 0.06041412711697752, 0.05550168612940088, 0.05116120234985175, 0.047094708690557485, 0.04314984651139157, 0.03951821990399282, 0.03608629424112006, 0.03295826203122314, 0.03008347130874556, 0.027446703234880056, 0.024931851082262434, 0.022721219696928436, 0.020683253161947617, 0.01880963109068905, 0.017118512613595007, 0.015543461480223841, 0.014162386787273233, 0.012894206241899766, 0.011718979715861607, 0.010630546728012127, 0.00965301830971468, 0.008731555701432045, 0.007908843248782615, 0.007155852780856419, 0.006480223001339884, 0.005883279009815506, 0.005342930528226239, 0.004831859229830449, 0.004384965581862333, 0.00397459968362114, 0.0035879859261982215, 0.0032380186626738646, 0.0029247590959210106, 0.002652140468123253, 0.002390552274712451, 0.0021665764652003166, 0.0019648506009395327, 0.0017754515026831628, 0.0015983584532600365, 0.001438835085049734, 0.0013140077013090695, 0.001191364506196137, 0.0010778795470296814, 0.000991718764319104, 0.000903709828053838, 0.0008252656836267031, 0.0007427412567698878, 0.0006709733819185149, 0.0006081303456654629, 0.0005556909635471706, 0.0005019777058826592, 0.0004616855967675824, 0.0004216792091500719, 0.00040588563772424525, 0.00038308496167514187, 0.00034890196369100036, 0.00031905630610079084, 0.0002921382745357091, 0.0002640918910727198, 0.00024373229915259566, 0.00022036403496252226, 0.00019834470742047925, 0.00019239068369338039, 0.0001739704902297539, 0.00015909809593091196, 0.00014342644486313268, 0.00013078493860253014, 0.0003492056649531045, 0.0005284795047615924, 0.0006620370953301459, 0.0007655368048867599, 0.000829896644472284, 0.000866785682853772, 0.0008803899705851453, 0.0008926927785426482, 0.0008770950781154228, 0.00085457139443346, 0.0008237536474727484, 0.0007851733542036381, 0.0007418773387443219, 0.0007020825088777924, 0.0006601041954802331, 0.0006170915679914115, 0.0005741321792023934, 0.0005349840227023471, 0.0004964392304171842, 0.00045838321525999075, 0.0004227566001364385, 0.0003894511666021148, 0.0003574368596753109, 0.000326634914127506, 0.00030041300873997085, 0.0002796828483396067, 0.0002591047018485975, 0.00023759479651042222, 0.0002188388486332771, 0.0001999861187729936, 0.00018292325845844574, 0.00016729713860083308, 0.00015260579261636942, 0.00013877095847821395, 0.0001262037514341597, 0.00011495125224509072, 0.00010405473306554482, 9.443169841065146e-05, 8.541964048536501e-05, 7.724369061818568e-05, 6.987109365627135e-05, 6.308675829139513e-05, 5.706731925128407e-05, 5.183131820994828e-05, 4.6713146842270546e-05, 4.228620902202199e-05, 3.820916060906166e-05, 3.447911938399891e-05, 3.105902485871338e-05, 2.7998350775078545e-05, 2.526677527688128e-05, 2.274377550472917e-05, 2.0469883791372734e-05, 1.8444641346139704e-05], "duration": 86761.923627, "accuracy_train": [0.49245390077057954, 0.598696258421004, 0.6381744013012182, 0.6704416989664083, 0.710890636535622, 0.7339528683324105, 0.7503168691168328, 0.7491274383421004, 0.7771909030084901, 0.7874898702819306, 0.7813046139604098, 0.8058324104835732, 0.8050665533291805, 0.8084798357327427, 0.8092947192806386, 0.8133383152800849, 0.8235213856012367, 0.8331707531607604, 0.8381916326365817, 0.8431870976951827, 0.8410050626961055, 0.8373527766242157, 0.8360506932908823, 0.8511852491117571, 0.8548368142072721, 0.8498148532668882, 0.8539972372185308, 0.8515780009920635, 0.8562493871700813, 0.8588789682539681, 0.8615761408730158, 0.8542794994693614, 0.8648313492063492, 0.8654104734796051, 0.8660160936346438, 0.8700833016103728, 0.8677352618009413, 0.8765243243009413, 0.8775942532415099, 0.8767110571820783, 0.8751056230389442, 0.8785246732535069, 0.8728509496700813, 0.8766620307885751, 0.8758743640988372, 0.8784770888127538, 0.8833370103128461, 0.8849853425502953, 0.8802681743724622, 0.8858925110741971, 0.8854990382175157, 0.880565396883075, 0.8805911717884828, 0.8824988752768549, 0.8876120397286821, 0.8805908113002953, 0.8878223845861019, 0.8890532715023993, 0.8863339288598191, 0.8805893693475452, 0.8767088942529531, 0.8929369909906791, 0.8897053946336286, 0.8887517231335363, 0.897121537871447, 0.8943070263473607, 0.8957954820736435, 0.8855891602643964, 0.8907273786452565, 0.8791027160622, 0.8949362584786821, 0.8907960516449798, 0.8762180895856404, 0.8939346420496493, 0.9036061796327058, 0.9022579538113695, 0.8963756878114618, 0.8977689746562385, 0.8835866483827058, 0.8938881390734589, 0.8824480464424143, 0.893168424407069, 0.8897254017280363, 0.9025362506921374, 0.8943756993470838, 0.8969573355020304, 0.8938173031446106, 0.8967008481566077, 0.9435050122277593, 0.946644323608804, 0.9482483157991879, 0.9507126130490956, 0.9512005338109081, 0.9520848113349022, 0.9526185140965301, 0.9559663678940569, 0.9545258570967147, 0.955688791989664, 0.9561069582871908, 0.9559907008467147, 0.9559198649178663, 0.9576640870131967, 0.957781065430048, 0.9578268474298633, 0.9578737108942414, 0.9591292912513842, 0.9591979642511074, 0.9589440003229974, 0.9593836156676817, 0.959780332918051, 0.9595707090370063, 0.9590827882751938, 0.9608738738349022, 0.9634311770371908, 0.9633385315729974, 0.962175596680048, 0.9633385315729974, 0.9624313630490956, 0.9629196442990956, 0.9632222741325213, 0.9630827652039498, 0.9627797748823367, 0.9630126502514765, 0.9634776800133813, 0.9625479809777593, 0.9631754106681433, 0.9627103809062385, 0.9627572443706165, 0.9629192838109081, 0.9626409869301403, 0.9630827652039498, 0.9637563373823367, 0.9625476204895718, 0.9634308165490033, 0.9632455256206165, 0.9630824047157622, 0.9627339928825213, 0.9629425352990033, 0.9631754106681433, 0.9625937629775747, 0.9623383570967147, 0.9628960323228128], "end": "2016-01-30 15:59:13.076000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0], "moving_var_accuracy_valid": [0.022055484228845186, 0.046438041760946694, 0.06701019791835205, 0.0840863967145451, 0.09823015209609562, 0.10877160491937371, 0.11557191268669773, 0.11778022715454549, 0.11878100737601109, 0.11780549474257478, 0.11448040540819951, 0.11093643301026893, 0.10619976474624286, 0.10063690367350417, 0.09470944708418803, 0.08874138070964731, 0.0829570560717189, 0.07740409632775362, 0.07215789200299011, 0.06695444458920008, 0.06174167702189848, 0.05680986764846709, 0.05204421330464396, 0.04781529653465126, 0.04388635600253175, 0.04017094321205983, 0.03664094021304636, 0.03336571414403476, 0.03034618073731396, 0.027571213664092098, 0.025108368558703863, 0.02273031640099363, 0.02060476531002996, 0.01869799108902706, 0.016958759349939755, 0.015358835604582014, 0.013890533003082812, 0.012588881092665238, 0.011433278984982014, 0.010352288121875577, 0.009353475287757916, 0.008469460029626473, 0.007656416018662902, 0.006926011561373972, 0.0062593105691127315, 0.005650727671749627, 0.005104389617141859, 0.004618830610253025, 0.004167294773931354, 0.0037719319491613906, 0.003406974049457096, 0.003070026770476724, 0.0027668880182831384, 0.002504399127427049, 0.002262107310311152, 0.002036405340991555, 0.0018394360025055978, 0.0016589208007627804, 0.0014979931710831903, 0.0013483661989615147, 0.0012143598585349783, 0.0011050345061707693, 0.0010041700604013484, 0.0009047953975089607, 0.000819458140939613, 0.0007519190446176747, 0.0006896398508942628, 0.000620676410563989, 0.0005610203143620523, 0.0005134480533937466, 0.0004625486428739737, 0.0004172586815193376, 0.00038091753852236433, 0.0003479083646112879, 0.0003269541379826532, 0.00029988001805231944, 0.0002721759699501198, 0.00024515511062389914, 0.00022593146010683053, 0.00020550821321941955, 0.00018769972542068814, 0.00016907443039683486, 0.0001525207964566759, 0.00014195940596819319, 0.00012779621934524917, 0.00011529731694766004, 0.00010377849688597136, 9.805393572768954e-05, 0.0002072300730334747, 0.000293338956253692, 0.0003432065914913059, 0.00038595000913959303, 0.0004018927817044232, 0.0004094804325482806, 0.00040454619765398236, 0.0003952052596552084, 0.00037905203402360766, 0.00036206273849717207, 0.00033681513400003293, 0.000311332158766473, 0.0002907116778329165, 0.0002715252770668728, 0.00024772739557188617, 0.00022746809939635092, 0.0002083889988926375, 0.0001902332004678092, 0.00017290889992048442, 0.00015940200544663975, 0.0001444134139439425, 0.00013102788419633543, 0.0001183677705358291, 0.00010803784772920382, 9.823008925114883e-05, 9.048456843918234e-05, 8.276740899690771e-05, 7.498876299641051e-05, 6.82902293679262e-05, 6.175901748550659e-05, 5.603773429546234e-05, 5.131663433998758e-05, 4.631696147501563e-05, 4.171896646952961e-05, 3.7958329171314146e-05, 3.449561632666013e-05, 3.111527700859821e-05, 2.8212157779904878e-05, 2.5397547814005314e-05, 2.324911016972575e-05, 2.0931227522593932e-05, 1.8843797749905552e-05, 1.696117833361183e-05, 1.5493566366975277e-05, 1.3944361538936322e-05, 1.267603700460069e-05, 1.1408532905801511e-05, 1.0286586502717e-05, 9.378036659101567e-06, 8.44817097794206e-06, 7.60340209434632e-06, 6.843984331980992e-06, 6.331982767095121e-06, 5.7035255474361425e-06], "accuracy_test": 0.6478495695153061, "start": "2016-01-29 15:53:11.152000", "learning_rate_per_epoch": [0.008762844838202, 0.008762844838202, 0.008762844838202, 0.008762844838202, 0.008762844838202, 0.008762844838202, 0.008762844838202, 0.008762844838202, 0.008762844838202, 0.008762844838202, 0.008762844838202, 0.008762844838202, 0.008762844838202, 0.008762844838202, 0.008762844838202, 0.008762844838202, 0.008762844838202, 0.008762844838202, 0.008762844838202, 0.008762844838202, 0.008762844838202, 0.008762844838202, 0.008762844838202, 0.008762844838202, 0.008762844838202, 0.008762844838202, 0.008762844838202, 0.008762844838202, 0.008762844838202, 0.008762844838202, 0.008762844838202, 0.008762844838202, 0.008762844838202, 0.008762844838202, 0.008762844838202, 0.008762844838202, 0.008762844838202, 0.008762844838202, 0.008762844838202, 0.008762844838202, 0.008762844838202, 0.008762844838202, 0.008762844838202, 0.008762844838202, 0.008762844838202, 0.008762844838202, 0.008762844838202, 0.008762844838202, 0.008762844838202, 0.008762844838202, 0.008762844838202, 0.008762844838202, 0.008762844838202, 0.008762844838202, 0.008762844838202, 0.008762844838202, 0.008762844838202, 0.008762844838202, 0.008762844838202, 0.008762844838202, 0.008762844838202, 0.008762844838202, 0.008762844838202, 0.008762844838202, 0.008762844838202, 0.008762844838202, 0.008762844838202, 0.008762844838202, 0.008762844838202, 0.008762844838202, 0.008762844838202, 0.008762844838202, 0.008762844838202, 0.008762844838202, 0.008762844838202, 0.008762844838202, 0.008762844838202, 0.008762844838202, 0.008762844838202, 0.008762844838202, 0.008762844838202, 0.008762844838202, 0.008762844838202, 0.008762844838202, 0.008762844838202, 0.008762844838202, 0.008762844838202, 0.0008762844954617321, 0.0008762844954617321, 0.0008762844954617321, 0.0008762844954617321, 0.0008762844954617321, 0.0008762844954617321, 0.0008762844954617321, 0.0008762844954617321, 0.0008762844954617321, 0.0008762844954617321, 0.0008762844954617321, 0.0008762844954617321, 0.0008762844954617321, 0.0008762844954617321, 0.0008762844954617321, 0.0008762844954617321, 0.0008762844954617321, 0.0008762844954617321, 0.0008762844954617321, 0.0008762844954617321, 0.0008762844954617321, 0.0008762844954617321, 0.0008762844954617321, 0.0008762844954617321, 0.0008762844954617321, 8.762844663579017e-05, 8.762845027376898e-06, 8.762845027376898e-07, 8.762845027376898e-08, 8.76284467210553e-09, 8.762844894150135e-10, 8.762844616594379e-11, 8.762844616594379e-12, 8.762844508174161e-13, 8.762844508174161e-14, 8.76284467758075e-15, 8.762844889338988e-16, 8.762845021687886e-17, 8.76284535256013e-18, 8.76284535256013e-19, 8.762845094066189e-20, 8.762845255624903e-21, 8.762845659521686e-22, 8.762845533303941e-23, 8.762845217759579e-24, 8.762845414974805e-25, 8.762845661493838e-26, 8.762845661493838e-27, 8.762845468900844e-28, 8.762845228159601e-29, 8.762845077696324e-30, 8.762844889617228e-31, 8.762844889617228e-32, 8.76284459574364e-33, 8.762844779414632e-34], "accuracy_train_first": 0.49245390077057954, "accuracy_train_last": 0.9628960323228128, "batch_size_eval": 1024, "accuracy_train_std": [0.01698724101877908, 0.017432362203252203, 0.01445730449203529, 0.013846721869231693, 0.014387716662576268, 0.015144602325147605, 0.014915713012983866, 0.018677913412447764, 0.015838992365042815, 0.013626200949989927, 0.014903206532545637, 0.01465233889803665, 0.011650460137119277, 0.01582967056689212, 0.013806417331489795, 0.014339214155763298, 0.014485839000909776, 0.014351278102186144, 0.013665337224067379, 0.013668047602850094, 0.014296953137811874, 0.014892697246411816, 0.014933539580741406, 0.015116794381146858, 0.01447387252471633, 0.014716379468820618, 0.016561180376416646, 0.016979162919347836, 0.016196833711264794, 0.016467628313918424, 0.014923631933309348, 0.014554546674310942, 0.014867398987629343, 0.015071240225174929, 0.015604940212778957, 0.01634561208663866, 0.017641010759705207, 0.01536081897672049, 0.015386955564925626, 0.015927025130109764, 0.0158663672796539, 0.015210786993250801, 0.01642548111340212, 0.016911214753702754, 0.016008422109881636, 0.016174666490414477, 0.015852786022820767, 0.016431799520985228, 0.015832884827355332, 0.016008480949982033, 0.015772314697576572, 0.018110983187548568, 0.016214580858085387, 0.01557054440883848, 0.01706886441291404, 0.017036665293063576, 0.01664368179391825, 0.016335276850711438, 0.016534981292001635, 0.01690163525017867, 0.015925925460868375, 0.016005754582553065, 0.015689908778808357, 0.01634467318362043, 0.014586938495344957, 0.01509437935554341, 0.016270274929926495, 0.016252462537237656, 0.015841535327972646, 0.016998821621718143, 0.015644722643429806, 0.01677768721753849, 0.01873448717413588, 0.016111589821111456, 0.016165284747703075, 0.016259511651939954, 0.017218082964793824, 0.01814078343899208, 0.018525908789020332, 0.016780835033207646, 0.017686971180538608, 0.016679556202812293, 0.01798932753284687, 0.017039058013125403, 0.016808308335945727, 0.016029966087266284, 0.01643238635271033, 0.01669020305339831, 0.010101401123399422, 0.010104028158769552, 0.008820117245165874, 0.008472974826680113, 0.008226343956089174, 0.008602092912578714, 0.0088501203362363, 0.008057670682163828, 0.008445961854866606, 0.008004823043483413, 0.007639676674800567, 0.007992558085821909, 0.008779885013267225, 0.007687032396101435, 0.007121057907127942, 0.008135857920180907, 0.008018054482500276, 0.007526368612848808, 0.008066637013424239, 0.0074893262852048165, 0.008818505226048773, 0.0075580149320802, 0.0079250177300019, 0.008116427278607074, 0.007695444056056344, 0.007377764784397889, 0.006709107633258635, 0.00684178448963861, 0.007122792369084388, 0.007015843953249238, 0.0070420942563681616, 0.006959929201730674, 0.006517188845276159, 0.006779626063320886, 0.006969788812740546, 0.006936279512779672, 0.007032424519656322, 0.007393592724337474, 0.00692767602008712, 0.007165781543868293, 0.006906554304548081, 0.006736281739972753, 0.006922666969262166, 0.007267058610378673, 0.007183919228900193, 0.006908501879445482, 0.00656419197800347, 0.006824998063734657, 0.007074419383946711, 0.007112126861251381, 0.006798410381389006, 0.006837421914561192, 0.006950579324814948, 0.007019119254767061], "accuracy_test_std": 0.010076410188893992, "error_valid": [0.5049637024661144, 0.40696771460843373, 0.3668257012424698, 0.3292162791792168, 0.29123035109186746, 0.2660735716302711, 0.2509971526731928, 0.25877876741340367, 0.23394701854292166, 0.22502559064382532, 0.2317600480045181, 0.2112713549510542, 0.2122170321912651, 0.21436429310993976, 0.21332596009036142, 0.20898290427334332, 0.20125129423945776, 0.19343879423945776, 0.18407908979668675, 0.18437470585466864, 0.19060029179216864, 0.18862657661897586, 0.1935196842055723, 0.18017283979668675, 0.17654161568147586, 0.1776505435805723, 0.1819230045180723, 0.18240099068147586, 0.18220832548945776, 0.18191270943147586, 0.17307217149849397, 0.1861248705760542, 0.1802140201430723, 0.17532091255647586, 0.1744252400225903, 0.17605333443147586, 0.1780373446912651, 0.17153673286897586, 0.1657067724021084, 0.16987775320030118, 0.1734486775225903, 0.16767019248870485, 0.16975568288780118, 0.16743634695030118, 0.16828054405120485, 0.1696644978350903, 0.1677319630082832, 0.16409044380647586, 0.16833201948418675, 0.16257412462349397, 0.1647816853350903, 0.16882030073418675, 0.1680775837725903, 0.16141372129141573, 0.16320359563253017, 0.16938947195030118, 0.16291974538780118, 0.16449636436370485, 0.1626241293298193, 0.16792462820030118, 0.17220738422439763, 0.15787368222891573, 0.1579648672816265, 0.1638757177146084, 0.15937970632530118, 0.15353062641189763, 0.15293939429593373, 0.16364187217620485, 0.15853550922439763, 0.17292951101280118, 0.16194318288780118, 0.1606710043298193, 0.17135289203689763, 0.15687652955572284, 0.15124070500753017, 0.15449689382530118, 0.15657208913780118, 0.1625844197100903, 0.16892178087349397, 0.1571103750941265, 0.1670495458396084, 0.16081366481551207, 0.16393748823418675, 0.1549336996423193, 0.16203436794051207, 0.1597253270896084, 0.16096662038780118, 0.15408950254141573, 0.12420139542545183, 0.12247182087725905, 0.12381459431475905, 0.12125111775225905, 0.12297039721385539, 0.12208501976656627, 0.12281744164156627, 0.12222768025225905, 0.12284832690135539, 0.12210560993975905, 0.12479115681475905, 0.12517795792545183, 0.12296010212725905, 0.12220709007906627, 0.1255338737763554, 0.12394695971385539, 0.12393666462725905, 0.12422198559864461, 0.12479115681475905, 0.12221738516566272, 0.12480145190135539, 0.12430287556475905, 0.1251676628388554, 0.12307187735316272, 0.12342779320406627, 0.12161732868975905, 0.12209531485316272, 0.12320424275225905, 0.12233945547816272, 0.12320424275225905, 0.12259389118975905, 0.12148496329066272, 0.12309246752635539, 0.12357045368975905, 0.12198353962725905, 0.12198353962725905, 0.12283803181475905, 0.12210560993975905, 0.12320424275225905, 0.12136289297816272, 0.12296010212725905, 0.12296010212725905, 0.12332631306475905, 0.12160703360316272, 0.12308217243975905, 0.12186146931475905, 0.12296010212725905, 0.12247182087725905, 0.12172910391566272, 0.12247182087725905, 0.12271596150225905, 0.12283803181475905, 0.12136289297816272, 0.12283803181475905], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.0018205612432265928, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 16, "valid_ratio": 0.15, "learning_rate": 0.008762845301224464, "optimization": "nesterov_momentum", "nb_data_augmentation": 2, "learning_rate_decay_method": "discrete", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 1.334562920421469e-05, "rotation_range": [0, 0], "momentum": 0.9629067553066193}, "accuracy_valid_max": 0.878748882247741, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.877161968185241, "accuracy_valid_std": [0.014179206591588637, 0.013065656169794874, 0.016006588686238044, 0.010886033385254528, 0.015740608819645698, 0.012287636573080518, 0.008427001933657925, 0.014994410109351783, 0.012137220453482195, 0.0091152556908203, 0.014611661857820356, 0.013306519615229889, 0.012133944862048029, 0.01420372312811816, 0.01489355034728878, 0.012769649982741145, 0.013176223946757657, 0.011424007501693433, 0.012947805387108949, 0.01207051871063688, 0.012229005037948227, 0.013496132440212966, 0.013320221771060852, 0.013363754669893847, 0.008172248665250575, 0.01245144591773204, 0.012431727326288345, 0.013360947344491883, 0.012731618135971677, 0.011100859987597078, 0.011399116079228371, 0.014647075907086736, 0.011840380693893255, 0.010002682518722737, 0.012206073999768518, 0.010733681040436156, 0.009498367221103574, 0.008983313514096819, 0.014459401535172486, 0.015359553536641334, 0.009415014039188613, 0.007626484382752396, 0.011636012232233782, 0.010637353839731774, 0.008699364957369882, 0.012678266303063401, 0.0161099500910311, 0.015233185443172734, 0.015007845307689652, 0.012335118354151945, 0.011395313802710139, 0.01353356619967988, 0.014073153060409953, 0.010689955938267767, 0.01508336876609224, 0.009534899110959848, 0.013294932224735566, 0.012263109625545665, 0.014724005381791857, 0.01476015381465453, 0.014590594914010706, 0.01561419395324237, 0.013355080125081997, 0.014198952329440769, 0.016350988395662618, 0.01432270603513938, 0.013937037805812059, 0.013374834145167176, 0.016794510204653793, 0.01280814825021164, 0.014810333322699227, 0.009943970449262516, 0.014856324070649202, 0.01529863611155879, 0.01592571794872381, 0.009475174947854475, 0.009076198099898913, 0.011152174030179086, 0.01211329188390282, 0.013290025699116774, 0.009383865717408764, 0.012296734845954328, 0.010853662896795596, 0.01325533022131449, 0.008696856159901342, 0.011522420076090998, 0.010376892729516248, 0.00728445894782386, 0.010090000270113574, 0.008321147224035838, 0.008650089052136684, 0.008086640832814344, 0.007742372252089981, 0.00994322214384976, 0.009878989078552109, 0.008910191682220034, 0.010039897504962342, 0.007636425228050905, 0.008163875832496009, 0.008687514388803831, 0.00845411127615017, 0.01026672711845366, 0.008358125873511868, 0.0066780604313224756, 0.008148390454339268, 0.007019430439444448, 0.009437171983383542, 0.008580307547820942, 0.009226279773693814, 0.010849189586884425, 0.009662349091262321, 0.007503899197126144, 0.007895266036331167, 0.007591719481271836, 0.007894162422904043, 0.0073356518141203045, 0.00828753818512481, 0.008224358037311919, 0.007555688400939376, 0.008235110622691739, 0.008299927408089517, 0.008143574859054308, 0.007259880855257572, 0.00820057335542648, 0.008459025996797882, 0.008521743073439027, 0.007432516331136844, 0.00793788863568173, 0.007930254007399336, 0.007732366202757919, 0.008082188292897356, 0.008589895722101382, 0.007013283392657363, 0.007298344369493087, 0.007975223154193111, 0.008713048337919575, 0.00799938382140088, 0.007984852146705246, 0.007802538418842762, 0.006941908963862121, 0.007180945912316158, 0.008186889996303402], "accuracy_valid": [0.49503629753388556, 0.5930322853915663, 0.6331742987575302, 0.6707837208207832, 0.7087696489081325, 0.7339264283697289, 0.7490028473268072, 0.7412212325865963, 0.7660529814570783, 0.7749744093561747, 0.7682399519954819, 0.7887286450489458, 0.7877829678087349, 0.7856357068900602, 0.7866740399096386, 0.7910170957266567, 0.7987487057605422, 0.8065612057605422, 0.8159209102033133, 0.8156252941453314, 0.8093997082078314, 0.8113734233810241, 0.8064803157944277, 0.8198271602033133, 0.8234583843185241, 0.8223494564194277, 0.8180769954819277, 0.8175990093185241, 0.8177916745105422, 0.8180872905685241, 0.826927828501506, 0.8138751294239458, 0.8197859798569277, 0.8246790874435241, 0.8255747599774097, 0.8239466655685241, 0.8219626553087349, 0.8284632671310241, 0.8342932275978916, 0.8301222467996988, 0.8265513224774097, 0.8323298075112951, 0.8302443171121988, 0.8325636530496988, 0.8317194559487951, 0.8303355021649097, 0.8322680369917168, 0.8359095561935241, 0.8316679805158133, 0.837425875376506, 0.8352183146649097, 0.8311796992658133, 0.8319224162274097, 0.8385862787085843, 0.8367964043674698, 0.8306105280496988, 0.8370802546121988, 0.8355036356362951, 0.8373758706701807, 0.8320753717996988, 0.8277926157756024, 0.8421263177710843, 0.8420351327183735, 0.8361242822853916, 0.8406202936746988, 0.8464693735881024, 0.8470606057040663, 0.8363581278237951, 0.8414644907756024, 0.8270704889871988, 0.8380568171121988, 0.8393289956701807, 0.8286471079631024, 0.8431234704442772, 0.8487592949924698, 0.8455031061746988, 0.8434279108621988, 0.8374155802899097, 0.831078219126506, 0.8428896249058735, 0.8329504541603916, 0.8391863351844879, 0.8360625117658133, 0.8450663003576807, 0.8379656320594879, 0.8402746729103916, 0.8390333796121988, 0.8459104974585843, 0.8757986045745482, 0.877528179122741, 0.876185405685241, 0.878748882247741, 0.8770296027861446, 0.8779149802334337, 0.8771825583584337, 0.877772319747741, 0.8771516730986446, 0.877894390060241, 0.875208843185241, 0.8748220420745482, 0.877039897872741, 0.8777929099209337, 0.8744661262236446, 0.8760530402861446, 0.876063335372741, 0.8757780144013554, 0.875208843185241, 0.8777826148343373, 0.8751985480986446, 0.875697124435241, 0.8748323371611446, 0.8769281226468373, 0.8765722067959337, 0.878382671310241, 0.8779046851468373, 0.876795757247741, 0.8776605445218373, 0.876795757247741, 0.877406108810241, 0.8785150367093373, 0.8769075324736446, 0.876429546310241, 0.878016460372741, 0.878016460372741, 0.877161968185241, 0.877894390060241, 0.876795757247741, 0.8786371070218373, 0.877039897872741, 0.877039897872741, 0.876673686935241, 0.8783929663968373, 0.876917827560241, 0.878138530685241, 0.877039897872741, 0.877528179122741, 0.8782708960843373, 0.877528179122741, 0.877284038497741, 0.877161968185241, 0.8786371070218373, 0.877161968185241], "seed": 111434799, "model": "residualv3", "loss_std": [0.3058839440345764, 0.2680394947528839, 0.27097415924072266, 0.2680339515209198, 0.2636592984199524, 0.26098623871803284, 0.2568906545639038, 0.25575074553489685, 0.25475186109542847, 0.25028735399246216, 0.24671807885169983, 0.24271105229854584, 0.24126240611076355, 0.239310160279274, 0.23865045607089996, 0.23757895827293396, 0.23508426547050476, 0.2339838445186615, 0.23166950047016144, 0.22961708903312683, 0.23047058284282684, 0.2285512238740921, 0.2266111969947815, 0.2258654236793518, 0.2254648059606552, 0.22891999781131744, 0.2234695553779602, 0.21931128203868866, 0.22008301317691803, 0.21874944865703583, 0.2161867618560791, 0.21753481030464172, 0.21854470670223236, 0.21568460762500763, 0.21494881808757782, 0.21020300686359406, 0.2138870358467102, 0.2127210795879364, 0.20938792824745178, 0.20881430804729462, 0.20495180785655975, 0.205582395195961, 0.2043861448764801, 0.2057380974292755, 0.2054455280303955, 0.20353369414806366, 0.2033485621213913, 0.20469921827316284, 0.20050138235092163, 0.2015259712934494, 0.1975255012512207, 0.19812799990177155, 0.19958700239658356, 0.1982632428407669, 0.19818678498268127, 0.19839973747730255, 0.19749757647514343, 0.19539621472358704, 0.19505153596401215, 0.19403164088726044, 0.19646063446998596, 0.19712235033512115, 0.19326533377170563, 0.1931913048028946, 0.1912572830915451, 0.19349397718906403, 0.19057251513004303, 0.1914375126361847, 0.19309130311012268, 0.19030213356018066, 0.19144654273986816, 0.19079560041427612, 0.18840593099594116, 0.18905507028102875, 0.19309502840042114, 0.18867814540863037, 0.18841557204723358, 0.18938368558883667, 0.18669728934764862, 0.18822835385799408, 0.18887874484062195, 0.18627330660820007, 0.1873590350151062, 0.18778935074806213, 0.18437010049819946, 0.18796184659004211, 0.1849997192621231, 0.18322013318538666, 0.1705133020877838, 0.15346190333366394, 0.15390615165233612, 0.15227261185646057, 0.14962077140808105, 0.14834681153297424, 0.1462043821811676, 0.14621031284332275, 0.14538052678108215, 0.145170658826828, 0.1438044160604477, 0.14428435266017914, 0.14518533647060394, 0.14299283921718597, 0.14044436812400818, 0.14365696907043457, 0.1405545324087143, 0.13938984274864197, 0.13896313309669495, 0.13842430710792542, 0.135973259806633, 0.139316126704216, 0.13820666074752808, 0.13753096759319305, 0.13706763088703156, 0.1308855265378952, 0.12953658401966095, 0.1293153464794159, 0.1298092007637024, 0.13020475208759308, 0.1263446807861328, 0.13048036396503448, 0.1285199075937271, 0.12970703840255737, 0.12936456501483917, 0.12825781106948853, 0.13053713738918304, 0.1299353688955307, 0.12998902797698975, 0.1310671716928482, 0.12925592064857483, 0.1285720318555832, 0.1283053457736969, 0.1301986575126648, 0.12968139350414276, 0.12660114467144012, 0.12937307357788086, 0.1327165961265564, 0.12834009528160095, 0.1291300356388092, 0.1297299563884735, 0.13062846660614014, 0.12834779918193817, 0.12965436279773712]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:23 2016", "state": "available"}], "summary": "92114505ceff0477ded4365c0786d2d4"}