{"content": {"hp_model": {"f0": 64, "f1": 64, "f2": 16, "f3": 32, "nonlin": "very_leaky_rectify", "nbg1": 6, "nbg3": 4, "nbg2": 4, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.01581903029017568, 0.023142826739450485, 0.023527086486398518, 0.02355658896212279, 0.015274647233757944, 0.02102052264262545, 0.023062891163390425, 0.018855477873292906, 0.017420816287786557, 0.011480473817454288, 0.01421286065993214, 0.015164604004011735, 0.01161403926826894, 0.015630435026026127, 0.01816925491243578, 0.018449344584620806, 0.019214963562554355, 0.01657589347186713, 0.016756280734288517, 0.011420154891831428, 0.01606255853508662, 0.015058305627495005, 0.0146801972716326, 0.013113573886264445, 0.012787754392143769, 0.01204279267271444, 0.013875825185454577, 0.013759206831844076, 0.020305465387749196, 0.013028766186682582, 0.011790302941213478, 0.016701584280944353, 0.013057388486064858, 0.012748430707027428, 0.017596055901923115, 0.015608540899089583, 0.012284768298905955, 0.011095941585367697, 0.015169861484932975, 0.00955021175919503, 0.016749305619176837, 0.01309867694976912, 0.012971414160298775, 0.012934618783570896, 0.009209523450470918, 0.013137655969606369, 0.007871056460490073, 0.016324441480273313, 0.011490849675813248, 0.015233314374130716, 0.010652442614139156, 0.012020487035341096, 0.007162521413545482, 0.011825180475914524, 0.01572817771710701, 0.01073933577634825, 0.017922946543462114, 0.01414778554543627, 0.014536641367859648, 0.012898513593182719, 0.010666566217528097, 0.01191893143168019, 0.010331485766072113, 0.011511136175861037, 0.012178293090246645, 0.0126627278741347, 0.012140886855204364, 0.011057577456555704, 0.010377960759321802, 0.01117350446216469, 0.011043540546563712, 0.011498338432354647, 0.01134277250598152, 0.011536769881796574, 0.012085795609146023, 0.011745647015665002, 0.011636972880353258, 0.011471451835965898, 0.0115789433550952, 0.011506650214844994, 0.011173705790057439, 0.010779171386887125, 0.011207820972200657, 0.011229896700573403, 0.011079450708211222, 0.011543965405292771, 0.011403523894975882, 0.01141396449333624, 0.010754684528377134, 0.010819858336365291, 0.010518451149332825, 0.010273266358079203, 0.010079904727739394, 0.010123846372290706, 0.010550111873900285, 0.010448447004015232, 0.01039859287045414, 0.01020604010547588, 0.010068222920966005, 0.010127250647721377, 0.010348485816036215, 0.010458472827634305, 0.010684007930940351, 0.010942786594606187, 0.01103336079501066, 0.010861064530423403, 0.010592446509535886, 0.010749207704378231, 0.010698204401421102, 0.010815543340326167, 0.010370614581638628, 0.010370614581638628, 0.010682619805386874, 0.010526229669275865, 0.010525231864382943, 0.010979784494341685], "moving_avg_accuracy_train": [0.029561401232004424, 0.06360272255098744, 0.09857588168200904, 0.13403877832716177, 0.1701535586420554, 0.20682716852282032, 0.24320677575230276, 0.27871015831227, 0.31319925503987467, 0.3462339511386465, 0.37743910567986677, 0.40742292270970215, 0.4354846135448135, 0.4622903967224473, 0.4876011751334436, 0.5114851050949516, 0.534012611594731, 0.5545008845980025, 0.5728309762092616, 0.5903717260355024, 0.6046839681433308, 0.6180582780761831, 0.6327876072395411, 0.6472036759614157, 0.6606107236361306, 0.6735907779278794, 0.6858189483701412, 0.6970195782193581, 0.7074998904347978, 0.7160577352322187, 0.7233650249357411, 0.7320869409949079, 0.7428613421623513, 0.7516101832310461, 0.7584654005749312, 0.7658466789582834, 0.7748818668960191, 0.784043324620869, 0.7902500938972004, 0.7976355990827387, 0.8060611483937339, 0.8136976932938861, 0.8206519639123565, 0.8273501163987048, 0.8346245531542292, 0.8414948140163625, 0.8458833583506989, 0.8505047999646416, 0.8577168736088198, 0.8637868158564188, 0.8702566975090568, 0.876949016407099, 0.8819072213093938, 0.8863812675143256, 0.8918866676928026, 0.898255254378441, 0.9045516691168612, 0.9110810004921353, 0.9176594855239018, 0.9245125067251106, 0.9311150646823985, 0.9372735696344243, 0.9429091939948099, 0.9480277588953473, 0.9526414427522596, 0.9568239851580045, 0.9606045493648415, 0.964016357746233, 0.9670916355871043, 0.9698756616855552, 0.9723929109182086, 0.9746677358228347, 0.9767243788322363, 0.9785916335823643, 0.9802791383039082, 0.9818025428509168, 0.9831805823896531, 0.9844231431233252, 0.9855414477836302, 0.9865525722755237, 0.9874695597646566, 0.988294848504876, 0.9890422586686926, 0.9897149278161275, 0.9903226551976284, 0.9908835607338364, 0.9913907008652332, 0.9918448018346806, 0.9922511675583738, 0.9926215470073169, 0.9929595388089846, 0.993261406281676, 0.9935330870070983, 0.9937822499575975, 0.9940064966130466, 0.9942083186029509, 0.9943922835426743, 0.9945601771372348, 0.9947136065211488, 0.9948516929666715, 0.9949759707676419, 0.9950878207885152, 0.9951931361049202, 0.9952879198896847, 0.9953685749983537, 0.995441164596156, 0.9955064952341779, 0.9955699431060167, 0.9956247210418621, 0.9956694069353226, 0.9957096242394371, 0.9957504701107591, 0.9957895565437586, 0.9958270594822676, 0.9958608121269257, 0.995891189507118], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 931067244, "moving_var_accuracy_train": [0.007864887985195976, 0.017507703200956605, 0.02676502961729478, 0.035407080001748366, 0.043604868216310794, 0.05134896434985857, 0.05812535031441543, 0.06365722684176833, 0.06799698429536614, 0.07101890618287345, 0.07268087059405906, 0.07350404708776098, 0.07324076881171324, 0.07238364203643877, 0.0709109973667299, 0.06895387662371288, 0.06662588590322006, 0.0637412212888073, 0.06039102948622096, 0.057121027677799724, 0.05325248737739765, 0.04953708813527779, 0.04653595756017296, 0.04375276914069997, 0.040995232572798655, 0.038412045600269486, 0.0359165944115276, 0.033454021951547376, 0.03109715225359047, 0.028646567396422122, 0.02626247900208074, 0.024320877479561027, 0.022933579216257997, 0.021329101275039713, 0.019619137191022862, 0.01814757290707346, 0.017067527205997957, 0.016116165254196072, 0.01485126459242296, 0.013857049314791191, 0.013110253314040183, 0.01232407934474454, 0.011526928328784368, 0.010778022716479301, 0.010176477315822503, 0.009583633943064109, 0.008798604441127626, 0.008110963500334594, 0.0077679932065425755, 0.007322791675891004, 0.006967246825694172, 0.006673606333222598, 0.00622749986257861, 0.005784903681315531, 0.005479198193310549, 0.005296308441331927, 0.005123481144222491, 0.0049948225436734816, 0.004884828477124722, 0.004819020725670223, 0.004729462597317296, 0.0045978609867827185, 0.004423917245486787, 0.004217322880707236, 0.003987165301220311, 0.0037458917198809677, 0.0034999365393910283, 0.003254706813333922, 0.003014352136187519, 0.0027826741344204684, 0.0025614356142720698, 0.0023518655079652305, 0.0021547469813817935, 0.0019706520459605004, 0.001799215891031548, 0.0016401811546530114, 0.0014932539759205954, 0.0013578241929203116, 0.001233297221447619, 0.0011191688539458197, 0.0010148197630482714, 0.0009194677002860416, 0.0008325485278342244, 0.0007533660290879995, 0.0006813534193112336, 0.0006160496125650493, 0.0005567593713244021, 0.0005029393034060406, 0.00045413157097797126, 0.0004099530423059679, 0.0003699858841973225, 0.0003338074115172122, 0.00030109096411458483, 0.0002715406072862391, 0.00024483912561993655, 0.0002207218020984232, 0.00019895420978000768, 0.00017931248313385708, 0.0001615931000031055, 0.0001456054008007286, 0.00013118386546698198, 0.00011817807276480805, 0.00010646008733115268, 9.589493429072602e-05, 8.636398808064314e-05, 7.777501251996051e-05, 7.003592409834361e-05, 6.306856238047707e-05, 5.6788711742728664e-05, 5.112781203012509e-05, 4.60295877110647e-05, 4.144164440679477e-05, 3.731122970931683e-05, 3.359276497195641e-05, 3.0243741643953497e-05, 2.7227672546604252e-05], "duration": 56205.308755, "accuracy_train": [0.2956140123200443, 0.3699746144218346, 0.41333431386120345, 0.4532048481335364, 0.4951865814760982, 0.5368896574497047, 0.5706232408176449, 0.5982406013519749, 0.6236011255883167, 0.6435462160275932, 0.658285496550849, 0.6772772759782207, 0.6880398310608158, 0.7035424453211517, 0.7153981808324105, 0.7264404747485235, 0.7367601700927464, 0.7388953416274455, 0.7378018007105943, 0.7482384744716685, 0.7334941471137874, 0.738427067471853, 0.7653515697097637, 0.7769482944582872, 0.781274152708564, 0.7904112665536176, 0.7958724823504982, 0.7978252468623109, 0.8018227003737541, 0.793078338409007, 0.7891306322674418, 0.8105841855274087, 0.8398309526693429, 0.8303497528492986, 0.8201623566698967, 0.8322781844084534, 0.8561985583356404, 0.8664964441445183, 0.8461110173841824, 0.8641051457525839, 0.8818910921926911, 0.8824265973952565, 0.8832403994785898, 0.8876334887758398, 0.9000944839539498, 0.9033271617755629, 0.8853802573597268, 0.8920977744901256, 0.9226255364064231, 0.9184162960848099, 0.9284856323827981, 0.9371798864894795, 0.926531065430048, 0.9266476833587117, 0.9414352692990956, 0.9555725345491879, 0.9612194017626431, 0.9698449828696014, 0.9768658508098007, 0.9861896975359912, 0.9905380862979882, 0.9927001142026578, 0.9936298132382798, 0.9940948430001846, 0.9941645974644703, 0.9944668668097084, 0.994629627226375, 0.994722633178756, 0.9947691361549464, 0.9949318965716132, 0.9950481540120893, 0.9951411599644703, 0.9952341659168512, 0.995396926333518, 0.9954666807978036, 0.9955131837739941, 0.9955829382382798, 0.995606189726375, 0.995606189726375, 0.9956526927025655, 0.9957224471668512, 0.9957224471668512, 0.9957689501430418, 0.9957689501430418, 0.995792201631137, 0.9959317105597084, 0.9959549620478036, 0.9959317105597084, 0.9959084590716132, 0.9959549620478036, 0.9960014650239941, 0.9959782135358989, 0.9959782135358989, 0.9960247165120893, 0.9960247165120893, 0.9960247165120893, 0.9960479680001846, 0.9960712194882798, 0.996094470976375, 0.996094470976375, 0.996094470976375, 0.996094470976375, 0.9961409739525655, 0.9961409739525655, 0.996094470976375, 0.996094470976375, 0.996094470976375, 0.9961409739525655, 0.9961177224644703, 0.9960715799764673, 0.9960715799764673, 0.9961180829526578, 0.996141334440753, 0.9961645859288483, 0.9961645859288483, 0.9961645859288483], "end": "2016-01-25 16:57:51.325000", "learning_rate_per_epoch": [0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897, 0.00016510944988112897], "accuracy_valid": [0.2866725691829819, 0.3522052075489458, 0.3974344644201807, 0.43575571818524095, 0.48499417592243976, 0.5202327866152108, 0.5472412109375, 0.5668548216302711, 0.580578172063253, 0.5943735881024097, 0.5992461055158133, 0.6132135965737951, 0.6130106362951807, 0.6169271813817772, 0.6195009530308735, 0.6205084007906627, 0.6175581231174698, 0.6123296898531627, 0.6069071206701807, 0.6016478021460843, 0.5877406108810241, 0.5893995905496988, 0.6038553628576807, 0.5992475762424698, 0.6030214608433735, 0.5989210749246988, 0.5978327371987951, 0.6019625376506024, 0.6042303981551205, 0.598290133189006, 0.596947359751506, 0.5994490657944277, 0.6100000588290663, 0.5988004753388554, 0.5954133918486446, 0.590855609939759, 0.6002638483621988, 0.6148622811558735, 0.6038259483245482, 0.6033567865210843, 0.613194477127259, 0.6181684746799698, 0.6091352715549698, 0.6109045557228916, 0.6096441429781627, 0.6144563605986446, 0.6100103539156627, 0.6107824854103916, 0.6204981057040663, 0.6124002847326807, 0.6209555016942772, 0.6193685876317772, 0.6269281226468373, 0.6208437264683735, 0.6216482139495482, 0.6262868858245482, 0.6289621376129518, 0.6268457619540663, 0.6360025061182228, 0.6385042121611446, 0.6390439688441265, 0.6393895896084337, 0.6392572242093373, 0.638636577560241, 0.639613140060241, 0.639735210372741, 0.638758647872741, 0.6396234351468373, 0.6391351538968373, 0.6390130835843373, 0.6393792945218373, 0.6393792945218373, 0.6386468726468373, 0.6395013648343373, 0.6390130835843373, 0.6390130835843373, 0.6390233786709337, 0.6381688864834337, 0.6378026755459337, 0.6378026755459337, 0.6379247458584337, 0.6378026755459337, 0.6380468161709337, 0.6381688864834337, 0.6380468161709337, 0.6381688864834337, 0.6385350974209337, 0.6376703101468373, 0.6374261695218373, 0.6369378882718373, 0.636195171310241, 0.636805522872741, 0.637415874435241, 0.637537944747741, 0.6372835090361446, 0.6378938605986446, 0.6377717902861446, 0.6378938605986446, 0.638514507247741, 0.638514507247741, 0.638514507247741, 0.638636577560241, 0.638636577560241, 0.638758647872741, 0.638880718185241, 0.638636577560241, 0.638270366622741, 0.6386468726468373, 0.6385248023343373, 0.6386468726468373, 0.6387689429593373, 0.6387689429593373, 0.6390130835843373, 0.6391351538968373, 0.6390130835843373, 0.6390130835843373], "accuracy_test": 0.5697943239795918, "start": "2016-01-25 01:21:06.016000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0], "accuracy_train_last": 0.9961645859288483, "batch_size_eval": 1024, "accuracy_train_std": [0.013407234851453174, 0.014014140711549598, 0.015326500857889641, 0.016862097169116495, 0.01663884004520408, 0.013089035810101863, 0.015201786846476786, 0.014194093495448035, 0.014101110069782442, 0.016078879166493965, 0.015511997605204244, 0.016099803644237177, 0.016942619555201387, 0.02011954887588801, 0.02238352932067034, 0.023372801997404123, 0.025504847818440105, 0.02735829875434017, 0.027852278164928183, 0.032845596753482406, 0.03396571612295153, 0.03547135325893549, 0.037051107331727944, 0.0384368335636358, 0.037403148156293496, 0.03929063781558844, 0.038723492402904416, 0.03589564966191131, 0.03981104792374207, 0.038335266412454, 0.03755316993032042, 0.03848909011079174, 0.03881774160238929, 0.037383570695596, 0.03997152591834966, 0.03733380979020069, 0.03359460925567368, 0.03452566579790795, 0.035296599723078116, 0.035400903441669664, 0.03312175795277713, 0.03149587117098434, 0.02981579453972812, 0.03262618824523668, 0.02939632029504164, 0.027927940613136467, 0.03045417733939401, 0.0284155547715699, 0.023458720921652743, 0.024701635416792912, 0.02139770765838504, 0.020923974153934703, 0.022508971517380884, 0.022039543392869194, 0.017616136202682466, 0.013896425575005433, 0.013993990746609647, 0.011908959367865592, 0.009665859360948296, 0.006339427838525646, 0.004254575302978777, 0.00354871765741903, 0.0035679189197827836, 0.0035146970288979906, 0.0034726692341683803, 0.003229897052633286, 0.0030459401834533727, 0.00307418601170341, 0.003145452363706335, 0.0030233319241038774, 0.002948434477954679, 0.0029259558482885857, 0.0028289841998402525, 0.0027973289190336243, 0.002752598508490311, 0.002721376834816797, 0.0026208670354833695, 0.002586077893469332, 0.002586077893469332, 0.0026031510548897895, 0.0025746266483128535, 0.0025922053359204982, 0.002536534626532189, 0.002527566982081935, 0.002516950294145262, 0.0024008187237164946, 0.0023205417892922663, 0.0023043003097008817, 0.0023465043685112095, 0.0023785273956731766, 0.002293342666648875, 0.0022773820133878656, 0.0022773820133878656, 0.002259253363749721, 0.002259253363749721, 0.002259253363749721, 0.002244721764168429, 0.0022298530382182436, 0.002245188470982245, 0.0022552792139594606, 0.0022552792139594606, 0.0022552792139594606, 0.002274836450473011, 0.002274836450473011, 0.0023730236166439106, 0.0023730236166439106, 0.0023730236166439106, 0.0023045867448327224, 0.0022801848237339853, 0.0022991204717858122, 0.0023382915118797024, 0.002299098597525345, 0.0023135042184433754, 0.002288234179935752, 0.002288234179935752, 0.002288234179935752], "accuracy_test_std": 0.01586662666833315, "error_valid": [0.7133274308170181, 0.6477947924510542, 0.6025655355798193, 0.564244281814759, 0.5150058240775602, 0.4797672133847892, 0.4527587890625, 0.4331451783697289, 0.419421827936747, 0.4056264118975903, 0.40075389448418675, 0.38678640342620485, 0.3869893637048193, 0.38307281861822284, 0.3804990469691265, 0.3794915992093373, 0.3824418768825302, 0.3876703101468373, 0.3930928793298193, 0.39835219785391573, 0.41225938911897586, 0.4106004094503012, 0.3961446371423193, 0.4007524237575302, 0.3969785391566265, 0.4010789250753012, 0.40216726280120485, 0.39803746234939763, 0.3957696018448795, 0.40170986681099397, 0.40305264024849397, 0.4005509342055723, 0.38999994117093373, 0.4011995246611446, 0.4045866081513554, 0.40914439006024095, 0.3997361516378012, 0.3851377188441265, 0.39617405167545183, 0.39664321347891573, 0.38680552287274095, 0.3818315253200302, 0.3908647284450302, 0.3890954442771084, 0.3903558570218373, 0.3855436394013554, 0.3899896460843373, 0.3892175145896084, 0.37950189429593373, 0.3875997152673193, 0.37904449830572284, 0.38063141236822284, 0.3730718773531627, 0.3791562735316265, 0.37835178605045183, 0.37371311417545183, 0.37103786238704817, 0.37315423804593373, 0.36399749388177716, 0.3614957878388554, 0.3609560311558735, 0.36061041039156627, 0.3607427757906627, 0.36136342243975905, 0.36038685993975905, 0.36026478962725905, 0.36124135212725905, 0.3603765648531627, 0.3608648461031627, 0.3609869164156627, 0.3606207054781627, 0.3606207054781627, 0.3613531273531627, 0.3604986351656627, 0.3609869164156627, 0.3609869164156627, 0.36097662132906627, 0.36183111351656627, 0.36219732445406627, 0.36219732445406627, 0.36207525414156627, 0.36219732445406627, 0.36195318382906627, 0.36183111351656627, 0.36195318382906627, 0.36183111351656627, 0.36146490257906627, 0.3623296898531627, 0.3625738304781627, 0.3630621117281627, 0.36380482868975905, 0.36319447712725905, 0.36258412556475905, 0.36246205525225905, 0.3627164909638554, 0.3621061394013554, 0.3622282097138554, 0.3621061394013554, 0.36148549275225905, 0.36148549275225905, 0.36148549275225905, 0.36136342243975905, 0.36136342243975905, 0.36124135212725905, 0.36111928181475905, 0.36136342243975905, 0.36172963337725905, 0.3613531273531627, 0.3614751976656627, 0.3613531273531627, 0.3612310570406627, 0.3612310570406627, 0.3609869164156627, 0.3608648461031627, 0.3609869164156627, 0.3609869164156627], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "momentum": 0.9693638091495429, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.0001651094547875029, "patience_threshold": 1, "do_flip": true, "batch_size": 64, "optimization": "nesterov_momentum", "nb_data_augmentation": 0, "learning_rate_decay_method": "none", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 7.817187791662252e-06, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.07794022749422752}, "accuracy_valid_max": 0.639735210372741, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.6390130835843373, "loss_train": [2.410357713699341, 2.0361034870147705, 1.8698025941848755, 1.7673587799072266, 1.6711633205413818, 1.575285792350769, 1.488242268562317, 1.41282320022583, 1.3463224172592163, 1.2860379219055176, 1.229690432548523, 1.1748827695846558, 1.1215181350708008, 1.0705403089523315, 1.016810417175293, 0.9654526114463806, 0.9167370200157166, 0.8677029609680176, 0.8276684880256653, 0.7943715453147888, 0.781633734703064, 0.7799628376960754, 0.7548880577087402, 0.7180198431015015, 0.697190523147583, 0.6696054339408875, 0.6431883573532104, 0.623633623123169, 0.6013067960739136, 0.5751734972000122, 0.5568589568138123, 0.5348080992698669, 0.5065823197364807, 0.48994654417037964, 0.4757208526134491, 0.45927008986473083, 0.4464942514896393, 0.4340459704399109, 0.430663526058197, 0.41656526923179626, 0.40753448009490967, 0.38462188839912415, 0.3696795105934143, 0.3636164963245392, 0.3561639189720154, 0.34935182332992554, 0.34059756994247437, 0.3287988603115082, 0.3211216926574707, 0.30608686804771423, 0.30232134461402893, 0.2937428057193756, 0.2891545593738556, 0.28221336007118225, 0.2781359553337097, 0.2702810764312744, 0.2642713189125061, 0.2574099600315094, 0.25232306122779846, 0.24637611210346222, 0.24228745698928833, 0.23982736468315125, 0.23871754109859467, 0.23812325298786163, 0.23771238327026367, 0.23738054931163788, 0.2370942234992981, 0.23683880269527435, 0.23660507798194885, 0.23638848960399628, 0.23618485033512115, 0.23599150776863098, 0.2358061671257019, 0.23562787473201752, 0.23545525968074799, 0.23528768122196198, 0.2351243942975998, 0.23496504127979279, 0.23480887711048126, 0.23465560376644135, 0.23450492322444916, 0.23435643315315247, 0.2342100441455841, 0.23406560719013214, 0.2339227944612503, 0.23378172516822815, 0.23364202678203583, 0.23350366950035095, 0.23336659371852875, 0.2332305908203125, 0.23309573531150818, 0.23296178877353668, 0.23282872140407562, 0.232696533203125, 0.23256514966487885, 0.2324344962835312, 0.23230448365211487, 0.23217517137527466, 0.2320464700460434, 0.23191840946674347, 0.23179087042808533, 0.23166389763355255, 0.2315373718738556, 0.23141130805015564, 0.2312857061624527, 0.23116058111190796, 0.23103584349155426, 0.2309115082025528, 0.23078756034374237, 0.23066392540931702, 0.2305406779050827, 0.23041781783103943, 0.23029524087905884, 0.23017296195030212, 0.2300509810447693, 0.22992929816246033], "accuracy_train_first": 0.2956140123200443, "model": "residualv5", "loss_std": [0.14828573167324066, 0.102626733481884, 0.09424616396427155, 0.09478379786014557, 0.09757006913423538, 0.10210512578487396, 0.1078672856092453, 0.1113906055688858, 0.11334986984729767, 0.11480134725570679, 0.11535381525754929, 0.11384712159633636, 0.11331164091825485, 0.1121373251080513, 0.11128220707178116, 0.1085890531539917, 0.10530625283718109, 0.10371560603380203, 0.10161727666854858, 0.09756876528263092, 0.09762275218963623, 0.09998521953821182, 0.09906943887472153, 0.097654327750206, 0.09722738713026047, 0.09349647909402847, 0.08937589824199677, 0.08785144239664078, 0.08556648343801498, 0.08206847310066223, 0.08432651311159134, 0.07853835076093674, 0.07314866781234741, 0.07585995644330978, 0.06718506664037704, 0.06506694853305817, 0.06393031775951385, 0.06295926123857498, 0.06464893370866776, 0.06114710494875908, 0.060791973024606705, 0.05174839869141579, 0.05063470080494881, 0.04789433628320694, 0.0459979772567749, 0.04510873928666115, 0.04420344531536102, 0.03791714087128639, 0.03743871673941612, 0.03198344260454178, 0.029690461233258247, 0.029570447281003, 0.027223028242588043, 0.023927273228764534, 0.022521762177348137, 0.01929081603884697, 0.015569798648357391, 0.012184757739305496, 0.009370499290525913, 0.005670851096510887, 0.00295430445112288, 0.0014101183041930199, 0.0009082715841941535, 0.0007266977918334305, 0.0006304204580374062, 0.00056539784418419, 0.0005162747111171484, 0.0004772086685989052, 0.0004456000169739127, 0.0004194201319478452, 0.00039683724753558636, 0.0003772815107367933, 0.00036000018008053303, 0.00034503109054639935, 0.000331331801135093, 0.0003190083079971373, 0.00030778872314840555, 0.00029759996687062085, 0.00028824221226386726, 0.0002795456093735993, 0.0002714807342272252, 0.00026398978661745787, 0.00025698496028780937, 0.00025044544599950314, 0.00024429208133369684, 0.00023858924396336079, 0.0002332315780222416, 0.0002281443594256416, 0.00022330986394081265, 0.00021872056822758168, 0.00021436731913127005, 0.00021028688934165984, 0.00020633004896808416, 0.0002026169968303293, 0.00019906046509277076, 0.00019564783724490553, 0.00019233758212067187, 0.0001891622378025204, 0.00018616524175740778, 0.0001832669076975435, 0.00018047976482193917, 0.00017782683426048607, 0.00017526229203213006, 0.00017279612075071782, 0.00017041791579686105, 0.0001681059948168695, 0.00016586929268669337, 0.00016368473006878048, 0.00016159727238118649, 0.00015957525465637445, 0.000157632734044455, 0.00015576394798699766, 0.00015392008936032653, 0.00015215858002193272, 0.00015044199244584888, 0.0001487631379859522]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:13 2016", "state": "available"}], "summary": "febd7f72cf80e8dbe93bdfaff1edbfa1"}