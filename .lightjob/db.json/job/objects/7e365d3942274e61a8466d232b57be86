{"content": {"hp_model": {"f0": 16, "f1": 64, "f2": 32, "f3": 16, "nonlin": "leaky_rectify", "nbg1": 4, "nbg3": 7, "nbg2": 1, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.015278863007436734, 0.020101034538620898, 0.01959027643192992, 0.01292709398792396, 0.008605645707357732, 0.013202943051892898, 0.015568942542617358, 0.014023847182480197, 0.012714873156877797, 0.016539082299697347, 0.0166015625, 0.011769786393682492, 0.013632566734064832, 0.01596852315824678, 0.013497828640519795, 0.017492891852053872, 0.01990830313862915, 0.018129601621722488, 0.017538789193687464, 0.017638952470912298, 0.02015734619854734, 0.02000426669547813, 0.017780668932798598, 0.019471084292807014, 0.01538092322167396, 0.017492850057515212, 0.017185068608529728, 0.024917245989954713, 0.020299043053419945, 0.015712610846571662, 0.024380607480349925, 0.01618264513100113, 0.015900738037377148, 0.02213784893334407, 0.016599206668607876, 0.02431504570972204, 0.018818419186687085, 0.026715010579201462, 0.02712628148541286, 0.022985479416484766, 0.024971589993924227, 0.021657927686639506, 0.019405412012631726, 0.02616318537392383, 0.024489205537951832, 0.017430603553420652, 0.022759285449601627, 0.018990796608276496, 0.02137384614114998, 0.021917718982994744, 0.01846313763224574, 0.022660809519426024, 0.018693400970758133, 0.021433211361223493, 0.014988710977782092, 0.01696553717881804, 0.017434609660619545, 0.017360967728667253, 0.019073156964020323, 0.0191721607356498, 0.02045693950728986, 0.013094856512972583, 0.017336283777115446, 0.017380372297731816, 0.016813014124763756, 0.021217406189898685, 0.015113473866672635, 0.020917115041380347, 0.008485562107062644, 0.01748623017430145, 0.013261142810230709, 0.012309306213287961, 0.01569519136162018, 0.02062997439850433, 0.011902508599765072, 0.016168943249179633, 0.016835291762329473, 0.018344682870833143, 0.01462691356450124, 0.018102231365600303, 0.02018226865380161, 0.023077348510013765, 0.017114117549045252, 0.01317676402907415, 0.014270929940169902, 0.021167611146295208, 0.013760851742449554, 0.023395925758488246, 0.01539922335430735, 0.01769089203375649, 0.015358411107087543, 0.01622708673022334, 0.01691937030648504, 0.016200249528594887, 0.02085474794607435, 0.01474533940388656, 0.012731581693465025, 0.018266611165089285, 0.016229812910215524, 0.020478817875936636, 0.016573860789364233, 0.008481531055539827, 0.015470415638027401, 0.014287116864465434, 0.016164563925782367, 0.02078887351392865, 0.017333318192653678, 0.012632758139267702, 0.012213650914616974, 0.01614828554636173, 0.025560039651404763, 0.019525075460589698, 0.01839800959719893, 0.012562286961105645, 0.01293452906516944, 0.016691211900907683, 0.014136022901098083, 0.016416245868429987, 0.01840349290697176, 0.01732702142053722, 0.016994507567704813, 0.016712081530073954, 0.01588116086457154, 0.01006312878998724, 0.013486488701840748, 0.011188158527700363, 0.01987468191088078, 0.016789877801380665, 0.016977477369341246, 0.017843094371446173, 0.0214808097928496, 0.021416753236510898, 0.009422146591678037, 0.015223986001091866, 0.01888844522687412, 0.019244044167401206, 0.014977449834734391, 0.013217291424087209, 0.013423323967572261, 0.018361443844465117, 0.018522263080378663, 0.01657659426933091, 0.014076915429184928, 0.016152386592375015, 0.007991590329098807, 0.01657858695332754, 0.014425661303218316, 0.022814036243435754, 0.01688286424081559, 0.012995231464965224, 0.016913866501999136, 0.012835465479568865, 0.015771030340788944, 0.00838617147930348, 0.015490499079804593, 0.014468726300572839, 0.016624736003830524, 0.014476070140855855, 0.01623806376679205, 0.015091710360715669, 0.017213331446330835, 0.018436500949490522, 0.012127668638558572, 0.0227881461680321, 0.010545894076177453, 0.01934457494664331, 0.016512623080812545, 0.016857887490931086, 0.014971339640472106, 0.018549618477888438, 0.013589874582733242, 0.015099576343689063, 0.0153906248362961, 0.013829037774957385, 0.015253270181548614, 0.01556528908417505, 0.01810940884635419, 0.02128283249734482, 0.01258513647598549, 0.01685868196822759, 0.01822336729568788, 0.01959789615232192, 0.01790748847294144, 0.012176562295199365, 0.011936429052215931, 0.01729950736919104, 0.012012356220843137, 0.017294610321334322, 0.016374669554749494, 0.011592733980844684, 0.01611149710985416, 0.012731358109182873, 0.014740440220006288, 0.013329173456884207, 0.020267133252334084, 0.013798255519412411, 0.018019069064263395, 0.0160694024831493, 0.023415684728042615, 0.012334650846221526, 0.014846166150366491, 0.015213007717784116, 0.01291426376828412, 0.013003873261330408, 0.011822692073682965, 0.019536541019859917, 0.014292367507999726, 0.013438936864639359, 0.013028585555575551, 0.01724214197372355, 0.012029573116676899, 0.01803967369884749, 0.018016005043410974, 0.00908382493926948, 0.011885515665375558, 0.014908863988531008, 0.02044871258933453, 0.011950020720308095, 0.014806104960732109, 0.013638983299491947, 0.012873604959751636, 0.016238791398146615, 0.010711134931883031, 0.014021671845462474, 0.013971494524776659, 0.015092305152560041, 0.008279842457616931, 0.014428104053981188, 0.018721745112922943, 0.013525921086275186, 0.016577682694351222, 0.01586044828077599, 0.020569253665026165, 0.01685077838335603, 0.014781830396439476, 0.01564961859950903, 0.010351771083480926, 0.01271029911987295, 0.013307312614555828, 0.012641499803550168, 0.013570216179058837, 0.01913939366528826, 0.01480828681335043, 0.012161366423225775, 0.01233043958272737, 0.012692466737289838, 0.015167718025259196, 0.015098495544097514, 0.01863455915362988, 0.015673118508027127], "moving_avg_accuracy_train": [0.050157425191491314, 0.10661340814184199, 0.1593479705956995, 0.21006349206349206, 0.2588367511939369, 0.3058081353099622, 0.3502905514070575, 0.3919241759336644, 0.4304663316088011, 0.4659427134557652, 0.4987130528406242, 0.5287264706439556, 0.5569056992716974, 0.5812443360974808, 0.6042321599512802, 0.6268715686850651, 0.6464287644086129, 0.6648068402621867, 0.6811240744887828, 0.6959093864474349, 0.710103941469635, 0.7215333926588194, 0.7289556037863022, 0.7409291645663099, 0.7503153628660465, 0.7589860114262488, 0.7680261777600913, 0.777043414664084, 0.7840408557396653, 0.7925704071742813, 0.8000957245975416, 0.8067246033940167, 0.8125206061060178, 0.8166653672873596, 0.819835579878022, 0.8253225519808898, 0.829891200310394, 0.8347938227927358, 0.8386430644291138, 0.8424098314911859, 0.8455906584541936, 0.8485088097553227, 0.8522445663478506, 0.8555088747382131, 0.8571704258372046, 0.859456372421535, 0.8610279845391544, 0.8638370560960824, 0.8654123060224193, 0.8685338963988595, 0.8711924814555866, 0.8740663335661187, 0.8754578181727775, 0.8773261385092004, 0.8772803296370548, 0.8784667081259149, 0.8810406765598165, 0.8820785244515462, 0.8830312608422166, 0.885290752277144, 0.8872896336293484, 0.8882817281117902, 0.8914227797030659, 0.8948355915375766, 0.8959169750517777, 0.8944700286943944, 0.8958881650310735, 0.8978036152661536, 0.8976398601325799, 0.899191803803938, 0.8997168025486826, 0.9017189969938881, 0.9013541856458283, 0.9035066089682424, 0.9058621365000358, 0.90768220313104, 0.9094760120203631, 0.9097118791184375, 0.9106028145685354, 0.9101934341879554, 0.9117408063180248, 0.9124778574172577, 0.9149012689577394, 0.9149551526228273, 0.9155287710641714, 0.9134785680591293, 0.9142347221889859, 0.9137741535726621, 0.914177661613098, 0.9155429219375762, 0.9170876962236082, 0.9187011352691137, 0.9192907083481916, 0.9177290146301591, 0.917407009629168, 0.9175006023401161, 0.9172432912467007, 0.9163400316472243, 0.9151899514303148, 0.9094058258770766, 0.9112855082049282, 0.9112892363619178, 0.9101604244770642, 0.910104672092573, 0.9121004456548181, 0.9125388631025331, 0.9135288210959895, 0.9093351875271344, 0.9111010620007037, 0.9128135098161834, 0.9131435626620844, 0.9127782672520498, 0.9107058560687883, 0.9124533904727529, 0.9148885493029231, 0.9158106610000764, 0.9157922427002256, 0.9166869803684183, 0.9147605909556075, 0.9146426026137842, 0.9140250966609605, 0.9134899432033362, 0.9160306380556678, 0.9159714406156879, 0.9173945963649348, 0.9158088164476643, 0.9128052357269012, 0.9154933842887626, 0.9175199841385409, 0.9199622693913996, 0.9202239097464642, 0.91864587814105, 0.9187993789032185, 0.9212833884450673, 0.9222612717708726, 0.9204425178749777, 0.9186683113936353, 0.9205353844566988, 0.9214441612968816, 0.9221245882827341, 0.9204726381177128, 0.9212362665286251, 0.9239276661769809, 0.9241345276796685, 0.9244646820141841, 0.924554954768838, 0.9258496395360479, 0.9229329038839381, 0.9240645612968547, 0.9243901586232415, 0.9260500953264397, 0.9271626779057374, 0.9281036204556952, 0.9299640894363346, 0.9314688117046335, 0.9324953599592347, 0.9296647869397324, 0.9307530469350818, 0.9316302825297335, 0.9314967465363578, 0.9324623725875485, 0.9336870756550211, 0.9351519595323946, 0.9344547033459049, 0.9353685661946846, 0.9317484404087801, 0.9332027189108147, 0.9326377519639009, 0.9330266810056983, 0.9329279994718966, 0.9325671076319422, 0.9320494978688864, 0.9327715838308903, 0.931803482064634, 0.933163792599865, 0.9288945385186806, 0.9297600233549816, 0.9287884110446311, 0.9300644342235751, 0.931849693516703, 0.9295998701771682, 0.9298230515333901, 0.9261413848767288, 0.9283399475414645, 0.9292376039385731, 0.9298457121424456, 0.9310417620926067, 0.9320483804858284, 0.9338701212313116, 0.9326270080618996, 0.9343025303950138, 0.935559528618663, 0.9367141866544988, 0.9376438445509722, 0.9368648105769104, 0.9358290748645959, 0.9372705652186678, 0.9341274130434234, 0.9324865829320951, 0.9349922750103326, 0.9358363209439153, 0.9348358246353304, 0.9358745520647468, 0.9364302993488128, 0.9378488335865968, 0.935675570348369, 0.9373926475766365, 0.938973074558314, 0.9379799176192785, 0.9370348510026995, 0.9360007484871009, 0.9366345569325031, 0.937516446327385, 0.9386635694018263, 0.9396866795735853, 0.9410840260876646, 0.9398055134290643, 0.939954429976754, 0.9409042937112677, 0.9421892875532731, 0.9415811382599244, 0.9423544163220826, 0.9419506793375765, 0.9427915628907605, 0.9434134994576737, 0.942785235521504, 0.9436822444825042, 0.9432341163855364, 0.9443046931017725, 0.944847360211861, 0.9445756892919576, 0.9437312249735499, 0.9430202154560767, 0.9416454877689039, 0.9411661592647156, 0.941750853640708, 0.9429790572266649, 0.9417270640005376, 0.9426857483350723, 0.9437485991314101, 0.9418081375779036, 0.9416845679023392, 0.9395041890467676, 0.9404339727076132, 0.9417821665451944, 0.9425144514883587, 0.9429013573800359, 0.9408364647552659, 0.9403287565583938, 0.9405854956704577], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 552664330, "moving_var_accuracy_train": [0.022641905716560433, 0.04906321724291697, 0.06918530221342346, 0.08541534905183225, 0.09828329140249868, 0.10831176059422554, 0.11528875261131911, 0.11936010557118956, 0.12079357489084885, 0.12004138042232841, 0.11770229867068517, 0.11403931603775265, 0.10978200476846825, 0.10413512747445784, 0.09847757513681181, 0.093242703073469, 0.0873607879072445, 0.0816644921652375, 0.07589431214396411, 0.07027232997699967, 0.06505846550980408, 0.05972831014919724, 0.054251282096465875, 0.05011644930659213, 0.04589771084263073, 0.041984561076458496, 0.03852162643490449, 0.03540125884385859, 0.032301810593928816, 0.02972640876361777, 0.02726344150814342, 0.024932575664214286, 0.02274166092473057, 0.02062210623951073, 0.018650347846389602, 0.01705627482746949, 0.015538500272750676, 0.01420097161031489, 0.012914224399860431, 0.011750498766773584, 0.010666507831613594, 0.009676497511598773, 0.008834450656306437, 0.008046906974082318, 0.007267063045165127, 0.006587386706726325, 0.005950877717887927, 0.005426807893206614, 0.004906459814859768, 0.004503512771678351, 0.0041167741650451955, 0.003779427982119567, 0.0034189112486027236, 0.003108435711657873, 0.002797611026566991, 0.0025305173691797605, 0.002337093453750285, 0.002113078262592566, 0.0019099397960042788, 0.001764893530304446, 0.0016243639172157153, 0.0014707857886529646, 0.0014125030556791725, 0.001376078311671241, 0.0012489949932471879, 0.0011429383777727747, 0.0010467445360201827, 0.0009750906288457824, 0.0008778229076551496, 0.0008117173793212507, 0.0007330262545269765, 0.0006958026724419874, 0.0006274201910748483, 0.0006063745073972173, 0.000595673646234823, 0.0005659200644829948, 0.0005382878110174265, 0.0004849597295072703, 0.00044360765034271414, 0.0004007552159724773, 0.00038222893895546793, 0.0003488952439658457, 0.00036686203102011977, 0.0003302019589623777, 0.00030014310611238993, 0.00030795878675810365, 0.00028230882969518793, 0.00025598705777875124, 0.00023185372064914378, 0.00022544377036658143, 0.00022437634168299357, 0.00022536737749675103, 0.00020595900748723685, 0.00020731309215899364, 0.00018751496792906427, 0.0001688423074960414, 0.0001525539577355892, 0.0001446414630984445, 0.0001420814773365411, 0.00042897830534349474, 0.00041787932569188293, 0.00037609151821508554, 0.00034995031283605873, 0.00031498325650784097, 0.000319332939862866, 0.00028912953460272727, 0.00026903673260173124, 0.00040041212193001605, 0.0003884357236446478, 0.00037598444896685246, 0.00033936641799995357, 0.00030663074282928914, 0.00031462166155892446, 0.0003106443838403931, 0.00033294993220976004, 0.0003073075488270256, 0.00027657984704824757, 0.00025612686179736814, 0.00026391296114573753, 0.00023764695627041948, 0.00021731408305933068, 0.00019816017776226271, 0.00023644033298001377, 0.0002128278387141139, 0.00020977340542223597, 0.0002114283463941787, 0.00027147898607001873, 0.0003093663716787536, 0.00031539369707096813, 0.00033753714267085257, 0.00030439952948235207, 0.00029637123026329267, 0.00026694616959284026, 0.00029578428326951434, 0.0002748121571325535, 0.00027710173302379545, 0.0002777218374673508, 0.00028132331012597415, 0.00026062385722065105, 0.00023872829944627462, 0.0002394159236310719, 0.00022072248641753767, 0.000263842926380304, 0.00023784375887392136, 0.00021504039994792425, 0.00019360970248522713, 0.00018933461005471412, 0.000246967270827837, 0.00023379638024693586, 0.0002113708647927949, 0.00021503228704113583, 0.00020466961829883307, 0.00019217101240984018, 0.00020410601462014945, 0.00020407311510056857, 0.00019315001546173568, 0.000245944306484171, 0.00023200866419305474, 0.0002157336783704641, 0.00019432079728715922, 0.0001832806205950871, 0.0001784516369668683, 0.00017991943623788052, 0.0001663029883204765, 0.00015718899724584775, 0.00025941789387319347, 0.00025251043813919165, 0.00023013208318521934, 0.00020848026706267857, 0.00018771988276243174, 0.00017012008076750044, 0.00015551935149204645, 0.00014466008957155017, 0.0001386290698828513, 0.00014142016566490883, 0.00029131692278580243, 0.0002689268065240251, 0.00025053040020624445, 0.00024013147656444265, 0.00024480268560129434, 0.00026587776257320573, 0.0002397382755757706, 0.00033775647235513627, 0.00034748392523655273, 0.00031998761577832783, 0.00029131701448904807, 0.0002750601323896672, 0.00025667364445685237, 0.00026087493410495145, 0.00024869541386214806, 0.00024909224827481403, 0.00023840342425564877, 0.00022656219844756554, 0.0002116843528430862, 0.00019597796295346004, 0.00018603490284998706, 0.0001861324625329316, 0.0002564338666503309, 0.0002550213910734728, 0.000286025687084606, 0.00026383484021812496, 0.0002464602919677418, 0.0002315248548245652, 0.00021115206473583056, 0.00020814701271613405, 0.00022983996936821113, 0.0002333911603019055, 0.00023253178927144146, 0.00021815585669428736, 0.0002043786292128076, 0.0001935650784064307, 0.0001778239888749553, 0.00016704115013070546, 0.00016218005724887542, 0.00015538284133599948, 0.0001574177527260848, 0.00015638732901728834, 0.00014094818135914193, 0.0001349735332505248, 0.0001363370624913982, 0.0001260319663092632, 0.00011881040033107283, 0.00010839639227188873, 0.00010392051939483855, 9.700971329472928e-05, 9.086118212667962e-05, 8.901668959904398e-05, 8.192238976076627e-05, 8.404536133280851e-05, 7.829121353087423e-05, 7.112633797627695e-05, 7.043178404422224e-05, 6.793841644523615e-05, 7.815346072562849e-05, 7.24059169874121e-05, 6.824213290852567e-05, 7.499427605468936e-05, 8.160223179363544e-05, 8.17136894938112e-05, 8.370918688191165e-05, 0.00010922678755945067, 9.844153398597755e-05, 0.00013138384817179668, 0.00012602594225839575, 0.00012978198764578464, 0.00012162996002307122, 0.0001108142295418952, 0.00013810684055416854, 0.00012661606501729184, 0.00011454769326053299], "duration": 154843.235905, "accuracy_train": [0.5015742519149132, 0.6147172546949982, 0.6339590326804172, 0.666503185273625, 0.6977960833679402, 0.7285505923541897, 0.7506322962809154, 0.7666267966731267, 0.7773457326850315, 0.7852301500784422, 0.7936461073043558, 0.7988472308739387, 0.8105187569213732, 0.8002920675295312, 0.8111225746354743, 0.8306262472891289, 0.8224435259205426, 0.8302095229443521, 0.827979182528147, 0.8289771940753045, 0.8378549366694352, 0.8243984533614802, 0.7957555039336471, 0.8486912115863787, 0.8347911475636766, 0.8370218484680694, 0.8493876747646733, 0.8581985468000184, 0.8470178254198967, 0.869336370085825, 0.8678235814068845, 0.8663845125622923, 0.8646846305140274, 0.8539682179194352, 0.8483674931939831, 0.8747053009067, 0.8710090352759321, 0.8789174251338132, 0.8732862391565154, 0.8763107350498339, 0.8742181011212625, 0.8747721714654854, 0.8858663756806018, 0.8848876502514765, 0.8721243857281286, 0.8800298916805095, 0.8751724935977298, 0.8891187001084349, 0.8795895553594499, 0.8966282097868217, 0.8951197469661315, 0.8999310025609081, 0.8879811796327058, 0.8941410215370063, 0.8768680497877446, 0.8891441145256552, 0.9042063924649317, 0.8914191554771133, 0.8916058883582503, 0.9056261751914912, 0.9052795657991879, 0.8972105784537652, 0.9196922440245479, 0.9255508980481728, 0.9056494266795865, 0.8814475114779439, 0.908651392061185, 0.9150426673818751, 0.8961660639304172, 0.9131592968461609, 0.9044417912513842, 0.9197387470007383, 0.898070883513289, 0.9228784188699704, 0.9270618842861758, 0.9240628028100776, 0.925620292024271, 0.9118346830011074, 0.9186212336194168, 0.9065090107627353, 0.925667155488649, 0.9191113173103543, 0.9367119728220746, 0.9154401056086194, 0.920691337036268, 0.8950267410137505, 0.9210401093576966, 0.9096290360257475, 0.917809233977021, 0.9278302648578812, 0.9309906647978959, 0.9332220866786637, 0.924596866059893, 0.9036737711678663, 0.9145089646202473, 0.918342936738649, 0.9149274914059615, 0.908210695251938, 0.9048392294781286, 0.8573486958979328, 0.9282026491555924, 0.9113227897748246, 0.9000011175133813, 0.9096029006321521, 0.9300624077150241, 0.9164846201319674, 0.9224384430370985, 0.8715924854074382, 0.9269939322628276, 0.9282255401555003, 0.9161140382751938, 0.9094906085617387, 0.8920541554194352, 0.9281812001084349, 0.9368049787744556, 0.9241096662744556, 0.9156264780015688, 0.9247396193821521, 0.8974230862403102, 0.9135807075373754, 0.9084675430855482, 0.9086735620847176, 0.9388968917266519, 0.9154386636558692, 0.930202998108158, 0.9015367971922297, 0.8857730092400333, 0.939686721345515, 0.9357593827865448, 0.9419428366671282, 0.9225786729420451, 0.904443593692322, 0.9201808857627353, 0.9436394743217055, 0.9310622217031194, 0.9040737328119232, 0.9027004530615541, 0.937339042024271, 0.9296231528585271, 0.928248431155408, 0.9056050866325213, 0.9281089222268365, 0.9481502630121816, 0.9259962812038575, 0.9274360710248246, 0.9253674095607235, 0.9375018024409376, 0.8966822830149501, 0.9342494780131044, 0.9273205345607235, 0.9409895256552234, 0.9371759211194168, 0.9365721034053157, 0.9467083102620893, 0.9450113121193245, 0.941734294250646, 0.9041896297642118, 0.9405473868932264, 0.9395254028815985, 0.9302949225959765, 0.9411530070482651, 0.9447094032622739, 0.948335914428756, 0.9281793976674971, 0.9435933318337025, 0.8991673083356404, 0.9462912254291252, 0.9275530494416758, 0.9365270423818751, 0.9320398656676817, 0.9293190810723514, 0.9273910100013842, 0.9392703574889257, 0.9230905661683279, 0.9454065874169435, 0.8904712517880213, 0.9375493868816908, 0.9200439002514765, 0.9415486428340717, 0.9479170271548542, 0.9093514601213547, 0.9318316837393872, 0.8930063849667773, 0.9481270115240864, 0.9373165115125508, 0.9353186859772978, 0.9418062116440569, 0.9411079460248246, 0.9502657879406607, 0.9214389895371908, 0.9493822313930418, 0.9468725126315062, 0.947106108977021, 0.9460107656192323, 0.9298535048103543, 0.9265074534537652, 0.9502439784053157, 0.9058390434662238, 0.9177191119301403, 0.9575435037144703, 0.9434327343461609, 0.9258313578580657, 0.9452230989294942, 0.941432024905408, 0.9506156417266519, 0.916116201204319, 0.9528463426310447, 0.953196917393411, 0.9290415051679586, 0.9285292514534883, 0.9266938258467147, 0.9423388329411223, 0.9454534508813216, 0.9489876770717978, 0.9488946711194168, 0.953660144714378, 0.9282988995016611, 0.9412946789059615, 0.94945306732189, 0.9537542321313216, 0.936107794619786, 0.9493139188815062, 0.938317046477021, 0.9503595148694168, 0.949010928559893, 0.9371308600959765, 0.9517553251315062, 0.9392009635128276, 0.9539398835478959, 0.9497313642026578, 0.9421306510128276, 0.9361310461078812, 0.9366211297988187, 0.9292729385843485, 0.936852202727021, 0.94701310302464, 0.9540328895002769, 0.9304591249653931, 0.9513139073458842, 0.9533142562984496, 0.9243439835963455, 0.9405724408222591, 0.9198807793466224, 0.9488020256552234, 0.9539159110834257, 0.9491050159768365, 0.9463835104051311, 0.9222524311323367, 0.9357593827865448, 0.942896147679033], "end": "2016-01-26 17:54:05.492000", "learning_rate_per_epoch": [0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155, 0.00023634437820874155], "accuracy_valid": [0.5072742140436747, 0.5999094032379518, 0.6192685782191265, 0.6452901449548193, 0.6717088078878012, 0.7014645496046686, 0.7123391025037651, 0.722400343561747, 0.7387489410768072, 0.7433361375188253, 0.74658203125, 0.7588199477597892, 0.7599803510918675, 0.7508853774472892, 0.7674163450677711, 0.7811499905873494, 0.7683531979480422, 0.7755965267319277, 0.7774981762989458, 0.7771628506212349, 0.7811102809676205, 0.7751082454819277, 0.7527076077748494, 0.7930937617658133, 0.7826560146837349, 0.7845591349774097, 0.7950777720256024, 0.7970926675451807, 0.7877535532756024, 0.8047213267131024, 0.8051493081701807, 0.8031138224774097, 0.8076716043862951, 0.7957293039344879, 0.7849150508283133, 0.813267719314759, 0.8060743952371988, 0.8127485528049698, 0.8026578972138554, 0.813145649002259, 0.810826313064759, 0.8089746682040663, 0.8152002541415663, 0.8171945594879518, 0.8073877541415663, 0.8060332148908133, 0.8008562570594879, 0.8150164133094879, 0.8087099374058735, 0.8253320900790663, 0.8184240869728916, 0.8251394248870482, 0.8119852456701807, 0.8182211266942772, 0.8000517695783133, 0.8125955972326807, 0.8266336831701807, 0.814354586314006, 0.8148031579442772, 0.8241010918674698, 0.8263395378388554, 0.8189226633094879, 0.8346403190888554, 0.840662944747741, 0.8220759012612951, 0.8034609139683735, 0.8235613351844879, 0.8300222373870482, 0.8123705760542168, 0.8309782097138554, 0.8198374552899097, 0.8367861092808735, 0.8213949548192772, 0.8382112434111446, 0.8393495858433735, 0.8371626153049698, 0.8427072548004518, 0.829991352127259, 0.8329916345067772, 0.8237245858433735, 0.8363801887236446, 0.837855327560241, 0.847132671310241, 0.8326254235692772, 0.8339373117469879, 0.8136133400790663, 0.8367964043674698, 0.8265939735504518, 0.8320459572665663, 0.8422395637236446, 0.8422998635165663, 0.844447124435241, 0.8375803016754518, 0.8218832360692772, 0.8290353798004518, 0.8304281579442772, 0.8349653496799698, 0.8222288568335843, 0.8219141213290663, 0.7844576548381024, 0.8437955925263554, 0.8161444606551205, 0.817650484751506, 0.8282308923192772, 0.8441618034638554, 0.8313650108245482, 0.8301531320594879, 0.7909567959337349, 0.8376611916415663, 0.8376303063817772, 0.8296869117093373, 0.8249555840549698, 0.8088114175451807, 0.8375082360692772, 0.8509462655308735, 0.8376508965549698, 0.8323709878576807, 0.8331239999058735, 0.8114969644201807, 0.8247923333960843, 0.8199198159826807, 0.8286985833960843, 0.8501535438629518, 0.8283220773719879, 0.8456266472138554, 0.8135103892131024, 0.8044065912085843, 0.8503873894013554, 0.8494211219879518, 0.8502859092620482, 0.8360036826995482, 0.8230436394013554, 0.8329916345067772, 0.8561452842620482, 0.8447015601468373, 0.8207948983433735, 0.8210081537085843, 0.8438970726656627, 0.8369390648531627, 0.8364404885165663, 0.8206419427710843, 0.8436941123870482, 0.8565629706325302, 0.8413850715361446, 0.8409262048192772, 0.8374582313629518, 0.8468576454254518, 0.814610492752259, 0.8446191994540663, 0.840733539627259, 0.852137554122741, 0.8469900108245482, 0.8398378670933735, 0.8522993340549698, 0.8527376105986446, 0.8517507530120482, 0.8251997246799698, 0.8494211219879518, 0.848790180252259, 0.8401437782379518, 0.8496755576995482, 0.857874858810241, 0.8541818641754518, 0.8469297110316265, 0.8474268166415663, 0.8158106057040663, 0.8484857398343373, 0.8355962914156627, 0.8477930275790663, 0.8437249976468373, 0.8345682534826807, 0.835972797439759, 0.8527273155120482, 0.8292986398719879, 0.8505403449736446, 0.8117205148719879, 0.8424837043486446, 0.836756694747741, 0.8464914344879518, 0.855921733810241, 0.8216802757906627, 0.842127788497741, 0.8117308099585843, 0.8538156532379518, 0.8438867775790663, 0.8437044074736446, 0.8472238563629518, 0.8528905661709337, 0.855189311935241, 0.8337343514683735, 0.8597367987575302, 0.8583528449736446, 0.857630718185241, 0.8492990516754518, 0.8472753317959337, 0.8360742775790663, 0.8523008047816265, 0.8177431405308735, 0.8335622764495482, 0.8664609610316265, 0.8535921027861446, 0.837925922439759, 0.852086078689759, 0.8522493293486446, 0.856114399002259, 0.8265630882906627, 0.8607942512236446, 0.8603368552334337, 0.8359625023531627, 0.8363493034638554, 0.8380376976656627, 0.8514360175075302, 0.8553525625941265, 0.8484136742281627, 0.8527273155120482, 0.8562982398343373, 0.836756694747741, 0.8510183311370482, 0.8548333960843373, 0.8600927146084337, 0.8435308617281627, 0.8600015295557228, 0.8393495858433735, 0.856287944747741, 0.8532155967620482, 0.8447824501129518, 0.8491049157567772, 0.8443853539156627, 0.8604795157191265, 0.8544465949736446, 0.8461252235504518, 0.8407026543674698, 0.8480886436370482, 0.836216938064759, 0.8479665733245482, 0.856410015060241, 0.8562261742281627, 0.8422895684299698, 0.8584955054593373, 0.858485210372741, 0.8341005624058735, 0.8475091773343373, 0.8277014307228916, 0.8555246376129518, 0.8569997764495482, 0.854823100997741, 0.855433452560241, 0.8397775673004518, 0.8449251105986446, 0.850794780685241], "accuracy_test": 0.10213647959183673, "start": "2016-01-24 22:53:22.256000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0, 211.0, 212.0, 213.0, 214.0, 215.0, 216.0, 217.0, 218.0, 219.0, 220.0, 221.0, 222.0, 223.0, 224.0, 225.0, 226.0, 227.0, 228.0, 229.0, 230.0, 231.0, 232.0, 233.0, 234.0, 235.0, 236.0, 237.0, 238.0, 239.0, 240.0, 241.0, 242.0, 243.0, 244.0, 245.0, 246.0, 247.0, 248.0, 249.0], "accuracy_train_last": 0.942896147679033, "batch_size_eval": 1024, "accuracy_train_std": [0.015118748731355408, 0.017689235904824303, 0.021743240626257543, 0.022321476081211915, 0.021809350489148043, 0.0214499240559923, 0.022921920520217128, 0.023041807141361732, 0.023663684816060064, 0.0202781055874651, 0.020119078046371655, 0.024708510941832157, 0.02352553470636172, 0.020927678951693774, 0.02288548119585375, 0.024010251610700847, 0.021411144400317043, 0.023644816127463296, 0.02395214930052603, 0.02242666820777545, 0.025135709580932045, 0.02233087937280273, 0.022959686748962368, 0.024210076642024653, 0.022189094844154177, 0.0221586326057252, 0.021868549654181717, 0.024260294942811515, 0.02303768963156318, 0.02366861598173126, 0.02328757861632592, 0.02309765576628906, 0.02323825482097963, 0.022984522012146343, 0.0216435506420428, 0.0223175749252606, 0.022387598036883928, 0.020792506438106314, 0.02338986598679055, 0.02282794662137084, 0.023586585542202654, 0.024286717246481827, 0.02009410085863799, 0.02161034185056322, 0.023503253422599114, 0.022272057746956477, 0.02222302706529647, 0.02387499530920346, 0.021374150724395097, 0.02344797783897014, 0.020975272971141636, 0.022280386251061348, 0.022244490865239122, 0.019949409800563176, 0.020391076422377453, 0.021674914245390524, 0.020596135061953645, 0.0220666371068821, 0.02390959743007913, 0.019226091500794698, 0.01977117167488442, 0.021982304729077994, 0.019896410439170148, 0.018474712837838317, 0.018582436743990363, 0.02011279714245451, 0.01965328804280743, 0.020019700621349036, 0.020920289071552235, 0.02030763805704761, 0.019097892498286747, 0.01826167557356422, 0.01845686722743101, 0.018808284479745318, 0.017580070000697553, 0.018488401645875095, 0.018677937885466372, 0.017790797161759197, 0.018894643246260803, 0.01884600110880894, 0.020107716685863003, 0.018720346260986828, 0.015659487574164424, 0.017515534964335635, 0.01847863200758306, 0.017075191904194934, 0.019257263709701714, 0.017869850431858944, 0.018400043224117802, 0.01649590940327749, 0.01707046822907947, 0.016347936904897848, 0.01777189152853446, 0.019958453909404816, 0.017672976917480022, 0.017826477585485865, 0.017807261495940082, 0.016821651967051876, 0.016655239162409158, 0.016521965544746185, 0.016331279647886866, 0.018226244113455005, 0.018986011989402272, 0.017897616572279368, 0.015912787498298834, 0.01781050644210941, 0.015509279370449231, 0.01804772650366079, 0.01762754435206346, 0.016202011642118524, 0.01654791125748305, 0.01561654679726583, 0.017388269855207254, 0.015797146933809667, 0.0157722370992847, 0.016017108194533086, 0.01617047077675462, 0.016247239421290305, 0.016200410655183028, 0.018196259995453927, 0.01536140946552755, 0.018790642993300962, 0.016205979825482067, 0.016068791230317642, 0.016161740706596785, 0.017663776405129592, 0.018843632816317412, 0.015830591710178615, 0.01570647729792778, 0.015287164029332799, 0.0151292245009369, 0.01772548824321826, 0.01666022046367656, 0.01539912432640374, 0.016879402319632673, 0.015244756302667365, 0.016717221704617632, 0.01559899273428445, 0.016042601381473887, 0.016782077594625813, 0.017017901519936773, 0.016937938852804462, 0.014590972751129836, 0.01493766323629998, 0.015622708536526907, 0.01566321154046191, 0.016108967140394004, 0.014922920555874384, 0.013501437579413416, 0.014891222588707967, 0.01401963737753812, 0.015631047245044844, 0.015640806515426757, 0.015770676574219687, 0.01422507812117402, 0.015399030548796435, 0.016083204509106414, 0.01598628380512554, 0.014471588958530943, 0.01622223553004838, 0.014952337964548678, 0.014319168553502116, 0.015141332971808074, 0.016029517005788346, 0.013638447340599758, 0.015785162268790773, 0.014810310077179202, 0.016183195091325504, 0.014728804334889123, 0.015491006590862996, 0.017333686610865793, 0.015165064187060132, 0.0132199944446877, 0.014651198108317885, 0.014561578623522725, 0.015795671996011715, 0.014621918380857087, 0.01498498253488463, 0.01397253020729322, 0.015114520027013209, 0.015495991408327678, 0.014804685085973171, 0.017965882312323513, 0.014275375699493902, 0.01321188005316352, 0.0162234333963843, 0.014388584462689473, 0.013830453398708356, 0.015862751941939198, 0.01643005389626437, 0.013896565158727553, 0.012418661010064967, 0.013304844145399225, 0.01412423353898556, 0.014794101217104021, 0.014950099391592007, 0.013234013488463672, 0.01545538331951862, 0.015874903449456777, 0.012140279979174357, 0.013685627961574439, 0.014941024648771709, 0.013383873707672434, 0.01297917590178336, 0.013695588991977118, 0.014812075576284701, 0.013381474596356278, 0.0123668651944114, 0.015431077747617592, 0.014983165174185107, 0.014142277039508721, 0.014479819165711312, 0.013697661810596864, 0.012439754899210611, 0.01272464327581792, 0.014141955774121674, 0.01261749641112073, 0.012573279734582452, 0.013065602352608651, 0.012027160777734924, 0.014230587108988649, 0.01206347097094527, 0.012954213341735936, 0.01182402274031365, 0.013753475992790632, 0.014319154654568286, 0.012898554947280776, 0.014646303616678907, 0.011889628053061721, 0.01282052903527008, 0.014644032034058037, 0.014108660396022639, 0.01334862913881587, 0.014160969667559883, 0.01358053666245374, 0.011796248324338697, 0.010916853561780286, 0.0160130397107838, 0.012973900460520283, 0.011367438123674961, 0.014103846320524822, 0.01341969709782989, 0.014908229017809032, 0.012408466034172819, 0.012088691337989798, 0.011437014108045401, 0.011805082967622454, 0.015003021923034437, 0.014453710975232584, 0.014853168258915767], "accuracy_test_std": 0.007720714668533387, "error_valid": [0.4927257859563253, 0.40009059676204817, 0.3807314217808735, 0.3547098550451807, 0.3282911921121988, 0.29853545039533136, 0.2876608974962349, 0.277599656438253, 0.2612510589231928, 0.2566638624811747, 0.25341796875, 0.24118005224021077, 0.24001964890813254, 0.24911462255271077, 0.23258365493222888, 0.21885000941265065, 0.23164680205195776, 0.2244034732680723, 0.2225018237010542, 0.2228371493787651, 0.21888971903237953, 0.2248917545180723, 0.24729239222515065, 0.20690623823418675, 0.2173439853162651, 0.2154408650225903, 0.20492222797439763, 0.2029073324548193, 0.21224644672439763, 0.19527867328689763, 0.1948506918298193, 0.1968861775225903, 0.19232839561370485, 0.20427069606551207, 0.21508494917168675, 0.18673228068524095, 0.19392560476280118, 0.18725144719503017, 0.1973421027861446, 0.18685435099774095, 0.18917368693524095, 0.19102533179593373, 0.18479974585843373, 0.18280544051204817, 0.19261224585843373, 0.19396678510918675, 0.19914374294051207, 0.18498358669051207, 0.1912900625941265, 0.17466790992093373, 0.1815759130271084, 0.17486057511295183, 0.1880147543298193, 0.18177887330572284, 0.19994823042168675, 0.1874044027673193, 0.1733663168298193, 0.18564541368599397, 0.18519684205572284, 0.17589890813253017, 0.1736604621611446, 0.18107733669051207, 0.1653596809111446, 0.15933705525225905, 0.17792409873870485, 0.1965390860316265, 0.17643866481551207, 0.16997776261295183, 0.1876294239457832, 0.1690217902861446, 0.1801625447100903, 0.1632138907191265, 0.17860504518072284, 0.1617887565888554, 0.1606504141566265, 0.16283738469503017, 0.15729274519954817, 0.17000864787274095, 0.16700836549322284, 0.1762754141566265, 0.1636198112763554, 0.16214467243975905, 0.15286732868975905, 0.16737457643072284, 0.16606268825301207, 0.18638665992093373, 0.16320359563253017, 0.17340602644954817, 0.16795404273343373, 0.1577604362763554, 0.15770013648343373, 0.15555287556475905, 0.16241969832454817, 0.17811676393072284, 0.17096462019954817, 0.16957184205572284, 0.16503465032003017, 0.17777114316641573, 0.17808587867093373, 0.21554234516189763, 0.1562044074736446, 0.18385553934487953, 0.18234951524849397, 0.17176910768072284, 0.1558381965361446, 0.16863498917545183, 0.16984686794051207, 0.2090432040662651, 0.16233880835843373, 0.16236969361822284, 0.17031308829066272, 0.17504441594503017, 0.1911885824548193, 0.16249176393072284, 0.1490537344691265, 0.16234910344503017, 0.1676290121423193, 0.1668760000941265, 0.1885030355798193, 0.17520766660391573, 0.1800801840173193, 0.17130141660391573, 0.14984645613704817, 0.17167792262801207, 0.1543733527861446, 0.18648961078689763, 0.19559340879141573, 0.1496126105986446, 0.15057887801204817, 0.14971409073795183, 0.16399631730045183, 0.1769563605986446, 0.16700836549322284, 0.14385471573795183, 0.15529843985316272, 0.1792051016566265, 0.17899184629141573, 0.15610292733433728, 0.16306093514683728, 0.16355951148343373, 0.17935805722891573, 0.15630588761295183, 0.14343702936746983, 0.1586149284638554, 0.15907379518072284, 0.16254176863704817, 0.15314235457454817, 0.18538950724774095, 0.15538080054593373, 0.15926646037274095, 0.14786244587725905, 0.15300998917545183, 0.1601621329066265, 0.14770066594503017, 0.1472623894013554, 0.14824924698795183, 0.17480027532003017, 0.15057887801204817, 0.15120981974774095, 0.15985622176204817, 0.15032444230045183, 0.14212514118975905, 0.14581813582454817, 0.1530702889683735, 0.15257318335843373, 0.18418939429593373, 0.15151426016566272, 0.16440370858433728, 0.15220697242093373, 0.15627500235316272, 0.1654317465173193, 0.16402720256024095, 0.14727268448795183, 0.17070136012801207, 0.1494596550263554, 0.18827948512801207, 0.1575162956513554, 0.16324330525225905, 0.15350856551204817, 0.14407826618975905, 0.17831972420933728, 0.15787221150225905, 0.18826919004141573, 0.14618434676204817, 0.15611322242093373, 0.1562955925263554, 0.15277614363704817, 0.14710943382906627, 0.14481068806475905, 0.1662656485316265, 0.14026320124246983, 0.1416471550263554, 0.14236928181475905, 0.15070094832454817, 0.15272466820406627, 0.16392572242093373, 0.1476991952183735, 0.1822568594691265, 0.16643772355045183, 0.1335390389683735, 0.1464078972138554, 0.16207407756024095, 0.14791392131024095, 0.1477506706513554, 0.14388560099774095, 0.17343691170933728, 0.1392057487763554, 0.13966314476656627, 0.16403749764683728, 0.1636506965361446, 0.16196230233433728, 0.14856398249246983, 0.1446474374058735, 0.15158632577183728, 0.14727268448795183, 0.14370176016566272, 0.16324330525225905, 0.14898166886295183, 0.14516660391566272, 0.13990728539156627, 0.15646913827183728, 0.13999847044427716, 0.1606504141566265, 0.14371205525225905, 0.14678440323795183, 0.15521754988704817, 0.15089508424322284, 0.15561464608433728, 0.1395204842808735, 0.1455534050263554, 0.15387477644954817, 0.15929734563253017, 0.15191135636295183, 0.16378306193524095, 0.15203342667545183, 0.14358998493975905, 0.14377382577183728, 0.15771043157003017, 0.14150449454066272, 0.14151478962725905, 0.1658994375941265, 0.15249082266566272, 0.1722985692771084, 0.14447536238704817, 0.14300022355045183, 0.14517689900225905, 0.14456654743975905, 0.16022243269954817, 0.1550748894013554, 0.14920521931475905], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "momentum": 0.6732320821831332, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.00023634438104262547, "patience_threshold": 1, "do_flip": true, "batch_size": 32, "optimization": "rmsprop", "nb_data_augmentation": 1, "learning_rate_decay_method": "none", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 7.472493896325545e-05, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.03596833697416144}, "accuracy_valid_max": 0.8664609610316265, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.850794780685241, "loss_train": [3.0338239669799805, 2.5263915061950684, 2.2635178565979004, 2.0857057571411133, 1.9530695676803589, 1.842848777770996, 1.7541202306747437, 1.676598310470581, 1.6113437414169312, 1.5527204275131226, 1.5000666379928589, 1.4571365118026733, 1.4146651029586792, 1.3800886869430542, 1.3453587293624878, 1.3176145553588867, 1.28778874874115, 1.2629286050796509, 1.2399955987930298, 1.2194818258285522, 1.2011291980743408, 1.1811537742614746, 1.1618188619613647, 1.1443971395492554, 1.1315279006958008, 1.1157792806625366, 1.1005886793136597, 1.0873336791992188, 1.075697660446167, 1.0623775720596313, 1.0525909662246704, 1.0388673543930054, 1.0307047367095947, 1.0206376314163208, 1.010532259941101, 1.0034806728363037, 0.9920297861099243, 0.9829385280609131, 0.9761145710945129, 0.9676889181137085, 0.9566276669502258, 0.9526308178901672, 0.943360447883606, 0.9353594779968262, 0.9345574378967285, 0.9263245463371277, 0.9166494011878967, 0.9097456932067871, 0.9094588160514832, 0.9009714722633362, 0.8922625184059143, 0.8886353969573975, 0.8848905563354492, 0.8788955211639404, 0.8710203766822815, 0.8697146773338318, 0.8619532585144043, 0.8597604036331177, 0.8504359722137451, 0.847576916217804, 0.8433144688606262, 0.839684784412384, 0.8372383117675781, 0.8311827182769775, 0.8230159282684326, 0.8217289447784424, 0.8197052478790283, 0.8150010108947754, 0.8145946860313416, 0.8106482625007629, 0.8031321167945862, 0.8001075387001038, 0.7976493835449219, 0.7948766946792603, 0.7911688089370728, 0.7890438437461853, 0.7836962938308716, 0.7810391187667847, 0.7800009846687317, 0.7739967107772827, 0.7708076238632202, 0.76958167552948, 0.7678447365760803, 0.7623536586761475, 0.7633017897605896, 0.759545087814331, 0.7529383301734924, 0.7521535158157349, 0.7503489255905151, 0.7487578988075256, 0.744541347026825, 0.7457078099250793, 0.7411607503890991, 0.7431148886680603, 0.7367423176765442, 0.7349973917007446, 0.7351875901222229, 0.7340366244316101, 0.7306528091430664, 0.7282858490943909, 0.7284867763519287, 0.7250900268554688, 0.7190656661987305, 0.7202128767967224, 0.7207155823707581, 0.7171581983566284, 0.7154147624969482, 0.714781641960144, 0.7163099050521851, 0.7100474834442139, 0.7122260332107544, 0.7083438038825989, 0.708108127117157, 0.7058261036872864, 0.704135537147522, 0.7049245238304138, 0.701707661151886, 0.6997527480125427, 0.6992055773735046, 0.6968059539794922, 0.6955741047859192, 0.6945807933807373, 0.6920358538627625, 0.6908853650093079, 0.6930403113365173, 0.6874609589576721, 0.691655695438385, 0.6892411708831787, 0.6864615082740784, 0.6862480044364929, 0.6839802265167236, 0.6797844171524048, 0.6800782680511475, 0.6796121597290039, 0.6760877370834351, 0.676952600479126, 0.674019992351532, 0.6749202013015747, 0.6719400882720947, 0.6683863997459412, 0.6710333824157715, 0.670120358467102, 0.6707122325897217, 0.6662253141403198, 0.6639825105667114, 0.6636983752250671, 0.6601319313049316, 0.6603139042854309, 0.6619802117347717, 0.6600947976112366, 0.6576977372169495, 0.6565496921539307, 0.6580146551132202, 0.6622017025947571, 0.6569075584411621, 0.6542043089866638, 0.659160315990448, 0.6489067077636719, 0.6505153775215149, 0.650579035282135, 0.6506475210189819, 0.6469371914863586, 0.6504107117652893, 0.6475414633750916, 0.6452416777610779, 0.6436986923217773, 0.6402133703231812, 0.6450884938240051, 0.6438831686973572, 0.6393258571624756, 0.639560878276825, 0.6366599798202515, 0.639650821685791, 0.6381478905677795, 0.6377878189086914, 0.6366282105445862, 0.6338308453559875, 0.6361426115036011, 0.633733332157135, 0.6352863311767578, 0.6332090497016907, 0.6347517371177673, 0.630767822265625, 0.6320734024047852, 0.6329014897346497, 0.6294066309928894, 0.6284494996070862, 0.629860520362854, 0.6282627582550049, 0.62730473279953, 0.6273090243339539, 0.6269586086273193, 0.6238564848899841, 0.6235846877098083, 0.6204135417938232, 0.6238245368003845, 0.6206443309783936, 0.6206311583518982, 0.6211039423942566, 0.6232355237007141, 0.6167395710945129, 0.6190574765205383, 0.620317816734314, 0.6179401874542236, 0.6184455156326294, 0.6174635887145996, 0.6173797249794006, 0.6139211654663086, 0.6168206334114075, 0.6167017221450806, 0.6144773364067078, 0.6134026646614075, 0.6139997243881226, 0.6132274270057678, 0.6127450466156006, 0.6110822558403015, 0.6105685830116272, 0.6101722717285156, 0.6107649803161621, 0.6106568574905396, 0.608860969543457, 0.6096212267875671, 0.6086157560348511, 0.6072746515274048, 0.605548083782196, 0.6084750890731812, 0.6033379435539246, 0.6052953004837036, 0.6030584573745728, 0.6042836308479309, 0.5975489020347595, 0.6049511432647705, 0.6034135222434998, 0.598408579826355, 0.6002964973449707, 0.6016908288002014, 0.5996570587158203, 0.6005867719650269, 0.5955732464790344, 0.5995428562164307, 0.5970171093940735, 0.5975731015205383, 0.596984326839447, 0.5964341759681702, 0.5937104821205139, 0.5943881869316101, 0.5923858880996704, 0.5915701985359192, 0.5941761136054993, 0.5947929620742798], "accuracy_train_first": 0.5015742519149132, "model": "residualv5", "loss_std": [0.27982550859451294, 0.18136148154735565, 0.17475056648254395, 0.17523321509361267, 0.1746307611465454, 0.17586365342140198, 0.1755252480506897, 0.17055948078632355, 0.16952860355377197, 0.16614381968975067, 0.16405722498893738, 0.16372191905975342, 0.16130554676055908, 0.15756724774837494, 0.1609187126159668, 0.1596430391073227, 0.15489505231380463, 0.1577463299036026, 0.1497965306043625, 0.15204539895057678, 0.14820626378059387, 0.14867530763149261, 0.1481715887784958, 0.14684408903121948, 0.1478525549173355, 0.14441832900047302, 0.1442723274230957, 0.14289525151252747, 0.14466050267219543, 0.1418682187795639, 0.1424926072359085, 0.13932903110980988, 0.13925489783287048, 0.135761097073555, 0.13807818293571472, 0.1360418051481247, 0.13322028517723083, 0.13738660514354706, 0.13365311920642853, 0.1340683102607727, 0.13103340566158295, 0.1308242380619049, 0.13259325921535492, 0.1317574679851532, 0.13052044808864594, 0.1322774440050125, 0.12778005003929138, 0.1276114583015442, 0.12829890847206116, 0.1293410211801529, 0.12679842114448547, 0.12342369556427002, 0.12323904782533646, 0.12131092697381973, 0.12455535680055618, 0.12252642959356308, 0.12495424598455429, 0.12121643126010895, 0.11831863969564438, 0.11597879976034164, 0.12174440920352936, 0.12019973993301392, 0.12019652128219604, 0.11866497993469238, 0.11600463092327118, 0.1147080585360527, 0.11485573649406433, 0.1194748803973198, 0.11977335810661316, 0.11761952936649323, 0.11211786419153214, 0.11258956044912338, 0.1155829057097435, 0.11490477621555328, 0.1130073294043541, 0.1100928783416748, 0.11011936515569687, 0.11312877386808395, 0.11522816121578217, 0.11028052121400833, 0.11200988292694092, 0.11301052570343018, 0.11214933544397354, 0.10847083479166031, 0.10910142213106155, 0.1077614426612854, 0.11034416407346725, 0.10817186534404755, 0.1077181026339531, 0.10873740911483765, 0.10363057255744934, 0.10513848811388016, 0.10738544911146164, 0.10967360436916351, 0.10854368656873703, 0.10727568715810776, 0.109085813164711, 0.10636825859546661, 0.10673471540212631, 0.1039016842842102, 0.10289359837770462, 0.10563171654939651, 0.10455868393182755, 0.1047489196062088, 0.10604636371135712, 0.10384878516197205, 0.10218000411987305, 0.10013251006603241, 0.10511867702007294, 0.10256917774677277, 0.10407373309135437, 0.10311820358037949, 0.10203631967306137, 0.10241089761257172, 0.10185076296329498, 0.10352043807506561, 0.1017976626753807, 0.10104735195636749, 0.10279686748981476, 0.10213947296142578, 0.10265495628118515, 0.10160450637340546, 0.09742756932973862, 0.10001476854085922, 0.10562807321548462, 0.09792676568031311, 0.10206376016139984, 0.10127907246351242, 0.09912511706352234, 0.10190179944038391, 0.10160204768180847, 0.10247441381216049, 0.0991961807012558, 0.10053153336048126, 0.10280659049749374, 0.1013379767537117, 0.09915610402822495, 0.1006116047501564, 0.09923691302537918, 0.09536708146333694, 0.09963002055883408, 0.09712357074022293, 0.09663978964090347, 0.09716936200857162, 0.09733108431100845, 0.09645899385213852, 0.09760637581348419, 0.09858649969100952, 0.09995321184396744, 0.10043787211179733, 0.09727883338928223, 0.09768465906381607, 0.09984268993139267, 0.09852565824985504, 0.09753008186817169, 0.09725692868232727, 0.09685054421424866, 0.09610980749130249, 0.09617273509502411, 0.0978839099407196, 0.09824614971876144, 0.09480611979961395, 0.09756838530302048, 0.09434910118579865, 0.0958533063530922, 0.09490184485912323, 0.09504644572734833, 0.09594850987195969, 0.09620997309684753, 0.09589686244726181, 0.0965929627418518, 0.09228040277957916, 0.09632711112499237, 0.09400520473718643, 0.09466675668954849, 0.09518720209598541, 0.09481167793273926, 0.09562501311302185, 0.09595291316509247, 0.09341864287853241, 0.09121078252792358, 0.09514590352773666, 0.0953243225812912, 0.09349750727415085, 0.09377582371234894, 0.09539991617202759, 0.09810209274291992, 0.0958978533744812, 0.09326373785734177, 0.09318571537733078, 0.0923181101679802, 0.09221023321151733, 0.08985474705696106, 0.09371863305568695, 0.0905889943242073, 0.09512198716402054, 0.09040078520774841, 0.09401091188192368, 0.09081057459115982, 0.09225651621818542, 0.09138527512550354, 0.09524184465408325, 0.09256722033023834, 0.09218387305736542, 0.09424926340579987, 0.09102559834718704, 0.09222164005041122, 0.09259587526321411, 0.09064454585313797, 0.09163601696491241, 0.08952368050813675, 0.09086397290229797, 0.09378974139690399, 0.09145834296941757, 0.09271841496229172, 0.09333466738462448, 0.09060763567686081, 0.09085599333047867, 0.09262436628341675, 0.08867685496807098, 0.08849485963582993, 0.0938362181186676, 0.08999846875667572, 0.09173616766929626, 0.09100105613470078, 0.09251666814088821, 0.09187771379947662, 0.09194888919591904, 0.08931175619363785, 0.09165753424167633, 0.08819731324911118, 0.09281598776578903, 0.09177716821432114, 0.08822526782751083, 0.0910520851612091, 0.08962133526802063, 0.0883944034576416, 0.09172367304563522, 0.08893679082393646, 0.09329405426979065, 0.09235506504774094, 0.08980920910835266, 0.09032094478607178, 0.0892629325389862, 0.0897083729505539, 0.08992014825344086, 0.08994054049253464, 0.09158535301685333, 0.09066188335418701, 0.08834826946258545]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:16 2016", "state": "available"}], "summary": "99af8cc971292d720fe9bce13eb29275"}