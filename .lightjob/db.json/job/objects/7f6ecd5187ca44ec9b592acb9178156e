{"content": {"hp_model": {"f0": 16, "f1": 16, "f2": 16, "f3": 64, "nonlin": "leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.7504695653915405, 1.4756453037261963, 1.362334966659546, 1.2759429216384888, 1.2040929794311523, 1.145267128944397, 1.0961072444915771, 1.0540437698364258, 1.0178282260894775, 0.9828707575798035, 0.950923502445221, 0.9208283424377441, 0.8929206132888794, 0.8668140769004822, 0.8421107530593872, 0.8208687901496887, 0.7968838214874268, 0.7781901955604553, 0.7594086527824402, 0.7396098971366882, 0.7220781445503235, 0.7069705724716187, 0.6896347403526306, 0.6747661232948303, 0.6605753898620605, 0.6451001763343811, 0.6315436363220215, 0.6192677617073059, 0.6075839996337891, 0.5963833332061768, 0.5860459208488464, 0.5739246606826782, 0.5632453560829163, 0.5517714023590088, 0.54228675365448, 0.5354948043823242, 0.5249536633491516, 0.5136449337005615, 0.5049278736114502, 0.4977922737598419, 0.49061644077301025, 0.480956107378006, 0.4742225408554077, 0.46911805868148804, 0.46021291613578796, 0.451678067445755, 0.4449458420276642, 0.4383937120437622, 0.43371686339378357, 0.4263378977775574, 0.42068856954574585, 0.41442522406578064, 0.4066898822784424, 0.40239423513412476, 0.3949769139289856, 0.3888542056083679, 0.38406530022621155, 0.3777090013027191, 0.3760083317756653, 0.3676029443740845, 0.3637595474720001, 0.35787060856819153, 0.3533450961112976, 0.34725967049598694, 0.3425796926021576, 0.3378441631793976, 0.33398669958114624, 0.3285875618457794, 0.32620078325271606, 0.3204846680164337, 0.3166702687740326, 0.31131628155708313, 0.30763253569602966, 0.30179089307785034, 0.29766225814819336, 0.2952665388584137, 0.28999224305152893, 0.287581205368042, 0.28232479095458984, 0.2781977951526642, 0.2752758264541626, 0.27011755108833313, 0.2673895061016083, 0.26506996154785156, 0.2620340585708618, 0.2565540373325348, 0.2545577585697174, 0.25122204422950745, 0.2492499202489853, 0.24490146338939667, 0.24137882888317108, 0.2375941425561905, 0.23498408496379852, 0.23286497592926025, 0.23035043478012085, 0.226745143532753, 0.2244090586900711, 0.21982143819332123, 0.21602089703083038, 0.21522164344787598, 0.21215829253196716, 0.20893245935440063, 0.20654135942459106, 0.20378561317920685, 0.2036139965057373, 0.1994691640138626, 0.19693440198898315, 0.19598443806171417, 0.19032278656959534, 0.19104574620723724, 0.18708674609661102, 0.18739821016788483, 0.18440969288349152, 0.18164336681365967, 0.17935718595981598, 0.1772029995918274, 0.17703789472579956, 0.1758374124765396, 0.17043177783489227, 0.1697472780942917, 0.1683938205242157, 0.16631144285202026, 0.1637435108423233, 0.16143661737442017, 0.15870793163776398, 0.15803727507591248, 0.15690620243549347, 0.15473385155200958, 0.15071094036102295, 0.15169988572597504, 0.14953070878982544, 0.14830027520656586, 0.14511696994304657, 0.14256376028060913, 0.14327014982700348, 0.13979020714759827, 0.14036132395267487, 0.1380140483379364, 0.1385434865951538, 0.13601231575012207, 0.1344117373228073, 0.13108550012111664, 0.13118287920951843, 0.13022810220718384, 0.12810386717319489, 0.1279778629541397, 0.12724816799163818, 0.12667308747768402, 0.1245788037776947, 0.12204798310995102, 0.12337076663970947, 0.12003107368946075, 0.11850172281265259, 0.11600935459136963, 0.1159784346818924, 0.11454284936189651, 0.11333229392766953, 0.11297061294317245, 0.11156808584928513, 0.10960999876260757, 0.10791802406311035, 0.10768725723028183, 0.1057916209101677, 0.10649476945400238, 0.10485590249300003, 0.10465152561664581, 0.1039784625172615, 0.10267920792102814, 0.10098780691623688, 0.09948764741420746, 0.09833627194166183, 0.09871605038642883, 0.09578593820333481, 0.09615974128246307, 0.09491340070962906, 0.09631501138210297, 0.0953902155160904, 0.09383664280176163, 0.09316502511501312, 0.09082093089818954, 0.09046370536088943, 0.08887272328138351, 0.08764293789863586, 0.0887879803776741, 0.08708697557449341, 0.0857660323381424, 0.0862036943435669, 0.0849989503622055, 0.08597645908594131, 0.08480816334486008, 0.0839357003569603, 0.08185240626335144, 0.082676462829113, 0.08020639419555664, 0.07888031005859375, 0.0806918665766716, 0.07995402812957764, 0.0788770541548729, 0.07886897027492523, 0.07704342156648636, 0.07577440142631531, 0.07540001720190048, 0.07569286227226257, 0.0743502676486969, 0.0755983293056488, 0.07395612448453903, 0.07366909831762314, 0.07259142398834229, 0.07190229743719101, 0.07132615894079208, 0.07128649950027466, 0.0696697011590004, 0.07021033763885498, 0.0691516175866127, 0.06938031315803528, 0.06694896519184113, 0.06832621991634369, 0.0665510818362236, 0.06613226979970932, 0.06583816558122635, 0.06584015488624573, 0.0670069009065628, 0.06481800973415375, 0.0649484321475029, 0.06478215008974075, 0.06324194371700287, 0.062487248331308365, 0.06339914351701736, 0.06044483929872513, 0.06314558535814285, 0.061811864376068115, 0.06166557967662811, 0.0606035552918911, 0.058845315128564835, 0.05916421487927437, 0.05918550863862038, 0.05989443510770798, 0.0592912882566452, 0.05844555422663689, 0.057053837925195694, 0.05654601752758026, 0.05719100683927536, 0.05708145722746849, 0.05600963532924652, 0.0557202473282814, 0.054381534457206726, 0.055696818977594376, 0.05445873364806175, 0.05591825023293495, 0.053899552673101425, 0.054380618035793304, 0.053901683539152145, 0.053310979157686234, 0.054978448897600174, 0.05214420706033707], "moving_avg_accuracy_train": [0.03923058823529411, 0.07933576470588233, 0.12055983529411761, 0.16042385176470583, 0.20117440776470583, 0.23960990816470584, 0.27641597617176467, 0.3124779079663529, 0.3446442348167764, 0.3753045172174517, 0.4044658302015889, 0.43186395306378295, 0.45684932246328697, 0.48199497845225237, 0.5042354806070272, 0.5260872266639716, 0.5453537981152214, 0.5628725359507581, 0.5791429294145058, 0.597096871767173, 0.6124107140022204, 0.6279014073078806, 0.6408900901065043, 0.6544740222723245, 0.6662007376921509, 0.6774747815699946, 0.6857790681188776, 0.6953729260128723, 0.7066309275292322, 0.7169137171292502, 0.725551757181031, 0.7321495226393985, 0.7409086880225174, 0.7484507603967363, 0.7556550961217685, 0.7632801747448859, 0.7690086278586326, 0.7766018827198281, 0.7794381650360805, 0.7837061132383548, 0.7869284430909899, 0.7926591281936556, 0.7972990977272312, 0.8017809526603904, 0.807471092688469, 0.811493395184328, 0.8157769968423658, 0.8200345912757763, 0.8223864262658458, 0.8246677836392612, 0.8301398288047468, 0.8346011400419191, 0.8378680848612566, 0.8408342175516016, 0.8448378546199709, 0.8483281868050325, 0.8522741916539411, 0.8555197136650176, 0.8588971540632216, 0.8609839092451348, 0.8632290477323861, 0.8660426135473828, 0.8683536463102914, 0.8708900463851447, 0.8725304535113361, 0.8743903493366731, 0.8778101379324176, 0.8792173594332935, 0.8817073881958465, 0.8840731199644971, 0.8880305138504003, 0.889530991877125, 0.8911990691600008, 0.8912509269498831, 0.892671716607836, 0.8945504272999936, 0.8963706786876412, 0.8980112578777005, 0.8995607203252246, 0.9011881777044669, 0.9020340658163731, 0.9046118357053241, 0.9049906521347917, 0.9059833516271949, 0.9068532517585931, 0.9080996912886162, 0.9096544280421075, 0.9121595734731909, 0.9125930278905777, 0.9135454898074022, 0.9132285878854856, 0.9130421996851723, 0.9128885679519492, 0.9132161817449895, 0.9140922106293141, 0.9117112248605004, 0.9128859847273916, 0.9141126803722994, 0.9154167064527166, 0.9170350358074449, 0.9185221204619946, 0.9180016731216775, 0.9192109175742156, 0.9192874728756175, 0.9201563726468793, 0.920778382441015, 0.9208817206675017, 0.9216970780125163, 0.9228050172700881, 0.9242915743666088, 0.9232577110475949, 0.9249766458251883, 0.9236719224191401, 0.9249635537066379, 0.9254036689242093, 0.927486831443553, 0.9299216777109625, 0.9300659805281015, 0.9281040883576444, 0.930008973639527, 0.9306174880402801, 0.932565151000958, 0.9313086359008622, 0.9322036546637172, 0.931585642138522, 0.9310388426305521, 0.9311537818969087, 0.9300689919425119, 0.9303891515717901, 0.9301572952381405, 0.9294309774790324, 0.9306808209075997, 0.931784503522722, 0.9330342884645674, 0.9342814478534047, 0.9348933030680643, 0.9339780904083167, 0.9358179284263086, 0.9355396649954425, 0.9372233455547218, 0.9393127757051319, 0.9395556157816776, 0.9400047600858628, 0.9391219311361, 0.93900973802249, 0.938308764220241, 0.9367390642688052, 0.937726334312513, 0.9377631126459676, 0.9384456249107825, 0.9395892977138219, 0.9413197797071455, 0.9441619193834898, 0.9435974921510232, 0.9448965664653326, 0.9435127921717406, 0.9420979835428018, 0.9411564204826394, 0.9412925431402578, 0.941563288826232, 0.9430022540612558, 0.9453279110080713, 0.9459574728484407, 0.9462534902694789, 0.9473057883013546, 0.9470481506476897, 0.9473409826417443, 0.9462562961422757, 0.9475530194692245, 0.9483459528164198, 0.9492901810641895, 0.9497470453107117, 0.9499535172502288, 0.9501016949369707, 0.9515762313256266, 0.9467339023107111, 0.9470040414914046, 0.9462754020481464, 0.9476972736080376, 0.9477393109531163, 0.948167732798981, 0.9493133124602593, 0.9504408047436451, 0.9520602536810453, 0.9519036400776466, 0.9531062172463526, 0.954419124933482, 0.9541136830283691, 0.9533281970784733, 0.9537200832529789, 0.954503369045328, 0.9535989144937363, 0.9534225524561274, 0.9558426501516911, 0.9575501498424043, 0.9588398407405168, 0.9588170331370534, 0.9600906239409951, 0.9598109733116015, 0.9611757583333825, 0.9617781825000442, 0.9618191877794515, 0.9628396219426828, 0.9609838950425321, 0.9626596231853377, 0.963388954984451, 0.9636924124271823, 0.9638384653021111, 0.9646640305366059, 0.96522115689471, 0.9655955117934742, 0.9650030194376562, 0.9647238939644789, 0.9644044457445016, 0.9641592952876985, 0.9646633657589286, 0.9660864409477417, 0.9669766203823793, 0.9651048406970826, 0.9654202389803155, 0.9653793915528721, 0.9661990994564085, 0.9661321306872381, 0.9652930352655731, 0.9664484376213687, 0.9669188879768789, 0.9660105285909557, 0.9663577110259778, 0.9677854693351446, 0.9680704518133949, 0.968496347808526, 0.9691714189100263, 0.9685036887837295, 0.9689450846112388, 0.9689823408559972, 0.968721753829221, 0.9688636960933578, 0.969822032366375, 0.9698210056003257, 0.9682553756285284, 0.9693145439480284, 0.9696772072002844, 0.9699730158920206, 0.970258067243995, 0.9710863781666542, 0.9717730344676359, 0.965414554550284, 0.9661436873305498, 0.9667316715386712, 0.9676373279142159, 0.9678547715933825, 0.9682198826693383, 0.9685390708729927, 0.9697792814327523, 0.9699072356424182], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.038773333333333326, 0.07850933333333332, 0.11964506666666666, 0.15858722666666664, 0.19794183733333331, 0.23537432026666663, 0.27058355490666663, 0.3046185327493333, 0.33475667947439997, 0.36324101152696, 0.3906502437075973, 0.41671855267017094, 0.43998003073648717, 0.4634086943295051, 0.4840011582298879, 0.5041477090735658, 0.5222529381662092, 0.538494311016255, 0.5536182132479628, 0.5699630585898332, 0.5840200860641832, 0.5982447441244315, 0.6101002697119884, 0.6228235760741229, 0.6337812184667105, 0.6440030966200395, 0.6514027869580356, 0.6600358415955654, 0.6703789241026755, 0.6795543650257413, 0.6869589285231672, 0.6924763690041839, 0.7002020654370988, 0.7062885255600556, 0.7123930063373833, 0.7187403723703117, 0.7235063351332807, 0.7297957016199526, 0.731842798124624, 0.7346985183121617, 0.7368819998142789, 0.7415937998328511, 0.7453810865162326, 0.748469644531276, 0.7529026800781484, 0.7555990787370003, 0.7587725041966336, 0.7616952537769702, 0.7634723950659398, 0.7644718222260125, 0.7687179733367445, 0.77184617600307, 0.7741948917360963, 0.77614873589582, 0.7784938623062381, 0.7807644760756143, 0.7837413618013862, 0.785780558954581, 0.7878558363924563, 0.7891102527532107, 0.7904658941445564, 0.792232638063434, 0.793382707590424, 0.7946711034980481, 0.7955639931482433, 0.7964075938334191, 0.7987401677834105, 0.7997328176717361, 0.8010795359045625, 0.8024915823141063, 0.805055757416029, 0.8058301816744261, 0.8069538301736501, 0.8065917804896183, 0.8071459357739899, 0.8075380088632576, 0.808797541310265, 0.8093311205125719, 0.8101846751279814, 0.8111662076151832, 0.8111029201869983, 0.8127126281682985, 0.812734698684802, 0.8132212288163218, 0.8133924392680228, 0.8136265286745539, 0.8143838758070985, 0.8158654882263886, 0.8158122727370831, 0.8160043787967082, 0.8156172742503708, 0.814708880158667, 0.8144379921428004, 0.8146741929285204, 0.8151001069690016, 0.8128700962721015, 0.8131164199782247, 0.8136447779804022, 0.814000300182362, 0.8148136034974592, 0.8157855764810465, 0.8150736854996086, 0.816299650282981, 0.8163230185880163, 0.8162507167292146, 0.8166523117229598, 0.8168804138839971, 0.8174723724955975, 0.8182451352460378, 0.8187006217214341, 0.8180305595492907, 0.8188541702610282, 0.817395419901592, 0.8182292112447661, 0.8179929567869562, 0.8191269944415939, 0.8202142949974345, 0.8201395321643576, 0.8185122456145885, 0.8194210210531295, 0.8194389189478166, 0.8205350270530349, 0.8196815243477316, 0.8203000385796251, 0.8197367013883293, 0.8189363645828297, 0.8182560614578801, 0.817110455312092, 0.8176127431142162, 0.8173714688027947, 0.8169409885891818, 0.8174868897302636, 0.8177115340905706, 0.8182603806815135, 0.8187410092800288, 0.819026908352026, 0.81827088418349, 0.819443795765141, 0.8190460828552936, 0.8198748079030975, 0.8207673271127878, 0.8212505944015089, 0.8211922016280246, 0.8203663147985555, 0.8202363499853667, 0.8201860483201634, 0.8187674434881471, 0.8194106991393324, 0.8192429625587325, 0.8197986663028592, 0.8200587996725733, 0.8210662530386493, 0.8229462944014511, 0.8222116649613059, 0.8230304984651754, 0.8215941152853246, 0.8198613704234587, 0.8195552333811129, 0.8195330433763349, 0.8197530723720348, 0.820884431801498, 0.8222226552880149, 0.8225337230925467, 0.822720350783292, 0.8229283157049628, 0.8224888174677999, 0.8225732690543532, 0.821515942148918, 0.8223776812673594, 0.8228065798072902, 0.8235259218265611, 0.824173329643905, 0.8245426633461812, 0.8241683970115631, 0.8252715573104068, 0.8216644015793662, 0.8218312947547628, 0.8211414986126199, 0.8222273487513578, 0.8222979472095554, 0.8225748191552665, 0.8233040039064066, 0.8245469368490992, 0.8251855764975227, 0.8257070188477704, 0.8263629836296601, 0.8268600186000273, 0.8267606834066912, 0.826204615066022, 0.8262508202260865, 0.8268124048701445, 0.8259978310497967, 0.8259313812781505, 0.8277115764836688, 0.8287137521686352, 0.829615710285105, 0.8296541392565945, 0.8307153919976017, 0.8306305194645082, 0.8319274675180574, 0.8325880540995849, 0.8326359153562931, 0.8333456571539971, 0.8316510914385974, 0.8330993156280709, 0.8332693840652639, 0.8334091123254042, 0.8335882010928637, 0.8341893809835774, 0.8340904428852196, 0.8341480652633644, 0.833893258737028, 0.8333705995299919, 0.833206872910326, 0.8328195189526267, 0.8334442337240308, 0.8344998103516277, 0.8352231626497982, 0.8339408463848184, 0.8339067617463365, 0.8336494189050362, 0.8342311436811992, 0.8342213626464127, 0.8337192263817714, 0.8341739704102609, 0.8343832400359015, 0.8339849160323114, 0.8340264244290803, 0.8351437819861722, 0.8353760704542217, 0.8358384634087995, 0.8363079504012529, 0.8358371553611277, 0.8364667731583482, 0.8363800958425134, 0.8361154195915954, 0.8364372109657692, 0.8370734898691923, 0.8369394742156063, 0.835658860127379, 0.8362796407813078, 0.836531676703177, 0.8365585090328592, 0.8366359914629067, 0.8373190589832827, 0.8375871530849545, 0.8330417711097924, 0.8336175939988131, 0.8344025012655985, 0.8346955844723719, 0.8348260260251348, 0.8348500900892879, 0.8347784144136925, 0.8355139063056565, 0.8355758490084242], "moving_var_accuracy_train": [0.01385135147958477, 0.02694204294925951, 0.03954265461710781, 0.049890647437903, 0.059847053022894926, 0.06715793693959103, 0.07263432302489407, 0.07707505704522247, 0.07867960458813472, 0.07927212038132368, 0.0789983479178206, 0.07785442735338546, 0.07568740277431399, 0.07380939863292112, 0.07088021819449787, 0.06808968562668273, 0.06462152404338951, 0.06092152721720294, 0.05721190582666913, 0.05439181165802825, 0.051063254368224724, 0.04811658314321252, 0.04482327775648064, 0.04200165889860301, 0.03903913569958105, 0.036279158717858855, 0.03327189342184659, 0.030773083063273393, 0.028836458140227294, 0.026904434183828704, 0.024885532388871364, 0.02278875373137687, 0.02120038516211865, 0.01959229234718838, 0.018100185191620325, 0.01681344308853677, 0.015427435355370642, 0.01440360949431721, 0.013035649021282868, 0.011896022555870248, 0.010799870987395875, 0.01001545065436952, 0.009207669444385157, 0.008467685712723596, 0.007912316383303518, 0.007266695001286907, 0.006705168689640914, 0.006197795813911494, 0.005627796382904978, 0.005111858067801612, 0.004870161765659485, 0.004562275270687822, 0.004202104099692405, 0.0038610751779537605, 0.003619229648135367, 0.003366948452180531, 0.0031703921953709635, 0.0029481536939533043, 0.002756002257348765, 0.00251959295631706, 0.0023129994821277844, 0.002152944907272868, 0.001985718268426722, 0.001845046369641488, 0.0016847601525342753, 0.0015474170496108009, 0.0014979299310059753, 0.0013659593890781242, 0.0012851656393153834, 0.0012070192565946714, 0.0012272660282488625, 0.0011248023342021283, 0.0010373644371767304, 0.0009336521965324006, 0.000858454766148473, 0.0008043752743170707, 0.000753757582913464, 0.0007026053253318216, 0.0006539522976852248, 0.0006123946256079527, 0.0005575949033279363, 0.000561639491398581, 0.000506767059243834, 0.0004649594238594084, 0.0004252740176209263, 0.0003967291193768703, 0.00037881106479309374, 0.0003974117409916851, 0.00035936151148008565, 0.0003315900136590874, 0.0002993348537462091, 0.0002697140334225323, 0.00024295505446535735, 0.0002196255261953344, 0.00020456981303133959, 0.0002351346708098485, 0.00022404175043258637, 0.000215180615236452, 0.0002089669098784791, 0.00021164112799401024, 0.0002103798021227819, 0.000191779610816892, 0.0001857620990491512, 0.0001672386355717908, 0.0001573096533271006, 0.00014506075365039594, 0.00013065078738683719, 0.00012356897704877577, 0.00012225984393011906, 0.0001299225275480489, 0.0001265501350548643, 0.0001404877524759601, 0.00014175970572497535, 0.0001425985375980657, 0.00013008199648090134, 0.0001561298915707996, 0.00019387318952697734, 0.000174673280301588, 0.00019184714026794112, 0.00020531971767534464, 0.00018812035389112635, 0.00020344883757758217, 0.00019731342559074276, 0.00018479161030443022, 0.00016974990460567118, 0.00015546582146234872, 0.00014003813863066908, 0.00013662524797404305, 0.00012388524287061482, 0.00011198053481863427, 0.000105530318721534, 0.00010903626421277757, 0.00010909567562581033, 0.000112243769671002, 0.00011501805157438894, 0.00010688554765030456, 0.00010373552079833459, 0.00012382700411053485, 0.000112141178532098, 0.00012644008271014306, 0.00015308753962011654, 0.00013830952738309505, 0.00012629415009862355, 0.00012067921767961395, 0.00010872458156432613, 0.00010227440185084824, 0.00011422258310360328, 0.0001115726440460667, 0.00010042755345376531, 9.457720503299496e-05, 9.68913718534034e-05, 0.00011415334603102082, 0.00017543783288657142, 0.00016076125250466368, 0.00015987347392108433, 0.0001611196081894325, 0.00016302279847916538, 0.0001546993875976126, 0.00013939621323910527, 0.00012611632095345761, 0.0001321402773865773, 0.00016760437175636426, 0.00015441106757837093, 0.00013975859764255727, 0.00013574871820930566, 0.00012277124083364852, 0.00011126587194096142, 0.00011072818796602919, 0.00011478879164930717, 0.00010896860212222395, 0.0001060958447649792, 9.736478474623417e-05, 8.80119822278822e-05, 7.940839364672739e-05, 9.103587233528717e-05, 0.0002929656377000003, 0.00026432585052251254, 0.00024267150441470458, 0.0002365998225686822, 0.0002129557445572453, 0.000193312077603649, 0.00018579204468629603, 0.00017865398985951786, 0.0001843921246211856, 0.0001661736625459926, 0.00016257202291162826, 0.00016182835997477786, 0.00014648517679389112, 0.00013738955271185428, 0.0001250327704045867, 0.00011805132305659276, 0.0001136085330739866, 0.00010252761188137409, 0.00014498670639789246, 0.00015672803250217575, 0.00015602495276602663, 0.00014042713917040568, 0.0001409827270763295, 0.00012758829463937883, 0.00013159320857654135, 0.0001217001216080901, 0.00010954524234373457, 0.0001079622910427676, 0.00012815956288997487, 0.0001406161898782941, 0.0001313418947492451, 0.00011903648305026216, 0.00010732481772571072, 0.0001027263575607972, 9.524722981476674e-05, 8.698378114534951e-05, 8.14448277561395e-05, 7.400154424851408e-05, 6.751981431088267e-05, 6.130872159803133e-05, 5.7464632797923826e-05, 6.994445645526827e-05, 7.008178564240676e-05, 9.460563979077286e-05, 8.604036050529201e-05, 7.745134106572144e-05, 7.575349638322836e-05, 6.821851008930314e-05, 6.773338922030515e-05, 7.297464173227754e-05, 6.766908939204675e-05, 6.832823141879483e-05, 6.258022906560607e-05, 7.466865026360236e-05, 6.793272035342895e-05, 6.277193490610444e-05, 6.0596230344222016e-05, 5.8549379003878226e-05, 5.444791359237484e-05, 4.901561448309889e-05, 4.47252034215055e-05, 4.04340115364893e-05, 4.465628609246425e-05, 4.0190666971454506e-05, 5.8232375151618484e-05, 6.250567539775076e-05, 5.7438829568807644e-05, 5.2482471650887396e-05, 4.796551294516065e-05, 4.934385251201535e-05, 4.865293914191387e-05, 0.00040766004696202033, 0.0003716787537671404, 0.00033762240725142875, 0.00031124208776136777, 0.00028054341476771677, 0.000253688828171016, 0.00022923687533808347, 0.00022015628789712695, 0.00019828800962535539], "duration": 119368.416967, "accuracy_train": [0.39230588235294117, 0.44028235294117646, 0.4915764705882353, 0.5192, 0.5679294117647059, 0.5855294117647059, 0.6076705882352941, 0.637035294117647, 0.6341411764705882, 0.6512470588235294, 0.6669176470588235, 0.6784470588235294, 0.6817176470588235, 0.7083058823529412, 0.7044, 0.7227529411764706, 0.7187529411764706, 0.7205411764705882, 0.7255764705882353, 0.7586823529411765, 0.7502352941176471, 0.7673176470588235, 0.7577882352941177, 0.7767294117647059, 0.7717411764705883, 0.7789411764705882, 0.7605176470588235, 0.7817176470588235, 0.8079529411764705, 0.8094588235294118, 0.8032941176470588, 0.7915294117647059, 0.8197411764705882, 0.8163294117647059, 0.8204941176470588, 0.8319058823529412, 0.820564705882353, 0.8449411764705882, 0.8049647058823529, 0.8221176470588235, 0.8159294117647059, 0.8442352941176471, 0.8390588235294117, 0.8421176470588235, 0.8586823529411765, 0.8476941176470588, 0.8543294117647059, 0.8583529411764705, 0.8435529411764706, 0.8452, 0.8793882352941177, 0.8747529411764706, 0.8672705882352941, 0.8675294117647059, 0.8808705882352941, 0.8797411764705882, 0.8877882352941177, 0.8847294117647059, 0.8892941176470588, 0.8797647058823529, 0.8834352941176471, 0.891364705882353, 0.8891529411764706, 0.8937176470588235, 0.8872941176470588, 0.8911294117647058, 0.9085882352941177, 0.8918823529411765, 0.9041176470588236, 0.905364705882353, 0.9236470588235294, 0.903035294117647, 0.9062117647058824, 0.8917176470588235, 0.9054588235294118, 0.9114588235294118, 0.9127529411764705, 0.9127764705882353, 0.9135058823529412, 0.9158352941176471, 0.9096470588235294, 0.9278117647058823, 0.9084, 0.9149176470588235, 0.9146823529411765, 0.9193176470588236, 0.9236470588235294, 0.9347058823529412, 0.9164941176470588, 0.9221176470588235, 0.9103764705882353, 0.911364705882353, 0.9115058823529412, 0.916164705882353, 0.9219764705882353, 0.8902823529411765, 0.9234588235294118, 0.9251529411764706, 0.9271529411764706, 0.9316, 0.9319058823529411, 0.9133176470588236, 0.9300941176470588, 0.9199764705882353, 0.9279764705882353, 0.9263764705882352, 0.9218117647058823, 0.9290352941176471, 0.9327764705882353, 0.9376705882352941, 0.9139529411764706, 0.9404470588235294, 0.9119294117647059, 0.9365882352941176, 0.929364705882353, 0.9462352941176471, 0.9518352941176471, 0.931364705882353, 0.9104470588235294, 0.9471529411764706, 0.9360941176470589, 0.9500941176470589, 0.92, 0.9402588235294118, 0.9260235294117647, 0.9261176470588235, 0.9321882352941177, 0.9203058823529412, 0.9332705882352941, 0.9280705882352941, 0.9228941176470589, 0.9419294117647059, 0.9417176470588235, 0.9442823529411765, 0.9455058823529412, 0.9404, 0.9257411764705883, 0.9523764705882353, 0.9330352941176471, 0.9523764705882353, 0.9581176470588235, 0.9417411764705882, 0.9440470588235295, 0.9311764705882353, 0.938, 0.932, 0.9226117647058824, 0.9466117647058824, 0.9380941176470589, 0.9445882352941176, 0.9498823529411765, 0.9568941176470588, 0.9697411764705882, 0.9385176470588236, 0.9565882352941176, 0.9310588235294117, 0.929364705882353, 0.9326823529411765, 0.9425176470588236, 0.944, 0.9559529411764706, 0.9662588235294117, 0.9516235294117648, 0.9489176470588235, 0.9567764705882353, 0.9447294117647059, 0.9499764705882353, 0.9364941176470588, 0.9592235294117647, 0.9554823529411764, 0.9577882352941176, 0.9538588235294118, 0.9518117647058824, 0.951435294117647, 0.9648470588235294, 0.9031529411764706, 0.949435294117647, 0.9397176470588235, 0.9604941176470588, 0.9481176470588235, 0.9520235294117647, 0.9596235294117647, 0.9605882352941176, 0.966635294117647, 0.9504941176470588, 0.9639294117647059, 0.9662352941176471, 0.9513647058823529, 0.9462588235294118, 0.9572470588235295, 0.9615529411764706, 0.9454588235294118, 0.9518352941176471, 0.9776235294117647, 0.9729176470588236, 0.9704470588235294, 0.9586117647058824, 0.9715529411764706, 0.9572941176470589, 0.9734588235294117, 0.9672, 0.9621882352941177, 0.9720235294117647, 0.9442823529411765, 0.9777411764705882, 0.9699529411764706, 0.9664235294117647, 0.9651529411764705, 0.9720941176470588, 0.9702352941176471, 0.968964705882353, 0.9596705882352942, 0.9622117647058823, 0.9615294117647059, 0.9619529411764706, 0.9692, 0.9788941176470588, 0.9749882352941176, 0.9482588235294117, 0.9682588235294117, 0.9650117647058823, 0.9735764705882353, 0.9655294117647059, 0.9577411764705882, 0.9768470588235294, 0.9711529411764706, 0.957835294117647, 0.9694823529411765, 0.980635294117647, 0.970635294117647, 0.9723294117647059, 0.9752470588235295, 0.9624941176470588, 0.9729176470588236, 0.9693176470588235, 0.9663764705882353, 0.9701411764705883, 0.9784470588235294, 0.9698117647058824, 0.9541647058823529, 0.9788470588235294, 0.9729411764705882, 0.972635294117647, 0.9728235294117648, 0.9785411764705882, 0.9779529411764706, 0.9081882352941176, 0.9727058823529412, 0.9720235294117647, 0.9757882352941176, 0.9698117647058824, 0.9715058823529412, 0.9714117647058823, 0.9809411764705882, 0.9710588235294118], "end": "2016-02-05 17:44:45.695000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0, 211.0, 212.0, 213.0, 214.0, 215.0, 216.0, 217.0, 218.0, 219.0, 220.0, 221.0, 222.0, 223.0, 224.0, 225.0, 226.0, 227.0, 228.0, 229.0, 230.0, 231.0, 232.0, 233.0, 234.0, 235.0, 236.0, 237.0, 238.0, 239.0, 240.0, 241.0, 242.0, 243.0, 244.0, 245.0, 246.0, 247.0, 248.0, 249.0, 250.0, 251.0, 252.0, 253.0, 254.0], "moving_var_accuracy_valid": [0.013530342399999997, 0.026387855423999997, 0.038978406893439996, 0.048728992633286394, 0.05779516179648198, 0.0646263626238224, 0.06932093819685135, 0.07281426182792351, 0.07370760663732599, 0.07363906052591787, 0.07303654855191485, 0.07184890428623712, 0.06953388111408082, 0.06752061350246598, 0.0645849982776165, 0.06177945004792693, 0.058551698927608156, 0.055070568763335166, 0.05162210365542996, 0.04886427901313393, 0.045756251304552215, 0.04300169424647588, 0.03996650620444323, 0.03742679830706149, 0.03476474781758986, 0.03222865417266441, 0.02949858750928206, 0.027219495449725024, 0.025460360106492507, 0.02367202254103729, 0.021798268332220264, 0.019892420844152287, 0.01844035622809905, 0.016929725576744235, 0.01557213518911669, 0.014377523170208772, 0.013144200462709943, 0.012185785593671998, 0.01100492247119974, 0.009977826464185354, 0.009022952140997608, 0.008320466461632993, 0.007617511679268767, 0.00694161322685249, 0.00642431814160575, 0.0058473214189922965, 0.0053532249394237065, 0.004894784631465558, 0.004433730248767648, 0.0039993469157255015, 0.0037616804174494933, 0.003473583242999001, 0.0031758731090501895, 0.002892643361149548, 0.0026528755859621528, 0.0024339892093730646, 0.0022703469260544986, 0.002080737158715426, 0.001911424430841269, 0.0017344440314122963, 0.0015775395005084313, 0.0014478780071316103, 0.0013149941456706467, 0.001198434407236629, 0.0010857662338597974, 0.0009835945695180772, 0.0009342032236558768, 0.0008496510854974249, 0.0007810088269353258, 0.0007208528198061418, 0.0007079424834054107, 0.0006425458314528147, 0.0005896545218558098, 0.0005318687894335957, 0.00048144570320300786, 0.0004346846246486582, 0.0004054939600493767, 0.00036750692493064854, 0.00033731323177096494, 0.00031225256280476243, 0.0002810633542113826, 0.0002762774568557978, 0.0002486540951395066, 0.000225919089745446, 0.00020359099793984685, 0.00018372507879811268, 0.0001705147430308634, 0.0001732198469767306, 0.00015592334927377579, 0.00014066315698970007, 0.00012794549065888643, 0.00012257756002557825, 0.00011098022687728223, 0.00010038432149012652, 9.19785142700258e-05, 0.00012753719221762576, 0.00011532955130964754, 0.00010630905578486807, 9.681571453115824e-05, 9.308730361917427e-05, 9.228115658467125e-05, 8.761413985127884e-05, 9.237963271677678e-05, 8.314658414422108e-05, 7.48789737588745e-05, 6.8842583233998e-05, 6.242660027342737e-05, 5.933767522671425e-05, 5.8778368120254696e-05, 5.476774267164901e-05, 5.3331818235321954e-05, 5.4103647852190046e-05, 6.784485656736699e-05, 6.731724294619939e-05, 6.108786417109511e-05, 6.655345037321161e-05, 7.053810782447212e-05, 6.353460237291192e-05, 8.101369577115546e-05, 8.0345181373301e-05, 7.231354624767895e-05, 7.589526842783915e-05, 7.486194339669869e-05, 7.081878775252216e-05, 6.65930480971438e-05, 6.569859430756516e-05, 6.32940459531548e-05, 6.877636232924467e-05, 6.416936342178445e-05, 5.827634671977379e-05, 5.411653097660546e-05, 5.138695038145481e-05, 4.670244114086887e-05, 4.474329025028921e-05, 4.2347995872657417e-05, 3.884884079971099e-05, 4.010810961043388e-05, 4.847879285472978e-05, 4.5054493597190626e-05, 4.673011108119096e-05, 4.922641483006677e-05, 4.640569879819177e-05, 4.179581636232922e-05, 4.3755036221911515e-05, 3.953155027372535e-05, 3.560116756405287e-05, 5.0153007832428505e-05, 4.8861707544222e-05, 4.4228756834041954e-05, 4.258514101176612e-05, 3.893565124093871e-05, 4.4176746680205505e-05, 7.157007174479349e-05, 6.927018829926535e-05, 6.837756423287025e-05, 8.010857756380989e-05, 9.911936261432942e-05, 9.005090535116302e-05, 8.105024638285514e-05, 7.338093657510774e-05, 7.756261034531436e-05, 8.592392820957134e-05, 7.82024039997606e-05, 7.069563265436097e-05, 6.401531406673475e-05, 5.9352210964285344e-05, 5.348117850209921e-05, 5.81945223165062e-05, 5.90584188591271e-05, 5.4808162591206714e-05, 5.3984422798285664e-05, 5.2358212456078835e-05, 4.83500576632041e-05, 4.477572949994006e-05, 5.125082035445026e-05, 0.00016322989053082186, 0.00014715758146568571, 0.00013672419177855483, 0.00013366340731487558, 0.00012034192386408685, 0.00010899765414657492, 0.00010288328234357349, 0.00010649889480949229, 9.951975073338843e-05, 9.201489478173651e-05, 8.668601345927841e-05, 8.024080596926307e-05, 7.230553269805279e-05, 6.785788742369801e-05, 6.109131293267747e-05, 5.78205774513856e-05, 5.8010294285410936e-05, 5.22490050062365e-05, 7.554595923336546e-05, 7.703056824187082e-05, 7.664926741247577e-05, 6.899763174387586e-05, 7.22341849921464e-05, 6.507559661479513e-05, 7.37067052357608e-05, 7.026340639743362e-05, 6.325768205673342e-05, 6.146551462573333e-05, 8.116293983743313e-05, 9.192282558047782e-05, 8.299085248239318e-05, 7.486748311429033e-05, 6.766938988253306e-05, 6.415520624326579e-05, 5.782778434469901e-05, 5.2074888956396575e-05, 4.745173735352971e-05, 4.516511743847354e-05, 4.0889863348511064e-05, 3.815126481056751e-05, 3.784855524000378e-05, 4.409187786656341e-05, 4.439183700532482e-05, 5.475166833567846e-05, 4.9286957365334546e-05, 4.495428967051775e-05, 4.350449414028355e-05, 3.9154905744028666e-05, 3.750868262403673e-05, 3.561894354465507e-05, 3.245119317613131e-05, 3.063403196504294e-05, 2.758613529155954e-05, 3.606391295591838e-05, 3.294314305182537e-05, 3.157309394663175e-05, 3.0399546876714864e-05, 2.9354423917302585e-05, 2.9986748660764215e-05, 2.7055690408410754e-05, 2.498060302776979e-05, 2.3414489921426764e-05, 2.471669851575559e-05, 2.2406670422834515e-05, 3.492575536724704e-05, 3.490149741315235e-05, 3.1983046625049337e-05, 2.879122172779002e-05, 2.596613129770549e-05, 2.756874930446848e-05, 2.5458744400182147e-05, 0.00020885734566132152, 0.00019095575909087133, 0.0001774048979388564, 0.0001604374880398045, 0.00014454687422400857, 0.00013009739851425984, 0.00011713389528508246, 0.0001102890406648784, 9.929466868422603e-05], "accuracy_test": 0.7527, "start": "2016-02-04 08:35:17.278000", "learning_rate_per_epoch": [0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857, 0.00014791381545364857], "accuracy_train_first": 0.39230588235294117, "accuracy_train_last": 0.9710588235294118, "batch_size_eval": 1024, "accuracy_train_std": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "accuracy_test_std": 0, "error_valid": [0.6122666666666667, 0.5638666666666667, 0.5101333333333333, 0.49093333333333333, 0.44786666666666664, 0.4277333333333333, 0.4125333333333333, 0.38906666666666667, 0.394, 0.38039999999999996, 0.3626666666666667, 0.3486666666666667, 0.3506666666666667, 0.3257333333333333, 0.33066666666666666, 0.31453333333333333, 0.31479999999999997, 0.31533333333333335, 0.3102666666666667, 0.28293333333333337, 0.28946666666666665, 0.2737333333333334, 0.2832, 0.2626666666666667, 0.26759999999999995, 0.264, 0.28200000000000003, 0.26226666666666665, 0.23653333333333337, 0.23786666666666667, 0.24639999999999995, 0.2578666666666667, 0.23026666666666662, 0.23893333333333333, 0.2326666666666667, 0.2241333333333333, 0.23360000000000003, 0.2136, 0.24973333333333336, 0.23960000000000004, 0.24346666666666672, 0.21599999999999997, 0.22053333333333336, 0.22373333333333334, 0.20720000000000005, 0.2201333333333333, 0.21266666666666667, 0.21199999999999997, 0.22053333333333336, 0.22653333333333336, 0.19306666666666672, 0.19999999999999996, 0.20466666666666666, 0.2062666666666667, 0.20040000000000002, 0.19879999999999998, 0.18946666666666667, 0.19586666666666663, 0.19346666666666668, 0.1996, 0.19733333333333336, 0.19186666666666663, 0.1962666666666667, 0.1937333333333333, 0.19640000000000002, 0.19599999999999995, 0.1802666666666667, 0.19133333333333336, 0.18679999999999997, 0.18479999999999996, 0.1718666666666666, 0.18720000000000003, 0.18293333333333328, 0.19666666666666666, 0.18786666666666663, 0.1889333333333333, 0.17986666666666662, 0.18586666666666662, 0.18213333333333337, 0.18000000000000005, 0.18946666666666667, 0.17279999999999995, 0.18706666666666671, 0.1824, 0.1850666666666667, 0.1842666666666667, 0.17879999999999996, 0.17079999999999995, 0.18466666666666665, 0.1822666666666667, 0.18786666666666663, 0.19346666666666668, 0.18799999999999994, 0.18320000000000003, 0.1810666666666667, 0.20720000000000005, 0.18466666666666665, 0.18159999999999998, 0.18279999999999996, 0.17786666666666662, 0.17546666666666666, 0.19133333333333336, 0.17266666666666663, 0.18346666666666667, 0.1844, 0.1797333333333333, 0.1810666666666667, 0.17720000000000002, 0.17479999999999996, 0.17720000000000002, 0.18799999999999994, 0.1737333333333333, 0.19573333333333331, 0.17426666666666668, 0.18413333333333337, 0.17066666666666663, 0.17000000000000004, 0.18053333333333332, 0.19613333333333338, 0.1724, 0.1804, 0.16959999999999997, 0.18799999999999994, 0.17413333333333336, 0.18533333333333335, 0.1882666666666667, 0.18786666666666663, 0.19320000000000004, 0.17786666666666662, 0.18479999999999996, 0.18693333333333328, 0.17759999999999998, 0.1802666666666667, 0.17679999999999996, 0.1769333333333334, 0.1784, 0.18853333333333333, 0.17000000000000004, 0.18453333333333333, 0.17266666666666663, 0.17120000000000002, 0.1744, 0.17933333333333334, 0.18706666666666671, 0.18093333333333328, 0.1802666666666667, 0.19399999999999995, 0.17479999999999996, 0.1822666666666667, 0.17520000000000002, 0.17759999999999998, 0.16986666666666672, 0.16013333333333335, 0.1844, 0.16959999999999997, 0.19133333333333336, 0.19573333333333331, 0.18320000000000003, 0.18066666666666664, 0.17826666666666668, 0.16893333333333338, 0.1657333333333333, 0.17466666666666664, 0.17559999999999998, 0.17520000000000002, 0.18146666666666667, 0.17666666666666664, 0.18799999999999994, 0.16986666666666672, 0.17333333333333334, 0.17000000000000004, 0.17000000000000004, 0.17213333333333336, 0.17920000000000003, 0.16479999999999995, 0.2108, 0.17666666666666664, 0.1850666666666667, 0.16800000000000004, 0.1770666666666667, 0.17493333333333339, 0.17013333333333336, 0.16426666666666667, 0.1690666666666667, 0.16959999999999997, 0.1677333333333333, 0.16866666666666663, 0.17413333333333336, 0.17879999999999996, 0.17333333333333334, 0.16813333333333336, 0.18133333333333335, 0.17466666666666664, 0.15626666666666666, 0.16226666666666667, 0.16226666666666667, 0.17000000000000004, 0.15973333333333328, 0.17013333333333336, 0.15639999999999998, 0.16146666666666665, 0.16693333333333338, 0.16026666666666667, 0.18359999999999999, 0.1538666666666667, 0.1652, 0.16533333333333333, 0.16479999999999995, 0.1604, 0.16679999999999995, 0.16533333333333333, 0.1684, 0.17133333333333334, 0.16826666666666668, 0.17066666666666663, 0.16093333333333337, 0.15600000000000003, 0.15826666666666667, 0.17759999999999998, 0.1664, 0.16866666666666663, 0.1605333333333333, 0.16586666666666672, 0.17079999999999995, 0.16173333333333328, 0.1637333333333333, 0.16959999999999997, 0.16559999999999997, 0.15480000000000005, 0.1625333333333333, 0.16000000000000003, 0.15946666666666665, 0.1684, 0.1578666666666667, 0.1644, 0.16626666666666667, 0.16066666666666662, 0.1572, 0.16426666666666667, 0.17586666666666662, 0.15813333333333335, 0.1612, 0.1632, 0.16266666666666663, 0.1565333333333333, 0.16000000000000003, 0.20786666666666664, 0.1612, 0.1585333333333333, 0.16266666666666663, 0.16400000000000003, 0.16493333333333338, 0.16586666666666672, 0.1578666666666667, 0.16386666666666672], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.05504216678567177, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 128, "valid_ratio": 0.15, "learning_rate": 0.00014791381611734225, "optimization": "rmsprop", "nb_data_augmentation": 3, "learning_rate_decay_method": "none", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 3.027523763309738e-07, "rotation_range": [0, 0], "momentum": 0.5138056428607813}, "accuracy_valid_max": 0.8461333333333333, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        y_pred = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            y_pred.extend((nnet.predict(X[mini_batch]) == y[mini_batch]).tolist())\n        return np.mean(y_pred)\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            acc = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = acc\n            status[\"accuracy_train_std\"] = 0\n            acc = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = acc\n            status[\"accuracy_valid_std\"] = 0\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n\n    # rescaling to [-1, 1]\n    X_min = X_train.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X_train.max(axis=(0, 2, 3))[None, :, None, None]\n    def preprocess(a):\n        return (a / 255.) * 2 - 1\n        # return 2 * ((a - X_min) / (X_max - X_min)) - 1\n    X_train = preprocess(X_train)\n    X_valid = preprocess(X_valid)\n\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = preprocess(X_test)\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    acc = evaluate(X_test, y_test, batch_size_eval)\n    light.set(\"accuracy_test\", acc)\n    light.set(\"accuracy_test_std\", 0)\n    print(\"Test accuracy : {}+-{}\".format(acc, 0))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8361333333333333, "accuracy_valid_std": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "accuracy_valid": [0.3877333333333333, 0.4361333333333333, 0.4898666666666667, 0.5090666666666667, 0.5521333333333334, 0.5722666666666667, 0.5874666666666667, 0.6109333333333333, 0.606, 0.6196, 0.6373333333333333, 0.6513333333333333, 0.6493333333333333, 0.6742666666666667, 0.6693333333333333, 0.6854666666666667, 0.6852, 0.6846666666666666, 0.6897333333333333, 0.7170666666666666, 0.7105333333333334, 0.7262666666666666, 0.7168, 0.7373333333333333, 0.7324, 0.736, 0.718, 0.7377333333333334, 0.7634666666666666, 0.7621333333333333, 0.7536, 0.7421333333333333, 0.7697333333333334, 0.7610666666666667, 0.7673333333333333, 0.7758666666666667, 0.7664, 0.7864, 0.7502666666666666, 0.7604, 0.7565333333333333, 0.784, 0.7794666666666666, 0.7762666666666667, 0.7928, 0.7798666666666667, 0.7873333333333333, 0.788, 0.7794666666666666, 0.7734666666666666, 0.8069333333333333, 0.8, 0.7953333333333333, 0.7937333333333333, 0.7996, 0.8012, 0.8105333333333333, 0.8041333333333334, 0.8065333333333333, 0.8004, 0.8026666666666666, 0.8081333333333334, 0.8037333333333333, 0.8062666666666667, 0.8036, 0.804, 0.8197333333333333, 0.8086666666666666, 0.8132, 0.8152, 0.8281333333333334, 0.8128, 0.8170666666666667, 0.8033333333333333, 0.8121333333333334, 0.8110666666666667, 0.8201333333333334, 0.8141333333333334, 0.8178666666666666, 0.82, 0.8105333333333333, 0.8272, 0.8129333333333333, 0.8176, 0.8149333333333333, 0.8157333333333333, 0.8212, 0.8292, 0.8153333333333334, 0.8177333333333333, 0.8121333333333334, 0.8065333333333333, 0.812, 0.8168, 0.8189333333333333, 0.7928, 0.8153333333333334, 0.8184, 0.8172, 0.8221333333333334, 0.8245333333333333, 0.8086666666666666, 0.8273333333333334, 0.8165333333333333, 0.8156, 0.8202666666666667, 0.8189333333333333, 0.8228, 0.8252, 0.8228, 0.812, 0.8262666666666667, 0.8042666666666667, 0.8257333333333333, 0.8158666666666666, 0.8293333333333334, 0.83, 0.8194666666666667, 0.8038666666666666, 0.8276, 0.8196, 0.8304, 0.812, 0.8258666666666666, 0.8146666666666667, 0.8117333333333333, 0.8121333333333334, 0.8068, 0.8221333333333334, 0.8152, 0.8130666666666667, 0.8224, 0.8197333333333333, 0.8232, 0.8230666666666666, 0.8216, 0.8114666666666667, 0.83, 0.8154666666666667, 0.8273333333333334, 0.8288, 0.8256, 0.8206666666666667, 0.8129333333333333, 0.8190666666666667, 0.8197333333333333, 0.806, 0.8252, 0.8177333333333333, 0.8248, 0.8224, 0.8301333333333333, 0.8398666666666667, 0.8156, 0.8304, 0.8086666666666666, 0.8042666666666667, 0.8168, 0.8193333333333334, 0.8217333333333333, 0.8310666666666666, 0.8342666666666667, 0.8253333333333334, 0.8244, 0.8248, 0.8185333333333333, 0.8233333333333334, 0.812, 0.8301333333333333, 0.8266666666666667, 0.83, 0.83, 0.8278666666666666, 0.8208, 0.8352, 0.7892, 0.8233333333333334, 0.8149333333333333, 0.832, 0.8229333333333333, 0.8250666666666666, 0.8298666666666666, 0.8357333333333333, 0.8309333333333333, 0.8304, 0.8322666666666667, 0.8313333333333334, 0.8258666666666666, 0.8212, 0.8266666666666667, 0.8318666666666666, 0.8186666666666667, 0.8253333333333334, 0.8437333333333333, 0.8377333333333333, 0.8377333333333333, 0.83, 0.8402666666666667, 0.8298666666666666, 0.8436, 0.8385333333333334, 0.8330666666666666, 0.8397333333333333, 0.8164, 0.8461333333333333, 0.8348, 0.8346666666666667, 0.8352, 0.8396, 0.8332, 0.8346666666666667, 0.8316, 0.8286666666666667, 0.8317333333333333, 0.8293333333333334, 0.8390666666666666, 0.844, 0.8417333333333333, 0.8224, 0.8336, 0.8313333333333334, 0.8394666666666667, 0.8341333333333333, 0.8292, 0.8382666666666667, 0.8362666666666667, 0.8304, 0.8344, 0.8452, 0.8374666666666667, 0.84, 0.8405333333333334, 0.8316, 0.8421333333333333, 0.8356, 0.8337333333333333, 0.8393333333333334, 0.8428, 0.8357333333333333, 0.8241333333333334, 0.8418666666666667, 0.8388, 0.8368, 0.8373333333333334, 0.8434666666666667, 0.84, 0.7921333333333334, 0.8388, 0.8414666666666667, 0.8373333333333334, 0.836, 0.8350666666666666, 0.8341333333333333, 0.8421333333333333, 0.8361333333333333], "seed": 333418319, "model": "residualv3", "loss_std": [0.28826913237571716, 0.08907391875982285, 0.08688615262508392, 0.08351415395736694, 0.08413906395435333, 0.08559583872556686, 0.084788478910923, 0.08366705477237701, 0.08317934721708298, 0.07968409359455109, 0.08074335008859634, 0.08041895180940628, 0.08098185807466507, 0.0825633779168129, 0.07972531020641327, 0.08037217706441879, 0.07938255369663239, 0.08059407025575638, 0.08010204881429672, 0.07949800044298172, 0.07799795269966125, 0.0800800621509552, 0.07694584131240845, 0.07817892730236053, 0.07774202525615692, 0.07670482993125916, 0.0751805305480957, 0.07738643139600754, 0.07690183073282242, 0.07715194672346115, 0.07735023647546768, 0.07340056449174881, 0.07552686333656311, 0.07162005454301834, 0.06981238722801208, 0.0720457062125206, 0.07001876085996628, 0.07266715914011002, 0.06911863386631012, 0.06906543672084808, 0.07064685225486755, 0.06909618526697159, 0.06589049100875854, 0.06909732520580292, 0.06587232649326324, 0.06544651091098785, 0.063859723508358, 0.06576093286275864, 0.06437494605779648, 0.0638427659869194, 0.06516976654529572, 0.06354781240224838, 0.0628126785159111, 0.06250971555709839, 0.06292683631181717, 0.060300592333078384, 0.060625430196523666, 0.06155187264084816, 0.06030360236763954, 0.05803228169679642, 0.059891801327466965, 0.059217847883701324, 0.058792952448129654, 0.05596273019909859, 0.056435294449329376, 0.05654175952076912, 0.056518569588661194, 0.056190118193626404, 0.054771799594163895, 0.05328144133090973, 0.05368831381201744, 0.05480865016579628, 0.05215505510568619, 0.05359471216797829, 0.05294010043144226, 0.05189312621951103, 0.051636796444654465, 0.052263230085372925, 0.05223214253783226, 0.05010513216257095, 0.05082092806696892, 0.049043282866477966, 0.04726352542638779, 0.048310741782188416, 0.04954184964299202, 0.048004668205976486, 0.0480317696928978, 0.0473320335149765, 0.047720812261104584, 0.047113414853811264, 0.047446783632040024, 0.04682626575231552, 0.0456874780356884, 0.042780447751283646, 0.04336899518966675, 0.04372710362076759, 0.04279414191842079, 0.044487979263067245, 0.04302801936864853, 0.04304716736078262, 0.04050922021269798, 0.04097536951303482, 0.041230496019124985, 0.038421567529439926, 0.04249562323093414, 0.03930027410387993, 0.0402090847492218, 0.0406557098031044, 0.03797027841210365, 0.0384187176823616, 0.037747953087091446, 0.039187632501125336, 0.04192866012454033, 0.037466950714588165, 0.03671104088425636, 0.03745795413851738, 0.03691637143492699, 0.037423957139253616, 0.03488488867878914, 0.03799714893102646, 0.03577783331274986, 0.03545112535357475, 0.034500546753406525, 0.034624047577381134, 0.03510792553424835, 0.03335822746157646, 0.0338112972676754, 0.03227396681904793, 0.03304517641663551, 0.03088446706533432, 0.03306134045124054, 0.03143338859081268, 0.03161243721842766, 0.031321100890636444, 0.03104349598288536, 0.03178510069847107, 0.03182370215654373, 0.030165838077664375, 0.03087601251900196, 0.03250054642558098, 0.031175274401903152, 0.030056629329919815, 0.02965349331498146, 0.02999032847583294, 0.02906438708305359, 0.030182482674717903, 0.029828930273652077, 0.029295476153492928, 0.027588147670030594, 0.027737464755773544, 0.027132032439112663, 0.028503507375717163, 0.027365827932953835, 0.027051139622926712, 0.028452325612306595, 0.02726368047297001, 0.02707700990140438, 0.027878640219569206, 0.026609783992171288, 0.025545664131641388, 0.025692207738757133, 0.026112215593457222, 0.025491993874311447, 0.024723587557673454, 0.025481611490249634, 0.02558368444442749, 0.026360251009464264, 0.026692254468798637, 0.02597188390791416, 0.024524791166186333, 0.024841465055942535, 0.02466554194688797, 0.023825811222195625, 0.02214805781841278, 0.025189818814396858, 0.021547304466366768, 0.02592884562909603, 0.02356569468975067, 0.024739570915699005, 0.023210864514112473, 0.02065938711166382, 0.021498464047908783, 0.021772054955363274, 0.021769830957055092, 0.02171243540942669, 0.022150935605168343, 0.022160764783620834, 0.023302413523197174, 0.022212175652384758, 0.02069377526640892, 0.023004719987511635, 0.021610863506793976, 0.021649101749062538, 0.021731438115239143, 0.021754203364253044, 0.02251979522407055, 0.021085819229483604, 0.01954841986298561, 0.020842324942350388, 0.01851555146276951, 0.019649004563689232, 0.019650235772132874, 0.020200662314891815, 0.018919751048088074, 0.020505543798208237, 0.02002459205687046, 0.020110853016376495, 0.01953539252281189, 0.020187795162200928, 0.019935742020606995, 0.01980137638747692, 0.017677990719676018, 0.01968657225370407, 0.02048417367041111, 0.018657375127077103, 0.019202863797545433, 0.017655830830335617, 0.017912840470671654, 0.018324503675103188, 0.01899278722703457, 0.01933625340461731, 0.017911922186613083, 0.01827727071940899, 0.018756521865725517, 0.018115505576133728, 0.01815321110188961, 0.017746079713106155, 0.01852024719119072, 0.017987797036767006, 0.018378714099526405, 0.017416760325431824, 0.017813153564929962, 0.01714773289859295, 0.0175407025963068, 0.01727481745183468, 0.018139518797397614, 0.01879870519042015, 0.01806337758898735, 0.017515243962407112, 0.018049191683530807, 0.016152918338775635, 0.016193484887480736, 0.01586775667965412, 0.01782253570854664, 0.017040308564901352, 0.015881996601819992, 0.01616615802049637, 0.015377289615571499, 0.016645686700940132, 0.016878943890333176, 0.016108786687254906, 0.0160013847053051, 0.01675214059650898, 0.016985202208161354, 0.016156446188688278]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:40 2016", "state": "available"}], "summary": "882780ef30ea1bc3fc57822e9e6ad5ff"}