{"content": {"hp_model": {"f0": 32, "f1": 32, "f2": 64, "f3": 64, "nonlin": "very_leaky_rectify", "nbg1": 2, "nbg3": 6, "nbg2": 2, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.5111727714538574, 1.0108686685562134, 0.7340242862701416, 0.6153218746185303, 0.542411744594574, 0.4912731647491455, 0.45090964436531067, 0.41897889971733093, 0.39487016201019287, 0.37314918637275696, 0.3549599349498749, 0.34019923210144043, 0.325381338596344, 0.31201592087745667, 0.3021860122680664, 0.2905524671077728, 0.2835538387298584, 0.27397724986076355, 0.26603963971138, 0.2590376138687134, 0.2547683119773865, 0.24756336212158203, 0.24086536467075348, 0.23674313724040985, 0.23041465878486633, 0.22755828499794006, 0.2206536829471588, 0.21627390384674072, 0.21369072794914246, 0.20992518961429596, 0.2054334133863449, 0.20269843935966492, 0.19944094121456146, 0.19619572162628174, 0.19246894121170044, 0.18997448682785034, 0.1875855028629303, 0.18605586886405945, 0.1817653328180313, 0.17957118153572083, 0.17644478380680084, 0.17562469840049744, 0.17331130802631378, 0.16986195743083954, 0.16721133887767792, 0.16676752269268036, 0.16534961760044098, 0.1615607887506485, 0.16095848381519318, 0.15940849483013153, 0.15823909640312195, 0.1545199155807495, 0.1529376059770584, 0.15200752019882202, 0.15044033527374268, 0.1487247198820114, 0.14802691340446472, 0.1462697833776474, 0.14507068693637848, 0.14268139004707336, 0.14164820313453674, 0.14088426530361176, 0.13896271586418152, 0.1382361352443695, 0.1373964101076126, 0.1359652280807495, 0.13453181087970734, 0.13431711494922638, 0.13273493945598602, 0.13135838508605957, 0.12921175360679626, 0.13059689104557037, 0.12887483835220337, 0.12807686626911163, 0.12520654499530792, 0.12622959911823273, 0.12317240238189697, 0.1232878565788269, 0.12251366674900055, 0.12150921672582626, 0.1197836697101593, 0.12054949998855591, 0.11935397237539291, 0.11838562786579132, 0.11778257042169571, 0.11549273133277893, 0.11452527344226837, 0.11477117240428925, 0.11476791650056839, 0.11352438479661942, 0.11214619129896164, 0.11163926124572754, 0.11219308525323868, 0.1103680357336998, 0.11029183119535446, 0.1091371700167656, 0.10785716027021408, 0.10806956887245178, 0.10728608071804047, 0.10626540333032608, 0.10573984682559967, 0.10526861250400543, 0.1034594252705574, 0.10402444750070572, 0.10434642434120178, 0.10235919803380966], "moving_avg_accuracy_train": [0.054430588235294106, 0.1180345882352941, 0.18286171764705877, 0.2461731929411764, 0.3062994030588234, 0.361674168635294, 0.4126620458894117, 0.4600546648298823, 0.5037856689351293, 0.5439035726298517, 0.5805202741903959, 0.6144211879478269, 0.6454049515059853, 0.6735162210612691, 0.6993881283669069, 0.7229057861184515, 0.7443093251536651, 0.7641089808735927, 0.7821145533744687, 0.7986230980370218, 0.8136407882333196, 0.82732847411587, 0.8400826855278124, 0.8515450052103253, 0.862046975277528, 0.8715528659850693, 0.8804046382100918, 0.8884441743890826, 0.8958515216560567, 0.9026569577257452, 0.9088265560708177, 0.9144897828166771, 0.9195678633585389, 0.9242675476109202, 0.9286854987321812, 0.9327298900354336, 0.9363910186789491, 0.9398742697522307, 0.9429127251299488, 0.9458496879110716, 0.9486247191199645, 0.9510963648550268, 0.9534055518989359, 0.9555967614149247, 0.9575970852734322, 0.9595079649813831, 0.961263050836186, 0.9629508633996262, 0.9645287182361342, 0.9659346699419326, 0.9672188500065628, 0.9684122591235536, 0.9695757390935511, 0.970618165184196, 0.971624583959894, 0.9725750667403752, 0.9733905012428082, 0.9742938040597039, 0.9750879530654982, 0.9758756283471838, 0.9766315949242301, 0.9772954942553366, 0.9779659448298028, 0.978611703287999, 0.9792187682533167, 0.9797510090750439, 0.9802770846381277, 0.980764670291962, 0.9812199679686482, 0.9816697358776657, 0.9821004093487227, 0.9825068390020857, 0.9828585080430536, 0.9831091278269836, 0.9835323326913441, 0.9838426288339744, 0.9840889541858712, 0.9843435881790488, 0.9846362881846733, 0.9848597181897354, 0.9851455110766442, 0.9853956658513328, 0.985653746325023, 0.9859707246336972, 0.986206593346798, 0.9863953457768241, 0.9866169876697299, 0.9867882300792276, 0.9869894070713048, 0.9871775251877037, 0.9874244785512863, 0.9876114424608635, 0.9877514746853655, 0.9879010330991819, 0.9880121062598519, 0.9881591309279845, 0.9883149825410683, 0.9884670136987261, 0.9885967829170887, 0.9887112222724387, 0.9888824529863713, 0.9889400900406753, 0.9890813751542548, 0.9891614729329469, 0.9892641491690639, 0.9893636166050988], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.05427999999999998, 0.1162653333333333, 0.1794921333333333, 0.2400762533333333, 0.2974552946666666, 0.3498164318666666, 0.3977414553466666, 0.4417139764786666, 0.48202257883079996, 0.5187136542810533, 0.5521089555196146, 0.5826713933009864, 0.6104442539708878, 0.6351598285737989, 0.657443845716419, 0.6777527944781104, 0.6959108483636327, 0.7125997635272695, 0.7275931205078758, 0.7411004751237549, 0.7534570942780461, 0.7645113848502414, 0.7747402463652172, 0.7838128883953621, 0.7919515995558258, 0.7994764396002433, 0.8061687956402189, 0.8122452494095304, 0.8178207244685773, 0.8227053186883863, 0.8271547868195477, 0.831159308137593, 0.8348033773238337, 0.8379497062581169, 0.8409680689656386, 0.8437512620690747, 0.8460694691955005, 0.8481958556092838, 0.8500562700483554, 0.8518773097101865, 0.8534895787391679, 0.8549939541985845, 0.8563612254453927, 0.8576717695675201, 0.8586912592774348, 0.859848800016358, 0.8607972533480556, 0.8616508613465833, 0.8622724418785916, 0.8630051976907325, 0.8636246779216592, 0.8641822101294933, 0.8647906557832106, 0.8653515902048895, 0.8657764311844005, 0.8663854547326271, 0.8667469092593644, 0.866992218333428, 0.8671596631667519, 0.8675103635167433, 0.8678659938317356, 0.8681860611152287, 0.8685274550037059, 0.8687680428366686, 0.8689712385530017, 0.8689407813643683, 0.8689933698945981, 0.8692806995718049, 0.8692059629479577, 0.8692853666531619, 0.8693168299878458, 0.8693718136557279, 0.8694612989568218, 0.8695818357278062, 0.8697303188216923, 0.8697972869395231, 0.8698575582455708, 0.869805135754347, 0.8698246221789123, 0.8697488266276878, 0.869773943964919, 0.8699565495684272, 0.8699742279449177, 0.870016805150426, 0.8700284579687168, 0.870025612171845, 0.8699297176213272, 0.8701100791925278, 0.8700457379399417, 0.8700544974792808, 0.8699690477313528, 0.8699588096248841, 0.8700695953290624, 0.870022635796156, 0.8700070388832071, 0.8698463349948864, 0.8698083681620644, 0.8698408646791913, 0.8698034448779388, 0.8698231003901449, 0.8698407903511304, 0.8698167113160173, 0.8698617068510823, 0.8697822028326408, 0.8697906492160434, 0.8699315842944391], "moving_var_accuracy_train": [0.02666420042076124, 0.06040699972268512, 0.09218931012034379, 0.11904546524356843, 0.13967736900721459, 0.15330691407033478, 0.16137409530523003, 0.1654512287450369, 0.16611771235101144, 0.16399095688764148, 0.159658906697443, 0.15403646360999768, 0.14727275968704875, 0.1396576750024323, 0.13171610779087273, 0.12352221904685454, 0.11529300049125633, 0.10729193774177967, 0.09948054973736042, 0.09198528318550399, 0.0848165340364414, 0.0780210553361716, 0.07168297898121914, 0.06569714403563433, 0.060120052009702704, 0.05492130443202592, 0.050134358832536446, 0.045702630227042525, 0.04162618634614021, 0.037880393352413735, 0.03443492951082806, 0.031280085794320414, 0.028384159332795082, 0.025744526688164314, 0.02334573864833654, 0.021158378692627305, 0.0191631755898639, 0.017356055373233162, 0.015703539735651303, 0.014210817515485476, 0.012859042947829892, 0.011628119946803772, 0.010513299055357212, 0.009505181742108128, 0.008590675227747546, 0.007764470856297117, 0.007015746707886968, 0.0063398104383420315, 0.005728236027473652, 0.005173202726517622, 0.004670724519811404, 0.004216470095714914, 0.003807006256908693, 0.0034360855006079385, 0.0031015928593158427, 0.0027995643310281797, 0.002525592298775186, 0.002280376672708772, 0.002058015059228532, 0.0018577974444500835, 0.0016771610691955754, 0.0015134118231726096, 0.001366116176610569, 0.0012332575948264994, 0.0011132485861928957, 0.0010044732502044215, 0.0009065167246666453, 0.0008180047101284058, 0.0007380699028851276, 0.0006660835331444529, 0.0006011444965780578, 0.0005425167124884472, 0.0004893780812689801, 0.0004410055656269559, 0.0003985169302792255, 0.00035953179051648415, 0.0003241246970757189, 0.0002922957736024811, 0.0002638372558818664, 0.00023790281899813817, 0.0002148476352661934, 0.00019392606844126925, 0.00017513291137524403, 0.00015852389747124905, 0.0001431722141725029, 0.00012917564007381937, 0.0001167002022246553, 0.00010529409766748467, 9.512893754000744e-05, 8.593453961746406e-05, 7.788995932978035e-05, 7.041556292816201e-05, 6.35504878504363e-05, 5.739674853768214e-05, 5.176810890710504e-05, 4.678584429374965e-05, 4.232586739208239e-05, 3.830130190896329e-05, 3.462273216837713e-05, 3.127832624601562e-05, 2.841437323795887e-05, 2.5602834184422608e-05, 2.322220411585297e-05, 2.095772459163045e-05, 1.895683381763595e-05, 1.7150194373354353e-05], "duration": 64638.097764, "accuracy_train": [0.5443058823529412, 0.6904705882352942, 0.7663058823529412, 0.8159764705882353, 0.8474352941176471, 0.8600470588235294, 0.8715529411764706, 0.8865882352941177, 0.897364705882353, 0.9049647058823529, 0.9100705882352941, 0.9195294117647059, 0.9242588235294118, 0.9265176470588236, 0.932235294117647, 0.934564705882353, 0.9369411764705883, 0.9423058823529412, 0.9441647058823529, 0.9472, 0.9488, 0.9505176470588236, 0.9548705882352941, 0.9547058823529412, 0.956564705882353, 0.9571058823529411, 0.9600705882352941, 0.9608, 0.9625176470588235, 0.9639058823529412, 0.9643529411764706, 0.9654588235294118, 0.9652705882352941, 0.966564705882353, 0.9684470588235294, 0.9691294117647059, 0.9693411764705883, 0.9712235294117647, 0.9702588235294117, 0.9722823529411765, 0.9736, 0.9733411764705883, 0.9741882352941177, 0.9753176470588235, 0.9756, 0.9767058823529412, 0.9770588235294118, 0.9781411764705882, 0.9787294117647058, 0.9785882352941176, 0.9787764705882352, 0.9791529411764706, 0.9800470588235294, 0.98, 0.9806823529411764, 0.9811294117647059, 0.9807294117647058, 0.9824235294117647, 0.9822352941176471, 0.982964705882353, 0.9834352941176471, 0.9832705882352941, 0.984, 0.9844235294117647, 0.9846823529411765, 0.9845411764705883, 0.9850117647058824, 0.9851529411764706, 0.9853176470588235, 0.9857176470588235, 0.9859764705882353, 0.9861647058823529, 0.9860235294117647, 0.9853647058823529, 0.9873411764705883, 0.986635294117647, 0.9863058823529411, 0.986635294117647, 0.9872705882352941, 0.9868705882352942, 0.9877176470588235, 0.9876470588235294, 0.9879764705882353, 0.9888235294117647, 0.9883294117647059, 0.9880941176470588, 0.9886117647058823, 0.9883294117647059, 0.9888, 0.9888705882352942, 0.9896470588235294, 0.9892941176470588, 0.9890117647058824, 0.9892470588235294, 0.9890117647058824, 0.9894823529411765, 0.9897176470588235, 0.989835294117647, 0.989764705882353, 0.9897411764705882, 0.9904235294117647, 0.9894588235294117, 0.9903529411764705, 0.9898823529411764, 0.9901882352941177, 0.9902588235294117], "end": "2016-02-06 11:23:48.077000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0], "moving_var_accuracy_valid": [0.026516865599999988, 0.05844481297599999, 0.08857898582255999, 0.11275500760587359, 0.13111069630427763, 0.1426748248737449, 0.14907861326639738, 0.15157299547209532, 0.15103874673712747, 0.14805098722268037, 0.14328310380374065, 0.13736135685162865, 0.13056720727457358, 0.12300822319868486, 0.11517659765892965, 0.10737101849128176, 0.09960135093033937, 0.092147894841357, 0.08495631213915239, 0.07810271858370914, 0.07166662105765599, 0.06559973501238114, 0.059981427982176025, 0.0547241006846228, 0.04984783819034164, 0.045372663330554085, 0.041238485661790864, 0.037446946709307, 0.03398202533758279, 0.030798556150054227, 0.02789688043490079, 0.025251518110290822, 0.022845879461368724, 0.020650385987096228, 0.018667342009294024, 0.01687032328302375, 0.01523165771325048, 0.013749185614551932, 0.012405417330062696, 0.011194721266106085, 0.010098643842291788, 0.009109147767768661, 0.008215057866952933, 0.007409009813322024, 0.006677463065407418, 0.0060217758639270775, 0.005427694351036043, 0.004891482735468794, 0.0044058117231418606, 0.00397006293054971, 0.0035765104393033205, 0.0032216569748379394, 0.002902823132375893, 0.002615372645967122, 0.002355459790091257, 0.0021232519982227823, 0.001912102642774594, 0.0017214339673734958, 0.001549542910586008, 0.0013956955361467645, 0.001257264238820562, 0.0011324598025321696, 0.0010202627703627585, 0.0009187574358748101, 0.0008272532887795543, 0.000744536308664654, 0.0006701075677797943, 0.0006038398360924491, 0.0005435061225497009, 0.0004892122548303323, 0.0004402999388201639, 0.0003962971537717514, 0.000356739506966583, 0.00032119631828835904, 0.0002892751115220529, 0.0002603879629290998, 0.00023438186030918406, 0.0002109684073365406, 0.0001898749840895676, 0.00017093919037087978, 0.0001538509492594581, 0.00013876595759140532, 0.00012489217455722288, 0.0001124192724673606, 0.0001011785673141916, 9.106078346981094e-05, 8.203746700620102e-05, 7.412649297287452e-05, 6.675110184664621e-05, 6.00766822277465e-05, 5.413472893976061e-05, 4.8722199415201114e-05, 4.396044072393344e-05, 3.958424343111704e-05, 3.562800846124717e-05, 3.229763927261502e-05, 2.90808486689043e-05, 2.6182268014642277e-05, 2.357664338691001e-05, 2.122245610065977e-05, 1.9103026903070815e-05, 1.7197942412151506e-05, 1.5496369554518367e-05, 1.4003620599601752e-05, 1.2603900612174827e-05, 1.1522274817858851e-05], "accuracy_test": 0.8592, "start": "2016-02-05 17:26:29.979000", "learning_rate_per_epoch": [0.002282546367496252, 0.001141273183748126, 0.0007608487503603101, 0.000570636591874063, 0.00045650923857465386, 0.00038042437518015504, 0.0003260780358687043, 0.0002853182959370315, 0.00025361625012010336, 0.00022825461928732693, 0.0002075041993521154, 0.00019021218759007752, 0.00017558048421051353, 0.00016303901793435216, 0.0001521697558928281, 0.00014265914796851575, 0.0001342674222541973, 0.00012680812506005168, 0.00012013401283184066, 0.00011412730964366347, 0.00010869267862290144, 0.0001037520996760577, 9.924114419845864e-05, 9.510609379503876e-05, 9.130185208050534e-05, 8.779024210525677e-05, 8.453874761471525e-05, 8.151950896717608e-05, 7.870849367463961e-05, 7.608487794641405e-05, 7.363052282016724e-05, 7.132957398425788e-05, 6.916806887602434e-05, 6.713371112709865e-05, 6.521560862893239e-05, 6.340406253002584e-05, 6.169043626869097e-05, 6.006700641592033e-05, 5.852682807017118e-05, 5.706365482183173e-05, 5.567185871768743e-05, 5.434633931145072e-05, 5.3082472732057795e-05, 5.187604983802885e-05, 5.072325075161643e-05, 4.962057209922932e-05, 4.856481245951727e-05, 4.755304689751938e-05, 4.658257603296079e-05, 4.565092604025267e-05, 4.475580863072537e-05, 4.389512105262838e-05, 4.3066909711342305e-05, 4.2269373807357624e-05, 4.150083987042308e-05, 4.075975448358804e-05, 4.0044669731287286e-05, 3.9354246837319806e-05, 3.868722342303954e-05, 3.804243897320703e-05, 3.7418791180243716e-05, 3.681526141008362e-05, 3.623089287430048e-05, 3.566478699212894e-05, 3.5116096114506945e-05, 3.458403443801217e-05, 3.406785617698915e-05, 3.3566855563549325e-05, 3.3080381399486214e-05, 3.260780431446619e-05, 3.214853859390132e-05, 3.170203126501292e-05, 3.126775845885277e-05, 3.084521813434549e-05, 3.043395008717198e-05, 3.0033503207960166e-05, 2.9643457310157828e-05, 2.926341403508559e-05, 2.8892991394968703e-05, 2.8531827410915866e-05, 2.817958375089802e-05, 2.7835929358843714e-05, 2.7500556825543754e-05, 2.717316965572536e-05, 2.6853485906030983e-05, 2.6541236366028897e-05, 2.6236164558213204e-05, 2.5938024919014424e-05, 2.5646586436778307e-05, 2.5361625375808217e-05, 2.508292527636513e-05, 2.481028604961466e-05, 2.4543507606722414e-05, 2.4282406229758635e-05, 2.402680183877237e-05, 2.377652344875969e-05, 2.3531405531684868e-05, 2.3291288016480394e-05, 2.305602356500458e-05, 2.2825463020126335e-05, 2.259946813865099e-05, 2.2377904315362684e-05, 2.2160642402013764e-05, 2.194756052631419e-05, 2.1738534996984527e-05, 2.1533454855671152e-05], "accuracy_train_first": 0.5443058823529412, "accuracy_train_last": 0.9902588235294117, "batch_size_eval": 1024, "accuracy_train_std": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "accuracy_test_std": 0, "error_valid": [0.45720000000000005, 0.32586666666666664, 0.2514666666666666, 0.21466666666666667, 0.18613333333333337, 0.1789333333333334, 0.17093333333333338, 0.1625333333333333, 0.1552, 0.15106666666666668, 0.14733333333333332, 0.14226666666666665, 0.13959999999999995, 0.14239999999999997, 0.14200000000000002, 0.13946666666666663, 0.14066666666666672, 0.1372, 0.13746666666666663, 0.1373333333333333, 0.1353333333333333, 0.136, 0.13319999999999999, 0.13453333333333328, 0.13480000000000003, 0.13280000000000003, 0.13360000000000005, 0.13306666666666667, 0.132, 0.1333333333333333, 0.13280000000000003, 0.13280000000000003, 0.13239999999999996, 0.13373333333333337, 0.1318666666666667, 0.13119999999999998, 0.13306666666666667, 0.1326666666666667, 0.13319999999999999, 0.13173333333333337, 0.132, 0.13146666666666662, 0.1313333333333333, 0.13053333333333328, 0.13213333333333332, 0.12973333333333337, 0.1306666666666667, 0.1306666666666667, 0.13213333333333332, 0.13039999999999996, 0.13080000000000003, 0.13080000000000003, 0.12973333333333337, 0.12960000000000005, 0.13039999999999996, 0.12813333333333332, 0.13, 0.13080000000000003, 0.1313333333333333, 0.1293333333333333, 0.12893333333333334, 0.12893333333333334, 0.12839999999999996, 0.12906666666666666, 0.12919999999999998, 0.1313333333333333, 0.13053333333333328, 0.12813333333333332, 0.13146666666666662, 0.13, 0.13039999999999996, 0.13013333333333332, 0.12973333333333337, 0.1293333333333333, 0.12893333333333334, 0.12960000000000005, 0.12960000000000005, 0.1306666666666667, 0.13, 0.13093333333333335, 0.13, 0.12839999999999996, 0.12986666666666669, 0.12960000000000005, 0.12986666666666669, 0.13, 0.13093333333333335, 0.12826666666666664, 0.13053333333333328, 0.12986666666666669, 0.13080000000000003, 0.13013333333333332, 0.12893333333333334, 0.13039999999999996, 0.13013333333333332, 0.13160000000000005, 0.13053333333333328, 0.12986666666666669, 0.13053333333333328, 0.13, 0.13, 0.13039999999999996, 0.12973333333333337, 0.13093333333333335, 0.13013333333333332, 0.12880000000000003], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.06084199463136295, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 128, "valid_ratio": 0.15, "learning_rate": 0.0022825462538983558, "optimization": "adam", "nb_data_augmentation": 4, "learning_rate_decay_method": "lin", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 1.0559161535537317e-06, "rotation_range": [0, 0], "momentum": 0.837566067892042}, "accuracy_valid_max": 0.8718666666666667, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        y_pred = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            y_pred.extend((nnet.predict(X[mini_batch]) == y[mini_batch]).tolist())\n        return np.mean(y_pred)\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            acc = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = acc\n            status[\"accuracy_train_std\"] = 0\n            acc = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = acc\n            status[\"accuracy_valid_std\"] = 0\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n\n    # rescaling to [-1, 1]\n    X_min = X_train.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X_train.max(axis=(0, 2, 3))[None, :, None, None]\n    def preprocess(a):\n        return (a / 255.) * 2 - 1\n        # return 2 * ((a - X_min) / (X_max - X_min)) - 1\n    X_train = preprocess(X_train)\n    X_valid = preprocess(X_valid)\n\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = preprocess(X_test)\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    acc = evaluate(X_test, y_test, batch_size_eval)\n    light.set(\"accuracy_test\", acc)\n    light.set(\"accuracy_test_std\", 0)\n    print(\"Test accuracy : {}+-{}\".format(acc, 0))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8712, "accuracy_valid_std": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "accuracy_valid": [0.5428, 0.6741333333333334, 0.7485333333333334, 0.7853333333333333, 0.8138666666666666, 0.8210666666666666, 0.8290666666666666, 0.8374666666666667, 0.8448, 0.8489333333333333, 0.8526666666666667, 0.8577333333333333, 0.8604, 0.8576, 0.858, 0.8605333333333334, 0.8593333333333333, 0.8628, 0.8625333333333334, 0.8626666666666667, 0.8646666666666667, 0.864, 0.8668, 0.8654666666666667, 0.8652, 0.8672, 0.8664, 0.8669333333333333, 0.868, 0.8666666666666667, 0.8672, 0.8672, 0.8676, 0.8662666666666666, 0.8681333333333333, 0.8688, 0.8669333333333333, 0.8673333333333333, 0.8668, 0.8682666666666666, 0.868, 0.8685333333333334, 0.8686666666666667, 0.8694666666666667, 0.8678666666666667, 0.8702666666666666, 0.8693333333333333, 0.8693333333333333, 0.8678666666666667, 0.8696, 0.8692, 0.8692, 0.8702666666666666, 0.8704, 0.8696, 0.8718666666666667, 0.87, 0.8692, 0.8686666666666667, 0.8706666666666667, 0.8710666666666667, 0.8710666666666667, 0.8716, 0.8709333333333333, 0.8708, 0.8686666666666667, 0.8694666666666667, 0.8718666666666667, 0.8685333333333334, 0.87, 0.8696, 0.8698666666666667, 0.8702666666666666, 0.8706666666666667, 0.8710666666666667, 0.8704, 0.8704, 0.8693333333333333, 0.87, 0.8690666666666667, 0.87, 0.8716, 0.8701333333333333, 0.8704, 0.8701333333333333, 0.87, 0.8690666666666667, 0.8717333333333334, 0.8694666666666667, 0.8701333333333333, 0.8692, 0.8698666666666667, 0.8710666666666667, 0.8696, 0.8698666666666667, 0.8684, 0.8694666666666667, 0.8701333333333333, 0.8694666666666667, 0.87, 0.87, 0.8696, 0.8702666666666666, 0.8690666666666667, 0.8698666666666667, 0.8712], "seed": 925323350, "model": "residualv5", "loss_std": [0.24174366891384125, 0.1398373693227768, 0.09279615432024002, 0.08335062116384506, 0.07932562381029129, 0.07657670229673386, 0.07346262037754059, 0.07127270102500916, 0.06766051054000854, 0.06664150208234787, 0.06299248337745667, 0.0620700903236866, 0.06016562506556511, 0.0602571964263916, 0.05776072293519974, 0.055201876908540726, 0.05522914603352547, 0.053650591522455215, 0.05258588865399361, 0.05173547565937042, 0.05077362805604935, 0.050605978816747665, 0.048840347677469254, 0.04951505735516548, 0.04783408343791962, 0.04768951237201691, 0.046357523649930954, 0.04547276720404625, 0.04586907848715782, 0.0441499687731266, 0.04372752085328102, 0.0430280901491642, 0.04251937195658684, 0.04145434871315956, 0.04050259292125702, 0.042327601462602615, 0.040024466812610626, 0.04023028910160065, 0.039162296801805496, 0.03923894837498665, 0.03909558802843094, 0.03796244040131569, 0.03768889978528023, 0.03758649900555611, 0.03757763281464577, 0.037224963307380676, 0.03733966127038002, 0.03608276695013046, 0.03659386932849884, 0.03454294800758362, 0.03473127633333206, 0.034770362079143524, 0.03502712771296501, 0.03424419090151787, 0.03436286747455597, 0.03365783393383026, 0.03463266044855118, 0.031954407691955566, 0.03375653177499771, 0.03216449171304703, 0.03212782368063927, 0.03270673006772995, 0.031877223402261734, 0.031039496883749962, 0.03161760792136192, 0.03163450211286545, 0.03109787590801716, 0.03247574344277382, 0.03070555254817009, 0.030356215313076973, 0.029567332938313484, 0.030112504959106445, 0.029086073860526085, 0.02890147641301155, 0.029130004346370697, 0.029922861605882645, 0.02843092754483223, 0.03006231226027012, 0.027432739734649658, 0.027677765116095543, 0.028637530282139778, 0.027968743816018105, 0.026689058169722557, 0.027045831084251404, 0.026212800294160843, 0.027234187349677086, 0.027295589447021484, 0.026224259287118912, 0.027539310976862907, 0.026374060660600662, 0.026136258617043495, 0.026084963232278824, 0.025945333763957024, 0.0252109095454216, 0.026127303019165993, 0.0266873762011528, 0.025775644928216934, 0.025381511077284813, 0.026018435135483742, 0.025127720087766647, 0.025870133191347122, 0.024356981739401817, 0.023465504869818687, 0.025736870244145393, 0.024675726890563965, 0.02389095164835453]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:41 2016", "state": "available"}], "summary": "eded00dfe1828aaca1772da78da3f2ae"}