{"content": {"hp_model": {"f0": 64, "f1": 16, "f2": 64, "f3": 16, "nonlin": "very_leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.586427092552185, 1.1071356534957886, 0.8991516828536987, 0.7699649333953857, 0.6867332458496094, 0.6223856806755066, 0.5726509094238281, 0.5331390500068665, 0.5002458691596985, 0.4705883860588074, 0.44598186016082764, 0.42266011238098145, 0.4001229405403137, 0.38145193457603455, 0.3595927655696869, 0.34699347615242004, 0.3320176303386688, 0.3197093904018402, 0.30278173089027405, 0.29265525937080383, 0.2771146893501282, 0.2681897282600403, 0.25913873314857483, 0.2531038522720337, 0.24356979131698608, 0.2348225712776184, 0.2276187241077423, 0.21997453272342682, 0.21345233917236328, 0.20794788002967834, 0.2004956752061844, 0.19808615744113922, 0.192066490650177, 0.1845569908618927, 0.18044772744178772, 0.17778527736663818, 0.172902449965477, 0.1711394339799881, 0.16457891464233398, 0.16050076484680176, 0.15546050667762756, 0.1532260924577713, 0.14932629466056824, 0.14748097956180573, 0.1446097195148468, 0.14094246923923492, 0.1397627741098404, 0.13530531525611877, 0.1342371702194214, 0.13010114431381226, 0.1299731433391571, 0.12587763369083405, 0.12354104965925217, 0.12635783851146698, 0.11900907009840012, 0.11703869700431824, 0.11682391911745071, 0.11341307312250137, 0.11190149188041687, 0.11115109920501709, 0.1060144305229187, 0.10482044517993927, 0.10721215605735779, 0.10551301389932632, 0.10159511864185333, 0.1029696837067604, 0.09933531284332275, 0.09524912387132645, 0.09811381995677948, 0.09495440125465393, 0.09387776255607605, 0.09310586005449295, 0.09065315127372742, 0.09051774442195892, 0.08664405345916748, 0.0889400839805603, 0.08730251342058182, 0.08509352058172226, 0.08800824731588364, 0.083516426384449, 0.08271131664514542, 0.0780111700296402, 0.08471675217151642, 0.08106807619333267, 0.07931619882583618, 0.07552800327539444, 0.07797885686159134, 0.07527647912502289, 0.07447957247495651, 0.07579273730516434, 0.07505794614553452, 0.07303767651319504, 0.07550382614135742, 0.07358679175376892, 0.06888291239738464, 0.0715436339378357, 0.07002373784780502, 0.049909692257642746, 0.03965869918465614, 0.036320190876722336, 0.032836105674505234, 0.03229575604200363, 0.031200535595417023, 0.030528990551829338, 0.028918365016579628, 0.02916303277015686, 0.02750806324183941, 0.02770189568400383, 0.026644466444849968, 0.026957010850310326, 0.02652517892420292, 0.025005845353007317, 0.025588707998394966, 0.02494480460882187, 0.025436336174607277, 0.024759748950600624, 0.023388534784317017, 0.02354401722550392, 0.02279340662062168, 0.0226457417011261, 0.022659700363874435, 0.02336391992866993, 0.022579606622457504, 0.02425616793334484, 0.02316265180706978, 0.022972313687205315, 0.02350562997162342, 0.023748748004436493, 0.02264985628426075, 0.024127604439854622, 0.023229751735925674, 0.02346649207174778, 0.02361924946308136, 0.023873863741755486, 0.023938043043017387, 0.02343790791928768, 0.023441132158041, 0.022536084055900574, 0.023348428308963776, 0.024468105286359787, 0.023652462288737297, 0.02403697744011879, 0.022931521758437157, 0.022547239437699318, 0.023207545280456543, 0.023826858028769493, 0.02292031981050968, 0.023738395422697067, 0.0230705663561821, 0.023589013144373894, 0.023389847949147224, 0.023585395887494087, 0.023552529513835907, 0.023014983162283897, 0.023437172174453735, 0.02304830402135849, 0.023255014792084694, 0.02365543134510517], "moving_avg_accuracy_train": [0.046534951492709475, 0.0945639210444352, 0.14420316861907068, 0.19291633764909788, 0.2370770596023959, 0.29175493252850737, 0.3397931431620077, 0.38470839902853116, 0.42274012796450233, 0.4601743792876534, 0.49181289215369245, 0.5240903796721272, 0.5514572874817233, 0.5836725905372608, 0.6071363122709563, 0.6290551532430116, 0.6451188291562521, 0.6672011475718377, 0.6886955023779152, 0.7105491130735863, 0.7274368452116872, 0.7446143255287889, 0.7619011561438226, 0.7794775328640194, 0.7865414557900113, 0.8014375637063037, 0.8144628445726612, 0.8262877597047269, 0.8358074608879548, 0.8466605248420718, 0.8518367984295239, 0.8601219558247128, 0.8672531126958684, 0.8726763506776196, 0.8752674979672774, 0.8825325950813101, 0.8855839279042975, 0.8922569072472657, 0.8929179367144919, 0.8992085045002226, 0.9028267143872711, 0.9087426769867039, 0.9133484641975942, 0.9167545277076982, 0.9223029914536319, 0.9269872558868494, 0.9307823500886775, 0.9332307450631985, 0.9361109909414763, 0.9400190301652043, 0.943175903449902, 0.9465587769811115, 0.9479549768485227, 0.9504181647660606, 0.9519632821788049, 0.9542838031788092, 0.9556608125919176, 0.9577440779839348, 0.9586007458534539, 0.9595808300847843, 0.9606792528787238, 0.9622118100682416, 0.9628262817757492, 0.964469620860079, 0.9657766371217086, 0.9653580078202982, 0.9659579120954851, 0.9671183522847461, 0.9686533548538905, 0.9695001810863862, 0.9710434665491763, 0.9723789450430682, 0.9722766114007031, 0.9729447626856512, 0.9739018105611429, 0.9730962382455693, 0.9734405753210217, 0.9728834144758889, 0.974153516815214, 0.9747059850741688, 0.9757984446024662, 0.9764910506255622, 0.9773701263665774, 0.9782520834835188, 0.978871422679233, 0.9786685403434803, 0.9796647606389126, 0.9796104090905253, 0.9797219640136525, 0.980426793988487, 0.9801683919694371, 0.978308442278538, 0.978924434694741, 0.9797624599752669, 0.9802818426979784, 0.9805772621853326, 0.9804061198942172, 0.9822306492738431, 0.9839169035428873, 0.9854484832778843, 0.9868408559322387, 0.9881009667675863, 0.9892490174122561, 0.9902752875460306, 0.9911873049223798, 0.9920150960075228, 0.9927717337281992, 0.9934596831232363, 0.9940648866859128, 0.9946188704875596, 0.9951174559090418, 0.9955522318955186, 0.9959621314738238, 0.9963310410942986, 0.9966560843062973, 0.9969462980482866, 0.997212140713696, 0.9974467488149454, 0.997667196701308, 0.9978562992037963, 0.9980311417536547, 0.9981885000485273, 0.9983301225139127, 0.99845525758395, 0.9985678791469836, 0.9986692385537139, 0.9987674374661997, 0.9988558164874368, 0.9989376827553599, 0.9990020618012525, 0.9990716286866035, 0.9991272634369908, 0.9991773347123393, 0.9992200737113435, 0.9992631891080662, 0.9992996678163072, 0.9993301735049146, 0.9993622789222802, 0.9993818732026712, 0.9994018332038326, 0.9994244475024969, 0.9994378249248662, 0.9994545149026177, 0.9994718610314035, 0.9994804971008822, 0.9994929198610321, 0.999504100345167, 0.9995164879296979, 0.9995276367557757, 0.9995376706992457, 0.9995397258019402, 0.9995392502455557, 0.9995411473936191, 0.9995498302733048, 0.9995506694185934, 0.9995560749469722, 0.999560939922513, 0.9995606681028808], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.04607933687876505, 0.09363410673945781, 0.14134268519390059, 0.18854904285109184, 0.23143171386718744, 0.28364872343867653, 0.3294670095097185, 0.37221713884865626, 0.4079995018180828, 0.4426745963463649, 0.4723735100174513, 0.5026315409340043, 0.5276053209049864, 0.5560411072915208, 0.5763525239061187, 0.5961252922704015, 0.610122190989521, 0.62906735023206, 0.6481536263157817, 0.6670880577824716, 0.6813121633370558, 0.6956560623083201, 0.7100924797973677, 0.7246692281466972, 0.7301128733854914, 0.7421026743846681, 0.7528171645704483, 0.7620156050693523, 0.769371614686664, 0.777928823276356, 0.7819773201373499, 0.7879628053675909, 0.7931664895333921, 0.7971581225360016, 0.7986568657699767, 0.8041227398104489, 0.8062973563602774, 0.8112861201124274, 0.8119062315104466, 0.8171358008481068, 0.818995968880766, 0.8232958084102345, 0.8268298970910484, 0.8294419939783291, 0.833720562605722, 0.8368835625857372, 0.8399327816283682, 0.8414307849979109, 0.844048666353165, 0.8474127956233455, 0.8501771048223061, 0.8532316540821688, 0.8542838239996297, 0.8562124869686426, 0.8572605718561157, 0.85908687253948, 0.8599370861232579, 0.8615751546193658, 0.8622975807989954, 0.8625824358591562, 0.8634360675085568, 0.8644383286040868, 0.8653302155761329, 0.8666842892745438, 0.8675214491083845, 0.8662545557506334, 0.8668437792041845, 0.8679508993071094, 0.8690408456151033, 0.8695935216898881, 0.870111226185056, 0.8713310547148937, 0.8705499000152417, 0.8714980456029494, 0.8725903697135882, 0.8718928620626359, 0.872741273522035, 0.8724438616254038, 0.8731190737779989, 0.8736566110538134, 0.8750732765203447, 0.8755293748378132, 0.8765878654884445, 0.8774581463812416, 0.8782261036275302, 0.8776914144808916, 0.8787931373494441, 0.8785547192282045, 0.8785273664224774, 0.8791222189650941, 0.8786321956284492, 0.8767822184884205, 0.8770124174321237, 0.8779246628406432, 0.8785919927726933, 0.8792587724110866, 0.8793461787731406, 0.8816690263043355, 0.883832831269911, 0.8858901190201789, 0.88780271315167, 0.8896715617536717, 0.8912568987541328, 0.8927712102906171, 0.8941208541335434, 0.8953222970522673, 0.8964402167728689, 0.8975195867089103, 0.8983699788475072, 0.8990467940275155, 0.8996325431356826, 0.9000722090969638, 0.9005910082832764, 0.9009226206985482, 0.9014296209122025, 0.9018401815054702, 0.9022717507043208, 0.9025858912871266, 0.9028930318741519, 0.9031949019736344, 0.9035642413131686, 0.9038111974999993, 0.9041311143181469, 0.9043091761732298, 0.9045304669990545, 0.9047540428047967, 0.904868782302555, 0.9050330830067874, 0.9051819831492562, 0.9052783426750686, 0.9053294746632092, 0.9054487356400359, 0.9055184199167703, 0.9056065793369908, 0.9057103368776893, 0.9057660680619084, 0.9057796050339555, 0.9058050248487075, 0.9058523167444844, 0.9058948794506836, 0.9059575999487628, 0.9060018413657841, 0.9060416586411032, 0.9060530801263904, 0.9060012947982393, 0.9059668950341534, 0.9060102069426356, 0.9060247735977696, 0.9060256765561402, 0.9060509032811738, 0.9060237497000443, 0.9060746126818471, 0.90603391063806, 0.9061437631736516, 0.906169388268184, 0.9062168649157631, 0.9062351798360844, 0.9062516632643735], "moving_var_accuracy_train": [0.01948951539385942, 0.038301601100278845, 0.05664793508823454, 0.07233989711194312, 0.0826574316716773, 0.1012987165940255, 0.11193787206243985, 0.11890050674219332, 0.12002816772070668, 0.12063725949775955, 0.11758249301535426, 0.11520076951834324, 0.11042122135403928, 0.10871953097727649, 0.10280249401791548, 0.0968461649221482, 0.08948392358454395, 0.08492419030555534, 0.08058983687176577, 0.07682907588853072, 0.07171292777059189, 0.0671972274639325, 0.06316701533195503, 0.05963067496625205, 0.054116698533566036, 0.05070207495969388, 0.04715878893855197, 0.043701367605621794, 0.04014685324062118, 0.03719226889128846, 0.033714186276429005, 0.0309605621463536, 0.0283221865166075, 0.025754671456807137, 0.02323963070961673, 0.021390702363341993, 0.01933542781497756, 0.017802642913284927, 0.016026311261565306, 0.014779821323010617, 0.013419662175790183, 0.012392683479512149, 0.011344334614048943, 0.010314312570357807, 0.009559950362781499, 0.008801436326026105, 0.008050917353430243, 0.007299777359648554, 0.006644461970557724, 0.006117470708669718, 0.005595416278223492, 0.005138869150354564, 0.004642526601946939, 0.004232879594206187, 0.0038310781251580583, 0.003496433672045403, 0.0031638556991549645, 0.002886530081481656, 0.002604481991881487, 0.002352678878597863, 0.002128269784446288, 0.0019365813898539436, 0.001746321430182496, 0.001595994357279021, 0.001451769545124597, 0.001308169845040132, 0.001180591826790606, 0.0010746522370072148, 0.0009883931092920138, 0.0008960078303751994, 0.0008278426175146083, 0.0007611098810319773, 0.0006850931424980171, 0.0006206016635044162, 0.0005667849628778236, 0.0005159469873906071, 0.0004654194008453257, 0.000421671314626935, 0.0003940226227354735, 0.0003573673510562989, 0.00033237182633937917, 0.0003034519716345003, 0.0002800617418970232, 0.0002590562029124328, 0.00023660281197532068, 0.00021331298195723306, 0.0002009137776547914, 0.00018084898670662122, 0.00016287608854382446, 0.00015105954733026823, 0.00013655453702828287, 0.0001540337989995335, 0.0001420454390109572, 0.00013416147244706613, 0.00012317315091621948, 0.0001116412898861754, 0.00010074076805183189, 0.00012062685836071124, 0.00013415525366347017, 0.00014185135665900395, 0.00014511453547044967, 0.00014489399577964762, 0.00014226677874622657, 0.00013751917435889956, 0.00013125323817587766, 0.00012429505708406947, 0.00011701805713881439, 0.00010957572075612174, 0.00010191459085099507, 9.44852142382791e-05, 8.727397961708182e-05, 8.024785308112532e-05, 7.373522675166643e-05, 6.758655284920924e-05, 6.177877537128628e-05, 5.6358913978512636e-05, 5.1359073485429304e-05, 4.6718534787433205e-05, 4.2484056744105524e-05, 3.855748887772083e-05, 3.4976869245117846e-05, 3.1702037017293035e-05, 2.8712345619880174e-05, 2.5982040129671307e-05, 2.3497988664845283e-05, 2.1240653362355043e-05, 1.920337526383999e-05, 1.735333540000956e-05, 1.567832063242133e-05, 1.4147790523129549e-05, 1.2776567434653463e-05, 1.1526767720244e-05, 1.0396655141754865e-05, 9.373429225902281e-06, 8.45281674022313e-06, 7.619511331595202e-06, 6.865935571772371e-06, 6.1886188350131554e-06, 5.573212373928218e-06, 5.0194767513526825e-06, 4.522131734754142e-06, 4.071529160141962e-06, 3.6668832423438666e-06, 3.302902911764177e-06, 2.9732838558521274e-06, 2.677344394994587e-06, 2.410734984524542e-06, 2.171042556326683e-06, 1.955056967600237e-06, 1.7604573910342547e-06, 1.584449662954593e-06, 1.4260067320440072e-06, 1.2834384513765795e-06, 1.1557731378356498e-06, 1.0402021615354228e-06, 9.36444923015362e-07, 8.430134425969456e-07, 7.587127633104634e-07], "duration": 72641.346634, "accuracy_train": [0.46534951492709486, 0.5268246470099668, 0.5909563967907899, 0.6313348589193429, 0.6345235571820782, 0.7838557888635106, 0.7721370388635106, 0.7889457018272426, 0.7650256883882429, 0.7970826411960132, 0.7765595079480436, 0.81458776733804, 0.7977594577680879, 0.8736103180370985, 0.8183098078742157, 0.8263247219915099, 0.7896919123754154, 0.8659420133121077, 0.8821446956326136, 0.9072316093346253, 0.8794264344545959, 0.8992116483827058, 0.9174826316791252, 0.9376649233457919, 0.8501167621239387, 0.9355025349529347, 0.9316903723698781, 0.9327119958933187, 0.9214847715370063, 0.9443381004291252, 0.8984232607165927, 0.9346883723814139, 0.931433524536268, 0.9214854925133813, 0.8985878235741971, 0.9479184691076044, 0.913045923311185, 0.9523137213339794, 0.8988672019195275, 0.9558236145717978, 0.9353906033707088, 0.9619863403815985, 0.9548005490956073, 0.9474090992986341, 0.9722391651670359, 0.9691456357858066, 0.9649381979051311, 0.9552662998338871, 0.9620332038459765, 0.975191383178756, 0.9715877630121816, 0.9770046387619971, 0.9605207756552234, 0.9725868560239018, 0.9658693388935032, 0.9751684921788483, 0.968053897309893, 0.9764934665120893, 0.9663107566791252, 0.9684015881667589, 0.9705650580241787, 0.9760048247739018, 0.9683565271433187, 0.9792596726190477, 0.977539783476375, 0.9615903441076044, 0.9713570505721669, 0.9775623139880952, 0.9824683779761905, 0.9771216171788483, 0.9849330357142857, 0.9843982514880952, 0.9713556086194168, 0.9789581242501846, 0.9825152414405685, 0.965846087405408, 0.9765396090000923, 0.9678689668696937, 0.98558443786914, 0.9796781994047619, 0.9856305803571429, 0.9827245048334257, 0.9852818080357143, 0.9861896975359912, 0.9844454754406607, 0.9768425993217055, 0.9886307432978036, 0.9791212451550388, 0.9807259583217978, 0.9867702637619971, 0.9778427737979882, 0.9615688950604466, 0.9844683664405685, 0.9873046875, 0.9849562872023809, 0.9832360375715209, 0.9788658392741787, 0.9986514136904762, 0.9990931919642857, 0.9992327008928571, 0.9993722098214286, 0.9994419642857143, 0.9995814732142857, 0.99951171875, 0.9993954613095238, 0.9994652157738095, 0.9995814732142857, 0.9996512276785714, 0.99951171875, 0.9996047247023809, 0.9996047247023809, 0.9994652157738095, 0.9996512276785714, 0.9996512276785714, 0.9995814732142857, 0.9995582217261905, 0.9996047247023809, 0.9995582217261905, 0.9996512276785714, 0.9995582217261905, 0.9996047247023809, 0.9996047247023809, 0.9996047247023809, 0.9995814732142857, 0.9995814732142857, 0.9995814732142857, 0.9996512276785714, 0.9996512276785714, 0.9996744791666666, 0.9995814732142857, 0.9996977306547619, 0.9996279761904762, 0.9996279761904762, 0.9996047247023809, 0.9996512276785714, 0.9996279761904762, 0.9996047247023809, 0.9996512276785714, 0.9995582217261905, 0.9995814732142857, 0.9996279761904762, 0.9995582217261905, 0.9996047247023809, 0.9996279761904762, 0.9995582217261905, 0.9996047247023809, 0.9996047247023809, 0.9996279761904762, 0.9996279761904762, 0.9996279761904762, 0.9995582217261905, 0.9995349702380952, 0.9995582217261905, 0.9996279761904762, 0.9995582217261905, 0.9996047247023809, 0.9996047247023809, 0.9995582217261905], "end": "2016-02-02 05:58:33.513000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0], "moving_var_accuracy_valid": [0.019109747584680457, 0.03755187805474349, 0.05428166637256263, 0.06890946156463432, 0.07856882666944308, 0.09525148879979962, 0.10462017796621022, 0.11060632219605238, 0.11106908747352923, 0.110783438351084, 0.10764332377515937, 0.10511892731216715, 0.10022024175530168, 0.09747556310655711, 0.09144098959992727, 0.08581555195902275, 0.07899721532690009, 0.07432776532273648, 0.07017356220315926, 0.06638282023754433, 0.06156546482324146, 0.05726064528019785, 0.05341027210144075, 0.0499815792232538, 0.04525012076230104, 0.04201890663806968, 0.03885021867373333, 0.035726698574866884, 0.03264102661479004, 0.030035956322938526, 0.027179873632145975, 0.024784320569904268, 0.02254959347299053, 0.020438032331939176, 0.018414445180277735, 0.016841882673486734, 0.015200255020387158, 0.013904219392321332, 0.01251725829640278, 0.01151166802587906, 0.010391643249278699, 0.009518876504163455, 0.008679396898981822, 0.007872864660420514, 0.007250333539872267, 0.006615341305747223, 0.006037486806101998, 0.0054539342523482505, 0.0049702205522251085, 0.004575054788720964, 0.0041863219579759935, 0.0038516622028067476, 0.003476459536342959, 0.0031622912503410394, 0.0028559484626890807, 0.002600371984094686, 0.002346840553927582, 0.0021363059141162955, 0.0019273724189697952, 0.001735365458720508, 0.0015683870957841834, 0.0014205891319382794, 0.0012856893800826023, 0.0011736220823008933, 0.0010625674033573713, 0.0009707558320408589, 0.0008768049073407054, 0.0008001558509073393, 0.0007308321124053906, 0.0006604979587576079, 0.0005968603243806998, 0.0005505661267224855, 0.000501001338033333, 0.000458992024729407, 0.00042383136992061167, 0.0003858268852367821, 0.0003537224147530606, 0.00031914625780407455, 0.0002913348350827768, 0.00026480186848051054, 0.0002563841510290208, 0.00023261796700289747, 0.00021943979241987322, 0.00020431231266919572, 0.00018918890639141905, 0.00017284304810407476, 0.00016648288280548943, 0.00015034618332975924, 0.0001353182985806136, 0.00012497111464967043, 0.00011463510901881325, 0.00013397333688458903, 0.00012105292717926847, 0.00011643735962962687, 0.00010880158681055463, 0.00010192278390508253, 9.17992643637219e-05, 0.00013117992380595696, 0.00016020039878680316, 0.00018227225489474532, 0.00019696717621159993, 0.00020870381446527238, 0.00021045307366402558, 0.00021004602116338807, 0.00020543526557178767, 0.000197882924797177, 0.00018934233283284872, 0.00018089345467903458, 0.00016931261031561813, 0.0001565040583750634, 0.00014394157069702428, 0.0001312871690449052, 0.00012058082550188269, 0.0001095124440973557, 0.00010087464263742881, 9.23042183803849e-05, 8.47500643029151e-05, 7.716321662451414e-05, 7.029591302384688e-05, 6.408645173411575e-05, 5.890551049025184e-05, 5.356384566515243e-05, 4.912858203344071e-05, 4.4501078048216706e-05, 4.0491696909742245e-05, 3.6892402486987225e-05, 3.332164860940092e-05, 3.0232436241162356e-05, 2.74087338888913e-05, 2.4751426923935163e-05, 2.2299814553442665e-05, 2.0197841723441242e-05, 1.8221760636913038e-05, 1.646953332358436e-05, 1.4919470636492139e-05, 1.3455477256893011e-05, 1.211157877771355e-05, 1.0906236402780447e-05, 9.835741473157902e-06, 8.868471581473064e-06, 8.017029171239517e-06, 7.232941980936044e-06, 6.5239165215670306e-06, 5.872698922345833e-06, 5.309564512016693e-06, 4.7892581547375654e-06, 4.327215632011183e-06, 3.896403755786209e-06, 3.5067707182119597e-06, 3.1618211352940365e-06, 2.8522748744780018e-06, 2.5903307732910512e-06, 2.3462076032779893e-06, 2.220195059133286e-06, 2.0040853624481178e-06, 1.8239631147915301e-06, 1.6445857300697487e-06, 1.4825724877362466e-06], "accuracy_test": 0.8887316645408163, "start": "2016-02-01 09:47:52.166000", "learning_rate_per_epoch": [0.00877666100859642, 0.00877666100859642, 0.00877666100859642, 0.00877666100859642, 0.00877666100859642, 0.00877666100859642, 0.00877666100859642, 0.00877666100859642, 0.00877666100859642, 0.00877666100859642, 0.00877666100859642, 0.00877666100859642, 0.00877666100859642, 0.00877666100859642, 0.00877666100859642, 0.00877666100859642, 0.00877666100859642, 0.00877666100859642, 0.00877666100859642, 0.00877666100859642, 0.00877666100859642, 0.00877666100859642, 0.00877666100859642, 0.00877666100859642, 0.00877666100859642, 0.00877666100859642, 0.00877666100859642, 0.00877666100859642, 0.00877666100859642, 0.00877666100859642, 0.00877666100859642, 0.00877666100859642, 0.00877666100859642, 0.00877666100859642, 0.00877666100859642, 0.00877666100859642, 0.00877666100859642, 0.00877666100859642, 0.00877666100859642, 0.00877666100859642, 0.00877666100859642, 0.00877666100859642, 0.00877666100859642, 0.00877666100859642, 0.00877666100859642, 0.00877666100859642, 0.00877666100859642, 0.00877666100859642, 0.00877666100859642, 0.00877666100859642, 0.00877666100859642, 0.00877666100859642, 0.00877666100859642, 0.00877666100859642, 0.00877666100859642, 0.00877666100859642, 0.00877666100859642, 0.00877666100859642, 0.00877666100859642, 0.00877666100859642, 0.00877666100859642, 0.00877666100859642, 0.00877666100859642, 0.00877666100859642, 0.00877666100859642, 0.00877666100859642, 0.00877666100859642, 0.00877666100859642, 0.00877666100859642, 0.00877666100859642, 0.00877666100859642, 0.00877666100859642, 0.00877666100859642, 0.00877666100859642, 0.00877666100859642, 0.00877666100859642, 0.00877666100859642, 0.00877666100859642, 0.00877666100859642, 0.00877666100859642, 0.00877666100859642, 0.00877666100859642, 0.00877666100859642, 0.00877666100859642, 0.00877666100859642, 0.00877666100859642, 0.00877666100859642, 0.00877666100859642, 0.00877666100859642, 0.00877666100859642, 0.00877666100859642, 0.00877666100859642, 0.00877666100859642, 0.00877666100859642, 0.00877666100859642, 0.00877666100859642, 0.0008776661125011742, 0.0008776661125011742, 0.0008776661125011742, 0.0008776661125011742, 0.0008776661125011742, 0.0008776661125011742, 0.0008776661125011742, 0.0008776661125011742, 0.0008776661125011742, 0.0008776661125011742, 0.0008776661125011742, 0.0008776661125011742, 0.0008776661125011742, 0.0008776661125011742, 0.0008776661125011742, 0.0008776661125011742, 0.0008776661125011742, 0.0008776661125011742, 0.0008776661125011742, 8.77666097949259e-05, 8.77666116139153e-06, 8.776661388765206e-07, 8.776661530873753e-08, 8.776661175602385e-09, 8.776661064580082e-10, 8.776661342135839e-11, 8.776661689080534e-12, 8.776661580660317e-13, 8.776661309609773e-14, 8.776661479016363e-15, 8.776661584895481e-16, 8.776661584895481e-17, 8.776661750331604e-18, 8.776661336741298e-19, 8.776661595235239e-20, 8.776661756793952e-21, 8.776661958742344e-22, 8.776661958742344e-23, 8.776662116514525e-24, 8.776662313729751e-25, 8.776662436989268e-26, 8.776662282914872e-27, 8.776662090321878e-28, 8.776661969951256e-29, 8.776662120414533e-30, 8.776662120414533e-31, 8.776662237963968e-32, 8.776662531837556e-33, 8.776662348166564e-34, 8.776662118577823e-35, 8.776662405563749e-36, 8.776662764296156e-37, 8.77666298850391e-38, 8.77666298850391e-39, 8.776668593697767e-40, 8.776612541759194e-41, 8.776332282066329e-42, 8.772128386673355e-43, 8.828180325246348e-44, 8.407790785948902e-45, 1.401298464324817e-45, 0.0], "accuracy_train_first": 0.46534951492709486, "accuracy_train_last": 0.9995582217261905, "batch_size_eval": 1024, "accuracy_train_std": [0.019343791913502936, 0.015560286656870748, 0.01837471651356335, 0.01733868035215706, 0.016257607807293264, 0.020441885736224574, 0.02103730034319741, 0.02294014748896449, 0.019751795094142783, 0.021369229174210146, 0.017764606540710937, 0.020817699710889096, 0.017639610871220127, 0.02210766042231952, 0.020216844021789554, 0.023580745802992707, 0.0194535407463835, 0.019975718924250832, 0.020370234407210332, 0.01845230115720948, 0.020265439289196135, 0.01931590321881389, 0.019365458926666582, 0.01704126891875322, 0.018600656960867545, 0.01866204877027196, 0.01713623202217587, 0.01668802262440574, 0.016573441580004667, 0.015299266839765846, 0.016223176166194453, 0.01609123318261473, 0.017060195930072523, 0.016739265192356915, 0.013513303164428472, 0.011840246246442575, 0.015845487868134854, 0.012651780434944361, 0.013695497488520957, 0.01153934933082553, 0.012298066950566617, 0.009594634046814904, 0.0120637271073876, 0.012850009407863154, 0.009127225756120004, 0.00974821383619777, 0.010152184488260486, 0.011080121887689085, 0.00952562185278557, 0.007629351154344249, 0.008650751104386994, 0.008450662803852602, 0.009752838993511571, 0.009005471636771779, 0.009828739573281876, 0.008403539293917288, 0.009214893634468466, 0.007125483625741247, 0.010129357242840156, 0.009141843632776982, 0.009185588775517168, 0.007649471204296553, 0.009249275316958001, 0.0072855487214601505, 0.008088361179040483, 0.008855833867667086, 0.00700435576264706, 0.006883893540806306, 0.006779074930361035, 0.007223402573783597, 0.006028012751246736, 0.005880604271985952, 0.007335809099588152, 0.007015430759561415, 0.00609511597090377, 0.009429122266448204, 0.006868510521672586, 0.0072073921502132645, 0.004731699536920847, 0.006766622482558995, 0.005879822774950151, 0.005584494565945325, 0.006723379969824483, 0.004788672206279046, 0.006193810165502496, 0.005971120581722372, 0.004149002884700756, 0.00650809865029699, 0.006019140929495139, 0.005238970925615625, 0.0066725566697357595, 0.008156139433148215, 0.005462373792531575, 0.004984069149725805, 0.005404008000142012, 0.005910122218318143, 0.005304817069851989, 0.0011855979151768937, 0.0009384503381050302, 0.0008124723404788015, 0.0007630598531167197, 0.0007113587491061362, 0.0005695428159373089, 0.0006481287139493282, 0.000766946730955666, 0.0008308951071989229, 0.0005695428159373089, 0.0005141681637717629, 0.0006120929399687585, 0.0006414208623574089, 0.0006414208623574089, 0.0006103238815293289, 0.0005141681637717629, 0.0005565805991250848, 0.0006081053213644913, 0.0006103238815293289, 0.0005662107357647842, 0.0006806771376629673, 0.0005141681637717629, 0.0005719109875487743, 0.0005245774819883965, 0.0005662107357647842, 0.0005662107357647842, 0.0005695428159373089, 0.0005695428159373089, 0.0005695428159373089, 0.0005565805991250848, 0.0005141681637717629, 0.0005072875797356638, 0.0006081053213644913, 0.0004992306211305778, 0.0005618975992184976, 0.0005618975992184976, 0.0006049856692363136, 0.0005565805991250848, 0.0006009508920814772, 0.0005662107357647842, 0.0005565805991250848, 0.0006103238815293289, 0.0006081053213644913, 0.0005618975992184976, 0.0006103238815293289, 0.0005662107357647842, 0.0006009508920814772, 0.0006103238815293289, 0.0005662107357647842, 0.0005662107357647842, 0.0005618975992184976, 0.0005618975992184976, 0.0005618975992184976, 0.0005719109875487743, 0.0006116511550393372, 0.0006103238815293289, 0.0006009508920814772, 0.0006103238815293289, 0.0005662107357647842, 0.0006049856692363136, 0.0006103238815293289], "accuracy_test_std": 0.009909311498416484, "error_valid": [0.5392066312123494, 0.4783729645143072, 0.42928010871611444, 0.38659373823418675, 0.38262424698795183, 0.24639819041792166, 0.25816841585090367, 0.24303169710090367, 0.26995923145707834, 0.24524955289909633, 0.2603362669427711, 0.2250461808170181, 0.24763065935617468, 0.18803681522966864, 0.2408447265625, 0.2259197924510542, 0.26390572053840367, 0.2004262165850903, 0.18006988893072284, 0.1625020590173193, 0.19067088667168675, 0.17524884695030118, 0.15997976280120485, 0.14414003670933728, 0.22089431946536142, 0.14998911662274095, 0.15075242375753017, 0.15519843044051207, 0.16442429875753017, 0.14505629941641573, 0.18158620811370485, 0.15816782756024095, 0.16000035297439763, 0.16691718044051207, 0.18785444512424698, 0.14668439382530118, 0.1741310946912651, 0.14381500611822284, 0.18251276590737953, 0.13579807511295183, 0.16426251882530118, 0.13800563582454817, 0.1413633047816265, 0.1470491340361446, 0.12777231974774095, 0.1346494375941265, 0.13262424698795183, 0.14508718467620485, 0.13239040144954817, 0.12231004094503017, 0.12494411238704817, 0.11927740257906627, 0.13624664674322284, 0.12642954631024095, 0.1333066641566265, 0.12447642131024095, 0.13241099162274095, 0.12368222891566272, 0.13120058358433728, 0.13485386859939763, 0.12888124764683728, 0.1265413215361446, 0.12664280167545183, 0.12112904743975905, 0.12494411238704817, 0.1451474844691265, 0.1278532097138554, 0.12208501976656627, 0.12114963761295183, 0.12543239363704817, 0.12522943335843373, 0.11769048851656627, 0.1364804922816265, 0.11996864410768071, 0.11757871329066272, 0.13438470679593373, 0.11962302334337349, 0.13023284544427716, 0.12080401684864461, 0.12150555346385539, 0.11217673428087349, 0.12036574030496983, 0.11388571865587349, 0.11470932558358427, 0.11486228115587349, 0.1271207878388554, 0.11129135683358427, 0.12359104386295183, 0.12171880882906627, 0.11552410815135539, 0.1257780144013554, 0.13986757577183728, 0.12091579207454817, 0.11386512848268071, 0.11540203783885539, 0.11474021084337349, 0.11986716396837349, 0.0974253459149097, 0.0966929240399097, 0.0955942912274097, 0.0949839396649097, 0.09350880082831325, 0.09447506824171681, 0.09359998588102414, 0.09373235128012047, 0.09386471667921681, 0.09349850574171681, 0.09276608386671681, 0.09397649190512047, 0.0948618693524097, 0.09509571489081325, 0.09597079725150603, 0.0947397990399097, 0.09609286756400603, 0.0940073771649097, 0.09446477315512047, 0.09384412650602414, 0.09458684346762047, 0.09434270284262047, 0.09408826713102414, 0.09311170463102414, 0.09396619681852414, 0.09298963431852414, 0.09408826713102414, 0.09347791556852414, 0.09323377494352414, 0.09409856221762047, 0.09348821065512047, 0.09347791556852414, 0.09385442159262047, 0.09421033744352414, 0.09347791556852414, 0.09385442159262047, 0.09359998588102414, 0.09335584525602414, 0.09373235128012047, 0.09409856221762047, 0.09396619681852414, 0.09372205619352414, 0.09372205619352414, 0.09347791556852414, 0.09359998588102414, 0.09359998588102414, 0.09384412650602414, 0.09446477315512047, 0.09434270284262047, 0.09359998588102414, 0.09384412650602414, 0.09396619681852414, 0.09372205619352414, 0.09422063253012047, 0.0934676204819277, 0.09433240775602414, 0.09286756400602414, 0.09359998588102414, 0.09335584525602414, 0.09359998588102414, 0.09359998588102414], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.08751996108433358, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 64, "valid_ratio": 0.15, "learning_rate": 0.0087766609231555, "optimization": "rmsprop", "nb_data_augmentation": 1, "learning_rate_decay_method": "discrete", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 2.669633838864514e-08, "rotation_range": [0, 0], "momentum": 0.7657919943047012}, "accuracy_valid_max": 0.9072339161332832, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.9064000141189759, "accuracy_valid_std": [0.009679446009067996, 0.012730289743451294, 0.02740306337088409, 0.022674462167714304, 0.009915570621532227, 0.013587241733104487, 0.014736874552991443, 0.014583945926795551, 0.01397241960827967, 0.010743350084366624, 0.010899806787075868, 0.013234584597542525, 0.02338161659626048, 0.011788309949647864, 0.014235199017756453, 0.014382629857998524, 0.010198042710783202, 0.014440265024238577, 0.018615763425805655, 0.015466067467576923, 0.015594984071757944, 0.010846999451349442, 0.01245960736818812, 0.011613730472786831, 0.014798087367740069, 0.013345991117053016, 0.009020352543660532, 0.008989366105639352, 0.015790316185734676, 0.012549872403654915, 0.010989656837910667, 0.01470978894032626, 0.013335911905671433, 0.009818093426213526, 0.014266375385256155, 0.014943868214298965, 0.014585914247901314, 0.009203223032711571, 0.01848136014119374, 0.013238995795324641, 0.010451667013579134, 0.009707309226050469, 0.009837475458653381, 0.01138419500257288, 0.01312915692889351, 0.01243849373609408, 0.01176319948141897, 0.011501414585992345, 0.011478291727240915, 0.015448810682087871, 0.011525672220202562, 0.01099950311406975, 0.011632287855722625, 0.010576188353722706, 0.013910062742833195, 0.01064832425891849, 0.012740919117345264, 0.014310760159022719, 0.01074228254106121, 0.014023170052936983, 0.008002222387182324, 0.009401639449641251, 0.010612336248159422, 0.008455587053908652, 0.013139901934090299, 0.008581928731865496, 0.010225468205658773, 0.011613305819028454, 0.015502505736826436, 0.011834198546907143, 0.014394774098291577, 0.009072355215151684, 0.01023235978501458, 0.01316536269031661, 0.009902177478658439, 0.016431347803600952, 0.01270188204506954, 0.014424606035896563, 0.012875785016585673, 0.013994901955456125, 0.011206771038235997, 0.010326978755243675, 0.014087182138738075, 0.015329325101550566, 0.016394928224818396, 0.010538704265134263, 0.013161491928091514, 0.0105841399465276, 0.01227193420083175, 0.009575606478953154, 0.006742233044665118, 0.006864457030691976, 0.012349992813256834, 0.014169664128703793, 0.009409007550924794, 0.01245317124953298, 0.015116716398137739, 0.015611935124745005, 0.013071167784161534, 0.01152570374892339, 0.01092440623515799, 0.011526190556440422, 0.012819617480933843, 0.014854733847402874, 0.015223551795816803, 0.013504774043281332, 0.013471984541550325, 0.015374637342134321, 0.015631433935072245, 0.013480988283005266, 0.015343025307069522, 0.014304625577292698, 0.012273612070295018, 0.012385526910764824, 0.014081372585454407, 0.0164349678578178, 0.016734485002838344, 0.016169567174274656, 0.016283174538168257, 0.016866160691169647, 0.016765289696450553, 0.016800895319788248, 0.01701782835004537, 0.01685909125312674, 0.016693307520873377, 0.016773046251786952, 0.016180089474228058, 0.015630882629861974, 0.01654986841048642, 0.01618350663930944, 0.017021583609645796, 0.016870891513609668, 0.016395731942750163, 0.016967532436504056, 0.01655821380006677, 0.016282970562957355, 0.016623442144129502, 0.016536281205873492, 0.0169153995229237, 0.01738116180869196, 0.016082268351881097, 0.0166412109639036, 0.017023645485518952, 0.016904586188625233, 0.016543410989697006, 0.015762375217692977, 0.01707957418232206, 0.016762955068442668, 0.016857563055770994, 0.01702078199456919, 0.016143804149865465, 0.017587381053133334, 0.016979273193006142, 0.016517743586523897, 0.016569421129120042, 0.016694442106630913, 0.016583803957847934, 0.016953475154835192], "accuracy_valid": [0.4607933687876506, 0.5216270354856928, 0.5707198912838856, 0.6134062617658133, 0.6173757530120482, 0.7536018095820783, 0.7418315841490963, 0.7569683028990963, 0.7300407685429217, 0.7547504471009037, 0.7396637330572289, 0.7749538191829819, 0.7523693406438253, 0.8119631847703314, 0.7591552734375, 0.7740802075489458, 0.7360942794615963, 0.7995737834149097, 0.8199301110692772, 0.8374979409826807, 0.8093291133283133, 0.8247511530496988, 0.8400202371987951, 0.8558599632906627, 0.7791056805346386, 0.850010883377259, 0.8492475762424698, 0.8448015695594879, 0.8355757012424698, 0.8549437005835843, 0.8184137918862951, 0.841832172439759, 0.8399996470256024, 0.8330828195594879, 0.812145554875753, 0.8533156061746988, 0.8258689053087349, 0.8561849938817772, 0.8174872340926205, 0.8642019248870482, 0.8357374811746988, 0.8619943641754518, 0.8586366952183735, 0.8529508659638554, 0.872227680252259, 0.8653505624058735, 0.8673757530120482, 0.8549128153237951, 0.8676095985504518, 0.8776899590549698, 0.8750558876129518, 0.8807225974209337, 0.8637533532567772, 0.873570453689759, 0.8666933358433735, 0.875523578689759, 0.867589008377259, 0.8763177710843373, 0.8687994164156627, 0.8651461314006024, 0.8711187523531627, 0.8734586784638554, 0.8733571983245482, 0.878870952560241, 0.8750558876129518, 0.8548525155308735, 0.8721467902861446, 0.8779149802334337, 0.8788503623870482, 0.8745676063629518, 0.8747705666415663, 0.8823095114834337, 0.8635195077183735, 0.8800313558923193, 0.8824212867093373, 0.8656152932040663, 0.8803769766566265, 0.8697671545557228, 0.8791959831513554, 0.8784944465361446, 0.8878232657191265, 0.8796342596950302, 0.8861142813441265, 0.8852906744164157, 0.8851377188441265, 0.8728792121611446, 0.8887086431664157, 0.8764089561370482, 0.8782811911709337, 0.8844758918486446, 0.8742219855986446, 0.8601324242281627, 0.8790842079254518, 0.8861348715173193, 0.8845979621611446, 0.8852597891566265, 0.8801328360316265, 0.9025746540850903, 0.9033070759600903, 0.9044057087725903, 0.9050160603350903, 0.9064911991716867, 0.9055249317582832, 0.9064000141189759, 0.9062676487198795, 0.9061352833207832, 0.9065014942582832, 0.9072339161332832, 0.9060235080948795, 0.9051381306475903, 0.9049042851091867, 0.904029202748494, 0.9052602009600903, 0.903907132435994, 0.9059926228350903, 0.9055352268448795, 0.9061558734939759, 0.9054131565323795, 0.9056572971573795, 0.9059117328689759, 0.9068882953689759, 0.9060338031814759, 0.9070103656814759, 0.9059117328689759, 0.9065220844314759, 0.9067662250564759, 0.9059014377823795, 0.9065117893448795, 0.9065220844314759, 0.9061455784073795, 0.9057896625564759, 0.9065220844314759, 0.9061455784073795, 0.9064000141189759, 0.9066441547439759, 0.9062676487198795, 0.9059014377823795, 0.9060338031814759, 0.9062779438064759, 0.9062779438064759, 0.9065220844314759, 0.9064000141189759, 0.9064000141189759, 0.9061558734939759, 0.9055352268448795, 0.9056572971573795, 0.9064000141189759, 0.9061558734939759, 0.9060338031814759, 0.9062779438064759, 0.9057793674698795, 0.9065323795180723, 0.9056675922439759, 0.9071324359939759, 0.9064000141189759, 0.9066441547439759, 0.9064000141189759, 0.9064000141189759], "seed": 452260440, "model": "residualv4", "loss_std": [0.2701541483402252, 0.15274573862552643, 0.13543155789375305, 0.1258632242679596, 0.12155689299106598, 0.11615925282239914, 0.11077744513750076, 0.1083650290966034, 0.1085638478398323, 0.10816829651594162, 0.09920306503772736, 0.0952383503317833, 0.09341117739677429, 0.09420941770076752, 0.08582951873540878, 0.08517573028802872, 0.08088168501853943, 0.08138931542634964, 0.08311893045902252, 0.08150368183851242, 0.07464440166950226, 0.07696421444416046, 0.0719415545463562, 0.07202441245317459, 0.0759430006146431, 0.06848982721567154, 0.06829372048377991, 0.0659024566411972, 0.06449658423662186, 0.06201764941215515, 0.06516820192337036, 0.06322770565748215, 0.0572969950735569, 0.05803879722952843, 0.058322787284851074, 0.05866019055247307, 0.05964448302984238, 0.05705596134066582, 0.05721810460090637, 0.05327853932976723, 0.05456874892115593, 0.05222640559077263, 0.052217721939086914, 0.053759798407554626, 0.05344622582197189, 0.0485195554792881, 0.04946453869342804, 0.04753817617893219, 0.04769236594438553, 0.0491255484521389, 0.04745052382349968, 0.048103805631399155, 0.04590273275971413, 0.04935270920395851, 0.04237332195043564, 0.04572436586022377, 0.046186938881874084, 0.044240374118089676, 0.04591716453433037, 0.04437875747680664, 0.04246102273464203, 0.04264359548687935, 0.04611785337328911, 0.042043861001729965, 0.04046839103102684, 0.04167083278298378, 0.04081156104803085, 0.040089789777994156, 0.041611332446336746, 0.03878406062722206, 0.04244160279631615, 0.039910390973091125, 0.03942146897315979, 0.039592381566762924, 0.03792355954647064, 0.04134051874279976, 0.03965432941913605, 0.03797975555062294, 0.03908165916800499, 0.03797871246933937, 0.038004595786333084, 0.0374070480465889, 0.040371883660554886, 0.036523401737213135, 0.035780712962150574, 0.035879261791706085, 0.03518244996666908, 0.03657244145870209, 0.03556067869067192, 0.03527950868010521, 0.03592083230614662, 0.03343770653009415, 0.037595730274915695, 0.0348067432641983, 0.03332544490695, 0.037933189421892166, 0.03551439195871353, 0.028756804764270782, 0.021621808409690857, 0.019356707111001015, 0.0174469742923975, 0.02105247974395752, 0.019309239462018013, 0.01869410090148449, 0.018257495015859604, 0.017376869916915894, 0.016980033367872238, 0.01889874041080475, 0.017263680696487427, 0.01735449768602848, 0.017853016033768654, 0.014941522851586342, 0.016955818980932236, 0.01666400395333767, 0.01740870252251625, 0.01819315366446972, 0.015735937282443047, 0.01596095785498619, 0.014890172518789768, 0.01460501179099083, 0.015160033479332924, 0.016456957906484604, 0.015076465904712677, 0.016471724957227707, 0.01543773990124464, 0.015502424910664558, 0.017008798196911812, 0.016029739752411842, 0.014971270225942135, 0.015744207426905632, 0.014425046741962433, 0.016171975061297417, 0.016441697254776955, 0.016115132719278336, 0.016299229115247726, 0.015403732657432556, 0.01627280004322529, 0.015251623466610909, 0.015836872160434723, 0.01622663252055645, 0.016128795221447945, 0.015592820011079311, 0.015337459743022919, 0.014901410788297653, 0.015596729703247547, 0.015277658589184284, 0.01677260361611843, 0.01514203567057848, 0.015623840503394604, 0.01659110002219677, 0.016051413491368294, 0.01574806682765484, 0.015620077028870583, 0.014922160655260086, 0.016054688021540642, 0.01635347492992878, 0.015490228310227394, 0.016280103474855423]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:32 2016", "state": "available"}], "summary": "c1caed8cb5a8052857442a3fe49bb3d3"}