{"content": {"hp_model": {"f0": 64, "f1": 16, "f2": 16, "f3": 32, "nonlin": "leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [2.7151412963867188, 2.3106491565704346, 2.1360833644866943, 2.056519031524658, 2.004563093185425, 1.9638558626174927, 1.928657054901123, 1.8972318172454834, 1.86962890625, 1.8462848663330078, 1.8269084692001343, 1.8096084594726562, 1.7952979803085327, 1.7837156057357788, 1.7732762098312378, 1.762882947921753, 1.7533818483352661, 1.7458775043487549, 1.7394505739212036, 1.731416940689087, 1.7253479957580566, 1.719717264175415, 1.7136142253875732, 1.708707571029663, 1.70418381690979, 1.6986544132232666, 1.6937583684921265, 1.6897679567337036, 1.6863278150558472, 1.6812744140625, 1.6774868965148926, 1.6723896265029907, 1.6686062812805176, 1.6654677391052246, 1.6618455648422241, 1.6577260494232178, 1.654725193977356, 1.6504133939743042, 1.6464909315109253, 1.6441316604614258, 1.6401559114456177, 1.6356419324874878, 1.6332300901412964, 1.6283336877822876, 1.6250572204589844, 1.6215745210647583, 1.6186403036117554, 1.6151225566864014, 1.611332893371582, 1.6083605289459229, 1.603763222694397, 1.6011433601379395, 1.5973916053771973, 1.593919038772583, 1.5909512042999268, 1.587283968925476, 1.583669662475586, 1.5807726383209229, 1.5768002271652222, 1.5731251239776611, 1.5703598260879517, 1.5667530298233032, 1.5619369745254517, 1.5591768026351929, 1.5561161041259766, 1.5523388385772705, 1.549174189567566, 1.545291781425476, 1.5434181690216064, 1.538674235343933, 1.534583330154419, 1.5302600860595703, 1.5284137725830078, 1.5256990194320679, 1.5208511352539062, 1.5180456638336182, 1.515700101852417, 1.511437177658081, 1.5081461668014526, 1.504301905632019, 1.5006253719329834, 1.4984757900238037, 1.4939624071121216, 1.491644263267517, 1.4883379936218262, 1.4844633340835571, 1.4822665452957153, 1.4775445461273193, 1.4749821424484253, 1.4723330736160278, 1.4688524007797241, 1.4674757719039917, 1.4637032747268677, 1.4599496126174927, 1.4573445320129395, 1.4537038803100586, 1.451625108718872, 1.4488205909729004, 1.4444103240966797, 1.442543387413025, 1.4402849674224854, 1.438529372215271, 1.4338669776916504, 1.432982087135315, 1.427229642868042, 1.4256565570831299, 1.422966718673706, 1.4192181825637817, 1.4165754318237305, 1.414379358291626, 1.411969542503357, 1.4101555347442627, 1.4058510065078735, 1.4040950536727905, 1.4014649391174316, 1.3992135524749756, 1.3965108394622803, 1.3941810131072998, 1.391308069229126, 1.3888590335845947, 1.3861398696899414, 1.3833900690078735, 1.380906105041504, 1.380195140838623, 1.375914216041565, 1.3732396364212036, 1.3711692094802856, 1.3683890104293823, 1.3678629398345947, 1.3638066053390503, 1.3612054586410522, 1.3612186908721924, 1.361499309539795, 1.3623448610305786, 1.3595761060714722, 1.3619846105575562, 1.360375165939331, 1.3611245155334473, 1.3604263067245483, 1.3610661029815674, 1.3597769737243652, 1.3617032766342163, 1.361769199371338, 1.3595848083496094, 1.3608982563018799, 1.3605642318725586, 1.3614369630813599, 1.3611235618591309, 1.360671043395996, 1.3600306510925293, 1.3612234592437744, 1.3617841005325317, 1.3605473041534424, 1.3613567352294922, 1.3611078262329102, 1.3597490787506104, 1.3614178895950317, 1.3611042499542236, 1.3610409498214722, 1.3607250452041626, 1.3615913391113281, 1.3610782623291016, 1.3605846166610718, 1.3600661754608154, 1.3603490591049194, 1.3603113889694214, 1.360153079032898, 1.3599059581756592, 1.3609012365341187, 1.3598036766052246, 1.3611376285552979, 1.3601330518722534, 1.3608230352401733, 1.3610926866531372, 1.3604410886764526, 1.3605232238769531, 1.3625377416610718, 1.3628844022750854, 1.3618974685668945, 1.3613401651382446, 1.3592880964279175, 1.362065076828003, 1.360081672668457, 1.3615106344223022, 1.360285997390747, 1.3597822189331055, 1.3607425689697266, 1.3607631921768188], "moving_avg_accuracy_train": [0.016075294117647056, 0.0353595294117647, 0.05616239999999999, 0.0760332188235294, 0.09516872047058822, 0.11336949548235292, 0.13082784005176468, 0.1475285854583529, 0.16363455044192937, 0.17903109539773643, 0.19392092703443337, 0.20820883433099002, 0.22164206854494986, 0.2343790381610431, 0.24628701669787997, 0.25747478561632725, 0.26802377764292984, 0.27798139987863685, 0.28720914224371435, 0.2957917574311076, 0.3038243463938792, 0.3112960294015501, 0.318335838226101, 0.32499401910937326, 0.33118873484549477, 0.33698515547859237, 0.3424113458130861, 0.3475655053494245, 0.352453660696835, 0.35701064756832795, 0.36127428869384814, 0.3653492127656398, 0.3691484091361346, 0.3727723917519329, 0.37618221140026903, 0.37940399026024213, 0.3824330029989238, 0.3853520556402079, 0.38811802654677535, 0.39075092977445075, 0.3933205426793586, 0.39580495899965806, 0.39818916898204515, 0.4004290756132524, 0.4026308739342801, 0.40473955124673444, 0.4068726549455904, 0.40889597768632546, 0.410876968152987, 0.4128292713376883, 0.41470869714509595, 0.4165695921364687, 0.4183761623345865, 0.4201597225717161, 0.42200257384395623, 0.4238046694007371, 0.4256289083430163, 0.4273671939793029, 0.42904223928725493, 0.43069566241735296, 0.43236491970502944, 0.43399666302864415, 0.43555699672577974, 0.4371942382296724, 0.43876422617141103, 0.4403678035542699, 0.44198514084590174, 0.4435583914671939, 0.4451319640851804, 0.4466446500296035, 0.4482201850266432, 0.4496758135828024, 0.45115764398922803, 0.4526183501785405, 0.4540906328077453, 0.4555403930563825, 0.4570616478683913, 0.4584331301403757, 0.4598015818322205, 0.4611955412960572, 0.46250892834292207, 0.46389332962627694, 0.4651863496048257, 0.4666041852325784, 0.4678449431799088, 0.46921103709721207, 0.4705111098580791, 0.47183882240168296, 0.4731043519262205, 0.47436568143948077, 0.47565381917788563, 0.476897849024803, 0.4780527700046756, 0.47928749300420803, 0.4804340378214343, 0.4817223987451732, 0.482985452988303, 0.48426102533653154, 0.4854984522146431, 0.4867650775814141, 0.48801798158797854, 0.4892044187232983, 0.4904228003803802, 0.491714637989401, 0.49299258595516676, 0.4942533273596501, 0.4955150534472145, 0.496688253984846, 0.49785707564518494, 0.499064309257137, 0.5002002312725998, 0.501410796380634, 0.502601481448453, 0.5037107450683135, 0.5049067293850116, 0.5060866446818045, 0.5072238625665653, 0.5083579468981441, 0.5095033286789179, 0.5105929958110261, 0.5116442844652176, 0.5127339736657547, 0.5138676351227086, 0.514942048081026, 0.516076078567041, 0.5170449412985721, 0.518055741286362, 0.5191042848047847, 0.5200173857360709, 0.5210180001036403, 0.522059729505041, 0.5230372859663016, 0.5237947338402598, 0.5245823192797632, 0.5253405579400221, 0.5260017962636669, 0.5265263225196531, 0.5270619255618054, 0.5275392624173896, 0.5279029832344742, 0.528298567263968, 0.5285863575963947, 0.5288947806602846, 0.5292335378883738, 0.5294490076289482, 0.5296782245131122, 0.5298586373559186, 0.5299857147967973, 0.5302271433171176, 0.5303691348677588, 0.5305463390280418, 0.5306705286546494, 0.5307587699068315, 0.5307887752690895, 0.5309098977421806, 0.5309742020856095, 0.5309967818770486, 0.5310312213364026, 0.53107633449688, 0.5311169363413096, 0.5312146544718845, 0.5313002478482255, 0.5313302230634029, 0.5313266125217686, 0.5313233630342976, 0.5313392620249855, 0.5313535711166046, 0.5313546845931795, 0.5313439220162145, 0.5314071768734165, 0.5314311650684278, 0.5314362838557026, 0.5314220672348382, 0.5314069193348838, 0.5314615215190425, 0.5315059576024324, 0.5315318324304245, 0.5315057080109115, 0.531583372503938, 0.5316579764300148, 0.5316568846693662, 0.5316088432612531, 0.5316503118763043, 0.531671163041615, 0.5316193408551005, 0.5316056420637081, 0.5315791955043961, 0.5316259818363095], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.015666666666666666, 0.03525999999999999, 0.055747333333333315, 0.07565259999999999, 0.09515400666666665, 0.11367860599999999, 0.13131074539999998, 0.14803300419333332, 0.16450970377399998, 0.18032540006326667, 0.19581286005693999, 0.2105649073845793, 0.22438841664612139, 0.23762957498150925, 0.24997328415002498, 0.26168262240168916, 0.27247436016152027, 0.2826002574787016, 0.2920868983974981, 0.30090487522441495, 0.30896105436864013, 0.31655828226510946, 0.32339578737193186, 0.3297228753014053, 0.33561725443793144, 0.3410288623274716, 0.34613930942805776, 0.35103204515191866, 0.35554217397006016, 0.35998795657305416, 0.3640558275824154, 0.3679435781575072, 0.3716292203417565, 0.37495963164091417, 0.37813033514348937, 0.3811973016291404, 0.383984238132893, 0.3865858143196037, 0.38914056622097665, 0.3916798429322123, 0.3940718586389911, 0.396344672775092, 0.39860353883091615, 0.4007565182811579, 0.40286753311970874, 0.40503411314107124, 0.4069307018269641, 0.40869096497760105, 0.4104085351465076, 0.4122343482985235, 0.4139042468020045, 0.4154338221218041, 0.4171304399096237, 0.41891072925199463, 0.42056632299346186, 0.422283024027449, 0.42396138829137076, 0.425618582795567, 0.4274300578493436, 0.4292203853977426, 0.43091168019130166, 0.4327938455055048, 0.43460779428828766, 0.4363070148594589, 0.43796964670684635, 0.43957268203616173, 0.4411220804992122, 0.4425165391159576, 0.44382488520436186, 0.44528239668392566, 0.44683415701553314, 0.4481774079806465, 0.4495596671825818, 0.4507370337976569, 0.4521033304178912, 0.4535329973761021, 0.45489969763849186, 0.456263061207976, 0.45781008842051174, 0.45946907957846056, 0.46094883828728117, 0.462613954458553, 0.4639525590126977, 0.46543730311142795, 0.46660023946695184, 0.4680602155202567, 0.46940086063489767, 0.4708607745714079, 0.47234803044760043, 0.47376656073617374, 0.475056571329223, 0.476404247529634, 0.47768382277667065, 0.47902210716567023, 0.48033322978243653, 0.4816732401375262, 0.4829059161237736, 0.48410865784472956, 0.4852844587269233, 0.48644934618756425, 0.4876977449021412, 0.48888797041192705, 0.49026584003740104, 0.4916659227003276, 0.4929126637636282, 0.4942347307205987, 0.4955179243152055, 0.49675279855035165, 0.49799751869531644, 0.4993177668257848, 0.5005859901432064, 0.5018340577955523, 0.5031573186826638, 0.5042015868143974, 0.5053814281329577, 0.5066566186529953, 0.5077776234543624, 0.5087198611089261, 0.5096612083313667, 0.5107750874982301, 0.5116309120817404, 0.5127211542068997, 0.5137157054528764, 0.5146774682409221, 0.5157163880834965, 0.5166247492751469, 0.5176022743476322, 0.5185887135795356, 0.5194231755549155, 0.5204408579994239, 0.5214101055328149, 0.5224557616462, 0.5232635188149133, 0.5240171669334219, 0.524628783573413, 0.5253925718827384, 0.5259333146944645, 0.5266466498916846, 0.5270086515691829, 0.5273477864122645, 0.5277730077710381, 0.5279690403272677, 0.5281588029612075, 0.5285962559984201, 0.5288299637319114, 0.5289736340253869, 0.5292362706228482, 0.5294059768938967, 0.5296120458711737, 0.5297441746173897, 0.5299297571556507, 0.5301634481067523, 0.530347103296077, 0.5302323929664693, 0.5303824870031557, 0.5303709049695068, 0.5303871478058895, 0.5304817663586339, 0.5304335897227705, 0.5304168974171601, 0.5304418743421108, 0.5304376869078997, 0.5304605848837765, 0.5304545263953988, 0.5304224070891923, 0.5305268330469397, 0.5305274830755791, 0.5305547347680212, 0.5305525946245524, 0.5306573351620971, 0.5306849349792208, 0.5307364414812986, 0.5306894639998354, 0.5306605175998519, 0.5307277991732, 0.5308816859225467, 0.530913517330292, 0.5309021655972628, 0.5308652823708698, 0.5308320874671162, 0.5308288787204045, 0.5308793241816974, 0.5308980584301943, 0.5308615859205082, 0.5307887606617908, 0.5308698845956117, 0.5307962294693839, 0.5307566065224455], "moving_var_accuracy_train": [0.0023257357287197232, 0.005440097733757785, 0.008790922782779792, 0.01146547547095959, 0.013614434733424951, 0.015234405159342363, 0.01645410879934691, 0.01731893199363332, 0.017921657766739714, 0.018262974359251435, 0.01843204069884892, 0.018426135283199002, 0.01820758778790301, 0.017846902564124252, 0.017338411883215735, 0.01673106625524734, 0.016059490724718527, 0.015345929817548096, 0.0145776978981995, 0.013782879659473413, 0.012985294062529632, 0.012189199078980731, 0.011416309345658677, 0.010673660765162159, 0.009951665216108106, 0.00925888512389949, 0.008597988485424916, 0.007977277881616682, 0.007394596657758774, 0.006842032156105531, 0.006321436661320018, 0.00583873805090582, 0.005384769283369469, 0.004964491605028997, 0.004572684274833709, 0.004208834578553464, 0.0038705253842379796, 0.0035601606607174705, 0.0032729999501495215, 0.0030080895697913016, 0.002766706807141793, 0.0025455870465007453, 0.002342188457011701, 0.0021531242467592664, 0.001981443064701663, 0.001823317438304034, 0.001681936876984288, 0.0015505877035044407, 0.0014308478422150328, 0.001322066447518483, 0.0012216499750565834, 0.0011306513490711712, 0.0010469594770906017, 0.0009708933134567692, 0.000904368889415467, 0.0008431599360358437, 0.0007887945718990118, 0.0007371098472889934, 0.0006886508536133241, 0.0006443900406762804, 0.0006050288156408014, 0.0005684892105441717, 0.0005335520607075057, 0.0005043218923153719, 0.00047607346231867746, 0.000451609259892159, 0.00042999035313707004, 0.0004092673754799291, 0.0003906258149886279, 0.000372157202387863, 0.0003572822768911471, 0.0003406237396435877, 0.00032632375785989723, 0.0003128943452173698, 0.0003011134559579559, 0.0002899183533689186, 0.00028175446385956637, 0.00027050769007491715, 0.00026031086136364115, 0.00025176788210865805, 0.00024211596371164404, 0.00023515346956067066, 0.00022668522858894014, 0.00022210902653597075, 0.00021375344643714548, 0.000209174015111467, 0.00020346831625225604, 0.00019898687001301756, 0.00019350226780900272, 0.0001884706102972957, 0.00018455723876549136, 0.0001800300072291324, 0.00017403158873396905, 0.00017034929783074163, 0.00016514545320884302, 0.00016356977271631546, 0.00016157054963447624, 0.0001600572580111157, 0.00015783255971806023, 0.00015648836212398383, 0.0001549674419585733, 0.0001521393954473085, 0.00015028554066340129, 0.00015027658626978624, 0.00014994728667165185, 0.000149257778005294, 0.00014865957468512963, 0.00014618121273010594, 0.00014385838812019244, 0.00014258926625261524, 0.00013994320905427068, 0.00013913809907595078, 0.00013798386754489895, 0.0001352596727955251, 0.00013460711188806212, 0.00013367620166770998, 0.00013194796215771582, 0.00013032849138213632, 0.00012910273705748023, 0.00012687883348090452, 0.00012413782064270022, 0.00012241084156233424, 0.00012173645209694716, 0.00011995207573225507, 0.00011953109444793332, 0.0001160262399360908, 0.00011361906548032527, 0.00011215215052252746, 0.00010844071526671683, 0.00010660770575332148, 0.00010571373648967271, 0.0001037429125552767, 9.853216683562142e-05, 9.426156757271967e-05, 9.000974360864975e-05, 8.494389433369468e-05, 7.892565503929578e-05, 7.361492510423173e-05, 6.830408685709937e-05, 6.266431366641541e-05, 5.780626281928851e-05, 5.277104601630423e-05, 4.835006449172715e-05, 4.4547866178798424e-05, 4.0510924442847295e-05, 3.693269541843527e-05, 3.353236502123723e-05, 3.0324466602936074e-05, 2.781660951645875e-05, 2.5216402968894236e-05, 2.297737450179904e-05, 2.0818444621831522e-05, 1.8806678826928387e-05, 1.693411384011367e-05, 1.5372738337491536e-05, 1.3872679940996898e-05, 1.2490000569730087e-05, 1.1251675200002417e-05, 1.0144824455236508e-05, 9.145178597652666e-06, 8.316600235274915e-06, 7.55087624640842e-06, 6.803875243491987e-06, 6.123605043240833e-06, 5.511339571436167e-06, 4.962480615436592e-06, 4.4680753048196075e-06, 4.0212789328083915e-06, 3.620193537093902e-06, 3.294184776021411e-06, 2.969945199918364e-06, 2.6731864977750166e-06, 2.407686858776729e-06, 2.168983302756314e-06, 1.978917559114771e-06, 1.7987968927665815e-06, 1.6249427640024886e-06, 1.4685908552562755e-06, 1.376017731024214e-06, 1.2885076699964273e-06, 1.1596676304686078e-06, 1.06447265946315e-06, 9.735022078251822e-07, 8.800649268960081e-07, 8.162282853426421e-07, 7.362943687788902e-07, 6.689597163959848e-07, 6.217643924415228e-07], "duration": 113204.579423, "accuracy_train": [0.1607529411764706, 0.20891764705882354, 0.24338823529411766, 0.2548705882352941, 0.26738823529411765, 0.2771764705882353, 0.2879529411764706, 0.29783529411764703, 0.30858823529411766, 0.3176, 0.32792941176470586, 0.3368, 0.34254117647058824, 0.34901176470588235, 0.35345882352941177, 0.35816470588235294, 0.36296470588235297, 0.3676, 0.37025882352941175, 0.3730352941176471, 0.37611764705882356, 0.3785411764705882, 0.38169411764705885, 0.38491764705882353, 0.38694117647058823, 0.3891529411764706, 0.3912470588235294, 0.39395294117647056, 0.39644705882352943, 0.3980235294117647, 0.3996470588235294, 0.4020235294117647, 0.40334117647058826, 0.40538823529411766, 0.4068705882352941, 0.4084, 0.4096941176470588, 0.4116235294117647, 0.41301176470588236, 0.4144470588235294, 0.4164470588235294, 0.41816470588235294, 0.41964705882352943, 0.42058823529411765, 0.4224470588235294, 0.42371764705882353, 0.42607058823529415, 0.42710588235294117, 0.42870588235294116, 0.4304, 0.4316235294117647, 0.43331764705882353, 0.43463529411764706, 0.43621176470588235, 0.43858823529411767, 0.4400235294117647, 0.4420470588235294, 0.4430117647058823, 0.4441176470588235, 0.4455764705882353, 0.44738823529411764, 0.4486823529411765, 0.4496, 0.45192941176470586, 0.45289411764705884, 0.4548, 0.45654117647058823, 0.4577176470588235, 0.4592941176470588, 0.4602588235294118, 0.4624, 0.4627764705882353, 0.46449411764705884, 0.4657647058823529, 0.46734117647058826, 0.46858823529411764, 0.4707529411764706, 0.4707764705882353, 0.47211764705882353, 0.4737411764705882, 0.4743294117647059, 0.4763529411764706, 0.4768235294117647, 0.4793647058823529, 0.47901176470588236, 0.4815058823529412, 0.48221176470588234, 0.48378823529411763, 0.4844941176470588, 0.48571764705882353, 0.4872470588235294, 0.48809411764705884, 0.4884470588235294, 0.4904, 0.4907529411764706, 0.4933176470588235, 0.4943529411764706, 0.49574117647058824, 0.49663529411764706, 0.49816470588235295, 0.49929411764705883, 0.49988235294117644, 0.5013882352941177, 0.5033411764705882, 0.5044941176470589, 0.5056, 0.5068705882352941, 0.5072470588235294, 0.5083764705882353, 0.5099294117647059, 0.5104235294117647, 0.5123058823529412, 0.5133176470588235, 0.5136941176470589, 0.5156705882352941, 0.5167058823529411, 0.5174588235294117, 0.5185647058823529, 0.5198117647058823, 0.5204, 0.5211058823529412, 0.5225411764705883, 0.5240705882352941, 0.5246117647058823, 0.5262823529411764, 0.5257647058823529, 0.5271529411764706, 0.5285411764705882, 0.528235294117647, 0.5300235294117647, 0.531435294117647, 0.5318352941176471, 0.5306117647058823, 0.5316705882352941, 0.532164705882353, 0.5319529411764706, 0.5312470588235294, 0.5318823529411765, 0.5318352941176471, 0.5311764705882352, 0.5318588235294117, 0.5311764705882352, 0.5316705882352941, 0.5322823529411764, 0.5313882352941176, 0.5317411764705883, 0.5314823529411765, 0.5311294117647058, 0.5324, 0.5316470588235294, 0.5321411764705882, 0.5317882352941177, 0.5315529411764706, 0.5310588235294118, 0.532, 0.5315529411764706, 0.5312, 0.5313411764705882, 0.5314823529411765, 0.5314823529411765, 0.5320941176470588, 0.5320705882352941, 0.5316, 0.5312941176470588, 0.5312941176470588, 0.5314823529411765, 0.5314823529411765, 0.531364705882353, 0.5312470588235294, 0.5319764705882353, 0.5316470588235294, 0.5314823529411765, 0.5312941176470588, 0.5312705882352942, 0.5319529411764706, 0.5319058823529412, 0.5317647058823529, 0.5312705882352942, 0.5322823529411764, 0.5323294117647058, 0.5316470588235294, 0.5311764705882352, 0.5320235294117647, 0.5318588235294117, 0.5311529411764706, 0.5314823529411765, 0.5313411764705882, 0.5320470588235294], "end": "2016-02-05 16:02:01.860000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0], "moving_var_accuracy_valid": [0.002209, 0.0054431884, 0.008676447004, 0.01137477907324, 0.0136600449237244, 0.015382487455496764, 0.01664226976833638, 0.017494748243863666, 0.01818860808112117, 0.018620973515037963, 0.018917628917034857, 0.01898447212854357, 0.018805829590424667, 0.018503201097947505, 0.018024185392484852, 0.017455744273863337, 0.01675832428136968, 0.01600529602153561, 0.015214733622681706, 0.014393070698293918, 0.013537881830099165, 0.012703554492487274, 0.011853962328010946, 0.011028854470215453, 0.01023866237183994, 0.00947836563420715, 0.00876557909689744, 0.008104470952979597, 0.007477095215287709, 0.006907270540336694, 0.00636547165724224, 0.005864955932325155, 0.0054007159638855, 0.004960469122290964, 0.004554902456373055, 0.0041840687615527115, 0.0038355650210809807, 0.0035129223068702245, 0.003220370891681321, 0.0029563651384592036, 0.00271222427688657, 0.002487493006073254, 0.0022846659881893185, 0.0020979172739888554, 0.0019282329994272079, 0.0017776563203851896, 0.0016322641261377825, 0.0014969244507594163, 0.001373782431249534, 0.0012664065311192487, 0.001164862927114676, 0.0010694330403336677, 0.0009883963435618137, 0.0009180815804886691, 0.0008509423381708705, 0.0007923716663146162, 0.0007384866591048415, 0.0006893546358170018, 0.000649952149069397, 0.0006138043887374638, 0.0005781682525721976, 0.0005522343437448833, 0.0005266246010494296, 0.0004999482958899101, 0.00047483256824044196, 0.00045047681181969675, 0.00042703485101345623, 0.0004018319994164513, 0.0003770547248581902, 0.00035846830978991385, 0.0003442931199516761, 0.00032610271635401006, 0.0003106882092306238, 0.0002920951176242032, 0.0002796865039519562, 0.00027011338205936005, 0.000259912870318371, 0.00025065042528990295, 0.0002471250215278474, 0.0002471827843344341, 0.00024217167842796464, 0.0002429080173596485, 0.00023474397499507595, 0.0002311097628439965, 0.00022017057526258922, 0.00021733728842234187, 0.00021177952349080465, 0.00020978370945987538, 0.00020871270888531116, 0.00020595149161317837, 0.0002003334884234751, 0.00019664621985151644, 0.00019171741318182376, 0.00018866471781620172, 0.00018526962868034672, 0.00018290331557804023, 0.00017828839480387473, 0.0001734788441494405, 0.00016857352916560404, 0.00016392884141267153, 0.00016156245142641987, 0.00015815593716108343, 0.00015942706578820896, 0.00016112644237663583, 0.0001590030676492508, 0.00015883351023274518, 0.00015776943142062984, 0.0001557167176682162, 0.0001540890000549261, 0.00015436759618348011, 0.00015340635001079685, 0.00015208477079321, 0.00015263546809211943, 0.00014718638466149723, 0.00014499597602818555, 0.00014513137618691, 0.0001419281044504127, 0.00013572560018447182, 0.0001301282515047965, 0.00012828196753966614, 0.0001220456922453651, 0.00012053877404407522, 0.00011738708626753183, 0.00011397326658500348, 0.00011229012988015761, 0.00010848719738260987, 0.00010623847505038557, 0.00010437218876949137, 0.00010020191098773461, 9.950281790970746e-05, 9.800750314759661e-05, 9.804732319997529e-05, 9.411483567244777e-05, 8.981522148398803e-05, 8.420037356441573e-05, 8.103068944113306e-05, 7.55592455929215e-05, 7.25829449659683e-05, 6.650405739997507e-05, 6.0888763636106095e-05, 5.6427206108110444e-05, 5.113034436521631e-05, 4.634139864385298e-05, 4.3429545217366264e-05, 3.9578164437872525e-05, 3.5806118373131375e-05, 3.284630837675272e-05, 2.9820879504976195e-05, 2.7220971365042452e-05, 2.4655996278727635e-05, 2.2500364557421527e-05, 2.0741831247320288e-05, 1.897121117968153e-05, 1.719251619918174e-05, 1.5676018557902926e-05, 1.4109623993643633e-05, 1.2701036061883052e-05, 1.1511506490405725e-05, 1.0381244735553173e-05, 9.34562795959717e-06, 8.416679784657366e-06, 7.575169617639079e-06, 6.822371511568416e-06, 6.140464707944373e-06, 5.535703085630646e-06, 5.080275802930804e-06, 4.572252025472812e-06, 4.12171071559416e-06, 3.7095808659613468e-06, 3.4373580012116963e-06, 3.1004779502378444e-06, 2.814306433020793e-06, 2.5527377436003367e-06, 2.3050050158883686e-06, 2.115245805309348e-06, 2.1168514093987613e-06, 1.914285415130331e-06, 1.7240166302021928e-06, 1.5638583186843543e-06, 1.4173896015328477e-06, 1.2757433058786965e-06, 1.1710716763762923e-06, 1.0571232573393656e-06, 9.633831272706559e-07, 9.147764793090191e-07, 8.825286651253421e-07, 8.431014971895529e-07, 7.729211487873397e-07], "accuracy_test": 0.5257, "start": "2016-02-04 08:35:17.281000", "learning_rate_per_epoch": [0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 0.00010467752872500569, 1.0467752872500569e-05, 1.0467753099874244e-06, 1.0467753241982791e-07, 1.0467752886711423e-08, 1.0467753330800633e-09, 1.0467753053244877e-10, 1.0467753400189572e-11, 1.0467752966508703e-12, 1.0467753102033975e-13, 1.0467753440847154e-14, 1.0467753864363627e-15, 1.0467753599665831e-16, 1.0467753930538076e-17, 1.046775351694777e-18, 1.046775351694777e-19, 1.0467753355389057e-20, 1.0467753557337448e-21, 1.0467753809772938e-22, 1.0467753494228576e-23, 1.046775329701335e-24, 1.0467752803975284e-25, 1.0467752803975284e-26, 1.0467752803975284e-27, 1.0467753044716527e-28, 1.0467752743789973e-29, 1.0467752743789973e-30, 1.0467752508691103e-31, 1.0467752214817515e-32, 1.0467752398488508e-33, 1.0467751939311027e-34, 1.046775208280399e-35, 1.0467752441536396e-36, 1.0467752217328642e-37, 1.0467752777848028e-38, 1.0467755580444957e-39, 1.0467699528506384e-40, 1.0467699528506384e-41, 1.0467699528506384e-42, 1.0509738482436128e-43, 1.1210387714598537e-44, 1.401298464324817e-45, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "accuracy_train_first": 0.1607529411764706, "accuracy_train_last": 0.5320470588235294, "batch_size_eval": 1024, "accuracy_train_std": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "accuracy_test_std": 0, "error_valid": [0.8433333333333333, 0.7884, 0.7598666666666667, 0.7452, 0.7293333333333334, 0.7196, 0.71, 0.7014666666666667, 0.6872, 0.6773333333333333, 0.6648000000000001, 0.6566666666666667, 0.6512, 0.6432, 0.6389333333333334, 0.6329333333333333, 0.6304000000000001, 0.6262666666666667, 0.6225333333333334, 0.6197333333333334, 0.6185333333333334, 0.6150666666666667, 0.6150666666666667, 0.6133333333333333, 0.6113333333333333, 0.6102666666666667, 0.6078666666666667, 0.6049333333333333, 0.6038666666666667, 0.6, 0.5993333333333333, 0.5970666666666666, 0.5952, 0.5950666666666666, 0.5933333333333333, 0.5912, 0.5909333333333333, 0.5900000000000001, 0.5878666666666666, 0.5854666666666667, 0.5844, 0.5831999999999999, 0.5810666666666666, 0.5798666666666666, 0.5781333333333334, 0.5754666666666667, 0.5760000000000001, 0.5754666666666667, 0.5741333333333334, 0.5713333333333334, 0.5710666666666666, 0.5708, 0.5676, 0.5650666666666666, 0.5645333333333333, 0.5622666666666667, 0.5609333333333333, 0.5594666666666667, 0.5562666666666667, 0.5546666666666666, 0.5538666666666667, 0.5502666666666667, 0.5490666666666666, 0.5484, 0.5470666666666666, 0.546, 0.5449333333333333, 0.5449333333333333, 0.5444, 0.5416000000000001, 0.5392, 0.5397333333333334, 0.538, 0.5386666666666666, 0.5356000000000001, 0.5336000000000001, 0.5327999999999999, 0.5314666666666666, 0.5282666666666667, 0.5256000000000001, 0.5257333333333334, 0.5224, 0.524, 0.5212, 0.5229333333333333, 0.5187999999999999, 0.5185333333333333, 0.516, 0.5142666666666666, 0.5134666666666667, 0.5133333333333333, 0.5114666666666667, 0.5107999999999999, 0.5089333333333333, 0.5078666666666667, 0.5062666666666666, 0.506, 0.5050666666666667, 0.5041333333333333, 0.5030666666666667, 0.5010666666666667, 0.5004, 0.4973333333333333, 0.49573333333333336, 0.4958666666666667, 0.4938666666666667, 0.49293333333333333, 0.4921333333333333, 0.4908, 0.4888, 0.488, 0.48693333333333333, 0.4849333333333333, 0.48640000000000005, 0.484, 0.48186666666666667, 0.4821333333333333, 0.4828, 0.48186666666666667, 0.47919999999999996, 0.4806666666666667, 0.4774666666666667, 0.4773333333333334, 0.4766666666666667, 0.4749333333333333, 0.47519999999999996, 0.4736, 0.47253333333333336, 0.47306666666666664, 0.47040000000000004, 0.46986666666666665, 0.4681333333333333, 0.4694666666666667, 0.46919999999999995, 0.46986666666666665, 0.46773333333333333, 0.46919999999999995, 0.4669333333333333, 0.46973333333333334, 0.4696, 0.46840000000000004, 0.4702666666666667, 0.4701333333333333, 0.4674666666666667, 0.46906666666666663, 0.46973333333333334, 0.46840000000000004, 0.46906666666666663, 0.46853333333333336, 0.46906666666666663, 0.46840000000000004, 0.46773333333333333, 0.46799999999999997, 0.4708, 0.4682666666666667, 0.46973333333333334, 0.4694666666666667, 0.4686666666666667, 0.47, 0.46973333333333334, 0.4693333333333334, 0.4696, 0.4693333333333334, 0.4696, 0.46986666666666665, 0.46853333333333336, 0.4694666666666667, 0.46919999999999995, 0.4694666666666667, 0.46840000000000004, 0.46906666666666663, 0.4688, 0.46973333333333334, 0.4696, 0.4686666666666667, 0.46773333333333333, 0.4688, 0.46919999999999995, 0.4694666666666667, 0.4694666666666667, 0.46919999999999995, 0.4686666666666667, 0.4689333333333333, 0.4694666666666667, 0.46986666666666665, 0.46840000000000004, 0.46986666666666665, 0.4696], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.09620918424659003, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 128, "valid_ratio": 0.15, "learning_rate": 0.0001046775287004667, "optimization": "nesterov_momentum", "nb_data_augmentation": 4, "learning_rate_decay_method": "discrete", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 8.726558566698343e-07, "rotation_range": [0, 0], "momentum": 0.7434614008999932}, "accuracy_valid_max": 0.5330666666666667, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        y_pred = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            y_pred.extend((nnet.predict(X[mini_batch]) == y[mini_batch]).tolist())\n        return np.mean(y_pred)\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            acc = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = acc\n            status[\"accuracy_train_std\"] = 0\n            acc = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = acc\n            status[\"accuracy_valid_std\"] = 0\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n\n    # rescaling to [-1, 1]\n    X_min = X_train.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X_train.max(axis=(0, 2, 3))[None, :, None, None]\n    def preprocess(a):\n        return (a / 255.) * 2 - 1\n        # return 2 * ((a - X_min) / (X_max - X_min)) - 1\n    X_train = preprocess(X_train)\n    X_valid = preprocess(X_valid)\n\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = preprocess(X_test)\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    acc = evaluate(X_test, y_test, batch_size_eval)\n    light.set(\"accuracy_test\", acc)\n    light.set(\"accuracy_test_std\", 0)\n    print(\"Test accuracy : {}+-{}\".format(acc, 0))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.5304, "accuracy_valid_std": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "accuracy_valid": [0.15666666666666668, 0.2116, 0.24013333333333334, 0.2548, 0.27066666666666667, 0.2804, 0.29, 0.2985333333333333, 0.3128, 0.32266666666666666, 0.3352, 0.3433333333333333, 0.3488, 0.3568, 0.36106666666666665, 0.36706666666666665, 0.3696, 0.3737333333333333, 0.3774666666666667, 0.38026666666666664, 0.3814666666666667, 0.38493333333333335, 0.38493333333333335, 0.38666666666666666, 0.38866666666666666, 0.3897333333333333, 0.39213333333333333, 0.3950666666666667, 0.39613333333333334, 0.4, 0.40066666666666667, 0.4029333333333333, 0.4048, 0.4049333333333333, 0.4066666666666667, 0.4088, 0.4090666666666667, 0.41, 0.41213333333333335, 0.4145333333333333, 0.4156, 0.4168, 0.4189333333333333, 0.42013333333333336, 0.42186666666666667, 0.4245333333333333, 0.424, 0.4245333333333333, 0.42586666666666667, 0.42866666666666664, 0.42893333333333333, 0.4292, 0.4324, 0.43493333333333334, 0.43546666666666667, 0.4377333333333333, 0.43906666666666666, 0.44053333333333333, 0.4437333333333333, 0.44533333333333336, 0.4461333333333333, 0.4497333333333333, 0.45093333333333335, 0.4516, 0.45293333333333335, 0.454, 0.4550666666666667, 0.4550666666666667, 0.4556, 0.4584, 0.4608, 0.46026666666666666, 0.462, 0.4613333333333333, 0.4644, 0.4664, 0.4672, 0.46853333333333336, 0.47173333333333334, 0.4744, 0.47426666666666667, 0.4776, 0.476, 0.4788, 0.4770666666666667, 0.4812, 0.48146666666666665, 0.484, 0.48573333333333335, 0.4865333333333333, 0.4866666666666667, 0.4885333333333333, 0.4892, 0.49106666666666665, 0.4921333333333333, 0.49373333333333336, 0.494, 0.49493333333333334, 0.4958666666666667, 0.49693333333333334, 0.49893333333333334, 0.4996, 0.5026666666666667, 0.5042666666666666, 0.5041333333333333, 0.5061333333333333, 0.5070666666666667, 0.5078666666666667, 0.5092, 0.5112, 0.512, 0.5130666666666667, 0.5150666666666667, 0.5136, 0.516, 0.5181333333333333, 0.5178666666666667, 0.5172, 0.5181333333333333, 0.5208, 0.5193333333333333, 0.5225333333333333, 0.5226666666666666, 0.5233333333333333, 0.5250666666666667, 0.5248, 0.5264, 0.5274666666666666, 0.5269333333333334, 0.5296, 0.5301333333333333, 0.5318666666666667, 0.5305333333333333, 0.5308, 0.5301333333333333, 0.5322666666666667, 0.5308, 0.5330666666666667, 0.5302666666666667, 0.5304, 0.5316, 0.5297333333333333, 0.5298666666666667, 0.5325333333333333, 0.5309333333333334, 0.5302666666666667, 0.5316, 0.5309333333333334, 0.5314666666666666, 0.5309333333333334, 0.5316, 0.5322666666666667, 0.532, 0.5292, 0.5317333333333333, 0.5302666666666667, 0.5305333333333333, 0.5313333333333333, 0.53, 0.5302666666666667, 0.5306666666666666, 0.5304, 0.5306666666666666, 0.5304, 0.5301333333333333, 0.5314666666666666, 0.5305333333333333, 0.5308, 0.5305333333333333, 0.5316, 0.5309333333333334, 0.5312, 0.5302666666666667, 0.5304, 0.5313333333333333, 0.5322666666666667, 0.5312, 0.5308, 0.5305333333333333, 0.5305333333333333, 0.5308, 0.5313333333333333, 0.5310666666666667, 0.5305333333333333, 0.5301333333333333, 0.5316, 0.5301333333333333, 0.5304], "seed": 674592866, "model": "residualv3", "loss_std": [0.20434948801994324, 0.09831232577562332, 0.061157822608947754, 0.053809285163879395, 0.053170815110206604, 0.05325422063469887, 0.054032303392887115, 0.055203862488269806, 0.056323613971471786, 0.05735918506979942, 0.05884391441941261, 0.06021181866526604, 0.062190692871809006, 0.06260479986667633, 0.06428977102041245, 0.06314493715763092, 0.0663408413529396, 0.06668509542942047, 0.06709839403629303, 0.06758644431829453, 0.06892374157905579, 0.06873596459627151, 0.06920910626649857, 0.06977449357509613, 0.06991031765937805, 0.07109110802412033, 0.07139389961957932, 0.07161686569452286, 0.0719035267829895, 0.0725967064499855, 0.07229621708393097, 0.07238595932722092, 0.07203282415866852, 0.07267201691865921, 0.07218016684055328, 0.07227765768766403, 0.07280523329973221, 0.07279939949512482, 0.07315920293331146, 0.07294483482837677, 0.07376547157764435, 0.07390128821134567, 0.07368717342615128, 0.07192373275756836, 0.0741138681769371, 0.07267945259809494, 0.07332921773195267, 0.07494576275348663, 0.07485763728618622, 0.07412166893482208, 0.07444816082715988, 0.07446228712797165, 0.0745939165353775, 0.07459518313407898, 0.07575203478336334, 0.07528100162744522, 0.0766080915927887, 0.07487042248249054, 0.07532626390457153, 0.07545983791351318, 0.07525712996721268, 0.0749380961060524, 0.07443808019161224, 0.07515160739421844, 0.07567214965820312, 0.07466419041156769, 0.07665535062551498, 0.07468580454587936, 0.07528715580701828, 0.07665114849805832, 0.07670201361179352, 0.07594616711139679, 0.07654575258493423, 0.07609277963638306, 0.0766664370894432, 0.07621770352125168, 0.07650664448738098, 0.07513070106506348, 0.07607285678386688, 0.07600448280572891, 0.07654327899217606, 0.07736765593290329, 0.07596104592084885, 0.07819584012031555, 0.07701627910137177, 0.07780548185110092, 0.07766272127628326, 0.0782112330198288, 0.07794616371393204, 0.07675019651651382, 0.07790008932352066, 0.07718860357999802, 0.0776849240064621, 0.07815957814455032, 0.0751301571726799, 0.0777183473110199, 0.07799015939235687, 0.07851208001375198, 0.07801306992769241, 0.07786284387111664, 0.07970315217971802, 0.07826127111911774, 0.07935748994350433, 0.07957828789949417, 0.07831989973783493, 0.0781739205121994, 0.0797724798321724, 0.0785878598690033, 0.07858731597661972, 0.07910466939210892, 0.07802849262952805, 0.08013927936553955, 0.07946328073740005, 0.07883726060390472, 0.07959684729576111, 0.07903563976287842, 0.08059414476156235, 0.07910904288291931, 0.07941348850727081, 0.07994125038385391, 0.07797472923994064, 0.07858049869537354, 0.07968880981206894, 0.07808372378349304, 0.0791725367307663, 0.07699015736579895, 0.07933159917593002, 0.08034533262252808, 0.07947824895381927, 0.07878028601408005, 0.07992838323116302, 0.0794924721121788, 0.07831038534641266, 0.07865006476640701, 0.08063014596700668, 0.07836145162582397, 0.07851006090641022, 0.07977709919214249, 0.07853063941001892, 0.07920467853546143, 0.07814871519804001, 0.07994524389505386, 0.07998034358024597, 0.07937372475862503, 0.07910218089818954, 0.07884380966424942, 0.07930029183626175, 0.07926028966903687, 0.07841096073389053, 0.07988341897726059, 0.0792899951338768, 0.07893575727939606, 0.07826357334852219, 0.07749068737030029, 0.07943207770586014, 0.07942205667495728, 0.07924605906009674, 0.07913705706596375, 0.07896111160516739, 0.07998043298721313, 0.08016208559274673, 0.07983127236366272, 0.07896238565444946, 0.0790589302778244, 0.07979610562324524, 0.07937315851449966, 0.07867711037397385, 0.07832309603691101, 0.0786106288433075, 0.08014657348394394, 0.07859567552804947, 0.07874691486358643, 0.08003661036491394, 0.07965406775474548, 0.07897911965847015, 0.07811225205659866, 0.07792560011148453, 0.07809612900018692, 0.0792335644364357, 0.08015675097703934, 0.0797332376241684, 0.07955372333526611, 0.07928544282913208, 0.08056198805570602, 0.07872791588306427, 0.08005151152610779, 0.07836642861366272, 0.07869328558444977]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:40 2016", "state": "available"}], "summary": "9323a3469fdfeda6c616912fffed1ec0"}