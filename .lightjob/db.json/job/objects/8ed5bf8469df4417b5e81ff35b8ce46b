{"content": {"hp_model": {"f0": 64, "f1": 16, "f2": 64, "f3": 64, "nonlin": "very_leaky_rectify", "nbg1": 2, "nbg3": 2, "nbg2": 2, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.06324490800008505, 0.06131102292392643, 0.05477153417744408, 0.05750032356895051, 0.05468550246552759, 0.04871043611947418, 0.05490715055676383, 0.054622848839714994, 0.05387922572169353, 0.05553468491521743, 0.05133836051167099, 0.05107435460123453, 0.04771984261758415, 0.047190683114508296, 0.05322648339577894, 0.050190938660454466, 0.04488967706868366, 0.052018574138390124, 0.05080722180757453, 0.050232496131832796, 0.04892233599287678, 0.049361835997632816, 0.05091487363382213, 0.04655985786527862, 0.05110367838932221, 0.05569054370022403, 0.050810731975380094, 0.05112461365466052, 0.051925233964726834, 0.0517063250700529, 0.05140016001988124, 0.05504048864911571, 0.05148856377731122, 0.051844799490956016, 0.05832993341114079, 0.050379270388978964, 0.048518199810894794, 0.05387823266912391, 0.04398301337055595, 0.04971752633385036, 0.05144455422844398, 0.05103103630798288, 0.05103767617358138, 0.04982466896272602, 0.05386631461030945, 0.0508054666327272, 0.05103103630798289, 0.05012978307569568, 0.05736057781605182, 0.04918411406163781, 0.04677425588881996, 0.05596655428075313, 0.05425691351676346, 0.05166560788983232, 0.05059969090502122, 0.04907811682194983, 0.058245483260254735, 0.04867234274023667, 0.04425385973938984, 0.05297256187689477, 0.05112845085678933, 0.05303346594995935, 0.05012053213439307, 0.04867087701485494, 0.05045532529304861, 0.04987118091797221, 0.05157543282613492, 0.05112356709500375, 0.052388910175356115, 0.051674927363417396, 0.05166698865865746, 0.05096214039942667, 0.046434431222437456, 0.04963459222703323, 0.048620649247396584, 0.05100831444198803, 0.051415771727979204, 0.051027191780362025, 0.05006249689634332, 0.05103348267476514, 0.04874667043030491, 0.04720693133337771, 0.04837905126653548, 0.0544069267167068, 0.0476745986559087, 0.048206955562404695, 0.0467395451381086, 0.0503240144089916, 0.05035023313901524, 0.043690082949408925, 0.05171529234212602, 0.0487547188423275, 0.04800155833227382, 0.05411109893979021, 0.04853547346444811, 0.051559177647388064, 0.055443404118516254, 0.051611037973697815, 0.054145695463305825, 0.04830268101052406, 0.04896679130851785, 0.05184858339370762, 0.04910245833414925, 0.05006071561076075, 0.05604234617253822, 0.04711503636226121, 0.04914929087746599, 0.05478423195430678, 0.0500143799177155, 0.050436233916205817, 0.04653993494700863, 0.0459584135664011, 0.05415853997180334, 0.049506148061131586, 0.05284514358350827, 0.05253746784546637, 0.047921237121115776, 0.05658864406017791, 0.047224684619830676, 0.04785941717633959, 0.04926780647889079, 0.04876569180893383, 0.05200245747907454, 0.05034421112535647, 0.05043446583057376, 0.05120060639934599, 0.046770442780488745, 0.045617202915376465, 0.04991407656153883, 0.05164316522224372, 0.05059228852243225, 0.04563362055892505, 0.048851926521398026, 0.04871373126144443, 0.046954642024650634, 0.05186715491244314, 0.04974549883257614, 0.047062769209359576, 0.04548447213121368], "moving_avg_accuracy_train": [0.04194747740963855, 0.09394625376506022, 0.15047783320783129, 0.20646732398343368, 0.25939457501882524, 0.3093210813723644, 0.3558103776025978, 0.3979590085170368, 0.4374082130870199, 0.47409378485060705, 0.5085250501908475, 0.5397932153524856, 0.5669791799618153, 0.5922536828993687, 0.6158690525612391, 0.6383771209496936, 0.66006981171617, 0.6772824276831072, 0.6936562180473266, 0.7097127536221121, 0.7234318020550816, 0.7359507265182482, 0.746721241215821, 0.7578289551966485, 0.7682706455203571, 0.7780564386490443, 0.7872378053263085, 0.7957669427153644, 0.8018994916667195, 0.8098731343675174, 0.8146444805693199, 0.8192516627834723, 0.8242923285834383, 0.8284383028034077, 0.8316072737278862, 0.8363442308430494, 0.8411816639334433, 0.8451612008533519, 0.8488839738403059, 0.8534416419683235, 0.8544632533739008, 0.8571146313497637, 0.8597150093292452, 0.8627471793300556, 0.8643983838368091, 0.867246949067586, 0.869575341510225, 0.8717297237748651, 0.8717484870901497, 0.874055001332942, 0.8754696254466358, 0.8760956674200445, 0.8764237889310521, 0.8772297345861396, 0.8781245133865618, 0.8791980748491105, 0.879526573087091, 0.881695338971153, 0.882800089712592, 0.8836602351088028, 0.8835919337364766, 0.8839093216881302, 0.8854727381638955, 0.8862091616366626, 0.8874249359850445, 0.8879755523262991, 0.8881887275153559, 0.8878275919626155, 0.8876625850253901, 0.8873799485108029, 0.8878691750452649, 0.888514204076883, 0.8897630283981104, 0.8903645681787813, 0.8915977838006622, 0.8929006371977045, 0.8935343310080546, 0.8947376561903816, 0.8960206676797771, 0.8968059314840885, 0.8967055341188122, 0.8961421907972923, 0.8971859159946715, 0.898141740810867, 0.8990255147719489, 0.8996938405537902, 0.8990152132755196, 0.8986444713154376, 0.9003462892441347, 0.8992188515847814, 0.8991054189865443, 0.899984598473432, 0.900764094198378, 0.9012632683628776, 0.9011901230024935, 0.9023126393167019, 0.9016968686079233, 0.9023545537350828, 0.9029464703495262, 0.9035992065977061, 0.9029277272029957, 0.9038176540308889, 0.9033384676940651, 0.9026789432138153, 0.9023042153080963, 0.9025081876025879, 0.9021270036314857, 0.9024640020635178, 0.9036732682728287, 0.9044933473190397, 0.9047466669546056, 0.9052499746567354, 0.9060394538476884, 0.9056957682520762, 0.9072642750112059, 0.9085512134739406, 0.9081022480000406, 0.9082064622060606, 0.908333199268587, 0.9071200788899211, 0.9072048518744229, 0.9080176874701131, 0.9089539646568368, 0.9106719906309121, 0.910441576206375, 0.9105471738568218, 0.9106469180675252, 0.9107225688812546, 0.9107012344328881, 0.908653607224539, 0.9081402796346152, 0.9092054873639248, 0.9087781615190986, 0.9093913092226105, 0.909860781463, 0.9099750421721217, 0.9104779144609335, 0.9108363730148402, 0.9112554653820308], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 1234, "moving_var_accuracy_train": [0.01583631774928923, 0.03858754065651075, 0.06349116185950873, 0.08535545336935926, 0.10203155315189355, 0.11426230216673408, 0.12228736392590224, 0.12604719132496645, 0.12744862986330915, 0.12681624745756997, 0.1248042310081834, 0.12112309128054438, 0.11566247219814751, 0.10984542946699632, 0.10388005767869776, 0.09805157019404205, 0.09248156866884759, 0.0858998791377902, 0.07972280032203381, 0.07407083130180958, 0.06835765878078409, 0.06293240413013591, 0.057683199598778004, 0.05302531142781905, 0.048704040356383226, 0.04469549204516206, 0.04098462028720726, 0.037540873919899055, 0.03412525993767605, 0.031284944745188344, 0.028361341971866603, 0.025716242926269558, 0.023373293439005127, 0.021190666015198475, 0.019161980804160338, 0.017447731588142362, 0.015913565259464465, 0.014464739160390266, 0.013142996592762787, 0.012015647982372835, 0.010823476392911601, 0.009804397000158461, 0.008884814990868165, 0.00807907998600568, 0.007295710274313218, 0.006639168161747817, 0.006024044047875481, 0.005463411909567697, 0.004917073887168932, 0.004473246570021871, 0.0040439323654670786, 0.003643066485892596, 0.00327972881083721, 0.0029576018653440796, 0.0026690473407248363, 0.0024125154145771786, 0.0021722350729506674, 0.001997343474794441, 0.0018085933951213853, 0.0016343927065328486, 0.0014709954215767185, 0.0013248024954257403, 0.0012143206855734154, 0.0010977694927972548, 0.001001295508913181, 0.0009038945632191724, 0.0008139140998483199, 0.000733696459850566, 0.000660571859469502, 0.0005952336241169538, 0.0005378643451234525, 0.00048782247267577884, 0.0004530762850758017, 0.00041102530753778775, 0.00038361016371446906, 0.000360525990110682, 0.0003280875017070973, 0.00030831067498619005, 0.0002922946738248608, 0.00026861495962363, 0.00024184418033985683, 0.00022051596358697948, 0.00020826862781707999, 0.00019566417474866854, 0.0001831272650023805, 0.0001688344726582059, 0.00015609584023770174, 0.0001417233026226211, 0.00015361663072227617, 0.00014969500873159963, 0.0001348413104475252, 0.00012831378853425034, 0.00012095093194770649, 0.00011109841237147021, 0.00010003672332803473, 0.00010137343687620918, 9.46486552806959e-05, 8.907673729100734e-05, 8.332235106799528e-05, 7.882469744838765e-05, 7.500018890123437e-05, 7.462789784214738e-05, 6.923168396652039e-05, 6.622326843030829e-05, 6.086473061719753e-05, 5.51526998277592e-05, 5.0945140823410185e-05, 4.687273822979834e-05, 5.534638729164819e-05, 5.58645153407941e-05, 5.08556013465839e-05, 4.804990899913419e-05, 4.885441463575057e-05, 4.50320512698574e-05, 6.267076722379255e-05, 7.130958596321003e-05, 6.599275733767802e-05, 5.9491227010537704e-05, 5.368666485664449e-05, 6.156294784919122e-05, 5.547133119438429e-05, 5.587051342553587e-05, 5.817299681639418e-05, 7.89202163631333e-05, 7.150601199013237e-05, 6.445576856513819e-05, 5.809973187674389e-05, 5.23412660996308e-05, 4.7111235917851616e-05, 8.013510698540817e-05, 7.449314321805998e-05, 7.725583645548156e-05, 7.117371920884141e-05, 6.743989824485374e-05, 6.267954608083589e-05, 5.6529091059593236e-05, 5.3152106803328485e-05, 4.8993328936815235e-05, 4.56747417532711e-05], "duration": 21464.902973, "accuracy_train": [0.41947477409638556, 0.5619352409638554, 0.6592620481927711, 0.7103727409638554, 0.7357398343373494, 0.7586596385542169, 0.7742140436746988, 0.7772966867469879, 0.7924510542168675, 0.8042639307228916, 0.8184064382530121, 0.8212067018072289, 0.8116528614457831, 0.8197242093373494, 0.8284073795180723, 0.8409497364457831, 0.8553040286144579, 0.8321959713855421, 0.8410203313253012, 0.8542215737951807, 0.8469032379518072, 0.848621046686747, 0.8436558734939759, 0.8577983810240963, 0.8622458584337349, 0.8661285768072289, 0.8698701054216867, 0.8725291792168675, 0.8570924322289156, 0.8816359186746988, 0.8575865963855421, 0.8607163027108434, 0.8696583207831325, 0.8657520707831325, 0.8601280120481928, 0.8789768448795181, 0.8847185617469879, 0.8809770331325302, 0.8823889307228916, 0.8944606551204819, 0.8636577560240963, 0.8809770331325302, 0.8831184111445783, 0.8900367093373494, 0.8792592243975904, 0.8928840361445783, 0.8905308734939759, 0.8911191641566265, 0.8719173569277109, 0.8948136295180723, 0.8882012424698795, 0.8817300451807228, 0.8793768825301205, 0.8844832454819277, 0.8861775225903614, 0.8888601280120482, 0.8824830572289156, 0.9012142319277109, 0.8927428463855421, 0.8914015436746988, 0.8829772213855421, 0.8867658132530121, 0.8995434864457831, 0.8928369728915663, 0.8983669051204819, 0.8929310993975904, 0.8901073042168675, 0.8845773719879518, 0.8861775225903614, 0.8848362198795181, 0.8922722138554217, 0.8943194653614458, 0.9010024472891566, 0.8957784262048193, 0.9026967243975904, 0.9046263177710844, 0.8992375753012049, 0.9055675828313253, 0.9075677710843374, 0.9038733057228916, 0.8958019578313253, 0.8910721009036144, 0.9065794427710844, 0.9067441641566265, 0.9069794804216867, 0.9057087725903614, 0.8929075677710844, 0.8953077936746988, 0.9156626506024096, 0.8890719126506024, 0.8980845256024096, 0.9078972138554217, 0.9077795557228916, 0.9057558358433735, 0.9005318147590361, 0.9124152861445783, 0.8961549322289156, 0.9082737198795181, 0.9082737198795181, 0.9094738328313253, 0.8968844126506024, 0.9118269954819277, 0.8990257906626506, 0.8967432228915663, 0.8989316641566265, 0.9043439382530121, 0.8986963478915663, 0.9054969879518072, 0.9145566641566265, 0.9118740587349398, 0.9070265436746988, 0.9097797439759037, 0.9131447665662651, 0.9026025978915663, 0.9213808358433735, 0.9201336596385542, 0.9040615587349398, 0.909144390060241, 0.9094738328313253, 0.8962019954819277, 0.9079678087349398, 0.9153332078313253, 0.9173804593373494, 0.9261342243975904, 0.9083678463855421, 0.9114975527108434, 0.9115446159638554, 0.9114034262048193, 0.9105092243975904, 0.8902249623493976, 0.9035203313253012, 0.9187923569277109, 0.9049322289156626, 0.9149096385542169, 0.914086031626506, 0.9110033885542169, 0.915003765060241, 0.9140625, 0.915027296686747], "end": "2016-01-18 06:13:43.412000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0], "accuracy_valid": [0.4149305555555556, 0.5527510683760684, 0.6439636752136753, 0.6940438034188035, 0.7114049145299145, 0.7311698717948718, 0.7458600427350427, 0.749465811965812, 0.7598824786324786, 0.765625, 0.7740384615384616, 0.7749732905982906, 0.7705662393162394, 0.7680288461538461, 0.7752403846153846, 0.7889957264957265, 0.797676282051282, 0.7724358974358975, 0.7832532051282052, 0.7891292735042735, 0.7813835470085471, 0.7872596153846154, 0.7759081196581197, 0.7881944444444444, 0.7951388888888888, 0.7980769230769231, 0.8011485042735043, 0.7959401709401709, 0.7857905982905983, 0.8082264957264957, 0.7875267094017094, 0.7833867521367521, 0.7903311965811965, 0.7905982905982906, 0.7859241452991453, 0.7987446581196581, 0.8031517094017094, 0.8042200854700855, 0.8040865384615384, 0.8098290598290598, 0.7852564102564102, 0.796875, 0.8022168803418803, 0.8028846153846154, 0.7946047008547008, 0.8126335470085471, 0.8020833333333334, 0.8084935897435898, 0.7819177350427351, 0.8075587606837606, 0.7999465811965812, 0.7988782051282052, 0.7984775641025641, 0.7944711538461539, 0.7967414529914529, 0.8060897435897436, 0.7930021367521367, 0.8087606837606838, 0.7991452991452992, 0.8004807692307693, 0.7984775641025641, 0.796340811965812, 0.8095619658119658, 0.8096955128205128, 0.8020833333333334, 0.8030181623931624, 0.8004807692307693, 0.7946047008547008, 0.7919337606837606, 0.7924679487179487, 0.8006143162393162, 0.8023504273504274, 0.8162393162393162, 0.8070245726495726, 0.8038194444444444, 0.8038194444444444, 0.8038194444444444, 0.8146367521367521, 0.8092948717948718, 0.8014155982905983, 0.8036858974358975, 0.7954059829059829, 0.8074252136752137, 0.8134348290598291, 0.8048878205128205, 0.811698717948718, 0.7920673076923077, 0.8019497863247863, 0.8181089743589743, 0.7984775641025641, 0.797676282051282, 0.8111645299145299, 0.8090277777777778, 0.8050213675213675, 0.8024839743589743, 0.8139690170940171, 0.7982104700854701, 0.8078258547008547, 0.8054220085470085, 0.8079594017094017, 0.7988782051282052, 0.8084935897435898, 0.7958066239316239, 0.7970085470085471, 0.7955395299145299, 0.8086271367521367, 0.8002136752136753, 0.8071581196581197, 0.8088942307692307, 0.8092948717948718, 0.8072916666666666, 0.8159722222222222, 0.8074252136752137, 0.8078258547008547, 0.8112980769230769, 0.8161057692307693, 0.8078258547008547, 0.8070245726495726, 0.8056891025641025, 0.797676282051282, 0.8082264957264957, 0.8074252136752137, 0.812767094017094, 0.8179754273504274, 0.8054220085470085, 0.8084935897435898, 0.8112980769230769, 0.8047542735042735, 0.8139690170940171, 0.7905982905982906, 0.8035523504273504, 0.812767094017094, 0.8080929487179487, 0.8048878205128205, 0.8115651709401709, 0.8044871794871795, 0.8150373931623932, 0.8114316239316239, 0.8114316239316239], "accuracy_test": 0.81640625, "start": "2016-01-18 00:15:58.509000", "learning_rate_per_epoch": [0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847, 0.002281801775097847], "accuracy_train_last": 0.915027296686747, "error_valid": [0.5850694444444444, 0.44724893162393164, 0.35603632478632474, 0.30595619658119655, 0.2885950854700855, 0.2688301282051282, 0.2541399572649573, 0.25053418803418803, 0.2401175213675214, 0.234375, 0.22596153846153844, 0.22502670940170943, 0.22943376068376065, 0.23197115384615385, 0.22475961538461542, 0.21100427350427353, 0.20232371794871795, 0.22756410256410253, 0.21674679487179482, 0.21087072649572647, 0.21861645299145294, 0.21274038461538458, 0.22409188034188032, 0.21180555555555558, 0.20486111111111116, 0.20192307692307687, 0.19885149572649574, 0.2040598290598291, 0.21420940170940173, 0.19177350427350426, 0.21247329059829057, 0.21661324786324787, 0.20966880341880345, 0.20940170940170943, 0.21407585470085466, 0.2012553418803419, 0.19684829059829057, 0.1957799145299145, 0.19591346153846156, 0.19017094017094016, 0.21474358974358976, 0.203125, 0.19778311965811968, 0.19711538461538458, 0.2053952991452992, 0.18736645299145294, 0.19791666666666663, 0.19150641025641024, 0.2180822649572649, 0.19244123931623935, 0.20005341880341876, 0.20112179487179482, 0.2015224358974359, 0.20552884615384615, 0.20325854700854706, 0.1939102564102564, 0.2069978632478633, 0.19123931623931623, 0.2008547008547008, 0.19951923076923073, 0.2015224358974359, 0.20365918803418803, 0.19043803418803418, 0.19030448717948723, 0.19791666666666663, 0.19698183760683763, 0.19951923076923073, 0.2053952991452992, 0.20806623931623935, 0.20753205128205132, 0.19938568376068377, 0.1976495726495726, 0.18376068376068377, 0.1929754273504274, 0.19618055555555558, 0.19618055555555558, 0.19618055555555558, 0.18536324786324787, 0.1907051282051282, 0.19858440170940173, 0.19631410256410253, 0.20459401709401714, 0.1925747863247863, 0.1865651709401709, 0.19511217948717952, 0.18830128205128205, 0.2079326923076923, 0.1980502136752137, 0.18189102564102566, 0.2015224358974359, 0.20232371794871795, 0.18883547008547008, 0.1909722222222222, 0.19497863247863245, 0.19751602564102566, 0.18603098290598286, 0.20178952991452992, 0.19217414529914534, 0.19457799145299148, 0.19204059829059827, 0.20112179487179482, 0.19150641025641024, 0.20419337606837606, 0.20299145299145294, 0.20446047008547008, 0.1913728632478633, 0.19978632478632474, 0.19284188034188032, 0.19110576923076927, 0.1907051282051282, 0.19270833333333337, 0.1840277777777778, 0.1925747863247863, 0.19217414529914534, 0.18870192307692313, 0.18389423076923073, 0.19217414529914534, 0.1929754273504274, 0.19431089743589747, 0.20232371794871795, 0.19177350427350426, 0.1925747863247863, 0.18723290598290598, 0.1820245726495726, 0.19457799145299148, 0.19150641025641024, 0.18870192307692313, 0.19524572649572647, 0.18603098290598286, 0.20940170940170943, 0.1964476495726496, 0.18723290598290598, 0.19190705128205132, 0.19511217948717952, 0.1884348290598291, 0.19551282051282048, 0.1849626068376068, 0.18856837606837606, 0.18856837606837606], "accuracy_train_std": [0.064144209747652, 0.06275577291115834, 0.06332209847214756, 0.057541481686046905, 0.05662620796647357, 0.05434369187039372, 0.05614832973231273, 0.05318605708490921, 0.05414958082531481, 0.05177794206124467, 0.051687703731354555, 0.05066644990126926, 0.05163051214866859, 0.05287253813033025, 0.05281243571451654, 0.050549652865897785, 0.04947724109951932, 0.05081174581307924, 0.05000873145564557, 0.04947294327671866, 0.05002792803290646, 0.04852633913252419, 0.04869088734745745, 0.04654533713987643, 0.047162710233960146, 0.04564683688318265, 0.04773937804242364, 0.04590165104400683, 0.04887499564904536, 0.046068915209212684, 0.048703440976625066, 0.04974471745957317, 0.04821822057490978, 0.04732152752883362, 0.048123044968518186, 0.04552196036584686, 0.04496173411681725, 0.046444897674155314, 0.04633661087889562, 0.04284658172476565, 0.048653639513260044, 0.044247542959918874, 0.04618652870489803, 0.045731771248042494, 0.04436713917945605, 0.04545784037496637, 0.04397722926316208, 0.04394527942340411, 0.046414479478397844, 0.0442119873936524, 0.04385602685224693, 0.04622260796375352, 0.04448398658331596, 0.046213041106050984, 0.04509680741477731, 0.043677532387809304, 0.042651850390194895, 0.042829603047732416, 0.04396813729045919, 0.04415821790190574, 0.04400209665815869, 0.04311394714235802, 0.04271631934265347, 0.04496408020585211, 0.04284229083315766, 0.04449902124782316, 0.04449792617890993, 0.04473796664856451, 0.04506418295822372, 0.043510251032342205, 0.04399664100756149, 0.04385961256251977, 0.039456349063209384, 0.04369537916228809, 0.04149371810672222, 0.04101766354693211, 0.0423688225851671, 0.039853315767056086, 0.03974034015006387, 0.042305329337921097, 0.04232041180332537, 0.04333919384296611, 0.041302958600411756, 0.04101993148008886, 0.04076464966078432, 0.04073149178743533, 0.043076986740800405, 0.04289963953640536, 0.03760963509141303, 0.04364768485646999, 0.04168379725839361, 0.040913507244780704, 0.038867344999306136, 0.03877120228368926, 0.041712056535840085, 0.03979278057243793, 0.04051257059807215, 0.04040820865461057, 0.04032623286050564, 0.04042582077890639, 0.041015915262868435, 0.039420461382051006, 0.04221581009441325, 0.04280024449955168, 0.041726444411572965, 0.040735624404163624, 0.041285653595397424, 0.0392337072215108, 0.03998044118740209, 0.040143982369240744, 0.041149804455074435, 0.039956424637521935, 0.039126967403839405, 0.04122772527274717, 0.0373412955577654, 0.037170645420270224, 0.040440556729996836, 0.03958709937707387, 0.039922449857079545, 0.042212976764863235, 0.03968338637321983, 0.04008272571356011, 0.03712266791354618, 0.033650184764382134, 0.04065739377659761, 0.04049567307129518, 0.03932871227714071, 0.03901903445042367, 0.039360969777568616, 0.043152608250619263, 0.04043037498032714, 0.0380669209865758, 0.039961773673378116, 0.03890238359364304, 0.03778014910662182, 0.039986015507716736, 0.038376843669407315, 0.03962323442058706, 0.039172794090686386], "accuracy_test_std": 0.047426806852381924, "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-3, 3], "momentum": 0.5556057228339502, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.0022818017471818173, "patience_threshold": 1, "do_flip": true, "batch_size": 64, "optimization": "adam", "nb_data_augmentation": 4, "learning_rate_decay_method": "none", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 6.02054573424287e-09, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.00390637445721096}, "accuracy_valid_max": 0.8181089743589743, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = 1234\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-10, -6], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128, 256],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-3, 3)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_optimizer.learning_rate = learning_rate\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8114316239316239, "loss_train": [1.6806467771530151, 1.2914096117019653, 1.0870373249053955, 0.9581131935119629, 0.8737342953681946, 0.8200303912162781, 0.7735678553581238, 0.7357191443443298, 0.7038888335227966, 0.6885749101638794, 0.6600605249404907, 0.6378082036972046, 0.6256317496299744, 0.6067872643470764, 0.5977906584739685, 0.5802698731422424, 0.5765997767448425, 0.5587384700775146, 0.549765944480896, 0.5482149720191956, 0.5320966243743896, 0.5308802723884583, 0.5264761447906494, 0.512951672077179, 0.5101425051689148, 0.5040450096130371, 0.5034178495407104, 0.4924749732017517, 0.48628130555152893, 0.48133277893066406, 0.47449538111686707, 0.4764670431613922, 0.47151368856430054, 0.46758508682250977, 0.4626009166240692, 0.46211716532707214, 0.45756587386131287, 0.45065751671791077, 0.4423926770687103, 0.4472534954547882, 0.44364434480667114, 0.4376189410686493, 0.4379864037036896, 0.42822393774986267, 0.4384790360927582, 0.42869776487350464, 0.4272562265396118, 0.4184196889400482, 0.42249011993408203, 0.41384318470954895, 0.4170525372028351, 0.41529807448387146, 0.4124842584133148, 0.4089635908603668, 0.4125406742095947, 0.4064052104949951, 0.4125004708766937, 0.4044968783855438, 0.40708228945732117, 0.4000534117221832, 0.400980681180954, 0.3993930518627167, 0.3976830840110779, 0.40010279417037964, 0.39044514298439026, 0.39586135745048523, 0.39064207673072815, 0.3889337182044983, 0.3951152265071869, 0.3877747058868408, 0.3769770562648773, 0.3901207447052002, 0.3904719948768616, 0.38147032260894775, 0.37995582818984985, 0.3804425597190857, 0.3772965669631958, 0.3792191445827484, 0.3755207061767578, 0.3833969831466675, 0.3748944401741028, 0.3697330951690674, 0.37998124957084656, 0.37375468015670776, 0.372672438621521, 0.381602942943573, 0.3649640679359436, 0.3733619749546051, 0.3717406690120697, 0.36464986205101013, 0.363884299993515, 0.3668195307254791, 0.36483022570610046, 0.36571136116981506, 0.36271151900291443, 0.37128159403800964, 0.36396434903144836, 0.36192935705184937, 0.356689989566803, 0.3626493811607361, 0.3561058044433594, 0.35392215847969055, 0.364153653383255, 0.3499200940132141, 0.35943058133125305, 0.36300715804100037, 0.35525980591773987, 0.3563426434993744, 0.3520219922065735, 0.3550393581390381, 0.3515600264072418, 0.35528847575187683, 0.3513365387916565, 0.35601067543029785, 0.3570069372653961, 0.34814292192459106, 0.34772610664367676, 0.3539104759693146, 0.34676045179367065, 0.3556762635707855, 0.3498343229293823, 0.3458632826805115, 0.350700318813324, 0.3496147394180298, 0.3429512679576874, 0.34869787096977234, 0.3485110402107239, 0.34682583808898926, 0.3449369966983795, 0.3370339870452881, 0.3438356816768646, 0.34698712825775146, 0.3409394919872284, 0.3408030867576599, 0.3380124568939209, 0.3397185504436493, 0.34056755900382996, 0.3365188539028168, 0.33965811133384705], "accuracy_train_first": 0.41947477409638556, "model": "residual", "loss_std": [0.23571063578128815, 0.16101455688476562, 0.14971989393234253, 0.1438671052455902, 0.141030952334404, 0.13623742759227753, 0.12951013445854187, 0.12629099190235138, 0.12622563540935516, 0.12657125294208527, 0.11682651191949844, 0.11769317835569382, 0.11560215801000595, 0.11379264295101166, 0.11475934833288193, 0.11407148838043213, 0.11567531526088715, 0.1131514236330986, 0.1136273443698883, 0.11477213352918625, 0.11005575209856033, 0.10657966881990433, 0.11307426542043686, 0.11205160617828369, 0.11132507026195526, 0.10535543411970139, 0.11287062615156174, 0.10420867055654526, 0.1044832244515419, 0.10413382202386856, 0.104970283806324, 0.10530021041631699, 0.10480032861232758, 0.10578455775976181, 0.10421442240476608, 0.10249335318803787, 0.10402656346559525, 0.10471385717391968, 0.10502119362354279, 0.10171434283256531, 0.1021113246679306, 0.09578422456979752, 0.10372593253850937, 0.10074777156114578, 0.10099887102842331, 0.10029210895299911, 0.10050718486309052, 0.09897398948669434, 0.09964227676391602, 0.09665308892726898, 0.09805530309677124, 0.09972459822893143, 0.09682927280664444, 0.09784924983978271, 0.10163300484418869, 0.096717469394207, 0.10062624514102936, 0.09562525898218155, 0.09842286258935928, 0.09810735285282135, 0.09450040757656097, 0.09880367666482925, 0.09186547249555588, 0.09575187414884567, 0.0945729911327362, 0.09574771672487259, 0.09503787010908127, 0.09444694966077805, 0.09737831354141235, 0.09466055035591125, 0.09537885338068008, 0.09670218825340271, 0.09630230069160461, 0.0948907658457756, 0.09302473813295364, 0.09368335455656052, 0.09422819316387177, 0.09045739471912384, 0.093671515583992, 0.09765055775642395, 0.0924069732427597, 0.09096958488225937, 0.09788527339696884, 0.09535010904073715, 0.09533187001943588, 0.09617368876934052, 0.08770227432250977, 0.09773296117782593, 0.0914420560002327, 0.09164939075708389, 0.09430438280105591, 0.09071013331413269, 0.09349671751260757, 0.09396469593048096, 0.09309910982847214, 0.09050973504781723, 0.09043318033218384, 0.09278793632984161, 0.0876278430223465, 0.09817899763584137, 0.08995742350816727, 0.08786527067422867, 0.09427067637443542, 0.09002497792243958, 0.08721039444208145, 0.09122460335493088, 0.09133046865463257, 0.09238293766975403, 0.08573546260595322, 0.095401331782341, 0.09037928283214569, 0.08895492553710938, 0.08752362430095673, 0.09198413789272308, 0.09202327579259872, 0.09160611778497696, 0.09132348001003265, 0.08770820498466492, 0.0906495600938797, 0.09466943144798279, 0.08851726353168488, 0.08967030048370361, 0.08835171163082123, 0.09074724465608597, 0.08656752109527588, 0.09075021743774414, 0.08868769556283951, 0.09212957322597504, 0.08722972869873047, 0.08516354113817215, 0.09026799350976944, 0.09278133511543274, 0.08996204286813736, 0.08983421325683594, 0.08512262254953384, 0.08908160775899887, 0.08783639967441559, 0.08710251748561859, 0.08666233718395233]}, "state": "available", "life": [{"dt": "Sun May 15 22:04:59 2016", "state": "available"}], "summary": "94d013253e913272e96731c5819789f2"}