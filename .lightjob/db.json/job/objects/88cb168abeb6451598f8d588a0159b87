{"content": {"hp_model": {"f0": 64, "f1": 16, "f2": 16, "f3": 16, "nonlin": "very_leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.7194629907608032, 1.3735500574111938, 1.235780119895935, 1.1433130502700806, 1.0729608535766602, 1.0161980390548706, 0.9710723161697388, 0.9343036413192749, 0.9004238843917847, 0.8731883764266968, 0.8497830629348755, 0.8288249969482422, 0.8103125691413879, 0.792890191078186, 0.7785308361053467, 0.7657126784324646, 0.7521518468856812, 0.7426219582557678, 0.7288147211074829, 0.7221769690513611, 0.7140939831733704, 0.706256628036499, 0.700247049331665, 0.6925243139266968, 0.6876444220542908, 0.6830486059188843, 0.6798723936080933, 0.6745105385780334, 0.6706570386886597, 0.6687852740287781, 0.6641566157341003, 0.6629725694656372, 0.6584044098854065, 0.6569383144378662, 0.6550942659378052, 0.6528780460357666, 0.649880051612854, 0.6471509337425232, 0.6461495161056519, 0.6474157571792603, 0.6452817916870117, 0.6431078314781189, 0.6431239247322083, 0.6396365761756897, 0.6406629681587219, 0.6405694484710693, 0.6377053260803223, 0.6379989981651306, 0.6399245262145996, 0.6371856331825256, 0.636988639831543, 0.6359878182411194, 0.6355817317962646, 0.6340437531471252, 0.6344146132469177, 0.6346290707588196, 0.6352916955947876, 0.6355037689208984, 0.6332129240036011, 0.631222665309906, 0.6330097913742065, 0.6333268880844116, 0.6335254311561584, 0.6321455836296082, 0.6334160566329956, 0.6312901377677917, 0.6315366625785828, 0.6315312385559082, 0.6307086944580078, 0.6324560046195984, 0.6300539970397949, 0.631346583366394, 0.6316614747047424, 0.6305888891220093, 0.6286308765411377, 0.6310715675354004, 0.6304898262023926, 0.6318761706352234, 0.6309965252876282, 0.6306013464927673, 0.6307961940765381, 0.6311660408973694, 0.6303685307502747, 0.6317248940467834, 0.6299141645431519, 0.6304906606674194, 0.6293739676475525, 0.6308981776237488, 0.630700945854187, 0.6307781934738159, 0.6313088536262512, 0.6296678781509399, 0.6326997876167297, 0.6310746073722839, 0.6308931708335876, 0.6316258907318115, 0.6300833225250244, 0.6328356266021729, 0.6310634613037109, 0.6295304894447327, 0.6311595439910889, 0.6307291984558105, 0.6290232539176941, 0.6293199062347412, 0.6310955286026001, 0.6290279030799866, 0.6324048042297363, 0.6310535073280334, 0.6315878629684448, 0.6309695243835449, 0.6300427913665771, 0.6298577785491943, 0.6316776275634766, 0.6290370225906372, 0.6294788122177124, 0.6304183006286621, 0.6321194171905518, 0.6311290860176086, 0.6326428055763245, 0.6301482915878296, 0.6308485269546509, 0.6305777430534363, 0.6305427551269531, 0.6322798132896423, 0.6309633851051331, 0.630506157875061, 0.6310778856277466, 0.6312126517295837, 0.6304807066917419, 0.630578875541687, 0.6296228766441345, 0.630670428276062, 0.6293849945068359, 0.6293631792068481, 0.6311466693878174, 0.6294007897377014, 0.6311535239219666, 0.6308633089065552, 0.6313310265541077, 0.6318091154098511, 0.631157636642456, 0.6316662430763245, 0.6302852034568787, 0.6309681534767151, 0.630908727645874, 0.63299161195755, 0.6316699385643005, 0.6290838718414307, 0.6290791630744934, 0.6308419108390808, 0.6320421099662781, 0.6301882863044739, 0.6287344098091125, 0.6314849257469177, 0.631365180015564, 0.6318413615226746, 0.6297001838684082, 0.6295168995857239, 0.631740152835846, 0.6317371726036072, 0.6293314695358276, 0.6288104057312012, 0.6311926245689392, 0.6310139894485474, 0.6309463381767273, 0.631540060043335, 0.6301230788230896, 0.6297298073768616, 0.6313762068748474, 0.6310076713562012, 0.6303848624229431, 0.631532609462738, 0.6290006041526794, 0.6308073401451111], "moving_avg_accuracy_train": [0.05462088178294572, 0.11086772030154113, 0.16393485005133349, 0.21598251679153974, 0.2656961825634747, 0.3122334245415311, 0.3563719040811949, 0.3975796922168185, 0.43591494596140645, 0.471332494571938, 0.5036129363119111, 0.5338858207099744, 0.5613895081860883, 0.5867563778097552, 0.6102632148234456, 0.6317170592810235, 0.651518414791644, 0.6698232657035833, 0.6866882204755099, 0.7018620655214436, 0.7156812864794506, 0.7285021267487721, 0.740215377298332, 0.7509688913346024, 0.7608282713791137, 0.769980839422773, 0.7782181506620663, 0.7857340733738681, 0.7928052513597095, 0.7990460065624243, 0.8050046633639615, 0.8102139946639163, 0.8151115480802766, 0.8196078099562191, 0.8236846004814535, 0.8274490070065363, 0.8309067633922153, 0.833932713633374, 0.8368421889016349, 0.8394699451406704, 0.8417930370284121, 0.843935009050008, 0.8459000222992153, 0.8476777266722838, 0.8493870146997305, 0.8509393608661086, 0.8522435025122865, 0.8534334699866947, 0.8544928149696144, 0.8554765605352221, 0.8564687081454134, 0.8573778809874335, 0.8580683254583653, 0.8587547936000518, 0.8593261840490167, 0.8599334053566474, 0.8603565995489727, 0.8608771274459119, 0.8613084001722049, 0.8617221222627732, 0.862006080440704, 0.8623476733067942, 0.862685369869618, 0.8629218314118643, 0.863125346204648, 0.8633410976503054, 0.8635747654323402, 0.8637177092183329, 0.8638858861554882, 0.8639976818203473, 0.8641564266389585, 0.8642457825042709, 0.8643635493592794, 0.8644881046704445, 0.8645582796742841, 0.8646051611360732, 0.86466363049335, 0.8647302038077563, 0.8647971312859692, 0.8649131335389707, 0.8650524488476337, 0.8650568888385164, 0.8651213386993584, 0.8651444663419733, 0.8652187956917645, 0.865299606950615, 0.8653467604466757, 0.8653776088979014, 0.8653913495135099, 0.865387440025891, 0.8654281353632336, 0.8654671223644702, 0.8654324562012974, 0.865503563202061, 0.8655768600979863, 0.8656009746257476, 0.865622641651914, 0.8656328774290445, 0.8656188381403667, 0.8656759572448424, 0.8656598990745755, 0.8657105508880021, 0.8657026951462857, 0.8656583865489699, 0.8656069191161566, 0.8655977287099397, 0.8656476942110387, 0.8656460880882, 0.8656050789990644, 0.8656356001343185, 0.8656236137239228, 0.8655942608129092, 0.8655748186394256, 0.8655548513392058, 0.8654835104928453, 0.8654960336418351, 0.8655631080473545, 0.865637425905179, 0.8656647483986404, 0.8656498471618125, 0.8656248103046198, 0.8656789709950419, 0.8656300593664218, 0.8656535042649587, 0.8656443777391181, 0.8656756913956234, 0.8656550455614782, 0.8657131581726432, 0.8657351965393489, 0.8657876192015362, 0.8657696954308381, 0.8657210480026952, 0.8657005168054618, 0.8657307947553142, 0.8657162282804288, 0.8657357065851841, 0.8657462255642164, 0.8657161651155836, 0.8656379934868234, 0.8656675843709298, 0.8657500197380543, 0.865738181062514, 0.8657461634938225, 0.8656952189617622, 0.8657237375959939, 0.8656750356537164, 0.865638215400914, 0.8656375932079065, 0.8656463338294377, 0.8656774158280923, 0.8656891135852148, 0.8656554997880628, 0.8656857372884924, 0.865680362906727, 0.8656336732845666, 0.8656567207424704, 0.865635646824831, 0.8656259808941936, 0.8656033306637628, 0.8655922100027945, 0.8655775150614852, 0.8656108646881348, 0.8656826959818721, 0.8657566086926549, 0.8657720129073687, 0.8657440240220398, 0.8658397778121577, 0.8658445760149305, 0.8658209926117117, 0.86578113030952, 0.8657545908816041, 0.8657307414452986, 0.8656766527716528, 0.8656814513879907], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.05386977597891566, 0.10874111681099397, 0.1607255682887801, 0.21106396778520328, 0.2590002456113516, 0.30403204404532186, 0.3465952658926722, 0.38603536044421827, 0.42237784873149825, 0.45584704216218874, 0.48653392821328917, 0.5152803200775777, 0.5415081356790067, 0.5651619978452928, 0.5872480488608388, 0.6072068259589416, 0.625700804742942, 0.6428001343307713, 0.6583451338400887, 0.6724577037109745, 0.684938260523612, 0.696600066766055, 0.7071434910005941, 0.7167068445078391, 0.7255335892268594, 0.7338937280451374, 0.7410872336291778, 0.7476468378735643, 0.753870953031991, 0.7593851484385058, 0.7646429520716883, 0.7691776038242334, 0.7731956962279546, 0.7770347944797826, 0.7804513027953586, 0.7834793911716962, 0.7861028964257615, 0.7886492156404896, 0.7912349011924046, 0.7934165633227875, 0.795402414285313, 0.7974257317799744, 0.7989638967891005, 0.8006066519708832, 0.8021716103618973, 0.80367772916381, 0.8047260012869621, 0.8057172448141394, 0.8066124525145779, 0.8074293169675628, 0.8083964285689993, 0.8092810950588614, 0.809818888226168, 0.8105491017190632, 0.811084223550169, 0.8116512824169141, 0.8120141215133251, 0.8126245269448239, 0.8130497625033535, 0.8135311602646899, 0.8137293961301636, 0.8140563518014092, 0.8143994400305303, 0.814709248945399, 0.8149015982413711, 0.815147954795246, 0.8155171895773932, 0.8157518446313256, 0.8159884777510243, 0.8160162830726839, 0.8161267570809276, 0.8161539710095066, 0.8163371549514776, 0.8164898134680015, 0.8165916145477827, 0.8165580766811069, 0.816639814899668, 0.8167388228675325, 0.816887935686201, 0.8170343442542525, 0.8171427274116585, 0.8171293794634144, 0.8172028155287446, 0.8171712517375418, 0.8172649146379592, 0.8173736253108349, 0.8175080860101731, 0.8174948232958276, 0.8176049571654165, 0.8175830368442061, 0.8175490425065475, 0.817593748807474, 0.817524121197058, 0.8176801534015239, 0.8177361626754529, 0.8178587837008292, 0.817810451217418, 0.8178412236785075, 0.8178556823535785, 0.8178809021923923, 0.8177815297348248, 0.8179006435629236, 0.817796208442324, 0.8177775180386037, 0.8177118685502553, 0.8177769133405611, 0.8177469159071075, 0.8177219772343184, 0.8176273197499679, 0.8176509617866429, 0.817535903258581, 0.8175198588193947, 0.8174932117928769, 0.817481436500261, 0.8174474541830662, 0.8174768757451812, 0.8175765973385848, 0.8175931045851479, 0.817683262311874, 0.8176636594899487, 0.8177711757886947, 0.8178180828239067, 0.8177249923031876, 0.8178619669057002, 0.8178499371955519, 0.817937796215078, 0.8178439118778322, 0.81790898887529, 0.8178810794455923, 0.8179037595752047, 0.8177776873168559, 0.8177852630881823, 0.8177320756347858, 0.8177818631767289, 0.8178144649332276, 0.8178926346390766, 0.8178897451868405, 0.8178983222024185, 0.8178205922976888, 0.8179235928382512, 0.8179674651997574, 0.817945915168863, 0.817950934203558, 0.8177835233886239, 0.8176938888114332, 0.8176620458169617, 0.8178042855594372, 0.8177491958589151, 0.8177718278072857, 0.8177810190382289, 0.8178259122398277, 0.8177421867914473, 0.8177177210302242, 0.8177557074927138, 0.8177054755988641, 0.8177090950193993, 0.817748973591631, 0.8178723725427088, 0.8178969528712693, 0.8178590695193834, 0.8177985014228667, 0.8178660604485017, 0.8178902424778233, 0.8179496569066222, 0.818001070875222, 0.8179883673080312, 0.81807253133024, 0.8180272381463878, 0.8179498531871707, 0.8179931585311042, 0.8180290448146654, 0.8179626567112109, 0.8178062806767615, 0.8178252631606666], "moving_var_accuracy_train": [0.02685096654071879, 0.052639231476679454, 0.07272039066794318, 0.08982898811904502, 0.10308912638749346, 0.11227164776706165, 0.11857833137501536, 0.12200323446478778, 0.12302923613526653, 0.12201593726795418, 0.1191925858115089, 0.11552135499836451, 0.11077729492158182, 0.10549086809996068, 0.09991492376745846, 0.09406583836880214, 0.08818809765244373, 0.0823848959893743, 0.07670624668556905, 0.0711078321783142, 0.06571578677145877, 0.06062357360121599, 0.05579601838702527, 0.05125715912547715, 0.047006309586688416, 0.0430596041441639, 0.03936432339782441, 0.03593629290592998, 0.032792677638304006, 0.029863933103975525, 0.027197090111484523, 0.02472161529367028, 0.022465328029498178, 0.02040074256426184, 0.018510250296715432, 0.01678676207541866, 0.015215690580881128, 0.013776528896550731, 0.012475061423925259, 0.011289701207198842, 0.010209301889748982, 0.009229664098045778, 0.008341449181867246, 0.007535746359222763, 0.006808466713347443, 0.0061493080495951145, 0.005549684313535266, 0.005007460085493083, 0.0045168139830793095, 0.004073842382812056, 0.003675317356454526, 0.003315224978119076, 0.0029879929024141313, 0.0026934347587586733, 0.002427029666289321, 0.0021876451591083547, 0.00197049248311728, 0.0017758817784289797, 0.0015999675660660787, 0.0014415113031734893, 0.0012980858630774648, 0.0011693274479451917, 0.0010534210538675592, 0.0009485821750294573, 0.0008540967219644476, 0.0007691059879447322, 0.0006926867948415084, 0.0006236020116909428, 0.0005614963618615668, 0.0005054592101115418, 0.0004551400883573109, 0.0004096979397575713, 0.0003688529670710615, 0.0003321072965938102, 0.0002989408877149042, 0.0002690665797865491, 0.00024219068979955744, 0.00021801150887532097, 0.00019625067157384834, 0.00017674671312077625, 0.00015924672060574924, 0.00014332222596684567, 0.00012902738743122405, 0.00011612946267877797, 0.00010456624008106366, 9.416839020897032e-05, 8.477156225778999e-05, 7.630297067449823e-05, 6.867437284770413e-05, 6.180707311977471e-05, 5.564127080213011e-05, 5.009082359830593e-05, 4.509255692429741e-05, 4.0628807081885994e-05, 3.661427828826799e-05, 3.295808405348357e-05, 2.9666500788341307e-05, 2.6700793649708366e-05, 2.4032488199376734e-05, 2.1658602708304026e-05, 1.9495063220964483e-05, 1.7568647354698606e-05, 1.5812338033329982e-05, 1.4248773496161911e-05, 1.2847736216309139e-05, 1.1563722766776126e-05, 1.0429819451799234e-05, 9.386860723294472e-06, 8.463310359490618e-06, 7.625363180816374e-06, 6.864119929042297e-06, 6.185462276602783e-06, 5.570318031930405e-06, 5.0168744664399655e-06, 4.5609926670309826e-06, 4.106304863673457e-06, 3.7361651601880677e-06, 3.412256940093928e-06, 3.0777499139250878e-06, 2.7719733442635778e-06, 2.5004176078000017e-06, 2.2767762705030017e-06, 2.070629770181134e-06, 1.8685137625696537e-06, 1.6824120275779677e-06, 1.5229957305737738e-06, 1.3745324117243587e-06, 1.2674728507394917e-06, 1.1450967721290728e-06, 1.055320314513401e-06, 9.526796370664046e-07, 8.787108237440003e-07, 7.946335119081078e-07, 7.234209489427319e-07, 6.529884937637483e-07, 5.911042835926026e-07, 5.32989695512291e-07, 4.878234011090987e-07, 4.940382928854945e-07, 4.5251504739679747e-07, 4.6842385043359214e-07, 4.2284285353718874e-07, 3.8113204106983857e-07, 3.663769450844862e-07, 3.3705906306200505e-07, 3.247000693902008e-07, 3.044316415990866e-07, 2.739919615564264e-07, 2.47280351583558e-07, 2.3124713218848999e-07, 2.0935395666491084e-07, 1.9858754722917824e-07, 1.869575503963349e-07, 1.685217511709483e-07, 1.712888634111149e-07, 1.5894064491237542e-07, 1.470435704631979e-07, 1.331800853526606e-07, 1.2447937326450898e-07, 1.1314445784142124e-07, 1.0377348375804511e-07, 1.0340591376123181e-07, 1.39502935224861e-07, 1.7472044103986595e-07, 1.5938400541443174e-07, 1.504960041906113e-07, 2.1796549866907682e-07, 1.9637615355080883e-07, 1.8174413036214262e-07, 1.7787074555023305e-07, 1.6642274210209825e-07, 1.5489962840068938e-07, 1.6573992711144767e-07, 1.4937317486912517e-07], "duration": 167509.389565, "accuracy_train": [0.5462088178294573, 0.6170892669689, 0.6415390177994648, 0.6844115174533961, 0.7131191745108896, 0.7310686023440385, 0.753618219938169, 0.7684497854374308, 0.7809322296626985, 0.790090432066722, 0.7941369119716685, 0.8063417802925433, 0.8089226954711147, 0.8150582044227574, 0.8218247479466593, 0.8248016593992249, 0.8297306143872278, 0.8345669239110374, 0.8384728134228497, 0.8384266709348468, 0.8400542751015135, 0.8438896891726652, 0.8456346322443706, 0.8477505176610374, 0.8495626917797158, 0.852353951815707, 0.852353951815707, 0.8533773777800849, 0.8564458532322813, 0.8552128033868586, 0.8586325745777963, 0.8570979763635106, 0.8591895288275194, 0.8600741668397011, 0.860375715208564, 0.8613286657322813, 0.862026570863326, 0.8611662658038022, 0.8630274663159838, 0.8631197512919897, 0.8627008640180879, 0.8632127572443706, 0.863585141542082, 0.8636770660299004, 0.8647706069467516, 0.8649104763635106, 0.8639807773278886, 0.8641431772563677, 0.8640269198158915, 0.8643302706256922, 0.8653980366371355, 0.8655604365656147, 0.8642823256967516, 0.8649330068752308, 0.8644686980897011, 0.8653983971253231, 0.8641653472799004, 0.8655618785183647, 0.8651898547088409, 0.8654456210778886, 0.864561704042082, 0.8654220091016058, 0.8657246389350315, 0.865049985292082, 0.8649569793397011, 0.865282860661222, 0.8656777754706534, 0.8650042032922666, 0.8653994785898856, 0.865003842804079, 0.86558513000646, 0.865049985292082, 0.8654234510543558, 0.8656091024709303, 0.8651898547088409, 0.8650270942921743, 0.8651898547088409, 0.8653293636374124, 0.8653994785898856, 0.8659571538159838, 0.8663062866255999, 0.86509684875646, 0.8657013874469361, 0.8653526151255077, 0.8658877598398856, 0.8660269082802695, 0.865771141911222, 0.8656552449589332, 0.8655150150539868, 0.8653522546373201, 0.8657943933993172, 0.8658180053755999, 0.8651204607327427, 0.8661435262089332, 0.8662365321613142, 0.8658180053755999, 0.8658176448874124, 0.865724999423219, 0.8654924845422666, 0.8661900291851238, 0.8655153755421743, 0.8661664172088409, 0.865631993470838, 0.8652596091731267, 0.865143712220838, 0.8655150150539868, 0.8660973837209303, 0.8656316329826504, 0.8652359971968439, 0.8659102903516058, 0.8655157360303618, 0.8653300846137875, 0.8653998390780732, 0.8653751456372278, 0.8648414428755999, 0.8656087419827427, 0.8661667776970285, 0.8663062866255999, 0.8659106508397933, 0.8655157360303618, 0.8653994785898856, 0.8661664172088409, 0.8651898547088409, 0.8658645083517904, 0.8655622390065523, 0.8659575143041713, 0.8654692330541713, 0.8662361716731267, 0.8659335418397011, 0.866259423161222, 0.8656083814945552, 0.8652832211494095, 0.8655157360303618, 0.8660032963039868, 0.86558513000646, 0.8659110113279809, 0.8658408963755077, 0.8654456210778886, 0.8649344488279809, 0.8659339023278886, 0.8664919380421743, 0.8656316329826504, 0.8658180053755999, 0.865236718173219, 0.865980405304079, 0.865236718173219, 0.8653068331256922, 0.865631993470838, 0.865724999423219, 0.8659571538159838, 0.8657943933993172, 0.8653529756136952, 0.8659578747923589, 0.865631993470838, 0.8652134666851238, 0.8658641478636029, 0.8654459815660761, 0.865538987518457, 0.8653994785898856, 0.865492124054079, 0.8654452605897011, 0.8659110113279809, 0.8663291776255077, 0.8664218230897011, 0.8659106508397933, 0.865492124054079, 0.866701561923219, 0.8658877598398856, 0.8656087419827427, 0.8654223695897933, 0.8655157360303618, 0.8655160965185493, 0.8651898547088409, 0.8657246389350315], "end": "2016-02-05 10:11:14.476000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0], "moving_var_accuracy_valid": [0.026117574876167032, 0.05060359379094127, 0.06986468317086386, 0.08568380502853097, 0.097796405112109, 0.10626753043267764, 0.11194542807565072, 0.11475057479219966, 0.11516250540717944, 0.1137279370465703, 0.11083030812153226, 0.10718447271631615, 0.10265711024568758, 0.09742694597955376, 0.0920743942267501, 0.08645212985334089, 0.08088516212937431, 0.0754281295676158, 0.07006013969855633, 0.06484660738394638, 0.05976382533073299, 0.05501142232118624, 0.05051075424037281, 0.04628279838907631, 0.04235572135118146, 0.03874917650561121, 0.03533997755833869, 0.03219323547309158, 0.029322568411330636, 0.02666396872902863, 0.024246372347531735, 0.0220068027114303, 0.01995142803937085, 0.018088933313918466, 0.016385092744160216, 0.014829107342678395, 0.01340814162677353, 0.012125681137985818, 0.010973284952147668, 0.009918793303793229, 0.008962406409822177, 0.008103010091997787, 0.007314002647155706, 0.0066068901837256005, 0.005968243018243489, 0.005391834261028414, 0.004862540704923174, 0.004385129708002394, 0.003953829308644474, 0.0035644517855909813, 0.0032164243506785806, 0.0029018256287952873, 0.002614246059332974, 0.0023576203591065335, 0.0021244355215630137, 0.0019148859712319012, 0.0017245822439976703, 0.0015554773727151326, 0.0014015570629657618, 0.0012634870509107624, 0.0011374920229449276, 0.0010247049207490722, 0.0009232938144708183, 0.0008318282670973252, 0.0007489784246525416, 0.0006746268061520217, 0.0006083911344559443, 0.0005480475879593739, 0.0004937467862634818, 0.00044437906586034697, 0.00040005099983278904, 0.00036005256523068845, 0.000324349315916984, 0.0002921241259292912, 0.00026300498447496363, 0.0002367146091239778, 0.00021310327843894174, 0.00019188117379435353, 0.00017289316810913935, 0.00015579677051741567, 0.0001403228156449579, 0.00012629213758996307, 0.00011371145953218744, 0.00010234928003520453, 9.219330668191565e-05, 8.308033810729799e-05, 7.493502141356683e-05, 6.744310236853647e-05, 6.08079573547585e-05, 5.473148612362032e-05, 4.9268738046193886e-05, 4.435985212165734e-05, 3.996749894668188e-05, 3.618986349148804e-05, 3.2599110491233646e-05, 2.947452268488951e-05, 2.654809467697484e-05, 2.3901807708530915e-05, 2.1513508417241083e-05, 1.9367881937945135e-05, 1.751996771205775e-05, 1.589566387725124e-05, 1.4404257739257977e-05, 1.2966975946053248e-05, 1.170906704933149e-05, 1.057623776711166e-05, 9.526712604524778e-06, 8.57963878067662e-06, 7.802315256701074e-06, 7.027114244114197e-06, 6.443549003620668e-06, 5.801510919517865e-06, 5.227750403766237e-06, 4.706223281035348e-06, 4.245994133869148e-06, 3.829185375337847e-06, 3.5357664035225625e-06, 3.184642165872193e-06, 2.9393336904808703e-06, 2.6488587570797185e-06, 2.488010671836301e-06, 2.2590120342239603e-06, 2.1111034362311466e-06, 2.068851468209297e-06, 1.8632687467246455e-06, 1.7464147378610299e-06, 1.6511016830955938e-06, 1.5241066551690763e-06, 1.3787064160466116e-06, 1.2454652689550906e-06, 1.263966670986045e-06, 1.138086534688158e-06, 1.0497380280086363e-06, 9.670734192024003e-07, 8.799319480234219e-07, 8.469332794336349e-07, 7.623150918982872e-07, 6.867456694744847e-07, 6.724485453307826e-07, 7.006856930031126e-07, 6.47940180639982e-07, 5.873257970599346e-07, 5.288199337373703e-07, 7.281753689757967e-07, 7.276670489316374e-07, 6.640261307107656e-07, 7.797128166951939e-07, 7.290554109581445e-07, 6.607597156457468e-07, 5.954440526174176e-07, 5.540382433037841e-07, 5.617239753318254e-07, 5.10938739048651e-07, 4.728316071360707e-07, 4.4825763486000786e-07, 4.0354977321910784e-07, 3.775075006063869e-07, 4.76802460689818e-07, 4.3455994759008933e-07, 4.0402028798212017e-07, 3.9663470802476555e-07, 3.9804923472505557e-07, 3.635072461315053e-07, 3.5892719066395733e-07, 3.46825037102217e-07, 3.1359495896635324e-07, 3.459877067791917e-07, 3.298521886325989e-07, 3.507628569866286e-07, 3.3256474660678017e-07, 3.1089870007665585e-07, 3.1947525259155345e-07, 5.076089046835144e-07, 4.600910264720558e-07], "accuracy_test": 0.8193658322704082, "start": "2016-02-03 11:39:25.086000", "learning_rate_per_epoch": [0.0004884247900918126, 0.0004442739300429821, 0.00040411404916085303, 0.0003675844054669142, 0.0003343568241689354, 0.00030413284548558295, 0.00027664093067869544, 0.0002516341337468475, 0.00022888781677465886, 0.00020819764176849276, 0.00018937773711513728, 0.00017225905321538448, 0.000156687805429101, 0.0001425241061951965, 0.00012964072811882943, 0.00011792193254223093, 0.00010726245091063902, 9.756652434589341e-05, 8.874705963535234e-05, 8.072482160059735e-05, 7.342774915741757e-05, 6.67902932036668e-05, 6.0752823628718033e-05, 5.526110908249393e-05, 5.026581493439153e-05, 4.572206671582535e-05, 4.1589046304579824e-05, 3.782962812692858e-05, 3.441004082560539e-05, 3.129956530756317e-05, 2.847026007657405e-05, 2.5896708393702284e-05, 2.3555790903628804e-05, 2.142648008884862e-05, 1.9489645637804642e-05, 1.7727890735841356e-05, 1.612538835615851e-05, 1.4667743926111143e-05, 1.3341861631488428e-05, 1.2135831639170647e-05, 1.1038820048270281e-05, 1.0040972483693622e-05, 9.133324965659995e-06, 8.307723874168005e-06, 7.5567522799246944e-06, 6.873664005979663e-06, 6.252323146327399e-06, 5.687148131983122e-06, 5.173061708774185e-06, 4.7054459173523355e-06, 4.280099801690085e-06, 3.893202574545285e-06, 3.5412788292887853e-06, 3.221167162337224e-06, 2.9299917514435947e-06, 2.6651368898455985e-06, 2.4242233394033974e-06, 2.205087184847798e-06, 2.005759597523138e-06, 1.8244501234221389e-06, 1.6595300849076011e-06, 1.5095179151103366e-06, 1.3730659702559933e-06, 1.2489484788602567e-06, 1.136050514105591e-06, 1.0333578757126816e-06, 9.399481086802552e-07, 8.549820904590888e-07, 7.776964707773004e-07, 7.073970778037619e-07, 6.434523243115109e-07, 5.852878643963777e-07, 5.323811365087749e-07, 4.842568728236074e-07, 4.404827507187292e-07, 4.0066558426588017e-07, 3.644476578301692e-07, 3.3150362810374645e-07, 3.015375682480226e-07, 2.742802678312728e-07, 2.4948687382675416e-07, 2.2693465950851532e-07, 2.0642103493173636e-07, 1.8776172794332524e-07, 1.7078912151191616e-07, 1.5535074737726973e-07, 1.4130790759736556e-07, 1.2853446662575152e-07, 1.1691567181060236e-07, 1.0634715152946228e-07, 9.673396306197901e-08, 8.798975414947563e-08, 8.003597429251386e-08, 7.280117131358566e-08, 6.622035186865105e-08, 6.023440590752216e-08, 5.47895524505293e-08, 4.9836884841170104e-08, 4.5331908893331274e-08, 4.123415919821127e-08, 3.750682253667037e-08, 3.411641458228587e-08, 3.1032481473403095e-08, 2.822731914875476e-08, 2.567572821021713e-08, 2.3354786549134587e-08, 2.124364506528309e-08, 1.93233393730452e-08, 1.7576617494796665e-08, 1.598778887057506e-08, 1.4542582249532643e-08, 1.3228014239530239e-08, 1.2032275620299515e-08, 1.0944625650211037e-08, 9.955292590291265e-09, 9.055390215451098e-09, 8.236833437536006e-09, 7.492269915587713e-09, 6.815010777927455e-09, 6.1989720023802874e-09, 5.638619793302269e-09, 5.128919955410538e-09, 4.6652943730407515e-09, 4.243577933493725e-09, 3.859982111720228e-09, 3.511061441585639e-09, 3.1936810973576257e-09, 2.904990248353556e-09, 2.6423954135879058e-09, 2.403537591177951e-09, 2.1862711641063015e-09, 1.988644582340271e-09, 1.8088822661965764e-09, 1.6453693962859006e-09, 1.496637258568967e-09, 1.3613495886133364e-09, 1.238291247318557e-09, 1.1263566745967069e-09, 1.024540341454383e-09, 9.319276461638992e-10, 8.476865875906014e-10, 7.710604932320564e-10, 7.013609693018452e-10, 6.379618500140793e-10, 5.802936464682773e-10, 5.27838328601149e-10, 4.801247177610435e-10, 4.367241290825774e-10, 3.9724670775065363e-10, 3.6133782077563126e-10, 3.2867489285770546e-10, 2.989645198070434e-10, 2.7193980400852524e-10, 2.4735796744224103e-10, 2.2499818674859284e-10, 2.0465960870463817e-10, 1.8615953223388715e-10, 1.6933175694955338e-10, 1.5402511210904635e-10, 1.4010211046855403e-10, 1.274376576487768e-10, 1.1591800436194788e-10, 1.0543966394438442e-10, 9.590851030027991e-11, 8.723891747886015e-11, 7.935301027384156e-11, 7.217994257846527e-11, 6.56552798239396e-11, 5.972040917345822e-11, 5.432201910515744e-11, 4.941161368954283e-11, 4.494507890862032e-11, 4.08822940778375e-11, 3.718676408470678e-11], "accuracy_train_first": 0.5462088178294573, "accuracy_train_last": 0.8657246389350315, "batch_size_eval": 1024, "accuracy_train_std": [0.017687579891366326, 0.01636114509242405, 0.018323767472795422, 0.014765211796193532, 0.016536648418475308, 0.016658670678204337, 0.016962039176382747, 0.0172835168123493, 0.017004929184886133, 0.017124714551560317, 0.01683478706891008, 0.01577549118495066, 0.015629134168961053, 0.016259259867938265, 0.015709233795569096, 0.014615880815106204, 0.015103242378754793, 0.014323859197219506, 0.014357724529584255, 0.015151122228801387, 0.0145786706131888, 0.013639967717730806, 0.013462075156044246, 0.013105331749069448, 0.013985076974062689, 0.013494585323377106, 0.012864006067583474, 0.012622477672377688, 0.012818789750589898, 0.012730846012420463, 0.012255490546843191, 0.012884071003330852, 0.011997447685647757, 0.011603332117007242, 0.012480430729864987, 0.012400816889946785, 0.012683874052425246, 0.011922888501430754, 0.012470047217554552, 0.012126685858457963, 0.012786834293041996, 0.012269037991993364, 0.011849159250065383, 0.011781091303372456, 0.0117311987507312, 0.01223028303693481, 0.01147659028335058, 0.011957271171057231, 0.012122508804427251, 0.011807402061997086, 0.012107771175728019, 0.012193206400262441, 0.012069454073304748, 0.012355761760864637, 0.011801124704356297, 0.01215076663220462, 0.012358156869795101, 0.011729511506513055, 0.012047176389517357, 0.011932717162542166, 0.01176633523594755, 0.011792721625152056, 0.011880635699274722, 0.01246121798617352, 0.011978189415570734, 0.011996163447727899, 0.012329174931517238, 0.012020120655937954, 0.01178367914102152, 0.012018631912389572, 0.011866452866946237, 0.012143759999149375, 0.012634891902215792, 0.011794716895630716, 0.012310065020091167, 0.011925172797241592, 0.011768778688036696, 0.012324526948952145, 0.011806779867973221, 0.012056672315939987, 0.011938359890463048, 0.012087870570447858, 0.012415200043339476, 0.011751856964080208, 0.011777317750223075, 0.011928247647685727, 0.012126452731106416, 0.011992255472753349, 0.012279524187858003, 0.012151129848365248, 0.012077554423420081, 0.011793501839184496, 0.012414836523134486, 0.012130062238522787, 0.012188113227615599, 0.01215194447314932, 0.012030299982164965, 0.011734051587276818, 0.011919961127097797, 0.01195487775785138, 0.012043981057431432, 0.012037587848693462, 0.011611146776033372, 0.012031200822035381, 0.012138520080526892, 0.011842756593675096, 0.011741060167812687, 0.012157999158149569, 0.011773017606516417, 0.011770008312026077, 0.012032248626409907, 0.012145442948045546, 0.011761923067340503, 0.011837751254400146, 0.012200617632666545, 0.01191592365862675, 0.01213671907163556, 0.011854377339870381, 0.012138194106707674, 0.012583859215314698, 0.012170429691941532, 0.01166785327798615, 0.011931649598286806, 0.011680826059488673, 0.01219416305795686, 0.011992038624146556, 0.012021918913194521, 0.011903917237131656, 0.011943886226590233, 0.011712511761162558, 0.011885818819847748, 0.012084391781164392, 0.012191600277898472, 0.011915802499545283, 0.011805061865043494, 0.01199664524600952, 0.012540221891026293, 0.011942227773521087, 0.01173273501866152, 0.011973952429285752, 0.012135849675720272, 0.012035991627405483, 0.012046851164021561, 0.011781945009608939, 0.012108817763214362, 0.01185878369466656, 0.011431979479917483, 0.011885595666856747, 0.0121242585135548, 0.012387367456084597, 0.012489773169663834, 0.011933587309685165, 0.011917414250614058, 0.012041231477563119, 0.01204777650801082, 0.011974979824930876, 0.012158509874444426, 0.012070308613942388, 0.01205796728828418, 0.011784359269173275, 0.011770164269524095, 0.01161192854875539, 0.012295577730471635, 0.011686780432731355, 0.012132580804760993, 0.012217639491327563, 0.011906744550229569, 0.01210251408216605, 0.012270790744115759, 0.011798194815373081, 0.012376446323749613, 0.012205966649623099, 0.012176533180075487, 0.011924512780850178], "accuracy_test_std": 0.010446986429511461, "error_valid": [0.4613022402108433, 0.3974168157003012, 0.3714143684111446, 0.33589043674698793, 0.30957325395331325, 0.2906817700489458, 0.2703357374811747, 0.25900378859186746, 0.2505397566829819, 0.24293021696159633, 0.23728409732680722, 0.22600215314382532, 0.22244152390813254, 0.22195324265813254, 0.21397749199924698, 0.21316418015813254, 0.2078533862010542, 0.2033058993787651, 0.2017498705760542, 0.2005291674510542, 0.20273672816265065, 0.19844367705195776, 0.1979656908885542, 0.19722297392695776, 0.19502570830195776, 0.19086502259036142, 0.19417121611445776, 0.19331672392695776, 0.19011201054216864, 0.19098709290286142, 0.18803681522966864, 0.19001053040286142, 0.1906414721385542, 0.1884133212537651, 0.18880012236445776, 0.1892678134412651, 0.19028555628765065, 0.18843391142695776, 0.18549392884036142, 0.1869484775037651, 0.18672492705195776, 0.1843644107680723, 0.1871926181287651, 0.1846085513930723, 0.18374376411897586, 0.18276720161897586, 0.18583954960466864, 0.1853615634412651, 0.18533067818147586, 0.1852189029555723, 0.1828995670180723, 0.18275690653237953, 0.1853409732680723, 0.18287897684487953, 0.18409967996987953, 0.18324518778237953, 0.18472032661897586, 0.18188182417168675, 0.18312311746987953, 0.1821362598832832, 0.1844864810805723, 0.18300104715737953, 0.18251276590737953, 0.1825024708207832, 0.18336725809487953, 0.18263483621987953, 0.1811596973832832, 0.1821362598832832, 0.18188182417168675, 0.18373346903237953, 0.18287897684487953, 0.1836011036332832, 0.1820141895707832, 0.1821362598832832, 0.18249217573418675, 0.18374376411897586, 0.1826245411332832, 0.18237010542168675, 0.1817700489457832, 0.1816479786332832, 0.18188182417168675, 0.1829907520707832, 0.1821362598832832, 0.1831128223832832, 0.1818921192582832, 0.1816479786332832, 0.1812817676957832, 0.1826245411332832, 0.1814038380082832, 0.18261424604668675, 0.18275690653237953, 0.18200389448418675, 0.18310252729668675, 0.1809155567582832, 0.18175975385918675, 0.1810376270707832, 0.1826245411332832, 0.18188182417168675, 0.1820141895707832, 0.1818921192582832, 0.1831128223832832, 0.18102733198418675, 0.1831437076430723, 0.18239069559487953, 0.18287897684487953, 0.18163768354668675, 0.18252306099397586, 0.1825024708207832, 0.18322459760918675, 0.1821362598832832, 0.18349962349397586, 0.1826245411332832, 0.1827466114457832, 0.1826245411332832, 0.18285838667168675, 0.1822583301957832, 0.1815259083207832, 0.1822583301957832, 0.1815053181475903, 0.18251276590737953, 0.1812611775225903, 0.18175975385918675, 0.1831128223832832, 0.18090526167168675, 0.1822583301957832, 0.18127147260918675, 0.18300104715737953, 0.1815053181475903, 0.18237010542168675, 0.1818921192582832, 0.1833569630082832, 0.18214655496987953, 0.1827466114457832, 0.1817700489457832, 0.1818921192582832, 0.1814038380082832, 0.1821362598832832, 0.18202448465737953, 0.18287897684487953, 0.18114940229668675, 0.18163768354668675, 0.18224803510918675, 0.18200389448418675, 0.1837231739457832, 0.1831128223832832, 0.1826245411332832, 0.1809155567582832, 0.1827466114457832, 0.18202448465737953, 0.1821362598832832, 0.1817700489457832, 0.18301134224397586, 0.1825024708207832, 0.18190241434487953, 0.1827466114457832, 0.1822583301957832, 0.1818921192582832, 0.1810170368975903, 0.18188182417168675, 0.1824818806475903, 0.1827466114457832, 0.1815259083207832, 0.1818921192582832, 0.18151561323418675, 0.18153620340737953, 0.18212596479668675, 0.18116999246987953, 0.1823804005082832, 0.1827466114457832, 0.18161709337349397, 0.1816479786332832, 0.18263483621987953, 0.1836011036332832, 0.18200389448418675], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.09039439816231694, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 16, "valid_ratio": 0.15, "learning_rate": 0.0005369633039446165, "optimization": "rmsprop", "nb_data_augmentation": 3, "learning_rate_decay_method": "exp", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 6.687196320402033e-05, "rotation_range": [0, 0], "momentum": 0.8447650291098445}, "accuracy_valid_max": 0.8190947383283133, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8179961055158133, "accuracy_valid_std": [0.014310105977304238, 0.015634073952574726, 0.011142842854206412, 0.009910794703116588, 0.012102221013988482, 0.009005154351792966, 0.010172787284256022, 0.01293952065989642, 0.012335189110091224, 0.01434930220775098, 0.011686476193310015, 0.007077319898872358, 0.00853913927914428, 0.009263920769853405, 0.005975069730141306, 0.007691395344978921, 0.006681255769508238, 0.009412860063727099, 0.00926543308071142, 0.007544574739130019, 0.010288497313086738, 0.01025963809822526, 0.010662647761704877, 0.009878575576660142, 0.011701141116854387, 0.010488339876958387, 0.009413222330033931, 0.010217905915043998, 0.007595103583166654, 0.009939699457435003, 0.008636621741835458, 0.009289588189086123, 0.010595490471355343, 0.011647046730366964, 0.009331543559615268, 0.009441188615077738, 0.011473842505483146, 0.010290599376922909, 0.011325704549800718, 0.010474693976809257, 0.01108409619915179, 0.010258991357593734, 0.01077626050230478, 0.009310810266326368, 0.010875839360737393, 0.011276521936055718, 0.011652470588124665, 0.012047704074898809, 0.010473138494260684, 0.010867421992334292, 0.010895428954929592, 0.0113631466218653, 0.011931702170142987, 0.012028653533462412, 0.011144551358179496, 0.011198588218875058, 0.011949327081513939, 0.010718331372360218, 0.011220714112070856, 0.011387648647337082, 0.013148153431824693, 0.01159641762100651, 0.011488722367532048, 0.011159402096055294, 0.011046334493189222, 0.011114741313760647, 0.010528904811573744, 0.01056213660693135, 0.011888948216221431, 0.011854565483005657, 0.011929137438937575, 0.011998421255056126, 0.011303012308310446, 0.011778728612250875, 0.011086836515997551, 0.010462427747602702, 0.011563929966815847, 0.012332023283328908, 0.010889562093475516, 0.011512894768342143, 0.011676484896014808, 0.010739923582760266, 0.011282480043154627, 0.011828616912029872, 0.010627004619109037, 0.011839600520210547, 0.01087345924724554, 0.011909269833489989, 0.010491872354725306, 0.011368889812470358, 0.010202311029582696, 0.011283874118973252, 0.011730267616284302, 0.010262465357228785, 0.011150970658985892, 0.011176378074206546, 0.011429130970861312, 0.011250117234220178, 0.010572797285582126, 0.01016840616864326, 0.011858812494885978, 0.011325799946283627, 0.011347461643224498, 0.01095035198884139, 0.011047114592210461, 0.011284438927535447, 0.01084605219640027, 0.011444193146621805, 0.012459051736397128, 0.01163617581568943, 0.01084353588549491, 0.011697175633704995, 0.011496001871583084, 0.011122539995664477, 0.01195917073913188, 0.010447863294982571, 0.010246769940821415, 0.01159440454768634, 0.011104380132570904, 0.01036501943996104, 0.012320382103531504, 0.012094571340684547, 0.011645799398398162, 0.010111414227957236, 0.011491128361670757, 0.011420199117863615, 0.011930830430072412, 0.01113653958220147, 0.011553447498069327, 0.01134322940768051, 0.011467708249557864, 0.010655567991785303, 0.011752383737309672, 0.011671569621999197, 0.01070523970905256, 0.011227351883131384, 0.010147692390677945, 0.01059898315596169, 0.01134523957562259, 0.010219956252476138, 0.011027987849596127, 0.011188453162855935, 0.01204029858377067, 0.01153678338111806, 0.010158726144055587, 0.011239817628648301, 0.01063889131059588, 0.011423183837167764, 0.010508620047729728, 0.011533273681943906, 0.011640888355442199, 0.010926925400897968, 0.010987154220989998, 0.011091748522476078, 0.01062290704676527, 0.011143528073013612, 0.011237645109374665, 0.010592821526482322, 0.011727420535751659, 0.011222999381438264, 0.011732079370289724, 0.010987807808695255, 0.010638216286279686, 0.011043483353998775, 0.011907996201603776, 0.011358272953050103, 0.010316154419672442, 0.010936259237367478, 0.010292327984001962, 0.011616004341235376, 0.010818943493027325, 0.010451428597360458, 0.012379839950449422, 0.010392974079906475], "accuracy_valid": [0.5386977597891567, 0.6025831842996988, 0.6285856315888554, 0.6641095632530121, 0.6904267460466867, 0.7093182299510542, 0.7296642625188253, 0.7409962114081325, 0.7494602433170181, 0.7570697830384037, 0.7627159026731928, 0.7739978468561747, 0.7775584760918675, 0.7780467573418675, 0.786022508000753, 0.7868358198418675, 0.7921466137989458, 0.7966941006212349, 0.7982501294239458, 0.7994708325489458, 0.7972632718373494, 0.8015563229480422, 0.8020343091114458, 0.8027770260730422, 0.8049742916980422, 0.8091349774096386, 0.8058287838855422, 0.8066832760730422, 0.8098879894578314, 0.8090129070971386, 0.8119631847703314, 0.8099894695971386, 0.8093585278614458, 0.8115866787462349, 0.8111998776355422, 0.8107321865587349, 0.8097144437123494, 0.8115660885730422, 0.8145060711596386, 0.8130515224962349, 0.8132750729480422, 0.8156355892319277, 0.8128073818712349, 0.8153914486069277, 0.8162562358810241, 0.8172327983810241, 0.8141604503953314, 0.8146384365587349, 0.8146693218185241, 0.8147810970444277, 0.8171004329819277, 0.8172430934676205, 0.8146590267319277, 0.8171210231551205, 0.8159003200301205, 0.8167548122176205, 0.8152796733810241, 0.8181181758283133, 0.8168768825301205, 0.8178637401167168, 0.8155135189194277, 0.8169989528426205, 0.8174872340926205, 0.8174975291792168, 0.8166327419051205, 0.8173651637801205, 0.8188403026167168, 0.8178637401167168, 0.8181181758283133, 0.8162665309676205, 0.8171210231551205, 0.8163988963667168, 0.8179858104292168, 0.8178637401167168, 0.8175078242658133, 0.8162562358810241, 0.8173754588667168, 0.8176298945783133, 0.8182299510542168, 0.8183520213667168, 0.8181181758283133, 0.8170092479292168, 0.8178637401167168, 0.8168871776167168, 0.8181078807417168, 0.8183520213667168, 0.8187182323042168, 0.8173754588667168, 0.8185961619917168, 0.8173857539533133, 0.8172430934676205, 0.8179961055158133, 0.8168974727033133, 0.8190844432417168, 0.8182402461408133, 0.8189623729292168, 0.8173754588667168, 0.8181181758283133, 0.8179858104292168, 0.8181078807417168, 0.8168871776167168, 0.8189726680158133, 0.8168562923569277, 0.8176093044051205, 0.8171210231551205, 0.8183623164533133, 0.8174769390060241, 0.8174975291792168, 0.8167754023908133, 0.8178637401167168, 0.8165003765060241, 0.8173754588667168, 0.8172533885542168, 0.8173754588667168, 0.8171416133283133, 0.8177416698042168, 0.8184740916792168, 0.8177416698042168, 0.8184946818524097, 0.8174872340926205, 0.8187388224774097, 0.8182402461408133, 0.8168871776167168, 0.8190947383283133, 0.8177416698042168, 0.8187285273908133, 0.8169989528426205, 0.8184946818524097, 0.8176298945783133, 0.8181078807417168, 0.8166430369917168, 0.8178534450301205, 0.8172533885542168, 0.8182299510542168, 0.8181078807417168, 0.8185961619917168, 0.8178637401167168, 0.8179755153426205, 0.8171210231551205, 0.8188505977033133, 0.8183623164533133, 0.8177519648908133, 0.8179961055158133, 0.8162768260542168, 0.8168871776167168, 0.8173754588667168, 0.8190844432417168, 0.8172533885542168, 0.8179755153426205, 0.8178637401167168, 0.8182299510542168, 0.8169886577560241, 0.8174975291792168, 0.8180975856551205, 0.8172533885542168, 0.8177416698042168, 0.8181078807417168, 0.8189829631024097, 0.8181181758283133, 0.8175181193524097, 0.8172533885542168, 0.8184740916792168, 0.8181078807417168, 0.8184843867658133, 0.8184637965926205, 0.8178740352033133, 0.8188300075301205, 0.8176195994917168, 0.8172533885542168, 0.818382906626506, 0.8183520213667168, 0.8173651637801205, 0.8163988963667168, 0.8179961055158133], "seed": 249947583, "model": "residualv3", "loss_std": [0.3231031894683838, 0.2556432783603668, 0.2536938488483429, 0.25099003314971924, 0.24742276966571808, 0.2452755868434906, 0.24502791464328766, 0.23901782929897308, 0.23953597247600555, 0.2365727424621582, 0.23409800231456757, 0.2329624742269516, 0.22943218052387238, 0.2290850132703781, 0.23030239343643188, 0.2270694077014923, 0.2256113886833191, 0.22337359189987183, 0.2203701138496399, 0.22032828629016876, 0.21999134123325348, 0.21890227496623993, 0.21951058506965637, 0.216531440615654, 0.21588389575481415, 0.21666407585144043, 0.2166905254125595, 0.21612733602523804, 0.2151552438735962, 0.21473293006420135, 0.21330836415290833, 0.21326647698879242, 0.2115812599658966, 0.21352462470531464, 0.21019545197486877, 0.2115536481142044, 0.2112940102815628, 0.21046075224876404, 0.2099922001361847, 0.2115461826324463, 0.21045610308647156, 0.2093285769224167, 0.2099463939666748, 0.208821639418602, 0.20923417806625366, 0.20870603621006012, 0.20775479078292847, 0.20814846456050873, 0.2087392359972, 0.20894436538219452, 0.21003322303295135, 0.2073030173778534, 0.20746126770973206, 0.20785313844680786, 0.20758016407489777, 0.20736141502857208, 0.20862199366092682, 0.20642581582069397, 0.2074529230594635, 0.20703379809856415, 0.20779964327812195, 0.20972464978694916, 0.2064327597618103, 0.20781956613063812, 0.20950628817081451, 0.20771291851997375, 0.20635855197906494, 0.20619773864746094, 0.20731283724308014, 0.2057853639125824, 0.20637498795986176, 0.20689432322978973, 0.20739798247814178, 0.20619812607765198, 0.2080104649066925, 0.20789457857608795, 0.2074754536151886, 0.20759329199790955, 0.20809584856033325, 0.20680959522724152, 0.20672662556171417, 0.20776498317718506, 0.20711448788642883, 0.20865584909915924, 0.207331120967865, 0.20450635254383087, 0.20505857467651367, 0.20789985358715057, 0.20613853633403778, 0.20737415552139282, 0.20864948630332947, 0.20577409863471985, 0.2074204981327057, 0.20630818605422974, 0.20658889412879944, 0.2079014629125595, 0.20628952980041504, 0.20857763290405273, 0.20809541642665863, 0.20778483152389526, 0.2079315334558487, 0.2058350294828415, 0.20842817425727844, 0.2059735506772995, 0.20511344075202942, 0.2061057984828949, 0.2063281387090683, 0.2071075141429901, 0.2062324732542038, 0.20741750299930573, 0.20669646561145782, 0.20747870206832886, 0.20801401138305664, 0.20461004972457886, 0.2071337103843689, 0.20740941166877747, 0.20846445858478546, 0.20738859474658966, 0.20773136615753174, 0.20568160712718964, 0.2086237221956253, 0.205475851893425, 0.20739860832691193, 0.2077159583568573, 0.20890018343925476, 0.20710043609142303, 0.2079538106918335, 0.20699478685855865, 0.20651185512542725, 0.20761045813560486, 0.20589324831962585, 0.20719149708747864, 0.2062884420156479, 0.2065286487340927, 0.20812614262104034, 0.20791199803352356, 0.20728367567062378, 0.20883050560951233, 0.2073703110218048, 0.20617543160915375, 0.20707866549491882, 0.20670272409915924, 0.2070312649011612, 0.20536544919013977, 0.20799146592617035, 0.20645953714847565, 0.2085021287202835, 0.20512212812900543, 0.20715026557445526, 0.20565569400787354, 0.20738492906093597, 0.20809337496757507, 0.20524205267429352, 0.2074570655822754, 0.20932134985923767, 0.2079210877418518, 0.20644526183605194, 0.2065913826227188, 0.2076481282711029, 0.20771285891532898, 0.20727261900901794, 0.2066953480243683, 0.20577840507030487, 0.20859664678573608, 0.2068571001291275, 0.20643223822116852, 0.20596753060817719, 0.2067541480064392, 0.2080879658460617, 0.2080734223127365, 0.2064369171857834, 0.2072424590587616, 0.20700688660144806, 0.2088661789894104]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:39 2016", "state": "available"}], "summary": "f2d8d373782273d34925eb8a0cfccb67"}