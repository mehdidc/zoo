{"content": {"hp_model": {"f0": 64, "f1": 32, "f2": 32, "f3": 16, "nonlin": "rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.9211374521255493, 1.620296835899353, 1.4344373941421509, 1.3192477226257324, 1.2417060136795044, 1.1737865209579468, 1.1144635677337646, 1.0592252016067505, 1.0205756425857544, 0.9855812191963196, 0.961555004119873, 0.940463125705719, 0.9207966923713684, 0.905087411403656, 0.8945252299308777, 0.8861162662506104, 0.8726387023925781, 0.8643172979354858, 0.8553739786148071, 0.8508179783821106, 0.8445148468017578, 0.8341475129127502, 0.8295109272003174, 0.8250370621681213, 0.8207412362098694, 0.8135753273963928, 0.8117930293083191, 0.80621337890625, 0.7997258901596069, 0.8005708456039429, 0.797454297542572, 0.7909672260284424, 0.7894559502601624, 0.7890564799308777, 0.7827695608139038, 0.7820293307304382, 0.7776107788085938, 0.7765110731124878, 0.7737038135528564, 0.7725627422332764, 0.7680116891860962, 0.7700624465942383, 0.7652778625488281, 0.7629244923591614, 0.7627599835395813, 0.7605717778205872, 0.7583127617835999, 0.7561585903167725, 0.7526567578315735, 0.7491547465324402, 0.7495831251144409, 0.7508056163787842, 0.7488620281219482, 0.7496673464775085, 0.7460157871246338, 0.7413761019706726, 0.7422380447387695, 0.7414460182189941, 0.7394472360610962, 0.7364753484725952, 0.7332347631454468, 0.7337357401847839, 0.7304648160934448, 0.7291939854621887, 0.7281125783920288, 0.7290823459625244, 0.7282373905181885, 0.722915530204773, 0.724044919013977, 0.7227380275726318, 0.7210666537284851, 0.7205732464790344, 0.71844083070755, 0.7195231318473816, 0.7204779386520386, 0.7177035808563232, 0.7156399488449097, 0.7152116298675537, 0.7142097353935242, 0.716312825679779, 0.7097499966621399, 0.7102487683296204, 0.7089717984199524, 0.7056138515472412, 0.709409773349762, 0.7072635889053345, 0.7075596451759338, 0.705959677696228, 0.7047850489616394, 0.7033419013023376, 0.7058966159820557, 0.7047401070594788, 0.7006270289421082, 0.7009665966033936, 0.6958908438682556, 0.6987795829772949, 0.6954690217971802, 0.6975235939025879, 0.6985287666320801, 0.6971367597579956, 0.693789541721344, 0.692931056022644, 0.6977021098136902, 0.6915360689163208, 0.6920861601829529, 0.6943035125732422, 0.6895138025283813, 0.691690981388092, 0.6893119215965271, 0.6900284886360168, 0.6905084848403931, 0.6931772232055664, 0.6883566379547119, 0.6883496642112732, 0.6876538991928101, 0.6867164373397827, 0.6870526075363159, 0.6858872771263123, 0.6838890314102173, 0.6854760050773621, 0.6858092546463013, 0.6825953125953674, 0.6821438670158386, 0.6823236346244812, 0.6836932897567749, 0.6809586882591248, 0.680394172668457, 0.6857314705848694, 0.6839472055435181, 0.6807357668876648, 0.6797884702682495, 0.6807672381401062, 0.6819429993629456, 0.6812009811401367, 0.6785435080528259, 0.6773577928543091, 0.6785852909088135, 0.6774849891662598, 0.6777983903884888, 0.677524209022522, 0.6767642498016357, 0.6795528531074524, 0.6765490174293518, 0.6760128140449524, 0.6730916500091553, 0.6741295456886292, 0.674339234828949, 0.6737462878227234, 0.6753310561180115, 0.6775684952735901, 0.6741273999214172, 0.6756213307380676, 0.6736738681793213, 0.6752294301986694, 0.6701958179473877, 0.6739011406898499, 0.6734296679496765, 0.6725409030914307, 0.6739060878753662, 0.6710562109947205, 0.6725881099700928, 0.6716612577438354, 0.6723019480705261, 0.6734035015106201, 0.6684999465942383, 0.6701062917709351, 0.6687520146369934, 0.6711446642875671, 0.6714597344398499, 0.6685978174209595, 0.6667010188102722, 0.669238805770874], "moving_avg_accuracy_train": [0.027392325783268726, 0.058898840165305444, 0.0804343386988395, 0.09751035258989131, 0.1147474998071388, 0.13616558461901443, 0.162992466281467, 0.18102932297788654, 0.20117252913063316, 0.216532631070606, 0.23433171791394222, 0.2431451337413649, 0.2734425980919423, 0.2928061078125561, 0.2999449697380189, 0.3076042750494237, 0.3150460965581857, 0.3342808842237957, 0.3437438605970049, 0.3506703538400914, 0.35753406074451893, 0.3736286432901759, 0.3921192780037017, 0.38978916703495764, 0.40795656816091924, 0.43024189213224884, 0.4420902775544983, 0.4686050487966029, 0.48161961313067464, 0.49397177646577073, 0.5088057712421357, 0.5254807525573831, 0.5311316747118977, 0.5338385890282679, 0.5400271695057475, 0.5294896849114389, 0.5460793023549111, 0.5601333409040269, 0.5565401983337885, 0.5667356905435991, 0.5720686672337482, 0.5709472016039503, 0.5670392150456687, 0.5613463367362385, 0.5599096031468247, 0.5584984650105291, 0.5552711700737066, 0.561283640437137, 0.5709531304793406, 0.566152378818219, 0.5629169700361571, 0.5665347508437726, 0.5674406409541166, 0.570782729930641, 0.5878271548600114, 0.5842584864451158, 0.5856605912082269, 0.5786100244277512, 0.5798966801095147, 0.5942791974067951, 0.5844376174709329, 0.5906880211504268, 0.5938679407442713, 0.5934588886871937, 0.6026377848233192, 0.5999517924820911, 0.6119012595517115, 0.6098885139721402, 0.6094691221598745, 0.6152080545821299, 0.6081852585081454, 0.6119844205149352, 0.6083221642677052, 0.6010540044289284, 0.6073821249174217, 0.5794651001909988, 0.5755210544691488, 0.5609709720819422, 0.5644414837123416, 0.5658611868834201, 0.5718028438099269, 0.5796612434163373, 0.5760092254100672, 0.5829933665456939, 0.5732899284007222, 0.5844205803500095, 0.592248994439178, 0.5909276182626099, 0.5748727660951252, 0.5802313779015411, 0.5581668647191759, 0.5559043935077326, 0.5554744507078083, 0.5776135438545432, 0.5806105628887087, 0.5811453834801443, 0.5879109143170229, 0.5882411473477607, 0.5856004504447492, 0.5919686898802484, 0.60999086000375, 0.5947394012701248, 0.5928295126842714, 0.5963532467413796, 0.5988800364890763, 0.6057247409427896, 0.6042690131109691, 0.6160817636259612, 0.6246954064597937, 0.611876480662467, 0.5957203140296368, 0.6032085843697056, 0.5972111689121555, 0.5991991950107425, 0.5954652347831547, 0.6021282248764985, 0.6001973841798693, 0.6040398044516665, 0.6042661700463209, 0.608861420255144, 0.5973400800188047, 0.6178191457699235, 0.6222823069242895, 0.6149167734909968, 0.6032968524691859, 0.6083006043761986, 0.6012640692823403, 0.5925953143647652, 0.590978961626609, 0.6057459963078942, 0.6233229646565253, 0.620627887298477, 0.60150666775929, 0.5928364358474714, 0.6108717767242414, 0.6157689336771404, 0.6166402561812887, 0.5977881321527685, 0.5906641341945054, 0.5929066497790693, 0.5959200053980154, 0.5966517917433652, 0.6099841547605661, 0.6207234834071101, 0.6010608940584166, 0.6151427869857699, 0.6112320873581139, 0.6116926438204845, 0.6040103816763468, 0.6185412653144984, 0.6255606600605033, 0.6362112914923912, 0.6299055671569671, 0.634098941608214, 0.6406886977738506, 0.6277188303093578, 0.6220929046196919, 0.5856714070183262, 0.5942876927999746, 0.5980488618620776, 0.608810061091383, 0.6014112266784426, 0.5936220731412739, 0.6053910705498302, 0.6207298964267409, 0.6072988035863924, 0.6143502734434693, 0.6231793505599473, 0.6294538821660051, 0.6336780055888472, 0.626610060546297, 0.6269715460706633], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.027059017319277106, 0.058648108057228907, 0.08077931452371986, 0.0975650467102786, 0.11572719908932604, 0.1365898848656344, 0.1630276113640107, 0.1804694967825795, 0.20029743532909564, 0.21580471292758666, 0.23300950966005388, 0.24171671828553945, 0.2710811145575126, 0.29025971243911075, 0.29736199444256417, 0.30512726572496435, 0.3129930041844709, 0.33150765684396055, 0.34053188998298317, 0.3472223886258746, 0.354314967628498, 0.3693533680324404, 0.3869266908677506, 0.38521588477984603, 0.40292723587264456, 0.4242163376336482, 0.43609672641358155, 0.461532816902871, 0.47351868360302063, 0.48499985246975175, 0.49843760842194823, 0.5142437028602503, 0.5203401874782163, 0.5222806604472622, 0.5270697254266323, 0.517430924005957, 0.5334759763719276, 0.5471414505532589, 0.5431607568815926, 0.5529808645989303, 0.5575137034025914, 0.5547367306526938, 0.5515163578980419, 0.5466026856812798, 0.5459170559026849, 0.5437148399114375, 0.5405762660765439, 0.5458714314963744, 0.5548852743218876, 0.5516581361969278, 0.548851368134464, 0.5522490697058068, 0.5530547303180574, 0.556197699492577, 0.5714454817835302, 0.5676009500773157, 0.5682272855571594, 0.5615519238124375, 0.5630518636789347, 0.5762673380791135, 0.5667808704138979, 0.5738392232727341, 0.5761581259172227, 0.575990955550651, 0.5844782269477697, 0.581098381239062, 0.5923718380134992, 0.5907296337396343, 0.5903270452244811, 0.5953461043974396, 0.589556258170798, 0.5928131052923478, 0.589613102415598, 0.5826680113476074, 0.587987047703434, 0.5619901066284972, 0.5572812362141415, 0.5436081001209654, 0.5452002033242302, 0.5473393391476807, 0.551919664221994, 0.5601641094620687, 0.5568079488923678, 0.5635767080788088, 0.5545933489708526, 0.564817548994701, 0.5720031098425955, 0.5716603560401884, 0.5570671508202057, 0.5629707931835919, 0.5427428439010008, 0.5408937877676778, 0.540054567829389, 0.5602280043540555, 0.5635632905620988, 0.5637013962762202, 0.5701002525540951, 0.5713679181401765, 0.5688180286228456, 0.5743746023136485, 0.5910673543375546, 0.5771041324109528, 0.5740079769823575, 0.5780486973093477, 0.5810811710930966, 0.5868998059133804, 0.5851903352523888, 0.5959651764447704, 0.6037133794798868, 0.5917427733654824, 0.5765023368610125, 0.5838785174256642, 0.5784428553009442, 0.5806280431330486, 0.5779498632549848, 0.5840886883470314, 0.5813459410296477, 0.5850159873634299, 0.5847014827065297, 0.588111276077443, 0.5781669535807679, 0.596074444028414, 0.6003347754162353, 0.5933471727804853, 0.582712333137979, 0.5878408903927052, 0.5814902656794889, 0.5726961783999437, 0.5706207258931572, 0.58412720681476, 0.6004854939928473, 0.5979760365080957, 0.5792465627989277, 0.5722589064719718, 0.5888274611843378, 0.5925956116208739, 0.5937692382882293, 0.5751520330211534, 0.5687772311685861, 0.5710764011710799, 0.5743623863363514, 0.5750095555528669, 0.5874607719667068, 0.5975813233939066, 0.5787713097009768, 0.5917079023754575, 0.5886957800125352, 0.5890651597955437, 0.5827996276732936, 0.5969713368280275, 0.6041388786440952, 0.613931451387743, 0.6085037206409115, 0.612436651711233, 0.6181613482682724, 0.6053543848928759, 0.599991786200275, 0.5652663098110458, 0.5731150023898056, 0.5774047410909606, 0.5870091347341236, 0.5805321070381962, 0.5732733828742862, 0.5839275899087701, 0.59793958665961, 0.584436455365637, 0.5916581077394649, 0.5997780415061811, 0.6058785153111353, 0.6106241657566784, 0.6040327582171552, 0.6039142739052289], "moving_var_accuracy_train": [0.00675305560635056, 0.015011694082264887, 0.017684523947829024, 0.018540383806712676, 0.019360418623742975, 0.021552985974446957, 0.025874821594583364, 0.026215293230509605, 0.027245502694467082, 0.026644347009477598, 0.026831179740639463, 0.0248471484534991, 0.03062386072281978, 0.030935984228840554, 0.028301055952273894, 0.025998934977726335, 0.023897467846268148, 0.024837514570511258, 0.023159694410019374, 0.021275511746836145, 0.01957195482438151, 0.019946079627814063, 0.021028603814014057, 0.018974608186752607, 0.02004763754112166, 0.02251259476757357, 0.021524793424843775, 0.025699611928549644, 0.024654060698945903, 0.02356183808056334, 0.023186080881734017, 0.02336996781033529, 0.021320367320069214, 0.019254277054107823, 0.017673536103633213, 0.01690552972744751, 0.017691915416989574, 0.017700367871150903, 0.016046527145806348, 0.015377406983828477, 0.014095632048844696, 0.012697388010389587, 0.011565100439808023, 0.010700270166841058, 0.009648820980819503, 0.008701860680294923, 0.007925413505748597, 0.0074582203540138935, 0.007553889657698976, 0.007005925640534934, 0.006399543906364827, 0.005877384557475895, 0.005297031833756478, 0.004867854678923888, 0.00699568100158798, 0.0064107314497284405, 0.005787351384656247, 0.00565601067352415, 0.0051053089517624625, 0.006456489290845331, 0.006682550622466494, 0.006365903475629536, 0.005820320125676419, 0.005239794025377372, 0.005474083831339667, 0.004991606441919929, 0.005777553666959404, 0.005236258603176218, 0.004714215748288356, 0.004539212281584454, 0.004529168035696956, 0.004206153919711773, 0.003906247615123968, 0.0039910581805896435, 0.003952358342782674, 0.010571364934685757, 0.00965422791112157, 0.010594149197279882, 0.00964313433634253, 0.00869696091655401, 0.008144994408189348, 0.007886284966736695, 0.007217691589726115, 0.006934926477374684, 0.007088844236136853, 0.00749498252786873, 0.007297040879445282, 0.006583051106500772, 0.008244570498928783, 0.007678545933462641, 0.011292276017889308, 0.010209117399943863, 0.009189869317250337, 0.012682137393763521, 0.01149476276220752, 0.01034786078357198, 0.009725026372757569, 0.008753505220173124, 0.007940914219357987, 0.0075118130589928184, 0.009683819296737424, 0.010808900308594957, 0.009760839347428819, 0.008896505728032949, 0.008064317153091239, 0.007679535249310255, 0.0069306540160622605, 0.007493458287021037, 0.00741186604413843, 0.008149603167100906, 0.00968383833280083, 0.009220122233694335, 0.00862183093985905, 0.007795218075791112, 0.007141178399042864, 0.006826619491994558, 0.0061775108549569375, 0.0056926375113673005, 0.00512383493267256, 0.004801498359740499, 0.005516020051339894, 0.008738947252553725, 0.008044330794706936, 0.007728157460048822, 0.00817054479502207, 0.007578828113842267, 0.007266560737601938, 0.007216230470230589, 0.006518120788774833, 0.007828896529401868, 0.009826555223420672, 0.008909270678771383, 0.0113089329408863, 0.010854595939440133, 0.012696598030367748, 0.011642777543322904, 0.010485332615146732, 0.01263542257711242, 0.01182864244158522, 0.010691038082749803, 0.009703657083250933, 0.008738110976223004, 0.009464067011202552, 0.009555658928088618, 0.012079649814338012, 0.012656382208661181, 0.0115283861319948, 0.010377456529090601, 0.009870865241042814, 0.010784097930688045, 0.010149135261021418, 0.010155145284000567, 0.009497490190149742, 0.008706000674730093, 0.008226224584160006, 0.008917559284162596, 0.008310662714537126, 0.01941832583081996, 0.018144656673777286, 0.016457508540823042, 0.015853988366415956, 0.014761274285805369, 0.013831185072655126, 0.013694650265413048, 0.014442701452411402, 0.014621979601144811, 0.013607290685337706, 0.01294813504136239, 0.012007649259104903, 0.01096747330141704, 0.010320328595395938, 0.009289471781915281], "duration": 94391.887887, "accuracy_train": [0.27392325783268734, 0.342457469603636, 0.274253825500646, 0.2511944776093577, 0.2698818247623662, 0.32892834792589515, 0.4044344012435401, 0.34336103324566264, 0.38246138450535255, 0.35477354853036175, 0.39452349950396826, 0.3224658761881691, 0.5461197772471392, 0.46707769529808046, 0.3641947270671834, 0.3765380228520672, 0.3820224901370432, 0.5073939732142857, 0.42891064795588774, 0.4130087930278701, 0.4193074228843669, 0.518479886201089, 0.5585349904254338, 0.36881816831626063, 0.5714631782945736, 0.6308098078742156, 0.5487257463547435, 0.7072379899755445, 0.59875069213732, 0.6051412464816354, 0.6423117242294205, 0.6755555843946106, 0.5819899741025286, 0.5582008178755998, 0.5957243938030639, 0.43465232356266154, 0.6953858593461609, 0.6866196878460686, 0.5242019152016427, 0.6584951204318937, 0.6200654574450905, 0.5608540109357696, 0.5318673360211333, 0.5101104319513657, 0.5469790008421004, 0.5457982217838685, 0.5262255156423035, 0.6153958737080103, 0.6579785408591731, 0.5229456138681248, 0.5337982909976006, 0.5990947781123108, 0.575593651947213, 0.6008615307193613, 0.7412269792243448, 0.5521404707110558, 0.5982795340762275, 0.5151549234034699, 0.5914765812453857, 0.7237218530823182, 0.49586339804817275, 0.646941654265873, 0.6224872170888704, 0.5897774201734958, 0.6852478500484496, 0.5757778614110374, 0.7194464631782945, 0.5917738037559985, 0.6056945958494832, 0.666858446382429, 0.544980093842285, 0.6461768785760429, 0.5753618580426357, 0.5356405658799372, 0.6643352093138611, 0.32821187765319304, 0.5400246429724991, 0.43002023059708383, 0.5956760883859358, 0.5786385154231266, 0.6252777561484865, 0.650386839874031, 0.5431410633536361, 0.6458506367663345, 0.48595898509597635, 0.6845964478935954, 0.6627047212416943, 0.5790352326734958, 0.430379096587763, 0.6284588841592839, 0.3595862460778885, 0.5355421526047435, 0.5516049655084903, 0.7768653821751569, 0.6075837341961978, 0.5859587688030639, 0.6488006918489295, 0.5912132446244002, 0.5618341783176449, 0.6492828447997416, 0.772190391115264, 0.45747627266749724, 0.575640515411591, 0.6280668532553525, 0.6216211442183462, 0.667327081026209, 0.5911674626245847, 0.7223965182608896, 0.7022181919642857, 0.4965061484865264, 0.4503148143341639, 0.6706030174303249, 0.5432344297942046, 0.6170914298980251, 0.5618595927348652, 0.6620951357165928, 0.5828198179102068, 0.6386215868978405, 0.6063034603982097, 0.6502186721345515, 0.4936480178917498, 0.8021307375299926, 0.6624507573135844, 0.5486269725913621, 0.49871756327288663, 0.6533343715393134, 0.5379352534376154, 0.5145765201065892, 0.5764317869832042, 0.738649308439461, 0.7815156797942044, 0.5963721910760429, 0.42941569190660767, 0.5148043486411037, 0.7731898446151717, 0.65984334625323, 0.6244821587186231, 0.42811901589608714, 0.5265481525701367, 0.613089290040144, 0.6230402059685308, 0.6032378688515135, 0.7299754219153747, 0.7173774412260059, 0.4240975899201735, 0.741879823331949, 0.5760357907092101, 0.6158376519818198, 0.5348700223791066, 0.7493192180578626, 0.6887352127745479, 0.7320669743793835, 0.5731540481381506, 0.6718393116694352, 0.699996503264581, 0.5109900231289222, 0.5714595734126984, 0.2578779286060354, 0.6718342648348099, 0.631899383421004, 0.7056608541551311, 0.5348217169619786, 0.5235196913067552, 0.7113120472268365, 0.7587793293189369, 0.48641896802325585, 0.6778135021571613, 0.7026410446082503, 0.6859246666205242, 0.671695116394426, 0.5629985551633444, 0.6302249157899593], "end": "2016-02-04 13:51:40.797000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0], "moving_var_accuracy_valid": [0.006589713764564447, 0.014911578270962973, 0.017828533140828738, 0.018581527072095733, 0.019692148376243807, 0.02164019845882824, 0.025766759054571958, 0.02592805745170511, 0.026873576029574388, 0.026350499353266908, 0.026379494693389856, 0.024423884562480444, 0.029741906021989645, 0.030078082970127047, 0.027524256360023543, 0.025314525666824614, 0.0233399016737624, 0.024091042774300086, 0.022414869550596985, 0.020576247544352076, 0.018971364882092955, 0.019109609774267303, 0.019978043876106882, 0.018006581205729896, 0.019029150702948288, 0.021205268316766765, 0.020355034223151387, 0.02414248309525013, 0.023021183790708923, 0.021905420558555586, 0.021340038067977164, 0.02145452785371219, 0.019643579190614827, 0.017713110189645723, 0.01614821546107082, 0.015369552350408645, 0.01614959046420842, 0.01621533807919328, 0.014736417570242745, 0.014130686453439515, 0.012902537456675354, 0.01168168790989089, 0.010606856325011939, 0.009763468264394755, 0.008791352231694943, 0.007955864805974402, 0.007248934136830679, 0.006776389714557934, 0.006829995005449637, 0.006240725289202798, 0.005687554282890722, 0.005222698238312795, 0.0047062702156807044, 0.004324547491200455, 0.005984546525211146, 0.005519115689050826, 0.004970734785345543, 0.0048747053966170665, 0.004407483233383329, 0.005538573782641032, 0.005794654023245552, 0.005663571726639556, 0.005145610339247149, 0.004631300818705571, 0.004816474718750144, 0.004437637460007159, 0.005137691162812373, 0.00464819356042504, 0.0041848329019953355, 0.003993068206630733, 0.003895462259921058, 0.003601379512487272, 0.00333340172693941, 0.0034341701637296035, 0.00334538247714809, 0.009093412736716893, 0.008383632608257958, 0.009227861203034822, 0.008327888216219957, 0.007536282513238484, 0.00697146866199208, 0.006886059691642184, 0.0062988280464044985, 0.0060812901500803054, 0.006199467802834768, 0.006520329417700259, 0.0063329870382193626, 0.005700745655919007, 0.007047325837659878, 0.006656270192286789, 0.009673172562669816, 0.008736626383660448, 0.007869302356237798, 0.01074507999154676, 0.009770689199198155, 0.008793791937972797, 0.008282920999159615, 0.007469091683586871, 0.00678069994418353, 0.006380509550397072, 0.008250290326541995, 0.009180005393031607, 0.008348280459670628, 0.007660399200552127, 0.006977122355739038, 0.006584118720711496, 0.005952007458107468, 0.006401681536786111, 0.006301825235567974, 0.006961301408727107, 0.008355609411475413, 0.00800972082782899, 0.007474666549653136, 0.006770175307442015, 0.0061577116038312105, 0.00588110700504477, 0.0053607002701634415, 0.004945853403976072, 0.004452158282191372, 0.004111582671463154, 0.004590430353577729, 0.007017491245412783, 0.006479095932677999, 0.006270625654766467, 0.006661461417286245, 0.0062320341711926625, 0.005971804662306418, 0.006070647935798116, 0.005502350670189644, 0.006593940845141302, 0.008342888795234135, 0.007565276307520704, 0.009965887345570372, 0.009408744679506174, 0.01093852325886557, 0.009972461552390318, 0.008987611993140234, 0.011208253781434172, 0.010453171291226197, 0.009455429806406884, 0.008607066112323657, 0.007750128953044539, 0.008370411169398552, 0.008455200103174063, 0.010794029629010521, 0.011220825536338304, 0.01018039891286742, 0.009163586994397538, 0.008600540329932321, 0.009548022360236405, 0.00905558302537848, 0.00901307505129732, 0.00837690989570849, 0.007678430427372743, 0.007205536740667072, 0.007961147864689095, 0.0074238502608611435, 0.01753419362890533, 0.016335192042777724, 0.014867289561617633, 0.014210760000731333, 0.013167250990622499, 0.012324727579949827, 0.012113863969757686, 0.012669502049291837, 0.013043562837043122, 0.012208576920414533, 0.011581119147755796, 0.010757949258784604, 0.009884845116267693, 0.009287380484809682, 0.008358768783118267], "accuracy_test": 0.09829001913265306, "start": "2016-02-03 11:38:28.909000", "learning_rate_per_epoch": [0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373, 0.00674752052873373], "accuracy_train_first": 0.27392325783268734, "accuracy_train_last": 0.6302249157899593, "batch_size_eval": 1024, "accuracy_train_std": [0.011921217395453213, 0.01596380512190811, 0.01617159747543272, 0.01434502709110118, 0.015586400038619382, 0.014982975404823648, 0.01506297035630691, 0.015622662062021135, 0.01839402970227006, 0.014662487185663839, 0.015791793051271652, 0.016176211359689892, 0.015546076078747556, 0.013708574660524483, 0.016374325834470946, 0.01583449177138613, 0.014260426255457075, 0.015774176936913156, 0.015595221126043116, 0.013044267999438579, 0.012810936033811804, 0.01605285055607254, 0.013701481699949452, 0.016780143789334227, 0.014807305528486511, 0.014652319937190635, 0.013107235609575815, 0.01809907979097496, 0.014800261486748763, 0.014548890392717527, 0.01798331406240919, 0.016082059924132537, 0.013689070032889175, 0.014120650431306357, 0.0180170047688546, 0.011566556710524952, 0.017500415862777494, 0.01649090489197615, 0.01206743642983573, 0.014120504839782092, 0.015373491586607946, 0.01661333108248521, 0.014194221538004172, 0.016271181409525534, 0.016267005664747897, 0.020850398892010586, 0.016152279241779564, 0.01946837520515599, 0.018314501411690515, 0.016770039146250013, 0.016653489966213395, 0.016017540121962385, 0.0156736003599874, 0.01441382331097946, 0.0158477149159129, 0.017544763725318692, 0.01673600400559008, 0.01727393814155067, 0.018202437297240755, 0.016188122903693473, 0.01591169347673648, 0.014392687825602971, 0.015411867635379967, 0.014646759951760804, 0.018597761013545402, 0.017699666945085513, 0.015585061830956646, 0.01828191787573185, 0.018654241670542495, 0.016159062774330168, 0.017841755260662045, 0.015260152708752377, 0.014399525495332072, 0.016360093238086147, 0.016598492682117934, 0.015979903433340575, 0.013079414475809657, 0.015161124713832824, 0.017522744208139637, 0.019015645960128297, 0.014834695694920562, 0.014523629527392692, 0.016348888804332103, 0.015008101568207439, 0.015726048402625988, 0.01780506489680628, 0.016548318575946998, 0.01636120483237064, 0.018077882682501645, 0.014167183646170147, 0.015636544963309337, 0.015975440792285103, 0.015225494492415254, 0.016006517398064656, 0.01633133148119485, 0.016704024504586922, 0.014969921204229873, 0.015712338535357657, 0.01683381810196834, 0.01992147609099599, 0.01684944166617973, 0.01462792566278608, 0.01294304479356588, 0.016822463279047156, 0.01769159214034729, 0.018109920769844523, 0.01866179893143959, 0.016572132750154216, 0.0164371576242713, 0.01725779007027171, 0.01831301127268375, 0.01554090601103998, 0.01686266268282017, 0.018633466255100112, 0.01764563201268666, 0.01715194462955779, 0.01760216190830183, 0.01703737165265961, 0.01783885205796653, 0.01649872316631988, 0.018388234223175978, 0.016733233975901197, 0.01570765947855799, 0.016908088938056506, 0.0172787261166418, 0.01614513407213368, 0.014688006534983828, 0.015366616973957128, 0.01861802206767283, 0.018345756816613726, 0.016563930679411143, 0.01688016354306322, 0.014129806539726954, 0.016362169240286172, 0.016443233094996124, 0.0142156574250401, 0.019107707444154663, 0.015515713542977932, 0.017556261954636884, 0.019313459366003885, 0.01714635653433569, 0.018401225256828045, 0.019459907210152658, 0.016992107530093843, 0.014329129173458448, 0.015792106123047104, 0.017171915691996963, 0.015691799594720616, 0.01569894458375792, 0.014337103995230914, 0.019387566735629337, 0.01514392616252583, 0.018858499046838156, 0.015984719538379908, 0.013679485156897112, 0.015678604203760545, 0.012895688855741532, 0.014463500075239267, 0.01912346483320583, 0.018879628562321216, 0.014261030837310828, 0.01712161486388558, 0.01804372523775895, 0.01954024450115538, 0.01661557356878728, 0.018020037324616, 0.017219641013207904, 0.015142080694718887, 0.016967263435361084, 0.01677090820879039, 0.017971855055228633, 0.018592323745303545], "accuracy_test_std": 0.0048035214608089395, "error_valid": [0.7294098268072289, 0.6570500753012049, 0.7200398272778614, 0.7513633636106928, 0.720813429499247, 0.6756459431475903, 0.5990328501506024, 0.6625535344503012, 0.621251117752259, 0.644629788685994, 0.612147319747741, 0.6799184040850903, 0.4646393189947289, 0.537132906626506, 0.6387174675263554, 0.6249852927334337, 0.6162153496799698, 0.5018604692206325, 0.5782500117658133, 0.5925631235881024, 0.5818518213478916, 0.49530102833207834, 0.45491340361445776, 0.6301813700112951, 0.43767060429216864, 0.3841817465173193, 0.4569797745670181, 0.30954236869352414, 0.41860851609563254, 0.41166962772966864, 0.3806225880082832, 0.3435014471950302, 0.4247914509600903, 0.4602550828313253, 0.4298286897590362, 0.5693182887801205, 0.3221185523343373, 0.32986928181475905, 0.49266548616340367, 0.3586381659450302, 0.40169074736445776, 0.47025602409638556, 0.4774669968938253, 0.49762036426957834, 0.46025361210466864, 0.4761051040097892, 0.4876708984375, 0.40647207972515065, 0.36399014024849397, 0.4773861069277108, 0.4764095444277108, 0.4171716161521084, 0.43969432417168675, 0.415515577936747, 0.2913244775978916, 0.46699983527861444, 0.426135695124247, 0.49852633189006024, 0.4234486775225903, 0.30479339231927716, 0.5185973385730421, 0.36263560099774095, 0.4029717502823795, 0.42551357774849397, 0.3391363304781627, 0.4493202301393072, 0.30616705101656627, 0.42405020472515065, 0.41329625141189763, 0.35948236304593373, 0.46255235786897586, 0.37787527061370485, 0.43918692347515065, 0.4798378082643072, 0.3641416250941265, 0.6719823630459337, 0.48509859751506024, 0.5794501247176205, 0.44047086784638556, 0.4334084384412651, 0.40685741010918675, 0.36563588337725905, 0.47339749623493976, 0.37550445924322284, 0.526256883000753, 0.3431646507906627, 0.3633268425263554, 0.43142442818147586, 0.5742716961596386, 0.38389642554593373, 0.6393086996423193, 0.4757477174322289, 0.4674984116152108, 0.2582110669239458, 0.40641913356551207, 0.43505565229668675, 0.3723100409450302, 0.4172230915850903, 0.45413097703313254, 0.3756162344691265, 0.2586978774472892, 0.5485648649284638, 0.453857421875, 0.38558481974774095, 0.3916265648531627, 0.36073248070406627, 0.4301949006965362, 0.30706125282379515, 0.32655279320406627, 0.5159926816641567, 0.5606615916792168, 0.3497358574924698, 0.4704781038215362, 0.39970526637801207, 0.4461537556475903, 0.36066188582454817, 0.4433387848268072, 0.3819535956325302, 0.4181290592055723, 0.3812005835843373, 0.5113319488893072, 0.24275814194277112, 0.3613222420933735, 0.4695412509412651, 0.5130012236445783, 0.36600209431475905, 0.47566535673945776, 0.5064506071159638, 0.44805834666792166, 0.29431446489081325, 0.25228992140436746, 0.42460908085466864, 0.5893187005835843, 0.49063000047063254, 0.26205554640436746, 0.3734910344503012, 0.3956681217055723, 0.5924028143825302, 0.4885959855045181, 0.40823106880647586, 0.39606374717620485, 0.41916592149849397, 0.3004782803087349, 0.31133371376129515, 0.5905188135353916, 0.2918627635542168, 0.4384133212537651, 0.4076104221573795, 0.47359016142695776, 0.27548328077936746, 0.33135324501129515, 0.2979353939194277, 0.4403458560805723, 0.3521669686558735, 0.3303163827183735, 0.5099082854856928, 0.44827160203313254, 0.7472629776920181, 0.3562467644013554, 0.3839876105986446, 0.3265513224774097, 0.47776114222515065, 0.49205513460090367, 0.3201845467808735, 0.27595244258283136, 0.5370917262801205, 0.34334702089608427, 0.3271425545933735, 0.33921722044427716, 0.34666498023343373, 0.4552899096385542, 0.3971520849021084], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.060697013735622564, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 128, "valid_ratio": 0.15, "learning_rate": 0.006747520425760113, "optimization": "rmsprop", "nb_data_augmentation": 2, "learning_rate_decay_method": "none", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 6.446380391674313e-05, "rotation_range": [0, 0], "momentum": 0.717364898283756}, "accuracy_valid_max": 0.7572418580572289, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.6028479150978916, "accuracy_valid_std": [0.01110475724216907, 0.009872390250734383, 0.01575636678605838, 0.009325150107919383, 0.014247587267966275, 0.01077031035084516, 0.013335346477035106, 0.012740230932545837, 0.009007185252876915, 0.014194464968016992, 0.011752937257380611, 0.015869699668478416, 0.005108456571353535, 0.020985375190039315, 0.012059307770231243, 0.016372497743900195, 0.01380818812301401, 0.017835732468520517, 0.009332307419989732, 0.008422357196880774, 0.006770164246295004, 0.016702679389053565, 0.013269175953913156, 0.020186228600042443, 0.005724458111159258, 0.010648153523407032, 0.011228864530267888, 0.01165865188060531, 0.025089800134532395, 0.014207165835090125, 0.020101768359601837, 0.019476060031212866, 0.01019567197050801, 0.012133608652830384, 0.016564543552094038, 0.006467522102939518, 0.025098401951322954, 0.017447750708836397, 0.008076939298765698, 0.02071907259570734, 0.023294410103478786, 0.02019966559013556, 0.01338969894921129, 0.011393658943197781, 0.010880711240433813, 0.017750241816708246, 0.007719439332891786, 0.022872373610230437, 0.0233628397183408, 0.009885636015108992, 0.009400380421303882, 0.009414036200697265, 0.011800308584274407, 0.021331365076463438, 0.02306448800016808, 0.013960595740969198, 0.021953701837153835, 0.019238752306710738, 0.011573332250800353, 0.02167203077062727, 0.016555077218363124, 0.01005444753736609, 0.014838310366052333, 0.009347610510103212, 0.011209670469562049, 0.01828097870175576, 0.026903426548864625, 0.015467684688699544, 0.017077215072600745, 0.01757028669854018, 0.014687980047647061, 0.018980508722187997, 0.01103002781807189, 0.013823474855696984, 0.018675966436158663, 0.01445638503542348, 0.021329520296782326, 0.014184384411042021, 0.027766319663540467, 0.018833706139992818, 0.015544092082911158, 0.008292791990148955, 0.012939585185483028, 0.013363122354817425, 0.012204165920467051, 0.011085006818916488, 0.010491521046926374, 0.015934546202681824, 0.018287242113238227, 0.014937388569774794, 0.019466597973905767, 0.006878410429480076, 0.012502693500581847, 0.016956378459153654, 0.0160499342102866, 0.010267626805788285, 0.013010993235431244, 0.012495753354877634, 0.0172951101563631, 0.013046087833367815, 0.011362349099716843, 0.014115445641690717, 0.021315867496369992, 0.012336282575310583, 0.01818664730849367, 0.01769990336443524, 0.02131365333962392, 0.013686485545322518, 0.018478952261379775, 0.016631593769645986, 0.019167165917028335, 0.009222686117313948, 0.018851977314503204, 0.0073472528560494885, 0.014921084334288485, 0.014933886728511998, 0.02485078802184541, 0.01738090003203804, 0.01423850931702557, 0.014794265697291859, 0.011636294784030932, 0.013762708947211745, 0.015295679411872976, 0.017096287153209933, 0.011536936372324341, 0.01489176183916763, 0.015859147441627947, 0.013274879827202022, 0.018878088730178356, 0.016498036535540897, 0.012106895970935953, 0.015192337201928157, 0.012302174685249629, 0.015938738546473624, 0.01234490345817027, 0.01783620137397458, 0.01972884459742807, 0.022819599919780283, 0.014578597253469588, 0.012832245663304044, 0.011040760978277447, 0.014972299886118472, 0.011089504864189803, 0.013892770714055144, 0.01117437934099876, 0.022401122005535963, 0.009838265555767292, 0.012087264444726195, 0.012711642108360718, 0.01199037316318258, 0.014355802522477808, 0.00939795288750616, 0.014535750927701232, 0.009171058859056233, 0.012025109765402605, 0.00851003150617176, 0.011668282229887587, 0.013654368140189355, 0.01102838434082802, 0.02032328615448685, 0.006744533085450459, 0.012894914573541994, 0.016928707296370044, 0.01689126786311449, 0.01369047520335032, 0.01895306172319433, 0.009417129278375746, 0.017724027098460198, 0.00964450600356563, 0.020489351971906326, 0.011323988612780575, 0.008497340364669653], "accuracy_valid": [0.2705901731927711, 0.34294992469879515, 0.2799601727221386, 0.24863663638930722, 0.279186570500753, 0.32435405685240964, 0.4009671498493976, 0.3374464655496988, 0.37874888224774095, 0.35537021131400603, 0.38785268025225905, 0.32008159591490964, 0.5353606810052711, 0.46286709337349397, 0.3612825324736446, 0.37501470726656627, 0.3837846503200301, 0.49813953077936746, 0.42174998823418675, 0.4074368764118976, 0.4181481786521084, 0.5046989716679217, 0.5450865963855422, 0.36981862998870485, 0.5623293957078314, 0.6158182534826807, 0.5430202254329819, 0.6904576313064759, 0.5813914839043675, 0.5883303722703314, 0.6193774119917168, 0.6564985528049698, 0.5752085490399097, 0.5397449171686747, 0.5701713102409638, 0.4306817112198795, 0.6778814476656627, 0.670130718185241, 0.5073345138365963, 0.6413618340549698, 0.5983092526355422, 0.5297439759036144, 0.5225330031061747, 0.5023796357304217, 0.5397463878953314, 0.5238948959902108, 0.5123291015625, 0.5935279202748494, 0.636009859751506, 0.5226138930722892, 0.5235904555722892, 0.5828283838478916, 0.5603056758283133, 0.584484422063253, 0.7086755224021084, 0.5330001647213856, 0.573864304875753, 0.5014736681099398, 0.5765513224774097, 0.6952066076807228, 0.48140266142695787, 0.637364399002259, 0.5970282497176205, 0.574486422251506, 0.6608636695218373, 0.5506797698606928, 0.6938329489834337, 0.5759497952748494, 0.5867037485881024, 0.6405176369540663, 0.5374476421310241, 0.6221247293862951, 0.5608130765248494, 0.5201621917356928, 0.6358583749058735, 0.32801763695406627, 0.5149014024849398, 0.4205498752823795, 0.5595291321536144, 0.5665915615587349, 0.5931425898908133, 0.634364116622741, 0.5266025037650602, 0.6244955407567772, 0.473743116999247, 0.6568353492093373, 0.6366731574736446, 0.5685755718185241, 0.4257283038403614, 0.6161035744540663, 0.3606913003576807, 0.5242522825677711, 0.5325015883847892, 0.7417889330760542, 0.5935808664344879, 0.5649443477033133, 0.6276899590549698, 0.5827769084149097, 0.5458690229668675, 0.6243837655308735, 0.7413021225527108, 0.45143513507153615, 0.546142578125, 0.614415180252259, 0.6083734351468373, 0.6392675192959337, 0.5698050993034638, 0.6929387471762049, 0.6734472067959337, 0.4840073183358434, 0.43933840832078314, 0.6502641425075302, 0.5295218961784638, 0.6002947336219879, 0.5538462443524097, 0.6393381141754518, 0.5566612151731928, 0.6180464043674698, 0.5818709407944277, 0.6187994164156627, 0.4886680511106928, 0.7572418580572289, 0.6386777579066265, 0.5304587490587349, 0.48699877635542166, 0.633997905685241, 0.5243346432605422, 0.49354939288403615, 0.5519416533320783, 0.7056855351091867, 0.7477100785956325, 0.5753909191453314, 0.4106812994164157, 0.5093699995293675, 0.7379444535956325, 0.6265089655496988, 0.6043318782944277, 0.4075971856174699, 0.5114040144954819, 0.5917689311935241, 0.6039362528237951, 0.580834078501506, 0.6995217196912651, 0.6886662862387049, 0.4094811864646084, 0.7081372364457832, 0.5615866787462349, 0.5923895778426205, 0.5264098385730422, 0.7245167192206325, 0.6686467549887049, 0.7020646060805723, 0.5596541439194277, 0.6478330313441265, 0.6696836172816265, 0.4900917145143072, 0.5517283979668675, 0.2527370223079819, 0.6437532355986446, 0.6160123894013554, 0.6734486775225903, 0.5222388577748494, 0.5079448653990963, 0.6798154532191265, 0.7240475574171686, 0.4629082737198795, 0.6566529791039157, 0.6728574454066265, 0.6607827795557228, 0.6533350197665663, 0.5447100903614458, 0.6028479150978916], "seed": 244369140, "model": "residualv3", "loss_std": [0.21459336578845978, 0.11289422959089279, 0.10785573720932007, 0.10970626026391983, 0.10645436495542526, 0.11319424957036972, 0.10191453993320465, 0.09989600628614426, 0.10471448302268982, 0.10305435210466385, 0.11142066866159439, 0.10954996943473816, 0.10377133637666702, 0.09054381400346756, 0.09992163628339767, 0.1048012301325798, 0.09924035519361496, 0.09785814583301544, 0.09336540848016739, 0.09470714628696442, 0.09799683094024658, 0.09666736423969269, 0.0873730480670929, 0.0918884351849556, 0.09711810946464539, 0.08819624036550522, 0.09056735783815384, 0.09209331125020981, 0.08257347345352173, 0.08694341778755188, 0.08955387771129608, 0.0839039608836174, 0.08229949325323105, 0.08976183831691742, 0.08285260945558548, 0.08563232421875, 0.08152257651090622, 0.08049020916223526, 0.08361449092626572, 0.08318644016981125, 0.0838531032204628, 0.08925256133079529, 0.0858989953994751, 0.08030710369348526, 0.08279035985469818, 0.08120550215244293, 0.08227648586034775, 0.08603022247552872, 0.0825604498386383, 0.07799846678972244, 0.08394459635019302, 0.08082188665866852, 0.08337884396314621, 0.08268848806619644, 0.08363742381334305, 0.07931230962276459, 0.08081818372011185, 0.08148229122161865, 0.08026280254125595, 0.08078843355178833, 0.07537113130092621, 0.08597178012132645, 0.0782347097992897, 0.08083735406398773, 0.07938403636217117, 0.07611210644245148, 0.08163091540336609, 0.07760857790708542, 0.07926442474126816, 0.078079953789711, 0.07750226557254791, 0.08451923727989197, 0.07664747536182404, 0.07470375299453735, 0.08132980018854141, 0.07579203695058823, 0.08257820457220078, 0.0773579552769661, 0.07679247111082077, 0.07672993838787079, 0.07966174930334091, 0.07609225809574127, 0.07671032100915909, 0.07877285033464432, 0.07775767147541046, 0.08074355125427246, 0.07817104458808899, 0.07900241762399673, 0.07444120943546295, 0.07710303366184235, 0.07489043474197388, 0.08186842501163483, 0.07604429870843887, 0.08475793898105621, 0.07565463334321976, 0.07806548476219177, 0.07445493340492249, 0.0750303640961647, 0.08175672590732574, 0.0779188796877861, 0.07932735234498978, 0.07172232866287231, 0.08448965102434158, 0.07103827595710754, 0.07496895641088486, 0.07484634220600128, 0.07263301312923431, 0.07792161405086517, 0.07212235778570175, 0.07398691773414612, 0.07533768564462662, 0.0795583724975586, 0.07542794197797775, 0.0790581926703453, 0.0711512342095375, 0.07073401659727097, 0.07622377574443817, 0.07320848852396011, 0.07465699315071106, 0.07516394555568695, 0.07551784068346024, 0.08025786280632019, 0.07715725898742676, 0.07340190559625626, 0.07946279644966125, 0.078541599214077, 0.07594748586416245, 0.0830434262752533, 0.07831288129091263, 0.07593025267124176, 0.0752161368727684, 0.07053865492343903, 0.07905629277229309, 0.0824291855096817, 0.08153936266899109, 0.07234132289886475, 0.0754840299487114, 0.07388994097709656, 0.07722216099500656, 0.08196470886468887, 0.0747266411781311, 0.07580501586198807, 0.08133316785097122, 0.07142770290374756, 0.07060196995735168, 0.07623305171728134, 0.0723152905702591, 0.07258729636669159, 0.0701175108551979, 0.08452986925840378, 0.07337569445371628, 0.07495740056037903, 0.07179360836744308, 0.0770077034831047, 0.07584019005298615, 0.07161331921815872, 0.07400614768266678, 0.07519613951444626, 0.07656495273113251, 0.07453875243663788, 0.07702606171369553, 0.07294304668903351, 0.07632423937320709, 0.0756097063422203, 0.07693672925233841, 0.07205738127231598, 0.07137562334537506, 0.0767122134566307, 0.07399781793355942, 0.0744791328907013, 0.07304598391056061, 0.07287613302469254]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:37 2016", "state": "available"}], "summary": "93195e38dd28fda5b0e88debd387953f"}