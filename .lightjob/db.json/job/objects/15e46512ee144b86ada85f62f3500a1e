{"content": {"hp_model": {"f0": 64, "f1": 16, "f2": 64, "f3": 16, "nonlin": "rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.8018494844436646, 1.4582244157791138, 1.184990406036377, 1.0551398992538452, 0.9717429876327515, 0.9139852523803711, 0.8673719167709351, 0.8272514343261719, 0.7932823300361633, 0.7595980167388916, 0.732501745223999, 0.7074698805809021, 0.6842787861824036, 0.6641458868980408, 0.6468028426170349, 0.6303492784500122, 0.6131811738014221, 0.5997957587242126, 0.5848963856697083, 0.5723147988319397, 0.5634812116622925, 0.5519331097602844, 0.5393862128257751, 0.5311218500137329, 0.5227713584899902, 0.5141803026199341, 0.5071109533309937, 0.49903562664985657, 0.4897158443927765, 0.4834357798099518, 0.4755276143550873, 0.4704955518245697, 0.46705082058906555, 0.458755761384964, 0.4529832601547241, 0.44619816541671753, 0.44063010811805725, 0.4380018413066864, 0.43333181738853455, 0.4268018901348114, 0.4209146201610565, 0.4177584648132324, 0.4144991934299469, 0.41059014201164246, 0.40553948283195496, 0.4018138647079468, 0.39920181035995483, 0.39445093274116516, 0.39029911160469055, 0.38685712218284607, 0.3848576843738556, 0.38101664185523987, 0.376198947429657, 0.3730108141899109, 0.3703782856464386, 0.3683585226535797, 0.36341118812561035, 0.3623420000076294, 0.35646653175354004, 0.3565855324268341, 0.35399091243743896, 0.35207727551460266, 0.3500722646713257, 0.3455740809440613, 0.34524959325790405, 0.34036022424697876, 0.3378998041152954, 0.3366335928440094, 0.33608558773994446, 0.3321876525878906, 0.33005526661872864, 0.3266218900680542, 0.3247207701206207, 0.3246249556541443, 0.3221460282802582, 0.32109323143959045, 0.3178345859050751, 0.31701549887657166, 0.31545647978782654, 0.31066495180130005, 0.30940601229667664, 0.3088422417640686, 0.3077249526977539, 0.30525413155555725, 0.3030177056789398, 0.2998708486557007, 0.3028508424758911, 0.29794761538505554, 0.2991916239261627, 0.2978029251098633, 0.29327884316444397, 0.2947016656398773, 0.29087957739830017, 0.2902088761329651, 0.28884199261665344, 0.2868194878101349, 0.28502199053764343, 0.28603503108024597, 0.28315481543540955, 0.28183504939079285, 0.28136521577835083, 0.28063762187957764, 0.2776641249656677, 0.2767939269542694, 0.27650687098503113, 0.2752511501312256, 0.27209150791168213, 0.2731454372406006, 0.26987648010253906, 0.27133452892303467, 0.269298255443573, 0.266091525554657, 0.26660609245300293, 0.26908737421035767, 0.2663566470146179, 0.26486948132514954, 0.2604465186595917, 0.2598901093006134, 0.26100093126296997, 0.260611355304718, 0.2571682631969452, 0.25567471981048584, 0.2563299238681793, 0.25603917241096497, 0.2536224126815796, 0.2523055374622345, 0.2518523931503296, 0.2494906336069107, 0.2520467936992645, 0.2488223761320114, 0.2501334249973297, 0.2472442090511322, 0.24774394929409027, 0.2459743767976761, 0.2474663108587265, 0.24507498741149902, 0.24592897295951843, 0.24435962736606598, 0.24421128630638123, 0.24459493160247803, 0.24286749958992004, 0.2398652285337448, 0.23954717814922333, 0.23944871127605438, 0.23674966394901276, 0.23763172328472137, 0.23609113693237305, 0.23606091737747192, 0.23456932604312897, 0.23476852476596832, 0.23353347182273865, 0.23449015617370605, 0.234654039144516, 0.23222002387046814, 0.23275011777877808, 0.2320576310157776, 0.23219086229801178, 0.23005525767803192, 0.23014529049396515, 0.22834695875644684, 0.22639864683151245, 0.2267802506685257, 0.2278481274843216, 0.2256506383419037, 0.22464442253112793, 0.22537927329540253, 0.22317548096179962, 0.22403769195079803, 0.2214389443397522, 0.2247503697872162, 0.22274060547351837, 0.22192694246768951, 0.22092919051647186, 0.22080346941947937, 0.22030434012413025, 0.21937881410121918, 0.21741889417171478], "moving_avg_accuracy_train": [0.021266171500092284, 0.04728025361064968, 0.07858296851928755, 0.1217795322016888, 0.1719849430284468, 0.22251653752105247, 0.2734000423260716, 0.3184931819542064, 0.36165262362174255, 0.40129593626418253, 0.4386813966244752, 0.4745273249414481, 0.5068724378815042, 0.5380379303428923, 0.5661218589367407, 0.5913646623437773, 0.614225163682766, 0.6368199529104197, 0.657689975343861, 0.6761149586661102, 0.6942736061584416, 0.710869685926503, 0.726422322152282, 0.739868778682901, 0.7521168215937268, 0.7638679399373073, 0.7752784585762362, 0.7861522117000171, 0.7960012243340021, 0.8050397218653029, 0.8136649039446456, 0.8214343990672077, 0.828261895160856, 0.8352041316379875, 0.841703152392378, 0.8475244774320716, 0.8532400010343795, 0.858460666138352, 0.8629666378689244, 0.8678308758216038, 0.8724503252111119, 0.8765010170117061, 0.8798118182036695, 0.8834565318359603, 0.8867087281240328, 0.8894217910928218, 0.8925492503706289, 0.8956292109290348, 0.8984707857006109, 0.9014515242736377, 0.9041039260060193, 0.9065585168806388, 0.9092186554392065, 0.9112198299931079, 0.9129906962059142, 0.9152284338712217, 0.9172352781282948, 0.9181045832335162, 0.9199053009091495, 0.921900259726734, 0.9235957773125693, 0.925342596227907, 0.9271239605957494, 0.9285156360339596, 0.929849379941407, 0.9314054611771483, 0.9327222289321725, 0.9341445211390844, 0.9350968462896185, 0.936293230407196, 0.9372676335165779, 0.938223723472183, 0.9394260013072275, 0.940226600206359, 0.9413075012322347, 0.9424012198936181, 0.9429414993150629, 0.9436346890384107, 0.9445096037632149, 0.945355263882233, 0.9461348510333693, 0.9468853436432106, 0.9477583525432399, 0.9484604633425796, 0.9492852783155383, 0.9501835049078955, 0.9511964498386175, 0.9521034139298296, 0.9523175041190726, 0.9533075310869641, 0.9540196630950082, 0.9540049980844184, 0.9549286182522132, 0.9556228007699228, 0.9565287638465388, 0.9573394803178742, 0.9580016958265999, 0.9588534201046818, 0.959515484453802, 0.9601391360072682, 0.9605563352768349, 0.9612224942694542, 0.9618754076389743, 0.9625304589870185, 0.9627805695228866, 0.963119673394472, 0.9636945120431663, 0.9642327931662767, 0.9645032964377812, 0.9648747046642966, 0.965569298035999, 0.9662571750395696, 0.9669111415749261, 0.9672672326246133, 0.9675691854764931, 0.9680919149193383, 0.9684973033000512, 0.9689876748295884, 0.9694638864383147, 0.969834384214749, 0.9704049973921114, 0.9706534822874517, 0.9709584989015914, 0.9713353204019362, 0.9716860854962941, 0.9720715645943206, 0.9724626736099256, 0.9729355073644277, 0.9731961163732784, 0.9734956244526357, 0.9737303405407332, 0.9741368975200209, 0.9745586023728083, 0.9748195541510314, 0.9749219133181081, 0.9752325284589348, 0.9747866717059261, 0.9751386767448665, 0.9755717747692079, 0.9757639613911243, 0.9760298632055926, 0.9764272849576616, 0.9767501233511996, 0.9767012782768307, 0.9767689609015748, 0.9770715825935786, 0.9772579116104297, 0.97755116576131, 0.9779383273840069, 0.9783030488861008, 0.9783499912808516, 0.9787991044289754, 0.9790684836801532, 0.9791643685335757, 0.9792041258766466, 0.97923067898781, 0.9796009879116573, 0.9797715055264531, 0.9799040450404837, 0.9801675619269391, 0.980521020614044, 0.9807925223097917, 0.9809811063133549, 0.981301930540362, 0.9814860406482397, 0.9814773535846155, 0.9818183075487822, 0.9820205344201037, 0.9820258993924067, 0.9821701646984133, 0.9823208937642863, 0.9824217087402479, 0.9825706069876702, 0.9828650146293885, 0.983076503084316, 0.9831994133782747, 0.9833704865118851], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.020829019201807224, 0.0469699501129518, 0.07920290144954818, 0.12219725621234936, 0.17080266707454814, 0.22071841175640056, 0.2706405523672816, 0.31437521936323415, 0.35685024213700106, 0.3950462890829395, 0.4308449240112419, 0.46515933385974123, 0.49573300890750205, 0.5252462691048543, 0.5521816207805887, 0.5753908401031322, 0.5963666457294907, 0.6170515114182133, 0.6361999994424312, 0.6531009602944984, 0.6694735520474281, 0.6845395039774744, 0.6984406575895162, 0.7101511793211519, 0.7207699210464162, 0.7310215598717446, 0.7408788294775671, 0.7504126403362863, 0.7586756872966335, 0.7660371283561267, 0.7732514513356495, 0.7799640685797201, 0.785723632871974, 0.791118878300891, 0.7958209082512989, 0.8003885021023738, 0.8046814125284316, 0.8086243040786758, 0.8115482888628263, 0.8147333096094503, 0.8178104363386408, 0.8208117839886623, 0.8232332646635913, 0.8256689449272774, 0.8277491348660255, 0.8296142463229471, 0.8317302407418572, 0.833791268107807, 0.8353939631155505, 0.8371781854975195, 0.8386629448374513, 0.8397346445178627, 0.8410328821086217, 0.8423243957614643, 0.8429374416427726, 0.844029380836929, 0.8449612389693505, 0.8448558518476413, 0.8458403697314013, 0.8466643711618757, 0.8475036286993025, 0.8482345464204868, 0.8487682430397333, 0.8494449120057147, 0.8500528845664383, 0.8506519765220686, 0.8512368988811568, 0.8518762808115652, 0.8521566967816134, 0.8525098159306358, 0.8530829413123463, 0.8532416617236568, 0.8535584970573151, 0.8538945359999269, 0.8542590357131872, 0.8546053224656636, 0.8543920781991423, 0.8544463580015925, 0.8544321156502284, 0.8547753604575701, 0.8548146965880179, 0.8550586481453306, 0.8552893820695023, 0.8554115933825069, 0.8555480566440303, 0.8560025224404707, 0.8562609392476284, 0.8566664718288896, 0.8564891942337265, 0.8568261617173569, 0.8569889780569164, 0.8566482610211796, 0.8570140319164262, 0.857233362440898, 0.857713580648992, 0.8579351689790475, 0.858096947873688, 0.8586617059760029, 0.8590845390493363, 0.8590612272754268, 0.8593983686197968, 0.8594433891561605, 0.8596771611215686, 0.8599119699529358, 0.8597113473646453, 0.8596222662332259, 0.8597191687044063, 0.8598440315308784, 0.8600561233420224, 0.8602205328922328, 0.8605729324926932, 0.8609064171989962, 0.8609318216952111, 0.8607206931307352, 0.8606781913063665, 0.8607030338380038, 0.8607406876737065, 0.8608712028671792, 0.8608788032600546, 0.8612172924747118, 0.861410010469334, 0.8615855156818133, 0.8614474130970657, 0.8613820969097237, 0.8613508149295948, 0.8612626554998883, 0.8616339426607429, 0.8616598367982831, 0.8616852005393885, 0.8617568560313833, 0.8615894123804287, 0.8616371141205484, 0.8618132935217466, 0.861959647951575, 0.8616152927196705, 0.8615861347297065, 0.8611030848391906, 0.8613194306286149, 0.8615151713477565, 0.8616801604723935, 0.8619018928720668, 0.8618817254692727, 0.8618625452980985, 0.8618432241267223, 0.8619356983537337, 0.8620677532830441, 0.8618448058444235, 0.8618740277260957, 0.8618535583119199, 0.861895141486752, 0.8614300190455316, 0.8615373407008429, 0.8616258411940115, 0.8615447412142941, 0.8615460229287081, 0.8613365684144517, 0.8613108687925096, 0.8610883085981231, 0.860998897213085, 0.8609397525030718, 0.8611856680660176, 0.8612849217601688, 0.8615197049512453, 0.8616832112068739, 0.8618059527744395, 0.8617465512564082, 0.86204709379643, 0.86215786116754, 0.862062239301539, 0.8620626583495478, 0.862037591921596, 0.8619498789455509, 0.8620103621541585, 0.8621014181356552, 0.8621589544565024, 0.8623450144890148, 0.8623283335408664], "moving_var_accuracy_train": [0.004070250452442037, 0.009753817619691237, 0.01759717550358525, 0.032630945978936454, 0.05205310086759477, 0.06982876915852151, 0.08614807179385103, 0.09583378578816809, 0.1030150438548324, 0.10685786960474536, 0.10875113646142998, 0.10944039800743703, 0.10791221518663813, 0.10586258495122189, 0.10237468986347911, 0.09787201299174786, 0.09278823438580228, 0.08810413144940088, 0.08321373883181185, 0.07794768504245711, 0.07312054484696819, 0.06828735913528206, 0.06363558366389657, 0.05889929003657535, 0.054359492029226686, 0.050166341867227515, 0.04632150710098887, 0.04275350295386241, 0.03935118010725573, 0.03615131203514013, 0.03320572472474245, 0.030428437742403842, 0.027805126294342515, 0.025458365490647998, 0.0232926643784772, 0.021268388367589347, 0.019435554421267247, 0.017737297076291052, 0.0161463013997924, 0.014744618557555737, 0.013462210515760206, 0.012263662400754797, 0.011135948801473674, 0.010141909358478968, 0.009222909448896442, 0.008366864900060328, 0.007618207423863374, 0.0069417620948490615, 0.006320256810006272, 0.00576819435097221, 0.005254692030424455, 0.0047834479746379005, 0.00436879021153112, 0.0039679534867346515, 0.0035993818423541144, 0.003284510886847323, 0.0029923066130119165, 0.0026998771740044023, 0.002459072713930004, 0.0022489841886917275, 0.002049958788777445, 0.0018724252968065364, 0.001713742098225049, 0.0015597987331304026, 0.001419828715113243, 0.0012996383429119507, 0.0011852794045067985, 0.001084957700152701, 0.00098462423886849, 0.0008990438295927651, 0.0008176845994096468, 0.0007441431114975617, 0.0006827380482815597, 0.000620232870829018, 0.000568724706995769, 0.0005226182208885165, 0.000472983515478795, 0.0004300097718639118, 0.00039389807665863333, 0.0003609445383248496, 0.0003303198896283153, 0.0003023570530823221, 0.00027898064862986256, 0.00025551921993782143, 0.00023609017560059152, 0.00021974245714149101, 0.00020700272832142183, 0.0001937057102540131, 0.00017474765071078254, 0.00016609426621407743, 0.00015404902756459794, 0.00013864606037095856, 0.00013245912226307803, 0.00012355021434781383, 0.00011858211477875731, 0.00011263925407293255, 0.00010532209308561077, 0.00010131879198991836, 9.513187561231061e-05, 8.911915939234727e-05, 8.177374052785502e-05, 7.759027670609653e-05, 7.366791184836931e-05, 7.016295108070376e-05, 6.370965349400348e-05, 5.837361106612142e-05, 5.5510205207802284e-05, 5.2566903794495696e-05, 4.79687615940978e-05, 4.441338207119748e-05, 4.431418343219523e-05, 4.4141338037347696e-05, 4.357625429790792e-05, 4.035983638912288e-05, 3.714443247303549e-05, 3.588920385948856e-05, 3.3779341126492955e-05, 3.256558514666909e-05, 3.135002409857324e-05, 2.945043910980083e-05, 2.9435789782436307e-05, 2.7047913493103474e-05, 2.5180438357904096e-05, 2.3940344510212273e-05, 2.2653635421970095e-05, 2.172561909491137e-05, 2.0929753544206772e-05, 2.084892402435495e-05, 1.9375285121366743e-05, 1.824510241563264e-05, 1.6916416952175323e-05, 1.671237245362571e-05, 1.6641650054043955e-05, 1.559034752365951e-05, 1.412560936305516e-05, 1.3581384318146732e-05, 1.401234008416284e-05, 1.3726274002701639e-05, 1.4041811690626723e-05, 1.2970051800356864e-05, 1.2309380594758626e-05, 1.2499938976441216e-05, 1.2187966733876972e-05, 1.0990642632100071e-05, 9.932806808120224e-06, 9.76374512354959e-06, 9.099837133880808e-06, 8.963835393569212e-06, 9.416498953016583e-06, 9.672045024521998e-06, 8.724672817894385e-06, 9.667529114463724e-06, 9.353862831703165e-06, 8.501221694575499e-06, 7.66532534207054e-06, 6.905138417275548e-06, 7.44878286727633e-06, 6.965590893149808e-06, 6.427132308849995e-06, 6.409389422989908e-06, 6.892847872100279e-06, 6.8669816220350715e-06, 6.500358797430532e-06, 6.776676579399676e-06, 6.404077707864688e-06, 5.7643491227479285e-06, 6.234160661602186e-06, 5.978805962802252e-06, 5.381184412872338e-06, 5.030378278239844e-06, 4.731813712106261e-06, 4.3501052752987805e-06, 4.114630940537618e-06, 4.483250582003582e-06, 4.437471822911887e-06, 4.12968710386955e-06, 3.980112546871911e-06], "duration": 75899.229896, "accuracy_train": [0.21266171500092287, 0.2814069926056663, 0.36030740269702843, 0.5105486053433002, 0.623833640469269, 0.6773008879545035, 0.731351585571244, 0.7243314386074198, 0.7500875986295681, 0.7580857500461425, 0.7751505398671096, 0.7971406797942044, 0.7979784543420081, 0.8185273624953857, 0.8188772162813769, 0.8185498930071059, 0.8199696757336655, 0.8401730559593023, 0.845520177244832, 0.8419398085663529, 0.8577014335894242, 0.860234403839055, 0.866396048184293, 0.8608868874584718, 0.8623492077911591, 0.8696280050295312, 0.8779731263265966, 0.8840159898140458, 0.884642338039867, 0.8863861996470099, 0.8912915426587301, 0.8913598551702658, 0.8897093600036915, 0.8976842599321706, 0.9001943391818937, 0.8999164027893135, 0.9046797134551495, 0.9054466520741048, 0.9035203834440754, 0.911609017395718, 0.914025369716685, 0.9129572432170543, 0.90960902893134, 0.916258954526578, 0.915978494716685, 0.9138393578119232, 0.9206963838708934, 0.923348855954688, 0.9240449586447952, 0.9282781714308784, 0.927975541597453, 0.928649834752215, 0.9331599024663161, 0.9292304009782208, 0.9289284921211702, 0.9353680728589886, 0.9352968764419527, 0.9259283291805095, 0.9361117599898486, 0.9398548890849945, 0.9388554355850868, 0.9410639664659468, 0.9431562399063308, 0.9410407149778516, 0.9418530751084349, 0.9454101922988187, 0.9445731387273901, 0.946945151001292, 0.943667772644426, 0.9470606874653931, 0.9460372615010151, 0.9468285330726283, 0.9502465018226283, 0.9474319902985419, 0.9510356104651162, 0.9522446878460686, 0.9478040141080657, 0.9498733965485419, 0.9523838362864526, 0.952966204953396, 0.9531511353935955, 0.9536397771317828, 0.9556154326435032, 0.9547794605366371, 0.9567086130721669, 0.9582675442391103, 0.9603129542151162, 0.9602660907507383, 0.9542443158222591, 0.9622177737979882, 0.9604288511674051, 0.9538730129891103, 0.9632411997623662, 0.9618704434293098, 0.9646824315360835, 0.964635928559893, 0.9639616354051311, 0.9665189386074198, 0.9654740635958842, 0.9657519999884644, 0.9643111287029347, 0.9672179252030271, 0.9677516279646549, 0.9684259211194168, 0.9650315643456996, 0.9661716082387413, 0.9688680598814139, 0.969077323274271, 0.9669378258813216, 0.9682173787029347, 0.9718206383813216, 0.9724480680717055, 0.9727968403931341, 0.9704720520717978, 0.970286761143411, 0.9727964799049464, 0.9721457987264673, 0.9734010185954227, 0.9737497909168512, 0.9731688642026578, 0.9755405159883721, 0.972889846345515, 0.9737036484288483, 0.9747267139050388, 0.974842971345515, 0.9755408764765596, 0.9759826547503692, 0.9771910111549464, 0.9755415974529347, 0.9761911971668512, 0.9758427853336102, 0.9777959103336102, 0.9783539460478959, 0.9771681201550388, 0.9758431458217978, 0.978028064726375, 0.9707739609288483, 0.9783067220953304, 0.9794696569882798, 0.9774936409883721, 0.9784229795358066, 0.9800040807262828, 0.9796556688930418, 0.9762616726075121, 0.977378104524271, 0.9797951778216132, 0.9789348727620893, 0.9801904531192323, 0.9814227819882798, 0.9815855424049464, 0.9787724728336102, 0.9828411227620893, 0.981492896940753, 0.980027332214378, 0.9795619419642857, 0.9794696569882798, 0.9829337682262828, 0.9813061640596161, 0.9810969006667589, 0.9825392139050388, 0.9837021487979882, 0.9832360375715209, 0.9826783623454227, 0.9841893485834257, 0.98314303161914, 0.9813991700119971, 0.9848868932262828, 0.9838405762619971, 0.9820741841431341, 0.9834685524524732, 0.9836774553571429, 0.9833290435239018, 0.9839106912144703, 0.9855146834048542, 0.9849798991786637, 0.9843056060239018, 0.984910144714378], "end": "2016-02-02 08:04:09.361000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0], "moving_var_accuracy_valid": [0.003904632368183288, 0.009664303551476065, 0.018048541563134974, 0.032880318280047935, 0.05085466013779151, 0.06819342822830726, 0.08380406651402972, 0.09263814973784851, 0.09961148280075184, 0.10278077654134365, 0.10403657925177816, 0.1042302298358574, 0.1022199533056063, 0.0998372507223352, 0.09638314417916109, 0.09159284051530234, 0.08639341625884504, 0.08160484765000497, 0.076744344227527, 0.07164069210428224, 0.06688917874022693, 0.062243107034230444, 0.05775797497651762, 0.05321640435190988, 0.04890958299916971, 0.04496448958669754, 0.04134253250476464, 0.038026321199696664, 0.03483819058534714, 0.031842088857063966, 0.029126298075833418, 0.02661920134063862, 0.02425583443410421, 0.022092229049838045, 0.020081987915745034, 0.018261555346465928, 0.016601261531154737, 0.015081052922032148, 0.013649894812990426, 0.012376204546099211, 0.011223802471656778, 0.010182495013937703, 0.009217017630475421, 0.008348708712550067, 0.007552782552926483, 0.006828812064354495, 0.006186227749346774, 0.005605835478640843, 0.005068369612367369, 0.004590183696705506, 0.0041510059197125875, 0.003746242189586276, 0.0033867867582061837, 0.0030631201500248757, 0.002760190562295691, 0.0024949024868997378, 0.0022532274744204057, 0.0020280046849871646, 0.001833927695659437, 0.001656645731310307, 0.001497320337106427, 0.0013523964698320544, 0.0012197203115814053, 0.001101869208428967, 0.0009950089632974068, 0.0008987382675093742, 0.0008119436482538899, 0.000734428566704896, 0.0006616934080807296, 0.0005966463054733132, 0.0005399379292544294, 0.00048617086584968575, 0.0004384572409226072, 0.0003956278163689118, 0.00035726077510072235, 0.0003226139282251164, 0.00029076179345744217, 0.00026171213078428427, 0.00023554274330700724, 0.0002130488219562094, 0.00019175786574101593, 0.00017311769042775184, 0.00015628506467884993, 0.00014079097865620166, 0.00012687948078629195, 0.00011605038514887011, 0.00010504635984997757, 9.602183393515961e-05, 8.67024966533648e-05, 7.905417075324593e-05, 7.138733612176929e-05, 6.529339539556397e-05, 5.996815098629236e-05, 5.4404288798348974e-05, 5.103934566497857e-05, 4.6377323590632276e-05, 4.197514292832876e-05, 4.064819406266916e-05, 3.819246492754366e-05, 3.437810938401458e-05, 3.196327702036544e-05, 2.878519095657917e-05, 2.6398515847217915e-05, 2.425488094808855e-05, 2.219163765967135e-05, 2.004389292547877e-05, 1.812401443321875e-05, 1.6451929518808097e-05, 1.5211582994116721e-05, 1.3933699196508561e-05, 1.365799858249989e-05, 1.3293107168291671e-05, 1.1969604947313871e-05, 1.1173821889221477e-05, 1.007269734597134e-05, 9.070981973777628e-06, 8.176644078487954e-06, 7.5122876121841615e-06, 6.761578744712497e-06, 7.116595406194419e-06, 6.7391978946357795e-06, 6.342496821638682e-06, 5.879898054700536e-06, 5.330304088190675e-06, 4.806080739898727e-06, 4.395421431324369e-06, 5.196566690530745e-06, 4.682944578708215e-06, 4.22043999510316e-06, 3.844606581390105e-06, 3.7124823094561604e-06, 3.3617131826046406e-06, 3.3048944970030965e-06, 3.1671816194762323e-06, 3.917688189188515e-06, 3.5335710656783475e-06, 5.280248729656704e-06, 5.173473362105685e-06, 5.000955888065486e-06, 4.7458530004953e-06, 4.713755014029707e-06, 4.246040029845844e-06, 3.8247469375577215e-06, 3.445632012772057e-06, 3.1780321554472226e-06, 3.0171754790989663e-06, 3.1628079746767805e-06, 2.8542124425252313e-06, 2.5725621705230275e-06, 2.330868397332784e-06, 4.0448315255407676e-06, 3.7440098122755115e-06, 3.440099866667943e-06, 3.1552847403926525e-06, 2.839771051479938e-06, 2.9506346882132005e-06, 2.661515454503609e-06, 2.8411612701814636e-06, 2.628994705133396e-06, 2.3975781051230936e-06, 2.702090471501518e-06, 2.5205430865754145e-06, 2.76459709922642e-06, 2.728746049970601e-06, 2.5914608766497565e-06, 2.3640716520845856e-06, 2.9405968521411113e-06, 2.756961861450624e-06, 2.563557546623152e-06, 2.3072033723719394e-06, 2.0821379674270923e-06, 1.9431662661846397e-06, 1.7817736062773673e-06, 1.6782169715467504e-06, 1.5401891283416883e-06, 1.6977352367942024e-06, 1.5304659993949677e-06], "accuracy_test": 0.790658880739796, "start": "2016-02-01 10:59:10.131000", "learning_rate_per_epoch": [0.0044298539869487286, 0.0022149269934743643, 0.0014766179956495762, 0.0011074634967371821, 0.0008859707741066813, 0.0007383089978247881, 0.0006328362505882978, 0.0005537317483685911, 0.0004922060179524124, 0.0004429853870533407, 0.0004027139802929014, 0.00036915449891239405, 0.0003407580079510808, 0.0003164181252941489, 0.00029532358166761696, 0.00027686587418429554, 0.00026057963259518147, 0.0002461030089762062, 0.00023315020371228456, 0.00022149269352667034, 0.00021094542171340436, 0.0002013569901464507, 0.00019260233966633677, 0.00018457724945619702, 0.00017719416064210236, 0.0001703790039755404, 0.0001640686678001657, 0.00015820906264707446, 0.00015275357873179018, 0.00014766179083380848, 0.00014289851242210716, 0.00013843293709214777, 0.00013423799828160554, 0.00013028981629759073, 0.00012656726175919175, 0.0001230515044881031, 0.00011972578067798167, 0.00011657510185614228, 0.00011358600022504106, 0.00011074634676333517, 0.00010804522025864571, 0.00010547271085670218, 0.0001030198618536815, 0.00010067849507322535, 9.844119631452486e-05, 9.630116983316839e-05, 9.425220923731104e-05, 9.228862472809851e-05, 9.040518489200622e-05, 8.859708032105118e-05, 8.685987995704636e-05, 8.51895019877702e-05, 8.35821483633481e-05, 8.203433390008286e-05, 8.054279896896333e-05, 7.910453132353723e-05, 7.771673699608073e-05, 7.637678936589509e-05, 7.508227281505242e-05, 7.383089541690424e-05, 7.262055441970006e-05, 7.144925621105358e-05, 7.031513814581558e-05, 6.921646854607388e-05, 6.815159576945007e-05, 6.711899914080277e-05, 6.611722346860915e-05, 6.514490814879537e-05, 6.420077988877892e-05, 6.328363087959588e-05, 6.239231151994318e-05, 6.152575224405155e-05, 6.068293077987619e-05, 5.986289033899084e-05, 5.9064717788714916e-05, 5.828755092807114e-05, 5.753057121182792e-05, 5.679300011252053e-05, 5.607409912045114e-05, 5.5373173381667584e-05, 5.4689553508069366e-05, 5.402261012932286e-05, 5.337173570296727e-05, 5.273635542835109e-05, 5.2115927246632054e-05, 5.150993092684075e-05, 5.0917860789923e-05, 5.0339247536612675e-05, 4.9773640057537705e-05, 4.922059815726243e-05, 4.8679714382160455e-05, 4.8150584916584194e-05, 4.763283868669532e-05, 4.712610461865552e-05, 4.663004074245691e-05, 4.6144312364049256e-05, 4.5668595703318715e-05, 4.520259244600311e-05, 4.474600063986145e-05, 4.429854016052559e-05, 4.3859938159585e-05, 4.342993997852318e-05, 4.3008290958823636e-05, 4.25947509938851e-05, 4.218908361508511e-05, 4.179107418167405e-05, 4.140050441492349e-05, 4.101716695004143e-05, 4.0640861698193476e-05, 4.027139948448166e-05, 3.990859477198683e-05, 3.9552265661768615e-05, 3.920224844478071e-05, 3.885836849804036e-05, 3.852046938845888e-05, 3.8188394682947546e-05, 3.7861998862354085e-05, 3.754113640752621e-05, 3.722566179931164e-05, 3.691544770845212e-05, 3.66103631677106e-05, 3.631027720985003e-05, 3.601507341954857e-05, 3.572462810552679e-05, 3.543883212842047e-05, 3.515756907290779e-05, 3.4880740713560954e-05, 3.460823427303694e-05, 3.4339951525907964e-05, 3.4075797884725034e-05, 3.381567876203917e-05, 3.3559499570401385e-05, 3.330717299832031e-05, 3.305861173430458e-05, 3.281373210484162e-05, 3.257245407439768e-05, 3.233470124541782e-05, 3.210038994438946e-05, 3.186945104971528e-05, 3.164181543979794e-05, 3.141740307910368e-05, 3.119615575997159e-05, 3.097800072282553e-05, 3.076287612202577e-05, 3.05507164739538e-05, 3.0341465389938094e-05, 3.0135061024338938e-05, 2.993144516949542e-05, 2.973056325572543e-05, 2.9532358894357458e-05, 2.9336781153688207e-05, 2.914377546403557e-05, 2.8953292712685652e-05, 2.876528560591396e-05, 2.8579703212017193e-05, 2.8396500056260265e-05, 2.8215630663908087e-05, 2.803704956022557e-05, 2.7860716727445833e-05, 2.7686586690833792e-05, 2.751462125161197e-05, 2.7344776754034683e-05, 2.7177018637303263e-05, 2.701130506466143e-05, 2.684759965632111e-05, 2.6685867851483636e-05, 2.6526071451371536e-05, 2.6368177714175545e-05, 2.62121538980864e-05, 2.6057963623316027e-05, 2.5905577786033973e-05, 2.5754965463420376e-05, 2.560609209467657e-05, 2.54589303949615e-05, 2.5313451260444708e-05, 2.5169623768306337e-05, 2.5027422452694736e-05], "accuracy_train_first": 0.21266171500092287, "accuracy_train_last": 0.984910144714378, "batch_size_eval": 1024, "accuracy_train_std": [0.01312559384726595, 0.013384841776818394, 0.014775250450100596, 0.014956967215904615, 0.014313885838135521, 0.01483238034179395, 0.011189641476479049, 0.014227314526893367, 0.011770720661741028, 0.012157046351579345, 0.011544176587417092, 0.012121998290630044, 0.011526111742250988, 0.010968975519902524, 0.011733025888725582, 0.013087420760042153, 0.011469692765061944, 0.010697848833190344, 0.01074426904986346, 0.0105217210790718, 0.010773063739238883, 0.01048638395219232, 0.010210530367687367, 0.011708028056678104, 0.011000994630945891, 0.010201943157151339, 0.010359373307243902, 0.01009401384733119, 0.010340333311417113, 0.01105357275482255, 0.010301455985672454, 0.010749265927269252, 0.010592280604015913, 0.009419821937597628, 0.011289557192655182, 0.010160457653527449, 0.01062648047721504, 0.010518926029251238, 0.009108766231132322, 0.010776081119625674, 0.010888233976746486, 0.009049324861690595, 0.010023560584398688, 0.00951333665881067, 0.011372129790417542, 0.011226922719966231, 0.011715576259798485, 0.01038399760838303, 0.010492478271055376, 0.009661286401538726, 0.011159912223073547, 0.010506644975227707, 0.009226662732208226, 0.010279909826971112, 0.010182573401494487, 0.010254251921922853, 0.010588020138952922, 0.01139186008474904, 0.010158273002518979, 0.010050351657101018, 0.009733594367406177, 0.009697157640998193, 0.009400618365538403, 0.009503538492094753, 0.010463947903563137, 0.009410045859896311, 0.010038571730775964, 0.008594187075520024, 0.009673970006427961, 0.009677273959556696, 0.009818371069798756, 0.009515864911668597, 0.009288965569443287, 0.009456543592582783, 0.009719164923694429, 0.008755753161720751, 0.009902776963344776, 0.00926579962015997, 0.009040911850585141, 0.008733063467466329, 0.00892356786909811, 0.009461792666959613, 0.008984833867767718, 0.009142555523023288, 0.008101261063791249, 0.007949672482064981, 0.00739131101061881, 0.008456092515962322, 0.008202702891177041, 0.008782325402770389, 0.007713893961704015, 0.008617201716592676, 0.008072561939053662, 0.007458066755643112, 0.00815566905365608, 0.008121133840564603, 0.008244111155325032, 0.008020738924985591, 0.006915673518536998, 0.007830132581091791, 0.008006253092160149, 0.007311877738500695, 0.007339059358830634, 0.007361103761596049, 0.006902845943498465, 0.007045533502154262, 0.00739717972543415, 0.006932739569749127, 0.007821727859947707, 0.006099070222731884, 0.0070190542611894575, 0.006455204381065371, 0.006940071980225359, 0.007591958119299425, 0.0072105243223955845, 0.007642993640416136, 0.007107204008595945, 0.006841019562723676, 0.0066878909759997334, 0.006796756215027559, 0.006629671945082331, 0.006911330768761156, 0.00725606101542933, 0.006961433528176977, 0.007041220848099967, 0.0066096813395498995, 0.006368514561776036, 0.006192559619371818, 0.005906349773738103, 0.006754352174890729, 0.006629989592565668, 0.006030634797267825, 0.00589287631206002, 0.006030343956889314, 0.006695312386035843, 0.0061202768838677585, 0.007796518222055968, 0.006346222477825221, 0.005874919062137075, 0.006244862031757698, 0.006372188155340122, 0.0057676848506544864, 0.005949513988425384, 0.006380664149728438, 0.005835844802979837, 0.005507831046236589, 0.005628868173483356, 0.006445422298822103, 0.005327390938356686, 0.005154427362152322, 0.005885639085135557, 0.00521369648911486, 0.005712427059706667, 0.006465462525030207, 0.00640868547474519, 0.0063503973112295185, 0.005443439288364898, 0.005788562236526412, 0.0058013098601808555, 0.005560145383588078, 0.004812688974432011, 0.005642823439804666, 0.005367862852965318, 0.005252288855486676, 0.00524293400591977, 0.0055661076539162595, 0.005200103706276381, 0.005911182955899061, 0.005735599101827101, 0.005209602238527758, 0.005307454571269999, 0.005760207756978442, 0.005032243863019752, 0.005047968378165057, 0.005224989446523658, 0.005740402774338514, 0.005107422596648292], "accuracy_test_std": 0.008441213442353067, "error_valid": [0.7917098079819277, 0.717761671686747, 0.6307005365210843, 0.49085355092243976, 0.3917486351656627, 0.3300398861069277, 0.2800601821347892, 0.2920127776731928, 0.26087455289909633, 0.26118928840361444, 0.2469673616340362, 0.2260109775037651, 0.22910391566265065, 0.20913438911897586, 0.20540021413780118, 0.21572618599397586, 0.2148511036332832, 0.1967846973832832, 0.1914636083396084, 0.19479039203689763, 0.18317312217620485, 0.1798669286521084, 0.1764489599021084, 0.1844541250941265, 0.18366140342620485, 0.17671369070030118, 0.17040574407003017, 0.16378306193524095, 0.16695689006024095, 0.16770990210843373, 0.1618196418486446, 0.1596223762236446, 0.16244028849774095, 0.1603239128388554, 0.16186082219503017, 0.15850315323795183, 0.15668239363704817, 0.1558896719691265, 0.1621358480798193, 0.15660150367093373, 0.1544954230986446, 0.1521760871611446, 0.15497340926204817, 0.15240993269954817, 0.15352915568524095, 0.15359975056475905, 0.14922580948795183, 0.1476594855986446, 0.15018178181475905, 0.14676381306475905, 0.14797422110316272, 0.15062005835843373, 0.14728297957454817, 0.14605198136295183, 0.15154514542545183, 0.14614316641566272, 0.1466520378388554, 0.15609263224774095, 0.14529896931475905, 0.1459196159638554, 0.1449430534638554, 0.1451871940888554, 0.14642848738704817, 0.14446506730045183, 0.14447536238704817, 0.14395619587725905, 0.14349879988704817, 0.14236928181475905, 0.14531955948795183, 0.14431211172816272, 0.14175893025225905, 0.14532985457454817, 0.14358998493975905, 0.14308111351656627, 0.14246046686746983, 0.14227809676204817, 0.14752712019954817, 0.1450651237763554, 0.14569606551204817, 0.1421354362763554, 0.14483127823795183, 0.1427457878388554, 0.14263401261295183, 0.14348850480045183, 0.14322377400225905, 0.13990728539156627, 0.14141330948795183, 0.13968373493975905, 0.14510630412274095, 0.14014113092996983, 0.14154567488704817, 0.14641819230045183, 0.1396940300263554, 0.1407926628388554, 0.13796445547816272, 0.14007053605045183, 0.14044704207454817, 0.13625547110316272, 0.13710996329066272, 0.14114857868975905, 0.1375673592808735, 0.14015142601656627, 0.13821889118975905, 0.13797475056475905, 0.14209425592996983, 0.14117946394954817, 0.13940870905496983, 0.1390322030308735, 0.1380350503576807, 0.1382997811558735, 0.13625547110316272, 0.13609222044427716, 0.1388395378388554, 0.14117946394954817, 0.13970432511295183, 0.13907338337725905, 0.13892042780496983, 0.13795416039156627, 0.13905279320406627, 0.1357363045933735, 0.13685552757906627, 0.1368349374058735, 0.13979551016566272, 0.1392057487763554, 0.13893072289156627, 0.13953077936746983, 0.13502447289156627, 0.1381071159638554, 0.13808652579066272, 0.13759824454066272, 0.13991758047816272, 0.1379335702183735, 0.13660109186746983, 0.13672316217996983, 0.14148390436746983, 0.13867628717996983, 0.14324436417545183, 0.13673345726656627, 0.13672316217996983, 0.1368349374058735, 0.1361025155308735, 0.1382997811558735, 0.13831007624246983, 0.13833066641566272, 0.13723203360316272, 0.13674375235316272, 0.14016172110316272, 0.1378629753388554, 0.13833066641566272, 0.13773060993975905, 0.14275608292545183, 0.1374967644013554, 0.13757765436746983, 0.13918515860316272, 0.13844244164156627, 0.1405485222138554, 0.13892042780496983, 0.1409147331513554, 0.13980580525225905, 0.13959254988704817, 0.13660109186746983, 0.13782179499246983, 0.13636724632906627, 0.13684523249246983, 0.13708937311746983, 0.1387880624058735, 0.1352480233433735, 0.13684523249246983, 0.13879835749246983, 0.1379335702183735, 0.13818800592996983, 0.1388395378388554, 0.1374452889683735, 0.1370790780308735, 0.1373232186558735, 0.1359804452183735, 0.13782179499246983], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.027200859023231185, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 128, "valid_ratio": 0.15, "learning_rate": 0.004429853920677941, "optimization": "rmsprop", "nb_data_augmentation": 2, "learning_rate_decay_method": "lin", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 1.5728989498654786e-05, "rotation_range": [0, 0], "momentum": 0.6596633544268842}, "accuracy_valid_max": 0.8649755271084337, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8621782050075302, "accuracy_valid_std": [0.01549293254095553, 0.010947509144896084, 0.01660143116598744, 0.019311259021649017, 0.018248235848258958, 0.013258503821113541, 0.01233598870290613, 0.01970002447216613, 0.010378192213819066, 0.01527764460278225, 0.017767549441300698, 0.01594908221884333, 0.01726928617998849, 0.01432060524717927, 0.01878828609285291, 0.017073245082470417, 0.020190846841529896, 0.014753913972491648, 0.015811498151198266, 0.01394277656401133, 0.014940278997816999, 0.01326119998047686, 0.014912595951175599, 0.017451998658712064, 0.014582710696338826, 0.011719086298383433, 0.013707289134794768, 0.015120701761936063, 0.01742936362206543, 0.015680904993504816, 0.015942535786441895, 0.016754897046066728, 0.015008253470025604, 0.02014501090053119, 0.015305109938795273, 0.017181987354362424, 0.016630658381308266, 0.012931419557561292, 0.014553879102799835, 0.01422028674635403, 0.014964380515764833, 0.013654793711553245, 0.014310925815821747, 0.0176963394745474, 0.014914024311961276, 0.017455733687210612, 0.016338059291403608, 0.013071237948143733, 0.0158147923305228, 0.016992184638386473, 0.017489414340950468, 0.012508315699374838, 0.015090074562258604, 0.0152335351019655, 0.016724576322157642, 0.01793577153068323, 0.014553464142288326, 0.014045006739269093, 0.016647980023927367, 0.014295450997658356, 0.015203678764985276, 0.01457850941774168, 0.013003767885847308, 0.014389923840112286, 0.015809553603294182, 0.013567353126568042, 0.012283693227781176, 0.014957201593724717, 0.014512931528792999, 0.015780402138197334, 0.015349507821407449, 0.013067073791796668, 0.015169990455033321, 0.018198489719998948, 0.017028191873875803, 0.013845615403707138, 0.01254608139352981, 0.015651961402650755, 0.013885185122058682, 0.014199311232718188, 0.014951734433768595, 0.014167406903034472, 0.013973164909644988, 0.013398600988256093, 0.015419758636012084, 0.015427155139168106, 0.012199776817225472, 0.015052895880470794, 0.014240952501218982, 0.0172068114442202, 0.014285537093329538, 0.013763976201001107, 0.013824406793118518, 0.013134751739151327, 0.01551629580416028, 0.013707354780247072, 0.010906082958752467, 0.01611631363363803, 0.015403460290147575, 0.014680565700325674, 0.016687670570830804, 0.01648954145155817, 0.014728133728112824, 0.015149644822199276, 0.018378802364183402, 0.01429192618762546, 0.01779115556643691, 0.017527835660466413, 0.019220534035026625, 0.017837703328623514, 0.014562000182580505, 0.017424875230242803, 0.015013004530628106, 0.013652024052129312, 0.015060200461367643, 0.014037590858602672, 0.01786482940105441, 0.017431525859201675, 0.0163448469038622, 0.01897709328997838, 0.015861974496325135, 0.01808610299866413, 0.015549323497175898, 0.01654241746245938, 0.01703641715975561, 0.01711274945938728, 0.01563136668352012, 0.013803215897955069, 0.015956889185036866, 0.014895105712870655, 0.014925695180042473, 0.017916352048788328, 0.01718995785471529, 0.016565277865620547, 0.0169934058721913, 0.017420627495518103, 0.014072215077818412, 0.015555216947085665, 0.015463585646873328, 0.017713149952749797, 0.018480336257084944, 0.018247345339645525, 0.017234537740182986, 0.01685038840836565, 0.016415294519439193, 0.016189358683045243, 0.014790897783006168, 0.014912407968245021, 0.01455792494224749, 0.01564759940131623, 0.014877083934891457, 0.014551940562049304, 0.018988794211392435, 0.017060490011020522, 0.017245475626356855, 0.014670408681562963, 0.01701729196200689, 0.01677211823645245, 0.01531512905880977, 0.014219329414676705, 0.01690323901812087, 0.018447400141868964, 0.015760645502935596, 0.018002853278142456, 0.01665086547228451, 0.01874060773485095, 0.01917068858081755, 0.01748555105118235, 0.017196327519654633, 0.019151293374694612, 0.017345315411862246, 0.015076393921275489, 0.017251808744539403, 0.017513572042222144, 0.018698754568938214, 0.017278927750198073, 0.018324208550202035], "accuracy_valid": [0.2082901920180723, 0.282238328313253, 0.3692994634789157, 0.5091464490775602, 0.6082513648343373, 0.6699601138930723, 0.7199398178652108, 0.7079872223268072, 0.7391254471009037, 0.7388107115963856, 0.7530326383659638, 0.7739890224962349, 0.7708960843373494, 0.7908656108810241, 0.7945997858621988, 0.7842738140060241, 0.7851488963667168, 0.8032153026167168, 0.8085363916603916, 0.8052096079631024, 0.8168268778237951, 0.8201330713478916, 0.8235510400978916, 0.8155458749058735, 0.8163385965737951, 0.8232863092996988, 0.8295942559299698, 0.836216938064759, 0.833043109939759, 0.8322900978915663, 0.8381803581513554, 0.8403776237763554, 0.837559711502259, 0.8396760871611446, 0.8381391778049698, 0.8414968467620482, 0.8433176063629518, 0.8441103280308735, 0.8378641519201807, 0.8433984963290663, 0.8455045769013554, 0.8478239128388554, 0.8450265907379518, 0.8475900673004518, 0.846470844314759, 0.846400249435241, 0.8507741905120482, 0.8523405144013554, 0.849818218185241, 0.853236186935241, 0.8520257788968373, 0.8493799416415663, 0.8527170204254518, 0.8539480186370482, 0.8484548545745482, 0.8538568335843373, 0.8533479621611446, 0.843907367752259, 0.854701030685241, 0.8540803840361446, 0.8550569465361446, 0.8548128059111446, 0.8535715126129518, 0.8555349326995482, 0.8555246376129518, 0.856043804122741, 0.8565012001129518, 0.857630718185241, 0.8546804405120482, 0.8556878882718373, 0.858241069747741, 0.8546701454254518, 0.856410015060241, 0.8569188864834337, 0.8575395331325302, 0.8577219032379518, 0.8524728798004518, 0.8549348762236446, 0.8543039344879518, 0.8578645637236446, 0.8551687217620482, 0.8572542121611446, 0.8573659873870482, 0.8565114951995482, 0.856776225997741, 0.8600927146084337, 0.8585866905120482, 0.860316265060241, 0.854893695877259, 0.8598588690700302, 0.8584543251129518, 0.8535818076995482, 0.8603059699736446, 0.8592073371611446, 0.8620355445218373, 0.8599294639495482, 0.8595529579254518, 0.8637445288968373, 0.8628900367093373, 0.858851421310241, 0.8624326407191265, 0.8598485739834337, 0.861781108810241, 0.862025249435241, 0.8579057440700302, 0.8588205360504518, 0.8605912909450302, 0.8609677969691265, 0.8619649496423193, 0.8617002188441265, 0.8637445288968373, 0.8639077795557228, 0.8611604621611446, 0.8588205360504518, 0.8602956748870482, 0.860926616622741, 0.8610795721950302, 0.8620458396084337, 0.8609472067959337, 0.8642636954066265, 0.8631444724209337, 0.8631650625941265, 0.8602044898343373, 0.8607942512236446, 0.8610692771084337, 0.8604692206325302, 0.8649755271084337, 0.8618928840361446, 0.8619134742093373, 0.8624017554593373, 0.8600824195218373, 0.8620664297816265, 0.8633989081325302, 0.8632768378200302, 0.8585160956325302, 0.8613237128200302, 0.8567556358245482, 0.8632665427334337, 0.8632768378200302, 0.8631650625941265, 0.8638974844691265, 0.8617002188441265, 0.8616899237575302, 0.8616693335843373, 0.8627679663968373, 0.8632562476468373, 0.8598382788968373, 0.8621370246611446, 0.8616693335843373, 0.862269390060241, 0.8572439170745482, 0.8625032355986446, 0.8624223456325302, 0.8608148413968373, 0.8615575583584337, 0.8594514777861446, 0.8610795721950302, 0.8590852668486446, 0.860194194747741, 0.8604074501129518, 0.8633989081325302, 0.8621782050075302, 0.8636327536709337, 0.8631547675075302, 0.8629106268825302, 0.8612119375941265, 0.8647519766566265, 0.8631547675075302, 0.8612016425075302, 0.8620664297816265, 0.8618119940700302, 0.8611604621611446, 0.8625547110316265, 0.8629209219691265, 0.8626767813441265, 0.8640195547816265, 0.8621782050075302], "seed": 969166568, "model": "residualv3", "loss_std": [0.24867677688598633, 0.12720827758312225, 0.11181186139583588, 0.09643377363681793, 0.09108315408229828, 0.08758760988712311, 0.08751159906387329, 0.08243221044540405, 0.08324801921844482, 0.08135184645652771, 0.08195580542087555, 0.08136489242315292, 0.08080555498600006, 0.07742208242416382, 0.07806488871574402, 0.07973252981901169, 0.07766153663396835, 0.07533153891563416, 0.0762648954987526, 0.07658509165048599, 0.07484685629606247, 0.07598388195037842, 0.07607586681842804, 0.07370226085186005, 0.07293641567230225, 0.07332295924425125, 0.07440522313117981, 0.07085950672626495, 0.07238510251045227, 0.07105415314435959, 0.06993450969457626, 0.07027672231197357, 0.07045980542898178, 0.06853951513767242, 0.06642521917819977, 0.06785515695810318, 0.06716698408126831, 0.06714571267366409, 0.06447084248065948, 0.06611602008342743, 0.06513570994138718, 0.06370767205953598, 0.06437868624925613, 0.06343653053045273, 0.06328968703746796, 0.06412952393293381, 0.0616852305829525, 0.061953816562891006, 0.0638103112578392, 0.06105871498584747, 0.061652496457099915, 0.05959569662809372, 0.06014806404709816, 0.06053601950407028, 0.059938061982393265, 0.05902658775448799, 0.06128215417265892, 0.059202466160058975, 0.05794427916407585, 0.05885213986039162, 0.06006527319550514, 0.05954527109861374, 0.05854656919836998, 0.058184608817100525, 0.0586707703769207, 0.05726199597120285, 0.05507829412817955, 0.05645110458135605, 0.05609963461756706, 0.059043727815151215, 0.05630932375788689, 0.0551573783159256, 0.05519920960068703, 0.05627868324518204, 0.05433754622936249, 0.05380314961075783, 0.05395684391260147, 0.05414268374443054, 0.054843250662088394, 0.053341712802648544, 0.051833536475896835, 0.05421445518732071, 0.05315839871764183, 0.053387682884931564, 0.05160534381866455, 0.05324094742536545, 0.05332093685865402, 0.05011856183409691, 0.05276922881603241, 0.052504587918519974, 0.05128571763634682, 0.05058218911290169, 0.052543602883815765, 0.05130333825945854, 0.05167457461357117, 0.04912998527288437, 0.0490511916577816, 0.051339294761419296, 0.04740573838353157, 0.04876081645488739, 0.04997329041361809, 0.04868975654244423, 0.049894727766513824, 0.048362698405981064, 0.04880405217409134, 0.049458909779787064, 0.048563748598098755, 0.048215847462415695, 0.04761526361107826, 0.04735440015792847, 0.04753189533948898, 0.0468161515891552, 0.04741581156849861, 0.04613656923174858, 0.046801015734672546, 0.04539145901799202, 0.04621293023228645, 0.04446374624967575, 0.046228453516960144, 0.04541490972042084, 0.04303407669067383, 0.044197384268045425, 0.0463036447763443, 0.045775290578603745, 0.04618346318602562, 0.043137677013874054, 0.044162727892398834, 0.045203760266304016, 0.04496081918478012, 0.04382629692554474, 0.046146459877491, 0.043561432510614395, 0.0437062568962574, 0.04297395423054695, 0.04417381435632706, 0.04346349835395813, 0.04349324107170105, 0.043857425451278687, 0.04475803300738335, 0.042342908680438995, 0.04367271065711975, 0.04269631952047348, 0.04130849242210388, 0.0425238311290741, 0.04216008633375168, 0.04287683963775635, 0.042471569031476974, 0.043813448399305344, 0.04200585186481476, 0.04229601100087166, 0.04119109362363815, 0.04018905758857727, 0.04227997362613678, 0.03935001790523529, 0.04180636629462242, 0.042809490114450455, 0.0409688837826252, 0.040546905249357224, 0.04051477089524269, 0.03937861695885658, 0.04045937582850456, 0.04145796224474907, 0.040783025324344635, 0.04117979481816292, 0.041565392166376114, 0.04075520858168602, 0.03870676830410957, 0.03806694224476814, 0.039711158722639084, 0.041731756180524826, 0.0393335297703743, 0.03874431923031807, 0.0385759100317955, 0.03772231563925743, 0.03974398598074913, 0.03945639729499817, 0.03862020745873451]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:32 2016", "state": "available"}], "summary": "826f060598397392c14c92468627488a"}