{"content": {"hp_model": {"f0": 64, "f1": 16, "f2": 32, "f3": 64, "nonlin": "very_leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.6135917901992798, 1.1975626945495605, 1.0076515674591064, 0.8821535706520081, 0.7913457155227661, 0.7239439487457275, 0.67212975025177, 0.6264815926551819, 0.5891509056091309, 0.5536178946495056, 0.5272350311279297, 0.5017526745796204, 0.47755298018455505, 0.45593902468681335, 0.4368581771850586, 0.4215242266654968, 0.40312254428863525, 0.3877297341823578, 0.3712351322174072, 0.35778748989105225, 0.34764882922172546, 0.33325615525245667, 0.3229641616344452, 0.31063586473464966, 0.30015480518341064, 0.2904164493083954, 0.2819468080997467, 0.271025151014328, 0.26357364654541016, 0.25915294885635376, 0.24961870908737183, 0.2428097128868103, 0.23628248274326324, 0.22958245873451233, 0.22185295820236206, 0.21752072870731354, 0.21481332182884216, 0.2098132073879242, 0.20183712244033813, 0.19447676837444305, 0.19420039653778076, 0.1885135918855667, 0.18231822550296783, 0.18086156249046326, 0.17682963609695435, 0.17093083262443542, 0.16736218333244324, 0.16385376453399658, 0.1612151712179184, 0.15777981281280518, 0.15505637228488922, 0.15327434241771698, 0.15003587305545807, 0.1452290564775467, 0.14356519281864166, 0.13880275189876556, 0.13886284828186035, 0.1362551897764206, 0.13452978432178497, 0.1332295686006546, 0.12975631654262543, 0.12895919382572174, 0.12578821182250977, 0.12183836102485657, 0.12214260548353195, 0.11859898269176483, 0.11737901717424393, 0.1158345565199852, 0.11174444109201431, 0.11128296703100204, 0.10852119326591492, 0.10613846778869629, 0.10534830391407013, 0.10427146404981613, 0.10218270123004913, 0.10081219673156738, 0.10212758928537369, 0.09938237071037292, 0.0973726138472557, 0.09549449384212494, 0.09667593985795975, 0.09453430771827698, 0.09217220544815063, 0.09188588708639145, 0.0901738628745079, 0.08777713775634766, 0.08843963593244553, 0.0881338119506836, 0.0854247510433197, 0.08234448730945587, 0.08474721014499664, 0.08236732333898544, 0.08163883537054062, 0.08135709911584854, 0.08034219592809677, 0.07775533199310303, 0.07870464026927948, 0.0776434987783432, 0.07643745839595795, 0.056483980268239975, 0.043378423899412155, 0.03984460607171059, 0.037733130156993866, 0.03639637678861618, 0.0354008674621582, 0.03465607389807701, 0.033971913158893585, 0.03450731560587883, 0.032807037234306335, 0.032186977565288544, 0.031229641288518906, 0.03097815439105034, 0.031087983399629593, 0.03036053664982319, 0.029196633026003838, 0.029988395050168037, 0.029445281252264977, 0.028113264590501785, 0.0287737175822258, 0.02691197395324707, 0.02689419500529766, 0.02776183746755123, 0.02746502496302128, 0.027542587369680405, 0.026928970590233803, 0.02767074853181839, 0.027164282277226448, 0.027679843828082085, 0.027133511379361153, 0.027614044025540352, 0.027180179953575134, 0.02793886512517929, 0.02685081772506237, 0.02731025405228138, 0.02707318775355816, 0.026712605729699135, 0.027321090921759605, 0.027872947975993156, 0.027247672900557518, 0.026466254144906998, 0.027200955897569656, 0.02740911766886711, 0.028166744858026505, 0.0276719368994236, 0.02729705348610878, 0.026899470016360283, 0.026864388957619667, 0.027753617614507675, 0.0274560134857893, 0.027231365442276, 0.02758879028260708, 0.027149653062224388, 0.026287490501999855, 0.02672680653631687, 0.026307079941034317, 0.02760658599436283, 0.02774779312312603, 0.027772124856710434], "moving_avg_accuracy_train": [0.04046218551010519, 0.0863114794579411, 0.13283639405828948, 0.18040159258273922, 0.22306847322618178, 0.2700816838989032, 0.31328814296610036, 0.35809495658223484, 0.39327948186693307, 0.414093850811792, 0.4516332713611499, 0.4854183533185657, 0.5188864991836453, 0.5455094548219899, 0.5704583031405477, 0.5975336170448392, 0.6251720548108702, 0.6491771873408113, 0.6697152842905618, 0.6870648628774709, 0.7052412929103218, 0.7223463445612885, 0.7392031933554365, 0.7554900321617348, 0.7687184008136401, 0.7764631418911447, 0.786355580182189, 0.7920783676097065, 0.8017597622014029, 0.8116027152159895, 0.8214332130338609, 0.8311803494639557, 0.8390644572593468, 0.8469322118264169, 0.8549710440998475, 0.8597881628233991, 0.8650421476496324, 0.8724043705692391, 0.878716548205237, 0.8853485660383678, 0.890608175652462, 0.8952838037313648, 0.9000544108190073, 0.904275913633609, 0.9085983165024372, 0.9114680631963703, 0.9140486542673756, 0.916240689507397, 0.9193178587614838, 0.9216480382092557, 0.925709662065748, 0.929160510441353, 0.9305226647139858, 0.933099334773595, 0.9355508352605673, 0.937515370222652, 0.9403831749777954, 0.9421295789812617, 0.9428015460439144, 0.9436644800157965, 0.9396822259049514, 0.9430564303228082, 0.9449074244548408, 0.9465314664950987, 0.9458729651540513, 0.9477074448648828, 0.9481866016046311, 0.9501035406620527, 0.9501503528126248, 0.9519405991158955, 0.9501787868914137, 0.9513060998666318, 0.9522671310240717, 0.9544618639407213, 0.9531637826765402, 0.9553572421315236, 0.956185273570826, 0.9575789660423242, 0.9591007174285864, 0.9599775784215159, 0.9591300648461601, 0.9610594284127623, 0.9619007814774938, 0.962402160769067, 0.9620399244875184, 0.9630435725138036, 0.9615753481470208, 0.9634065956608995, 0.9645037663019893, 0.9656003576777428, 0.9669895767087873, 0.9675866512165262, 0.968388904993683, 0.9695110031836373, 0.9702257058022429, 0.9694228037459204, 0.9708762788392131, 0.9716333101065007, 0.9722076453530027, 0.9747822677224643, 0.9771715074680751, 0.9793543753224581, 0.9813398827306885, 0.9831222251492955, 0.9847681499557945, 0.9862285559423579, 0.9875522219255031, 0.9887458464591432, 0.9898201085394194, 0.9907892695604775, 0.9916661647770488, 0.992455370471963, 0.9931563550021476, 0.9938081674185996, 0.99437619740293, 0.9949013752816847, 0.9953693850749448, 0.9957929190376884, 0.9961671241577291, 0.9965294854026705, 0.9968463099278796, 0.9971361022981869, 0.9973969154314635, 0.9976246718049838, 0.9978366279875807, 0.9980250634031083, 0.9981946552770831, 0.9983472879636605, 0.9984869825303897, 0.998612707640446, 0.9987305105371156, 0.9988318828464993, 0.9989231179249446, 0.9990052294955454, 0.9990791299090861, 0.9991433151324631, 0.9992010818335025, 0.9992507467156284, 0.9993024205559703, 0.999344276714659, 0.9993796221086693, 0.9994184084097071, 0.999448665783022, 0.9994758974190056, 0.9995004058913907, 0.9995224635165373, 0.9995376650815503, 0.9995536716388714, 0.999565752391651, 0.9995812753667715, 0.9995929208955706, 0.9996057270202993, 0.999617252532555, 0.9996253003447756, 0.9996348685245837, 0.9996411547376015, 0.9996468123293175, 0.999656554459481], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.04061455784073795, 0.0856569883047816, 0.13155023855186368, 0.1778591936594032, 0.21928693557923093, 0.2646025120702536, 0.30625940718288186, 0.3485531236087955, 0.3822339675920424, 0.4013930303584406, 0.4357602985292983, 0.46715573657922993, 0.4981451233881744, 0.5226459877906973, 0.5449947349736004, 0.5696946272989061, 0.5942103337614101, 0.6157648627911425, 0.6337682193207632, 0.6490801269161719, 0.66491059549338, 0.6797623387960752, 0.6934951187060008, 0.7064935042845725, 0.7174941915067177, 0.7233734021094345, 0.7320126538130242, 0.7373606930721585, 0.7443998732001835, 0.7518784782182676, 0.7596844709932029, 0.7671310806051025, 0.7730180996153754, 0.7791617904068499, 0.7847877388605173, 0.7881340190972066, 0.7917702889212962, 0.7969432576760039, 0.8017474729475602, 0.8065096903083011, 0.8105453682560101, 0.8143005782301078, 0.8170312355332265, 0.8197940028872834, 0.8225368411621846, 0.8247866985557553, 0.8264250632446074, 0.8277814921140473, 0.829753084054751, 0.8309211362008573, 0.8339724243126992, 0.836620927363357, 0.837159112300214, 0.838730051597301, 0.8408316087493178, 0.8421542801880156, 0.8443925772256899, 0.8457346283321872, 0.8460888645765288, 0.8463344350089361, 0.8433661776432081, 0.8458575848694746, 0.8472453591856144, 0.848296984552821, 0.8471101583678702, 0.8485812243740801, 0.8490242185123499, 0.8506833259815215, 0.8509059619451164, 0.8519333239113728, 0.850949534771365, 0.8518353206579333, 0.8523597081610406, 0.8542142870437317, 0.8527175218370995, 0.8543012972324557, 0.8546899798680203, 0.8557347125852846, 0.8569577927668917, 0.8567340285033802, 0.8565407296628313, 0.8582488496088223, 0.8588278320707563, 0.8592614080504276, 0.8588683174148125, 0.8593671160856053, 0.8581689681065628, 0.8598485415519306, 0.860806723211873, 0.8615958445183212, 0.8623447338051938, 0.8625842815090118, 0.8630979907357462, 0.8640496397984667, 0.864351660005367, 0.8632339356463966, 0.8639075535651907, 0.8646694125723764, 0.8650956494966147, 0.8676746164087755, 0.8702154131922202, 0.8725499289136608, 0.8744221479951862, 0.8762515705262399, 0.8776661172104382, 0.879109078155057, 0.8803843584513735, 0.882000095940197, 0.8830971672479093, 0.8841720396609196, 0.8853103232701288, 0.8861007859073479, 0.8869383906279835, 0.8874307396770074, 0.8879745985971078, 0.8884100959569, 0.888866167262942, 0.8893509031345394, 0.8899448273165673, 0.8903694957991426, 0.8908259691296199, 0.8913212148371399, 0.8916183925815886, 0.891886882060252, 0.892226178841049, 0.8925213979298356, 0.892737237476084, 0.8928806059253883, 0.8930961162571718, 0.8932778685245268, 0.8934302680425561, 0.8935796346400324, 0.8936265563416917, 0.8937298210294352, 0.8939112969931332, 0.8940115311868921, 0.8941526291035945, 0.8943284453536267, 0.8943625506488363, 0.8944431030481845, 0.8945044226850076, 0.8946074089744888, 0.8946014108763622, 0.8946702842842079, 0.8946579986551094, 0.8946214980177611, 0.8946385050778073, 0.8946538114318489, 0.8946442025966459, 0.8946477616762132, 0.8946753789103238, 0.894823334242183, 0.8948089801571966, 0.8948581261456184, 0.8948402928702885, 0.894786592320082, 0.8948491546148057, 0.8949186972199668], "moving_var_accuracy_train": [0.014734696106287502, 0.032180646295294324, 0.048443690772892276, 0.0639613546916359, 0.07394938355704828, 0.08644662300116264, 0.09460314364717483, 0.10321168420033651, 0.104032073155889, 0.09752800743145447, 0.10045807954494301, 0.10068515745627246, 0.1006976927994614, 0.09700695942180664, 0.09290826877142759, 0.0902150955014281, 0.08806853513060624, 0.08444789910756591, 0.07979943003366549, 0.07452855792458897, 0.0700491456107822, 0.06567747617754413, 0.06166710872120864, 0.05788774791380889, 0.05367388075714437, 0.04884632181064821, 0.04484243264766245, 0.04065294204636118, 0.03743121245288622, 0.034560044724023825, 0.031973788437746, 0.029631469611253354, 0.027227755051692133, 0.025062093603871797, 0.02313748966236777, 0.021032582391302117, 0.01917776336116052, 0.01774780796190631, 0.0163316194442912, 0.015094310444712763, 0.013833850839875537, 0.012647219237278013, 0.011587325541412188, 0.010588982761394179, 0.009698232984298841, 0.008802528700655017, 0.007982210883071284, 0.007227234961205617, 0.0065897322006497295, 0.005979626606914123, 0.005530135041387358, 0.005084296727851362, 0.004592566233428294, 0.004193062667450242, 0.0038278450924438497, 0.0034797951617547415, 0.0032058343826018776, 0.0029127002868315982, 0.0026254941157480495, 0.002369646599531698, 0.00227540706980861, 0.0021503336619090986, 0.0019661359092095604, 0.0017932599312253299, 0.0016178365542482472, 0.001486340741108494, 0.0013397729876288612, 0.0012388675870147951, 0.0011150005507102865, 0.001032345332076623, 0.0009570466396979606, 0.0008727794866250221, 0.0007938137659326511, 0.0007577840625182166, 0.0006971707909821552, 0.0006707550913098478, 0.0006098503067591222, 0.0005663466844292031, 0.0005305535615206018, 0.00048441817217683314, 0.0004424408683028601, 0.00043169877542176006, 0.00039489977269538233, 0.0003576722261720106, 0.00032308593966784123, 0.0002998431299470504, 0.0002892599620732758, 0.0002905151729797263, 0.00027229770642277885, 0.0002558905495888926, 0.0002476708602759487, 0.0002261122559584781, 0.00020929353046929302, 0.00019969611655345297, 0.00018432370339548359, 0.0001716931984643571, 0.00017353718723932377, 0.00016134133557225137, 0.00014817595079339707, 0.0001930164788220475, 0.00022509102999789573, 0.0002454661356253926, 0.0002563996790760927, 0.0002593504116429775, 0.00025779698669651927, 0.00025121235883717954, 0.0002418599476678824, 0.00023049660864686363, 0.00021783329893625055, 0.00020450342680527017, 0.0001909735911123537, 0.00017748184266108287, 0.00016415607219899843, 0.00015156419981526636, 0.00013931170240162654, 0.00012786283840046265, 0.00011704785305970268, 0.0001069574969121082, 9.752201246767932e-05, 8.895156226743058e-05, 8.09598060586535e-05, 7.36196420137829e-05, 6.686988922681035e-05, 6.064975699524144e-05, 5.498911010578657e-05, 4.98097702476336e-05, 4.508764585633507e-05, 4.078855190380809e-05, 3.688532786119017e-05, 3.333905630475909e-05, 3.013004837645712e-05, 2.7209530644799423e-05, 2.456349213616977e-05, 2.2167823712791525e-05, 2.0000192781605744e-05, 1.8037251189544886e-05, 1.6263558996331174e-05, 1.4659402501347305e-05, 1.3217493923193723e-05, 1.19115119730559e-05, 1.0731604447649992e-05, 9.671983397218777e-06, 8.713024635256211e-06, 7.848396229715633e-06, 7.068962593711956e-06, 6.36644518378475e-06, 5.731880453615856e-06, 5.160998297149751e-06, 4.646211968724254e-06, 4.183759436661185e-06, 3.7666040580641477e-06, 3.391419623732824e-06, 3.053473198254367e-06, 2.748708783962786e-06, 2.4746618561500776e-06, 2.227551318802018e-06, 2.0050842620180447e-06, 1.8054300177173404e-06], "duration": 59476.435951, "accuracy_train": [0.40462185510105203, 0.4989551249884644, 0.5515606254614249, 0.608488379302787, 0.607070399017165, 0.6932005799533961, 0.7021462745708749, 0.7613562791274455, 0.7099402094292174, 0.6014231713155224, 0.789488056305371, 0.7894840909353081, 0.8200998119693614, 0.7851160555670912, 0.7949979380075674, 0.8412114421834626, 0.8739179947051495, 0.8652233801102805, 0.8545581568383168, 0.843211070159653, 0.8688291632059801, 0.876291809419989, 0.8909148325027685, 0.9020715814184201, 0.8877737186807864, 0.8461658115886858, 0.8753875248015872, 0.8435834544573644, 0.8888923135266703, 0.9001892923472684, 0.9099076933947029, 0.9189045773348099, 0.9100214274178663, 0.917742002930048, 0.9273205345607235, 0.9031422313353636, 0.9123280110857327, 0.9386643768456996, 0.9355261469292175, 0.9450367265365448, 0.9379446621793098, 0.9373644564414912, 0.9429898746077889, 0.9422694389650241, 0.94749994232189, 0.9372957834417681, 0.9372739739064231, 0.9359690066675894, 0.9470123820482651, 0.9426196532392026, 0.9622642767741787, 0.9602181458217978, 0.9427820531676817, 0.9562893653100776, 0.9576143396433187, 0.9551961848814139, 0.9661934177740864, 0.9578472150124585, 0.9488492496077889, 0.9514308857627353, 0.9038419389073459, 0.973424270083518, 0.9615663716431341, 0.9611478448574198, 0.9399464530846253, 0.9642177622623662, 0.9524990122623662, 0.9673559921788483, 0.950571662167774, 0.9680528158453304, 0.934322476871078, 0.9614519166435955, 0.96091641144103, 0.9742144601905685, 0.941481051298911, 0.975098377226375, 0.9636375565245479, 0.9701221982858066, 0.9727964799049464, 0.9678693273578812, 0.9515024426679586, 0.9784237005121816, 0.9694729590600776, 0.9669145743932264, 0.9587797979535806, 0.9720764047503692, 0.9483613288459765, 0.9798878232858066, 0.9743783020717978, 0.9754696800595238, 0.9794925479881875, 0.9729603217861758, 0.9756091889880952, 0.9796098868932264, 0.9766580293696937, 0.962196685239018, 0.9839575546788483, 0.9784465915120893, 0.9773766625715209, 0.9979538690476191, 0.9986746651785714, 0.9990001860119048, 0.9992094494047619, 0.9991633069167589, 0.9995814732142857, 0.9993722098214286, 0.9994652157738095, 0.9994884672619048, 0.9994884672619048, 0.99951171875, 0.9995582217261905, 0.9995582217261905, 0.9994652157738095, 0.9996744791666666, 0.9994884672619048, 0.9996279761904762, 0.9995814732142857, 0.9996047247023809, 0.9995349702380952, 0.9997907366071429, 0.9996977306547619, 0.9997442336309523, 0.9997442336309523, 0.9996744791666666, 0.9997442336309523, 0.9997209821428571, 0.9997209821428571, 0.9997209821428571, 0.9997442336309523, 0.9997442336309523, 0.9997907366071429, 0.9997442336309523, 0.9997442336309523, 0.9997442336309523, 0.9997442336309523, 0.9997209821428571, 0.9997209821428571, 0.9996977306547619, 0.9997674851190477, 0.9997209821428571, 0.9996977306547619, 0.9997674851190477, 0.9997209821428571, 0.9997209821428571, 0.9997209821428571, 0.9997209821428571, 0.9996744791666666, 0.9996977306547619, 0.9996744791666666, 0.9997209821428571, 0.9996977306547619, 0.9997209821428571, 0.9997209821428571, 0.9996977306547619, 0.9997209821428571, 0.9996977306547619, 0.9996977306547619, 0.9997442336309523], "end": "2016-02-01 08:16:28.986000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0], "moving_var_accuracy_valid": [0.014845880777387835, 0.031620677578622934, 0.04741432358493233, 0.06197356513480811, 0.07122252882650998, 0.08258178919828253, 0.0899412824722749, 0.09704598026708776, 0.09755097550319339, 0.09109950512765512, 0.09261953670863847, 0.09222864481089847, 0.09164885918295806, 0.0878866044728996, 0.08359314253141754, 0.08072459040621102, 0.07806131013579086, 0.07443655861745399, 0.06990999037270232, 0.06502908196332507, 0.06078160738535833, 0.05668861515898485, 0.05271705683957648, 0.04896597340446179, 0.045158512138250825, 0.04095374698022562, 0.03753010231218486, 0.034034505796221554, 0.031077005728472454, 0.02847267095277382, 0.02617380556631752, 0.024055492962094086, 0.021961856601330508, 0.020105375370068696, 0.01837969949709173, 0.01664250787018467, 0.015097259207268465, 0.013828369738176262, 0.012653257123737723, 0.011592039839082434, 0.010579416123652818, 0.009648388928833608, 0.008750658439713928, 0.007944288546816317, 0.007217568148355051, 0.006541368058142189, 0.00591138940201107, 0.005336809555310612, 0.004838113172805381, 0.004366580967869053, 0.004013716103355362, 0.0036754756087039185, 0.003310534835069865, 0.0030016920040390692, 0.0027412716858039024, 0.002482889654836233, 0.002279690452012365, 0.0020679313173631838, 0.001862267535478112, 0.0016765835254657555, 0.0015882201390219634, 0.0014852621148236055, 0.001354069161314083, 0.0012286154883992472, 0.0011184309470988858, 0.001026064169140636, 0.0009252239464854444, 0.00085747529018525, 0.0007721738621172972, 0.0007044557293929601, 0.000642720726101639, 0.0005855102032230677, 0.0005294340231814967, 0.0005074457863524618, 0.0004768639624712782, 0.0004517526667505752, 0.00040793706779622315, 0.00037696655907130174, 0.0003527332293399317, 0.0003179105404165619, 0.0002864557663507235, 0.0002840692534646806, 0.0002586793143392574, 0.00023450327607666346, 0.00021244363069927125, 0.0001934384686552065, 0.00018701464900683766, 0.00019370188673161628, 0.00018259470689950584, 0.0001699396481361705, 0.00015799319979848778, 0.00014271032774027914, 0.00013081436949293932, 0.00012588365599083774, 0.00011411623624013893, 0.00011394838229984777, 0.00010663739397454705, 0.00010119751689856288, 9.271286644896346e-05, 0.00014330121281024973, 0.00018707192618209282, 0.0002174144064467649, 0.00022721980440514196, 0.00023461890513876876, 0.00022916549552087935, 0.00022498817255804887, 0.00021712641380980388, 0.00021890924112393016, 0.0002078504060993858, 0.00019746352182770197, 0.00018937837581988167, 0.00017606401886544713, 0.0001647718519911833, 0.00015047633506673797, 0.00013809074428481984, 0.00012598859140981197, 0.00011526174159458386, 0.00010585028722204488, 9.8439971905819e-05, 9.021906459607156e-05, 8.307246924939788e-05, 7.69726371218104e-05, 7.007020491578968e-05, 6.371196382558733e-05, 5.837686819216199e-05, 5.33235701664021e-05, 4.841049353728418e-05, 4.3754434793858904e-05, 3.979699364242174e-05, 3.611459925837807e-05, 3.271216985040017e-05, 2.9641746289335027e-05, 2.669738647518106e-05, 2.4123620189275775e-05, 2.2007659898949437e-05, 1.9897315951441082e-05, 1.8086761955176687e-05, 1.655628794363724e-05, 1.4911127689725531e-05, 1.3478413122119894e-05, 1.2164412690648982e-05, 1.1043427003973976e-05, 9.939408098206804e-06, 8.988159205160652e-06, 8.090701714785701e-06, 7.293622212048615e-06, 6.566863151666481e-06, 5.912285396766241e-06, 5.3218878245152354e-06, 4.789813045490012e-06, 4.317696145520294e-06, 4.0829435529983485e-06, 3.6765035555006763e-06, 3.3305911535523154e-06, 3.0003942695780174e-06, 2.72630858445261e-06, 2.4889040924972393e-06, 2.2835392486407223e-06], "accuracy_test": 0.884245455994898, "start": "2016-01-31 15:45:12.550000", "learning_rate_per_epoch": [0.0006469535292126238, 0.0006469535292126238, 0.0006469535292126238, 0.0006469535292126238, 0.0006469535292126238, 0.0006469535292126238, 0.0006469535292126238, 0.0006469535292126238, 0.0006469535292126238, 0.0006469535292126238, 0.0006469535292126238, 0.0006469535292126238, 0.0006469535292126238, 0.0006469535292126238, 0.0006469535292126238, 0.0006469535292126238, 0.0006469535292126238, 0.0006469535292126238, 0.0006469535292126238, 0.0006469535292126238, 0.0006469535292126238, 0.0006469535292126238, 0.0006469535292126238, 0.0006469535292126238, 0.0006469535292126238, 0.0006469535292126238, 0.0006469535292126238, 0.0006469535292126238, 0.0006469535292126238, 0.0006469535292126238, 0.0006469535292126238, 0.0006469535292126238, 0.0006469535292126238, 0.0006469535292126238, 0.0006469535292126238, 0.0006469535292126238, 0.0006469535292126238, 0.0006469535292126238, 0.0006469535292126238, 0.0006469535292126238, 0.0006469535292126238, 0.0006469535292126238, 0.0006469535292126238, 0.0006469535292126238, 0.0006469535292126238, 0.0006469535292126238, 0.0006469535292126238, 0.0006469535292126238, 0.0006469535292126238, 0.0006469535292126238, 0.0006469535292126238, 0.0006469535292126238, 0.0006469535292126238, 0.0006469535292126238, 0.0006469535292126238, 0.0006469535292126238, 0.0006469535292126238, 0.0006469535292126238, 0.0006469535292126238, 0.0006469535292126238, 0.0006469535292126238, 0.0006469535292126238, 0.0006469535292126238, 0.0006469535292126238, 0.0006469535292126238, 0.0006469535292126238, 0.0006469535292126238, 0.0006469535292126238, 0.0006469535292126238, 0.0006469535292126238, 0.0006469535292126238, 0.0006469535292126238, 0.0006469535292126238, 0.0006469535292126238, 0.0006469535292126238, 0.0006469535292126238, 0.0006469535292126238, 0.0006469535292126238, 0.0006469535292126238, 0.0006469535292126238, 0.0006469535292126238, 0.0006469535292126238, 0.0006469535292126238, 0.0006469535292126238, 0.0006469535292126238, 0.0006469535292126238, 0.0006469535292126238, 0.0006469535292126238, 0.0006469535292126238, 0.0006469535292126238, 0.0006469535292126238, 0.0006469535292126238, 0.0006469535292126238, 0.0006469535292126238, 0.0006469535292126238, 0.0006469535292126238, 0.0006469535292126238, 0.0006469535292126238, 6.469535583164543e-05, 6.469535583164543e-05, 6.469535583164543e-05, 6.469535583164543e-05, 6.469535583164543e-05, 6.469535583164543e-05, 6.469535583164543e-05, 6.469535583164543e-05, 6.469535583164543e-05, 6.469535583164543e-05, 6.469535583164543e-05, 6.469535583164543e-05, 6.469535583164543e-05, 6.469535583164543e-05, 6.469535583164543e-05, 6.469535583164543e-05, 6.469535583164543e-05, 6.469535583164543e-05, 6.469535583164543e-05, 6.469535583164543e-05, 6.469535492215073e-06, 6.469535378528235e-07, 6.469535662745329e-08, 6.469535573927487e-09, 6.469535462905185e-10, 6.469535324127307e-11, 6.469535410863481e-12, 6.469535194023046e-13, 6.469535329548318e-14, 6.469535160141728e-15, 6.469534948383491e-16, 6.46953508073239e-17, 6.4695350807323894e-18, 6.469535184129966e-19, 6.469535313376937e-20, 6.46953547493565e-21, 6.469535373961454e-22, 6.469535373961454e-23, 6.469535452847545e-24, 6.469535551455158e-25, 6.469535797974191e-26, 6.469536106122982e-27, 6.469536202419479e-28, 6.469536322790101e-29, 6.469536473253378e-30, 6.469536285174281e-31, 6.469536520273152e-32, 6.469536667209945e-33, 6.469536575374449e-34, 6.469536345785709e-35, 6.469536632771634e-36, 6.469536722454736e-37, 6.469536610350859e-38, 6.469537170870245e-39, 6.469542776064102e-40, 6.469514750094815e-41, 6.46979500978768e-42, 6.473998905180655e-43, 6.445972935894159e-44, 7.006492321624085e-45], "accuracy_train_first": 0.40462185510105203, "accuracy_train_last": 0.9997442336309523, "batch_size_eval": 1024, "accuracy_train_std": [0.013804598963719767, 0.01392523575136849, 0.013668442840656787, 0.014672212122641895, 0.01667518755372138, 0.014572438673929591, 0.016398345087872475, 0.016158066211932814, 0.014071969630328694, 0.012534667234451242, 0.015579698347230574, 0.017207953689809874, 0.01812565472508052, 0.018431247574644538, 0.014887010213816308, 0.01555626582551122, 0.016978145772225617, 0.017804704324938123, 0.014835701901825147, 0.018753480204302415, 0.014418626270314677, 0.01544987228154333, 0.01364767945250991, 0.016786105294413087, 0.017374612574535327, 0.01325694655419321, 0.015865710257548658, 0.017447527845606107, 0.014096331748859563, 0.015689460445634053, 0.014209877963130136, 0.013141148104948965, 0.016112819560066435, 0.014543894442385137, 0.013691663443807966, 0.014779792289590067, 0.013497786318071645, 0.01333954959647397, 0.012811290296493945, 0.013275226173980179, 0.013400303028799574, 0.01261416080579281, 0.013066862004711098, 0.012465288902371174, 0.011684218819366501, 0.01224634512829677, 0.013137834523943888, 0.014071636041392973, 0.012338732953282208, 0.011120841234333174, 0.010484868826877134, 0.01160208950308542, 0.010822873151821961, 0.010909690258133448, 0.010441310193260698, 0.011442257622917075, 0.010734331025486758, 0.011793728921928348, 0.011404763051004137, 0.01069026955634687, 0.011812868714436119, 0.008743100739149573, 0.009519245958218755, 0.01033520377520334, 0.011170246287789372, 0.010239812139633028, 0.012732496076921692, 0.0090523048532967, 0.009598934647728498, 0.010013053235889765, 0.009920302657156006, 0.00954731186754975, 0.010339577063516155, 0.008618018846932576, 0.0128964472759419, 0.008962808978314155, 0.008404629886316566, 0.009469330890870641, 0.009235793679893491, 0.008980516818380259, 0.009822177455869616, 0.007281650838405444, 0.008558312351829515, 0.009327527370952035, 0.00893466006633474, 0.009054609662986934, 0.011168701191263807, 0.007271630213385082, 0.008757856424802896, 0.007934210459881274, 0.006716290201187316, 0.006908174815153888, 0.006991896834893459, 0.006526683118558801, 0.006714922580276326, 0.008176616678622772, 0.006108411217026997, 0.007204514359336922, 0.007865349024627603, 0.0017022884317078803, 0.0012021266698949557, 0.0011374232522912025, 0.001049154963976793, 0.0010342395419830654, 0.0007113587491061363, 0.0008740209323480516, 0.0008031024943810707, 0.0006477115084256009, 0.000775359561036694, 0.0006822637998103241, 0.0006806771376629673, 0.0006464582763419756, 0.0007444108348950019, 0.0006625654226550877, 0.000775359561036694, 0.0006722857279948363, 0.0007113587491061362, 0.0006758948034667333, 0.0007143922756574227, 0.0004007088899649852, 0.0004992306211305778, 0.0004793417068472913, 0.0004793417068472913, 0.0005072875797356638, 0.0004293662879608303, 0.000489939255620012, 0.000489939255620012, 0.000489939255620012, 0.0004293662879608303, 0.0004793417068472913, 0.00045385094159093966, 0.0004293662879608303, 0.0004793417068472913, 0.0004293662879608303, 0.0004293662879608303, 0.000489939255620012, 0.000489939255620012, 0.0004992306211305778, 0.00041593526367183587, 0.000489939255620012, 0.0004992306211305778, 0.00041593526367183587, 0.000489939255620012, 0.000489939255620012, 0.000489939255620012, 0.000489939255620012, 0.000590056619254535, 0.0004992306211305778, 0.000590056619254535, 0.000489939255620012, 0.0004992306211305778, 0.00044116596821545473, 0.000489939255620012, 0.0004992306211305778, 0.000489939255620012, 0.0005831443547239794, 0.0005831443547239794, 0.0004293662879608303], "accuracy_test_std": 0.006893943780144945, "error_valid": [0.5938544215926205, 0.5089611375188253, 0.45541050922439763, 0.40536021037274095, 0.4078633871423193, 0.32755729951054224, 0.3188285368034638, 0.2708034285579819, 0.3146384365587349, 0.42617540474397586, 0.2549342879329819, 0.25028532097138556, 0.22295039533132532, 0.25684623258659633, 0.2538665403802711, 0.20800634177334332, 0.1851483080760542, 0.1902443759412651, 0.20420157191265065, 0.21311270472515065, 0.19261518731174698, 0.18657197147966864, 0.18290986210466864, 0.1765210255082832, 0.18349962349397586, 0.22371370246611444, 0.19023408085466864, 0.21450695359563254, 0.1922475056475903, 0.18081407661897586, 0.17006159403237953, 0.16584943288780118, 0.17399872929216864, 0.16554499246987953, 0.16457872505647586, 0.1817494587725903, 0.17550328266189763, 0.1565000235316265, 0.15501458960843373, 0.15063035344503017, 0.1531335302146084, 0.15190253200301207, 0.15839284873870485, 0.15534109092620485, 0.15277761436370485, 0.1549645849021084, 0.15882965455572284, 0.16001064806099397, 0.15250258847891573, 0.15856639448418675, 0.13856598268072284, 0.13954254518072284, 0.1579972232680723, 0.14713149472891573, 0.14025437688253017, 0.14594167686370485, 0.13546274943524095, 0.14218691170933728, 0.15072300922439763, 0.15145543109939763, 0.18334813864834332, 0.1317197500941265, 0.1402646719691265, 0.1422383871423193, 0.16357127729668675, 0.13817918157003017, 0.14698883424322284, 0.13438470679593373, 0.14709031438253017, 0.1388204183923193, 0.15790456748870485, 0.14019260636295183, 0.14292080431099397, 0.12909450301204817, 0.1607533650225903, 0.13144472420933728, 0.14181187641189763, 0.13486269295933728, 0.1320344855986446, 0.14527984986822284, 0.1451989599021084, 0.12637807087725905, 0.13596132577183728, 0.13683640813253017, 0.14466949830572284, 0.13614369587725905, 0.1526143637048193, 0.12503529743975905, 0.1305696418486446, 0.1313020637236446, 0.13091526261295183, 0.1352597891566265, 0.1322786262236446, 0.12738551863704817, 0.13293015813253017, 0.14682558358433728, 0.13002988516566272, 0.12847385636295183, 0.13106821818524095, 0.10911468138177716, 0.10691741575677716, 0.10643942959337349, 0.10872788027108427, 0.10728362669427716, 0.10960296263177716, 0.10790427334337349, 0.10813811888177716, 0.1034582666603916, 0.10702919098268071, 0.10615410862198793, 0.10444512424698793, 0.10678505035768071, 0.10552316688629515, 0.10813811888177716, 0.10713067112198793, 0.10767042780496983, 0.10702919098268071, 0.10628647402108427, 0.10470985504518071, 0.10580848785768071, 0.10506577089608427, 0.10422157379518071, 0.10570700771837349, 0.10569671263177716, 0.10472015013177716, 0.10482163027108427, 0.10532020660768071, 0.10582907803087349, 0.10496429075677716, 0.10508636106927716, 0.10519813629518071, 0.10507606598268071, 0.10595114834337349, 0.10534079678087349, 0.10445541933358427, 0.10508636106927716, 0.10457748964608427, 0.10408920839608427, 0.10533050169427716, 0.10483192535768071, 0.10494370058358427, 0.10446571442018071, 0.10545257200677716, 0.10470985504518071, 0.10545257200677716, 0.10570700771837349, 0.10520843138177716, 0.10520843138177716, 0.10544227692018071, 0.10532020660768071, 0.10507606598268071, 0.10384506777108427, 0.10532020660768071, 0.10469955995858427, 0.10532020660768071, 0.10569671263177716, 0.10458778473268071, 0.10445541933358427], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.07214150382314398, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 128, "valid_ratio": 0.15, "learning_rate": 0.0006469535284925969, "optimization": "rmsprop", "nb_data_augmentation": 2, "learning_rate_decay_method": "discrete", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 1.0001270024681018e-06, "rotation_range": [0, 0], "momentum": 0.954237320231532}, "accuracy_valid_max": 0.8965417333396084, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8955445806664157, "accuracy_valid_std": [0.018743341599289573, 0.019401183827024087, 0.01842786649079941, 0.015380168224822837, 0.013159197459686193, 0.01776282331918434, 0.018600872052152197, 0.012091171403308044, 0.013063612113049, 0.013640385182837994, 0.011116252842124277, 0.011142698428775417, 0.013021453356847474, 0.009822414982770516, 0.018192752429258163, 0.004831956209984116, 0.013359357465746543, 0.01324117319500769, 0.007401150132691227, 0.005688279240323652, 0.011697845485732465, 0.010568268744967062, 0.00941377460708618, 0.008228772159490238, 0.009356363050524438, 0.012680930266341154, 0.012282129712229483, 0.006568807116136584, 0.009758064664812934, 0.010481628985705335, 0.011994762520545928, 0.007066586613725861, 0.012955668433120645, 0.009104630031885108, 0.011638862716625077, 0.0044809013079861095, 0.005429680411837368, 0.00889906013684128, 0.011333747499670963, 0.006460920615911222, 0.009477982318886002, 0.007385808281882178, 0.005252587163659675, 0.009213052042221666, 0.004268098170010816, 0.007550063209706835, 0.011445009629353258, 0.005759161086613403, 0.00845362423981909, 0.010107125392667865, 0.009260038878507317, 0.01195563387582727, 0.01582886521018109, 0.007841398161986623, 0.009666098954687626, 0.010723146924950336, 0.007510358662943397, 0.008376731768176205, 0.010662098805324177, 0.011086433920750914, 0.012083097014195177, 0.011617308402269292, 0.00943216518522354, 0.00933849017424097, 0.010411398137580205, 0.007946753950307372, 0.01133689750546868, 0.011702633186775956, 0.011831543208713038, 0.014152016261228011, 0.006861986148896534, 0.009057316527256101, 0.013734883013054526, 0.010333487506751912, 0.009903613839409301, 0.01032411521368712, 0.012882612442564929, 0.007922754674027234, 0.006013028299502026, 0.006992076291065352, 0.009053485099739333, 0.006352597923981522, 0.006762643458487628, 0.00814510889351843, 0.008580787015627114, 0.010690702152911645, 0.010789608404711682, 0.007899348311813596, 0.009368018421261476, 0.007927938503008469, 0.009498658707491024, 0.00736413030915626, 0.007192745205604649, 0.007878151578282791, 0.005170188062382153, 0.007197564961214268, 0.009084566056339047, 0.007775388415359997, 0.008900666155674435, 0.006359893977574035, 0.007317771737664652, 0.00845797292751675, 0.007237517455656205, 0.008528741218795158, 0.006600264521654379, 0.007111519258194843, 0.006108781567108838, 0.008502335675486052, 0.006525668204627512, 0.009105243431125176, 0.006025991945891595, 0.005747580016347615, 0.009625670243991254, 0.005535475771765691, 0.008640181021687718, 0.005208331095310675, 0.006012241169575974, 0.0068612045158024685, 0.005413300310523763, 0.00650240689247123, 0.007202356714851186, 0.005344409405874607, 0.005343569932221901, 0.005217723431234112, 0.005632316855856542, 0.0059502190786303415, 0.006600510962289583, 0.005392283052551234, 0.005422758654898708, 0.005288180074359448, 0.0055676389001657264, 0.00629443501035825, 0.0050983931202100725, 0.005084517742614112, 0.006613450589825146, 0.00544369162908437, 0.006921280898812799, 0.005885235061373409, 0.005876160211813377, 0.005696878831261139, 0.006488813840977635, 0.005373421761781168, 0.005847262764133045, 0.005938373912903745, 0.005489750995089873, 0.005915322582605969, 0.005821044510926411, 0.005329252469047659, 0.006143846847925433, 0.00619043909775959, 0.005677053474072199, 0.0058634136765419904, 0.00516130079021694, 0.006805782955373909, 0.0058742522041061165, 0.005240520621460337, 0.005850610348743753, 0.006430672042099717], "accuracy_valid": [0.4061455784073795, 0.4910388624811747, 0.5445894907756024, 0.594639789627259, 0.5921366128576807, 0.6724427004894578, 0.6811714631965362, 0.7291965714420181, 0.6853615634412651, 0.5738245952560241, 0.7450657120670181, 0.7497146790286144, 0.7770496046686747, 0.7431537674134037, 0.7461334596197289, 0.7919936582266567, 0.8148516919239458, 0.8097556240587349, 0.7957984280873494, 0.7868872952748494, 0.807384812688253, 0.8134280285203314, 0.8170901378953314, 0.8234789744917168, 0.8165003765060241, 0.7762862975338856, 0.8097659191453314, 0.7854930464043675, 0.8077524943524097, 0.8191859233810241, 0.8299384059676205, 0.8341505671121988, 0.8260012707078314, 0.8344550075301205, 0.8354212749435241, 0.8182505412274097, 0.8244967173381024, 0.8434999764683735, 0.8449854103915663, 0.8493696465549698, 0.8468664697853916, 0.8480974679969879, 0.8416071512612951, 0.8446589090737951, 0.8472223856362951, 0.8450354150978916, 0.8411703454442772, 0.839989351939006, 0.8474974115210843, 0.8414336055158133, 0.8614340173192772, 0.8604574548192772, 0.8420027767319277, 0.8528685052710843, 0.8597456231174698, 0.8540583231362951, 0.864537250564759, 0.8578130882906627, 0.8492769907756024, 0.8485445689006024, 0.8166518613516567, 0.8682802499058735, 0.8597353280308735, 0.8577616128576807, 0.8364287227033133, 0.8618208184299698, 0.8530111657567772, 0.8656152932040663, 0.8529096856174698, 0.8611795816076807, 0.8420954325112951, 0.8598073936370482, 0.857079195689006, 0.8709054969879518, 0.8392466349774097, 0.8685552757906627, 0.8581881235881024, 0.8651373070406627, 0.8679655144013554, 0.8547201501317772, 0.8548010400978916, 0.873621929122741, 0.8640386742281627, 0.8631635918674698, 0.8553305016942772, 0.863856304122741, 0.8473856362951807, 0.874964702560241, 0.8694303581513554, 0.8686979362763554, 0.8690847373870482, 0.8647402108433735, 0.8677213737763554, 0.8726144813629518, 0.8670698418674698, 0.8531744164156627, 0.8699701148343373, 0.8715261436370482, 0.868931781814759, 0.8908853186182228, 0.8930825842432228, 0.8935605704066265, 0.8912721197289157, 0.8927163733057228, 0.8903970373682228, 0.8920957266566265, 0.8918618811182228, 0.8965417333396084, 0.8929708090173193, 0.8938458913780121, 0.8955548757530121, 0.8932149496423193, 0.8944768331137049, 0.8918618811182228, 0.8928693288780121, 0.8923295721950302, 0.8929708090173193, 0.8937135259789157, 0.8952901449548193, 0.8941915121423193, 0.8949342291039157, 0.8957784262048193, 0.8942929922816265, 0.8943032873682228, 0.8952798498682228, 0.8951783697289157, 0.8946797933923193, 0.8941709219691265, 0.8950357092432228, 0.8949136389307228, 0.8948018637048193, 0.8949239340173193, 0.8940488516566265, 0.8946592032191265, 0.8955445806664157, 0.8949136389307228, 0.8954225103539157, 0.8959107916039157, 0.8946694983057228, 0.8951680746423193, 0.8950562994164157, 0.8955342855798193, 0.8945474279932228, 0.8952901449548193, 0.8945474279932228, 0.8942929922816265, 0.8947915686182228, 0.8947915686182228, 0.8945577230798193, 0.8946797933923193, 0.8949239340173193, 0.8961549322289157, 0.8946797933923193, 0.8953004400414157, 0.8946797933923193, 0.8943032873682228, 0.8954122152673193, 0.8955445806664157], "seed": 611439353, "model": "residualv3", "loss_std": [0.33066821098327637, 0.11752025783061981, 0.10163990408182144, 0.10093153268098831, 0.08937904983758926, 0.08864687383174896, 0.08922788500785828, 0.08505379408597946, 0.08002980798482895, 0.08184986561536789, 0.0765681266784668, 0.07506486028432846, 0.07515723258256912, 0.07219277322292328, 0.06811228394508362, 0.07253707200288773, 0.06819981336593628, 0.06767039000988007, 0.06533573567867279, 0.06334500759840012, 0.061084821820259094, 0.06339921802282333, 0.059976279735565186, 0.05929548665881157, 0.05624580383300781, 0.054354261606931686, 0.058425188064575195, 0.05051162466406822, 0.05356111377477646, 0.04952695593237877, 0.049920402467250824, 0.048234064131975174, 0.05082150548696518, 0.047800078988075256, 0.045142777264118195, 0.0447094663977623, 0.04496591165661812, 0.04366298019886017, 0.04123738408088684, 0.04147283732891083, 0.04109818860888481, 0.04126749187707901, 0.03967531397938728, 0.040235456079244614, 0.04044664651155472, 0.03862234205007553, 0.03708641976118088, 0.035713374614715576, 0.03870469331741333, 0.035731758922338486, 0.03553768992424011, 0.03535658121109009, 0.03549094870686531, 0.032708361744880676, 0.03450644388794899, 0.034515105187892914, 0.034778863191604614, 0.03380795195698738, 0.031635016202926636, 0.031772006303071976, 0.03212392330169678, 0.03307294473052025, 0.02984921634197235, 0.03010472096502781, 0.030312610790133476, 0.02833772636950016, 0.029237736016511917, 0.030680542811751366, 0.030846312642097473, 0.027845071628689766, 0.027755502611398697, 0.02792717143893242, 0.028613470494747162, 0.026714971289038658, 0.028006117790937424, 0.025303570553660393, 0.02768993191421032, 0.024613013491034508, 0.026134032756090164, 0.02601506933569908, 0.025873828679323196, 0.025575440376996994, 0.02519998513162136, 0.025636665523052216, 0.02415352128446102, 0.02509467862546444, 0.023547258228063583, 0.0249076709151268, 0.025280578061938286, 0.023071086034178734, 0.02404734119772911, 0.024178462103009224, 0.02195323444902897, 0.023585937917232513, 0.02205144241452217, 0.022065669298171997, 0.025208089500665665, 0.022178862243890762, 0.022924641147255898, 0.021033715456724167, 0.013697422109544277, 0.012387275695800781, 0.011685929261147976, 0.01161415409296751, 0.011226753704249859, 0.01093169767409563, 0.010934671387076378, 0.010841066017746925, 0.010268229991197586, 0.010005129501223564, 0.009349520318210125, 0.009359024465084076, 0.009202594868838787, 0.00976016465574503, 0.009204734116792679, 0.009106413461267948, 0.00930110178887844, 0.008183804340660572, 0.009128374978899956, 0.008407329209148884, 0.008532507345080376, 0.009381658397614956, 0.008821227587759495, 0.0087540652602911, 0.00927160494029522, 0.009107992984354496, 0.00890988577157259, 0.00881103053689003, 0.008616723120212555, 0.008715596981346607, 0.00896801520138979, 0.008604456670582294, 0.008976892568171024, 0.008518020622432232, 0.00810425914824009, 0.008487693965435028, 0.009132827632129192, 0.009364739060401917, 0.008599229156970978, 0.00869861152023077, 0.009003697894513607, 0.00947259459644556, 0.01095445267856121, 0.0092826122418046, 0.009030869230628014, 0.008065987378358841, 0.008395979180932045, 0.009290056303143501, 0.009504452347755432, 0.008746407926082611, 0.008939267136156559, 0.008640162646770477, 0.00822195503860712, 0.009591777808964252, 0.00817817822098732, 0.009360326454043388, 0.00884244218468666, 0.008457999676465988]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:27 2016", "state": "available"}], "summary": "5e023df11d4f048e2bc7cd86430eed33"}