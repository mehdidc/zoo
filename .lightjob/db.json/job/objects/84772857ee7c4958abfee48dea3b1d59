{"content": {"hp_model": {"f0": 16, "f1": 16, "f2": 64, "f3": 32, "nonlin": "leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.5321483612060547, 1.1888540983200073, 1.0396465063095093, 0.9392880797386169, 0.855442464351654, 0.7807731032371521, 0.7108055949211121, 0.6426525115966797, 0.5739684700965881, 0.5041791200637817, 0.4332506060600281, 0.36305275559425354, 0.2938637137413025, 0.23311544954776764, 0.18312382698059082, 0.14407502114772797, 0.11302266269922256, 0.0885571539402008, 0.06879620999097824, 0.051642101258039474, 0.03911063075065613, 0.029314963147044182, 0.022852778434753418, 0.018986191600561142, 0.016586298123002052, 0.015013322234153748, 0.01388795766979456, 0.013024333864450455, 0.012358950451016426, 0.01182642299681902, 0.011387532576918602, 0.01102419476956129, 0.010717302560806274, 0.01045417319983244, 0.01022952701896429, 0.010036260820925236, 0.009866269305348396, 0.009718184359371662, 0.009586519561707973, 0.009470894001424313, 0.009367716498672962, 0.00927579216659069, 0.00919309351593256, 0.009118495509028435, 0.009051267988979816, 0.00899057649075985, 0.008935545571148396, 0.008885531686246395, 0.008840266615152359, 0.008798742666840553, 0.008760977536439896, 0.008726740255951881, 0.008695432916283607, 0.00866700615733862, 0.008640902116894722, 0.008617212064564228, 0.008595702238380909, 0.008576004765927792, 0.008558045141398907, 0.008541639894247055, 0.008526663295924664, 0.00851298775523901, 0.008500544354319572, 0.008489157073199749, 0.00847869273275137, 0.008469137363135815, 0.00846044346690178, 0.0084524629637599, 0.008445175364613533, 0.00843853410333395, 0.008432449772953987, 0.008426892571151257, 0.00842180848121643, 0.008417177945375443, 0.008412957191467285, 0.008409101516008377, 0.008405599743127823, 0.008402402512729168, 0.008399494923651218, 0.008396844379603863, 0.008394436910748482, 0.00839224923402071, 0.008390264585614204, 0.008388464339077473, 0.008386828936636448, 0.008385352790355682, 0.008384013548493385, 0.00838280376046896, 0.00838171225041151, 0.00838073343038559, 0.008379853330552578, 0.008379065431654453, 0.008378362283110619, 0.008377735503017902, 0.008377177640795708, 0.00837668590247631, 0.008376250974833965], "moving_avg_accuracy_train": [0.05534647240679217, 0.11218349137135473, 0.16732371767141932, 0.21959754485915, 0.26810393043875735, 0.31336094796497205, 0.3554380562646358, 0.3940813317508983, 0.42998990547295685, 0.46352109715964973, 0.4941129380193123, 0.5211758065870286, 0.5474362886359669, 0.573813893391787, 0.5967890521042417, 0.623585405147555, 0.6489133091233864, 0.6729335598313412, 0.6957191904149752, 0.7189116966687896, 0.7417635097388524, 0.763662451769766, 0.7841621222904639, 0.8032093529543208, 0.8205820142351161, 0.8363685440604509, 0.8507135686341954, 0.863738059091051, 0.8755368304129352, 0.8862510196550026, 0.8959472683954824, 0.9047575615702383, 0.9127333284037091, 0.9199510460835948, 0.9264702434835871, 0.9323886744173897, 0.9377385137459073, 0.9425789457784779, 0.9469516106494581, 0.9508986347773879, 0.9544649073853819, 0.9576861784766241, 0.9605969482027896, 0.9632259415515767, 0.9656129619047709, 0.9677612802226456, 0.9697063924527805, 0.9714593546575301, 0.9730556218322809, 0.9744922622895567, 0.9757922141475334, 0.9769551953732839, 0.9780088539228879, 0.9789641220639601, 0.9798261885397346, 0.9805973980703127, 0.981296136945452, 0.9819319773795059, 0.9825088840677735, 0.9830211246407857, 0.9834867914541158, 0.9839058915861129, 0.9842854068537197, 0.9846292957433753, 0.9849387957440654, 0.9852173457446866, 0.9854633904476265, 0.9856848306802726, 0.985884126889654, 0.9860634934780973, 0.9862272485565057, 0.9863769532758827, 0.9865116875233221, 0.9866329483460176, 0.9867420830864434, 0.9868403043528267, 0.9869287034925717, 0.9870129130159613, 0.9870887015870119, 0.9871569113009574, 0.9872183000435084, 0.9872735499118043, 0.98732559994208, 0.9873724449693283, 0.9874146054938517, 0.9874502248171133, 0.9874822822080487, 0.9875111338598905, 0.9875371003465482, 0.9875604701845401, 0.9875815030387328, 0.9876004326075063, 0.9876174692194024, 0.987632802170109, 0.9876466018257448, 0.9876613466646266, 0.9876746170196201], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.05528593867658131, 0.1102111228115587, 0.1630761704042733, 0.2121938286838761, 0.2570771378335608, 0.29796554486157517, 0.3349360096242881, 0.3676245199193894, 0.39689563579266124, 0.4234878987382445, 0.4466976320195405, 0.46658953344484544, 0.48485845566511987, 0.5022729301287283, 0.5161310804516989, 0.5315984035059266, 0.5452839721350026, 0.5574392039689421, 0.5693087060118068, 0.581062535147072, 0.592267657997199, 0.6019798805728557, 0.6112254872068351, 0.6193746052312571, 0.6268329407830562, 0.633457934543606, 0.6394204289281009, 0.6446391599904866, 0.649533389463953, 0.654037911257392, 0.6582039031700564, 0.6618434326102044, 0.6651068020750877, 0.6680804556872325, 0.6706936497645937, 0.673059790482788, 0.6751893171291629, 0.6770692700171502, 0.6788100557413388, 0.6804377980493586, 0.6819393872203263, 0.683328468076607, 0.6846773266059192, 0.6858546781885502, 0.6869753297691681, 0.6880205372854742, 0.6889856381126497, 0.6898054007321076, 0.6905065659958698, 0.6911376147332557, 0.691681144534403, 0.6922069424491856, 0.69269236760374, 0.6930926291490889, 0.6934650715711528, 0.6937748261798509, 0.6940169842339291, 0.6942349264825994, 0.6944310745064027, 0.6946076077278257, 0.6947664876271065, 0.6949094795364591, 0.6950381722548765, 0.6951539957014522, 0.6952460297721202, 0.6953410674669716, 0.6954266013923377, 0.6955157889564172, 0.6955960577640887, 0.6956805067222431, 0.6957565107845821, 0.6958249144406872, 0.6958864777311817, 0.6959418846926269, 0.6959917509579274, 0.696036630596698, 0.6960770222715914, 0.6961133747789955, 0.6961460920356592, 0.6961755375666566, 0.6962020385445542, 0.6962380964559121, 0.696270548576134, 0.696299755484334, 0.6963260417017139, 0.6963496992973557, 0.6963709911334335, 0.6963901537859034, 0.6964074001731264, 0.6964229219216271, 0.6964368914952777, 0.696449464111563, 0.6964607794662201, 0.6964709632854114, 0.6964801287226834, 0.6964883776162283, 0.6964958016204188], "moving_var_accuracy_train": [0.027569088070882267, 0.05388619978679645, 0.07586158081591782, 0.09286839981400064, 0.1047573848105784, 0.1127154250478335, 0.11737822992880483, 0.11908013159868894, 0.11877694943959267, 0.11701832183900109, 0.11373923619974725, 0.10895690227579388, 0.10426772830519801, 0.10010295776856656, 0.09484338325247228, 0.09182144575502206, 0.08841282565780029, 0.0847642950886773, 0.08096053022965413, 0.07770550832368771, 0.07463480573662074, 0.07148739812161849, 0.06812078673257113, 0.06457388102297355, 0.060832777160471405, 0.056992430159759945, 0.053145204713975956, 0.04935742040752483, 0.0456745774091274, 0.042140264328248175, 0.03877239305215867, 0.035593745139369165, 0.03260688633466929, 0.02981505673776116, 0.02721605047664564, 0.0248096958514448, 0.022586313293868898, 0.020538550004839427, 0.018656776787020622, 0.016931310103516697, 0.015352643795995779, 0.013910768703385654, 0.012595945056635947, 0.011398555005224055, 0.010309980300200717, 0.009320519714534889, 0.008422518897371781, 0.00760792289605613, 0.0068700632264892016, 0.006201632326071616, 0.00559667796696197, 0.005049182898248808, 0.004554256375476311, 0.004107043572920808, 0.0037030276431066164, 0.0033380777560564445, 0.003008664104591478, 0.002711436331650531, 0.002443288090428189, 0.0022013207950271293, 0.001983140325753749, 0.0017864070974641338, 0.001609062674262841, 0.0014492207429524148, 0.0013051607809110182, 0.0011753430137455309, 0.0010583535543335814, 0.0009529595208899321, 0.000858021039612603, 0.0007725084870087906, 0.0006954989798392525, 0.0006261507853823614, 0.0005636990867010229, 0.0005074615157150077, 0.00045682255766761745, 0.00041122712865538524, 0.0003701747454610157, 0.0003332210921093796, 0.0002999506780659587, 0.00026999748334505166, 0.00024303165220995442, 0.00021875595992047942, 0.00019690474677929693, 0.00017723402221056825, 0.00015952661757796425, 0.00014358537444587435, 0.00012923608608810918, 0.0001163199692396243, 0.00010469404064152596, 9.422955192132328e-05, 8.481057815779038e-05, 7.633274529917688e-05, 6.870208298456328e-05, 6.183399058050326e-05, 5.565230539691395e-05, 5.0089031549685405e-05, 4.508171331561177e-05], "duration": 30252.939198, "accuracy_train": [0.5534647240679218, 0.6237166620524179, 0.6635857543720007, 0.6900619895487264, 0.7046614006552233, 0.7206741057009044, 0.7341320309616095, 0.7418708111272609, 0.7531670689714839, 0.7653018223398856, 0.7694395057562754, 0.7647416236964747, 0.783780627076412, 0.8112123361941677, 0.8035654805163345, 0.8647525825373754, 0.8768644449058692, 0.8891158162029347, 0.9007898656676817, 0.9276442529531194, 0.9474298273694168, 0.9607529300479882, 0.9686591569767442, 0.974634428929033, 0.9769359657622739, 0.9784473124884644, 0.9798187897978959, 0.9809584732027501, 0.981725772309893, 0.9826787228336102, 0.9832135070598007, 0.9840502001430418, 0.9845152299049464, 0.9849105052025655, 0.985143020083518, 0.9856545528216132, 0.9858870677025655, 0.9861428340716132, 0.9863055944882798, 0.986421851928756, 0.9865613608573275, 0.9866776182978036, 0.9867938757382798, 0.9868868816906607, 0.987096145083518, 0.987096145083518, 0.9872124025239941, 0.9872360145002769, 0.9874220264050388, 0.9874220264050388, 0.9874917808693245, 0.9874220264050388, 0.9874917808693245, 0.9875615353336102, 0.9875847868217055, 0.987538283845515, 0.9875847868217055, 0.9876545412859912, 0.9877010442621816, 0.9876312897978959, 0.9876777927740864, 0.9876777927740864, 0.9877010442621816, 0.9877242957502769, 0.9877242957502769, 0.9877242957502769, 0.9876777927740864, 0.9876777927740864, 0.9876777927740864, 0.9876777927740864, 0.9877010442621816, 0.9877242957502769, 0.9877242957502769, 0.9877242957502769, 0.9877242957502769, 0.9877242957502769, 0.9877242957502769, 0.9877707987264673, 0.9877707987264673, 0.9877707987264673, 0.9877707987264673, 0.9877707987264673, 0.9877940502145626, 0.9877940502145626, 0.9877940502145626, 0.9877707987264673, 0.9877707987264673, 0.9877707987264673, 0.9877707987264673, 0.9877707987264673, 0.9877707987264673, 0.9877707987264673, 0.9877707987264673, 0.9877707987264673, 0.9877707987264673, 0.9877940502145626, 0.9877940502145626], "end": "2016-01-30 00:32:01.866000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0], "moving_var_accuracy_valid": [0.02750881513815639, 0.05190891629469131, 0.07187044397804201, 0.0863962987740843, 0.09588727185871138, 0.10134530113643744, 0.10351210840573272, 0.10277774591297595, 0.10021115534187694, 0.09655437584479262, 0.09174716373121347, 0.08613363703891859, 0.08052405500684068, 0.07520102479374967, 0.06940935728774113, 0.0646215643011416, 0.059845060969337796, 0.05519030182083536, 0.050939237347461974, 0.047088686106784836, 0.04350981049888432, 0.04000777485522773, 0.03677632854797753, 0.033696368814363394, 0.03082737285575611, 0.028139650451126413, 0.02564564745957996, 0.02332619909873555, 0.02120916052811252, 0.01927086092458937, 0.017499974229677903, 0.01586919237762145, 0.014378119362238291, 0.013019890968259663, 0.011779360921007292, 0.010651812425991234, 0.009627445137030694, 0.008696508629077091, 0.007854130780607232, 0.007092563607738366, 0.006403600177309839, 0.005780606070206423, 0.005218920237174665, 0.004709503624199311, 0.004249856001465651, 0.0038347025300883705, 0.0034596150535390665, 0.0031197016449555047, 0.002812156175003914, 0.00253452456008413, 0.0022837309258783344, 0.0020578460043152087, 0.0018541821421097547, 0.001670205811640945, 0.0015044336506966267, 0.0013548538168854508, 0.0012198961999053, 0.0010983340693285694, 0.00098884692882089, 0.0008902427117431948, 0.0008014456259704343, 0.0007214850835486536, 0.0006494856315357511, 0.0005846578040191658, 0.0005262682560487229, 0.00047372271991483433, 0.0004264162923948477, 0.00038384625294964086, 0.00034551961538804187, 0.000311031838488038, 0.00027998064419666236, 0.00025202469131851296, 0.00022685633253529032, 0.00020419832866415047, 0.0001838008755974707, 0.00016543891567550917, 0.00014890970749456455, 0.0001340306302882592, 0.00012063720102938569, 0.00010858128428010855, 9.772947656856345e-05, 8.79682304684505e-05, 7.918088568256761e-05, 7.127047450569019e-05, 6.414964574213844e-05, 5.773971830440859e-05, 5.1969826554519764e-05, 4.6776148764314934e-05, 4.2101210828733624e-05, 3.7893258067948905e-05, 3.410568860204582e-05, 3.06965423779636e-05, 2.7628040475426352e-05, 2.4866169819443598e-05, 2.2380308884662743e-05, 2.0142890394398905e-05, 1.812909739750299e-05], "accuracy_test": 0.6973712531887755, "start": "2016-01-29 16:07:48.927000", "learning_rate_per_epoch": [0.001995969796553254, 0.0018393336795270443, 0.0016949897399172187, 0.0015619733603671193, 0.0014393955934792757, 0.0013264373410493135, 0.0012223436497151852, 0.0011264188215136528, 0.0010380217572674155, 0.0009565617656335235, 0.000881494430359453, 0.0008123181178234518, 0.0007485704845748842, 0.0006898255087435246, 0.0006356906378641725, 0.0005858040531165898, 0.0005398323992267251, 0.0004974683979526162, 0.00045842898543924093, 0.0004224532167427242, 0.00038930069422349334, 0.0003587498504202813, 0.00033059652196243405, 0.0003046525816898793, 0.00028074459987692535, 0.0002587128255981952, 0.00023841002257540822, 0.00021970050875097513, 0.0002024592540692538, 0.0001865710219135508, 0.00017192964151035994, 0.0001584372657816857, 0.0001460037165088579, 0.00013454590225592256, 0.000123987250844948, 0.00011425720731494948, 0.00010529073915677145, 9.702792158350348e-05, 8.941353735281155e-05, 8.239670569309965e-05, 7.593052578158677e-05, 6.997178570600227e-05, 6.448066415032372e-05, 5.94204684603028e-05, 5.475737634696998e-05, 5.04602248838637e-05, 4.65002995042596e-05, 4.285113391233608e-05, 3.94883390981704e-05, 3.638944326667115e-05, 3.353373904246837e-05, 3.0902137950761244e-05, 2.8477055820985697e-05, 2.6242285457556136e-05, 2.4182891138480045e-05, 2.2285110389930196e-05, 2.0536259398795664e-05, 1.8924651158158667e-05, 1.7439517250750214e-05, 1.607092963240575e-05, 1.4809744243393652e-05, 1.3647531886817887e-05, 1.2576525477925315e-05, 1.1589567293412983e-05, 1.0680062587198336e-05, 9.841932296694722e-06, 9.069574844033923e-06, 8.357828846783377e-06, 7.701938557147514e-06, 7.097519755916437e-06, 6.540533377119573e-06, 6.0272573136899155e-06, 5.5542609516123775e-06, 5.118383796798298e-06, 4.7167127377178986e-06, 4.346562946011545e-06, 4.005461050837766e-06, 3.691127631100244e-06, 3.4014619814115576e-06, 3.134528242298984e-06, 2.8885424399049953e-06, 2.661860662556137e-06, 2.452967919452931e-06, 2.2604683636018308e-06, 2.083075514747179e-06, 1.9196036191715393e-06, 1.7689604874249198e-06, 1.6301391951856203e-06, 1.5022120578578324e-06, 1.3843241504218895e-06, 1.2756876230923808e-06, 1.1755764717236161e-06, 1.0833217629624414e-06, 9.98306745714217e-07, 9.199634405376855e-07, 8.477682058583014e-07, 7.812386115801928e-07], "accuracy_train_first": 0.5534647240679218, "accuracy_train_last": 0.9877940502145626, "batch_size_eval": 1024, "accuracy_train_std": [0.016889660601415613, 0.018154368895830178, 0.01847438810974632, 0.020057688428616733, 0.018504408045474794, 0.019951228187069556, 0.022323565974254483, 0.02393969034395334, 0.025510438661379125, 0.027087347796063078, 0.029196953196631126, 0.02834650003433226, 0.029935111781929525, 0.0288958624338647, 0.029165948258966106, 0.02913825227292552, 0.027055591346468053, 0.025858193515326484, 0.022496946522888, 0.020108678187894795, 0.015481578974765126, 0.01247380787918632, 0.010174841042976168, 0.008595043247440518, 0.0077275271098128165, 0.006978176394438921, 0.006645102455872779, 0.00602357453245528, 0.005352893114361093, 0.005300265792561212, 0.004849025769740608, 0.004798611640441564, 0.004764849067121223, 0.00465697001304425, 0.004464421874076068, 0.004324582042738179, 0.004375251996741923, 0.004206051012662412, 0.004136618652842128, 0.00402148840365884, 0.003970321177348906, 0.003935904580798646, 0.003949800686900578, 0.0038679042677990565, 0.00364663395525327, 0.0035583923794323107, 0.003489252001187158, 0.003333708537936469, 0.0032073548369049502, 0.0032144265604464495, 0.003242786052822658, 0.003249554342665106, 0.003249780676763767, 0.0032133703154468597, 0.0032008037068487092, 0.003204533155883761, 0.0032500824310436375, 0.00325379983420232, 0.0032554455013694052, 0.003273602698610741, 0.0033305635721249107, 0.0033305635721249107, 0.0033448909141113184, 0.0033589962017314297, 0.0033589962017314297, 0.0033589962017314297, 0.0033712211562105378, 0.003364479011882152, 0.003364479011882152, 0.003364479011882152, 0.003358440315485222, 0.003325024837473566, 0.003325024837473566, 0.003325024837473566, 0.003325024837473566, 0.003325024837473566, 0.003325024837473566, 0.003277517841593172, 0.003277517841593172, 0.003277517841593172, 0.003277517841593172, 0.003277517841593172, 0.0032775926443564343, 0.0032775926443564343, 0.0032775926443564343, 0.003277517841593172, 0.003277517841593172, 0.003277517841593172, 0.003277517841593172, 0.003277517841593172, 0.003277517841593172, 0.003277517841593172, 0.003277517841593172, 0.003277517841593172, 0.003277517841593172, 0.0032637076181801857, 0.0032637076181801857], "accuracy_test_std": 0.013306372568166107, "error_valid": [0.44714061323418675, 0.3954622199736446, 0.36113840126129515, 0.3457472467996988, 0.33897307981927716, 0.33403879188629515, 0.33232980751129515, 0.3381788874246988, 0.3396643213478916, 0.33718173475150603, 0.34441476844879515, 0.3543833537274097, 0.3507212443524097, 0.34099679969879515, 0.35914556664156627, 0.32919568900602414, 0.33154591020331325, 0.33316370952560237, 0.3238657756024097, 0.31315300263554224, 0.3068862363516567, 0.3106101162462349, 0.30556405308734935, 0.3072833325489458, 0.306042039250753, 0.3069171216114458, 0.3069171216114458, 0.30839226044804224, 0.30641854527484935, 0.3054213926016567, 0.3043021696159638, 0.3054008024284638, 0.3055228727409638, 0.3051566618034638, 0.3057876035391567, 0.3056449430534638, 0.3056449430534638, 0.3060111539909638, 0.3055228727409638, 0.3049125211784638, 0.3045463102409638, 0.30416980421686746, 0.3031829466302711, 0.3035491575677711, 0.3029388060052711, 0.3025725950677711, 0.3023284544427711, 0.3028167356927711, 0.3031829466302711, 0.3031829466302711, 0.3034270872552711, 0.3030608763177711, 0.3029388060052711, 0.3033050169427711, 0.3031829466302711, 0.30343738234186746, 0.30380359327936746, 0.30380359327936746, 0.30380359327936746, 0.30380359327936746, 0.30380359327936746, 0.30380359327936746, 0.30380359327936746, 0.30380359327936746, 0.30392566359186746, 0.30380359327936746, 0.30380359327936746, 0.30368152296686746, 0.30368152296686746, 0.30355945265436746, 0.30355945265436746, 0.30355945265436746, 0.30355945265436746, 0.30355945265436746, 0.30355945265436746, 0.30355945265436746, 0.30355945265436746, 0.30355945265436746, 0.30355945265436746, 0.30355945265436746, 0.30355945265436746, 0.30343738234186746, 0.30343738234186746, 0.30343738234186746, 0.30343738234186746, 0.30343738234186746, 0.30343738234186746, 0.30343738234186746, 0.30343738234186746, 0.30343738234186746, 0.30343738234186746, 0.30343738234186746, 0.30343738234186746, 0.30343738234186746, 0.30343738234186746, 0.30343738234186746, 0.30343738234186746], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.0784761875833005, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 32, "valid_ratio": 0.15, "learning_rate": 0.00216594468168683, "optimization": "nesterov_momentum", "nb_data_augmentation": 0, "learning_rate_decay_method": "exp", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 1.202311924056768e-06, "rotation_range": [0, 0], "momentum": 0.8884591742449961}, "accuracy_valid_max": 0.6976715455572289, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.6965626176581325, "accuracy_valid_std": [0.012871714919996991, 0.014737982605660026, 0.011726923913314665, 0.00818100345287577, 0.0073015000665376945, 0.007680530436416395, 0.008343102290949164, 0.01219991014711204, 0.008923738672451617, 0.012371770468876796, 0.005872083888873324, 0.011751289319652691, 0.010979379451246737, 0.006985760267076556, 0.00851385864754966, 0.010485567745370938, 0.01046986290286213, 0.011282173771986127, 0.008500747149332886, 0.013206465007003185, 0.013943777470329977, 0.012501248473698089, 0.013318037886748114, 0.013624900571349147, 0.015481417820323818, 0.014782377151319217, 0.01379801731807364, 0.012138944741059103, 0.012266443567196231, 0.013215312968150017, 0.01420055155758005, 0.014597844476340523, 0.014579107936526165, 0.014235761829016324, 0.013140074779657718, 0.014551133708163277, 0.014353168510625382, 0.01429488085226261, 0.013693664134161319, 0.01328910641910804, 0.013294745529367227, 0.013660115520624317, 0.013927940701353673, 0.014001009718435705, 0.014044443676426363, 0.013989721108928386, 0.013770012108772756, 0.014007445281940523, 0.014030272810235835, 0.01405573942796848, 0.01407126265987291, 0.014198321408501122, 0.014120629072113838, 0.014291057506426062, 0.014257834925590391, 0.013436353642989176, 0.013487518257100446, 0.013487518257100446, 0.013487518257100446, 0.013487518257100446, 0.013487518257100446, 0.013487518257100446, 0.013487518257100446, 0.01362820039481526, 0.013508207615457103, 0.01362820039481526, 0.01362820039481526, 0.013676507889991224, 0.013676507889991224, 0.01360140535749641, 0.01360140535749641, 0.01360140535749641, 0.01360140535749641, 0.01360140535749641, 0.01360140535749641, 0.01360140535749641, 0.01360140535749641, 0.01360140535749641, 0.01360140535749641, 0.01360140535749641, 0.01360140535749641, 0.013656356164695163, 0.013656356164695163, 0.013656356164695163, 0.013656356164695163, 0.013656356164695163, 0.013656356164695163, 0.013656356164695163, 0.013656356164695163, 0.013656356164695163, 0.013656356164695163, 0.013656356164695163, 0.013656356164695163, 0.013656356164695163, 0.013656356164695163, 0.013656356164695163, 0.013656356164695163], "accuracy_valid": [0.5528593867658133, 0.6045377800263554, 0.6388615987387049, 0.6542527532003012, 0.6610269201807228, 0.6659612081137049, 0.6676701924887049, 0.6618211125753012, 0.6603356786521084, 0.662818265248494, 0.6555852315512049, 0.6456166462725903, 0.6492787556475903, 0.6590032003012049, 0.6408544333584337, 0.6708043109939759, 0.6684540897966867, 0.6668362904743976, 0.6761342243975903, 0.6868469973644578, 0.6931137636483433, 0.6893898837537651, 0.6944359469126506, 0.6927166674510542, 0.693957960749247, 0.6930828783885542, 0.6930828783885542, 0.6916077395519578, 0.6935814547251506, 0.6945786073983433, 0.6956978303840362, 0.6945991975715362, 0.6944771272590362, 0.6948433381965362, 0.6942123964608433, 0.6943550569465362, 0.6943550569465362, 0.6939888460090362, 0.6944771272590362, 0.6950874788215362, 0.6954536897590362, 0.6958301957831325, 0.6968170533697289, 0.6964508424322289, 0.6970611939947289, 0.6974274049322289, 0.6976715455572289, 0.6971832643072289, 0.6968170533697289, 0.6968170533697289, 0.6965729127447289, 0.6969391236822289, 0.6970611939947289, 0.6966949830572289, 0.6968170533697289, 0.6965626176581325, 0.6961964067206325, 0.6961964067206325, 0.6961964067206325, 0.6961964067206325, 0.6961964067206325, 0.6961964067206325, 0.6961964067206325, 0.6961964067206325, 0.6960743364081325, 0.6961964067206325, 0.6961964067206325, 0.6963184770331325, 0.6963184770331325, 0.6964405473456325, 0.6964405473456325, 0.6964405473456325, 0.6964405473456325, 0.6964405473456325, 0.6964405473456325, 0.6964405473456325, 0.6964405473456325, 0.6964405473456325, 0.6964405473456325, 0.6964405473456325, 0.6964405473456325, 0.6965626176581325, 0.6965626176581325, 0.6965626176581325, 0.6965626176581325, 0.6965626176581325, 0.6965626176581325, 0.6965626176581325, 0.6965626176581325, 0.6965626176581325, 0.6965626176581325, 0.6965626176581325, 0.6965626176581325, 0.6965626176581325, 0.6965626176581325, 0.6965626176581325, 0.6965626176581325], "seed": 717241597, "model": "residualv3", "loss_std": [0.2688097357749939, 0.2009069174528122, 0.19573430716991425, 0.19153961539268494, 0.18532486259937286, 0.1795913577079773, 0.1728007197380066, 0.16622266173362732, 0.15767443180084229, 0.14872707426548004, 0.13768212497234344, 0.1243891641497612, 0.10965833067893982, 0.09203764796257019, 0.07689929008483887, 0.06251020729541779, 0.054225701838731766, 0.04398634284734726, 0.033175088465213776, 0.025481896474957466, 0.018471620976924896, 0.013098973780870438, 0.00825242418795824, 0.005797072779387236, 0.004458297975361347, 0.003691282356157899, 0.0032078612130135298, 0.002844719449058175, 0.0025794128887355328, 0.0023766413796693087, 0.002210973296314478, 0.002078478457406163, 0.0019683807622641325, 0.0018763273255899549, 0.0017973637441173196, 0.0017315204022452235, 0.0016734263626858592, 0.0016234059585258365, 0.0015788896707817912, 0.0015401890268549323, 0.0015059865545481443, 0.0014753638533875346, 0.0014478781959041953, 0.0014233564725145698, 0.0014011857565492392, 0.0013812183169648051, 0.0013633057242259383, 0.0013470160774886608, 0.0013324216706678271, 0.001318825874477625, 0.0013065981911495328, 0.0012953075347468257, 0.0012850344646722078, 0.0012756962096318603, 0.0012670910218730569, 0.0012592728016898036, 0.0012521715834736824, 0.001245622057467699, 0.0012396068777889013, 0.0012341061374172568, 0.0012290487065911293, 0.0012244414538145065, 0.0012202054494991899, 0.0012163303326815367, 0.0012127585941925645, 0.0012094653211534023, 0.0012065063929185271, 0.0012037722626700997, 0.001201270497404039, 0.0011989808408543468, 0.001196883269585669, 0.0011949562467634678, 0.0011931763729080558, 0.001191545045003295, 0.0011900451499968767, 0.0011886634165421128, 0.0011873987969011068, 0.0011862304527312517, 0.0011851602466776967, 0.0011841710656881332, 0.001183267100714147, 0.0011824378743767738, 0.0011816781479865313, 0.0011809797724708915, 0.0011803393717855215, 0.001179750426672399, 0.0011792097939178348, 0.0011787187540903687, 0.0011782702058553696, 0.001177860889583826, 0.0011774906888604164, 0.0011771569261327386, 0.001176855294033885, 0.0011765873059630394, 0.0011763470247387886, 0.0011761363130062819, 0.0011759514454752207]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:23 2016", "state": "available"}], "summary": "e7ac3e297d374a0db121fe6fb7bfb6e6"}