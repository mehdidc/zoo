{"content": {"hp_model": {"f0": 64, "f1": 64, "f2": 64, "f3": 32, "nonlin": "rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.05230714295089574, 0.0593300527994656, 0.06383435252749894, 0.061789128360103536, 0.06274634818091734, 0.05363238552193368, 0.05485255407262152, 0.050860900873584455, 0.05076754004494325, 0.04873020362902761, 0.05541348019028962, 0.04979566646887613, 0.047933145052684416, 0.04531594579901365, 0.04502812232558691, 0.045126638832367755, 0.039686162530102286, 0.044857881568036696, 0.04912424646623992, 0.05496461318611501, 0.045854683430111155, 0.046367552138711285, 0.048675640460973106, 0.05768952539587279, 0.04422322027739305, 0.04187971161548252, 0.044622290405632856, 0.04565901717919351, 0.05528007389203061, 0.045606645595067295, 0.05519968149699254, 0.052228321095668044, 0.047262435380975056, 0.04688165785381544, 0.041981793085480196, 0.04376635196851785, 0.045072065922733225, 0.04494605868730194, 0.04381481767465486, 0.04724507377726901, 0.04347811508918793, 0.04542286953461562, 0.04771834763504564, 0.0434399495655691, 0.04556752310292606, 0.04806690600772862, 0.04104740619676844, 0.04340627040516752, 0.049968715101147726, 0.04364229604534045, 0.04336557412735516, 0.042156879608865085, 0.04196904648026402, 0.03889314454971666, 0.043220978556065257, 0.03980328207307085, 0.047254510222773285, 0.04541737225182606, 0.04651923673164333, 0.05003826597936315, 0.045700402931595274, 0.040442038635855555, 0.04278967861623636, 0.046367167497369584, 0.04083569671853808, 0.043873800164516016, 0.04121131721896725, 0.04691132136773536, 0.042079812617041315, 0.0436292170074786, 0.04285465032840956, 0.04183241468790429, 0.04166452643528056, 0.042545151027999946, 0.041393971912302104, 0.04127229193426323, 0.04528169263456877, 0.046770442780488745, 0.04284424482466298, 0.054201991107331746, 0.049564474900699235, 0.04630288757046202, 0.045265147347139434, 0.04527460253746965, 0.04121001890440473, 0.04454508465472681, 0.0444324364325698, 0.04120482523707134, 0.04212937186693392, 0.046910180810203594, 0.04172483885721904, 0.04400044611427028, 0.05102824031662073, 0.04369008294940893, 0.042342194744984504, 0.05221397706533078, 0.04222578174876919, 0.04668112712701765, 0.04223591735551979, 0.038471738616574686, 0.04334747462872378, 0.04478227677826465, 0.04408669736487791, 0.043679059842648534, 0.04142325969731677, 0.04068035663571799, 0.04526514734713943, 0.041564240340008676, 0.046813131852773394, 0.041769695790060346, 0.045904829503839895, 0.04316688857016436, 0.042829256457989635, 0.0430862471731661, 0.03962814803980621, 0.046386011172241656, 0.04237377344945775, 0.04272794746120162, 0.037694338097240626, 0.042577416968776656, 0.04371620069234277, 0.04178634467223173, 0.04223296138815525, 0.04096259247314165, 0.04006321224081966, 0.041797440242772356, 0.041059135827134316, 0.04011659666386229, 0.04398585172988456, 0.04381156115692956, 0.046679216808059934, 0.04081997085541595, 0.047966992076606194, 0.04398301337055595, 0.04161312785934443, 0.04519969479541812, 0.05109006586653528, 0.04417237644980038, 0.043174324797901516, 0.04331001757621826, 0.046252016272169114, 0.03824390885827475, 0.046653610987210374, 0.047673850461107135, 0.042196628318063666, 0.04415945441011926, 0.055989493744481325, 0.04775533469792097, 0.04616749251704671, 0.0411580527389641, 0.043649651281938616, 0.05108378194027149, 0.04366109029823224, 0.04275423580754519, 0.05086019955189536, 0.04248306455708834, 0.043391887193830275, 0.04016191763105125, 0.041182311776470706, 0.044642270119179865, 0.04395664842433466, 0.04613039218390765, 0.040959544612602754, 0.041658105081381666, 0.04119443593799995, 0.04299757434521917, 0.041341805575366763, 0.04234303714940818, 0.044407944799420174, 0.04751534251201285, 0.04259416884988389, 0.03963714809625525, 0.04316895431747109, 0.04440111685520564, 0.04569220684496979, 0.0486404532464027, 0.04642598056689414, 0.04396029989846994, 0.04301167476639449, 0.043338833566920025, 0.045682838088235, 0.04494843945193529, 0.04220634836577832, 0.038731855892971, 0.04728846583494457, 0.041232949776501944, 0.03971715875094253, 0.04267532218703519, 0.045789294584023095, 0.043750456568587046, 0.03878155461644251, 0.03977369821537241, 0.04143789582723517, 0.04253970111677486, 0.04336557412735516, 0.03747653632586079, 0.04113724789200332, 0.03927417490917603, 0.04170559968813095, 0.04113724789200332, 0.04051869939471404, 0.04214333957973509, 0.04247340783597556, 0.041782503211316775, 0.047008167612105356, 0.04051341710292277, 0.03908572114603102, 0.0479264472052329, 0.04247928610156817, 0.03872771145382876, 0.04517996158973315, 0.04684969152657874, 0.04030023008845313, 0.04173894194503927, 0.04120569089375348, 0.042023405051905015, 0.038686242642863175, 0.04230426918401569, 0.04382255093437554, 0.04081472755456299, 0.04063298042791059, 0.040665009280274056, 0.04469058406240456, 0.04350599987339176, 0.041182744843701484, 0.042023405051905015, 0.03879167062426875, 0.04242677275996748, 0.04538201667656921, 0.04888294837103922, 0.0399254188017483, 0.042870461857036656, 0.04033871342754806, 0.05238891017535611, 0.04295649087718016, 0.04139526445739182, 0.040531902112394255], "moving_avg_accuracy_train": [0.03426440135542168, 0.08267342808734938, 0.13292453407379515, 0.1808472538591867, 0.2254131308829066, 0.2797166973126882, 0.3198973242681664, 0.37328503913050637, 0.42195757060902195, 0.46512514186137277, 0.49694705915113907, 0.5330392508263866, 0.5682071819184467, 0.5983499426422646, 0.6274221396430983, 0.6512551327571018, 0.675982782131994, 0.6988565483465055, 0.7158190674576381, 0.7268919988142839, 0.7458537778485181, 0.761883987413064, 0.7780736948464564, 0.7773841717473529, 0.7928827651147863, 0.8064644057719823, 0.8185796368815311, 0.8298669103921732, 0.8270359987204258, 0.8364963672218771, 0.8386430407406532, 0.8445377728111662, 0.848948829867399, 0.8577757089288518, 0.865562238186569, 0.8728501408739361, 0.8743217156419641, 0.8800065433247557, 0.8863653581187863, 0.8890009420358234, 0.8937896656033253, 0.8967605672658843, 0.8942032981898983, 0.899589538401029, 0.9006438563982755, 0.90488481714399, 0.9092193775982658, 0.9123604104709693, 0.911892912345559, 0.9138112077073887, 0.917575512388457, 0.9205068730472017, 0.9220579364954936, 0.9252446503760647, 0.9277644247962895, 0.9300651660516004, 0.9310674973380066, 0.9326237747126398, 0.93488568187993, 0.9329939698666359, 0.9340305103799723, 0.9357964164202883, 0.936277392248139, 0.9333970174811563, 0.9358051508234022, 0.9375465483916644, 0.9392997060524979, 0.9395691895135132, 0.9419389836645715, 0.9432693699366687, 0.9439113711960138, 0.9450021617872557, 0.946075646662747, 0.9477477318458698, 0.9484972432998371, 0.9480281665602148, 0.9489849508078079, 0.9478341025643765, 0.9493585801091436, 0.9404261106524462, 0.9382980327197317, 0.9398560306525778, 0.9340410789427418, 0.937462554632805, 0.9394382494707293, 0.9407951587104034, 0.9428023333514113, 0.9448370473054267, 0.945056373448378, 0.9457808754107692, 0.947866003231138, 0.9486060407092289, 0.9431138477828843, 0.9441314351431501, 0.945037851116787, 0.9433098566677589, 0.9451032121154408, 0.9467478231328124, 0.9482915084400131, 0.9500949818429997, 0.9516310408876154, 0.9530299661663237, 0.9509663332545106, 0.9515610591158066, 0.9524045766982018, 0.9538085090886226, 0.9547449586315675, 0.9559760350575675, 0.9555662139313288, 0.9558633199478345, 0.9571002183747378, 0.9581851890071436, 0.959373447214863, 0.9608029134873526, 0.9613623058735571, 0.960056176942828, 0.9611961729533645, 0.9614103282483895, 0.9626972886464421, 0.9634413963781834, 0.9622520948427747, 0.9612499651777744, 0.9616046373346958, 0.9630180629084552, 0.9637936286055614, 0.9632374020401859, 0.9631768395470107, 0.9643389183935145, 0.9641046688734402, 0.960373512980072, 0.9612488122844745, 0.9619495146403644, 0.957840877182352, 0.9582752570845987, 0.9584897117978256, 0.9595886886602117, 0.9527958589508171, 0.9544971653750125, 0.9561412929640173, 0.9560137977037602, 0.9478394698912155, 0.9497963926310096, 0.9463288956871857, 0.9460013525040093, 0.9488103851752951, 0.9507878745192114, 0.9389357436937963, 0.9343470450774287, 0.9374249535214931, 0.9387055191633197, 0.9409804868253009, 0.9367356007933733, 0.9398146762562045, 0.9428399857390178, 0.9453203885205377, 0.9474186207528212, 0.9500976924124789, 0.9516546588640021, 0.9541783872547104, 0.9556167232280345, 0.9549133905136648, 0.9553651990526597, 0.9575343455630564, 0.9587876881151846, 0.9603745631289673, 0.9603979125389621, 0.9611154631525358, 0.9619448053914992, 0.963171258587289, 0.9625384324273554, 0.9641620364737764, 0.964992632525194, 0.9656789867425541, 0.9661108056887806, 0.9665488591560472, 0.9597494890536955, 0.9602330303591693, 0.9603623063895175, 0.9611398935216501, 0.9607525607959911, 0.9616087806200064, 0.9626688174676444, 0.9634581292449763, 0.9645756069831293, 0.9587712842366236, 0.9606021753912745, 0.9628006174907012, 0.9636096935428359, 0.9645967098813233, 0.9641884319654801, 0.9655999728050766, 0.9663761954040869, 0.9671806880624734, 0.9679565010333344, 0.9689135805986756, 0.9694219778098924, 0.969663044336132, 0.9704141721314344, 0.9670074499484115, 0.9675982147427269, 0.9682687396539964, 0.9688298551464282, 0.9677958907161227, 0.9687713844758358, 0.967787977202951, 0.9676982796332583, 0.9674057671819807, 0.9637657175722164, 0.963842929700537, 0.9651878347726519, 0.9649134037050253, 0.958122270412836, 0.9586532286125163, 0.9608041896368069, 0.96133992278156, 0.9626998122805124, 0.963714281353666, 0.9644578758086608, 0.9647694300952646, 0.9664028974773043, 0.9664940648078871, 0.9674044286584237, 0.9667977395576415, 0.9665482178609135, 0.9673331551109667, 0.9685384691179424, 0.9692373330495216, 0.9687744431180634, 0.9676707186857751, 0.9643100850702097, 0.9655847429788513, 0.9664754403677132, 0.9675053247947972, 0.9555815995442332, 0.9551123891380027, 0.9571562142302266, 0.9587697531987702], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 1234, "moving_var_accuracy_train": [0.010566442802208809, 0.03060070334418035, 0.05026719588551139, 0.0659097599416227, 0.07719384050150034, 0.09601435239429365, 0.10094326219768197, 0.11650116886191644, 0.12617218986046866, 0.13032592374486285, 0.12640704115034712, 0.1254901537346177, 0.12407218875681893, 0.11984224409761714, 0.11546475343395302, 0.10903038213750693, 0.10363045371622472, 0.09797629097212737, 0.09076820536627447, 0.08279487310910794, 0.07775132737548529, 0.07228890320608605, 0.06741897252648703, 0.0606813542527761, 0.05677507639481994, 0.05275771742180876, 0.04880295510316788, 0.04506928248258747, 0.040634480782367845, 0.03737651985378035, 0.03368034173316825, 0.030625038355499633, 0.02773765133912974, 0.025665110350906406, 0.023644269656747346, 0.02175786442129742, 0.01960156776984875, 0.017932266384911175, 0.016502950476683084, 0.014915172152268475, 0.01363004179769517, 0.012346473928123018, 0.011170683161453665, 0.010314719097816307, 0.009293251465988537, 0.008525798051809915, 0.00784231397561486, 0.007146877365620012, 0.006434156619533369, 0.0058238596714369865, 0.005369003611880506, 0.004909439128497179, 0.004440147396033105, 0.004087528964639417, 0.003735919436334851, 0.0034099681856163684, 0.0030780133791241102, 0.0027920100346128522, 0.0025588550474525172, 0.0023351767117784404, 0.0021113287867226857, 0.0019282617253394386, 0.0017375175925282845, 0.0016384348624598852, 0.0015267833319602205, 0.0014013971881809437, 0.001288919525416504, 0.0011606811648967007, 0.0010951563672725424, 0.001001570079242147, 0.0009051225618709395, 0.0008253187227093242, 0.0007531581784395688, 0.0007030051803321838, 0.0006377605690756181, 0.0005759648090569483, 0.0005266072530192319, 0.0004858665928319908, 0.0004581962196092857, 0.001130477693001861, 0.0010581883648910326, 0.000974215746430704, 0.001181117142277165, 0.0011683638911286875, 0.0010866578328492258, 0.0009945628737267184, 0.0009313653367095914, 0.0008754893509106208, 0.0007883733514323958, 0.0007142601441307333, 0.0006819639519631461, 0.0006186964559876454, 0.0008283044586505861, 0.0007547933691074816, 0.000686708341452109, 0.0006449111906497452, 0.0006093651854403453, 0.0005727713754824506, 0.0005369409168832111, 0.0005125194720324121, 0.00048250282132608185, 0.00045186546661215654, 0.0004450061471034046, 0.0004036888220439117, 0.00036972363704580985, 0.0003504905087530823, 0.00032333389759611075, 0.00030464045033637324, 0.0002756879855023398, 0.00024891363481750067, 0.00023779153080203324, 0.000224606829180476, 0.00021485376437634072, 0.00021175875235637467, 0.00019339915569642935, 0.00018941299517997675, 0.0001821680137983309, 0.000164363974831983, 0.0001628339809441864, 0.00015153384969770282, 0.00014911040800706212, 0.00014323774199561957, 0.00013004609884611481, 0.00013502143563451534, 0.00012693281142581563, 0.00011702402221149805, 0.00010535463033056463, 0.00010697301250693141, 9.676956679513378e-05, 0.00021238632882116047, 0.00019804303578963085, 0.0001826575863346147, 0.0003163199435535922, 0.0002863861222935159, 0.0002581614274803913, 0.00024321503602889238, 0.0006341763515735165, 0.0005968087083572418, 0.0005614562372818575, 0.0005054569089261641, 0.0010562879347160226, 0.00098512506073013, 0.0009948243701559663, 0.0008963074939719776, 0.0008776927255099399, 0.0008251176299066676, 0.0020068629128407976, 0.001995682016483402, 0.0018813754983454457, 0.001707996583778142, 0.0015837762261678733, 0.0015875701203675783, 0.0015141394596831115, 0.0014450979909160003, 0.0013559597734515455, 0.0012599870026117348, 0.001198585126968788, 0.001100543915052428, 0.0010478123684577857, 0.0009616504249614321, 0.0008699374746292129, 0.0007847809057694705, 0.0007486495844446182, 0.0006879224339769312, 0.0006417937413635502, 0.0005776192739817192, 0.0005244912565309068, 0.0004782324078217738, 0.00044394685401276455, 0.00040315638914975626, 0.00038656556113077516, 0.00035411801322337135, 0.00032294595090622637, 0.0002923295642364853, 0.0002648236253744947, 0.0006544241669358328, 0.0005910860599891435, 0.0005321278646184324, 0.0004843568538891129, 0.0004372714082634994, 0.0004001422789204813, 0.0003702411540935842, 0.00033882415642074007, 0.00031618054923607345, 0.0005877739572228971, 0.0005591660232822175, 0.0005467477499347834, 0.0004979644114645459, 0.00045693578159006285, 0.00041274242114014377, 0.0003894002069027704, 0.0003558828799214237, 0.0003261194678658613, 0.00029892449297108214, 0.0002772760553235183, 0.0002518746593105235, 0.00022721021101013055, 0.00020956692659300087, 0.00029306203822440717, 0.00026689686178179, 0.00024425360851330745, 0.00022266190302459835, 0.0002100174547103707, 0.0001975800019164852, 0.00018652581050410084, 0.00016794564033976973, 0.00015192114811316486, 0.00025597868375575755, 0.00023043447079501987, 0.00022366995059252325, 0.0002019807672311792, 0.0005968581130385953, 0.000539709551223005, 0.0005273782960528571, 0.00047722355646905436, 0.000446144895866399, 0.0004107927337832248, 0.00037468985482639407, 0.00033809446400526546, 0.0003282989587984306, 0.00029554386625807796, 0.0002734483406955444, 0.00024941615161106043, 0.0002250348861441966, 0.0002080765359084671, 0.00020034391901632475, 0.00018470522426845397, 0.00016816310563941685, 0.0001623106636773457, 0.0002477243219922286, 0.00023757466484957093, 0.00022095727491134172, 0.00020840750481856076, 0.0014671437689951561, 0.001322410817743476, 0.0012277647250375618, 0.0011284198245608843], "duration": 145233.312599, "accuracy_train": [0.34264401355421686, 0.5183546686746988, 0.5851844879518072, 0.6121517319277109, 0.6265060240963856, 0.7684487951807228, 0.6815229668674698, 0.8537744728915663, 0.8600103539156626, 0.8536332831325302, 0.7833443147590361, 0.8578689759036144, 0.8847185617469879, 0.8696347891566265, 0.8890719126506024, 0.8657520707831325, 0.8985316265060241, 0.9047204442771084, 0.8684817394578314, 0.8265483810240963, 0.9165097891566265, 0.9061558734939759, 0.9237810617469879, 0.7711784638554217, 0.9323701054216867, 0.928699171686747, 0.9276167168674698, 0.9314523719879518, 0.8015577936746988, 0.9216396837349398, 0.8579631024096386, 0.8975903614457831, 0.888648343373494, 0.9372176204819277, 0.9356410015060241, 0.938441265060241, 0.8875658885542169, 0.9311699924698795, 0.9435946912650602, 0.9127211972891566, 0.9368881777108434, 0.9234986822289156, 0.8711878765060241, 0.9480657003012049, 0.910132718373494, 0.9430534638554217, 0.948230421686747, 0.9406297063253012, 0.9076854292168675, 0.9310758659638554, 0.9514542545180723, 0.9468891189759037, 0.9360175075301205, 0.9539250753012049, 0.9504423945783133, 0.9507718373493976, 0.9400884789156626, 0.9466302710843374, 0.9552428463855421, 0.9159685617469879, 0.943359375, 0.9516895707831325, 0.9406061746987951, 0.9074736445783133, 0.9574783509036144, 0.9532191265060241, 0.955078125, 0.9419945406626506, 0.9632671310240963, 0.9552428463855421, 0.9496893825301205, 0.9548192771084337, 0.9557370105421686, 0.9627964984939759, 0.9552428463855421, 0.9438064759036144, 0.9575960090361446, 0.937476468373494, 0.9630788780120482, 0.8600338855421686, 0.9191453313253012, 0.9538780120481928, 0.8817065135542169, 0.9682558358433735, 0.9572195030120482, 0.9530073418674698, 0.9608669051204819, 0.9631494728915663, 0.9470303087349398, 0.9523013930722891, 0.9666321536144579, 0.9552663780120482, 0.8936841114457831, 0.9532897213855421, 0.9531955948795181, 0.927757906626506, 0.9612434111445783, 0.9615493222891566, 0.9621846762048193, 0.9663262424698795, 0.9654555722891566, 0.9656202936746988, 0.9323936370481928, 0.9569135918674698, 0.959996234939759, 0.9664439006024096, 0.9631730045180723, 0.9670557228915663, 0.9518778237951807, 0.9585372740963856, 0.9682323042168675, 0.9679499246987951, 0.9700677710843374, 0.973668109939759, 0.9663968373493976, 0.9483010165662651, 0.9714561370481928, 0.9633377259036144, 0.9742799322289156, 0.9701383659638554, 0.9515483810240963, 0.9522307981927711, 0.9647966867469879, 0.9757388930722891, 0.9707737198795181, 0.9582313629518072, 0.9626317771084337, 0.9747976280120482, 0.9619964231927711, 0.926793109939759, 0.9691265060240963, 0.9682558358433735, 0.920863140060241, 0.9621846762048193, 0.9604198042168675, 0.9694794804216867, 0.8916603915662651, 0.9698089231927711, 0.9709384412650602, 0.9548663403614458, 0.8742705195783133, 0.9674086972891566, 0.9151214231927711, 0.9430534638554217, 0.9740916792168675, 0.9685852786144579, 0.8322665662650602, 0.8930487575301205, 0.9651261295180723, 0.950230609939759, 0.9614551957831325, 0.8985316265060241, 0.9675263554216867, 0.9700677710843374, 0.9676440135542169, 0.9663027108433735, 0.9742093373493976, 0.9656673569277109, 0.9768919427710844, 0.9685617469879518, 0.9485833960843374, 0.9594314759036144, 0.9770566641566265, 0.9700677710843374, 0.9746564382530121, 0.9606080572289156, 0.9675734186746988, 0.9694088855421686, 0.9742093373493976, 0.9568429969879518, 0.9787744728915663, 0.9724679969879518, 0.9718561746987951, 0.9699971762048193, 0.9704913403614458, 0.8985551581325302, 0.9645849021084337, 0.9615257906626506, 0.9681381777108434, 0.9572665662650602, 0.9693147590361446, 0.9722091490963856, 0.9705619352409639, 0.974632906626506, 0.9065323795180723, 0.9770801957831325, 0.9825865963855421, 0.9708913780120482, 0.9734798569277109, 0.9605139307228916, 0.9783038403614458, 0.9733621987951807, 0.9744211219879518, 0.9749388177710844, 0.977527296686747, 0.9739975527108434, 0.9718326430722891, 0.9771743222891566, 0.9363469503012049, 0.9729150978915663, 0.9743034638554217, 0.9738798945783133, 0.9584902108433735, 0.977550828313253, 0.9589373117469879, 0.9668910015060241, 0.9647731551204819, 0.9310052710843374, 0.9645378388554217, 0.9772919804216867, 0.9624435240963856, 0.8970020707831325, 0.9634318524096386, 0.9801628388554217, 0.9661615210843374, 0.9749388177710844, 0.9728445030120482, 0.9711502259036144, 0.9675734186746988, 0.9811041039156626, 0.9673145707831325, 0.975597703313253, 0.9613375376506024, 0.9643025225903614, 0.9743975903614458, 0.9793862951807228, 0.9755271084337349, 0.9646084337349398, 0.9577371987951807, 0.9340643825301205, 0.9770566641566265, 0.9744917168674698, 0.9767742846385542, 0.8482680722891566, 0.9508894954819277, 0.975550640060241, 0.9732916039156626], "end": "2016-01-20 07:44:51.217000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0, 211.0, 212.0, 213.0, 214.0, 215.0, 216.0, 217.0, 218.0, 219.0, 220.0, 221.0, 222.0, 223.0, 224.0, 225.0, 226.0, 227.0, 228.0, 229.0, 230.0, 231.0, 232.0, 233.0, 234.0, 235.0, 236.0], "accuracy_valid": [0.33800747863247865, 0.5100160256410257, 0.5787927350427351, 0.5988247863247863, 0.6100427350427351, 0.7374465811965812, 0.6537126068376068, 0.8028846153846154, 0.8138354700854701, 0.8020833333333334, 0.7361111111111112, 0.8032852564102564, 0.8209134615384616, 0.8062232905982906, 0.8247863247863247, 0.8063568376068376, 0.8287927350427351, 0.8334668803418803, 0.8094284188034188, 0.7634882478632479, 0.8470886752136753, 0.8299946581196581, 0.8448183760683761, 0.7183493589743589, 0.8517628205128205, 0.8460202991452992, 0.8472222222222222, 0.8509615384615384, 0.7374465811965812, 0.8396100427350427, 0.7847222222222222, 0.8167735042735043, 0.8059561965811965, 0.8556356837606838, 0.8492254273504274, 0.859642094017094, 0.8142361111111112, 0.8500267094017094, 0.859375, 0.828926282051282, 0.8532318376068376, 0.8421474358974359, 0.7944711538461539, 0.8575053418803419, 0.8249198717948718, 0.8621794871794872, 0.8604433760683761, 0.8485576923076923, 0.8307959401709402, 0.8453525641025641, 0.8669871794871795, 0.860176282051282, 0.8482905982905983, 0.8649839743589743, 0.8617788461538461, 0.8636485042735043, 0.8530982905982906, 0.8616452991452992, 0.8729967948717948, 0.8306623931623932, 0.8524305555555556, 0.8632478632478633, 0.8534989316239316, 0.8269230769230769, 0.8667200854700855, 0.8624465811965812, 0.8688568376068376, 0.8526976495726496, 0.8723290598290598, 0.8628472222222222, 0.8530982905982906, 0.8660523504273504, 0.8655181623931624, 0.8691239316239316, 0.8667200854700855, 0.8575053418803419, 0.8660523504273504, 0.8477564102564102, 0.8705929487179487, 0.7837873931623932, 0.8318643162393162, 0.8583066239316239, 0.8063568376068376, 0.8768696581196581, 0.8623130341880342, 0.8597756410256411, 0.8687232905982906, 0.8716613247863247, 0.8584401709401709, 0.8603098290598291, 0.8731303418803419, 0.8632478632478633, 0.813034188034188, 0.8629807692307693, 0.8651175213675214, 0.8414797008547008, 0.8659188034188035, 0.8709935897435898, 0.8680555555555556, 0.8712606837606838, 0.8719284188034188, 0.8729967948717948, 0.8426816239316239, 0.8657852564102564, 0.8633814102564102, 0.8717948717948718, 0.8655181623931624, 0.8724626068376068, 0.8628472222222222, 0.8639155982905983, 0.874465811965812, 0.8748664529914529, 0.8770032051282052, 0.8800747863247863, 0.8759348290598291, 0.8607104700854701, 0.8780715811965812, 0.8754006410256411, 0.8798076923076923, 0.8766025641025641, 0.858840811965812, 0.8587072649572649, 0.8713942307692307, 0.8812767094017094, 0.8824786324786325, 0.8673878205128205, 0.8655181623931624, 0.8792735042735043, 0.8675213675213675, 0.843215811965812, 0.8715277777777778, 0.8727297008547008, 0.8382745726495726, 0.8717948717948718, 0.8720619658119658, 0.8717948717948718, 0.8055555555555556, 0.8782051282051282, 0.8787393162393162, 0.8649839743589743, 0.7887286324786325, 0.8754006410256411, 0.8353365384615384, 0.8494925213675214, 0.8799412393162394, 0.8772702991452992, 0.7616185897435898, 0.8076923076923077, 0.8691239316239316, 0.8555021367521367, 0.8677884615384616, 0.8151709401709402, 0.8767361111111112, 0.8802083333333334, 0.8716613247863247, 0.8701923076923077, 0.8852831196581197, 0.8751335470085471, 0.8818108974358975, 0.8729967948717948, 0.8608440170940171, 0.8615117521367521, 0.8802083333333334, 0.8783386752136753, 0.8828792735042735, 0.8645833333333334, 0.8677884615384616, 0.8778044871794872, 0.8770032051282052, 0.8608440170940171, 0.8808760683760684, 0.8806089743589743, 0.8729967948717948, 0.8791399572649573, 0.8727297008547008, 0.8103632478632479, 0.8745993589743589, 0.8656517094017094, 0.8723290598290598, 0.8639155982905983, 0.8704594017094017, 0.8784722222222222, 0.8794070512820513, 0.8850160256410257, 0.8247863247863247, 0.8838141025641025, 0.8891559829059829, 0.8733974358974359, 0.8743322649572649, 0.8700587606837606, 0.8818108974358975, 0.8812767094017094, 0.8819444444444444, 0.8826121794871795, 0.8826121794871795, 0.8834134615384616, 0.8820779914529915, 0.8848824786324786, 0.8454861111111112, 0.8820779914529915, 0.8791399572649573, 0.8832799145299145, 0.8640491452991453, 0.8839476495726496, 0.8664529914529915, 0.8740651709401709, 0.8704594017094017, 0.8421474358974359, 0.8661858974358975, 0.8870192307692307, 0.8715277777777778, 0.811965811965812, 0.8696581196581197, 0.8819444444444444, 0.8705929487179487, 0.8842147435897436, 0.8774038461538461, 0.8783386752136753, 0.8731303418803419, 0.8852831196581197, 0.8778044871794872, 0.8780715811965812, 0.8667200854700855, 0.8717948717948718, 0.8756677350427351, 0.8842147435897436, 0.8823450854700855, 0.8683226495726496, 0.8664529914529915, 0.8424145299145299, 0.8775373931623932, 0.8818108974358975, 0.8867521367521367, 0.7705662393162394, 0.8581730769230769, 0.8824786324786325, 0.8799412393162394], "accuracy_test": 0.8828125, "start": "2016-01-18 15:24:17.904000", "learning_rate_per_epoch": [0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276, 0.0006676187040284276], "accuracy_train_last": 0.9732916039156626, "error_valid": [0.6619925213675213, 0.48998397435897434, 0.4212072649572649, 0.4011752136752137, 0.3899572649572649, 0.26255341880341876, 0.3462873931623932, 0.19711538461538458, 0.18616452991452992, 0.19791666666666663, 0.26388888888888884, 0.1967147435897436, 0.17908653846153844, 0.19377670940170943, 0.17521367521367526, 0.19364316239316237, 0.1712072649572649, 0.16653311965811968, 0.19057158119658124, 0.23651175213675213, 0.15291132478632474, 0.1700053418803419, 0.15518162393162394, 0.2816506410256411, 0.14823717948717952, 0.1539797008547008, 0.1527777777777778, 0.14903846153846156, 0.26255341880341876, 0.1603899572649573, 0.2152777777777778, 0.18322649572649574, 0.19404380341880345, 0.14436431623931623, 0.1507745726495726, 0.14035790598290598, 0.18576388888888884, 0.14997329059829057, 0.140625, 0.17107371794871795, 0.14676816239316237, 0.1578525641025641, 0.20552884615384615, 0.1424946581196581, 0.1750801282051282, 0.13782051282051277, 0.13955662393162394, 0.1514423076923077, 0.16920405982905984, 0.1546474358974359, 0.13301282051282048, 0.13982371794871795, 0.15170940170940173, 0.13501602564102566, 0.13822115384615385, 0.13635149572649574, 0.14690170940170943, 0.1383547008547008, 0.12700320512820518, 0.1693376068376068, 0.14756944444444442, 0.1367521367521367, 0.14650106837606836, 0.17307692307692313, 0.1332799145299145, 0.13755341880341876, 0.13114316239316237, 0.1473023504273504, 0.12767094017094016, 0.1371527777777778, 0.14690170940170943, 0.1339476495726496, 0.13448183760683763, 0.13087606837606836, 0.1332799145299145, 0.1424946581196581, 0.1339476495726496, 0.15224358974358976, 0.12940705128205132, 0.2162126068376068, 0.16813568376068377, 0.14169337606837606, 0.19364316239316237, 0.12313034188034189, 0.13768696581196582, 0.14022435897435892, 0.13127670940170943, 0.12833867521367526, 0.1415598290598291, 0.1396901709401709, 0.1268696581196581, 0.1367521367521367, 0.18696581196581197, 0.13701923076923073, 0.1348824786324786, 0.1585202991452992, 0.13408119658119655, 0.12900641025641024, 0.13194444444444442, 0.12873931623931623, 0.12807158119658124, 0.12700320512820518, 0.15731837606837606, 0.1342147435897436, 0.13661858974358976, 0.1282051282051282, 0.13448183760683763, 0.1275373931623932, 0.1371527777777778, 0.13608440170940173, 0.12553418803418803, 0.12513354700854706, 0.12299679487179482, 0.1199252136752137, 0.12406517094017089, 0.13928952991452992, 0.12192841880341876, 0.12459935897435892, 0.12019230769230771, 0.1233974358974359, 0.14115918803418803, 0.1412927350427351, 0.12860576923076927, 0.11872329059829057, 0.11752136752136755, 0.13261217948717952, 0.13448183760683763, 0.12072649572649574, 0.13247863247863245, 0.15678418803418803, 0.1284722222222222, 0.1272702991452992, 0.1617254273504274, 0.1282051282051282, 0.12793803418803418, 0.1282051282051282, 0.19444444444444442, 0.1217948717948718, 0.12126068376068377, 0.13501602564102566, 0.21127136752136755, 0.12459935897435892, 0.16466346153846156, 0.1505074786324786, 0.12005876068376065, 0.12272970085470081, 0.23838141025641024, 0.1923076923076923, 0.13087606837606836, 0.1444978632478633, 0.13221153846153844, 0.18482905982905984, 0.12326388888888884, 0.11979166666666663, 0.12833867521367526, 0.1298076923076923, 0.11471688034188032, 0.12486645299145294, 0.11818910256410253, 0.12700320512820518, 0.13915598290598286, 0.13848824786324787, 0.11979166666666663, 0.12166132478632474, 0.11712072649572647, 0.13541666666666663, 0.13221153846153844, 0.12219551282051277, 0.12299679487179482, 0.13915598290598286, 0.11912393162393164, 0.11939102564102566, 0.12700320512820518, 0.1208600427350427, 0.1272702991452992, 0.18963675213675213, 0.12540064102564108, 0.13434829059829057, 0.12767094017094016, 0.13608440170940173, 0.12954059829059827, 0.12152777777777779, 0.12059294871794868, 0.11498397435897434, 0.17521367521367526, 0.11618589743589747, 0.11084401709401714, 0.1266025641025641, 0.1256677350427351, 0.12994123931623935, 0.11818910256410253, 0.11872329059829057, 0.11805555555555558, 0.11738782051282048, 0.11738782051282048, 0.11658653846153844, 0.11792200854700852, 0.1151175213675214, 0.15451388888888884, 0.11792200854700852, 0.1208600427350427, 0.1167200854700855, 0.13595085470085466, 0.1160523504273504, 0.13354700854700852, 0.1259348290598291, 0.12954059829059827, 0.1578525641025641, 0.13381410256410253, 0.11298076923076927, 0.1284722222222222, 0.18803418803418803, 0.13034188034188032, 0.11805555555555558, 0.12940705128205132, 0.11578525641025639, 0.12259615384615385, 0.12166132478632474, 0.1268696581196581, 0.11471688034188032, 0.12219551282051277, 0.12192841880341876, 0.1332799145299145, 0.1282051282051282, 0.1243322649572649, 0.11578525641025639, 0.1176549145299145, 0.1316773504273504, 0.13354700854700852, 0.15758547008547008, 0.12246260683760679, 0.11818910256410253, 0.11324786324786329, 0.22943376068376065, 0.14182692307692313, 0.11752136752136755, 0.12005876068376065], "accuracy_train_std": [0.05735760951999052, 0.06125733920594357, 0.06169014151551829, 0.06187038774958145, 0.05810960665002392, 0.056896261288626966, 0.05757886558500551, 0.048527189249782485, 0.047366206537464484, 0.049426723681066854, 0.05255171743283358, 0.047030956538989414, 0.04329532156167358, 0.04633739959347161, 0.043156335807011593, 0.0463242466206022, 0.04245428904005633, 0.03970035067244328, 0.046073740891344826, 0.05036954960027576, 0.036812610563388325, 0.03949299633546781, 0.03791188956576141, 0.05511651484135884, 0.03390472155302685, 0.03618619430389772, 0.03610987701844616, 0.03437376250541674, 0.054212654287108225, 0.03712163120761848, 0.04783997609625359, 0.04036809204235991, 0.041461417086477416, 0.03329937177942189, 0.032186646131123674, 0.03095202416978333, 0.04203902518335907, 0.03291198092894307, 0.03247309038672824, 0.03765883022626667, 0.033051105569405465, 0.036391300910530375, 0.04368572785253593, 0.031340372174099726, 0.03894937730661165, 0.03136814358993554, 0.02977285514671444, 0.031509884742291425, 0.040292185100833246, 0.032938418893404646, 0.028771539563680587, 0.03014348082728571, 0.03326202690019024, 0.0266267814678134, 0.029095711923292467, 0.0295752012593452, 0.03142468769586562, 0.031433461776885524, 0.026706353834265378, 0.03720211726694836, 0.03035246892350682, 0.027248759841147486, 0.029901725857173402, 0.03806099285856453, 0.027208656203374558, 0.028569991855591673, 0.02699953442929214, 0.031356057879064037, 0.02462509103431283, 0.027210986352364578, 0.029581154584404606, 0.027281102738820923, 0.02694377400424366, 0.024395283844349577, 0.02767987526632906, 0.030595024078251955, 0.02699277581912622, 0.03254674055412915, 0.02585095637720475, 0.04516856324825485, 0.03602525248603252, 0.027282888857974735, 0.04432988082424069, 0.022413759985518138, 0.026738017068511553, 0.028447381239273758, 0.0265972656151029, 0.024740190144414722, 0.029594058702457522, 0.02855183504421705, 0.02226121717113101, 0.028263031266864237, 0.03925286910846921, 0.02779338128508688, 0.029933847596202608, 0.03329987064766048, 0.02545115931771279, 0.026031339999917608, 0.02468869128452227, 0.022922159674461515, 0.023535203041456974, 0.023967709583150597, 0.03249634125586083, 0.02610877174265835, 0.026106152316284915, 0.02311792786022054, 0.025942388252712927, 0.023218359073084918, 0.02909787192774533, 0.0271604720234446, 0.023004208038132263, 0.023486543787361626, 0.023179404630466652, 0.020945725334627015, 0.0251838980535647, 0.029342862609631967, 0.02153316126491967, 0.02676500894853638, 0.02084555479476937, 0.022539895921136123, 0.028712894930554695, 0.02885077215789664, 0.0247015284287863, 0.019854767158867505, 0.021479980090110245, 0.025060541453429314, 0.024795858945859686, 0.02036103134293719, 0.02409462750007965, 0.03304127790107103, 0.02360643230298185, 0.021983111475682177, 0.034813186431509276, 0.02455429101781655, 0.02596266864440903, 0.02330813247778608, 0.04094254164297861, 0.02313711800714016, 0.022113507179815032, 0.027237672171588474, 0.04082773818453181, 0.023594689124711937, 0.03594972567593282, 0.031624965225304294, 0.021484555416970332, 0.02205483413096462, 0.04615309683593089, 0.03910494019032762, 0.02448316187127702, 0.027933844629680713, 0.024744487134735014, 0.03883582529793008, 0.023247913173290027, 0.022273402358808064, 0.022342594721717363, 0.02399339843539555, 0.0213604961487446, 0.02360285483059685, 0.020071635991524034, 0.022328029411478306, 0.029111342170141056, 0.025350454243500825, 0.020159655809153455, 0.022714736681322425, 0.02070653437118008, 0.025465308225758224, 0.022963925828180628, 0.024047802586200078, 0.020713820327813565, 0.027403546482952974, 0.018458817411584224, 0.02056653386975194, 0.020932939250935013, 0.022236166856304657, 0.022879857447499298, 0.04007517517865636, 0.023994217716739773, 0.025243096476934606, 0.023539108373825123, 0.027356181269435457, 0.023273242580224195, 0.02243114573561522, 0.0225587554810263, 0.02054390509387983, 0.03763283183789254, 0.020012621142840078, 0.016883839528098733, 0.022627576943843916, 0.021232869688597535, 0.025217253380541933, 0.01928738010164571, 0.021830451107153186, 0.021399954058795374, 0.02146258495281621, 0.01854601219486706, 0.021076023478347906, 0.021567900688640805, 0.018953056245146108, 0.031187484745375218, 0.02041773750743814, 0.020617572668981714, 0.020423228652401752, 0.025937446439790308, 0.019510939868181663, 0.026893746088158116, 0.02308661241074388, 0.02494218098154205, 0.03358245898766857, 0.025269932113317656, 0.01982138337659524, 0.02527783047870728, 0.039448376844417134, 0.0238502060535647, 0.01889246713025345, 0.022211412385442355, 0.019934926802522075, 0.02178205106039331, 0.021251703598122704, 0.02336077851121432, 0.0171710425344422, 0.023620303077520097, 0.01956856726315854, 0.024447706554946522, 0.024550050959375288, 0.020448644943785774, 0.01865320432743234, 0.0202483595590264, 0.024551144874326886, 0.02644586326858553, 0.03284418287138855, 0.019149467423785416, 0.020494304029529176, 0.019364670012851533, 0.04597402964234281, 0.02826875163604188, 0.01993848197351278, 0.02132571747041013], "accuracy_test_std": 0.04255243395408383, "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-3, 3], "momentum": 0.6630630792726219, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.0006676187033197515, "patience_threshold": 1, "do_flip": true, "batch_size": 64, "optimization": "rmsprop", "nb_data_augmentation": 2, "learning_rate_decay_method": "none", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 1.6367439341395816e-06, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.03547850169542808}, "accuracy_valid_max": 0.8891559829059829, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = 1234\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-10, -4], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128, 256],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-3, 3)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_optimizer.learning_rate = learning_rate\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8799412393162394, "loss_train": [5.871683120727539, 1.4289156198501587, 1.1430338621139526, 0.974642813205719, 0.861142635345459, 0.7825239300727844, 0.7268618941307068, 0.679068922996521, 0.6392353773117065, 0.6086350679397583, 0.5841240882873535, 0.5608744621276855, 0.5427638292312622, 0.5243011713027954, 0.5086850523948669, 0.4953520596027374, 0.4847651720046997, 0.4729979634284973, 0.4624127149581909, 0.45167505741119385, 0.4438503086566925, 0.433813214302063, 0.42942893505096436, 0.4210989773273468, 0.4164634346961975, 0.4075755178928375, 0.40292730927467346, 0.3985820710659027, 0.3924162685871124, 0.39254507422447205, 0.38157129287719727, 0.38033002614974976, 0.37679171562194824, 0.37203705310821533, 0.36762723326683044, 0.3655243515968323, 0.35916343331336975, 0.3594147861003876, 0.3576672673225403, 0.3537045121192932, 0.3493157625198364, 0.34710049629211426, 0.3477175235748291, 0.34301042556762695, 0.33952003717422485, 0.33761683106422424, 0.3378154933452606, 0.3351742923259735, 0.3293972313404083, 0.33176884055137634, 0.32773557305336, 0.3266250491142273, 0.3258255422115326, 0.3246145248413086, 0.32279983162879944, 0.32078883051872253, 0.3161631226539612, 0.3205890953540802, 0.312874436378479, 0.31575676798820496, 0.3147214651107788, 0.3089502453804016, 0.3093312382698059, 0.30977118015289307, 0.3050156533718109, 0.3090861141681671, 0.30412086844444275, 0.3073789179325104, 0.30200251936912537, 0.30051669478416443, 0.2998237907886505, 0.29548341035842896, 0.2964436709880829, 0.2943221628665924, 0.2938883602619171, 0.29322105646133423, 0.2957983911037445, 0.29034537076950073, 0.2941526174545288, 0.2886001765727997, 0.2887646555900574, 0.289610356092453, 0.2883758544921875, 0.2913883328437805, 0.28550904989242554, 0.2847249507904053, 0.28236114978790283, 0.2813628315925598, 0.28323960304260254, 0.2816298007965088, 0.2810046672821045, 0.2821124792098999, 0.28110188245773315, 0.2785215377807617, 0.2783992290496826, 0.2770192623138428, 0.275851845741272, 0.2763370871543884, 0.2754236161708832, 0.2713935673236847, 0.27474072575569153, 0.27364689111709595, 0.2685171365737915, 0.2725171744823456, 0.2687060832977295, 0.2707073390483856, 0.2717261016368866, 0.2734975814819336, 0.2672727108001709, 0.26967889070510864, 0.26632267236709595, 0.266000360250473, 0.26470205187797546, 0.2669946253299713, 0.2656268775463104, 0.2658293545246124, 0.2679228186607361, 0.2641615867614746, 0.2652585804462433, 0.26264822483062744, 0.2610950767993927, 0.2652345895767212, 0.2627096176147461, 0.26164138317108154, 0.25768256187438965, 0.2608475685119629, 0.257231205701828, 0.2571699917316437, 0.2592005133628845, 0.2588595747947693, 0.2599288523197174, 0.25664612650871277, 0.25620079040527344, 0.25754857063293457, 0.2575243413448334, 0.2545219659805298, 0.2543252408504486, 0.25254979729652405, 0.25447970628738403, 0.2525561451911926, 0.2517934739589691, 0.25567951798439026, 0.2525320053100586, 0.2526921331882477, 0.2515188157558441, 0.24985778331756592, 0.25129348039627075, 0.2509913742542267, 0.24821573495864868, 0.2527671754360199, 0.25022444128990173, 0.24954448640346527, 0.2492562234401703, 0.24493111670017242, 0.24787606298923492, 0.24820800125598907, 0.24776366353034973, 0.2468898445367813, 0.24485896527767181, 0.24769529700279236, 0.2479526549577713, 0.2444269359111786, 0.24617190659046173, 0.24620985984802246, 0.2457924783229828, 0.24240723252296448, 0.24241149425506592, 0.24456889927387238, 0.2458941638469696, 0.23884522914886475, 0.24333378672599792, 0.24171969294548035, 0.24087217450141907, 0.2418815940618515, 0.23981942236423492, 0.2380327433347702, 0.23954173922538757, 0.23888035118579865, 0.23863254487514496, 0.24021631479263306, 0.2395954132080078, 0.24051307141780853, 0.23820209503173828, 0.23591051995754242, 0.23674876987934113, 0.24096661806106567, 0.23717904090881348, 0.240144744515419, 0.23628486692905426, 0.23786763846874237, 0.2338845580816269, 0.23704291880130768, 0.2374304234981537, 0.23410236835479736, 0.23455099761486053, 0.23429569602012634, 0.2372218519449234, 0.23200605809688568, 0.23596632480621338, 0.23236671090126038, 0.23410336673259735, 0.23166513442993164, 0.23517794907093048, 0.2325030118227005, 0.23240984976291656, 0.23117601871490479, 0.22990626096725464, 0.23462574183940887, 0.2315099686384201, 0.2309419810771942, 0.23263035714626312, 0.22877225279808044, 0.2328835427761078, 0.230803981423378, 0.22990897297859192, 0.2316308319568634, 0.23049326241016388, 0.23012150824069977, 0.2297666221857071, 0.22964969277381897, 0.23030926287174225, 0.22939515113830566, 0.22706177830696106, 0.22860334813594818, 0.22984471917152405, 0.2280726432800293, 0.23082874715328217, 0.22796887159347534, 0.22737722098827362, 0.22875146567821503, 0.22703219950199127, 0.2287304401397705, 0.22836966812610626, 0.22727514803409576, 0.22833086550235748, 0.22884626686573029, 0.22899839282035828], "accuracy_train_first": 0.34264401355421686, "model": "residualv2", "loss_std": [56.01085662841797, 0.19179880619049072, 0.16566865146160126, 0.16114814579486847, 0.15248264372348785, 0.14602269232273102, 0.14180395007133484, 0.1398620456457138, 0.12625464797019958, 0.12618277966976166, 0.12217476218938828, 0.12032108753919601, 0.12151637673377991, 0.11526214331388474, 0.1101212128996849, 0.1143219918012619, 0.11011730134487152, 0.11282121390104294, 0.10401551425457001, 0.10425830632448196, 0.10016237199306488, 0.10100686550140381, 0.10365531593561172, 0.10206460952758789, 0.0999956950545311, 0.09773010015487671, 0.10032118856906891, 0.1004175916314125, 0.09412388503551483, 0.10342999547719955, 0.09660651534795761, 0.09035912901163101, 0.09191709011793137, 0.09116658568382263, 0.09020856022834778, 0.095370814204216, 0.08680461347103119, 0.08991962671279907, 0.09073768556118011, 0.09517532587051392, 0.08869639784097672, 0.08930595964193344, 0.09705790877342224, 0.08963656425476074, 0.08551457524299622, 0.08514407277107239, 0.08931031078100204, 0.0871940553188324, 0.08234179764986038, 0.08000200986862183, 0.08479607105255127, 0.08446820825338364, 0.0864616110920906, 0.08182389289140701, 0.07790627330541611, 0.08781129121780396, 0.08124177157878876, 0.08741205930709839, 0.0826033353805542, 0.08365591615438461, 0.08316070586442947, 0.08417505025863647, 0.07551541924476624, 0.08150696754455566, 0.07854543626308441, 0.08134246617555618, 0.08004403114318848, 0.08017365634441376, 0.07681077718734741, 0.0811246857047081, 0.0771966278553009, 0.07269062101840973, 0.07824426889419556, 0.07862524688243866, 0.08082762360572815, 0.0793403908610344, 0.08197086304426193, 0.08335481584072113, 0.0809214860200882, 0.07767508924007416, 0.07897312939167023, 0.07799038290977478, 0.07450683414936066, 0.08729484677314758, 0.08063671737909317, 0.0742383822798729, 0.07713846862316132, 0.07767438888549805, 0.07847540080547333, 0.07765553146600723, 0.0768446996808052, 0.07393363118171692, 0.08225645124912262, 0.06938842684030533, 0.07450257986783981, 0.07631520181894302, 0.07832631468772888, 0.07483205199241638, 0.07641454041004181, 0.07407420128583908, 0.07617145031690598, 0.0766700878739357, 0.07461664825677872, 0.07427015155553818, 0.07459826022386551, 0.0709521621465683, 0.0812402293086052, 0.08117201179265976, 0.07259900122880936, 0.07446930557489395, 0.07030833512544632, 0.07357073575258255, 0.06978102028369904, 0.07344602793455124, 0.0731721743941307, 0.07750784605741501, 0.07070896029472351, 0.0772104263305664, 0.07648830115795135, 0.06775148957967758, 0.07175251096487045, 0.07478240132331848, 0.06945275515317917, 0.07291214913129807, 0.07795459777116776, 0.07344112545251846, 0.07362301647663116, 0.07098493725061417, 0.07204137742519379, 0.07229312509298325, 0.07663194835186005, 0.07049345225095749, 0.07041304558515549, 0.06993979960680008, 0.07047741860151291, 0.07188324630260468, 0.07256058603525162, 0.06571976840496063, 0.07164088636636734, 0.06891148537397385, 0.06991259008646011, 0.07152347266674042, 0.07323028892278671, 0.06534422188997269, 0.06880536675453186, 0.06997177004814148, 0.06771066784858704, 0.07120268046855927, 0.06998768448829651, 0.072052001953125, 0.0692833885550499, 0.07365884631872177, 0.07165709137916565, 0.0700908750295639, 0.0687260702252388, 0.0750073492527008, 0.07065601646900177, 0.06968005001544952, 0.06685949116945267, 0.06986591219902039, 0.0684652179479599, 0.0736566036939621, 0.07475273311138153, 0.0748511552810669, 0.07483327388763428, 0.06562948226928711, 0.06934882700443268, 0.06700775027275085, 0.07590439915657043, 0.06137100234627724, 0.06842964887619019, 0.0750676766037941, 0.06813351064920425, 0.06733330339193344, 0.07165535539388657, 0.06459640711545944, 0.06310546398162842, 0.06783299893140793, 0.0676027312874794, 0.06275121122598648, 0.06788838654756546, 0.06517447531223297, 0.06530708074569702, 0.06601925939321518, 0.06423190981149673, 0.06926501542329788, 0.06900536268949509, 0.06857958436012268, 0.06679888069629669, 0.07270977646112442, 0.05946538597345352, 0.06909073144197464, 0.07306618243455887, 0.06396239995956421, 0.06867348402738571, 0.06959770619869232, 0.06602346897125244, 0.06685516238212585, 0.06794483214616776, 0.06236203387379646, 0.07216798514127731, 0.06303320825099945, 0.0665149912238121, 0.06541889905929565, 0.06778853386640549, 0.06603728234767914, 0.06193948909640312, 0.07226945459842682, 0.06878815591335297, 0.0665762722492218, 0.06693538278341293, 0.06418056786060333, 0.07103507965803146, 0.06616897881031036, 0.06431754678487778, 0.0659661516547203, 0.06368912756443024, 0.07011052966117859, 0.06418271362781525, 0.065398208796978, 0.06379740685224533, 0.06432876735925674, 0.06627333164215088, 0.06623432785272598, 0.06223507225513458, 0.06557086110115051, 0.06581363081932068, 0.06849443912506104, 0.07129497081041336, 0.06248297914862633, 0.06277267634868622, 0.06472785770893097, 0.06677485257387161, 0.06115337833762169, 0.06283413618803024, 0.06770873814821243, 0.06495290994644165]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:03 2016", "state": "available"}], "summary": "b2b5e594516aa671fd27f1af629641d3"}