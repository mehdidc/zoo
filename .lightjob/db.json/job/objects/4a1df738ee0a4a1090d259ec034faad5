{"content": {"hp_model": {"f0": 16, "f1": 32, "f2": 32, "f3": 32, "nonlin": "leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.04639412228627707, 0.0401366960064513, 0.04361479421069553, 0.039877287504532684, 0.04100141175863279, 0.03785884289331025, 0.03866192500196172, 0.03635581075319422, 0.0359715017927401, 0.03655265494199513, 0.03168846721468625, 0.03238954416481182, 0.031865467237101326, 0.03395171512863169, 0.029780506359925817, 0.031066281919417793, 0.030446876008948694, 0.03304393329239121, 0.025060034120710378, 0.029261475963285596, 0.030230086506692965, 0.02997089608695002, 0.029323415569370213, 0.029921517362843774, 0.03367483836099139, 0.03094075398157862, 0.029649535303363696, 0.02689955258683043, 0.03061683217613896, 0.032646205088538126, 0.030570277448776686, 0.03102771202857337, 0.03193031081679893, 0.030011126510828354, 0.0297753273268278, 0.03467401541640343, 0.03330835350644111, 0.030596380543145344, 0.032043471338791506, 0.032748029211676404, 0.0330491491012535, 0.030188344874385386, 0.03085149258610397, 0.03190956383585353, 0.030882058413333306, 0.02925682520655885, 0.032705895281580945, 0.033630628659803284, 0.0320052287289862, 0.030515923381490277, 0.030363333867654184, 0.03129670308049643, 0.0328520230799324, 0.029102621581381823, 0.029022712050667084, 0.031595288827959395, 0.029665441379363656, 0.033787794561903374, 0.029421322708945142, 0.030291243451008476, 0.03038603230265794, 0.029224559605116978, 0.028542370775447686, 0.03013300083229533, 0.026344448436041078, 0.02517849120383901, 0.028439205242316124, 0.030783492610411488, 0.031711075365435694, 0.025766481037108498, 0.02561711910892053, 0.027905633971074484, 0.027758631670490707, 0.02807320184306622, 0.028174809737785352, 0.029997823169791087, 0.03217794930053148, 0.028473635285484078, 0.028945089318344195, 0.028161927468474096, 0.02995787770115545, 0.031217469691532274, 0.026175864343806542, 0.023895944893897292, 0.027989057131069653, 0.027575010461043343, 0.02611201729184559, 0.024706762981056035, 0.026733111783249445, 0.028436334190343505, 0.027820982453758362, 0.03093840829687932, 0.029302370793480324, 0.028731176973684488, 0.024132052302328323, 0.02783011114255379, 0.02879803776133824, 0.0247994853627791, 0.025704793532912, 0.025055689705632246, 0.023685457586566018, 0.02840217833642758, 0.02812324525507627, 0.02690056431096226, 0.02678294940007881, 0.027014980898107055, 0.02743218934165476, 0.03001112651082836, 0.027150307873673472, 0.02122299468829301, 0.024571269458671486, 0.026020485651459833, 0.02510632777205292, 0.025417675297394554, 0.02380656275851106, 0.025317543371704162, 0.02749297072293419, 0.02404882964059462, 0.02676804174840387, 0.029272634764719564, 0.026265818410638155, 0.029012707883947848, 0.03149780289275465, 0.027942020209459625, 0.028071909219350506, 0.026883359822512616, 0.027537809866488395, 0.02733412700128566, 0.026239556041234426, 0.028767465081720397, 0.02658440334601055, 0.023282549251906128, 0.024256537922692528, 0.027123898525454607, 0.026042789128108392, 0.027217052425626446, 0.02517092376465103, 0.02334947115582945, 0.02525332242319816, 0.02781837370674585, 0.028283109899031905, 0.027287623409699566, 0.029774108599883515, 0.02533473695152747, 0.025275943935247527, 0.02825358547180971, 0.026924834285203536, 0.02302198055547116, 0.025960450110665455, 0.026715459889147693, 0.024578652430941907, 0.02688977062437628, 0.025796038641221052, 0.025779152731507674, 0.025250448400094626, 0.028176097640774972, 0.023490081722308865, 0.028741279134892807, 0.026992136380614904, 0.027868223667696538, 0.02838939924044585, 0.03291712800453864, 0.025313243151992175, 0.024607056193013073, 0.024305481828965435, 0.023347528458689377, 0.028638830137442336, 0.0270364639877034, 0.024994425656774143, 0.03017752467874234, 0.022903856545698768, 0.02267576563741268, 0.02448990986500757, 0.024723280529705002, 0.024674797700930038, 0.024391920138825858, 0.025633757856390688, 0.025179932362972685, 0.02794753897966141, 0.027697123147001554, 0.02532757437838624, 0.02565781183919326, 0.024023542290272017, 0.024765808299161876, 0.02488092648495435, 0.023902397871469288, 0.02924038660850034, 0.025031057110908264, 0.024578652430941907, 0.024175620134396853, 0.02679784875862373, 0.028729598190117046, 0.0276850016520845, 0.023801608417146156, 0.02693830815597892, 0.025980011797100522, 0.026345825815512175, 0.024354699864848204, 0.026315851543908843, 0.025121860402747342, 0.026714101568021682, 0.027468211941407843, 0.026987094545484355, 0.02234043085551502, 0.022992013247146406, 0.026301713895385614, 0.024175620134396857, 0.024681782157427494, 0.027931953762576713, 0.028334063281118548, 0.02683708923580231, 0.02832637807779077, 0.02583855623418839, 0.026271689295027486, 0.02538481837587028, 0.024834212664852404, 0.027657790865463997, 0.023539848491714144, 0.027718077542525622, 0.0252443400145854, 0.026528721987754744, 0.024993336772737025, 0.023935394699613335, 0.026919442848568547, 0.02628549796818144, 0.027119884728975453, 0.024908621364684937, 0.022897518351507547, 0.026435544710901718, 0.024957013452803753, 0.024402703399397055, 0.02817770743673243, 0.02384577999989742, 0.024284571314088226, 0.023044036793184137], "moving_avg_accuracy_train": [0.04966820406626505, 0.10634247929216864, 0.1655453925075301, 0.2222495128953313, 0.2785519524190512, 0.3325679921169051, 0.38203662588714227, 0.42705726676228345, 0.46814060258605505, 0.5050497162732327, 0.5396986414832589, 0.5709579753771016, 0.599813796815295, 0.62543576803738, 0.6501733471071359, 0.6726677782096754, 0.6937646110814789, 0.7126293962082708, 0.7290970665272027, 0.7449133576154463, 0.7600186897755884, 0.7733381686895958, 0.7844973864591904, 0.7959972901325485, 0.8075614353662816, 0.8157124830947136, 0.8233731624960855, 0.8322420774211757, 0.8395510963356846, 0.8467339761599475, 0.853276222369254, 0.859601932210642, 0.8644032224233127, 0.8705104340665236, 0.8756680691237266, 0.878530949711354, 0.8807216235655199, 0.8848981434378835, 0.8892500083109627, 0.8930607943774568, 0.8965093271385063, 0.8999895126475472, 0.9029193076177323, 0.9062338339342724, 0.9087133308119295, 0.911015472881339, 0.9140121936654942, 0.9161515428230411, 0.9187287831190503, 0.9201164469758198, 0.9223230816457078, 0.9246384956196912, 0.9247457115697704, 0.9254869724911066, 0.9268953535552489, 0.92856528732623, 0.9303717857020408, 0.9316376003547282, 0.9330121498072073, 0.9342398316638358, 0.9350906037685366, 0.9374329176386709, 0.938663270453117, 0.9401212092210584, 0.9417463247447356, 0.9433477653124307, 0.9446996416426334, 0.9449585931410207, 0.9458811261461957, 0.9468714209110942, 0.9480850694826354, 0.9483113893416008, 0.9497881382086455, 0.9505665721287448, 0.9510036084399667, 0.9520652393128375, 0.952347702580349, 0.9537384970813503, 0.9545784086683959, 0.9559955678015563, 0.9566568355695934, 0.9575720066813087, 0.9582097608324549, 0.9592026025202938, 0.9601455764550113, 0.9614672386890282, 0.9620166744586796, 0.9623276199646189, 0.9627580733296027, 0.9627124994303774, 0.9628197321680626, 0.9624950255175214, 0.9627981396826367, 0.9632521359553369, 0.9633054050405261, 0.9635039496268349, 0.9642521051159587, 0.9648972071043629, 0.965498977357782, 0.9662782400135701, 0.967200773692936, 0.9678592731308713, 0.9676377283479046, 0.9676030594287768, 0.9679742482148148, 0.9684706863451406, 0.968809235180506, 0.9689492077467927, 0.9693340309480171, 0.9705369230339382, 0.9709653466943998, 0.9717627314526707, 0.9722803589098132, 0.9726309186513621, 0.9728946528404427, 0.9737461890624226, 0.9739854632284696, 0.9741843378393575, 0.9746127602301206, 0.975012459357711, 0.9743344438436267, 0.9743807622604688, 0.9746154081729761, 0.9747159908496543, 0.9751053669152914, 0.9754534522117141, 0.9756208328941571, 0.9761997511107655, 0.9761207210298094, 0.9765061075111657, 0.9766435238684829, 0.9774472625960924, 0.9782576944690133, 0.9779328662871722, 0.9784147114355635, 0.978401271165501, 0.9788951048923243, 0.979525455095863, 0.979386821483867, 0.9792879360222273, 0.9791989391067515, 0.9781258072442691, 0.9776894501644205, 0.9782026964130387, 0.9783845916813734, 0.978948335073477, 0.979599247048057, 0.980220365264938, 0.9803040328047093, 0.9806758320844793, 0.9807751351712121, 0.9814480922866209, 0.9815266452567539, 0.9816867631105965, 0.9819791184260429, 0.9817104234509084, 0.9819674684552152, 0.9825353112181274, 0.9829192989216159, 0.9829236792704181, 0.9813910063734969, 0.9809175683867496, 0.981148006578195, 0.9817366132998935, 0.9820804595000246, 0.9825969993933957, 0.9831230675263453, 0.983116483665277, 0.9830870265638095, 0.9831993517688744, 0.9839899211100592, 0.9845202399930292, 0.9850375307527625, 0.9853995532798958, 0.9849770678314244, 0.9853427834880409, 0.9856225111633332, 0.9853400981494095, 0.9850906327621793, 0.985329686955841, 0.9849777235313413, 0.9850562877745928, 0.9855599775212298, 0.9858179857932032, 0.9864478777259311, 0.9868735907063501, 0.9872567323887271, 0.9867850124631075, 0.9871934841083629, 0.9875940528662013, 0.9873592145976534, 0.9870066703969242, 0.9872423738391596, 0.9872662539251231, 0.9874289357615265, 0.9879047921853739, 0.9880530366114149, 0.9882005755707552, 0.9886180933148846, 0.9884643976882154, 0.9884790271965023, 0.9884380710129966, 0.9883494408695283, 0.988333209131973, 0.9884927346043179, 0.9884033444270186, 0.9879369745927505, 0.9882608411394996, 0.9884981982906098, 0.988582395780826, 0.9887452405400927, 0.9886329529318666, 0.9889319317350656, 0.9892363100977035, 0.989526722762632, 0.9898186852755254, 0.9898908453624307, 0.9896145808563082, 0.9899024638851351, 0.9899121233701156, 0.9899137574186462, 0.9899575849900346, 0.9901382195633203, 0.9902184299865063, 0.9903941585240004, 0.9908935227920822, 0.9908040763863679, 0.991008307301948, 0.9903590955476568, 0.9905772334326501, 0.9905900108423972, 0.990177941234061, 0.9903600718094501, 0.9905498741164569, 0.9908830644156545], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 1234423, "moving_var_accuracy_train": [0.022202374456513338, 0.04888989826229525, 0.07554577283473629, 0.09692941097185093, 0.11576615214156505, 0.13044912982916973, 0.13942852839009742, 0.1437273984943636, 0.14454522298660646, 0.14235124474650285, 0.13892105243574243, 0.13382326079154894, 0.1279348605902502, 0.12104974321497333, 0.11445229925756786, 0.10756106420745312, 0.1008106450016951, 0.09393250156244586, 0.08697990889779857, 0.0805333135821112, 0.07453352176091409, 0.06867684625168888, 0.06292991489758312, 0.057827153468293445, 0.05324800321634595, 0.048521159106351956, 0.04419721727573221, 0.04048541441569539, 0.036917668791559693, 0.033690245775531945, 0.030706430067147412, 0.02799591850540955, 0.025403798144225186, 0.023199100636297404, 0.021118601367117275, 0.01908050599773667, 0.017215646865380935, 0.01565107204304108, 0.014256413389598831, 0.012961470864640218, 0.011772355182012483, 0.010704124884407282, 0.009710965683072448, 0.00883874387709253, 0.00801020063228008, 0.007256879292021776, 0.0066120143819432915, 0.005992004277110035, 0.005452583357289391, 0.004924655520374916, 0.004476013097434586, 0.004076662064529387, 0.0036690993154160105, 0.0033071345936559115, 0.0029942729692868336, 0.00271994378155332, 0.0024773203308342496, 0.00224400887836545, 0.0020366124663046977, 0.0018465160443440851, 0.0016683787584769094, 0.0015509187910252298, 0.0014094508243548472, 0.0012876360109789586, 0.0011826414140687354, 0.0010874587796885988, 0.0009951610282292, 0.0008962484283129331, 0.0008142831897923755, 0.0007416810243056064, 0.0006807694075718813, 0.0006131534529217521, 0.0005714651925764383, 0.0005197723076304449, 0.0004695140835033385, 0.0004327062161450964, 0.0003901536640080259, 0.0003685470817033609, 0.0003380414367995074, 0.0003223123531978564, 0.0002940165934274742, 0.0002721527775581918, 0.000248598073018111, 0.0002326098772702962, 0.00021735168811727788, 0.00021133763885299004, 0.00019292079195244282, 0.000174498896726173, 0.0001587166179483894, 0.00014286364897616585, 0.00012868077381883233, 0.00011676160611710024, 0.00010591234927923227, 9.717612789194002e-05, 8.748405346167807e-05, 7.909042769028318e-05, 7.621901464440862e-05, 7.234252235895438e-05, 6.836741706416008e-05, 6.699592793809678e-05, 6.795595065036618e-05, 6.506294917317968e-05, 5.8998393073599345e-05, 5.310937117182083e-05, 4.9038464088561865e-05, 4.635267503487761e-05, 4.274894535673565e-05, 3.8650381694878176e-05, 3.611814359119528e-05, 4.5528873565422055e-05, 4.262790770446968e-05, 4.408751900852679e-05, 4.2090210767165024e-05, 3.898721888200132e-05, 3.571449849621147e-05, 3.8669074082683285e-05, 3.531743581325189e-05, 3.2141652229629303e-05, 3.0579398710830444e-05, 2.8959293373115818e-05, 3.0200709371855868e-05, 2.7199946996319153e-05, 2.4975480634994844e-05, 2.2568984445125304e-05, 2.1676609485031164e-05, 2.0599418898798973e-05, 1.8791623644614995e-05, 1.9928777993842756e-05, 1.799211197772178e-05, 1.7529605440060097e-05, 1.5946594193378968e-05, 2.0165898254375833e-05, 2.40605068147526e-05, 2.2604076262741076e-05, 2.2433241359720316e-05, 2.019154299148243e-05, 2.036723444006872e-05, 2.1906583407972868e-05, 1.9888898572550924e-05, 1.7988013726009137e-05, 1.6260496412085995e-05, 2.499895471935151e-05, 2.4212726757622383e-05, 2.4162249487345893e-05, 2.2043797536394235e-05, 2.2699677292019283e-05, 2.424288715068225e-05, 2.5290688989687423e-05, 2.2824622405621058e-05, 2.1786272504996566e-05, 1.9696395181808856e-05, 2.1802597176243384e-05, 1.9677872580669627e-05, 1.7940824866675345e-05, 1.6915987054235526e-05, 1.587416125577428e-05, 1.4881394338348329e-05, 1.6295263535039534e-05, 1.5992756189408683e-05, 1.4393653257568481e-05, 3.409606381242664e-05, 3.2703749176841774e-05, 2.9911290099847785e-05, 3.0038281945321707e-05, 2.809852563489112e-05, 2.768999422439574e-05, 2.7411723926501768e-05, 2.4670941658890687e-05, 2.22116569804434e-05, 2.0104043847634768e-05, 2.3718638411864245e-05, 2.3877917629388608e-05, 2.389843343739811e-05, 2.268813288502671e-05, 2.202576518405562e-05, 2.1026920139100698e-05, 1.9628456276110525e-05, 1.8383424642400612e-05, 1.7105178992993353e-05, 1.5908983261258696e-05, 1.5432989204803205e-05, 1.3945241347181882e-05, 1.483404746026997e-05, 1.3949757129903311e-05, 1.6125656039153786e-05, 1.6144174310512964e-05, 1.5850934818434043e-05, 1.626851853063051e-05, 1.61433084423672e-05, 1.5973075565936325e-05, 1.4872109120713629e-05, 1.4503484929852385e-05, 1.3553141451001222e-05, 1.2202959632451742e-05, 1.122085208826665e-05, 1.2136720904491097e-05, 1.1120836502711798e-05, 1.0204662553149976e-05, 1.0753085897800273e-05, 9.89037841893533e-06, 8.903266779656247e-06, 8.028036782396756e-06, 7.2959308251380395e-06, 6.5687089663607945e-06, 6.140873456666392e-06, 5.5987015451780805e-06, 6.996338791497771e-06, 7.240710773276314e-06, 7.023685450597373e-06, 6.385119861765894e-06, 5.985273616175271e-06, 5.5002228172080605e-06, 5.754695458347339e-06, 6.013041601292466e-06, 6.170793084720764e-06, 6.320892756664099e-06, 5.735667184277307e-06, 5.84899916193798e-06, 6.009988990323752e-06, 5.409829842142167e-06, 4.868870888959353e-06, 4.399271504187664e-06, 4.2530039953638625e-06, 3.885607003716634e-06, 3.774970973352807e-06, 5.641755926150228e-06, 5.149586268991976e-06, 5.0100200440008485e-06, 8.302301156789342e-06, 7.900328272934808e-06, 7.11176480543992e-06, 7.928800583924357e-06, 7.434464443955951e-06, 7.015242241266295e-06, 7.312859996454651e-06], "duration": 158053.224541, "accuracy_train": [0.4966820406626506, 0.6164109563253012, 0.6983716114457831, 0.7325865963855421, 0.7852739081325302, 0.8187123493975904, 0.8272543298192772, 0.8322430346385542, 0.837890625, 0.8372317394578314, 0.851538968373494, 0.8522919804216867, 0.8595161897590361, 0.8560335090361446, 0.8728115587349398, 0.8751176581325302, 0.8836361069277109, 0.8824124623493976, 0.8773060993975904, 0.8872599774096386, 0.8959666792168675, 0.8932134789156626, 0.8849303463855421, 0.8994964231927711, 0.9116387424698795, 0.8890719126506024, 0.8923192771084337, 0.9120623117469879, 0.9053322665662651, 0.9113798945783133, 0.9121564382530121, 0.9165333207831325, 0.9076148343373494, 0.9254753388554217, 0.9220867846385542, 0.904296875, 0.9004376882530121, 0.9224868222891566, 0.9284167921686747, 0.9273578689759037, 0.9275461219879518, 0.9313111822289156, 0.9292874623493976, 0.9360645707831325, 0.9310288027108434, 0.9317347515060241, 0.9409826807228916, 0.9354056852409639, 0.9419239457831325, 0.932605421686747, 0.9421827936746988, 0.9454772213855421, 0.9257106551204819, 0.9321583207831325, 0.9395707831325302, 0.9435946912650602, 0.9466302710843374, 0.9430299322289156, 0.9453830948795181, 0.945288968373494, 0.9427475527108434, 0.9585137424698795, 0.9497364457831325, 0.9532426581325302, 0.9563723644578314, 0.9577607304216867, 0.9568665286144579, 0.947289156626506, 0.9541839231927711, 0.9557840737951807, 0.959007906626506, 0.9503482680722891, 0.9630788780120482, 0.9575724774096386, 0.9549369352409639, 0.9616199171686747, 0.9548898719879518, 0.9662556475903614, 0.9621376129518072, 0.96875, 0.9626082454819277, 0.965808546686747, 0.9639495481927711, 0.9681381777108434, 0.9686323418674698, 0.9733621987951807, 0.9669615963855421, 0.9651261295180723, 0.9666321536144579, 0.9623023343373494, 0.9637848268072289, 0.9595726656626506, 0.9655261671686747, 0.9673381024096386, 0.9637848268072289, 0.9652908509036144, 0.9709855045180723, 0.970703125, 0.9709149096385542, 0.9732916039156626, 0.9755035768072289, 0.9737857680722891, 0.9656438253012049, 0.9672910391566265, 0.9713149472891566, 0.9729386295180723, 0.9718561746987951, 0.9702089608433735, 0.9727974397590361, 0.9813629518072289, 0.9748211596385542, 0.9789391942771084, 0.9769390060240963, 0.9757859563253012, 0.9752682605421686, 0.981410015060241, 0.9761389307228916, 0.9759742093373494, 0.9784685617469879, 0.9786097515060241, 0.9682323042168675, 0.9747976280120482, 0.9767272213855421, 0.975621234939759, 0.9786097515060241, 0.9785862198795181, 0.9771272590361446, 0.981410015060241, 0.9754094503012049, 0.9799745858433735, 0.9778802710843374, 0.9846809111445783, 0.9855515813253012, 0.9750094126506024, 0.9827513177710844, 0.9782803087349398, 0.9833396084337349, 0.9851986069277109, 0.9781391189759037, 0.9783979668674698, 0.9783979668674698, 0.9684676204819277, 0.9737622364457831, 0.9828219126506024, 0.9800216490963856, 0.9840220256024096, 0.9854574548192772, 0.9858104292168675, 0.9810570406626506, 0.9840220256024096, 0.9816688629518072, 0.9875047063253012, 0.9822336219879518, 0.9831278237951807, 0.9846103162650602, 0.9792921686746988, 0.9842808734939759, 0.9876458960843374, 0.9863751882530121, 0.9829631024096386, 0.9675969503012049, 0.9766566265060241, 0.9832219503012049, 0.9870340737951807, 0.9851750753012049, 0.9872458584337349, 0.9878576807228916, 0.9830572289156626, 0.9828219126506024, 0.9842102786144579, 0.9911050451807228, 0.989293109939759, 0.9896931475903614, 0.9886577560240963, 0.9811746987951807, 0.9886342243975904, 0.9881400602409639, 0.9827983810240963, 0.9828454442771084, 0.9874811746987951, 0.9818100527108434, 0.9857633659638554, 0.9900931852409639, 0.9881400602409639, 0.9921169051204819, 0.9907050075301205, 0.9907050075301205, 0.9825395331325302, 0.9908697289156626, 0.991199171686747, 0.9852456701807228, 0.9838337725903614, 0.9893637048192772, 0.9874811746987951, 0.9888930722891566, 0.9921875, 0.9893872364457831, 0.9895284262048193, 0.9923757530120482, 0.9870811370481928, 0.9886106927710844, 0.9880694653614458, 0.9875517695783133, 0.9881871234939759, 0.9899284638554217, 0.9875988328313253, 0.9837396460843374, 0.991175640060241, 0.9906344126506024, 0.9893401731927711, 0.990210843373494, 0.9876223644578314, 0.9916227409638554, 0.9919757153614458, 0.9921404367469879, 0.9924463478915663, 0.9905402861445783, 0.9871282003012049, 0.9924934111445783, 0.9899990587349398, 0.9899284638554217, 0.9903520331325302, 0.9917639307228916, 0.9909403237951807, 0.9919757153614458, 0.9953878012048193, 0.9899990587349398, 0.9928463855421686, 0.9845161897590361, 0.9925404743975904, 0.9907050075301205, 0.9864693147590361, 0.9919992469879518, 0.9922580948795181, 0.9938817771084337], "end": "2016-01-22 08:53:43.942000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0, 211.0, 212.0, 213.0, 214.0, 215.0, 216.0, 217.0, 218.0, 219.0, 220.0, 221.0, 222.0, 223.0, 224.0, 225.0, 226.0, 227.0, 228.0, 229.0, 230.0, 231.0, 232.0, 233.0, 234.0], "accuracy_valid": [0.49137931034482757, 0.6089709051724138, 0.6845366379310345, 0.724542025862069, 0.7672413793103449, 0.7991648706896551, 0.8022629310344828, 0.8023976293103449, 0.8137122844827587, 0.8095366379310345, 0.8169450431034483, 0.8217941810344828, 0.8188308189655172, 0.8174838362068966, 0.8333782327586207, 0.830145474137931, 0.8374191810344828, 0.8371497844827587, 0.8333782327586207, 0.8389008620689655, 0.8413254310344828, 0.8415948275862069, 0.8332435344827587, 0.8411907327586207, 0.8527747844827587, 0.8353987068965517, 0.8351293103448276, 0.8527747844827587, 0.8463092672413793, 0.8490032327586207, 0.8483297413793104, 0.8511584051724138, 0.8441540948275862, 0.8562769396551724, 0.8556034482758621, 0.83984375, 0.8372844827586207, 0.8537176724137931, 0.8599137931034483, 0.8542564655172413, 0.8596443965517241, 0.8616648706896551, 0.8634159482758621, 0.8615301724137931, 0.8607219827586207, 0.8622036637931034, 0.8634159482758621, 0.8619342672413793, 0.8643588362068966, 0.8608566810344828, 0.8654364224137931, 0.8646282327586207, 0.8572198275862069, 0.8597790948275862, 0.8635506465517241, 0.865301724137931, 0.8685344827586207, 0.8675915948275862, 0.8665140086206896, 0.8669181034482759, 0.8622036637931034, 0.8762122844827587, 0.8662446120689655, 0.8745959051724138, 0.8733836206896551, 0.8786368534482759, 0.8756734913793104, 0.86328125, 0.8693426724137931, 0.8739224137931034, 0.8762122844827587, 0.8671875, 0.8794450431034483, 0.8735183189655172, 0.8706896551724138, 0.872979525862069, 0.8674568965517241, 0.8713631465517241, 0.8759428879310345, 0.8798491379310345, 0.8754040948275862, 0.8782327586206896, 0.8776939655172413, 0.880926724137931, 0.8791756465517241, 0.8821390086206896, 0.8747306034482759, 0.8782327586206896, 0.880792025862069, 0.8725754310344828, 0.8733836206896551, 0.8693426724137931, 0.8752693965517241, 0.8786368534482759, 0.8721713362068966, 0.875, 0.8810614224137931, 0.8786368534482759, 0.8780980603448276, 0.8821390086206896, 0.8855064655172413, 0.8821390086206896, 0.8728448275862069, 0.8779633620689655, 0.8779633620689655, 0.8811961206896551, 0.8801185344827587, 0.8780980603448276, 0.8787715517241379, 0.8883351293103449, 0.8822737068965517, 0.8877963362068966, 0.8826778017241379, 0.8825431034482759, 0.8834859913793104, 0.8855064655172413, 0.8814655172413793, 0.8771551724137931, 0.8817349137931034, 0.8814655172413793, 0.8728448275862069, 0.8780980603448276, 0.8802532327586207, 0.8825431034482759, 0.8832165948275862, 0.8853717672413793, 0.880926724137931, 0.8828125, 0.8841594827586207, 0.8852370689655172, 0.8806573275862069, 0.888604525862069, 0.892645474137931, 0.880792025862069, 0.88671875, 0.8830818965517241, 0.8873922413793104, 0.8923760775862069, 0.8805226293103449, 0.8820043103448276, 0.8837553879310345, 0.8728448275862069, 0.8803879310344828, 0.8830818965517241, 0.8855064655172413, 0.8899515086206896, 0.8895474137931034, 0.8876616379310345, 0.8869881465517241, 0.8872575431034483, 0.8873922413793104, 0.8879310344827587, 0.8871228448275862, 0.8890086206896551, 0.8859105603448276, 0.8852370689655172, 0.8871228448275862, 0.8907596982758621, 0.8871228448275862, 0.8851023706896551, 0.8719019396551724, 0.8798491379310345, 0.8838900862068966, 0.8934536637931034, 0.8898168103448276, 0.890625, 0.8934536637931034, 0.884698275862069, 0.8876616379310345, 0.8856411637931034, 0.89453125, 0.892510775862069, 0.8908943965517241, 0.8902209051724138, 0.8821390086206896, 0.8914331896551724, 0.8930495689655172, 0.88671875, 0.8872575431034483, 0.8826778017241379, 0.8849676724137931, 0.8927801724137931, 0.8942618534482759, 0.8910290948275862, 0.8970905172413793, 0.8921066810344828, 0.8943965517241379, 0.8856411637931034, 0.8938577586206896, 0.8956088362068966, 0.8852370689655172, 0.8863146551724138, 0.8949353448275862, 0.8863146551724138, 0.8927801724137931, 0.8956088362068966, 0.8927801724137931, 0.8935883620689655, 0.8966864224137931, 0.8900862068965517, 0.8915678879310345, 0.8907596982758621, 0.8888739224137931, 0.8919719827586207, 0.8941271551724138, 0.8898168103448276, 0.8877963362068966, 0.8895474137931034, 0.8934536637931034, 0.8917025862068966, 0.8930495689655172, 0.8865840517241379, 0.8948006465517241, 0.8953394396551724, 0.8949353448275862, 0.8952047413793104, 0.8921066810344828, 0.8899515086206896, 0.8923760775862069, 0.8927801724137931, 0.8880657327586207, 0.8894127155172413, 0.8930495689655172, 0.892510775862069, 0.8912984913793104, 0.8969558189655172, 0.8927801724137931, 0.8939924568965517, 0.8845635775862069, 0.8937230603448276, 0.8937230603448276, 0.88671875, 0.8949353448275862, 0.8930495689655172, 0.892510775862069], "accuracy_test": 0.27904647435897434, "start": "2016-01-20 12:59:30.717000", "learning_rate_per_epoch": [0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313, 0.0021512771490961313], "accuracy_train_last": 0.9938817771084337, "error_valid": [0.5086206896551724, 0.3910290948275862, 0.3154633620689655, 0.27545797413793105, 0.23275862068965514, 0.20083512931034486, 0.19773706896551724, 0.19760237068965514, 0.18628771551724133, 0.19046336206896552, 0.1830549568965517, 0.17820581896551724, 0.18116918103448276, 0.18251616379310343, 0.16662176724137934, 0.16985452586206895, 0.16258081896551724, 0.16285021551724133, 0.16662176724137934, 0.16109913793103448, 0.15867456896551724, 0.15840517241379315, 0.16675646551724133, 0.15880926724137934, 0.14722521551724133, 0.1646012931034483, 0.16487068965517238, 0.14722521551724133, 0.15369073275862066, 0.15099676724137934, 0.1516702586206896, 0.1488415948275862, 0.1558459051724138, 0.14372306034482762, 0.1443965517241379, 0.16015625, 0.16271551724137934, 0.14628232758620685, 0.1400862068965517, 0.14574353448275867, 0.1403556034482759, 0.13833512931034486, 0.1365840517241379, 0.13846982758620685, 0.13927801724137934, 0.13779633620689657, 0.1365840517241379, 0.13806573275862066, 0.13564116379310343, 0.13914331896551724, 0.13456357758620685, 0.13537176724137934, 0.14278017241379315, 0.1402209051724138, 0.1364493534482759, 0.13469827586206895, 0.13146551724137934, 0.1324084051724138, 0.1334859913793104, 0.1330818965517241, 0.13779633620689657, 0.12378771551724133, 0.13375538793103448, 0.1254040948275862, 0.12661637931034486, 0.12136314655172409, 0.12432650862068961, 0.13671875, 0.13065732758620685, 0.12607758620689657, 0.12378771551724133, 0.1328125, 0.12055495689655171, 0.12648168103448276, 0.1293103448275862, 0.12702047413793105, 0.1325431034482759, 0.1286368534482759, 0.12405711206896552, 0.12015086206896552, 0.12459590517241381, 0.12176724137931039, 0.12230603448275867, 0.11907327586206895, 0.12082435344827591, 0.11786099137931039, 0.1252693965517241, 0.12176724137931039, 0.11920797413793105, 0.12742456896551724, 0.12661637931034486, 0.13065732758620685, 0.12473060344827591, 0.12136314655172409, 0.12782866379310343, 0.125, 0.11893857758620685, 0.12136314655172409, 0.12190193965517238, 0.11786099137931039, 0.11449353448275867, 0.11786099137931039, 0.12715517241379315, 0.12203663793103448, 0.12203663793103448, 0.11880387931034486, 0.11988146551724133, 0.12190193965517238, 0.1212284482758621, 0.11166487068965514, 0.11772629310344829, 0.11220366379310343, 0.1173221982758621, 0.11745689655172409, 0.11651400862068961, 0.11449353448275867, 0.11853448275862066, 0.12284482758620685, 0.11826508620689657, 0.11853448275862066, 0.12715517241379315, 0.12190193965517238, 0.11974676724137934, 0.11745689655172409, 0.11678340517241381, 0.11462823275862066, 0.11907327586206895, 0.1171875, 0.11584051724137934, 0.11476293103448276, 0.11934267241379315, 0.11139547413793105, 0.10735452586206895, 0.11920797413793105, 0.11328125, 0.11691810344827591, 0.11260775862068961, 0.10762392241379315, 0.11947737068965514, 0.11799568965517238, 0.11624461206896552, 0.12715517241379315, 0.11961206896551724, 0.11691810344827591, 0.11449353448275867, 0.11004849137931039, 0.11045258620689657, 0.11233836206896552, 0.11301185344827591, 0.11274245689655171, 0.11260775862068961, 0.11206896551724133, 0.11287715517241381, 0.11099137931034486, 0.11408943965517238, 0.11476293103448276, 0.11287715517241381, 0.1092403017241379, 0.11287715517241381, 0.11489762931034486, 0.12809806034482762, 0.12015086206896552, 0.11610991379310343, 0.10654633620689657, 0.11018318965517238, 0.109375, 0.10654633620689657, 0.11530172413793105, 0.11233836206896552, 0.11435883620689657, 0.10546875, 0.10748922413793105, 0.10910560344827591, 0.10977909482758619, 0.11786099137931039, 0.10856681034482762, 0.10695043103448276, 0.11328125, 0.11274245689655171, 0.1173221982758621, 0.11503232758620685, 0.10721982758620685, 0.10573814655172409, 0.10897090517241381, 0.10290948275862066, 0.10789331896551724, 0.1056034482758621, 0.11435883620689657, 0.10614224137931039, 0.10439116379310343, 0.11476293103448276, 0.11368534482758619, 0.10506465517241381, 0.11368534482758619, 0.10721982758620685, 0.10439116379310343, 0.10721982758620685, 0.10641163793103448, 0.10331357758620685, 0.10991379310344829, 0.10843211206896552, 0.1092403017241379, 0.11112607758620685, 0.10802801724137934, 0.10587284482758619, 0.11018318965517238, 0.11220366379310343, 0.11045258620689657, 0.10654633620689657, 0.10829741379310343, 0.10695043103448276, 0.1134159482758621, 0.10519935344827591, 0.10466056034482762, 0.10506465517241381, 0.10479525862068961, 0.10789331896551724, 0.11004849137931039, 0.10762392241379315, 0.10721982758620685, 0.11193426724137934, 0.11058728448275867, 0.10695043103448276, 0.10748922413793105, 0.10870150862068961, 0.10304418103448276, 0.10721982758620685, 0.10600754310344829, 0.11543642241379315, 0.10627693965517238, 0.10627693965517238, 0.11328125, 0.10506465517241381, 0.10695043103448276, 0.10748922413793105], "accuracy_train_std": [0.04657818432610452, 0.046063427860564304, 0.04147549138171903, 0.04078605884531849, 0.03860548901675428, 0.03626076282599141, 0.03449607228087909, 0.03456496369076395, 0.033190664742081814, 0.031454902161030154, 0.03291463913975139, 0.0313618850267261, 0.03229694949471688, 0.03302133710120966, 0.031105383548376, 0.030424081680112496, 0.02799590152337998, 0.030614447122974793, 0.030796885270928335, 0.029561296139097748, 0.030092552748711695, 0.028407381077490832, 0.029349702652260404, 0.027356859360224348, 0.0264415391090518, 0.02896279626133331, 0.030106580352792883, 0.027685306106177036, 0.0274826328375341, 0.025085067048079765, 0.026897163781454694, 0.027339689379117797, 0.02543084117373787, 0.025295382085199488, 0.02553504608667565, 0.027794696195517086, 0.02825060702551705, 0.02452591604223762, 0.025399554107482174, 0.02477766297622528, 0.02427764874641996, 0.024257203696042142, 0.024262773028344016, 0.023025176477718477, 0.024858164694399266, 0.024034662360826783, 0.02256642494407335, 0.02383858296153589, 0.0215901871705367, 0.023263056999117036, 0.021471406783672323, 0.02147625466764708, 0.023667657926073005, 0.02370964058377323, 0.02284434981841596, 0.021748863128929697, 0.02306012953260561, 0.02190766624323722, 0.02261910811762771, 0.02252962457811441, 0.02208549394653496, 0.0190839459863213, 0.021427791077006316, 0.019615428063230207, 0.020610427323067795, 0.01890518337310833, 0.02049510107718649, 0.022315427402464984, 0.020060142274762386, 0.019744408905025785, 0.019468777100784127, 0.02188835953526451, 0.019555036480006012, 0.019467170042628594, 0.019775122641605417, 0.019697461384385904, 0.02077664060264204, 0.017140783062869026, 0.019821662738299387, 0.017553268486616758, 0.01813949589806656, 0.01746894824434584, 0.019273766836986818, 0.01660586050971517, 0.017931083956883906, 0.014891783045735778, 0.01662045959866424, 0.018274546607072593, 0.018795451202661868, 0.018411358719681303, 0.017786843415037376, 0.018411358719681303, 0.017836212288476913, 0.01727432964865194, 0.01821577416871533, 0.017107265891507625, 0.016190220769324536, 0.015269451047215835, 0.01635229640548381, 0.015617963735197056, 0.014571131998397834, 0.015893658421663744, 0.017254957258002825, 0.016980575606887944, 0.016175438770126362, 0.01608055697238003, 0.016312968057899044, 0.016452696680535984, 0.014495176318950224, 0.012824714857116792, 0.014812668245807669, 0.013815830550671824, 0.014433771052314304, 0.014875915576521983, 0.01556540164575634, 0.013244522567819196, 0.014018925757392577, 0.014547246938253756, 0.014098783094114617, 0.013868475430370815, 0.016103268176699572, 0.01547162836864427, 0.013645645084072662, 0.014150473150394806, 0.013396482841302676, 0.01391823599331138, 0.014491202818561455, 0.012501917113520125, 0.015398442635464167, 0.013851695611301274, 0.014505716100477694, 0.011444864859117393, 0.011714591062943711, 0.014495099915753299, 0.012867712204240506, 0.013965876603296623, 0.01196073478737426, 0.011354710839287699, 0.01451157458663896, 0.013282597454978852, 0.014487457561258415, 0.017244556477639086, 0.015767577010313658, 0.012388904119717402, 0.012919420154990464, 0.01160304869368604, 0.011078673764716015, 0.011046814229280073, 0.012770607417576447, 0.011001205270980727, 0.011672896475577994, 0.010234147580632146, 0.012188269496969268, 0.011963581669373311, 0.010708001936508303, 0.013743759223225889, 0.01164756087233112, 0.009989473403765228, 0.011223845830724872, 0.011766942716872675, 0.017394233866933034, 0.01336728951511802, 0.012674156755110074, 0.010454691447257327, 0.011135706697892973, 0.010084393359854836, 0.01009218761572305, 0.011746595751484871, 0.012179384302556431, 0.012088272348330183, 0.00899006688913667, 0.009309576594220936, 0.009608167986172299, 0.009277908894710491, 0.012793416249951246, 0.009666974325118856, 0.010457789467116828, 0.011847303453937103, 0.012114263250651345, 0.011209751609656801, 0.011984114702881894, 0.012176633350741484, 0.009731600802555262, 0.009363340341635495, 0.00843453723551524, 0.00869292172091047, 0.008714044394541128, 0.012716922921239462, 0.008876134508871843, 0.008365749716997956, 0.012149135151260716, 0.01215314538356627, 0.00906135687939172, 0.009885075855768614, 0.009615887566212457, 0.008575332811694559, 0.009742974348662828, 0.010384115109181379, 0.007546868141668562, 0.010477758935492076, 0.009874763252562797, 0.010007417286670582, 0.010327006507988283, 0.01038776725917719, 0.009589823514269602, 0.010383475184254282, 0.011904003049327075, 0.009466238412216294, 0.00864876113429159, 0.009343774499712638, 0.0091433617805579, 0.010206434406534013, 0.00764919038174324, 0.008475665909501917, 0.007975381680478516, 0.008074434540996225, 0.00884175619596948, 0.01137459045415654, 0.007699805655529105, 0.009125266209535675, 0.00968520125529776, 0.008949569774127386, 0.008146838054106589, 0.008645142961607552, 0.008166355938317908, 0.006363815039113468, 0.009538962120806578, 0.008265732940254617, 0.011278205586445749, 0.007863190795411064, 0.008839720566566197, 0.010737514953333723, 0.008110461065037605, 0.008280558358480222, 0.007722390043662903], "accuracy_test_std": 0.04184589549694987, "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-3, 3], "momentum": 0.6726644832685935, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.0021512771575941695, "patience_threshold": 1, "do_flip": true, "batch_size": 128, "optimization": "adam", "nb_data_augmentation": 4, "learning_rate_decay_method": "none", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 1.0904939046397225e-08, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.004263911763151307}, "accuracy_valid_max": 0.8970905172413793, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = 1234423\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-10, -4], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128, 256],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-3, 3)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_optimizer.learning_rate = learning_rate\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.892510775862069, "loss_train": [1.4874913692474365, 1.095205545425415, 0.8949370384216309, 0.7694337368011475, 0.6949624419212341, 0.6272215843200684, 0.5769426226615906, 0.5374925136566162, 0.5022493004798889, 0.4726823568344116, 0.44725894927978516, 0.43011099100112915, 0.4070846140384674, 0.38636279106140137, 0.3700248897075653, 0.3553435206413269, 0.3433072865009308, 0.32950639724731445, 0.31605663895606995, 0.30635973811149597, 0.29376810789108276, 0.28466883301734924, 0.27753791213035583, 0.2692462205886841, 0.2551030218601227, 0.24836254119873047, 0.24290525913238525, 0.23432111740112305, 0.22796320915222168, 0.22428683936595917, 0.2127504050731659, 0.2103928178548813, 0.20445659756660461, 0.19640026986598969, 0.1935385912656784, 0.18877197802066803, 0.18363851308822632, 0.20403940975666046, 0.20116540789604187, 0.16399052739143372, 0.16072911024093628, 0.15673451125621796, 0.15888704359531403, 0.14968113601207733, 0.14937610924243927, 0.14585335552692413, 0.14209716022014618, 0.13837024569511414, 0.13686983287334442, 0.13246871531009674, 0.12713515758514404, 0.12451813369989395, 0.12259066849946976, 0.12325457483530045, 0.11760975420475006, 0.11452671885490417, 0.11594844609498978, 0.1100982129573822, 0.10925982892513275, 0.11401049047708511, 0.10273715108633041, 0.1024911105632782, 0.09876600652933121, 0.10074598342180252, 0.09873561561107635, 0.09540511667728424, 0.10079469531774521, 0.09374794363975525, 0.08947901427745819, 0.08713966608047485, 0.09123092144727707, 0.08544997125864029, 0.08501069992780685, 0.08256588131189346, 0.08384351432323456, 0.08146373927593231, 0.07828718423843384, 0.07826314866542816, 0.08061730861663818, 0.08302905410528183, 0.07696640491485596, 0.0713522657752037, 0.07355684041976929, 0.07297534495592117, 0.06742332875728607, 0.07163997739553452, 0.06847397983074188, 0.065961554646492, 0.0667453482747078, 0.06485722213983536, 0.07009737938642502, 0.06019657477736473, 0.06094052642583847, 0.06245039775967598, 0.06605308502912521, 0.06286417692899704, 0.06238143518567085, 0.05840457230806351, 0.05982692167162895, 0.06079736724495888, 0.05664950981736183, 0.05505264550447464, 0.053904324769973755, 0.057072293013334274, 0.05691760778427124, 0.05580291524529457, 0.05274925008416176, 0.06070813909173012, 0.055294621735811234, 0.04997924715280533, 0.048961181193590164, 0.05154797062277794, 0.05316120386123657, 0.048617422580718994, 0.04949568957090378, 0.05046522617340088, 0.0502520389854908, 0.04642556607723236, 0.04643402621150017, 0.04329562932252884, 0.04914521425962448, 0.04625989496707916, 0.044002655893564224, 0.042870692908763885, 0.04475744068622589, 0.04594781622290611, 0.043647848069667816, 0.044386908411979675, 0.043893467634916306, 0.04641127586364746, 0.04170994460582733, 0.05634787678718567, 0.03622996807098389, 0.04029955714941025, 0.04025215283036232, 0.040174633264541626, 0.03760132938623428, 0.04017845541238785, 0.04196878522634506, 0.03937707096338272, 0.038139499723911285, 0.03847288712859154, 0.0363035649061203, 0.035432882606983185, 0.038705408573150635, 0.04071556404232979, 0.03528990224003792, 0.03521459922194481, 0.036402471363544464, 0.04590623080730438, 0.03190716728568077, 0.04288939759135246, 0.03482774645090103, 0.03219783678650856, 0.0348498560488224, 0.03199757635593414, 0.034592922776937485, 0.03357094153761864, 0.033848002552986145, 0.03181034326553345, 0.03846079856157303, 0.036474987864494324, 0.03172972798347473, 0.030774930492043495, 0.03347151726484299, 0.03132706135511398, 0.030270487070083618, 0.030960792675614357, 0.03994632884860039, 0.030623676255345345, 0.03194234520196915, 0.03326854854822159, 0.02926803193986416, 0.03313009440898895, 0.02641933225095272, 0.04273433983325958, 0.025803321972489357, 0.029297230765223503, 0.03103111870586872, 0.02411152794957161, 0.027774283662438393, 0.03480705991387367, 0.031132958829402924, 0.024685507640242577, 0.028631238266825676, 0.028510382398962975, 0.030093926936388016, 0.024917880073189735, 0.03448210656642914, 0.02561110444366932, 0.027732720598578453, 0.024925051257014275, 0.029698653146624565, 0.02555064484477043, 0.0259441826492548, 0.028994986787438393, 0.028085164725780487, 0.02665986865758896, 0.03122805617749691, 0.023297565057873726, 0.024249685928225517, 0.02714025415480137, 0.027185777202248573, 0.025347527116537094, 0.023934975266456604, 0.026668783277273178, 0.024511657655239105, 0.02597581222653389, 0.02463258057832718, 0.02353021502494812, 0.02455451525747776, 0.025413690134882927, 0.027407556772232056, 0.02659463696181774, 0.022011773660779, 0.021688338369131088, 0.020726054906845093, 0.029977329075336456, 0.023510700091719627, 0.024268178269267082, 0.02610243856906891, 0.02272847853600979, 0.02374630607664585, 0.02522619441151619, 0.021510038524866104, 0.022829167544841766, 0.02155468799173832, 0.021664131432771683, 0.026185814291238785, 0.02169811725616455, 0.020350340753793716, 0.020921409130096436, 0.02363307774066925, 0.026954691857099533, 0.01827031560242176], "accuracy_train_first": 0.4966820406626506, "model": "residualv3", "loss_std": [0.2874477505683899, 0.14754943549633026, 0.1167110800743103, 0.10161340981721878, 0.1043446883559227, 0.09182893484830856, 0.08403219282627106, 0.08486528694629669, 0.08004452288150787, 0.07678286731243134, 0.07466404139995575, 0.07877414673566818, 0.07161837071180344, 0.07161441445350647, 0.0677599236369133, 0.06552623212337494, 0.0665103867650032, 0.06370890885591507, 0.06161414831876755, 0.06013210117816925, 0.05736352875828743, 0.057182617485523224, 0.05623466894030571, 0.05759219825267792, 0.05207141116261482, 0.056316036731004715, 0.0518965870141983, 0.054029010236263275, 0.05064340680837631, 0.04950384795665741, 0.04922422021627426, 0.04644664376974106, 0.048371393233537674, 0.04523890092968941, 0.049250103533267975, 0.046182453632354736, 0.04296613112092018, 0.0825391560792923, 0.07874250411987305, 0.041124723851680756, 0.039630405604839325, 0.03749953210353851, 0.04148879274725914, 0.03857225552201271, 0.03642404079437256, 0.03679278492927551, 0.03818793594837189, 0.037602588534355164, 0.035313352942466736, 0.03715755417943001, 0.034614209085702896, 0.033195603638887405, 0.03096584789454937, 0.0325586274266243, 0.03170212730765343, 0.033533573150634766, 0.03238976374268532, 0.029643643647432327, 0.029600970447063446, 0.03442997485399246, 0.028976691886782646, 0.029350830242037773, 0.029033083468675613, 0.027036042883992195, 0.02743358165025711, 0.026733973994851112, 0.03089350275695324, 0.025416702032089233, 0.02742740511894226, 0.02632092870771885, 0.025548454374074936, 0.025981642305850983, 0.02590085007250309, 0.024725094437599182, 0.02405489981174469, 0.024955544620752335, 0.023423410952091217, 0.024672918021678925, 0.02595037780702114, 0.026989728212356567, 0.024348903447389603, 0.02145365998148918, 0.021875612437725067, 0.02352815680205822, 0.022593453526496887, 0.023411208763718605, 0.022491907700896263, 0.020792338997125626, 0.021408556029200554, 0.02089357003569603, 0.022629691287875175, 0.0202283076941967, 0.020565040409564972, 0.020075475797057152, 0.022420788183808327, 0.020652320235967636, 0.02122553437948227, 0.02042359858751297, 0.020805906504392624, 0.021843265742063522, 0.01946600154042244, 0.018714664503932, 0.01789272576570511, 0.020462214946746826, 0.021141134202480316, 0.020493360236287117, 0.01802094094455242, 0.02305345982313156, 0.019915809854865074, 0.0184185653924942, 0.01754882000386715, 0.01991097442805767, 0.02019936591386795, 0.016285086050629616, 0.0185838732868433, 0.018561189994215965, 0.017427444458007812, 0.01647477224469185, 0.017153743654489517, 0.015405025333166122, 0.018473105505108833, 0.01784525066614151, 0.0164299625903368, 0.01578185148537159, 0.016793593764305115, 0.017375556752085686, 0.017323356121778488, 0.017404286190867424, 0.016909776255488396, 0.017890995368361473, 0.017373913899064064, 0.032749105244874954, 0.012692484073340893, 0.018667981028556824, 0.015491819009184837, 0.01583138294517994, 0.015123693272471428, 0.016526751220226288, 0.016034869477152824, 0.015281370840966702, 0.016481351107358932, 0.01608332246541977, 0.014496836811304092, 0.014265344478189945, 0.015256552025675774, 0.016104111447930336, 0.01449793390929699, 0.014473418705165386, 0.015376008115708828, 0.023303663358092308, 0.013310885056853294, 0.021326467394828796, 0.014697203412652016, 0.01493045687675476, 0.014593519270420074, 0.01355886273086071, 0.01328825019299984, 0.013428032398223877, 0.014906522817909718, 0.012861489318311214, 0.01679583266377449, 0.014841655269265175, 0.013870720751583576, 0.013928011991083622, 0.014836958609521389, 0.013641417026519775, 0.013591410592198372, 0.015400256030261517, 0.016833800822496414, 0.013214956037700176, 0.012799099087715149, 0.014154811389744282, 0.012468277476727962, 0.015238098800182343, 0.012226476334035397, 0.024478062987327576, 0.011202066205441952, 0.013482442125678062, 0.0161709226667881, 0.010835599154233932, 0.012363793328404427, 0.014863407239317894, 0.013503179885447025, 0.011624185368418694, 0.014034198597073555, 0.012350988574326038, 0.013261211104691029, 0.011846653185784817, 0.018062248826026917, 0.011598825454711914, 0.012945566326379776, 0.011952199973165989, 0.01363386306911707, 0.01198507659137249, 0.011540848761796951, 0.013663515448570251, 0.012372026219964027, 0.012877834029495716, 0.015364481136202812, 0.010952197015285492, 0.011237132363021374, 0.015361722558736801, 0.013449078425765038, 0.012447290122509003, 0.011150344274938107, 0.012811828404664993, 0.012267917394638062, 0.012442399747669697, 0.013292449526488781, 0.011300185695290565, 0.011808443814516068, 0.013202640227973461, 0.013059813529253006, 0.013450113125145435, 0.010919434949755669, 0.010081942193210125, 0.009384903125464916, 0.014459832571446896, 0.012078463099896908, 0.012655787169933319, 0.013487517833709717, 0.011430028825998306, 0.013214381411671638, 0.012081438675522804, 0.01063134241849184, 0.012078673578798771, 0.011225971393287182, 0.010621543042361736, 0.012949992902576923, 0.011190279386937618, 0.010497181676328182, 0.010201790370047092, 0.012383872643113136, 0.014283750206232071, 0.009337362833321095]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:07 2016", "state": "available"}], "summary": "8971ce3433eb02921ac603f5f590aa03"}