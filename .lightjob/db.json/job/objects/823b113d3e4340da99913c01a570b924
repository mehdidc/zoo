{"content": {"hp_model": {"f0": 16, "f1": 16, "f2": 32, "f3": 64, "nonlin": "rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.09103009248374701, 0.08383859537099765, 0.07726972673291074, 0.07382347693403844, 0.07048208282552892, 0.07124905000564974, 0.06830312636634454, 0.06833706256728272, 0.07367390375642573, 0.06945317581532882, 0.06829633710262399, 0.06949732946837724, 0.06514685347236425, 0.06911131884691979, 0.06460087211392776, 0.06626690841263061, 0.06280302776645533, 0.06767184172320519, 0.06774612159658987, 0.06512727647038806, 0.06720692667525553, 0.06328846136712604, 0.06444497668198226, 0.06155575530090991, 0.06463026760755222, 0.06580277297323016, 0.06210969723704755, 0.0616807934521149, 0.060134276844392154, 0.06540763837057295, 0.06109874158516359, 0.0650346508662103, 0.06337294552341179, 0.06373663024481452, 0.058620443809306845, 0.062128502719244284, 0.06070809639227975, 0.06127057578158945, 0.06614083288332252, 0.06090871020429826, 0.06448329442660697, 0.0632816977620749, 0.06151242479712073, 0.06202564897552445, 0.062990175720055, 0.06348260683870004, 0.06251612058337097, 0.06270967095065967, 0.05911593756866214, 0.060411831150435406, 0.058913608553087, 0.0617728180570287, 0.060234882611066555, 0.0610898379460741, 0.05793108528820513, 0.06108414477539498, 0.05937182194069937, 0.06135565839146565, 0.061641456945246476, 0.06582783886481816, 0.05884681915283491, 0.06257642890635126, 0.0604382475984683, 0.06304720186535342, 0.062019466575300594, 0.06054748022966129, 0.05813514652692018, 0.05730224983438165, 0.06188862898716315, 0.05943667113547489, 0.06156053572850653, 0.059337116554071735, 0.06274379000011122, 0.06161439852891285, 0.06108940002870482, 0.06201083894026362, 0.06191167875667262, 0.06231552991584052, 0.0589195114548869, 0.06362502413815807, 0.059841727568027546, 0.05995472436911329, 0.05954414689766118, 0.060877664216122104, 0.06665213328463755, 0.06471561816428298, 0.061890358017647897, 0.065599862040603, 0.06293068901400038, 0.06456773439607437, 0.06151242479712073, 0.061746250468058955, 0.062398330265430985, 0.06031906072176384, 0.06080188654150153, 0.05874687216274008, 0.06164608607238918, 0.061569226464121184, 0.06339376763712748, 0.06266998420747992, 0.06389188134374864, 0.06408378130269732, 0.06441508132407346, 0.06369800333762789, 0.062118741805494565, 0.0596339359078819, 0.06351518751800292, 0.05830011445157897, 0.06031373834732951, 0.06002666923182065, 0.06442352540027595, 0.05893403915580338, 0.05944087188260587, 0.061725450465439646, 0.06167862481305888, 0.05866287022107195, 0.061720249369520024, 0.061172110715970895, 0.061255583189190464, 0.0634876635572823, 0.06016807780537899, 0.0632315117770206, 0.06336999048003707, 0.06247716728086593, 0.06352347045662014, 0.05990249542507144, 0.06140389215195992, 0.059792233639217165, 0.06228461247481415, 0.06282787110228416, 0.06270000051118473, 0.06252638993773743, 0.06375006020467293, 0.06122631522757164, 0.06357567023439127, 0.060801446550186204, 0.06145499021429146, 0.06130855030351696, 0.06077019902428643, 0.06335647995512986, 0.06394656945147616, 0.06069149557211249, 0.0620405992105019, 0.06299697062407855, 0.06137643842346641, 0.06005058219891455, 0.06029421895479865, 0.058625919910273556, 0.06103375898416157, 0.05872440240639772, 0.06190375636645466, 0.0624030461470354, 0.06087239068317595, 0.06218531524659113, 0.05920411670369463, 0.060068844697214636, 0.058625919910273556, 0.06416165923670754, 0.06046494746715477, 0.06182865937134752, 0.060140208211625544, 0.05873457559072411, 0.05877722305114198, 0.06045064016424538, 0.06169597179110728, 0.06102806058200228, 0.06047187865421334, 0.06127130348397163, 0.06019519379346786, 0.05957109781092494, 0.06128105386217594, 0.05981534584124688, 0.06172545046543964, 0.06091456616352825, 0.06223362243269047, 0.06300079243553446, 0.05924913536000976, 0.06524820385832211, 0.06276894091663578, 0.062265710925651925, 0.0631399183887759, 0.06410381609354288, 0.061916287680877376, 0.0606057748431946, 0.06280302776645533, 0.061670383289153044, 0.061865714771232753, 0.06209016796243041, 0.05942181611261053, 0.0630177754599789, 0.06142654312879941, 0.06302683120186035, 0.06012582363509845, 0.05992437462555623, 0.05949200717873862, 0.06281026883887891, 0.06036339559205885, 0.05914986817743237, 0.06085290394566877, 0.06072616115319515, 0.06367546015411482, 0.06322967838888714, 0.06219735971736449, 0.06210969723704755, 0.06641570806694345, 0.061897417724196976, 0.061890934350408704, 0.0633818098268999, 0.0635556092634846, 0.06034802985881343, 0.06348864676134713, 0.0625821288133546, 0.06196350938840129, 0.06222617095172335, 0.0606221049414452, 0.05894311715058237, 0.06382387446984962, 0.059904877222197644, 0.058449667387297705, 0.06212003377920142, 0.060086804828353164, 0.05912936133709491, 0.06220252091946871, 0.06293749034037215, 0.06463233720883217, 0.06246988759765154, 0.05980953135388863, 0.06105085099935356, 0.05707271682110359, 0.0613386513109123, 0.06206948320954503, 0.06217039981176546, 0.06259865560869789, 0.05990294201924627, 0.06269388461048185, 0.06081963020463162, 0.060823588828039565, 0.06041699728611918, 0.061038580293548396, 0.06086828873056507, 0.05856793854316039, 0.05896489863579945, 0.06175188259640716, 0.06348808493231739, 0.06013887370500253, 0.06262728223001718, 0.05683033282971769, 0.06248530239376568, 0.060223630230197736, 0.06103434340558346, 0.06151532411108041, 0.06362123982513164], "moving_avg_accuracy_train": [0.061692865210843356, 0.12834690323795178, 0.1939548898719879, 0.25627297392695775, 0.3144394453595632, 0.3685964985645707, 0.4190768336478727, 0.46542216233127814, 0.5084436697427286, 0.5482313622564076, 0.5848944835608874, 0.6182983898734733, 0.6493925907957645, 0.6775397398487182, 0.7032510331831234, 0.7270830270033652, 0.7488800895138721, 0.7689586656528463, 0.7873894180634653, 0.8043794860462754, 0.8198493875922503, 0.8338687786523025, 0.846742725335265, 0.858828147831859, 0.8697473850065044, 0.8796876502709142, 0.8888692052739433, 0.897294972999561, 0.9048781639526169, 0.9118371660814515, 0.9182744020335473, 0.9240914460169395, 0.9293385514152456, 0.9340115298580584, 0.9383207496132163, 0.9426908583868344, 0.946492179174657, 0.9499721969499624, 0.9531371572248457, 0.9559879746348912, 0.9585160597015225, 0.9607866299361896, 0.9629901582076309, 0.9649474488627714, 0.966803136958422, 0.9685014941963147, 0.9700747258007796, 0.9715471101484124, 0.9730510964227279, 0.9741834867804551, 0.9754426606927711, 0.9764747312198795, 0.9774012415316264, 0.9782751045772591, 0.9790309902038705, 0.9796759898280617, 0.9801788351223639, 0.9808455336884407, 0.9814643876991147, 0.981969586730408, 0.9823395520031504, 0.9828160636703053, 0.9833272848635157, 0.9837591459855979, 0.9841948842484839, 0.9845188069682138, 0.9849679993135612, 0.9854263951653376, 0.9858601298957917, 0.9861445988339235, 0.9864923942216155, 0.9867912910946347, 0.9871473652984243, 0.9873007575336421, 0.9875800003043742, 0.9876218873221295, 0.9878666639513622, 0.9878587061405634, 0.9879644959180733, 0.9882950229828925, 0.9884983708352056, 0.9886837370649381, 0.9888293882078418, 0.9887439832725998, 0.9889142009091952, 0.9891191663604444, 0.9892306872244, 0.9893875319055745, 0.9896651755523664, 0.9896256158284551, 0.9897947372275373, 0.9898481136553859, 0.9899032119284015, 0.9900892838078504, 0.9902920459391136, 0.9904015838150818, 0.9905236995299592, 0.99063830999865, 0.9908002884867368, 0.9909131248489066, 0.9906381715507628, 0.9907389816547226, 0.990968547344672, 0.9908339478812891, 0.990999894207618, 0.9910457067446875, 0.9911834176967247, 0.9912885322523534, 0.9912395924307325, 0.9913461490009122, 0.9912302652755197, 0.9913448140491725, 0.9914267294816047, 0.9916510557804321, 0.9918294178228708, 0.9917922779984151, 0.991669431975682, 0.9918177184467886, 0.9918052801864471, 0.9918976249087662, 0.9918842554901787, 0.9918886951520042, 0.9918197428054784, 0.991901228615292, 0.9919886848200278, 0.9920838675428443, 0.9922260078969936, 0.9923704063542821, 0.9924368295742756, 0.9924542535445589, 0.9924958199069704, 0.9924038056873578, 0.9925610154800678, 0.9924201247754345, 0.9924909888039152, 0.9926488929355719, 0.9927957129793641, 0.992880787765765, 0.9929338234470199, 0.9929933213734022, 0.9928868544469054, 0.9929439897853474, 0.9929554078248849, 0.9929986283375771, 0.9928869243893615, 0.9929887628239193, 0.993047473137913, 0.9931167845590615, 0.9930097371272517, 0.9931016474506711, 0.9931020060489775, 0.9931964552934773, 0.9932744001255754, 0.9934080858660299, 0.9935048714059329, 0.9935166771870264, 0.993595544106878, 0.9935465130395637, 0.993648281163318, 0.9936551586192754, 0.9937837127874684, 0.993680567412336, 0.993764223773512, 0.993830101847968, 0.9938823326270266, 0.993783444243842, 0.9938850508736746, 0.994018853768235, 0.9940663283311705, 0.9941725908293787, 0.9940187918368023, 0.9940756852434834, 0.9941410082854002, 0.9942327433002337, 0.9943341301147886, 0.9943736086695748, 0.9944467899712921, 0.994486768353681, 0.9944262692291563, 0.9944330022459997, 0.9944225898226047, 0.994448516081308, 0.9944789092020929, 0.994336835299956, 0.9944419318904423, 0.9944565112917595, 0.9944743390782462, 0.9944880309234336, 0.994377989126271, 0.9944577918702704, 0.9945154953639662, 0.9945744879962444, 0.9945240422086682, 0.9945774738311749, 0.9946820381950453, 0.9947384955201191, 0.9947351843717216, 0.9948286840068387, 0.9947857628953114, 0.9947918439852983, 0.9947996701289372, 0.994896133838935, 0.9949641258767282, 0.9950653224758024, 0.9950646260715956, 0.9950451740066047, 0.9950488456119684, 0.9949815551772776, 0.9949774696896703, 0.9950326218170888, 0.9950869650570666, 0.9951241081597937, 0.9950634104462239, 0.9950464331064208, 0.9951064547054174, 0.9951487083312612, 0.9951443796668098, 0.9952181382362735, 0.9952586361596341, 0.9952644931762008, 0.9953403593706288, 0.9954133452709153, 0.9953778465871973, 0.9954682622296823, 0.9954649224524973, 0.9955019204180909, 0.9955563970509806, 0.9956619019241958, 0.995676848779969, 0.9957044199260685, 0.9956162821503292, 0.9956146125196336, 0.9956625262676703, 0.9955621057192166, 0.9955917385207889, 0.9956372333434087, 0.9956405280813571, 0.9956011364177996, 0.9954927358784292, 0.995465770272514, 0.9955309214079133, 0.9955730852912184, 0.9956298580873977, 0.9955562359834771, 0.9956546974754908, 0.9956421268243273, 0.9957108207684006, 0.9957302883903556, 0.9957784003645731, 0.9958522922558266, 0.9957846646868704, 0.9957237998748099], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 1234423, "moving_var_accuracy_train": [0.034254086561309585, 0.07081352497305157, 0.1024718436672932, 0.1271765517031045, 0.14490894212307498, 0.15681492561741764, 0.1640678111267779, 0.16699203543105612, 0.1669504827875319, 0.16450297878884632, 0.16015034108404463, 0.15417769558810035, 0.14746157000825266, 0.1398457710057101, 0.13181082934948962, 0.12374142177957308, 0.11564328700839825, 0.10770730128547569, 0.09999380486672196, 0.09259238607059435, 0.08548700814811432, 0.0787071972645549, 0.07232812406686126, 0.06640982859246576, 0.06084191339750462, 0.05564700191949565, 0.050841010298008915, 0.04639585132410257, 0.0422738092569669, 0.03848227772693235, 0.03500699201456571, 0.03181083481945162, 0.02887754037305485, 0.026186316883492305, 0.023734809569227272, 0.021533209268543834, 0.019509938699276842, 0.01766793954279713, 0.01599129835039172, 0.01446531295450132, 0.013076302585988308, 0.01181507173010448, 0.010677264388681402, 0.009644016830191567, 0.008710607351947463, 0.007865506372520242, 0.007101231254399803, 0.006410619369964211, 0.0057899152053877525, 0.005222464456149439, 0.004714487681007608, 0.00425262543906318, 0.003835088687376824, 0.003458452548241841, 0.003117749561142317, 0.002809718825664947, 0.00253102262360847, 0.0022819207440497036, 0.002057175492223479, 0.0018537549775521087, 0.0016696113485242157, 0.0015046937839922063, 0.001356576529568473, 0.0012225974128705205, 0.0011020464820871547, 0.0009927861672336543, 0.0008953235143783563, 0.0008076823037528532, 0.0007286072057251875, 0.0006564747883435245, 0.0005919159641944704, 0.0005335284218413291, 0.00048131667920463543, 0.0004333967738845979, 0.0003907588852211938, 0.00035169878739938236, 0.00031706814904341145, 0.00028536190407984466, 0.00025692643696509056, 0.0002322170265337833, 0.00020936747702176838, 0.00018873997507171857, 0.00017005690586340932, 0.00015311686130374161, 0.0001380659415676407, 0.00012463744493672855, 0.00011228563257093225, 0.0001012784715999535, 9.184439839139353e-05, 8.267404329805766e-05, 7.46640573968996e-05, 6.722329304465861e-05, 6.052828611739649e-05, 5.478706220455204e-05, 4.9678368320966196e-05, 4.481851840531411e-05, 4.0470876795162764e-05, 3.6542009151448266e-05, 3.3123941511729434e-05, 2.9926135762205685e-05, 2.7613916031425865e-05, 2.4943988521826862e-05, 2.292389332366181e-05, 2.07945571311822e-05, 1.8962945067062574e-05, 1.7085539657331036e-05, 1.5547664448397055e-05, 1.40923396318026e-05, 1.2704661623884884e-05, 1.1536384185332686e-05, 1.050360710709686e-05, 9.571339190295516e-06, 8.674596513900868e-06, 8.26003745762153e-06, 7.720350875505578e-06, 6.960730087000461e-06, 6.4004773860124225e-06, 5.9583295450301325e-06, 5.3638889834100274e-06, 4.904248014730834e-06, 4.41543188543805e-06, 3.974066092268369e-06, 3.619449317864151e-06, 3.317263820886553e-06, 3.054374728519123e-06, 2.830475012172075e-06, 2.7292624334540976e-06, 2.643994420314638e-06, 2.419303375671897e-06, 2.1801053907686015e-06, 1.97764471404892e-06, 1.8560797921424395e-06, 1.8929060832435112e-06, 1.8822671907877425e-06, 1.7392358665015162e-06, 1.7897157129995845e-06, 1.8047492690319375e-06, 1.6894138156592327e-06, 1.5457874854687763e-06, 1.4230687661161007e-06, 1.3827787474436636e-06, 1.2738808947892282e-06, 1.1476661499522346e-06, 1.049711649413426e-06, 1.057040432894554e-06, 1.0446759903837903e-06, 9.712305000684583e-07, 9.173441079761885e-07, 9.287420710921527e-07, 9.118954319424829e-07, 8.207070460829427e-07, 8.189222795539598e-07, 7.917086232557034e-07, 8.733846557379569e-07, 8.703531567730581e-07, 7.84572229300791e-07, 7.620949257925792e-07, 7.075218432711419e-07, 7.299804180565916e-07, 6.574080708549403e-07, 7.404028312074597e-07, 7.621132637875007e-07, 7.488874182956842e-07, 7.130579627124213e-07, 6.663046549708098e-07, 6.876844004334912e-07, 7.118311254237127e-07, 8.017769442156511e-07, 7.418837569273768e-07, 7.69320847963775e-07, 9.052759342251146e-07, 8.438800783167528e-07, 7.978959687323579e-07, 7.938441883775945e-07, 8.069733450302204e-07, 7.403030171192664e-07, 7.144722416965724e-07, 6.574094570527838e-07, 6.246098079618348e-07, 5.625568288079739e-07, 5.072769129758176e-07, 4.625987596914332e-07, 4.2465255984167366e-07, 5.6385224687344e-07, 6.068746621727635e-07, 5.481002264404033e-07, 4.961506735355011e-07, 4.482228058036789e-07, 5.123832993283619e-07, 5.184612709440467e-07, 4.96582382512005e-07, 4.782453202286671e-07, 4.5332378556349487e-07, 4.336858515604033e-07, 4.8872062212908e-07, 4.685354259065491e-07, 4.217805566492832e-07, 4.5828213688745846e-07, 4.2903391953131465e-07, 3.8646334447704577e-07, 3.483682467476431e-07, 3.9727864819180705e-07, 3.9915703820212825e-07, 4.5140809935950374e-07, 4.062716542329259e-07, 3.6904993430132113e-07, 3.3226626704470445e-07, 3.397916637481449e-07, 3.0596271825423295e-07, 3.027422608578959e-07, 2.990467243537336e-07, 2.8155854264010867e-07, 2.8656060026943885e-07, 2.6049861084360895e-07, 2.668720808740854e-07, 2.5625319285920637e-07, 2.3079650959668295e-07, 2.5667979776085936e-07, 2.457725541534673e-07, 2.215040405256851e-07, 2.511547515861571e-07, 2.7398175119324206e-07, 2.5792498498533924e-07, 3.0570738214073027e-07, 2.752370309314723e-07, 2.60032972960941e-07, 2.607390074438381e-07, 3.3484661114883835e-07, 3.0337262651149545e-07, 2.798768767355008e-07, 3.2180359667252574e-07, 2.896483260052093e-07, 2.813450386629561e-07, 3.43969113762474e-07, 3.1747512874744804e-07, 3.0435562583965753e-07, 2.7401776093902267e-07, 2.605813132654898e-07, 3.402792743609332e-07, 3.1279564204620884e-07, 3.197181118358257e-07, 3.0374643815050706e-07, 3.023801478096793e-07, 3.209240606998187e-07, 3.760836433159718e-07, 3.3989747042045456e-07, 3.483774449495152e-07, 3.169505951958551e-07, 3.0608839424417907e-07, 3.2461965915697467e-07, 3.333190859875479e-07, 3.333279055132685e-07], "duration": 167347.141129, "accuracy_train": [0.6169286521084337, 0.7282332454819277, 0.7844267695783133, 0.8171357304216867, 0.8379376882530121, 0.8560099774096386, 0.8733998493975904, 0.8825301204819277, 0.8956372364457831, 0.9063205948795181, 0.9148625753012049, 0.918933546686747, 0.9292403990963856, 0.9308640813253012, 0.9346526731927711, 0.9415709713855421, 0.9450536521084337, 0.9496658509036144, 0.9532661897590361, 0.9572900978915663, 0.9590785015060241, 0.9600432981927711, 0.9626082454819277, 0.9675969503012049, 0.9680205195783133, 0.9691500376506024, 0.9715032003012049, 0.9731268825301205, 0.9731268825301205, 0.9744681852409639, 0.9762095256024096, 0.9764448418674698, 0.9765625, 0.9760683358433735, 0.9771037274096386, 0.9820218373493976, 0.9807040662650602, 0.9812923569277109, 0.9816217996987951, 0.9816453313253012, 0.9812688253012049, 0.9812217620481928, 0.9828219126506024, 0.9825630647590361, 0.9835043298192772, 0.9837867093373494, 0.9842338102409639, 0.9847985692771084, 0.9865869728915663, 0.984375, 0.9867752259036144, 0.9857633659638554, 0.9857398343373494, 0.9861398719879518, 0.9858339608433735, 0.9854809864457831, 0.9847044427710844, 0.9868458207831325, 0.9870340737951807, 0.9865163780120482, 0.9856692394578314, 0.9871046686746988, 0.9879282756024096, 0.9876458960843374, 0.9881165286144579, 0.9874341114457831, 0.9890107304216867, 0.9895519578313253, 0.9897637424698795, 0.9887048192771084, 0.9896225527108434, 0.9894813629518072, 0.9903520331325302, 0.9886812876506024, 0.9900931852409639, 0.9879988704819277, 0.9900696536144579, 0.9877870858433735, 0.9889166039156626, 0.9912697665662651, 0.9903285015060241, 0.9903520331325302, 0.9901402484939759, 0.9879753388554217, 0.9904461596385542, 0.9909638554216867, 0.990234375, 0.9907991340361446, 0.992163968373494, 0.989269578313253, 0.9913168298192772, 0.9903285015060241, 0.9903990963855421, 0.9917639307228916, 0.9921169051204819, 0.9913874246987951, 0.9916227409638554, 0.9916698042168675, 0.9922580948795181, 0.9919286521084337, 0.9881635918674698, 0.9916462725903614, 0.9930346385542169, 0.9896225527108434, 0.9924934111445783, 0.9914580195783133, 0.9924228162650602, 0.9922345632530121, 0.9907991340361446, 0.9923051581325302, 0.9901873117469879, 0.9923757530120482, 0.992163968373494, 0.9936699924698795, 0.9934346762048193, 0.9914580195783133, 0.9905638177710844, 0.993152296686747, 0.9916933358433735, 0.9927287274096386, 0.9917639307228916, 0.9919286521084337, 0.991199171686747, 0.9926346009036144, 0.9927757906626506, 0.9929405120481928, 0.9935052710843374, 0.9936699924698795, 0.9930346385542169, 0.9926110692771084, 0.9928699171686747, 0.9915756777108434, 0.9939759036144579, 0.9911521084337349, 0.993128765060241, 0.9940700301204819, 0.994117093373494, 0.9936464608433735, 0.9934111445783133, 0.9935288027108434, 0.9919286521084337, 0.9934582078313253, 0.9930581701807228, 0.9933876129518072, 0.9918815888554217, 0.9939053087349398, 0.9935758659638554, 0.9937405873493976, 0.9920463102409639, 0.9939288403614458, 0.9931052334337349, 0.9940464984939759, 0.9939759036144579, 0.9946112575301205, 0.9943759412650602, 0.9936229292168675, 0.9943053463855421, 0.9931052334337349, 0.9945641942771084, 0.9937170557228916, 0.9949407003012049, 0.9927522590361446, 0.9945171310240963, 0.9944230045180723, 0.9943524096385542, 0.9928934487951807, 0.9947995105421686, 0.9952230798192772, 0.9944935993975904, 0.995128953313253, 0.9926346009036144, 0.9945877259036144, 0.9947289156626506, 0.9950583584337349, 0.9952466114457831, 0.9947289156626506, 0.995105421686747, 0.9948465737951807, 0.9938817771084337, 0.9944935993975904, 0.9943288780120482, 0.9946818524096386, 0.9947524472891566, 0.9930581701807228, 0.9953878012048193, 0.9945877259036144, 0.9946347891566265, 0.9946112575301205, 0.9933876129518072, 0.9951760165662651, 0.9950348268072289, 0.995105421686747, 0.9940700301204819, 0.9950583584337349, 0.9956231174698795, 0.9952466114457831, 0.9947053840361446, 0.9956701807228916, 0.9943994728915663, 0.9948465737951807, 0.9948701054216867, 0.9957643072289156, 0.9955760542168675, 0.9959760918674698, 0.9950583584337349, 0.9948701054216867, 0.995081890060241, 0.9943759412650602, 0.9949407003012049, 0.9955289909638554, 0.9955760542168675, 0.9954583960843374, 0.9945171310240963, 0.9948936370481928, 0.9956466490963856, 0.9955289909638554, 0.995105421686747, 0.9958819653614458, 0.9956231174698795, 0.9953172063253012, 0.9960231551204819, 0.996070218373494, 0.9950583584337349, 0.9962820030120482, 0.9954348644578314, 0.9958349021084337, 0.9960466867469879, 0.9966114457831325, 0.9958113704819277, 0.9959525602409639, 0.9948230421686747, 0.9955995858433735, 0.99609375, 0.9946583207831325, 0.9958584337349398, 0.9960466867469879, 0.9956701807228916, 0.9952466114457831, 0.9945171310240963, 0.9952230798192772, 0.996117281626506, 0.9959525602409639, 0.9961408132530121, 0.9948936370481928, 0.9965408509036144, 0.9955289909638554, 0.9963290662650602, 0.9959054969879518, 0.9962114081325302, 0.9965173192771084, 0.9951760165662651, 0.9951760165662651], "end": "2016-01-22 11:36:01.274000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0, 211.0, 212.0, 213.0, 214.0, 215.0, 216.0, 217.0, 218.0, 219.0, 220.0, 221.0, 222.0, 223.0, 224.0, 225.0, 226.0, 227.0, 228.0, 229.0, 230.0, 231.0, 232.0, 233.0, 234.0, 235.0, 236.0, 237.0, 238.0, 239.0, 240.0, 241.0, 242.0, 243.0, 244.0, 245.0, 246.0, 247.0, 248.0, 249.0, 250.0, 251.0], "accuracy_valid": [0.6020299145299145, 0.7096688034188035, 0.7487980769230769, 0.7744391025641025, 0.7872596153846154, 0.8010149572649573, 0.8056891025641025, 0.8126335470085471, 0.8199786324786325, 0.8237179487179487, 0.8227831196581197, 0.8226495726495726, 0.8306623931623932, 0.8282585470085471, 0.8295940170940171, 0.8340010683760684, 0.8380074786324786, 0.8370726495726496, 0.8378739316239316, 0.8377403846153846, 0.8410790598290598, 0.8414797008547008, 0.8418803418803419, 0.844284188034188, 0.8444177350427351, 0.8469551282051282, 0.8481570512820513, 0.8480235042735043, 0.8452190170940171, 0.8465544871794872, 0.8489583333333334, 0.8462873931623932, 0.8470886752136753, 0.8476228632478633, 0.8476228632478633, 0.8555021367521367, 0.8518963675213675, 0.8509615384615384, 0.8534989316239316, 0.8524305555555556, 0.8497596153846154, 0.8548344017094017, 0.8529647435897436, 0.8533653846153846, 0.8502938034188035, 0.8540331196581197, 0.859642094017094, 0.8584401709401709, 0.8557692307692307, 0.859375, 0.8575053418803419, 0.858573717948718, 0.8557692307692307, 0.8563034188034188, 0.8581730769230769, 0.8580395299145299, 0.8544337606837606, 0.8625801282051282, 0.8589743589743589, 0.8543002136752137, 0.8576388888888888, 0.8603098290598291, 0.8616452991452992, 0.8597756410256411, 0.8612446581196581, 0.859909188034188, 0.8627136752136753, 0.8620459401709402, 0.8611111111111112, 0.8592414529914529, 0.8607104700854701, 0.8612446581196581, 0.8648504273504274, 0.8612446581196581, 0.8604433760683761, 0.8572382478632479, 0.8597756410256411, 0.859642094017094, 0.8627136752136753, 0.8645833333333334, 0.8607104700854701, 0.8648504273504274, 0.8639155982905983, 0.8625801282051282, 0.8592414529914529, 0.8613782051282052, 0.8627136752136753, 0.8631143162393162, 0.8663194444444444, 0.8619123931623932, 0.8657852564102564, 0.8639155982905983, 0.8619123931623932, 0.8660523504273504, 0.8627136752136753, 0.8641826923076923, 0.8675213675213675, 0.8644497863247863, 0.8641826923076923, 0.8668536324786325, 0.8581730769230769, 0.8625801282051282, 0.8624465811965812, 0.8583066239316239, 0.8621794871794872, 0.8636485042735043, 0.8657852564102564, 0.8623130341880342, 0.8612446581196581, 0.8629807692307693, 0.859375, 0.8624465811965812, 0.8621794871794872, 0.8663194444444444, 0.8639155982905983, 0.8651175213675214, 0.8620459401709402, 0.8643162393162394, 0.8668536324786325, 0.8660523504273504, 0.8655181623931624, 0.8625801282051282, 0.8624465811965812, 0.8648504273504274, 0.8632478632478633, 0.8660523504273504, 0.8655181623931624, 0.8657852564102564, 0.8639155982905983, 0.8637820512820513, 0.8645833333333334, 0.8620459401709402, 0.8660523504273504, 0.8624465811965812, 0.8664529914529915, 0.8660523504273504, 0.8641826923076923, 0.8633814102564102, 0.8632478632478633, 0.8627136752136753, 0.8627136752136753, 0.8669871794871795, 0.8603098290598291, 0.8663194444444444, 0.8643162393162394, 0.8676549145299145, 0.8692574786324786, 0.8687232905982906, 0.8675213675213675, 0.8644497863247863, 0.8652510683760684, 0.8692574786324786, 0.8681891025641025, 0.8675213675213675, 0.8681891025641025, 0.8651175213675214, 0.8639155982905983, 0.8623130341880342, 0.8665865384615384, 0.8645833333333334, 0.8657852564102564, 0.8669871794871795, 0.8660523504273504, 0.8672542735042735, 0.8676549145299145, 0.8623130341880342, 0.8659188034188035, 0.8639155982905983, 0.8659188034188035, 0.8644497863247863, 0.8653846153846154, 0.8683226495726496, 0.8663194444444444, 0.8673878205128205, 0.8665865384615384, 0.8656517094017094, 0.8696581196581197, 0.8671207264957265, 0.8676549145299145, 0.8679220085470085, 0.8640491452991453, 0.8657852564102564, 0.8675213675213675, 0.8636485042735043, 0.8668536324786325, 0.8683226495726496, 0.8668536324786325, 0.8703258547008547, 0.8676549145299145, 0.8667200854700855, 0.8671207264957265, 0.8653846153846154, 0.8659188034188035, 0.8680555555555556, 0.8676549145299145, 0.8683226495726496, 0.8668536324786325, 0.8689903846153846, 0.8683226495726496, 0.8672542735042735, 0.8677884615384616, 0.874732905982906, 0.8688568376068376, 0.8705929487179487, 0.8688568376068376, 0.8665865384615384, 0.8653846153846154, 0.8675213675213675, 0.8681891025641025, 0.8657852564102564, 0.8653846153846154, 0.8693910256410257, 0.8645833333333334, 0.8695245726495726, 0.8675213675213675, 0.8664529914529915, 0.8676549145299145, 0.8681891025641025, 0.8725961538461539, 0.8657852564102564, 0.8721955128205128, 0.8703258547008547, 0.8715277777777778, 0.8700587606837606, 0.8688568376068376, 0.8668536324786325, 0.8700587606837606, 0.8692574786324786, 0.8697916666666666, 0.8717948717948718, 0.8700587606837606, 0.8685897435897436, 0.8688568376068376, 0.8699252136752137, 0.8695245726495726, 0.8689903846153846, 0.8693910256410257, 0.8712606837606838, 0.8684561965811965, 0.8703258547008547, 0.8720619658119658, 0.8715277777777778, 0.8691239316239316, 0.8627136752136753, 0.8669871794871795, 0.8660523504273504, 0.8713942307692307, 0.8687232905982906, 0.8704594017094017, 0.8669871794871795, 0.8684561965811965, 0.8681891025641025], "accuracy_test": 0.8540665064102564, "start": "2016-01-20 13:06:54.133000", "learning_rate_per_epoch": [0.0010000000474974513, 0.0007071067811921239, 0.0005773502634838223, 0.0005000000237487257, 0.00044721359154209495, 0.0004082482773810625, 0.000377964461222291, 0.00035355339059606194, 0.00033333332976326346, 0.0003162277571391314, 0.0003015113470610231, 0.00028867513174191117, 0.00027735010371543467, 0.0002672612317837775, 0.00025819888105615973, 0.0002500000118743628, 0.00024253562150988728, 0.00023570226039737463, 0.00022941573115531355, 0.00022360679577104747, 0.00021821788686793298, 0.00021320072119124234, 0.00020851440785918385, 0.00020412413869053125, 0.00019999999494757503, 0.0001961161324288696, 0.00019245008297730237, 0.0001889822306111455, 0.00018569533131085336, 0.00018257419287692755, 0.00017960529658012092, 0.00017677669529803097, 0.00017407764971721917, 0.00017149858467746526, 0.00016903085634112358, 0.00016666666488163173, 0.0001643989817239344, 0.00016222141857724637, 0.00016012815467547625, 0.0001581138785695657, 0.00015617375902365893, 0.00015430334315169603, 0.00015249857096932828, 0.00015075567353051156, 0.00014907120203133672, 0.00014744195505045354, 0.00014586499310098588, 0.00014433756587095559, 0.0001428571413271129, 0.00014142136205919087, 0.00014002800162415951, 0.00013867505185771734, 0.00013736056280322373, 0.00013608275912702084, 0.0001348399673588574, 0.00013363061589188874, 0.0001324532349826768, 0.00013130642764735967, 0.00013018891331739724, 0.00012909944052807987, 0.00012803687423001975, 0.00012700012302957475, 0.00012598815374076366, 0.0001250000059371814, 0.00012403473374433815, 0.00012309149315115064, 0.00012216944014653563, 0.00012126781075494364, 0.0001203858555527404, 0.00011952286149607971, 0.0001186781664728187, 0.00011785113019868731, 0.00011704114876920357, 0.00011624764010775834, 0.00011547005124157295, 0.00011470786557765678, 0.00011396057379897684, 0.00011322770296828821, 0.00011250878742430359, 0.00011180339788552374, 0.00011111111234640703, 0.00011043152335332707, 0.00010976425983244553, 0.00010910894343396649, 0.00010846523218788207, 0.00010783276957226917, 0.00010721124999690801, 0.00010660036059562117, 0.00010599978850223124, 0.00010540925723034889, 0.00010482848301762715, 0.00010425720392959192, 0.00010369517258368433, 0.00010314212704543024, 0.00010259783448418602, 0.00010206206934526563, 0.00010153461334994063, 0.00010101525549544021, 0.00010050378477899358, 9.999999747378752e-05, 9.95037189568393e-05, 9.901475277729332e-05, 9.853292431216687e-05, 9.80580662144348e-05, 9.759000386111438e-05, 9.712858445709571e-05, 9.667364793131128e-05, 9.622504148865119e-05, 9.578262688592076e-05, 9.534625860396773e-05, 9.49157983995974e-05, 9.449111530557275e-05, 9.407208563061431e-05, 9.365857840748504e-05, 9.32504772208631e-05, 9.284766565542668e-05, 9.245003457181156e-05, 9.205746027873829e-05, 9.16698481887579e-05, 9.128709643846378e-05, 9.09090886125341e-05, 9.053574467543513e-05, 9.016696276376024e-05, 8.980264829006046e-05, 8.944272121880203e-05, 8.908707968657836e-05, 8.87356509338133e-05, 8.838834764901549e-05, 8.804508979665115e-05, 8.770580461714417e-05, 8.737040479900315e-05, 8.703882485860959e-05, 8.671099931234494e-05, 8.638684084871784e-05, 8.606629853602499e-05, 8.574929233873263e-05, 8.543576404917985e-05, 8.512565545970574e-05, 8.481889381073415e-05, 8.451542817056179e-05, 8.421519305557013e-05, 8.391813753405586e-05, 8.362420339835808e-05, 8.333333244081587e-05, 8.304548100568354e-05, 8.276059088530019e-05, 8.247861114796251e-05, 8.21994908619672e-05, 8.19231936475262e-05, 8.164966129697859e-05, 8.137884287862107e-05, 8.111070928862318e-05, 8.084520959528163e-05, 8.058229286689311e-05, 8.032192999962717e-05, 8.006407733773813e-05, 7.980869122548029e-05, 7.955572800710797e-05, 7.930515857879072e-05, 7.905693928478286e-05, 7.881104102125391e-05, 7.856742013245821e-05, 7.832604751456529e-05, 7.808687951182947e-05, 7.78498942963779e-05, 7.761505548842251e-05, 7.738232670817524e-05, 7.715167157584801e-05, 7.69230755395256e-05, 7.669650221941993e-05, 7.647190795978531e-05, 7.624928548466414e-05, 7.602859113831073e-05, 7.580980309285223e-05, 7.55928922444582e-05, 7.537783676525578e-05, 7.51646002754569e-05, 7.495316822314635e-05, 7.474351150449365e-05, 7.453560101566836e-05, 7.432941492879763e-05, 7.412493141600862e-05, 7.392212864942849e-05, 7.372097752522677e-05, 7.352146349148825e-05, 7.332355744438246e-05, 7.312724483199418e-05, 7.293249655049294e-05, 7.273929804796353e-05, 7.25476274965331e-05, 7.23574630683288e-05, 7.216878293547779e-05, 7.198157254606485e-05, 7.179581734817475e-05, 7.161148823797703e-05, 7.142857066355646e-05, 7.124705007299781e-05, 7.106690463842824e-05, 7.088811980793253e-05, 7.071068102959543e-05, 7.053455919958651e-05, 7.035975431790575e-05, 7.018623728072271e-05, 7.001400081207976e-05, 6.984303036006168e-05, 6.967330409679562e-05, 6.950480747036636e-05, 6.933752592885867e-05, 6.917144492035732e-05, 6.900655716890469e-05, 6.884284084662795e-05, 6.868028140161186e-05, 6.851887155789882e-05, 6.835858948761597e-05, 6.819943519076332e-05, 6.804137956351042e-05, 6.788442260585725e-05, 6.77285497658886e-05, 6.757373921573162e-05, 6.74199836794287e-05, 6.726727588102221e-05, 6.711560854455456e-05, 6.696495256619528e-05, 6.681530794594437e-05, 6.666666740784422e-05, 6.651900912402198e-05, 6.637233309447765e-05, 6.62266174913384e-05, 6.608186231460422e-05, 6.593804573640227e-05, 6.579516775673255e-05, 6.565321382367983e-05, 6.55121766612865e-05, 6.537204171763733e-05, 6.523280899273232e-05, 6.509445665869862e-05, 6.495697743957862e-05, 6.482037133537233e-05, 6.468462379416451e-05, 6.454972026403993e-05, 6.44156607449986e-05, 6.42824379610829e-05, 6.415003008442e-05, 6.401843711500987e-05, 6.388765905285254e-05, 6.375767407007515e-05, 6.36284748907201e-05, 6.350006151478738e-05, 6.337242666631937e-05, 6.324555579340085e-05, 6.311944162007421e-05, 6.299407687038183e-05], "accuracy_train_last": 0.9951760165662651, "error_valid": [0.3979700854700855, 0.29033119658119655, 0.25120192307692313, 0.22556089743589747, 0.21274038461538458, 0.1989850427350427, 0.19431089743589747, 0.18736645299145294, 0.18002136752136755, 0.17628205128205132, 0.17721688034188032, 0.1773504273504274, 0.1693376068376068, 0.17174145299145294, 0.17040598290598286, 0.16599893162393164, 0.1619925213675214, 0.1629273504273504, 0.16212606837606836, 0.16225961538461542, 0.15892094017094016, 0.1585202991452992, 0.1581196581196581, 0.15571581196581197, 0.1555822649572649, 0.1530448717948718, 0.15184294871794868, 0.15197649572649574, 0.15478098290598286, 0.15344551282051277, 0.15104166666666663, 0.1537126068376068, 0.15291132478632474, 0.1523771367521367, 0.1523771367521367, 0.1444978632478633, 0.14810363247863245, 0.14903846153846156, 0.14650106837606836, 0.14756944444444442, 0.15024038461538458, 0.14516559829059827, 0.1470352564102564, 0.14663461538461542, 0.14970619658119655, 0.14596688034188032, 0.14035790598290598, 0.1415598290598291, 0.14423076923076927, 0.140625, 0.1424946581196581, 0.14142628205128205, 0.14423076923076927, 0.14369658119658124, 0.14182692307692313, 0.14196047008547008, 0.14556623931623935, 0.1374198717948718, 0.14102564102564108, 0.1456997863247863, 0.14236111111111116, 0.1396901709401709, 0.1383547008547008, 0.14022435897435892, 0.1387553418803419, 0.14009081196581197, 0.13728632478632474, 0.13795405982905984, 0.13888888888888884, 0.14075854700854706, 0.13928952991452992, 0.1387553418803419, 0.1351495726495726, 0.1387553418803419, 0.13955662393162394, 0.14276175213675213, 0.14022435897435892, 0.14035790598290598, 0.13728632478632474, 0.13541666666666663, 0.13928952991452992, 0.1351495726495726, 0.13608440170940173, 0.1374198717948718, 0.14075854700854706, 0.13862179487179482, 0.13728632478632474, 0.13688568376068377, 0.13368055555555558, 0.1380876068376068, 0.1342147435897436, 0.13608440170940173, 0.1380876068376068, 0.1339476495726496, 0.13728632478632474, 0.1358173076923077, 0.13247863247863245, 0.1355502136752137, 0.1358173076923077, 0.13314636752136755, 0.14182692307692313, 0.1374198717948718, 0.13755341880341876, 0.14169337606837606, 0.13782051282051277, 0.13635149572649574, 0.1342147435897436, 0.13768696581196582, 0.1387553418803419, 0.13701923076923073, 0.140625, 0.13755341880341876, 0.13782051282051277, 0.13368055555555558, 0.13608440170940173, 0.1348824786324786, 0.13795405982905984, 0.13568376068376065, 0.13314636752136755, 0.1339476495726496, 0.13448183760683763, 0.1374198717948718, 0.13755341880341876, 0.1351495726495726, 0.1367521367521367, 0.1339476495726496, 0.13448183760683763, 0.1342147435897436, 0.13608440170940173, 0.13621794871794868, 0.13541666666666663, 0.13795405982905984, 0.1339476495726496, 0.13755341880341876, 0.13354700854700852, 0.1339476495726496, 0.1358173076923077, 0.13661858974358976, 0.1367521367521367, 0.13728632478632474, 0.13728632478632474, 0.13301282051282048, 0.1396901709401709, 0.13368055555555558, 0.13568376068376065, 0.1323450854700855, 0.1307425213675214, 0.13127670940170943, 0.13247863247863245, 0.1355502136752137, 0.13474893162393164, 0.1307425213675214, 0.13181089743589747, 0.13247863247863245, 0.13181089743589747, 0.1348824786324786, 0.13608440170940173, 0.13768696581196582, 0.13341346153846156, 0.13541666666666663, 0.1342147435897436, 0.13301282051282048, 0.1339476495726496, 0.13274572649572647, 0.1323450854700855, 0.13768696581196582, 0.13408119658119655, 0.13608440170940173, 0.13408119658119655, 0.1355502136752137, 0.13461538461538458, 0.1316773504273504, 0.13368055555555558, 0.13261217948717952, 0.13341346153846156, 0.13434829059829057, 0.13034188034188032, 0.13287927350427353, 0.1323450854700855, 0.13207799145299148, 0.13595085470085466, 0.1342147435897436, 0.13247863247863245, 0.13635149572649574, 0.13314636752136755, 0.1316773504273504, 0.13314636752136755, 0.12967414529914534, 0.1323450854700855, 0.1332799145299145, 0.13287927350427353, 0.13461538461538458, 0.13408119658119655, 0.13194444444444442, 0.1323450854700855, 0.1316773504273504, 0.13314636752136755, 0.13100961538461542, 0.1316773504273504, 0.13274572649572647, 0.13221153846153844, 0.12526709401709402, 0.13114316239316237, 0.12940705128205132, 0.13114316239316237, 0.13341346153846156, 0.13461538461538458, 0.13247863247863245, 0.13181089743589747, 0.1342147435897436, 0.13461538461538458, 0.13060897435897434, 0.13541666666666663, 0.1304754273504274, 0.13247863247863245, 0.13354700854700852, 0.1323450854700855, 0.13181089743589747, 0.12740384615384615, 0.1342147435897436, 0.12780448717948723, 0.12967414529914534, 0.1284722222222222, 0.12994123931623935, 0.13114316239316237, 0.13314636752136755, 0.12994123931623935, 0.1307425213675214, 0.13020833333333337, 0.1282051282051282, 0.12994123931623935, 0.1314102564102564, 0.13114316239316237, 0.1300747863247863, 0.1304754273504274, 0.13100961538461542, 0.13060897435897434, 0.12873931623931623, 0.13154380341880345, 0.12967414529914534, 0.12793803418803418, 0.1284722222222222, 0.13087606837606836, 0.13728632478632474, 0.13301282051282048, 0.1339476495726496, 0.12860576923076927, 0.13127670940170943, 0.12954059829059827, 0.13301282051282048, 0.13154380341880345, 0.13181089743589747], "accuracy_train_std": [0.08522176312336464, 0.08061133985667235, 0.07350454132065691, 0.06915662388613204, 0.06754978310432959, 0.062385776982042174, 0.060140650312475334, 0.058632276279130015, 0.05469312441754015, 0.05302159013557168, 0.051302637309146665, 0.049989366888910945, 0.047002119321357796, 0.046592691789832254, 0.04514874165188459, 0.04351052465332514, 0.042837275632032844, 0.04114129898969034, 0.038426418369462374, 0.03692782541832053, 0.03629909563493496, 0.03490605680209843, 0.033495474622547726, 0.03175412297901179, 0.03224913902536979, 0.03130621801686794, 0.029590391099234607, 0.029130252774858728, 0.029256200368644885, 0.02857703626606213, 0.026643361322496294, 0.027581144278363612, 0.02753469795532473, 0.027362809645088602, 0.026598764565155315, 0.023240099292994504, 0.025025793516814034, 0.02445214552704148, 0.024878694576370708, 0.023700635540863157, 0.023654236298283282, 0.024322505894326007, 0.023527060927577854, 0.023819295402123835, 0.022865271115721245, 0.022321071898379424, 0.021761297040518476, 0.021553890891471673, 0.02144427194895998, 0.023232664138706255, 0.019930329137454822, 0.020924141805143005, 0.021480598782830554, 0.02061229448519636, 0.021576897600552584, 0.021150612857617823, 0.02196109766915293, 0.020503799040161485, 0.02026365458237262, 0.01992292341672157, 0.02120938537087131, 0.02010851493228154, 0.020595631806560166, 0.020137449754891203, 0.019759827787698008, 0.02017070848997575, 0.019531746142450006, 0.01822241509321822, 0.01770990586048646, 0.018521647403989223, 0.018363098904876973, 0.018362857664077168, 0.017435966638341568, 0.01911322955572363, 0.017978270452233096, 0.019818882914007255, 0.018067324772312304, 0.019482580959926762, 0.0185899104724374, 0.016480590962846733, 0.017528123995788732, 0.01726644219249739, 0.017922249692761693, 0.019105347703696807, 0.017276188768187088, 0.016825228141193017, 0.017006692478762776, 0.017571666004188597, 0.01570712713743829, 0.01777022680823619, 0.016726716449677326, 0.01735949953729193, 0.017671233130661247, 0.016400108756655866, 0.016326794629581767, 0.01708901647787481, 0.016055951403421875, 0.0167523366278197, 0.01570698612121632, 0.016006471119088067, 0.019601364636359486, 0.016497247832020837, 0.014603872253930325, 0.017752253421492614, 0.014911533294278688, 0.016842974342658367, 0.01543380525433997, 0.015904803362191404, 0.017105129433589963, 0.01591599268559034, 0.0179889857889784, 0.01564738154234657, 0.016168522179776216, 0.0143623716583842, 0.014562540931758187, 0.016534530477490508, 0.017739022864483603, 0.014833942412972168, 0.016069723447788828, 0.015697817349560175, 0.016489542763321045, 0.016006471119088067, 0.016949502406974005, 0.01546482667694504, 0.01585925137084116, 0.015201066451862883, 0.015113973704202763, 0.014666358770782774, 0.014452020787392897, 0.015713136777255613, 0.014997438679184858, 0.016438959161265906, 0.013576422991041259, 0.016662252720902722, 0.015044643184042777, 0.013990694847589588, 0.014010549408005845, 0.01432633560411874, 0.01447590996006359, 0.014503559123755028, 0.016279787394986443, 0.015239122022315558, 0.014790128023039759, 0.01454123146262349, 0.016028596370771354, 0.014514550631182649, 0.014269074609587058, 0.013739810226003377, 0.016042357381681218, 0.013770727132714239, 0.014812126186824913, 0.013362131130808641, 0.013897612067237764, 0.013047793551639616, 0.01322902334271022, 0.01413494876486474, 0.013337347266564393, 0.015058310511393929, 0.013362628411193282, 0.014836032694265967, 0.01278248262014375, 0.015305618215642934, 0.013343386740469013, 0.013577870841569419, 0.013357343860472225, 0.014885609206518397, 0.012954774849529315, 0.012425924340285369, 0.013250708823813675, 0.012448541800373716, 0.015176841048393171, 0.01306622035658357, 0.013235405113671727, 0.01256782734016696, 0.012581918614333042, 0.012954625245057269, 0.012586230911481007, 0.01285970555810828, 0.013856431975182118, 0.013416164291755416, 0.013484402246185264, 0.012992100607487789, 0.012706359270988146, 0.015231034989374313, 0.01228017268197132, 0.013178299120905242, 0.013253132379827488, 0.012934583999996108, 0.014841555591963745, 0.01222863712703363, 0.012411299043448589, 0.012760305189414536, 0.013778847400703145, 0.012684310677585171, 0.011693584776275228, 0.01222621429870838, 0.013253779979238316, 0.011521176617868086, 0.0131554209882764, 0.012686995194740223, 0.013067576424239658, 0.01193181094056024, 0.011801151749874525, 0.011343488838036948, 0.012742153037149087, 0.012783110745049954, 0.012547786127991538, 0.013284494166455808, 0.012666902092501701, 0.012272504649312146, 0.012108708267470115, 0.01215615218961052, 0.0131770174825658, 0.01276375465060797, 0.011670903914611565, 0.011907561061259741, 0.01252766865384352, 0.011693276971539565, 0.01194248012543633, 0.012283396336266914, 0.01122986318566298, 0.011048518396715189, 0.012799734008125098, 0.0109550273459537, 0.01193357433324863, 0.011486109663470074, 0.011718655495094816, 0.01001463560110722, 0.01194841363247116, 0.011686479524909546, 0.013105235833713867, 0.012147585388762224, 0.01141679245425457, 0.012667076952501402, 0.011590180089396588, 0.010939751676029848, 0.011521176617868086, 0.012698272690840803, 0.01356203805968079, 0.012366602971465926, 0.011584612798005923, 0.011686479524909546, 0.011560712347046339, 0.013815109093192866, 0.010803835892116948, 0.01196915774331075, 0.010420871269413188, 0.011543360171987701, 0.010623767103054068, 0.010624627089721901, 0.012407729282771479, 0.012466854960701651], "accuracy_test_std": 0.06012611098957531, "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-3, 3], "rotation_range": [0, 0], "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.001, "patience_threshold": 1, "do_flip": true, "nb_data_augmentation": 1, "optimization": "adam", "batch_size": 32, "learning_rate_decay_method": "sqrt", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0, "valid_ratio": 0.15, "momentum": 0.9, "learning_rate_decay": 0.05}, "accuracy_valid_max": 0.874732905982906, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = 1234423\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-10, -4], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128, 256],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-3, 3)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_optimizer.learning_rate = learning_rate\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8681891025641025, "loss_train": [1.4475387334823608, 0.9990137219429016, 0.7878983020782471, 0.6776606440544128, 0.6002211570739746, 0.5443806052207947, 0.4948522746562958, 0.45812132954597473, 0.42595401406288147, 0.39555636048316956, 0.37050846219062805, 0.3499015271663666, 0.3274146318435669, 0.30883345007896423, 0.2925935983657837, 0.2766248285770416, 0.263767808675766, 0.2506279945373535, 0.2397850602865219, 0.23106297850608826, 0.2219437211751938, 0.21348708868026733, 0.20343688130378723, 0.1969565451145172, 0.19214874505996704, 0.1814853698015213, 0.17994794249534607, 0.1708817034959793, 0.16500981152057648, 0.1622021645307541, 0.15775275230407715, 0.15186801552772522, 0.1480020433664322, 0.1430206000804901, 0.14183709025382996, 0.13462644815444946, 0.13276660442352295, 0.13076846301555634, 0.12932905554771423, 0.12541314959526062, 0.11958134174346924, 0.11703488975763321, 0.11600103229284286, 0.1131821796298027, 0.11019597947597504, 0.1076560989022255, 0.10422290116548538, 0.10233648121356964, 0.1004798486828804, 0.10007285326719284, 0.09824718534946442, 0.09408979117870331, 0.09190479665994644, 0.08857239782810211, 0.08943890780210495, 0.08747223764657974, 0.08637121319770813, 0.08614964038133621, 0.08149126917123795, 0.08222348242998123, 0.07771997153759003, 0.07561682909727097, 0.07559270411729813, 0.07572400569915771, 0.07378493249416351, 0.07391971349716187, 0.07107078284025192, 0.06786196678876877, 0.06914864480495453, 0.065668486058712, 0.06504454463720322, 0.0651029720902443, 0.0626072958111763, 0.06179267540574074, 0.06206174194812775, 0.06172099709510803, 0.05936037376523018, 0.058533214032649994, 0.05732493847608566, 0.05908145755529404, 0.05646209418773651, 0.0552956685423851, 0.05321376398205757, 0.05530678853392601, 0.052335672080516815, 0.05253151059150696, 0.051204897463321686, 0.05057153105735779, 0.04798594117164612, 0.04881814867258072, 0.04813770204782486, 0.047459978610277176, 0.04610399156808853, 0.04420693963766098, 0.04505886882543564, 0.04450824484229088, 0.04349462687969208, 0.04355599358677864, 0.042761050164699554, 0.041949499398469925, 0.04047128185629845, 0.04031019285321236, 0.03879135847091675, 0.04101936146616936, 0.040135011076927185, 0.038950592279434204, 0.03633299469947815, 0.03835374861955643, 0.037607066333293915, 0.036995820701122284, 0.03517685830593109, 0.03653479367494583, 0.03549771383404732, 0.03342793509364128, 0.03493399918079376, 0.03469953313469887, 0.03423141688108444, 0.03304208815097809, 0.03171005845069885, 0.033200331032276154, 0.03196161612868309, 0.03346796706318855, 0.03203317150473595, 0.030617492273449898, 0.030744174495339394, 0.0298850666731596, 0.03192860633134842, 0.029094260185956955, 0.029882578179240227, 0.02823510952293873, 0.028286172077059746, 0.027714349329471588, 0.029606763273477554, 0.027574196457862854, 0.02836708165705204, 0.026928192004561424, 0.027002539485692978, 0.026555528864264488, 0.0259464830160141, 0.02587313950061798, 0.026448914781212807, 0.02447316236793995, 0.02622489258646965, 0.026258952915668488, 0.024792322888970375, 0.024975089356303215, 0.023295283317565918, 0.02423134818673134, 0.023353014141321182, 0.02349587343633175, 0.023314200341701508, 0.023159043863415718, 0.023273134604096413, 0.02167227305471897, 0.022998765110969543, 0.02058781497180462, 0.02300574630498886, 0.02199036069214344, 0.021817438304424286, 0.0215513426810503, 0.022640233859419823, 0.019906997680664062, 0.022100264206528664, 0.021797575056552887, 0.02087871916592121, 0.020560909062623978, 0.02011745795607567, 0.019899072125554085, 0.019870683550834656, 0.019858689978718758, 0.021040810272097588, 0.020192410796880722, 0.01852244697511196, 0.02010807767510414, 0.019810207188129425, 0.019391421228647232, 0.01856374368071556, 0.019070588052272797, 0.0176651943475008, 0.019991835579276085, 0.01770426332950592, 0.018377704545855522, 0.01654985547065735, 0.018624842166900635, 0.017732443287968636, 0.017777174711227417, 0.01790546625852585, 0.016896050423383713, 0.018009111285209656, 0.017993668094277382, 0.01756584644317627, 0.015790455043315887, 0.016591722145676613, 0.01673593744635582, 0.017232222482562065, 0.01603509485721588, 0.015560301952064037, 0.016719814389944077, 0.01612628996372223, 0.016509633511304855, 0.015737779438495636, 0.015432227402925491, 0.015734240412712097, 0.014066528528928757, 0.015345193445682526, 0.014668219722807407, 0.015232929028570652, 0.01528611034154892, 0.015029448084533215, 0.014311094768345356, 0.014969144947826862, 0.014920991845428944, 0.01518295332789421, 0.014851579442620277, 0.014151706360280514, 0.014070646837353706, 0.014264505356550217, 0.014922930859029293, 0.014108060859143734, 0.013724122196435928, 0.013831611722707748, 0.013230929151177406, 0.014137791469693184, 0.014350997284054756, 0.012669123709201813, 0.01284856628626585, 0.013934423215687275, 0.012255558744072914, 0.013754443265497684, 0.012627164833247662, 0.01281213853508234, 0.013673140667378902, 0.012975969351828098, 0.01220235787332058, 0.012491417117416859, 0.01272991206496954, 0.012735364027321339, 0.012754186056554317, 0.012948159128427505, 0.01410158071666956, 0.01233292743563652, 0.012231644243001938, 0.012586823664605618, 0.012123768217861652, 0.012279924005270004, 0.010353539139032364, 0.01232499722391367, 0.011833610013127327, 0.011294287629425526, 0.011731903068721294, 0.01102166622877121, 0.011763117276132107], "accuracy_train_first": 0.6169286521084337, "model": "residualv4", "loss_std": [0.2944452464580536, 0.19384509325027466, 0.17746350169181824, 0.16776303946971893, 0.16106173396110535, 0.15297651290893555, 0.14806516468524933, 0.1420540064573288, 0.13509854674339294, 0.12821340560913086, 0.1262245774269104, 0.12012669444084167, 0.11619404703378677, 0.11134717613458633, 0.10967723280191422, 0.10208465903997421, 0.10182789713144302, 0.09597014635801315, 0.09352371841669083, 0.09495437890291214, 0.09088066220283508, 0.08804154396057129, 0.08553778380155563, 0.08462189882993698, 0.08299688994884491, 0.07846206426620483, 0.07898789644241333, 0.07689396291971207, 0.07559385150671005, 0.0734192356467247, 0.07326631247997284, 0.07063234597444534, 0.07053729146718979, 0.06931956857442856, 0.06742730736732483, 0.06606429070234299, 0.06467750668525696, 0.06439528614282608, 0.06418437510728836, 0.06454665958881378, 0.06249368563294411, 0.06040020287036896, 0.06174354627728462, 0.061565011739730835, 0.05960075557231903, 0.06132741644978523, 0.05716174468398094, 0.05739094316959381, 0.055386051535606384, 0.05531011149287224, 0.05731407180428505, 0.05409562960267067, 0.05294716730713844, 0.052674081176519394, 0.05452727526426315, 0.051370274275541306, 0.051067788153886795, 0.0513954795897007, 0.0475742481648922, 0.05090569704771042, 0.04687277600169182, 0.046080369502305984, 0.047175150364637375, 0.04715825989842415, 0.047242484986782074, 0.048176202923059464, 0.04594842344522476, 0.04249345511198044, 0.04603840038180351, 0.043517205864191055, 0.04232111945748329, 0.043277375400066376, 0.04233574494719505, 0.04231501743197441, 0.04623496159911156, 0.042864758521318436, 0.04176837578415871, 0.04086124897003174, 0.04115096107125282, 0.041516371071338654, 0.03960045054554939, 0.04031282290816307, 0.03860730677843094, 0.04006318747997284, 0.03745415434241295, 0.03826465457677841, 0.038262173533439636, 0.03857842832803726, 0.036000628024339676, 0.03745638206601143, 0.03736903890967369, 0.03485710546374321, 0.03711071237921715, 0.03376585617661476, 0.036543577909469604, 0.03438371419906616, 0.03429349884390831, 0.03310925513505936, 0.034486785531044006, 0.033961281180381775, 0.03210030496120453, 0.03237365931272507, 0.032232895493507385, 0.03395191952586174, 0.03260241448879242, 0.0322735570371151, 0.030020564794540405, 0.0332682766020298, 0.0321592316031456, 0.03126983344554901, 0.03178095445036888, 0.03168729320168495, 0.03253640606999397, 0.029865937307476997, 0.030188191682100296, 0.030034299939870834, 0.031662460416555405, 0.02906731143593788, 0.028967631980776787, 0.029306137934327126, 0.029593944549560547, 0.03129767253994942, 0.03052341751754284, 0.027546890079975128, 0.02842492237687111, 0.028274593874812126, 0.0306679867208004, 0.0283407773822546, 0.028945202007889748, 0.02747180499136448, 0.027468889951705933, 0.02641032263636589, 0.02928200736641884, 0.02639790065586567, 0.02782106213271618, 0.027478208765387535, 0.02718541771173477, 0.027099404484033585, 0.025753464549779892, 0.026086023077368736, 0.027460439130663872, 0.024939892813563347, 0.026945870369672775, 0.02677101455628872, 0.0257762111723423, 0.025919657200574875, 0.024759789928793907, 0.025670133531093597, 0.025645487010478973, 0.023719916120171547, 0.02509252354502678, 0.024620553478598595, 0.027182135730981827, 0.022864582017064095, 0.025241870433092117, 0.023929936811327934, 0.025326060131192207, 0.023693934082984924, 0.023659735918045044, 0.02405492775142193, 0.024231133982539177, 0.021978680044412613, 0.0243484266102314, 0.025122344493865967, 0.02254011482000351, 0.023324977606534958, 0.02230782061815262, 0.02239874377846718, 0.022313192486763, 0.023150548338890076, 0.02529132179915905, 0.024606978520751, 0.022022346034646034, 0.023873796686530113, 0.02336549200117588, 0.0234939306974411, 0.02146182209253311, 0.021843627095222473, 0.02117818593978882, 0.02383098565042019, 0.021210135892033577, 0.023262498900294304, 0.01980140618979931, 0.023216024041175842, 0.0213885847479105, 0.02304692193865776, 0.02106456086039543, 0.021144894883036613, 0.022462883964180946, 0.022816265001893044, 0.020998988300561905, 0.020253824070096016, 0.021754033863544464, 0.021139010787010193, 0.02194938436150551, 0.020970702171325684, 0.019622141495347023, 0.021320722997188568, 0.02086055465042591, 0.02112726680934429, 0.020707901567220688, 0.019880041480064392, 0.021059460937976837, 0.01853485405445099, 0.020229674875736237, 0.019640643149614334, 0.020961379632353783, 0.02030215784907341, 0.02128024958074093, 0.018736643716692924, 0.01992790400981903, 0.02008853852748871, 0.020099986344575882, 0.020747797563672066, 0.019909324124455452, 0.0189498458057642, 0.018713416531682014, 0.020152300596237183, 0.01895073615014553, 0.018091697245836258, 0.01905362494289875, 0.018820373341441154, 0.020709849894046783, 0.020115027204155922, 0.01774732954800129, 0.01735851541161537, 0.019562650471925735, 0.017425918951630592, 0.019535360857844353, 0.01757325418293476, 0.018047906458377838, 0.019166767597198486, 0.019207745790481567, 0.017240328714251518, 0.01837920770049095, 0.018671929836273193, 0.018719293177127838, 0.01898179017007351, 0.01862601935863495, 0.021295875310897827, 0.018082112073898315, 0.01783670112490654, 0.01905418373644352, 0.017098214477300644, 0.01866687275469303, 0.014921699650585651, 0.017685456201434135, 0.01789994351565838, 0.016857944428920746, 0.017224643379449844, 0.016771478578448296, 0.017782295122742653]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:07 2016", "state": "available"}], "summary": "79aa948692ac0628d29e61204ec18410"}