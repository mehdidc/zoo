{"content": {"hp_model": {"f0": 16, "f1": 64, "f2": 32, "f3": 16, "nonlin": "very_leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.012060299985381986, 0.013694762141826937, 0.018020752448264295, 0.014314175006948619, 0.01835791129341014, 0.016744459514589426, 0.017712522271807685, 0.019028780914082312, 0.01890985590547904, 0.01733932888003205, 0.017901123183541118, 0.01687067361730316, 0.019185065007269377, 0.019379718611622365, 0.019195037944494016, 0.020286449892031894, 0.01985859983768539, 0.020295831763610897, 0.021057831142764518, 0.021507144498218906, 0.021556527252140155, 0.022036877010025574, 0.021893510090155676, 0.020255164842247757, 0.020263831724359957, 0.020032984764376385, 0.02039264285612565, 0.020912232024416957, 0.02091546538150968, 0.018344194713475953, 0.019917800793462802, 0.017803550881876625, 0.02018967695264722, 0.018203374783686995, 0.019494971255979536, 0.020211007133555257, 0.018345545780993337, 0.02047180673250551, 0.020683932223014453, 0.020341554778676003, 0.020938579577040622, 0.020907470619812445, 0.021507405332574645, 0.0202713820627272, 0.01933822766496132, 0.021913236071964968, 0.02101685312297518, 0.02181512405296556, 0.022009989064034414, 0.021244793881663754, 0.020774879366772896, 0.021098677802516147, 0.021209948676947508, 0.02138511808242886, 0.02106324994519486, 0.020116029533934758, 0.021951283452212485, 0.02075014413575209, 0.021422686467773046, 0.020550938793401308, 0.019293295214408018, 0.020497032775962272, 0.020418766362132725, 0.019955118395607208, 0.020133733544323906, 0.02106428978335182, 0.021750953178374405, 0.02062902963289831, 0.018962101769322726, 0.021764897944611276, 0.020763740762666714, 0.020540150952714376, 0.02047201466896432, 0.020641083485928344, 0.02076737194234724, 0.01997562055076262, 0.020519057933794726, 0.020089952139432505, 0.02168191806716524, 0.020534346412320787, 0.020096671196663484, 0.02074622051220564, 0.02157168483621156, 0.02189372428335892, 0.02040241502886449, 0.020495806014256136, 0.019831683490774403, 0.020551703175189097, 0.020621325669726086, 0.021725697179921257, 0.020078081078790513, 0.021499552465353188, 0.020571832837019785, 0.020369301851056665, 0.02002886379149994, 0.021370165022115386, 0.02058341913853208, 0.02033854865421283, 0.0200983600269721, 0.020185754384059625, 0.019841602867697497, 0.02083824301584823, 0.021103867752872478, 0.021790025470219406, 0.021487274374987143, 0.01995285156288431, 0.021385952729780337, 0.020102731610299136, 0.020297650924590068, 0.021750953178374405, 0.02035378355586969, 0.02065751144590123, 0.02108424604432236, 0.020284042802952935, 0.021108975629761977, 0.019843064108540136, 0.0207332830188248, 0.02013923854230302, 0.020697103941088903, 0.020762180024858592, 0.021734505019220727, 0.01925824071251428, 0.020670109388172382, 0.020866826900203476, 0.02078437384329944, 0.01940208804514379, 0.020660140271435697, 0.02069316545227961, 0.021321012238111905, 0.021322494182054418, 0.021038346679535077, 0.020588146011438758, 0.020450552896099563, 0.020886309503083338, 0.020843567187125725, 0.022084063154532235, 0.020853676292198975, 0.02110220455531278, 0.02096618685397576, 0.02095013852041757, 0.020794028704172525, 0.020284821667321613, 0.021153898073396106, 0.02010421900596714, 0.020948934678576076, 0.019308892348191213], "moving_avg_accuracy_train": [0.013350950391057584, 0.02937903927014119, 0.04674981416833932, 0.06440374145282507, 0.08227283415993274, 0.099784659674818, 0.11654962272865403, 0.1324935279460987, 0.14765208628105692, 0.16188526843368212, 0.17512986310078824, 0.18747553658214536, 0.19902842098917628, 0.2097864150209803, 0.21972194272338577, 0.22899169154005605, 0.23764596331789797, 0.24565569705486048, 0.2530130866978425, 0.25989984038963077, 0.26616999832533544, 0.2719712145376985, 0.2773690204383491, 0.2823967455632111, 0.28702407682084347, 0.29119092800388463, 0.29515275470792585, 0.29872530209035403, 0.3021010300023965, 0.3051554972137202, 0.3079742721681972, 0.31052504842244616, 0.312995205309622, 0.3151834332271186, 0.31725278370285637, 0.3191547987584198, 0.3208712265572271, 0.32239973553448703, 0.32395439402353554, 0.32530715578512626, 0.32662458672054867, 0.32785910268742885, 0.32892598923024, 0.32987220017709423, 0.3307633896566624, 0.3316049156203982, 0.33232047235800766, 0.3330481427301803, 0.33368913122109733, 0.33427063511172295, 0.3347777486204381, 0.33517831115803437, 0.33568533786568977, 0.3360416444549513, 0.33639952276623913, 0.33668906116306485, 0.3369520069178363, 0.3372351971221398, 0.33751786194527095, 0.33773974425157444, 0.337960400715352, 0.33817054517916195, 0.33839222727992424, 0.338533648499191, 0.33867952878700724, 0.3388108210460419, 0.3389172862374879, 0.3389969370145789, 0.3390569609210944, 0.33914117332266347, 0.3392309514257515, 0.33936751924114056, 0.3394300485035806, 0.3395397672135769, 0.33965010374780247, 0.33967274881552867, 0.33974188540384476, 0.33977853169642447, 0.33978368367166945, 0.33982087253272325, 0.33985663160766244, 0.3398516844917928, 0.3399262150493965, 0.33994911472385886, 0.3400254919534848, 0.3400686548232434, 0.34006564872745465, 0.340056003843635, 0.3400497206946443, 0.34007422069743903, 0.3401125106928022, 0.3401168529005615, 0.34015556602205016, 0.34017413178972333, 0.34017921523658157, 0.3401442628089921, 0.3401059022753705, 0.3400620411510542, 0.34004342038081775, 0.34008944070546204, 0.3400983069143086, 0.3401389106832413, 0.340149877438376, 0.3400993657465871, 0.3401026612513396, 0.3401311317448841, 0.34016380273314023, 0.34012581335591335, 0.34011948865330466, 0.34017414214354824, 0.3401815497038335, 0.34012550958787063, 0.3401214683132382, 0.3401922359279738, 0.34019082261456923, 0.3402012124253715, 0.3401687105765221, 0.3401161713756436, 0.3401246896662815, 0.34011611613500775, 0.34013397659376615, 0.3401337389161633, 0.3401475119479966, 0.34014130648617036, 0.34018454969552675, 0.3401653398637094, 0.34017831399841636, 0.34013186199941453, 0.3401458227229227, 0.3401607125228896, 0.34012303216668777, 0.340126250129421, 0.3401384829399377, 0.34014716732059325, 0.34012010603104037, 0.3401886847251862, 0.340199252276108, 0.3402134133695566, 0.34020061776557436, 0.34021467835889513, 0.34022275469290225, 0.34016255802921375, 0.34017119609856994, 0.34018590975860036, 0.34017353936690425, 0.34018794660246376], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 498113622, "moving_var_accuracy_train": [0.0016042308871003262, 0.0037559044964325266, 0.006096008431864107, 0.008291357925770888, 0.010335962400770676, 0.01206234245646769, 0.013385684086589304, 0.014334988700176108, 0.014969526847307483, 0.015295825430285011, 0.015345016478319935, 0.015182255713862504, 0.014865252385576223, 0.014420337067313584, 0.013866735757109625, 0.013253416369516102, 0.012602142512607255, 0.01191933077217985, 0.011214578336189798, 0.010519966890273055, 0.009821804126093875, 0.009142510699367747, 0.008490486406300858, 0.007868939945051293, 0.007274755701656918, 0.00670354397052573, 0.006174454210968844, 0.005671876643069208, 0.005207248829187572, 0.004770491875774276, 0.00436495211839273, 0.003987015042046619, 0.003643228613267317, 0.0033220008247107904, 0.003028340644762636, 0.002758065531730681, 0.00250877409805428, 0.0022789237454909302, 0.002072784038099973, 0.0018819753137425735, 0.0017093984007947878, 0.0015521748277676475, 0.0014072015670479672, 0.0012745392467466907, 0.0011542332902684585, 0.0010451834547703853, 0.0009452733022959919, 0.0008555115096012338, 0.000773658154850503, 0.000699335660338767, 0.0006317165713013826, 0.0005699889672899745, 0.0005153037553014599, 0.00046491596924127496, 0.00041957706428835944, 0.0003783738502086511, 0.0003411587294173567, 0.0003077646267019421, 0.00027770725865186986, 0.00025037961860733784, 0.00022577986022166553, 0.0002035993204605287, 0.00018368167499866123, 0.00016549350715012507, 0.00014913568596047267, 0.00013437725627996711, 0.00012104154418487713, 0.00010899448798301022, 9.812746500888967e-05, 8.837854406520292e-05, 7.96132306288294e-05, 7.181976447974768e-05, 6.467297720972455e-05, 5.8314023246661396e-05, 5.2592188279059404e-05, 4.7337584642984386e-05, 4.2646844989281844e-05, 3.8394247047192205e-05, 3.455506122811331e-05, 3.111200220778029e-05, 2.8012310389966823e-05, 2.5211299616568985e-05, 2.2740162891062498e-05, 2.047086615777058e-05, 1.847628087284151e-05, 1.664542008548952e-05, 1.4980959406447586e-05, 1.348370067985788e-05, 1.2135685913523243e-05, 1.0927519573403386e-05, 9.847962729767237e-06, 8.863336149704536e-06, 7.990490886712675e-06, 7.194543987605054e-06, 6.475322161732193e-06, 5.838784995308566e-06, 5.268150270635327e-06, 4.75864942760841e-06, 4.285905082605363e-06, 3.87637540686815e-06, 3.489445353115132e-06, 3.1553388122675094e-06, 2.8408873585044203e-06, 2.5797615017203455e-06, 2.3218830947124754e-06, 2.0969899062652454e-06, 1.8968974569014191e-06, 1.720196446250071e-06, 1.5485368183928583e-06, 1.4205661725157816e-06, 1.2790034028086221e-06, 1.1793675139019805e-06, 1.061577749617672e-06, 1.0004924723142158e-06, 9.004612021758113e-07, 8.113866154747819e-07, 7.39755285534993e-07, 6.906230656420492e-07, 6.22213810556375e-07, 5.606539784472593e-07, 5.074595444860651e-07, 4.567140984532449e-07, 4.127499562608466e-07, 3.718215304430539e-07, 3.514691537977105e-07, 3.1964339716399575e-07, 2.8919401099014276e-07, 2.7969470379253617e-07, 2.5347934962112416e-07, 2.3012676994649104e-07, 2.198923761433211e-07, 1.9799633608636056e-07, 1.795434773559706e-07, 1.6226789582670699e-07, 1.5263192677441222e-07, 1.7969606971374962e-07, 1.6273152093473125e-07, 1.4826319795020628e-07, 1.349104254866184e-07, 1.231986854987393e-07, 1.1146586148781777e-07, 1.329320202121046e-07, 1.2031036437072027e-07, 1.1022775405706492e-07, 1.0058221796780155e-07, 9.239211209921879e-08], "duration": 77494.049943, "accuracy_train": [0.13350950391057587, 0.17363183918189368, 0.20308678825212254, 0.22328908701319677, 0.24309466852390182, 0.2573910893087855, 0.26743429021317827, 0.27598867490310075, 0.28407911129568103, 0.289983907807309, 0.2943312151047434, 0.29858659791435954, 0.3030043806524548, 0.3066083613072167, 0.30914169204503505, 0.31241943089008856, 0.31553440931847543, 0.3177433006875231, 0.31922959348468066, 0.32188062361572534, 0.3226014197466777, 0.3241821604489664, 0.3259492735442045, 0.32764627168696936, 0.32867005813953487, 0.32869258865125506, 0.3308091950442968, 0.33087822853220744, 0.3324825812107789, 0.33264570211563305, 0.3333432467584902, 0.3334820347106866, 0.3352266172942045, 0.3348774844845884, 0.33587693798449614, 0.3362729342584902, 0.33631907674649314, 0.3361563163298265, 0.3379463204249723, 0.3374820116394426, 0.3384814651393503, 0.3389697463893503, 0.33852796811554076, 0.33838809869878184, 0.33878409497277595, 0.33917864929402, 0.33876048299649314, 0.3395971760797342, 0.3394580276393503, 0.3395041701273533, 0.3393417701988741, 0.3387833739964009, 0.3402485782345884, 0.33924840375830567, 0.33962042756782945, 0.33929490673449614, 0.3393185187107789, 0.3397839089608712, 0.34006184535345146, 0.33973668500830567, 0.3399463088893503, 0.34006184535345146, 0.3403873661867848, 0.33980643947259137, 0.3399924513773533, 0.3399924513773533, 0.339875472960502, 0.3397137940083979, 0.3395971760797342, 0.3398990849367848, 0.34003895435354375, 0.34059662957964193, 0.33999281186554076, 0.34052723560354375, 0.3406431325558324, 0.3398765544250646, 0.34036411469868955, 0.34010834832964193, 0.3398300514488741, 0.34015557228220744, 0.3401784632821152, 0.3398071604489664, 0.34059699006782945, 0.34015521179402, 0.34071288702011815, 0.34045712065107053, 0.34003859386535623, 0.33996919988925806, 0.3399931723537283, 0.34029472072259137, 0.34045712065107053, 0.34015593277039496, 0.3405039841154485, 0.34034122369878184, 0.34022496625830567, 0.3398296909606866, 0.33976065747277595, 0.33966729103220744, 0.33987583344868955, 0.340503623627261, 0.34017810279392763, 0.340504344603636, 0.3402485782345884, 0.33964476052048725, 0.3401323207941122, 0.3403873661867848, 0.3404578416274455, 0.3397839089608712, 0.3400625663298265, 0.3406660235557401, 0.3402482177464009, 0.3396211485442045, 0.3400850968415467, 0.3408291444605943, 0.34017810279392763, 0.34029472072259137, 0.33987619393687707, 0.33964331856773716, 0.3402013542820229, 0.34003895435354375, 0.34029472072259137, 0.34013159981773716, 0.34027146923449614, 0.3400854573297342, 0.3405737385797342, 0.3399924513773533, 0.3402950812107789, 0.3397137940083979, 0.34027146923449614, 0.34029472072259137, 0.3397839089608712, 0.34015521179402, 0.3402485782345884, 0.34022532674649314, 0.3398765544250646, 0.3408058929724991, 0.34029436023440385, 0.3403408632105943, 0.3400854573297342, 0.34034122369878184, 0.3402954416989664, 0.339620788056017, 0.34024893872277595, 0.3403183326988741, 0.340062205841639, 0.3403176117224991], "end": "2016-01-25 07:01:59.017000", "learning_rate_per_epoch": [0.0009573838324286044, 0.0008709345129318535, 0.0007922913646325469, 0.0007207494927570224, 0.0006556676235049963, 0.0005964625161141157, 0.0005426034913398325, 0.0004936077748425305, 0.00044903624802827835, 0.00040848940261639655, 0.0003716038481798023, 0.0003380489652045071, 0.0003075239947065711, 0.00027975536067970097, 0.00025449416716583073, 0.000231513986364007, 0.0002106088650180027, 0.00019159141811542213, 0.00017429119907319546, 0.0001585531426826492, 0.00014423619722947478, 0.00013121202937327325, 0.00011936391820199788, 0.00010858565656235442, 9.878064884105697e-05, 8.986100874608383e-05, 8.174678805517033e-05, 7.436526357196271e-05, 6.76502677379176e-05, 6.154162110760808e-05, 5.598456846200861e-05, 5.092930587125011e-05, 4.633051867131144e-05, 4.214699220028706e-05, 3.834122617263347e-05, 3.487911089905538e-05, 3.172961442032829e-05, 2.8864511477877386e-05, 2.6258119760314003e-05, 2.38870779867284e-05, 2.1730134903918952e-05, 1.9767958292504773e-05, 1.798296216293238e-05, 1.6359146684408188e-05, 1.4881957213219721e-05, 1.3538154234993272e-05, 1.2315693311393261e-05, 1.120361775974743e-05, 1.019195951812435e-05, 9.271651833842043e-06, 8.43444558995543e-06, 7.672836545680184e-06, 6.979998943279497e-06, 6.349722752929665e-06, 5.776359103037976e-06, 5.25476843904471e-06, 4.780276412930107e-06, 4.348629772721324e-06, 3.95595952795702e-06, 3.598746388888685e-06, 3.273788706792402e-06, 2.9781738248857437e-06, 2.7092523851024453e-06, 2.4646137717354577e-06, 2.242065420432482e-06, 2.0396125819388544e-06, 1.8554407006377005e-06, 1.68789915733214e-06, 1.5354861488958704e-06, 1.3968356142868288e-06, 1.2707049563687178e-06, 1.155963559540396e-06, 1.0515830126678338e-06, 9.566277867634199e-07, 8.702467084731325e-07, 7.916656272755063e-07, 7.201802532108559e-07, 6.551497904183634e-07, 5.95991423324449e-07, 5.42174916517979e-07, 4.932178967465006e-07, 4.4868156123811787e-07, 4.081667555055901e-07, 3.713103353675251e-07, 3.3778195529521327e-07, 3.072811125548469e-07, 2.79534418723415e-07, 2.542931838434015e-07, 2.3133115689688566e-07, 2.1044253628588194e-07, 1.9144010821037227e-07, 1.7415355557659495e-07, 1.5842793743559014e-07, 1.4412229631943774e-07, 1.311084218968972e-07, 1.1926965726161143e-07, 1.0849991127770409e-07, 9.870264250366745e-08, 8.979004206821628e-08, 8.168223075699643e-08, 7.430653425899436e-08, 6.759684367807495e-08, 6.149301867708346e-08, 5.5940354570793716e-08, 5.088908139327941e-08, 4.629392691413159e-08, 4.211370452367191e-08, 3.8310943750730075e-08, 3.485156341298534e-08, 3.1704555425449144e-08, 2.884171479422548e-08, 2.623738204476922e-08, 2.386821229549696e-08, 2.171297275310735e-08, 1.9752347313328755e-08, 1.7968760701592146e-08, 1.634622748269976e-08, 1.4870204623207428e-08, 1.3527462705553717e-08, 1.2305966912151689e-08, 1.1194769555800121e-08, 1.0183910603700497e-08, 9.264329747793454e-09, 8.427784692344176e-09, 7.666777435133554e-09, 6.974486765898291e-09, 6.344708314287573e-09, 5.771797262354994e-09, 5.250618606567059e-09, 4.776500972525355e-09, 4.345194870580826e-09, 3.952834948250938e-09, 3.5959040189936786e-09, 3.271203086185892e-09, 2.9758218111908263e-09, 2.7071127561839603e-09, 2.462667403335672e-09, 2.2402948385291666e-09, 2.0380019893906365e-09, 1.8539755286539616e-09, 1.6865662200160614e-09, 1.5342735970591548e-09, 1.3957326405744652e-09, 1.2697015661089495e-09, 1.1550507217350514e-09, 1.0507525960434805e-09], "accuracy_valid": [0.1282900155308735, 0.16527879094503012, 0.19549192865210843, 0.22294892460466867, 0.24275520048945784, 0.25882877211972893, 0.2657970750188253, 0.2738743058170181, 0.2824604080384036, 0.28945959619728917, 0.29337614128388556, 0.2975574171686747, 0.30059887989457834, 0.30291821583207834, 0.30377270801957834, 0.30621411426957834, 0.3078113234186747, 0.3117175734186747, 0.3136809935052711, 0.3152679075677711, 0.31784167921686746, 0.31942859327936746, 0.32052722609186746, 0.32189058970256024, 0.3244643613516566, 0.324474656438253, 0.326549851750753, 0.3259292051016566, 0.328869187688253, 0.3282794262989458, 0.3299884106739458, 0.3302531414721386, 0.3315650296498494, 0.33134147919804213, 0.3314532544239458, 0.3322974515248494, 0.33244011201054213, 0.331554734563253, 0.3340167309864458, 0.3325415921498494, 0.3329078030873494, 0.3330298733998494, 0.3344947171498494, 0.3335284497364458, 0.3334063794239458, 0.3349829983998494, 0.3341285062123494, 0.3346167874623494, 0.3355933499623494, 0.3349829983998494, 0.3340064358998494, 0.3342608716114458, 0.3347388577748494, 0.3344947171498494, 0.3352374341114458, 0.3337725903614458, 0.3349829983998494, 0.3356036450489458, 0.3359698559864458, 0.33598015107304213, 0.3364787274096386, 0.33659050263554213, 0.3357257153614458, 0.3346270825489458, 0.33634636201054213, 0.3354815747364458, 0.3358374905873494, 0.33671257294804213, 0.3362345867846386, 0.3355933499623494, 0.3360919262989458, 0.3351153637989458, 0.3352374341114458, 0.3353595044239458, 0.3356036450489458, 0.3348712231739458, 0.3356036450489458, 0.3352374341114458, 0.3364581372364458, 0.3351153637989458, 0.3347491528614458, 0.33646843232304213, 0.3352271390248494, 0.3351050687123494, 0.33671257294804213, 0.3356036450489458, 0.33549186982304213, 0.33683464326054213, 0.3362139966114458, 0.3363360669239458, 0.3352374341114458, 0.3363360669239458, 0.3354815747364458, 0.33646843232304213, 0.3351153637989458, 0.33707878388554213, 0.3354815747364458, 0.3353595044239458, 0.33573601044804213, 0.3354815747364458, 0.33646843232304213, 0.3360919262989458, 0.3354815747364458, 0.3351050687123494, 0.3348609280873494, 0.33610222138554213, 0.3360919262989458, 0.3353595044239458, 0.3354815747364458, 0.3358374905873494, 0.33549186982304213, 0.3352374341114458, 0.33683464326054213, 0.3357257153614458, 0.3356036450489458, 0.33561394013554213, 0.3353595044239458, 0.3349932934864458, 0.3357257153614458, 0.3354815747364458, 0.3358374905873494, 0.33536979951054213, 0.3354815747364458, 0.3360919262989458, 0.3359698559864458, 0.33585808076054213, 0.3360919262989458, 0.3354815747364458, 0.3365802075489458, 0.3356036450489458, 0.3367022778614458, 0.3352374341114458, 0.3349932934864458, 0.3357257153614458, 0.33634636201054213, 0.3362037015248494, 0.3353595044239458, 0.3363360669239458, 0.3363360669239458, 0.33671257294804213, 0.3362139966114458, 0.3352374341114458, 0.3359698559864458, 0.33646843232304213, 0.3338843655873494, 0.33500358857304213], "accuracy_test": 0.33811383928571426, "start": "2016-01-24 09:30:24.967000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0], "accuracy_train_last": 0.3403176117224991, "batch_size_eval": 1024, "accuracy_train_std": [0.011345467895131916, 0.0116724582571878, 0.01302852370754875, 0.013609793562228295, 0.012849042755065711, 0.012850161966135615, 0.013433031230057872, 0.013966603685009812, 0.014031773151916147, 0.014862946970856626, 0.015173022973247952, 0.014631051447672802, 0.015940106704724562, 0.01576483213822848, 0.01540727228714393, 0.015785951525870003, 0.01588566912635985, 0.016730642790579876, 0.016345454539712874, 0.0162322813229638, 0.016936411443723952, 0.01731494023197578, 0.01662598319345574, 0.01697951947390029, 0.01664947207942482, 0.01636175747535968, 0.01678379529776806, 0.016804270483609354, 0.017149729305935286, 0.01711087307525704, 0.0171816028794933, 0.01615790927728149, 0.016771749457556627, 0.017153170842485643, 0.017105645528314797, 0.017348263978187943, 0.01732641737135551, 0.01711896829996646, 0.016834994344621254, 0.017565417660766088, 0.01748322827388193, 0.017542280052904768, 0.017917773243716666, 0.017425017682139124, 0.0173378508123377, 0.01754309289633811, 0.017789680618164955, 0.0173287393879592, 0.01757983482567732, 0.017656885651391188, 0.0175697927896743, 0.017732954991656412, 0.017227128646784384, 0.017315675690167658, 0.01728116193365521, 0.01741527526414454, 0.01778682795247459, 0.017392960852443767, 0.017057043297112273, 0.01732492113433055, 0.017040103914837658, 0.016990352347884433, 0.017557608844136832, 0.01674878543465359, 0.016939591152483776, 0.017209549678552514, 0.017205426611937847, 0.01751825122585601, 0.017012686054643216, 0.017244354582729837, 0.017792933589131858, 0.017459728272917842, 0.01694354448753563, 0.017231570033781642, 0.017219313260003873, 0.01682920159279399, 0.016635954899769618, 0.017329765818461452, 0.017106239174842174, 0.017389106009924476, 0.017082442780951567, 0.017371471262821736, 0.017382831261483146, 0.017143254365749466, 0.017679878939193493, 0.01730715848673509, 0.01675851786535, 0.017238078646160893, 0.01738654601531669, 0.01727869137658633, 0.01697867177781688, 0.01755918301720372, 0.01734869751191377, 0.017651856340128797, 0.01721124744617139, 0.017321313991108624, 0.017531350631727873, 0.016716620081293144, 0.01737101488490712, 0.016808909924641874, 0.017140818019429536, 0.01729618819998088, 0.017342730780624725, 0.017465961887871473, 0.017515060045745153, 0.01731145746250314, 0.017342475229445667, 0.016885552528889864, 0.017554270314436637, 0.017516879924920806, 0.017105508815803208, 0.01760639027827732, 0.017768085688110995, 0.01683753292155199, 0.01707844338424781, 0.017057819151894974, 0.017879902672250897, 0.016819988310587397, 0.01699600647334703, 0.01735754255596312, 0.017900901427078373, 0.01700334138130182, 0.017473335295394184, 0.01741442335096544, 0.017517195804801444, 0.017558299686351527, 0.017750521816997517, 0.01722019011857287, 0.017175816752502442, 0.017357360382514373, 0.01732887322196763, 0.01735126606773056, 0.017664448773606248, 0.01724676015684865, 0.0173592003923746, 0.01752022351005908, 0.017198333609244575, 0.017356082148891814, 0.017076035357668516, 0.01746303882777529, 0.01738850355368099, 0.017610395880186425, 0.01726252693685613, 0.01716923304664926, 0.01737883715618758, 0.017310228203170096], "accuracy_test_std": 0.008326521184640398, "error_valid": [0.8717099844691265, 0.8347212090549698, 0.8045080713478916, 0.7770510753953314, 0.7572447995105421, 0.7411712278802711, 0.7342029249811747, 0.7261256941829819, 0.7175395919615963, 0.7105404038027108, 0.7066238587161144, 0.7024425828313253, 0.6994011201054217, 0.6970817841679217, 0.6962272919804217, 0.6937858857304217, 0.6921886765813253, 0.6882824265813253, 0.6863190064947289, 0.6847320924322289, 0.6821583207831325, 0.6805714067206325, 0.6794727739081325, 0.6781094102974398, 0.6755356386483433, 0.675525343561747, 0.673450148249247, 0.6740707948983433, 0.671130812311747, 0.6717205737010542, 0.6700115893260542, 0.6697468585278614, 0.6684349703501506, 0.6686585208019579, 0.6685467455760542, 0.6677025484751506, 0.6675598879894579, 0.668445265436747, 0.6659832690135542, 0.6674584078501506, 0.6670921969126506, 0.6669701266001506, 0.6655052828501506, 0.6664715502635542, 0.6665936205760542, 0.6650170016001506, 0.6658714937876506, 0.6653832125376506, 0.6644066500376506, 0.6650170016001506, 0.6659935641001506, 0.6657391283885542, 0.6652611422251506, 0.6655052828501506, 0.6647625658885542, 0.6662274096385542, 0.6650170016001506, 0.6643963549510542, 0.6640301440135542, 0.6640198489269579, 0.6635212725903614, 0.6634094973644579, 0.6642742846385542, 0.6653729174510542, 0.6636536379894579, 0.6645184252635542, 0.6641625094126506, 0.6632874270519579, 0.6637654132153614, 0.6644066500376506, 0.6639080737010542, 0.6648846362010542, 0.6647625658885542, 0.6646404955760542, 0.6643963549510542, 0.6651287768260542, 0.6643963549510542, 0.6647625658885542, 0.6635418627635542, 0.6648846362010542, 0.6652508471385542, 0.6635315676769579, 0.6647728609751506, 0.6648949312876506, 0.6632874270519579, 0.6643963549510542, 0.6645081301769579, 0.6631653567394579, 0.6637860033885542, 0.6636639330760542, 0.6647625658885542, 0.6636639330760542, 0.6645184252635542, 0.6635315676769579, 0.6648846362010542, 0.6629212161144579, 0.6645184252635542, 0.6646404955760542, 0.6642639895519579, 0.6645184252635542, 0.6635315676769579, 0.6639080737010542, 0.6645184252635542, 0.6648949312876506, 0.6651390719126506, 0.6638977786144579, 0.6639080737010542, 0.6646404955760542, 0.6645184252635542, 0.6641625094126506, 0.6645081301769579, 0.6647625658885542, 0.6631653567394579, 0.6642742846385542, 0.6643963549510542, 0.6643860598644579, 0.6646404955760542, 0.6650067065135542, 0.6642742846385542, 0.6645184252635542, 0.6641625094126506, 0.6646302004894579, 0.6645184252635542, 0.6639080737010542, 0.6640301440135542, 0.6641419192394579, 0.6639080737010542, 0.6645184252635542, 0.6634197924510542, 0.6643963549510542, 0.6632977221385542, 0.6647625658885542, 0.6650067065135542, 0.6642742846385542, 0.6636536379894579, 0.6637962984751506, 0.6646404955760542, 0.6636639330760542, 0.6636639330760542, 0.6632874270519579, 0.6637860033885542, 0.6647625658885542, 0.6640301440135542, 0.6635315676769579, 0.6661156344126506, 0.6649964114269579], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-3, 3], "momentum": 0.5103866463705088, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.0010524142211458816, "patience_threshold": 1, "do_flip": true, "batch_size": 128, "optimization": "adadelta", "nb_data_augmentation": 1, "learning_rate_decay_method": "exp", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 2.7700683686659843e-07, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.09029746108786255}, "accuracy_valid_max": 0.33707878388554213, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-3, 3)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.33500358857304213, "loss_train": [2.745243549346924, 2.4228875637054443, 2.2869513034820557, 2.2028512954711914, 2.143083333969116, 2.0959742069244385, 2.0574400424957275, 2.024576425552368, 1.9989054203033447, 1.9765409231185913, 1.9591728448867798, 1.9420592784881592, 1.9266095161437988, 1.914390206336975, 1.9041608572006226, 1.894287109375, 1.8858710527420044, 1.8796111345291138, 1.8724130392074585, 1.8664036989212036, 1.8611310720443726, 1.8566290140151978, 1.8530577421188354, 1.8488391637802124, 1.8456063270568848, 1.8420804738998413, 1.8405669927597046, 1.8377227783203125, 1.8359991312026978, 1.834550142288208, 1.8324861526489258, 1.8298251628875732, 1.828584909439087, 1.8274375200271606, 1.826300859451294, 1.8256621360778809, 1.8245058059692383, 1.8228206634521484, 1.8226557970046997, 1.8221774101257324, 1.822029948234558, 1.8211369514465332, 1.820361614227295, 1.8195998668670654, 1.819220781326294, 1.818978190422058, 1.8184363842010498, 1.8186825513839722, 1.8176929950714111, 1.817103624343872, 1.8171507120132446, 1.8172229528427124, 1.8169535398483276, 1.8164150714874268, 1.8162050247192383, 1.8164253234863281, 1.8169728517532349, 1.8165979385375977, 1.8165719509124756, 1.8158142566680908, 1.8165498971939087, 1.8150737285614014, 1.8145891427993774, 1.8155508041381836, 1.815739631652832, 1.8147006034851074, 1.8154410123825073, 1.815916895866394, 1.8151136636734009, 1.8154551982879639, 1.8154927492141724, 1.8150064945220947, 1.8152186870574951, 1.8146206140518188, 1.8162317276000977, 1.8155781030654907, 1.8154208660125732, 1.8149731159210205, 1.8155909776687622, 1.8157222270965576, 1.814314603805542, 1.815546989440918, 1.8146371841430664, 1.8141236305236816, 1.8156520128250122, 1.8138086795806885, 1.815672516822815, 1.814967393875122, 1.8155708312988281, 1.8152724504470825, 1.8146440982818604, 1.815129041671753, 1.8153669834136963, 1.8154869079589844, 1.8148428201675415, 1.8149571418762207, 1.815513253211975, 1.8147886991500854, 1.8157668113708496, 1.8157966136932373, 1.815406084060669, 1.8152885437011719, 1.8152302503585815, 1.8147215843200684, 1.8149656057357788, 1.815967082977295, 1.8158197402954102, 1.8153505325317383, 1.815687894821167, 1.8159124851226807, 1.8151235580444336, 1.8146809339523315, 1.8151378631591797, 1.8164187669754028, 1.8157663345336914, 1.8156254291534424, 1.8147733211517334, 1.8149875402450562, 1.815413236618042, 1.8154696226119995, 1.8155155181884766, 1.8152602910995483, 1.8151402473449707, 1.8143681287765503, 1.8152823448181152, 1.815502405166626, 1.8153904676437378, 1.8151851892471313, 1.8144643306732178, 1.8153713941574097, 1.8148988485336304, 1.814937710762024, 1.8154734373092651, 1.815132737159729, 1.815049171447754, 1.815187692642212, 1.8147156238555908, 1.8156943321228027, 1.815351963043213, 1.815616250038147, 1.8149776458740234, 1.8150867223739624, 1.8153307437896729, 1.8154433965682983, 1.8144153356552124, 1.8158999681472778], "accuracy_train_first": 0.13350950391057587, "model": "residualv3", "loss_std": [0.18758706748485565, 0.10771991312503815, 0.100926473736763, 0.09833718091249466, 0.09806772321462631, 0.09726868569850922, 0.09704359620809555, 0.09537229686975479, 0.09449832886457443, 0.09448482096195221, 0.0916946604847908, 0.09223910421133041, 0.09035762399435043, 0.09018686413764954, 0.08945338428020477, 0.0892336368560791, 0.08996310830116272, 0.09005540609359741, 0.08840072900056839, 0.08812373876571655, 0.08950373530387878, 0.08707115054130554, 0.0882253423333168, 0.08694762736558914, 0.08867067098617554, 0.08719182759523392, 0.08809002488851547, 0.08677513152360916, 0.08832027018070221, 0.09001709520816803, 0.08751137554645538, 0.08613211661577225, 0.08855143189430237, 0.08684984594583511, 0.08698185533285141, 0.08679614216089249, 0.08730369806289673, 0.08577575534582138, 0.08636485785245895, 0.08764002472162247, 0.08802728354930878, 0.08790063112974167, 0.08937481045722961, 0.08726783096790314, 0.08775888383388519, 0.08746499568223953, 0.08832567930221558, 0.08875203132629395, 0.08705726265907288, 0.08656120300292969, 0.08706412464380264, 0.08710505068302155, 0.08680851757526398, 0.0859198123216629, 0.08646460622549057, 0.08652392774820328, 0.08860472589731216, 0.0874265730381012, 0.08737718313932419, 0.08576160669326782, 0.08812196552753448, 0.08695776760578156, 0.0893128514289856, 0.08775319904088974, 0.08711279183626175, 0.08554045110940933, 0.08816035091876984, 0.08733229339122772, 0.0871211364865303, 0.08532728999853134, 0.08915631473064423, 0.08726498484611511, 0.08685722202062607, 0.08676370233297348, 0.08650074899196625, 0.08708325773477554, 0.08695048093795776, 0.08665040135383606, 0.08703706413507462, 0.08669377863407135, 0.08679556101560593, 0.08813156932592392, 0.08721321821212769, 0.08622981607913971, 0.08939890563488007, 0.08791244775056839, 0.08566363155841827, 0.08687833696603775, 0.08934130519628525, 0.08596856892108917, 0.08815904706716537, 0.0868351012468338, 0.08826897293329239, 0.08871825784444809, 0.08720764517784119, 0.08616787195205688, 0.08882933109998703, 0.08531779050827026, 0.0861470177769661, 0.08824712038040161, 0.08740371465682983, 0.08939998596906662, 0.08760932087898254, 0.0875476822257042, 0.09007775038480759, 0.08898351341485977, 0.08776307106018066, 0.08879102766513824, 0.0883171483874321, 0.08818946033716202, 0.08565042167901993, 0.08572670817375183, 0.08738833665847778, 0.08815713226795197, 0.08660413324832916, 0.08901084214448929, 0.08613735437393188, 0.08557868003845215, 0.086883045732975, 0.08808588236570358, 0.08663041144609451, 0.08747293800115585, 0.08784130960702896, 0.08773458003997803, 0.08826125413179398, 0.08716848492622375, 0.08680115640163422, 0.08669519424438477, 0.08583594113588333, 0.08611731231212616, 0.08606338500976562, 0.08697838336229324, 0.08696450293064117, 0.08789284527301788, 0.08624278008937836, 0.0859995186328888, 0.08639704436063766, 0.08741150051355362, 0.0882253348827362, 0.08818255364894867, 0.08643954992294312, 0.0880681648850441, 0.08722803741693497, 0.08680719137191772, 0.08621953427791595, 0.08682912588119507]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:12 2016", "state": "available"}], "summary": "60fc56600b9adf67a31b1d102dc8e387"}