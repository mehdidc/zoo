{"content": {"hp_model": {"f0": 64, "f1": 16, "f2": 64, "f3": 32, "nonlin": "rectify", "nbg1": 7, "nbg3": 2, "nbg2": 2, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.012769038135394775, 0.013112003749888105, 0.01788302692669284, 0.015842970098016874, 0.016891542926269366, 0.009377785766825748, 0.013313269123976868, 0.013924220411241616, 0.018267004297956763, 0.014606838674650667, 0.01200710168760295, 0.012555147435179468, 0.013160912513957634, 0.015617457105943361, 0.013195515618034311, 0.015644198188354618, 0.014820139103964167, 0.01739543551716076, 0.01661916013482643, 0.010957178418171754, 0.015659846532894667, 0.01726012967002054, 0.014580121237798893, 0.01359993623485905, 0.013479192396749494, 0.013283287584471037, 0.015397969451135283, 0.012697433137301598, 0.014751035856579211, 0.00976115941546573, 0.014355337009606714, 0.012793429691344825, 0.011038678120678878, 0.008417179124171987, 0.012208019874952165, 0.010567131214796233, 0.012216185306072705, 0.010377316807201532, 0.010777518556889326, 0.012014427090552655, 0.011284323437841307, 0.013453774011122598, 0.01207312471405555, 0.011875757669202644, 0.014217625802702584, 0.013413166359078, 0.01356341823342736, 0.013490234706102767, 0.01622897225304558, 0.013693491562913867, 0.013262086862499077, 0.013166767030915277, 0.015160160132289987, 0.014800480489247104, 0.01354916405300524, 0.012192151806075639, 0.01569475806445765, 0.012581081010237598, 0.01319494573047696, 0.012227867927515368, 0.012984117356276102, 0.013364369313302968, 0.013527373702478848, 0.013633838786467987, 0.013862076697122375, 0.01425473404284198, 0.012296235270281876, 0.014466480600665117, 0.01190106485308093, 0.01680834544795694, 0.01561392624139932, 0.01624542482695823, 0.012933894663896255, 0.015422502840449439, 0.01387683746754041, 0.0131669895463373, 0.014470966196168899, 0.014805665220654123, 0.01423715964409373, 0.01401881219650832, 0.01380874836628868, 0.015720708819834374, 0.015670257796159864, 0.014485518792367985, 0.013667365567824592, 0.013946576043193002, 0.013860759735926936, 0.013747121068193494, 0.013771931452578141, 0.012173115514697607, 0.012136092744556452, 0.01638830786071543, 0.011764844004222745, 0.015041587664438145, 0.014039466072278973, 0.015247535874059914, 0.01375358641579086, 0.014403747481893337, 0.014899315494955656, 0.015032498461832699, 0.014233476409597604, 0.012147958514290003, 0.013986790748563661, 0.014644636669891454, 0.012087867139131311, 0.013571543005410759, 0.012765704644677318, 0.014088695411333329, 0.01530382359013414, 0.012239442021863124, 0.013970145609044589, 0.012410164082450381, 0.016513779707886717, 0.012880935312322789, 0.014462993958125277, 0.013192005824547843, 0.015029775227343474, 0.01264653137289735, 0.013632890727993995, 0.015435535160267114, 0.0137948884960383, 0.012679253754323633, 0.012104042498915965, 0.013612146879476207, 0.012374747488668136, 0.012532046479683555, 0.014633459144220432, 0.015606291300904225, 0.01249091396392586, 0.011682184674710433, 0.012602472299188344, 0.014490525062626538, 0.014818459605934153, 0.012653286936177878, 0.013414249352383572, 0.013425951342857274, 0.01486782651632423, 0.014986007787708736, 0.012469570681077603, 0.011601503451078337, 0.013091558400666295, 0.012479295166037001, 0.012205224337430471, 0.01563681561204471, 0.01329092058511966, 0.01444835033305522, 0.013595725514138176, 0.013087345176945838, 0.014017214531884575, 0.014035617534526845, 0.014510544200630936, 0.014079281570339422, 0.012508541800574379, 0.013080594184934883, 0.014844648204907739, 0.012953486374468559, 0.0143039311930553, 0.014514609049026878, 0.015061203153161166, 0.012788452620486437, 0.012375859917798814, 0.012696403486271825, 0.015430346823193058, 0.014066601476575364, 0.014808866113079943, 0.013673784469793114, 0.012637499395927964, 0.013106799024369996, 0.012890676678702048, 0.012003603454279671, 0.012860011599237177, 0.013688639801686778, 0.012988033569622056, 0.012984891648086975, 0.012887055077180864, 0.013409270343223792], "moving_avg_accuracy_train": [0.0381439760981912, 0.08045885459925248, 0.12291001025228404, 0.16480741333142995, 0.20576509340609428, 0.24455633825291562, 0.28107702900542025, 0.31533376452196016, 0.34749216201771543, 0.3774367507543437, 0.4054959405506332, 0.4316417080219634, 0.455921632711646, 0.47836171943471356, 0.4993620826806571, 0.5185344880615393, 0.5363476525698003, 0.5529142848534256, 0.5681983505252906, 0.582351682174035, 0.5953244846388481, 0.6072718690238005, 0.6184614266535357, 0.6288507541512958, 0.6383220205885564, 0.6471576582249295, 0.6554538180726562, 0.663078436005839, 0.6700847153230755, 0.6765623195740367, 0.6826457127665961, 0.6882764074148626, 0.6935090460661412, 0.6982114454058633, 0.7027178642246809, 0.7069295342783108, 0.7109362040682261, 0.7146259482851116, 0.7180698788695757, 0.7211997154277547, 0.7239746796027257, 0.7266347996304099, 0.7290476890898956, 0.7312145311593574, 0.7334530435230728, 0.7353653260051602, 0.737156206800962, 0.7387889258564694, 0.7403769195468929, 0.7418873859301512, 0.7432004108453495, 0.7445215700999618, 0.7456362447160269, 0.7466859187978572, 0.7477586528513033, 0.7486704908815107, 0.7495353229360784, 0.7503555965613982, 0.751124033709891, 0.7517319217863917, 0.7523720991052608, 0.752964498685091, 0.7535185485974051, 0.7540242050137351, 0.754418769821747, 0.7548367292644524, 0.7553035014688212, 0.7557585097337146, 0.7561005518078238, 0.7563851742352455, 0.7567180643306394, 0.7569131058177028, 0.757174565515556, 0.7573982534995762, 0.75762743842209, 0.7579058565630852, 0.7580424645494954, 0.7581584723396547, 0.7583116714269793, 0.7584309133662765, 0.7585871313342817, 0.7587067651173818, 0.7588772505888477, 0.7589702696929382, 0.7591027068651635, 0.7592405015106423, 0.7592622822415918, 0.7592586694601697, 0.7593366539699482, 0.7593557228037582, 0.759382221398244, 0.7594758245975668, 0.7595577423281479, 0.7595942659047185, 0.7597155288272126, 0.7598153288134005, 0.7598446588831034, 0.7598966325827406, 0.7599759970445663, 0.7600776519947332, 0.7600900863903596, 0.7601849827035662, 0.7602238503604428, 0.7602589033492693, 0.7602462371630134, 0.7602627393810975, 0.7602705798821259, 0.7602009064223372, 0.7602149662680603, 0.7601810811042018, 0.7601785943888997, 0.760115794329624, 0.7601057772524663, 0.7601991044794622, 0.7602575583956724, 0.7602636639440712, 0.7602947355745349, 0.7602528734800289, 0.7602314736366402, 0.7602656922002096, 0.7602476968312406, 0.7603407469443975, 0.7603454369867149, 0.7603078053462291, 0.7603343907388396, 0.7603700154338741, 0.7603067465582145, 0.7602264809843882, 0.7602937148476971, 0.7602984938508842, 0.7602702068216005, 0.7602912514714356, 0.7602520268872304, 0.7602051350662168, 0.7602118326499421, 0.7602364616657711, 0.7602121608526454, 0.7601716168327186, 0.7601815941421561, 0.7602301733480494, 0.7602366562035823, 0.7602424907735618, 0.7603198214996386, 0.7603150504400217, 0.7602827465541959, 0.7602444445593521, 0.7602123339616211, 0.760216058604634, 0.7602797565059369, 0.7602928346920911, 0.7602906902155915, 0.7602934825819984, 0.7602982487629366, 0.7602676971424569, 0.7602332612864153, 0.7602557113897783, 0.7602294135066144, 0.7602452008438914, 0.7602803718355451, 0.7603608178042147, 0.7603774877022262, 0.7603785036687607, 0.7603166750696033, 0.7602773053720283, 0.7602162599584874, 0.7601730169279857, 0.7601990942207445, 0.7602272501306652, 0.7602316641103082, 0.7602333115431773, 0.7602022421494262, 0.7602649604986217, 0.7603516339474214, 0.7604087137120554, 0.7604694942419203, 0.7604403832151997], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 682541897, "moving_var_accuracy_train": [0.013094666213214439, 0.02790014007492919, 0.041329031613937486, 0.05299465991553156, 0.06279297793786538, 0.07005652623497324, 0.07505472128883664, 0.07811096451420448, 0.07960733082823901, 0.07971670329686734, 0.07883089615539837, 0.07710021694984234, 0.07469582794128803, 0.07175826257640837, 0.06855157362692173, 0.0650046464170295, 0.06135996124351187, 0.0576940448661484, 0.054027064350690476, 0.0504272090864556, 0.04689913061192961, 0.04349387749351289, 0.040271345543704075, 0.037215654122035105, 0.03430143270116181, 0.03157390586301728, 0.029035951690686845, 0.026655569709261286, 0.024431804287175358, 0.022366258069946455, 0.020462701317569314, 0.01870177368581054, 0.01707802088252318, 0.015569231830222836, 0.014195078942335886, 0.0129352145298681, 0.01178617370213007, 0.010730084243391416, 0.00976382173988774, 0.00887560245782719, 0.00805734604759582, 0.00731529758989142, 0.0066361661507955555, 0.006014806376701909, 0.00545842417745428, 0.004945493178330535, 0.004479809146720427, 0.004055820175676335, 0.003672933673756128, 0.003326173884635095, 0.0030090728060229678, 0.0027238746814051014, 0.0024626697087618906, 0.0022263190789882986, 0.002014043996234275, 0.0018201226339508406, 0.0016448417808992257, 0.001486413242192861, 0.00134308637883423, 0.0012121034921727732, 0.0010945815859518461, 0.000988281862716308, 0.0008922164181926946, 0.0008052959720758065, 0.0007261675073577188, 0.0006551229674836658, 0.0005915715573522404, 0.0005342776943071084, 0.0004819028599005457, 0.0004344416632462143, 0.00039199483926209493, 0.0003531377259709686, 0.0003184392039362852, 0.00028704561037041213, 0.0002588137808917397, 0.00023363005275368247, 0.00021043500315587367, 0.00018951262310668522, 0.00017077259043923074, 0.00015382329915609403, 0.00013866060572223328, 0.00012492335532853944, 0.00011269260745951426, 0.00010150121969709503, 9.150895416866856e-05, 8.25289450307055e-05, 7.428032012980118e-05, 6.68524055865275e-05, 6.022189928176331e-05, 5.4202981937392807e-05, 4.878900332324101e-05, 4.398895702122824e-05, 3.9650455950357414e-05, 3.569741610013124e-05, 3.226001675746461e-05, 2.912365541690615e-05, 2.6219032152114458e-05, 2.362144032598885e-05, 2.131598475359777e-05, 1.9277389838278817e-05, 1.735104238220227e-05, 1.5696985936323838e-05, 1.414088359545121e-05, 1.2737853644137142e-05, 1.1465512170191835e-05, 1.0321411861987889e-05, 9.289823936896473e-06, 8.404531062197321e-06, 7.565857069333403e-06, 6.819605201367487e-06, 6.13770033500768e-06, 5.5594249285122104e-06, 5.004385512174033e-06, 4.58233670264533e-06, 4.154854775263684e-06, 3.739704797228566e-06, 3.3744233334827563e-06, 3.052752914742272e-06, 2.7515992029415672e-06, 2.486977473482111e-06, 2.241194225872842e-06, 2.0949997153120916e-06, 1.8856977122533295e-06, 1.7098732043188803e-06, 1.5452469317892273e-06, 1.402144308676991e-06, 1.2979564334541976e-06, 1.2261438511836924e-06, 1.1442129974444776e-06, 1.0299972475431903e-06, 9.341989270201386e-07, 8.447649298982803e-07, 7.741355489630961e-07, 7.165115799684867e-07, 6.452641406214564e-07, 5.861970223456479e-07, 5.3289208577819e-07, 4.943972351667781e-07, 4.458534319826285e-07, 4.2250754199136415e-07, 3.8063503453496217e-07, 3.4287791094308103e-07, 3.624104906088982e-07, 3.263743086368177e-07, 3.031287471281076e-07, 2.8601925769638196e-07, 2.6669714630656647e-07, 2.401522883660688e-07, 2.526538632030423e-07, 2.2892782746048912e-07, 2.060764337295571e-07, 1.8553896614795438e-07, 1.671895178597815e-07, 1.5887117969922465e-07, 1.5365651536112942e-07, 1.4282692809405982e-07, 1.3476844321475983e-07, 1.23534759057937e-07, 1.2231427103727585e-07, 1.683268288102009e-07, 1.539951154266156e-07, 1.386048935759467e-07, 1.5914938528219764e-07, 1.5718420453826544e-07, 1.7500466671392485e-07, 1.7433383722531e-07, 1.6302068028144673e-07, 1.5385340962449084e-07, 1.3864341760863853e-07, 1.2480350216329978e-07, 1.2101091699952086e-07, 1.4431214723177045e-07, 1.9749151305009839e-07, 2.0706525752115038e-07, 2.1960718706503482e-07, 2.0527353524909973e-07], "duration": 139072.970181, "accuracy_train": [0.3814397609819121, 0.461292761108804, 0.5049704111295681, 0.5418840410437431, 0.574384214078073, 0.5936775418743079, 0.6097632457779623, 0.6236443841708195, 0.6369177394795128, 0.6469380493839978, 0.6580286487172389, 0.666953615263935, 0.6744409549187892, 0.680322499942322, 0.6883653518941492, 0.6910861364894795, 0.6966661331441492, 0.7020139754060539, 0.7057549415720745, 0.7097316670127354, 0.7120797068221668, 0.7147983284883721, 0.7191674453211517, 0.722354701631137, 0.7235634185239018, 0.7266783969522886, 0.7301192567021964, 0.731699997404485, 0.7331412291782022, 0.7348607578326873, 0.7373962514996308, 0.7389526592492617, 0.7406027939276486, 0.7405330394633628, 0.7432756335940385, 0.7448345647609819, 0.746996232177464, 0.74783364623708, 0.7490652541297527, 0.7493682444513657, 0.748949357177464, 0.7505758798795681, 0.7507636942252676, 0.7507161097845146, 0.7535996547965117, 0.7525758683439461, 0.7532741339631783, 0.7534833973560355, 0.7546688627607051, 0.7554815833794758, 0.7550176350821337, 0.7564120033914729, 0.7556683162606128, 0.7561329855343301, 0.7574132593323182, 0.7568770331533776, 0.7573188114271872, 0.7577380591892765, 0.7580399680463271, 0.7572029144748985, 0.7581336949750831, 0.7582960949035622, 0.7585049978082319, 0.7585751127607051, 0.7579698530938538, 0.7585983642488003, 0.7595044513081396, 0.7598535841177556, 0.7591789304748062, 0.7589467760820414, 0.7597140751891842, 0.7586684792012736, 0.7595277027962348, 0.7594114453557586, 0.7596901027247139, 0.7604116198320414, 0.7592719364271872, 0.759202542451089, 0.7596904632129015, 0.759504090819952, 0.7599930930463271, 0.7597834691652824, 0.7604116198320414, 0.7598074416297527, 0.7602946414151901, 0.760480653319952, 0.7594583088201367, 0.7592261544273717, 0.7600385145579549, 0.7595273423080473, 0.7596207087486158, 0.7603182533914729, 0.7602950019033776, 0.7599229780938538, 0.7608068951296604, 0.7607135286890919, 0.7601086295104282, 0.7603643958794758, 0.7606902772009967, 0.7609925465462348, 0.7602019959509967, 0.7610390495224253, 0.760573659272333, 0.760574380248708, 0.760132241486711, 0.7604112593438538, 0.7603411443913806, 0.7595738452842378, 0.7603415048795681, 0.7598761146294758, 0.7601562139511813, 0.7595505937961425, 0.7600156235580473, 0.7610390495224253, 0.7607836436415651, 0.7603186138796604, 0.760574380248708, 0.7598761146294758, 0.7600388750461425, 0.760573659272333, 0.7600857385105205, 0.7611781979628092, 0.760387647367571, 0.7599691205818567, 0.760573659272333, 0.7606906376891842, 0.7597373266772794, 0.759504090819952, 0.7608988196174787, 0.7603415048795681, 0.7600156235580473, 0.760480653319952, 0.7598990056293835, 0.7597831086770949, 0.7602721109034699, 0.7604581228082319, 0.7599934535345146, 0.7598067206533776, 0.7602713899270949, 0.760667386201089, 0.7602950019033776, 0.7602950019033776, 0.7610157980343301, 0.7602721109034699, 0.7599920115817644, 0.7598997266057586, 0.7599233385820414, 0.7602495803917497, 0.7608530376176633, 0.7604105383674787, 0.7602713899270949, 0.7603186138796604, 0.7603411443913806, 0.7599927325581396, 0.7599233385820414, 0.7604577623200444, 0.7599927325581396, 0.7603872868793835, 0.7605969107604282, 0.7610848315222407, 0.7605275167843301, 0.760387647367571, 0.7597602176771872, 0.7599229780938538, 0.7596668512366187, 0.7597838296534699, 0.760433789855574, 0.760480653319952, 0.7602713899270949, 0.7602481384389996, 0.7599226176056663, 0.7608294256413806, 0.7611316949866187, 0.7609224315937615, 0.7610165190107051, 0.7601783839747139], "end": "2016-01-27 09:36:39.084000", "learning_rate_per_epoch": [0.0008676245342940092, 0.0008068952593021095, 0.0007504167151637375, 0.0006978914025239646, 0.0006490426021628082, 0.0006036129780113697, 0.000561363180167973, 0.0005220706225372851, 0.00048552837688475847, 0.00045154389226809144, 0.0004199381510261446, 0.00039054465014487505, 0.00036320852814242244, 0.0003377858083695173, 0.00031414252589456737, 0.00029215417453087866, 0.00027170489192940295, 0.0002526869357097894, 0.00023500014503952116, 0.000218551344005391, 0.000203253875952214, 0.00018902715237345546, 0.0001757962309056893, 0.0001634913933230564, 0.00015204783994704485, 0.0001414052676409483, 0.00013150762242730707, 0.00012230276479385793, 0.00011374220048310235, 0.00010578083310974762, 9.837671677814797e-05, 9.149085235549137e-05, 8.508696191711351e-05, 7.913131412351504e-05, 7.359252776950598e-05, 6.844142626505345e-05, 6.365087756421417e-05, 5.919564500800334e-05, 5.5052252719178796e-05, 5.11988764628768e-05, 4.761521995533258e-05, 4.4282398448558524e-05, 4.118285869481042e-05, 3.830026980722323e-05, 3.5619446862256154e-05, 3.312626722618006e-05, 3.080759779550135e-05, 2.8651224056375213e-05, 2.6645786419976503e-05, 2.4780718376860023e-05, 2.304619556525722e-05, 2.143308120139409e-05, 1.9932875147787854e-05, 1.8537675714469515e-05, 1.7240134184248745e-05, 1.6033412975957617e-05, 1.491115654062014e-05, 1.386745225318009e-05, 1.2896802218165249e-05, 1.1994092346867546e-05, 1.1154567800986115e-05, 1.0373805707786232e-05, 9.64769242273178e-06, 8.972403520601802e-06, 8.344381058122963e-06, 7.760317203064915e-06, 7.21713513485156e-06, 6.711972673656419e-06, 6.242169092729455e-06, 5.805249202239793e-06, 5.398911525844596e-06, 5.021015567763243e-06, 4.6695704440935515e-06, 4.3427244236227125e-06, 4.03875583288027e-06, 3.7560637338174274e-06, 3.493158601486357e-06, 3.2486555028299335e-06, 3.0212663659767713e-06, 2.80979315903096e-06, 2.613121978356503e-06, 2.430216909488081e-06, 2.2601141154154902e-06, 2.1019177438574843e-06, 1.9547942429198883e-06, 1.8179687231167918e-06, 1.690720296210202e-06, 1.5723785509180743e-06, 1.4623201423091814e-06, 1.3599652675111429e-06, 1.2647747098526452e-06, 1.1762469966924982e-06, 1.093915784622368e-06, 1.0173473583563464e-06, 9.461383001507784e-07, 8.799135002846015e-07, 8.18324053852848e-07, 7.610455554640794e-07, 7.077762802509824e-07, 6.582355922546412e-07, 6.121624664956471e-07, 5.693142384188832e-07, 5.294651828080532e-07, 4.924053200738854e-07, 4.579394783377211e-07, 4.258860712980095e-07, 3.9607624557902454e-07, 3.6835294281445385e-07, 3.42570132261244e-07, 3.185919865700271e-07, 2.9629219966409437e-07, 2.755532761966606e-07, 2.5626596311667527e-07, 2.3832866702377942e-07, 2.2164688573411695e-07, 2.0613273932212905e-07, 1.917045011623486e-07, 1.782861716037587e-07, 1.6580705164415122e-07, 1.5420141608046833e-07, 1.4340811560487055e-07, 1.333702783767876e-07, 1.2403504001667898e-07, 1.1535322386180269e-07, 1.0727909227625787e-07, 9.977010506645456e-08, 9.278671342372036e-08, 8.629211833977024e-08, 8.025211428730472e-08, 7.46348760571891e-08, 6.941081664990634e-08, 6.455241674530043e-08, 6.003408259402931e-08, 5.583200390901766e-08, 5.1924050836760216e-08, 4.828963540148834e-08, 4.4909608476473295e-08, 4.176616741347061e-08, 3.884274946130972e-08, 3.6123957158906705e-08, 3.359546596470864e-08, 3.1243956755133695e-08, 2.90570412175839e-08, 2.7023197901598905e-08, 2.5131713599080285e-08, 2.3372622948159005e-08, 2.1736658695203914e-08, 2.021520373318708e-08, 1.8800243140049133e-08, 1.7484323322491946e-08, 1.626051115977134e-08, 1.5122358476560294e-08, 1.406387095670425e-08, 1.3079472616084331e-08, 1.2163976492729489e-08, 1.1312560665999172e-08, 1.0520739834873893e-08, 9.784342225316323e-09, 9.099488273989209e-09, 8.46257108833015e-09, 7.870234242091101e-09, 7.3193584526620725e-09, 6.807041152967486e-09, 6.330583168789872e-09, 5.887474952004368e-09, 5.4753819256347924e-09, 5.092133381623398e-09, 4.735710046332997e-09, 4.404234754673553e-09, 4.095960903782725e-09, 3.809264903509302e-09, 3.5426359623613735e-09, 3.294669648212789e-09, 3.0640596726527747e-09, 2.8495912296477854e-09, 2.6501345562479628e-09], "accuracy_valid": [0.3909147331513554, 0.467639601374247, 0.5042415756777108, 0.5425334149096386, 0.5690844432417168, 0.5843947077371988, 0.596092867564006, 0.6097147378576807, 0.6210569818335843, 0.6286562264683735, 0.6389821983245482, 0.6502744375941265, 0.6566117987575302, 0.6622064429593373, 0.6700807134789157, 0.6707719550075302, 0.6754723974021084, 0.6798566335655121, 0.6829186864646084, 0.6861748752823795, 0.6868249364646084, 0.6890427922628012, 0.6944447712725903, 0.6970391330948795, 0.6967949924698795, 0.6987687076430723, 0.6988495976091867, 0.7020851962537651, 0.7015454395707832, 0.7039265460278614, 0.7041398013930723, 0.7081681217055723, 0.7083210772778614, 0.7086975833019578, 0.7075474750564759, 0.7115052004894578, 0.7123493975903614, 0.7110169192394578, 0.7143334078501506, 0.7150452395519578, 0.7146893237010542, 0.7134377353162651, 0.7134480304028614, 0.7127156085278614, 0.7150246493787651, 0.7140274967055723, 0.7161232821912651, 0.7174866458019578, 0.7132847797439759, 0.7179749270519578, 0.7175984210278614, 0.7185749835278614, 0.7166012683546686, 0.7170895496046686, 0.7171101397778614, 0.7198162768260542, 0.7176793109939759, 0.7167439288403614, 0.7172219150037651, 0.7174660556287651, 0.7172322100903614, 0.7168454089796686, 0.7177101962537651, 0.7177307864269578, 0.7193074054028614, 0.7168351138930723, 0.7191956301769578, 0.7170895496046686, 0.7190735598644578, 0.7171807346573795, 0.7166924534073795, 0.7186558734939759, 0.7180764071912651, 0.7180558170180723, 0.7200295321912651, 0.7190529696912651, 0.7198971667921686, 0.7196530261671686, 0.7194191806287651, 0.7192971103162651, 0.7183205478162651, 0.7182896625564759, 0.7174248752823795, 0.7172013248305723, 0.7208943194653614, 0.7186764636671686, 0.7202736728162651, 0.7184426181287651, 0.7201721926769578, 0.7183308429028614, 0.7186970538403614, 0.7184117328689759, 0.7182087725903614, 0.7178116763930723, 0.7201413074171686, 0.7191647449171686, 0.7202839679028614, 0.7185543933546686, 0.7172013248305723, 0.7173131000564759, 0.7178219714796686, 0.7193074054028614, 0.7183102527296686, 0.7183102527296686, 0.7190632647778614, 0.7188088290662651, 0.7201721926769578, 0.7183102527296686, 0.7169468891189759, 0.7177204913403614, 0.7175778308546686, 0.7191853350903614, 0.7170586643448795, 0.7189308993787651, 0.7179337467055723, 0.7173336902296686, 0.7187779438064759, 0.7178425616528614, 0.7188088290662651, 0.7190323795180723, 0.7194191806287651, 0.7200398272778614, 0.7202942629894578, 0.7190529696912651, 0.7191750400037651, 0.7210163897778614, 0.7195206607680723, 0.7181675922439759, 0.7201618975903614, 0.7200501223644578, 0.7193177004894578, 0.7184220279555723, 0.7174454654555723, 0.7183308429028614, 0.7178219714796686, 0.7199177569653614, 0.7195206607680723, 0.7178116763930723, 0.7207825442394578, 0.7183308429028614, 0.7190632647778614, 0.7196736163403614, 0.7178425616528614, 0.7181675922439759, 0.7172116199171686, 0.7175778308546686, 0.7200398272778614, 0.7179543368787651, 0.7179234516189759, 0.7168351138930723, 0.7192868152296686, 0.7184323230421686, 0.7172219150037651, 0.7184426181287651, 0.7168351138930723, 0.7181984775037651, 0.7190529696912651, 0.7180558170180723, 0.7176896060805723, 0.7204266283885542, 0.7201618975903614, 0.7207722491528614, 0.7187882388930723, 0.7195412509412651, 0.7186764636671686, 0.7176999011671686, 0.7190632647778614, 0.7183308429028614, 0.7193074054028614, 0.7196839114269578, 0.7184426181287651, 0.7195412509412651, 0.7183102527296686, 0.7196736163403614, 0.7195515460278614, 0.7194191806287651], "accuracy_test": 0.7087751116071429, "start": "2016-01-25 18:58:46.114000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0], "accuracy_train_last": 0.7601783839747139, "batch_size_eval": 1024, "accuracy_train_std": [0.014651726577693203, 0.015415606644211523, 0.015370125424293288, 0.015692077224579135, 0.016019943256778822, 0.015289966897125248, 0.016281991005755232, 0.014293487248018676, 0.014454699061682973, 0.014777401998215176, 0.014104059487595591, 0.015169325574756826, 0.014566369424424271, 0.01278015005327915, 0.013578296541156915, 0.013341953796991806, 0.01421857042925735, 0.012806049581851331, 0.013544883111692803, 0.013444210082861379, 0.013392928233370632, 0.014042772453907646, 0.012532213400750317, 0.01273146599968413, 0.011591692622430155, 0.012161468620743719, 0.01112854205020841, 0.012239836816404325, 0.011925215652009885, 0.012495301950446272, 0.01240312179296669, 0.012859000075210134, 0.011684543665855909, 0.011511325884252833, 0.01174506253551648, 0.011614384405358186, 0.011510293320441454, 0.011783328123395038, 0.011940524459334106, 0.011597944356916757, 0.011231470136644961, 0.011813787967040265, 0.011208851330673892, 0.010972213785686916, 0.011241536006761886, 0.01156336983218944, 0.011217820484064132, 0.011108717233373819, 0.010464396803404094, 0.011190340411816395, 0.010810430493055119, 0.011045914078857948, 0.010919225158973651, 0.011009007803171886, 0.010888347796349757, 0.01087170178016254, 0.010947468352154677, 0.011182895284890495, 0.01151654494595876, 0.010127457658891345, 0.010798622456445644, 0.011025383614323044, 0.011330856045522652, 0.010839165028298592, 0.011160731380072009, 0.010722708715597534, 0.011573249308975053, 0.01174767444773689, 0.011090177233385694, 0.010536992377306756, 0.010599743107067018, 0.010340251774981038, 0.011042642311037557, 0.010621262022862774, 0.00995890349130232, 0.011332468563857043, 0.011510561111556517, 0.010623445038699544, 0.010951009658388034, 0.010488860628326005, 0.010614554823834967, 0.011312296650704372, 0.010204354718148118, 0.010656783492521185, 0.011272164958912052, 0.011039939850259901, 0.010900686017060247, 0.010552141932996755, 0.010822358235965892, 0.010440595043198508, 0.010496102093933871, 0.010703331593657913, 0.010598986689022222, 0.01032250267166422, 0.010842632384153748, 0.010822402311674672, 0.011204625197431138, 0.010553845158505593, 0.010846417319323946, 0.010643930160810592, 0.01065662543679642, 0.010719266398246802, 0.010406553293735913, 0.010514042665312133, 0.01083566649509867, 0.010770046692945132, 0.0113261601795939, 0.010688710223729475, 0.010401920277644292, 0.010654753972462057, 0.010578037895008205, 0.010685080612404169, 0.010402190904867811, 0.011022139819747102, 0.010431832577093302, 0.010613283395476633, 0.011224327796409363, 0.010756561217351227, 0.010849306242582653, 0.010386897221557154, 0.0106276860216982, 0.010537603337909058, 0.010760252598426659, 0.010302313505468008, 0.010480476877828137, 0.010522810498055922, 0.010596616956826933, 0.010917400603761951, 0.010911919368929308, 0.01031423399749235, 0.01074366104213971, 0.010627036207156361, 0.010672880379388312, 0.010381147601870875, 0.010084695929640092, 0.010438928206118638, 0.010615999343797483, 0.010399225125585138, 0.010725138457094614, 0.010250496408577675, 0.010194954744242764, 0.010114456583547215, 0.010441979928854801, 0.010269877126637808, 0.010709544818944079, 0.010407172578342533, 0.010016016307115292, 0.009799873136495642, 0.010760852501391496, 0.01062005855588837, 0.010716666584022563, 0.010651723783550164, 0.010177288357545349, 0.010477322059655611, 0.010558986508427709, 0.010817652652440274, 0.010647154995299965, 0.010750003896849322, 0.010546695711504791, 0.01136640013868137, 0.01032418094216432, 0.01101675281978892, 0.010209800166271165, 0.010804007934474209, 0.010679348488389837, 0.010827686001549373, 0.011153634532599157, 0.010690944318851942, 0.010463660988208496, 0.010758971899682009, 0.01047578442364794, 0.010992199120083803, 0.010612422780872594, 0.011146970746336589, 0.010999810617588306, 0.010132662233497918], "accuracy_test_std": 0.01274935300848575, "error_valid": [0.6090852668486446, 0.532360398625753, 0.4957584243222892, 0.4574665850903614, 0.4309155567582832, 0.4156052922628012, 0.40390713243599397, 0.3902852621423193, 0.37894301816641573, 0.3713437735316265, 0.36101780167545183, 0.3497255624058735, 0.3433882012424698, 0.3377935570406627, 0.32991928652108427, 0.3292280449924698, 0.3245276025978916, 0.32014336643448793, 0.3170813135353916, 0.3138251247176205, 0.3131750635353916, 0.3109572077371988, 0.3055552287274097, 0.3029608669051205, 0.3032050075301205, 0.3012312923569277, 0.30115040239081325, 0.2979148037462349, 0.2984545604292168, 0.2960734539721386, 0.2958601986069277, 0.2918318782944277, 0.2916789227221386, 0.29130241669804224, 0.29245252494352414, 0.28849479951054224, 0.2876506024096386, 0.28898308076054224, 0.28566659214984935, 0.28495476044804224, 0.2853106762989458, 0.2865622646837349, 0.2865519695971386, 0.2872843914721386, 0.2849753506212349, 0.2859725032944277, 0.2838767178087349, 0.28251335419804224, 0.28671522025602414, 0.28202507294804224, 0.2824015789721386, 0.2814250164721386, 0.28339873164533136, 0.28291045039533136, 0.2828898602221386, 0.2801837231739458, 0.28232068900602414, 0.2832560711596386, 0.2827780849962349, 0.2825339443712349, 0.2827677899096386, 0.28315459102033136, 0.2822898037462349, 0.28226921357304224, 0.2806925945971386, 0.2831648861069277, 0.28080436982304224, 0.28291045039533136, 0.28092644013554224, 0.2828192653426205, 0.2833075465926205, 0.28134412650602414, 0.2819235928087349, 0.2819441829819277, 0.2799704678087349, 0.2809470303087349, 0.28010283320783136, 0.28034697383283136, 0.2805808193712349, 0.2807028896837349, 0.2816794521837349, 0.28171033744352414, 0.2825751247176205, 0.2827986751694277, 0.2791056805346386, 0.28132353633283136, 0.2797263271837349, 0.2815573818712349, 0.27982780732304224, 0.2816691570971386, 0.2813029461596386, 0.28158826713102414, 0.2817912274096386, 0.2821883236069277, 0.27985869258283136, 0.28083525508283136, 0.2797160320971386, 0.28144560664533136, 0.2827986751694277, 0.28268689994352414, 0.28217802852033136, 0.2806925945971386, 0.28168974727033136, 0.28168974727033136, 0.2809367352221386, 0.2811911709337349, 0.27982780732304224, 0.28168974727033136, 0.28305311088102414, 0.2822795086596386, 0.28242216914533136, 0.2808146649096386, 0.2829413356551205, 0.2810691006212349, 0.2820662532944277, 0.28266630977033136, 0.28122205619352414, 0.2821574383471386, 0.2811911709337349, 0.2809676204819277, 0.2805808193712349, 0.2799601727221386, 0.27970573701054224, 0.2809470303087349, 0.2808249599962349, 0.2789836102221386, 0.2804793392319277, 0.28183240775602414, 0.2798381024096386, 0.27994987763554224, 0.28068229951054224, 0.2815779720444277, 0.2825545345444277, 0.2816691570971386, 0.28217802852033136, 0.2800822430346386, 0.2804793392319277, 0.2821883236069277, 0.27921745576054224, 0.2816691570971386, 0.2809367352221386, 0.2803263836596386, 0.2821574383471386, 0.28183240775602414, 0.28278838008283136, 0.28242216914533136, 0.2799601727221386, 0.2820456631212349, 0.28207654838102414, 0.2831648861069277, 0.28071318477033136, 0.28156767695783136, 0.2827780849962349, 0.2815573818712349, 0.2831648861069277, 0.2818015224962349, 0.2809470303087349, 0.2819441829819277, 0.2823103939194277, 0.2795733716114458, 0.2798381024096386, 0.2792277508471386, 0.2812117611069277, 0.2804587490587349, 0.28132353633283136, 0.28230009883283136, 0.2809367352221386, 0.2816691570971386, 0.2806925945971386, 0.28031608857304224, 0.2815573818712349, 0.2804587490587349, 0.28168974727033136, 0.2803263836596386, 0.2804484539721386, 0.2805808193712349], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "momentum": 0.7681796953257862, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.000932924474185812, "patience_threshold": 1, "do_flip": true, "batch_size": 16, "l2_decay": 7.838484823664058e-06, "optimization": "nesterov_momentum", "nb_data_augmentation": 1, "learning_rate_decay_method": "exp", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.06999485591874059}, "accuracy_valid_max": 0.7210163897778614, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        #nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        nb_data_augmentation=make_constant_param(1),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.7194191806287651, "loss_train": [1.975880742073059, 1.6959608793258667, 1.5591182708740234, 1.4563579559326172, 1.3863277435302734, 1.3322983980178833, 1.2861210107803345, 1.2480573654174805, 1.2122950553894043, 1.186578392982483, 1.1588828563690186, 1.1368625164031982, 1.1154699325561523, 1.0988149642944336, 1.0813242197036743, 1.0662717819213867, 1.0533618927001953, 1.0406321287155151, 1.028184175491333, 1.0163462162017822, 1.007386565208435, 0.9989275336265564, 0.9889970421791077, 0.9822587966918945, 0.9750754833221436, 0.9659850001335144, 0.9592062830924988, 0.9541892409324646, 0.9493093490600586, 0.9446290731430054, 0.9401717782020569, 0.9354158639907837, 0.9316084384918213, 0.9271801114082336, 0.9240473508834839, 0.9182337522506714, 0.9157662391662598, 0.9136003255844116, 0.911949098110199, 0.9075433611869812, 0.9087468385696411, 0.9044303894042969, 0.902753472328186, 0.9007134437561035, 0.8972939848899841, 0.8999407291412354, 0.8963006734848022, 0.894993245601654, 0.8929765224456787, 0.8903424739837646, 0.8894885778427124, 0.8887996077537537, 0.8882044553756714, 0.889143705368042, 0.8874841332435608, 0.8868266344070435, 0.885345995426178, 0.8837958574295044, 0.8847595453262329, 0.8837213516235352, 0.8846390247344971, 0.8807124495506287, 0.8811272978782654, 0.8818754553794861, 0.879824161529541, 0.8817120790481567, 0.8773358464241028, 0.8781002163887024, 0.8794547915458679, 0.8789590001106262, 0.8790168762207031, 0.877963125705719, 0.877464771270752, 0.8770685195922852, 0.876166045665741, 0.8823647499084473, 0.8761526942253113, 0.8768219351768494, 0.8784323930740356, 0.8761175274848938, 0.8767748475074768, 0.8769233822822571, 0.87601637840271, 0.8779963254928589, 0.8769747018814087, 0.8774466514587402, 0.8738161325454712, 0.8759021162986755, 0.8774333000183105, 0.876730740070343, 0.8748157024383545, 0.8766708374023438, 0.8750492334365845, 0.8768075108528137, 0.8737826943397522, 0.8753303289413452, 0.8746042251586914, 0.8748752474784851, 0.874529242515564, 0.8758238554000854, 0.8765413761138916, 0.8771629929542542, 0.8743082880973816, 0.8744711875915527, 0.8747465014457703, 0.8744134306907654, 0.8731043934822083, 0.8741859197616577, 0.875590443611145, 0.8761821985244751, 0.8773965835571289, 0.8771921992301941, 0.8767245411872864, 0.8746771216392517, 0.8756086230278015, 0.8755055665969849, 0.8765822649002075, 0.8744382262229919, 0.8765994310379028, 0.8750819563865662, 0.8768104910850525, 0.875586986541748, 0.8737616539001465, 0.8758431673049927, 0.8747860193252563, 0.8734094500541687, 0.8739855885505676, 0.8767392039299011, 0.8754457235336304, 0.8752113580703735, 0.8758730888366699, 0.8756693005561829, 0.8777393102645874, 0.8743200302124023, 0.8737110495567322, 0.875892698764801, 0.8738139271736145, 0.8756818771362305, 0.875135600566864, 0.8765445351600647, 0.8753364682197571, 0.8760854601860046, 0.8769770264625549, 0.8762322664260864, 0.8760355710983276, 0.8748666048049927, 0.8727024793624878, 0.8735653758049011, 0.8764203786849976, 0.8741565942764282, 0.874511182308197, 0.8740649819374084, 0.8768531680107117, 0.8753649592399597, 0.8760239481925964, 0.8740819692611694, 0.8770672678947449, 0.8740824460983276, 0.8757521510124207, 0.8732955455780029, 0.8742645382881165, 0.8744993805885315, 0.8753257393836975, 0.8767148852348328, 0.8750676512718201, 0.8765944242477417, 0.8761193752288818, 0.877244770526886, 0.8751426339149475, 0.875042736530304, 0.874237060546875, 0.8753097653388977, 0.8742507696151733, 0.8749258518218994, 0.876871645450592, 0.8761643767356873], "accuracy_train_first": 0.3814397609819121, "model": "residualv5", "loss_std": [0.20407918095588684, 0.18546022474765778, 0.2056679129600525, 0.21642786264419556, 0.22502978146076202, 0.2320670634508133, 0.23512540757656097, 0.23694398999214172, 0.23772650957107544, 0.24000532925128937, 0.24038204550743103, 0.23958751559257507, 0.2402576059103012, 0.2394668310880661, 0.24050424993038177, 0.24037042260169983, 0.23823252320289612, 0.23822744190692902, 0.23956412076950073, 0.23669646680355072, 0.2377697229385376, 0.23745861649513245, 0.23707903921604156, 0.23700103163719177, 0.2374548614025116, 0.23642316460609436, 0.23724959790706635, 0.23546579480171204, 0.23650908470153809, 0.2344374805688858, 0.23504862189292908, 0.236053928732872, 0.23609615862369537, 0.23421736061573029, 0.23552383482456207, 0.23508822917938232, 0.23228873312473297, 0.23345960676670074, 0.2333943098783493, 0.23371118307113647, 0.23276221752166748, 0.231671541929245, 0.23363423347473145, 0.23394910991191864, 0.23296105861663818, 0.23373019695281982, 0.23232315480709076, 0.23390215635299683, 0.23247767984867096, 0.23271402716636658, 0.23285004496574402, 0.2327597439289093, 0.2315186858177185, 0.23345409333705902, 0.2321377843618393, 0.23345769941806793, 0.23401030898094177, 0.23052746057510376, 0.23144137859344482, 0.2324228733778, 0.23191943764686584, 0.23081113398075104, 0.2329132854938507, 0.23342417180538177, 0.23206807672977448, 0.2300788015127182, 0.23090319335460663, 0.23140166699886322, 0.23341739177703857, 0.23021793365478516, 0.23045533895492554, 0.23128058016300201, 0.2312484234571457, 0.22997784614562988, 0.22998692095279694, 0.23248594999313354, 0.23131753504276276, 0.23217207193374634, 0.23183389008045197, 0.23084084689617157, 0.23086604475975037, 0.23133687674999237, 0.22932779788970947, 0.23043778538703918, 0.23103933036327362, 0.23296236991882324, 0.23224881291389465, 0.23056337237358093, 0.23220016062259674, 0.23190633952617645, 0.23357732594013214, 0.2318454384803772, 0.23068289458751678, 0.23135137557983398, 0.23262399435043335, 0.23220902681350708, 0.23043574392795563, 0.230414018034935, 0.23151758313179016, 0.2304198145866394, 0.2311013787984848, 0.23221567273139954, 0.23003390431404114, 0.2319953441619873, 0.22959241271018982, 0.22968505322933197, 0.22988274693489075, 0.23072412610054016, 0.23220795392990112, 0.23286332190036774, 0.23191015422344208, 0.23064981400966644, 0.23230451345443726, 0.230760395526886, 0.23358754813671112, 0.23190775513648987, 0.23273876309394836, 0.23141492903232574, 0.22946105897426605, 0.23119090497493744, 0.23228536546230316, 0.23163007199764252, 0.23075979948043823, 0.23068110644817352, 0.23217962682247162, 0.23149394989013672, 0.23063109815120697, 0.23182570934295654, 0.23180794715881348, 0.23198579251766205, 0.23018305003643036, 0.23255054652690887, 0.2312496304512024, 0.22996889054775238, 0.23029069602489471, 0.2318601906299591, 0.23095552623271942, 0.2306697964668274, 0.23154202103614807, 0.2318161278963089, 0.23015116155147552, 0.23170888423919678, 0.23117463290691376, 0.23229490220546722, 0.2309829294681549, 0.23083814978599548, 0.23147958517074585, 0.22860011458396912, 0.23079916834831238, 0.23105499148368835, 0.23282228410243988, 0.23047780990600586, 0.231044739484787, 0.23062843084335327, 0.23000454902648926, 0.22891372442245483, 0.22930556535720825, 0.23107165098190308, 0.23436574637889862, 0.23132580518722534, 0.23277589678764343, 0.23020265996456146, 0.2306809276342392, 0.23187287151813507, 0.2315458506345749, 0.23058462142944336, 0.23068973422050476, 0.2304767221212387, 0.23244060575962067, 0.2322271168231964, 0.23098954558372498, 0.231981560587883, 0.2296508401632309, 0.23133644461631775, 0.23043575882911682, 0.23206588625907898]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:17 2016", "state": "available"}], "summary": "68bddce30f2f4f1e10ba2ab3be42664b"}