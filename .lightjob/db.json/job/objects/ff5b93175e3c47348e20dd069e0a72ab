{"content": {"hp_model": {"f0": 32, "f1": 32, "f2": 16, "f3": 64, "nonlin": "very_leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.03803574986910743, 0.036982956428817514, 0.040008789258053824, 0.04339375566818218, 0.04639803288097929, 0.0464636816599721, 0.04970366379310345, 0.045909788601476084, 0.038019289287734084, 0.04419745804917205, 0.04632759167102428, 0.04106618862858317, 0.03724035853598351, 0.036349572004138735, 0.041740168190587734, 0.04282303724140865, 0.03951988649282212, 0.03765870875590412, 0.039476477593400144, 0.03700968415695435, 0.043693969782620135, 0.04027726213403013, 0.039632435561493935, 0.03794309609670069, 0.033931402043479164, 0.03650397831341948, 0.03655066940979877, 0.042083889053943814, 0.033832603311292245, 0.03528169298220884, 0.037471307580044806, 0.03402324869578412, 0.03563908675236478, 0.03706357132025953, 0.03423271284495437, 0.03339756761323612, 0.0383341448549292, 0.03404963539807257, 0.036384744551561946, 0.03569809274545733, 0.03640742652268567, 0.03228377958639872, 0.03427191102495009, 0.03204120638234133, 0.038948071101070646, 0.03655265494199513, 0.03691715826788943, 0.03898066636253009, 0.03624860494107187, 0.040724570779533634, 0.03569072233120517, 0.032648428073753476, 0.03148368703662792, 0.03475554856795227, 0.03371764486284016, 0.036110444784193146, 0.03309112012340286, 0.032070921207776576, 0.03813864594253497, 0.03500496954909652, 0.031110344899250107, 0.028502613499346047, 0.03213647940014747, 0.03572222655557312, 0.03669730862900393, 0.03710735813797694, 0.03387333579525842, 0.03969030468253655, 0.0376435292813285, 0.03842585572840946, 0.035344375851297426, 0.03637851076452157, 0.03983973343922275, 0.03786171824847085, 0.031501258872893734, 0.03493986037647547, 0.03132191116661008, 0.03842207816415212, 0.03458153619040582, 0.0370723817623132, 0.038244111756726866, 0.032789002330443094, 0.03266148505635505, 0.029943035896630398, 0.024681782157427494, 0.03232562179735584, 0.03748970268350844, 0.03264815020887912, 0.028741279134892807, 0.03428461432373225, 0.03906714449202046, 0.034519570531302626, 0.03550749771209107, 0.038912884177872274, 0.02793195376257672, 0.0358671942572214, 0.03415072837348214, 0.03342824781774333, 0.03641365535953184, 0.03646220376800068, 0.032806981108974606, 0.03572222655557312, 0.036127272875517695, 0.0366599614451354, 0.02951859779204085, 0.03724888361792616, 0.03243292822364222, 0.03371333974984216, 0.0373065593432321, 0.0315944274397498, 0.029537952942207287, 0.034732310225321764, 0.033303995485552844, 0.03923513613156656, 0.03274609002415661, 0.03935864259493554, 0.029711887239142754, 0.03120962249135177, 0.038490960355700626, 0.03377356139356369, 0.03172594788748468, 0.03231523650604214, 0.029623822831662226, 0.030788207406466234, 0.0356886888456136, 0.037557396374662606, 0.03781472673608297, 0.029312585586412806, 0.03472604104379745, 0.03241278285063585, 0.0356869094506709, 0.029593490166508092, 0.03217710351027596, 0.030507598370085102, 0.0346991229449236, 0.033792090189358075, 0.03215905468569808, 0.030937235387835327, 0.031232867721269326, 0.03794979002468959, 0.034787638928192234, 0.03754580040988006, 0.03813650510600515, 0.03486084022179408, 0.03276243101138653, 0.03152774213490281, 0.034961403919833926, 0.0350911629962788, 0.03760084939249589, 0.03208873689510366, 0.030333142556963892, 0.036676537405658516, 0.035501109885976066, 0.03102771202857337, 0.03708803960259946, 0.033359517606042326, 0.035102536090064716, 0.029457068599985238, 0.033050521545725804, 0.03541207169109303, 0.03286417106770382, 0.030812948249839385, 0.030163091713402534, 0.03018113184163817, 0.035102536090064716, 0.036385741858377674, 0.027639088423880617, 0.031685031653921426, 0.029979672736620097, 0.029139692420236982, 0.030112220592281407, 0.027393801333806384, 0.030851786632478, 0.02840856572874879, 0.023037737165730942, 0.02568926022770498, 0.03316751878783846, 0.02731188159048291, 0.030354668148713952, 0.026130424014971794, 0.030489453867750503, 0.0272450363788158, 0.026371982357055453, 0.03276270790703025, 0.03041110033592783, 0.025438011010659487, 0.025030694686005633, 0.025561814995408053, 0.03230962144518308, 0.029609119965590704, 0.024711168740234112, 0.030409907088641178, 0.028585246525583072, 0.028149361579693194, 0.03290499956760815], "moving_avg_accuracy_train": [0.0325277673192771, 0.07072595067771083, 0.11222263271837346, 0.1475765177899096, 0.18675570712537645, 0.22674212812970626, 0.2672409198348079, 0.30876795736939944, 0.3499473551565559, 0.38685821075535815, 0.4238453941978946, 0.4587904858022015, 0.49056815785451147, 0.5176855702317109, 0.5426512940820338, 0.5628190524750353, 0.5834126178600618, 0.6052247822788749, 0.6247168936594212, 0.6399372223657682, 0.6530448743761793, 0.6679620548602481, 0.6801285752778377, 0.6923985679006563, 0.7057617796346871, 0.7162638207977244, 0.7250755976034942, 0.7334368254937471, 0.7438210232154567, 0.75138781020114, 0.758096732494279, 0.7649489568352126, 0.7692757855492818, 0.7783774803377271, 0.7848559032376893, 0.7892722330946432, 0.7918773893032511, 0.7989754184452152, 0.8055377787091275, 0.808954256862311, 0.8129585864471642, 0.818054388194014, 0.8214546157902752, 0.8242018499943803, 0.8274250196636169, 0.8322389936008697, 0.8335807004154815, 0.8349317794703188, 0.8392680442943713, 0.8420741088408378, 0.8445007341013323, 0.8481624829803557, 0.8490272399534045, 0.8515750995424014, 0.8547223712146673, 0.854942905177538, 0.8561085355935191, 0.8592801556787454, 0.8586119292674973, 0.8584270352865306, 0.8617433114265522, 0.8649821015188368, 0.8666051263067122, 0.8657997529832698, 0.8660867769319308, 0.8691053582748823, 0.8696924692847435, 0.8701267426875945, 0.8707623176658229, 0.8669409804775539, 0.8681775111948586, 0.8701987096235655, 0.8661207626069921, 0.8690182872499074, 0.8714613380429889, 0.8730553209555574, 0.8760524055768691, 0.8784979933324352, 0.8764727421919627, 0.8733840146595134, 0.8745339415068152, 0.8764230737115553, 0.8774502881777491, 0.8785347962575645, 0.8816992947944586, 0.8817659052246514, 0.8843649171118249, 0.8853250744970279, 0.8880740994268432, 0.8889598370745203, 0.8883733413188755, 0.8884902617050602, 0.8881648612875661, 0.8849329007612191, 0.8887777130947357, 0.8880023514238163, 0.8891941155284226, 0.8926975202406406, 0.8933985889997091, 0.8939942534431118, 0.8963211082192826, 0.8952196826383182, 0.8965768559407514, 0.8957016439912545, 0.8970435654355026, 0.8989572435305065, 0.9006254310750463, 0.9008749173350116, 0.9033020152099441, 0.90402744245401, 0.9046262042327055, 0.9055415958576276, 0.9054453617236721, 0.906921251003112, 0.9069529587341261, 0.9076521470474604, 0.9092909233065698, 0.9096410101927803, 0.9086500831192854, 0.90904778188567, 0.9091445097211994, 0.9110340873635373, 0.9110051326934486, 0.9107296382494049, 0.9094698333100066, 0.9104726805512952, 0.9089232475865271, 0.9100560546049827, 0.9093248279095446, 0.9101962796065419, 0.9108723406519118, 0.9122196886650339, 0.9110720797382895, 0.911658207607834, 0.9115974320277734, 0.913046404939454, 0.9143387147467135, 0.9149582130009578, 0.9153016236285728, 0.9157989462054745, 0.9162441833620356, 0.9165672424354706, 0.9139118359630078, 0.9149364091438156, 0.9167009572354582, 0.9174889752167317, 0.9179393435083115, 0.9181470093080828, 0.9186609981363106, 0.9179799510335229, 0.9187553746048694, 0.9190038017528163, 0.9172624953727154, 0.9169307300221909, 0.9140836660561165, 0.9140180140589386, 0.9146154596409966, 0.9159297043395476, 0.916470111164629, 0.9149186384517805, 0.9129010780704578, 0.9130148671007615, 0.913500842740083, 0.9149453744299302, 0.9140640711736842, 0.9092799343876411, 0.8287576261898408, 0.757179397456399, 0.6926178018372651, 0.6336134576475144, 0.5810131246839678, 0.5351317858601493, 0.49257963890064044, 0.4537085349503354, 0.4182115519372296, 0.3873679005085669, 0.3614087836504813, 0.3361559888697705, 0.31668289751291395, 0.2956085460146346, 0.279785454967388, 0.2618007912477576, 0.24633701483382522, 0.23495632539863548, 0.22426660400335025, 0.21210443908494292, 0.2030692587306655, 0.19451873346000859, 0.1893905611682246, 0.1842622166477877, 0.17960199648903302, 0.17454183449073213, 0.17000180766816494, 0.16607579858809543, 0.1639236968919365], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 1234, "moving_var_accuracy_train": [0.009522500820993283, 0.021702161645854666, 0.03502971706472388, 0.04277582006511397, 0.05231331795146182, 0.06147221093893521, 0.07008635901120058, 0.07859817672767486, 0.08600004427392316, 0.08966174119585152, 0.09300803272737296, 0.09469766429973629, 0.09431628183934011, 0.09150284014172191, 0.08796214243388495, 0.08282657449788278, 0.07836077146550079, 0.07480662896865073, 0.07074544772643025, 0.0657558286071505, 0.060726540617469765, 0.056656587018071344, 0.05232314628790885, 0.04844580612979418, 0.0452084043674516, 0.04168019974801761, 0.03821100646748816, 0.035019097007234046, 0.03248767136742268, 0.029754210618260758, 0.027183876301453084, 0.024888065477074117, 0.022567751949854744, 0.02105654438706738, 0.01932861961779744, 0.017571293380666504, 0.01587524559244109, 0.014741159192498517, 0.013654624423349032, 0.01239421288775476, 0.011299103497796463, 0.010402897907005599, 0.009466662045662428, 0.008587921503046025, 0.007822628757191608, 0.007248934987089387, 0.006540243082967831, 0.0059026475061828305, 0.00548161148918338, 0.0050043163244154676, 0.004556881283367752, 0.004221868798708239, 0.003806412160439343, 0.0034841952407626002, 0.003224923587497766, 0.002902868945807004, 0.0026248102996262474, 0.002452861835348727, 0.002211594390644059, 0.0019907426236374323, 0.0018906475482055802, 0.0017959906447419466, 0.0016400994654262727, 0.0014819271545946581, 0.0013344758838591366, 0.0012830347953893568, 0.0011578336098915227, 0.0010437475893981844, 0.0009430084304349175, 0.0009801311485494568, 0.0008958791076280573, 0.0008430583846591171, 0.0009084194130230254, 0.0008931383132274332, 0.0008575409565028774, 0.0007946538945826353, 0.0007960311511701012, 0.0007702561312846649, 0.0007301452977940622, 0.0007429929079420484, 0.00068059460293515, 0.0006446545270245135, 0.0005896856003580838, 0.0005413024602989407, 0.0005772986731790939, 0.0005196087384058786, 0.0005284416296723092, 0.00048389458654431796, 0.0005035193704725992, 0.00046022821404995276, 0.00041730118808746175, 0.0003756941026690659, 0.0003390776612875065, 0.00039918001475354106, 0.0004923052501978424, 0.0004484853966646359, 0.00041641957212742525, 0.0004852422161130025, 0.0004411414711461784, 0.00040022066919376747, 0.00040892688061888835, 0.0003789524373506244, 0.00035763446797110003, 0.0003287649847828684, 0.0003120952647673763, 0.00031384521295232037, 0.0003075063388109051, 0.00027731589547501763, 0.0003026015427780362, 0.00027707759067812894, 0.00025259647261895423, 0.00023487830179985862, 0.0002114738206967163, 0.00020993068111353866, 0.00018894666142403935, 0.00017445177395916575, 0.00018117688521003387, 0.00016416224414009947, 0.00015658344791095595, 0.0001423485818989157, 0.00012819793017651993, 0.00014751267015667698, 0.00013276894849728878, 0.00012017512834585019, 0.00012244159187925697, 0.00011924875599557076, 0.00012893056300680348, 0.00012758677237568623, 0.00011964032745920862, 0.00011451114725508382, 0.00010717355936317526, 0.00011279432344303371, 0.00011336794733742014, 0.00010512306551878955, 9.464400200709581e-05, 0.00010407530429544067, 0.0001086983556073464, 0.00010128252282971748, 9.221564827917647e-05, 8.52200511607247e-05, 7.848217117489479e-05, 7.157325854176341e-05, 0.000127876584493559, 0.00012453667786967583, 0.00014010567979218198, 0.00013168386286225645, 0.00012034096095857578, 0.00010869499062226994, 0.00010020315219993042, 9.435726338587693e-05, 9.033307248228708e-05, 8.185520966459117e-05, 0.0001009590198825532, 9.185373212457607e-05, 0.00015562031795439803, 0.00014009707782155928, 0.00012929984105108843, 0.00013191500909500294, 0.00012135186401485353, 0.00013088028582178756, 0.0001544272062701531, 0.0001391010171338947, 0.0001273164663186301, 0.00013336486591352016, 0.00012701863818739693, 0.00032030845745679586, 0.05864285666922487, 0.09888955645985475, 0.12651439747386636, 0.14519657142584272, 0.15557806953414216, 0.15896613785112224, 0.15936569096378278, 0.15702778636824324, 0.15266532995871343, 0.14596077446391773, 0.13742957874999165, 0.12942595367312273, 0.11989616988874281, 0.11190370751952579, 0.10296666866017831, 0.09558103495613418, 0.0881750868893416, 0.08052325902858959, 0.07349936441731, 0.06748069227510184, 0.061467333403900544, 0.05597860340514778, 0.05061742642412099, 0.04579238303939155, 0.04140860360280497, 0.03749819039756591, 0.03393387794975599, 0.030679212080451487, 0.02765297474780183], "duration": 79449.66147, "accuracy_train": [0.32527767319277107, 0.41450960090361444, 0.48569277108433734, 0.4657614834337349, 0.5393684111445783, 0.5866199171686747, 0.6317300451807228, 0.6825112951807228, 0.7205619352409639, 0.7190559111445783, 0.7567300451807228, 0.7732963102409639, 0.7765672063253012, 0.761742281626506, 0.7673428087349398, 0.7443288780120482, 0.7687547063253012, 0.8015342620481928, 0.8001458960843374, 0.7769201807228916, 0.7710137424698795, 0.8022166792168675, 0.7896272590361446, 0.8028285015060241, 0.8260306852409639, 0.8107821912650602, 0.8043815888554217, 0.8086878765060241, 0.8372788027108434, 0.8194888930722891, 0.8184770331325302, 0.8266189759036144, 0.8082172439759037, 0.8602927334337349, 0.8431617093373494, 0.8290192018072289, 0.8153237951807228, 0.8628576807228916, 0.8645990210843374, 0.8397025602409639, 0.8489975527108434, 0.8639166039156626, 0.8520566641566265, 0.8489269578313253, 0.856433546686747, 0.8755647590361446, 0.8456560617469879, 0.8470914909638554, 0.8782944277108434, 0.8673286897590361, 0.8663403614457831, 0.8811182228915663, 0.8568100527108434, 0.8745058358433735, 0.8830478162650602, 0.8569277108433735, 0.8665992093373494, 0.8878247364457831, 0.8525978915662651, 0.8567629894578314, 0.891589796686747, 0.8941312123493976, 0.8812123493975904, 0.8585513930722891, 0.8686699924698795, 0.8962725903614458, 0.874976468373494, 0.874035203313253, 0.8764824924698795, 0.8325489457831325, 0.8793062876506024, 0.8883894954819277, 0.8294192394578314, 0.8950960090361446, 0.8934487951807228, 0.8874011671686747, 0.9030261671686747, 0.9005082831325302, 0.8582454819277109, 0.8455854668674698, 0.8848832831325302, 0.8934252635542169, 0.886695218373494, 0.8882953689759037, 0.910179781626506, 0.8823653990963856, 0.9077560240963856, 0.8939664909638554, 0.9128153237951807, 0.8969314759036144, 0.8830948795180723, 0.8895425451807228, 0.8852362575301205, 0.8558452560240963, 0.9233810240963856, 0.8810240963855421, 0.8999199924698795, 0.9242281626506024, 0.8997082078313253, 0.8993552334337349, 0.9172628012048193, 0.8853068524096386, 0.9087914156626506, 0.8878247364457831, 0.9091208584337349, 0.9161803463855421, 0.9156391189759037, 0.9031202936746988, 0.9251458960843374, 0.9105562876506024, 0.9100150602409639, 0.9137801204819277, 0.9045792545180723, 0.9202042545180723, 0.907238328313253, 0.9139448418674698, 0.9240399096385542, 0.9127917921686747, 0.8997317394578314, 0.9126270707831325, 0.9100150602409639, 0.9280402861445783, 0.9107445406626506, 0.9082501882530121, 0.8981315888554217, 0.9194983057228916, 0.8949783509036144, 0.9202513177710844, 0.9027437876506024, 0.9180393448795181, 0.916956890060241, 0.9243458207831325, 0.9007435993975904, 0.9169333584337349, 0.9110504518072289, 0.9260871611445783, 0.9259695030120482, 0.9205336972891566, 0.9183923192771084, 0.9202748493975904, 0.9202513177710844, 0.9194747740963856, 0.8900131777108434, 0.9241575677710844, 0.932581890060241, 0.9245811370481928, 0.9219926581325302, 0.9200160015060241, 0.9232868975903614, 0.9118505271084337, 0.9257341867469879, 0.9212396460843374, 0.9015907379518072, 0.9139448418674698, 0.8884600903614458, 0.9134271460843374, 0.9199924698795181, 0.927757906626506, 0.9213337725903614, 0.9009553840361446, 0.8947430346385542, 0.914038968373494, 0.9178746234939759, 0.9279461596385542, 0.9061323418674698, 0.866222703313253, 0.10405685240963855, 0.11297533885542169, 0.11156344126506024, 0.10257435993975904, 0.1076101280120482, 0.12219973644578314, 0.10961031626506024, 0.10386859939759036, 0.09873870481927711, 0.10977503765060241, 0.12777673192771086, 0.10888083584337349, 0.14142507530120482, 0.10593938253012049, 0.13737763554216867, 0.09993881777108433, 0.10716302710843373, 0.13253012048192772, 0.12805911144578314, 0.10264495481927711, 0.12175263554216867, 0.11756400602409639, 0.14323701054216867, 0.13810711596385541, 0.13766001506024098, 0.12900037650602408, 0.12914156626506024, 0.13074171686746988, 0.14455478162650603], "end": "2016-01-19 12:11:31.636000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0], "accuracy_valid": [0.32287176724137934, 0.3978987068965517, 0.4738685344827586, 0.45393318965517243, 0.5239762931034483, 0.5686961206896551, 0.6159752155172413, 0.6629849137931034, 0.7009698275862069, 0.7017780172413793, 0.7327586206896551, 0.7493265086206896, 0.7547144396551724, 0.7400323275862069, 0.7433997844827587, 0.716864224137931, 0.7431303879310345, 0.7782866379310345, 0.7667025862068966, 0.7433997844827587, 0.7388200431034483, 0.7677801724137931, 0.755926724137931, 0.7638739224137931, 0.7830010775862069, 0.7720905172413793, 0.7640086206896551, 0.7657596982758621, 0.7885237068965517, 0.7823275862068966, 0.7747844827586207, 0.7867726293103449, 0.7657596982758621, 0.8153286637931034, 0.7956627155172413, 0.7815193965517241, 0.7770743534482759, 0.8119612068965517, 0.8180226293103449, 0.7891971982758621, 0.7990301724137931, 0.8071120689655172, 0.7959321120689655, 0.7947198275862069, 0.7984913793103449, 0.8232758620689655, 0.7941810344827587, 0.7971443965517241, 0.818426724137931, 0.8102101293103449, 0.8087284482758621, 0.8266433189655172, 0.7966056034482759, 0.8172144396551724, 0.8238146551724138, 0.802667025862069, 0.8053609913793104, 0.8252963362068966, 0.7924299568965517, 0.7944504310344828, 0.8286637931034483, 0.828125, 0.8153286637931034, 0.794989224137931, 0.8041487068965517, 0.8240840517241379, 0.8089978448275862, 0.8045528017241379, 0.8131734913793104, 0.767645474137931, 0.8098060344827587, 0.8181573275862069, 0.7720905172413793, 0.8236799568965517, 0.8279903017241379, 0.8185614224137931, 0.8345905172413793, 0.8277209051724138, 0.7944504310344828, 0.7757273706896551, 0.8155980603448276, 0.8173491379310345, 0.8197737068965517, 0.8125, 0.8370150862068966, 0.8126346982758621, 0.8337823275862069, 0.8192349137931034, 0.8358028017241379, 0.8282596982758621, 0.8081896551724138, 0.8168103448275862, 0.8142510775862069, 0.7827316810344828, 0.8465786637931034, 0.8087284482758621, 0.8267780172413793, 0.8484644396551724, 0.8304148706896551, 0.8211206896551724, 0.8306842672413793, 0.814385775862069, 0.8267780172413793, 0.806707974137931, 0.8336476293103449, 0.8318965517241379, 0.8380926724137931, 0.8248922413793104, 0.8430765086206896, 0.8366109913793104, 0.8308189655172413, 0.8324353448275862, 0.828125, 0.8433459051724138, 0.8282596982758621, 0.8327047413793104, 0.8426724137931034, 0.8293372844827587, 0.8250269396551724, 0.8278556034482759, 0.8321659482758621, 0.8480603448275862, 0.8278556034482759, 0.8255657327586207, 0.8246228448275862, 0.8335129310344828, 0.8207165948275862, 0.83984375, 0.8205818965517241, 0.8324353448275862, 0.8325700431034483, 0.8393049568965517, 0.8234105603448276, 0.8317618534482759, 0.8290678879310345, 0.837957974137931, 0.8411907327586207, 0.8375538793103449, 0.8358028017241379, 0.8352640086206896, 0.8359375, 0.8337823275862069, 0.8079202586206896, 0.8433459051724138, 0.8515625, 0.8352640086206896, 0.8343211206896551, 0.8368803879310345, 0.8375538793103449, 0.8243534482758621, 0.8394396551724138, 0.8345905172413793, 0.8232758620689655, 0.8285290948275862, 0.802667025862069, 0.8265086206896551, 0.8344558189655172, 0.8352640086206896, 0.8304148706896551, 0.8173491379310345, 0.8089978448275862, 0.8314924568965517, 0.8352640086206896, 0.8425377155172413, 0.8188308189655172, 0.7873114224137931, 0.10964439655172414, 0.12271012931034483, 0.11543642241379311, 0.10829741379310345, 0.11422413793103449, 0.12230603448275862, 0.11328125, 0.10304418103448276, 0.0980603448275862, 0.10735452586206896, 0.1336206896551724, 0.1089709051724138, 0.1462823275862069, 0.1089709051724138, 0.1414331896551724, 0.10237068965517242, 0.11045258620689655, 0.13833512931034483, 0.13321659482758622, 0.10008081896551724, 0.12419181034482758, 0.11570581896551724, 0.140625, 0.13011853448275862, 0.1314655172413793, 0.1302532327586207, 0.12904094827586207, 0.12513469827586207, 0.14951508620689655], "accuracy_test": 0.8410456730769231, "start": "2016-01-18 14:07:21.974000", "learning_rate_per_epoch": [0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888, 0.002425843384116888], "accuracy_train_last": 0.14455478162650603, "error_valid": [0.6771282327586207, 0.6021012931034483, 0.5261314655172413, 0.5460668103448276, 0.4760237068965517, 0.43130387931034486, 0.3840247844827587, 0.3370150862068966, 0.29903017241379315, 0.29822198275862066, 0.26724137931034486, 0.2506734913793104, 0.24528556034482762, 0.25996767241379315, 0.2566002155172413, 0.28313577586206895, 0.2568696120689655, 0.22171336206896552, 0.23329741379310343, 0.2566002155172413, 0.2611799568965517, 0.23221982758620685, 0.24407327586206895, 0.23612607758620685, 0.21699892241379315, 0.22790948275862066, 0.23599137931034486, 0.2342403017241379, 0.2114762931034483, 0.21767241379310343, 0.22521551724137934, 0.21322737068965514, 0.2342403017241379, 0.18467133620689657, 0.20433728448275867, 0.2184806034482759, 0.2229256465517241, 0.1880387931034483, 0.18197737068965514, 0.2108028017241379, 0.20096982758620685, 0.19288793103448276, 0.20406788793103448, 0.20528017241379315, 0.20150862068965514, 0.17672413793103448, 0.20581896551724133, 0.2028556034482759, 0.18157327586206895, 0.18978987068965514, 0.1912715517241379, 0.17335668103448276, 0.2033943965517241, 0.18278556034482762, 0.1761853448275862, 0.19733297413793105, 0.1946390086206896, 0.17470366379310343, 0.2075700431034483, 0.20554956896551724, 0.1713362068965517, 0.171875, 0.18467133620689657, 0.20501077586206895, 0.1958512931034483, 0.1759159482758621, 0.1910021551724138, 0.1954471982758621, 0.1868265086206896, 0.23235452586206895, 0.19019396551724133, 0.18184267241379315, 0.22790948275862066, 0.1763200431034483, 0.1720096982758621, 0.18143857758620685, 0.16540948275862066, 0.1722790948275862, 0.20554956896551724, 0.22427262931034486, 0.18440193965517238, 0.18265086206896552, 0.1802262931034483, 0.1875, 0.16298491379310343, 0.1873653017241379, 0.16621767241379315, 0.18076508620689657, 0.1641971982758621, 0.1717403017241379, 0.1918103448275862, 0.1831896551724138, 0.18574892241379315, 0.21726831896551724, 0.15342133620689657, 0.1912715517241379, 0.17322198275862066, 0.15153556034482762, 0.16958512931034486, 0.17887931034482762, 0.16931573275862066, 0.18561422413793105, 0.17322198275862066, 0.19329202586206895, 0.16635237068965514, 0.1681034482758621, 0.16190732758620685, 0.1751077586206896, 0.1569234913793104, 0.1633890086206896, 0.16918103448275867, 0.1675646551724138, 0.171875, 0.1566540948275862, 0.1717403017241379, 0.1672952586206896, 0.15732758620689657, 0.17066271551724133, 0.17497306034482762, 0.1721443965517241, 0.1678340517241379, 0.1519396551724138, 0.1721443965517241, 0.17443426724137934, 0.1753771551724138, 0.16648706896551724, 0.1792834051724138, 0.16015625, 0.1794181034482759, 0.1675646551724138, 0.1674299568965517, 0.1606950431034483, 0.17658943965517238, 0.1682381465517241, 0.17093211206896552, 0.16204202586206895, 0.15880926724137934, 0.16244612068965514, 0.1641971982758621, 0.1647359913793104, 0.1640625, 0.16621767241379315, 0.1920797413793104, 0.1566540948275862, 0.1484375, 0.1647359913793104, 0.16567887931034486, 0.16311961206896552, 0.16244612068965514, 0.1756465517241379, 0.1605603448275862, 0.16540948275862066, 0.17672413793103448, 0.1714709051724138, 0.19733297413793105, 0.17349137931034486, 0.16554418103448276, 0.1647359913793104, 0.16958512931034486, 0.18265086206896552, 0.1910021551724138, 0.1685075431034483, 0.1647359913793104, 0.15746228448275867, 0.18116918103448276, 0.21268857758620685, 0.8903556034482758, 0.8772898706896551, 0.8845635775862069, 0.8917025862068966, 0.8857758620689655, 0.8776939655172413, 0.88671875, 0.8969558189655172, 0.9019396551724138, 0.892645474137931, 0.8663793103448276, 0.8910290948275862, 0.8537176724137931, 0.8910290948275862, 0.8585668103448276, 0.8976293103448276, 0.8895474137931034, 0.8616648706896551, 0.8667834051724138, 0.8999191810344828, 0.8758081896551724, 0.8842941810344828, 0.859375, 0.8698814655172413, 0.8685344827586207, 0.8697467672413793, 0.8709590517241379, 0.8748653017241379, 0.8504849137931034], "accuracy_train_std": [0.04233058370648343, 0.04550568177869864, 0.04954166928894856, 0.04824419036129057, 0.04586899542728092, 0.044493638973716346, 0.044210033512917864, 0.044584776031953946, 0.04248283740727265, 0.039975427097510785, 0.04023473255581949, 0.038147923825339054, 0.039571513871225504, 0.040260701219639004, 0.039092958746178776, 0.03823977640314308, 0.0408561287653224, 0.03866970162125009, 0.038913505893135296, 0.040445273564811, 0.03873888251249591, 0.0386265968636096, 0.03777106810540259, 0.04037743923627853, 0.037524164994677284, 0.039055319358791356, 0.03508163036598969, 0.03655082033161453, 0.03648603541682398, 0.03703141641132673, 0.03698761507157007, 0.03543922327259843, 0.03835060287234614, 0.034291732698682464, 0.03654117621458269, 0.037696490531899954, 0.03924708484130867, 0.03748711445691966, 0.03503197741796498, 0.03496194728473584, 0.03643561394839076, 0.035530254726750636, 0.03491137865077231, 0.03700883770235897, 0.035294388359571896, 0.032587012366579224, 0.03585073224242042, 0.033364726390747404, 0.033253510492904384, 0.03442943146049477, 0.03479645728613825, 0.032896170233114405, 0.036218001770354925, 0.03272046223149178, 0.03283831523124753, 0.03570733389243091, 0.034576560366318475, 0.03269425450304082, 0.03469612049993158, 0.03726050825699938, 0.031220385378672867, 0.03272736620014559, 0.03168255407706731, 0.034910426963932685, 0.0347936802493985, 0.0316722056022344, 0.03341904398899386, 0.033509523629081495, 0.034218278237931075, 0.0368478522798633, 0.03251320647306999, 0.03166439830741954, 0.03681876977113683, 0.027982616616692327, 0.03305294845544206, 0.030920190104709283, 0.03120427656573502, 0.02972028590865379, 0.0361108584317847, 0.03664807383096671, 0.03346763191815329, 0.029879671356602657, 0.03493066058302246, 0.03200414535278516, 0.030496724672442133, 0.03084710826724954, 0.0295558727741576, 0.031911488492030994, 0.028936314006581348, 0.032483601363388945, 0.03165205833171961, 0.031136921571306916, 0.0316331148492201, 0.03475451554547144, 0.027733157462173478, 0.03318056132698999, 0.030790834303239523, 0.026506933257272002, 0.03157544459634426, 0.030167682845930466, 0.03075272994395231, 0.03206921279035825, 0.02909574998643286, 0.032298243930880224, 0.02790334985532443, 0.029437265312428292, 0.030250022570366488, 0.030165553553766766, 0.027255546398751072, 0.028845925478247783, 0.029291043490484104, 0.028420360271465273, 0.030432816728432073, 0.028459953059568475, 0.0294656182899916, 0.029391397151204698, 0.026481351201130543, 0.02861906315736067, 0.02985979817906204, 0.030487063471318066, 0.030120334854686112, 0.027538840413472817, 0.028950854032401614, 0.029715841932046516, 0.02877688945811246, 0.026577563224503248, 0.032074245710917444, 0.0280003317209387, 0.028666920485120807, 0.026834670775206817, 0.03020730431633133, 0.029149170107949703, 0.029077435864031533, 0.02819175280771888, 0.02826170875648769, 0.024907212341728047, 0.026751764803173796, 0.027618742325666985, 0.029212900787273732, 0.027912913428135656, 0.027530222998143065, 0.02817053165078934, 0.03047551867992219, 0.02673063302574414, 0.026142217910687514, 0.026924242913113086, 0.02661437360828901, 0.02887163717304503, 0.02820475269790633, 0.02840036282727125, 0.027114152624254935, 0.02726903322127108, 0.029625000045457872, 0.027397231136368553, 0.03220179064605908, 0.027208289873591397, 0.028726478227491, 0.026192507924419902, 0.027943635561462097, 0.02833922094238551, 0.029683400334500558, 0.028618356926480588, 0.028217118632703063, 0.025175233386502637, 0.029584664238228208, 0.031988223502744335, 0.0261869260891633, 0.029214369781175004, 0.027777158938525347, 0.02548703286354576, 0.026934113024053685, 0.02726074692789679, 0.028684772800876918, 0.02628123467070996, 0.026551100308217453, 0.028260013893456802, 0.030324625583915184, 0.02849176647869021, 0.03075341416902453, 0.028043420359947085, 0.02975108666861693, 0.02741646567369366, 0.026698722523010553, 0.03045017929028966, 0.03026708750184774, 0.026561484334902293, 0.028294175641066694, 0.02841279954002722, 0.03144905703349184, 0.031251514988929245, 0.029613409011127297, 0.027471034852774287, 0.028242344035918816, 0.029658364382591278, 0.03209454203439181], "accuracy_test_std": 0.031717924113090414, "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-3, 3], "momentum": 0.5781881100196087, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.0024258433429171335, "patience_threshold": 1, "do_flip": true, "batch_size": 128, "optimization": "adam", "nb_data_augmentation": 4, "learning_rate_decay_method": "none", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 1.454457791138812e-07, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.01909322543090476}, "accuracy_valid_max": 0.8515625, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = 1234\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-10, -6], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128, 256],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-3, 3)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_optimizer.learning_rate = learning_rate\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.14951508620689655, "loss_train": [3.6738228797912598, 1.5623035430908203, 1.4401637315750122, 1.3131352663040161, 1.2099963426589966, 1.109637975692749, 1.0307161808013916, 0.9546425342559814, 0.8896015286445618, 0.8338811993598938, 0.7799686193466187, 0.7329582571983337, 0.6990754008293152, 0.6561071276664734, 0.6400500535964966, 0.6151196956634521, 0.5996469855308533, 0.5877723693847656, 0.5697532892227173, 0.5694411396980286, 0.5612053275108337, 0.5495378375053406, 0.5414014458656311, 0.5309724807739258, 0.525209903717041, 0.5176504254341125, 0.5288535356521606, 0.5231408476829529, 0.5113853216171265, 0.5353530049324036, 0.4975043535232544, 0.5023743510246277, 0.4899299740791321, 0.491333544254303, 0.4759061932563782, 0.4653160870075226, 0.4637824296951294, 0.4600038230419159, 0.45869505405426025, 0.4509471654891968, 0.44788119196891785, 0.45046180486679077, 0.43457865715026855, 0.43734291195869446, 0.4331926703453064, 0.4146382212638855, 0.4190942943096161, 0.41133996844291687, 0.4243162274360657, 0.44293326139450073, 0.4028792381286621, 0.39569422602653503, 0.39576107263565063, 0.39628276228904724, 0.39651939272880554, 0.39156466722488403, 0.3968665301799774, 0.3793044090270996, 0.3855598270893097, 0.3714226484298706, 0.3867203891277313, 0.3711646497249603, 0.36954444646835327, 0.3759012222290039, 0.3720911145210266, 0.37108805775642395, 0.3656425178050995, 0.36438924074172974, 0.3635137379169464, 0.3567211627960205, 0.3620215654373169, 0.3562929928302765, 0.3580814003944397, 0.3708747327327728, 0.354413777589798, 0.35155582427978516, 0.3505918085575104, 0.3477235734462738, 0.3539714515209198, 0.35183531045913696, 0.3449164032936096, 0.3446979820728302, 0.3488738536834717, 0.3433086574077606, 0.3423445224761963, 0.33664143085479736, 0.34637096524238586, 0.335847407579422, 0.33149898052215576, 0.32653993368148804, 0.3381182849407196, 0.34704965353012085, 0.3440891206264496, 0.3277866840362549, 0.32736682891845703, 0.32932984828948975, 0.33970674872398376, 0.3267475962638855, 0.32857537269592285, 0.32799574732780457, 0.31725698709487915, 0.31376105546951294, 0.33552753925323486, 0.3170512318611145, 0.32961317896842957, 0.3232194781303406, 0.3149928152561188, 0.3181018829345703, 0.3105084002017975, 0.3147485554218292, 0.3097906708717346, 0.31898510456085205, 0.3155302107334137, 0.3079703748226166, 0.3081735074520111, 0.30970075726509094, 0.3126833438873291, 0.30563709139823914, 0.31047841906547546, 0.3122338354587555, 0.30303630232810974, 0.32081520557403564, 0.31499621272087097, 0.3073938190937042, 0.3052504360675812, 0.29859936237335205, 0.3073497414588928, 0.3074716627597809, 0.29723066091537476, 0.3067370057106018, 0.2961714565753937, 0.299390584230423, 0.30148714780807495, 0.30679386854171753, 0.2950212359428406, 0.2988854646682739, 0.3119633197784424, 0.2909148633480072, 0.2925724387168884, 0.30696892738342285, 0.29159897565841675, 0.3013937175273895, 0.2898535132408142, 0.30694380402565, 0.29237422347068787, 0.2914840579032898, 0.3037251830101013, 0.28901591897010803, 0.2870182394981384, 0.29433152079582214, 0.29812487959861755, 0.30300697684288025, 0.29252055287361145, 0.28777965903282166, 0.2868231236934662, 0.29629412293434143, 0.29304155707359314, 0.29204222559928894, 0.2853755056858063, 0.2935137152671814, 0.30611884593963623, 0.2905091941356659, 0.2842831015586853, 0.2848256826400757, 0.2879963517189026, 0.2839680314064026, 5.000709725008953e+18, 941933102891008.0, 546596831887360.0, 367617323827200.0, 270364584181760.0, 218092114280448.0, 181677502496768.0, 165406857756672.0, 139799507763200.0, 110258051612672.0, 98156888981504.0, 85300147650560.0, 74483884883968.0, 64037664587776.0, 55767788945408.0, 44213777465344.0, 40512195133440.0, 35841854406656.0, 30357682388992.0, 24000281444352.0, 20002669330432.0, 16425154510848.0, 13323393302528.0, 11626150363136.0, 10246123683840.0, 8724553924608.0, 7785168240640.0, 6869879881728.0, 5704653996032.0], "accuracy_train_first": 0.32527767319277107, "model": "residualv2", "loss_std": [13.452149391174316, 0.12402614206075668, 0.13053642213344574, 0.1222933754324913, 0.12609244883060455, 0.11618448048830032, 0.1177390068769455, 0.10887909680604935, 0.10978901386260986, 0.11054779589176178, 0.10682723671197891, 0.1042318120598793, 0.09877322614192963, 0.09409074485301971, 0.10246512293815613, 0.09168395400047302, 0.08927053958177567, 0.09299890697002411, 0.09086817502975464, 0.09463958442211151, 0.09445180743932724, 0.08832921832799911, 0.08479896187782288, 0.08666156977415085, 0.08765826374292374, 0.0825432762503624, 0.09320828318595886, 0.09181824326515198, 0.08835262060165405, 0.11781932413578033, 0.08430532366037369, 0.09803172200918198, 0.08357546478509903, 0.08972551673650742, 0.08328419923782349, 0.07983089238405228, 0.08307021856307983, 0.08213310688734055, 0.08315712213516235, 0.08073965460062027, 0.08345410972833633, 0.08322212845087051, 0.08157830685377121, 0.08061414211988449, 0.0797252431511879, 0.07679002732038498, 0.0740089863538742, 0.0760268121957779, 0.08004205673933029, 0.09861421585083008, 0.0740656778216362, 0.0771835520863533, 0.07210932672023773, 0.08076682686805725, 0.07187296450138092, 0.07374103367328644, 0.07876453548669815, 0.07363759726285934, 0.0713120773434639, 0.07230779528617859, 0.0744021013379097, 0.07295896112918854, 0.07068735361099243, 0.07572895288467407, 0.07193955034017563, 0.07456467300653458, 0.07341060042381287, 0.07094550877809525, 0.06645528972148895, 0.07083398103713989, 0.08161530643701553, 0.06924755871295929, 0.0668933317065239, 0.08988910168409348, 0.07018779963254929, 0.06915266811847687, 0.0715813860297203, 0.06944932788610458, 0.07045014947652817, 0.07196970283985138, 0.06490449607372284, 0.07413361221551895, 0.07178875058889389, 0.06897775828838348, 0.0718432366847992, 0.06645727157592773, 0.06791047006845474, 0.06757144629955292, 0.06801490485668182, 0.06907542794942856, 0.06970404833555222, 0.07316812872886658, 0.07263638824224472, 0.06874971091747284, 0.0644076019525528, 0.06665555387735367, 0.07610241323709488, 0.06395376473665237, 0.06652937829494476, 0.06585796922445297, 0.063787080347538, 0.06594927608966827, 0.07188434898853302, 0.06373585015535355, 0.0709233433008194, 0.06637544929981232, 0.06630169600248337, 0.06735191494226456, 0.06106223165988922, 0.06375985592603683, 0.06588003784418106, 0.0656810998916626, 0.06923297047615051, 0.06040177494287491, 0.06468973308801651, 0.06570690870285034, 0.06571995466947556, 0.059020742774009705, 0.06818058341741562, 0.06577374786138535, 0.06413764506578445, 0.07836105674505234, 0.06844960898160934, 0.06695453077554703, 0.06681981682777405, 0.0626511499285698, 0.06752470880746841, 0.07207266241312027, 0.061803240329027176, 0.06044461578130722, 0.06017720699310303, 0.06491950154304504, 0.06400222331285477, 0.07072476297616959, 0.06373600661754608, 0.06219767779111862, 0.07054214179515839, 0.06033727154135704, 0.06285128742456436, 0.07001998275518417, 0.06318735331296921, 0.06747101247310638, 0.05935834348201752, 0.06776205450296402, 0.06327570974826813, 0.06295984983444214, 0.06857120245695114, 0.06433618813753128, 0.06255076080560684, 0.06467875093221664, 0.07121103256940842, 0.061644166707992554, 0.06401445716619492, 0.061240456998348236, 0.0657065361738205, 0.06679761409759521, 0.06420435011386871, 0.06434915959835052, 0.05943713337182999, 0.07040661573410034, 0.07424630224704742, 0.0694747045636177, 0.06341501325368881, 0.061774011701345444, 0.06755388528108597, 0.060172971338033676, Infinity, 210129546903552.0, 99024539484160.0, 63078787973120.0, 45420751355904.0, 38560623230976.0, 42754260336640.0, 49426701746176.0, 39236635983872.0, 29547598708736.0, 31606114877440.0, 23319030005760.0, 19205483134976.0, 17241937543168.0, 14930484920320.0, 12024109072384.0, 10106918928384.0, 10312439824384.0, 10763663048704.0, 7696877617152.0, 7192762122240.0, 3578610581504.0, 3975237861376.0, 2630555271168.0, 2458383548416.0, 2311193624576.0, 1876368687104.0, 1844788723712.0, 1412848680960.0]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:02 2016", "state": "available"}], "summary": "1bc91d55786e6dd6603040138ba04893"}