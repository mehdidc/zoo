{"content": {"hp_model": {"f0": 64, "f1": 64, "f2": 64, "f3": 16, "nonlin": "rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.5925499200820923, 1.2073760032653809, 0.9665568470954895, 0.8350844383239746, 0.757279634475708, 0.7028844952583313, 0.6598083972930908, 0.6271682977676392, 0.6020610332489014, 0.5760650038719177, 0.5562677383422852, 0.5377466678619385, 0.522405743598938, 0.5068185925483704, 0.4933754503726959, 0.480480432510376, 0.4688056409358978, 0.45650428533554077, 0.44899997115135193, 0.4389919638633728, 0.43097057938575745, 0.42022591829299927, 0.4146191477775574, 0.4102315902709961, 0.4019899070262909, 0.3937963843345642, 0.3903839886188507, 0.3835805356502533, 0.37771645188331604, 0.371520072221756, 0.3661508560180664, 0.3617292046546936, 0.35603970289230347, 0.3523433804512024, 0.35090479254722595, 0.3449344336986542, 0.3413197994232178, 0.33731284737586975, 0.33465710282325745, 0.3300861716270447, 0.32662445306777954, 0.32244428992271423, 0.3192697763442993, 0.3180670440196991, 0.3145328760147095, 0.3107021450996399, 0.30751827359199524, 0.3034604489803314, 0.30054423213005066, 0.29917222261428833, 0.29814958572387695, 0.2938186526298523, 0.29169708490371704, 0.2876441478729248, 0.2886141538619995, 0.2843630611896515, 0.28112176060676575, 0.2800656855106354, 0.27764028310775757, 0.2764296233654022, 0.27741649746894836, 0.2735520899295807, 0.27083373069763184, 0.2671154737472534, 0.2656001150608063, 0.2641293406486511, 0.26136383414268494, 0.2620404362678528, 0.25963377952575684, 0.25764164328575134, 0.2573833167552948, 0.2554934322834015, 0.25368329882621765, 0.25252023339271545, 0.2493538111448288, 0.24806514382362366, 0.2486707866191864, 0.24585893750190735, 0.24460306763648987, 0.24401432275772095, 0.24184326827526093, 0.23813724517822266, 0.2377740889787674, 0.23840832710266113, 0.23636463284492493, 0.2351965457201004, 0.23405922949314117, 0.2331770807504654, 0.23221442103385925, 0.2297808974981308, 0.22658832371234894, 0.22636458277702332, 0.22799339890480042, 0.22384196519851685, 0.22556379437446594, 0.2251083105802536, 0.22556273639202118, 0.22060526907444, 0.22058679163455963, 0.21876966953277588, 0.21816234290599823, 0.21640338003635406, 0.2167239934206009, 0.21525603532791138, 0.21163035929203033, 0.2139003872871399, 0.2134677618741989, 0.21144448220729828, 0.21257399022579193, 0.20973090827465057, 0.21062518656253815, 0.20894277095794678, 0.20574192702770233, 0.20991173386573792, 0.20566633343696594, 0.2038973867893219, 0.2066664695739746, 0.20159487426280975, 0.20376108586788177, 0.20164678990840912, 0.20151865482330322, 0.20086832344532013, 0.20105980336666107, 0.1987253874540329, 0.19777370989322662, 0.19979159533977509, 0.19700632989406586, 0.19619901478290558, 0.1952587068080902, 0.19226354360580444, 0.19242927432060242, 0.19419723749160767, 0.19249418377876282, 0.19037578999996185, 0.18990689516067505, 0.19259272515773773, 0.19107232987880707, 0.19085340201854706, 0.18844908475875854, 0.18995220959186554, 0.18697987496852875, 0.18770650029182434, 0.18559947609901428, 0.18714360892772675, 0.18629829585552216, 0.18620245158672333, 0.18558946251869202, 0.1832442581653595, 0.18331663310527802, 0.18130257725715637, 0.18319012224674225, 0.1827448159456253, 0.1826818287372589, 0.17982445657253265, 0.1812361478805542, 0.1780565083026886, 0.17959356307983398, 0.17811229825019836, 0.17759749293327332, 0.17856945097446442, 0.17757003009319305, 0.17633891105651855, 0.17670483887195587, 0.1751050353050232, 0.1752745658159256, 0.17659582197666168, 0.17550435662269592, 0.17308373749256134, 0.17380285263061523, 0.17389866709709167, 0.17442913353443146, 0.1716664433479309, 0.17565366625785828, 0.17093728482723236, 0.167997345328331, 0.17081651091575623, 0.16920726001262665, 0.167905792593956, 0.17025865614414215, 0.17084220051765442, 0.1698065996170044, 0.16918544471263885, 0.16554024815559387, 0.16897185146808624, 0.1671888530254364, 0.16593427956104279, 0.16692276298999786, 0.16449880599975586, 0.16638997197151184, 0.16782262921333313, 0.16613681614398956, 0.16375970840454102, 0.16482190787792206, 0.16469299793243408], "moving_avg_accuracy_train": [0.05378197169735142, 0.11430604401704964, 0.1748243396937869, 0.23438452191696083, 0.2906738002069923, 0.3427105306168025, 0.39089657225104324, 0.4355587931693959, 0.47640103916968585, 0.5136311378759176, 0.5479632940507198, 0.5793970188342321, 0.6082730202488432, 0.6345334278838888, 0.6587375283114005, 0.6810535696270859, 0.7011657644016419, 0.7196738570333215, 0.7365192611624921, 0.7522009221632603, 0.7665213913568181, 0.779590994994069, 0.7916512573152139, 0.8026241120423488, 0.8127461110217611, 0.8220698237937083, 0.830716823511052, 0.8386270064411853, 0.8459649513593129, 0.8526177136177153, 0.8587471418741147, 0.8644680601560185, 0.8697447337454369, 0.8746355380044757, 0.8791024741507335, 0.8832365768763947, 0.8871432091366142, 0.8907521841231927, 0.8941932850111226, 0.8974436635340506, 0.9004225186761236, 0.9032312993908755, 0.9058103913567805, 0.908394207795115, 0.9105568461241307, 0.912780057523853, 0.9148204392645464, 0.9166173273990459, 0.9181858067415518, 0.9198134606961803, 0.921334224924412, 0.9227377539131446, 0.9240751905696338, 0.9253393734783405, 0.9265794085949768, 0.927823359433292, 0.9289801896663655, 0.9301188128820379, 0.9311854625035332, 0.9322128404295364, 0.93319553718554, 0.9341009627028665, 0.9349715050446138, 0.9358759729879191, 0.9366016745309508, 0.9373454867232507, 0.938138078485588, 0.9388957691431661, 0.939603159225435, 0.9403652962375537, 0.9409838623306219, 0.9415638593512973, 0.9421694538805916, 0.9427725455795571, 0.9433106778110072, 0.9438974115133875, 0.9445323565931303, 0.9452084388613274, 0.9456913188181718, 0.9460746854078848, 0.9466430377707968, 0.9470616210426743, 0.9474778014194885, 0.9478849518907733, 0.9484117865339681, 0.9488743480176144, 0.9492628236648193, 0.9496541962794189, 0.9500204185742345, 0.950447602791931, 0.9508157925461912, 0.951089070653606, 0.9514489171931273, 0.9518238963036871, 0.9521707501960666, 0.95257127435397, 0.9529411187889588, 0.9532599557899539, 0.9535724136301169, 0.9538583841303389, 0.9541203357805202, 0.954393258597817, 0.9546249021917083, 0.9549868772964576, 0.9552220461847981, 0.9554057963985904, 0.9556362036600324, 0.9559203001060448, 0.9561830344515219, 0.9564799131826801, 0.9568494826859791, 0.9570470563639021, 0.9572482323085842, 0.9574641318421223, 0.9577280877401361, 0.9579913328317093, 0.958197954381964, 0.9584467648926878, 0.9587543636606634, 0.9590311665030227, 0.9592850114564025, 0.9594390671525397, 0.9596590974873964, 0.9598408847959196, 0.9600509963497809, 0.9602586618899135, 0.960389793353423, 0.9606611633455538, 0.9608567124087466, 0.961153542206078, 0.9613904620891524, 0.9616269054231957, 0.9617654078083864, 0.9620434116300303, 0.9622796641766526, 0.9624829908733745, 0.9627007900349296, 0.9627898884839099, 0.962932892154668, 0.962975564952398, 0.9631883566310693, 0.9633170901240162, 0.9634817783926685, 0.9637020053499132, 0.9638537787328805, 0.964006614770399, 0.9642210052613361, 0.9642999162650566, 0.9643151686457953, 0.9645195219420223, 0.9646430220883977, 0.964809867645108, 0.9650181934152041, 0.965075514323776, 0.9652619617724432, 0.9654484017155384, 0.9656301485571812, 0.9657564822848888, 0.9657796820803478, 0.9658701000676342, 0.9659956540835729, 0.9661853826086321, 0.9663328507442712, 0.9664819202056506, 0.9666160106232546, 0.966685646871745, 0.9667924973227673, 0.9669142033167734, 0.9669795969328165, 0.9671360713884367, 0.9671235827723412, 0.9671935790309136, 0.96735419586481, 0.9674267074510403, 0.9675128581691146, 0.9676462694844475, 0.9678383471837049, 0.9678903814725788, 0.9679604638206606, 0.9680443561267635, 0.9682129012034559, 0.9682856088105928, 0.9683835616915305, 0.9684786586819844, 0.9685758717174404, 0.9687446715600467, 0.9688338844981728, 0.9689118870424954, 0.9689867035811861, 0.9690075354898172, 0.9690542220421182, 0.9690660130046653, 0.9692044720066628, 0.9692942799739551], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.054221867940512036, 0.11376814876694274, 0.1724645466632153, 0.23012043574689378, 0.28422638563041225, 0.33455954173039815, 0.38056636052422577, 0.42320246614160745, 0.4618292498361816, 0.49699824620986766, 0.5293512912707333, 0.5591424775615816, 0.5859077761156645, 0.6103424597239776, 0.632566638073869, 0.6532530218474308, 0.6716276561272961, 0.6885585405051539, 0.7037332422716565, 0.7176356439951685, 0.7300480902790102, 0.7415774138753561, 0.7520392543308175, 0.7612829827945731, 0.7697997099292724, 0.7776011007115711, 0.7844982230858205, 0.7910861102087144, 0.797076243775569, 0.8024663344770784, 0.8072288783637079, 0.8117888700924726, 0.8157829993671109, 0.8194234553133064, 0.8228016399495209, 0.825976283465864, 0.8286747712243229, 0.8312112144708665, 0.8335326935038251, 0.8356220246334878, 0.8379795263775938, 0.8399436160496988, 0.8416471730723645, 0.8433787754187425, 0.8446666038256634, 0.8459741927842115, 0.8470746921334259, 0.8479918993602188, 0.8487807647705824, 0.8494052944211596, 0.8500670863739985, 0.8508315385517342, 0.8515816101766059, 0.8522312310678308, 0.8530020838646623, 0.8538321877428797, 0.8543442591135465, 0.8549526372308063, 0.8553536931613401, 0.8557400870699802, 0.856012540382937, 0.8565893972256674, 0.8569844390543054, 0.8572433499587393, 0.8575374049289798, 0.8579139767007655, 0.8582009746443938, 0.8586301712311593, 0.8586146456366578, 0.8589312919540162, 0.8592253921449098, 0.8593059473393043, 0.8594160976166692, 0.8593788965052281, 0.8595651420674312, 0.8596117222695736, 0.8597990993178422, 0.8599209695536032, 0.8600652148422189, 0.8601228229231326, 0.8602845334772048, 0.8605175812119391, 0.86064187495445, 0.8606285804842309, 0.860716330728353, 0.8609794409254725, 0.8612915413076994, 0.8614452137959053, 0.86159778508386, 0.8615977333732903, 0.8615865093111871, 0.8617727496639539, 0.8617715265612633, 0.8617816032914322, 0.8618659735534034, 0.8618167479506986, 0.8619067222520143, 0.8620141722030177, 0.8618880620704419, 0.8618010360309429, 0.8618793449843246, 0.8621095439572777, 0.862183475197845, 0.8624829764167653, 0.8625297124253146, 0.8625738338503284, 0.8624761772631118, 0.8626243379630054, 0.8624758913655001, 0.8624612712142664, 0.8625722424079753, 0.8624890110135632, 0.8626491248783213, 0.8625104066205345, 0.8628148652995954, 0.8630512275083407, 0.8631062915986212, 0.8632423280072832, 0.86337387928035, 0.8634597723669987, 0.8637079745824825, 0.8636708908855294, 0.8637382603342505, 0.8639087561193496, 0.8640998529283483, 0.864208745882878, 0.863988337220795, 0.8640605831297397, 0.8640380962117205, 0.8643027377388918, 0.8642672128825478, 0.8642372995291575, 0.864110662243787, 0.8642805389316823, 0.8645412322147189, 0.8645815731781115, 0.8646890632153454, 0.8648000702974254, 0.864803349929957, 0.8646821722694161, 0.8650034469946581, 0.8652061155199664, 0.8652410033090842, 0.8656762638591998, 0.8656142791807346, 0.8657558644874352, 0.8657490139197158, 0.865667547203949, 0.8656929129184185, 0.8657757477090314, 0.8657160216768331, 0.8658087526228546, 0.8659329496026625, 0.8658707399210107, 0.8658991709176145, 0.8658647531669674, 0.8657106773702254, 0.8659148355368174, 0.8656082376194307, 0.8656893919260117, 0.865688159105775, 0.8658223564199715, 0.8657823835791791, 0.8659315725085354, 0.866090256607456, 0.8660590853330056, 0.8662182546893887, 0.8662862059053144, 0.8661255764198281, 0.866094991198779, 0.8661508547012655, 0.8662611375010938, 0.866337007467099, 0.8662811611066843, 0.8663550287121303, 0.866335030829622, 0.8663536538291147, 0.866211723122408, 0.8661338431200317, 0.8661847919217334, 0.8663181540793341, 0.8663984704014458, 0.866520612725006, 0.8665583281373699, 0.8665800649772473, 0.8666718408119773, 0.8669141599781441, 0.8670610640575135, 0.8671413610779669, 0.867042729958875, 0.866979405522852, 0.8668969699592717, 0.8667861568582994, 0.8667973178573339], "moving_var_accuracy_train": [0.026032504316892385, 0.0563977238566437, 0.08372012847553228, 0.1072748533860982, 0.12506371370120187, 0.13692773413677106, 0.144132012198485, 0.14767123677487418, 0.1479169146225206, 0.14559994540735044, 0.14164822339511443, 0.1363761125394934, 0.13024291240481364, 0.12342510224676109, 0.11635513831963068, 0.10920167578769799, 0.10192201161677533, 0.09481275589086331, 0.0878853890642527, 0.08131008058351454, 0.0750247550664758, 0.06905961041294183, 0.06346269871694112, 0.05820006071301228, 0.053302148411764054, 0.048754318149271586, 0.04455182177135013, 0.04065977854010866, 0.03707840960669108, 0.03376890185702339, 0.030730140688074174, 0.027951686773160744, 0.025407107653368063, 0.023081676584733347, 0.020953090593072717, 0.019011598781882324, 0.01724779488424338, 0.015640237699902786, 0.014182784507800715, 0.012859590701901446, 0.011653493833328392, 0.010559147691927617, 0.00956309836105222, 0.008666873491430067, 0.007842279183166209, 0.007102535285200289, 0.006429750175510052, 0.0058158344206701955, 0.005256392125633985, 0.004754596229634747, 0.004299951121212096, 0.003887685051690803, 0.003515015177812811, 0.0031778970858715233, 0.0028739465610987903, 0.0026004786281822166, 0.002352475071057372, 0.0021288957293970484, 0.0019262458291926696, 0.001743120794898952, 0.0015774999516373954, 0.0014271281147804878, 0.0012912358990214133, 0.0011694748694634745, 0.0010572671670831537, 0.0009565197595715643, 0.0008665215989299304, 0.0007850362952301684, 0.0007110362722635824, 0.0006451603204643957, 0.0005840879045213988, 0.0005287066829651895, 0.0004791367172738726, 0.00043449652192273597, 0.00039365314641719027, 0.000357386139713051, 0.00032527592303035254, 0.0002968621158276523, 0.00026927446171938523, 0.00024366974502641956, 0.00022220999019962806, 0.00020156589877912628, 0.00018296816385562014, 0.0001661632910264652, 0.0001520449545952497, 0.00013876612727110307, 0.00012624773450023465, 0.00011500151376133829, 0.00010470843130818436, 9.587996538000682e-05, 8.751204209828585e-05, 7.943296620438741e-05, 7.265507537199756e-05, 6.665505183500457e-05, 6.107231525543362e-05, 5.6408860139468974e-05, 5.199903828035134e-05, 4.771404775114888e-05, 4.38213120929477e-05, 4.017519302662781e-05, 3.6775241727259485e-05, 3.376809933234459e-05, 3.0874218190427764e-05, 2.8966030159510303e-05, 2.6567166797949366e-05, 2.421432738777269e-05, 2.2270682204123e-05, 2.0770011099441854e-05, 1.9314274016137195e-05, 1.8176079443650707e-05, 1.7587706059203535e-05, 1.6180253677155143e-05, 1.4926474155908393e-05, 1.3853340217554964e-05, 1.3095060640665318e-05, 1.2409236380735792e-05, 1.1552544927929056e-05, 1.0954450467356193e-05, 1.0710558439161495e-05, 1.0329080917088926e-05, 9.876108168588032e-06, 9.102095769339936e-06, 8.627606326720588e-06, 8.06226532390947e-06, 7.653360577112801e-06, 7.276149308428797e-06, 6.703293524085381e-06, 6.695739225339146e-06, 6.370320227845688e-06, 6.526259562314887e-06, 6.3788128850471576e-06, 6.244080648464532e-06, 5.792318779949377e-06, 5.908662025591561e-06, 5.820133215102083e-06, 5.610195603990656e-06, 5.476104316558627e-06, 4.9999406873989755e-06, 4.683997067311786e-06, 4.231986069575508e-06, 4.216310149223433e-06, 3.9438299441580255e-06, 3.7935469822273654e-06, 3.850691498280251e-06, 3.6729387864483535e-06, 3.515874597082793e-06, 3.5779566808132023e-06, 3.2762035313055827e-06, 2.9506768942388114e-06, 3.031451631924656e-06, 2.8655770441250507e-06, 2.8295562978581458e-06, 2.9371973064473806e-06, 2.6730487548381794e-06, 2.71860773938445e-06, 2.759585636878378e-06, 2.78091430321509e-06, 2.6464647697018184e-06, 2.386662367315722e-06, 2.2215748424083995e-06, 2.1412916564325225e-06, 2.251134709779159e-06, 2.2217428980613805e-06, 2.199563947098879e-06, 2.1414297132279625e-06, 1.9709296058394357e-06, 1.8765898152083976e-06, 1.8222419744805086e-06, 1.6785047022053312e-06, 1.7310125293393896e-06, 1.5593149661932613e-06, 1.4474787555012057e-06, 1.5349107859293594e-06, 1.4287410785751677e-06, 1.3526644867400314e-06, 1.3775852495957362e-06, 1.5718713076041117e-06, 1.4390522818111938e-06, 1.3393508732438991e-06, 1.2687570571290559e-06, 1.3975483373113923e-06, 1.3053710688003772e-06, 1.2611868638764006e-06, 1.2164591158291118e-06, 1.1798665726094031e-06, 1.3183203971236074e-06, 1.2581188923728944e-06, 1.1870665754228215e-06, 1.1187375480353791e-06, 1.0107695089867725e-06, 9.293092655799116e-07, 8.376295802020089e-07, 9.264046792890254e-07, 9.063534502628085e-07], "duration": 179836.438778, "accuracy_train": [0.5378197169735143, 0.6590226948943337, 0.7194890007844224, 0.770426161925526, 0.7972773048172758, 0.8110411043050941, 0.82457094695921, 0.8375187814345699, 0.843981253172296, 0.8487020262320044, 0.8569526996239387, 0.8623005418858435, 0.8681570329803433, 0.8708770965992986, 0.876574432159007, 0.881897941468254, 0.8821755173726468, 0.8862466907184385, 0.8881278983250278, 0.8933358711701735, 0.8954056140988372, 0.8972174277293282, 0.9001936182055187, 0.9013798045865633, 0.903844101836471, 0.905983238741233, 0.9085398209671466, 0.9098186528123846, 0.9120064556224622, 0.912492573943337, 0.9139119961817092, 0.9159563246931525, 0.917234796050203, 0.918652776335825, 0.9193048994670543, 0.9204435014073459, 0.9223028994785898, 0.9232329590023993, 0.9251631930024916, 0.9266970702404023, 0.9272322149547803, 0.9285103258236435, 0.9290222190499261, 0.9316485557401256, 0.9300205910852714, 0.9327889601213547, 0.9331838749307864, 0.9327893206095422, 0.9323021208241048, 0.9344623462878369, 0.9350211029784975, 0.9353695148117387, 0.9361121204780363, 0.9367170196567, 0.9377397246447029, 0.9390189169781286, 0.9393916617640274, 0.9403664218230897, 0.9407853090969915, 0.9414592417635659, 0.9420398079895718, 0.942249792358804, 0.9428063861203396, 0.944016184477667, 0.9431329884182356, 0.9440397964539498, 0.9452714043466224, 0.9457149850613695, 0.9459696699658545, 0.9472245293466224, 0.9465509571682356, 0.9467838325373754, 0.9476198046442414, 0.9482003708702473, 0.9481538678940569, 0.9491780148348099, 0.9502468623108158, 0.9512931792751015, 0.9500372384297711, 0.9495249847153008, 0.9517582090370063, 0.9508288704895718, 0.9512234248108158, 0.9515493061323367, 0.9531532983227206, 0.9530374013704319, 0.952759104489664, 0.9531765498108158, 0.9533164192275747, 0.9542922607511997, 0.954129500334533, 0.9535485736203396, 0.9546875360488187, 0.9551987082987264, 0.9552924352274824, 0.9561759917751015, 0.9562697187038575, 0.956129488798911, 0.9563845341915835, 0.9564321186323367, 0.9564779006321521, 0.9568495639534883, 0.9567096945367294, 0.9582446532392026, 0.9573385661798633, 0.9570595483227206, 0.9577098690130121, 0.958477168120155, 0.9585476435608158, 0.9591518217631044, 0.9601756082156699, 0.9588252194652085, 0.9590588158107235, 0.9594072276439645, 0.9601036908222591, 0.9603605386558692, 0.9600575483342562, 0.9606860594892026, 0.9615227525724437, 0.9615223920842562, 0.9615696160368217, 0.960825568417774, 0.9616393705011074, 0.9614769705726283, 0.961942000334533, 0.9621276517511074, 0.9615699765250092, 0.9631034932747323, 0.9626166539774824, 0.9638250103820598, 0.9635227410368217, 0.9637548954295865, 0.9630119292751015, 0.9645454460248246, 0.9644059370962532, 0.9643129311438722, 0.9646609824889257, 0.9635917745247323, 0.9642199251914912, 0.9633596201319674, 0.9651034817391103, 0.9644756915605389, 0.9649639728105389, 0.9656840479651162, 0.9652197391795865, 0.9653821391080657, 0.9661505196797711, 0.9650101152985419, 0.9644524400724437, 0.9663587016080657, 0.9657545234057769, 0.9663114776555003, 0.9668931253460686, 0.9655914025009228, 0.9669399888104466, 0.967126361203396, 0.9672658701319674, 0.9668934858342562, 0.9659884802394795, 0.9666838619532114, 0.967125640227021, 0.9678929393341639, 0.9676600639650241, 0.9678235453580657, 0.9678228243816908, 0.967312373108158, 0.9677541513819674, 0.9680095572628276, 0.9675681394772055, 0.968544341489018, 0.9670111852274824, 0.9678235453580657, 0.9687997473698781, 0.9680793117271133, 0.9682882146317828, 0.9688469713224437, 0.969567046477021, 0.9683586900724437, 0.968591204953396, 0.9687993868816908, 0.9697298068936876, 0.9689399772748246, 0.9692651376199704, 0.9693345315960686, 0.9694507890365448, 0.9702638701435032, 0.9696368009413067, 0.969613909941399, 0.9696600524294019, 0.9691950226674971, 0.9694744010128276, 0.9691721316675894, 0.97045060302464, 0.9701025516795865], "end": "2016-02-01 21:21:32.924000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0], "moving_var_accuracy_valid": [0.026460098666624948, 0.055725924842303806, 0.0811607364920514, 0.10296247675711116, 0.11901331339658033, 0.1299128214837922, 0.13597118571516584, 0.13873460466359797, 0.13828939996452497, 0.13559218472146356, 0.13145344197171083, 0.12629573080008402, 0.1201135885802783, 0.11347571358959574, 0.1065733691605862, 0.09976737050717134, 0.09282927812072343, 0.08612624392099857, 0.07958606369221942, 0.07336694828613469, 0.0674168728622645, 0.06187151329934139, 0.056669412920847005, 0.05177149027196712, 0.04724715301455272, 0.04307019299634066, 0.03919130637011496, 0.03566277804379939, 0.03241943554075826, 0.02943896968661692, 0.026699209136403876, 0.024216429943861115, 0.02193836456743771, 0.019863804386159655, 0.017980133130470893, 0.016272825070526537, 0.01471107908911685, 0.013297873079291596, 0.012016589155466632, 0.010854217981044367, 0.009818816513201097, 0.008871653696041617, 0.008010607285202717, 0.007236532576856282, 0.006527805837221706, 0.005890413353460188, 0.005312271907472761, 0.004788616138597416, 0.004315355302458689, 0.0038873301077728723, 0.0035025388142951655, 0.0031575444170540523, 0.0028468534423305874, 0.002565966163818372, 0.0023147174737459795, 0.002089447378409067, 0.00188286259436607, 0.0016979074503315084, 0.0015295643180331048, 0.0013779515885035019, 0.0012408245069228218, 0.0011197369305835836, 0.0010091677599425887, 0.000908854297656243, 0.0008187470828203272, 0.0007381486312320472, 0.0006650750784856641, 0.0006002254580279176, 0.0005402050816218875, 0.00048708695747236796, 0.00043915671602568385, 0.000395299446677211, 0.00035587869976192165, 0.0003203032850899615, 0.0002885851432659284, 0.0002597461563764202, 0.00023408753216273866, 0.00021081245013574468, 0.00018991846545176033, 0.00017095648712546328, 0.00015409619114260226, 0.0001391753732483253, 0.00012539687633333907, 0.00011285877938645081, 0.00010164220239589712, 9.210102493876185e-05, 8.376758228216094e-05, 7.560336115662757e-05, 6.825252702213808e-05, 6.142727434399012e-05, 5.528568072572196e-05, 5.006928187413819e-05, 4.50623671505461e-05, 4.055704429990956e-05, 3.656540493986446e-05, 3.2930672885532875e-05, 2.9710463971054788e-05, 2.6843327001684984e-05, 2.4302128191361208e-05, 2.1940077156183037e-05, 1.980126007018224e-05, 1.8298058167501885e-05, 1.6517444805738237e-05, 1.567300914637722e-05, 1.4125366522195647e-05, 1.2730350171283253e-05, 1.1543146435396004e-05, 1.0586396128793162e-05, 9.726084046712108e-06, 8.755399381439775e-06, 7.990690895794402e-06, 7.253968991356832e-06, 6.759300139411238e-06, 6.256554920861148e-06, 6.465155214075262e-06, 6.321443536174237e-06, 5.716587668902471e-06, 5.311482042347056e-06, 4.936085475121969e-06, 4.508875528616189e-06, 4.612427033694453e-06, 4.163561135542368e-06, 3.788052805577066e-06, 3.67086683964821e-06, 3.6324420693690016e-06, 3.3759169423478218e-06, 3.4755450530039407e-06, 3.174965789936615e-06, 2.8620201642809517e-06, 3.2061343889850124e-06, 2.8968790888508655e-06, 2.6152444583652674e-06, 2.4980530309429827e-06, 2.5079705296614854e-06, 2.8688223670788714e-06, 2.5965866703179516e-06, 2.4409149762273347e-06, 2.307726629051668e-06, 2.0770507700523796e-06, 2.0015019217745967e-06, 2.7303087713117404e-06, 2.826948674536269e-06, 2.5552082275484106e-06, 4.004753123176608e-06, 3.6388567141388155e-06, 3.4553886343865435e-06, 3.1102721434505956e-06, 2.858976361105938e-06, 2.5788695002303065e-06, 2.3827369730303223e-06, 2.176568066026692e-06, 2.03630251457441e-06, 1.9714962712576283e-06, 1.8091770445528198e-06, 1.6355342342084768e-06, 1.4826420448240102e-06, 1.5480320006165342e-06, 1.7683538134304362e-06, 2.4375389785993128e-06, 2.2530592740292052e-06, 2.027767025237909e-06, 1.987070594952343e-06, 1.8027439874661912e-06, 1.8227856185015573e-06, 1.867132845903459e-06, 1.689164396470881e-06, 1.7482619129266902e-06, 1.614992031345912e-06, 1.6857093126795272e-06, 1.5255574831311646e-06, 1.4010883130086466e-06, 1.3704401451494315e-06, 1.2852023963089923e-06, 1.184751500422226e-06, 1.1153841585889791e-06, 1.0074449804733917e-06, 9.098218274169688e-07, 1.0001385742315137e-06, 9.547123697396148e-07, 8.826031563191549e-07, 9.544120264066075e-07, 9.170272281440119e-07, 9.595932301718548e-07, 8.764359781226526e-07, 7.930447921811194e-07, 7.895455475266518e-07, 1.23905819739961e-06, 1.3093796544777564e-06, 1.2364701924733265e-06, 1.2003760521058289e-06, 1.1164283046739725e-06, 1.0659460734919205e-06, 1.0698673562667136e-06, 9.640017317350794e-07], "accuracy_test": 0.8512954400510203, "start": "2016-01-30 19:24:16.485000", "learning_rate_per_epoch": [0.001511449576355517, 0.0007557247881777585, 0.000503816525451839, 0.00037786239408887923, 0.0003022899036295712, 0.0002519082627259195, 0.00021592136181425303, 0.00018893119704443961, 0.00016793883696664125, 0.0001511449518147856, 0.0001374045095872134, 0.00012595413136295974, 0.00011626534978859127, 0.00010796068090712652, 0.00010076330363517627, 9.446559852221981e-05, 8.890879689715803e-05, 8.396941848332062e-05, 7.954998000059277e-05, 7.55724759073928e-05, 7.197378727141768e-05, 6.87022547936067e-05, 6.571519770659506e-05, 6.297706568147987e-05, 6.0457983636297286e-05, 5.813267489429563e-05, 5.597961353487335e-05, 5.398034045356326e-05, 5.211895040702075e-05, 5.0381651817588136e-05, 4.875643571722321e-05, 4.7232799261109903e-05, 4.58015019830782e-05, 4.445439844857901e-05, 4.318427454563789e-05, 4.198470924166031e-05, 4.0849987271940336e-05, 3.9774990000296384e-05, 3.8755115383537486e-05, 3.77862379536964e-05, 3.686462514451705e-05, 3.598689363570884e-05, 3.514999116305262e-05, 3.435112739680335e-05, 3.358776666573249e-05, 3.285759885329753e-05, 3.2158503017853945e-05, 3.1488532840739936e-05, 3.0845909350318834e-05, 3.0228991818148643e-05, 2.9636265026056208e-05, 2.9066337447147816e-05, 2.8517915779957548e-05, 2.7989806767436676e-05, 2.7480900826049037e-05, 2.699017022678163e-05, 2.6516658181208186e-05, 2.6059475203510374e-05, 2.561778819654137e-05, 2.5190825908794068e-05, 2.4777860744507052e-05, 2.4378217858611606e-05, 2.3991262423805892e-05, 2.3616399630554952e-05, 2.3253069230122492e-05, 2.29007509915391e-05, 2.2558948330697604e-05, 2.2227199224289507e-05, 2.1905065295868553e-05, 2.1592137272818945e-05, 2.1288022253429517e-05, 2.0992354620830156e-05, 2.0704788767034188e-05, 2.0424993635970168e-05, 2.0152659999439493e-05, 1.9887495000148192e-05, 1.9629214875749312e-05, 1.9377557691768743e-05, 1.9132272427668795e-05, 1.88931189768482e-05, 1.865987178462092e-05, 1.8432312572258525e-05, 1.8210235793958418e-05, 1.799344681785442e-05, 1.7781760107027367e-05, 1.757499558152631e-05, 1.7372984075336717e-05, 1.7175563698401675e-05, 1.698257983662188e-05, 1.6793883332866244e-05, 1.6609335943940096e-05, 1.6428799426648766e-05, 1.6252146451734006e-05, 1.6079251508926973e-05, 1.590999454492703e-05, 1.5744266420369968e-05, 1.5581954357912764e-05, 1.5422954675159417e-05, 1.5267167327692732e-05, 1.5114495909074321e-05, 1.4964846741349902e-05, 1.4818132513028104e-05, 1.467426773160696e-05, 1.4533168723573908e-05, 1.4394757272384595e-05, 1.4258957889978774e-05, 1.4125696907285601e-05, 1.3994903383718338e-05, 1.3866510016669054e-05, 1.3740450413024519e-05, 1.3616662727145012e-05, 1.3495085113390815e-05, 1.3375659364101011e-05, 1.3258329090604093e-05, 1.3143039723217953e-05, 1.3029737601755187e-05, 1.2918372704007197e-05, 1.2808894098270684e-05, 1.2701257219305262e-05, 1.2595412954397034e-05, 1.2491318557295017e-05, 1.2388930372253526e-05, 1.2288207472010981e-05, 1.2189108929305803e-05, 1.2091596545360517e-05, 1.1995631211902946e-05, 1.1901177458639722e-05, 1.1808199815277476e-05, 1.1716662811522838e-05, 1.1626534615061246e-05, 1.1537782484083436e-05, 1.145037549576955e-05, 1.1364282727299724e-05, 1.1279474165348802e-05, 1.119592252507573e-05, 1.1113599612144753e-05, 1.1032478141714819e-05, 1.0952532647934277e-05, 1.0873737664951477e-05, 1.0796068636409473e-05, 1.0719500096456613e-05, 1.0644011126714759e-05, 1.056957717082696e-05, 1.0496177310415078e-05, 1.0423789717606269e-05, 1.0352394383517094e-05, 1.028196948027471e-05, 1.0212496817985084e-05, 1.0143956387764774e-05, 1.0076329999719746e-05, 1.0009599463955965e-05, 9.943747500074096e-06, 9.878755008685403e-06, 9.814607437874656e-06, 9.751287507242523e-06, 9.688778845884372e-06, 9.627066901884973e-06, 9.566136213834397e-06, 9.505972229817417e-06, 9.4465594884241e-06, 9.387885256728623e-06, 9.32993589231046e-06, 9.27269684325438e-06, 9.216156286129262e-06, 9.16030057851458e-06, 9.105117896979209e-06, 9.050596418092027e-06, 8.99672340892721e-06, 8.943488865043037e-06, 8.890880053513683e-06, 8.838886060402729e-06, 8.787497790763155e-06, 8.736702511669137e-06, 8.686492037668359e-06, 8.636854545329697e-06, 8.587781849200837e-06, 8.539263035345357e-06, 8.49128991831094e-06, 8.443852493655868e-06, 8.396941666433122e-06, 8.350550160685088e-06, 8.304667971970048e-06, 8.259286914835684e-06, 8.214399713324383e-06, 8.169997272489127e-06, 8.126073225867003e-06, 8.082617569016293e-06, 8.039625754463486e-06, 7.997087777766865e-06, 7.954997272463515e-06, 7.913348781585228e-06, 7.872133210184984e-06, 7.831345101294573e-06, 7.790977178956382e-06], "accuracy_train_first": 0.5378197169735143, "accuracy_train_last": 0.9701025516795865, "batch_size_eval": 1024, "accuracy_train_std": [0.015210415774017109, 0.01719695629966626, 0.016711947335866062, 0.016924571300046303, 0.01538980987071102, 0.013900152160143776, 0.013790342721023287, 0.014653462346727207, 0.012619442624154016, 0.01245460825856815, 0.013125956721615943, 0.013343399615666727, 0.014365347447806345, 0.013133489907939798, 0.013157033619865794, 0.012901947904538458, 0.013480611636031738, 0.012471513445356647, 0.012937034328357586, 0.012575920017990943, 0.011550888252058258, 0.012536634773604146, 0.012129509758479557, 0.012167374995435807, 0.010679518840053429, 0.010896091345862101, 0.010773845423590535, 0.010888154556087336, 0.010985908726965515, 0.011210412911103305, 0.01116239392977621, 0.01051382502507107, 0.010596163416407805, 0.010735511671661015, 0.010471292131418137, 0.010342410328139052, 0.010679300994599029, 0.009985803450670676, 0.0100280616915382, 0.009749763421449776, 0.009514528497028123, 0.008990805658597351, 0.009089693959674586, 0.009155823710262164, 0.00985445398428043, 0.008363568276274134, 0.008661052545869966, 0.008919291641928822, 0.008957598269720583, 0.009110750852471157, 0.007995466974571888, 0.008187371447089295, 0.008392797430979223, 0.008893630675158474, 0.008820587903573521, 0.008269711184466714, 0.008737722641136555, 0.009272882179535074, 0.008345206126086298, 0.008085792539600506, 0.0086474772342493, 0.008909832642627824, 0.00896504919017232, 0.008876418092436543, 0.008874501443928182, 0.008563113334581467, 0.009016154037869394, 0.008742285684432048, 0.008143571336837989, 0.008911115289890734, 0.008746484802599876, 0.008196730572689765, 0.009132037205089566, 0.009142170166111657, 0.008281721616758683, 0.008302710813252002, 0.008827620059201942, 0.008351845108384433, 0.008763212351377878, 0.009510157820304696, 0.008629315951846829, 0.007988976982828416, 0.008959826899579006, 0.008183119128308927, 0.008147818714847188, 0.007870541339043533, 0.007849546580077504, 0.008463468672023433, 0.007929068513086283, 0.008412733353628533, 0.008499422289442714, 0.0075776585013770555, 0.007925590340142265, 0.008025515529061476, 0.007943411111293264, 0.007656581738913926, 0.00698295809930487, 0.007919566847956422, 0.007869053708437839, 0.007696481351066555, 0.007662559003526881, 0.008713848552129047, 0.008696837689824814, 0.007877095899636095, 0.007762689913980407, 0.007919247211675034, 0.007882226568875472, 0.007602451118415537, 0.007743493858494876, 0.007230230618728216, 0.006739158743549073, 0.008056788845607012, 0.007475718275614921, 0.007957718579509665, 0.00819761559355587, 0.0075318230663431835, 0.007147903536062406, 0.006678342208330525, 0.007585529420395762, 0.007455817209070465, 0.0071657758880005265, 0.0069682473904086155, 0.0074894448216668505, 0.007282818075494054, 0.00724276783318752, 0.006850217067286405, 0.006884042955785037, 0.006919480869281024, 0.006337959296548056, 0.006617870472242932, 0.006916259130949091, 0.007266779417582809, 0.006493083135373889, 0.007223365263894767, 0.00710207297919863, 0.007140387591730152, 0.007073575319659013, 0.006964203092947712, 0.007113737966099659, 0.007285829959632642, 0.006405225757160063, 0.006850600579864517, 0.006690170560597172, 0.007329558316411158, 0.006614069327464866, 0.0071082360690349535, 0.006188932679089493, 0.007023958102544602, 0.007120748738340403, 0.0067370841723031465, 0.00693367984275387, 0.007104856395066185, 0.006974175885419442, 0.006856446937053275, 0.00651529145237298, 0.006574060639696715, 0.006523949820974759, 0.006519630639959238, 0.006072763284685259, 0.00649689718951049, 0.007201073559648247, 0.0064787932774239545, 0.006358675697278377, 0.005800846363132381, 0.006766033253178325, 0.006418657225853161, 0.006369554720887611, 0.00638528313296272, 0.0061669873983036866, 0.006391117008510573, 0.00621960660809588, 0.006165182280072773, 0.006725121968766151, 0.006528298115830289, 0.006527603101210498, 0.006093091560378543, 0.006097749803288216, 0.006711398018885164, 0.00625546430117176, 0.006599116098658232, 0.006127690059813991, 0.005955183169213729, 0.006512842793143488, 0.0065438344077898844, 0.006140000839211473, 0.006176300244944123, 0.006134440885947681, 0.006094675505347889, 0.006088294937627407, 0.006425349597446709, 0.0059927819140193645, 0.006428339851088436, 0.0062486192557588294, 0.0059571322299680676], "accuracy_test_std": 0.0043418002911413175, "error_valid": [0.4577813205948795, 0.3503153237951807, 0.29926787227033136, 0.2509765625, 0.22882006541792166, 0.21244205336972888, 0.20537227033132532, 0.19307258330195776, 0.19052969691265065, 0.18648078642695776, 0.17947130318147586, 0.1727368458207832, 0.1732045368975903, 0.16974538780120485, 0.1674157567771084, 0.16056952419051207, 0.16300063535391573, 0.1590635000941265, 0.1596944418298193, 0.15724274049322284, 0.15823989316641573, 0.15465867375753017, 0.15380418157003017, 0.1555234610316265, 0.15354974585843373, 0.15218638224774095, 0.15342767554593373, 0.14962290568524095, 0.14901255412274095, 0.14902284920933728, 0.1499082266566265, 0.1471712043486446, 0.1482698371611446, 0.14781244117093373, 0.14679469832454817, 0.14545192488704817, 0.14703883894954817, 0.14596079631024095, 0.14557399519954817, 0.14557399519954817, 0.14080295792545183, 0.1423795769013554, 0.1430208137236446, 0.1410368034638554, 0.14374294051204817, 0.1422575065888554, 0.1430208137236446, 0.1437532355986446, 0.1441194465361446, 0.1449739387236446, 0.14397678605045183, 0.1422883918486446, 0.14166774519954817, 0.1419221809111446, 0.1400602409638554, 0.13869687735316272, 0.14104709855045183, 0.1395719597138554, 0.1410368034638554, 0.14078236775225905, 0.14153537980045183, 0.13821889118975905, 0.13946018448795183, 0.1404264519013554, 0.1398161003388554, 0.13869687735316272, 0.13921604386295183, 0.13750705948795183, 0.1415250847138554, 0.13821889118975905, 0.13812770613704817, 0.1399690559111446, 0.13959254988704817, 0.14095591349774095, 0.13875864787274095, 0.1399690559111446, 0.13851450724774095, 0.13898219832454817, 0.13863657756024095, 0.1393587043486446, 0.1382600715361446, 0.13738498917545183, 0.13823948136295183, 0.13949106974774095, 0.13849391707454817, 0.13665256730045183, 0.13589955525225905, 0.13717173381024095, 0.13702907332454817, 0.13840273202183728, 0.13851450724774095, 0.1365510871611446, 0.13823948136295183, 0.13812770613704817, 0.1373746940888554, 0.1386262824736446, 0.1372835090361446, 0.13701877823795183, 0.13924692912274095, 0.13898219832454817, 0.13741587443524095, 0.1358186652861446, 0.13715114363704817, 0.13482151261295183, 0.13704966349774095, 0.13702907332454817, 0.13840273202183728, 0.13604221573795183, 0.13886012801204817, 0.13767031014683728, 0.1364290168486446, 0.1382600715361446, 0.1359098503388554, 0.13873805769954817, 0.1344450065888554, 0.13482151261295183, 0.1363981315888554, 0.13553334431475905, 0.13544215926204817, 0.13576718985316272, 0.13405820547816272, 0.13666286238704817, 0.13565541462725905, 0.13455678181475905, 0.13418027579066272, 0.1348112175263554, 0.13799534073795183, 0.13528920368975905, 0.13616428605045183, 0.13331548851656627, 0.13605251082454817, 0.1360319206513554, 0.13702907332454817, 0.13419057087725905, 0.13311252823795183, 0.1350553581513554, 0.13434352644954817, 0.1342008659638554, 0.13516713337725905, 0.13640842667545183, 0.13210508047816272, 0.13296986775225905, 0.1344450065888554, 0.13040639118975905, 0.13494358292545183, 0.13296986775225905, 0.13431264118975905, 0.13506565323795183, 0.1340787956513554, 0.13347873917545183, 0.13482151261295183, 0.13335666886295183, 0.13294927757906627, 0.1346891472138554, 0.13384495011295183, 0.1344450065888554, 0.13567600480045183, 0.1322477409638554, 0.13715114363704817, 0.13358021931475905, 0.1343229362763554, 0.13296986775225905, 0.13457737198795183, 0.13272572712725905, 0.13248158650225905, 0.13422145613704817, 0.13234922110316272, 0.1331022331513554, 0.13532008894954817, 0.13418027579066272, 0.1333463737763554, 0.13274631730045183, 0.1329801628388554, 0.13422145613704817, 0.1329801628388554, 0.13384495011295183, 0.13347873917545183, 0.13506565323795183, 0.1345670769013554, 0.13335666886295183, 0.13248158650225905, 0.13287868269954817, 0.13238010636295183, 0.1331022331513554, 0.1332243034638554, 0.13250217667545183, 0.1309049675263554, 0.13161679922816272, 0.13213596573795183, 0.13384495011295183, 0.1335905144013554, 0.13384495011295183, 0.13421116105045183, 0.1331022331513554], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.027841537930468563, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 16, "valid_ratio": 0.15, "learning_rate": 0.0015114495566272955, "optimization": "adam", "nb_data_augmentation": 1, "learning_rate_decay_method": "lin", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 1.8865141879201206e-07, "rotation_range": [0, 0], "momentum": 0.5531773054877485}, "accuracy_valid_max": 0.869593608810241, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8668977668486446, "accuracy_valid_std": [0.018192528725947642, 0.014672288743653387, 0.015244150330364836, 0.009838594570122015, 0.012238389303661534, 0.009952291443531054, 0.015328861213456798, 0.014390398831075889, 0.014119869341790315, 0.01707213820534418, 0.012736866381702629, 0.012245335853959668, 0.012417774980116028, 0.013961678100941644, 0.013184487574990742, 0.011212576867600507, 0.01390104041347917, 0.013440793087608375, 0.011058900229307674, 0.012869544930447466, 0.013251539930824308, 0.013640637316251893, 0.01360590580930513, 0.01351585826727998, 0.011996164067437599, 0.012781605274951828, 0.0151309381277209, 0.013669908932491951, 0.012175558147236344, 0.012617574077012557, 0.011301743466056164, 0.01358826755452528, 0.013323442905092738, 0.010798341430982003, 0.013321736435113182, 0.014573297675249472, 0.012913834227701832, 0.01105868311960636, 0.013735250262968934, 0.013778577309791313, 0.011972662337533201, 0.01383063895710154, 0.012910429556630288, 0.012816941924436397, 0.012494079089040846, 0.01337701918664524, 0.011535257764063733, 0.011049981824746312, 0.012481875437119412, 0.010784829547714585, 0.013329658335914398, 0.012415918598773827, 0.012737066094204422, 0.011001189148278606, 0.010790779109869652, 0.013766568965409492, 0.010925030314620061, 0.01222728142127614, 0.013531779928871102, 0.014256626666700132, 0.012628350652676593, 0.012777861433562465, 0.011017999373862017, 0.012863788114148195, 0.011870176078376803, 0.012137476367779225, 0.011032752254075696, 0.008857813408123586, 0.01230748074794037, 0.011143298247945568, 0.009842916355340596, 0.00813942333681381, 0.009568861389944219, 0.007894403543278722, 0.009059494630333748, 0.010052826707185268, 0.00883199252911324, 0.01079416211480337, 0.009363950191458885, 0.008177108876014413, 0.009331410042779728, 0.010768051040574458, 0.010852541727592311, 0.009734594644023492, 0.010380610766059293, 0.009985044675137639, 0.011889863595866202, 0.007998544454400697, 0.009896332360109126, 0.007710077000780637, 0.008346223203686649, 0.008401649057638442, 0.010797479754721678, 0.011568832358495545, 0.010590365552162207, 0.008306846302876, 0.008312928007076401, 0.009986725672621447, 0.0090152342224817, 0.008946278173526537, 0.008668334155997307, 0.008254497875063197, 0.008650973217323076, 0.008295002128045032, 0.008805853377022338, 0.008813161462160457, 0.009047465600570882, 0.009602835551179285, 0.010118739628695207, 0.008378371173536455, 0.008362764254031183, 0.008993126838305446, 0.010802784738036753, 0.007949848706607286, 0.010945956431015346, 0.010144008927233604, 0.011438598672974927, 0.01261396803566587, 0.008581085976208264, 0.011425283875040071, 0.012222848767422637, 0.009589103063119638, 0.012742328137638318, 0.01183670746401653, 0.010458599815256178, 0.010824641611486436, 0.009776114684342816, 0.009859016115271176, 0.010088719087866458, 0.01206257798689033, 0.008806966530200041, 0.010239267745272175, 0.0100517080991274, 0.010494639523228051, 0.008459378354477045, 0.011199805786938606, 0.009649210515383397, 0.011215912296509133, 0.011468486845683329, 0.0111108530121954, 0.011900751418417878, 0.011100335430678307, 0.009348210279613618, 0.010032165361956705, 0.010528978768985514, 0.01160437583286784, 0.011990027606223882, 0.010033357184192834, 0.01135176451815326, 0.009520416936544, 0.010913907663834311, 0.011019205095998762, 0.013054363254042122, 0.010741552797456606, 0.01013763552465465, 0.012434752701140235, 0.012183909996190047, 0.010385324859499674, 0.010206116932327071, 0.01110175052793581, 0.010968902930419385, 0.010784401136809908, 0.00890309845017321, 0.010906105965364146, 0.011107255561980089, 0.01050117412435086, 0.011588110494612437, 0.009001637138833535, 0.010195354585486585, 0.01248494825862251, 0.010720179666435865, 0.011325684591876327, 0.009809428394337648, 0.011140149710745662, 0.011321350406344953, 0.01293766164989885, 0.012169519966256287, 0.01089899391674813, 0.012131622493092367, 0.010844730687391923, 0.012529491042035905, 0.010667949943663145, 0.011301810452240482, 0.010379419638814212, 0.011440827186316362, 0.011353169900777949, 0.011513011253042094, 0.013932135087122478, 0.011033500770361571, 0.011231058488971246, 0.012086075430942617, 0.01137868774903833, 0.011239184084724895, 0.012637547320692525], "accuracy_valid": [0.5422186794051205, 0.6496846762048193, 0.7007321277296686, 0.7490234375, 0.7711799345820783, 0.7875579466302711, 0.7946277296686747, 0.8069274166980422, 0.8094703030873494, 0.8135192135730422, 0.8205286968185241, 0.8272631541792168, 0.8267954631024097, 0.8302546121987951, 0.8325842432228916, 0.8394304758094879, 0.8369993646460843, 0.8409364999058735, 0.8403055581701807, 0.8427572595067772, 0.8417601068335843, 0.8453413262424698, 0.8461958184299698, 0.8444765389683735, 0.8464502541415663, 0.847813617752259, 0.8465723244540663, 0.850377094314759, 0.850987445877259, 0.8509771507906627, 0.8500917733433735, 0.8528287956513554, 0.8517301628388554, 0.8521875588290663, 0.8532053016754518, 0.8545480751129518, 0.8529611610504518, 0.854039203689759, 0.8544260048004518, 0.8544260048004518, 0.8591970420745482, 0.8576204230986446, 0.8569791862763554, 0.8589631965361446, 0.8562570594879518, 0.8577424934111446, 0.8569791862763554, 0.8562467644013554, 0.8558805534638554, 0.8550260612763554, 0.8560232139495482, 0.8577116081513554, 0.8583322548004518, 0.8580778190888554, 0.8599397590361446, 0.8613031226468373, 0.8589529014495482, 0.8604280402861446, 0.8589631965361446, 0.859217632247741, 0.8584646201995482, 0.861781108810241, 0.8605398155120482, 0.8595735480986446, 0.8601838996611446, 0.8613031226468373, 0.8607839561370482, 0.8624929405120482, 0.8584749152861446, 0.861781108810241, 0.8618722938629518, 0.8600309440888554, 0.8604074501129518, 0.859044086502259, 0.861241352127259, 0.8600309440888554, 0.861485492752259, 0.8610178016754518, 0.861363422439759, 0.8606412956513554, 0.8617399284638554, 0.8626150108245482, 0.8617605186370482, 0.860508930252259, 0.8615060829254518, 0.8633474326995482, 0.864100444747741, 0.862828266189759, 0.8629709266754518, 0.8615972679781627, 0.861485492752259, 0.8634489128388554, 0.8617605186370482, 0.8618722938629518, 0.8626253059111446, 0.8613737175263554, 0.8627164909638554, 0.8629812217620482, 0.860753070877259, 0.8610178016754518, 0.862584125564759, 0.8641813347138554, 0.8628488563629518, 0.8651784873870482, 0.862950336502259, 0.8629709266754518, 0.8615972679781627, 0.8639577842620482, 0.8611398719879518, 0.8623296898531627, 0.8635709831513554, 0.8617399284638554, 0.8640901496611446, 0.8612619423004518, 0.8655549934111446, 0.8651784873870482, 0.8636018684111446, 0.864466655685241, 0.8645578407379518, 0.8642328101468373, 0.8659417945218373, 0.8633371376129518, 0.864344585372741, 0.865443218185241, 0.8658197242093373, 0.8651887824736446, 0.8620046592620482, 0.864710796310241, 0.8638357139495482, 0.8666845114834337, 0.8639474891754518, 0.8639680793486446, 0.8629709266754518, 0.865809429122741, 0.8668874717620482, 0.8649446418486446, 0.8656564735504518, 0.8657991340361446, 0.864832866622741, 0.8635915733245482, 0.8678949195218373, 0.867030132247741, 0.8655549934111446, 0.869593608810241, 0.8650564170745482, 0.867030132247741, 0.865687358810241, 0.8649343467620482, 0.8659212043486446, 0.8665212608245482, 0.8651784873870482, 0.8666433311370482, 0.8670507224209337, 0.8653108527861446, 0.8661550498870482, 0.8655549934111446, 0.8643239951995482, 0.8677522590361446, 0.8628488563629518, 0.866419780685241, 0.8656770637236446, 0.867030132247741, 0.8654226280120482, 0.867274272872741, 0.867518413497741, 0.8657785438629518, 0.8676507788968373, 0.8668977668486446, 0.8646799110504518, 0.8658197242093373, 0.8666536262236446, 0.8672536826995482, 0.8670198371611446, 0.8657785438629518, 0.8670198371611446, 0.8661550498870482, 0.8665212608245482, 0.8649343467620482, 0.8654329230986446, 0.8666433311370482, 0.867518413497741, 0.8671213173004518, 0.8676198936370482, 0.8668977668486446, 0.8667756965361446, 0.8674978233245482, 0.8690950324736446, 0.8683832007718373, 0.8678640342620482, 0.8661550498870482, 0.8664094855986446, 0.8661550498870482, 0.8657888389495482, 0.8668977668486446], "seed": 813422842, "model": "residualv3", "loss_std": [0.3114743232727051, 0.2634764015674591, 0.2564287483692169, 0.25125688314437866, 0.24680817127227783, 0.24065998196601868, 0.23628006875514984, 0.23126281797885895, 0.22713324427604675, 0.22108328342437744, 0.2176007330417633, 0.21651148796081543, 0.21349672973155975, 0.21078135073184967, 0.20832590758800507, 0.20403330028057098, 0.2012045830488205, 0.19876821339130402, 0.19925501942634583, 0.19630365073680878, 0.19441507756710052, 0.19033965468406677, 0.19008250534534454, 0.18788011372089386, 0.18574810028076172, 0.1853458434343338, 0.18340155482292175, 0.18035531044006348, 0.18106745183467865, 0.17814156413078308, 0.17628036439418793, 0.1759234368801117, 0.17309555411338806, 0.16975228488445282, 0.17179682850837708, 0.17105819284915924, 0.17135825753211975, 0.1678776890039444, 0.16750261187553406, 0.16542735695838928, 0.16461209952831268, 0.16257384419441223, 0.15927481651306152, 0.16142019629478455, 0.16146235167980194, 0.1599927693605423, 0.15872345864772797, 0.15564753115177155, 0.15607863664627075, 0.15356698632240295, 0.1533273309469223, 0.1532181054353714, 0.15286840498447418, 0.14871466159820557, 0.15025442838668823, 0.15047718584537506, 0.14846011996269226, 0.14843276143074036, 0.14617906510829926, 0.14855767786502838, 0.14702852070331573, 0.1462765336036682, 0.14476148784160614, 0.14422987401485443, 0.14188194274902344, 0.14329794049263, 0.14195391535758972, 0.14158815145492554, 0.1386934220790863, 0.1387190967798233, 0.14070568978786469, 0.1380356252193451, 0.13849413394927979, 0.13926467299461365, 0.13821548223495483, 0.13986147940158844, 0.13597451150417328, 0.13377247750759125, 0.13342230021953583, 0.13544033467769623, 0.13330964744091034, 0.1328432857990265, 0.1315726935863495, 0.13250337541103363, 0.1309986114501953, 0.13101215660572052, 0.1304473727941513, 0.13518951833248138, 0.13015685975551605, 0.13031215965747833, 0.12529662251472473, 0.1277673840522766, 0.12800727784633636, 0.12494684755802155, 0.12933868169784546, 0.12813258171081543, 0.1298137605190277, 0.1258489191532135, 0.12551288306713104, 0.12378111481666565, 0.12442724406719208, 0.12314856797456741, 0.12385478615760803, 0.12138456851243973, 0.12023606151342392, 0.12329498678445816, 0.1235373467206955, 0.12147899717092514, 0.12222360819578171, 0.12249037623405457, 0.12112785130739212, 0.1232263371348381, 0.1198238953948021, 0.12176785618066788, 0.11840556561946869, 0.11842384934425354, 0.12034526467323303, 0.1169973611831665, 0.11935340613126755, 0.11835798621177673, 0.11865898966789246, 0.11823388189077377, 0.11776155978441238, 0.11801381409168243, 0.11843878030776978, 0.11807926744222641, 0.1147865578532219, 0.11524342745542526, 0.11513695865869522, 0.11360770463943481, 0.11148471385240555, 0.11575747281312943, 0.11575962603092194, 0.11334554851055145, 0.11237622797489166, 0.11626170575618744, 0.11235099285840988, 0.11448296159505844, 0.11232617497444153, 0.11231648176908493, 0.11481388658285141, 0.1115068718791008, 0.11107289046049118, 0.1130368635058403, 0.11160096526145935, 0.11425482481718063, 0.111873559653759, 0.1105678603053093, 0.11275044083595276, 0.10741996020078659, 0.10973198711872101, 0.11039204150438309, 0.11095549911260605, 0.10744654387235641, 0.10965349525213242, 0.10848996788263321, 0.11037234961986542, 0.10791108757257462, 0.10796771943569183, 0.1087588518857956, 0.10938804596662521, 0.10735030472278595, 0.10889807343482971, 0.10748166590929031, 0.10840880870819092, 0.10826104879379272, 0.10864105075597763, 0.10465720295906067, 0.10686859488487244, 0.1070156916975975, 0.10929793119430542, 0.10729704052209854, 0.10858294367790222, 0.10619914531707764, 0.1018221452832222, 0.10588371008634567, 0.10197951644659042, 0.1046951562166214, 0.10577049106359482, 0.10626546293497086, 0.10477735102176666, 0.104164257645607, 0.10246661305427551, 0.10407127439975739, 0.10503450781106949, 0.10236240178346634, 0.10321778804063797, 0.10357237607240677, 0.10283385217189789, 0.1046542301774025, 0.10668933391571045, 0.10196292400360107, 0.10503967851400375, 0.10188642889261246]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:29 2016", "state": "available"}], "summary": "9744bcd67306ef5dda248b5656018eaa"}