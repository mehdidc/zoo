{"content": {"hp_model": {"f0": 16, "f1": 16, "f2": 16, "f3": 16, "nonlin": "rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.5648466348648071, 1.209876298904419, 1.0517098903656006, 0.9430813193321228, 0.8668293952941895, 0.8119905591011047, 0.7689797282218933, 0.7330282330513, 0.7008009552955627, 0.6756598949432373, 0.6511321663856506, 0.6298821568489075, 0.610733687877655, 0.5959106087684631, 0.5828890800476074, 0.5682650804519653, 0.5566255450248718, 0.54671311378479, 0.5369634628295898, 0.528751015663147, 0.5190040469169617, 0.5132092237472534, 0.5067493319511414, 0.5013799667358398, 0.4959506690502167, 0.4908333718776703, 0.486167311668396, 0.48350635170936584, 0.4778115153312683, 0.4759942889213562, 0.4729304313659668, 0.469386488199234, 0.4667704999446869, 0.46375972032546997, 0.46217212080955505, 0.4605874717235565, 0.4577879309654236, 0.4572999179363251, 0.45586779713630676, 0.4545380175113678, 0.45350900292396545, 0.452195405960083, 0.4507102370262146, 0.4496070146560669, 0.4510476589202881, 0.4489430785179138, 0.4473797380924225, 0.4464648962020874, 0.4457952678203583, 0.4472118616104126, 0.4466707110404968, 0.44545474648475647, 0.44506826996803284, 0.4436661899089813, 0.44387295842170715, 0.44437751173973083, 0.44251421093940735, 0.4429153501987457, 0.44349223375320435, 0.4435947835445404, 0.44208428263664246, 0.44449830055236816, 0.4412553608417511, 0.4424217641353607, 0.44335639476776123, 0.44177761673927307, 0.4423823654651642, 0.4424227476119995, 0.44244110584259033, 0.4418918192386627, 0.4407784938812256, 0.4420343041419983, 0.4418197274208069, 0.4412917494773865, 0.44145798683166504, 0.4403759241104126, 0.44260627031326294, 0.4391915202140808, 0.4415896534919739, 0.4406164884567261, 0.43922102451324463, 0.44108298420906067, 0.4409024715423584, 0.4424152374267578, 0.44234055280685425, 0.4412829279899597, 0.43989795446395874, 0.4415789842605591, 0.4404146671295166, 0.4419260621070862, 0.4407503604888916, 0.4395310580730438, 0.43953436613082886, 0.44167861342430115, 0.44095757603645325, 0.44065821170806885, 0.43976977467536926, 0.44179001450538635, 0.4415224492549896, 0.4406529664993286, 0.4400099515914917, 0.4415808320045471, 0.44024452567100525, 0.44150400161743164, 0.440919429063797, 0.4401353895664215, 0.4409349262714386, 0.439949631690979, 0.43952932953834534, 0.4405757486820221, 0.4400862753391266, 0.44061174988746643, 0.4414743483066559, 0.44042637944221497, 0.4419325292110443, 0.4405631124973297, 0.44006603956222534, 0.4396912157535553, 0.43969210982322693, 0.44069012999534607, 0.4402386546134949, 0.43982452154159546, 0.4403517544269562, 0.44124674797058105, 0.44223883748054504, 0.4409036338329315, 0.4423101246356964, 0.44027701020240784, 0.4391671419143677, 0.43986761569976807, 0.43982434272766113, 0.4412163197994232, 0.4399931728839874, 0.4402869939804077, 0.44082340598106384, 0.4391126036643982, 0.442457914352417, 0.4400774836540222, 0.44136443734169006, 0.4411664307117462, 0.4403671622276306, 0.441388338804245, 0.44130751490592957, 0.44022026658058167, 0.44120198488235474, 0.44079363346099854, 0.4416497051715851, 0.43811094760894775, 0.43991678953170776, 0.43994516134262085, 0.4412829279899597, 0.44034114480018616, 0.4413527846336365, 0.4411516487598419, 0.4394783079624176, 0.43993985652923584, 0.4374161958694458, 0.4397647976875305, 0.4399096667766571, 0.44019171595573425, 0.4412691593170166, 0.4406152069568634, 0.44165971875190735, 0.43979355692863464, 0.43897873163223267, 0.44009146094322205, 0.439441442489624, 0.43925878405570984, 0.4411375820636749, 0.4412292242050171, 0.4406169652938843, 0.4399040937423706, 0.4413214325904846, 0.4400840997695923, 0.4419710636138916, 0.4406224191188812, 0.4410768449306488, 0.44115149974823, 0.4409537613391876, 0.4404939115047455, 0.44135817885398865, 0.44007956981658936, 0.4401284456253052, 0.43957147002220154, 0.4410123825073242, 0.4419116675853729, 0.4420652687549591, 0.44218695163726807, 0.4415660500526428, 0.44015365839004517, 0.44175100326538086, 0.4403105080127716, 0.43977922201156616, 0.4406362473964691, 0.4389481246471405, 0.4389786124229431, 0.43911856412887573, 0.4407229721546173, 0.4400796890258789, 0.44149547815322876, 0.4408232867717743], "moving_avg_accuracy_train": [0.055994215606542996, 0.11613884526981816, 0.17574154709273482, 0.23322647639739635, 0.28722295741444887, 0.337391338583303, 0.383770019474419, 0.4261572596942898, 0.4658098227325666, 0.5021713865729589, 0.5350919983828557, 0.5652947165724402, 0.5934095476156853, 0.6190268266927104, 0.6426053921489008, 0.6643936176130897, 0.6844795678903558, 0.7025452973958477, 0.7192136440924478, 0.7344010959265124, 0.7484022267592949, 0.7612497463314274, 0.7729567452701747, 0.7835395472912376, 0.7934570913566413, 0.8026408283380868, 0.8109597060928255, 0.8184282390768892, 0.8253776031017861, 0.831676244600393, 0.8374822057289011, 0.842767880418331, 0.8476482565745415, 0.8520244272199204, 0.8560629261507523, 0.8597835696456346, 0.8631926747577138, 0.8662677366585574, 0.8691654286050124, 0.8717316068247069, 0.8740365169248127, 0.8761666314398804, 0.8781558141165365, 0.8799529458254993, 0.8815309449802602, 0.8829442048219351, 0.8842348119675563, 0.8855126158390916, 0.8866021854544257, 0.8875990381010743, 0.8884614724461902, 0.8892863833353383, 0.8900195025403335, 0.8907025613129244, 0.8913498302427707, 0.8918417996225173, 0.8922938726595273, 0.8927076777904461, 0.8931149435915972, 0.8934931446054994, 0.893866041552526, 0.8941575070262878, 0.8944221511014829, 0.8946138998906056, 0.8948328686305502, 0.895064853777462, 0.895241124375168, 0.8953718300785704, 0.8954778394675849, 0.8956522669284029, 0.8957627847157675, 0.8958505889315292, 0.8959366602697809, 0.8960350147646744, 0.8961305813541446, 0.8961630047155924, 0.8962015223849523, 0.8962919918588048, 0.8962757941840909, 0.8962681917232769, 0.8963031300894782, 0.8963555370071639, 0.8963887162914049, 0.8964603942769747, 0.8965458668520919, 0.8965856618863826, 0.8966586437493778, 0.8966987868379874, 0.8966883405439081, 0.8966696743328174, 0.8966389238499787, 0.8966810028797095, 0.8967188379576485, 0.896752961625431, 0.8967883232240544, 0.8967526472497017, 0.8967717642442314, 0.8967610317047752, 0.8967907918025702, 0.8968177200858606, 0.8967350347444025, 0.8967628163382532, 0.8967390637453562, 0.8966968321701007, 0.8966981710380388, 0.8967320002001541, 0.8967136543698765, 0.8966878785762075, 0.8967320015309251, 0.8967345819068562, 0.8967925996701664, 0.8967797114904791, 0.896775231770464, 0.8967455873367269, 0.8967770000177829, 0.8967959347866764, 0.8968129760786806, 0.8968050617533891, 0.8968351051927606, 0.8969016357691378, 0.8968290519033721, 0.8968172408956208, 0.8968019967398442, 0.8967859158020169, 0.8968016698924963, 0.896790271937023, 0.8967916034723259, 0.8967486600755363, 0.8967565139946162, 0.8968077603491689, 0.8967934281992187, 0.8968037807523589, 0.8967805820156705, 0.8967666425502606, 0.8967332067409248, 0.896765857481561, 0.8968022185945622, 0.8968139812081588, 0.896808291518729, 0.8967521617197076, 0.8967433894327037, 0.896779672201781, 0.8968193021403792, 0.8968223809529655, 0.8967880576497971, 0.8967687563721743, 0.8967909127520758, 0.8967109081439963, 0.8966946354705159, 0.8967172284941546, 0.8967584525058964, 0.8967862535212259, 0.8968182138326323, 0.8968399305688319, 0.8968503192314486, 0.896848043283756, 0.8968158040451275, 0.8967913669303434, 0.8967809992710855, 0.896799642261105, 0.8968163488544851, 0.8967988687540127, 0.8967854618123969, 0.8967501440768475, 0.896769439291025, 0.8967891661814131, 0.8967604174065719, 0.896774034990158, 0.8967886159641949, 0.8968087142872567, 0.8968268748756499, 0.896845436407557, 0.8969109699112734, 0.8968932922515414, 0.8968890081018301, 0.8968363602909089, 0.8968586956765466, 0.896876472374811, 0.8968901102056208, 0.8968536282259871, 0.8968393956347931, 0.896898593818176, 0.8968890931653636, 0.8969176728611472, 0.8968342567885799, 0.8968149498458791, 0.8968162108367433, 0.8968033948356641, 0.8967267923168448, 0.8967717102439366, 0.8967563688557094, 0.8967099734741529, 0.8967473087390946, 0.8967320823525421, 0.8967648455320166, 0.8967757312030674, 0.8967622047212803, 0.8967826190198241, 0.896770873100446, 0.8967835172122822, 0.8967878493688687], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.055430364034262034, 0.11400336208113701, 0.17185836755224015, 0.22776617490469686, 0.2800903021902061, 0.32871510221403183, 0.3734235406936829, 0.4138676253479592, 0.45171493778492533, 0.4860318076171256, 0.5175405785685154, 0.5455769715776277, 0.5719267421815366, 0.5958836173327353, 0.6177123301476546, 0.6379451386897416, 0.6562980622266409, 0.673032478519037, 0.6886560061283532, 0.7024089166695089, 0.7152901129637779, 0.7267733263473699, 0.7374733998214431, 0.7472967194307897, 0.7560543168777709, 0.764058224892554, 0.7713604278645185, 0.7778215177493769, 0.7839060828419091, 0.7893445408227785, 0.7944110809517205, 0.7987013828716087, 0.8026206012297792, 0.8064073339343615, 0.8098153933684855, 0.8127382215015165, 0.8156230554601751, 0.8181695483893082, 0.8204105048832087, 0.8224294247450383, 0.8243085172855947, 0.8261461849470955, 0.8277380211775365, 0.8292093538960027, 0.8304969322488723, 0.831631338703955, 0.8326878960986197, 0.833809696191318, 0.8345731166324272, 0.8353710878193351, 0.8360658773337117, 0.8368264947490605, 0.8374744293291243, 0.8380565409425221, 0.8385560273320801, 0.8388957018014324, 0.8392746510113493, 0.8396157053002746, 0.8400325174415575, 0.8403842638148716, 0.8407374566446043, 0.8408589881827041, 0.8410538157857439, 0.8411315043784797, 0.8412634887768516, 0.8414544874142268, 0.8415409369691143, 0.8416065345372631, 0.8417042524596663, 0.8417789620499195, 0.8418828217748975, 0.8419162898797872, 0.8419453816655283, 0.8420325994289454, 0.8421965446347707, 0.8422098179762635, 0.8421474922874473, 0.8421900849261725, 0.8421419395736155, 0.8421840579750642, 0.8421863729512777, 0.8422616986173698, 0.842181977833193, 0.8422699500423436, 0.8423491250305791, 0.8422596320964217, 0.8424008740354995, 0.8423815074056694, 0.842487177259982, 0.842543600017794, 0.8425221678209844, 0.8425262633976963, 0.8426174576528063, 0.8427595381299956, 0.8427765177695562, 0.8426941431951609, 0.8425833849844551, 0.8425091461659795, 0.8425033663856014, 0.8425947913246016, 0.8425183823634517, 0.842622571753236, 0.8426044199054726, 0.8426124973049856, 0.8426706541068665, 0.8427097586886497, 0.8427327457810045, 0.842729020101624, 0.8427999386863412, 0.842948185122677, 0.8430216012677888, 0.8430011970709799, 0.8430550459726921, 0.8430302677967332, 0.8430578250720298, 0.8429605563072966, 0.8430439128565368, 0.8429358282821029, 0.8428385521651125, 0.8427855657362519, 0.8428508297575061, 0.8428342661718158, 0.8428936306408541, 0.8429348516317385, 0.8428498802110346, 0.8427967904862413, 0.8429087306488371, 0.8428874064826732, 0.8429170428581259, 0.8430169577835331, 0.84299598842649, 0.8431246298888109, 0.8430684792587401, 0.8429945591378359, 0.8429036169665222, 0.8428807451512705, 0.842912077168523, 0.8429036548903002, 0.84300490861249, 0.8429983807124608, 0.8429925056024345, 0.843036046128411, 0.8430009609056301, 0.842945999651287, 0.8427978487637185, 0.8427998198173166, 0.8427405586093049, 0.8426861940134346, 0.8427125670819706, 0.8427841014599935, 0.8429471681588736, 0.8428111074517965, 0.8428707287755175, 0.8430251327428452, 0.8428558320062113, 0.84282759067306, 0.8427767299020642, 0.8428174339355776, 0.8428540675657397, 0.8429226294179759, 0.8428246141700789, 0.8427740510493812, 0.8427906089056629, 0.8427190322489069, 0.8427166779227361, 0.8426769084267727, 0.8427153875765653, 0.8427500188113787, 0.8428554586188702, 0.8428872602720435, 0.8428914676973993, 0.8429817331076292, 0.8430497354369265, 0.8429156250332941, 0.8429912676786846, 0.8429260982244456, 0.842915244331971, 0.8429909250474937, 0.8430844812626239, 0.8430699960975814, 0.8429348891365431, 0.8429455111980394, 0.842945952548115, 0.8429575272857733, 0.8428580812684159, 0.8427797573753845, 0.8428811938178158, 0.8428249727323444, 0.8427520187102394, 0.8428074008941854, 0.842795180194827, 0.8426997618553143, 0.8426860980285931, 0.8427969004057037, 0.8427623452013532, 0.8428777298924377, 0.8428839198644138, 0.8429016978704423, 0.8427335630984584, 0.8427053416248325, 0.8426321436822287], "moving_var_accuracy_train": [0.02821816963252822, 0.05795274096526808, 0.08412980545006489, 0.10545747877951582, 0.12115231056158847, 0.13168887772736054, 0.1378788283254242, 0.14026104869399506, 0.14038587562413618, 0.13824675798599287, 0.13417598232483482, 0.12896822176670647, 0.1231853931113478, 0.11677305868600468, 0.1100992915581509, 0.1033619033222408, 0.09665672157688399, 0.089928384662286, 0.08343605023043996, 0.07716837344630424, 0.07121582108304406, 0.0655797678071466, 0.060255275443798405, 0.0552377091869717, 0.05059915739087755, 0.04629831087628913, 0.0422913133325448, 0.03856419286369675, 0.03514241652048186, 0.03198523083098543, 0.029090090409518612, 0.026432526580869073, 0.024003636565617135, 0.021775630734712715, 0.019744852923770415, 0.017894956323537464, 0.01621005867017056, 0.014674156854399685, 0.013282310736508672, 0.01201334709875491, 0.010859825884005551, 0.009814679786230712, 0.00886882343709762, 0.008011008234802094, 0.00723231814331372, 0.006527062059403169, 0.005889346854701808, 0.005315107213838622, 0.004794280949974693, 0.004323796291769396, 0.0038981107995891763, 0.003514424021405574, 0.003167818793183611, 0.00285523603744657, 0.002573483047309813, 0.002318313047414306, 0.0020883210729499982, 0.0018810300778323709, 0.0016944198589442187, 0.0015262651971120473, 0.0013748901465987589, 0.0012381657010404397, 0.0011149794593152188, 0.001003812421766867, 0.0009038627053718367, 0.0008139607888101422, 0.0007328443518416686, 0.0006597136724856189, 0.00059384344715209, 0.0005347329268886681, 0.000481369561831717, 0.0004333019918712951, 0.00039003846716158144, 0.000351121682905415, 0.00031609171137208017, 0.0002844920017041804, 0.0002560561540314386, 0.00023052420115958705, 0.00020747414232562362, 0.00018672724826975513, 0.00016806550964767496, 0.00015128367704809922, 0.0001361652171274141, 0.0001225949350172107, 0.00011040119156536416, 9.937532521161547e-05, 8.948572986139016e-05, 8.055166008331924e-05, 7.249747620052725e-05, 6.525086442740287e-05, 5.8734288314415945e-05, 5.2876795285662134e-05, 4.760199919519975e-05, 4.2852279098006204e-05, 3.8578305172120385e-05, 3.473192963122245e-05, 3.126202580341882e-05, 2.8136859909705585e-05, 2.5331144889521884e-05, 2.2804556592538417e-05, 2.0585632724512754e-05, 1.8534015804673412e-05, 1.6685691895230017e-05, 1.5033174259244013e-05, 1.3529872966425816e-05, 1.2187185379668029e-05, 1.0971495967098361e-05, 9.880325894241965e-06, 8.909814821014996e-06, 8.01889326397301e-06, 7.247298485311496e-06, 6.524063583361233e-06, 5.8718378360478284e-06, 5.292563184507365e-06, 4.772187674836755e-06, 4.298195636610547e-06, 3.870989723648042e-06, 3.4844544801866106e-06, 3.1441325064112638e-06, 2.8695561141080275e-06, 2.6300162608227506e-06, 2.3682701338774074e-06, 2.1335345790577238e-06, 1.9225084902045617e-06, 1.732491363485595e-06, 1.5604114476377847e-06, 1.4043862597503713e-06, 1.2805448517258122e-06, 1.1530455229574411e-06, 1.0613766703562466e-06, 9.570876980203542e-07, 8.623435064269944e-07, 7.809527882397587e-07, 7.046062876789803e-07, 6.442072390245225e-07, 5.893811528989007e-07, 5.423422124571494e-07, 4.893532229190334e-07, 4.407092537193945e-07, 4.249933173909815e-07, 3.831865628254072e-07, 3.5671586053015517e-07, 3.35179062776811e-07, 3.0174646828160384e-07, 2.8217462371699023e-07, 2.5731001520612655e-07, 2.3599716021854127e-07, 2.7000408002226144e-07, 2.4538687113985544e-07, 2.2544218648012186e-07, 2.1819274012889656e-07, 2.0332953419617732e-07, 1.9218973432329838e-07, 1.772153105714644e-07, 1.604650983129752e-07, 1.4446520792277545e-07, 1.3937300369658309e-07, 1.3081025653765357e-07, 1.1869662611030772e-07, 1.0995501319109745e-07, 1.0147150423329534e-07, 9.407433902272912e-08, 8.628461987184787e-08, 8.888223988369842e-08, 8.334476350675075e-08, 7.851263899552649e-08, 7.809980358980358e-08, 7.19587704753376e-08, 6.667633666261164e-08, 6.364418630544426e-08, 6.024803041198128e-08, 5.732400157144121e-08, 9.024336239846138e-08, 8.403152304101659e-08, 7.579355618564378e-08, 9.316032852031579e-08, 8.833412073258564e-08, 8.234480767001002e-08, 7.578424076577391e-08, 8.018423023113742e-08, 7.398890707691796e-08, 9.812984061185705e-08, 8.912921818543256e-08, 8.756748746668091e-08, 1.414349091829682e-07, 1.3064624059271577e-07, 1.1759592741508269e-07, 1.0731458362655279e-07, 1.4939463826900695e-07, 1.5261375601013568e-07, 1.3947060414375656e-07, 1.448963265973767e-07, 1.4295199201201664e-07, 1.3074337843783124e-07, 1.2732987395753348e-07, 1.156633670698227e-07, 1.0574372174864454e-07, 9.892004183909918e-08, 9.026973725352595e-08, 8.268162560530266e-08, 7.458237127097993e-08], "duration": 151468.154659, "accuracy_train": [0.5599421560654301, 0.6574405122392949, 0.7121658634989848, 0.7505908401393503, 0.7731912865679217, 0.7889067691029901, 0.8011781474944629, 0.8076424216731267, 0.822682890077058, 0.8294254611364894, 0.8313775046719268, 0.8371191802787007, 0.8464430270048912, 0.8495823383859358, 0.8548124812546143, 0.86048764679079, 0.8652531203857512, 0.865136862945275, 0.8692287643618494, 0.8710881624330934, 0.8744124042543374, 0.8768774224806202, 0.8783197357189, 0.8787847654808048, 0.882714987945275, 0.8852944611710963, 0.8858296058854743, 0.8856450359334626, 0.8879218793258582, 0.8883640180878553, 0.8897358558854743, 0.8903389526232004, 0.8915716419804356, 0.8914099630283315, 0.8924094165282392, 0.8932693610995754, 0.8938746207664268, 0.8939432937661499, 0.8952446561231081, 0.8948272108019564, 0.8947807078257659, 0.895337662075489, 0.8960584582064415, 0.8961271312061646, 0.8957329373731081, 0.8956635433970099, 0.895850276278147, 0.8970128506829088, 0.8964083119924326, 0.8965707119209118, 0.8962233815522334, 0.8967105813376707, 0.8966175753852897, 0.8968500902662422, 0.8971752506113879, 0.8962695240402363, 0.8963625299926172, 0.8964319239687154, 0.8967803358019564, 0.8968969537306202, 0.8972221140757659, 0.896780696290144, 0.8968039477782392, 0.8963396389927095, 0.8968035872900517, 0.8971527200996677, 0.896827559754522, 0.8965481814091916, 0.8964319239687154, 0.8972221140757659, 0.8967574448020488, 0.896640826873385, 0.8967113023140458, 0.8969202052187154, 0.8969906806593761, 0.8964548149686231, 0.8965481814091916, 0.8971062171234773, 0.8961300151116648, 0.8961997695759505, 0.8966175753852897, 0.8968271992663345, 0.8966873298495754, 0.8971054961471022, 0.897315120028147, 0.8969438171949982, 0.8973154805163345, 0.8970600746354743, 0.8965943238971945, 0.8965016784330011, 0.8963621695044297, 0.8970597141472868, 0.8970593536590993, 0.8970600746354743, 0.8971065776116648, 0.8964315634805279, 0.8969438171949982, 0.8966644388496677, 0.8970586326827242, 0.8970600746354743, 0.8959908666712809, 0.8970128506829088, 0.8965252904092839, 0.8963167479928018, 0.8967102208494832, 0.8970364626591916, 0.8965485418973791, 0.8964558964331857, 0.897129108123385, 0.8967578052902363, 0.8973147595399593, 0.8966637178732927, 0.8967349142903286, 0.8964787874330934, 0.8970597141472868, 0.8969663477067183, 0.8969663477067183, 0.8967338328257659, 0.8971054961471022, 0.8975004109565338, 0.8961757971114802, 0.8967109418258582, 0.8966647993378553, 0.8966411873615725, 0.8969434567068106, 0.896687690337763, 0.8968035872900517, 0.8963621695044297, 0.8968271992663345, 0.897268977540144, 0.8966644388496677, 0.8968969537306202, 0.8965717933854743, 0.8966411873615725, 0.8964322844569029, 0.8970597141472868, 0.8971294686115725, 0.8969198447305279, 0.8967570843138611, 0.8962469935285161, 0.8966644388496677, 0.8971062171234773, 0.897175971587763, 0.8968500902662422, 0.8964791479212809, 0.8965950448735696, 0.8969903201711886, 0.8959908666712809, 0.8965481814091916, 0.8969205657069029, 0.8971294686115725, 0.8970364626591916, 0.8971058566352897, 0.897035381194629, 0.8969438171949982, 0.896827559754522, 0.8965256508974714, 0.8965714328972868, 0.896687690337763, 0.8969674291712809, 0.8969667081949059, 0.89664154784976, 0.8966647993378553, 0.8964322844569029, 0.8969430962186231, 0.8969667081949059, 0.8965016784330011, 0.8968965932424326, 0.8969198447305279, 0.8969895991948136, 0.8969903201711886, 0.8970124901947213, 0.8975007714447213, 0.8967341933139534, 0.8968504507544297, 0.8963625299926172, 0.8970597141472868, 0.8970364626591916, 0.8970128506829088, 0.8965252904092839, 0.8967113023140458, 0.8974313774686231, 0.8968035872900517, 0.8971748901232004, 0.8960835121354743, 0.8966411873615725, 0.896827559754522, 0.8966880508259505, 0.8960373696474714, 0.897175971587763, 0.8966182963616648, 0.896292415040144, 0.8970833261235696, 0.8965950448735696, 0.8970597141472868, 0.8968737022425249, 0.8966404663851975, 0.8969663477067183, 0.8966651598260429, 0.8968973142188077, 0.896826838778147], "end": "2016-02-03 07:20:55.139000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0], "moving_var_accuracy_valid": [0.027652727312737296, 0.05576461948325657, 0.0803129724574833, 0.10041282151836967, 0.11501186803224425, 0.12479002182523324, 0.1303006198843086, 0.13199207374757815, 0.13168463790113233, 0.12911500210674068, 0.12513872571787096, 0.11969920714273648, 0.11397808012637042, 0.1077456589168245, 0.10125952735354803, 0.09481787349169994, 0.08836755436369141, 0.0820511651251472, 0.07604290014726406, 0.07014089306771493, 0.06462013072268684, 0.059344895356936135, 0.05444082997239761, 0.04986522544848422, 0.04556896252102615, 0.04158862916050553, 0.037909665758648875, 0.03449441032528597, 0.03137816668404472, 0.028506541442527383, 0.025886915758178263, 0.023463884397434603, 0.02125573841054233, 0.019259218670671676, 0.017437830625563207, 0.01577093388166402, 0.014268740896218886, 0.012900228442740122, 0.011655402572534104, 0.01052654665195711, 0.00950567088574517, 0.008585496999077786, 0.00774975278243091, 0.006994260883903682, 0.00630975551764632, 0.005690361867929687, 0.005131372502890705, 0.004629561171633436, 0.004171850351399222, 0.003760396138395516, 0.0033887011167795552, 0.003055037854774386, 0.0027533124422773297, 0.00248103088342367, 0.0022351731749614867, 0.0020126942661715056, 0.0018127172620876256, 0.001632492398130812, 0.0014708067495678176, 0.0013248396042112925, 0.0011934783503649348, 0.0010742634445612175, 0.0009671787202592519, 0.0008705151678902982, 0.0007836204300339907, 0.0007055867113459039, 0.0006350953019411759, 0.0005716244991155816, 0.0005145479883352527, 0.0004631434232076097, 0.00041692616246910113, 0.0003752436272485952, 0.0003377268815117142, 0.00030402265580484206, 0.0002738622924989759, 0.0002464776488834278, 0.00022186484441846262, 0.0001996946871724785, 0.0001797460802299862, 0.0001617874378446529, 0.00014560874229222143, 0.00013109893366674929, 0.00011804623893094216, 0.00010631126702409345, 9.573655843054296e-05, 8.623498345486542e-05, 7.779102867756876e-05, 7.001530140697066e-05, 6.311426632926756e-05, 5.683149144473285e-05, 5.1152476351800304e-05, 4.60373796803577e-05, 4.150848924180734e-05, 3.7539322075611536e-05, 3.3787984641486876e-05, 3.047025631189958e-05, 2.7533637111858485e-05, 2.482987602019055e-05, 2.2347189070922463e-05, 2.0187696839070752e-05, 1.822147211925977e-05, 1.6497023767826515e-05, 1.4850286797238881e-05, 1.3365845316961025e-05, 1.2059700707710114e-05, 1.0867493151787077e-05, 9.785499494342769e-06, 8.807074471090113e-06, 7.971632034905605e-06, 7.37226188439132e-06, 6.683545069219901e-06, 6.018937543524696e-06, 5.443141127112768e-06, 4.904352636436177e-06, 4.4207520035884945e-06, 4.063827716563958e-06, 3.7199797736187014e-06, 3.453122273331637e-06, 3.192973832429058e-06, 2.8989445039767913e-06, 2.6473845858116856e-06, 2.3851152985688195e-06, 2.178321030369734e-06, 1.9757814581382414e-06, 1.8431845933524808e-06, 1.684232803924811e-06, 1.628584923549991e-06, 1.4698189117582342e-06, 1.3307418533320938e-06, 1.2875145988711956e-06, 1.162720564397276e-06, 1.195386140410038e-06, 1.104223565685218e-06, 1.0429788675870206e-06, 1.0131152875375864e-06, 9.165118381799626e-07, 8.336959121079427e-07, 7.50964733831306e-07, 7.681391067638993e-07, 6.917087173966204e-07, 6.228484979173375e-07, 5.77625644746338e-07, 5.309418359899493e-07, 5.050343077016902e-07, 6.520690463174367e-07, 5.868971071562698e-07, 5.5981441341578e-07, 5.3043255563142e-07, 4.836491487643464e-07, 4.813387390399126e-07, 6.725215996890013e-07, 7.718820838130517e-07, 7.266861956118898e-07, 8.685828421896953e-07, 1.039689212793804e-06, 9.428984475978602e-07, 8.718899650746241e-07, 7.996123336655483e-07, 7.317293060286908e-07, 7.008629236644373e-07, 7.17239530680765e-07, 6.685252401848554e-07, 6.041401796081977e-07, 5.898351217787464e-07, 5.309014952663387e-07, 4.920458610223288e-07, 4.5616707963895817e-07, 4.213442734973396e-07, 4.79267823182496e-07, 4.404431471651881e-07, 3.9655815430179776e-07, 4.302329374274503e-07, 4.2882849479344874e-07, 5.478160485761452e-07, 5.44530731933547e-07, 5.283011786324088e-07, 4.765313236058357e-07, 4.804263275636203e-07, 5.111585833125503e-07, 4.6193110503807365e-07, 5.800230128232033e-07, 5.23036165254762e-07, 4.707343018382887e-07, 4.248666426212036e-07, 4.7138557167341074e-07, 4.794587044823434e-07, 5.24117000712433e-07, 5.00152594705368e-07, 4.98037939306397e-07, 4.758388220633536e-07, 4.2959904929227134e-07, 4.6858108000150447e-07, 4.23403273447354e-07, 4.915574470628037e-07, 4.531482616858569e-07, 5.276560779473977e-07, 4.752353119302435e-07, 4.305562982223635e-07, 6.419243823511271e-07, 5.849000082786003e-07, 5.746314566635113e-07], "accuracy_test": 0.8357182716836735, "start": "2016-02-01 13:16:26.984000", "learning_rate_per_epoch": [0.0011861615348607302, 0.0010737086413428187, 0.0009719167137518525, 0.000879775092471391, 0.0007963688694871962, 0.0007208698661997914, 0.000652528484351933, 0.0005906661390326917, 0.0005346686230041087, 0.0004839798784814775, 0.00043809664202854037, 0.0003965633222833276, 0.0003589675179682672, 0.0003249359433539212, 0.0002941307029686868, 0.00026624591555446386, 0.00024100473092403263, 0.00021815650688949972, 0.00019747439364437014, 0.00017875302000902593, 0.00016180651437025517, 0.00014646659838035703, 0.0001325809716945514, 0.00012001176219200715, 0.00010863415809581056, 9.833520016400144e-05, 8.901262481231242e-05, 8.057386730797589e-05, 7.293513772310689e-05, 6.602059147553518e-05, 5.976157262921333e-05, 5.409593359217979e-05, 4.8967420298140496e-05, 4.432511195773259e-05, 4.012291174149141e-05, 3.631909567047842e-05, 3.28758978866972e-05, 2.9759130484308116e-05, 2.693784517759923e-05, 2.438402771076653e-05, 2.2072323190513998e-05, 1.9979777789558284e-05, 1.8085613191942684e-05, 1.6371024685213342e-05, 1.4818985619058367e-05, 1.3414085515250918e-05, 1.2142376363044605e-05, 1.0991229828505311e-05, 9.949217201210558e-06, 9.005991159938276e-06, 8.152186637744308e-06, 7.379326234513428e-06, 6.679736088699428e-06, 6.046469934517518e-06, 5.473239980346989e-06, 4.954354608344147e-06, 4.484661531023448e-06, 4.059497314301552e-06, 3.6746405385201797e-06, 3.3262697343161562e-06, 3.010925865964964e-06, 2.7254779979557497e-06, 2.467091690050438e-06, 2.233201485069003e-06, 2.0214849882904673e-06, 1.8298400163985207e-06, 1.6563637927902164e-06, 1.4993338481872343e-06, 1.3571909676102223e-06, 1.2285237289688666e-06, 1.1120547469545272e-06, 1.006627485367062e-06, 9.111951158047304e-07, 8.248101153185416e-07, 7.466147735613049e-07, 6.758326662748004e-07, 6.117609814282332e-07, 5.537635843211319e-07, 5.012645374335989e-07, 4.5374264345809934e-07, 4.107260167529603e-07, 3.717875358688616e-07, 3.365405802924215e-07, 3.04635165093714e-07, 2.757545303211373e-07, 2.496118725048291e-07, 2.2594765880512568e-07, 2.0452691273931123e-07, 1.8513694044486328e-07, 1.675852132621003e-07, 1.5169746347964974e-07, 1.3731593639931816e-07, 1.242978271420725e-07, 1.1251388798427797e-07, 1.0184711385363698e-07, 9.219159835538449e-08, 8.345146085275701e-08, 7.553992276143617e-08, 6.83784335819837e-08, 6.189588219740472e-08, 5.602790054126672e-08, 5.071622766195105e-08, 4.590812352489593e-08, 4.155584676368562e-08, 3.7616185721844886e-08, 3.405001791634277e-08, 3.082193700265634e-08, 2.7899892174332308e-08, 2.5254870195112744e-08, 2.2860605852770277e-08, 2.0693327940080053e-08, 1.8731515893932738e-08, 1.6955691961584307e-08, 1.5348224025046875e-08, 1.389315062994001e-08, 1.257602377791045e-08, 1.138376592990653e-08, 1.030453855577207e-08, 9.327626671051803e-09, 8.443329591045767e-09, 7.642867672075226e-09, 6.918293049551494e-09, 6.262411034185789e-09, 5.6687090577156596e-09, 5.131292724058767e-09, 4.644825413180342e-09, 4.204477210834057e-09, 3.805875614659726e-09, 3.445063123663772e-09, 3.1184572701903335e-09, 2.822814870739876e-09, 2.5552004956352903e-09, 2.3129571591340436e-09, 2.093679452030983e-09, 1.895190226974819e-09, 1.7155185050299337e-09, 1.55288037984036e-09, 1.4056610320167806e-09, 1.2723986309026714e-09, 1.1517701237195865e-09, 1.042577690846258e-09, 9.43737088476837e-10, 8.542669904798572e-10, 7.732789963910136e-10, 6.999690271847214e-10, 6.33609109623734e-10, 5.735403818540874e-10, 5.191664320669531e-10, 4.699473588054559e-10, 4.2539444189415576e-10, 3.850653407244664e-10, 3.4855959785140556e-10, 3.155147254574331e-10, 2.8560265263877227e-10, 2.5852636698076026e-10, 2.3401702797798407e-10, 2.1183126086565807e-10, 1.9174879739569661e-10, 1.7357024151287703e-10, 1.5711508483118308e-10, 1.4221994415475336e-10, 1.2873692389892e-10, 1.16532145044701e-10, 1.0548442674895853e-10, 9.548407897685962e-11, 8.643179921774546e-11, 7.823771491777265e-11, 7.082046060702396e-11, 6.410639380449723e-11, 5.802884908701067e-11, 5.2527478894282e-11, 4.754766025349966e-11, 4.3039950076151356e-11, 3.895958902710994e-11, 3.52660609048705e-11, 3.192269712459783e-11, 2.8896296813685218e-11, 2.615681109208179e-11, 2.3677039495684227e-11, 2.1432361094197994e-11, 1.9400486425680263e-11, 1.7561242390828902e-11, 1.5896365820888825e-11, 1.4389325668495712e-11, 1.3025159076307524e-11, 1.1790321323423836e-11, 1.0672551384316442e-11, 9.660750498186044e-12, 8.744872014021521e-12, 7.915822511361803e-12, 7.1653707273133804e-12, 6.486064723504326e-12, 5.871159461379172e-12, 5.3145495816642185e-12, 4.810708689045873e-12, 4.3546338410194174e-12, 3.941796975631684e-12, 3.5680987244685047e-12, 3.2298285140147653e-12, 2.92362770278054e-12, 2.6464559710337454e-12], "accuracy_train_first": 0.5599421560654301, "accuracy_train_last": 0.896826838778147, "batch_size_eval": 1024, "accuracy_train_std": [0.020252032402372767, 0.017140482652053625, 0.017817681989046295, 0.01965722870128587, 0.016187729638961337, 0.015164558361959842, 0.01610828926862797, 0.014332587105112574, 0.014929194186542689, 0.01397865013266511, 0.014244070429785486, 0.013677745027480098, 0.013261319583622547, 0.012345008885050163, 0.013424712963749925, 0.01139175819464326, 0.011251634137088662, 0.01136667781931092, 0.010720910049882864, 0.011315805704924273, 0.011746691777052138, 0.010774851190232488, 0.01066865181366295, 0.010527783169244373, 0.010553492762345873, 0.01009149990216604, 0.00996370540093439, 0.009600701221067398, 0.010232927657052034, 0.010009865728867967, 0.009195642055349541, 0.010636593900496758, 0.010756519849870897, 0.009539457309581665, 0.010226896210778599, 0.009753766609630878, 0.009653116871184754, 0.009716798000399183, 0.00999241177442367, 0.010118571356948105, 0.0096700511688745, 0.010170094005017227, 0.010117402035444718, 0.010794161106673732, 0.00996489838665064, 0.01052344636834336, 0.009826252948728223, 0.00998829965092501, 0.00987403156631362, 0.010120896887941391, 0.009941797428850813, 0.009875182795295718, 0.009911299518496846, 0.0101790389240361, 0.009959543259908903, 0.010194163258380748, 0.009611032633524572, 0.010104244092581191, 0.010206921107640592, 0.009769043485422966, 0.009850185054291872, 0.009776745118702924, 0.009632108871233381, 0.009499227165543, 0.009398961775733454, 0.009887820678048363, 0.009908214615254537, 0.009336821592112377, 0.009186112030392251, 0.009510060542095577, 0.009718021980207176, 0.00939220650204444, 0.009866784780740007, 0.009545834139857627, 0.009525047445375608, 0.009749313092376217, 0.009631305423114531, 0.009738158612883812, 0.00972275046950903, 0.009337924003167066, 0.009516251917189866, 0.009516280053152333, 0.009242652902220153, 0.009816052008510457, 0.009724412632592754, 0.009837943546320229, 0.009744696429979206, 0.01001147919648716, 0.009678610382077514, 0.009687916675263053, 0.009338671617544342, 0.009684910539460285, 0.009744046614402242, 0.010052221188873802, 0.0098532188354552, 0.00963794947249978, 0.00945664935722214, 0.009382613731429524, 0.009793822193840892, 0.009765783020029267, 0.009650052309592587, 0.009728024148466602, 0.009118309172274104, 0.009671835365175236, 0.009855003108114584, 0.009464543578386435, 0.00960686478370582, 0.009532498653128142, 0.009768778192038589, 0.009986844652543797, 0.009447850398762538, 0.0099179676060672, 0.010046575767078054, 0.010096841732096226, 0.009454756743396719, 0.009758401479893417, 0.009460674053382693, 0.009990071821889705, 0.009395283490968862, 0.009768354762592102, 0.009499042095706534, 0.009929540498050669, 0.009540154935721457, 0.009700717353629457, 0.009167355678930206, 0.0097767991302787, 0.009398961775733454, 0.010289584848004275, 0.00972632246170566, 0.009746221354096399, 0.009666312373022366, 0.009438028493496045, 0.009414487497616822, 0.009874709964169002, 0.009249292682663546, 0.009734021084513591, 0.009826020452957223, 0.009470124420313704, 0.00981172163717041, 0.009730065841508272, 0.009992038667437841, 0.009475805148457615, 0.009562827401520568, 0.009829507707072423, 0.009502718084565626, 0.00967239816681565, 0.009289522884254968, 0.00950304686357699, 0.00943600408080997, 0.009200976238955947, 0.009504161633408286, 0.009829374846016798, 0.009787699766174949, 0.009789360784418137, 0.009660938901392914, 0.01023733481010079, 0.010084575020870212, 0.009702764283729282, 0.009650569716279641, 0.009589211598779126, 0.009750723132705483, 0.009551270314290471, 0.009719359493387793, 0.009811569595302895, 0.0099496155527733, 0.009813396675117141, 0.010221662593500469, 0.00940258976327705, 0.010044913290798825, 0.009530009711253302, 0.00987482971501492, 0.00956380290254891, 0.009876580168211118, 0.010018907815481622, 0.009941377520737689, 0.00941289881384769, 0.009710666086674745, 0.010071265784807042, 0.009522789369689234, 0.009148142921566661, 0.009409745360441818, 0.00958790518421206, 0.010081989378525454, 0.009585880087697516, 0.009940372102784402, 0.009768361991291172, 0.010248418869636984, 0.009437985491249119, 0.009290310748172598, 0.009666738988379543, 0.009557565885043792, 0.009914250701087693, 0.009395822176290753, 0.009881420036571657, 0.009670833148246991, 0.009265419222527464, 0.009622043197169706, 0.009103518352383917, 0.009510554527181853, 0.009728611131285416, 0.009759640852413204], "accuracy_test_std": 0.008632688602417223, "error_valid": [0.4456963596573795, 0.35883965549698793, 0.30744658320783136, 0.2690635589231928, 0.24899255224021077, 0.2336616975715362, 0.22420051298945776, 0.2221356127635542, 0.20765925028237953, 0.2051163638930723, 0.19888048286897586, 0.20209549134036142, 0.1909253223832832, 0.18850450630647586, 0.1858292545180723, 0.17995958443147586, 0.1785256259412651, 0.17635777484939763, 0.17073224538780118, 0.1738148884600903, 0.16877912038780118, 0.16987775320030118, 0.16622593891189763, 0.1642934040850903, 0.16512730609939763, 0.16390660297439763, 0.16291974538780118, 0.16402867328689763, 0.16133283132530118, 0.16170933734939763, 0.15999005788780118, 0.16268589984939763, 0.16210643354668675, 0.15951207172439763, 0.15951207172439763, 0.16095632530120485, 0.15841343891189763, 0.15891201524849397, 0.15942088667168675, 0.15940029649849397, 0.15877964984939763, 0.15731480609939763, 0.15793545274849397, 0.15754865163780118, 0.15791486257530118, 0.15815900320030118, 0.15780308734939763, 0.15609410297439763, 0.1585560993975903, 0.15744717149849397, 0.15768101703689763, 0.15632794851280118, 0.15669415945030118, 0.15670445453689763, 0.15694859516189763, 0.15804722797439763, 0.15731480609939763, 0.15731480609939763, 0.15621617328689763, 0.15645001882530118, 0.15608380788780118, 0.15804722797439763, 0.15719273578689763, 0.15816929828689763, 0.15754865163780118, 0.15682652484939763, 0.15768101703689763, 0.15780308734939763, 0.15741628623870485, 0.15754865163780118, 0.15718244070030118, 0.15778249717620485, 0.15779279226280118, 0.15718244070030118, 0.15632794851280118, 0.15767072195030118, 0.15841343891189763, 0.15742658132530118, 0.15829136859939763, 0.15743687641189763, 0.15779279226280118, 0.15706037038780118, 0.15853550922439763, 0.15693830007530118, 0.15693830007530118, 0.15854580431099397, 0.15632794851280118, 0.15779279226280118, 0.15656179405120485, 0.15694859516189763, 0.15767072195030118, 0.15743687641189763, 0.15656179405120485, 0.15596173757530118, 0.15707066547439763, 0.15804722797439763, 0.15841343891189763, 0.15815900320030118, 0.15754865163780118, 0.15658238422439763, 0.15816929828689763, 0.15643972373870485, 0.15755894672439763, 0.15731480609939763, 0.15680593467620485, 0.15693830007530118, 0.15706037038780118, 0.15730451101280118, 0.15656179405120485, 0.15571759695030118, 0.15631765342620485, 0.15718244070030118, 0.15646031391189763, 0.15719273578689763, 0.15669415945030118, 0.15791486257530118, 0.15620587820030118, 0.15803693288780118, 0.15803693288780118, 0.15769131212349397, 0.15656179405120485, 0.15731480609939763, 0.15657208913780118, 0.15669415945030118, 0.15791486257530118, 0.15768101703689763, 0.15608380788780118, 0.15730451101280118, 0.15681622976280118, 0.15608380788780118, 0.15719273578689763, 0.15571759695030118, 0.15743687641189763, 0.15767072195030118, 0.15791486257530118, 0.15732510118599397, 0.15680593467620485, 0.15717214561370485, 0.15608380788780118, 0.15706037038780118, 0.15706037038780118, 0.15657208913780118, 0.15731480609939763, 0.15754865163780118, 0.15853550922439763, 0.15718244070030118, 0.15779279226280118, 0.15780308734939763, 0.15705007530120485, 0.15657208913780118, 0.15558523155120485, 0.15841343891189763, 0.15659267931099397, 0.15558523155120485, 0.15866787462349397, 0.15742658132530118, 0.15768101703689763, 0.15681622976280118, 0.15681622976280118, 0.15646031391189763, 0.15805752306099397, 0.15768101703689763, 0.15706037038780118, 0.15792515766189763, 0.15730451101280118, 0.15768101703689763, 0.15693830007530118, 0.15693830007530118, 0.15619558311370485, 0.15682652484939763, 0.15707066547439763, 0.15620587820030118, 0.15633824359939763, 0.15829136859939763, 0.15632794851280118, 0.15766042686370485, 0.15718244070030118, 0.15632794851280118, 0.15607351280120485, 0.15706037038780118, 0.15828107351280118, 0.15695889024849397, 0.15705007530120485, 0.15693830007530118, 0.15803693288780118, 0.15792515766189763, 0.15620587820030118, 0.15768101703689763, 0.15790456748870485, 0.15669415945030118, 0.15731480609939763, 0.15815900320030118, 0.15743687641189763, 0.15620587820030118, 0.15754865163780118, 0.15608380788780118, 0.15706037038780118, 0.15693830007530118, 0.15877964984939763, 0.15754865163780118, 0.15802663780120485], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.09480404552647027, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 16, "valid_ratio": 0.15, "learning_rate": 0.0013103919103876495, "optimization": "adam", "nb_data_augmentation": 3, "learning_rate_decay_method": "exp", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 2.375931336215917e-06, "rotation_range": [0, 0], "momentum": 0.5874517455931525}, "accuracy_valid_max": 0.8444147684487951, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8419733621987951, "accuracy_valid_std": [0.01194202070255515, 0.020612415374755826, 0.015840621612087956, 0.021499547082799823, 0.01715286999111916, 0.015014380912401292, 0.017061192709777094, 0.015220512864739291, 0.013831262644603274, 0.011230796557783824, 0.011231215644751526, 0.013295096953765764, 0.011783361286793505, 0.01291469379703947, 0.012274885917047606, 0.011578734128472361, 0.013057893810417394, 0.010809493148783317, 0.010256890196976494, 0.011615519342517354, 0.007168491154692702, 0.009461864365789222, 0.008693895949908147, 0.010132153390871684, 0.009782667015266654, 0.0072760609895479375, 0.008248415047146917, 0.008295725325126645, 0.007502553646004984, 0.008665134051000254, 0.007296183337519295, 0.0075676431263779235, 0.009220822788029916, 0.006162873989726592, 0.008914361927637919, 0.007398341595897931, 0.00812375395920782, 0.008346782589480115, 0.00990258508029876, 0.007600712515611941, 0.007260400599710344, 0.00756726353857275, 0.007888716209617402, 0.007056925818069176, 0.007990901729867895, 0.0066979230079604, 0.007824986767842003, 0.009014497723261412, 0.008750184373641735, 0.009028885215744917, 0.008269801568131162, 0.007091440559712767, 0.0076525281524082505, 0.009284661739118646, 0.007475472986302397, 0.008671471717309634, 0.008613431173372404, 0.008912710323950728, 0.009536261389683635, 0.009033639399708615, 0.0086500924279392, 0.009384509844183939, 0.00928644051880847, 0.008300571608870188, 0.007856279714776544, 0.008279814374128367, 0.008006127875098955, 0.009249276349275212, 0.008333370197745455, 0.008828000585393935, 0.0077635819614486025, 0.007912994354869737, 0.008442713898290119, 0.00806483579833845, 0.00800729709701282, 0.008748071555837593, 0.009848339640413904, 0.007898158727626205, 0.009009517366174337, 0.009023664346168808, 0.0076889576949155745, 0.008231507579349054, 0.008789426236449011, 0.008903119830035456, 0.008589640003436386, 0.00943441322546607, 0.00842798585628709, 0.009289767943914399, 0.008911248231875526, 0.009359079409292597, 0.00832923757453273, 0.008863717915361443, 0.008871025268411763, 0.008681282742331852, 0.008626302696275389, 0.009269479910095883, 0.009158426274331301, 0.009227941942206256, 0.008384759972978694, 0.008449842132814342, 0.008883302840511937, 0.009326400737966522, 0.009235484689847263, 0.00869607428976046, 0.008187743870083183, 0.0083503772096779, 0.008202492296288841, 0.0088200250112262, 0.008790027180742423, 0.008545088943864741, 0.009075461781287656, 0.0070218195813804745, 0.009458682895770591, 0.00889299837716846, 0.00868843520086395, 0.008468903327276636, 0.008982818448840418, 0.00826563546118027, 0.00741412767272275, 0.009280184046602044, 0.008273033930272062, 0.008557892396029367, 0.008809597060398244, 0.008564060895842445, 0.008226129571191468, 0.009885095002363752, 0.009726913234799659, 0.008683815717047438, 0.009129763968062048, 0.008035676705201864, 0.009552212710702194, 0.009085994069039552, 0.009180824646050969, 0.008414672307797686, 0.009249356796807436, 0.009674702317968503, 0.00722891401095544, 0.009101073188469241, 0.00947863225868002, 0.008516225021152984, 0.008899539809774206, 0.007971342684867162, 0.00931812112342431, 0.008975310507488466, 0.00920027116546717, 0.008482671468830246, 0.008856182887758584, 0.00984851282083426, 0.010040017398900121, 0.009063054306882812, 0.009323811270092795, 0.008840516157098771, 0.01040327532078415, 0.009563650432927747, 0.009610757472929348, 0.008381451210687178, 0.008759809829261096, 0.00816475900289866, 0.008646937103094242, 0.009756471084422924, 0.01048078234084775, 0.008921619314566054, 0.009508284915984025, 0.009078173078777384, 0.00801267634376097, 0.00911984483862316, 0.007910514659038387, 0.008644974823852345, 0.00797808021222546, 0.0085350372286154, 0.008444741362495817, 0.008822131507499139, 0.009276101429721082, 0.009697732025730621, 0.008110840699553954, 0.008438018979382528, 0.008580480481990367, 0.008895907895388102, 0.006997059005334515, 0.009019289109329514, 0.008409902449829343, 0.009351911708983699, 0.008704481521868668, 0.00826427783108111, 0.008850667200803925, 0.008825150052282894, 0.009321475759849854, 0.00984885005817695, 0.00842191853545284, 0.008032537058796549, 0.009292499346925089, 0.0092537424670742, 0.008322690111865181, 0.00952389666087854, 0.008036302253277198, 0.009158821158738194, 0.009215413974651025, 0.0081772719530837, 0.010359115381273255, 0.008719303051442999, 0.008641885711468912], "accuracy_valid": [0.5543036403426205, 0.6411603445030121, 0.6925534167921686, 0.7309364410768072, 0.7510074477597892, 0.7663383024284638, 0.7757994870105422, 0.7778643872364458, 0.7923407497176205, 0.7948836361069277, 0.8011195171310241, 0.7979045086596386, 0.8090746776167168, 0.8114954936935241, 0.8141707454819277, 0.8200404155685241, 0.8214743740587349, 0.8236422251506024, 0.8292677546121988, 0.8261851115399097, 0.8312208796121988, 0.8301222467996988, 0.8337740610881024, 0.8357065959149097, 0.8348726939006024, 0.8360933970256024, 0.8370802546121988, 0.8359713267131024, 0.8386671686746988, 0.8382906626506024, 0.8400099421121988, 0.8373141001506024, 0.8378935664533133, 0.8404879282756024, 0.8404879282756024, 0.8390436746987951, 0.8415865610881024, 0.841087984751506, 0.8405791133283133, 0.840599703501506, 0.8412203501506024, 0.8426851939006024, 0.842064547251506, 0.8424513483621988, 0.8420851374246988, 0.8418409967996988, 0.8421969126506024, 0.8439058970256024, 0.8414439006024097, 0.842552828501506, 0.8423189829631024, 0.8436720514871988, 0.8433058405496988, 0.8432955454631024, 0.8430514048381024, 0.8419527720256024, 0.8426851939006024, 0.8426851939006024, 0.8437838267131024, 0.8435499811746988, 0.8439161921121988, 0.8419527720256024, 0.8428072642131024, 0.8418307017131024, 0.8424513483621988, 0.8431734751506024, 0.8423189829631024, 0.8421969126506024, 0.8425837137612951, 0.8424513483621988, 0.8428175592996988, 0.8422175028237951, 0.8422072077371988, 0.8428175592996988, 0.8436720514871988, 0.8423292780496988, 0.8415865610881024, 0.8425734186746988, 0.8417086314006024, 0.8425631235881024, 0.8422072077371988, 0.8429396296121988, 0.8414644907756024, 0.8430616999246988, 0.8430616999246988, 0.841454195689006, 0.8436720514871988, 0.8422072077371988, 0.8434382059487951, 0.8430514048381024, 0.8423292780496988, 0.8425631235881024, 0.8434382059487951, 0.8440382624246988, 0.8429293345256024, 0.8419527720256024, 0.8415865610881024, 0.8418409967996988, 0.8424513483621988, 0.8434176157756024, 0.8418307017131024, 0.8435602762612951, 0.8424410532756024, 0.8426851939006024, 0.8431940653237951, 0.8430616999246988, 0.8429396296121988, 0.8426954889871988, 0.8434382059487951, 0.8442824030496988, 0.8436823465737951, 0.8428175592996988, 0.8435396860881024, 0.8428072642131024, 0.8433058405496988, 0.8420851374246988, 0.8437941217996988, 0.8419630671121988, 0.8419630671121988, 0.842308687876506, 0.8434382059487951, 0.8426851939006024, 0.8434279108621988, 0.8433058405496988, 0.8420851374246988, 0.8423189829631024, 0.8439161921121988, 0.8426954889871988, 0.8431837702371988, 0.8439161921121988, 0.8428072642131024, 0.8442824030496988, 0.8425631235881024, 0.8423292780496988, 0.8420851374246988, 0.842674898814006, 0.8431940653237951, 0.8428278543862951, 0.8439161921121988, 0.8429396296121988, 0.8429396296121988, 0.8434279108621988, 0.8426851939006024, 0.8424513483621988, 0.8414644907756024, 0.8428175592996988, 0.8422072077371988, 0.8421969126506024, 0.8429499246987951, 0.8434279108621988, 0.8444147684487951, 0.8415865610881024, 0.843407320689006, 0.8444147684487951, 0.841332125376506, 0.8425734186746988, 0.8423189829631024, 0.8431837702371988, 0.8431837702371988, 0.8435396860881024, 0.841942476939006, 0.8423189829631024, 0.8429396296121988, 0.8420748423381024, 0.8426954889871988, 0.8423189829631024, 0.8430616999246988, 0.8430616999246988, 0.8438044168862951, 0.8431734751506024, 0.8429293345256024, 0.8437941217996988, 0.8436617564006024, 0.8417086314006024, 0.8436720514871988, 0.8423395731362951, 0.8428175592996988, 0.8436720514871988, 0.8439264871987951, 0.8429396296121988, 0.8417189264871988, 0.843041109751506, 0.8429499246987951, 0.8430616999246988, 0.8419630671121988, 0.8420748423381024, 0.8437941217996988, 0.8423189829631024, 0.8420954325112951, 0.8433058405496988, 0.8426851939006024, 0.8418409967996988, 0.8425631235881024, 0.8437941217996988, 0.8424513483621988, 0.8439161921121988, 0.8429396296121988, 0.8430616999246988, 0.8412203501506024, 0.8424513483621988, 0.8419733621987951], "seed": 991159115, "model": "residualv3", "loss_std": [0.3117538392543793, 0.25638529658317566, 0.2561412453651428, 0.2496434897184372, 0.24696698784828186, 0.24161531031131744, 0.23878207802772522, 0.2377251833677292, 0.23381583392620087, 0.23081150650978088, 0.22881853580474854, 0.22564738988876343, 0.22364158928394318, 0.22120030224323273, 0.21888935565948486, 0.21612372994422913, 0.21387110650539398, 0.21324817836284637, 0.21043747663497925, 0.21012268960475922, 0.20719952881336212, 0.20528961718082428, 0.20509977638721466, 0.2037295252084732, 0.20219896733760834, 0.20122818648815155, 0.1998046487569809, 0.19857920706272125, 0.19744372367858887, 0.19650310277938843, 0.19633865356445312, 0.19577473402023315, 0.1948142647743225, 0.19442957639694214, 0.19308190047740936, 0.19383364915847778, 0.19255629181861877, 0.1923292875289917, 0.19218136370182037, 0.19202542304992676, 0.19035956263542175, 0.1899421662092209, 0.1901254653930664, 0.18810443580150604, 0.19113706052303314, 0.1893811672925949, 0.18881219625473022, 0.18799906969070435, 0.1872335523366928, 0.19118589162826538, 0.1900031864643097, 0.1893899291753769, 0.18873728811740875, 0.18809236586093903, 0.18824894726276398, 0.18868863582611084, 0.18747593462467194, 0.1877353936433792, 0.18766701221466064, 0.18854767084121704, 0.18645772337913513, 0.1876312792301178, 0.18566274642944336, 0.18764393031597137, 0.1850176304578781, 0.18785059452056885, 0.18714624643325806, 0.18833625316619873, 0.18611247837543488, 0.18798959255218506, 0.1882883757352829, 0.18738149106502533, 0.1879240721464157, 0.1862231194972992, 0.188060462474823, 0.18902869522571564, 0.18810637295246124, 0.18618550896644592, 0.18804579973220825, 0.18753740191459656, 0.18711614608764648, 0.1860262006521225, 0.18684326112270355, 0.18761004507541656, 0.18977448344230652, 0.18680258095264435, 0.18770141899585724, 0.18886007368564606, 0.18806897103786469, 0.18864981830120087, 0.186336949467659, 0.1864739954471588, 0.18577727675437927, 0.187835693359375, 0.1881902664899826, 0.18736188113689423, 0.18561798334121704, 0.18750128149986267, 0.18636992573738098, 0.18502551317214966, 0.18642346560955048, 0.18909646570682526, 0.18845003843307495, 0.18808916211128235, 0.187191441655159, 0.187615305185318, 0.18635062873363495, 0.18644694983959198, 0.1857772022485733, 0.18689070641994476, 0.18696901202201843, 0.18734651803970337, 0.18893177807331085, 0.18516717851161957, 0.18793781101703644, 0.1854155957698822, 0.1867060512304306, 0.18601706624031067, 0.18642933666706085, 0.18689244985580444, 0.18569913506507874, 0.18696299195289612, 0.1877431571483612, 0.18907125294208527, 0.18902112543582916, 0.18732748925685883, 0.18952779471874237, 0.18658334016799927, 0.18623536825180054, 0.18834339082241058, 0.18847107887268066, 0.1876448094844818, 0.1867038458585739, 0.18828754127025604, 0.1879304051399231, 0.18748478591442108, 0.18786506354808807, 0.18877150118350983, 0.1881769299507141, 0.18714618682861328, 0.18733468651771545, 0.18874694406986237, 0.18637129664421082, 0.18764150142669678, 0.18665610253810883, 0.18590126931667328, 0.18714174628257751, 0.18669846653938293, 0.1880299150943756, 0.185693621635437, 0.18688270449638367, 0.18770495057106018, 0.18821369111537933, 0.18778692185878754, 0.18669939041137695, 0.18788304924964905, 0.18579599261283875, 0.18683657050132751, 0.18782274425029755, 0.1878979653120041, 0.1871015578508377, 0.18800541758537292, 0.18743056058883667, 0.1865701526403427, 0.1866067796945572, 0.18701966106891632, 0.18546803295612335, 0.1873064488172531, 0.1863788515329361, 0.18779435753822327, 0.18846648931503296, 0.18646501004695892, 0.18740732967853546, 0.18599119782447815, 0.18787045776844025, 0.18826518952846527, 0.18567103147506714, 0.1873074620962143, 0.18889032304286957, 0.18730399012565613, 0.18669122457504272, 0.18710337579250336, 0.18660272657871246, 0.1878003031015396, 0.18888819217681885, 0.18765008449554443, 0.18817020952701569, 0.1870691031217575, 0.18789267539978027, 0.18709443509578705, 0.18707187473773956, 0.18740180134773254, 0.1864706575870514, 0.18886087834835052, 0.1870792657136917, 0.18605533242225647, 0.18644164502620697, 0.18693706393241882, 0.18907439708709717, 0.18803924322128296, 0.18740646541118622]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:33 2016", "state": "available"}], "summary": "be25aec72a858978ca64c709bfe84809"}