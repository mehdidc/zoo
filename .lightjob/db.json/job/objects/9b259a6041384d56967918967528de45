{"content": {"hp_model": {"f0": 32, "f1": 16, "f2": 64, "f3": 64, "nonlin": "rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.5867962837219238, 1.1888490915298462, 0.9557494521141052, 0.8244466185569763, 0.7300131916999817, 0.6535106897354126, 0.5871829390525818, 0.527807891368866, 0.47338151931762695, 0.42319315671920776, 0.3772484362125397, 0.33572739362716675, 0.29891684651374817, 0.26725244522094727, 0.24043354392051697, 0.2182643860578537, 0.20128673315048218, 0.18944087624549866, 0.18358315527439117, 0.18121647834777832, 0.17756281793117523, 0.17419826984405518, 0.17298272252082825, 0.1697310507297516, 0.16825911402702332, 0.16633561253547668, 0.1616816222667694, 0.15790100395679474, 0.16257700324058533, 0.15448574721813202, 0.15134939551353455, 0.15090222656726837, 0.1462971419095993, 0.1434919536113739, 0.1424006223678589, 0.13972295820713043, 0.13668224215507507, 0.13342243432998657, 0.1346758008003235, 0.12990018725395203, 0.1274796575307846, 0.1281263530254364, 0.12412981688976288, 0.12217716127634048, 0.11825229227542877, 0.11769375950098038, 0.11358100175857544, 0.11218622326850891, 0.1098720133304596, 0.10550855100154877, 0.10283026099205017, 0.10020284354686737, 0.09860814362764359, 0.09703785926103592, 0.09498454630374908, 0.09314097464084625, 0.09161195904016495, 0.0903078019618988, 0.08754714578390121, 0.08388518542051315, 0.08413448184728622, 0.08174760639667511, 0.08297369629144669, 0.08312982320785522, 0.08095733076334, 0.08019509166479111, 0.07891412824392319, 0.07648206502199173, 0.07352427393198013, 0.07052489370107651, 0.06762832403182983, 0.06487803906202316, 0.06227525696158409, 0.05981883034110069, 0.06232805177569389, 0.06031355261802673, 0.058935657143592834, 0.058526456356048584, 0.060087647289037704, 0.05985625088214874, 0.058020949363708496, 0.057535625994205475, 0.05689569562673569, 0.05559092015028, 0.05380621179938316, 0.0519627183675766, 0.0501851849257946, 0.051551103591918945, 0.0492846705019474, 0.04827439785003662, 0.04792413115501404, 0.047336794435977936, 0.048734188079833984, 0.047454025596380234, 0.04599340260028839, 0.045694347470998764, 0.04518971964716911, 0.045968249440193176, 0.045727651566267014, 0.04392722621560097, 0.04360548406839371, 0.0431859977543354, 0.04233232140541077, 0.041233502328395844, 0.04493315890431404, 0.04079049453139305], "moving_avg_accuracy_train": [0.06223999999999998, 0.1261336470588235, 0.19085675294117643, 0.2524793129411764, 0.3103396169411764, 0.36423741995294107, 0.4144419132517646, 0.4610377219265881, 0.5041292438515763, 0.5440292606428893, 0.5810286875197768, 0.6154058187677991, 0.647164060420431, 0.6762147132019173, 0.7027885359993726, 0.7274720353406119, 0.7501224788653742, 0.7709737603906015, 0.7893634431750708, 0.8065870988575637, 0.8221189772071015, 0.8371800206628619, 0.8505773127142228, 0.8628654637957417, 0.8735506821220499, 0.8837697315569037, 0.8935527584012134, 0.9023268943257979, 0.9104847931285123, 0.9179798432274258, 0.9247442118458596, 0.9303121436024502, 0.9359138704186758, 0.9410518951415141, 0.9443490585685392, 0.9486741527116853, 0.9526232080287521, 0.9561044166376416, 0.9592798573268186, 0.9621942245353133, 0.9648289197288408, 0.9671860277559566, 0.9694580132156551, 0.9714910354235013, 0.9733089907046807, 0.9746722092812714, 0.9762191060002031, 0.977613665988418, 0.9788899464483998, 0.9799797753329716, 0.9802406213290862, 0.981263618019707, 0.9822713738647951, 0.9831618835371392, 0.9831304010657782, 0.9839867727239062, 0.9848069189809273, 0.9854697564945993, 0.9859863102569041, 0.9864817968782724, 0.9868100877786805, 0.9872184907655184, 0.9876707593360254, 0.9880001539906581, 0.9884895503562982, 0.9889158894383153, 0.9892760652003662, 0.9895861057391531, 0.9898674951652378, 0.9900783927075375, 0.9903105534367838, 0.9904865569166348, 0.9906708424014419, 0.9908131699260035, 0.9907718529334032, 0.9901323146988864, 0.9904461420525272, 0.9907732925531569, 0.9904630221213707, 0.9905367199092336, 0.9908618714477221, 0.991201566655891, 0.9914790570491254, 0.9917123278148011, 0.9918752126803798, 0.9920571031770477, 0.9921760987416958, 0.9918714300439967, 0.9920372282160675, 0.9923158583356373, 0.9925925077961912, 0.9927944334871603, 0.9920114607266797, 0.9921561970069529, 0.9924817537768459, 0.9927959313403377, 0.9929939852651275, 0.9920122337974383, 0.9920910104176945, 0.9925007329053369, 0.992864777261862, 0.9931006524768522, 0.9931858813468141, 0.9933049402709562, 0.9933791521262135, 0.993615354560651], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.06213333333333332, 0.12507999999999997, 0.18665199999999996, 0.2442134666666666, 0.29719211999999995, 0.34551290799999995, 0.3894149505333333, 0.4289667888133333, 0.4646034432653333, 0.4965697656054666, 0.5252594557115866, 0.5511201768070946, 0.5741281591263852, 0.5946753432137466, 0.6132211422257052, 0.6298190280031347, 0.6444904585361546, 0.6576547460158725, 0.6690626047476186, 0.67964967760619, 0.6895780431789043, 0.6985535721943471, 0.706764881641579, 0.7143283934774212, 0.7212822207963456, 0.7273939987167111, 0.7332012655117066, 0.7379878056272027, 0.742935691731149, 0.7475087892247008, 0.7512379103022307, 0.7547141192720077, 0.7577893740114736, 0.7606637699436596, 0.7628640596159604, 0.7655376536543643, 0.7677305549555945, 0.7695041661267017, 0.7714070828473649, 0.7729997078959617, 0.7742864037730323, 0.7758710967290623, 0.7771639870561561, 0.7781942550172071, 0.7788548295154863, 0.7798093465639377, 0.7805217452408773, 0.7814562373834562, 0.7820706136451105, 0.7823302189472662, 0.7820971970525396, 0.7826474773472856, 0.7833160629458903, 0.7835177899846346, 0.7831793443195045, 0.7839414098875541, 0.7844939355654653, 0.7848045420089187, 0.7848307544746934, 0.7848810123605574, 0.7851929111245016, 0.7856869533453847, 0.7858915913441795, 0.7858624322097616, 0.7861561889887854, 0.7864872367565735, 0.7867051797475828, 0.7867413284394913, 0.7866805289288755, 0.7863858093693212, 0.7862538950990557, 0.7860818389224835, 0.7861003216969018, 0.7858636228605449, 0.7858105939078238, 0.7844828678503747, 0.7846079143986705, 0.7848404562921367, 0.7846497439962564, 0.7849447695966307, 0.7853569593036344, 0.7856212633732709, 0.7858058037026104, 0.785731889999016, 0.7854787009991143, 0.7849174975658696, 0.7844524144759494, 0.7838738396950211, 0.783926455725519, 0.7840004768196338, 0.7840004291376704, 0.7838403862239034, 0.7827630142681797, 0.7827800461746951, 0.782595374890559, 0.7824425040681697, 0.7823849203280194, 0.7818130949618841, 0.7817917854656957, 0.7819059402524595, 0.7819953462272136, 0.7818091449378255, 0.7814948971107096, 0.781158740732972, 0.7811495333263414, 0.7813279133270407], "moving_var_accuracy_train": [0.03486435839999999, 0.06811950577029757, 0.09900927910879224, 0.12328441030649542, 0.14108630228659763, 0.1531224305833928, 0.1604946078515781, 0.16398567154096713, 0.1642990177431761, 0.162197218028382, 0.15829811452850712, 0.15310438745125018, 0.1468712219219278, 0.13977956357300933, 0.1321571197383434, 0.12442488402206968, 0.11659977894667875, 0.10885278452320944, 0.10101112996710915, 0.09357990580601987, 0.08639306843100145, 0.07979527685768804, 0.07343113608070435, 0.06744701038565402, 0.061729874363216475, 0.05649674766906271, 0.05170844143028479, 0.04723046643826408, 0.043106381610315624, 0.03930132543315107, 0.035783003035090445, 0.03248371950799584, 0.029517761647108842, 0.026803579164870438, 0.024221062828363997, 0.021967314499651284, 0.019910938390761437, 0.01802891387209275, 0.016316773297017802, 0.01476153779334956, 0.013347858582879776, 0.012063076348855247, 0.010903225975331454, 0.009850101991476672, 0.008894836444968314, 0.008022078084459542, 0.007241406281144962, 0.006534768831077038, 0.005895951974282115, 0.005317046319832727, 0.004785954053552656, 0.004316777348258581, 0.0038942397600225066, 0.0035119528513091, 0.003160766486492217, 0.0028512901895946003, 0.0025722149295812923, 0.0023189476187489406, 0.0020894543069782073, 0.0018827184392079825, 0.0016954165695248007, 0.0015273760495692432, 0.0013764793663511349, 0.001239807937262527, 0.0011179827227605899, 0.0010078203356002288, 0.0009082058412563259, 0.0008182503833519148, 0.0007371379650987338, 0.0006638244685489931, 0.0005979271091319312, 0.0005384131932430151, 0.0004848775241779089, 0.00043657208587834874, 0.0003929302411354117, 0.0003573182994025502, 0.00032247285793333367, 0.0002911888191905602, 0.00026293634693907184, 0.00023669159452058765, 0.00021397394677536124, 0.00019361508760790176, 0.00017494658711214817, 0.0001579416656520037, 0.00014238628240171469, 0.0001284454115365461, 0.00011572830988254493, 0.00010499088603250877, 9.473919873401643e-05, 8.596399155239767e-05, 7.805640671338083e-05, 7.061773190410291e-05, 6.907337580658611e-05, 6.235457554337341e-05, 5.70730028828442e-05, 5.225407046717504e-05, 4.7381691634579164e-05, 5.131804596991067e-05, 4.624209317601046e-05, 4.312873651032758e-05, 4.0008617500955006e-05, 3.65084898042799e-05, 3.292301646632666e-05, 2.9758290066454914e-05, 2.6832027654956023e-05, 2.4650949199768118e-05], "duration": 59829.310631, "accuracy_train": [0.6224, 0.7011764705882353, 0.773364705882353, 0.8070823529411765, 0.8310823529411765, 0.8493176470588235, 0.8662823529411765, 0.8804, 0.8919529411764706, 0.9031294117647058, 0.9140235294117647, 0.9248, 0.9329882352941177, 0.9376705882352941, 0.9419529411764705, 0.9496235294117648, 0.9539764705882353, 0.958635294117647, 0.9548705882352941, 0.9616, 0.9619058823529412, 0.9727294117647058, 0.9711529411764706, 0.9734588235294117, 0.9697176470588236, 0.9757411764705882, 0.9816, 0.9812941176470589, 0.9839058823529412, 0.9854352941176471, 0.9856235294117647, 0.9804235294117647, 0.9863294117647059, 0.9872941176470589, 0.9740235294117647, 0.9876, 0.988164705882353, 0.9874352941176471, 0.9878588235294118, 0.9884235294117647, 0.9885411764705883, 0.9884, 0.9899058823529412, 0.9897882352941176, 0.9896705882352941, 0.9869411764705882, 0.9901411764705882, 0.990164705882353, 0.9903764705882353, 0.9897882352941176, 0.9825882352941177, 0.9904705882352941, 0.9913411764705883, 0.9911764705882353, 0.9828470588235294, 0.9916941176470588, 0.9921882352941176, 0.9914352941176471, 0.9906352941176471, 0.9909411764705882, 0.989764705882353, 0.9908941176470588, 0.9917411764705882, 0.990964705882353, 0.9928941176470588, 0.9927529411764706, 0.9925176470588235, 0.9923764705882353, 0.9924, 0.9919764705882353, 0.9924, 0.9920705882352941, 0.9923294117647059, 0.9920941176470588, 0.9904, 0.9843764705882353, 0.9932705882352941, 0.9937176470588235, 0.9876705882352941, 0.9912, 0.9937882352941176, 0.9942588235294118, 0.9939764705882352, 0.9938117647058824, 0.9933411764705883, 0.9936941176470588, 0.9932470588235294, 0.9891294117647059, 0.9935294117647059, 0.9948235294117647, 0.9950823529411764, 0.9946117647058823, 0.984964705882353, 0.9934588235294117, 0.9954117647058823, 0.9956235294117647, 0.9947764705882353, 0.9831764705882353, 0.9928, 0.9961882352941176, 0.9961411764705882, 0.9952235294117647, 0.9939529411764706, 0.9943764705882353, 0.9940470588235294, 0.9957411764705882], "end": "2016-02-04 17:25:59.627000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0], "moving_var_accuracy_valid": [0.034744959999999984, 0.06693100959999998, 0.09435790929599999, 0.11474202036976, 0.12852845771390561, 0.13668969891898355, 0.14036723307447269, 0.14040964096897088, 0.13779841713685506, 0.13321518729874932, 0.12730155343434119, 0.12059039015112394, 0.11329565638965462, 0.10576577171596845, 0.09828471449329931, 0.09093565135449475, 0.08377934408401232, 0.076961095859249, 0.07043623944091539, 0.06440139050223816, 0.058848403438523475, 0.05368860418463465, 0.04892657419171499, 0.04454877717416181, 0.04052910088617842, 0.03681237526169138, 0.03343465686417656, 0.030297389874254173, 0.02748798507890739, 0.024927405557186415, 0.022559822097565674, 0.020412596147023124, 0.018456451257734277, 0.016685165499735557, 0.0150602204215403, 0.013618531325125973, 0.01229995753766581, 0.011098273053175714, 0.01002103557627016, 0.009041760109551905, 0.008152484375117348, 0.007359837203489637, 0.006638897571721707, 0.005984560883193651, 0.005390032022884279, 0.00485922874575791, 0.004377873478056266, 0.003947945610331515, 0.003556548173016323, 0.003201499909930857, 0.002881838611768569, 0.002596380026216784, 0.0023407650839190612, 0.0021070548197106003, 0.0018973802469537488, 0.001712868917628434, 0.0015443295874883523, 0.0013907649160039505, 0.0012516946082438134, 0.0011265478801152557, 0.0010147686196542798, 0.0009154884571329885, 0.0008243165018146464, 0.0007418925039292618, 0.0006684798909433378, 0.0006026182354700216, 0.0005427839042489903, 0.0004885172743754314, 0.00043969881616230834, 0.000396510671115132, 0.00035701621637591576, 0.0003215810246893941, 0.0002894259967370065, 0.0002609876341155001, 0.00023491417933239043, 0.00022728846975181413, 0.0002047003525297991, 0.0001847169988667712, 0.00016657263959829363, 0.0001506987365823505, 0.00013715796611515283, 0.00012407087927467563, 0.00011197028754558291, 0.00010082242791123595, 9.131712714715288e-05, 8.501995807380899e-05, 7.84646827911964e-05, 7.363095350621258e-05, 6.629277417557951e-05, 5.971280885938708e-05, 5.37415279939105e-05, 4.859789880274287e-05, 5.418468190128723e-05, 4.876882448371444e-05, 4.419887338400353e-05, 3.9989311440644805e-05, 3.60202232807476e-05, 3.5361059196874564e-05, 3.182904012883734e-05, 2.876341795402333e-05, 2.5959017013516498e-05, 2.3675153593692696e-05, 2.2196403505946947e-05, 2.0993773147995394e-05, 1.8895158820227594e-05, 1.7292017760049836e-05], "accuracy_test": 0.1, "start": "2016-02-04 00:48:50.317000", "learning_rate_per_epoch": [0.0007902093348093331, 0.00039510466740466654, 0.00026340311160311103, 0.00019755233370233327, 0.00015804186114110053, 0.00013170155580155551, 0.00011288704263279215, 9.877616685116664e-05, 8.78010323503986e-05, 7.902093057055026e-05, 7.183721027104184e-05, 6.585077790077776e-05, 6.0785332607338205e-05, 5.644352131639607e-05, 5.2680621593026444e-05, 4.938808342558332e-05, 4.648289905162528e-05, 4.39005161751993e-05, 4.158996307523921e-05, 3.951046528527513e-05, 3.762901542359032e-05, 3.591860513552092e-05, 3.4356926335021853e-05, 3.292538895038888e-05, 3.1608371500624344e-05, 3.0392666303669102e-05, 2.9267011996125802e-05, 2.8221760658198036e-05, 2.7248597689322196e-05, 2.6340310796513222e-05, 2.5490622647339478e-05, 2.469404171279166e-05, 2.3945736757013947e-05, 2.324144952581264e-05, 2.257740925415419e-05, 2.195025808759965e-05, 2.1357009245548397e-05, 2.0794981537619606e-05, 2.02617775357794e-05, 1.9755232642637566e-05, 1.92733987205429e-05, 1.881450771179516e-05, 1.8376960724708624e-05, 1.795930256776046e-05, 1.756020719767548e-05, 1.7178463167510927e-05, 1.681296453170944e-05, 1.646269447519444e-05, 1.6126719856401905e-05, 1.5804185750312172e-05, 1.5494300896534696e-05, 1.5196333151834551e-05, 1.4909609490132425e-05, 1.4633505998062901e-05, 1.4367442418006249e-05, 1.4110880329099018e-05, 1.3863321328244638e-05, 1.3624298844661098e-05, 1.3393378139880951e-05, 1.3170155398256611e-05, 1.2954251360497437e-05, 1.2745311323669739e-05, 1.2543005141196772e-05, 1.234702085639583e-05, 1.2157066521467641e-05, 1.1972868378506973e-05, 1.1794169040513225e-05, 1.162072476290632e-05, 1.1452309081505518e-05, 1.1288704627077095e-05, 1.1129708582302555e-05, 1.0975129043799825e-05, 1.0824785022123251e-05, 1.0678504622774199e-05, 1.0536124136706349e-05, 1.0397490768809803e-05, 1.0262458999932278e-05, 1.01308887678897e-05, 1.0002649105445016e-05, 9.877616321318783e-06, 9.755670362210367e-06, 9.63669936027145e-06, 9.52059417613782e-06, 9.40725385589758e-06, 9.296580174122937e-06, 9.188480362354312e-06, 9.08286529011093e-06, 8.97965128388023e-06, 8.87875648913905e-06, 8.78010359883774e-06, 8.683618943905458e-06, 8.589231583755463e-06, 8.496874215779826e-06, 8.40648226585472e-06, 8.317992978845723e-06, 8.23134723759722e-06, 8.146487743942998e-06, 8.063359928200953e-06, 7.981911949173082e-06, 7.902092875156086e-06, 7.823854502930772e-06, 7.747150448267348e-06, 7.671935236430727e-06, 7.598166575917276e-06, 7.5258030847180635e-06, 7.4548047450662125e-06], "accuracy_train_first": 0.6224, "accuracy_train_last": 0.9957411764705882, "batch_size_eval": 1024, "accuracy_train_std": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "accuracy_test_std": 0, "error_valid": [0.3786666666666667, 0.3084, 0.2592, 0.23773333333333335, 0.22599999999999998, 0.21960000000000002, 0.2154666666666667, 0.21506666666666663, 0.21466666666666667, 0.21573333333333333, 0.21653333333333336, 0.2161333333333333, 0.2188, 0.22040000000000004, 0.21986666666666665, 0.2208, 0.2234666666666667, 0.22386666666666666, 0.22826666666666662, 0.22506666666666664, 0.22106666666666663, 0.22066666666666668, 0.21933333333333338, 0.21760000000000002, 0.2161333333333333, 0.21760000000000002, 0.21453333333333335, 0.2189333333333333, 0.21253333333333335, 0.21133333333333337, 0.21519999999999995, 0.21399999999999997, 0.21453333333333335, 0.2134666666666667, 0.21733333333333338, 0.21040000000000003, 0.21253333333333335, 0.21453333333333335, 0.2114666666666667, 0.21266666666666667, 0.2141333333333333, 0.20986666666666665, 0.21120000000000005, 0.21253333333333335, 0.21519999999999995, 0.2116, 0.21306666666666663, 0.21013333333333328, 0.21240000000000003, 0.21533333333333338, 0.21999999999999997, 0.21240000000000003, 0.21066666666666667, 0.21466666666666667, 0.21986666666666665, 0.20920000000000005, 0.21053333333333335, 0.21240000000000003, 0.2149333333333333, 0.21466666666666667, 0.21199999999999997, 0.20986666666666665, 0.21226666666666671, 0.21440000000000003, 0.21120000000000005, 0.21053333333333335, 0.21133333333333337, 0.2129333333333333, 0.21386666666666665, 0.21626666666666672, 0.2149333333333333, 0.2154666666666667, 0.21373333333333333, 0.21626666666666672, 0.21466666666666667, 0.2274666666666667, 0.21426666666666672, 0.21306666666666663, 0.21706666666666663, 0.21240000000000003, 0.2109333333333333, 0.21199999999999997, 0.21253333333333335, 0.2149333333333333, 0.2168, 0.2201333333333333, 0.21973333333333334, 0.22133333333333338, 0.2156, 0.21533333333333338, 0.21599999999999997, 0.21760000000000002, 0.22693333333333332, 0.21706666666666663, 0.21906666666666663, 0.2189333333333333, 0.2181333333333333, 0.22333333333333338, 0.21840000000000004, 0.21706666666666663, 0.21719999999999995, 0.21986666666666665, 0.22133333333333338, 0.22186666666666666, 0.2189333333333333, 0.21706666666666663], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.07311714715652474, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 16, "valid_ratio": 0.15, "learning_rate": 0.0007902093140888501, "optimization": "adam", "nb_data_augmentation": 0, "learning_rate_decay_method": "lin", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 3.792233728408764e-05, "rotation_range": [0, 0], "momentum": 0.6111728015154464}, "accuracy_valid_max": 0.7908, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        y_pred = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            y_pred.extend((nnet.predict(X[mini_batch]) == y[mini_batch]).tolist())\n        return np.mean(y_pred)\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            acc = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = acc\n            status[\"accuracy_train_std\"] = 0\n            acc = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = acc\n            status[\"accuracy_valid_std\"] = 0\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n\n    # rescaling to [-1, 1]\n    X_min = X_train.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X_train.max(axis=(0, 2, 3))[None, :, None, None]\n    def preprocess(a):\n        return (a / 255.) * 2 - 1\n        # return 2 * ((a - X_min) / (X_max - X_min)) - 1\n    X_train = preprocess(X_train)\n    X_valid = preprocess(X_valid)\n\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = preprocess(X_test)\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    acc = evaluate(X_test, y_test, batch_size_eval)\n    light.set(\"accuracy_test\", acc)\n    light.set(\"accuracy_test_std\", 0)\n    print(\"Test accuracy : {}+-{}\".format(acc, 0))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.7829333333333334, "accuracy_valid_std": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "accuracy_valid": [0.6213333333333333, 0.6916, 0.7408, 0.7622666666666666, 0.774, 0.7804, 0.7845333333333333, 0.7849333333333334, 0.7853333333333333, 0.7842666666666667, 0.7834666666666666, 0.7838666666666667, 0.7812, 0.7796, 0.7801333333333333, 0.7792, 0.7765333333333333, 0.7761333333333333, 0.7717333333333334, 0.7749333333333334, 0.7789333333333334, 0.7793333333333333, 0.7806666666666666, 0.7824, 0.7838666666666667, 0.7824, 0.7854666666666666, 0.7810666666666667, 0.7874666666666666, 0.7886666666666666, 0.7848, 0.786, 0.7854666666666666, 0.7865333333333333, 0.7826666666666666, 0.7896, 0.7874666666666666, 0.7854666666666666, 0.7885333333333333, 0.7873333333333333, 0.7858666666666667, 0.7901333333333334, 0.7888, 0.7874666666666666, 0.7848, 0.7884, 0.7869333333333334, 0.7898666666666667, 0.7876, 0.7846666666666666, 0.78, 0.7876, 0.7893333333333333, 0.7853333333333333, 0.7801333333333333, 0.7908, 0.7894666666666666, 0.7876, 0.7850666666666667, 0.7853333333333333, 0.788, 0.7901333333333334, 0.7877333333333333, 0.7856, 0.7888, 0.7894666666666666, 0.7886666666666666, 0.7870666666666667, 0.7861333333333334, 0.7837333333333333, 0.7850666666666667, 0.7845333333333333, 0.7862666666666667, 0.7837333333333333, 0.7853333333333333, 0.7725333333333333, 0.7857333333333333, 0.7869333333333334, 0.7829333333333334, 0.7876, 0.7890666666666667, 0.788, 0.7874666666666666, 0.7850666666666667, 0.7832, 0.7798666666666667, 0.7802666666666667, 0.7786666666666666, 0.7844, 0.7846666666666666, 0.784, 0.7824, 0.7730666666666667, 0.7829333333333334, 0.7809333333333334, 0.7810666666666667, 0.7818666666666667, 0.7766666666666666, 0.7816, 0.7829333333333334, 0.7828, 0.7801333333333333, 0.7786666666666666, 0.7781333333333333, 0.7810666666666667, 0.7829333333333334], "seed": 342085428, "model": "residualv3", "loss_std": [0.3690442442893982, 0.27054551243782043, 0.25145992636680603, 0.23826958239078522, 0.22572988271713257, 0.21310056746006012, 0.20027665793895721, 0.18670134246349335, 0.17146050930023193, 0.15467314422130585, 0.1365939825773239, 0.11780979484319687, 0.09911686182022095, 0.08158522844314575, 0.0652252808213234, 0.04940229281783104, 0.03534096106886864, 0.023018337786197662, 0.017732681706547737, 0.015977786853909492, 0.014720109291374683, 0.011044611223042011, 0.020173832774162292, 0.00860587414354086, 0.017347121611237526, 0.00940837524831295, 0.002686313819140196, 0.001807322259992361, 0.026798252016305923, 0.0018160833278670907, 0.0015497587155550718, 0.026753129437565804, 0.005530607886612415, 0.0009887519991025329, 0.02050495520234108, 0.00690471101552248, 0.0008479667012579739, 0.0014752363786101341, 0.021959705278277397, 0.000968320993706584, 0.0011327373795211315, 0.019150909036397934, 0.0009998511523008347, 0.0009100434836000204, 0.001402703463099897, 0.02121354453265667, 0.0022124447859823704, 0.0005287929670885205, 0.0009981723269447684, 0.0014463923871517181, 0.016419941559433937, 0.003909323364496231, 0.00046546541852876544, 0.0007119970978237689, 0.01982014626264572, 0.006336720660328865, 0.00042091900832019746, 0.0006152756977826357, 0.0010552579769864678, 0.0011085121659561992, 0.014245903119444847, 0.002935161581262946, 0.012596852146089077, 0.01434120163321495, 0.0014360917266458273, 0.0003193934971932322, 0.0005682097980752587, 0.0008249758393503726, 0.0008762306533753872, 0.0008630633819848299, 0.0008266819640994072, 0.0007840909529477358, 0.0007436218438670039, 0.0006955102435313165, 0.022826461121439934, 0.010849284008145332, 0.004788098391145468, 0.004738152027130127, 0.010485067963600159, 0.011071966029703617, 0.0010276691755279899, 0.00022089207777753472, 0.0002952974464278668, 0.0004813043342437595, 0.0005414915503934026, 0.0005303595680743456, 0.0005111066275276244, 0.016062255948781967, 0.004584033042192459, 0.0004960555816069245, 0.00018032611114904284, 0.0002651659888215363, 0.013237223960459232, 0.008384482935070992, 0.00038043115637265146, 0.0001776687422534451, 0.00023706164211034775, 0.01719558984041214, 0.009357440285384655, 0.0004822917690034956, 0.0001693943195277825, 0.00021579688473138958, 0.00033492466900497675, 0.00035350184771232307, 0.022997301071882248, 0.0004325297486502677]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:37 2016", "state": "available"}], "summary": "f42a897c1d469d7f128f435b14a40393"}