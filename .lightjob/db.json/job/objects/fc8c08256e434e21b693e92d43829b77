{"content": {"hp_model": {"f0": 32, "f1": 32, "f2": 32, "f3": 32, "nonlin": "rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.5095243453979492, 1.1020272970199585, 0.9326969385147095, 0.8223440647125244, 0.7420710921287537, 0.6830819249153137, 0.6342188119888306, 0.5942991971969604, 0.5617765188217163, 0.5306380987167358, 0.5039904713630676, 0.480331152677536, 0.4591870903968811, 0.43877559900283813, 0.4210376739501953, 0.4016847610473633, 0.3864534795284271, 0.3735153079032898, 0.36073052883148193, 0.34941601753234863, 0.3367087244987488, 0.32658904790878296, 0.31610041856765747, 0.3076581358909607, 0.29663121700286865, 0.2905021011829376, 0.2812783420085907, 0.27552202343940735, 0.27044469118118286, 0.2609040439128876, 0.2541186511516571, 0.24997708201408386, 0.242159903049469, 0.23947685956954956, 0.2333693504333496, 0.23023714125156403, 0.22390234470367432, 0.2190936654806137, 0.21458026766777039, 0.21210184693336487, 0.20890063047409058, 0.20315784215927124, 0.19823215901851654, 0.19531282782554626, 0.19457146525382996, 0.19513201713562012, 0.18553012609481812, 0.18553628027439117, 0.18255281448364258, 0.17908088862895966, 0.17546045780181885, 0.1739514172077179, 0.17211365699768066, 0.16950103640556335, 0.168728306889534, 0.1637502908706665, 0.1631433516740799, 0.15903732180595398, 0.15874174237251282, 0.1538078486919403, 0.1549917310476303, 0.15284450352191925, 0.15073896944522858, 0.1472829133272171, 0.1433132141828537, 0.14554926753044128, 0.14498834311962128, 0.1406985968351364, 0.14199301600456238, 0.13867029547691345, 0.13619697093963623, 0.1345089077949524, 0.13501939177513123, 0.1311606764793396, 0.12931260466575623, 0.13095934689044952, 0.12671077251434326, 0.12856486439704895, 0.12533031404018402, 0.1260107010602951, 0.1230759471654892, 0.12413353472948074, 0.1191737949848175, 0.11888346076011658, 0.1174096092581749, 0.11771419644355774, 0.11614971607923508, 0.11518359184265137, 0.11480088531970978, 0.11245463788509369, 0.11105741560459137, 0.08213052898645401, 0.06684520095586777, 0.06279788911342621, 0.06288629025220871, 0.05943188816308975, 0.05834796652197838, 0.05798729136586189, 0.05680331215262413, 0.05613521486520767, 0.05396455526351929, 0.05246203765273094, 0.05281836539506912, 0.051707133650779724, 0.05235527455806732, 0.05146610364317894, 0.0500214621424675, 0.0489555224776268, 0.04913696274161339, 0.04916691780090332, 0.0490536130964756, 0.04896242171525955, 0.04637882113456726, 0.047454141080379486, 0.045635294169187546, 0.046500202268362045, 0.04622096195816994, 0.04690329730510712, 0.0472535640001297, 0.045628394931554794, 0.0452013835310936, 0.045213811099529266, 0.045521244406700134, 0.045976828783750534, 0.04529142379760742, 0.0450490303337574, 0.046300120651721954, 0.04526562988758087, 0.04558337852358818, 0.0465400256216526, 0.046511296182870865, 0.04643140360713005, 0.04527241364121437, 0.046011656522750854, 0.045296892523765564, 0.04629680886864662, 0.045608047395944595, 0.04451235756278038, 0.04700077325105667, 0.04480377584695816, 0.04543016478419304, 0.04502934217453003, 0.045232828706502914, 0.045048411935567856, 0.04568425193428993, 0.04485682025551796, 0.04697239026427269, 0.044858597218990326, 0.046678029000759125, 0.046036247164011, 0.04427962377667427, 0.04570791497826576, 0.04627356305718422, 0.045983392745256424, 0.0462593212723732, 0.04576200991868973, 0.04564274474978447, 0.0455072820186615, 0.04660893976688385, 0.04738267511129379, 0.04628719389438629, 0.044466763734817505, 0.04708390310406685, 0.04506997391581535, 0.04671821743249893, 0.044574350118637085, 0.047695666551589966, 0.04462624713778496, 0.046251118183135986, 0.04580247029662132, 0.04745660349726677, 0.04582735151052475, 0.04595431685447693, 0.04518800228834152, 0.0469425693154335, 0.04552239179611206, 0.046026747673749924, 0.04600796848535538, 0.04679326340556145, 0.04616203531622887, 0.04653143882751465, 0.046880923211574554, 0.04468872770667076, 0.04609202221035957, 0.0466424897313118, 0.04555828869342804, 0.046372175216674805, 0.04445325955748558, 0.04649602621793747, 0.04563751071691513, 0.04759645089507103, 0.046162109822034836, 0.04632947966456413, 0.04580676555633545, 0.047461144626140594, 0.04590412601828575, 0.04433372989296913, 0.04753384739160538, 0.04564159736037254, 0.04632604494690895, 0.04576680809259415, 0.046180982142686844, 0.047089748084545135, 0.04537207633256912, 0.046368636190891266, 0.046389926224946976, 0.04698333889245987, 0.04728049412369728, 0.04733604937791824, 0.04552468657493591, 0.04709189012646675], "moving_avg_accuracy_train": [0.05237647058823528, 0.10687999999999998, 0.16340847058823527, 0.21803232941176467, 0.27132086117647053, 0.3230923044705882, 0.3680442504941176, 0.4103292372094117, 0.45083984290023527, 0.4900476233160941, 0.5245840374550729, 0.5569679866507421, 0.5872570703386091, 0.6135901868341599, 0.6393252857978027, 0.6634892278062576, 0.6850226579668083, 0.7043815686407157, 0.7213363529531147, 0.7381533058930974, 0.7538015047155524, 0.7684237071851736, 0.7816072188195975, 0.7948159087023436, 0.8066519648909328, 0.8165208860488983, 0.8254593856793025, 0.8336193294643135, 0.8412856318119998, 0.8480135392190351, 0.8549415970618375, 0.8611697902968303, 0.8674057524436178, 0.8737592948463149, 0.8788774830087422, 0.8836062052961033, 0.8882385259429636, 0.8932076145251377, 0.8963268530726239, 0.9003035795300673, 0.9037743980476488, 0.9049357817722957, 0.9090233800656543, 0.9116151597061477, 0.9145618790296506, 0.9156798087737443, 0.9178600631904875, 0.9201446451067329, 0.9224148864784125, 0.9252886919482184, 0.9266751168710436, 0.9285440757721746, 0.9312237858420159, 0.933369642551932, 0.93493856064968, 0.9361929398788296, 0.9367618811850642, 0.9389327518900872, 0.9409994767010784, 0.9428642349133235, 0.9439848702455206, 0.9449722655739097, 0.9460279801929894, 0.9480251821736905, 0.9483379580739685, 0.9488688681489246, 0.9496055107457968, 0.9503273126123936, 0.951428698998213, 0.9532011232160388, 0.9538739520709055, 0.9551053803932268, 0.9550607247068453, 0.956135828706749, 0.9565198928948976, 0.957098491840702, 0.9574262897154553, 0.9588483666262627, 0.9602294123165777, 0.961227647555508, 0.9619754710352513, 0.962027335696432, 0.9629728374209064, 0.963012024267051, 0.9627931747815224, 0.9626197396563113, 0.9630589421612684, 0.9622871655922004, 0.9631784490329803, 0.9633476629532117, 0.9642223084225965, 0.9671224305215133, 0.9698878345281854, 0.972369639310661, 0.9746432636148891, 0.9767695254886942, 0.9786549258810012, 0.9803753156458423, 0.9819542546694935, 0.9833258880260735, 0.9846097698117015, 0.9857205575364136, 0.9867932076651252, 0.9877562398397891, 0.9885617923263984, 0.9893338483878762, 0.9900804635490886, 0.9907218289588856, 0.9912919990041734, 0.9918286814566972, 0.9922575780169098, 0.9926835849211012, 0.9931469911348734, 0.9935664096684449, 0.9939485922310122, 0.9943019683020285, 0.9946105950012375, 0.9948930649128784, 0.9951402290098259, 0.9953603237559021, 0.9955654678509002, 0.9957548034187513, 0.9959228524886409, 0.996071743710365, 0.9961963340452108, 0.9963108182877485, 0.9964232658707384, 0.9965127039895468, 0.9966096688847097, 0.9966922314080034, 0.9967665376789678, 0.9968287074404828, 0.9968987778729052, 0.9969524294973793, 0.9970007159594061, 0.997022997304642, 0.9970642269859424, 0.9970989807579365, 0.997130259152731, 0.9971537038256932, 0.9971700981490063, 0.9971919118635174, 0.9972233089124598, 0.9972327427270962, 0.9972459390426219, 0.9972601686677716, 0.9972894459186414, 0.9972946189738361, 0.9972992747235112, 0.9973105237217483, 0.9973206478201617, 0.9973274065675574, 0.9973358423813898, 0.9973504934373685, 0.997351914681867, 0.9973626055666215, 0.9973628155981946, 0.9973677105089634, 0.9973721159286554, 0.997380786688731, 0.9973862374316226, 0.9973864372178721, 0.997393675849026, 0.9973907788523588, 0.9973999362612406, 0.9974081779292341, 0.9974061836657224, 0.9974090947109149, 0.9974093617104116, 0.9974048961276057, 0.9974079359266098, 0.9974036129221842, 0.9973903104534951, 0.9973971617610867, 0.9974056808790956, 0.9974039363205979, 0.9974023662179499, 0.9974103648902725, 0.9974175636953628, 0.9974146308552383, 0.9974143442403027, 0.9974164392280371, 0.9974324423640569, 0.9974374334217688, 0.9974348665501802, 0.9974396151892798, 0.9974344771997636, 0.997434558891552, 0.9974252206494556, 0.9974215221139219, 0.997415840490765, 0.9974083740887474, 0.9974110660916373, 0.9974158418354148, 0.9974107282401086, 0.9974037730631566, 0.9974092781097821, 0.9974095267693921, 0.9974121035042176, 0.9974144225655606, 0.9974188626619457, 0.9974181528663394, 0.9974151611091172, 0.9974171744099701, 0.9974307510866202, 0.9974312053897229, 0.9974269083801623, 0.9974371587186167, 0.9974369722585197, 0.9974320985620795, 0.9974347710588127], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.050879999999999995, 0.10337866666666665, 0.15742746666666663, 0.2096313866666666, 0.2608549146666666, 0.31052942319999993, 0.35308981421333324, 0.3932608327919999, 0.4310014161794666, 0.46774127456151987, 0.49931381377203454, 0.5287557657281644, 0.5563335224886813, 0.5799535035731465, 0.6026648198824984, 0.6234116712275819, 0.6418971707714904, 0.6585874536943414, 0.6733287083249072, 0.6873158374924164, 0.7004375870765082, 0.7124338283688574, 0.7229771121986384, 0.7335460676454412, 0.7431781275475637, 0.7509536481261407, 0.7582849499801934, 0.7648831216488406, 0.7709014761506232, 0.7757179952022276, 0.7810128623486715, 0.7855782427804711, 0.7901937518357572, 0.7950810433188482, 0.7987196056536301, 0.8018876450882672, 0.8052588805794404, 0.8089463258548297, 0.8110383599360135, 0.8134411906090787, 0.8158570715481708, 0.8164313643933537, 0.8194682279540183, 0.8219880718252832, 0.8242959313094216, 0.8251196715118128, 0.8269143710272981, 0.828489600591235, 0.8300806405321115, 0.8316459098122336, 0.8325346521643436, 0.8332278536145759, 0.8349984015864517, 0.8364585614278064, 0.8379727052850258, 0.8384554347565232, 0.8392098912808709, 0.8409555688194504, 0.8419400119375053, 0.8433860107437547, 0.844514076336046, 0.8449960020357747, 0.8461897351655305, 0.8474507616489775, 0.8480390188174132, 0.8485417836023386, 0.8486209385754381, 0.8488121780512276, 0.8493842935794381, 0.851112530888161, 0.8517612777993449, 0.852411816686077, 0.8519706350174693, 0.852560238182389, 0.8528375476974834, 0.853313792927735, 0.8533157469682948, 0.8550375056047986, 0.8556937550443188, 0.8568977128732203, 0.8574612749192315, 0.8573684807606418, 0.8582449660179109, 0.8583271360827865, 0.8580277558078413, 0.8580783135603905, 0.8584704822043514, 0.8575967673172497, 0.8583970905855247, 0.858224048193639, 0.859161643374275, 0.8620721457035142, 0.8648382644664961, 0.8670344380198465, 0.8690643275511952, 0.8709178947960756, 0.8725594386498015, 0.8741434947848212, 0.875609145306339, 0.8769148974423718, 0.878156741031468, 0.8791677335949878, 0.8800776269021556, 0.8810031975452733, 0.8817295444574126, 0.882303256678338, 0.8830329310105042, 0.8833696379094537, 0.8841260074518417, 0.8844467400399908, 0.8849620660359917, 0.8854391927657259, 0.8858686068224867, 0.8863217461402381, 0.8867295715262142, 0.8870699477069262, 0.8874162862695669, 0.8876613243092769, 0.8879218585450159, 0.8881696726905144, 0.8883527054214629, 0.8886241015459834, 0.8887883580580518, 0.8889095222522466, 0.8890052366936886, 0.8891313796909863, 0.8893115750552211, 0.889380417549699, 0.8894823757947291, 0.8895874715485895, 0.8896287243937305, 0.889692518621024, 0.889736600092255, 0.8898029400830295, 0.8897826460747265, 0.8897643814672538, 0.8897479433205283, 0.8898531489884755, 0.8898411674229613, 0.8898970506806652, 0.8899473456125987, 0.8899926110513389, 0.8900066832795382, 0.8900326816182511, 0.8899894134564259, 0.8900438054441167, 0.890039424899705, 0.8900754824097346, 0.8900412675020944, 0.890037140751885, 0.8900600933433631, 0.8900140840090267, 0.8898793422747907, 0.8899314080473116, 0.8899116005759138, 0.8899337738516557, 0.8899537297998235, 0.8900116901531745, 0.8899971878045237, 0.8899974690240714, 0.8901443887883309, 0.8901299499094978, 0.890116954918548, 0.8901719260933598, 0.8902347334840238, 0.8902645934689548, 0.8901714674553927, 0.8901276540431867, 0.8902082219722014, 0.8902673997749813, 0.8902139931308165, 0.8902192604844015, 0.8901706677692947, 0.8901669343256985, 0.890123574226462, 0.8901245501371491, 0.8900987617901008, 0.8901555522777574, 0.8900999970499817, 0.8901433306783169, 0.8901956642771518, 0.8902160978494367, 0.890234488064493, 0.8901177059247104, 0.890092601998906, 0.8900833417990154, 0.8900750076191138, 0.8901608401905359, 0.8901580895048156, 0.8901556138876673, 0.8901533858322338, 0.890124713915677, 0.890098909190776, 0.8900890182716984, 0.8900001164445286, 0.8900001048000757, 0.8900400943200681, 0.8900360848880613, 0.8901124763992552, 0.8901412287593297, 0.8901137725500634, 0.8901290619617237, 0.890102822432218, 0.8900258735223295, 0.8900499528367631, 0.8900182908864201, 0.8900831284644448, 0.8901148156180003, 0.8901433340562003, 0.8902090006505802, 0.8902147672521887], "moving_var_accuracy_train": [0.024689652041522483, 0.04895639930242213, 0.07281997125558476, 0.09239186770498213, 0.10870968949322653, 0.12196126161070822, 0.12795123251135812, 0.1312482901738357, 0.1328934437173886, 0.13343934975189356, 0.13083028989091572, 0.1271857423913933, 0.12272402546810944, 0.11669252014061277, 0.11098392599456791, 0.10514059823560293, 0.09879973594235647, 0.09229266915044376, 0.08565058463511914, 0.07963081532727755, 0.07387152893203353, 0.06840865528439355, 0.06313203456708909, 0.058389056506147216, 0.053810980890427455, 0.04930644324478397, 0.04509486990109021, 0.041184645054152036, 0.03759513027391209, 0.03424299988921965, 0.03125068176955663, 0.028474727111352648, 0.025977239415282887, 0.023742822983320422, 0.021604303335582484, 0.0196451203322631, 0.01787373385021474, 0.016308587037230714, 0.01476529517555276, 0.013431094837853463, 0.012196404584705998, 0.010988903435638272, 0.010040389230345262, 0.009096806202654621, 0.008265273975332707, 0.007449994480014002, 0.0067477766159081565, 0.006119972785105659, 0.005554361469566266, 0.00507325414351421, 0.004583228295762467, 0.004156342532553269, 0.0038053358938236258, 0.003466244613616689, 0.0031417736882319905, 0.00284175752466349, 0.0025604950200866015, 0.0023468596346392845, 0.0021506158341746584, 0.0019668501594684125, 0.001781467555451487, 0.0016120953457170603, 0.0014609166113578, 0.001350724291987466, 0.0012165323216628719, 0.0010974158790657937, 0.0009925580719989547, 0.0008979912462106628, 0.0008191095893274128, 0.0007654720188660885, 0.000692999104990951, 0.0006373469359089894, 0.0005736301894910264, 0.0005266698080374042, 0.0004753303749392283, 0.00043081032810607866, 0.00038869635831570605, 0.00036802744714639943, 0.00034839028722039587, 0.0003225195208285395, 0.00029530072835738425, 0.0002657948650093604, 0.00024726114010728176, 0.00022254884657675047, 0.00020072501779492074, 0.00018092323369934144, 0.00016456699989265264, 0.00015347105155644896, 0.0001452734219470811, 0.00013100377990957364, 0.00012478844419265295, 0.00018800597347101933, 0.0002380325100049855, 0.00026966345380935496, 0.00028922141571940637, 0.0003009881801514448, 0.00030288197389010443, 0.0002992314449878227, 0.0002917457364527138, 0.00027950356539139024, 0.00026638838080745604, 0.0002508541870510523, 0.00023612397303357182, 0.00022085845445515644, 0.0002046128422877831, 0.00018951619311758646, 0.0001755814815963975, 0.00016172547973671472, 0.00014847877668793587, 0.00013622315151276535, 0.00012425640669574888, 0.00011346410296794241, 0.00010405040054181264, 9.52285676443607e-05, 8.702028248009906e-05, 7.944212606019228e-05, 7.235516740935434e-05, 6.583775392726098e-05, 5.9803789351913514e-05, 5.4259385691975445e-05, 4.9212204020190915e-05, 4.46136152334535e-05, 4.040641811912492e-05, 3.6565293670371014e-05, 3.304846906716694e-05, 2.986158193655523e-05, 2.6989223873182056e-05, 2.436229407972791e-05, 2.2010684389800814e-05, 1.9870965083094394e-05, 1.7933561371926618e-05, 1.6174990947955428e-05, 1.4601680642658527e-05, 1.3167419049671156e-05, 1.1871661386439614e-05, 1.068896337290533e-05, 9.635366015196076e-06, 8.68269983568676e-06, 7.82323489394642e-06, 7.045858278764515e-06, 6.343691415420093e-06, 5.713604817145047e-06, 5.151116307571149e-06, 4.636805648541375e-06, 4.174692368378315e-06, 3.7590454716275597e-06, 3.3908553412313007e-06, 3.052010651608596e-06, 2.747004670493082e-06, 2.4734430630958205e-06, 2.2270212331043967e-06, 2.0047302357911742e-06, 1.804897678807206e-06, 1.6263397918980955e-06, 1.463723992131605e-06, 1.3183802480699453e-06, 1.1865426202823061e-06, 1.0681039996169832e-06, 9.614682691592399e-07, 8.659980809659193e-07, 7.796656682519614e-07, 7.016994606576748e-07, 6.320010946207554e-07, 5.688765184658939e-07, 5.127435898561553e-07, 4.6208055669238974e-07, 4.1590829480573546e-07, 3.743937329821752e-07, 3.36955001282539e-07, 3.034389740224497e-07, 2.731782400220748e-07, 2.4602861132524814e-07, 2.2301835125172008e-07, 2.0113897986798835e-07, 1.8167826022604775e-07, 1.635378255626131e-07, 1.4720623000727965e-07, 1.3306141583688012e-07, 1.202216794057588e-07, 1.0827692542594651e-07, 9.744997221644363e-08, 8.774447575726627e-08, 8.127493144375841e-08, 7.337163421313952e-08, 6.60937702595992e-08, 5.968733939332599e-08, 5.395619588041199e-08, 4.8560636354305336e-08, 4.448939760792827e-08, 4.016357033298816e-08, 3.643774087496231e-08, 3.3295691219272134e-08, 3.003134401338148e-08, 2.7233479169693228e-08, 2.4745470965322612e-08, 2.2706294246697263e-08, 2.0708414667169166e-08, 1.8638129684867172e-08, 1.683407277762888e-08, 1.519906790947717e-08, 1.3856591221711579e-08, 1.2475466387765407e-08, 1.1308475250476518e-08, 1.0214108148350768e-08, 1.0851632673256384e-08, 9.768326927712792e-09, 8.9576728554101e-09, 9.007530515728245e-09, 8.107090370465243e-09, 7.510157586339651e-09, 6.823421976807501e-09], "duration": 114144.264099, "accuracy_train": [0.5237647058823529, 0.5974117647058823, 0.6721647058823529, 0.7096470588235294, 0.7509176470588236, 0.7890352941176471, 0.7726117647058823, 0.7908941176470589, 0.815435294117647, 0.8429176470588235, 0.8354117647058823, 0.8484235294117647, 0.8598588235294118, 0.8505882352941176, 0.8709411764705882, 0.880964705882353, 0.8788235294117647, 0.8786117647058823, 0.8739294117647058, 0.8895058823529411, 0.8946352941176471, 0.9000235294117647, 0.9002588235294118, 0.9136941176470589, 0.9131764705882353, 0.9053411764705882, 0.9059058823529412, 0.9070588235294118, 0.9102823529411764, 0.9085647058823529, 0.9172941176470588, 0.9172235294117647, 0.9235294117647059, 0.9309411764705883, 0.9249411764705883, 0.9261647058823529, 0.9299294117647059, 0.9379294117647059, 0.9244, 0.9360941176470589, 0.9350117647058823, 0.9153882352941176, 0.9458117647058824, 0.9349411764705883, 0.9410823529411765, 0.9257411764705883, 0.9374823529411764, 0.9407058823529412, 0.9428470588235294, 0.9511529411764705, 0.9391529411764706, 0.945364705882353, 0.9553411764705882, 0.9526823529411764, 0.9490588235294117, 0.9474823529411764, 0.9418823529411765, 0.9584705882352941, 0.9596, 0.9596470588235294, 0.9540705882352941, 0.9538588235294118, 0.9555294117647058, 0.966, 0.9511529411764705, 0.9536470588235294, 0.9562352941176471, 0.9568235294117647, 0.9613411764705883, 0.9691529411764706, 0.9599294117647059, 0.9661882352941177, 0.9546588235294118, 0.9658117647058824, 0.9599764705882353, 0.9623058823529411, 0.9603764705882353, 0.9716470588235294, 0.9726588235294118, 0.9702117647058823, 0.9687058823529412, 0.9624941176470588, 0.9714823529411765, 0.9633647058823529, 0.9608235294117647, 0.9610588235294117, 0.9670117647058823, 0.9553411764705882, 0.9712, 0.9648705882352941, 0.9720941176470588, 0.9932235294117647, 0.9947764705882353, 0.9947058823529412, 0.9951058823529412, 0.9959058823529412, 0.9956235294117647, 0.9958588235294118, 0.996164705882353, 0.9956705882352941, 0.996164705882353, 0.9957176470588235, 0.9964470588235295, 0.9964235294117647, 0.9958117647058824, 0.9962823529411765, 0.9968, 0.9964941176470589, 0.9964235294117647, 0.9966588235294118, 0.9961176470588236, 0.9965176470588235, 0.9973176470588235, 0.9973411764705883, 0.9973882352941177, 0.9974823529411765, 0.9973882352941177, 0.9974352941176471, 0.9973647058823529, 0.9973411764705883, 0.9974117647058823, 0.9974588235294117, 0.9974352941176471, 0.9974117647058823, 0.9973176470588235, 0.9973411764705883, 0.9974352941176471, 0.9973176470588235, 0.9974823529411765, 0.9974352941176471, 0.9974352941176471, 0.9973882352941177, 0.9975294117647059, 0.9974352941176471, 0.9974352941176471, 0.9972235294117647, 0.9974352941176471, 0.9974117647058823, 0.9974117647058823, 0.9973647058823529, 0.9973176470588235, 0.9973882352941177, 0.9975058823529411, 0.9973176470588235, 0.9973647058823529, 0.9973882352941177, 0.9975529411764706, 0.9973411764705883, 0.9973411764705883, 0.9974117647058823, 0.9974117647058823, 0.9973882352941177, 0.9974117647058823, 0.9974823529411765, 0.9973647058823529, 0.9974588235294117, 0.9973647058823529, 0.9974117647058823, 0.9974117647058823, 0.9974588235294117, 0.9974352941176471, 0.9973882352941177, 0.9974588235294117, 0.9973647058823529, 0.9974823529411765, 0.9974823529411765, 0.9973882352941177, 0.9974352941176471, 0.9974117647058823, 0.9973647058823529, 0.9974352941176471, 0.9973647058823529, 0.9972705882352941, 0.9974588235294117, 0.9974823529411765, 0.9973882352941177, 0.9973882352941177, 0.9974823529411765, 0.9974823529411765, 0.9973882352941177, 0.9974117647058823, 0.9974352941176471, 0.9975764705882353, 0.9974823529411765, 0.9974117647058823, 0.9974823529411765, 0.9973882352941177, 0.9974352941176471, 0.9973411764705883, 0.9973882352941177, 0.9973647058823529, 0.9973411764705883, 0.9974352941176471, 0.9974588235294117, 0.9973647058823529, 0.9973411764705883, 0.9974588235294117, 0.9974117647058823, 0.9974352941176471, 0.9974352941176471, 0.9974588235294117, 0.9974117647058823, 0.9973882352941177, 0.9974352941176471, 0.9975529411764706, 0.9974352941176471, 0.9973882352941177, 0.9975294117647059, 0.9974352941176471, 0.9973882352941177, 0.9974588235294117], "end": "2016-02-05 16:17:52.627000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0], "moving_var_accuracy_valid": [0.0232989696, 0.045774062655999995, 0.06748811142336, 0.08526654365132161, 0.10035453767309051, 0.11252709508803532, 0.11757686752810222, 0.12034257737812025, 0.12112748435014516, 0.12116309066053066, 0.11801820868427298, 0.11401784463072923, 0.10946085417913641, 0.10353590031909721, 0.09782454528371844, 0.09191597732196152, 0.08579980283025594, 0.07972691244363364, 0.07370996249201889, 0.0680997242839543, 0.06283937466488737, 0.05785062544469701, 0.05306601040546519, 0.048764734738047204, 0.04472325046586518, 0.040795053901689526, 0.03719928039339769, 0.03387117517837839, 0.030810042978722696, 0.027937828382820634, 0.02539636610742499, 0.02304431378306601, 0.020931608719314272, 0.019053418409749084, 0.017267228791551015, 0.01563083417713065, 0.014170037818050102, 0.012875409310176009, 0.011627257838529916, 0.010516494411867737, 0.009517373297087782, 0.008568604277827258, 0.007794746712619366, 0.007072418559577388, 0.006413112642206395, 0.005777908309275074, 0.005229105995505519, 0.004728527529566873, 0.004278457449451364, 0.003872662315779876, 0.003492504850917794, 0.0031475791200814523, 0.0028610347691597265, 0.0025941198931045024, 0.0023553415883772483, 0.0021219046792233932, 0.001914837053125231, 0.0017507798584310192, 0.0015844240268620892, 0.0014447998371049544, 0.0013117726412190606, 0.001182685648517686, 0.0010772420727316078, 0.0009838295555860382, 0.0008885610184933771, 0.0008019798685046887, 0.0007218382712421173, 0.0006499835969518079, 0.0005879310828552041, 0.0005560192123270384, 0.0005042051440872705, 0.0004575934372669001, 0.0004135858649226494, 0.0003753559654591348, 0.0003385124740176785, 0.00030670251228994777, 0.0002760322954254236, 0.00027510914110426185, 0.00025147419693567015, 0.00023937240732606113, 0.00021829358621079515, 0.00019654172439253118, 0.00018380158960916966, 0.00016548219792430758, 0.0001497406350731139, 0.00013478957634288795, 0.0001226947849163547, 0.00011729570576020918, 0.00011133079118787019, 0.00010046720509358922, 9.833224708899884e-05, 0.00016473823665665819, 0.0002171271300892778, 0.00023882302156827122, 0.0002520247829967547, 0.00025774370847872344, 0.00025622133364419463, 0.00025318230482982156, 0.0002471972574078702, 0.00023782242943387035, 0.00022791976598849609, 0.00021432674306107877, 0.00020034522122883115, 0.00018802082824456102, 0.00017396696395107405, 0.0001595325789679191, 0.00014837114275032687, 0.00013455437229749638, 0.0001262477890296164, 0.00011454883466456252, 0.0001054839991374953, 9.698444846978715e-05, 8.894557151210192e-05, 8.189903153252096e-05, 7.520602228828854e-05, 6.872812355902392e-05, 6.293486480286982e-05, 5.7181771090727016e-05, 5.207449677358312e-05, 4.741975375260674e-05, 4.2979287202732465e-05, 3.93442611901013e-05, 3.5652656886902833e-05, 3.2219518055806476e-05, 2.9080017538930834e-05, 2.631522428694323e-05, 2.3975935181873997e-05, 2.1620995265100047e-05, 1.95524550921566e-05, 1.7696615640256345e-05, 1.5942270251320794e-05, 1.4384670557112622e-05, 1.2963692086354308e-05, 1.1706931827102522e-05, 1.0539945265349273e-05, 9.488953101789516e-06, 8.542489705620451e-06, 7.787854828172306e-06, 7.010361366564617e-06, 6.337431676332529e-06, 5.726454730303015e-06, 5.172249896771738e-06, 4.65680715555304e-06, 4.197209662540184e-06, 3.794337900735682e-06, 3.441530505586658e-06, 3.0975501575520755e-06, 2.7994964380626114e-06, 2.5300827333997467e-06, 2.2772277306653922e-06, 2.0542463506989262e-06, 1.8678734452436682e-06, 1.8444841152236228e-06, 1.6844333057150679e-06, 1.519520998452142e-06, 1.3719937860210867e-06, 1.2383785662244477e-06, 1.1447753326471371e-06, 1.0321906624299228e-06, 9.289723079468362e-07, 1.0303438313228193e-06, 9.291857791881467e-07, 8.377870293773954e-07, 7.812047969814333e-07, 7.385872321814985e-07, 6.727530772640345e-07, 6.835298591554313e-07, 6.324534090420273e-07, 6.2762878880923e-07, 5.963840210049793e-07, 5.624160456729822e-07, 5.064241462297903e-07, 4.77032999259875e-07, 4.294551467436569e-07, 4.034305159214951e-07, 3.630960359443689e-07, 3.3277178194125545e-07, 3.285210391415867e-07, 3.234463852263652e-07, 3.080019768059373e-07, 3.0185102922853716e-07, 2.754237041925519e-07, 2.50925133861659e-07, 3.485752340253918e-07, 3.1938957443997527e-07, 2.8822237871410263e-07, 2.60025267834373e-07, 3.003278139030679e-07, 2.703631289601454e-07, 2.433819741865119e-07, 2.1908845484698908e-07, 2.045783185536037e-07, 1.901134411432074e-07, 1.7198256955068355e-07, 2.259161264628439e-07, 2.0332451503689905e-07, 1.9738451891622941e-07, 1.7779074692976163e-07, 2.1253263907913945e-07, 1.9871965905989812e-07, 1.856322839993816e-07, 1.6917295057971364e-07, 1.584522716998685e-07, 1.9589725712709944e-07, 1.8152585186677008e-07, 1.7239557857578243e-07, 1.9299122443513464e-07, 1.8272878329567874e-07, 1.717756168223981e-07, 1.9340696969734221e-07, 1.7436555597462423e-07], "accuracy_test": 0.8906, "start": "2016-02-04 08:35:28.363000", "learning_rate_per_epoch": [0.0005684653297066689, 0.0005684653297066689, 0.0005684653297066689, 0.0005684653297066689, 0.0005684653297066689, 0.0005684653297066689, 0.0005684653297066689, 0.0005684653297066689, 0.0005684653297066689, 0.0005684653297066689, 0.0005684653297066689, 0.0005684653297066689, 0.0005684653297066689, 0.0005684653297066689, 0.0005684653297066689, 0.0005684653297066689, 0.0005684653297066689, 0.0005684653297066689, 0.0005684653297066689, 0.0005684653297066689, 0.0005684653297066689, 0.0005684653297066689, 0.0005684653297066689, 0.0005684653297066689, 0.0005684653297066689, 0.0005684653297066689, 0.0005684653297066689, 0.0005684653297066689, 0.0005684653297066689, 0.0005684653297066689, 0.0005684653297066689, 0.0005684653297066689, 0.0005684653297066689, 0.0005684653297066689, 0.0005684653297066689, 0.0005684653297066689, 0.0005684653297066689, 0.0005684653297066689, 0.0005684653297066689, 0.0005684653297066689, 0.0005684653297066689, 0.0005684653297066689, 0.0005684653297066689, 0.0005684653297066689, 0.0005684653297066689, 0.0005684653297066689, 0.0005684653297066689, 0.0005684653297066689, 0.0005684653297066689, 0.0005684653297066689, 0.0005684653297066689, 0.0005684653297066689, 0.0005684653297066689, 0.0005684653297066689, 0.0005684653297066689, 0.0005684653297066689, 0.0005684653297066689, 0.0005684653297066689, 0.0005684653297066689, 0.0005684653297066689, 0.0005684653297066689, 0.0005684653297066689, 0.0005684653297066689, 0.0005684653297066689, 0.0005684653297066689, 0.0005684653297066689, 0.0005684653297066689, 0.0005684653297066689, 0.0005684653297066689, 0.0005684653297066689, 0.0005684653297066689, 0.0005684653297066689, 0.0005684653297066689, 0.0005684653297066689, 0.0005684653297066689, 0.0005684653297066689, 0.0005684653297066689, 0.0005684653297066689, 0.0005684653297066689, 0.0005684653297066689, 0.0005684653297066689, 0.0005684653297066689, 0.0005684653297066689, 0.0005684653297066689, 0.0005684653297066689, 0.0005684653297066689, 0.0005684653297066689, 0.0005684653297066689, 0.0005684653297066689, 0.0005684653297066689, 5.6846532970666885e-05, 5.6846532970666885e-05, 5.6846532970666885e-05, 5.6846532970666885e-05, 5.6846532970666885e-05, 5.6846532970666885e-05, 5.6846532970666885e-05, 5.6846532970666885e-05, 5.6846532970666885e-05, 5.6846532970666885e-05, 5.6846532970666885e-05, 5.6846532970666885e-05, 5.6846532970666885e-05, 5.6846532970666885e-05, 5.6846532970666885e-05, 5.6846532970666885e-05, 5.6846532970666885e-05, 5.6846532970666885e-05, 5.6846532970666885e-05, 5.6846532970666885e-05, 5.6846532970666885e-05, 5.684653388016159e-06, 5.684653388016159e-07, 5.684653459070432e-08, 5.684653636706116e-09, 5.684653858750721e-10, 5.684653789361782e-11, 5.684653702625608e-12, 5.684653594205391e-13, 5.684653594205391e-14, 5.684653678908686e-15, 5.684653573029567e-16, 5.684653837727363e-17, 5.684654003163486e-18, 5.684654003163486e-19, 5.684654003163486e-20, 5.684654164722199e-21, 5.684654366670591e-22, 5.684654114235101e-23, 5.68465395646292e-24, 5.68465395646292e-25, 5.684653709943887e-26, 5.684653786981085e-27, 5.684653786981085e-28, 5.684653666610464e-29, 5.6846538170737406e-30, 5.684654005152837e-31, 5.684654240251707e-32, 5.684654313720104e-33, 5.6846544055556e-34, 5.68465452034997e-35, 5.68465452034997e-36, 5.6846544306668685e-37, 5.684654206459114e-38, 5.684654486718807e-39, 5.6846474802264854e-40, 5.684647480226485e-41, 5.685067869765783e-42, 5.689271765158757e-43, 5.74532370373175e-44, 5.605193857299268e-45, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "accuracy_train_first": 0.5237647058823529, "accuracy_train_last": 0.9974588235294117, "batch_size_eval": 1024, "accuracy_train_std": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "accuracy_test_std": 0, "error_valid": [0.49119999999999997, 0.42413333333333336, 0.3561333333333333, 0.32053333333333334, 0.27813333333333334, 0.24239999999999995, 0.2638666666666667, 0.24519999999999997, 0.22933333333333328, 0.2016, 0.21653333333333336, 0.2062666666666667, 0.19546666666666668, 0.2074666666666667, 0.1929333333333333, 0.18986666666666663, 0.1917333333333333, 0.19120000000000004, 0.19399999999999995, 0.18679999999999997, 0.18146666666666667, 0.17959999999999998, 0.18213333333333337, 0.17133333333333334, 0.17013333333333336, 0.1790666666666667, 0.1757333333333333, 0.1757333333333333, 0.17493333333333339, 0.18093333333333328, 0.17133333333333334, 0.17333333333333334, 0.16826666666666668, 0.16093333333333337, 0.1685333333333333, 0.16959999999999997, 0.1644, 0.1578666666666667, 0.17013333333333336, 0.16493333333333338, 0.1624, 0.1784, 0.1532, 0.15533333333333332, 0.15493333333333337, 0.16746666666666665, 0.15693333333333337, 0.15733333333333333, 0.15559999999999996, 0.15426666666666666, 0.15946666666666665, 0.1605333333333333, 0.14906666666666668, 0.15039999999999998, 0.14839999999999998, 0.1572, 0.15400000000000003, 0.1433333333333333, 0.1492, 0.14359999999999995, 0.14533333333333331, 0.15066666666666662, 0.14306666666666668, 0.1412, 0.1466666666666666, 0.14693333333333336, 0.15066666666666662, 0.14946666666666664, 0.14546666666666663, 0.1333333333333333, 0.14239999999999997, 0.14173333333333338, 0.15200000000000002, 0.14213333333333333, 0.14466666666666672, 0.14239999999999997, 0.1466666666666666, 0.12946666666666662, 0.13839999999999997, 0.13226666666666664, 0.13746666666666663, 0.14346666666666663, 0.1338666666666667, 0.14093333333333335, 0.14466666666666672, 0.14146666666666663, 0.138, 0.15026666666666666, 0.13439999999999996, 0.1433333333333333, 0.13239999999999996, 0.11173333333333335, 0.11026666666666662, 0.11319999999999997, 0.11266666666666669, 0.11240000000000006, 0.11266666666666669, 0.11160000000000003, 0.11119999999999997, 0.11133333333333328, 0.11066666666666669, 0.11173333333333335, 0.11173333333333335, 0.11066666666666669, 0.11173333333333335, 0.11253333333333337, 0.11040000000000005, 0.11360000000000003, 0.10906666666666665, 0.11266666666666669, 0.11040000000000005, 0.11026666666666662, 0.11026666666666662, 0.10960000000000003, 0.10960000000000003, 0.10986666666666667, 0.10946666666666671, 0.1101333333333333, 0.10973333333333335, 0.10960000000000003, 0.10999999999999999, 0.10893333333333333, 0.10973333333333335, 0.10999999999999999, 0.1101333333333333, 0.10973333333333335, 0.10906666666666665, 0.10999999999999999, 0.10960000000000003, 0.10946666666666671, 0.10999999999999999, 0.10973333333333335, 0.10986666666666667, 0.10960000000000003, 0.11040000000000005, 0.11040000000000005, 0.11040000000000005, 0.10919999999999996, 0.11026666666666662, 0.10960000000000003, 0.10960000000000003, 0.10960000000000003, 0.10986666666666667, 0.10973333333333335, 0.11040000000000005, 0.10946666666666671, 0.10999999999999999, 0.10960000000000003, 0.11026666666666662, 0.10999999999999999, 0.10973333333333335, 0.11040000000000005, 0.11133333333333328, 0.10960000000000003, 0.11026666666666662, 0.10986666666666667, 0.10986666666666667, 0.10946666666666671, 0.1101333333333333, 0.10999999999999999, 0.10853333333333337, 0.10999999999999999, 0.10999999999999999, 0.10933333333333328, 0.10919999999999996, 0.10946666666666671, 0.11066666666666669, 0.11026666666666662, 0.10906666666666665, 0.10919999999999996, 0.11026666666666662, 0.10973333333333335, 0.11026666666666662, 0.10986666666666667, 0.11026666666666662, 0.10986666666666667, 0.1101333333333333, 0.10933333333333328, 0.11040000000000005, 0.10946666666666671, 0.10933333333333328, 0.10960000000000003, 0.10960000000000003, 0.11093333333333333, 0.1101333333333333, 0.10999999999999999, 0.10999999999999999, 0.10906666666666665, 0.10986666666666667, 0.10986666666666667, 0.10986666666666667, 0.1101333333333333, 0.1101333333333333, 0.10999999999999999, 0.11080000000000001, 0.10999999999999999, 0.10960000000000003, 0.10999999999999999, 0.10919999999999996, 0.10960000000000003, 0.1101333333333333, 0.10973333333333335, 0.1101333333333333, 0.11066666666666669, 0.10973333333333335, 0.11026666666666662, 0.10933333333333328, 0.10960000000000003, 0.10960000000000003, 0.10919999999999996, 0.10973333333333335], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.08160794086402309, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 32, "valid_ratio": 0.15, "learning_rate": 0.0005684653049458449, "optimization": "rmsprop", "nb_data_augmentation": 1, "learning_rate_decay_method": "discrete", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 3.3433187991276586e-08, "rotation_range": [0, 0], "momentum": 0.7637066433966516}, "accuracy_valid_max": 0.8914666666666666, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        y_pred = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            y_pred.extend((nnet.predict(X[mini_batch]) == y[mini_batch]).tolist())\n        return np.mean(y_pred)\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            acc = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = acc\n            status[\"accuracy_train_std\"] = 0\n            acc = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = acc\n            status[\"accuracy_valid_std\"] = 0\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n\n    # rescaling to [-1, 1]\n    X_min = X_train.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X_train.max(axis=(0, 2, 3))[None, :, None, None]\n    def preprocess(a):\n        return (a / 255.) * 2 - 1\n        # return 2 * ((a - X_min) / (X_max - X_min)) - 1\n    X_train = preprocess(X_train)\n    X_valid = preprocess(X_valid)\n\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = preprocess(X_test)\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    acc = evaluate(X_test, y_test, batch_size_eval)\n    light.set(\"accuracy_test\", acc)\n    light.set(\"accuracy_test_std\", 0)\n    print(\"Test accuracy : {}+-{}\".format(acc, 0))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8902666666666667, "accuracy_valid_std": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "accuracy_valid": [0.5088, 0.5758666666666666, 0.6438666666666667, 0.6794666666666667, 0.7218666666666667, 0.7576, 0.7361333333333333, 0.7548, 0.7706666666666667, 0.7984, 0.7834666666666666, 0.7937333333333333, 0.8045333333333333, 0.7925333333333333, 0.8070666666666667, 0.8101333333333334, 0.8082666666666667, 0.8088, 0.806, 0.8132, 0.8185333333333333, 0.8204, 0.8178666666666666, 0.8286666666666667, 0.8298666666666666, 0.8209333333333333, 0.8242666666666667, 0.8242666666666667, 0.8250666666666666, 0.8190666666666667, 0.8286666666666667, 0.8266666666666667, 0.8317333333333333, 0.8390666666666666, 0.8314666666666667, 0.8304, 0.8356, 0.8421333333333333, 0.8298666666666666, 0.8350666666666666, 0.8376, 0.8216, 0.8468, 0.8446666666666667, 0.8450666666666666, 0.8325333333333333, 0.8430666666666666, 0.8426666666666667, 0.8444, 0.8457333333333333, 0.8405333333333334, 0.8394666666666667, 0.8509333333333333, 0.8496, 0.8516, 0.8428, 0.846, 0.8566666666666667, 0.8508, 0.8564, 0.8546666666666667, 0.8493333333333334, 0.8569333333333333, 0.8588, 0.8533333333333334, 0.8530666666666666, 0.8493333333333334, 0.8505333333333334, 0.8545333333333334, 0.8666666666666667, 0.8576, 0.8582666666666666, 0.848, 0.8578666666666667, 0.8553333333333333, 0.8576, 0.8533333333333334, 0.8705333333333334, 0.8616, 0.8677333333333334, 0.8625333333333334, 0.8565333333333334, 0.8661333333333333, 0.8590666666666666, 0.8553333333333333, 0.8585333333333334, 0.862, 0.8497333333333333, 0.8656, 0.8566666666666667, 0.8676, 0.8882666666666666, 0.8897333333333334, 0.8868, 0.8873333333333333, 0.8876, 0.8873333333333333, 0.8884, 0.8888, 0.8886666666666667, 0.8893333333333333, 0.8882666666666666, 0.8882666666666666, 0.8893333333333333, 0.8882666666666666, 0.8874666666666666, 0.8896, 0.8864, 0.8909333333333334, 0.8873333333333333, 0.8896, 0.8897333333333334, 0.8897333333333334, 0.8904, 0.8904, 0.8901333333333333, 0.8905333333333333, 0.8898666666666667, 0.8902666666666667, 0.8904, 0.89, 0.8910666666666667, 0.8902666666666667, 0.89, 0.8898666666666667, 0.8902666666666667, 0.8909333333333334, 0.89, 0.8904, 0.8905333333333333, 0.89, 0.8902666666666667, 0.8901333333333333, 0.8904, 0.8896, 0.8896, 0.8896, 0.8908, 0.8897333333333334, 0.8904, 0.8904, 0.8904, 0.8901333333333333, 0.8902666666666667, 0.8896, 0.8905333333333333, 0.89, 0.8904, 0.8897333333333334, 0.89, 0.8902666666666667, 0.8896, 0.8886666666666667, 0.8904, 0.8897333333333334, 0.8901333333333333, 0.8901333333333333, 0.8905333333333333, 0.8898666666666667, 0.89, 0.8914666666666666, 0.89, 0.89, 0.8906666666666667, 0.8908, 0.8905333333333333, 0.8893333333333333, 0.8897333333333334, 0.8909333333333334, 0.8908, 0.8897333333333334, 0.8902666666666667, 0.8897333333333334, 0.8901333333333333, 0.8897333333333334, 0.8901333333333333, 0.8898666666666667, 0.8906666666666667, 0.8896, 0.8905333333333333, 0.8906666666666667, 0.8904, 0.8904, 0.8890666666666667, 0.8898666666666667, 0.89, 0.89, 0.8909333333333334, 0.8901333333333333, 0.8901333333333333, 0.8901333333333333, 0.8898666666666667, 0.8898666666666667, 0.89, 0.8892, 0.89, 0.8904, 0.89, 0.8908, 0.8904, 0.8898666666666667, 0.8902666666666667, 0.8898666666666667, 0.8893333333333333, 0.8902666666666667, 0.8897333333333334, 0.8906666666666667, 0.8904, 0.8904, 0.8908, 0.8902666666666667], "seed": 601052756, "model": "residualv3", "loss_std": [0.32931268215179443, 0.2009768933057785, 0.18919232487678528, 0.1863947957754135, 0.18362128734588623, 0.18035224080085754, 0.17734363675117493, 0.1693243533372879, 0.16641582548618317, 0.16186295449733734, 0.16125519573688507, 0.15462516248226166, 0.15365254878997803, 0.14920568466186523, 0.14728321135044098, 0.1414404660463333, 0.14028802514076233, 0.1357976496219635, 0.1328330636024475, 0.13141293823719025, 0.12804162502288818, 0.12792852520942688, 0.12390530854463577, 0.12300042062997818, 0.11891622841358185, 0.12111064046621323, 0.11429234594106674, 0.11677908897399902, 0.11219711601734161, 0.10994236171245575, 0.10753259807825089, 0.10986481606960297, 0.10575912147760391, 0.10700370371341705, 0.10334015637636185, 0.09931476414203644, 0.09869937598705292, 0.09642118215560913, 0.09530003368854523, 0.09734806418418884, 0.09337984770536423, 0.09466210752725601, 0.09437495470046997, 0.09144698828458786, 0.09183373302221298, 0.08925266563892365, 0.08610879629850388, 0.08633873611688614, 0.08698438107967377, 0.08796345442533493, 0.0867757499217987, 0.08482623100280762, 0.081939198076725, 0.08402696251869202, 0.08333417773246765, 0.08154870569705963, 0.0845361202955246, 0.07878393679857254, 0.07878103107213974, 0.07694532722234726, 0.07879739999771118, 0.07889845222234726, 0.07932788133621216, 0.07883711159229279, 0.07679910212755203, 0.07833235710859299, 0.0779736340045929, 0.07576584070920944, 0.07558868080377579, 0.07471922039985657, 0.074346624314785, 0.0727703794836998, 0.07566413283348083, 0.073198601603508, 0.0724542960524559, 0.07058583199977875, 0.07104232162237167, 0.07043424993753433, 0.07144474983215332, 0.0716891884803772, 0.06833818554878235, 0.07168486714363098, 0.06892276555299759, 0.06873805820941925, 0.06785011291503906, 0.06784658879041672, 0.06807461380958557, 0.06446480005979538, 0.06794881820678711, 0.06549280136823654, 0.06644431501626968, 0.05411092936992645, 0.046922288835048676, 0.045265890657901764, 0.04509361833333969, 0.042745187878608704, 0.044017452746629715, 0.04557017982006073, 0.042136482894420624, 0.042555276304483414, 0.04135647416114807, 0.040728338062763214, 0.04055485129356384, 0.04154808074235916, 0.0396290086209774, 0.03974999487400055, 0.038422174751758575, 0.040066275745630264, 0.03799626603722572, 0.0398540161550045, 0.039336882531642914, 0.04065549746155739, 0.0375470332801342, 0.03807320445775986, 0.03821364417672157, 0.03792016953229904, 0.038192059844732285, 0.03798091784119606, 0.039132535457611084, 0.03773345425724983, 0.03531113266944885, 0.03769178315997124, 0.037289563566446304, 0.037100352346897125, 0.03651793673634529, 0.036769263446331024, 0.03747762739658356, 0.037613850086927414, 0.03580192103981972, 0.036883119493722916, 0.037805747240781784, 0.03788856789469719, 0.036774035543203354, 0.039072662591934204, 0.037024226039648056, 0.03875218331813812, 0.03812330588698387, 0.03640437871217728, 0.03811681643128395, 0.03631925582885742, 0.0374816469848156, 0.03543514013290405, 0.035430897027254105, 0.034881841391325, 0.03751268610358238, 0.0360504686832428, 0.04000835865736008, 0.03682699427008629, 0.03762679919600487, 0.03720312938094139, 0.03482704237103462, 0.03759004920721054, 0.037481360137462616, 0.037157416343688965, 0.03776434808969498, 0.03802729398012161, 0.03656909242272377, 0.036739714443683624, 0.03855550289154053, 0.03899453207850456, 0.03875678777694702, 0.036025602370500565, 0.040534261614084244, 0.03682089224457741, 0.03795924037694931, 0.03513564541935921, 0.03939064219594002, 0.036751627922058105, 0.03572923317551613, 0.038048289716243744, 0.03938956931233406, 0.03802597522735596, 0.036859191954135895, 0.03692657873034477, 0.039007991552352905, 0.03730228170752525, 0.03747605159878731, 0.03813232108950615, 0.03894176706671715, 0.039829205721616745, 0.03820015490055084, 0.038988661020994186, 0.035805851221084595, 0.036138519644737244, 0.038365960121154785, 0.03684329241514206, 0.03656947240233421, 0.03458355739712715, 0.03853992000222206, 0.03746720030903816, 0.03774440661072731, 0.03759196028113365, 0.036641404032707214, 0.03893328830599785, 0.037059638649225235, 0.0382193848490715, 0.036005619913339615, 0.03921055421233177, 0.036516979336738586, 0.03742868825793266, 0.0381774827837944, 0.039255499839782715, 0.03767868131399155, 0.0378282405436039, 0.03752753138542175, 0.03764818236231804, 0.03916917368769646, 0.037993207573890686, 0.03919236361980438, 0.03584747761487961, 0.03980515897274017]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:40 2016", "state": "available"}], "summary": "f721e76f20e7c98e77e3baf22474281b"}