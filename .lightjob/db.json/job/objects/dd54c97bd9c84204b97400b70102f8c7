{"content": {"hp_model": {"f0": 64, "f1": 64, "f2": 16, "f3": 16, "nonlin": "leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.6198084354400635, 1.313450574874878, 1.1425851583480835, 1.0321871042251587, 0.9488238096237183, 0.8837620615959167, 0.829030454158783, 0.7848186492919922, 0.7440292835235596, 0.7088629007339478, 0.6797037124633789, 0.6513703465461731, 0.6294205784797668, 0.6034078001976013, 0.5856949090957642, 0.5662954449653625, 0.5483728051185608, 0.530035674571991, 0.5154134035110474, 0.4988878071308136, 0.48688334226608276, 0.47379401326179504, 0.4615773558616638, 0.4470449984073639, 0.4371556043624878, 0.42457523941993713, 0.4141005277633667, 0.40394285321235657, 0.3930058479309082, 0.38285571336746216, 0.3757188320159912, 0.36522841453552246, 0.3591914474964142, 0.3492828905582428, 0.3433375358581543, 0.3359512686729431, 0.32580170035362244, 0.3223838210105896, 0.3122417628765106, 0.30530303716659546, 0.30062252283096313, 0.29349640011787415, 0.28519684076309204, 0.2810087203979492, 0.27570950984954834, 0.2694348990917206, 0.2668248414993286, 0.2592930495738983, 0.25401565432548523, 0.24913737177848816, 0.24700966477394104, 0.2402784526348114, 0.23801136016845703, 0.23175086081027985, 0.22508575022220612, 0.218871608376503, 0.2178431749343872, 0.21662569046020508, 0.20840494334697723, 0.2092297375202179, 0.19954605400562286, 0.20225711166858673, 0.19744957983493805, 0.19351379573345184, 0.18717406690120697, 0.1853885054588318, 0.18653790652751923, 0.1806289553642273, 0.17665483057498932, 0.17376479506492615, 0.17064985632896423, 0.16684208810329437, 0.16494120657444, 0.16262514889240265, 0.15922540426254272, 0.15884317457675934, 0.15522967278957367, 0.1568855494260788, 0.1532260626554489, 0.1452140361070633, 0.14607134461402893, 0.14773261547088623, 0.14154654741287231, 0.14132246375083923, 0.1360548883676529, 0.13670338690280914, 0.1321798861026764, 0.13156326115131378, 0.12882913649082184, 0.12781912088394165, 0.12764012813568115, 0.1242370754480362, 0.12164568156003952, 0.11956527829170227, 0.11845382302999496, 0.11794010549783707, 0.11683764308691025, 0.11282876878976822, 0.11469114571809769, 0.1145513653755188, 0.1116008386015892, 0.10798896849155426, 0.10782371461391449, 0.10724517703056335, 0.1060483455657959, 0.10491351038217545, 0.1042444184422493, 0.09629657864570618, 0.09681781381368637, 0.09611555188894272, 0.09685049206018448, 0.09618211537599564, 0.0929466113448143, 0.09052196890115738, 0.09249746799468994, 0.07088322937488556, 0.0524635873734951, 0.0484207421541214, 0.04432813823223114, 0.04400062933564186, 0.04117347300052643, 0.04057169705629349, 0.038780298084020615, 0.038074854761362076, 0.03727911785244942, 0.03715995326638222, 0.03688715025782585, 0.036010488867759705, 0.035159602761268616, 0.03465490788221359, 0.034061498939991, 0.0344526432454586, 0.03373803198337555, 0.03287752717733383, 0.032087285071611404, 0.03159112483263016, 0.0317787304520607, 0.031236469745635986, 0.03158910945057869, 0.03254934400320053, 0.031626272946596146, 0.030735043808817863, 0.031729064881801605, 0.031378768384456635, 0.03167072683572769, 0.031818561255931854, 0.03139232099056244, 0.03184216842055321, 0.03137107193470001, 0.03181510418653488, 0.03158336132764816, 0.0319456085562706, 0.03119518794119358, 0.03143272176384926, 0.03168118745088577, 0.0320836640894413, 0.03122129663825035, 0.03170638903975487, 0.03253497555851936, 0.03149929642677307, 0.03207949921488762, 0.03202935308218002, 0.03212562948465347, 0.03184066340327263, 0.03214125335216522, 0.03168054670095444, 0.03148318827152252, 0.030719107016921043, 0.03123820759356022, 0.031230170279741287, 0.03145185858011246, 0.032014600932598114, 0.031062250956892967, 0.03185554966330528, 0.030857549980282784, 0.03150692954659462, 0.03199846297502518], "moving_avg_accuracy_train": [0.052277564368770756, 0.10647354593484679, 0.15969013444767438, 0.21358268632913435, 0.265142544340047, 0.3122138784534768, 0.356893098247018, 0.39705116998031725, 0.436064200246036, 0.4713527829899817, 0.5041075269547339, 0.5353350921348603, 0.5644021879647482, 0.5902717864151819, 0.6156792144954706, 0.6362587824619903, 0.6564216242520131, 0.6759046737938661, 0.6936024311399317, 0.7106344618989082, 0.7244732657320887, 0.7392480388151182, 0.7530778657648634, 0.7642947423482146, 0.7758684375255378, 0.7859616035494237, 0.7948363698221576, 0.8046182778115641, 0.811162671335548, 0.8187006513296233, 0.8238784078386414, 0.8298983484085979, 0.8370389778476846, 0.8433934286809486, 0.8476223024345314, 0.8522582587912998, 0.8574465653468781, 0.8623485921766698, 0.865649139387805, 0.8685173613790262, 0.8729143779519948, 0.876499633009324, 0.8805865955228065, 0.8784483848792559, 0.8816896468319911, 0.8842464205727953, 0.8865264103561397, 0.8853218770459317, 0.887899113367732, 0.8926942246119388, 0.8957780005461399, 0.8996108729606104, 0.9004940706290196, 0.9018773193258537, 0.9049259097504574, 0.9073676601779126, 0.9107020891352043, 0.9101158031694967, 0.91316357579067, 0.9145909695093605, 0.9170054298846979, 0.918492705567786, 0.9216351365729305, 0.9237821640228264, 0.924780247540948, 0.9271684744595183, 0.9292016212457556, 0.9294621221022155, 0.9311891742134687, 0.9328690431004921, 0.9350109322797379, 0.9365226832458856, 0.9380064199046858, 0.9381935198738887, 0.9379200955235429, 0.9391037277843578, 0.9405317200333768, 0.9414751161824939, 0.9424496947035947, 0.940990653648933, 0.9413163140246266, 0.9428181613043345, 0.9374471156957727, 0.9368993791977163, 0.9393939802279723, 0.9413042636778126, 0.9429631370112588, 0.9446723258018273, 0.9456366083967552, 0.9474342699142687, 0.9479921497646192, 0.9497426302477364, 0.947484793516106, 0.9491441394978749, 0.9508560788207433, 0.9471362020663914, 0.9483189826014927, 0.9494299520104557, 0.9472472004417467, 0.9487885077023893, 0.9507522129952732, 0.951631449060087, 0.9530039045255254, 0.9535975175682663, 0.9550244822543152, 0.9542930987956334, 0.9555878344875448, 0.9568785464995231, 0.9577451457532252, 0.9588922062517307, 0.9594548806408618, 0.96017999577445, 0.9611371217910896, 0.9621287074905798, 0.9629073465237292, 0.9655123464309262, 0.9680915782307092, 0.9704640401243234, 0.9725947136774133, 0.9745820022418425, 0.9763380098664954, 0.977962594556064, 0.9794014692885805, 0.9807057931919023, 0.981914525888216, 0.9830093247125081, 0.9839854151567704, 0.984868474756588, 0.9856818295869, 0.9864231495294189, 0.9870740974848381, 0.9876925027280486, 0.9882560789421855, 0.9888097644622803, 0.9893057562815561, 0.9897498237700949, 0.9901425090633511, 0.9904936006784723, 0.9908282203713762, 0.991124691748552, 0.9914217068737152, 0.9916588296006571, 0.9918768543037051, 0.9920800880316956, 0.9922443611475922, 0.9923992184471464, 0.9925455654631737, 0.9926796389752266, 0.9928049193848745, 0.992915346604748, 0.9930147311026342, 0.9930879011090651, 0.9931467786684243, 0.993192793025419, 0.993280672874086, 0.9933481750426574, 0.993392650952705, 0.9934489553134146, 0.9934856783451961, 0.993535005115466, 0.9935840134575094, 0.9936049055260718, 0.9936353341318256, 0.993653419281766, 0.9936580701726647, 0.9936785320161402, 0.9936922973776491, 0.9937046862030071, 0.9937135109970198, 0.993702852121155, 0.993746701506677, 0.9937373738774655, 0.9937522304992704, 0.9937609511612757, 0.9937780643034999, 0.9937865267338919, 0.9937755417307684], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.05243537627070782, 0.10543702642601654, 0.1570316720985504, 0.20924916212702366, 0.2589600116334478, 0.3040751056320006, 0.34730334902211374, 0.3857337233631252, 0.42344414832538196, 0.4567212625779341, 0.4875079500908486, 0.5159788347692638, 0.5422924017817953, 0.5660418243012814, 0.589682841419346, 0.6083615711140078, 0.6267481643791131, 0.6440417567326174, 0.6600160284426388, 0.6746949602369292, 0.6869873829820616, 0.6995032001714308, 0.7114205853500708, 0.7205591708756811, 0.7300159255689865, 0.7384334665775999, 0.7458189414559845, 0.7536408323725095, 0.7589643432617645, 0.764860312926552, 0.7689398054479029, 0.7733377406128265, 0.7785573245146462, 0.7830697855402147, 0.7857617539459071, 0.7893452229884398, 0.7931939332291289, 0.7964380458832492, 0.7987565142147285, 0.8005644330116292, 0.8036716992360686, 0.8064061741731545, 0.8092762106997397, 0.8069161312807446, 0.8091318800220225, 0.8102887692035552, 0.8110562672361363, 0.8097074117380498, 0.8118383154249376, 0.8151030202152751, 0.8171836738131, 0.8199800254641695, 0.8201518153688067, 0.8205017387829802, 0.8225369788259923, 0.8245823914479111, 0.8266744629205899, 0.8256041081194947, 0.8279703668764459, 0.8287206052264519, 0.8305616647621651, 0.831763869662078, 0.8339272793826172, 0.8353615057459368, 0.8355170555666744, 0.8368686350250671, 0.8381156476520786, 0.8373049829715846, 0.8383627588649383, 0.8395305127694837, 0.8410178558825956, 0.8422873405315349, 0.8431551449761223, 0.8431377114743686, 0.8420964836251245, 0.8426883459930337, 0.8434500142645888, 0.8441965508652384, 0.8444075080716362, 0.8431579693785539, 0.8429102317874605, 0.8438907635785939, 0.8395209831544694, 0.8381365088506941, 0.8398232580032753, 0.8413098586901465, 0.842121867455921, 0.8430794613955699, 0.843970857547052, 0.845200360177136, 0.8450312042422536, 0.8460723115100464, 0.8435626601312556, 0.8448950471678288, 0.8460463968844044, 0.8427702939994278, 0.8438411502828435, 0.8446207859605078, 0.8428972298136287, 0.8444670583194647, 0.8460315358930152, 0.8462532775879757, 0.8477204644187866, 0.8482576235491971, 0.8494348085304972, 0.8486071856405499, 0.8492499556401245, 0.8499016908272415, 0.850485163969668, 0.8513938553099, 0.8514689605545124, 0.8515518508318924, 0.8522897498036731, 0.8533008032965739, 0.8537092336502748, 0.8559659693590425, 0.8583683899777316, 0.8603677590936631, 0.8623544148013902, 0.8640915177960253, 0.8655684317637873, 0.8666646912323634, 0.8679850326324705, 0.8690990681964073, 0.8700131624592213, 0.870872468389504, 0.8718951318950566, 0.872507264742825, 0.8730683323197473, 0.8736241802812966, 0.8739351609259832, 0.8742812262057494, 0.8745825369436082, 0.8747784154028618, 0.8749791200786902, 0.8752085824119356, 0.8754150985118565, 0.8755155137830354, 0.8756791297145963, 0.8758762416866608, 0.8759783412566995, 0.8761211180120536, 0.8762963861995531, 0.876257785559643, 0.8763715883760431, 0.876523868544463, 0.8765744419686311, 0.8765701004167228, 0.8765295719262554, 0.8766894382934942, 0.8767590463278496, 0.8766863867063598, 0.8767573294080883, 0.8767346991122342, 0.8767631599709657, 0.8768253958375739, 0.8769699458622502, 0.87691384688973, 0.8769620435731215, 0.8770054205881738, 0.8770810809954708, 0.8770881402057881, 0.877020221798914, 0.8769967458351371, 0.8771231313513975, 0.8772358488073723, 0.8772539043163188, 0.8772548587171417, 0.877268954217792, 0.8772816401683774, 0.8771211295777445, 0.8770865333274248, 0.8771296683982968, 0.8771074548058315, 0.8771484977288628, 0.8771986728995007, 0.8771573518256651], "moving_var_accuracy_train": [0.024596493626978733, 0.048571684025474945, 0.06920256325942, 0.08842197126814066, 0.1035055447642759, 0.11309638474481154, 0.11975284040256647, 0.12229159289034114, 0.12376058237593202, 0.1225920807870255, 0.11998873197809125, 0.11676630622399355, 0.11269374014144919, 0.10744749124318437, 0.10251257873376135, 0.09607298841958267, 0.09012455127906982, 0.0845283991262155, 0.07889445474931592, 0.07361581992035685, 0.06797785035212041, 0.06314471059380351, 0.05855161655556222, 0.05382881978258121, 0.049651491584841445, 0.04560319042982881, 0.041751724674406826, 0.038437723722185096, 0.034979413129337444, 0.031992862097923416, 0.029034858350331188, 0.026457529675490338, 0.024270674007018468, 0.022207018014847955, 0.02014726657237684, 0.018325968737215908, 0.01673563858772624, 0.015278343532313596, 0.013848551686118624, 0.012537736794025095, 0.011457966907309226, 0.01042785670101324, 0.009535400394191417, 0.008623007857578009, 0.007855259083236445, 0.0071285670025677955, 0.006462495483020406, 0.005829304039176969, 0.005306152958784935, 0.004982475489505314, 0.004569815006666003, 0.004245051704509886, 0.0038275668771522494, 0.003462030582052664, 0.003199472656040262, 0.002933184696786034, 0.002739931975348465, 0.0024690323589158895, 0.0023057293845776604, 0.0020934935215733076, 0.00193661073955265, 0.001762857566214928, 0.001675445663192284, 0.0015493886387085163, 0.001403415311219978, 0.0013144064304292384, 0.0012201689600757934, 0.001098762810334161, 0.001015730910255601, 0.0009395554545283435, 0.0008868891123810385, 0.0008187687199957704, 0.0007567051182502002, 0.0006813496640114617, 0.0006138875454885732, 0.0005651076588992893, 0.0005269493497786856, 0.00048226438144833786, 0.0004425861729476257, 0.00041748676284556054, 0.0003766925786836767, 0.0003593232280794029, 0.0005830240836347324, 0.0005274218127129871, 0.00053068694014308, 0.000510460891857375, 0.0004841815492994055, 0.0004620553312657092, 0.00042421836644506827, 0.0004108808121845013, 0.00037259380031289533, 0.000362912057577576, 0.00037250129218011405, 0.00036003202474701535, 0.00035040544847896423, 0.0004399022512391773, 0.0004085027542631917, 0.00037876075608573613, 0.00038376432017347316, 0.00036676854080151247, 0.00036479693301706706, 0.0003352747442343832, 0.00031869997585244863, 0.0002900013662678131, 0.00027932728357810935, 0.00025620885109299877, 0.00024567503059088193, 0.0002361009650125786, 0.0002192498169099752, 0.00020916656530406205, 0.00019109933098731245, 0.00017672152550120946, 0.0001672941848566428, 0.000159413946165879, 0.00014892906024478684, 0.00019511037486877743, 0.00023547126747500515, 0.00026258131965736786, 0.0002771811158001657, 0.00028500684676494926, 0.0002842582270890083, 0.0002795858831023344, 0.0002702605392549716, 0.0002585458329324621, 0.00024584056221945485, 0.00023204376618855162, 0.00021741416256811807, 0.00020269089462277652, 0.00018837571988042628, 0.00017448414520697018, 0.00016084932985225227, 0.00014820622227049965, 0.00013624416338571679, 0.00012537885594360957, 0.00011505504131234577, 0.00010532430059050508, 9.617968618731261e-05, 8.767110546845673e-05, 7.991172797152346e-05, 7.27116126717317e-05, 6.623441326574025e-05, 6.011701662785743e-05, 5.453312790532417e-05, 4.945155064852807e-05, 4.4749266493132406e-05, 4.04901668928461e-05, 3.6633907245462444e-05, 3.313229788062394e-05, 2.996032472193543e-05, 2.7074039787742957e-05, 2.445553131474954e-05, 2.2058162831844513e-05, 1.9883545651625008e-05, 1.7914246975909303e-05, 1.619232808853405e-05, 1.4614104164537314e-05, 1.31704967072547e-05, 1.1881978665843449e-05, 1.070591802882811e-05, 9.657224398332716e-06, 8.71311831680793e-06, 7.845734791886508e-06, 7.069494413130964e-06, 6.365488625653159e-06, 5.7291344401632075e-06, 5.159989179492632e-06, 4.645695628140608e-06, 4.182507412270312e-06, 3.7649575639475803e-06, 3.389484312265126e-06, 3.067840798534533e-06, 2.761839760681435e-06, 2.48764225751639e-06, 2.2395624812770656e-06, 2.0182419698804383e-06, 1.8170622874456476e-06, 1.6364420913436741e-06], "duration": 193003.214179, "accuracy_train": [0.5227756436877077, 0.5942373800295312, 0.6386394310631229, 0.6986156532622739, 0.7291812664382613, 0.7358558854743448, 0.7590060763888888, 0.758473815580011, 0.7871814726375047, 0.7889500276854927, 0.7989002226375047, 0.8163831787559985, 0.8260060504337394, 0.8230981724690846, 0.8443460672180694, 0.8214748941606681, 0.8378872003622186, 0.8512521196705426, 0.852882247254522, 0.8639227387296974, 0.8490225002307125, 0.8722209965623846, 0.8775463083125692, 0.8652466315983758, 0.880031694121447, 0.8768000977643964, 0.8747092662767626, 0.8926554497162238, 0.8700622130514026, 0.8865424712763011, 0.8704782164198044, 0.8840778135382059, 0.9013046427994648, 0.9005834861803249, 0.8856821662167773, 0.893981866002215, 0.9041413243470838, 0.9064668336447952, 0.8953540642880213, 0.8943313593000184, 0.9124875271087117, 0.9087669285252861, 0.9173692581441492, 0.8592044890873015, 0.9108610044066077, 0.9072573842400333, 0.9070463184062385, 0.8744810772540605, 0.911094240263935, 0.9358502258098007, 0.9235319839539498, 0.9341067246908453, 0.9084428496447029, 0.9143265575973607, 0.93236322357189, 0.9293434140250092, 0.9407119497508305, 0.9048392294781286, 0.9405935293812293, 0.9274375129775747, 0.9387355732627353, 0.9318781867155776, 0.9499170156192323, 0.94310541107189, 0.9337629992040422, 0.9486625167266519, 0.94749994232189, 0.9318066298103543, 0.9467326432147471, 0.9479878630837025, 0.9542879348929494, 0.9501284419412146, 0.9513600498338871, 0.9398774195967147, 0.9354592763704319, 0.9497564181316908, 0.9533836502745479, 0.9499656815245479, 0.9512209013935032, 0.9278592841569768, 0.9442472574058692, 0.9563347868217055, 0.8891077052187154, 0.9319697507152085, 0.9618453895002769, 0.958496814726375, 0.9578929970122739, 0.9600550249169435, 0.9543151517511074, 0.96361322357189, 0.953013068417774, 0.9654969545957919, 0.9271642629314323, 0.9640782533337948, 0.9662635327265596, 0.9136573112772242, 0.9589640074174051, 0.9594286766911223, 0.9276024363233666, 0.9626602730481728, 0.9684255606312293, 0.959544573643411, 0.9653560037144703, 0.9589400349529347, 0.967867164428756, 0.9477106476674971, 0.9672404557147471, 0.9684949546073275, 0.9655445390365448, 0.9692157507382798, 0.9645189501430418, 0.9667060319767442, 0.9697512559408453, 0.9710529787859912, 0.9699150978220746, 0.9889573455956996, 0.991304664428756, 0.9918161971668512, 0.9917707756552234, 0.9924675993217055, 0.9921420784883721, 0.9925838567621816, 0.9923513418812293, 0.9924447083217978, 0.9927931201550388, 0.992862514131137, 0.9927702291551311, 0.9928160111549464, 0.9930020230597084, 0.9930950290120893, 0.9929326290836102, 0.9932581499169435, 0.9933282648694168, 0.9937929341431341, 0.9937696826550388, 0.9937464311669435, 0.9936766767026578, 0.9936534252145626, 0.9938397976075121, 0.9937929341431341, 0.9940948430001846, 0.9937929341431341, 0.993839076631137, 0.9939091915836102, 0.9937228191906607, 0.9937929341431341, 0.9938626886074198, 0.9938863005837025, 0.9939324430717055, 0.9939091915836102, 0.9939091915836102, 0.9937464311669435, 0.9936766767026578, 0.9936069222383721, 0.9940715915120893, 0.9939556945598007, 0.9937929341431341, 0.9939556945598007, 0.9938161856312293, 0.9939789460478959, 0.9940250885358989, 0.9937929341431341, 0.9939091915836102, 0.9938161856312293, 0.993699928190753, 0.9938626886074198, 0.9938161856312293, 0.9938161856312293, 0.9937929341431341, 0.9936069222383721, 0.994141345976375, 0.9936534252145626, 0.993885940095515, 0.9938394371193245, 0.993932082583518, 0.9938626886074198, 0.9936766767026578], "end": "2016-02-02 16:02:20.393000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0], "moving_var_accuracy_valid": [0.02474521816185639, 0.04755327061834239, 0.06675601071517703, 0.08462040602752269, 0.09839888245262356, 0.1068773395658455, 0.11300773484861473, 0.11499900441166575, 0.11629778932800515, 0.11463430739200134, 0.11170125780496178, 0.10782645349381013, 0.10327544242473596, 0.09802421381234405, 0.09325187164449927, 0.08706673896710551, 0.08140266637746268, 0.07595401476811908, 0.07065520950129747, 0.06552892789876058, 0.060335968021389956, 0.05571218233851033, 0.05141918073012393, 0.0470288863657916, 0.043130869613176616, 0.03945547762154415, 0.03600083701200298, 0.03295139110839284, 0.02991130991124571, 0.027233041044713982, 0.0246595172733284, 0.0223676420494294, 0.020376074349459688, 0.018521727655079194, 0.01673477513464649, 0.015176868874590958, 0.013792495121782935, 0.01250796401181825, 0.011305545269273078, 0.01020440787573145, 0.009270863018664187, 0.008411072895431727, 0.007644099592863952, 0.006929819407353233, 0.006281023348978183, 0.005664966547285492, 0.005103771371627088, 0.004609768934856843, 0.004189658796076287, 0.003866617592781136, 0.003518917908050298, 0.0032374023602532213, 0.002913927730169917, 0.002623636974715007, 0.00239855309553763, 0.0021963512011290115, 0.002016106948437277, 0.001824807188195596, 0.0016927190939196677, 0.0015285129027640787, 0.0014061671144140367, 0.0012785580725650067, 0.0011928253398788167, 0.001092055853242104, 0.0009830680296384771, 0.0009012021299657737, 0.0008250772813965282, 0.0007484841482746793, 0.0006837057420122511, 0.0006276080104452537, 0.0005847569152258201, 0.0005407855451682694, 0.0004934847516378546, 0.00044413901181691975, 0.00040948250954160094, 0.00037168696815036503, 0.00033973951833837155, 0.0003107814185695188, 0.0002801038031989476, 0.00026614554538864117, 0.00024008335607614365, 0.00022472800380933994, 0.0003741100320239577, 0.00035394995070188787, 0.0003441610599652994, 0.00032963478838862455, 0.0003026055336710139, 0.00028059785568318273, 0.0002596893540047595, 0.00024732550906073384, 0.00022285048172741352, 0.0002103205726421293, 0.00024597366576551447, 0.0002373535961260187, 0.00022554869204214405, 0.00029958947385449666, 0.0002799511250866239, 0.0002574264986869473, 0.00025841966094125514, 0.00025475694868674496, 0.0002513095645213531, 0.0002266211324827735, 0.00022333275400303928, 0.00020359633798518566, 0.0001957085845084553, 0.00018230236288929244, 0.00016779050605154043, 0.00015483428423352502, 0.0001424148239815695, 0.00013560482114972612, 0.00012209510621466813, 0.00010994743277595865, 0.00010385314353135704, 0.00010266789166778278, 9.390244068542374e-05, 0.0001303479011499268, 0.00016925773449685797, 0.00018830925280284447, 0.00020499953563196053, 0.0002116573233944957, 0.00021012306486857863, 0.00019992682178370522, 0.00019562385232086395, 0.00018723114422821886, 0.0001760281446971837, 0.0001650709903638362, 0.00015797645713775505, 0.00014555117103383337, 0.00013382922536331131, 0.00012322700543420745, 0.00011177468554311401, 0.00010167506758953913, 9.232465427732627e-05, 8.343750418678987e-05, 7.545629507020489e-05, 6.838454222459041e-05, 6.192992809787024e-05, 5.5827684328256435e-05, 5.0485847452975736e-05, 4.578694087345844e-05, 4.130206568593146e-05, 3.735532593416343e-05, 3.3896263778691186e-05, 3.052004748543531e-05, 2.758460246607727e-05, 2.5034845466715153e-05, 2.255437996113248e-05, 2.0299111606675985e-05, 1.828398347286453e-05, 1.6685600423945626e-05, 1.506064788757246e-05, 1.3602097884170553e-05, 1.2287183898110327e-05, 1.1063074680913254e-05, 9.964057397139482e-06, 9.002511385257855e-06, 8.290312633437446e-06, 7.489605222554101e-06, 6.7615509829081236e-06, 6.10232997353095e-06, 5.543617451269108e-06, 4.989704198194943e-06, 4.532249968306154e-06, 4.083985059352845e-06, 3.819346241901193e-06, 3.551758641643764e-06, 3.1995167901092424e-06, 2.8795733090266955e-06, 2.5934041263712664e-06, 2.335512113814403e-06, 2.3338337497809287e-06, 2.1112224796284414e-06, 1.9168459407177755e-06, 1.7296023398578932e-06, 1.5718027996506897e-06, 1.437280449422419e-06, 1.3089192847665292e-06], "accuracy_test": 0.8747309470663265, "start": "2016-01-31 10:25:37.179000", "learning_rate_per_epoch": [0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0066672866232693195, 0.0006667286506853998, 0.0006667286506853998, 0.0006667286506853998, 0.0006667286506853998, 0.0006667286506853998, 0.0006667286506853998, 0.0006667286506853998, 0.0006667286506853998, 0.0006667286506853998, 0.0006667286506853998, 0.0006667286506853998, 0.0006667286506853998, 0.0006667286506853998, 0.0006667286506853998, 0.0006667286506853998, 0.0006667286506853998, 0.0006667286506853998, 0.0006667286506853998, 6.66728665237315e-05, 6.66728647047421e-06, 6.667286243100534e-07, 6.667286100991987e-08, 6.667286278627671e-09, 6.667286167605369e-10, 6.667286167605369e-11, 6.667286080869195e-12, 6.667286080869195e-13, 6.667286351919738e-14, 6.6672864366230326e-15, 6.667286436623033e-16, 6.667286171925237e-17, 6.667286254643298e-18, 6.667286151245721e-19, 6.667286409739663e-20, 6.667286571298376e-21, 6.667286773246768e-22, 6.667287025682257e-23, 6.667286710137895e-24, 6.667286710137895e-25, 6.667286586878379e-26, 6.667286740952774e-27, 6.667286837249272e-28, 6.667286596508029e-29, 6.667286596508029e-30, 6.6672864084289325e-31, 6.667286408428933e-32, 6.667286408428933e-33, 6.6672864084289325e-34, 6.667286293634562e-35, 6.667286437127525e-36, 6.6672866164937285e-37, 6.667286728597606e-38, 6.667287008857299e-39, 6.66729401534962e-40, 6.667237963411047e-41, 6.66737809325748e-42, 6.670180690186129e-43, 6.726232628759122e-44, 7.006492321624085e-45, 0.0, 0.0, 0.0, 0.0], "accuracy_train_first": 0.5227756436877077, "accuracy_train_last": 0.9936766767026578, "batch_size_eval": 1024, "accuracy_train_std": [0.015569268508013165, 0.018231038321090853, 0.01656997479750191, 0.02008687150219306, 0.02193181563615479, 0.01957119378887985, 0.019605341994294978, 0.01870861584580317, 0.018753281933710206, 0.017841148789991733, 0.016725483159865575, 0.018212483156477412, 0.016920026451490117, 0.018740316398184215, 0.019071814247461016, 0.01515554149112349, 0.016327970415270544, 0.01713644230232966, 0.015467195560494855, 0.01771236452768372, 0.016059638843315415, 0.017914353079344195, 0.01719360299110027, 0.019367846562501952, 0.017307635553968463, 0.018627007373333764, 0.018168254453004275, 0.019132557496092526, 0.015445299431860179, 0.017809577066849793, 0.01818888757440337, 0.016719399920975448, 0.016804628208645925, 0.017705645171296754, 0.016176051275171563, 0.01715018927175564, 0.016864815481909132, 0.01726676739129022, 0.017144096518232545, 0.015884818759327277, 0.017352749723868112, 0.016728836076038135, 0.01566167220098123, 0.016980542635292084, 0.014847413478813926, 0.016569784088808673, 0.017797571090503606, 0.016524915402111415, 0.01513492815267608, 0.015762833532084346, 0.013735172618086005, 0.015233433611071117, 0.01534422500742199, 0.014431373108863561, 0.014742161111021387, 0.013938632050538129, 0.013164643688788022, 0.015049549770210016, 0.015526551292997742, 0.01403382466130146, 0.012444785746920525, 0.014393391532884712, 0.013266868018760354, 0.01450168880691647, 0.011933483391348237, 0.012472369038335861, 0.011570955525929984, 0.014459908330677049, 0.013054490455444927, 0.012778865144140757, 0.012107448974564049, 0.012920026287429576, 0.011927623519466735, 0.011257243655440423, 0.012461047858476536, 0.012059858882807474, 0.009770282419089997, 0.011392938816279482, 0.011574202966346963, 0.013462503316907171, 0.012981800111047761, 0.011971950405521478, 0.013903245241083982, 0.013320887054070942, 0.010855589112879616, 0.010946224688201675, 0.011516227644903716, 0.010734647264490629, 0.010756534258209684, 0.010291318915537272, 0.010915610362250638, 0.009185329033584497, 0.01184254160678001, 0.009531739458258649, 0.009195234581752554, 0.013192771152575307, 0.010252739984839965, 0.010833112960128457, 0.013112507944711348, 0.009973272528594921, 0.009135881381653369, 0.008653375105434017, 0.010090878596178418, 0.010724738373092523, 0.009905770492445082, 0.012929551251619067, 0.009915678261707868, 0.009825745766957454, 0.00878313699896917, 0.009405786774124139, 0.010071632589182752, 0.009246991581340427, 0.008253225261540388, 0.008286194269797946, 0.007913766966166068, 0.003524865281282769, 0.0031820419322520534, 0.0034433758457397723, 0.0032390594647352037, 0.0029786223848999797, 0.003161183998110011, 0.002946186620396773, 0.002678693686555238, 0.002670562046179736, 0.0022352268827711816, 0.002823850830132832, 0.0026275268900134902, 0.0028345636906205343, 0.002610106407193681, 0.0026658253863947067, 0.0026775505407149653, 0.0025912240722738796, 0.0026688697923511707, 0.0024745618418486994, 0.002583488010688582, 0.0026708532166407712, 0.0024966890075782716, 0.0026639998338242902, 0.002524939407653246, 0.0024561413199898747, 0.0027125003796281857, 0.0026602876960095645, 0.0027749318870457524, 0.0027770706535558973, 0.0025053804806759983, 0.002313316719230513, 0.0022177099402076843, 0.0025745291874625232, 0.0023104452676441325, 0.0025378249860132287, 0.0025997013479311672, 0.0027709950662302696, 0.002505767160621311, 0.002585752425559875, 0.002443335889028293, 0.0025771243797858565, 0.0025200241313455087, 0.002550555014272109, 0.0026504035653935236, 0.0025918620424464135, 0.00264684862738844, 0.0026431618376799885, 0.0025467565102143272, 0.0026072157076123645, 0.002637876573519245, 0.002451152970208154, 0.002252082894556102, 0.002759530553258807, 0.0025379810377415568, 0.0025592726346035785, 0.0025310234026529603, 0.002845344002701274, 0.0026537386893858836, 0.0026488637738944325, 0.002535884389939874, 0.0026812106431976894, 0.0024966890075782716], "accuracy_test_std": 0.007349717107924123, "error_valid": [0.47564623729292166, 0.41754812217620485, 0.3786165168486446, 0.3207934276167168, 0.2936423428087349, 0.28988904838102414, 0.26364246046686746, 0.2683929075677711, 0.23716202701430722, 0.24378470914909633, 0.23541186229292166, 0.227783203125, 0.22088549510542166, 0.22021337302334332, 0.1975480045180723, 0.2235298616340362, 0.20777249623493976, 0.20031591208584332, 0.19621552616716864, 0.19319465361445776, 0.20238081231174698, 0.18785444512424698, 0.18132294804216864, 0.19719355939382532, 0.1848732821912651, 0.18580866434487953, 0.1877117846385542, 0.1759621493787651, 0.19312405873493976, 0.18207596009036142, 0.19434476185993976, 0.18708084290286142, 0.17446642036897586, 0.17631806522966864, 0.19001053040286142, 0.1784035556287651, 0.17216767460466864, 0.17436494022966864, 0.18037727080195776, 0.1831642978162651, 0.16836290474397586, 0.1689835513930723, 0.16489346056099397, 0.21432458349021077, 0.17092638130647586, 0.17929922816265065, 0.18203625047063254, 0.20243228774472888, 0.1689835513930723, 0.15551463667168675, 0.16409044380647586, 0.15485280967620485, 0.17830207548945776, 0.17634895048945776, 0.15914586078689763, 0.1570088949548193, 0.15449689382530118, 0.18402908509036142, 0.15073330431099397, 0.16452724962349397, 0.15286879941641573, 0.15741628623870485, 0.14660203313253017, 0.15173045698418675, 0.16308299604668675, 0.15096714984939763, 0.1506612387048193, 0.16999099915286142, 0.15211725809487953, 0.1499597020896084, 0.14559605609939763, 0.14628729762801207, 0.1490346150225903, 0.15701919004141573, 0.1672745670180723, 0.1519848926957832, 0.14969497129141573, 0.14908461972891573, 0.1536938770707832, 0.16808787885918675, 0.15931940653237953, 0.14728445030120485, 0.19980704066265065, 0.1743237598832832, 0.14499599962349397, 0.14531073512801207, 0.1505700536521084, 0.1483021931475903, 0.1480065770896084, 0.1437341161521084, 0.15649119917168675, 0.1445577230798193, 0.17902420227786142, 0.14311346950301207, 0.14359145566641573, 0.18671463196536142, 0.14652114316641573, 0.14836249294051207, 0.1726147755082832, 0.14140448512801207, 0.13988816594503017, 0.15175104715737953, 0.13907485410391573, 0.1469079442771084, 0.13997052663780118, 0.15884142036897586, 0.14496511436370485, 0.14423269248870485, 0.14426357774849397, 0.14042792262801207, 0.14785509224397586, 0.14770213667168675, 0.14106915945030118, 0.1375997152673193, 0.14261489316641573, 0.12372340926204817, 0.12000982445406627, 0.12163791886295183, 0.11976568382906627, 0.12027455525225905, 0.12113934252635539, 0.12346897355045183, 0.12013189476656627, 0.12087461172816272, 0.12175998917545183, 0.12139377823795183, 0.11890089655496983, 0.12198353962725905, 0.12188205948795183, 0.12137318806475905, 0.12326601327183728, 0.12260418627635539, 0.12270566641566272, 0.12345867846385539, 0.12321453783885539, 0.12272625658885539, 0.12272625658885539, 0.12358074877635539, 0.12284832690135539, 0.12234975056475905, 0.12310276261295183, 0.12259389118975905, 0.12212620011295183, 0.12408962019954817, 0.12260418627635539, 0.12210560993975905, 0.12297039721385539, 0.12346897355045183, 0.12383518448795183, 0.12187176440135539, 0.12261448136295183, 0.12396754988704817, 0.12260418627635539, 0.12346897355045183, 0.12298069230045183, 0.12261448136295183, 0.12172910391566272, 0.12359104386295183, 0.12260418627635539, 0.12260418627635539, 0.12223797533885539, 0.12284832690135539, 0.12359104386295183, 0.12321453783885539, 0.12173939900225905, 0.12174969408885539, 0.12258359610316272, 0.12273655167545183, 0.12260418627635539, 0.12260418627635539, 0.12432346573795183, 0.12322483292545183, 0.12248211596385539, 0.12309246752635539, 0.12248211596385539, 0.12234975056475905, 0.12321453783885539], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.01524113906407294, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 16, "valid_ratio": 0.15, "learning_rate": 0.006667286717309247, "optimization": "nesterov_momentum", "nb_data_augmentation": 2, "learning_rate_decay_method": "discrete", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 1.5021265406120413e-08, "rotation_range": [0, 0], "momentum": 0.5946291175670184}, "accuracy_valid_max": 0.8810991034450302, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8767854621611446, "accuracy_valid_std": [0.012667775599983317, 0.01814598045270064, 0.018101634801594355, 0.015340829832581048, 0.015257753281386866, 0.0184059224103138, 0.01846227441881179, 0.015605848605217566, 0.011782137932451845, 0.018623505669981676, 0.01371653951958438, 0.015392480728660268, 0.016786625370854017, 0.013888819855154371, 0.01259357326761986, 0.013980894070904498, 0.016518208722627235, 0.01499643915216198, 0.014227061066676103, 0.015512462900017225, 0.014015549088225493, 0.019910050153784314, 0.01603562942924274, 0.019638806344262173, 0.015402408748944463, 0.014797692118696118, 0.013707368114441152, 0.014819295109387506, 0.01592354929304584, 0.01650428143988006, 0.02073134855337157, 0.014506574909663958, 0.014671151476413418, 0.014190677976987316, 0.015647517497205346, 0.013357974020152761, 0.0182310720501526, 0.017369534551256397, 0.012983409575185694, 0.013690397153227288, 0.016247546526179886, 0.015117973240284542, 0.014194464968016986, 0.014955044576586042, 0.01072759103136687, 0.018952880946766903, 0.021863842016517997, 0.01656937798430406, 0.017267638518095513, 0.015294992685726804, 0.012810177348582821, 0.012422809756664056, 0.01633718097579414, 0.017213389564242506, 0.013787029956325027, 0.009164958690316881, 0.013386034045061106, 0.010350634419511352, 0.014662957347292553, 0.009475304612642592, 0.010843695366249302, 0.01283793170620942, 0.010760186808384564, 0.013186271680451025, 0.013772983487241975, 0.012552525102103551, 0.01306292759878595, 0.015560489798932766, 0.018525250315141052, 0.012015692321329255, 0.01540877559743021, 0.010098889137285193, 0.016136071598232105, 0.01668507954783307, 0.018656685603842465, 0.014588979803153148, 0.01163625760655488, 0.013330122508300834, 0.015484007770810706, 0.013248536175003104, 0.014599879074869293, 0.012288169679272841, 0.013087743240500542, 0.010211887889679354, 0.01983236543770485, 0.011442383068858737, 0.013808126481550857, 0.015560025573862176, 0.014139095739350124, 0.012798124776859258, 0.013673846795961663, 0.012013324225590588, 0.013999918579746794, 0.011265124120556348, 0.010438851778277454, 0.010734440839609226, 0.013629504357222945, 0.011405679989821317, 0.010092769606225516, 0.013044463973357295, 0.008018632507523629, 0.017589402798227655, 0.008028958010372532, 0.011677645227233375, 0.01430565038268779, 0.01760831763009332, 0.013191186626312816, 0.012556099089715264, 0.014659179611625986, 0.009569317095004212, 0.01666628589399123, 0.016515237492751095, 0.01253314667297754, 0.013204414817723626, 0.010315950508981698, 0.008712143782619962, 0.008448459058064727, 0.008411747537349322, 0.009068168893851728, 0.009662177499189944, 0.010849160378681395, 0.011239567448498052, 0.010674525157840377, 0.01052227200624029, 0.009822329539224465, 0.009104094045813582, 0.009963354885158477, 0.009278171754540555, 0.009218227479288474, 0.009245907682344503, 0.010244099986228529, 0.007691434013752604, 0.009381981948944858, 0.008771928667298637, 0.009100013720205228, 0.008990451364479932, 0.009923265117419503, 0.009287024674375033, 0.009515647272192358, 0.011051440165881022, 0.008844385672407251, 0.009536449310121231, 0.009575163384820994, 0.009657765428283912, 0.01010314314588246, 0.009233433256491806, 0.009610892510466724, 0.009441111062236968, 0.00889018340493483, 0.010162321368870532, 0.009165139473447385, 0.008551451330511793, 0.009097340079328968, 0.008355996241159201, 0.008984725720822592, 0.00895461245962763, 0.0099157660779546, 0.009120053198111962, 0.008790788883804981, 0.00908422687628747, 0.009413890986696141, 0.008894012098704772, 0.009616324397676753, 0.009509976037781242, 0.008985393043989582, 0.009647622047649705, 0.010407121108418092, 0.008572095338324521, 0.00934299467632808, 0.009900524510421919, 0.007937678402817205, 0.008713761290216077, 0.01007933004195589, 0.00880181025853272, 0.009839945249473863, 0.009255068186540242, 0.009034276773069087], "accuracy_valid": [0.5243537627070783, 0.5824518778237951, 0.6213834831513554, 0.6792065723832832, 0.7063576571912651, 0.7101109516189759, 0.7363575395331325, 0.7316070924322289, 0.7628379729856928, 0.7562152908509037, 0.7645881377070783, 0.772216796875, 0.7791145048945783, 0.7797866269766567, 0.8024519954819277, 0.7764701383659638, 0.7922275037650602, 0.7996840879141567, 0.8037844738328314, 0.8068053463855422, 0.797619187688253, 0.812145554875753, 0.8186770519578314, 0.8028064406061747, 0.8151267178087349, 0.8141913356551205, 0.8122882153614458, 0.8240378506212349, 0.8068759412650602, 0.8179240399096386, 0.8056552381400602, 0.8129191570971386, 0.8255335796310241, 0.8236819347703314, 0.8099894695971386, 0.8215964443712349, 0.8278323253953314, 0.8256350597703314, 0.8196227291980422, 0.8168357021837349, 0.8316370952560241, 0.8310164486069277, 0.835106539439006, 0.7856754165097892, 0.8290736186935241, 0.8207007718373494, 0.8179637495293675, 0.7975677122552711, 0.8310164486069277, 0.8444853633283133, 0.8359095561935241, 0.8451471903237951, 0.8216979245105422, 0.8236510495105422, 0.8408541392131024, 0.8429911050451807, 0.8455031061746988, 0.8159709149096386, 0.849266695689006, 0.835472750376506, 0.8471312005835843, 0.8425837137612951, 0.8533979668674698, 0.8482695430158133, 0.8369170039533133, 0.8490328501506024, 0.8493387612951807, 0.8300090008471386, 0.8478827419051205, 0.8500402979103916, 0.8544039439006024, 0.8537127023719879, 0.8509653849774097, 0.8429808099585843, 0.8327254329819277, 0.8480151073042168, 0.8503050287085843, 0.8509153802710843, 0.8463061229292168, 0.8319121211408133, 0.8406805934676205, 0.8527155496987951, 0.8001929593373494, 0.8256762401167168, 0.855004000376506, 0.8546892648719879, 0.8494299463478916, 0.8516978068524097, 0.8519934229103916, 0.8562658838478916, 0.8435088008283133, 0.8554422769201807, 0.8209757977221386, 0.8568865304969879, 0.8564085443335843, 0.8132853680346386, 0.8534788568335843, 0.8516375070594879, 0.8273852244917168, 0.8585955148719879, 0.8601118340549698, 0.8482489528426205, 0.8609251458960843, 0.8530920557228916, 0.8600294733621988, 0.8411585796310241, 0.8550348856362951, 0.8557673075112951, 0.855736422251506, 0.8595720773719879, 0.8521449077560241, 0.8522978633283133, 0.8589308405496988, 0.8624002847326807, 0.8573851068335843, 0.8762765907379518, 0.8799901755459337, 0.8783620811370482, 0.8802343161709337, 0.879725444747741, 0.8788606574736446, 0.8765310264495482, 0.8798681052334337, 0.8791253882718373, 0.8782400108245482, 0.8786062217620482, 0.8810991034450302, 0.878016460372741, 0.8781179405120482, 0.878626811935241, 0.8767339867281627, 0.8773958137236446, 0.8772943335843373, 0.8765413215361446, 0.8767854621611446, 0.8772737434111446, 0.8772737434111446, 0.8764192512236446, 0.8771516730986446, 0.877650249435241, 0.8768972373870482, 0.877406108810241, 0.8778737998870482, 0.8759103798004518, 0.8773958137236446, 0.877894390060241, 0.8770296027861446, 0.8765310264495482, 0.8761648155120482, 0.8781282355986446, 0.8773855186370482, 0.8760324501129518, 0.8773958137236446, 0.8765310264495482, 0.8770193076995482, 0.8773855186370482, 0.8782708960843373, 0.8764089561370482, 0.8773958137236446, 0.8773958137236446, 0.8777620246611446, 0.8771516730986446, 0.8764089561370482, 0.8767854621611446, 0.878260600997741, 0.8782503059111446, 0.8774164038968373, 0.8772634483245482, 0.8773958137236446, 0.8773958137236446, 0.8756765342620482, 0.8767751670745482, 0.8775178840361446, 0.8769075324736446, 0.8775178840361446, 0.877650249435241, 0.8767854621611446], "seed": 653557002, "model": "residualv3", "loss_std": [0.27438539266586304, 0.24883219599723816, 0.24771977961063385, 0.24779152870178223, 0.24604958295822144, 0.24810883402824402, 0.2423257678747177, 0.24059215188026428, 0.24100232124328613, 0.2388325035572052, 0.23508991301059723, 0.23273031413555145, 0.22760070860385895, 0.22327202558517456, 0.22022733092308044, 0.2201395332813263, 0.21544842422008514, 0.2122439593076706, 0.20887291431427002, 0.20506244897842407, 0.2055111974477768, 0.2014772891998291, 0.19895626604557037, 0.19410812854766846, 0.19085736572742462, 0.191396564245224, 0.18698297441005707, 0.18503984808921814, 0.18319374322891235, 0.17851652204990387, 0.17842994630336761, 0.175788015127182, 0.1762402504682541, 0.1720656156539917, 0.16794681549072266, 0.16857284307479858, 0.16286547482013702, 0.16494029760360718, 0.15684685111045837, 0.15664280951023102, 0.15649040043354034, 0.15241281688213348, 0.15088051557540894, 0.14919647574424744, 0.14808273315429688, 0.14337928593158722, 0.1443169265985489, 0.14202940464019775, 0.13721628487110138, 0.13530762493610382, 0.13491730391979218, 0.1341347098350525, 0.133879154920578, 0.1333874613046646, 0.12895533442497253, 0.1270843893289566, 0.12583452463150024, 0.12595617771148682, 0.12303982675075531, 0.12337613105773926, 0.1188720241189003, 0.11774710565805435, 0.11827825009822845, 0.11490020900964737, 0.11487647145986557, 0.11187378317117691, 0.11400584131479263, 0.11042887717485428, 0.10774227976799011, 0.10631213337182999, 0.10578776895999908, 0.10547652840614319, 0.10324054956436157, 0.10332505404949188, 0.10210651904344559, 0.10278353095054626, 0.10124523937702179, 0.1015343889594078, 0.10166020691394806, 0.09355563670396805, 0.0955100879073143, 0.0975811704993248, 0.09388000518083572, 0.09274403005838394, 0.09232082217931747, 0.09333813935518265, 0.09108199924230576, 0.09160818159580231, 0.08731210231781006, 0.09019074589014053, 0.08867475390434265, 0.0865032896399498, 0.08966013789176941, 0.08488530665636063, 0.08467958122491837, 0.08372598141431808, 0.08308285474777222, 0.08110400289297104, 0.08403675258159637, 0.08299048990011215, 0.0798170194029808, 0.07827457040548325, 0.0784284770488739, 0.07809162884950638, 0.08144399523735046, 0.07921361923217773, 0.08093510568141937, 0.0727633386850357, 0.07451116293668747, 0.07341857999563217, 0.07363933324813843, 0.07403964549303055, 0.07327432930469513, 0.06950736045837402, 0.07395510375499725, 0.06388334184885025, 0.047248661518096924, 0.042781658470630646, 0.041197314858436584, 0.04060275852680206, 0.03978506103157997, 0.03865138068795204, 0.03722212836146355, 0.03653307631611824, 0.03612582013010979, 0.03604283928871155, 0.036590468138456345, 0.036714985966682434, 0.03567054867744446, 0.03571425750851631, 0.03557892516255379, 0.0354861356317997, 0.034281667321920395, 0.034314267337322235, 0.03416622057557106, 0.03290896490216255, 0.03271118924021721, 0.03201473131775856, 0.032693974673748016, 0.033016033470630646, 0.03251774236559868, 0.031430378556251526, 0.032190244644880295, 0.03256417065858841, 0.03260176256299019, 0.032127510756254196, 0.032600171864032745, 0.034875232726335526, 0.03227551281452179, 0.03244485706090927, 0.03189752995967865, 0.03329430893063545, 0.030892254784703255, 0.03242557495832443, 0.03237326443195343, 0.03289813920855522, 0.03295345604419708, 0.03147467225790024, 0.03396303579211235, 0.03283398225903511, 0.03343205526471138, 0.032693423330783844, 0.03193727880716324, 0.03338875621557236, 0.032762300223112106, 0.033024340867996216, 0.032384127378463745, 0.03271913155913353, 0.03252691403031349, 0.03164670243859291, 0.032006580382585526, 0.03246481716632843, 0.032347917556762695, 0.03234414383769035, 0.03290395066142082, 0.031173331663012505, 0.033058278262615204]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:32 2016", "state": "available"}], "summary": "3658c66d5505ebd61899ac746a1bacd0"}