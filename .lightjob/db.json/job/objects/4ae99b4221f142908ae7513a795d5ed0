{"content": {"hp_model": {"f0": 32, "f1": 16, "f2": 32, "f3": 32, "nonlin": "very_leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.010180663123403738, 0.012982949834162545, 0.012739993492949369, 0.005813666019798394, 0.009417133297980504, 0.007807213202566292, 0.013493793565003246, 0.007512902487186417, 0.007819464988877858, 0.008632187648871964, 0.012712078395929777, 0.011142861489609558, 0.010222906522137788, 0.01194274446897148, 0.013024657537809183, 0.01416383533828451, 0.014058418704246277, 0.014790725875615049, 0.01317139365796621, 0.014695138586525831, 0.014657091262918965, 0.012689487464324044, 0.011877514814876854, 0.012835224830279438, 0.012263553139311338, 0.013571127016394145, 0.015315259065354707, 0.010824183302534669, 0.01588954348499323, 0.015858167241076488, 0.013099297921450854, 0.013163808754484583, 0.014074702496845561, 0.014165535799425429, 0.016078301038492337, 0.01731416652846406, 0.01311387243586308, 0.015976919443997623, 0.013959658950009857, 0.015451254348630815, 0.020424188603740897, 0.01711282523550687, 0.015513755721315747, 0.014869702564853154, 0.016045335293461777, 0.017572140602108825, 0.014973559893876966, 0.017667650724829936, 0.016176123353291764, 0.014672444348097918, 0.018230040575251075, 0.013746079485368533, 0.01467107385183557, 0.0157039052067071, 0.014023717928841673, 0.013354155280884643, 0.01328319802273436, 0.013702068762452956, 0.013680694932896857, 0.016018038764922183, 0.014829791304897593, 0.013038836393836933, 0.015521252853225354, 0.012059423371443665, 0.011944292287149327, 0.012891104641808327, 0.013015511456467056, 0.012886396517713963, 0.013202099382844723, 0.010639958351281086, 0.011121944891489849, 0.011961370530434123, 0.012688249871888178, 0.01171559400990261, 0.011903790814030356, 0.013882188756083774, 0.01198128633012921, 0.010428694052851316, 0.009644526188437415, 0.012021451596433462, 0.011278543130210521, 0.009085605542295198, 0.01129414044227871, 0.013114022862699012, 0.012549636705582424, 0.01170784802202274, 0.010470319367850395, 0.011292750602727736, 0.011676032050292806, 0.009156055665281588, 0.012014983750899771, 0.011962953909112145, 0.01009150002854942, 0.009750916291544743, 0.008799163634378295, 0.009747965629106887, 0.008875937604104562, 0.011017564225356378, 0.011167121164666035, 0.009881170925837334, 0.008979671521918768, 0.0076764425331538235, 0.0059213506360001455, 0.0067194640865879006, 0.007886817874784506, 0.008746400298196825, 0.008920922610413254, 0.007605051032527508, 0.009074577381167062, 0.009103258524095147, 0.007522984076004653, 0.007796351831869186, 0.0073724198796673406, 0.007861158763437646, 0.008290480293018383, 0.006932983463920765, 0.008173150779792759, 0.008275405391271314, 0.005740902899697007, 0.005788144676387667, 0.008670966206574171, 0.005256129131860435, 0.007197124380383669, 0.00408510127962808, 0.006474089796020613, 0.0061105384685694025, 0.005484247214410846, 0.007126366063194974, 0.005855494415002802, 0.005757266339620728, 0.0071411639788095446, 0.008306187094195942, 0.006933880227558606, 0.00830532247891562, 0.007808566920372439, 0.009147637950007479, 0.007472547652239926, 0.006460768118303893, 0.008498193588781776, 0.009019984570479122, 0.0068172264177602014, 0.00719783527696207, 0.006886878859172336, 0.005716002944043652], "moving_avg_accuracy_train": [0.03582558644218345, 0.09119919459729141, 0.14441766828309566, 0.20158994405583813, 0.2582501352641801, 0.3110760199301301, 0.36101153376629763, 0.40651378498430607, 0.4485911831043232, 0.4876601856122279, 0.5232688966848644, 0.5556116700608593, 0.5855922050492824, 0.6133650929388264, 0.6382282306549105, 0.6610260146803664, 0.6821182239127728, 0.7013824110326159, 0.7190621926083871, 0.7354085465122309, 0.7503620805018808, 0.7639155200961187, 0.7763672371952647, 0.7881011948272517, 0.7986549614937052, 0.8083856861303719, 0.8173827565331154, 0.8256660957515278, 0.8335024975504984, 0.840541128032621, 0.8469804911141411, 0.853080620528013, 0.8585545330564687, 0.8637273759106132, 0.8684294374555338, 0.8728752065364386, 0.8768647008675676, 0.8806295237798418, 0.8840715591164202, 0.8872855041645419, 0.8901918253566148, 0.8930216444128691, 0.8956265021372799, 0.8982500000928486, 0.9005924028671093, 0.9026913008175247, 0.9047710072240982, 0.9067192926566349, 0.9086495329530792, 0.9103796656269942, 0.9120043945930878, 0.9135248875292663, 0.914997746575343, 0.9163884959811162, 0.9177563918379694, 0.9189084430496136, 0.9200800756734083, 0.9211671692157943, 0.9223314932110661, 0.923281692507992, 0.9241277515240809, 0.9249752351445133, 0.9257865461861713, 0.9264726564427387, 0.9273714986796018, 0.9281245810237125, 0.9288907468369928, 0.929499024007068, 0.9301556833565457, 0.9308256957817808, 0.931568179844245, 0.9321759616314153, 0.9327462167279637, 0.9332966126469908, 0.9338733852312673, 0.9343087751999732, 0.9348354487539423, 0.935311780101324, 0.9358289421151857, 0.9365524073478809, 0.937022128401345, 0.93756109874112, 0.9381065898671465, 0.9385255604139313, 0.9389561123286567, 0.9393505124007007, 0.9396055271155493, 0.9400047401731897, 0.9403804161131889, 0.940827770404417, 0.9412606162010462, 0.9417152815846791, 0.9420639544632636, 0.9425614107611233, 0.9430625998518161, 0.9434601916108205, 0.9438226744915436, 0.9442326504901559, 0.944508622936526, 0.9448104765608782, 0.9451007099644527, 0.9453548364347848, 0.9456929043497689, 0.9461064474673022, 0.94633683824452, 0.9466813737237778, 0.9470426089289193, 0.9472956049516328, 0.9476418839613606, 0.9479349699284582, 0.9482126981917032, 0.9485463229369477, 0.9488512355052869, 0.9491814603882207, 0.9494623146435569, 0.9497103971269217, 0.9499640424917489, 0.9501597351879413, 0.9504033600276282, 0.950599226699976, 0.9508661875086605, 0.9510530098626764, 0.9512142105836808, 0.9514382741944711, 0.9516516292858676, 0.9519063197395251, 0.9521216263037786, 0.952313113111616, 0.9524668139993747, 0.9526563701698045, 0.9528571255600777, 0.9529820739375325, 0.9532037373736519, 0.9533521172411686, 0.9535786290254958, 0.953722143908799, 0.9539627702513539, 0.9541561545691956, 0.9542999374719106, 0.9545362989295921, 0.9547536745391245, 0.9549539628853229, 0.9551761471731102, 0.955366776388062], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 751171178, "moving_var_accuracy_train": [0.011551253795337233, 0.03799225673684246, 0.05968288453617798, 0.08313261813587083, 0.10371275173217662, 0.11845664337562123, 0.1290529789167978, 0.1347817748182788, 0.1372381642294047, 0.13725183041912822, 0.13493847011550597, 0.13085911801081407, 0.1258626985134612, 0.12021842837764166, 0.11376016609367817, 0.10706180009255231, 0.10035955169602954, 0.09366357667492153, 0.08711039109653221, 0.08080418156042637, 0.07473623701340029, 0.06891587483557217, 0.06341969468046943, 0.05831689706780588, 0.0534876452786748, 0.048991063768399026, 0.04482048287404644, 0.040955957964107385, 0.03741304490609082, 0.03411762128705652, 0.031079047730411664, 0.02830604716716437, 0.025745115915770583, 0.023411429052936597, 0.021269270592592926, 0.019320227297820187, 0.01753144915320117, 0.01590586926192812, 0.014421910801059598, 0.013072684705904756, 0.011841436561157713, 0.010729363788062204, 0.009717494963135787, 0.008807690140528072, 0.007976302783287044, 0.007218320858414658, 0.006535415381211078, 0.005916036188229681, 0.00535796501782487, 0.004849108747666519, 0.004387955570819236, 0.00396996710265803, 0.0035924942163187167, 0.0032506524498737716, 0.0029424274565631628, 0.002660129708855104, 0.002406471245015853, 0.002176460071843344, 0.001971014917952701, 0.0017820393344923368, 0.001610277743771452, 0.0014557140257764186, 0.0013160666536556217, 0.0011886967138475628, 0.0010770982987637278, 0.0009744926660404564, 0.0008823264899173658, 0.0007974238509663412, 0.0007215622793810163, 0.000653446301292639, 0.0005930632144104956, 0.000537081481276788, 0.000486300051025364, 0.0004403964669319637, 0.0003993508197645249, 0.0003611218176117212, 0.0003275061011426029, 0.0002967975150008288, 0.0002695248724379792, 0.0002472830026804493, 0.00022454044322501058, 0.00020470080014692473, 0.0001869087652493944, 0.00016979771559611336, 0.00015448631859796508, 0.00014043764948962258, 0.00012697917708376472, 0.00011571559896390315, 0.0001054142307745616, 9.667394045402783e-05, 8.869274576156098e-05, 8.168395668507155e-05, 7.460971600290808e-05, 6.937590931713956e-05, 6.469903292709059e-05, 5.965184249583569e-05, 5.486920279560763e-05, 5.089500539099056e-05, 4.649095197229126e-05, 4.266189726987317e-05, 3.915382639983933e-05, 3.58196661261666e-05, 3.326630874982489e-05, 3.1478839065374505e-05, 2.880867435087994e-05, 2.6996149183998925e-05, 2.547095212650228e-05, 2.3499919801431513e-05, 2.22291101944911e-05, 2.077929363202786e-05, 1.939556116267078e-05, 1.8457754282159423e-05, 1.744872392292428e-05, 1.668528779040988e-05, 1.57266710260334e-05, 1.470790819040237e-05, 1.3816141111247065e-05, 1.277918768220981e-05, 1.203544647660087e-05, 1.1177175608970633e-05, 1.070087070843526e-05, 9.944906965231848e-06, 9.184287320779755e-06, 8.717699103825137e-06, 8.255612748665402e-06, 8.013856518457763e-06, 7.629683116107683e-06, 7.196719582678539e-06, 6.6896632904908735e-06, 6.344080837173945e-06, 6.072397293970041e-06, 5.605666437830223e-06, 5.487311904257494e-06, 5.136729979590146e-06, 5.0848252775830036e-06, 4.761711445390307e-06, 4.8066496314336275e-06, 4.6625621177742285e-06, 4.382367614015154e-06, 4.44693150070993e-06, 4.4275077512155015e-06, 4.345795770699537e-06, 4.355508913285891e-06, 4.247013500295587e-06], "duration": 104926.024342, "accuracy_train": [0.3582558644218346, 0.5895616679932631, 0.6233839314553341, 0.7161404260105205, 0.768191856139258, 0.7865089819236802, 0.8104311582918051, 0.8160340459463824, 0.8272877661844776, 0.8392812081833703, 0.8437472963385935, 0.8466966304448136, 0.8554170199450905, 0.8633210839447213, 0.8619964700996677, 0.8662060709094684, 0.8719481070044297, 0.8747600951112033, 0.8781802267903286, 0.8825257316468254, 0.8849438864087301, 0.8858964764442598, 0.8884326910875784, 0.8937068135151348, 0.8936388614917867, 0.8959622078603728, 0.8983563901578073, 0.9002161487172389, 0.904030113741233, 0.9038888023717239, 0.904934758847822, 0.9079817852528608, 0.9078197458125692, 0.9102829615979143, 0.9107479913598191, 0.912887128264581, 0.9127701498477298, 0.9145129299903102, 0.9150498771456257, 0.9162110095976375, 0.9163487160852714, 0.9184900159191584, 0.9190702216569768, 0.9218614816929678, 0.9216740278354559, 0.9215813823712625, 0.9234883648832595, 0.9242538615494648, 0.926021695621078, 0.9259508596922297, 0.9266269552879292, 0.9272093239548725, 0.9282534779900333, 0.928905240633075, 0.9300674545496493, 0.9292769039544113, 0.93062476928756, 0.9309510110972684, 0.9328104091685124, 0.9318334861803249, 0.9317422826688816, 0.9326025877284054, 0.9330883455610927, 0.9326476487518457, 0.9354610788113695, 0.9349023221207088, 0.9357862391565154, 0.9349735185377446, 0.9360656175018457, 0.9368558076088963, 0.9382505364064231, 0.9376459977159468, 0.9378785125968992, 0.9382501759182356, 0.9390643384897563, 0.9382272849183279, 0.939575510739664, 0.9395987622277593, 0.9404834002399409, 0.9430635944421374, 0.9412496178825213, 0.9424118317990956, 0.9430160100013842, 0.9422962953349945, 0.942831079561185, 0.9429001130490956, 0.9419006595491879, 0.9435976576919527, 0.943761499573182, 0.9448539590254706, 0.9451562283707088, 0.9458072700373754, 0.9452020103705242, 0.9470385174418604, 0.947573301668051, 0.9470385174418604, 0.947085020418051, 0.947922434477667, 0.9469923749538575, 0.947527159180048, 0.9477128105966224, 0.947641974667774, 0.9487355155846253, 0.9498283355251015, 0.9484103552394795, 0.9497821930370985, 0.9502937257751938, 0.9495725691560539, 0.950758395048911, 0.9505727436323367, 0.9507122525609081, 0.9515489456441492, 0.9515954486203396, 0.9521534843346253, 0.9519900029415835, 0.9519431394772055, 0.9522468507751938, 0.9519209694536729, 0.9525959835848099, 0.9523620267511074, 0.9532688347868217, 0.9527344110488187, 0.9526650170727206, 0.9534548466915835, 0.9535718251084349, 0.9541985338224437, 0.9540593853820598, 0.9540364943821521, 0.9538501219892026, 0.9543623757036729, 0.954663924072536, 0.9541066093346253, 0.9551987082987264, 0.9546875360488187, 0.9556172350844407, 0.9550137778585271, 0.9561284073343485, 0.9558966134297711, 0.9555939835963455, 0.9566635520487264, 0.9567100550249169, 0.9567565580011074, 0.9571758057631967, 0.9570824393226283], "end": "2016-01-25 09:35:53.508000", "learning_rate_per_epoch": [0.008321454748511314, 0.004160727374255657, 0.002773818327113986, 0.0020803636871278286, 0.001664291019551456, 0.001386909163556993, 0.0011887792497873306, 0.0010401818435639143, 0.0009246061090379953, 0.000832145509775728, 0.0007564959232695401, 0.0006934545817784965, 0.0006401119171641767, 0.0005943896248936653, 0.000554763653781265, 0.0005200909217819571, 0.0004894973244518042, 0.00046230305451899767, 0.0004379713209345937, 0.000416072754887864, 0.00039625976933166385, 0.00037824796163477004, 0.00036180237657390535, 0.00034672729088924825, 0.0003328582097310573, 0.00032005595858208835, 0.00030820202664472163, 0.00029719481244683266, 0.00028694671345874667, 0.0002773818268906325, 0.0002684340288396925, 0.0002600454608909786, 0.00025216530775651336, 0.0002447486622259021, 0.00023775585577823222, 0.00023115152725949883, 0.00022490418632514775, 0.00021898566046729684, 0.00021337064390536398, 0.000208036377443932, 0.00020296231377869844, 0.00019812988466583192, 0.00019352220988366753, 0.00018912398081738502, 0.00018492122762836516, 0.00018090118828695267, 0.0001770522358128801, 0.00017336364544462413, 0.00016982560919132084, 0.00016642910486552864, 0.00016316577966790646, 0.00016002797929104418, 0.0001570085878483951, 0.00015410101332236081, 0.00015129918756429106, 0.00014859740622341633, 0.0001459904306102544, 0.00014347335672937334, 0.0001410416152793914, 0.00013869091344531626, 0.00013641729310620576, 0.00013421701441984624, 0.00013208658492658287, 0.0001300227304454893, 0.0001280223805224523, 0.00012608265387825668, 0.0001242008147528395, 0.00012237433111295104, 0.00012060079461662099, 0.00011887792788911611, 0.00011720359179889783, 0.00011557576362974942, 0.00011399253708077595, 0.00011245209316257387, 0.00011095272930106148, 0.00010949283023364842, 0.00010807084618136287, 0.00010668532195268199, 0.00010533487511565909, 0.000104018188721966, 0.00010273401130689308, 0.00010148115688934922, 0.00010025849041994661, 9.906494233291596e-05, 9.789947216631845e-05, 9.676110494183376e-05, 9.56489093368873e-05, 9.456199040869251e-05, 9.349949687020853e-05, 9.246061381418258e-05, 9.144456271314994e-05, 9.045059414347634e-05, 8.947800961323082e-05, 8.852611790644005e-05, 8.759426418691874e-05, 8.668182272231206e-05, 8.578819688409567e-05, 8.491280459566042e-05, 8.405510016018525e-05, 8.321455243276432e-05, 8.239064482040703e-05, 8.158288983395323e-05, 8.079082181211561e-05, 8.001398964552209e-05, 7.92519495007582e-05, 7.850429392419755e-05, 7.777060818625614e-05, 7.705050666118041e-05, 7.634362555108964e-05, 7.564959378214553e-05, 7.496806210838258e-05, 7.429870311170816e-05, 7.364119664998725e-05, 7.29952153051272e-05, 7.236047531478107e-05, 7.173667836468667e-05, 7.112354796845466e-05, 7.05208076396957e-05, 6.992819544393569e-05, 6.934545672265813e-05, 6.877235136926174e-05, 6.820864655310288e-05, 6.765410216758028e-05, 6.710850720992312e-05, 6.657163612544537e-05, 6.604329246329144e-05, 6.552326522069052e-05, 6.501136522274464e-05, 6.450740329455584e-05, 6.401119026122615e-05, 6.352255877573043e-05, 6.304132693912834e-05, 6.256732740439475e-05, 6.210040737641975e-05, 6.164040678413585e-05, 6.118716555647552e-05, 6.07405454502441e-05, 6.0300397308310494e-05, 5.9866582887480035e-05, 5.9438963944558054e-05, 5.9017409512307495e-05, 5.8601795899448916e-05, 5.819199213874526e-05, 5.778788181487471e-05], "accuracy_valid": [0.35585996329066266, 0.5852197853915663, 0.6179360998682228, 0.7056973009224398, 0.7534282638365963, 0.7725741834525602, 0.7957896037274097, 0.8018519390060241, 0.8078333843185241, 0.8182402461408133, 0.8239378412085843, 0.8258806711219879, 0.8329504541603916, 0.8387598244540663, 0.8370302499058735, 0.8396143166415663, 0.8442838737763554, 0.8451383659638554, 0.8477018425263554, 0.8494108269013554, 0.849400531814759, 0.8518316429781627, 0.8556467079254518, 0.8581292945218373, 0.8557893684111446, 0.8562776496611446, 0.8609575018825302, 0.8574777626129518, 0.8641519201807228, 0.8588823065700302, 0.8606927710843373, 0.8627782614834337, 0.8613237128200302, 0.8615678534450302, 0.8602250800075302, 0.8628194418298193, 0.8611810523343373, 0.8632974279932228, 0.8610692771084337, 0.8620664297816265, 0.8643048757530121, 0.8640298498682228, 0.8647519766566265, 0.8633989081325302, 0.8651284826807228, 0.8636739340173193, 0.8633989081325302, 0.8668580572289157, 0.8626767813441265, 0.8632768378200302, 0.8655255788780121, 0.8625341208584337, 0.8654741034450302, 0.8648843420557228, 0.8685464514307228, 0.8658403143825302, 0.8677934393825302, 0.8653520331325302, 0.8665727362575302, 0.8650064123682228, 0.8657285391566265, 0.8666948065700302, 0.8652402579066265, 0.865687358810241, 0.8663080054593373, 0.8665727362575302, 0.8670610175075302, 0.8663285956325302, 0.8675492987575302, 0.865199077560241, 0.8669183570218373, 0.8655858786709337, 0.8668065817959337, 0.8672639777861446, 0.8660638648343373, 0.8668271719691265, 0.8655858786709337, 0.8645784309111446, 0.8675287085843373, 0.8675287085843373, 0.8670404273343373, 0.867396343185241, 0.8664403708584337, 0.8676816641566265, 0.8682817206325302, 0.8682714255459337, 0.867030132247741, 0.867274272872741, 0.8672948630459337, 0.8674978233245482, 0.8687597067959337, 0.8683934958584337, 0.8675287085843373, 0.8693391730986446, 0.868494975997741, 0.867640483810241, 0.8692171027861446, 0.8683832007718373, 0.8673860480986446, 0.868372905685241, 0.8671419074736446, 0.8660226844879518, 0.8666330360504518, 0.8673757530120482, 0.8670095420745482, 0.8668977668486446, 0.8682611304593373, 0.8655241081513554, 0.8676301887236446, 0.8681184699736446, 0.8673757530120482, 0.8653108527861446, 0.8681081748870482, 0.8683317253388554, 0.8675081184111446, 0.8676095985504518, 0.866734516189759, 0.8678743293486446, 0.8678743293486446, 0.8656564735504518, 0.8679861045745482, 0.8680978798004518, 0.8668668815888554, 0.8677213737763554, 0.866124164627259, 0.8677316688629518, 0.8687082313629518, 0.8672536826995482, 0.8689626670745482, 0.8669889519013554, 0.8670095420745482, 0.8677522590361446, 0.8684846809111446, 0.8662874152861446, 0.8676198936370482, 0.865687358810241, 0.8671419074736446, 0.8640695594879518, 0.8665315559111446, 0.8651681923004518, 0.8652799675263554, 0.8654329230986446, 0.8665109657379518, 0.8661344597138554], "accuracy_test": 0.74345703125, "start": "2016-01-24 04:27:07.484000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0], "accuracy_train_last": 0.9570824393226283, "batch_size_eval": 1024, "accuracy_train_std": [0.016874346118492086, 0.018570922225927496, 0.018158068148992724, 0.0158939349641061, 0.015858830803071735, 0.016829805767454215, 0.016826630191293865, 0.01682501896241537, 0.014776819866836193, 0.016743512887533372, 0.015615611546472693, 0.01619609197312717, 0.014665402563266431, 0.01370165749525555, 0.013864294539878816, 0.013674343584424382, 0.01383366904984041, 0.013732778072848615, 0.011827331527214466, 0.012963586795741467, 0.013031331484686014, 0.01231753536482776, 0.012677744554789091, 0.012357793748457527, 0.011283459782235699, 0.012584253638329889, 0.013288544648910739, 0.011863766373790049, 0.012419351267781731, 0.013204535686991403, 0.012453094576461463, 0.012128966437990994, 0.011243542544674839, 0.011587289571308796, 0.011767433189531532, 0.011544362011123015, 0.011875698467160445, 0.012286266934177713, 0.011204992070893333, 0.011166996917513678, 0.011910813444081281, 0.010493395535115869, 0.011802252640996753, 0.010636971430396, 0.011125953689371148, 0.010715897682249174, 0.010911822422026676, 0.010832362656045603, 0.01048048504748644, 0.010584835191884561, 0.009713726199527586, 0.009195329940361008, 0.010553542403061864, 0.009742019798417033, 0.009372746310888868, 0.00937872000560231, 0.009445618320062927, 0.009621458054809438, 0.009754500290972001, 0.010605295532188896, 0.009398386317707891, 0.008667779064507476, 0.01010921623721082, 0.009923665134244877, 0.009227520309019865, 0.010245073122448786, 0.009098866232883222, 0.009013098281375032, 0.009490125784326663, 0.009773909055242974, 0.01009365562933315, 0.009483421391524275, 0.009479384890326827, 0.009571995937498905, 0.009564679016923396, 0.009809021670962585, 0.009236465084845793, 0.009563447355671162, 0.008620806416757958, 0.00888094749623208, 0.008770563819372945, 0.008879561303634152, 0.00910239101048828, 0.008468796801424835, 0.008331587706471149, 0.008164491548868447, 0.008502812208149038, 0.008614835713327823, 0.008007342120731151, 0.007939325837891762, 0.007350006025133538, 0.007735770765668781, 0.00855130484213746, 0.007900570507096586, 0.008198318024117255, 0.00819402055507244, 0.008069531886300605, 0.007201472350719355, 0.00813665535878861, 0.008104885913274644, 0.007912724890509405, 0.00903488033423205, 0.008790588762749489, 0.00820392855356781, 0.008143375680530283, 0.008446943440975345, 0.007929902527937386, 0.008106270485894507, 0.00788492422427048, 0.007548716848216105, 0.007803852629738853, 0.007904004794904733, 0.00777710085836682, 0.00753609444442738, 0.00783460336938383, 0.008281449352916763, 0.007249376974480315, 0.007622558305080351, 0.007305865060349584, 0.00793696172054325, 0.007704290544075971, 0.007484739733459491, 0.0071784715361597334, 0.007289528032107305, 0.007508586843566271, 0.007783588628314291, 0.007999917860593248, 0.007303758747352816, 0.007804951869577851, 0.007588844572349657, 0.008168652847759798, 0.00746211041013933, 0.007819153342518231, 0.007642637971225198, 0.007847908552319927, 0.007891034757893736, 0.007666740836582294, 0.007506564290473004, 0.0074319353421510425, 0.007524575676367721, 0.007702638858158391, 0.007953859395788413, 0.007096811559197973, 0.007143226625688096], "accuracy_test_std": 0.010707062602562633, "error_valid": [0.6441400367093373, 0.41478021460843373, 0.38206390013177716, 0.29430269907756024, 0.24657173616340367, 0.22742581654743976, 0.2042103962725903, 0.19814806099397586, 0.19216661568147586, 0.18175975385918675, 0.17606215879141573, 0.17411932887801207, 0.1670495458396084, 0.16124017554593373, 0.1629697500941265, 0.16038568335843373, 0.1557161262236446, 0.1548616340361446, 0.1522981574736446, 0.1505891730986446, 0.15059946818524095, 0.14816835702183728, 0.14435329207454817, 0.14187070547816272, 0.1442106315888554, 0.1437223503388554, 0.13904249811746983, 0.14252223738704817, 0.13584807981927716, 0.14111769342996983, 0.13930722891566272, 0.13722173851656627, 0.13867628717996983, 0.13843214655496983, 0.13977491999246983, 0.1371805581701807, 0.13881894766566272, 0.13670257200677716, 0.13893072289156627, 0.1379335702183735, 0.13569512424698793, 0.13597015013177716, 0.1352480233433735, 0.13660109186746983, 0.13487151731927716, 0.1363260659826807, 0.13660109186746983, 0.13314194277108427, 0.1373232186558735, 0.13672316217996983, 0.13447442112198793, 0.13746587914156627, 0.13452589655496983, 0.13511565794427716, 0.13145354856927716, 0.13415968561746983, 0.13220656061746983, 0.13464796686746983, 0.13342726374246983, 0.13499358763177716, 0.1342714608433735, 0.13330519342996983, 0.1347597420933735, 0.13431264118975905, 0.13369199454066272, 0.13342726374246983, 0.13293898249246983, 0.13367140436746983, 0.13245070124246983, 0.13480092243975905, 0.13308164297816272, 0.13441412132906627, 0.13319341820406627, 0.1327360222138554, 0.13393613516566272, 0.1331728280308735, 0.13441412132906627, 0.1354215690888554, 0.13247129141566272, 0.13247129141566272, 0.13295957266566272, 0.13260365681475905, 0.13355962914156627, 0.1323183358433735, 0.13171827936746983, 0.13172857445406627, 0.13296986775225905, 0.13272572712725905, 0.13270513695406627, 0.13250217667545183, 0.13124029320406627, 0.13160650414156627, 0.13247129141566272, 0.1306608269013554, 0.13150502400225905, 0.13235951618975905, 0.1307828972138554, 0.13161679922816272, 0.1326139519013554, 0.13162709431475905, 0.1328580925263554, 0.13397731551204817, 0.13336696394954817, 0.13262424698795183, 0.13299045792545183, 0.1331022331513554, 0.13173886954066272, 0.1344758918486446, 0.1323698112763554, 0.1318815300263554, 0.13262424698795183, 0.1346891472138554, 0.13189182511295183, 0.1316682746611446, 0.1324918815888554, 0.13239040144954817, 0.13326548381024095, 0.1321256706513554, 0.1321256706513554, 0.13434352644954817, 0.13201389542545183, 0.13190212019954817, 0.1331331184111446, 0.1322786262236446, 0.13387583537274095, 0.13226833113704817, 0.13129176863704817, 0.13274631730045183, 0.13103733292545183, 0.1330110480986446, 0.13299045792545183, 0.1322477409638554, 0.1315153190888554, 0.1337125847138554, 0.13238010636295183, 0.13431264118975905, 0.1328580925263554, 0.13593044051204817, 0.1334684440888554, 0.13483180769954817, 0.1347200324736446, 0.1345670769013554, 0.13348903426204817, 0.1338655402861446], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-3, 3], "momentum": 0.559745441071551, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.008321454943993426, "patience_threshold": 1, "do_flip": true, "batch_size": 32, "optimization": "rmsprop", "nb_data_augmentation": 4, "learning_rate_decay_method": "lin", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 3.372677202886078e-07, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.07139537294447477}, "accuracy_valid_max": 0.8693391730986446, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-3, 3)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8661344597138554, "loss_train": [1.670907974243164, 1.252321481704712, 0.981151819229126, 0.8403873443603516, 0.7566641569137573, 0.698857843875885, 0.6564080715179443, 0.6234474778175354, 0.5959917306900024, 0.5717324614524841, 0.5531224608421326, 0.5343568921089172, 0.520017683506012, 0.5075987577438354, 0.49396812915802, 0.4834320545196533, 0.4725620150566101, 0.46328118443489075, 0.45331981778144836, 0.4448988139629364, 0.43726348876953125, 0.4286441504955292, 0.42408430576324463, 0.418006956577301, 0.40981540083885193, 0.403169184923172, 0.39864885807037354, 0.3945333659648895, 0.38822415471076965, 0.3820323944091797, 0.37911486625671387, 0.37455058097839355, 0.37068936228752136, 0.366591215133667, 0.3614075183868408, 0.3570820689201355, 0.3539538085460663, 0.351172536611557, 0.3476295471191406, 0.3442658483982086, 0.3407086730003357, 0.3384380638599396, 0.33594048023223877, 0.33199697732925415, 0.3297102451324463, 0.3274511694908142, 0.323003888130188, 0.3206171691417694, 0.31814780831336975, 0.31766772270202637, 0.31265789270401, 0.3109928071498871, 0.3106980323791504, 0.30771946907043457, 0.30610695481300354, 0.30184924602508545, 0.3000885844230652, 0.2988464832305908, 0.29597169160842896, 0.29492780566215515, 0.29307445883750916, 0.29160428047180176, 0.2894512116909027, 0.2876664996147156, 0.2856643795967102, 0.28548258543014526, 0.28260061144828796, 0.27999183535575867, 0.2780338227748871, 0.27673065662384033, 0.27606305480003357, 0.274440199136734, 0.27362653613090515, 0.2726624011993408, 0.2713645100593567, 0.27073025703430176, 0.2683653235435486, 0.26671549677848816, 0.26685744524002075, 0.26336055994033813, 0.2626616060733795, 0.26101312041282654, 0.26127418875694275, 0.2590426504611969, 0.257686585187912, 0.2570095956325531, 0.2556901276111603, 0.253868967294693, 0.2530922293663025, 0.25265517830848694, 0.25220802426338196, 0.2504192888736725, 0.24947258830070496, 0.24890950322151184, 0.2468557059764862, 0.24673517048358917, 0.24355186522006989, 0.2439381629228592, 0.24295909702777863, 0.24180205166339874, 0.24109429121017456, 0.23987138271331787, 0.23771071434020996, 0.23771151900291443, 0.23732037842273712, 0.23705816268920898, 0.23564286530017853, 0.23465529084205627, 0.2329963594675064, 0.23445037007331848, 0.2314491719007492, 0.2337847203016281, 0.23143945634365082, 0.23040032386779785, 0.22917351126670837, 0.2270074337720871, 0.22686301171779633, 0.2261618971824646, 0.22711940109729767, 0.22533012926578522, 0.22466939687728882, 0.22301502525806427, 0.22408467531204224, 0.22288745641708374, 0.22173194587230682, 0.219668909907341, 0.22067324817180634, 0.21981093287467957, 0.21984730660915375, 0.21842792630195618, 0.2172156423330307, 0.21809744834899902, 0.21579574048519135, 0.21550750732421875, 0.21454142034053802, 0.21581725776195526, 0.21409979462623596, 0.2130633294582367, 0.21245479583740234, 0.21079237759113312, 0.21132799983024597, 0.2101808339357376, 0.21021756529808044, 0.2094350904226303], "accuracy_train_first": 0.3582558644218346, "model": "residualv3", "loss_std": [0.3230101466178894, 0.20205119252204895, 0.19188982248306274, 0.1855308711528778, 0.18023379147052765, 0.17637285590171814, 0.17186060547828674, 0.1713872104883194, 0.16645175218582153, 0.16466233134269714, 0.16255182027816772, 0.1594357043504715, 0.15820492804050446, 0.15723085403442383, 0.15422652661800385, 0.15363913774490356, 0.15154734253883362, 0.15004907548427582, 0.14883525669574738, 0.1476115733385086, 0.1454841047525406, 0.14405253529548645, 0.14418083429336548, 0.14426842331886292, 0.14125235378742218, 0.14050795137882233, 0.13997384905815125, 0.13882368803024292, 0.1360578089952469, 0.13693895936012268, 0.13699181377887726, 0.134561687707901, 0.13438501954078674, 0.13500289618968964, 0.13348236680030823, 0.13082720339298248, 0.1317310631275177, 0.1314462572336197, 0.13058030605316162, 0.13072645664215088, 0.12901556491851807, 0.12897220253944397, 0.1290183663368225, 0.1257353574037552, 0.1276743859052658, 0.12765568494796753, 0.12520185112953186, 0.12489891797304153, 0.12534327805042267, 0.12646308541297913, 0.1231301799416542, 0.12291286885738373, 0.12211917340755463, 0.12326617538928986, 0.12232528626918793, 0.12031736224889755, 0.12134487926959991, 0.12031057476997375, 0.1185607984662056, 0.11888264119625092, 0.11773756891489029, 0.11881309747695923, 0.11843094229698181, 0.11674830317497253, 0.11625055223703384, 0.11820640414953232, 0.11568214744329453, 0.11528132855892181, 0.11543005704879761, 0.11338329315185547, 0.11430946737527847, 0.11416083574295044, 0.11161201447248459, 0.11338294297456741, 0.11459629237651825, 0.11238007992506027, 0.11282297223806381, 0.1110428050160408, 0.11143629252910614, 0.11182643473148346, 0.11040574312210083, 0.1120845302939415, 0.11052064597606659, 0.10977191478013992, 0.10988018661737442, 0.111140176653862, 0.10734862089157104, 0.10712186247110367, 0.10926016420125961, 0.10789331048727036, 0.1090797483921051, 0.10681170225143433, 0.10650187730789185, 0.10681776702404022, 0.10486164689064026, 0.10702057927846909, 0.10405179113149643, 0.10567367076873779, 0.10495641827583313, 0.1033843383193016, 0.10542430728673935, 0.10428345203399658, 0.10358791053295135, 0.10463166236877441, 0.10328304767608643, 0.10326560586690903, 0.10347968339920044, 0.1026223823428154, 0.10214719921350479, 0.10281269252300262, 0.10099762678146362, 0.10129614919424057, 0.10150810331106186, 0.10184838622808456, 0.10049323737621307, 0.0998859629034996, 0.1003631055355072, 0.10038205981254578, 0.09911730885505676, 0.09915034472942352, 0.09960699826478958, 0.09787921607494354, 0.09929398447275162, 0.0984358936548233, 0.09723746031522751, 0.09604520350694656, 0.0976693332195282, 0.09858133643865585, 0.09753085672855377, 0.09866634756326675, 0.09705605357885361, 0.09613273292779922, 0.09657785296440125, 0.09496115893125534, 0.09681437164545059, 0.0973036140203476, 0.09635838866233826, 0.09369082003831863, 0.09606467932462692, 0.09254259616136551, 0.09592966735363007, 0.09446458518505096, 0.09491552412509918, 0.0940900519490242]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:12 2016", "state": "available"}], "summary": "6802bb2422958a062263ebf243c11826"}