{"content": {"hp_model": {"f0": 64, "f1": 32, "f2": 32, "f3": 16, "nonlin": "rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [2.2850513458251953, 1.892858624458313, 1.7437434196472168, 1.6558817625045776, 1.6011615991592407, 1.5600711107254028, 1.5241117477416992, 1.4898602962493896, 1.4564158916473389, 1.4236574172973633, 1.3918788433074951, 1.361655831336975, 1.3335161209106445, 1.3075718879699707, 1.283647894859314, 1.2615212202072144, 1.2408643960952759, 1.2212692499160767, 1.202599287033081, 1.1844508647918701, 1.1669245958328247, 1.1498727798461914, 1.1333096027374268, 1.1170058250427246, 1.1011759042739868, 1.0857040882110596, 1.0707719326019287, 1.0560346841812134, 1.041557788848877, 1.027552604675293, 1.0139104127883911, 1.000731348991394, 0.9879163503646851, 0.9754139184951782, 0.963236927986145, 0.9513704776763916, 0.9399531483650208, 0.9286478161811829, 0.917690098285675, 0.9069317579269409, 0.8963894844055176, 0.8859676122665405, 0.8757018446922302, 0.8658173680305481, 0.8559922575950623, 0.8463999629020691, 0.8369052410125732, 0.8275384902954102, 0.8183296322822571, 0.8091384768486023, 0.8000940680503845, 0.7912328839302063, 0.7824648022651672, 0.7737158536911011, 0.7650930285453796, 0.7565710544586182, 0.7480602264404297, 0.7395597696304321, 0.7311364412307739, 0.7228304743766785, 0.7145149111747742, 0.7062260508537292, 0.6978927850723267, 0.6895134449005127, 0.6811383366584778, 0.6727585196495056, 0.6644178032875061, 0.6561064720153809, 0.6477251052856445, 0.639548659324646, 0.6310734748840332, 0.6228660345077515, 0.6145104169845581, 0.6061893105506897, 0.5977999567985535, 0.5894510746002197, 0.5810511112213135, 0.5725648403167725, 0.5640552639961243, 0.5554912090301514, 0.547078549861908, 0.538528561592102, 0.5299068689346313, 0.5210919976234436, 0.5122944116592407, 0.5036520957946777, 0.4947796165943146, 0.4758583605289459, 0.464486300945282, 0.461586594581604, 0.4594458043575287, 0.4575822055339813, 0.4558943808078766, 0.45426979660987854, 0.4526905119419098, 0.4511871039867401, 0.44973891973495483, 0.44829651713371277, 0.4468935430049896, 0.4455001950263977, 0.4441331923007965, 0.4427800476551056, 0.4414371848106384, 0.4401169419288635, 0.4388033449649811, 0.43750715255737305, 0.4362119734287262, 0.43490540981292725, 0.43303021788597107, 0.43002259731292725, 0.42971867322921753, 0.4297029972076416, 0.4297028183937073, 0.4297028183937073, 0.4297028183937073, 0.4297028183937073, 0.4297028183937073, 0.4297028183937073, 0.4297028183937073, 0.4297028183937073, 0.4297028183937073, 0.4297028183937073, 0.4297028183937073, 0.4297028183937073, 0.4297028183937073, 0.4297028183937073, 0.4297028183937073, 0.4297028183937073, 0.4297028183937073, 0.4297028183937073, 0.4297028183937073, 0.4297028183937073, 0.4297028183937073, 0.4297028183937073, 0.4297028183937073, 0.4297028183937073, 0.4297028183937073, 0.4297028183937073, 0.4297028183937073, 0.4297028183937073, 0.4297028183937073, 0.4297028183937073, 0.4297028183937073, 0.4297028183937073, 0.4297028183937073, 0.4297028183937073, 0.4297028183937073, 0.4297028183937073, 0.4297028183937073, 0.4297028183937073, 0.4297028183937073, 0.4297028183937073, 0.4297028183937073, 0.4297028183937073, 0.4297028183937073, 0.4297028183937073, 0.4297028183937073, 0.4297028183937073, 0.4297028183937073], "moving_avg_accuracy_train": [0.028679999999999994, 0.06084729411764705, 0.09364021176470586, 0.1255538376470588, 0.15618904211764703, 0.185233667317647, 0.21285382999764701, 0.23898021170376466, 0.26378101406279997, 0.28736526559769643, 0.309729915508515, 0.330834571016487, 0.3506969962677795, 0.36959670840570746, 0.3874464493298426, 0.40418180439685836, 0.4199753886630549, 0.4347778497967494, 0.44856594717001563, 0.4614599406883082, 0.473742181913595, 0.4852079637222355, 0.4960754026441296, 0.5063737447326578, 0.5160587232005686, 0.5251987332334529, 0.5339353304983429, 0.5423794445073321, 0.5503885588801283, 0.5580461735803507, 0.5654674385752567, 0.5725559888353781, 0.5795450958341932, 0.5863364686037151, 0.5928816452727553, 0.5991534807454798, 0.6053487209062259, 0.6113314958744269, 0.6171442286399255, 0.6226698057759329, 0.6281063546101043, 0.6335027779726233, 0.6387360295871257, 0.6438553678048837, 0.6487663016126307, 0.653496730274897, 0.6581235278356425, 0.6625817632873724, 0.6668553516645175, 0.671115698851007, 0.675222952495318, 0.6791618337163745, 0.6830244738741489, 0.6867384970749693, 0.6901117061910017, 0.6933970061601369, 0.6965890702500055, 0.6997395749897108, 0.7027303233730927, 0.7055914086828422, 0.7084463854616168, 0.7112840998566315, 0.7140027486944978, 0.7167671797074009, 0.7193351676190138, 0.7218722390924065, 0.72431560341846, 0.7267381607236729, 0.7291678740630704, 0.731434616068528, 0.7336017426969693, 0.7357356860743313, 0.7377362351139569, 0.7397179057202082, 0.7418025857364227, 0.743845856574545, 0.7459530356229729, 0.7479883202959697, 0.7499471353251963, 0.7519359512044413, 0.7537588266722325, 0.7554982381226563, 0.7571860613692142, 0.7586721611146457, 0.76013906265024, 0.7614592740322748, 0.7627956995702238, 0.7696267178484955, 0.7759205166518812, 0.781657876751399, 0.7868191478997885, 0.7915395860509861, 0.7958562156811816, 0.7997717705836517, 0.8033310641135218, 0.8065956047609932, 0.809561926637835, 0.8122692633858162, 0.814729395870764, 0.8169741033425111, 0.8190037518317894, 0.8208586707662575, 0.8225633919249259, 0.8241494056736098, 0.8255979945180135, 0.8269699597720945, 0.8282353167360615, 0.8294117850624554, 0.8310682536150334, 0.8325873106064712, 0.8339544618987652, 0.8351872510030064, 0.8362967611968234, 0.8372953203712588, 0.8381940236282506, 0.8390028565595432, 0.8397308061977066, 0.8403859608720536, 0.840975600078966, 0.8415062753651871, 0.841983883122786, 0.8424137301046252, 0.8428005923882803, 0.84314876844357, 0.8434621268933307, 0.8437441494981153, 0.8439979698424215, 0.844226408152297, 0.844432002631185, 0.8446170376621842, 0.8447835691900835, 0.8449334475651928, 0.8450683381027912, 0.8451897395866298, 0.8452990009220845, 0.8453973361239938, 0.8454858378057121, 0.8455654893192586, 0.8456371756814504, 0.845701693407423, 0.8457597593607984, 0.8458120187188363, 0.8458590521410704, 0.845901382221081, 0.8459394792930907, 0.8459737666578993, 0.846004625286227, 0.846032398051722, 0.8460573935406676, 0.8460798894807184, 0.8461001358267642, 0.8461183575382055, 0.8461347570785026, 0.84614951666477, 0.8461628002924106, 0.8461747555572873, 0.8461855152956762, 0.8461951990602262, 0.8462039144483213], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.028893333333333326, 0.06163066666666665, 0.09433426666666665, 0.1258341733333333, 0.1561974226666666, 0.18545768039999994, 0.21333857902666659, 0.23953805445733325, 0.2645442490115999, 0.2880364907771066, 0.3102195083660626, 0.33099755752945637, 0.3504178017765108, 0.36852268826552637, 0.3853770861056404, 0.40130604416174304, 0.4161487730789021, 0.4300672291043452, 0.4429405061939107, 0.45483312224118627, 0.4659764766837343, 0.4764988290153609, 0.4863822794471581, 0.49571738483577565, 0.5043323130188647, 0.5124590817169782, 0.5201731735452804, 0.527502522857419, 0.5343256039050105, 0.5408130435145094, 0.5469584058297252, 0.552995898580086, 0.5589229753887441, 0.5646440111832031, 0.5702196100648828, 0.5755709823917278, 0.5807605508192216, 0.5856711624039661, 0.5903440461635695, 0.5947363082138792, 0.5989560107258247, 0.602887076319909, 0.6065850353545847, 0.610153198485793, 0.6135778786372136, 0.6168334241068255, 0.6198834150294763, 0.622908406859862, 0.6255775661738758, 0.6282064762231548, 0.6306524952675061, 0.6330939124074222, 0.63538452116668, 0.6375127357166787, 0.6395214621450107, 0.641542649263843, 0.643361717670792, 0.6451588792370462, 0.6468429913133417, 0.6483853588486742, 0.6499068229638068, 0.6511561406674261, 0.6523871932673502, 0.6534284739406152, 0.654458959879887, 0.6553463972252317, 0.6562517575027085, 0.6571465817524377, 0.6578185902438606, 0.6583300645528078, 0.6587903914308604, 0.6591913522877744, 0.659472217058997, 0.6596583286864306, 0.6599191624844543, 0.6600872462360089, 0.6603851882790747, 0.6606533361178338, 0.6608146691727171, 0.6608798689221121, 0.6607385486965676, 0.6606113604935775, 0.6603502244442198, 0.6598352019997978, 0.659291681799818, 0.6585625136198363, 0.6577462622578527, 0.6608116360320675, 0.6635838057621941, 0.6659587585193081, 0.6680962160007106, 0.6701132610673062, 0.6719019349605756, 0.6735250747978513, 0.6749325673180662, 0.6761859772529262, 0.6773673795276336, 0.6784306415748702, 0.6794009107507165, 0.6802741530089782, 0.6810467377080803, 0.6817420639372723, 0.6823145242102118, 0.6827764051225239, 0.6831787646102715, 0.6835408881492443, 0.6839201326676532, 0.6842347860675546, 0.6849446407941324, 0.6855568433813859, 0.6861211590432473, 0.6866290431389226, 0.6870861388250303, 0.6874975249425272, 0.6878677724482745, 0.688200995203447, 0.6885008956831022, 0.688770806114792, 0.6890137255033127, 0.6892323529529815, 0.6894291176576833, 0.689606205891915, 0.6897655853027235, 0.6899090267724511, 0.690038124095206, 0.6901543116856854, 0.6902588805171168, 0.6903529924654052, 0.6904376932188646, 0.6905139238969781, 0.6905825315072802, 0.6906442783565522, 0.6906998505208969, 0.6907498654688071, 0.6907948789219264, 0.6908353910297337, 0.6908718519267603, 0.6909046667340842, 0.6909342000606757, 0.6909607800546081, 0.6909847020491473, 0.6910062318442326, 0.6910256086598093, 0.6910430477938284, 0.6910587430144456, 0.691072868713001, 0.6910855818417009, 0.6910970236575308, 0.6911073212917777, 0.6911165891625999, 0.6911249302463399, 0.6911324372217059, 0.6911391934995353, 0.6911452741495818, 0.6911507467346236, 0.6911556720611612, 0.6911601048550451, 0.6911640943695405, 0.6911676849325864], "moving_var_accuracy_train": [0.0074028815999999985, 0.015975206737660896, 0.024056065094155844, 0.03081677423736935, 0.036181738590225165, 0.040155877008278926, 0.043006149785678016, 0.04484882519659405, 0.04589966085580202, 0.046315647054373216, 0.0461856804396372, 0.04557577075266505, 0.044568837109167175, 0.04332674546831932, 0.041861590181016095, 0.04019608014588629, 0.038421407867058714, 0.036551282780883675, 0.03460715916536735, 0.03264273886847855, 0.030736146027275913, 0.028845708796898482, 0.02702404897569852, 0.025276146726079834, 0.023592721324786892, 0.021985307242919236, 0.020473729704547264, 0.019068084286663813, 0.01773858907532621, 0.016492481733867158, 0.015338910127602011, 0.014257247017954212, 0.013271150865926772, 0.01235914047618552, 0.011508780467227508, 0.010711925702277083, 0.009986162137893271, 0.009309688290985128, 0.008682810221714512, 0.008089317223716776, 0.0075463900703820735, 0.007053843529311731, 0.006594941478526784, 0.006171315944764287, 0.005771239788064503, 0.005395508407217171, 0.005048622867508547, 0.004722643350845241, 0.004414751034316145, 0.004136630954229386, 0.003874793651294807, 0.0036269473536276456, 0.0033985325191609374, 0.003182824982270934, 0.002966949341708203, 0.0027673931705221757, 0.00258235731185442, 0.0024134527017031296, 0.0022526086145671274, 0.0021010200354573973, 0.0019642760635777373, 0.001840322064109031, 0.0017228093212308118, 0.0016193070985336382, 0.001516727445907982, 0.0014229852862671128, 0.0013344170207088776, 0.00125379437371135, 0.0011815464985450263, 0.0011096349225642801, 0.001040939370721147, 0.0009778288626891122, 0.0009160657445597269, 0.0008598023356288802, 0.0008129351189960279, 0.0007692162085577192, 0.0007322564195811468, 0.0006963122309242543, 0.0006612136147003434, 0.0006306907506441454, 0.0005975275503194046, 0.0005650047650322534, 0.0005341430143336195, 0.0005006051449806023, 0.0004699108315187008, 0.0004386063712061194, 0.00041082003305184777, 0.0007897033262094065, 0.0010672401239859653, 0.0012567718197912152, 0.0013708431166168757, 0.0014343016320087268, 0.0014585710910863907, 0.0014506981137260685, 0.0014196454362394395, 0.0013735959233664293, 0.0013154279203230616, 0.0012498521786934804, 0.0011793372274155913, 0.001106751909377489, 0.001033151975350007, 0.0009608032960960415, 0.0008908776345457419, 0.0008244288275902962, 0.0007608716315924448, 0.0007017250663588501, 0.0006459627139393034, 0.0005938231420524445, 0.0005591358204383184, 0.0005239900456836121, 0.0004884129650194426, 0.0004532495892973191, 0.0004190037461992419, 0.000386077455402958, 0.00035473871775981074, 0.00032515274238051995, 0.00029740666422378703, 0.0002715290466272767, 0.0002475052115135023, 0.0002252892366968042, 0.000204813295558192, 0.000185994881852538, 0.00016874235550591796, 0.0001529591590446195, 0.0001385469848024851, 0.0001254081170687219, 0.00011344712826650279, 0.00010257207199262149, 9.269528660110219e-05, 8.373389960526366e-05, 7.561010439279736e-05, 6.825126569944626e-05, 6.158989824370382e-05, 5.556355330183719e-05, 5.01146403264815e-05, 4.519020460124404e-05, 4.0741677070122294e-05, 3.672460863560221e-05, 3.309839818276063e-05, 2.982602119716666e-05, 2.687376397192249e-05, 2.421096693925295e-05, 2.180977953059105e-05, 1.9644928098595284e-05, 1.769349777079706e-05, 1.5934728604187004e-05, 1.434982603824872e-05, 1.2921785372952984e-05, 1.163522980586628e-05, 1.0476261431148613e-05, 9.43232451878761e-06, 8.492080343819474e-06, 7.645292813735131e-06, 6.8827241408426784e-06, 6.196039819628069e-06, 5.577722192889685e-06, 5.020991921332498e-06, 4.5197367068619925e-06, 4.068446658082616e-06], "duration": 34957.667551, "accuracy_train": [0.2868, 0.3503529411764706, 0.3887764705882353, 0.4127764705882353, 0.4319058823529412, 0.4466352941176471, 0.46143529411764705, 0.47411764705882353, 0.48698823529411767, 0.4996235294117647, 0.5110117647058824, 0.5207764705882353, 0.5294588235294118, 0.5396941176470588, 0.5480941176470588, 0.5548, 0.5621176470588235, 0.568, 0.5726588235294118, 0.5775058823529412, 0.5842823529411765, 0.5884, 0.5938823529411764, 0.5990588235294118, 0.6032235294117647, 0.6074588235294117, 0.6125647058823529, 0.6183764705882353, 0.6224705882352941, 0.626964705882353, 0.6322588235294118, 0.6363529411764706, 0.6424470588235294, 0.6474588235294118, 0.6517882352941177, 0.6556, 0.6611058823529412, 0.6651764705882353, 0.6694588235294118, 0.6724, 0.6770352941176471, 0.6820705882352941, 0.6858352941176471, 0.6899294117647059, 0.6929647058823529, 0.6960705882352941, 0.699764705882353, 0.7027058823529412, 0.7053176470588235, 0.7094588235294118, 0.7121882352941177, 0.7146117647058824, 0.7177882352941176, 0.7201647058823529, 0.7204705882352941, 0.722964705882353, 0.7253176470588235, 0.7280941176470588, 0.7296470588235294, 0.7313411764705883, 0.7341411764705882, 0.7368235294117647, 0.7384705882352941, 0.7416470588235294, 0.7424470588235295, 0.7447058823529412, 0.7463058823529412, 0.7485411764705883, 0.751035294117647, 0.751835294117647, 0.7531058823529412, 0.7549411764705882, 0.7557411764705882, 0.7575529411764705, 0.7605647058823529, 0.762235294117647, 0.7649176470588235, 0.7663058823529412, 0.7675764705882353, 0.7698352941176471, 0.770164705882353, 0.7711529411764706, 0.7723764705882353, 0.7720470588235294, 0.7733411764705882, 0.7733411764705882, 0.7748235294117647, 0.8311058823529411, 0.832564705882353, 0.8332941176470589, 0.8332705882352941, 0.8340235294117647, 0.8347058823529412, 0.8350117647058823, 0.8353647058823529, 0.8359764705882353, 0.8362588235294117, 0.836635294117647, 0.8368705882352941, 0.8371764705882353, 0.8372705882352941, 0.8375529411764706, 0.8379058823529412, 0.8384235294117647, 0.838635294117647, 0.8393176470588235, 0.8396235294117647, 0.84, 0.8459764705882353, 0.8462588235294117, 0.8462588235294117, 0.8462823529411765, 0.8462823529411765, 0.8462823529411765, 0.8462823529411765, 0.8462823529411765, 0.8462823529411765, 0.8462823529411765, 0.8462823529411765, 0.8462823529411765, 0.8462823529411765, 0.8462823529411765, 0.8462823529411765, 0.8462823529411765, 0.8462823529411765, 0.8462823529411765, 0.8462823529411765, 0.8462823529411765, 0.8462823529411765, 0.8462823529411765, 0.8462823529411765, 0.8462823529411765, 0.8462823529411765, 0.8462823529411765, 0.8462823529411765, 0.8462823529411765, 0.8462823529411765, 0.8462823529411765, 0.8462823529411765, 0.8462823529411765, 0.8462823529411765, 0.8462823529411765, 0.8462823529411765, 0.8462823529411765, 0.8462823529411765, 0.8462823529411765, 0.8462823529411765, 0.8462823529411765, 0.8462823529411765, 0.8462823529411765, 0.8462823529411765, 0.8462823529411765, 0.8462823529411765, 0.8462823529411765, 0.8462823529411765, 0.8462823529411765, 0.8462823529411765, 0.8462823529411765, 0.8462823529411765], "end": "2016-02-05 19:32:06.497000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0], "moving_var_accuracy_valid": [0.007513422399999998, 0.016407677104, 0.024392638470239997, 0.030883571703294397, 0.03609255672366846, 0.04018876519489145, 0.043165989249476476, 0.04502710294010779, 0.04615218054086953, 0.046503931295303724, 0.04628231458994074, 0.04553962907427433, 0.04437997914638413, 0.04289206346476697, 0.04115949365726584, 0.03932712963431695, 0.03737717608625967, 0.03538296924082345, 0.03333616368396369, 0.03127545616359857, 0.02926547968132901, 0.027335410800514093, 0.025481013052402822, 0.023717209480711904, 0.02201344142103876, 0.020406496604188626, 0.01890141185838907, 0.017494744924604285, 0.01616426034698187, 0.014926616166465746, 0.013773843851686632, 0.012724521334913903, 0.011768241356884103, 0.010885989476249015, 0.010077176254628603, 0.00932719330119045, 0.008636858555244178, 0.007990199654945802, 0.007387702273128108, 0.006822559739082615, 0.00630055676877822, 0.00580958058224533, 0.005351696633220058, 0.004931113063076273, 0.004543557664024461, 0.004184589084364415, 0.0038498521775822423, 0.00354722213998912, 0.0032566196289824873, 0.002993158178509044, 0.0027476894431460973, 0.002526565157691171, 0.00232113063831395, 0.0021297812490199894, 0.0019531179608929103, 0.0017945729411276202, 0.001644896735837299, 0.001509475169510558, 0.0013840537539292178, 0.0012670584570627249, 0.0011611862888391777, 0.0010591148124764497, 0.0009668427457628228, 0.000879916860151177, 0.0008014822855753914, 0.0007284219623950632, 0.0006629568612438532, 0.0006038675690605979, 0.0005475451708674384, 0.0004951451074991125, 0.00044753770426111977, 0.0004042308603140023, 0.00036451773946002725, 0.0003283777033548185, 0.0002961522414510594, 0.0002667912866337834, 0.00024091108311964096, 0.00021746710417855795, 0.0001959546489520839, 0.000176397443122766, 0.00015893744146582107, 0.0001431892888700576, 0.0001294840883095194, 0.00011892291254289296, 0.00010968934915867823, 0.0001035055903550922, 9.915142789304423e-05, 0.00017380493248453076, 0.00022558876434974843, 0.00025379349330148076, 0.0002695326643345655, 0.00027919563510720824, 0.0002800702602646582, 0.00027577448062035727, 0.0002660263493084688, 0.00025356304256087485, 0.0002407681403169412, 0.00022686606191509167, 0.00021265225618595995, 0.0001982499989418897, 0.00018379698310328193, 0.00016976859187797452, 0.0001557411295670221, 0.00014208702240474467, 0.00012933535858069485, 0.00011758202383992935, 0.00010711825909862525, 9.729749404738803e-05, 9.210278823825381e-05, 8.626563748498654e-05, 8.050514323248664e-05, 7.477614520099697e-05, 6.917895887722207e-05, 6.378420982852296e-05, 5.863953778527942e-05, 5.37749206478346e-05, 4.920689126232848e-05, 4.49418669063103e-05, 4.0978768679553175e-05, 3.731107346733573e-05, 3.392841326174984e-05, 3.081781411990447e-05, 2.7964648877221023e-05, 2.5353363086637592e-05, 2.2968021846656155e-05, 2.0792715667623223e-05, 1.8811855865423383e-05, 1.701038380817666e-05, 1.537391338608844e-05, 1.3888822094050456e-05, 1.2542302922367816e-05, 1.1322386690686188e-05, 1.0217942410667249e-05, 9.218661624730772e-06, 8.315031360913201e-06, 7.4982993027328445e-06, 6.760433945567445e-06, 6.094081855228089e-06, 5.4925236261213666e-06, 4.949629728206262e-06, 4.4598171117902334e-06, 4.018007189298935e-06, 3.619585619206098e-06, 3.260364167843504e-06, 2.936544810611146e-06, 2.6446861477871457e-06, 2.3816721457804943e-06, 2.1446831675478165e-06, 1.9311692222327864e-06, 1.7388253408757071e-06, 1.5655689698897578e-06, 1.4095192650130953e-06, 1.2689781641227595e-06, 1.1424131164553721e-06, 1.0284413474931948e-06, 9.25815542317397e-07, 8.334108350402101e-07, 7.502129975693772e-07, 6.753077270993223e-07], "accuracy_test": 0.6857, "start": "2016-02-05 09:49:28.829000", "learning_rate_per_epoch": [0.00015604516374878585, 0.00015604516374878585, 0.00015604516374878585, 0.00015604516374878585, 0.00015604516374878585, 0.00015604516374878585, 0.00015604516374878585, 0.00015604516374878585, 0.00015604516374878585, 0.00015604516374878585, 0.00015604516374878585, 0.00015604516374878585, 0.00015604516374878585, 0.00015604516374878585, 0.00015604516374878585, 0.00015604516374878585, 0.00015604516374878585, 0.00015604516374878585, 0.00015604516374878585, 0.00015604516374878585, 0.00015604516374878585, 0.00015604516374878585, 0.00015604516374878585, 0.00015604516374878585, 0.00015604516374878585, 0.00015604516374878585, 0.00015604516374878585, 0.00015604516374878585, 0.00015604516374878585, 0.00015604516374878585, 0.00015604516374878585, 0.00015604516374878585, 0.00015604516374878585, 0.00015604516374878585, 0.00015604516374878585, 0.00015604516374878585, 0.00015604516374878585, 0.00015604516374878585, 0.00015604516374878585, 0.00015604516374878585, 0.00015604516374878585, 0.00015604516374878585, 0.00015604516374878585, 0.00015604516374878585, 0.00015604516374878585, 0.00015604516374878585, 0.00015604516374878585, 0.00015604516374878585, 0.00015604516374878585, 0.00015604516374878585, 0.00015604516374878585, 0.00015604516374878585, 0.00015604516374878585, 0.00015604516374878585, 0.00015604516374878585, 0.00015604516374878585, 0.00015604516374878585, 0.00015604516374878585, 0.00015604516374878585, 0.00015604516374878585, 0.00015604516374878585, 0.00015604516374878585, 0.00015604516374878585, 0.00015604516374878585, 0.00015604516374878585, 0.00015604516374878585, 0.00015604516374878585, 0.00015604516374878585, 0.00015604516374878585, 0.00015604516374878585, 0.00015604516374878585, 0.00015604516374878585, 0.00015604516374878585, 0.00015604516374878585, 0.00015604516374878585, 0.00015604516374878585, 0.00015604516374878585, 0.00015604516374878585, 0.00015604516374878585, 0.00015604516374878585, 0.00015604516374878585, 0.00015604516374878585, 0.00015604516374878585, 0.00015604516374878585, 0.00015604516374878585, 0.00015604516374878585, 1.5604517102474347e-05, 1.5604517102474347e-05, 1.5604517102474347e-05, 1.5604517102474347e-05, 1.5604517102474347e-05, 1.5604517102474347e-05, 1.5604517102474347e-05, 1.5604517102474347e-05, 1.5604517102474347e-05, 1.5604517102474347e-05, 1.5604517102474347e-05, 1.5604517102474347e-05, 1.5604517102474347e-05, 1.5604517102474347e-05, 1.5604517102474347e-05, 1.5604517102474347e-05, 1.5604517102474347e-05, 1.5604517102474347e-05, 1.5604517102474347e-05, 1.5604517102474347e-05, 1.5604517102474347e-05, 1.5604516647726996e-06, 1.5604516079292807e-07, 1.5604516789835543e-08, 1.5604516567790938e-09, 1.5604516290235182e-10, 1.5604515943290487e-11, 1.5604516376971356e-12, 1.56045166480219e-13, 1.5604516986835078e-14, 1.5604517410351551e-15, 1.5604517410351551e-16, 1.560451741035155e-17, 1.5604517203556398e-18, 1.5604516686568515e-19, 1.5604516686568515e-20, 1.5604516888516907e-21, 1.5604516888516907e-22, 1.560451751960563e-23, 1.5604517716820858e-24, 1.560451796333989e-25, 1.56045176551911e-26, 1.5604518040377088e-27, 1.5604517558894602e-28, 1.5604517859821156e-29, 1.5604518235979348e-30, 1.5604518235979348e-31, 1.5604518529852936e-32, 1.5604518529852936e-33, 1.5604518759441676e-34, 1.5604518185469825e-35, 1.560451890293464e-36, 1.560451890293464e-37, 1.5604519183194332e-38, 1.5604523387089725e-39, 1.5604579439028298e-40, 1.5604859698721163e-41, 1.5610464892578462e-42, 1.555441295400547e-43, 1.5414283107572988e-44, 1.401298464324817e-45, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "accuracy_train_first": 0.2868, "accuracy_train_last": 0.8462823529411765, "batch_size_eval": 1024, "accuracy_train_std": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "accuracy_test_std": 0, "error_valid": [0.7110666666666667, 0.6437333333333333, 0.6113333333333333, 0.5906666666666667, 0.5705333333333333, 0.5512, 0.5357333333333334, 0.5246666666666666, 0.5104, 0.5005333333333333, 0.4901333333333333, 0.482, 0.4748, 0.46853333333333336, 0.4629333333333333, 0.45533333333333337, 0.4502666666666667, 0.44466666666666665, 0.44120000000000004, 0.4381333333333334, 0.4337333333333333, 0.42879999999999996, 0.42466666666666664, 0.4202666666666667, 0.41813333333333336, 0.4144, 0.4104, 0.4065333333333333, 0.40426666666666666, 0.40080000000000005, 0.3977333333333334, 0.3926666666666667, 0.3877333333333334, 0.3838666666666667, 0.37960000000000005, 0.37626666666666664, 0.3725333333333334, 0.3701333333333333, 0.36760000000000004, 0.36573333333333335, 0.36306666666666665, 0.36173333333333335, 0.3601333333333333, 0.35773333333333335, 0.3556, 0.35386666666666666, 0.3526666666666667, 0.34986666666666666, 0.35040000000000004, 0.3481333333333333, 0.3473333333333334, 0.3449333333333333, 0.344, 0.3433333333333334, 0.34240000000000004, 0.3402666666666667, 0.3402666666666667, 0.33866666666666667, 0.33799999999999997, 0.33773333333333333, 0.33640000000000003, 0.3376, 0.33653333333333335, 0.33720000000000006, 0.3362666666666667, 0.33666666666666667, 0.3356, 0.3348, 0.3361333333333333, 0.3370666666666666, 0.3370666666666666, 0.33720000000000006, 0.33799999999999997, 0.33866666666666667, 0.33773333333333333, 0.33840000000000003, 0.3369333333333333, 0.3369333333333333, 0.33773333333333333, 0.33853333333333335, 0.34053333333333335, 0.34053333333333335, 0.34199999999999997, 0.3448, 0.3456, 0.348, 0.3496, 0.3116, 0.31146666666666667, 0.31266666666666665, 0.31266666666666665, 0.3117333333333333, 0.31200000000000006, 0.3118666666666666, 0.3124, 0.31253333333333333, 0.31200000000000006, 0.31200000000000006, 0.3118666666666666, 0.3118666666666666, 0.31200000000000006, 0.31200000000000006, 0.31253333333333333, 0.3130666666666667, 0.31320000000000003, 0.31320000000000003, 0.31266666666666665, 0.3129333333333333, 0.30866666666666664, 0.3089333333333333, 0.30879999999999996, 0.30879999999999996, 0.30879999999999996, 0.30879999999999996, 0.30879999999999996, 0.30879999999999996, 0.30879999999999996, 0.30879999999999996, 0.30879999999999996, 0.30879999999999996, 0.30879999999999996, 0.30879999999999996, 0.30879999999999996, 0.30879999999999996, 0.30879999999999996, 0.30879999999999996, 0.30879999999999996, 0.30879999999999996, 0.30879999999999996, 0.30879999999999996, 0.30879999999999996, 0.30879999999999996, 0.30879999999999996, 0.30879999999999996, 0.30879999999999996, 0.30879999999999996, 0.30879999999999996, 0.30879999999999996, 0.30879999999999996, 0.30879999999999996, 0.30879999999999996, 0.30879999999999996, 0.30879999999999996, 0.30879999999999996, 0.30879999999999996, 0.30879999999999996, 0.30879999999999996, 0.30879999999999996, 0.30879999999999996, 0.30879999999999996, 0.30879999999999996, 0.30879999999999996, 0.30879999999999996, 0.30879999999999996, 0.30879999999999996, 0.30879999999999996, 0.30879999999999996, 0.30879999999999996, 0.30879999999999996], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.09242420701772915, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 64, "valid_ratio": 0.15, "learning_rate": 0.00015604515760513082, "optimization": "nesterov_momentum", "nb_data_augmentation": 0, "learning_rate_decay_method": "discrete", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 2.9667423637609405e-06, "rotation_range": [0, 0], "momentum": 0.8950051332903401}, "accuracy_valid_max": 0.6913333333333334, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        y_pred = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            y_pred.extend((nnet.predict(X[mini_batch]) == y[mini_batch]).tolist())\n        return np.mean(y_pred)\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            acc = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = acc\n            status[\"accuracy_train_std\"] = 0\n            acc = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = acc\n            status[\"accuracy_valid_std\"] = 0\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n\n    # rescaling to [-1, 1]\n    X_min = X_train.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X_train.max(axis=(0, 2, 3))[None, :, None, None]\n    def preprocess(a):\n        return (a / 255.) * 2 - 1\n        # return 2 * ((a - X_min) / (X_max - X_min)) - 1\n    X_train = preprocess(X_train)\n    X_valid = preprocess(X_valid)\n\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = preprocess(X_test)\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    acc = evaluate(X_test, y_test, batch_size_eval)\n    light.set(\"accuracy_test\", acc)\n    light.set(\"accuracy_test_std\", 0)\n    print(\"Test accuracy : {}+-{}\".format(acc, 0))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.6912, "accuracy_valid_std": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "accuracy_valid": [0.2889333333333333, 0.3562666666666667, 0.38866666666666666, 0.4093333333333333, 0.42946666666666666, 0.4488, 0.46426666666666666, 0.47533333333333333, 0.4896, 0.49946666666666667, 0.5098666666666667, 0.518, 0.5252, 0.5314666666666666, 0.5370666666666667, 0.5446666666666666, 0.5497333333333333, 0.5553333333333333, 0.5588, 0.5618666666666666, 0.5662666666666667, 0.5712, 0.5753333333333334, 0.5797333333333333, 0.5818666666666666, 0.5856, 0.5896, 0.5934666666666667, 0.5957333333333333, 0.5992, 0.6022666666666666, 0.6073333333333333, 0.6122666666666666, 0.6161333333333333, 0.6204, 0.6237333333333334, 0.6274666666666666, 0.6298666666666667, 0.6324, 0.6342666666666666, 0.6369333333333334, 0.6382666666666666, 0.6398666666666667, 0.6422666666666667, 0.6444, 0.6461333333333333, 0.6473333333333333, 0.6501333333333333, 0.6496, 0.6518666666666667, 0.6526666666666666, 0.6550666666666667, 0.656, 0.6566666666666666, 0.6576, 0.6597333333333333, 0.6597333333333333, 0.6613333333333333, 0.662, 0.6622666666666667, 0.6636, 0.6624, 0.6634666666666666, 0.6628, 0.6637333333333333, 0.6633333333333333, 0.6644, 0.6652, 0.6638666666666667, 0.6629333333333334, 0.6629333333333334, 0.6628, 0.662, 0.6613333333333333, 0.6622666666666667, 0.6616, 0.6630666666666667, 0.6630666666666667, 0.6622666666666667, 0.6614666666666666, 0.6594666666666666, 0.6594666666666666, 0.658, 0.6552, 0.6544, 0.652, 0.6504, 0.6884, 0.6885333333333333, 0.6873333333333334, 0.6873333333333334, 0.6882666666666667, 0.688, 0.6881333333333334, 0.6876, 0.6874666666666667, 0.688, 0.688, 0.6881333333333334, 0.6881333333333334, 0.688, 0.688, 0.6874666666666667, 0.6869333333333333, 0.6868, 0.6868, 0.6873333333333334, 0.6870666666666667, 0.6913333333333334, 0.6910666666666667, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912], "seed": 115716557, "model": "residualv3", "loss_std": [0.30285346508026123, 0.0972352996468544, 0.09413886070251465, 0.09502332657575607, 0.09804841130971909, 0.10070200264453888, 0.10270240157842636, 0.10446592420339584, 0.10634024441242218, 0.10862237960100174, 0.11102500557899475, 0.11337210237979889, 0.11540547013282776, 0.116967111825943, 0.1181127279996872, 0.11911111325025558, 0.1199273094534874, 0.12059704959392548, 0.12127897888422012, 0.12175983935594559, 0.12216061353683472, 0.12264058738946915, 0.12306229025125504, 0.12339862436056137, 0.12372084707021713, 0.12382853031158447, 0.12402491271495819, 0.12416673451662064, 0.12432195991277695, 0.1244070753455162, 0.12454906105995178, 0.12457918375730515, 0.12471932172775269, 0.12477759271860123, 0.1247955933213234, 0.12482435256242752, 0.12477516382932663, 0.12468830496072769, 0.1244804635643959, 0.12425221502780914, 0.12400060147047043, 0.12368353456258774, 0.12326876819133759, 0.1229308471083641, 0.12263908982276917, 0.12236032634973526, 0.12201359868049622, 0.12160206586122513, 0.12132017314434052, 0.12090741097927094, 0.1206369549036026, 0.12029153108596802, 0.11994415521621704, 0.11953815817832947, 0.11925205588340759, 0.11882937699556351, 0.11858385056257248, 0.11810988932847977, 0.1178063377737999, 0.11738002300262451, 0.11690232902765274, 0.1165054589509964, 0.11607354879379272, 0.1155836209654808, 0.11517657339572906, 0.11469897627830505, 0.11427727341651917, 0.11371813714504242, 0.11321897804737091, 0.11274194717407227, 0.11219612509012222, 0.11155996471643448, 0.11091917753219604, 0.11028053611516953, 0.10951314866542816, 0.10858055204153061, 0.10785429924726486, 0.10686511546373367, 0.10598040372133255, 0.1051330491900444, 0.104149229824543, 0.10316277295351028, 0.1021396592259407, 0.10119415819644928, 0.10012754052877426, 0.09916230291128159, 0.09794338792562485, 0.09438800066709518, 0.09231111407279968, 0.09180018305778503, 0.09145627170801163, 0.09118528664112091, 0.09095527231693268, 0.09072747826576233, 0.09050939977169037, 0.09031834453344345, 0.09012538939714432, 0.08993366360664368, 0.08973578363656998, 0.08954879641532898, 0.08935930579900742, 0.08917300403118134, 0.08898970484733582, 0.08881514519453049, 0.08863162994384766, 0.08845112472772598, 0.0882592722773552, 0.08807306736707687, 0.08784174174070358, 0.08720775693655014, 0.08721679449081421, 0.0872170552611351, 0.0872170478105545, 0.0872170627117157, 0.0872170627117157, 0.0872170627117157, 0.0872170627117157, 0.0872170627117157, 0.0872170627117157, 0.0872170627117157, 0.0872170627117157, 0.0872170552611351, 0.0872170552611351, 0.0872170627117157, 0.0872170552611351, 0.0872170701622963, 0.0872170552611351, 0.0872170552611351, 0.0872170552611351, 0.0872170627117157, 0.0872170627117157, 0.0872170627117157, 0.0872170552611351, 0.0872170552611351, 0.0872170627117157, 0.0872170552611351, 0.0872170552611351, 0.0872170627117157, 0.0872170478105545, 0.0872170701622963, 0.0872170552611351, 0.0872170627117157, 0.0872170552611351, 0.0872170552611351, 0.0872170552611351, 0.0872170478105545, 0.0872170627117157, 0.0872170627117157, 0.0872170627117157, 0.0872170552611351, 0.0872170627117157, 0.0872170627117157, 0.0872170627117157, 0.0872170627117157, 0.0872170552611351, 0.0872170627117157, 0.0872170627117157, 0.0872170701622963, 0.0872170627117157]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:41 2016", "state": "available"}], "summary": "2676ff09378a7c0d1267a230bcec34e6"}