{"content": {"hp_model": {"f0": 32, "f1": 16, "f2": 16, "f3": 16, "nonlin": "rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [2.023503541946411, 1.658425211906433, 1.560943841934204, 1.4814273118972778, 1.4223122596740723, 1.3737316131591797, 1.3382925987243652, 1.3016077280044556, 1.2720081806182861, 1.2437160015106201, 1.2147552967071533, 1.1889680624008179, 1.1622236967086792, 1.141352891921997, 1.1201609373092651, 1.096864938735962, 1.076345682144165, 1.0597310066223145, 1.0413450002670288, 1.0245118141174316, 1.0081491470336914, 0.9914270639419556, 0.9796887636184692, 0.965555727481842, 0.9491108655929565, 0.9382680654525757, 0.9245854616165161, 0.9113776683807373, 0.9009390473365784, 0.8926622271537781, 0.8799577951431274, 0.8688473701477051, 0.8559116721153259, 0.8492326140403748, 0.8395048379898071, 0.8294460773468018, 0.8212981224060059, 0.8144745826721191, 0.8062430620193481, 0.7962552309036255, 0.7912087440490723, 0.7815055251121521, 0.7749988436698914, 0.772699773311615, 0.760330080986023, 0.7501835823059082, 0.7457491755485535, 0.7419657111167908, 0.7344503998756409, 0.727537214756012, 0.7244542837142944, 0.7163639068603516, 0.7089126110076904, 0.7056097388267517, 0.6974532604217529, 0.6905646920204163, 0.6863065361976624, 0.6796208024024963, 0.6718586087226868, 0.6685019731521606, 0.6639668941497803, 0.6578908562660217, 0.6551895141601562, 0.6471974849700928, 0.6418616771697998, 0.6376569867134094, 0.6310878396034241, 0.626284122467041, 0.6226353049278259, 0.61976158618927, 0.6158843040466309, 0.61003178358078, 0.6022763252258301, 0.6013119220733643, 0.5953628420829773, 0.5923185348510742, 0.5878661870956421, 0.5848261117935181, 0.5802438259124756, 0.5755701661109924, 0.5716171860694885, 0.5672494769096375, 0.5638294219970703, 0.562846302986145, 0.558249294757843, 0.5548684000968933, 0.5521523952484131, 0.5472306609153748, 0.5433122515678406, 0.5404891967773438, 0.5370355844497681, 0.5343749523162842, 0.5322713851928711, 0.5296434164047241, 0.5280283093452454, 0.5218455195426941, 0.5210109949111938, 0.506588876247406, 0.48484885692596436, 0.4836382269859314, 0.483396053314209, 0.48091229796409607, 0.47933831810951233, 0.48042529821395874, 0.47788822650909424, 0.4787386357784271, 0.4779643714427948, 0.47840115427970886, 0.4749046564102173, 0.476154088973999, 0.47395437955856323, 0.4728947877883911, 0.47661492228507996, 0.4714621603488922, 0.46583303809165955, 0.4678810238838196, 0.4658166468143463, 0.4672080874443054, 0.46901965141296387, 0.4675942659378052, 0.46630775928497314, 0.4674914479255676, 0.46624577045440674, 0.4689108729362488, 0.46850457787513733, 0.46420472860336304, 0.4682022035121918, 0.46685460209846497, 0.46712926030158997, 0.4682677686214447, 0.4667891263961792, 0.46819984912872314, 0.4672696590423584, 0.4672214686870575, 0.46840205788612366, 0.4681780934333801, 0.4674219787120819, 0.46954160928726196, 0.47022610902786255, 0.4649849236011505, 0.4689611494541168, 0.46890273690223694, 0.4677254259586334, 0.4654773771762848, 0.46545493602752686, 0.46774256229400635, 0.46893465518951416, 0.46573352813720703, 0.4644325077533722, 0.467440664768219, 0.46540698409080505, 0.4679206311702728, 0.4677402079105377, 0.4662214517593384, 0.46778690814971924, 0.46683159470558167, 0.46805182099342346, 0.46620550751686096, 0.46619272232055664, 0.4650311768054962, 0.4675785005092621, 0.4667901396751404, 0.46654200553894043, 0.46544116735458374, 0.4645847678184509, 0.46755582094192505, 0.46705326437950134, 0.46752989292144775, 0.4668777585029602, 0.46573030948638916, 0.4666568338871002, 0.4666196405887604, 0.4677511751651764, 0.46812763810157776, 0.46905452013015747, 0.46689823269844055, 0.46494603157043457], "moving_avg_accuracy_train": [0.037067576394656694, 0.07532007565926079, 0.11306455532340115, 0.1501286391102286, 0.18598319990043313, 0.22009349802939132, 0.2522853316370742, 0.2825456096410024, 0.3110979308289878, 0.33775516216123297, 0.36228359949115985, 0.38585895011905563, 0.40756944489004965, 0.4282082890338427, 0.44747596286440083, 0.46541654141184774, 0.48272313111640863, 0.498506288485111, 0.5135922894181152, 0.5282435485196315, 0.5415507336443665, 0.554543110042296, 0.566526892401623, 0.5782863175832953, 0.5892208256194009, 0.599712744274469, 0.6096925804390302, 0.6188744318335732, 0.6279097951516924, 0.6363299405903807, 0.6446518307136977, 0.6522068883330809, 0.6595688378118806, 0.6661923753404471, 0.673115990479206, 0.6795772896409569, 0.6857690969448568, 0.6914907132862699, 0.6972001323196306, 0.70218736663058, 0.707136293023539, 0.7120529232438411, 0.7160761083327091, 0.7205916885697131, 0.7247787273770091, 0.7287144369690424, 0.73236585759592, 0.7359684645446612, 0.7398733700627865, 0.7433549806040346, 0.7461490665114237, 0.7496031399959402, 0.7523511557247976, 0.7557147576795419, 0.7587399266317335, 0.7610275956172313, 0.763202683047018, 0.7658881414576633, 0.7686767894462176, 0.7712191247192499, 0.7735122012019668, 0.7759966056780234, 0.7782951684802377, 0.7800315950153811, 0.7825545572088872, 0.7848507637711288, 0.7876402907128144, 0.7902996384353221, 0.7926604993022458, 0.7946946653765432, 0.7971064857528793, 0.799265390201078, 0.801143444233065, 0.8030077906320177, 0.8054088957684744, 0.8069026808294084, 0.8084426522259803, 0.8109348926327659, 0.8127337334786163, 0.8145854214137465, 0.8164678910040994, 0.8183177183615614, 0.8200873749237996, 0.8221216278107109, 0.8242617722982353, 0.8255323545679965, 0.8266294837810473, 0.8283023863857775, 0.8301197849634234, 0.831755407634486, 0.8327738837764914, 0.8339627710079232, 0.8353905540423283, 0.835717921903138, 0.835993879689753, 0.8373604580309937, 0.838153358708376, 0.8418124281762243, 0.8453660434127731, 0.8485480931816379, 0.8514329364105394, 0.8540361986653419, 0.8565511236089313, 0.8587424764450666, 0.860805482947616, 0.8626806818439304, 0.8644335371149175, 0.8659389190992546, 0.8673822627351948, 0.8686672129682276, 0.8698539672101184, 0.8708221006778294, 0.8716864093035219, 0.8729106074916175, 0.8740774179299328, 0.875113632480378, 0.8759555808210261, 0.8767900281895048, 0.877494563893764, 0.8781309711764068, 0.8787316034676809, 0.8792605828345986, 0.8797854202921871, 0.8802275831183116, 0.8806139399665949, 0.8809848765693263, 0.881302443470118, 0.8815952291272591, 0.8818843128555908, 0.8820747697956223, 0.8822995873666322, 0.8824973089317408, 0.8826914983331865, 0.8828593654456965, 0.8830034704005271, 0.8831354539598653, 0.8832333488728027, 0.88335862062658, 0.8834317655775802, 0.8834976681311179, 0.883563991924549, 0.8836190330410181, 0.8836871351874976, 0.8837135498871864, 0.8837745254978587, 0.8838270783986542, 0.8838650754141321, 0.8838574200494907, 0.8838110387403704, 0.8838157985383526, 0.8838712356303461, 0.8838908660297976, 0.8838690419083609, 0.8838981922752491, 0.8839314391006958, 0.8839845406340555, 0.883957999349812, 0.883964303079698, 0.8840187685127766, 0.8840027192846994, 0.8839906001282395, 0.8840145340707496, 0.8840500615606846, 0.8840564957135401, 0.8840808876415862, 0.8840772637399229, 0.8841111325117408, 0.8841253744135291, 0.8841382281739573, 0.8841334844678572, 0.8841199505859478, 0.8841123843410297, 0.8841567279944129, 0.8841129679741336, 0.8840991245439683, 0.8840982912008672, 0.8841347435730287], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.036268560570406616, 0.07454108916133281, 0.11189575636530494, 0.1488079138330666, 0.18478382072724486, 0.21909496590414385, 0.25101568274558184, 0.28112886997742426, 0.30946879618563067, 0.3361619003302453, 0.3607370694839677, 0.38401850772570645, 0.405253593370681, 0.4254801283295466, 0.4445639455511853, 0.4622175142867294, 0.4794129080011288, 0.49497421156283816, 0.5097738713082862, 0.5240955711503492, 0.5365669734197269, 0.548971346531444, 0.5603692749430587, 0.5716375055812829, 0.5819112785547811, 0.5915787432727216, 0.6005877258260971, 0.6090793756361832, 0.6174308977864202, 0.6248475524543144, 0.6321726028376481, 0.6386521963754194, 0.6449884368753022, 0.6506178111376966, 0.6563007765883095, 0.661495747169313, 0.6666280283917643, 0.6714790150857204, 0.676076836704031, 0.6799554399782816, 0.6839762328121552, 0.6880151329684397, 0.6912930506768669, 0.6949532434442706, 0.6984447884522531, 0.7016360070844374, 0.7040289411086744, 0.706758371216557, 0.7097804997857899, 0.7123203985553285, 0.7143468712656842, 0.7167779597415254, 0.7186129649721922, 0.7209776250355905, 0.7234862960787182, 0.7247512124516445, 0.7266638276993265, 0.728318998722692, 0.7304262107668384, 0.7323185835719317, 0.733791844520085, 0.7351471939065554, 0.7367180708073154, 0.7376841708237225, 0.7399209954111545, 0.7413511414931566, 0.7432628905780277, 0.7450974460703003, 0.7463395369301378, 0.7474331517141571, 0.7488577405807685, 0.7501429590866977, 0.7521013488425611, 0.7529105746040128, 0.7546755930095754, 0.7555753682812836, 0.7565815080344804, 0.7581350359772673, 0.759094787509435, 0.7604336085984765, 0.7613221942747734, 0.762548137968531, 0.7635569195688918, 0.7646214553981472, 0.7659885467276849, 0.7667641802420399, 0.7674968124813901, 0.7682761927919861, 0.7691352969691129, 0.7700621816641444, 0.7710316847420824, 0.7715450860626785, 0.7722757019387149, 0.7723055500900994, 0.7725409624662551, 0.7733368591601417, 0.7733450112994739, 0.7755141693374482, 0.777603777441354, 0.7795607554483481, 0.7815142596286639, 0.7830425388145175, 0.7841839974707163, 0.7855551661536145, 0.7868980517408133, 0.7877394083231326, 0.7887662134433796, 0.789589593275623, 0.7900844354823228, 0.7906162721957622, 0.791072570192677, 0.7913824936139213, 0.7918842397815201, 0.7925911294799495, 0.7933382229984455, 0.7939628085487516, 0.7944608118617981, 0.7950830018070189, 0.7954700153028983, 0.7957318487217802, 0.7961892843785932, 0.7964778766485652, 0.79683526594154, 0.7971091176888769, 0.7974664770513898, 0.7976650006564918, 0.7980400139097433, 0.79823001195401, 0.7983633595914403, 0.7984467513713777, 0.7985818096209115, 0.7986667409517421, 0.7989049590817184, 0.7990206696400375, 0.7989772952588651, 0.7988884006821503, 0.7988959037991762, 0.7989372186809303, 0.7990242597081686, 0.7991901048687523, 0.799313921942118, 0.7993154940268973, 0.7992304301757889, 0.7991772572636316, 0.7992789745436691, 0.7993074259221336, 0.7994571614925708, 0.7995664799348048, 0.799467495015496, 0.7994038521592777, 0.7992865679410909, 0.7994495668322228, 0.7994365449193319, 0.7994990968938896, 0.7994943585147416, 0.799588779732168, 0.7995008013730326, 0.7994948630373107, 0.7996258548962303, 0.7995209324807789, 0.7994142952756228, 0.7994902497371419, 0.7995850818323283, 0.7995452718795172, 0.7994128161806467, 0.7993800847790731, 0.799483874352747, 0.799603758048873, 0.7995132523494074, 0.7995691630896172, 0.7994699098548271, 0.7995403028584257, 0.7995070298203241, 0.799549296764873, 0.7995374793813073, 0.7996133224635078, 0.7997202613485577], "moving_var_accuracy_train": [0.012366046977963395, 0.024298725580063883, 0.03469066472810785, 0.04358531501790976, 0.05079672928124503, 0.0561886682991382, 0.05989662882844704, 0.06214812576947755, 0.06327042859952671, 0.06333885758028154, 0.062419769962886756, 0.061179967381652135, 0.05930408089229916, 0.0572073297913953, 0.05482778610582275, 0.0522417867227945, 0.049713270475333166, 0.046983915936526885, 0.044333811160229636, 0.04183236458354454, 0.03924285870868578, 0.036837789438006495, 0.034446509850927315, 0.032246415591264725, 0.030097845226063194, 0.028078783917037956, 0.026167279694177516, 0.02430930928004299, 0.022613118464652422, 0.020989896261065146, 0.01951419133197969, 0.018076482259471532, 0.01675661873467997, 0.01547579810574293, 0.014359646314475503, 0.013299417164746734, 0.012314521747469713, 0.01137770161474768, 0.010533308644559439, 0.009703830334754297, 0.008953874153265201, 0.008276046012447368, 0.007594115575536242, 0.007018218201874017, 0.006474178027450836, 0.0059661685146411595, 0.005489547516526528, 0.0050574017563179505, 0.004688896164635519, 0.004329101055820337, 0.003966453194759136, 0.0036771834880111845, 0.0033774294532244974, 0.0031415108708916857, 0.0029097246085062548, 0.0026658530121405036, 0.002441846758871396, 0.0022625672648620013, 0.0021062995568124207, 0.001953840818895719, 0.0018057805348064531, 0.001680752871731656, 0.0015602281031599982, 0.0014313418868515508, 0.0013454957422351498, 0.0012583992491999663, 0.0012025924693054766, 0.0011459823951577952, 0.0010815471319387602, 0.0010106329033052865, 0.000961921510724147, 0.0009076771753997969, 0.0008486532403833831, 0.0007950700038026333, 0.0007674507563092418, 0.0007107882249527448, 0.0006610530095778069, 0.0006508490688269556, 0.0006148866174425581, 0.0005842566895802645, 0.0005577242464496698, 0.0005327485730764353, 0.0005076588749032429, 0.000494136650684081, 0.00048594495146299973, 0.00045187987005478324, 0.0004175251156404714, 0.0004009600322006441, 0.0003905904672908456, 0.000375608774260607, 0.0003473835397010549, 0.0003253662613725014, 0.00031117671477526814, 0.000281023570744361, 0.0002536065879698657, 0.00024505375643761135, 0.00022620660415159082, 0.0003240850480712855, 0.00040533017450904667, 0.0004559261236419355, 0.00048523439537579403, 0.0004977037251437276, 0.000504856979876352, 0.0004975895271606605, 0.0004861345369106462, 0.0004691684213262315, 0.00044990409360285615, 0.000425309258511467, 0.0004015275003230013, 0.00037623462420303955, 0.00035128663245854774, 0.00032459351091441107, 0.0002988574244269882, 0.0002824596328179177, 0.000266466688926777, 0.00024948368538508955, 0.00023091520992146071, 0.00021409041062616526, 0.00019714870459073265, 0.00018107896219626645, 0.00016621789832052983, 0.0001521144810240994, 0.000139382122133681, 0.0001272034816035713, 0.00011582657797115267, 0.0001054822658432508, 9.584167788723127e-05, 8.702902106775565e-05, 7.907824357885535e-05, 7.149688383502544e-05, 6.480208191363619e-05, 5.8673718078053716e-05, 5.31457319829529e-05, 4.8084773091819825e-05, 4.346319192469812e-05, 3.927364967164856e-05, 3.543253543029496e-05, 3.2030518997915455e-05, 2.8875618752835265e-05, 2.602714519661679e-05, 2.346402028713108e-05, 2.1144883978937415e-05, 1.907213670223982e-05, 1.717120265925267e-05, 1.5487544619199113e-05, 1.3963646423717422e-05, 1.258027574001272e-05, 1.1322775607481575e-05, 1.020985907925488e-05, 9.18907707242087e-06, 8.297828805697001e-06, 7.471514098370946e-06, 6.728649319022228e-06, 6.063432082127435e-06, 5.467037036535219e-06, 4.945711288488083e-06, 4.457480117562921e-06, 4.012089738900899e-06, 3.6375791156147154e-06, 3.276139403550116e-06, 2.9498473287748177e-06, 2.6600180983340373e-06, 2.4053761113703725e-06, 2.1652110851400475e-06, 1.9540446720103157e-06, 1.7587583987786675e-06, 1.5932064022409226e-06, 1.4357112479157499e-06, 1.293627095538483e-06, 1.1644669107127078e-06, 1.0496687132772726e-06, 9.45217074508999e-07, 8.683926034164077e-07, 7.987877974483186e-07, 7.20633782732162e-07, 6.485766546054642e-07, 5.956779680706091e-07], "duration": 40356.88395, "accuracy_train": [0.37067576394656704, 0.41959256904069764, 0.4527648723006645, 0.4837053931916759, 0.5086742470122739, 0.5270861811900148, 0.54201183410622, 0.5548881116763565, 0.5680688215208564, 0.5776702441514396, 0.583039535460502, 0.5980371057701181, 0.602963897828996, 0.6139578863279808, 0.6208850273394242, 0.6268817483388704, 0.6384824384574566, 0.640554704803433, 0.6493662978151532, 0.660104880433278, 0.6613153997669804, 0.6714744976236618, 0.6743809336355666, 0.6841211442183462, 0.6876313979443521, 0.6941400121700811, 0.6995111059200811, 0.7015110943844591, 0.7092280650147655, 0.7121112495385751, 0.7195488418235512, 0.7202024069075305, 0.7258263831210778, 0.7258042130975453, 0.7354285267280363, 0.7377289820967147, 0.7414953626799556, 0.7429852603589886, 0.7485849036198782, 0.7470724754291251, 0.7516766305601698, 0.7563025952265596, 0.7522847741325213, 0.7612319107027501, 0.7624620766426725, 0.7641358232973422, 0.7652286432378184, 0.7683919270833334, 0.7750175197259136, 0.7746894754752677, 0.7712958396779254, 0.780689801356589, 0.7770832972845146, 0.7859871752722407, 0.7859664472014581, 0.781616616486711, 0.7827784699150978, 0.7900572671534699, 0.7937746213432078, 0.7941001421765411, 0.7941498895464194, 0.7983562459625323, 0.7989822337001661, 0.7956594338316721, 0.805261216950443, 0.805516622831303, 0.8127460331879846, 0.8142337679378923, 0.8139082471045589, 0.8130021600452197, 0.818812869139904, 0.8186955302348652, 0.8180459305209486, 0.8197869082225913, 0.8270188419965854, 0.8203467463778147, 0.8223023947951275, 0.8333650562938354, 0.8289233010912699, 0.8312506128299187, 0.8334101173172758, 0.8349661645787191, 0.8360142839839424, 0.8404299037929125, 0.8435230726859542, 0.8369675949958472, 0.8365036466985051, 0.8433585098283499, 0.846476372162237, 0.8464760116740495, 0.8419401690545404, 0.8446627560908084, 0.8482406013519748, 0.8386642326504246, 0.8384774997692875, 0.8496596631021595, 0.8452894648048173, 0.8747440533868586, 0.8773485805417128, 0.8771865411014212, 0.8773965254706534, 0.877465558958564, 0.8791854481012367, 0.8784646519702842, 0.8793725414705611, 0.8795574719107604, 0.8802092345538022, 0.8794873569582872, 0.8803723554586563, 0.8802317650655224, 0.8805347553871355, 0.8795353018872278, 0.8794651869347545, 0.8839283911844776, 0.8845787118747692, 0.8844395634343853, 0.8835331158868586, 0.884300054505814, 0.8838353852320967, 0.8838586367201919, 0.8841372940891473, 0.8840213971368586, 0.8845089574104835, 0.884207048553433, 0.8840911516011444, 0.8843233059939092, 0.8841605455772426, 0.8842303000415282, 0.8844860664105758, 0.8837888822559062, 0.8843229455057217, 0.8842768030177187, 0.8844392029461978, 0.8843701694582872, 0.8843004149940015, 0.8843233059939092, 0.8841144030892396, 0.8844860664105758, 0.8840900701365817, 0.8840907911129567, 0.8841609060654301, 0.8841144030892396, 0.884300054505814, 0.8839512821843853, 0.8843233059939092, 0.884300054505814, 0.884207048553433, 0.8837885217677187, 0.8833936069582872, 0.8838586367201919, 0.8843701694582872, 0.8840675396248615, 0.8836726248154301, 0.8841605455772426, 0.8842306605297158, 0.884462454434293, 0.8837191277916205, 0.884021036648671, 0.8845089574104835, 0.8838582762320044, 0.8838815277200996, 0.8842299395533407, 0.8843698089700996, 0.8841144030892396, 0.8843004149940015, 0.8840446486249538, 0.8844159514581026, 0.8842535515296235, 0.884253912017811, 0.8840907911129567, 0.8839981456487633, 0.8840442881367663, 0.8845558208748615, 0.8837191277916205, 0.8839745336724806, 0.8840907911129567, 0.8844628149224806], "end": "2016-02-01 22:11:47.067000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0], "moving_var_accuracy_valid": [0.011838676372643285, 0.023837886738068328, 0.0340124385215371, 0.04287376098970689, 0.050234777782463456, 0.05580659215436927, 0.059396322411973734, 0.061617926578116336, 0.06268449667768394, 0.06282874328979274, 0.06198131941122011, 0.06066141576953316, 0.05865363395372448, 0.05647028500633195, 0.05410098522343899, 0.05149572310300032, 0.04900728487763912, 0.046285943906732234, 0.043628618873288345, 0.04111175676325483, 0.0384004039580111, 0.035945179812862195, 0.03351987678026261, 0.031310646297682286, 0.029129535367912903, 0.027057720697775244, 0.025082404527820925, 0.023223137123513045, 0.02152855471119485, 0.019870760138240524, 0.018366591392481858, 0.016907798444966222, 0.015578350093520791, 0.014305723775443698, 0.013165816264715064, 0.012092124112280974, 0.011119974495969805, 0.010219765693517277, 0.009388048796869778, 0.008584635987414043, 0.007871673363789007, 0.007231320457662019, 0.006604891112424797, 0.006064975101033304, 0.005568195569814887, 0.005103030900059002, 0.0046442630092522605, 0.004246884806751376, 0.003904395675876999, 0.0035720158801248268, 0.003251773616924692, 0.002979787975828534, 0.0027121143760148493, 0.0024912274933522396, 0.0022987456176406626, 0.002083271176751071, 0.0019078669328469527, 0.001741736559611558, 0.0016075259870413658, 0.0014790030618383383, 0.0013506372360466842, 0.0012321062600766678, 0.0011311045222050769, 0.001026394213159884, 0.0009687852499583575, 0.000890314585305313, 0.0008341761878463305, 0.0007810489137497431, 0.0007168291297115941, 0.0006559101564028661, 0.0006085842217124379, 0.0005625918790130394, 0.0005408503050345707, 0.0004926588915280878, 0.00047143061212305093, 0.00043157391076694275, 0.0003975273745169175, 0.00037949567868640285, 0.00034983621784924956, 0.0003309845732404833, 0.0003049923764535156, 0.00028801958027054004, 0.0002683763850985236, 0.0002517378753745876, 0.0002433845361668031, 0.00022446054868743905, 0.00020684524380191324, 0.00019162762243862325, 0.00017910740007916963, 0.00016892869721220506, 0.00016049525345416537, 0.00014681795635265613, 0.00013694035674223815, 0.000123254339277284, 0.00011142767623118117, 0.00010598597253411865, 9.538797339708802e-05, 0.0001281963954007555, 0.00015467491411185838, 0.00017367528897940045, 0.00019065336732406176, 0.0001926087660208725, 0.00018507424019308696, 0.00018348774818642658, 0.00018136904867053995, 0.00016960307189099367, 0.00016213172349658297, 0.0001520201402802307, 0.00013902194553799094, 0.00012766540359205006, 0.00011677273399074074, 0.0001059599333349889, 9.762968295179113e-05, 9.236395206832137e-05, 8.815089538989815e-05, 8.284676983776837e-05, 7.679415855223938e-05, 7.259882564842018e-05, 6.668695809751359e-05, 6.063527294095246e-05, 5.6454972067972284e-05, 5.155904434576343e-05, 4.755268387178439e-05, 4.347236850028157e-05, 4.027448307603412e-05, 3.6601739364474915e-05, 3.4207279889055325e-05, 3.111144521157611e-05, 2.8160335022092745e-05, 2.5406889220533453e-05, 2.3030366875384603e-05, 2.0792250166455944e-05, 1.9223756046855423e-05, 1.7421880841928607e-05, 1.569662479021456e-05, 1.419808252311688e-05, 1.2778780941691138e-05, 1.1516265122611193e-05, 1.0432823874154357e-05, 9.637083042340356e-06, 8.811350747018034e-06, 7.930237915371207e-06, 7.202336852722617e-06, 6.507549394735857e-06, 5.949912100786199e-06, 5.362206219136306e-06, 5.0277722667101824e-06, 4.632549736351148e-06, 4.257476890971153e-06, 3.868182920202612e-06, 3.6051649187035454e-06, 3.4837661734251953e-06, 3.136915688020733e-06, 2.858438864908355e-06, 2.572797048550071e-06, 2.3957556403976917e-06, 2.2258418014434644e-06, 2.0035749957794357e-06, 1.957647300130246e-06, 1.8609609894945274e-06, 1.7772083322568365e-06, 1.651409221052983e-06, 1.5672064354446874e-06, 1.424749282985655e-06, 1.4401749641562814e-06, 1.3057995695814664e-06, 1.2721700930542352e-06, 1.2743019891200726e-06, 1.2205933249299123e-06, 1.1266680902742568e-06, 1.1026621227934108e-06, 1.036992485114781e-06, 9.432570921838965e-07, 8.650098343790202e-07, 7.797657059301492e-07, 7.535586933963205e-07, 7.811261502779774e-07], "accuracy_test": 0.7942422672193878, "start": "2016-02-01 10:59:10.184000", "learning_rate_per_epoch": [0.0007021723431535065, 0.0007021723431535065, 0.0007021723431535065, 0.0007021723431535065, 0.0007021723431535065, 0.0007021723431535065, 0.0007021723431535065, 0.0007021723431535065, 0.0007021723431535065, 0.0007021723431535065, 0.0007021723431535065, 0.0007021723431535065, 0.0007021723431535065, 0.0007021723431535065, 0.0007021723431535065, 0.0007021723431535065, 0.0007021723431535065, 0.0007021723431535065, 0.0007021723431535065, 0.0007021723431535065, 0.0007021723431535065, 0.0007021723431535065, 0.0007021723431535065, 0.0007021723431535065, 0.0007021723431535065, 0.0007021723431535065, 0.0007021723431535065, 0.0007021723431535065, 0.0007021723431535065, 0.0007021723431535065, 0.0007021723431535065, 0.0007021723431535065, 0.0007021723431535065, 0.0007021723431535065, 0.0007021723431535065, 0.0007021723431535065, 0.0007021723431535065, 0.0007021723431535065, 0.0007021723431535065, 0.0007021723431535065, 0.0007021723431535065, 0.0007021723431535065, 0.0007021723431535065, 0.0007021723431535065, 0.0007021723431535065, 0.0007021723431535065, 0.0007021723431535065, 0.0007021723431535065, 0.0007021723431535065, 0.0007021723431535065, 0.0007021723431535065, 0.0007021723431535065, 0.0007021723431535065, 0.0007021723431535065, 0.0007021723431535065, 0.0007021723431535065, 0.0007021723431535065, 0.0007021723431535065, 0.0007021723431535065, 0.0007021723431535065, 0.0007021723431535065, 0.0007021723431535065, 0.0007021723431535065, 0.0007021723431535065, 0.0007021723431535065, 0.0007021723431535065, 0.0007021723431535065, 0.0007021723431535065, 0.0007021723431535065, 0.0007021723431535065, 0.0007021723431535065, 0.0007021723431535065, 0.0007021723431535065, 0.0007021723431535065, 0.0007021723431535065, 0.0007021723431535065, 0.0007021723431535065, 0.0007021723431535065, 0.0007021723431535065, 0.0007021723431535065, 0.0007021723431535065, 0.0007021723431535065, 0.0007021723431535065, 0.0007021723431535065, 0.0007021723431535065, 0.0007021723431535065, 0.0007021723431535065, 0.0007021723431535065, 0.0007021723431535065, 0.0007021723431535065, 0.0007021723431535065, 0.0007021723431535065, 0.0007021723431535065, 0.0007021723431535065, 0.0007021723431535065, 0.0007021723431535065, 7.021723286015913e-05, 7.021723286015913e-05, 7.021723286015913e-05, 7.021723286015913e-05, 7.021723286015913e-05, 7.021723286015913e-05, 7.021723286015913e-05, 7.021723286015913e-05, 7.021723286015913e-05, 7.021723286015913e-05, 7.021723286015913e-05, 7.021723286015913e-05, 7.021723286015913e-05, 7.021723286015913e-05, 7.021723286015913e-05, 7.021723286015913e-05, 7.021723376965383e-06, 7.021723149591708e-07, 7.021723291700255e-08, 7.021723202882413e-09, 7.021723202882413e-10, 7.021723480438169e-11, 7.021723480438169e-12, 7.021723697278603e-13, 7.021723561753332e-14, 7.021723561753332e-15, 7.02172366763245e-16, 7.021723799981348e-17, 7.021723634545226e-18, 7.021723737942802e-19, 7.021723608695832e-20, 7.021723770254545e-21, 7.021723871228741e-22, 7.021724123664231e-23, 7.021724281436412e-24, 7.021724380044025e-25, 7.021724256784508e-26, 7.021724102710113e-27, 7.021723910117118e-28, 7.021723910117118e-29, 7.021723910117118e-30, 7.02172381607757e-31, 7.02172381607757e-32, 7.021723669140776e-33, 7.0217236691407764e-34, 7.021723669140776e-35, 7.021723382154851e-36, 7.0217232027886475e-37, 7.021723314892525e-38, 7.021723034632832e-39, 7.021724435931296e-40, 7.021766474885226e-41, 7.021906604731658e-42, 7.0205053062673335e-43, 7.006492321624085e-44, 7.006492321624085e-45, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "accuracy_train_first": 0.37067576394656704, "accuracy_train_last": 0.8844628149224806, "batch_size_eval": 1024, "accuracy_train_std": [0.016538769146036983, 0.01581953040604757, 0.01742120787521906, 0.016382138737387298, 0.016656396577195604, 0.016297099709481944, 0.01650151132493807, 0.01672102796238987, 0.016537484728419375, 0.015997761094085988, 0.016114460477277017, 0.016911741162069595, 0.01700133422409874, 0.016503235787415475, 0.016286126271445777, 0.016882177743776847, 0.015688064745880238, 0.015875408397026548, 0.015835645279857376, 0.015858455676692078, 0.01656860554499251, 0.01530827932650003, 0.014943118730850805, 0.014800280250381985, 0.015497041277014047, 0.014293929692055668, 0.01607859536271576, 0.014923815749101187, 0.014753768493764136, 0.014371775125837028, 0.013705083042242099, 0.015093730705561817, 0.014322870920559143, 0.014520516697033818, 0.013911696215380163, 0.013414268649503304, 0.014307100006667997, 0.015088430936110774, 0.01419003562292413, 0.014016579247897972, 0.013165214048201231, 0.014003842018738836, 0.016125781961279423, 0.013913943354953167, 0.014511515462463381, 0.015101250015903797, 0.014240517666363466, 0.015623425601174138, 0.014138677956402994, 0.014022976570900155, 0.013776259987142794, 0.015470125049740385, 0.014327824529773353, 0.014322956374568756, 0.013952233828547048, 0.014703413220193608, 0.013769619979427529, 0.014216310243333445, 0.013074649914859206, 0.013622578310411465, 0.014018622172042453, 0.013862659367815194, 0.01322064150544758, 0.014609401824505541, 0.015116623928507834, 0.013393364588871318, 0.01260275744753859, 0.014391780178248622, 0.013341358792509951, 0.013059198322122637, 0.012467134463718848, 0.014225604579782797, 0.01187140358411384, 0.0138819687399448, 0.012429909013835876, 0.012290399165416811, 0.011709891586910527, 0.011844598645722232, 0.012037398661534415, 0.011556772990109402, 0.012504633214737892, 0.01212017526585727, 0.012172511086775015, 0.011259929255879144, 0.011840450416395676, 0.011386549241741754, 0.011633687143495195, 0.011199448959012965, 0.012174154419394975, 0.011697678336731722, 0.011747154831996946, 0.011019592380790153, 0.013609350231217066, 0.011823249851850612, 0.012746410943271988, 0.010735361056037453, 0.011745617002334204, 0.010370168402247585, 0.010511493109722946, 0.01027001398950469, 0.010679448811179599, 0.010889036193627816, 0.01018724516007346, 0.010759437156150298, 0.010944508064863624, 0.010406393383950227, 0.010245082129893921, 0.010918482481837237, 0.010747625159687671, 0.010170575730581085, 0.010583012134195421, 0.010379232261981905, 0.010869887874170968, 0.0102148237012978, 0.010232126911401688, 0.01049157657534323, 0.010110896731246023, 0.010046170405746105, 0.009917931521282605, 0.01015199107920446, 0.010255499480020398, 0.009807095295072503, 0.010364022852561147, 0.010386015250134092, 0.010369079606596396, 0.010043830445062647, 0.010424876378445311, 0.010131610872476311, 0.009545610612904137, 0.009874444958216023, 0.010198683360702523, 0.010425530635254742, 0.010155105869540078, 0.010374713919789873, 0.010160595758456185, 0.009925575587818307, 0.00975103843543509, 0.009955469903823032, 0.010274623114807264, 0.010060275600207622, 0.010376664245213814, 0.010347550392167199, 0.00988898571623774, 0.010110763789072207, 0.0104363136262088, 0.010269705258180004, 0.010453569616130382, 0.010405339501329167, 0.010128844303280054, 0.010066639284795025, 0.009688886980640249, 0.01007560522779637, 0.00980568305533584, 0.01023584506110121, 0.010394702678322203, 0.01047392007535829, 0.010048575605396497, 0.010485369559854782, 0.010328908978465902, 0.009948835538409763, 0.009718897548427845, 0.010679384884334075, 0.010189403407201777, 0.010203918367168996, 0.009982488377882686, 0.009975251041967333, 0.010282106111946052, 0.010364340668564873, 0.010153888428327651, 0.009898728085033247, 0.010132163795546208, 0.010414207706274287, 0.010214955306680018, 0.010366674474558815, 0.010102500400850945, 0.010030891053999872, 0.010214844531760338], "accuracy_test_std": 0.01104359000773239, "error_valid": [0.6373143942959337, 0.5810061535203314, 0.5519122387989458, 0.5189826689570783, 0.49143301722515065, 0.4721047275037651, 0.46169786568147586, 0.44785244493599397, 0.43547186794051207, 0.42360016236822284, 0.4180864081325302, 0.4064485480986446, 0.40363063582454817, 0.3924810570406627, 0.38368169945406627, 0.3789003670933735, 0.36582854856927716, 0.36497405638177716, 0.3570291909826807, 0.34700913027108427, 0.3511904061558735, 0.33938929546310237, 0.3370493693524097, 0.3269484186746988, 0.3256247646837349, 0.32141407426581325, 0.31833143119352414, 0.31449577607304224, 0.3074054028614458, 0.3084025555346386, 0.30190194371234935, 0.3030314617846386, 0.297985398625753, 0.298717820500753, 0.2925525343561747, 0.2917495176016567, 0.2871814406061747, 0.2848621046686747, 0.2825427687311747, 0.2851371305534638, 0.2798366316829819, 0.275634765625, 0.2792056899472892, 0.27210502164909633, 0.27013130647590367, 0.26964302522590367, 0.2744346526731928, 0.2686767578125, 0.26302034309111444, 0.2648205125188253, 0.26741487434111444, 0.26134224397590367, 0.2648719879518072, 0.2577404343938253, 0.25393566453313254, 0.2638645401920181, 0.2561226350715362, 0.2567844620670181, 0.2506088808358433, 0.2506500611822289, 0.2529488069465362, 0.2526546616152108, 0.24914403708584332, 0.25362092902861444, 0.23994758330195776, 0.24577754376882532, 0.23953136765813254, 0.23839155449924698, 0.24248164533132532, 0.24272431522966864, 0.23832095961972888, 0.23829007435993976, 0.23027314335466864, 0.23980639354292166, 0.22943924134036142, 0.23632665427334332, 0.23436323418674698, 0.22788321253765065, 0.2322674487010542, 0.22751700160015065, 0.2306805346385542, 0.22641836878765065, 0.22736404602786142, 0.2257977221385542, 0.22170763130647586, 0.2262551181287651, 0.22590949736445776, 0.22470938441265065, 0.22313276543674698, 0.2215958560805723, 0.22024278755647586, 0.22383430205195776, 0.22114875517695776, 0.22742581654743976, 0.22534032614834332, 0.21950007059487953, 0.2265816194465362, 0.2049634083207832, 0.20358974962349397, 0.20282644248870485, 0.20090420274849397, 0.20320294851280118, 0.20554287462349397, 0.20210431570030118, 0.20101597797439763, 0.20468838243599397, 0.20199254047439763, 0.20299998823418675, 0.20546198465737953, 0.2045971973832832, 0.2048207478350903, 0.20582819559487953, 0.2036000447100903, 0.20104686323418675, 0.1999379353350903, 0.20041592149849397, 0.2010571583207832, 0.19931728868599397, 0.20104686323418675, 0.2019116505082832, 0.1996937947100903, 0.20092479292168675, 0.19994823042168675, 0.2004262165850903, 0.19931728868599397, 0.2005482868975903, 0.19858486681099397, 0.2000600056475903, 0.20043651167168675, 0.20080272260918675, 0.2002026661332832, 0.2005688770707832, 0.19895107774849397, 0.1999379353350903, 0.20141307417168675, 0.2019116505082832, 0.2010365681475903, 0.2006909473832832, 0.20019237104668675, 0.19931728868599397, 0.1995717243975903, 0.2006703572100903, 0.20153514448418675, 0.2013012989457832, 0.19980556993599397, 0.20043651167168675, 0.19919521837349397, 0.1994496540850903, 0.2014233692582832, 0.20116893354668675, 0.2017689900225903, 0.1990834431475903, 0.20068065229668675, 0.1999379353350903, 0.2005482868975903, 0.19956142931099397, 0.20129100385918675, 0.20055858198418675, 0.19919521837349397, 0.2014233692582832, 0.2015454395707832, 0.19982616010918675, 0.19956142931099397, 0.2008130176957832, 0.20177928510918675, 0.2009144978350903, 0.19958201948418675, 0.19931728868599397, 0.2013012989457832, 0.19992764024849397, 0.2014233692582832, 0.19982616010918675, 0.2007924275225903, 0.20007030073418675, 0.2005688770707832, 0.19970408979668675, 0.19931728868599397], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.031187074213432853, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 128, "valid_ratio": 0.15, "learning_rate": 0.0007021723146388575, "optimization": "nesterov_momentum", "nb_data_augmentation": 1, "learning_rate_decay_method": "discrete", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 1.7602059413949333e-06, "rotation_range": [0, 0], "momentum": 0.9526551553617517}, "accuracy_valid_max": 0.801415133189006, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.800682711314006, "accuracy_valid_std": [0.006832068311629688, 0.013901971118787368, 0.006740751981190769, 0.008434042657230013, 0.019482079512921517, 0.016060269836834642, 0.01585883339941423, 0.020362477297754287, 0.01696371184892561, 0.01607040715041988, 0.01828558515515556, 0.017807134128665608, 0.015598750661500878, 0.017884307791534222, 0.015849881842979325, 0.014576300590638245, 0.011887927871001777, 0.014870127828769527, 0.01238745271632199, 0.009803273907861256, 0.009248099373324136, 0.010738615500950545, 0.011525174130037757, 0.01408780375785198, 0.014306416122580826, 0.013986158531005698, 0.015153779324810756, 0.015026958799850592, 0.017506099859457776, 0.015367063965689078, 0.01693302752722302, 0.014728430684712891, 0.014497612578882641, 0.015674434716936417, 0.016955873805325067, 0.012869530139920097, 0.017333065115267932, 0.014854154006584044, 0.016627898405959688, 0.014242368639171596, 0.014774374425669659, 0.014090531279793043, 0.013293306870230477, 0.013665643559000273, 0.014801713401973865, 0.015443826599500714, 0.01370503466969841, 0.015840475161534792, 0.015841303186755376, 0.017701815542851024, 0.016080441223904833, 0.013906795581701357, 0.014495842442262026, 0.016277227057241713, 0.017936043600062783, 0.016062613679607176, 0.01850274387412544, 0.015574219427846833, 0.02002781172096851, 0.01717784231048604, 0.016742319135802416, 0.012925634740548446, 0.018236326167004213, 0.01412739544812714, 0.020929801812722013, 0.015346261268578106, 0.0168624575331619, 0.018113539182076165, 0.015617262372010736, 0.02055770330834162, 0.012008317974917014, 0.013032737197060806, 0.021162430082661752, 0.010607149726792872, 0.0174045253170195, 0.015373317626001711, 0.014413127237904463, 0.014572017604676643, 0.01675065742187739, 0.01308036796737759, 0.016155680550349574, 0.01272216711289779, 0.016822999742290505, 0.012230575610063524, 0.01600774775752035, 0.01701152637541211, 0.017581566202917773, 0.009857316959786396, 0.010542905653702854, 0.015687301315235602, 0.01575511419188955, 0.0127007341357274, 0.011697028436786825, 0.012115414415812938, 0.01237088882447217, 0.017362816387955712, 0.015347219126636101, 0.016053545416610544, 0.018767293698910257, 0.019731653769120346, 0.01747952215134964, 0.018811663490644958, 0.019196762658742803, 0.019552177815662986, 0.017384328265906853, 0.018473788518203648, 0.01729686374830755, 0.01477856627413949, 0.01677806923026072, 0.014223371187068672, 0.016165645571032505, 0.017009226625972332, 0.015732994248199034, 0.016031791568055686, 0.016220548149677317, 0.016945574169814744, 0.0152088562496635, 0.017137735939135753, 0.015927350086381586, 0.015855548165517604, 0.016133762760661387, 0.016255604754433075, 0.01485288998581619, 0.016213595838227998, 0.016857201723758385, 0.016317674012356478, 0.016218222330113115, 0.016674989611107884, 0.015004704810381632, 0.016748168098827544, 0.014250760513213587, 0.015213010845818204, 0.01665257883600913, 0.016381434339404918, 0.01622633304628443, 0.016307726418917017, 0.017279615995452597, 0.015369358360633722, 0.015871427651422524, 0.017338285149157355, 0.01606656157574077, 0.016296299033048873, 0.01583360732898196, 0.015364702709903248, 0.01670515580917487, 0.015326980400256724, 0.016365901546270434, 0.01606506974172107, 0.015289297369664705, 0.016627238927970057, 0.0169984871667877, 0.016624093835750385, 0.0160433386652117, 0.01600600023372657, 0.01728958734355589, 0.016818603920136444, 0.015964774033044374, 0.014782113104819548, 0.016811445069550502, 0.014627845774159858, 0.015872155941544633, 0.014990178188761352, 0.016618956573972544, 0.015086994766622867, 0.016279117668073585, 0.016692191064525137, 0.015304925646287161, 0.01657191922474341, 0.015496038371417218, 0.017320626637398754, 0.015582763556696081, 0.015374913089406169, 0.016303254618927586, 0.015120872492766183, 0.014896273219531114, 0.015545010269535723, 0.016607847533227416], "accuracy_valid": [0.36268560570406627, 0.41899384647966864, 0.4480877612010542, 0.48101733104292166, 0.5085669827748494, 0.5278952724962349, 0.5383021343185241, 0.552147555064006, 0.5645281320594879, 0.5763998376317772, 0.5819135918674698, 0.5935514519013554, 0.5963693641754518, 0.6075189429593373, 0.6163183005459337, 0.6210996329066265, 0.6341714514307228, 0.6350259436182228, 0.6429708090173193, 0.6529908697289157, 0.6488095938441265, 0.6606107045368976, 0.6629506306475903, 0.6730515813253012, 0.6743752353162651, 0.6785859257341867, 0.6816685688064759, 0.6855042239269578, 0.6925945971385542, 0.6915974444653614, 0.6980980562876506, 0.6969685382153614, 0.702014601374247, 0.701282179499247, 0.7074474656438253, 0.7082504823983433, 0.7128185593938253, 0.7151378953313253, 0.7174572312688253, 0.7148628694465362, 0.7201633683170181, 0.724365234375, 0.7207943100527108, 0.7278949783509037, 0.7298686935240963, 0.7303569747740963, 0.7255653473268072, 0.7313232421875, 0.7369796569088856, 0.7351794874811747, 0.7325851256588856, 0.7386577560240963, 0.7351280120481928, 0.7422595656061747, 0.7460643354668675, 0.7361354598079819, 0.7438773649284638, 0.7432155379329819, 0.7493911191641567, 0.7493499388177711, 0.7470511930534638, 0.7473453383847892, 0.7508559629141567, 0.7463790709713856, 0.7600524166980422, 0.7542224562311747, 0.7604686323418675, 0.761608445500753, 0.7575183546686747, 0.7572756847703314, 0.7616790403802711, 0.7617099256400602, 0.7697268566453314, 0.7601936064570783, 0.7705607586596386, 0.7636733457266567, 0.765636765813253, 0.7721167874623494, 0.7677325512989458, 0.7724829983998494, 0.7693194653614458, 0.7735816312123494, 0.7726359539721386, 0.7742022778614458, 0.7782923686935241, 0.7737448818712349, 0.7740905026355422, 0.7752906155873494, 0.776867234563253, 0.7784041439194277, 0.7797572124435241, 0.7761656979480422, 0.7788512448230422, 0.7725741834525602, 0.7746596738516567, 0.7804999294051205, 0.7734183805534638, 0.7950365916792168, 0.796410250376506, 0.7971735575112951, 0.799095797251506, 0.7967970514871988, 0.794457125376506, 0.7978956842996988, 0.7989840220256024, 0.795311617564006, 0.7980074595256024, 0.7970000117658133, 0.7945380153426205, 0.7954028026167168, 0.7951792521649097, 0.7941718044051205, 0.7963999552899097, 0.7989531367658133, 0.8000620646649097, 0.799584078501506, 0.7989428416792168, 0.800682711314006, 0.7989531367658133, 0.7980883494917168, 0.8003062052899097, 0.7990752070783133, 0.8000517695783133, 0.7995737834149097, 0.800682711314006, 0.7994517131024097, 0.801415133189006, 0.7999399943524097, 0.7995634883283133, 0.7991972773908133, 0.7997973338667168, 0.7994311229292168, 0.801048922251506, 0.8000620646649097, 0.7985869258283133, 0.7980883494917168, 0.7989634318524097, 0.7993090526167168, 0.7998076289533133, 0.800682711314006, 0.8004282756024097, 0.7993296427899097, 0.7984648555158133, 0.7986987010542168, 0.800194430064006, 0.7995634883283133, 0.800804781626506, 0.8005503459149097, 0.7985766307417168, 0.7988310664533133, 0.7982310099774097, 0.8009165568524097, 0.7993193477033133, 0.8000620646649097, 0.7994517131024097, 0.800438570689006, 0.7987089961408133, 0.7994414180158133, 0.800804781626506, 0.7985766307417168, 0.7984545604292168, 0.8001738398908133, 0.800438570689006, 0.7991869823042168, 0.7982207148908133, 0.7990855021649097, 0.8004179805158133, 0.800682711314006, 0.7986987010542168, 0.800072359751506, 0.7985766307417168, 0.8001738398908133, 0.7992075724774097, 0.7999296992658133, 0.7994311229292168, 0.8002959102033133, 0.800682711314006], "seed": 486495487, "model": "residualv3", "loss_std": [0.41378694772720337, 0.0866660550236702, 0.08908919245004654, 0.08888416737318039, 0.08557260781526566, 0.090621717274189, 0.0905262753367424, 0.08846664428710938, 0.08747634291648865, 0.0875244140625, 0.09030701220035553, 0.08998966962099075, 0.08854949474334717, 0.08919432014226913, 0.09063016623258591, 0.08967743068933487, 0.09277361631393433, 0.09233256429433823, 0.09390096366405487, 0.09039276093244553, 0.09491275995969772, 0.09116252511739731, 0.09144202619791031, 0.09244729578495026, 0.09490881115198135, 0.09295012801885605, 0.09499553591012955, 0.0925038754940033, 0.09266682714223862, 0.09299157559871674, 0.09071347862482071, 0.09279175847768784, 0.09295853972434998, 0.09479478001594543, 0.09165250509977341, 0.09123731404542923, 0.09123020619153976, 0.0925469771027565, 0.09421347081661224, 0.09014856815338135, 0.09187187254428864, 0.0917658805847168, 0.08783160150051117, 0.09098305553197861, 0.09000268578529358, 0.08896350115537643, 0.09050420671701431, 0.08972933143377304, 0.08745405822992325, 0.09124338626861572, 0.088399238884449, 0.09239687770605087, 0.08777270466089249, 0.09076331555843353, 0.08784570544958115, 0.08958810567855835, 0.08734191954135895, 0.08623991161584854, 0.08737573027610779, 0.08742017298936844, 0.08627409487962723, 0.0876421183347702, 0.08817046135663986, 0.08728843182325363, 0.08705361187458038, 0.08752640336751938, 0.0851622223854065, 0.08808694034814835, 0.08432698994874954, 0.08379712700843811, 0.08584237098693848, 0.08395255357027054, 0.08313855528831482, 0.08167634159326553, 0.08426011353731155, 0.085849829018116, 0.08150382339954376, 0.08338985592126846, 0.08270635455846786, 0.08345040678977966, 0.08012688159942627, 0.08174046874046326, 0.08194781839847565, 0.08199603855609894, 0.07895702123641968, 0.08300228416919708, 0.0807606652379036, 0.08090900629758835, 0.08320245891809464, 0.07936134189367294, 0.07778911292552948, 0.08064042031764984, 0.0787750706076622, 0.07773087173700333, 0.07918134331703186, 0.07646245509386063, 0.07954531908035278, 0.08078742772340775, 0.07258381694555283, 0.07471784949302673, 0.07619508355855942, 0.0732954815030098, 0.07251635938882828, 0.07478559017181396, 0.0729709342122078, 0.07502996921539307, 0.07659631967544556, 0.07528354972600937, 0.07376167178153992, 0.07311496883630753, 0.07322752475738525, 0.07267917692661285, 0.0750432163476944, 0.07605970650911331, 0.07465681433677673, 0.07399404048919678, 0.07501668483018875, 0.07176433503627777, 0.07447700202465057, 0.07236676663160324, 0.07224971055984497, 0.07395002990961075, 0.07310930639505386, 0.07268685102462769, 0.07169504463672638, 0.0721077024936676, 0.0743856281042099, 0.07372192293405533, 0.07447074353694916, 0.07240310311317444, 0.07338783890008926, 0.07163970917463303, 0.07239653170108795, 0.07350105047225952, 0.07284761965274811, 0.07238752394914627, 0.07029195874929428, 0.07263345271348953, 0.07258515059947968, 0.0726926252245903, 0.07383210957050323, 0.07500247657299042, 0.07353705167770386, 0.07343821972608566, 0.07281394302845001, 0.07415492832660675, 0.07159721851348877, 0.07218006998300552, 0.07310333102941513, 0.0716516524553299, 0.07171790301799774, 0.07435642182826996, 0.07064422219991684, 0.07245926558971405, 0.07200880348682404, 0.07221432775259018, 0.07458723336458206, 0.0714266449213028, 0.07160942256450653, 0.0720982626080513, 0.07314878702163696, 0.07398203760385513, 0.07062942534685135, 0.07400566339492798, 0.0725153312087059, 0.07423389703035355, 0.07395146042108536, 0.07357504963874817, 0.07436994463205338, 0.07144881039857864, 0.07440831512212753, 0.07203943282365799, 0.07186098396778107, 0.07268112897872925, 0.0722319483757019, 0.07261265069246292, 0.07260890305042267]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:29 2016", "state": "available"}], "summary": "e644936bb7fe649421cee11df86e933b"}