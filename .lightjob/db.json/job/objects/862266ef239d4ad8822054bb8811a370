{"content": {"hp_model": {"f0": 64, "f1": 64, "f2": 64, "f3": 32, "nonlin": "leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.006197540917399723, 0.007419062360977903, 0.015278330905293696, 0.013101207961734317, 0.015091117552073749, 0.0182039052158816, 0.016454683270300387, 0.01890241015011763, 0.017788408451596398, 0.017241941000705696, 0.015582347191791789, 0.015103164080925623, 0.014118160162957258, 0.014912595951175599, 0.01715152365734249, 0.014462527333855646, 0.014850888229646586, 0.01156981092365935, 0.012461551580599193, 0.011881639297726107, 0.015009895591522797, 0.01001251686281356, 0.011028049437593756, 0.01194667089804739, 0.010132575330142508, 0.011161747494164972, 0.011626522407232049, 0.013186972838112243, 0.009340577006927529, 0.011414273102844475, 0.011342633677445717, 0.012178131016277067, 0.009684456183670528, 0.008963688418742368, 0.009765544818488644, 0.011911725259073393, 0.0090890049451586, 0.01641381599786984, 0.01155780927608093, 0.018453454394699455, 0.015250823299647262, 0.011558050602693505, 0.012316141461302763, 0.017058312697242027, 0.0166709813145923, 0.016266784678689257, 0.016843137632587376, 0.011589931406669784, 0.015847182778052245, 0.009559550852100184, 0.015531688987711597, 0.01229795388372218, 0.013066373897920008, 0.011044263752332806, 0.01448449558757186, 0.013906145728234853, 0.010628240553247618, 0.008799640518096842, 0.015452384594809934, 0.016500634381938276, 0.009100296455209855, 0.013392332517440578, 0.012300766329717824, 0.01471240078576022, 0.01294306499503843, 0.010313667272714287, 0.015315674217377968, 0.016511494145140238, 0.01389253101931745, 0.011972719156432044, 0.012295296576022187, 0.01298513676978996, 0.008891481470493134, 0.01260008187069083, 0.010703420765110171, 0.015227700962103005, 0.01012375578105544, 0.011794429679042246, 0.013064789893685846, 0.011125697793559715, 0.014050979729572175, 0.0095588986073799, 0.010245285838927498, 0.008562944836364737, 0.01638120141570991, 0.010554609061821364, 0.010704301328978092, 0.012290056444730734, 0.018841557607550907, 0.012150695308142622, 0.006646545390654345, 0.014830209035012637, 0.016969557987656952, 0.010575974015100477, 0.012854655037877043, 0.006624079716554568, 0.014265985494674864, 0.019127454606148817, 0.018270153377662534, 0.009771252894703422, 0.01146586264486661, 0.01760328948992313, 0.009066495804269947, 0.011209038309257952, 0.015317084831030265, 0.009920933695694804, 0.014082599643919252, 0.013821499444470923, 0.007473024674428289, 0.014664253743988652, 0.010492250863343888, 0.015090762085046975, 0.011896375375488662, 0.011248498125496121, 0.015161962917286585, 0.01050848584275733, 0.011078704613115513, 0.015265755527040336, 0.014590826180168071, 0.016433516114743155, 0.017307205286848475, 0.013927508722171304, 0.007845651693584957, 0.011389369521919802, 0.008381067187376533, 0.010065242354438848, 0.014924583961930202, 0.007076625627680519, 0.012383582640083246, 0.011069853749421954, 0.009702434026313475, 0.013524330769184339, 0.015798344412645476, 0.007588845656034665, 0.011472236494293159, 0.013625967132866413, 0.010318517602561294, 0.01363866666641152, 0.013330614411159902, 0.009682105236453359, 0.011397696625008107, 0.012454879496367095, 0.012206088176550615, 0.016334616723103636, 0.011444022282679848, 0.01620750704978822, 0.01472420069131592, 0.007300585054824559, 0.012373619401413998, 0.014380280011815589, 0.009827937397639246, 0.014113283963329764, 0.013220417463267485, 0.0066137130553328765, 0.011326807242090786, 0.0082123868628623, 0.015286762866449961, 0.009981938726198297, 0.013669237610581631, 0.015846836149105245, 0.01098224821747361, 0.010043038189019781, 0.015845296394329692, 0.01174518063014398, 0.015871853945055595, 0.00885342217554382, 0.015908395981262702, 0.009344180070117034, 0.015687331442986646, 0.014632314128596906, 0.011766927183803219, 0.005910061040285662, 0.006524584883938424, 0.008554064726297874, 0.01648143276302152, 0.0085305574329999, 0.0129338487570215, 0.009845166835867256, 0.015109493825857752, 0.012935659237451713, 0.012301153447940711, 0.013366434777426195, 0.008094808974189809, 0.013253339246234123, 0.012113083226305091, 0.011832370163854087, 0.01208500872662508, 0.007909911841555232, 0.01638740821541302, 0.010048218063935903, 0.01249901148419856, 0.013942575272638553, 0.020560356993742705, 0.013888394991488834, 0.011386626407723185, 0.01019726139893094, 0.008610861166474306, 0.009448997456333897, 0.01162498819273975, 0.011162976831278752, 0.016014946443862466, 0.009680025218239054, 0.016218934581102816, 0.00833795905133256, 0.012893935585858435, 0.012629077561886757, 0.005521862377767188, 0.007514514464168542, 0.017210797440246582, 0.010879296519378585, 0.009803986894117529, 0.009698581346356825, 0.010142699165928847, 0.017509613958318807, 0.0107809379249486, 0.007831333029365782], "moving_avg_accuracy_train": [0.05999511178017716, 0.12330360026762638, 0.1835169818775378, 0.24299772950119963, 0.29809028882313576, 0.349747120267511, 0.3995744605721091, 0.44546988991287717, 0.4874846943647419, 0.5250211814678117, 0.5605408339236514, 0.5938963465826428, 0.6241232822686015, 0.651108816202594, 0.6755471116599001, 0.6990572141070976, 0.7193050282203358, 0.7395389902031194, 0.7586028495519013, 0.7759974520955575, 0.7912573911848666, 0.8052168037021311, 0.8186918414474588, 0.8305681872491876, 0.8412568624219248, 0.8518903268118845, 0.8613535600152475, 0.8701307784184846, 0.8755915543683951, 0.8823800342732407, 0.888703579878078, 0.8950388731914884, 0.9008381852771015, 0.905932152313714, 0.911079264463295, 0.9156839078074787, 0.919035247122015, 0.9223164113229272, 0.9250231375252137, 0.928240369009634, 0.9300127944217935, 0.9325334946653838, 0.9354599978024353, 0.9381334502531811, 0.9397188159779091, 0.9423687094527926, 0.9443815176730265, 0.9455699051902845, 0.9472812310760641, 0.949707342118513, 0.950200639116287, 0.9510630250535417, 0.9524737938267959, 0.9545293139227061, 0.9558492562269102, 0.957004580119723, 0.9581746160054067, 0.9593811441727693, 0.9596556146865093, 0.9604860342071533, 0.9605361555234257, 0.962241132567521, 0.963047948476282, 0.9638159354727382, 0.9648186216123876, 0.9652328660345283, 0.9654174931561585, 0.9663018401572185, 0.9665002252629437, 0.9669299960759811, 0.9676491779410205, 0.9687103541564699, 0.9692284289694589, 0.9697130811987128, 0.9702608753478983, 0.9707050619571653, 0.9703864310210002, 0.9703576465498526, 0.9708898843865617, 0.971310733670543, 0.9720662081820878, 0.972411277765088, 0.9717013524041385, 0.9722223804446862, 0.9732028744680932, 0.9733554026070628, 0.9736505357094517, 0.974037063239697, 0.974438524585993, 0.9748300667321832, 0.9751802016125825, 0.9755069127001706, 0.9750615553575714, 0.9752210585635086, 0.9757947639786141, 0.9763297000426852, 0.976911051801521, 0.9769810085618913, 0.9773741047283581, 0.9773790468591121, 0.977862439382734, 0.9785230320885174, 0.9787805991904354, 0.9787076708928388, 0.9791279915261925, 0.978969278867667, 0.9787380097225946, 0.9792018354979819, 0.9792100164565355, 0.9794545805466239, 0.9793328913527035, 0.9794534887126805, 0.9789086955700023, 0.979211185388002, 0.9797089656587257, 0.9798546985571388, 0.9802208063419288, 0.9805130649184687, 0.9810505372945159, 0.9813435641817586, 0.9817187873766781, 0.9812148204318767, 0.9813379231351361, 0.9819741631502031, 0.9820817854506775, 0.9821855849187142, 0.9819487972601761, 0.9810639727032707, 0.9814997431781909, 0.9817454522306192, 0.9812807435766325, 0.9816785969713687, 0.9822064008897265, 0.9822397182400765, 0.9823556262148876, 0.9818205995672361, 0.9822481727200548, 0.98259807527663, 0.9824433435668426, 0.9828551092470723, 0.9829652816926124, 0.9826598970495601, 0.9828384909374889, 0.9831317228699489, 0.9831514909841629, 0.983064650590527, 0.9833096538719597, 0.983397659391925, 0.9835093803944084, 0.9838122893407095, 0.9836850538947799, 0.9833356659148627, 0.983035203674613, 0.9829668953607231, 0.9832798389341931, 0.9837521864015157, 0.9831960863244871, 0.9827839158611045, 0.9829919244976315, 0.9829491588312755, 0.983117427731509, 0.9830386439607575, 0.9826398925849382, 0.9829994512800251, 0.9834858145222699, 0.984121143040281, 0.9840861469648428, 0.9840686013898056, 0.9839737553127482, 0.9838163142303014, 0.9838815915489656, 0.9839518587333548, 0.9834478710362374, 0.9837801824064509, 0.983753705757491, 0.9842553964531982, 0.9847045208328876, 0.984646136307969, 0.9850864136367052, 0.9854570865956629, 0.9855489128313532, 0.9858407837875127, 0.9855338782873605, 0.9855691611800622, 0.9861845281346843, 0.9859896604771775, 0.9863607256044782, 0.9868365182964297, 0.9870856231632152, 0.9865983580564267, 0.9868294623174599, 0.987002578920247, 0.9866049984460887, 0.9865029423883938, 0.98669014584243, 0.9863912379915296, 0.9865895758364335, 0.9868703503956473, 0.987237051888244, 0.9873974194661231, 0.9871417886421576, 0.9867628753279604, 0.9867635781225453, 0.9868409765972048, 0.987001279979151, 0.9873316009764832, 0.9875846759978825, 0.9879124970135889, 0.9870869583968092, 0.9868693851750223, 0.9871269732932713, 0.9872935542377537, 0.987610959899711, 0.9876105595942638, 0.9877475272943981, 0.9874521993411671], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 771944835, "moving_var_accuracy_train": [0.03239472093764358, 0.06522693127496866, 0.09133510007157916, 0.11404322410524906, 0.12995561252849364, 0.14097590538949764, 0.1492231894270199, 0.1532583843936788, 0.15381974009246682, 0.15111865685957054, 0.14736160256886627, 0.1426387543346769, 0.13659788766987696, 0.12949207027821324, 0.12191793581411922, 0.1147006664864068, 0.10692036562504434, 0.09991304802022653, 0.0931926198176351, 0.08659650761473757, 0.08003264852234865, 0.07378317045055824, 0.06803904318564448, 0.06250456717350021, 0.05728234044868479, 0.052571741488209166, 0.04812054238333934, 0.044001844211088534, 0.03987004045575577, 0.03629778754494663, 0.03302789385160007, 0.030086327938742545, 0.02738038333086533, 0.024875881499309648, 0.02262672822070196, 0.02055488006157597, 0.018600475332228788, 0.016837322145626037, 0.01521952723167073, 0.013790729714322764, 0.012439930169465508, 0.011253122519981283, 0.010204890053483705, 0.009248727180192921, 0.008346474922503911, 0.007575024849107593, 0.006853984936579802, 0.006181296826942395, 0.00558952487083421, 0.005083546516863424, 0.004577381942529198, 0.00412633713381925, 0.003731615837221627, 0.0033964807192816845, 0.0030725128765313648, 0.002777274548553967, 0.0025118679494626588, 0.0022737825464841477, 0.0020470822984019477, 0.0018485804377841527, 0.0016637450033228415, 0.0015235330234785827, 0.0013770382883263933, 0.0012446426957342881, 0.0011292268416126644, 0.0010178485434228703, 0.0009163704736469565, 0.0008317720528468159, 0.0007489490574136967, 0.0006757164782379772, 0.0006127998334091935, 0.0005616547047103948, 0.0005079048478460381, 0.00045922835311132046, 0.0004160062236691259, 0.0003761813169968824, 0.00033947691635852683, 0.00030553668163468736, 0.0002775325075046416, 0.0002513732838326259, 0.00023137263108770828, 0.00020930702513294455, 0.0001929122687827244, 0.00017606427387578536, 0.00016711016325763803, 0.00015060853043047192, 0.0001363316093205563, 0.00012404308017323855, 0.00011308931306904348, 0.00010316012903232888, 9.39474660393458e-05, 8.551338064818877e-05, 7.874713104683304e-05, 7.110138939648795e-05, 6.69534915867304e-05, 6.283355176185174e-05, 5.959192539318156e-05, 5.367677838875689e-05, 4.969982191469951e-05, 4.473005954513708e-05, 4.2360068577664686e-05, 4.205150622630688e-05, 3.8443422911589765e-05, 3.464694744974371e-05, 3.277227761817444e-05, 2.9721757228143052e-05, 2.7230950262491208e-05, 2.644406438546452e-05, 2.380026029966377e-05, 2.1958538617144418e-05, 1.989595909468298e-05, 1.8037256694315425e-05, 1.890472713966615e-05, 1.7837755235641372e-05, 1.8284046493372272e-05, 1.6646784543154105e-05, 1.6188420279593305e-05, 1.5338313931683974e-05, 1.640437153363987e-05, 1.553671719010054e-05, 1.5250177485139964e-05, 1.6011003869698456e-05, 1.4546291962676457e-05, 1.6734874977361475e-05, 1.5165630515659809e-05, 1.3746036430176191e-05, 1.2876048344282111e-05, 1.8634673978379147e-05, 1.848026974185125e-05, 1.7175599213672744e-05, 1.74016264901159e-05, 1.7086049754432066e-05, 1.788463756509319e-05, 1.6106164221092963e-05, 1.4616459726606969e-05, 1.57310953772216e-05, 1.5803355048601025e-05, 1.532490573562152e-05, 1.4007892280182968e-05, 1.4133061830900199e-05, 1.2828997357616735e-05, 1.238543564376474e-05, 1.1433954070638045e-05, 1.106442335950254e-05, 9.961498028608505e-06, 9.033219511449233e-06, 8.670137031519517e-06, 7.872828072266912e-06, 7.1978795066032655e-06, 7.303876023686337e-06, 6.719188149625748e-06, 7.145916979259151e-06, 7.243823301675727e-06, 6.561435203226444e-06, 6.786694804489117e-06, 8.116034493015452e-06, 1.0087656704755256e-05, 1.0607851452245232e-05, 9.936474642849322e-06, 8.959287298534232e-06, 8.318188373752842e-06, 7.542231479181963e-06, 8.219032268723057e-06, 8.560671138763683e-06, 9.833546855549297e-06, 1.248297310217804e-05, 1.1245698319624927e-05, 1.0123899112492925e-05, 9.192471206242119e-06, 8.4963133355962e-06, 7.685032157024483e-06, 6.960966236139842e-06, 8.550902002136361e-06, 8.689689422881083e-06, 7.827029597054321e-06, 9.309568624781076e-06, 1.0194026138185552e-05, 9.205302299116633e-06, 1.0029369204996211e-05, 1.0263018267019084e-05, 9.312604958366567e-06, 9.148042357975722e-06, 9.080956996390927e-06, 8.184065239408465e-06, 1.077374711503501e-05, 1.0038133039011256e-05, 1.0273523693398352e-05, 1.1283579495489442e-05, 1.0713700657846953e-05, 1.1779176150704544e-05, 1.1081941150843488e-05, 1.0243471259203896e-05, 1.0641756234171208e-05, 9.671319560964058e-06, 9.019593803695479e-06, 8.921747553295184e-06, 8.383613904455724e-06, 8.254761691925435e-06, 8.639515384785958e-06, 8.00702368662092e-06, 7.79444538141074e-06, 8.30717854035355e-06, 7.476465131600253e-06, 6.78273333335673e-06, 6.3357345683915065e-06, 6.684168763059349e-06, 6.59217458485971e-06, 6.90015669142271e-06, 1.2343767092431263e-05, 1.1535433344736568e-05, 1.0979054758230349e-05, 1.013089218198914e-05, 1.0024520151973664e-05, 9.022069578976357e-06, 8.28870397899957e-06, 8.244800980735419e-06], "duration": 207507.165183, "accuracy_train": [0.5999511178017718, 0.6930799966546696, 0.7254374163667405, 0.7783244581141565, 0.7939233227205611, 0.8146586032668882, 0.848020523313492, 0.8585287539797897, 0.8656179344315246, 0.8628495653954411, 0.8802177060262089, 0.8940959605135659, 0.8961657034422297, 0.8939786216085271, 0.8954917707756552, 0.9106481361318751, 0.9015353552394795, 0.9216446480481728, 0.9301775836909376, 0.9325488749884644, 0.928596842988649, 0.9308515163575121, 0.939967181155408, 0.9374552994647471, 0.9374549389765596, 0.9475915063215209, 0.946522658845515, 0.9491257440476191, 0.9247385379175894, 0.9434763534168512, 0.9456154903216132, 0.9520565130121816, 0.9530319940476191, 0.9517778556432264, 0.9574032738095238, 0.9571256979051311, 0.9491973009528424, 0.951846889131137, 0.9493836733457919, 0.9571954523694168, 0.9459646231312293, 0.9552197968576966, 0.9617985260358989, 0.962194522309893, 0.9539871075004615, 0.9662177507267442, 0.9624967916551311, 0.9562653928456073, 0.9626831640480805, 0.9715423415005537, 0.9546403120962532, 0.9588244984888336, 0.9651707127860835, 0.9730289947858989, 0.9677287369647471, 0.9674024951550388, 0.9687049389765596, 0.970239897679033, 0.9621258493101699, 0.9679598098929494, 0.9609872473698781, 0.977585925964378, 0.9703092916551311, 0.9707278184408453, 0.9738427968692323, 0.9689610658337948, 0.9670791372508305, 0.9742609631667589, 0.9682856912144703, 0.9707979333933187, 0.974121814726375, 0.978260940095515, 0.9738911022863603, 0.9740749512619971, 0.9751910226905685, 0.9747027414405685, 0.967518752595515, 0.9700985863095238, 0.9756800249169435, 0.975098377226375, 0.9788654787859912, 0.9755169040120893, 0.9653120241555924, 0.9769116328096161, 0.982027320678756, 0.9747281558577889, 0.9763067336309523, 0.9775158110119048, 0.9780516767026578, 0.9783539460478959, 0.9783314155361758, 0.9784473124884644, 0.9710533392741787, 0.9766565874169435, 0.9809581127145626, 0.9811441246193245, 0.9821432176310447, 0.9776106194052234, 0.9809119702265596, 0.9774235260358989, 0.9822129720953304, 0.9844683664405685, 0.9810987031076966, 0.9780513162144703, 0.982910877226375, 0.9775408649409376, 0.9766565874169435, 0.9833762674764673, 0.979283645083518, 0.9816556573574198, 0.9782376886074198, 0.9805388649524732, 0.9740055572858989, 0.98193359375, 0.9841889880952381, 0.9811662946428571, 0.9835157764050388, 0.9831433921073275, 0.9858877886789406, 0.9839808061669435, 0.9850957961309523, 0.9766791179286637, 0.9824458474644703, 0.9877003232858066, 0.9830503861549464, 0.9831197801310447, 0.9798177083333334, 0.9731005516911223, 0.9854216774524732, 0.9839568337024732, 0.977098365690753, 0.9852592775239941, 0.9869566361549464, 0.9825395743932264, 0.9833987979881875, 0.9770053597383721, 0.9860963310954227, 0.9857471982858066, 0.981050758178756, 0.98656100036914, 0.9839568337024732, 0.9799114352620893, 0.9844458359288483, 0.9857708102620893, 0.9833294040120893, 0.9822830870478036, 0.9855146834048542, 0.9841897090716132, 0.9845148694167589, 0.9865384698574198, 0.9825399348814139, 0.9801911740956073, 0.9803310435123662, 0.9823521205357143, 0.9860963310954227, 0.9880033136074198, 0.9781911856312293, 0.9790743816906607, 0.984864002226375, 0.9825642678340717, 0.9846318478336102, 0.9823295900239941, 0.9790511302025655, 0.9862354795358066, 0.9878630837024732, 0.9898390997023809, 0.9837711822858989, 0.9839106912144703, 0.9831201406192323, 0.9823993444882798, 0.9844690874169435, 0.9845842633928571, 0.9789119817621816, 0.9867709847383721, 0.9835154159168512, 0.9887706127145626, 0.9887466402500923, 0.9841206755837025, 0.9890489095953304, 0.9887931432262828, 0.9863753489525655, 0.9884676223929494, 0.9827717287859912, 0.985886707214378, 0.9917228307262828, 0.9842358515596161, 0.9897003117501846, 0.9911186525239941, 0.9893275669642857, 0.9822129720953304, 0.9889094006667589, 0.9885606283453304, 0.9830267741786637, 0.98558443786914, 0.988374976928756, 0.9837010673334257, 0.9883746164405685, 0.9893973214285714, 0.9905373653216132, 0.9888407276670359, 0.9848411112264673, 0.9833526555001846, 0.9867699032738095, 0.98753756286914, 0.9884440104166666, 0.9903044899524732, 0.9898623511904762, 0.9908628861549464, 0.9796571108457919, 0.9849112261789406, 0.9894452663575121, 0.9887927827380952, 0.9904676108573275, 0.9876069568452381, 0.9889802365956073, 0.9847942477620893], "end": "2016-01-26 06:42:27.078000", "learning_rate_per_epoch": [0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414, 0.00012773256457876414], "accuracy_valid": [0.5912615304969879, 0.6718411732868976, 0.7028382083019578, 0.7465408509036144, 0.7612716490963856, 0.7771716749811747, 0.8027976162462349, 0.8098776943712349, 0.8144560664533133, 0.8089320171310241, 0.8209360881024097, 0.8281073512801205, 0.8270087184676205, 0.8235510400978916, 0.8245261318712349, 0.8315562052899097, 0.8238348903426205, 0.8376906061746988, 0.8391142695783133, 0.8413424204631024, 0.8312811794051205, 0.8366831584149097, 0.8440382624246988, 0.8433058405496988, 0.8444147684487951, 0.8515257318335843, 0.8489416650978916, 0.8476591914533133, 0.8304163921310241, 0.8417086314006024, 0.8466929240399097, 0.850487398814006, 0.8495623117469879, 0.8468355845256024, 0.8562658838478916, 0.8547804499246988, 0.8481077630835843, 0.8479842220444277, 0.8446177287274097, 0.8520331325301205, 0.8352580242846386, 0.8534376764871988, 0.8572218561746988, 0.855858492564006, 0.8526640742658133, 0.859398531626506, 0.856346773814006, 0.8472223856362951, 0.8614943171121988, 0.8662668251129518, 0.8466517436935241, 0.8551775461219879, 0.8580763483621988, 0.8676698983433735, 0.8624708796121988, 0.8641901590737951, 0.8610986916415663, 0.8667948159826807, 0.8591749811746988, 0.8591440959149097, 0.8573851068335843, 0.8697039133094879, 0.8633665521460843, 0.8660006235881024, 0.8688597162085843, 0.8699892342808735, 0.859032320689006, 0.8677610833960843, 0.8676287179969879, 0.8625017648719879, 0.8682802499058735, 0.8771928534450302, 0.8687685311558735, 0.8686052804969879, 0.8714437829442772, 0.8677199030496988, 0.8623796945594879, 0.8660520990210843, 0.8753603280308735, 0.8675684182040663, 0.8726541909826807, 0.8733866128576807, 0.8621458490210843, 0.8726953713290663, 0.8746073159826807, 0.8708128412085843, 0.8715658532567772, 0.8710981621799698, 0.870995211314006, 0.8708128412085843, 0.8767545769013554, 0.8700701242469879, 0.8601103633283133, 0.8737528237951807, 0.8759397943335843, 0.8752588478915663, 0.8786768166415663, 0.868309664439006, 0.873558687876506, 0.8718408791415663, 0.8751161874058735, 0.8775061182228916, 0.8794092385165663, 0.8731836525790663, 0.8750647119728916, 0.8734072030308735, 0.8722879800451807, 0.8808534920933735, 0.8723188653049698, 0.8788591867469879, 0.8716982186558735, 0.8769060617469879, 0.8704569253576807, 0.8795313088290663, 0.8799784097326807, 0.8797754494540663, 0.8767031014683735, 0.8728571512612951, 0.8856039391942772, 0.8808123117469879, 0.8749220514871988, 0.8685846903237951, 0.8751470726656627, 0.8830713478915663, 0.8763883659638554, 0.8727762612951807, 0.8670992564006024, 0.8690229668674698, 0.8812197030308735, 0.8853906838290663, 0.8672728021460843, 0.8797754494540663, 0.8851053628576807, 0.875401508377259, 0.8725218255835843, 0.8702024896460843, 0.8817182793674698, 0.8821756753576807, 0.8759809746799698, 0.8803755059299698, 0.8781679452183735, 0.8797548592808735, 0.8863569512424698, 0.8804358057228916, 0.8779032144201807, 0.8755529932228916, 0.8787371164344879, 0.8837228798004518, 0.8842920510165663, 0.8828066170933735, 0.883824359939759, 0.8756853586219879, 0.8777708490210843, 0.8780973503388554, 0.8847803322665663, 0.884434711502259, 0.8695612528237951, 0.8740175545933735, 0.8809755624058735, 0.8816770990210843, 0.8820844903049698, 0.8784429711031627, 0.8738234186746988, 0.8790224374058735, 0.8843626458960843, 0.896204936935241, 0.8768957666603916, 0.8737940041415663, 0.8743322900978916, 0.8771502023719879, 0.8819521249058735, 0.8789724326995482, 0.875401508377259, 0.8861642860504518, 0.8777502588478916, 0.8883718467620482, 0.8819315347326807, 0.885777484939759, 0.8918398202183735, 0.8812094079442772, 0.8874555840549698, 0.8885748070406627, 0.8816476844879518, 0.8817079842808735, 0.8858789650790663, 0.8793989434299698, 0.8846582619540663, 0.8894704795745482, 0.8851156579442772, 0.8784944465361446, 0.8816065041415663, 0.8879335702183735, 0.8766207407756024, 0.8775575936558735, 0.8872217385165663, 0.8824712914156627, 0.8879850456513554, 0.887660015060241, 0.888218891189759, 0.885167133377259, 0.8759089090737951, 0.8774561135165663, 0.8819006494728916, 0.883702289627259, 0.8834169686558735, 0.8887174675263554, 0.8902543768825302, 0.8908441382718373, 0.8734572077371988, 0.8813726586031627, 0.888585102127259, 0.8854009789156627, 0.8909250282379518, 0.8813314782567772, 0.890782367752259, 0.8821256706513554], "accuracy_test": 0.7824398118622449, "start": "2016-01-23 21:03:59.913000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0, 211.0, 212.0, 213.0, 214.0, 215.0], "accuracy_train_last": 0.9847942477620893, "batch_size_eval": 1024, "accuracy_train_std": [0.01911090033973466, 0.019258879267324336, 0.02043407193269722, 0.020212907048235012, 0.02029927683980949, 0.021730354144291055, 0.019623418746266396, 0.018957606002543974, 0.020088991337376644, 0.02083292191741756, 0.020391481729728808, 0.019436222115459294, 0.018630209101295477, 0.02045952728747693, 0.018256530072772906, 0.01819427306648498, 0.01773427043544406, 0.018284920034573084, 0.017775967356464743, 0.01744870890375049, 0.017475638654308373, 0.018776254754399705, 0.01444299809812198, 0.01618624557730933, 0.017161679484960144, 0.015731402467842874, 0.014352603272173166, 0.0144373580554621, 0.015545236900397531, 0.013531434791436894, 0.014736669065831752, 0.013497733384038011, 0.01463728730487401, 0.013088991970426287, 0.012527784405714466, 0.01219360879035214, 0.013947693275173992, 0.013077490511722962, 0.013384228113412106, 0.013185127856043265, 0.013739737600904646, 0.012603269880137671, 0.011246716485309935, 0.010249348728005155, 0.0108349677161452, 0.01018706876418839, 0.011378498730273419, 0.01226931446380374, 0.011170615205021439, 0.008444302453365683, 0.01056611904207375, 0.011316913945747588, 0.010709128620012373, 0.009469767888090088, 0.009713733858518219, 0.01020502131897131, 0.0097060253098266, 0.00980699090978402, 0.009766162991178832, 0.010042385645252849, 0.010718963767258838, 0.0076725435263064546, 0.008461173735430235, 0.00884020349072192, 0.009700870498255649, 0.008633259270609518, 0.00912492746950646, 0.00809007221991737, 0.00878679272522379, 0.008836356566004474, 0.008624629947242014, 0.0076652586476781935, 0.007658237096657287, 0.007978106312940663, 0.00861007001014137, 0.007695896295565451, 0.010016605067772098, 0.00948957010509786, 0.00881663651340442, 0.008742220621258182, 0.007187463540670918, 0.008682615103383749, 0.009341838937514998, 0.007151380475628076, 0.006380682785039682, 0.007441849099831382, 0.008379218225530805, 0.008166305279789008, 0.008075481851542105, 0.00812779210379955, 0.006942024743696291, 0.006436525131011506, 0.0088281556035325, 0.007439242385473027, 0.006838856476097683, 0.006725795423051853, 0.005795230818086075, 0.006319506051126777, 0.0065333501031374225, 0.007089639060071684, 0.006957058404590635, 0.005630227617020671, 0.0063091208010690415, 0.007083281197866108, 0.006033759019758348, 0.007713079628685801, 0.007124297013637992, 0.0068326213206865525, 0.006307536762113333, 0.006350808564300521, 0.0068460419225757916, 0.007156185502258413, 0.007808587585457449, 0.006482162929935695, 0.005512907723780149, 0.006706109693400431, 0.005965789965111182, 0.006002829168348605, 0.005000463218053343, 0.004963729282877201, 0.005581761743490793, 0.006913692495369037, 0.0061686790603927965, 0.005320010001790625, 0.0062211454089313376, 0.005652355783185178, 0.006399019054137965, 0.007500145362908644, 0.004977656885224041, 0.005849161303791695, 0.005863307737091825, 0.004794372196755738, 0.004440085288413325, 0.005024077392131696, 0.005707165009990551, 0.0061947500596467265, 0.005128347818294311, 0.005142176485303505, 0.005595892867476986, 0.004722017609437059, 0.0060439022290868305, 0.005995963362959822, 0.0050016042182810345, 0.005273915953754407, 0.005275325880492411, 0.005544827984022728, 0.004934233291992188, 0.005787696195362682, 0.004877165565220862, 0.005087322901966437, 0.005544032680320153, 0.005890486199563219, 0.005501043268874576, 0.005040212583565884, 0.004906598747991263, 0.0036761365098966093, 0.006847312670140266, 0.006067588634102511, 0.005534977202611288, 0.004730748223180795, 0.004637208094465385, 0.005910487688710054, 0.006632769589177682, 0.004817255053490975, 0.004517111974177917, 0.003635643601911865, 0.005786627306548942, 0.0049869176522216434, 0.005073016688763553, 0.0058513221702105895, 0.005521275586019933, 0.005174226739185281, 0.005494539612135064, 0.003874466637634828, 0.0059843833308516605, 0.0041240155137360094, 0.004106795554680665, 0.005261426774209653, 0.004782634406465511, 0.003891020408367093, 0.0049989056579742485, 0.0037179692969772217, 0.004981170862271803, 0.004842633980229523, 0.003445023522338069, 0.005109110618575992, 0.0038264132436706, 0.0033353780082743642, 0.004358956417401784, 0.004701754815989907, 0.004459753969900128, 0.0038265946079136313, 0.005307049186158004, 0.005327617176773359, 0.004176256508166834, 0.005373063305191462, 0.004398214825915397, 0.004671640012974093, 0.004102808000544807, 0.004517376883583492, 0.005205493490087866, 0.005837467678340128, 0.00471340411935331, 0.00479826585636088, 0.004314576604879601, 0.003989736889920148, 0.004760142408954313, 0.0037701119552934963, 0.005729044842397821, 0.004458225277568426, 0.003760621342118808, 0.004122535394455857, 0.004121161282441816, 0.005031624172745118, 0.003938041186689456, 0.004592871037457725], "accuracy_test_std": 0.010333863178812906, "error_valid": [0.40873846950301207, 0.32815882671310237, 0.29716179169804224, 0.25345914909638556, 0.23872835090361444, 0.22282832501882532, 0.1972023837537651, 0.1901223056287651, 0.18554393354668675, 0.19106798286897586, 0.1790639118975903, 0.17189264871987953, 0.17299128153237953, 0.1764489599021084, 0.1754738681287651, 0.1684437947100903, 0.17616510965737953, 0.16230939382530118, 0.16088573042168675, 0.15865757953689763, 0.16871882059487953, 0.1633168415850903, 0.15596173757530118, 0.15669415945030118, 0.15558523155120485, 0.14847426816641573, 0.1510583349021084, 0.15234080854668675, 0.16958360786897586, 0.15829136859939763, 0.1533070759600903, 0.14951260118599397, 0.15043768825301207, 0.15316441547439763, 0.1437341161521084, 0.14521955007530118, 0.15189223691641573, 0.1520157779555723, 0.1553822712725903, 0.14796686746987953, 0.16474197571536142, 0.14656232351280118, 0.14277814382530118, 0.14414150743599397, 0.14733592573418675, 0.14060146837349397, 0.14365322618599397, 0.15277761436370485, 0.13850568288780118, 0.13373317488704817, 0.15334825630647586, 0.14482245387801207, 0.14192365163780118, 0.1323301016566265, 0.13752912038780118, 0.13580984092620485, 0.13890130835843373, 0.1332051840173193, 0.14082501882530118, 0.1408559040850903, 0.14261489316641573, 0.13029608669051207, 0.13663344785391573, 0.13399937641189763, 0.13114028379141573, 0.1300107657191265, 0.14096767931099397, 0.13223891660391573, 0.13237128200301207, 0.13749823512801207, 0.1317197500941265, 0.12280714655496983, 0.1312314688441265, 0.13139471950301207, 0.12855621705572284, 0.13228009695030118, 0.13762030544051207, 0.13394790097891573, 0.12463967196912651, 0.13243158179593373, 0.1273458090173193, 0.1266133871423193, 0.13785415097891573, 0.12730462867093373, 0.1253926840173193, 0.12918715879141573, 0.12843414674322284, 0.12890183782003017, 0.12900478868599397, 0.12918715879141573, 0.12324542309864461, 0.12992987575301207, 0.13988963667168675, 0.1262471762048193, 0.12406020566641573, 0.12474115210843373, 0.12132318335843373, 0.13169033556099397, 0.12644131212349397, 0.12815912085843373, 0.12488381259412651, 0.1224938817771084, 0.12059076148343373, 0.12681634742093373, 0.1249352880271084, 0.1265927969691265, 0.1277120199548193, 0.11914650790662651, 0.12768113469503017, 0.12114081325301207, 0.1283017813441265, 0.12309393825301207, 0.1295430746423193, 0.12046869117093373, 0.12002159026731929, 0.12022455054593373, 0.12329689853162651, 0.12714284873870485, 0.11439606080572284, 0.11918768825301207, 0.12507794851280118, 0.13141530967620485, 0.12485292733433728, 0.11692865210843373, 0.12361163403614461, 0.1272237387048193, 0.13290074359939763, 0.13097703313253017, 0.11878029696912651, 0.11460931617093373, 0.13272719785391573, 0.12022455054593373, 0.11489463714231929, 0.12459849162274095, 0.12747817441641573, 0.12979751035391573, 0.11828172063253017, 0.11782432464231929, 0.12401902532003017, 0.11962449407003017, 0.12183205478162651, 0.12024514071912651, 0.11364304875753017, 0.1195641942771084, 0.12209678557981929, 0.1244470067771084, 0.12126288356551207, 0.11627712019954817, 0.11570794898343373, 0.11719338290662651, 0.11617564006024095, 0.12431464137801207, 0.12222915097891573, 0.12190264966114461, 0.11521966773343373, 0.11556528849774095, 0.13043874717620485, 0.1259824454066265, 0.11902443759412651, 0.11832290097891573, 0.11791550969503017, 0.12155702889683728, 0.12617658132530118, 0.12097756259412651, 0.11563735410391573, 0.10379506306475905, 0.1231042333396084, 0.12620599585843373, 0.1256677099021084, 0.12284979762801207, 0.11804787509412651, 0.12102756730045183, 0.12459849162274095, 0.11383571394954817, 0.1222497411521084, 0.11162815323795183, 0.11806846526731929, 0.11422251506024095, 0.10816017978162651, 0.11879059205572284, 0.11254441594503017, 0.11142519295933728, 0.11835231551204817, 0.11829201571912651, 0.11412103492093373, 0.12060105657003017, 0.11534173804593373, 0.11052952042545183, 0.11488434205572284, 0.12150555346385539, 0.11839349585843373, 0.11206642978162651, 0.12337925922439763, 0.12244240634412651, 0.11277826148343373, 0.11752870858433728, 0.11201495434864461, 0.11233998493975905, 0.11178110881024095, 0.11483286662274095, 0.12409109092620485, 0.12254388648343373, 0.1180993505271084, 0.11629771037274095, 0.11658303134412651, 0.11128253247364461, 0.10974562311746983, 0.10915586172816272, 0.12654279226280118, 0.11862734139683728, 0.11141489787274095, 0.11459902108433728, 0.10907497176204817, 0.11866852174322284, 0.10921763224774095, 0.11787432934864461], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-3, 3], "momentum": 0.7964848349077005, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.0001277325601433117, "patience_threshold": 1, "do_flip": true, "batch_size": 16, "optimization": "adam", "nb_data_augmentation": 1, "learning_rate_decay_method": "none", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 4.460510892300767e-06, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.009256794329071227}, "accuracy_valid_max": 0.896204936935241, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-3, 3)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8821256706513554, "loss_train": [1.7120158672332764, 1.283082365989685, 1.0961841344833374, 0.980995237827301, 0.8968320488929749, 0.8330473899841309, 0.7837325930595398, 0.7438263893127441, 0.7058515548706055, 0.6700567007064819, 0.6418703198432922, 0.6153286099433899, 0.5895506143569946, 0.5673845410346985, 0.5479949712753296, 0.5300387740135193, 0.5144855976104736, 0.49634721875190735, 0.48349428176879883, 0.469452828168869, 0.460464745759964, 0.4481627643108368, 0.4417271018028259, 0.43027013540267944, 0.42286333441734314, 0.4138624668121338, 0.4070308804512024, 0.4050922989845276, 0.3947487473487854, 0.39030012488365173, 0.3854804039001465, 0.37970197200775146, 0.3765280544757843, 0.37003764510154724, 0.36456194519996643, 0.36172112822532654, 0.35838180780410767, 0.35720890760421753, 0.34839823842048645, 0.34512990713119507, 0.34452196955680847, 0.3395193815231323, 0.3385689854621887, 0.3343735337257385, 0.3308050334453583, 0.32763585448265076, 0.3257942795753479, 0.3233645558357239, 0.319758802652359, 0.318527489900589, 0.3148496448993683, 0.31554725766181946, 0.31452682614326477, 0.3100450336933136, 0.3077712059020996, 0.30758124589920044, 0.30545729398727417, 0.30580464005470276, 0.30090978741645813, 0.3016303777694702, 0.2973502278327942, 0.2976704239845276, 0.294903963804245, 0.29253166913986206, 0.2930814027786255, 0.29361075162887573, 0.28839346766471863, 0.2875911295413971, 0.28866907954216003, 0.2861505448818207, 0.2859765887260437, 0.2835962474346161, 0.28120896220207214, 0.2828463912010193, 0.27932292222976685, 0.2797355651855469, 0.2767326533794403, 0.2778344750404358, 0.2764207124710083, 0.27783194184303284, 0.2748459279537201, 0.2724406123161316, 0.27236151695251465, 0.27436089515686035, 0.2721620798110962, 0.2710030674934387, 0.2701261639595032, 0.2691793739795685, 0.26520442962646484, 0.2692945897579193, 0.26536649465560913, 0.265109121799469, 0.2654132843017578, 0.26487237215042114, 0.26548904180526733, 0.2636488676071167, 0.26101353764533997, 0.26238998770713806, 0.26196613907814026, 0.2607297897338867, 0.2572549879550934, 0.25995272397994995, 0.2580920457839966, 0.25863057374954224, 0.2576701045036316, 0.2588460445404053, 0.2555156648159027, 0.2544843852519989, 0.25691118836402893, 0.2545822560787201, 0.2541164457798004, 0.25244414806365967, 0.2554231882095337, 0.2533585727214813, 0.2500605583190918, 0.25152140855789185, 0.25076824426651, 0.25211143493652344, 0.2502477765083313, 0.2496921867132187, 0.24851559102535248, 0.2505432665348053, 0.248805433511734, 0.24794860184192657, 0.24773931503295898, 0.2470749020576477, 0.24686972796916962, 0.2465258687734604, 0.24630239605903625, 0.24595807492733002, 0.24499571323394775, 0.24685755372047424, 0.24477039277553558, 0.2432149201631546, 0.2415248602628708, 0.24492275714874268, 0.24182163178920746, 0.2454574853181839, 0.24020718038082123, 0.2431158423423767, 0.24160723388195038, 0.24082720279693604, 0.24108964204788208, 0.23972268402576447, 0.2393181324005127, 0.23790940642356873, 0.23953881859779358, 0.23857447504997253, 0.23774243891239166, 0.2373218536376953, 0.2377927005290985, 0.23749962449073792, 0.23600119352340698, 0.23744142055511475, 0.23470450937747955, 0.2372453808784485, 0.23347783088684082, 0.2355017513036728, 0.23449404537677765, 0.23234723508358002, 0.23545841872692108, 0.23335431516170502, 0.23354242742061615, 0.23459245264530182, 0.23349808156490326, 0.2325323224067688, 0.23219609260559082, 0.2310745120048523, 0.2348494976758957, 0.23097099363803864, 0.23184919357299805, 0.23251403868198395, 0.22893838584423065, 0.22975754737854004, 0.23131434619426727, 0.2284015417098999, 0.23341004550457, 0.22742806375026703, 0.2277032881975174, 0.231559619307518, 0.22815148532390594, 0.2295360565185547, 0.22717387974262238, 0.22900179028511047, 0.22779254615306854, 0.22704195976257324, 0.22627925872802734, 0.22865749895572662, 0.2256072759628296, 0.22736425697803497, 0.22500422596931458, 0.2259082794189453, 0.22258260846138, 0.225545272231102, 0.22377663850784302, 0.22618785500526428, 0.22488300502300262, 0.22446317970752716, 0.22445990145206451, 0.22539092600345612, 0.22202880680561066, 0.22522766888141632, 0.2209545075893402, 0.22285017371177673, 0.22433464229106903, 0.22315794229507446, 0.22016435861587524, 0.22121579945087433, 0.2208220213651657, 0.222509503364563, 0.2192593663930893, 0.2210664004087448, 0.2212418019771576, 0.21850243210792542, 0.2198961079120636, 0.21978767216205597], "accuracy_train_first": 0.5999511178017718, "model": "residualv3", "loss_std": [0.37758949398994446, 0.2650488615036011, 0.2532269060611725, 0.24797041714191437, 0.23900258541107178, 0.2357311099767685, 0.22732898592948914, 0.22296708822250366, 0.21469615399837494, 0.20421376824378967, 0.19999471306800842, 0.19244727492332458, 0.18538860976696014, 0.17990414798259735, 0.17112326622009277, 0.16973114013671875, 0.1604437381029129, 0.1559925526380539, 0.1516738086938858, 0.14748631417751312, 0.14520739018917084, 0.13832280039787292, 0.13503240048885345, 0.1327926069498062, 0.1303802877664566, 0.12751701474189758, 0.1256500482559204, 0.12417648732662201, 0.12164630740880966, 0.12176135927438736, 0.11466630548238754, 0.11600902676582336, 0.11334125697612762, 0.10831265896558762, 0.10846033692359924, 0.10915675014257431, 0.10206040740013123, 0.10570129752159119, 0.10269037634134293, 0.09936713427305222, 0.10258262604475021, 0.09830755740404129, 0.09797781705856323, 0.09778492897748947, 0.09442629665136337, 0.09277278184890747, 0.09347357600927353, 0.09225655347108841, 0.08902090787887573, 0.09006333351135254, 0.08963212370872498, 0.09002041816711426, 0.08828616142272949, 0.08554140478372574, 0.08331737667322159, 0.08606074750423431, 0.08483312278985977, 0.08311058580875397, 0.08267232030630112, 0.08395442366600037, 0.07787884771823883, 0.07934050261974335, 0.07897903025150299, 0.07639575749635696, 0.0794275775551796, 0.08050607144832611, 0.07811059802770615, 0.07545635849237442, 0.07579106092453003, 0.07231327146291733, 0.07555699348449707, 0.0772462710738182, 0.07348506152629852, 0.075656458735466, 0.07110220938920975, 0.07216185331344604, 0.07358039915561676, 0.0712154433131218, 0.06968889385461807, 0.0737539604306221, 0.0716673955321312, 0.07051647454500198, 0.06877072155475616, 0.06969445943832397, 0.0716138705611229, 0.07006519287824631, 0.06943174451589584, 0.06955110281705856, 0.06365244090557098, 0.067902572453022, 0.06509409844875336, 0.06820443272590637, 0.06659199297428131, 0.06810986995697021, 0.06788185983896255, 0.0672731101512909, 0.06367042660713196, 0.0651901587843895, 0.0673525407910347, 0.06612931936979294, 0.06288900971412659, 0.06505530327558517, 0.06179653853178024, 0.06598589569330215, 0.06261463463306427, 0.0647713914513588, 0.06203159689903259, 0.061336636543273926, 0.06466417014598846, 0.06140997260808945, 0.06256967037916183, 0.05890541523694992, 0.06275349110364914, 0.06210106238722801, 0.061656590551137924, 0.06044274568557739, 0.06106787174940109, 0.06443251669406891, 0.06362961232662201, 0.0582992322742939, 0.060160402208566666, 0.06336523592472076, 0.06261195242404938, 0.05923764035105705, 0.060616809874773026, 0.060307826846838, 0.058495018631219864, 0.06166794151067734, 0.0596056804060936, 0.06006942316889763, 0.05912291631102562, 0.06198219582438469, 0.05903848260641098, 0.05833861231803894, 0.055637191981077194, 0.06179259344935417, 0.05607294291257858, 0.0621815100312233, 0.055996134877204895, 0.05902918428182602, 0.0575038306415081, 0.05709538981318474, 0.05874203145503998, 0.055735815316438675, 0.05550919845700264, 0.054767537862062454, 0.05787491053342819, 0.059905149042606354, 0.056463345885276794, 0.05633997172117233, 0.05633622035384178, 0.05779542773962021, 0.05581381544470787, 0.055953849107027054, 0.05386233702301979, 0.05938582867383957, 0.054176926612854004, 0.05586038529872894, 0.0544559620320797, 0.05253130570054054, 0.0579402893781662, 0.05368606373667717, 0.05536719411611557, 0.05733717232942581, 0.05748821049928665, 0.05506844446063042, 0.056681204587221146, 0.05364401638507843, 0.06107005104422569, 0.0566907562315464, 0.05553688108921051, 0.05646071583032608, 0.053224269300699234, 0.0537327341735363, 0.05439990758895874, 0.05217995494604111, 0.05752705782651901, 0.04986383765935898, 0.0551254078745842, 0.057964105159044266, 0.055497512221336365, 0.05552778020501137, 0.05265452712774277, 0.056355640292167664, 0.05357671156525612, 0.053752873092889786, 0.05294119566679001, 0.05500325933098793, 0.0514095239341259, 0.057814937084913254, 0.05184098333120346, 0.05537120997905731, 0.05116061121225357, 0.05471997335553169, 0.05312567204236984, 0.05594969913363457, 0.05355144292116165, 0.053998738527297974, 0.0540987029671669, 0.05367567762732506, 0.051353570073843, 0.055242862552404404, 0.0516991913318634, 0.05411696061491966, 0.05421049892902374, 0.05324028804898262, 0.04973207041621208, 0.05016791447997093, 0.051888518035411835, 0.055827800184488297, 0.04964084550738335, 0.053502943366765976, 0.0542079322040081, 0.05204130709171295, 0.05379661172628403, 0.05260401591658592]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:15 2016", "state": "available"}], "summary": "7accf36cf5bbb065519125567a30413e"}