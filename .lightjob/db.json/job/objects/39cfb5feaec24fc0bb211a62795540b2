{"content": {"hp_model": {"f0": 16, "f1": 64, "f2": 64, "f3": 64, "nonlin": "very_leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.026143266309825608, 0.02431294551157003, 0.016024000385736047, 0.018681830223691607, 0.0184394018950944, 0.019466580660588293, 0.01783824274903539, 0.020762768176721652, 0.02283205309311428, 0.02579111472624025, 0.023785595030844067, 0.025633050044601226, 0.023714548627889888, 0.026936287505024876, 0.024979176960824, 0.025684669044295318, 0.027012294310571387, 0.029333313769000426, 0.028761157395306575, 0.02895918953968382, 0.02759309879897982, 0.03102391088493381, 0.029994496412597646, 0.03249887273095329, 0.03644727268380983, 0.03846148821761959, 0.0391112396583034, 0.039713383033558404, 0.03731361058876753, 0.0380455274211258, 0.037965802847139726, 0.036515905119982574, 0.036865517943155936, 0.03537772711234293, 0.03694516135958404, 0.03752936665976625, 0.03430180920863912, 0.03267370388568449, 0.03254740726422331, 0.0341979796739801, 0.031444475132359644, 0.032825502774435474, 0.031411568617951566, 0.032154258763713965, 0.03201628128608961, 0.03188453583084804, 0.03050670626983285, 0.03196410236960038, 0.030094139154775695, 0.03194479728780444, 0.032187533370161524, 0.03319212600660639, 0.03254238980720151, 0.03069584272366138, 0.028824172144601843, 0.029676448283970727, 0.0290580117086445, 0.0299412180272242, 0.03262702555431613, 0.03508987036601596, 0.031184323635962213, 0.030291243451008476, 0.03218076844083709, 0.029204375501869936, 0.03193400407072608, 0.029634232948220537, 0.02916707584575107, 0.031180832527791642, 0.031076500755232283, 0.0294025078656083, 0.030535834731260245, 0.03001928700582328, 0.030377373058875345, 0.02872959819011705, 0.028834870956696308, 0.033476247906262434, 0.028007525876823418, 0.029969685312873667, 0.030002359062394724, 0.02875043115241771, 0.028754848325233676, 0.028048308372457698, 0.0295201343786894, 0.030363632641437283, 0.030784966061746112, 0.02986780506796379, 0.029310419109629504, 0.029422247718698377, 0.028888933636927565, 0.02767189136027953, 0.028575089202165442, 0.030517409751690695, 0.03157460902504923, 0.028636612688351146, 0.02727432210576049, 0.030593415404066297, 0.028685356971680634, 0.027112189958134168, 0.026839117355150436, 0.02594332153556787, 0.02785031401957884, 0.02993455156157063, 0.027850965481293285, 0.029506609669919564, 0.02957785210818528, 0.028919064209077472, 0.027494290565028175, 0.02767844728241664, 0.028341106220115604, 0.02900801724310861, 0.028309719646773156, 0.02563375785639069, 0.027022367636580884, 0.02443725210354725, 0.026466754506067103, 0.028297539976279314, 0.029218661078635192, 0.02509873857511232, 0.027116204893609167, 0.02774653704651667, 0.02871443745123011, 0.027142287497975745, 0.024649416451935077, 0.025265174312720864, 0.028287279344123558, 0.02575028035307076, 0.026920116837197958, 0.026267199913251108, 0.028642947801737137, 0.023570273936230644, 0.027193710466813525, 0.024885301409786963, 0.02733611824446798, 0.02294461675632283, 0.024692806224330045, 0.02844590323667901, 0.026742951048001357, 0.02232743278555331, 0.027687622965507225, 0.02833086136641857, 0.026620550812643216, 0.026114796496503757, 0.02408124911372084, 0.028472042221153772, 0.024499539168313364, 0.025158666894296813, 0.026348580358445686, 0.02947584861646852, 0.021985960772946497, 0.02531324315199218, 0.02614049013202126, 0.025883107014197174], "moving_avg_accuracy_train": [0.018799416415662646, 0.035617705195783125, 0.0502035250376506, 0.06364138036521083, 0.07548366175640059, 0.08662411335184486, 0.09873071557087723, 0.11276342338125939, 0.1278142647780732, 0.14353199116171167, 0.16012994038891398, 0.1776142166813479, 0.19596678221200825, 0.21459252492454237, 0.23330175887787125, 0.25179669594189136, 0.27008229366697933, 0.2881512480352212, 0.305945215852181, 0.32328932378503517, 0.340117959177616, 0.35673681085021586, 0.3724914994941099, 0.388035553610964, 0.4029570547257712, 0.4177394742531941, 0.43183431447847703, 0.445352690259545, 0.45810751912515674, 0.4706999110379423, 0.48213895607872637, 0.49372362974796213, 0.5048416658695515, 0.5154903117825963, 0.5256859153934933, 0.5352290520167946, 0.5443520428994525, 0.5532686834890252, 0.5615666268871107, 0.5696089476321347, 0.5773623789231381, 0.5850275905790171, 0.5922416048644888, 0.5990871921190039, 0.6051187967022842, 0.6113331971525376, 0.6174250280396936, 0.6241454393923508, 0.6292972546398627, 0.6349104508626235, 0.6409624215896141, 0.646159760002942, 0.6517150942436116, 0.6554771315059975, 0.659328891246964, 0.6655463221523881, 0.6703631131299204, 0.6741970013651211, 0.6792100007768017, 0.6831922386509287, 0.6882940426472817, 0.6910666715150836, 0.6952727867430932, 0.6993853800567358, 0.704187994159496, 0.7085668227555946, 0.7128183859619629, 0.7156023417934774, 0.7200798523430453, 0.7230789265966926, 0.7267640685755776, 0.731123147410791, 0.7328437581215191, 0.7363595517370781, 0.7403685513826475, 0.7451861766660695, 0.7466158835476553, 0.7507781845001187, 0.7534465068633599, 0.7532053953336504, 0.7571746713123335, 0.7597539850545942, 0.7641649758563637, 0.7687372772165104, 0.7712851421153413, 0.7745477235363373, 0.7752344233212577, 0.7798410638204573, 0.7829045854504597, 0.7842851547668596, 0.7867183674528242, 0.7903107438099514, 0.7911460097904021, 0.79503216182341, 0.7984896948880569, 0.8007919866944319, 0.8039182661876393, 0.8066519102014055, 0.8018573893619878, 0.8058325126245842, 0.8067392839524873, 0.8117887177560338, 0.8157025605888641, 0.8175401546805802, 0.8143605932787872, 0.8186078923846434, 0.8207832477244923, 0.8249789252110792, 0.8251782277200916, 0.8271177656408536, 0.8269596411852019, 0.8308012335425853, 0.8320090431702545, 0.8354045243953977, 0.8395076148775447, 0.8425297449560553, 0.840957493352016, 0.8431122146493445, 0.8460233199916389, 0.8500881566671739, 0.8486095556088902, 0.8510132837829409, 0.8551674147419962, 0.8580919383280375, 0.8542292806398121, 0.8573864542324573, 0.8571758585080067, 0.8601489729584109, 0.8625306306324493, 0.8653235954306502, 0.8665806748936092, 0.8678438235187061, 0.8686818056246669, 0.8710149616585857, 0.8748632019385102, 0.8738085459012856, 0.8779845437207956, 0.8823171134451016, 0.8842727138475793, 0.8875999605351106, 0.8905356534876236, 0.8945190798557288, 0.8975699956653366, 0.8957012679361523, 0.8973632571063925, 0.8997485428415364, 0.8966406878043707, 0.8901162086323673, 0.8904541736125041, 0.8953470092633019, 0.8990822631562488, 0.9012815293105034], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 1234423, "moving_var_accuracy_train": [0.003180762518125376, 0.005408379803736577, 0.006782257087497651, 0.007729214980987839, 0.008218450139822128, 0.008513592081593848, 0.008981361229043382, 0.009855477102563086, 0.010908679833075178, 0.01204123415380598, 0.013316538005364458, 0.014736183462059509, 0.016293915069868214, 0.01778678818722305, 0.0191584282845843, 0.020321149729144453, 0.02129830251370369, 0.022106856269987766, 0.022745798259028, 0.023178581153003662, 0.02340954976029111, 0.02355427086250487, 0.023432735704648787, 0.023264020699673104, 0.022941479389378527, 0.022614010794203276, 0.022140590403569253, 0.02157124971703581, 0.020878295679851614, 0.02021758111863313, 0.0193734887697756, 0.018643981869002207, 0.017892080226910672, 0.017123415142252265, 0.01634662662494186, 0.015531607071946622, 0.01472750702855749, 0.013970314640334299, 0.013192985958041335, 0.012455797668929772, 0.01175125917309559, 0.011104932483350835, 0.01046281725401467, 0.009838294112345804, 0.009181886985752659, 0.008611267243782392, 0.008084134151423509, 0.007682196095021465, 0.007152847288619784, 0.00672113430627474, 0.006378658022770434, 0.0059839031597372595, 0.005663268490493544, 0.005224317960716404, 0.004835410642563937, 0.004699777601881046, 0.004438613119584069, 0.004127040098625758, 0.003940507556676778, 0.0036891807673842883, 0.0035545183268006824, 0.003268253731667733, 0.0031006510063026167, 0.002942806719543107, 0.002856111967569073, 0.002743068029678263, 0.0026314433339901357, 0.0024380526912375373, 0.0023746803286072114, 0.0022181623131565004, 0.002118568524481711, 0.0020777257866579826, 0.0018965977189530356, 0.001818185189782576, 0.0017810153742278912, 0.0018117994571483042, 0.0016490160673387586, 0.001640037203574781, 0.0015401129813248581, 0.0013866248961202018, 0.0013897587726627416, 0.0013106586298255978, 0.0013547043255226934, 0.001407387350522424, 0.0013250731553544324, 0.0012883657775766431, 0.0011637732091704675, 0.0012383861184532084, 0.0011990139896053242, 0.0010962663353812536, 0.0010399244176193818, 0.0010520784868786632, 0.0009531496615136806, 0.0009937542939751816, 0.0010019696786158063, 0.0009494776388095403, 0.0009424924861554294, 0.0009154985238858839, 0.0010308355422137875, 0.0010699664325679243, 0.000970369897481095, 0.0011028039433605638, 0.0011303870405053856, 0.0010477391048680329, 0.0010339516907511735, 0.0010929124689275193, 0.00102621075972625, 0.0010820230698966955, 0.0009741782563179144, 0.0009106166968007843, 0.000819780057211982, 0.0008706225380535441, 0.0007966895211184032, 0.0008207842037592642, 0.0008902239469255037, 0.0008834009841358996, 0.0008173086616799461, 0.0007773632103344015, 0.0007758976981263796, 0.0008470140031027041, 0.0007819889525984514, 0.0007557912395511342, 0.0008355233518208564, 0.0008289465604865796, 0.0008803330241855859, 0.0008820094276138992, 0.0007942076398849209, 0.0007943415617132463, 0.0007659580450286769, 0.0007595681118017115, 0.0006978335396072817, 0.0006424100856883115, 0.0005844890032086736, 0.0005750326565953137, 0.0006508099702040877, 0.0005957396673953658, 0.0006931163207527992, 0.0007927451324212745, 0.0007478899755866891, 0.0007727361127052077, 0.0007730271394375992, 0.0008385335961648742, 0.0008384530220442229, 0.0007860370097722001, 0.0007322931808129411, 0.000710270155076175, 0.0007261720059568825, 0.001036674261554341, 0.0009340348183490964, 0.0010560899028656463, 0.0010760500073820577, 0.0010119759511991017], "duration": 34022.503141, "accuracy_train": [0.1879941641566265, 0.18698230421686746, 0.18147590361445784, 0.18458207831325302, 0.18206419427710843, 0.18688817771084337, 0.20769013554216867, 0.2390577936746988, 0.2632718373493976, 0.2849915286144578, 0.3095114834337349, 0.334972703313253, 0.36113987198795183, 0.3822242093373494, 0.4016848644578313, 0.4182511295180723, 0.43465267319277107, 0.4507718373493976, 0.4660909262048193, 0.4793862951807229, 0.4915756777108434, 0.5063064759036144, 0.5142836972891566, 0.5279320406626506, 0.5372505647590361, 0.55078125, 0.5586878765060241, 0.5670180722891566, 0.5729009789156626, 0.5840314382530121, 0.5850903614457831, 0.5979856927710844, 0.6049039909638554, 0.611328125, 0.6174463478915663, 0.621117281626506, 0.6264589608433735, 0.6335184487951807, 0.6362481174698795, 0.6419898343373494, 0.6471432605421686, 0.6540144954819277, 0.6571677334337349, 0.6606974774096386, 0.6594032379518072, 0.6672628012048193, 0.6722515060240963, 0.6846291415662651, 0.6756635918674698, 0.6854292168674698, 0.6954301581325302, 0.6929358057228916, 0.7017131024096386, 0.6893354668674698, 0.6939947289156626, 0.7215032003012049, 0.7137142319277109, 0.7087019954819277, 0.7243269954819277, 0.7190323795180723, 0.7342102786144579, 0.7160203313253012, 0.7331278237951807, 0.7363987198795181, 0.7474115210843374, 0.7479762801204819, 0.7510824548192772, 0.7406579442771084, 0.7603774472891566, 0.7500705948795181, 0.7599303463855421, 0.7703548569277109, 0.7483292545180723, 0.7680016942771084, 0.7764495481927711, 0.7885448042168675, 0.7594832454819277, 0.7882388930722891, 0.7774614081325302, 0.7510353915662651, 0.7928981551204819, 0.7829678087349398, 0.8038638930722891, 0.8098879894578314, 0.7942159262048193, 0.8039109563253012, 0.7814147213855421, 0.821300828313253, 0.8104762801204819, 0.7967102786144579, 0.808617281626506, 0.8226421310240963, 0.7986634036144579, 0.8300075301204819, 0.8296074924698795, 0.8215126129518072, 0.832054781626506, 0.8312547063253012, 0.7587067018072289, 0.8416086219879518, 0.8149002259036144, 0.8572336219879518, 0.8509271460843374, 0.8340785015060241, 0.7857445406626506, 0.8568335843373494, 0.8403614457831325, 0.8627400225903614, 0.8269719503012049, 0.8445736069277109, 0.8255365210843374, 0.8653755647590361, 0.8428793298192772, 0.8659638554216867, 0.8764354292168675, 0.8697289156626506, 0.8268072289156626, 0.8625047063253012, 0.8722232680722891, 0.8866716867469879, 0.8353021460843374, 0.8726468373493976, 0.892554593373494, 0.8844126506024096, 0.8194653614457831, 0.8858010165662651, 0.8552804969879518, 0.8869070030120482, 0.8839655496987951, 0.8904602786144579, 0.877894390060241, 0.8792121611445783, 0.8762236445783133, 0.8920133659638554, 0.9094973644578314, 0.8643166415662651, 0.9155685240963856, 0.9213102409638554, 0.9018731174698795, 0.9175451807228916, 0.916956890060241, 0.9303699171686747, 0.9250282379518072, 0.878882718373494, 0.9123211596385542, 0.9212161144578314, 0.8686699924698795, 0.8313958960843374, 0.8934958584337349, 0.9393825301204819, 0.9326995481927711, 0.9210749246987951], "end": "2016-01-20 20:07:54.378000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0], "accuracy_valid": [0.18736530172413793, 0.1841325431034483, 0.1796875, 0.17874461206896552, 0.17699353448275862, 0.18238146551724138, 0.2023168103448276, 0.22952586206896552, 0.25309806034482757, 0.2707435344827586, 0.29108297413793105, 0.31559806034482757, 0.33836206896551724, 0.35560344827586204, 0.37473060344827586, 0.3895474137931034, 0.40315193965517243, 0.4131196120689655, 0.42254849137931033, 0.43386314655172414, 0.4436961206896552, 0.45501077586206895, 0.46120689655172414, 0.4735991379310345, 0.4800646551724138, 0.4927262931034483, 0.49407327586206895, 0.5005387931034483, 0.5032327586206896, 0.5115840517241379, 0.5072737068965517, 0.5222252155172413, 0.5233028017241379, 0.5316540948275862, 0.5344827586206896, 0.5366379310344828, 0.5417564655172413, 0.5431034482758621, 0.5459321120689655, 0.5503771551724138, 0.5486260775862069, 0.5549568965517241, 0.552667025862069, 0.5584590517241379, 0.5579202586206896, 0.5618265086206896, 0.5633081896551724, 0.5674838362068966, 0.564385775862069, 0.5697737068965517, 0.5726023706896551, 0.5705818965517241, 0.5746228448275862, 0.5711206896551724, 0.5686961206896551, 0.5831088362068966, 0.5789331896551724, 0.5752963362068966, 0.5802801724137931, 0.5787984913793104, 0.5847252155172413, 0.5778556034482759, 0.5832435344827587, 0.5860721982758621, 0.5866109913793104, 0.5882273706896551, 0.5883620689655172, 0.5831088362068966, 0.5903825431034483, 0.5875538793103449, 0.5866109913793104, 0.5933459051724138, 0.5831088362068966, 0.5933459051724138, 0.5988685344827587, 0.5973868534482759, 0.5874191810344828, 0.5953663793103449, 0.5929418103448276, 0.5782596982758621, 0.5981950431034483, 0.5876885775862069, 0.5988685344827587, 0.6003502155172413, 0.5955010775862069, 0.5971174568965517, 0.5867456896551724, 0.599542025862069, 0.5969827586206896, 0.5829741379310345, 0.5901131465517241, 0.59765625, 0.5890355603448276, 0.6002155172413793, 0.6004849137931034, 0.5952316810344828, 0.5979256465517241, 0.5992726293103449, 0.5602101293103449, 0.5994073275862069, 0.5827047413793104, 0.6038523706896551, 0.6003502155172413, 0.5941540948275862, 0.5699084051724138, 0.5971174568965517, 0.5980603448275862, 0.59765625, 0.5872844827586207, 0.591729525862069, 0.5837823275862069, 0.5991379310344828, 0.5887661637931034, 0.5936153017241379, 0.6006196120689655, 0.5979256465517241, 0.5794719827586207, 0.595770474137931, 0.5981950431034483, 0.5994073275862069, 0.5804148706896551, 0.5983297413793104, 0.6023706896551724, 0.5955010775862069, 0.5750269396551724, 0.5949622844827587, 0.5771821120689655, 0.5946928879310345, 0.595770474137931, 0.5934806034482759, 0.5893049568965517, 0.5860721982758621, 0.5871497844827587, 0.5924030172413793, 0.5961745689655172, 0.5823006465517241, 0.599676724137931, 0.6021012931034483, 0.5899784482758621, 0.6010237068965517, 0.5991379310344828, 0.6022359913793104, 0.5969827586206896, 0.5785290948275862, 0.5906519396551724, 0.5977909482758621, 0.5786637931034483, 0.564385775862069, 0.5824353448275862, 0.6004849137931034, 0.5972521551724138, 0.587957974137931], "accuracy_test": 0.5901442307692307, "start": "2016-01-20 10:40:51.875000", "learning_rate_per_epoch": [0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691, 0.004052986390888691], "accuracy_train_last": 0.9210749246987951, "error_valid": [0.8126346982758621, 0.8158674568965517, 0.8203125, 0.8212553879310345, 0.8230064655172413, 0.8176185344827587, 0.7976831896551724, 0.7704741379310345, 0.7469019396551724, 0.7292564655172413, 0.708917025862069, 0.6844019396551724, 0.6616379310344828, 0.6443965517241379, 0.6252693965517242, 0.6104525862068966, 0.5968480603448276, 0.5868803879310345, 0.5774515086206897, 0.5661368534482758, 0.5563038793103448, 0.544989224137931, 0.5387931034482758, 0.5264008620689655, 0.5199353448275862, 0.5072737068965517, 0.505926724137931, 0.4994612068965517, 0.4967672413793104, 0.4884159482758621, 0.4927262931034483, 0.4777747844827587, 0.4766971982758621, 0.4683459051724138, 0.4655172413793104, 0.46336206896551724, 0.4582435344827587, 0.4568965517241379, 0.4540678879310345, 0.4496228448275862, 0.45137392241379315, 0.4450431034482759, 0.44733297413793105, 0.4415409482758621, 0.4420797413793104, 0.4381734913793104, 0.4366918103448276, 0.4325161637931034, 0.43561422413793105, 0.4302262931034483, 0.42739762931034486, 0.4294181034482759, 0.4253771551724138, 0.4288793103448276, 0.43130387931034486, 0.4168911637931034, 0.4210668103448276, 0.4247036637931034, 0.41971982758620685, 0.4212015086206896, 0.4152747844827587, 0.4221443965517241, 0.4167564655172413, 0.4139278017241379, 0.4133890086206896, 0.41177262931034486, 0.41163793103448276, 0.4168911637931034, 0.4096174568965517, 0.41244612068965514, 0.4133890086206896, 0.4066540948275862, 0.4168911637931034, 0.4066540948275862, 0.4011314655172413, 0.4026131465517241, 0.41258081896551724, 0.40463362068965514, 0.4070581896551724, 0.4217403017241379, 0.4018049568965517, 0.41231142241379315, 0.4011314655172413, 0.3996497844827587, 0.40449892241379315, 0.4028825431034483, 0.4132543103448276, 0.40045797413793105, 0.4030172413793104, 0.4170258620689655, 0.4098868534482759, 0.40234375, 0.4109644396551724, 0.39978448275862066, 0.3995150862068966, 0.40476831896551724, 0.4020743534482759, 0.40072737068965514, 0.43978987068965514, 0.40059267241379315, 0.4172952586206896, 0.39614762931034486, 0.3996497844827587, 0.4058459051724138, 0.4300915948275862, 0.4028825431034483, 0.4019396551724138, 0.40234375, 0.41271551724137934, 0.40827047413793105, 0.41621767241379315, 0.40086206896551724, 0.4112338362068966, 0.4063846982758621, 0.3993803879310345, 0.4020743534482759, 0.42052801724137934, 0.40422952586206895, 0.4018049568965517, 0.40059267241379315, 0.41958512931034486, 0.4016702586206896, 0.3976293103448276, 0.40449892241379315, 0.4249730603448276, 0.4050377155172413, 0.4228178879310345, 0.4053071120689655, 0.40422952586206895, 0.4065193965517241, 0.4106950431034483, 0.4139278017241379, 0.4128502155172413, 0.40759698275862066, 0.40382543103448276, 0.4176993534482759, 0.40032327586206895, 0.3978987068965517, 0.4100215517241379, 0.3989762931034483, 0.40086206896551724, 0.3977640086206896, 0.4030172413793104, 0.4214709051724138, 0.4093480603448276, 0.4022090517241379, 0.4213362068965517, 0.43561422413793105, 0.4175646551724138, 0.3995150862068966, 0.4027478448275862, 0.41204202586206895], "accuracy_train_std": [0.023335983108718324, 0.02172600029193854, 0.021814921978037843, 0.023314367925405702, 0.025146986539960333, 0.02523161026644646, 0.02490406631332937, 0.026306221978390026, 0.02681721808078759, 0.026767377717855947, 0.026878979131595173, 0.02684310922461447, 0.027881561639453087, 0.028381687784758754, 0.0285206808027616, 0.02882819216728353, 0.030121888277244446, 0.029615578001244407, 0.02911524128871167, 0.02944046296860735, 0.029133940290599555, 0.030705986535783453, 0.031095822415150173, 0.030420131887067554, 0.03050770784664793, 0.03101379102002422, 0.03005024637785085, 0.029906169985667534, 0.031186854432346083, 0.029155305389796847, 0.0311707638201594, 0.030540135059065176, 0.0309760234941796, 0.028962385201590765, 0.03065347306484748, 0.030507272225799124, 0.028600443793040317, 0.02951113654791675, 0.028314169854459708, 0.02726924643823427, 0.029408480175400462, 0.027435424344457414, 0.028004464621467808, 0.02745749599822125, 0.02674526449613517, 0.026162809035391785, 0.026756649344767545, 0.027850592091289206, 0.026747013937944656, 0.026692831649927515, 0.02650188777274263, 0.025959949154238438, 0.02546800443535932, 0.02685077167917106, 0.026785235008482674, 0.024202606752872752, 0.025736930522637758, 0.02694864429133995, 0.02441831199861698, 0.025534362987737726, 0.024684272417649148, 0.02495506524316909, 0.02463837709974804, 0.02589240372666421, 0.024411678034413794, 0.025194274137616702, 0.0244758780904775, 0.02547744978917774, 0.024397213144377913, 0.02541856842354065, 0.02558843392081816, 0.023624264660104446, 0.026763819312813714, 0.024078879834131588, 0.023207529046534007, 0.022932170688451176, 0.025265658736865253, 0.023515066179631876, 0.023696581570915608, 0.02538489956118529, 0.022975943194477617, 0.02507258087543462, 0.02258998150486844, 0.021687237735287124, 0.025008672682036125, 0.024247956693709376, 0.025189273486614164, 0.02150976024149801, 0.023944699025196064, 0.026767377717855947, 0.025884500357781035, 0.02388418359033618, 0.02479295563515986, 0.021565397314056118, 0.021742930017618893, 0.02407464804906634, 0.022984039619692337, 0.02144356182777061, 0.02993285789907072, 0.021547634265163648, 0.02669967655620091, 0.022078597946136026, 0.021703827756105295, 0.023956131944480066, 0.028404349796044833, 0.022336509436635735, 0.02277127179586434, 0.024465661311032682, 0.026743038709491, 0.026402453480346567, 0.02498850442559577, 0.021590610352524225, 0.028205469284938584, 0.021676498504019502, 0.022168902920027305, 0.026006269566558167, 0.027244583439469543, 0.022106319334021123, 0.02091774976175272, 0.02137816862121035, 0.027361575168563294, 0.023519245613450358, 0.022179079167001067, 0.023699093478400213, 0.0266908400748597, 0.02324816326936815, 0.025329213662424816, 0.02202127779730425, 0.0251524469150453, 0.024218113915228896, 0.025647022462947748, 0.025832935551964257, 0.02336622973171285, 0.02081623428710905, 0.022041435313732625, 0.025979043551610306, 0.021350954193203233, 0.0190222892004451, 0.02324372068606559, 0.02041048150801509, 0.021239883849939536, 0.019294340979848938, 0.021313263274708158, 0.024587802312367957, 0.022289705161049878, 0.021225631461288107, 0.022888048345563322, 0.02505738152779511, 0.023142155326181347, 0.018854634056232014, 0.02099928385950662, 0.01792387169340175], "accuracy_test_std": 0.027327785738449746, "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-3, 3], "momentum": 0.7227711510444743, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.004052986416888268, "patience_threshold": 1, "do_flip": true, "batch_size": 256, "optimization": "adadelta", "nb_data_augmentation": 0, "learning_rate_decay_method": "none", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 6.2140738641089e-05, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.008451457599326894}, "accuracy_valid_max": 0.6038523706896551, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = 1234423\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-10, -4], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128, 256],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-3, 3)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_optimizer.learning_rate = learning_rate\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.587957974137931, "loss_train": [39.38199234008789, 8.436734199523926, 6.5786542892456055, 5.8490705490112305, 5.476511001586914, 5.2591681480407715, 5.118205547332764, 5.0170063972473145, 4.942159652709961, 4.881741046905518, 4.829553127288818, 4.784770965576172, 4.745640754699707, 4.710097789764404, 4.677635669708252, 4.648787498474121, 4.621441841125488, 4.595494747161865, 4.5706377029418945, 4.548800468444824, 4.524868488311768, 4.507602691650391, 4.485896110534668, 4.468057155609131, 4.452305793762207, 4.430702209472656, 4.417591094970703, 4.396915435791016, 4.386551856994629, 4.36752462387085, 4.3507080078125, 4.337325572967529, 4.328449726104736, 4.3064446449279785, 4.29582405090332, 4.2787370681762695, 4.265368461608887, 4.257627487182617, 4.2389068603515625, 4.2277703285217285, 4.218347072601318, 4.202022552490234, 4.191181182861328, 4.17764139175415, 4.165905952453613, 4.155725002288818, 4.14515495300293, 4.13141393661499, 4.1200785636901855, 4.109273910522461, 4.097739219665527, 4.088237762451172, 4.077477931976318, 4.0640387535095215, 4.0578765869140625, 4.046761512756348, 4.032750129699707, 4.026199817657471, 4.015270709991455, 4.001516819000244, 3.99825382232666, 3.982125759124756, 3.975529670715332, 3.9669225215911865, 3.95993971824646, 3.951005220413208, 3.9362545013427734, 3.9281086921691895, 3.9232981204986572, 3.9106504917144775, 3.899609327316284, 3.8943393230438232, 3.881990909576416, 3.878713369369507, 3.867363214492798, 3.8506529331207275, 3.8486053943634033, 3.843865156173706, 3.827982187271118, 3.820683002471924, 3.8171584606170654, 3.808867931365967, 3.7995502948760986, 3.7878642082214355, 3.7852914333343506, 3.778858184814453, 3.768446207046509, 3.7605788707733154, 3.74818754196167, 3.752068519592285, 3.7343530654907227, 3.7277379035949707, 3.721989631652832, 3.712965250015259, 3.710561513900757, 3.6954784393310547, 3.693310499191284, 3.6891937255859375, 3.678744077682495, 3.6651611328125, 3.6635475158691406, 3.6547749042510986, 3.6535205841064453, 3.64001202583313, 3.636056423187256, 3.628232955932617, 3.6172730922698975, 3.6088104248046875, 3.615675210952759, 3.609348773956299, 3.588548183441162, 3.5935628414154053, 3.5758302211761475, 3.581430435180664, 3.570175886154175, 3.569650173187256, 3.5622851848602295, 3.5550646781921387, 3.546340227127075, 3.5336434841156006, 3.5481343269348145, 3.52494215965271, 3.519408702850342, 3.5168581008911133, 3.5155603885650635, 3.523648500442505, 3.4869561195373535, 3.4981119632720947, 3.4929468631744385, 3.484499216079712, 3.4795684814453125, 3.466597318649292, 3.468574285507202, 3.462416172027588, 3.4697725772857666, 3.454141855239868, 3.467787265777588, 3.432415008544922, 3.4339139461517334, 3.450047731399536, 3.4161763191223145, 3.425999164581299, 3.4177048206329346, 3.4097137451171875, 3.4112679958343506, 3.407761573791504, 3.4013760089874268, 3.387014150619507, 3.4020936489105225, 3.3730175495147705, 3.3908979892730713, 3.4007980823516846], "accuracy_train_first": 0.1879941641566265, "model": "residualv2", "loss_std": [75.4239501953125, 1.0834851264953613, 0.4744349420070648, 0.36745402216911316, 0.2598070502281189, 0.22024111449718475, 0.19638720154762268, 0.1717780977487564, 0.1539907455444336, 0.14429058134555817, 0.13843373954296112, 0.1335851401090622, 0.12892815470695496, 0.12519362568855286, 0.12269619852304459, 0.12000659853219986, 0.11819467693567276, 0.11678017675876617, 0.1157904863357544, 0.11408873647451401, 0.11333967000246048, 0.11313138902187347, 0.11170618236064911, 0.11099527776241302, 0.11204784363508224, 0.10843299329280853, 0.10949582606554031, 0.1089915931224823, 0.107703797519207, 0.10503088682889938, 0.10434761643409729, 0.10491738468408585, 0.10724221915006638, 0.10172206908464432, 0.10453980416059494, 0.1005479246377945, 0.10011912882328033, 0.10247775912284851, 0.09817767888307571, 0.09946130216121674, 0.09620430320501328, 0.09690668433904648, 0.09540762007236481, 0.09805739670991898, 0.09314979612827301, 0.09561533480882645, 0.09348982572555542, 0.09301184862852097, 0.09140799194574356, 0.0903715193271637, 0.09024964272975922, 0.08934793621301651, 0.0919477567076683, 0.08727256953716278, 0.0872872844338417, 0.08648262172937393, 0.08403154462575912, 0.08289780467748642, 0.09034744650125504, 0.08302941173315048, 0.08812294900417328, 0.08225420117378235, 0.08360771089792252, 0.08201639354228973, 0.08283764868974686, 0.07686875015497208, 0.08453826606273651, 0.07719394564628601, 0.08135639131069183, 0.07816895097494125, 0.0831684097647667, 0.08065121620893478, 0.07364567369222641, 0.07620048522949219, 0.07617795467376709, 0.07848383486270905, 0.07127415388822556, 0.07971727102994919, 0.07524585723876953, 0.07410132884979248, 0.0702534094452858, 0.0730215311050415, 0.07593841850757599, 0.07218009978532791, 0.0689607560634613, 0.07108510285615921, 0.07154002040624619, 0.07166644930839539, 0.06767255812883377, 0.07075008749961853, 0.06594382226467133, 0.07229515165090561, 0.07090558111667633, 0.06871893256902695, 0.06702372431755066, 0.06817285716533661, 0.06622295081615448, 0.07149667292833328, 0.0759192630648613, 0.06834044307470322, 0.06511886417865753, 0.06461440026760101, 0.07501690089702606, 0.07177825272083282, 0.0687934160232544, 0.06541714817285538, 0.07063872367143631, 0.06623876094818115, 0.05793505534529686, 0.06911216676235199, 0.06177463009953499, 0.06460046023130417, 0.07243939489126205, 0.07074088603258133, 0.07712779194116592, 0.0881168395280838, 0.07422719895839691, 0.05989953875541687, 0.06427787244319916, 0.06638380140066147, 0.10921559482812881, 0.06290888041257858, 0.06729748845100403, 0.06809666752815247, 0.0640845000743866, 0.11108866333961487, 0.051914770156145096, 0.05912420526146889, 0.056962303817272186, 0.07807290554046631, 0.06495344638824463, 0.09765796363353729, 0.05869118496775627, 0.05708882957696915, 0.09012432396411896, 0.0880260095000267, 0.10232050716876984, 0.0628938302397728, 0.0929979532957077, 0.12634266912937164, 0.08095037937164307, 0.07064846903085709, 0.08494988828897476, 0.06564304977655411, 0.06997273117303848, 0.10075949132442474, 0.10193797200918198, 0.07430093735456467, 0.09923955798149109, 0.06379110366106033, 0.14170172810554504, 0.12822911143302917]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:04 2016", "state": "available"}], "summary": "cb1479e95968096f5a81bbc814f13f0f"}