{"content": {"hp_model": {"f0": 64, "f1": 64, "f2": 64, "f3": 16, "nonlin": "leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.818509578704834, 1.531097412109375, 1.3565744161605835, 1.2412654161453247, 1.167647123336792, 1.1095571517944336, 1.059206247329712, 1.0075657367706299, 0.9683632254600525, 0.9356235861778259, 0.9117497801780701, 0.8909955024719238, 0.8711677193641663, 0.8567937016487122, 0.8481265306472778, 0.8345445990562439, 0.825835108757019, 0.8190785646438599, 0.812141478061676, 0.8039203882217407, 0.7976841330528259, 0.7928914427757263, 0.7861465811729431, 0.7832468152046204, 0.7800225019454956, 0.7728829383850098, 0.7701199650764465, 0.7643920183181763, 0.7630099058151245, 0.7627130150794983, 0.7584793567657471, 0.7564463019371033, 0.7538837790489197, 0.7506776452064514, 0.7485554218292236, 0.7444004416465759, 0.7419900894165039, 0.7397335171699524, 0.7412300109863281, 0.7381863594055176, 0.7337326407432556, 0.736535906791687, 0.7307680249214172, 0.7278625965118408, 0.7270237803459167, 0.7268933653831482, 0.7245530486106873, 0.7256101965904236, 0.7196447849273682, 0.7180655598640442, 0.7204280495643616, 0.7180503606796265, 0.7159430384635925, 0.7190105319023132, 0.7115481495857239, 0.7127710580825806, 0.7135308980941772, 0.7138401865959167, 0.70937579870224, 0.7102500796318054, 0.710824728012085, 0.7097539901733398, 0.7120653986930847, 0.7085452675819397, 0.7064608931541443, 0.7068800330162048, 0.7033866047859192, 0.7046952247619629, 0.7059147357940674, 0.7040141820907593, 0.7073181867599487, 0.7011197209358215, 0.702597975730896, 0.7020076513290405, 0.7026192545890808, 0.7002042531967163, 0.698357105255127, 0.6976943612098694, 0.6971667408943176, 0.7004238963127136, 0.6959744095802307, 0.6962496638298035, 0.6957083344459534, 0.6955395936965942, 0.6967284679412842, 0.6944026350975037, 0.6920434236526489, 0.691354513168335, 0.6929348111152649, 0.6900675296783447, 0.690625011920929, 0.6931920647621155, 0.6917644739151001, 0.6914322972297668, 0.6896137595176697, 0.6865197420120239, 0.6903862953186035, 0.6882100105285645, 0.6876884698867798, 0.6844993829727173, 0.6858986020088196, 0.6867139339447021, 0.6849567890167236, 0.6880361437797546, 0.6887534856796265, 0.6865890622138977, 0.683872401714325, 0.6831260919570923, 0.687231183052063, 0.6837794780731201, 0.6828130483627319, 0.6823285222053528, 0.6790701746940613, 0.6772306561470032, 0.6805477142333984, 0.6792005300521851, 0.6833699941635132, 0.6788474917411804, 0.6824521422386169, 0.6794975996017456, 0.6759229898452759, 0.6786806583404541], "moving_avg_accuracy_train": [0.02638764520464193, 0.05791757329445828, 0.09984756666868538, 0.1347810080223041, 0.17758823506245094, 0.2100193888310305, 0.2433454350785107, 0.2641703282862373, 0.2719009175333334, 0.2905514589991307, 0.3055910840017647, 0.32738855898106406, 0.3580087284094158, 0.3814130656321508, 0.393960778166158, 0.41982740972108146, 0.4289479969201158, 0.4582315300134955, 0.4654830708855771, 0.48388168551628846, 0.5023045231016104, 0.5253728008636623, 0.543398524018806, 0.5633267364009785, 0.5779961946880217, 0.5784106230113014, 0.5900915101149331, 0.6030841247745823, 0.6010951848596471, 0.6171343439588153, 0.6293497549625278, 0.6444703314169486, 0.6632139143351468, 0.6571663641484021, 0.6534071930113398, 0.6591943799659736, 0.6726716361485013, 0.6832574481473463, 0.6841973456360447, 0.6936629224765561, 0.7010644462761043, 0.7079234192956885, 0.7197426953236223, 0.7291710745142668, 0.7278984528913285, 0.7372854587811363, 0.74584104536657, 0.7565118203980193, 0.7453795524524404, 0.7530278931766501, 0.7633682472294631, 0.7591215798812398, 0.7496655093850408, 0.7549395734970629, 0.7597419626716736, 0.7640872562704623, 0.7715433317116146, 0.770558999001878, 0.7783647761792557, 0.7838418591175778, 0.7721037379725697, 0.769387941711996, 0.7535829694077215, 0.7607456456696995, 0.7659528220876409, 0.7736685171767727, 0.780507975011744, 0.7833184991467822, 0.786629148770679, 0.7793679669766215, 0.7689908743325768, 0.7807456014411243, 0.7725316693539517, 0.7730520624845403, 0.7690385387642331, 0.7755878264702665, 0.7827680287461819, 0.7873169377636364, 0.7682401991137032, 0.7758874475728109, 0.7825817422788927, 0.7896757794297409, 0.7774375029760157, 0.7630406836261808, 0.7743257279435185, 0.7786145655519906, 0.7780457597962102, 0.7864515553501348, 0.7708973841497466, 0.7715355319473965, 0.7797855596981165, 0.7824449308347095, 0.7859143680241714, 0.7828509382700193, 0.7788828454985268, 0.7855031758953795, 0.7755885655259874, 0.7801343723927777, 0.7885335585846553, 0.7881677637207911, 0.7871365336469309, 0.7944519262574593, 0.7902025688395853, 0.7747668046189712, 0.7826077106453999, 0.7622011660885657, 0.7702850873799952, 0.7671722301739263, 0.7596161259099944, 0.7486423143987292, 0.7615577857435555, 0.7685576378504938, 0.7753036449276168, 0.782068197983997, 0.7902928551489601, 0.7613764925588057, 0.7712610219781744, 0.7604578970090391, 0.7714686888130724, 0.7646795972481181, 0.7674460759692272, 0.7623383298756268], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.02598568100527108, 0.05840540874435239, 0.09990770601939003, 0.135022798322195, 0.17753373149825863, 0.21030114265754418, 0.2432811116937476, 0.26452773108724936, 0.27208404118635576, 0.29094482593255455, 0.30599361564728106, 0.3260493157354144, 0.35593173591262595, 0.37959590418883327, 0.3915274799973596, 0.4162476553315845, 0.4251274961143146, 0.4528863603018289, 0.45894564525244413, 0.47601553620536236, 0.4935930582625369, 0.5158096066343254, 0.5326040518914501, 0.5518025251849858, 0.5662479433178427, 0.5664482031087994, 0.5768013061318803, 0.5889727497845055, 0.586494608371567, 0.6020059509041391, 0.6144330737165866, 0.6288676430862684, 0.645711323996015, 0.6400099969898624, 0.6366255847344002, 0.6419558432319692, 0.654332024486709, 0.6641714947601767, 0.6645193055157103, 0.6728689739381603, 0.6794793256972057, 0.6866850840631026, 0.6974439041112351, 0.7062427883612562, 0.7052868312495282, 0.7138670642578585, 0.7220276975816962, 0.731812791387156, 0.7214291873237416, 0.7286647689208102, 0.7379121968668316, 0.733523832743025, 0.7244349978026834, 0.7289863916594632, 0.7339372853907308, 0.7373900541690522, 0.744130240911409, 0.743975676439997, 0.7505751438110575, 0.7552216015685059, 0.7444715670234776, 0.7429258304547895, 0.7276790987835576, 0.7345644209741025, 0.7394254969960447, 0.7463426164417715, 0.7522922626948082, 0.7546246010431737, 0.757449067943751, 0.7504254819795717, 0.740819386624235, 0.7518281035529258, 0.743084781620073, 0.7435881975186831, 0.7404727006677937, 0.7465679734285746, 0.7535969523940755, 0.7577804788696529, 0.7401937901694347, 0.7470574648516629, 0.7535968649685297, 0.7600988536881677, 0.7483167779955557, 0.7361577635054881, 0.7464748867219573, 0.7502343363498368, 0.7495649595672628, 0.7568485567957774, 0.7414464336086696, 0.7421614829247605, 0.7495771352723447, 0.7529216443072337, 0.7561270149386339, 0.753264984096126, 0.7502477912682152, 0.7557500029169961, 0.7464303560703266, 0.7508738815513963, 0.7585688434997959, 0.7579602178094549, 0.7560339425928919, 0.7626269609371419, 0.7591334666506566, 0.7445558927642054, 0.7519050615826645, 0.733673886890889, 0.7406483244913332, 0.7385205567071096, 0.7309859191463384, 0.7214866954075329, 0.7334052027775476, 0.7393953842128953, 0.7454730827080214, 0.751036696641662, 0.7588914230618331, 0.7320480733008305, 0.7409846733973288, 0.7306350000617374, 0.7404167972600065, 0.7336171814929667, 0.7365799997706278, 0.731771420915553], "moving_var_accuracy_train": [0.006266770375014562, 0.014587320625654019, 0.028951707662353183, 0.03703964481937834, 0.04982780851924061, 0.05431104528016782, 0.058875568978553734, 0.056891097674717325, 0.05173984599821146, 0.04969644567109715, 0.04676251398596611, 0.04636243182662813, 0.05016454162635398, 0.0500779544712381, 0.04648716483263901, 0.04786019200135859, 0.04382283879891944, 0.047158282712907076, 0.042915718046791604, 0.04167072742507729, 0.04055826318482596, 0.04129174581650799, 0.0400869114922501, 0.03965242318176587, 0.03762391792150698, 0.033863071886872514, 0.03170475280993538, 0.03005354985018902, 0.027083797803037116, 0.026690709644209273, 0.02536458507369491, 0.024885813057151308, 0.02555912885693848, 0.023332371740595388, 0.02112631687527536, 0.01931510898337878, 0.01901832599292625, 0.018125028134707632, 0.01632047598684022, 0.015494802692468838, 0.014438365414219454, 0.013417938470747952, 0.0133334021960936, 0.01280011098394743, 0.011534675977709219, 0.011174251296115862, 0.010715608722891972, 0.010668836808548993, 0.010717299634203576, 0.01017204371328565, 0.010117145639394825, 0.009267738727553524, 0.009145720277859924, 0.00848149002039339, 0.007840907494413824, 0.007226750933109509, 0.007004413388655958, 0.006312692247741478, 0.006229794439053169, 0.005876800932767186, 0.006529172231624434, 0.005942634952422504, 0.00759654580303022, 0.00729862660383232, 0.006812796119677178, 0.006667304064085534, 0.006421577308964134, 0.005850510991290412, 0.005364103500551249, 0.005302215999913302, 0.005741150865609729, 0.006410598263616531, 0.00637675656024903, 0.005741518185317401, 0.005312341720666872, 0.005167146073707776, 0.005114429208844545, 0.0047892194472018024, 0.007585595120142326, 0.007353359289086019, 0.007021345594684287, 0.0067721393030943985, 0.007442904067805222, 0.008564029327550716, 0.008853796421994112, 0.008133963932081302, 0.007323479398763453, 0.007227048048936511, 0.008681733419621718, 0.007817225171164356, 0.007648069275036762, 0.006946912641112378, 0.00636055432670573, 0.005808960310762777, 0.005369776121875039, 0.005227257480758967, 0.005589227221674797, 0.0052162837401387415, 0.005329572324297319, 0.004797819344809453, 0.004327608329515611, 0.004376482217979613, 0.004101347342365221, 0.005835577961798219, 0.005805338431455989, 0.008972648135060922, 0.008663531372569075, 0.007884387155180543, 0.007609800844489183, 0.007932641611803258, 0.00864066205115418, 0.008217577211709837, 0.0078053969939001966, 0.007436689896983406, 0.0073018257766156855, 0.014097047427961648, 0.013566677981746753, 0.013260377765460863, 0.013025477814280687, 0.012137755911348625, 0.010992860960842912, 0.010128376496168838], "duration": 77754.754811, "accuracy_train": [0.26387645204641935, 0.3416869261028055, 0.47721750703672944, 0.44918198020487266, 0.5628532784237725, 0.5018997727482466, 0.5432798513058325, 0.45159436715577705, 0.34147622075719825, 0.4584063321913068, 0.44094770902547065, 0.5235658337947582, 0.633590253264581, 0.5920521006367663, 0.5068901909722222, 0.6526270937153931, 0.5110332817114249, 0.7217833278539129, 0.5307469387343116, 0.649469217192691, 0.668110061369509, 0.7329873007221299, 0.7056300324150978, 0.7426806478405316, 0.7100213192714101, 0.5821404779208195, 0.6952194940476191, 0.7200176567114249, 0.5831947256252308, 0.761486775851329, 0.7392884539959395, 0.7805555195067367, 0.8319061605989295, 0.6027384124677002, 0.6195746527777778, 0.711279062557678, 0.7939669417912514, 0.7785297561369509, 0.6926564230343301, 0.7788531140411591, 0.7676781604720377, 0.7696541764719453, 0.8261161795750278, 0.8140264872300664, 0.7164448582848837, 0.8217685117894058, 0.8228413246354743, 0.8525487956810631, 0.6451891409422297, 0.8218629596945367, 0.8564314337047803, 0.7209015737472315, 0.6645608749192506, 0.8024061505052602, 0.802963465243171, 0.8031948986595607, 0.838648010681986, 0.7617000046142488, 0.8486167707756552, 0.8331356055624769, 0.6664606476674972, 0.7449457753668328, 0.6113382186692506, 0.825209732027501, 0.812817409849114, 0.843109772978959, 0.8420630955264857, 0.8086132163621264, 0.8164249953857512, 0.7140173308301033, 0.6755970405361756, 0.886538145418051, 0.6986062805693983, 0.7777356006598376, 0.7329168252814692, 0.8345314158245662, 0.8473898492294205, 0.8282571189207272, 0.5965495512643042, 0.8447126837047803, 0.8428303946336286, 0.8535221137873754, 0.6672930148924879, 0.633469309477667, 0.875891126799557, 0.8172141040282392, 0.772926507994186, 0.8621037153354559, 0.6309098433462532, 0.7772788621262459, 0.8540358094545959, 0.8063792710640458, 0.8171393027293282, 0.7552800704826504, 0.7431700105550941, 0.8450861494670543, 0.686357072201458, 0.8210466341938908, 0.8641262343115541, 0.7848756099460132, 0.777855462982189, 0.860290459752215, 0.7519583520787191, 0.635844926633444, 0.8531758648832595, 0.578542265077058, 0.8430403790028608, 0.739156515319306, 0.6916111875346069, 0.6498780107973422, 0.8777970278469915, 0.8315563068129384, 0.8360177086217239, 0.8429491754914176, 0.8643147696336286, 0.501129229247416, 0.8602217867524916, 0.6632297722868217, 0.8705658150493725, 0.703577773163529, 0.79234438445921, 0.7163686150332226], "end": "2016-02-01 13:15:20.019000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0], "moving_var_accuracy_valid": [0.006077300555769357, 0.01492891922027785, 0.028937993410200438, 0.03714182143609101, 0.04969225424797964, 0.05438635792991674, 0.05873682735558549, 0.05692591414089786, 0.051747203127632785, 0.049774045626051505, 0.046834835710348875, 0.04577143209354035, 0.049230920205013115, 0.0493477639263537, 0.04569425004518982, 0.04662460865766422, 0.04267181194283753, 0.04533962161738156, 0.041136093862658187, 0.03964491507069305, 0.03846114709865789, 0.03905720758279625, 0.03768996734796765, 0.03723820300439432, 0.03539241364925251, 0.031853533220182126, 0.029632860578022654, 0.028002870885521666, 0.025257854460732185, 0.024897484739124023, 0.023797636697772507, 0.023293084163988634, 0.023517162026894044, 0.021457991990884415, 0.019415281008630277, 0.01772945780862541, 0.017335040789814432, 0.01647287328819507, 0.014826674710270547, 0.013971459904127228, 0.012967584667119338, 0.012138132783056643, 0.01196608938420386, 0.011466263722190893, 0.010327862035966976, 0.009957659418665448, 0.009561256903014069, 0.009466863759747509, 0.009490550483884561, 0.009012678204926646, 0.0088810446969857, 0.00816625988443515, 0.008093096181146614, 0.007470223237387753, 0.0069438030522937155, 0.00635671725719331, 0.006129916587370563, 0.005517139940215912, 0.0053574026724295554, 0.005015968532412379, 0.005554440863644859, 0.005020500491138391, 0.006610615881915485, 0.0063762232487324195, 0.005951271464679087, 0.005786763191049252, 0.005526671486770791, 0.005022962557635021, 0.0045924648213236255, 0.004577195177357214, 0.004949969271403881, 0.005545698980007918, 0.005679140187800673, 0.005113507017123367, 0.004689513201062145, 0.004554933031210785, 0.004544098635766803, 0.004247205816136835, 0.006606109809468629, 0.006369489099812911, 0.006117413974827924, 0.00588615529313783, 0.006546895532461983, 0.007222780679542854, 0.007458489894762851, 0.006839842058827641, 0.00615989044043832, 0.0060213584936795305, 0.007554251232349188, 0.006803427768834248, 0.006618012089612902, 0.0060568825468117, 0.005543663900092311, 0.005063018494974282, 0.004638647718524021, 0.0044472519439234404, 0.004784229105650826, 0.004483510463393989, 0.00456807137154044, 0.004114598061464883, 0.003736533081207897, 0.0037540907910756567, 0.003488522232935438, 0.00505222095337639, 0.005033091398938637, 0.007521163834823085, 0.00720683246992319, 0.006526895784623093, 0.00638514307471044, 0.0065587460319983725, 0.007181328790160407, 0.006786136373799819, 0.006439968507398764, 0.00607455585668229, 0.006022370815235673, 0.011905222571235765, 0.011433465705674785, 0.011254160778488351, 0.010989896708492089, 0.010307020008857267, 0.009355322637289528, 0.00862789224900983], "accuracy_test": 0.09998804209183673, "start": "2016-01-31 15:39:25.264000", "learning_rate_per_epoch": [0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437, 0.00743525056168437], "accuracy_train_first": 0.26387645204641935, "accuracy_train_last": 0.7163686150332226, "batch_size_eval": 1024, "accuracy_train_std": [0.012726003364865388, 0.012486112769913649, 0.014961177074912194, 0.012406243956628869, 0.015564324688631193, 0.01708336672577315, 0.01798640174649156, 0.01454867310393419, 0.013135233123031545, 0.013005421707350423, 0.016596236607477548, 0.01629717183639899, 0.018597661658110585, 0.01919848288836644, 0.021460043061641926, 0.019493300403284338, 0.01818415015213228, 0.021403571850317625, 0.01892347041211247, 0.018714880804896108, 0.01892701375657988, 0.024027239326501887, 0.021005816138005468, 0.022985532278258475, 0.02151079420658238, 0.020910418318550894, 0.017635912684897177, 0.023145082541238454, 0.01762933841676979, 0.02400649691233313, 0.022692938722992557, 0.021313323202590138, 0.022183135417216494, 0.01997889630156256, 0.016672633884052275, 0.021360545233034895, 0.022187404954752084, 0.0216090363422903, 0.020437584458506512, 0.02372719875426361, 0.02030786086482039, 0.018687082395128438, 0.023704418154295342, 0.023072050856390195, 0.01756200340089586, 0.023404543489236113, 0.021853029177334133, 0.022350159276210136, 0.020043202914220046, 0.023528185931890093, 0.02351571883349974, 0.023141970999984968, 0.0185093730225836, 0.020507021567904446, 0.02369874467295059, 0.02099708074220256, 0.022415711568636785, 0.02286088772327502, 0.026718953723543198, 0.022776506578549075, 0.020079992646965427, 0.01916888707801379, 0.021086929876461893, 0.0250218097072253, 0.024352432521427506, 0.02552850217376072, 0.022937887161723316, 0.02148967624965929, 0.020077053517813928, 0.021650812138566507, 0.01991504891811893, 0.02090511720414496, 0.022640222684174816, 0.025677503964196664, 0.022850221335695375, 0.02210203598758122, 0.020772118660929227, 0.019857966895356995, 0.022271125037920975, 0.02510979365580447, 0.023764937937497822, 0.0255642849000579, 0.0234459675593281, 0.022452202122047446, 0.022319534487178022, 0.02050490152665527, 0.023328350772377437, 0.020576952345876635, 0.023348211540037526, 0.019437680744072865, 0.024248006942055433, 0.024017192297524748, 0.02624207486459981, 0.024293395014066334, 0.021349861479742317, 0.02317719510194598, 0.021737304231697376, 0.024299300128290385, 0.02367949999185208, 0.02090067739454141, 0.023149937821616846, 0.02303768671313762, 0.024186843381402496, 0.01559041477677732, 0.0236465987953667, 0.0157727952639614, 0.0228514311290809, 0.019125377393304017, 0.020496698121366327, 0.022580105244113094, 0.02226934061562647, 0.020383332095835437, 0.02407589152327555, 0.02104837167428589, 0.022441163256045886, 0.016324618308423015, 0.021704111445867518, 0.017440212608911577, 0.023084220496074472, 0.025672971413702762, 0.022243035473872276, 0.022097304526523805], "accuracy_test_std": 0.007350288587607382, "error_valid": [0.7401431899472892, 0.6498170416039157, 0.5265716185052711, 0.5489413709525602, 0.43986786991716864, 0.49479215690888556, 0.45989916698042166, 0.5442526943712349, 0.6599091679216867, 0.5393081113516567, 0.5585672769201807, 0.49344938347138556, 0.3751264824924698, 0.4074265813253012, 0.5010883377259037, 0.3612707666603916, 0.49495393684111444, 0.29728386201054224, 0.4865207901920181, 0.3703554452183735, 0.3482092432228916, 0.28424145801957834, 0.3162459407944277, 0.2754112151731928, 0.3037432934864458, 0.4317494587725903, 0.3300207666603916, 0.30148425734186746, 0.4358086643448795, 0.2583919663027108, 0.27372282097138556, 0.24122123258659633, 0.2026955478162651, 0.41130194606551207, 0.39383412556475905, 0.3100718302899097, 0.23428234422063254, 0.24727327277861444, 0.33235039768448793, 0.2519840102597892, 0.26102750847138556, 0.24846309064382532, 0.2057267154555723, 0.2145672533885542, 0.30331678275602414, 0.20891083866716864, 0.2045266025037651, 0.18012136436370485, 0.37202324924698793, 0.2062149967055723, 0.17886095161897586, 0.3059714443712349, 0.3573645166603916, 0.2300510636295181, 0.22150467102786142, 0.2315350268260542, 0.19520807840737953, 0.2574154038027108, 0.19002964984939763, 0.20296027861445776, 0.35227874388177716, 0.27098579866340367, 0.4095414862575302, 0.20346767931099397, 0.21682481880647586, 0.19140330854668675, 0.19416092102786142, 0.2243843538215362, 0.2171307299510542, 0.31278679169804224, 0.34563547157379515, 0.1490934440888554, 0.33560511577560237, 0.2518810593938253, 0.2875667709902108, 0.19857457172439763, 0.18314223691641573, 0.20456778285015065, 0.4180864081325302, 0.1911694630082832, 0.18754853397966864, 0.1813832478350903, 0.35772190323795183, 0.3732733669051205, 0.1606710043298193, 0.21593061699924698, 0.25645943147590367, 0.1775990681475903, 0.3971726750753012, 0.25140307323042166, 0.18368199359939763, 0.2169777743787651, 0.2150246493787651, 0.2724932934864458, 0.2769069441829819, 0.19473009224397586, 0.3374464655496988, 0.20913438911897586, 0.1721764989646084, 0.24751741340361444, 0.2613025343561747, 0.1780358739646084, 0.2723079819277108, 0.3866422722138554, 0.18195241905120485, 0.4304066853350903, 0.19658173710466864, 0.28062935335090367, 0.33682581890060237, 0.3640063182417168, 0.1593282308923193, 0.20669298286897586, 0.19982763083584332, 0.1988907779555723, 0.1704160391566265, 0.5095420745481928, 0.17858592573418675, 0.36251205995858427, 0.1715470279555723, 0.3275793604103916, 0.23675463573042166, 0.3115057887801205], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.0004601957292821202, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 64, "valid_ratio": 0.15, "learning_rate": 0.007435250552103416, "optimization": "rmsprop", "nb_data_augmentation": 1, "learning_rate_decay_method": "discrete", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 2.0217035344561916e-05, "rotation_range": [0, 0], "momentum": 0.5198153185012594}, "accuracy_valid_max": 0.8509065559111446, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.6884942112198795, "accuracy_valid_std": [0.015350248262299051, 0.018999531409922644, 0.012005447108487288, 0.010237680718454156, 0.014826797714285897, 0.018815300307239623, 0.02195894468122316, 0.020750049483783867, 0.01806845490253258, 0.021438332382371566, 0.022598045260292354, 0.01888029660014886, 0.01598566813871471, 0.010468418693808887, 0.014099314531370845, 0.01261962927548542, 0.010099358085826824, 0.01718173733540672, 0.014171558116309883, 0.011107576977178225, 0.012232560468102272, 0.014331779328300744, 0.014305230640952111, 0.01592454985070691, 0.0119767855692633, 0.01528729941509983, 0.009429366080321846, 0.014350313120740145, 0.018907482288705944, 0.016226996087385013, 0.010502783944399385, 0.011145388918587434, 0.012858468872563533, 0.018214439363678517, 0.014672076157746813, 0.016827548152361517, 0.007897508953885057, 0.012973222732900221, 0.012511063481454022, 0.009627924325543397, 0.013408213807861488, 0.013431413989766184, 0.011908183658542069, 0.01230783646033243, 0.010501855073743356, 0.01089422462354552, 0.014636396735717488, 0.012581067857758166, 0.02191967219721032, 0.010875811019645779, 0.009216276410913502, 0.01220411842070175, 0.014219673428703705, 0.009389087644581713, 0.014521270047429508, 0.017350536746836114, 0.010051121901809791, 0.014706686206075625, 0.012657353530658201, 0.011185761815504713, 0.01380830153632134, 0.01431146743668885, 0.014072348881534318, 0.01500378498689006, 0.014218348742676057, 0.009383866293673628, 0.011681019516920233, 0.011980483285255743, 0.013374346618106946, 0.012014845488382674, 0.014847027192688394, 0.015230413234592842, 0.006951613850801475, 0.0123016915070897, 0.018289676119163503, 0.018557181832103847, 0.016030600974585956, 0.006807659125966548, 0.01476530927088501, 0.009519656241197017, 0.012956091077357247, 0.010901769551524589, 0.013957095236319507, 0.02670492620742911, 0.012243533976084705, 0.012400186093093278, 0.008607504623606323, 0.011897986310775939, 0.017151460663585402, 0.011091397493238425, 0.01099080160967694, 0.014744771618006771, 0.014074365620550994, 0.015372154057342465, 0.012175806860329593, 0.00954390960648355, 0.009300342792623336, 0.013076156844960794, 0.015313831898850386, 0.009707525030234901, 0.012047402714339102, 0.013008837167381177, 0.015413997857979625, 0.016342444602389424, 0.011806530461640992, 0.022388526695976307, 0.008965372973418902, 0.017358653964188942, 0.016800303145183222, 0.02097057185535936, 0.005807861951451261, 0.01154698997202195, 0.011726523003314894, 0.011069728693439666, 0.014474256647281676, 0.015603087641588185, 0.007015954880560743, 0.0141827361575478, 0.010548869246585862, 0.0140820957618468, 0.009758165300924883, 0.01610059761697125], "accuracy_valid": [0.25985681005271083, 0.3501829583960843, 0.4734283814947289, 0.45105862904743976, 0.5601321300828314, 0.5052078430911144, 0.5401008330195783, 0.4557473056287651, 0.34009083207831325, 0.4606918886483434, 0.4414327230798193, 0.5065506165286144, 0.6248735175075302, 0.5925734186746988, 0.4989116622740964, 0.6387292333396084, 0.5050460631588856, 0.7027161379894578, 0.5134792098079819, 0.6296445547816265, 0.6517907567771084, 0.7157585419804217, 0.6837540592055723, 0.7245887848268072, 0.6962567065135542, 0.5682505412274097, 0.6699792333396084, 0.6985157426581325, 0.5641913356551205, 0.7416080336972892, 0.7262771790286144, 0.7587787674134037, 0.7973044521837349, 0.5886980539344879, 0.606165874435241, 0.6899281697100903, 0.7657176557793675, 0.7527267272213856, 0.6676496023155121, 0.7480159897402108, 0.7389724915286144, 0.7515369093561747, 0.7942732845444277, 0.7854327466114458, 0.6966832172439759, 0.7910891613328314, 0.7954733974962349, 0.8198786356362951, 0.6279767507530121, 0.7937850032944277, 0.8211390483810241, 0.6940285556287651, 0.6426354833396084, 0.7699489363704819, 0.7784953289721386, 0.7684649731739458, 0.8047919215926205, 0.7425845961972892, 0.8099703501506024, 0.7970397213855422, 0.6477212561182228, 0.7290142013365963, 0.5904585137424698, 0.796532320689006, 0.7831751811935241, 0.8085966914533133, 0.8058390789721386, 0.7756156461784638, 0.7828692700489458, 0.6872132083019578, 0.6543645284262049, 0.8509065559111446, 0.6643948842243976, 0.7481189406061747, 0.7124332290097892, 0.8014254282756024, 0.8168577630835843, 0.7954322171498494, 0.5819135918674698, 0.8088305369917168, 0.8124514660203314, 0.8186167521649097, 0.6422780967620482, 0.6267266330948795, 0.8393289956701807, 0.784069383000753, 0.7435405685240963, 0.8224009318524097, 0.6028273249246988, 0.7485969267695783, 0.8163180064006024, 0.7830222256212349, 0.7849753506212349, 0.7275067065135542, 0.7230930558170181, 0.8052699077560241, 0.6625535344503012, 0.7908656108810241, 0.8278235010353916, 0.7524825865963856, 0.7386974656438253, 0.8219641260353916, 0.7276920180722892, 0.6133577277861446, 0.8180475809487951, 0.5695933146649097, 0.8034182628953314, 0.7193706466490963, 0.6631741810993976, 0.6359936817582832, 0.8406717691076807, 0.7933070171310241, 0.8001723691641567, 0.8011092220444277, 0.8295839608433735, 0.4904579254518072, 0.8214140742658133, 0.6374879400414157, 0.8284529720444277, 0.6724206395896084, 0.7632453642695783, 0.6884942112198795], "seed": 187798735, "model": "residualv3", "loss_std": [0.2462858408689499, 0.14708490669727325, 0.15001514554023743, 0.1409444361925125, 0.13962359726428986, 0.13484744727611542, 0.1337793618440628, 0.1335887908935547, 0.13122393190860748, 0.13158053159713745, 0.12876105308532715, 0.12788096070289612, 0.12494838982820511, 0.12632513046264648, 0.1242259070277214, 0.1214459016919136, 0.11856463551521301, 0.12353049218654633, 0.1177663505077362, 0.11702742427587509, 0.11656299978494644, 0.11993219703435898, 0.11769033223390579, 0.11608835309743881, 0.1177196279168129, 0.11317190527915955, 0.11656282842159271, 0.10974418371915817, 0.1091381162405014, 0.1151324063539505, 0.11388128250837326, 0.1159415915608406, 0.1124560758471489, 0.10914318263530731, 0.11046352982521057, 0.10954353958368301, 0.10620298981666565, 0.10924017429351807, 0.10941208899021149, 0.10716886073350906, 0.10523628443479538, 0.10342002660036087, 0.10586719214916229, 0.1044037863612175, 0.10547041893005371, 0.10250066965818405, 0.10544722527265549, 0.10438884794712067, 0.10286930203437805, 0.10295557975769043, 0.10444726049900055, 0.1075255423784256, 0.10305299609899521, 0.10399703681468964, 0.10319475829601288, 0.10291343182325363, 0.10288186371326447, 0.10021830350160599, 0.10336076468229294, 0.10261816531419754, 0.10030696541070938, 0.10086160898208618, 0.10390358418226242, 0.1026102676987648, 0.09863154590129852, 0.10197404026985168, 0.09711436927318573, 0.09791727364063263, 0.09968076646327972, 0.09650730341672897, 0.10329411178827286, 0.10109963268041611, 0.09875239431858063, 0.09661618620157242, 0.10368888825178146, 0.09920427203178406, 0.10099060833454132, 0.09868808835744858, 0.09929756075143814, 0.10420987010002136, 0.10189647227525711, 0.0994606465101242, 0.09932117164134979, 0.09987912327051163, 0.09908883273601532, 0.09899649024009705, 0.1003410816192627, 0.09809474647045135, 0.10254254937171936, 0.09617126733064651, 0.09778498113155365, 0.09637574106454849, 0.09633275866508484, 0.09739591926336288, 0.09748141467571259, 0.10064207017421722, 0.09319581091403961, 0.0971720963716507, 0.0964144840836525, 0.09688156843185425, 0.09862633794546127, 0.09749902784824371, 0.09651287645101547, 0.09821953624486923, 0.09760721027851105, 0.09382017701864243, 0.09844692796468735, 0.09810490906238556, 0.09477487206459045, 0.0940290093421936, 0.09675347059965134, 0.09678749740123749, 0.09323103725910187, 0.0958271473646164, 0.09586957842111588, 0.0961572602391243, 0.09720047563314438, 0.09662976861000061, 0.09640777111053467, 0.09542383253574371, 0.09482303261756897, 0.09573298692703247]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:28 2016", "state": "available"}], "summary": "e2b7255a4a36a986945f1556e92cdad9"}