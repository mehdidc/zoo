{"content": {"hp_model": {"f0": 32, "f1": 16, "f2": 16, "f3": 32, "nonlin": "rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.018276420854086317, 0.013466381181328748, 0.011347561145681185, 0.014993938744749462, 0.014062282920940385, 0.014472041100355328, 0.013513464782601205, 0.018675598185142212, 0.010191447670281602, 0.010685659739461253, 0.012065184540140099, 0.007554605042544702, 0.01323465691875558, 0.012340412968762685, 0.011939294049375904, 0.012425779161281986, 0.010134875353412705, 0.011851105908178445, 0.011493001433384808, 0.01061389823173271, 0.011126011191104403, 0.009855626180518594, 0.00791447401036231, 0.010807147660168824, 0.009136225964425307, 0.010637485097159586, 0.010511443821049479, 0.012533027933727585, 0.014324110466459355, 0.011795028172739098, 0.011320329727398559, 0.009735619381210562, 0.013895374393463705, 0.011245738825683586, 0.015820181568580897, 0.01223102613955504, 0.011545285472067514, 0.011728193693989149, 0.0134711489710318, 0.01333643514099245, 0.013943231805342004, 0.013359569892268976, 0.01130129751933804, 0.012791369611862426, 0.011852954668538387, 0.011769325467495177, 0.01261454179301512, 0.010552302851298262, 0.013527138086704681, 0.012763239375391216, 0.012192061680327269, 0.013524593702571214, 0.013550245754645862, 0.01229799776718023, 0.012561459671216836, 0.011803389787418654, 0.012789874330999663, 0.013716381192363673, 0.01252848082377248, 0.011535306986715356, 0.011924744241509043, 0.011886952566178985, 0.011668927233044487, 0.011589142305248339, 0.01123852894889399, 0.011300724365836515, 0.011901976393035188, 0.010832196366763634, 0.011031629188343078, 0.011661664601361263, 0.010592882683717853, 0.011417747203035468, 0.01086741014952873, 0.010853036307930587, 0.011113547805460573, 0.011134171292656085, 0.011251495230307756, 0.010009527380005033, 0.010773440384985362, 0.011236414412588706, 0.011850532058555187, 0.011618097173163845, 0.010997283120635271, 0.01178722657495262, 0.011963555272283854, 0.010949084851217093, 0.011675347329767594, 0.011670302576954084, 0.010461723556204984, 0.011493364380419802, 0.011199728244315312, 0.0115244384036016, 0.011086182620547346, 0.011284948314032851, 0.011758822926830073, 0.0119298615336797, 0.01206021829034575, 0.010960186506274217, 0.01175964892583661, 0.011340896746889977, 0.011778128555714005, 0.01081016378090194, 0.010541764463334418, 0.012360786123432017, 0.011677000242535119, 0.01133339852649503, 0.01149748745205417, 0.011449582651388972, 0.010469633474949211, 0.011323953370699146, 0.010608821861770259, 0.01165903629371009, 0.011744210138993417, 0.011778044719826688, 0.011729349555335526, 0.011566648610817874, 0.011226850287347883, 0.011281766068544512, 0.010940578026714698, 0.010398148546343678], "moving_avg_accuracy_train": [0.03636498491717422, 0.08943423361653283, 0.14727892539192272, 0.2050487401478578, 0.2604565681506523, 0.3120159972471319, 0.3585310905768207, 0.3992510618474424, 0.43867003659085435, 0.4753909963753828, 0.5097603284123554, 0.5420595182086245, 0.5708896231372544, 0.5974222585360152, 0.622168802754396, 0.6448870490247297, 0.6662913238310977, 0.6859084135317827, 0.7050377223147247, 0.7229910281967167, 0.7392421175893465, 0.7542379408987028, 0.7685105652865919, 0.7818884223618917, 0.7939842252034526, 0.805170391957286, 0.8156842264118896, 0.8254372489245859, 0.8345196718265164, 0.8427868583906348, 0.8503224771554381, 0.857688218492589, 0.8644148977507498, 0.8704014437187996, 0.8762636293983685, 0.8816814666361803, 0.8866156488704489, 0.8912329439467207, 0.8955094893510981, 0.8994979251924279, 0.9034733840055494, 0.9070070830611591, 0.9101851591600358, 0.9132941104251877, 0.9159875709162145, 0.9187140268010142, 0.9211282735187534, 0.9233359367480427, 0.9252972930663171, 0.9272903783360974, 0.9290632287396139, 0.9307749433967987, 0.9323991558965892, 0.9338796925321518, 0.9352284875946435, 0.9365283976080196, 0.9377401332498109, 0.9389259903797947, 0.939995550896771, 0.9410070916335059, 0.9420057979025105, 0.9429696656136437, 0.94384880834653, 0.9446888649311276, 0.9455053336774943, 0.9462007361659187, 0.9468311405566635, 0.9474799207654859, 0.9481312162200835, 0.9485918240935071, 0.9490738365438832, 0.9495401637837363, 0.9499622555460511, 0.9503722929690207, 0.9507506632937502, 0.9510331039145875, 0.951324430756656, 0.9515982867073839, 0.9518749839975629, 0.9521658642372953, 0.9524113804113878, 0.9526068043799851, 0.9527780356541036, 0.9529460586448486, 0.9531368429150998, 0.9532875863702214, 0.9534442178679352, 0.953585150167059, 0.9536933880457943, 0.9538280405664271, 0.9539399272397586, 0.9540359388993191, 0.954147962078647, 0.9542208451055092, 0.9543399182523041, 0.9544215434963336, 0.9544973313647697, 0.9545399638094573, 0.9546201856882476, 0.9546947105279684, 0.9547617468348985, 0.9548058034694689, 0.9548315035477251, 0.9548453330229174, 0.954908968873219, 0.9549545793456239, 0.9550049293660264, 0.9550525695331982, 0.9550698690467481, 0.9550923780065528, 0.9551242978632434, 0.9551251239485506, 0.9551142777300984, 0.9551300567215772, 0.9551768459460603, 0.9551840429671336, 0.9551835808884896, 0.9551901044153199, 0.9551750492501814, 0.955161463552738], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 895362224, "moving_var_accuracy_train": [0.011901709152236782, 0.03605864465464249, 0.06256685548848698, 0.08634633341205378, 0.10534194670673373, 0.11873312459485454, 0.12633269730259603, 0.12862247211485875, 0.12974492503176857, 0.1289062925160643, 0.12664692212646692, 0.12337136886727894, 0.11851480653231337, 0.11299915254991416, 0.10721076035169322, 0.1011347527389197, 0.09514456428490639, 0.08909357973133825, 0.08347759584882274, 0.07803072699277175, 0.07260453545151982, 0.06736795435689673, 0.06246452918346694, 0.057828779804464596, 0.05336267784145536, 0.04915258299711094, 0.04523219113184914, 0.04156506505186266, 0.038150972198601994, 0.03495099234191525, 0.03196696305923982, 0.029258554062328163, 0.026739932580674884, 0.024388487916255558, 0.02225892611310569, 0.020297210144813897, 0.018486604519221256, 0.016829818791691407, 0.015311436477883578, 0.013923461414238858, 0.012673353727788406, 0.011518401616150118, 0.010457462963747369, 0.009498706869094434, 0.008614128746935493, 0.007819617927467775, 0.007090113419648025, 0.006424966070088827, 0.0058170917305450575, 0.005271134057524087, 0.004772307638750917, 0.004321446578484417, 0.003913044516836254, 0.003541467963715816, 0.003203694400429653, 0.0028985328547725692, 0.0026218942986855976, 0.0023723611830116396, 0.0021454207020057455, 0.001940087563763838, 0.0017550555352931947, 0.0015879113504449621, 0.001436076242903547, 0.0012988198742011252, 0.001174937477705156, 0.0010617959915228026, 0.000959193079633355, 0.0008670620135042562, 0.000784173484076446, 0.0007076655721863394, 0.0006389900389885639, 0.0005770481849413687, 0.0005209468195495582, 0.0004703653137887227, 0.0004246172593335739, 0.00038287348773890687, 0.00034534998092520196, 0.00031148995656842356, 0.00028103001342511246, 0.00025368851390740254, 0.0002288621662423317, 0.00020631966436561935, 0.00018595157927218367, 0.00016761050687373553, 0.00015117704392633957, 0.0001362638518370637, 0.00012285826748804232, 0.00011075119795566496, 9.978151710563639e-05, 8.996654710688775e-05, 8.108256004522158e-05, 7.305726818964347e-05, 6.586448410503985e-05, 5.9325843114977145e-05, 5.352086453206834e-05, 4.822874220302734e-05, 4.345756219174333e-05, 3.912816370062943e-05, 3.527326727909636e-05, 3.1795926116805474e-05, 2.8656778303146244e-05, 2.5808569356278592e-05, 2.3233656866852086e-05, 2.091201246962376e-05, 1.8857256915653695e-05, 1.6990254060825348e-05, 1.5314044775733644e-05, 1.380306656791371e-05, 1.2425453369643902e-05, 1.1187467912122924e-05, 1.0077891016170973e-05, 9.07010805630629e-06, 8.164156014768101e-06, 7.349981202440115e-06, 6.634686165945685e-06, 5.971683723362068e-06, 5.3745172726759195e-06, 4.837448553029074e-06, 4.355743619702291e-06, 3.921830398307271e-06], "duration": 56721.708122, "accuracy_train": [0.3636498491717423, 0.5670574719107604, 0.6678811513704319, 0.7249770729512736, 0.7591270201758029, 0.7760508591154485, 0.7771669305440199, 0.7657308032830381, 0.7934408092815615, 0.8058796344361389, 0.8190843167451088, 0.8327522263750462, 0.8303605674949243, 0.8362159771248615, 0.8448877007198228, 0.8493512654577334, 0.8589297970884091, 0.8624622208379475, 0.8772015013612033, 0.8845707811346438, 0.8855019221230158, 0.8892003506829088, 0.8969641847775931, 0.9022891360395902, 0.902846450777501, 0.9058458927417867, 0.9103087365033223, 0.913214451538852, 0.9162614779438908, 0.9171915374677003, 0.9181430460386674, 0.9239798905269472, 0.9249550110741971, 0.9242803574312477, 0.9290233005144887, 0.9304420017764857, 0.9310232889788667, 0.9327885996331673, 0.9339983979904946, 0.9353938477643964, 0.9392525133236435, 0.9388103745616464, 0.9387878440499261, 0.9412746718115541, 0.9402287153354559, 0.9432521297642118, 0.9428564939784054, 0.9432049058116464, 0.9429494999307864, 0.9452281457641197, 0.9450188823712625, 0.9461803753114618, 0.9470170683947029, 0.947204522252215, 0.947367643157069, 0.9482275877284054, 0.9486457540259321, 0.9495987045496493, 0.949621595549557, 0.9501109582641197, 0.9509941543235512, 0.9516444750138427, 0.9517610929425065, 0.9522493741925065, 0.9528535523947952, 0.9524593585617387, 0.9525047800733666, 0.9533189426448875, 0.9539928753114618, 0.952737294954319, 0.9534119485972684, 0.9537371089424143, 0.9537610814068845, 0.9540626297757475, 0.9541559962163161, 0.9535750695021227, 0.9539463723352714, 0.954062990263935, 0.9543652596091732, 0.9547837863948875, 0.9546210259782208, 0.9543656200973607, 0.9543191171211702, 0.9544582655615541, 0.9548539013473607, 0.9546442774663161, 0.9548539013473607, 0.9548535408591732, 0.9546675289544113, 0.9550399132521227, 0.9549469072997416, 0.9549000438353636, 0.9551561706925988, 0.9548767923472684, 0.9554115765734589, 0.9551561706925988, 0.9551794221806941, 0.9549236558116464, 0.9553421825973607, 0.9553654340854559, 0.9553650735972684, 0.9552023131806018, 0.9550628042520304, 0.9549697982996493, 0.9554816915259321, 0.9553650735972684, 0.9554580795496493, 0.9554813310377446, 0.955225564668697, 0.9552949586447952, 0.9554115765734589, 0.9551325587163161, 0.9550166617640274, 0.9552720676448875, 0.9555979489664084, 0.9552488161567922, 0.9551794221806941, 0.9552488161567922, 0.955039552763935, 0.9550391922757475], "end": "2016-01-24 03:14:44.019000", "learning_rate_per_epoch": [0.002068033441901207, 0.001930150669068098, 0.0018014610977843404, 0.0016813516849651933, 0.0015692503657191992, 0.001464623142965138, 0.0013669717591255903, 0.0012758311349898577, 0.0011907671578228474, 0.0011113747023046017, 0.0010372756514698267, 0.0009681170340627432, 0.0009035694529302418, 0.0008433254552073777, 0.0007870981353335083, 0.0007346196798607707, 0.0006856401450932026, 0.0006399262347258627, 0.0005972601938992739, 0.000557438877876848, 0.0005202725296840072, 0.00048558422713540494, 0.0004532086895778775, 0.0004229917540214956, 0.0003947894729208201, 0.0003684675320982933, 0.00034390055225230753, 0.00032097153598442674, 0.00029957128572277725, 0.00027959785074926913, 0.00026095611974596977, 0.00024355729692615569, 0.00022731850913260132, 0.00021216242748778313, 0.00019801684538833797, 0.00018481440201867372, 0.00017249220400117338, 0.00016099156346172094, 0.00015025772154331207, 0.00014023952826391906, 0.0001308892824454233, 0.00012216245522722602, 0.00011401747178751975, 0.0001064155439962633, 9.932045941241086e-05, 9.269842848880216e-05, 8.651790994917974e-05, 8.07494725449942e-05, 7.53656349843368e-05, 7.034075679257512e-05, 6.565090006915852e-05, 6.127373490016907e-05, 5.718840839108452e-05, 5.337546463124454e-05, 4.981674283044413e-05, 4.649529364542104e-05, 4.3395295506343246e-05, 4.0501985495211557e-05, 3.780158294830471e-05, 3.528122397256084e-05, 3.2928906875895336e-05, 3.0733426683582366e-05, 2.868432602554094e-05, 2.6771846023621038e-05, 2.49868771788897e-05, 2.3320917534874752e-05, 2.1766034478787333e-05, 2.03148192667868e-05, 1.896036155812908e-05, 1.76962112163892e-05, 1.6516345567652024e-05, 1.5415145753649995e-05, 1.4387366718437988e-05, 1.3428112652036361e-05, 1.2532815162558109e-05, 1.1697210538841318e-05, 1.091731792257633e-05, 1.0189423846895806e-05, 9.510060408501886e-06, 8.875992534740362e-06, 8.28420070320135e-06, 7.731865480309352e-06, 7.2163561526394915e-06, 6.735217539244331e-06, 6.286158168222755e-06, 5.867038908036193e-06, 5.475863872561604e-06, 5.110769507155055e-06, 4.770017312694108e-06, 4.451984295883449e-06, 4.155155693297274e-06, 3.878117695421679e-06, 3.61955062544439e-06, 3.3782230275392067e-06, 3.1529855277767638e-06, 2.94276537715632e-06, 2.746561449384899e-06, 2.563439011282753e-06, 2.3925258574308828e-06, 2.233008217444876e-06, 2.084125981127727e-06, 1.945170424733078e-06, 1.8154794361180393e-06, 1.6944353546932689e-06, 1.5814616745046806e-06, 1.476020315749338e-06, 1.3776091236650245e-06, 1.2857592537329765e-06, 1.2000333526884788e-06, 1.1200230574104353e-06, 1.045347403305641e-06, 9.75650550572027e-07, 9.106006473302841e-07, 8.49887840104202e-07, 7.932229664220358e-07, 7.403361337310344e-07, 6.909754688422254e-07, 6.449058105317818e-07, 6.019078000463196e-07, 5.617765737042646e-07], "accuracy_valid": [0.35679681617093373, 0.5653105586408133, 0.662085843373494, 0.710437452936747, 0.7420139542545181, 0.7592876388365963, 0.7611083984375, 0.7475880082831325, 0.7679546310240963, 0.7737125258847892, 0.7837840620293675, 0.795421922063253, 0.7936011624623494, 0.7993693524096386, 0.8053919780685241, 0.8055140483810241, 0.8163988963667168, 0.8188505977033133, 0.8258497858621988, 0.8333063700112951, 0.8331945947853916, 0.8335916909826807, 0.8361036921121988, 0.8409262048192772, 0.8412615304969879, 0.8396540262612951, 0.8450971856174698, 0.8442118081701807, 0.8451780755835843, 0.8470400155308735, 0.8483621987951807, 0.8515257318335843, 0.8483210184487951, 0.8465208490210843, 0.8505697595067772, 0.8503256188817772, 0.8530111657567772, 0.8539980233433735, 0.8534788568335843, 0.8554628670933735, 0.8559305581701807, 0.8560938088290663, 0.8543539391942772, 0.8571924416415663, 0.8571924416415663, 0.8588911309299698, 0.8560629235692772, 0.858555805252259, 0.8574365822665663, 0.8572027367281627, 0.858799945877259, 0.8586881706513554, 0.858311664627259, 0.8579351586031627, 0.8582910744540663, 0.8580469338290663, 0.8587896507906627, 0.858311664627259, 0.859410297439759, 0.8587896507906627, 0.8579248635165663, 0.8592985222138554, 0.858311664627259, 0.857579242752259, 0.8581587090549698, 0.8601530144013554, 0.8600412391754518, 0.8589117211031627, 0.8585249199924698, 0.8610280967620482, 0.8601530144013554, 0.8585352150790663, 0.859410297439759, 0.858555805252259, 0.8594000023531627, 0.8597868034638554, 0.8599088737763554, 0.8590234963290663, 0.8582910744540663, 0.8595426628388554, 0.859654438064759, 0.8581690041415663, 0.8587793557040663, 0.859532367752259, 0.858677875564759, 0.859288227127259, 0.8596441429781627, 0.8584234398531627, 0.8590337914156627, 0.8590337914156627, 0.8587896507906627, 0.8590337914156627, 0.858922016189759, 0.8589117211031627, 0.8586675804781627, 0.8598882836031627, 0.8590337914156627, 0.8591558617281627, 0.859654438064759, 0.8595220726656627, 0.8600103539156627, 0.8589117211031627, 0.8587896507906627, 0.8591558617281627, 0.8597662132906627, 0.8596441429781627, 0.8595220726656627, 0.8591558617281627, 0.859532367752259, 0.8591558617281627, 0.8590337914156627, 0.8589117211031627, 0.8592779320406627, 0.8591558617281627, 0.8594000023531627, 0.8589117211031627, 0.8585455101656627, 0.8591558617281627, 0.8594000023531627, 0.858922016189759], "accuracy_test": 0.8534817442602041, "start": "2016-01-23 11:29:22.311000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0], "accuracy_train_last": 0.9550391922757475, "batch_size_eval": 1024, "accuracy_train_std": [0.015970608650364336, 0.018593951852570595, 0.020121539583129795, 0.019946226109576252, 0.017422910602070695, 0.01799259498339588, 0.016279429217455878, 0.015834989802015115, 0.01687389254979558, 0.015507253966567155, 0.015442549559086846, 0.014891929607521176, 0.016324654381280453, 0.01614712611510587, 0.015728042123536654, 0.014129427503564674, 0.01552585397600022, 0.015513484563966336, 0.014840624060864973, 0.01347855425351455, 0.011962520920375485, 0.01320156130618687, 0.012905322193100989, 0.012690061822224012, 0.012205693269144748, 0.01249796321469313, 0.012828411959292515, 0.012397317152206488, 0.011764615413846298, 0.012177884703735773, 0.01257468410206128, 0.011181134092281313, 0.010388692917410918, 0.010750829143000771, 0.010452555033936176, 0.01018415861241478, 0.010336294058510267, 0.010040170238862013, 0.009516982745857898, 0.010056799430144934, 0.009779315945165004, 0.010010580830425185, 0.00940430969993461, 0.00893544981277729, 0.008768429784260368, 0.008293043448611566, 0.00911679571719846, 0.00900678280180352, 0.008184643701453295, 0.007841189891486198, 0.007849173333647173, 0.00817196817897635, 0.007963708158489525, 0.008091532690770096, 0.007888846189076348, 0.007723124482400745, 0.007952136871106305, 0.007557236722010251, 0.007798639392886673, 0.007449568534231567, 0.007269901633021, 0.007316431398682825, 0.007378918365262689, 0.00720950988119796, 0.006817257526404733, 0.007194607760369782, 0.007217369043275035, 0.006804623258756222, 0.0067756187233464936, 0.006909273053692713, 0.006982541828973776, 0.00695369208659965, 0.006922966668170428, 0.006844726000152339, 0.006749076723609702, 0.00637822563159505, 0.006683605711692235, 0.0066166760830867885, 0.006616681466908508, 0.0066107845068466365, 0.006220792815708085, 0.006569618238826878, 0.0067114987726725125, 0.006466484678168057, 0.006519559092300771, 0.006361397802574889, 0.006285490324606734, 0.006211625098092274, 0.006368291463686645, 0.006278532116776494, 0.006320535131335067, 0.0063218196253090695, 0.006250978849651585, 0.006188618961532997, 0.006073603887892442, 0.006240071905894469, 0.006230633574953277, 0.006332571943574926, 0.00638964974013766, 0.006180902248161992, 0.006116948449262608, 0.006252690528849382, 0.006311730018895118, 0.00631081895499556, 0.006143062928070611, 0.005874574224489079, 0.0062113754564616045, 0.006306062287453119, 0.006232178692326255, 0.006417279334314906, 0.00643308303948716, 0.006175128960316756, 0.006200102184798592, 0.0062708627515702395, 0.006198757917521957, 0.00631663369296535, 0.006175726581143703, 0.006211512085894282, 0.006324209710212502, 0.006225107485878049], "accuracy_test_std": 0.00549270378125209, "error_valid": [0.6432031838290663, 0.43468944135918675, 0.33791415662650603, 0.289562547063253, 0.2579860457454819, 0.24071236116340367, 0.2388916015625, 0.25241199171686746, 0.23204536897590367, 0.22628747411521077, 0.21621593797063254, 0.20457807793674698, 0.20639883753765065, 0.20063064759036142, 0.19460802193147586, 0.19448595161897586, 0.1836011036332832, 0.18114940229668675, 0.17415021413780118, 0.16669362998870485, 0.1668054052146084, 0.1664083090173193, 0.16389630788780118, 0.15907379518072284, 0.15873846950301207, 0.16034597373870485, 0.15490281438253017, 0.1557881918298193, 0.15482192441641573, 0.1529599844691265, 0.1516378012048193, 0.14847426816641573, 0.15167898155120485, 0.15347915097891573, 0.14943024049322284, 0.14967438111822284, 0.14698883424322284, 0.1460019766566265, 0.14652114316641573, 0.1445371329066265, 0.1440694418298193, 0.14390619117093373, 0.14564606080572284, 0.14280755835843373, 0.14280755835843373, 0.14110886907003017, 0.14393707643072284, 0.14144419474774095, 0.14256341773343373, 0.14279726327183728, 0.14120005412274095, 0.1413118293486446, 0.14168833537274095, 0.14206484139683728, 0.14170892554593373, 0.14195306617093373, 0.14121034920933728, 0.14168833537274095, 0.14058970256024095, 0.14121034920933728, 0.14207513648343373, 0.1407014777861446, 0.14168833537274095, 0.14242075724774095, 0.14184129094503017, 0.1398469855986446, 0.13995876082454817, 0.14108827889683728, 0.14147508000753017, 0.13897190323795183, 0.1398469855986446, 0.14146478492093373, 0.14058970256024095, 0.14144419474774095, 0.14059999764683728, 0.1402131965361446, 0.1400911262236446, 0.14097650367093373, 0.14170892554593373, 0.1404573371611446, 0.14034556193524095, 0.14183099585843373, 0.14122064429593373, 0.14046763224774095, 0.14132212443524095, 0.14071177287274095, 0.14035585702183728, 0.14157656014683728, 0.14096620858433728, 0.14096620858433728, 0.14121034920933728, 0.14096620858433728, 0.14107798381024095, 0.14108827889683728, 0.14133241952183728, 0.14011171639683728, 0.14096620858433728, 0.14084413827183728, 0.14034556193524095, 0.14047792733433728, 0.13998964608433728, 0.14108827889683728, 0.14121034920933728, 0.14084413827183728, 0.14023378670933728, 0.14035585702183728, 0.14047792733433728, 0.14084413827183728, 0.14046763224774095, 0.14084413827183728, 0.14096620858433728, 0.14108827889683728, 0.14072206795933728, 0.14084413827183728, 0.14059999764683728, 0.14108827889683728, 0.14145448983433728, 0.14084413827183728, 0.14059999764683728, 0.14107798381024095], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-3, 3], "momentum": 0.9321815675236813, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.002215766030421673, "patience_threshold": 1, "do_flip": true, "batch_size": 128, "optimization": "adam", "nb_data_augmentation": 4, "learning_rate_decay_method": "exp", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 4.40102538034775e-09, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.06667335350076721}, "accuracy_valid_max": 0.8610280967620482, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import os\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-3, 3)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.858922016189759, "loss_train": [1.4758634567260742, 1.104568362236023, 0.9232190847396851, 0.806786060333252, 0.7280561923980713, 0.6675531268119812, 0.6200788021087646, 0.5844538807868958, 0.5523990392684937, 0.5269521474838257, 0.5021288990974426, 0.48315727710723877, 0.463030070066452, 0.44669193029403687, 0.42855578660964966, 0.41404852271080017, 0.40070515871047974, 0.3878544867038727, 0.37746143341064453, 0.36479994654655457, 0.35392695665359497, 0.34719061851501465, 0.3377346098423004, 0.3279821574687958, 0.32230687141418457, 0.3148503303527832, 0.30817264318466187, 0.3033003509044647, 0.2960161566734314, 0.2927979826927185, 0.28582051396369934, 0.2832702398300171, 0.27724868059158325, 0.27356407046318054, 0.26995810866355896, 0.2648821771144867, 0.262298047542572, 0.25884440541267395, 0.2561759948730469, 0.2535528242588043, 0.25052571296691895, 0.24721890687942505, 0.24465887248516083, 0.241654634475708, 0.24196195602416992, 0.23782366514205933, 0.23736920952796936, 0.23517651855945587, 0.2338322401046753, 0.2320958524942398, 0.23105715215206146, 0.22847309708595276, 0.2282731831073761, 0.22847001254558563, 0.2267940789461136, 0.22477790713310242, 0.22410890460014343, 0.22313609719276428, 0.22275036573410034, 0.22112853825092316, 0.22029849886894226, 0.2204102724790573, 0.21980170905590057, 0.21870256960391998, 0.217380553483963, 0.2174932062625885, 0.21630744636058807, 0.21586567163467407, 0.2167375385761261, 0.21435537934303284, 0.2162197232246399, 0.21396781504154205, 0.21406401693820953, 0.212522491812706, 0.21276791393756866, 0.21247757971286774, 0.21246154606342316, 0.21156840026378632, 0.21239176392555237, 0.21159055829048157, 0.2128130942583084, 0.21194861829280853, 0.21154217422008514, 0.21160297095775604, 0.2108302265405655, 0.2117397040128708, 0.21019449830055237, 0.21054960787296295, 0.21130576729774475, 0.20974506437778473, 0.21013465523719788, 0.21081551909446716, 0.21066197752952576, 0.20888149738311768, 0.20994645357131958, 0.21109303832054138, 0.209457129240036, 0.20939315855503082, 0.20982328057289124, 0.21028342843055725, 0.2086709439754486, 0.20985795557498932, 0.209244504570961, 0.20890381932258606, 0.2099144011735916, 0.2082311511039734, 0.20840923488140106, 0.20866884291172028, 0.20840491354465485, 0.20984794199466705, 0.2085549235343933, 0.20934481918811798, 0.2086227983236313, 0.20977909862995148, 0.2076794058084488, 0.20879776775836945, 0.20895977318286896, 0.20792098343372345, 0.20797857642173767, 0.20991602540016174], "accuracy_train_first": 0.3636498491717423, "model": "residualv3", "loss_std": [0.24056342244148254, 0.13713401556015015, 0.11182112991809845, 0.09945590049028397, 0.09102562069892883, 0.08585765957832336, 0.08278217911720276, 0.0787680447101593, 0.07743385434150696, 0.07461623102426529, 0.07356617599725723, 0.07277077436447144, 0.07100198417901993, 0.0699765533208847, 0.0688849464058876, 0.06757232546806335, 0.06565678864717484, 0.0660233125090599, 0.06483414024114609, 0.06298355013132095, 0.06142145395278931, 0.05939820781350136, 0.06053360924124718, 0.05830274522304535, 0.058858178555965424, 0.057026538997888565, 0.056665848940610886, 0.05582774430513382, 0.056320369243621826, 0.056037724018096924, 0.05518980696797371, 0.053993646055459976, 0.05429978296160698, 0.05216450244188309, 0.051662422716617584, 0.0513564758002758, 0.051073264330625534, 0.05059553682804108, 0.0494106151163578, 0.0484430156648159, 0.049715425819158554, 0.050425440073013306, 0.048190705478191376, 0.04900236800312996, 0.049610428512096405, 0.04855991527438164, 0.04854239895939827, 0.04764951765537262, 0.047171998769044876, 0.046059880405664444, 0.046324409544467926, 0.046974316239356995, 0.04649416729807854, 0.04597639665007591, 0.04631618782877922, 0.04698490723967552, 0.046381961554288864, 0.045230451971292496, 0.04549456387758255, 0.04653548449277878, 0.04573454335331917, 0.04595015570521355, 0.04509863257408142, 0.045519597828388214, 0.044384513050317764, 0.045375365763902664, 0.044906482100486755, 0.04556397348642349, 0.04469216614961624, 0.044121019542217255, 0.04483719915151596, 0.04461856558918953, 0.04534492269158363, 0.04447358474135399, 0.04518197104334831, 0.04525228217244148, 0.044460274279117584, 0.04470274969935417, 0.0437949113547802, 0.04562615603208542, 0.04404691234230995, 0.04338579997420311, 0.04553815349936485, 0.04511557146906853, 0.04382189363241196, 0.043915942311286926, 0.04484758898615837, 0.04573901742696762, 0.04444018751382828, 0.04469582065939903, 0.04509551823139191, 0.04484883323311806, 0.04325257986783981, 0.04483986645936966, 0.04518212005496025, 0.044955991208553314, 0.04311492666602135, 0.04238848388195038, 0.04389289766550064, 0.044001948088407516, 0.04441455006599426, 0.043736252933740616, 0.044569388031959534, 0.04374705255031586, 0.04409853741526604, 0.04374508187174797, 0.044618163257837296, 0.04396352916955948, 0.043111011385917664, 0.04414399340748787, 0.04481012374162674, 0.043356020003557205, 0.044483527541160583, 0.044974684715270996, 0.04363571107387543, 0.04314150661230087, 0.042846884578466415, 0.04445169121026993, 0.04371165856719017, 0.04299396276473999]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:09 2016", "state": "available"}], "summary": "63996d68c321e40136d8af891540e174"}