{"content": {"hp_model": {"f0": 16, "f1": 16, "f2": 32, "f3": 16, "nonlin": "very_leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.007520133600165852, 0.009275142061816226, 0.01488646644175678, 0.011810599516746975, 0.01691580104197994, 0.014559002340416405, 0.012733390610955294, 0.016663581405675656, 0.01638110211843394, 0.0165881722887194, 0.009697424106328673, 0.013087246671494525, 0.013331510142118962, 0.01275189830732235, 0.01269514544057288, 0.009498349344498853, 0.012267514684013622, 0.012718062396298763, 0.010979341625447786, 0.011342419042896786, 0.006593286957611427, 0.008728165722802479, 0.00862720371580257, 0.010129228259109061, 0.011368787357381288, 0.010279015398111983, 0.011851878663035605, 0.011457038816706995, 0.010976480277603106, 0.010656092114377737, 0.010368216541745509, 0.009641099986426822, 0.008997630190217881, 0.008889920630336239, 0.009309732264308694, 0.009422594356227528, 0.009838105166898868, 0.009724064619495725, 0.010142110549922725, 0.010220298270490057, 0.011571265998352405, 0.010633568841892024, 0.011133384470830461, 0.010866442678098004, 0.011258289402858251, 0.011002938520270338, 0.011673926721539439, 0.010508504471003953, 0.011562839131375752, 0.012579714798188793, 0.010637612794543564, 0.010488941646226043, 0.011357808657838558, 0.011117902494028454, 0.012670333876473316, 0.010736834847753155, 0.010902105357886152, 0.010455101921443367, 0.008793476401844035, 0.009313532943493588, 0.010290049596105463, 0.0098954297359942, 0.009486843039056104, 0.009501513434956673, 0.010259436648424816, 0.010117581808794143, 0.008753071181858023, 0.009762244290707208, 0.009720206827404711, 0.0098847253024086, 0.0099777391591325, 0.010041912674758898, 0.009744278680605158, 0.009671026132273603, 0.00977319885247592, 0.010579280134592995, 0.009478467039445812, 0.010141678449908632, 0.010161909817574458, 0.009824309524475723, 0.010719529939462775, 0.00955813916421532, 0.009535949277068658, 0.009999630934795665, 0.009980806538603216, 0.009087617869113359, 0.009684800027055024, 0.008465970720453003, 0.009882367278236055, 0.009942453331906851, 0.009658691830264457, 0.009846066708302008, 0.009261454782636727, 0.009387461843218764, 0.00954156574209028, 0.01012819636147257, 0.009309260482995702, 0.0102556593910949, 0.009403465109346863, 0.008676933170732988, 0.0092596033428475, 0.01035300310339643, 0.008831694714412114, 0.009275595756793452, 0.008100955608803447, 0.010217092672389733, 0.009932481563099905, 0.009768891836756197, 0.011189637038499234, 0.009120304124450536, 0.011029976046390904, 0.010132520574027153, 0.009727121376974533, 0.011002457165976916, 0.009692330921164737, 0.009318158148452787, 0.011029437626114375, 0.008795212494401954, 0.010495172609937218, 0.008958556194344252, 0.010910144063877159, 0.008472670160249022, 0.00926486589092641, 0.008865892467923266, 0.010256849706740534, 0.008926143355247138, 0.010700914980134008, 0.009879123952841715, 0.009441847845778095, 0.009376583167803652, 0.009325860798089525, 0.009826602677447128, 0.008974284879520471, 0.009296591049524683, 0.010037914469149364, 0.010408402167680025, 0.008619877116373732, 0.008880814098103778, 0.00972157351012615, 0.009353618846540578, 0.00952698256464019, 0.009014336834742996, 0.009453705451140304, 0.009976954795562041, 0.008929090401438033, 0.010050490582432124, 0.009436323317676533, 0.00946515937300845, 0.010148949930471895, 0.009027997211262769, 0.009658691830264457, 0.0104250300974421, 0.009666904065694195, 0.008958556194344252, 0.010228552650407462, 0.010202217000760953, 0.009709458920722367, 0.009844536590585352, 0.009785224471276588, 0.009766478936553821, 0.00919687184836871, 0.009790553783414008, 0.010488228407682019], "moving_avg_accuracy_train": [0.019128116059892945, 0.04099757535645071, 0.06300952734230804, 0.08408244339614479, 0.10386630393263774, 0.12305516985951054, 0.14158051325792642, 0.1593526130199429, 0.1763703880378545, 0.19256307480162163, 0.20768969806158497, 0.22179880754552428, 0.2350572948465274, 0.2473803421245177, 0.25893586209488245, 0.26980311288128755, 0.2800370065580905, 0.28953804632194735, 0.29832632753208377, 0.30643323802592226, 0.313908529977529, 0.32092675609107174, 0.32738502971945993, 0.33341131757784803, 0.33900242341350173, 0.34412964162032433, 0.34899053168263666, 0.3535490915923077, 0.3577145745288687, 0.3616379313813067, 0.3652270091711014, 0.36863614354261254, 0.37176009595076365, 0.3746669842192901, 0.37732972268597315, 0.37982384355598786, 0.3821614501449258, 0.3843327614392649, 0.38633580577798887, 0.3881478823268973, 0.38989264746376273, 0.3914955602679125, 0.3929242308987901, 0.39425185109633265, 0.3955025128455495, 0.3966118684269968, 0.3976544662776803, 0.3986555833611526, 0.3996054168612776, 0.4005277684245039, 0.4013228273551709, 0.4019850461654272, 0.4026856387422676, 0.40337658988165287, 0.4040610807296818, 0.40457728528937326, 0.4050860832692953, 0.40560434717381644, 0.4061127455129132, 0.4065934113109205, 0.40698662719464024, 0.4073311127482936, 0.40767134063228666, 0.4079775817766992, 0.40828110059238476, 0.4085588457264833, 0.40888325715789553, 0.4090985696330897, 0.40934114293694573, 0.40954321891756823, 0.40982961085010083, 0.41008743568701767, 0.41019388395570977, 0.41038970484516096, 0.4105729190920956, 0.41069127288932755, 0.410842005183036, 0.4109636773056977, 0.41110577034824536, 0.4112359792353477, 0.41131600090160625, 0.4114042964429056, 0.4114976772741134, 0.4116212475519623, 0.4116976556675209, 0.41179886690840084, 0.41184814039544004, 0.4119366643611563, 0.4119744832517295, 0.4119991836091885, 0.4120004154939784, 0.4120783261986411, 0.4121391452375994, 0.41218687087741457, 0.4121950909163804, 0.4122186928954788, 0.41225853586714356, 0.41230136998807043, 0.4123166331599906, 0.4122978539802042, 0.4123507071826821, 0.41238661327204584, 0.41242361509891107, 0.4124731206871189, 0.412524723260572, 0.41250838655882266, 0.41244249420462, 0.41243895860844737, 0.4124706898528537, 0.412443408352572, 0.4124328058951756, 0.4123327270752225, 0.41240763355628457, 0.4124006806761545, 0.4124292642673615, 0.4124526643506382, 0.4124644238303492, 0.4125308109335177, 0.4124812773323217, 0.41243673314006407, 0.4123966073182134, 0.4123744810202237, 0.4123173649710807, 0.4123659058768427, 0.4123887024015615, 0.41243475986189443, 0.41245292403928013, 0.4124809335917936, 0.41248056555215096, 0.4124081907521961, 0.4124522633286469, 0.4124454617200808, 0.4124672420580856, 0.4124658819741855, 0.4124948848331992, 0.41249544681822553, 0.41251222864641585, 0.4125157425965583, 0.4125560714838202, 0.41254815360615615, 0.412566640201982, 0.4125368472596724, 0.41254018844847995, 0.4125641218576925, 0.412534544700993, 0.412563728831392, 0.41258530820231326, 0.4126442211170856, 0.4126205849273039, 0.4126248168957677, 0.41262168626977525, 0.4126119293087722, 0.41257764350460224, 0.4125955423082118, 0.4126348666707369, 0.41271901462437194, 0.4126948745302902, 0.41266377575274105, 0.4126892652755659, 0.4125914062544692, 0.41261257908071114, 0.4126479106659955, 0.4126912987879803], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 568856360, "moving_var_accuracy_train": [0.0032929634160066105, 0.007268126323720123, 0.010902047963397308, 0.01380845328616603, 0.01595021819709605, 0.017669109557421948, 0.01899089373394283, 0.019934432130108152, 0.020547430916139635, 0.02085251576619099, 0.020826596770811667, 0.02053553982759858, 0.020064073214436474, 0.019424383340933084, 0.018683715382909267, 0.017878218101509934, 0.01703298950945191, 0.016142118367856225, 0.015223011510726529, 0.014292208339449856, 0.013365907413360679, 0.012472616152051936, 0.011600738221178955, 0.010767509707230458, 0.009972102916696745, 0.009211487923890427, 0.008502993401282367, 0.007839718277204668, 0.007211907682337232, 0.006629251475027656, 0.006082259641955674, 0.005578633452227273, 0.005108601814840082, 0.004673791628007344, 0.004270224050484191, 0.003899187395663959, 0.003558448297179378, 0.0032450348020937607, 0.0029566410014904305, 0.002690529494113321, 0.002448874393147379, 0.002227110918952006, 0.0020227697250005962, 0.0018363559310008428, 0.0016667977311993463, 0.0015111939863342054, 0.0013698576802050333, 0.0012418920309179102, 0.0011258224809277575, 0.0010208968244906549, 0.0009244962103706895, 0.0008359933931075355, 0.0007568115234252975, 0.000685427092375928, 0.000621101132627653, 0.0005613892236919041, 0.0005075801797820683, 0.00045923953907642744, 0.0004156418050095518, 0.00037615698099296264, 0.00033993285147455144, 0.0003070075989971791, 0.00027734863421487887, 0.0002504578235401709, 0.00022624115422943027, 0.00020431132004212604, 0.00018482737302939165, 0.00016676187088422077, 0.00015061526006549138, 0.00013592124637644323, 0.00012306730478997666, 0.00011135883712975969, 0.00010032493452195163, 9.063755345646549e-05, 8.187590525333732e-05, 7.381438331987654e-05, 6.663742700718835e-05, 6.0106921255366545e-05, 5.4277943024493837e-05, 4.90027379105684e-05, 4.416009532314865e-05, 3.981425071435384e-05, 3.5911305459651946e-05, 3.245760143579565e-05, 2.9264385093325117e-05, 2.6430140021516594e-05, 2.3808976908089958e-05, 2.149860764983613e-05, 1.9361619301210194e-05, 1.7430948340016603e-05, 1.568786716387616e-05, 1.4173711148597881e-05, 1.2789630633236428e-05, 1.1531167200174704e-05, 1.037865860152263e-05, 9.345806222126616e-06, 8.425512761433653e-06, 7.5994743425304605e-06, 6.841623588030993e-06, 6.160635147568956e-06, 5.569712781921605e-06, 5.0243447290100274e-06, 4.5342324728312694e-06, 4.1028664549223695e-06, 3.716545239712989e-06, 3.347292706158111e-06, 3.0516396566236488e-06, 2.7465881949239446e-06, 2.4809912222756857e-06, 2.239590622366687e-06, 2.016643269055593e-06, 1.9051208739789687e-06, 1.7651076147270244e-06, 1.5890319361332569e-06, 1.4374819376965275e-06, 1.2986618190031137e-06, 1.170040205370463e-06, 1.0927014120373323e-06, 1.0055134696605948e-06, 9.228197882695103e-07, 8.450285436552582e-07, 7.649318468542816e-07, 7.17798849796294e-07, 6.672249406064134e-07, 6.051795803990794e-07, 5.637532292300678e-07, 5.103473423679473e-07, 4.663734234191986e-07, 4.197373001558857e-07, 4.2490657515684406e-07, 3.9989744559622673e-07, 3.6032405794837693e-07, 3.2856110026598533e-07, 2.9572163869332465e-07, 2.7371996730271817e-07, 2.4635081301697477e-07, 2.2425039953197659e-07, 2.019364901892124e-07, 1.963806135003005e-07, 1.7730678723059456e-07, 1.6265189653459774e-07, 1.5437528158435394e-07, 1.3903822530974978e-07, 1.3028967546758905e-07, 1.2513398170666249e-07, 1.2028600474032849e-07, 1.1244842751054269e-07, 1.324401685022779e-07, 1.2422417685859716e-07, 1.1196294518644335e-07, 1.0085485803973904e-07, 9.162615682789126e-08, 9.304318845332131e-08, 8.662217414386418e-08, 9.187760612150374e-08, 1.4641774841807042e-07, 1.37020670856729e-07, 1.3202280945650948e-07, 1.2466797047539567e-07, 1.9838866551779033e-07, 1.8258439610563348e-07, 1.7556084476342792e-07, 1.7494752245139185e-07], "duration": 77450.753477, "accuracy_train": [0.1912811605989295, 0.23782270902547067, 0.261117095215024, 0.2737386878806755, 0.2819210487610742, 0.29575496320136585, 0.30830860384366926, 0.31930151087809155, 0.3295303631990587, 0.33829725567552604, 0.34382930740125506, 0.3487807929009782, 0.3543836805555556, 0.3582877676264304, 0.3629355418281654, 0.36760836995893315, 0.37214204964931713, 0.3750474041966593, 0.3774208584233112, 0.3793954324704688, 0.3811861575419897, 0.3840907911129568, 0.38550949237495385, 0.38764790830334067, 0.38932237593438535, 0.3902746054817276, 0.3927385422434478, 0.39457613077934667, 0.39520392095791806, 0.3969481430532484, 0.3975287092792544, 0.39931835288621265, 0.3998756676241233, 0.40082897863602807, 0.40129436888612036, 0.40227093138612036, 0.40319990944536727, 0.4038745630883167, 0.40436320482650423, 0.40445657126707274, 0.40559553369555185, 0.40592177550526026, 0.4057822665766888, 0.4062004328742156, 0.4067584685885013, 0.4065960686600222, 0.40703784693383166, 0.4076656371124031, 0.4081539183624031, 0.4088289324935401, 0.40847835773117386, 0.4079450154577335, 0.40899097193383166, 0.40959515013612036, 0.4102214983619417, 0.4092231263265965, 0.4096652650885936, 0.4102687223145072, 0.41068833056478404, 0.4109194034929863, 0.4105255701481174, 0.41043148273117386, 0.4107333915882244, 0.41073375207641194, 0.4110127699335548, 0.41105855193337026, 0.41180296004060535, 0.41103638190983754, 0.41152430267165, 0.4113619027431709, 0.4124071382428941, 0.4124078592192691, 0.41115191837393866, 0.4121520928502215, 0.4122218473145072, 0.4117564570644149, 0.41219859582641194, 0.41205872640965296, 0.41238460773117386, 0.4124078592192691, 0.4120361958979328, 0.41219895631459946, 0.41233810475498334, 0.4127333800526024, 0.4123853287075489, 0.41270976807631965, 0.41229160177879287, 0.4127333800526024, 0.4123148532668881, 0.41222148682631965, 0.4120115024570875, 0.41277952254060535, 0.4126865165882244, 0.41261640163575125, 0.41226907126707274, 0.4124311107073643, 0.41261712261212624, 0.41268687707641194, 0.41245400170727203, 0.41212884136212624, 0.41282638600498334, 0.41270976807631965, 0.41275663154069764, 0.41291867098098933, 0.41298914642165, 0.4123613562430786, 0.4118494630167958, 0.4124071382428941, 0.4127562710525101, 0.41219787485003695, 0.4123373837786084, 0.41143201769564414, 0.41308179188584343, 0.41233810475498334, 0.4126865165882244, 0.4126632651001292, 0.4125702591477482, 0.41312829486203395, 0.41203547492155773, 0.41203583540974525, 0.41203547492155773, 0.4121753443383167, 0.41180332052879287, 0.4128027740287006, 0.412593871124031, 0.41284927700489105, 0.41261640163575125, 0.4127330195644149, 0.41247725319536727, 0.4117568175526024, 0.41284891651670363, 0.4123842472429863, 0.4126632651001292, 0.41245364121908457, 0.41275591056432265, 0.4125005046834625, 0.4126632651001292, 0.4125473681478405, 0.4129190314691768, 0.4124768927071798, 0.4127330195644149, 0.41226871077888516, 0.4125702591477482, 0.41277952254060535, 0.41226835029069764, 0.41282638600498334, 0.41277952254060535, 0.41317443735003695, 0.4124078592192691, 0.4126629046119417, 0.41259351063584343, 0.41252411665974525, 0.41226907126707274, 0.41275663154069764, 0.4129887859334625, 0.4134763462070875, 0.4124776136835548, 0.41238388675479887, 0.41291867098098933, 0.41171067506459946, 0.4128031345168881, 0.4129658949335548, 0.41308179188584343], "end": "2016-01-24 08:46:15.769000", "learning_rate_per_epoch": [0.004341676831245422, 0.0040591382421553135, 0.003794986056163907, 0.003548023756593466, 0.0033171328250318766, 0.003101267386227846, 0.0028994495514780283, 0.002710765227675438, 0.0025343596935272217, 0.0023694338742643595, 0.0022152408491820097, 0.002071081893518567, 0.001936304266564548, 0.001810297486372292, 0.0016924906522035599, 0.0015823502326384187, 0.0014793772716075182, 0.0013831054093316197, 0.0012930985540151596, 0.0012089490192010999, 0.0011302755447104573, 0.0010567217832431197, 0.0009879546705633402, 0.0009236626210622489, 0.0008635544218122959, 0.0008073578355833888, 0.0007548182620666921, 0.0007056977483443916, 0.0006597738247364759, 0.0006168384570628405, 0.0005766971153207123, 0.0005391680169850588, 0.0005040811374783516, 0.00047127759899012744, 0.0004406087682582438, 0.00041193573269993067, 0.0003851286310236901, 0.0003600660420488566, 0.00033663440262898803, 0.0003147276001982391, 0.0002942464197985828, 0.0002750980493146926, 0.00025719578843563795, 0.0002404585393378511, 0.00022481047199107707, 0.00021018071856815368, 0.00019650300964713097, 0.00018371539772488177, 0.00017175995162688196, 0.00016058250912465155, 0.0001501324586570263, 0.00014036244829185307, 0.00013122822565492243, 0.00012268843420315534, 0.00011470437311800197, 0.00010723988089011982, 0.00010026114614447579, 9.373656212119386e-05, 8.76365666044876e-05, 8.193353278329596e-05, 7.660163100808859e-05, 7.161671237554401e-05, 6.695618503727019e-05, 6.259894871618599e-05, 5.852526373928413e-05, 5.471667827805504e-05, 5.11559410369955e-05, 4.782692121807486e-05, 4.471453939913772e-05, 4.180469841230661e-05, 3.9084217860363424e-05, 3.654077590908855e-05, 3.416285107959993e-05, 3.1939671316649765e-05, 2.986116669490002e-05, 2.791792394418735e-05, 2.610113915579859e-05, 2.4402583221672103e-05, 2.2814561816630885e-05, 2.1329882656573318e-05, 1.9941820937674493e-05, 1.864408841356635e-05, 1.7430807929486036e-05, 1.6296482499456033e-05, 1.5235973478411324e-05, 1.4244478734326549e-05, 1.331750627286965e-05, 1.2450856957002543e-05, 1.1640606317087077e-05, 1.0883082723012194e-05, 1.0174855560762808e-05, 9.512717042525765e-06, 8.89366856426932e-06, 8.314905244333204e-06, 7.773805009492207e-06, 7.267917226272402e-06, 6.794950877520023e-06, 6.352763193717692e-06, 5.939351012784755e-06, 5.552842139877612e-06, 5.19148579769535e-06, 4.853644895774778e-06, 4.537789664027514e-06, 4.242489012540318e-06, 3.966405074606882e-06, 3.7082875223859446e-06, 3.4669672004383756e-06, 3.2413511235063197e-06, 3.0304172469186597e-06, 2.8332099191175075e-06, 2.6488362436793977e-06, 2.476460849720752e-06, 2.315302936040098e-06, 2.1646324057655875e-06, 2.0237669104972156e-06, 1.8920684397016885e-06, 1.7689403648546431e-06, 1.6538249383302173e-06, 1.5462007922906196e-06, 1.445580323888862e-06, 1.3515078762793564e-06, 1.2635572375074844e-06, 1.1813300488938694e-06, 1.1044538723581354e-06, 1.0325804851163412e-06, 9.653844017520896e-07, 9.025611120705435e-07, 8.438261147603043e-07, 7.889133257776848e-07, 7.375740551651688e-07, 6.895757564961968e-07, 6.447009468502074e-07, 6.027464110047731e-07, 5.635221214106423e-07, 5.26850385540456e-07, 4.925651069243031e-07, 4.6051098934185575e-07, 4.305427978579246e-07, 4.025248188099795e-07, 3.7633014926541364e-07, 3.5184012858735514e-07, 3.289437984221877e-07, 3.0753747637390916e-07, 2.87524187569943e-07, 2.688132951789157e-07, 2.5132001724159636e-07, 2.3496512824294769e-07, 2.1967456120819406e-07, 2.0537903822059889e-07, 1.9201380041522498e-07, 1.795183237618403e-07, 1.6783600642611418e-07, 1.5691392718508723e-07, 1.467026038426411e-07, 1.3715579427753255e-07, 1.282302548588632e-07, 1.1988555570496828e-07, 1.1208388883687803e-07, 1.0478992606977044e-07, 9.797062716643268e-08, 9.159509772871388e-08, 8.563446129983276e-08, 8.006171725583044e-08], "accuracy_valid": [0.18707054781626506, 0.23209537368222893, 0.25828166180346385, 0.26632800734186746, 0.27758053699171686, 0.28759030261671686, 0.29700001176581325, 0.30988946018448793, 0.3193197595067771, 0.32889271931475905, 0.3350771249058735, 0.34219838337725905, 0.34833278426204817, 0.35323618693524095, 0.35812929452183734, 0.3598176887236446, 0.3649961172816265, 0.3690450277673193, 0.3704995764307229, 0.3754029791039157, 0.37559564429593373, 0.3789224279932229, 0.3796548498682229, 0.3821168462914157, 0.38322577419051207, 0.3849244634789157, 0.38578925075301207, 0.3868775884789157, 0.3888513036521084, 0.38993964137801207, 0.39091620387801207, 0.3921163168298193, 0.3923604574548193, 0.3926957831325301, 0.3932046545557229, 0.3941812170557229, 0.39579901637801207, 0.3972741552146084, 0.3959313817771084, 0.3976403661521084, 0.39716237998870485, 0.39862722373870485, 0.39850515342620485, 0.39862722373870485, 0.3987595891378012, 0.39826101280120485, 0.3999905873493976, 0.4002347279743976, 0.40036709337349397, 0.40049945877259036, 0.40244228868599397, 0.4012112904743976, 0.40183193712349397, 0.4012112904743976, 0.40306293533509036, 0.4014554310993976, 0.4014554310993976, 0.4015672063253012, 0.4004479833396084, 0.40204519248870485, 0.4005906438253012, 0.4005700536521084, 0.4016686864646084, 0.4020554875753012, 0.4015672063253012, 0.4013024755271084, 0.4013024755271084, 0.4011804052146084, 0.40265554405120485, 0.4018113469503012, 0.40216726280120485, 0.40192312217620485, 0.4021569677146084, 0.40265554405120485, 0.4019128270896084, 0.4036424016378012, 0.40204519248870485, 0.40289968467620485, 0.40338796592620485, 0.40204519248870485, 0.40180105186370485, 0.40228933311370485, 0.40277761436370485, 0.40289968467620485, 0.40204519248870485, 0.4024011083396084, 0.40277761436370485, 0.4017907567771084, 0.4019128270896084, 0.4017907567771084, 0.40265554405120485, 0.4017907567771084, 0.4025231786521084, 0.4019128270896084, 0.4014245458396084, 0.4021569677146084, 0.4016686864646084, 0.4028893895896084, 0.4022790380271084, 0.40178046169051207, 0.4019128270896084, 0.40216726280120485, 0.4028893895896084, 0.40031561794051207, 0.4021569677146084, 0.40117011012801207, 0.40178046169051207, 0.40302175498870485, 0.4029099797628012, 0.4022790380271084, 0.4030423451618976, 0.4019128270896084, 0.40375417686370485, 0.40131277061370485, 0.4026452489646084, 0.40228933311370485, 0.40143484092620485, 0.40214667262801207, 0.4022996282003012, 0.4017907567771084, 0.4021569677146084, 0.40226874294051207, 0.4030114599021084, 0.40202460231551207, 0.4021569677146084, 0.40092596950301207, 0.40265554405120485, 0.4013024755271084, 0.4033776708396084, 0.4025231786521084, 0.40178046169051207, 0.40314382530120485, 0.40190253200301207, 0.4020348974021084, 0.4017907567771084, 0.4021775578878012, 0.40165839137801207, 0.40190253200301207, 0.40289968467620485, 0.40180105186370485, 0.40202460231551207, 0.40204519248870485, 0.4014245458396084, 0.4020348974021084, 0.40277761436370485, 0.40241140342620485, 0.40129218044051207, 0.4025231786521084, 0.4026452489646084, 0.4014245458396084, 0.40265554405120485, 0.40167898155120485, 0.40192312217620485, 0.4017907567771084, 0.40204519248870485, 0.40216726280120485, 0.4032556005271084, 0.40351003623870485, 0.40239081325301207, 0.4026658391378012, 0.4025231786521084, 0.40253347373870485, 0.4024011083396084], "accuracy_test": 0.4123007015306122, "start": "2016-01-23 11:15:25.016000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0], "accuracy_train_last": 0.41308179188584343, "batch_size_eval": 1024, "accuracy_train_std": [0.010344726584515315, 0.012168766895551634, 0.012342650292747073, 0.011709718658614995, 0.014661680881525193, 0.01499743125866922, 0.015981472740317307, 0.015566638735741833, 0.01584955103810174, 0.016189926054514955, 0.015447892379381238, 0.015844775609180364, 0.015798370503220794, 0.015372859277371071, 0.014102258829139054, 0.014272521430575818, 0.014574946079076126, 0.014239322696897191, 0.014333818384449151, 0.014435900019317967, 0.014151646197260443, 0.013628017273646265, 0.01352138225819754, 0.014115203665828857, 0.013751261349368498, 0.013769988454926285, 0.013158065243368482, 0.013551308444739119, 0.013448000442551173, 0.013248588215576932, 0.013307323778711263, 0.0133389249607587, 0.012864462617769986, 0.013041703315518658, 0.012739431106040087, 0.013338709950242772, 0.01393387752165895, 0.01384934629212735, 0.013467788308122456, 0.01419284744493583, 0.013948906235989275, 0.014043193260259849, 0.014352741689138379, 0.014412363791087771, 0.014047527670808954, 0.014127399992125972, 0.014157053952466941, 0.013555713016154916, 0.013938596520029456, 0.014333229800461136, 0.013997603645863946, 0.014474027373776384, 0.014283569922908756, 0.01425975514924195, 0.013777027096688037, 0.013680739622539562, 0.014229564728541637, 0.013737612600036947, 0.014162504483651101, 0.014403030438258259, 0.01435318318032531, 0.013551411494272867, 0.013228725997331124, 0.013963740022337716, 0.014366395301587752, 0.013686354550181228, 0.014355837041673421, 0.013798272493313097, 0.013985077400341049, 0.01379506169816713, 0.014511521896684532, 0.013622490760750745, 0.013447490337552225, 0.014053100204967965, 0.01420938330033903, 0.014023512122473509, 0.013672880431225084, 0.013737541971253367, 0.014132648843237963, 0.014229051754981282, 0.014110264268520939, 0.014383505426258365, 0.013842913741729879, 0.014035260425913366, 0.014191949895990007, 0.014138496057495167, 0.013936782847414324, 0.01406919363501747, 0.014436636848868463, 0.014098547762624451, 0.013176467047736649, 0.01432603594434615, 0.013815373472880903, 0.013683292227976537, 0.014067592686043762, 0.014441590969867685, 0.013881366513401234, 0.014223653685048734, 0.013350167500841819, 0.014166604821399784, 0.01387131329775183, 0.013994831481255902, 0.014221448530382819, 0.013834688829918187, 0.013836529086431635, 0.014204397316509028, 0.013671381332521867, 0.013815751827252304, 0.014152497968132902, 0.013788244544462374, 0.013609273828196244, 0.01405379902457235, 0.014108134874529284, 0.013567891967991784, 0.0136500270302509, 0.014210873411900563, 0.013640304436080658, 0.014241031454246794, 0.014045551814947373, 0.014212833702380448, 0.013169497631884642, 0.013568822766584054, 0.013508500647884555, 0.014106903215632897, 0.014132647508792711, 0.014064236168532035, 0.014121019332755488, 0.013911631386234904, 0.013583081928755498, 0.014002602656125445, 0.013443252990353653, 0.013809078816581437, 0.013731426351136609, 0.013419472657852715, 0.013929097334737454, 0.013538058223373324, 0.013728118717119138, 0.014257328990075719, 0.014320414556481371, 0.013710954591788194, 0.014437069383621038, 0.01443726483553459, 0.01410997711195334, 0.013982736157987511, 0.013890557127246106, 0.014578453943209821, 0.014195472332347556, 0.014361271735211033, 0.013645806586799607, 0.013644928720780321, 0.014166214904093212, 0.013980611716628977, 0.014691334988400702, 0.01407379112162798, 0.014326605947855517, 0.014288543412125996, 0.014045106775391919, 0.013539831234557561, 0.013601300422457236, 0.014226642050537669, 0.013874950620179164, 0.013928759223151134, 0.014191580211434596], "accuracy_test_std": 0.010688539494688414, "error_valid": [0.8129294521837349, 0.7679046263177711, 0.7417183381965362, 0.7336719926581325, 0.7224194630082832, 0.7124096973832832, 0.7029999882341867, 0.6901105398155121, 0.6806802404932228, 0.671107280685241, 0.6649228750941265, 0.657801616622741, 0.6516672157379518, 0.646763813064759, 0.6418707054781627, 0.6401823112763554, 0.6350038827183735, 0.6309549722326807, 0.6295004235692772, 0.6245970208960843, 0.6244043557040663, 0.6210775720067772, 0.6203451501317772, 0.6178831537085843, 0.6167742258094879, 0.6150755365210843, 0.6142107492469879, 0.6131224115210843, 0.6111486963478916, 0.6100603586219879, 0.6090837961219879, 0.6078836831701807, 0.6076395425451807, 0.6073042168674698, 0.6067953454442772, 0.6058187829442772, 0.6042009836219879, 0.6027258447853916, 0.6040686182228916, 0.6023596338478916, 0.6028376200112951, 0.6013727762612951, 0.6014948465737951, 0.6013727762612951, 0.6012404108621988, 0.6017389871987951, 0.6000094126506024, 0.5997652720256024, 0.599632906626506, 0.5995005412274097, 0.597557711314006, 0.5987887095256024, 0.598168062876506, 0.5987887095256024, 0.5969370646649097, 0.5985445689006024, 0.5985445689006024, 0.5984327936746988, 0.5995520166603916, 0.5979548075112951, 0.5994093561746988, 0.5994299463478916, 0.5983313135353916, 0.5979445124246988, 0.5984327936746988, 0.5986975244728916, 0.5986975244728916, 0.5988195947853916, 0.5973444559487951, 0.5981886530496988, 0.5978327371987951, 0.5980768778237951, 0.5978430322853916, 0.5973444559487951, 0.5980871729103916, 0.5963575983621988, 0.5979548075112951, 0.5971003153237951, 0.5966120340737951, 0.5979548075112951, 0.5981989481362951, 0.5977106668862951, 0.5972223856362951, 0.5971003153237951, 0.5979548075112951, 0.5975988916603916, 0.5972223856362951, 0.5982092432228916, 0.5980871729103916, 0.5982092432228916, 0.5973444559487951, 0.5982092432228916, 0.5974768213478916, 0.5980871729103916, 0.5985754541603916, 0.5978430322853916, 0.5983313135353916, 0.5971106104103916, 0.5977209619728916, 0.5982195383094879, 0.5980871729103916, 0.5978327371987951, 0.5971106104103916, 0.5996843820594879, 0.5978430322853916, 0.5988298898719879, 0.5982195383094879, 0.5969782450112951, 0.5970900202371988, 0.5977209619728916, 0.5969576548381024, 0.5980871729103916, 0.5962458231362951, 0.5986872293862951, 0.5973547510353916, 0.5977106668862951, 0.5985651590737951, 0.5978533273719879, 0.5977003717996988, 0.5982092432228916, 0.5978430322853916, 0.5977312570594879, 0.5969885400978916, 0.5979753976844879, 0.5978430322853916, 0.5990740304969879, 0.5973444559487951, 0.5986975244728916, 0.5966223291603916, 0.5974768213478916, 0.5982195383094879, 0.5968561746987951, 0.5980974679969879, 0.5979651025978916, 0.5982092432228916, 0.5978224421121988, 0.5983416086219879, 0.5980974679969879, 0.5971003153237951, 0.5981989481362951, 0.5979753976844879, 0.5979548075112951, 0.5985754541603916, 0.5979651025978916, 0.5972223856362951, 0.5975885965737951, 0.5987078195594879, 0.5974768213478916, 0.5973547510353916, 0.5985754541603916, 0.5973444559487951, 0.5983210184487951, 0.5980768778237951, 0.5982092432228916, 0.5979548075112951, 0.5978327371987951, 0.5967443994728916, 0.5964899637612951, 0.5976091867469879, 0.5973341608621988, 0.5974768213478916, 0.5974665262612951, 0.5975988916603916], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-3, 3], "momentum": 0.964236654636524, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.004643881625154529, "patience_threshold": 1, "do_flip": true, "batch_size": 64, "optimization": "adadelta", "nb_data_augmentation": 2, "learning_rate_decay_method": "exp", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 2.8280579587210003e-05, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.06507592853614705}, "accuracy_valid_max": 0.40375417686370485, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import os\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-3, 3)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.4024011083396084, "loss_train": [2.612919330596924, 2.4717280864715576, 2.4052650928497314, 2.3605988025665283, 2.328030586242676, 2.301287889480591, 2.279005289077759, 2.2596328258514404, 2.2425124645233154, 2.2269506454467773, 2.2125110626220703, 2.1989104747772217, 2.186781406402588, 2.174722194671631, 2.1645424365997314, 2.1543233394622803, 2.1451008319854736, 2.136991262435913, 2.128361463546753, 2.1213724613189697, 2.1146080493927, 2.107534170150757, 2.103142261505127, 2.0972402095794678, 2.0918898582458496, 2.0873820781707764, 2.0838069915771484, 2.0793893337249756, 2.0756964683532715, 2.0731093883514404, 2.069406270980835, 2.066253185272217, 2.0631461143493652, 2.0599098205566406, 2.058457612991333, 2.055941104888916, 2.0538735389709473, 2.052583694458008, 2.0508170127868652, 2.048696994781494, 2.0474016666412354, 2.0460891723632812, 2.0454983711242676, 2.0432257652282715, 2.0425469875335693, 2.042217254638672, 2.040782928466797, 2.0392391681671143, 2.038909673690796, 2.0385284423828125, 2.0369551181793213, 2.035773277282715, 2.035646438598633, 2.0349676609039307, 2.0347421169281006, 2.0343973636627197, 2.0336616039276123, 2.033095121383667, 2.032426357269287, 2.0325980186462402, 2.030848503112793, 2.0317001342773438, 2.0315563678741455, 2.0304391384124756, 2.030139684677124, 2.0300400257110596, 2.030038356781006, 2.029658555984497, 2.028874397277832, 2.0286595821380615, 2.0288283824920654, 2.0289487838745117, 2.028240203857422, 2.02862286567688, 2.0271120071411133, 2.0283639430999756, 2.0280988216400146, 2.0277421474456787, 2.0281431674957275, 2.0274817943573, 2.0273475646972656, 2.0277910232543945, 2.028019666671753, 2.0274925231933594, 2.02748703956604, 2.0276107788085938, 2.0278470516204834, 2.0264625549316406, 2.026890754699707, 2.0273489952087402, 2.026979923248291, 2.026982545852661, 2.027792453765869, 2.0271246433258057, 2.0269381999969482, 2.026397705078125, 2.0271894931793213, 2.0272068977355957, 2.0261447429656982, 2.026501178741455, 2.0263078212738037, 2.0259151458740234, 2.02774715423584, 2.026557683944702, 2.0258944034576416, 2.026390314102173, 2.026611328125, 2.026258945465088, 2.0261266231536865, 2.0259199142456055, 2.0257108211517334, 2.0257811546325684, 2.0261456966400146, 2.0264132022857666, 2.0269687175750732, 2.0261800289154053, 2.026851177215576, 2.0264928340911865, 2.026247978210449, 2.025975465774536, 2.0262346267700195, 2.0258629322052, 2.026123523712158, 2.0260746479034424, 2.026198625564575, 2.026367664337158, 2.026355504989624, 2.0264270305633545, 2.0262277126312256, 2.0259039402008057, 2.026323080062866, 2.0253829956054688, 2.026108741760254, 2.026907205581665, 2.026402711868286, 2.026642084121704, 2.0265657901763916, 2.025963068008423, 2.026325225830078, 2.026271104812622, 2.0268335342407227, 2.026414394378662, 2.026639699935913, 2.02557373046875, 2.026054620742798, 2.026597261428833, 2.0261380672454834, 2.026477098464966, 2.0257585048675537, 2.025933265686035, 2.0256431102752686, 2.0267205238342285, 2.0267832279205322, 2.026686906814575, 2.0258822441101074, 2.026608943939209, 2.0256736278533936, 2.025700092315674, 2.026184320449829, 2.0257766246795654, 2.0266990661621094, 2.0264832973480225, 2.0270843505859375], "accuracy_train_first": 0.1912811605989295, "model": "residualv4", "loss_std": [0.07982595264911652, 0.04884911701083183, 0.05322686955332756, 0.05650903284549713, 0.0597456619143486, 0.0627598837018013, 0.06469109654426575, 0.06702805310487747, 0.06834182143211365, 0.07076775282621384, 0.07065314799547195, 0.07241726666688919, 0.07391781359910965, 0.07480952143669128, 0.07567975670099258, 0.07635968178510666, 0.07749869674444199, 0.07866022735834122, 0.07905444502830505, 0.07981085032224655, 0.08122344315052032, 0.0799764022231102, 0.08165904879570007, 0.08101858198642731, 0.0818902850151062, 0.08265284448862076, 0.08276201784610748, 0.08413748443126678, 0.0826982781291008, 0.08463834971189499, 0.08398096263408661, 0.08458945900201797, 0.08435636758804321, 0.08446234464645386, 0.08495979011058807, 0.08526994287967682, 0.08533681184053421, 0.08601845800876617, 0.08549558371305466, 0.08601667732000351, 0.08629667013883591, 0.08539977669715881, 0.0859917551279068, 0.08610810339450836, 0.08696671575307846, 0.08762139827013016, 0.08643888682126999, 0.08697163313627243, 0.08675618469715118, 0.08597871661186218, 0.08729606866836548, 0.08719231933355331, 0.08665817975997925, 0.0867224931716919, 0.08621503412723541, 0.08717352151870728, 0.08635414391756058, 0.08701460808515549, 0.08685994893312454, 0.08740640431642532, 0.08749361336231232, 0.08691182732582092, 0.08749282360076904, 0.08736035972833633, 0.08674603700637817, 0.08786308020353317, 0.08713150769472122, 0.0873815193772316, 0.08799823373556137, 0.08626379817724228, 0.08744227886199951, 0.08782801777124405, 0.08771312236785889, 0.0880875512957573, 0.0870501846075058, 0.08761828392744064, 0.08772352337837219, 0.0874486044049263, 0.08719687908887863, 0.08797140419483185, 0.08767654746770859, 0.08760705590248108, 0.0875866711139679, 0.08698514103889465, 0.08712561428546906, 0.08778850734233856, 0.08713264018297195, 0.08769481629133224, 0.08722426742315292, 0.08695574849843979, 0.08824014663696289, 0.08696842193603516, 0.088436059653759, 0.08747609704732895, 0.08739490807056427, 0.08660051971673965, 0.0878201350569725, 0.087631955742836, 0.08761756867170334, 0.08733140677213669, 0.08828962594270706, 0.08777499198913574, 0.08818937838077545, 0.08734063059091568, 0.08666620403528214, 0.08697078377008438, 0.08749345690011978, 0.08581504970788956, 0.08722714334726334, 0.08687108755111694, 0.08800273388624191, 0.08811946213245392, 0.08739673346281052, 0.08714669942855835, 0.08679910004138947, 0.08738355338573456, 0.0873497873544693, 0.08728630095720291, 0.08743724226951599, 0.08755823224782944, 0.08791948854923248, 0.08731861412525177, 0.08801259845495224, 0.08789044618606567, 0.08795865625143051, 0.08743292838335037, 0.08636605739593506, 0.08773092180490494, 0.08873994648456573, 0.08807618916034698, 0.08723586797714233, 0.08758480846881866, 0.08787962049245834, 0.08801566064357758, 0.0875682383775711, 0.08826667070388794, 0.08812784403562546, 0.08731938898563385, 0.08757679909467697, 0.08788324147462845, 0.08741398900747299, 0.08759494870901108, 0.08832936733961105, 0.08808229118585587, 0.08728775382041931, 0.087776780128479, 0.08754941821098328, 0.08796443790197372, 0.08796291053295135, 0.08789260685443878, 0.0874410942196846, 0.08661507815122604, 0.08667191863059998, 0.08735327422618866, 0.08765101432800293, 0.08750767260789871, 0.08695844560861588, 0.08716681599617004, 0.08779650181531906, 0.08873448520898819, 0.08752351254224777, 0.08788187801837921, 0.08780732750892639]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:09 2016", "state": "available"}], "summary": "c3ce2273ba999e06748432fd25c70b11"}