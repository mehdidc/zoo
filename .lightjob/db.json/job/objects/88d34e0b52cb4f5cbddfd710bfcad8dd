{"content": {"hp_model": {"f1": 193, "f2": 213, "f3": 246, "nonlin": "very_leaky_rectify", "ds2": 218, "ds1": 3465, "do2": 0.7394414335099088, "do3": 0.5342731381649226, "do1": 0.4629582855871711, "do4": 0.4601705524679842, "do5": 0.9766613226729483}, "accuracy_valid_std": [0.03166097229028674, 0.0315277421349028, 0.031193049696976376, 0.02872612456083929, 0.02771120362809127, 0.03114910371013082, 0.029967566340529453, 0.029523207312063537, 0.02933826161651289, 0.03221880269172034, 0.03499771237693414, 0.03343910132143611, 0.031130458877298803, 0.03326256580936972, 0.032784851981410554, 0.0316346005635988, 0.03301399509678126, 0.03348681494673431, 0.034090108562759805, 0.033121536399361384, 0.036240595527077826, 0.03555243552649678, 0.03514153848948346, 0.036137566797841345, 0.036634959628492454, 0.03788543160177922, 0.037778244096907894, 0.036879787736608854, 0.03621680714437474, 0.03627487340920051, 0.035539164359037194, 0.03716403300370902, 0.037339615733170683, 0.036245601618233037, 0.03700282016121139, 0.0362378418821614, 0.03550443169898527, 0.035988646928725714, 0.03602266088458357, 0.036085313632560276, 0.03600754753762737, 0.036216556657243285, 0.03460382715044054, 0.03570596978173817, 0.035558303886049955, 0.03571714710859882, 0.03641938494853705, 0.03568385885290632, 0.0350629728524223, 0.0360196387225006, 0.035709018490580814, 0.035607508969009286, 0.035455339431008544, 0.035410022203480065, 0.03531176381712912, 0.03557692313727609, 0.03597831039826842, 0.03632160925435164, 0.036964309146476394, 0.03732795211468046, 0.0374337632332456, 0.03746864438270487, 0.03658862403820335, 0.03640069818170676, 0.03652286062517743, 0.03637177707447583, 0.03696725407955696, 0.03707336057107277, 0.03699987805894378], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 1234, "duration": 146910.80107, "accuracy_train": [0.15100244728915663, 0.13857774849397592, 0.1451195406626506, 0.14455478162650603, 0.15104951054216867, 0.15472044427710843, 0.1608621987951807, 0.16820406626506024, 0.17385165662650603, 0.17895801957831325, 0.1848409262048193, 0.1885589231927711, 0.19001788403614459, 0.19086502259036145, 0.18942959337349397, 0.18804122740963855, 0.1898531626506024, 0.19039439006024098, 0.1929593373493976, 0.19225338855421686, 0.19114740210843373, 0.19150037650602408, 0.19117093373493976, 0.19065323795180722, 0.19124152861445784, 0.1908179593373494, 0.19041792168674698, 0.18961784638554216, 0.18848832831325302, 0.18830007530120482, 0.18834713855421686, 0.1880882906626506, 0.18747646837349397, 0.18688817771084337, 0.18655873493975902, 0.18759412650602408, 0.1873117469879518, 0.18738234186746988, 0.18738234186746988, 0.1873117469879518, 0.1881824171686747, 0.18775884789156627, 0.1880882906626506, 0.18782944277108435, 0.1880882906626506, 0.18825301204819278, 0.1883706701807229, 0.1882765436746988, 0.18797063253012047, 0.18834713855421686, 0.1883942018072289, 0.18865304969879518, 0.18912368222891565, 0.1892648719879518, 0.18919427710843373, 0.18952371987951808, 0.18997082078313254, 0.18987669427710843, 0.19001788403614459, 0.1901355421686747, 0.19011201054216867, 0.1903237951807229, 0.19079442771084337, 0.1907238328313253, 0.19126506024096385, 0.19147684487951808, 0.19157097138554216, 0.1911003388554217, 0.19098268072289157], "end": "2016-01-18 14:09:52.028000", "learning_rate_per_epoch": [0.005625145509839058, 0.002812572754919529, 0.001875048503279686, 0.0014062863774597645, 0.0011250290554016829, 0.000937524251639843, 0.0008035922073759139, 0.0007031431887298822, 0.0006250161677598953, 0.0005625145277008414, 0.0005113768856972456, 0.0004687621258199215, 0.0004327034985180944, 0.00040179610368795693, 0.0003750096948351711, 0.0003515715943649411, 0.0003308909072075039, 0.00031250808387994766, 0.00029606028692796826, 0.0002812572638504207, 0.0002678640594240278, 0.0002556884428486228, 0.0002445715363137424, 0.00023438106290996075, 0.00022500581690110266, 0.0002163517492590472, 0.00020833872258663177, 0.00020089805184397846, 0.00019397053983993828, 0.00018750484741758555, 0.00018145630019716918, 0.00017578579718247056, 0.00017045895219780505, 0.00016544545360375196, 0.00016071843856479973, 0.00015625404193997383, 0.00015203095972537994, 0.00014803014346398413, 0.00014423449465539306, 0.00014062863192521036, 0.00013719867274630815, 0.0001339320297120139, 0.00013081733777653426, 0.0001278442214243114, 0.0001250032364623621, 0.0001222857681568712, 0.00011968394392170012, 0.00011719053145498037, 0.00011479888780741021, 0.00011250290845055133, 0.00011029696906916797, 0.0001081758746295236, 0.00010613482299959287, 0.00010416936129331589, 0.00010227537131868303, 0.00010044902592198923, 9.868675988400355e-05, 9.698526991996914e-05, 9.534144919598475e-05, 9.375242370879278e-05, 9.221550135407597e-05, 9.072815009858459e-05, 8.928801980800927e-05, 8.789289859123528e-05, 8.654069824842736e-05, 8.522947609890252e-05, 8.395739860134199e-05, 8.272272680187598e-05, 8.152384543791413e-05], "accuracy_valid": [0.15692349137931033, 0.14129849137931033, 0.15032327586206898, 0.14938038793103448, 0.1567887931034483, 0.16177262931034483, 0.16689116379310345, 0.1732219827586207, 0.17995689655172414, 0.18332435344827586, 0.18723060344827586, 0.19463900862068967, 0.19383081896551724, 0.1961206896551724, 0.19814116379310345, 0.19517780172413793, 0.1958512931034483, 0.19652478448275862, 0.19962284482758622, 0.19935344827586207, 0.19854525862068967, 0.19733297413793102, 0.19692887931034483, 0.19544719827586207, 0.1961206896551724, 0.19504310344827586, 0.1919450431034483, 0.19127155172413793, 0.19113685344827586, 0.19113685344827586, 0.1919450431034483, 0.19154094827586207, 0.18898168103448276, 0.18992456896551724, 0.18952047413793102, 0.1905980603448276, 0.1905980603448276, 0.1900592672413793, 0.19019396551724138, 0.19032866379310345, 0.19073275862068967, 0.18992456896551724, 0.1905980603448276, 0.18911637931034483, 0.18911637931034483, 0.1888469827586207, 0.18992456896551724, 0.18978987068965517, 0.1888469827586207, 0.18857758620689655, 0.1880387931034483, 0.18790409482758622, 0.18817349137931033, 0.18911637931034483, 0.18898168103448276, 0.1900592672413793, 0.19073275862068967, 0.19046336206896552, 0.18992456896551724, 0.1892510775862069, 0.18871228448275862, 0.1892510775862069, 0.18844288793103448, 0.18952047413793102, 0.1896551724137931, 0.18952047413793102, 0.18938577586206898, 0.1892510775862069, 0.18898168103448276], "accuracy_test": 0.19861778846153846, "start": "2016-01-16 21:21:21.227000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0], "accuracy_train_last": 0.19098268072289157, "error_valid": [0.8430765086206897, 0.8587015086206897, 0.849676724137931, 0.8506196120689655, 0.8432112068965517, 0.8382273706896551, 0.8331088362068966, 0.8267780172413793, 0.8200431034482758, 0.8166756465517242, 0.8127693965517242, 0.8053609913793103, 0.8061691810344828, 0.8038793103448276, 0.8018588362068966, 0.8048221982758621, 0.8041487068965517, 0.8034752155172413, 0.8003771551724138, 0.8006465517241379, 0.8014547413793103, 0.802667025862069, 0.8030711206896551, 0.8045528017241379, 0.8038793103448276, 0.8049568965517242, 0.8080549568965517, 0.8087284482758621, 0.8088631465517242, 0.8088631465517242, 0.8080549568965517, 0.8084590517241379, 0.8110183189655172, 0.8100754310344828, 0.810479525862069, 0.8094019396551724, 0.8094019396551724, 0.8099407327586207, 0.8098060344827587, 0.8096713362068966, 0.8092672413793103, 0.8100754310344828, 0.8094019396551724, 0.8108836206896551, 0.8108836206896551, 0.8111530172413793, 0.8100754310344828, 0.8102101293103449, 0.8111530172413793, 0.8114224137931034, 0.8119612068965517, 0.8120959051724138, 0.8118265086206897, 0.8108836206896551, 0.8110183189655172, 0.8099407327586207, 0.8092672413793103, 0.8095366379310345, 0.8100754310344828, 0.8107489224137931, 0.8112877155172413, 0.8107489224137931, 0.8115571120689655, 0.810479525862069, 0.8103448275862069, 0.810479525862069, 0.810614224137931, 0.8107489224137931, 0.8110183189655172], "accuracy_train_std": [0.03006022297029044, 0.02955754953383362, 0.03043674668230846, 0.03097522798752692, 0.032455931345799216, 0.03168894152412376, 0.0321154203888748, 0.03232769333900285, 0.03159153942763311, 0.0327160618835848, 0.033871313986491756, 0.03545799172609562, 0.036428804743561576, 0.03684751415568703, 0.03674844538891167, 0.03685749127086472, 0.03794010497712614, 0.0377544322637481, 0.037974008707594306, 0.03839579862477491, 0.0374149827481637, 0.03713463641427355, 0.036444404756028205, 0.036442429478360065, 0.037114582331314816, 0.03763173561317152, 0.03753039923434935, 0.03737803138813368, 0.03733148482569783, 0.037084021718072846, 0.0371671593188761, 0.03737683880056601, 0.03826606425759408, 0.03812881659881341, 0.037972317156541445, 0.038321165259454594, 0.03787203100809956, 0.038034585106033386, 0.03811184292073269, 0.037949619751783556, 0.037463672289165355, 0.037566994365190336, 0.037778009147973114, 0.037817630029258274, 0.03849626661305844, 0.03846234094651154, 0.03858155665733066, 0.03855496687331946, 0.03812117683693326, 0.0380905443169341, 0.037623266384768796, 0.037200956252523926, 0.03731664147082133, 0.037334859177841365, 0.03701422373953938, 0.037535385880568206, 0.03752575869767465, 0.037619903182310864, 0.037552015540366565, 0.037389377585346446, 0.037672678898096376, 0.0378594546219475, 0.03805883959208002, 0.0381926562445383, 0.03832341937935064, 0.038309220546027305, 0.038212832871473784, 0.03816382227880351, 0.03833572800317746], "accuracy_test_std": 0.03650521891030412, "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1.0, 1.2], "translation_range": [-3, 3], "momentum": 0.7068154313345686, "shear_range": [1, 1.1], "patience_check_each": 1, "learning_rate": 0.005625145476407676, "patience_threshold": 1, "do_flip": true, "batch_size": 128, "optimization": "adadelta", "nb_data_augmentation": 2, "learning_rate_decay_method": "lin", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 1.4135857564055166e-10, "valid_ratio": 0.15, "rotation_range": [-90, 90], "learning_rate_decay": 0.9777937013097829}, "accuracy_valid_max": 0.19962284482758622, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    args = parser.parse_args()\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = 1234\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.98, interval=[0.8, 1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-10, -6], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128, 256, 512],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1.0, 1.2)),\n        rotation_range=make_constant_param((-90, 90)),\n        shear_range=make_constant_param((1, 1.1)),\n        translation_range=make_constant_param((-3, 3)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    model_class = vgg\n\n    instantiate = instantiate_random\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n    )\n    batch_optimizer.learning_rate = learning_rate\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.18898168103448276, "loss_train": [2.3174281120300293, 2.304745674133301, 2.303560972213745, 2.3020856380462646, 2.3015239238739014, 2.300945281982422, 2.3003551959991455, 2.300078868865967, 2.2994258403778076, 2.2992403507232666, 2.2992584705352783, 2.2983925342559814, 2.298203945159912, 2.297650098800659, 2.297024726867676, 2.2965149879455566, 2.2970056533813477, 2.2958614826202393, 2.295356512069702, 2.295152425765991, 2.2947826385498047, 2.295167922973633, 2.294435501098633, 2.2934744358062744, 2.293240785598755, 2.292509078979492, 2.2928812503814697, 2.2922770977020264, 2.291973829269409, 2.2908806800842285, 2.291069984436035, 2.2906277179718018, 2.290198802947998, 2.2896738052368164, 2.289330005645752, 2.2892777919769287, 2.2888004779815674, 2.2887229919433594, 2.287801742553711, 2.2881343364715576, 2.287336587905884, 2.2865827083587646, 2.2870709896087646, 2.2864131927490234, 2.286325454711914, 2.285392999649048, 2.2864439487457275, 2.2850492000579834, 2.2843868732452393, 2.2839667797088623, 2.28364634513855, 2.2840256690979004, 2.283351421356201, 2.282978057861328, 2.2818543910980225, 2.2827255725860596, 2.2826828956604004, 2.281925678253174, 2.281651496887207, 2.281548500061035, 2.2812321186065674, 2.280237913131714, 2.280660629272461, 2.2804958820343018, 2.279637336730957, 2.2799599170684814, 2.28049635887146, 2.278902769088745, 2.2795138359069824], "accuracy_train_first": 0.15100244728915663, "model": "vgg", "loss_std": [0.017414070665836334, 0.004852761514484882, 0.004667710047215223, 0.0041864304803311825, 0.00353148952126503, 0.0036995860282331705, 0.0036420191172510386, 0.0036542313173413277, 0.0038076667115092278, 0.004052098840475082, 0.004097131080925465, 0.0040464322082698345, 0.004133203998208046, 0.00430864654481411, 0.004450902808457613, 0.004702801816165447, 0.004538691136986017, 0.004801630508154631, 0.0048154923133552074, 0.00560598261654377, 0.005256858188658953, 0.005330564454197884, 0.005516980309039354, 0.005250261165201664, 0.005340637173503637, 0.006200428586453199, 0.005976357031613588, 0.005781079642474651, 0.005548438988626003, 0.0064615835435688496, 0.006338903680443764, 0.007157918531447649, 0.006609969772398472, 0.006394003983587027, 0.006897900253534317, 0.007116811349987984, 0.007028164807707071, 0.007099879439920187, 0.006915783043950796, 0.00731298653408885, 0.0074608162976801395, 0.007241863291710615, 0.008243405260145664, 0.0075233918614685535, 0.007641613483428955, 0.008939167484641075, 0.007958848960697651, 0.008116855286061764, 0.008368518203496933, 0.008587252348661423, 0.00855638924986124, 0.00847277045249939, 0.008568238466978073, 0.00921565480530262, 0.008855031803250313, 0.009030292741954327, 0.01027903612703085, 0.009066564030945301, 0.00963603239506483, 0.009053445421159267, 0.009292824193835258, 0.00986955501139164, 0.009152058511972427, 0.00991006288677454, 0.009630714543163776, 0.010730749927461147, 0.010237597860395908, 0.010053463280200958, 0.009714116342365742]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:00 2016", "state": "available"}], "summary": "eb49b5011ca80478c3ebd22da83a8854"}