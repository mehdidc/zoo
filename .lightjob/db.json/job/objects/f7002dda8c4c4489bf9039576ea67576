{"content": {"hp_model": {"f0": 64, "f1": 32, "f2": 16, "f3": 64, "nonlin": "leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.005608041774909419, 0.009025148490815503, 0.006337246992482726, 0.007299913054780577, 0.008423986310768662, 0.010740465339176145, 0.010905739142958804, 0.010877656218633418, 0.007881068528478035, 0.009715635878110576, 0.011532263131623454, 0.01274270701868188, 0.010676780960944472, 0.008884380604291444, 0.00615867397703854, 0.007941788855667226, 0.008546901178026244, 0.010805185924473711, 0.012437704906939545, 0.013300295322903062, 0.012893838286928536, 0.014210147256732782, 0.013615739245336055, 0.012023789038242612, 0.01295134077450665, 0.013807918762675684, 0.013401820069273953, 0.013374315646721377, 0.013591387285440102, 0.01284467257183083, 0.012653495916720672, 0.012174342933068774, 0.011132553293455421, 0.011117488766389415, 0.010784294432570306, 0.011338751129754712, 0.01187734508536484, 0.012121283256047359, 0.012086627091331735, 0.011659473845036891, 0.01178373621653312, 0.011403112753030674, 0.0113068879977052, 0.011029696004449759, 0.009937188723014733, 0.010136669672910218, 0.010839919860196464, 0.010946475344122299, 0.010624725523623344, 0.010317121813520801, 0.010581718649370945, 0.00982274012390209, 0.010017289304885926, 0.009140116896085881, 0.008960110992754313, 0.009084912723600376, 0.008967611639525124, 0.009447732719288414, 0.009100568366927901, 0.008187105983001673, 0.009169400179146565, 0.009005932685832987, 0.009422169548593862, 0.00923277249860216, 0.009562581820177279, 0.009404473372134828, 0.009109674262716836, 0.009639994634112522, 0.008632585682955267, 0.008085919667173518, 0.008198000700540482, 0.007302995212865755, 0.007126354073910578, 0.007276594144033057, 0.007966526366373924, 0.007738032318610788, 0.008151339664644078, 0.007956367510224091, 0.007385461522839563, 0.0076241579040652, 0.007968931632281236, 0.008224748325597078, 0.008458607137444259, 0.008478662003407004, 0.008294032029703436, 0.010221165433802816, 0.009624612225467698, 0.00907229167582388, 0.010046205989063762, 0.009035359390645502, 0.008862182450717519, 0.008246848952195069, 0.008432672125052722, 0.008273452249787153, 0.009861441029632994, 0.009933706793314288, 0.00954205176641018, 0.00958268055815713, 0.009356223877013266, 0.00919722968706333, 0.00895928425504843, 0.009298798708403107, 0.009133912586183542, 0.010197841741076182, 0.011362433908706025, 0.012561483864773914, 0.012800442989262894, 0.012874730651713925, 0.012633574425397653, 0.012756270736852862, 0.012858656519326031, 0.013150575574492078, 0.012786129019907218, 0.01262490368727818, 0.013058189326234728, 0.014093829746016065, 0.014948307074006769, 0.015630022629635223, 0.01567970258624416, 0.01570379198501373, 0.015724836368850175, 0.015963340504587226, 0.016407467449440723, 0.016481596813241288, 0.016438491782753095, 0.016015708387239962, 0.015636057546602886, 0.015846392529429147, 0.015908515156994957, 0.01593846067902244, 0.016083272753424048, 0.015921442836243315, 0.016031304763097776, 0.01623065892122255, 0.016181732982234107, 0.015824171084497542, 0.015339520744895415, 0.015521276265634654, 0.0158174134366185, 0.015496109560340334, 0.015669394228253822, 0.01537001418091329, 0.015883238617745623, 0.015319910045607583, 0.015312126737537362, 0.014849880295099349, 0.014476098755055192, 0.015147816795796118, 0.015275188172904897, 0.01407036897928104, 0.013476413933831893, 0.013353612249259557, 0.013554831890466915, 0.012686214912046438, 0.012477773842091987, 0.012206478386014118, 0.012030934470656462, 0.012402027737169826, 0.012464702556770305, 0.011893028357960892, 0.011957710589280107, 0.011957710589280107, 0.011582976953712019, 0.013216687125061017, 0.012990124487017989, 0.012721436256189059, 0.012397051171962147, 0.013000423704384634, 0.012884603625279331, 0.012843593759100267, 0.013279462597656787, 0.013319770732271342, 0.014452022882775611, 0.014850416096569239, 0.014578139893566393, 0.015114079036502585, 0.015367279816537053, 0.015406711075719616, 0.01535214904988017, 0.015349932827150095, 0.015517897545517119, 0.015005667461579506, 0.01553552202152781, 0.015409609343759439, 0.015210117068260063, 0.0152323798129643, 0.015099517825339593, 0.015147287301604985, 0.015367253424763659, 0.015557601842464923, 0.01402289109381967, 0.01416682389399104, 0.01396919614198985, 0.013945682903056577, 0.01400484707024666, 0.014563964414966258, 0.014448916892728727, 0.01491802438653955, 0.013311006749262914, 0.013964176225626936, 0.013502640032301987, 0.012533536968559994, 0.012255098481713942, 0.01171453603870035, 0.011734817213564699, 0.01153037692800785, 0.011725572920082923, 0.011864994213676762, 0.012067180111407316, 0.012515439684016477, 0.01173949015613577, 0.012730063162954556, 0.01346509290755147, 0.01378877131711708, 0.013388893785474039, 0.013754094076413896, 0.013790301101515374, 0.013699181476766745, 0.0133599303781767, 0.012966003611254356, 0.012718371250524342, 0.012671476260766024, 0.013090226629178438, 0.012750418670051427, 0.013342734087608027, 0.01407112615569918, 0.0138722318762491, 0.013668889871067442, 0.013873625080015486, 0.013569766675159986, 0.013459954087132154, 0.01417396347286201, 0.014239505841676258, 0.013276699684331605, 0.014664043622956188, 0.014533391830895396, 0.014484093573148706], "moving_avg_accuracy_train": [0.0407892384182355, 0.0846248002895441, 0.12722237182482005, 0.16757125968271272, 0.20528474599641816, 0.240393819990584, 0.2731310571602687, 0.30341992634272835, 0.3314982167926194, 0.35742422596653206, 0.38133888537661564, 0.4034220792205986, 0.42375718894919395, 0.4424610024001587, 0.45973853792864594, 0.4756114074423521, 0.4902457983749349, 0.5037283922523732, 0.5162089576217741, 0.5277669512387495, 0.5384319233583224, 0.5482907067861482, 0.5574728566628583, 0.5660203515602029, 0.5739851033297086, 0.5813998456960734, 0.5882661011769921, 0.5946503802538757, 0.6005938690718805, 0.6062104011211801, 0.6114908194000735, 0.6163734762820485, 0.6209166409508168, 0.6251752250158034, 0.6291428093052438, 0.6329065864681118, 0.6364962378123029, 0.6399244535244281, 0.6431469593474652, 0.6461703753774659, 0.6490588244699333, 0.6518187197257362, 0.6544862401142737, 0.6569985435091771, 0.6593898248979234, 0.6617070637132714, 0.6639274012292181, 0.666053588178094, 0.6680973647654157, 0.6700320947951957, 0.6718803086672357, 0.6736576694925571, 0.675401309266262, 0.6770914928006916, 0.678710350280497, 0.6803324796754354, 0.6818876551344331, 0.6834105098856169, 0.6848833136116639, 0.6863273835055732, 0.6877525323481684, 0.6890979092755424, 0.6904226808018457, 0.6917195347743097, 0.6929842875018898, 0.6942505202388731, 0.695490075052149, 0.6967056557829068, 0.6978996598393983, 0.6990789672843067, 0.7002471926835062, 0.7013706751558809, 0.702446877498866, 0.7034828168253914, 0.7045197939156927, 0.7055554158934019, 0.7065733980328361, 0.7075778655154514, 0.7085724949557392, 0.7095560171067599, 0.7105224591045557, 0.7114782153108867, 0.7124313658001469, 0.7133473299607191, 0.7142321876231006, 0.715093591588273, 0.7159362484235856, 0.7167690443372717, 0.7176301317536276, 0.7184423128093003, 0.7192267541820248, 0.7200024698329438, 0.7207726574830474, 0.7215495317252835, 0.7223184730075817, 0.7230895752211738, 0.723846310182445, 0.7245901506654464, 0.7253200249203765, 0.7260257398748136, 0.7267120726564351, 0.7273855757313232, 0.7280753978070466, 0.7287497160978166, 0.729417092477376, 0.73007578784158, 0.7307383320848306, 0.7313927145751755, 0.732014246948638, 0.7326363690537927, 0.7332822734055655, 0.7339449314816756, 0.7345947661239749, 0.7351981824437018, 0.7357993858516941, 0.7364102233831729, 0.7370134195353041, 0.7376213641900701, 0.738219667653169, 0.7388185946390057, 0.7393971564560206, 0.7399620399187149, 0.7405146128625208, 0.7410653708857464, 0.7416098812316494, 0.7421348177751049, 0.7426908578749015, 0.7432122203040042, 0.7437465146080446, 0.7442878333507286, 0.7448284986417631, 0.7453825267191705, 0.7459137040721704, 0.7464452421124894, 0.746979429920205, 0.7475253031138158, 0.7480421656249703, 0.7485654345564285, 0.7490875298685505, 0.7495946180304127, 0.7500928500546601, 0.7505854367038637, 0.7510589916226708, 0.7515503312650825, 0.7520459793170533, 0.7525384934423801, 0.7530165973384984, 0.7534933938211952, 0.753964327285375, 0.7544393567257651, 0.7549110610494972, 0.755396012761085, 0.755899934665809, 0.7563836913145843, 0.7568423237865773, 0.757299270838752, 0.7577453283202147, 0.7581885966832838, 0.7586224154421887, 0.7590616804502031, 0.7595267734217018, 0.7599825234281843, 0.7604391293125714, 0.7609082754263955, 0.7613723596074086, 0.7618249126024632, 0.7622601120837267, 0.7627006557906825, 0.7631622132447906, 0.763621792780869, 0.7640656052490445, 0.7645161897442121, 0.7649728690636723, 0.7654140713368917, 0.7658274294244557, 0.7662227031913585, 0.7666248804601241, 0.7670286926805846, 0.7674316151599421, 0.7678407483675544, 0.7682484957841674, 0.7686666217329285, 0.7690964135094327, 0.7695088027451912, 0.7699125051407071, 0.770292113338338, 0.7706825527923872, 0.7710758009796029, 0.7714669267290495, 0.7718816828725896, 0.7722944909315378, 0.7727241469048292, 0.7731433893641249, 0.7735509345120146, 0.773968878418925, 0.7743892057625253, 0.7748185815479376, 0.7752561730286183, 0.7757080980326502, 0.7761683089588979, 0.776601099982997, 0.7770347897320672, 0.7774552653431166, 0.7778824854692423, 0.7783065111125174, 0.7787322759700271, 0.7791503415739289, 0.7795498521055357, 0.7799628900066009, 0.7803764407473122, 0.7807927781925145, 0.7812116236717588, 0.7816118360911739, 0.7820231444936383, 0.7824490895784662, 0.7828603779893442, 0.783286341130563, 0.783702296089812, 0.7840952567436122, 0.7844953882594042, 0.7848927090045694, 0.7853061012466467, 0.7857386081335638, 0.7861417791758275, 0.7865650509340938, 0.7869831978974858, 0.7873920822478719, 0.7877856548001242, 0.7881886261245138, 0.7885978032926548, 0.7890055902737437, 0.789423787879352, 0.7898350429565423, 0.7902191594676894, 0.7906228849015038, 0.7910164286776417, 0.7913985198618801], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 874115617, "moving_var_accuracy_train": [0.014973857736656935, 0.03077048032415116, 0.044024410198062594, 0.05427426394057552, 0.06164760099592442, 0.0665766645868823, 0.06956453840573174, 0.07086482493232794, 0.07087385599039114, 0.06983589195652426, 0.06799950117317453, 0.06558855810901541, 0.06275135248718, 0.05962471097693839, 0.05634885898468781, 0.0529815049656113, 0.04961084305075913, 0.04628578178465862, 0.04305908421365177, 0.03995546074033699, 0.03698358933910468, 0.03415999090128519, 0.03150279869838196, 0.029010055849724964, 0.02667998570150107, 0.02450679277058703, 0.02248042267249154, 0.0205992115792262, 0.018857215955271317, 0.01725540325009147, 0.01578080827988297, 0.014417290495938548, 0.013161324553212587, 0.012008411942038348, 0.010949246273678838, 0.009981815813096494, 0.009099604602742518, 0.008295418109188024, 0.007559337192284792, 0.006885672873470495, 0.006272193829561434, 0.00571352764301232, 0.005206215863920458, 0.0047423992926608, 0.004319623403516204, 0.003935987424710779, 0.003586757770402187, 0.0032687680318360997, 0.002979484433302447, 0.0027152246125653934, 0.0024744452019600655, 0.0022554317852945456, 0.0020572511237090985, 0.0018772364947586993, 0.0017130991411421245, 0.0015654709609932236, 0.0014306910012683173, 0.0013084936804803147, 0.0011971666697714052, 0.001096218043520721, 0.0010048756821806014, 0.0009206784656529343, 0.000844405795459776, 0.0007751016879468595, 0.0007119879143094655, 0.0006552192309763954, 0.000603525773094794, 0.0005564719244022214, 0.0005136555431442636, 0.00047480688327638577, 0.00043960895019875965, 0.00040700797097048297, 0.00037673107722085606, 0.0003487165020929468, 0.00032352274525594167, 0.0003008230866567736, 0.0002800673667169608, 0.00026114122435794737, 0.00024393069143353473, 0.00022824346468411794, 0.0002138252094316375, 0.00020066391782193685, 0.00018877398873633506, 0.00017744750295377675, 0.00016674951040247452, 0.00015675271048315987, 0.0001474680743137354, 0.00013896320818703226, 0.00013174013121578721, 0.00012450286069895075, 0.00011759070903423224, 0.00011124725107053478, 0.00010546122711082949, 0.00010034690669399505, 9.563365228519645e-05, 9.142167467093719e-05, 8.743333741833629e-05, 8.366969165386756e-05, 8.009717034056837e-05, 7.656975567875651e-05, 7.315225429503733e-05, 6.991948639248647e-05, 6.721022821863522e-05, 6.458155181217521e-05, 6.213191771890234e-05, 5.982364219242773e-05, 5.779196184156603e-05, 5.58667136504396e-05, 5.3756764706753214e-05, 5.1864411459576416e-05, 5.0432702198370374e-05, 4.934147351103894e-05, 4.820789172092658e-05, 4.666410384304885e-05, 4.5250703298778104e-05, 4.408373537766827e-05, 4.294997222141455e-05, 4.1981345328600076e-05, 4.100491410134568e-05, 4.0132844500481563e-05, 3.913216403540166e-05, 3.809078756969158e-05, 3.702974053675898e-05, 3.6056776084409264e-05, 3.511952212712727e-05, 3.408759528631049e-05, 3.3461461090916056e-05, 3.256168402414315e-05, 3.187474925169825e-05, 3.132450815715637e-05, 3.082292795380608e-05, 3.050315915342609e-05, 2.9992187661143203e-05, 2.9535763089783793e-05, 2.9150396306013638e-05, 2.891715456693719e-05, 2.8429760809174963e-05, 2.805107809992318e-05, 2.769922192438884e-05, 2.7243545367056826e-05, 2.6753307180222034e-05, 2.626175092496258e-05, 2.5653864182603583e-05, 2.526120956219116e-05, 2.4946091528774914e-05, 2.463461384871516e-05, 2.4228402483194335e-05, 2.3851576208083806e-05, 2.346242353643541e-05, 2.31470579059283e-05, 2.283489683658341e-05, 2.2668010616071987e-05, 2.268664512900998e-05, 2.252416507321798e-05, 2.216484226519416e-05, 2.182756351509493e-05, 2.1435512654503884e-05, 2.106034296433406e-05, 2.064809710810094e-05, 2.031987112268415e-05, 2.023468725965315e-05, 2.0080591149366682e-05, 1.994893243734307e-05, 1.99349218786548e-05, 1.9879796834388658e-05, 1.973505507094606e-05, 1.946613686027931e-05, 1.926623199389715e-05, 1.9256926345493023e-05, 1.923215386078153e-05, 1.9081664036876134e-05, 1.900073511875689e-05, 1.8977665614285692e-05, 1.8831834065902193e-05, 1.8486434836303567e-05, 1.8043963509887648e-05, 1.7695286158504562e-05, 1.7393336327192907e-05, 1.7115121413818437e-05, 1.691011910657664e-05, 1.671542879770975e-05, 1.6617349699185672e-05, 1.6618103469622094e-05, 1.6486877058584888e-05, 1.630496997003425e-05, 1.5971394426408903e-05, 1.5746241689271974e-05, 1.556341475108058e-05, 1.5383887442893255e-05, 1.5393702626043234e-05, 1.538802680523154e-05, 1.551066242317321e-05, 1.5541474337941757e-05, 1.548216433226451e-05, 1.5506041882949644e-05, 1.5545513376657362e-05, 1.5650234124877428e-05, 1.580858744806771e-05, 1.60658545866837e-05, 1.6365415997755466e-05, 1.6414647032847008e-05, 1.6465963515599357e-05, 1.641056481942623e-05, 1.6412161662985903e-05, 1.6389125212081147e-05, 1.6381694115886125e-05, 1.6316534346788596e-05, 1.6121358895892003e-05, 1.6044625775749183e-05, 1.59793811344598e-05, 1.594147483551235e-05, 1.59262111713116e-05, 1.5775119880067326e-05, 1.5720179309500728e-05, 1.5781024316150886e-05, 1.5725345296838813e-05, 1.5785812146247635e-05, 1.576439768473728e-05, 1.557772059517977e-05, 1.5460895605031585e-05, 1.5335580015375743e-05, 1.5340060326124706e-05, 1.5489614158588684e-05, 1.5403574746610342e-05, 1.547564810406163e-05, 1.5501705240601153e-05, 1.5456212424457404e-05, 1.5304685366989166e-05, 1.5235689824812773e-05, 1.5218954436683125e-05, 1.5193670990525102e-05, 1.5248307027501036e-05, 1.5245652971384195e-05, 1.5048997121468663e-05, 1.5011045442498621e-05, 1.4903831231880876e-05, 1.4727391166347307e-05], "duration": 110127.555174, "accuracy_train": [0.4078923841823551, 0.47914485713132154, 0.5106005156423035, 0.5307112504037468, 0.5447061228197674, 0.5563754859380767, 0.5677661916874308, 0.5760197489848652, 0.584202830841639, 0.590758308531746, 0.596570820067368, 0.6021708238164452, 0.6067731765065523, 0.6107953234588409, 0.6152363576850314, 0.618467233065707, 0.6219553167681802, 0.6250717371493171, 0.6285340459463824, 0.6317888937915282, 0.6344166724344776, 0.6370197576365817, 0.6401122055532484, 0.6429478056363049, 0.6456678692552602, 0.6481325269933554, 0.6500624005052602, 0.6521088919458288, 0.654085268433924, 0.6567591895648763, 0.6590145839101145, 0.6603173882198229, 0.6618051229697306, 0.6635024816006829, 0.6648510679102068, 0.666780580933924, 0.6688030999100222, 0.6707783949335548, 0.6721495117547989, 0.6733811196474714, 0.6750548663021411, 0.6766577770279623, 0.6784939236111112, 0.6796092740633075, 0.6809113573966409, 0.6825622130514027, 0.6839104388727391, 0.6851892707179771, 0.6864913540513105, 0.6874446650632152, 0.6885142335155962, 0.6896539169204503, 0.691094067229605, 0.6923031446105574, 0.6932800675987449, 0.6949316442298819, 0.6958842342654116, 0.6971162026462716, 0.6981385471460871, 0.6993240125507567, 0.7005788719315246, 0.7012063016219084, 0.7023456245385751, 0.7033912205264857, 0.7043670620501108, 0.7056466148717239, 0.7066460683716317, 0.7076458823597269, 0.7086456963478222, 0.7096927342884828, 0.7107612212763013, 0.7114820174072536, 0.7121326985857328, 0.7128062707641196, 0.7138525877284053, 0.7148760136927833, 0.7157352372877446, 0.7166180728589886, 0.7175241599183279, 0.7184077164659468, 0.7192204370847176, 0.7200800211678663, 0.7210097202034883, 0.7215910074058692, 0.722195906584533, 0.7228462272748247, 0.7235201599413991, 0.7242642075604466, 0.7253799185008306, 0.7257519423103543, 0.7262867265365449, 0.7269839106912145, 0.7277043463339793, 0.7285413999054079, 0.729238944548265, 0.7300294951435031, 0.730656924833887, 0.7312847150124584, 0.7318888932147471, 0.7323771744647471, 0.7328890676910299, 0.7334471034053156, 0.7342837964885567, 0.7348185807147471, 0.7354234798934108, 0.7360040461194168, 0.7367012302740864, 0.7372821569882798, 0.7376080383098007, 0.7382354680001846, 0.7390954125715209, 0.7399088541666666, 0.7404432779046696, 0.740628929321244, 0.741210216523625, 0.741907761166482, 0.742442184904485, 0.7430928660829641, 0.7436043988210594, 0.7442089375115356, 0.7446042128091547, 0.7450459910829641, 0.7454877693567736, 0.7460221930947767, 0.7465104743447767, 0.7468592466662053, 0.7476952187730712, 0.7479044821659284, 0.7485551633444075, 0.7491597020348837, 0.7496944862610742, 0.7503687794158361, 0.7506943002491694, 0.7512290844753599, 0.7517871201896457, 0.7524381618563123, 0.7526939282253599, 0.7532748549395534, 0.7537863876776486, 0.7541584114871723, 0.7545769382728866, 0.7550187165466962, 0.7553209858919343, 0.7559723880467885, 0.7565068117847914, 0.7569711205703212, 0.7573195324035622, 0.757784562165467, 0.7582027284629937, 0.7587146216892765, 0.759156399963086, 0.7597605781653747, 0.7604352318083242, 0.7607375011535622, 0.7609700160345146, 0.7614117943083242, 0.7617598456533776, 0.7621780119509044, 0.762526784272333, 0.763015065522333, 0.7637126101651901, 0.7640842734865264, 0.7645485822720561, 0.7651305904508121, 0.7655491172365264, 0.7658978895579549, 0.7661769074150978, 0.7666655491532853, 0.7673162303317644, 0.767758008605574, 0.7680599174626246, 0.7685714502007198, 0.769082982938815, 0.7693848917958656, 0.7695476522125323, 0.7697801670934846, 0.7702444758790143, 0.7706630026647286, 0.7710579174741602, 0.771522947236065, 0.7719182225336839, 0.7724297552717792, 0.7729645394979696, 0.7732203058670173, 0.7735458267003507, 0.7737085871170173, 0.7741965078788298, 0.7746150346645441, 0.7749870584740679, 0.7756144881644518, 0.7760097634620708, 0.7765910506644518, 0.776916571497785, 0.7772188408430232, 0.7777303735811184, 0.7781721518549279, 0.7786829636166482, 0.7791944963547435, 0.7797754230689369, 0.7803102072951275, 0.7804962191998893, 0.7809379974736989, 0.7812395458425618, 0.7817274666043743, 0.7821227419019934, 0.7825641596876154, 0.782912932009044, 0.7831454468899963, 0.7836802311161868, 0.7840983974137136, 0.7845398151993356, 0.7849812329849575, 0.7852137478659099, 0.7857249201158176, 0.7862825953419158, 0.7865619736872462, 0.7871200094015319, 0.7874458907230528, 0.7876319026278147, 0.7880965719015319, 0.7884685957110558, 0.7890266314253415, 0.7896311701158176, 0.7897703185562015, 0.7903744967584901, 0.790746520568014, 0.7910720414013473, 0.7913278077703949, 0.7918153680440199, 0.7922803978059246, 0.7926756731035437, 0.7931875663298265, 0.7935363386512551, 0.793676208068014, 0.7942564138058323, 0.7945583226628831, 0.794837340520026], "end": "2016-01-22 16:18:01.612000", "learning_rate_per_epoch": [0.00026502361288294196, 0.00018740000086836517, 0.00015301145322155207, 0.00013251180644147098, 0.000118522162665613, 0.00010819543967954814, 0.00010016951273428276, 9.370000043418258e-05, 8.834120671963319e-05, 8.380782674066722e-05, 7.990762969711795e-05, 7.650572661077604e-05, 7.350432861130685e-05, 7.083053787937388e-05, 6.842880247859284e-05, 6.625590322073549e-05, 6.427767220884562e-05, 6.246666453080252e-05, 6.080058665247634e-05, 5.92610813328065e-05, 5.7832894526654854e-05, 5.6503224186599255e-05, 5.5261243687709793e-05, 5.409771983977407e-05, 5.300472548697144e-05, 5.197540667722933e-05, 5.100381895317696e-05, 5.008475636714138e-05, 4.9213649617740884e-05, 4.838647146243602e-05, 4.759964576805942e-05, 4.685000021709129e-05, 4.61346899101045e-05, 4.5451175537891686e-05, 4.4797168811783195e-05, 4.4170603359816596e-05, 4.356961289886385e-05, 4.299250940675847e-05, 4.2437743104528636e-05, 4.190391337033361e-05, 4.13897359976545e-05, 4.0894032281357795e-05, 4.04157217417378e-05, 3.995381484855898e-05, 3.950738755520433e-05, 3.9075599488569424e-05, 3.865766848321073e-05, 3.825286330538802e-05, 3.78605182049796e-05, 3.7480000173673034e-05, 3.7110730772837996e-05, 3.675216430565342e-05, 3.640379509306513e-05, 3.606514655984938e-05, 3.5735778510570526e-05, 3.541526893968694e-05, 3.5103235859423876e-05, 3.4799304557964206e-05, 3.450313670327887e-05, 3.421440123929642e-05, 3.393279621377587e-05, 3.3658034226391464e-05, 3.3389838790753856e-05, 3.3127951610367745e-05, 3.287213257863186e-05, 3.262215250288136e-05, 3.237778946640901e-05, 3.213883610442281e-05, 3.1905095966067165e-05, 3.1676379876444116e-05, 3.145251685054973e-05, 3.123333226540126e-05, 3.1018669687910005e-05, 3.0808369047008455e-05, 3.060229209950194e-05, 3.040029332623817e-05, 3.02022435789695e-05, 3.000801552843768e-05, 2.9817487302352674e-05, 2.963054066640325e-05, 2.9447068300214596e-05, 2.9266962883411907e-05, 2.909012073359918e-05, 2.8916447263327427e-05, 2.874584788514767e-05, 2.8578231649589725e-05, 2.8413514883141033e-05, 2.8251612093299627e-05, 2.8092446882510558e-05, 2.7935942853218876e-05, 2.7782023607869633e-05, 2.7630621843854897e-05, 2.7481668439577334e-05, 2.733509973040782e-05, 2.7190850232727826e-05, 2.7048859919887036e-05, 2.6909070584224537e-05, 2.6771427656058222e-05, 2.6635876565705985e-05, 2.650236274348572e-05, 2.6370835257694125e-05, 2.6241248633596115e-05, 2.6113553758477792e-05, 2.5987703338614665e-05, 2.5863657356239855e-05, 2.574136851762887e-05, 2.562080044299364e-05, 2.550190947658848e-05, 2.5384659238625318e-05, 2.5269009711337276e-05, 2.515492815291509e-05, 2.504237818357069e-05, 2.493132524250541e-05, 2.482173658790998e-05, 2.4713579477975145e-05, 2.4606824808870442e-05, 2.4501441657776013e-05, 2.43974009208614e-05, 2.4294675313285552e-05, 2.419323573121801e-05, 2.4093056708807126e-05, 2.3994110961211845e-05, 2.3896374841569923e-05, 2.379982288402971e-05, 2.370443326071836e-05, 2.361018050578423e-05, 2.351704279135447e-05, 2.3425000108545646e-05, 2.333402881049551e-05, 2.3244108888320625e-05, 2.315522033313755e-05, 2.306734495505225e-05, 2.2980462745181285e-05, 2.2894553694641218e-05, 2.2809601432527415e-05, 2.2725587768945843e-05, 2.2642496332991868e-05, 2.2560308934771456e-05, 2.2479009203379974e-05, 2.2398584405891597e-05, 2.2319014533422887e-05, 2.2240288672037423e-05, 2.2162388631841168e-05, 2.2085301679908298e-05, 2.2009013264323585e-05, 2.1933510652161203e-05, 2.1858779291505925e-05, 2.1784806449431926e-05, 2.1711581212002784e-05, 2.1639087208313867e-05, 2.156731534341816e-05, 2.1496254703379236e-05, 2.1425888917292468e-05, 2.1356212528189644e-05, 2.1287209165166132e-05, 2.1218871552264318e-05, 2.1151186956558377e-05, 2.108414628310129e-05, 2.1017740436946042e-05, 2.0951956685166806e-05, 2.088678775180597e-05, 2.082222272292711e-05, 2.0758252503583208e-05, 2.069486799882725e-05, 2.0632060113712214e-05, 2.056982157228049e-05, 2.0508143279585056e-05, 2.0447016140678898e-05, 2.03864328796044e-05, 2.0326384401414543e-05, 2.0266863430151716e-05, 2.02078608708689e-05, 2.0149373085587285e-05, 2.0091389160370454e-05, 2.003390181926079e-05, 1.997690742427949e-05, 1.9920395061490126e-05, 1.9864359273924492e-05, 1.9808794604614377e-05, 1.9753693777602166e-05, 1.969905133591965e-05, 1.9644858184619807e-05, 1.959110886673443e-05, 1.9537799744284712e-05, 1.9484923541313037e-05, 1.9432474800851196e-05, 1.9380446246941574e-05, 1.9328834241605364e-05, 1.927763150888495e-05, 1.922683441080153e-05, 1.9176435671397485e-05, 1.912643165269401e-05, 1.9076816897722892e-05, 1.9027587768505327e-05, 1.8978735170094296e-05, 1.89302591024898e-05, 1.888215047074482e-05, 1.8834409274859354e-05, 1.8787026419886388e-05, 1.8740000086836517e-05, 1.869332481874153e-05, 1.864699697762262e-05, 1.8601011106511578e-05, 1.8555365386418998e-05, 1.8510052541387267e-05, 1.8465070752426982e-05, 1.842041456256993e-05, 1.837608215282671e-05, 1.833206624723971e-05, 1.8288366845808923e-05, 1.8244978491566144e-05, 1.8201897546532564e-05, 1.8159118553739972e-05, 1.811664151318837e-05, 1.8074460967909545e-05, 1.803257327992469e-05, 1.7990974811255e-05, 1.794966374291107e-05, 1.7908636436914094e-05, 1.7867889255285263e-05, 1.782741856004577e-05, 1.778722071321681e-05, 1.774729389580898e-05, 1.770763446984347e-05, 1.7668240616330877e-05, 1.7629108697292395e-05, 1.7590235074749216e-05, 1.7551617929711938e-05, 1.7513253624201752e-05, 1.7475140339229256e-05, 1.7437274436815642e-05, 1.7399652278982103e-05, 1.736227386572864e-05, 1.7325135559076443e-05, 1.7288235540036112e-05, 1.7251568351639435e-05, 1.721513399388641e-05], "accuracy_valid": [0.40605292262801207, 0.4722576830760542, 0.5024208160768072, 0.5173442794615963, 0.5329192747552711, 0.5441100338855422, 0.5536726986069277, 0.560326266001506, 0.5655649943524097, 0.5707125376506024, 0.5756262354103916, 0.5799089914344879, 0.5839476068335843, 0.5877111963478916, 0.5913836008094879, 0.5961855233433735, 0.6001123635165663, 0.6047819206513554, 0.6083528449736446, 0.6119134742093373, 0.6132562476468373, 0.6148637518825302, 0.6182817206325302, 0.6193700583584337, 0.6221982657191265, 0.6238057699548193, 0.6264913168298193, 0.6268472326807228, 0.6284238516566265, 0.6297563300075302, 0.6312211737575302, 0.6323198065700302, 0.6326654273343373, 0.6340082007718373, 0.6362260565700302, 0.6378438558923193, 0.6380879965173193, 0.6378438558923193, 0.6382100668298193, 0.6383218420557228, 0.6396749105798193, 0.6405191076807228, 0.6417398108057228, 0.6438150061182228, 0.6449239340173193, 0.6462770025414157, 0.6474977056664157, 0.6467549887048193, 0.6480977621423193, 0.6484639730798193, 0.6495626058923193, 0.6510377447289157, 0.6522790380271084, 0.6528996846762049, 0.6536321065512049, 0.6546086690512049, 0.6547307393637049, 0.6554631612387049, 0.6560632177146084, 0.6565514989646084, 0.6582810735128012, 0.6588914250753012, 0.6603665639118976, 0.6601224232868976, 0.6601224232868976, 0.6604783391378012, 0.6606004094503012, 0.6618314076618976, 0.6621976185993976, 0.6625638295368976, 0.663916898060994, 0.6652699665850903, 0.6663685993975903, 0.6664906697100903, 0.6678437382341867, 0.6675893025225903, 0.6675995976091867, 0.6682099491716867, 0.6688100056475903, 0.6690541462725903, 0.6689320759600903, 0.6693085819841867, 0.6700512989457832, 0.6712720020707832, 0.6716485080948795, 0.6732560123305723, 0.6737442935805723, 0.6747208560805723, 0.6757077136671686, 0.6750870670180723, 0.6750870670180723, 0.6755753482680723, 0.6755753482680723, 0.6760636295180723, 0.6770607821912651, 0.6770607821912651, 0.6774269931287651, 0.6779152743787651, 0.6785256259412651, 0.6786476962537651, 0.6788918368787651, 0.6788918368787651, 0.6790139071912651, 0.6791462725903614, 0.6789124270519578, 0.6794110033885542, 0.6802654955760542, 0.6802654955760542, 0.6809979174510542, 0.6812420580760542, 0.6812420580760542, 0.6813641283885542, 0.6819744799510542, 0.6812317629894578, 0.6817200442394578, 0.6817303393260542, 0.6821068453501506, 0.682727491999247, 0.682971632624247, 0.684070265436747, 0.684314406061747, 0.684436476374247, 0.684314406061747, 0.684070265436747, 0.684558546686747, 0.684680616999247, 0.685046827936747, 0.685290968561747, 0.685535109186747, 0.685535109186747, 0.685657179499247, 0.685290968561747, 0.685657179499247, 0.685413038874247, 0.685779249811747, 0.685779249811747, 0.685413038874247, 0.685046827936747, 0.684924757624247, 0.684802687311747, 0.684436476374247, 0.684680616999247, 0.684314406061747, 0.684558546686747, 0.684558546686747, 0.685413038874247, 0.685657179499247, 0.6862778261483433, 0.6865219667733433, 0.686755812311747, 0.6862572359751506, 0.6872337984751506, 0.6873558687876506, 0.6868572924510542, 0.6868572924510542, 0.6872235033885542, 0.6871014330760542, 0.6869793627635542, 0.6873455737010542, 0.6874676440135542, 0.6875897143260542, 0.6875897143260542, 0.6875897143260542, 0.687854445124247, 0.688831007624247, 0.689075148249247, 0.689197218561747, 0.689441359186747, 0.689319288874247, 0.689441359186747, 0.6898178652108433, 0.6895737245858433, 0.6894619493599398, 0.6899502306099398, 0.6905605821724398, 0.6905708772590362, 0.6902046663215362, 0.6908150178840362, 0.6910591585090362, 0.6916695100715362, 0.6913032991340362, 0.6917915803840362, 0.6921680864081325, 0.6922901567206325, 0.6927784379706325, 0.6926563676581325, 0.6930225785956325, 0.6927784379706325, 0.6924122270331325, 0.6926563676581325, 0.6920254259224398, 0.6917812852974398, 0.6911606386483433, 0.6909164980233433, 0.6907944277108433, 0.6911709337349398, 0.6911709337349398, 0.6917915803840362, 0.6912827089608433, 0.6914150743599398, 0.6904282167733433, 0.690051710749247, 0.689929640436747, 0.6897972750376506, 0.6894310641001506, 0.6900414156626506, 0.6899193453501506, 0.6901634859751506, 0.6904076266001506, 0.690417921686747, 0.6895531344126506, 0.690173781061747, 0.6906723573983433, 0.6906723573983433, 0.690051710749247, 0.6901840761483433, 0.6898178652108433, 0.6893295839608433, 0.6890854433358433, 0.6893295839608433, 0.6901840761483433, 0.6906723573983433, 0.6904282167733433, 0.690051710749247, 0.690173781061747, 0.6901840761483433, 0.6903061464608433, 0.689685499811747, 0.689319288874247, 0.689563429499247, 0.689441359186747, 0.6898178652108433, 0.6896957948983433, 0.689563429499247, 0.6900723009224398, 0.6900723009224398, 0.6900723009224398], "accuracy_test": 0.6789, "start": "2016-01-21 09:42:34.057000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0, 211.0, 212.0, 213.0, 214.0, 215.0, 216.0, 217.0, 218.0, 219.0, 220.0, 221.0, 222.0, 223.0, 224.0, 225.0, 226.0, 227.0, 228.0, 229.0, 230.0, 231.0, 232.0, 233.0, 234.0, 235.0, 236.0], "accuracy_train_last": 0.794837340520026, "batch_size_eval": 1024, "accuracy_train_std": [0.01404819254108928, 0.015292529334648804, 0.014621237985429077, 0.016286494891546387, 0.016371980023416582, 0.016548754895078975, 0.018269723540269445, 0.018501964759078025, 0.018376347106908806, 0.018361309511933488, 0.018249262608252007, 0.01725657784388288, 0.017479991611284527, 0.017239668559495058, 0.017220440995531827, 0.016699143081658513, 0.016494524874217893, 0.01636524892538471, 0.015490687010124685, 0.015358303006257738, 0.01564226697872359, 0.01610133774271424, 0.01624723009916581, 0.01686010225109161, 0.017291132715109542, 0.018295278499227192, 0.018067240237991278, 0.018140473317129334, 0.01792759861307117, 0.018196598653992972, 0.018083832542803255, 0.018360085382671466, 0.017988966886146728, 0.017983668416537812, 0.017568800671656382, 0.017289690788780482, 0.016871613378056614, 0.016427534133665253, 0.01598675690313018, 0.015805387592798474, 0.016021007170560538, 0.015465873096695353, 0.015302721938369748, 0.014979078892955737, 0.014728611123910614, 0.01453229614855223, 0.01468720378740061, 0.015144883337135632, 0.015277994504656774, 0.015413877793585213, 0.015272421791928444, 0.01525082260033211, 0.015014322329841762, 0.015135473801038628, 0.014732568419213532, 0.014980448947829258, 0.015097579653747535, 0.015138893765352601, 0.01544696324328014, 0.014726449076495092, 0.014835477979409612, 0.015213884943570255, 0.015208196423322648, 0.014855530637534609, 0.014893024372396101, 0.014883086596553609, 0.015044735266886404, 0.015122185332869856, 0.01507771136120031, 0.01521965235677357, 0.014857056878698007, 0.014756850505737218, 0.014749293694785528, 0.01476914491935738, 0.014995382157779203, 0.015260534070399722, 0.014974709982414057, 0.014829430612072293, 0.014951195137152104, 0.014817726563828933, 0.015051526877336982, 0.015117996512837671, 0.015033329587961407, 0.015063825134333547, 0.01463565445474422, 0.014611565484817972, 0.014741639634515984, 0.014851364078946298, 0.01440546233747666, 0.014445329970428742, 0.014289330083323505, 0.014209522836925127, 0.014307923877944, 0.014110534779892716, 0.013988503456035018, 0.013897395267867773, 0.013781001801008318, 0.013788609906935425, 0.014046841610164675, 0.01433439908202071, 0.014320416112771588, 0.014142774782220592, 0.013940424525709504, 0.013949136685921612, 0.014220033734061829, 0.014177482134151166, 0.01387351931960203, 0.013880817917766262, 0.014045115768776333, 0.01374509119176929, 0.013499825758798994, 0.01363806317166564, 0.013649191141769103, 0.013336661755560624, 0.013346803369666324, 0.013355830430532947, 0.013697463562177492, 0.013771922271749339, 0.013807887439543254, 0.013780280367868679, 0.01386221093029139, 0.01408512632966953, 0.0138634727926085, 0.013755168821508573, 0.013665385865525846, 0.014235745183266419, 0.014359678838499411, 0.014099216320528442, 0.014370708560254795, 0.01429691963284828, 0.014654547927513095, 0.01403169584343413, 0.014183602012823385, 0.014245661571221717, 0.014531199927833405, 0.01432571192957989, 0.014239219541288055, 0.014121623206681663, 0.01413843558753581, 0.014065844008285497, 0.014180788953246827, 0.013914330365928863, 0.01376779745133786, 0.013778670630878785, 0.01371600024846517, 0.013667642757684173, 0.013699319329566344, 0.01357834186240943, 0.013584952493766192, 0.013568466437480244, 0.013609656325984426, 0.013605473614367904, 0.013617661770168762, 0.013491308296950272, 0.013775868846140236, 0.01377713755193579, 0.013838279358221192, 0.013615998791695276, 0.013697083244868697, 0.013738256634168384, 0.013574417995348918, 0.013638648777701618, 0.013595343818080691, 0.013520580719883588, 0.013598041078830073, 0.013658001336770585, 0.013331458981805844, 0.012965586291963581, 0.013182831332130747, 0.013098676499331769, 0.013201071231510316, 0.013458941459613979, 0.013520985608537464, 0.013527064082731032, 0.013442814241222328, 0.013345577264649898, 0.01357217578219146, 0.013363105358249496, 0.013443345373365797, 0.013407780818032327, 0.01327911204174651, 0.013444737726151105, 0.013533846240116672, 0.01365142780315739, 0.013649399018156066, 0.013424869057087273, 0.013496417776935, 0.013483209132485855, 0.013364226798549502, 0.013370909505024498, 0.01337458139039329, 0.01327781610158892, 0.013115141241631174, 0.012784044524176823, 0.012823152711689333, 0.012676547349659936, 0.013188181135361068, 0.013153819281999032, 0.013125333404726552, 0.013254329526521587, 0.013157934985472436, 0.013174952996833564, 0.013371977798006224, 0.01347710989884795, 0.01336502058509584, 0.013342669446202366, 0.013080806899829767, 0.013141043544060678, 0.013181691630125511, 0.013276000216160476, 0.013434669284401518, 0.013459460491194404, 0.013375864403121492, 0.01350321912339028, 0.013549467402667818, 0.013406363376905225, 0.013271194019730752, 0.01315096919970688, 0.013018181325576634, 0.013049557401308125, 0.012851038761850418, 0.012729856253871187, 0.012770651235501959, 0.012904221834673962, 0.012954262752381342, 0.013002300294109568, 0.013107571019134323, 0.013016173903425533, 0.01325159098261825, 0.013122206250147733, 0.013045920655754904, 0.013025947364004605, 0.01280878579621727, 0.012439534749308971, 0.012615210026094758, 0.012755815199344746, 0.012787378213052251], "accuracy_test_std": 0.12181354604476466, "error_valid": [0.5939470773719879, 0.5277423169239458, 0.4975791839231928, 0.48265572053840367, 0.4670807252447289, 0.45588996611445776, 0.4463273013930723, 0.43967373399849397, 0.4344350056475903, 0.42928746234939763, 0.4243737645896084, 0.42009100856551207, 0.41605239316641573, 0.4122888036521084, 0.40861639919051207, 0.4038144766566265, 0.39988763648343373, 0.3952180793486446, 0.3916471550263554, 0.3880865257906627, 0.3867437523531627, 0.3851362481174698, 0.3817182793674698, 0.38062994164156627, 0.3778017342808735, 0.3761942300451807, 0.3735086831701807, 0.37315276731927716, 0.3715761483433735, 0.3702436699924698, 0.3687788262424698, 0.3676801934299698, 0.3673345726656627, 0.3659917992281627, 0.3637739434299698, 0.3621561441076807, 0.3619120034826807, 0.3621561441076807, 0.3617899331701807, 0.36167815794427716, 0.3603250894201807, 0.35948089231927716, 0.35826018919427716, 0.35618499388177716, 0.3550760659826807, 0.35372299745858427, 0.35250229433358427, 0.3532450112951807, 0.3519022378576807, 0.3515360269201807, 0.3504373941076807, 0.34896225527108427, 0.3477209619728916, 0.34710031532379515, 0.34636789344879515, 0.34539133094879515, 0.34526926063629515, 0.34453683876129515, 0.3439367822853916, 0.3434485010353916, 0.3417189264871988, 0.3411085749246988, 0.33963343608810237, 0.33987757671310237, 0.33987757671310237, 0.3395216608621988, 0.3393995905496988, 0.33816859233810237, 0.33780238140060237, 0.33743617046310237, 0.33608310193900603, 0.3347300334149097, 0.3336314006024097, 0.3335093302899097, 0.33215626176581325, 0.3324106974774097, 0.33240040239081325, 0.33179005082831325, 0.3311899943524097, 0.3309458537274097, 0.3310679240399097, 0.33069141801581325, 0.3299487010542168, 0.3287279979292168, 0.3283514919051205, 0.3267439876694277, 0.3262557064194277, 0.3252791439194277, 0.32429228633283136, 0.3249129329819277, 0.3249129329819277, 0.3244246517319277, 0.3244246517319277, 0.3239363704819277, 0.3229392178087349, 0.3229392178087349, 0.3225730068712349, 0.3220847256212349, 0.3214743740587349, 0.3213523037462349, 0.3211081631212349, 0.3211081631212349, 0.3209860928087349, 0.3208537274096386, 0.32108757294804224, 0.3205889966114458, 0.3197345044239458, 0.3197345044239458, 0.3190020825489458, 0.3187579419239458, 0.3187579419239458, 0.3186358716114458, 0.3180255200489458, 0.31876823701054224, 0.31827995576054224, 0.3182696606739458, 0.31789315464984935, 0.317272508000753, 0.317028367375753, 0.315929734563253, 0.315685593938253, 0.315563523625753, 0.315685593938253, 0.315929734563253, 0.315441453313253, 0.315319383000753, 0.314953172063253, 0.314709031438253, 0.314464890813253, 0.314464890813253, 0.314342820500753, 0.314709031438253, 0.314342820500753, 0.314586961125753, 0.314220750188253, 0.314220750188253, 0.314586961125753, 0.314953172063253, 0.315075242375753, 0.315197312688253, 0.315563523625753, 0.315319383000753, 0.315685593938253, 0.315441453313253, 0.315441453313253, 0.314586961125753, 0.314342820500753, 0.3137221738516567, 0.3134780332266567, 0.313244187688253, 0.31374276402484935, 0.31276620152484935, 0.31264413121234935, 0.3131427075489458, 0.3131427075489458, 0.3127764966114458, 0.3128985669239458, 0.3130206372364458, 0.3126544262989458, 0.3125323559864458, 0.3124102856739458, 0.3124102856739458, 0.3124102856739458, 0.312145554875753, 0.311168992375753, 0.310924851750753, 0.310802781438253, 0.310558640813253, 0.310680711125753, 0.310558640813253, 0.3101821347891567, 0.3104262754141567, 0.31053805064006024, 0.31004976939006024, 0.30943941782756024, 0.3094291227409638, 0.3097953336784638, 0.3091849821159638, 0.3089408414909638, 0.3083304899284638, 0.3086967008659638, 0.3082084196159638, 0.30783191359186746, 0.30770984327936746, 0.30722156202936746, 0.30734363234186746, 0.30697742140436746, 0.30722156202936746, 0.30758777296686746, 0.30734363234186746, 0.30797457407756024, 0.30821871470256024, 0.3088393613516567, 0.3090835019766567, 0.3092055722891567, 0.30882906626506024, 0.30882906626506024, 0.3082084196159638, 0.3087172910391567, 0.30858492564006024, 0.3095717832266567, 0.309948289250753, 0.310070359563253, 0.31020272496234935, 0.31056893589984935, 0.30995858433734935, 0.31008065464984935, 0.30983651402484935, 0.30959237339984935, 0.309582078313253, 0.31044686558734935, 0.309826218938253, 0.3093276426016567, 0.3093276426016567, 0.309948289250753, 0.3098159238516567, 0.3101821347891567, 0.3106704160391567, 0.3109145566641567, 0.3106704160391567, 0.3098159238516567, 0.3093276426016567, 0.3095717832266567, 0.309948289250753, 0.309826218938253, 0.3098159238516567, 0.3096938535391567, 0.310314500188253, 0.310680711125753, 0.310436570500753, 0.310558640813253, 0.3101821347891567, 0.3103042051016567, 0.310436570500753, 0.30992769907756024, 0.30992769907756024, 0.30992769907756024], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-3, 3], "momentum": 0.6851717984027034, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.0002650236185311197, "patience_threshold": 1, "do_flip": true, "batch_size": 16, "optimization": "nesterov_momentum", "nb_data_augmentation": 0, "learning_rate_decay_method": "sqrt", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 1.077762154018271e-06, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.006656598548269311}, "accuracy_valid_max": 0.6930225785956325, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import os\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-6, -3], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128, 256],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-3, 3)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.6900723009224398, "loss_train": [1.8979250192642212, 1.6130030155181885, 1.4990640878677368, 1.437687635421753, 1.3955988883972168, 1.3627917766571045, 1.3357527256011963, 1.312737226486206, 1.2925536632537842, 1.2746696472167969, 1.2585763931274414, 1.2439355850219727, 1.230536699295044, 1.21816086769104, 1.2066456079483032, 1.1958954334259033, 1.1858521699905396, 1.1763174533843994, 1.167319893836975, 1.158786654472351, 1.1506457328796387, 1.142887830734253, 1.1354402303695679, 1.1282802820205688, 1.12138831615448, 1.1147105693817139, 1.1082779169082642, 1.1020185947418213, 1.0959134101867676, 1.090004324913025, 1.0842443704605103, 1.0786237716674805, 1.0731697082519531, 1.0678396224975586, 1.0626087188720703, 1.057519555091858, 1.0525413751602173, 1.047642469406128, 1.0428677797317505, 1.038158893585205, 1.0335750579833984, 1.0290430784225464, 1.0246267318725586, 1.0202839374542236, 1.0160130262374878, 1.011829137802124, 1.007702350616455, 1.003641128540039, 0.9996545910835266, 0.9957262277603149, 0.9918602108955383, 0.9880447387695312, 0.9842852354049683, 0.9806166887283325, 0.9770017266273499, 0.97342449426651, 0.9699074029922485, 0.9664416313171387, 0.9630244374275208, 0.9596397876739502, 0.9562927484512329, 0.9529891014099121, 0.9497248530387878, 0.9465204477310181, 0.9433439373970032, 0.9401933550834656, 0.9370948076248169, 0.9340289235115051, 0.9309867024421692, 0.9280044436454773, 0.9250335097312927, 0.9221016764640808, 0.9191869497299194, 0.916308581829071, 0.9134661555290222, 0.9106438159942627, 0.9078594446182251, 0.9051044583320618, 0.9023808836936951, 0.8996803760528564, 0.8969991207122803, 0.8943310976028442, 0.8916870355606079, 0.8890770673751831, 0.8864873647689819, 0.8839222192764282, 0.8813893795013428, 0.8788626790046692, 0.8763542175292969, 0.8738836050033569, 0.8714194893836975, 0.8689678907394409, 0.8665317893028259, 0.8641214370727539, 0.8617247939109802, 0.8593713045120239, 0.8570182919502258, 0.854671061038971, 0.8523337841033936, 0.8500193953514099, 0.8477322459220886, 0.845449686050415, 0.8431780934333801, 0.8409324884414673, 0.8386989235877991, 0.8364722728729248, 0.8342617154121399, 0.8320516347885132, 0.8298601508140564, 0.8276823163032532, 0.8255071043968201, 0.8233477473258972, 0.8212019801139832, 0.8190646767616272, 0.8169291019439697, 0.8148083686828613, 0.812699556350708, 0.810608983039856, 0.8085211515426636, 0.8064454793930054, 0.8043877482414246, 0.8023253083229065, 0.8002833127975464, 0.7982505559921265, 0.7962397933006287, 0.7942222356796265, 0.7922163605690002, 0.7902207970619202, 0.7882296442985535, 0.7862501740455627, 0.7842699289321899, 0.7823156118392944, 0.7803535461425781, 0.7784087061882019, 0.7764613628387451, 0.7745260000228882, 0.7726027369499207, 0.770687460899353, 0.7687740921974182, 0.766861617565155, 0.7649584412574768, 0.7630736827850342, 0.7611754536628723, 0.7592877149581909, 0.7574118971824646, 0.7555331587791443, 0.7536648511886597, 0.7517990469932556, 0.7499423027038574, 0.7480874061584473, 0.7462452054023743, 0.7444102168083191, 0.7425727844238281, 0.7407504320144653, 0.7389293909072876, 0.7371187806129456, 0.7353063821792603, 0.7335057258605957, 0.7317039966583252, 0.7299067974090576, 0.728114664554596, 0.7263380289077759, 0.7245664000511169, 0.7227781414985657, 0.7210119366645813, 0.7192277908325195, 0.7174596786499023, 0.7156975269317627, 0.7139360904693604, 0.7121771574020386, 0.7104361653327942, 0.7086893916130066, 0.706953763961792, 0.7052043676376343, 0.7034708857536316, 0.7017486095428467, 0.700027346611023, 0.6982946395874023, 0.6965723633766174, 0.6948680281639099, 0.6931521892547607, 0.6914455890655518, 0.6897352337837219, 0.6880376935005188, 0.6863353252410889, 0.6846345067024231, 0.6829231381416321, 0.6812205910682678, 0.6795232892036438, 0.6778266429901123, 0.676134467124939, 0.6744390726089478, 0.6727595329284668, 0.6710784435272217, 0.6694105267524719, 0.6677436232566833, 0.6660685539245605, 0.6644003391265869, 0.662728488445282, 0.6610645055770874, 0.6594014763832092, 0.6577343940734863, 0.6560749411582947, 0.6544286608695984, 0.6527790427207947, 0.6511191725730896, 0.6494678258895874, 0.6478301882743835, 0.6461824774742126, 0.6445385813713074, 0.6429060697555542, 0.6412733197212219, 0.6396259069442749, 0.6380012631416321, 0.636363685131073, 0.6347407698631287, 0.6331210732460022, 0.6314889788627625, 0.6298807263374329, 0.6282498240470886, 0.6266210675239563, 0.6250129342079163, 0.6233859658241272, 0.6217556595802307, 0.6201391816139221, 0.6185119152069092, 0.6168928742408752, 0.6152817606925964, 0.613677442073822, 0.6120739579200745, 0.6104743480682373, 0.6088683605194092, 0.6072657704353333, 0.6056557297706604, 0.6040495038032532, 0.6024501919746399, 0.6008384227752686], "accuracy_train_first": 0.4078923841823551, "model": "residualv3", "loss_std": [0.2911449372768402, 0.22615855932235718, 0.23582503199577332, 0.24071148037910461, 0.24328620731830597, 0.24497845768928528, 0.2462783008813858, 0.24726209044456482, 0.24815671145915985, 0.2488880306482315, 0.2494978904724121, 0.2500137388706207, 0.2504311501979828, 0.2507781982421875, 0.25105229020118713, 0.2513611316680908, 0.25156375765800476, 0.25177064538002014, 0.2519484758377075, 0.2520785629749298, 0.2522057592868805, 0.2522960901260376, 0.252348929643631, 0.2523972690105438, 0.2524324953556061, 0.2524873614311218, 0.25250449776649475, 0.2525021731853485, 0.25247350335121155, 0.25247979164123535, 0.25244075059890747, 0.2524428963661194, 0.252414345741272, 0.25239187479019165, 0.25237682461738586, 0.25236356258392334, 0.25234293937683105, 0.2523080110549927, 0.2522609829902649, 0.2522270679473877, 0.252183735370636, 0.25211799144744873, 0.25203120708465576, 0.2519455850124359, 0.2518270015716553, 0.25172561407089233, 0.25159940123558044, 0.25151437520980835, 0.251399427652359, 0.2512679100036621, 0.251174658536911, 0.2510582208633423, 0.2509227991104126, 0.25083088874816895, 0.25071197748184204, 0.25059372186660767, 0.2504904270172119, 0.2503569424152374, 0.2502346634864807, 0.25009283423423767, 0.249949648976326, 0.24981175363063812, 0.2496422380208969, 0.2495226114988327, 0.24939538538455963, 0.2492617964744568, 0.24910032749176025, 0.2489609271287918, 0.24882590770721436, 0.2486894726753235, 0.24856685101985931, 0.24842579662799835, 0.24828314781188965, 0.2481459528207779, 0.24801097810268402, 0.24786147475242615, 0.24770469963550568, 0.24755288660526276, 0.2473955899477005, 0.24722257256507874, 0.24707593023777008, 0.24691268801689148, 0.2467561513185501, 0.24659109115600586, 0.2464628368616104, 0.24630782008171082, 0.24614523351192474, 0.2460106909275055, 0.24584779143333435, 0.24568328261375427, 0.2455226182937622, 0.24535717070102692, 0.24519161880016327, 0.2450290471315384, 0.24484674632549286, 0.24469785392284393, 0.24452778697013855, 0.24433943629264832, 0.24417263269424438, 0.24401138722896576, 0.24382351338863373, 0.2436295747756958, 0.2434368133544922, 0.2432798594236374, 0.2431173473596573, 0.24294953048229218, 0.24275550246238708, 0.24258582293987274, 0.24242223799228668, 0.24225273728370667, 0.24208001792430878, 0.2419130802154541, 0.24173292517662048, 0.24154379963874817, 0.24137713015079498, 0.24119959771633148, 0.24101287126541138, 0.24083037674427032, 0.24066953361034393, 0.24047423899173737, 0.24029552936553955, 0.24010084569454193, 0.23989863693714142, 0.23971766233444214, 0.23952066898345947, 0.23934030532836914, 0.23914514482021332, 0.23896971344947815, 0.23877736926078796, 0.23858657479286194, 0.23839490115642548, 0.23820391297340393, 0.23801928758621216, 0.23780988156795502, 0.2376030832529068, 0.23741483688354492, 0.23721246421337128, 0.23702476918697357, 0.23681549727916718, 0.2366129606962204, 0.2364307940006256, 0.23623859882354736, 0.23604288697242737, 0.23583140969276428, 0.23561568558216095, 0.2354060411453247, 0.23520784080028534, 0.2349862903356552, 0.2348051220178604, 0.23459118604660034, 0.23439832031726837, 0.23420847952365875, 0.2339896857738495, 0.23378384113311768, 0.23358950018882751, 0.23337888717651367, 0.23317837715148926, 0.23295943439006805, 0.23275300860404968, 0.23253773152828217, 0.23232628405094147, 0.23213621973991394, 0.23193877935409546, 0.23173299431800842, 0.23154316842556, 0.23133330047130585, 0.23112702369689941, 0.2309006154537201, 0.23069411516189575, 0.23046644032001495, 0.2302321046590805, 0.23001046478748322, 0.22978995740413666, 0.22957105934619904, 0.22932040691375732, 0.22910058498382568, 0.228854238986969, 0.22861413657665253, 0.22838522493839264, 0.22815965116024017, 0.22790905833244324, 0.22766540944576263, 0.2274230271577835, 0.2271765023469925, 0.22692453861236572, 0.22668594121932983, 0.22644764184951782, 0.2262030839920044, 0.22594423592090607, 0.22568194568157196, 0.2254231721162796, 0.22516748309135437, 0.2249419391155243, 0.2246953696012497, 0.2244269847869873, 0.2241697609424591, 0.22392401099205017, 0.2236691564321518, 0.22339962422847748, 0.22313445806503296, 0.2228829711675644, 0.22263255715370178, 0.22237461805343628, 0.22213181853294373, 0.22187666594982147, 0.22161048650741577, 0.22133080661296844, 0.22107632458209991, 0.22079764306545258, 0.22054438292980194, 0.2202584445476532, 0.21999496221542358, 0.21973103284835815, 0.21945428848266602, 0.2191866934299469, 0.218926340341568, 0.21867108345031738, 0.21839112043380737, 0.21812878549098969, 0.21786150336265564, 0.21760061383247375, 0.2173287719488144, 0.21703721582889557, 0.2167641967535019, 0.21648836135864258, 0.21622051298618317, 0.21595707535743713, 0.21568256616592407, 0.21542024612426758, 0.21514862775802612, 0.2148699164390564, 0.21457478404045105, 0.21429698169231415, 0.21400518715381622, 0.21372371912002563, 0.21343323588371277, 0.2131231278181076]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:07 2016", "state": "available"}], "summary": "b1501a26015b568d36ec3c42fe5d0d2c"}