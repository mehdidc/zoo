{"content": {"hp_model": {"f0": 32, "f1": 32, "f2": 32, "f3": 32, "nonlin": "leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.01915756755394025, 0.019167020789887766, 0.02003506568876337, 0.016558435285583123, 0.018788193818426002, 0.02047540723221454, 0.017324119987829484, 0.015514637711641766, 0.015615490264735944, 0.017512033085812576, 0.01639229257943545, 0.01930811668648878, 0.01967685270850197, 0.02335402623328955, 0.021794122243456133, 0.019204198756841144, 0.022454807945725724, 0.02540408083245611, 0.022525581438717523, 0.024648114925660713, 0.023217868996432366, 0.022555004499823384, 0.02021580359014671, 0.020780005135621935, 0.02416123820414057, 0.022378705940059102, 0.01893286795552436, 0.019928753872036024, 0.01972940862357803, 0.021687547917557913, 0.02174826355516762, 0.017885428866074003, 0.019057006073930342, 0.01812037138445583, 0.0188674592013937, 0.01908333936348319, 0.02006340697476992, 0.020764061198672745, 0.017497590936968815, 0.01995849922312835, 0.021166904670488904, 0.021184679163677417, 0.019846763849594298, 0.021527823819707113, 0.02075806918540384, 0.01922655369666252, 0.020081716410256138, 0.02019865379289015, 0.01972010361177784, 0.01873372219920255, 0.021610914493812364, 0.01850955020965856, 0.020326738923285945, 0.01865877093950849, 0.02124909843475014, 0.022783979205577804, 0.021890517489191172, 0.02317703033292563, 0.021529772922187884, 0.022762724868506175, 0.02138317420487969, 0.021852329614986295, 0.02140712283538866, 0.02045593261054562, 0.022082467611982404, 0.02013087405312454, 0.021218250441714128, 0.02050109883753956, 0.020419852578118135, 0.022391315612930226, 0.022214064968087522, 0.02192493432687681, 0.021493074165654182, 0.021451117763657296, 0.02056808021769967, 0.021051109378861538, 0.020356592608709815, 0.020683429155443626, 0.020922003018448872, 0.020775440762926797, 0.02117143179747689, 0.02134158873543883, 0.021206126295201112, 0.0203651279571263, 0.020332784059919496, 0.019540829910949974, 0.02049070209996449, 0.0196690713460834, 0.01946508346899278, 0.018971729704072644, 0.02026230699298818, 0.020415177805169205, 0.019495869515487877, 0.018976271465819244, 0.018914643986060624, 0.02027972347642371, 0.020240549925399913, 0.019744437826698023, 0.018685952946996588, 0.019371523560122228, 0.018889830652826328, 0.019554893512468512, 0.01856275976062831, 0.019546355432150428, 0.020104132072095576, 0.019412698491888973, 0.01960148376027153, 0.019094694022496722, 0.019350529808277197, 0.01988335276625724, 0.020461923088301366, 0.019042011356484343, 0.01888654192553931, 0.020047011854768114, 0.018984256129197703, 0.018644978529465214, 0.01922692467034167, 0.01967845231561609, 0.019251448070207473, 0.019640403378253884, 0.018927282703392025, 0.020174433972123824, 0.019634332845335615, 0.020104473403732403, 0.02022165014850164, 0.02056682325026659, 0.02072848134385513, 0.0198416299034561, 0.020349363861959326, 0.020653283263973116, 0.02005811366555696, 0.019471592688178545, 0.020252044055382914, 0.019817070012324228, 0.019658475874290943, 0.0202759985643918, 0.02045837175829422, 0.020328843670881343, 0.02074033311153702, 0.020698659344691766, 0.020446714597762823, 0.020536248298305877, 0.020013054985860385, 0.021267713953200557, 0.02049027647007086, 0.020262925236364384, 0.020885907469873302, 0.020891614332530915, 0.021253666471498775, 0.019821528077719468, 0.01899877972119242, 0.020284320004892005, 0.020544352354548233, 0.020257047665437806, 0.020330428844606237, 0.020805782315302373, 0.020171851116516894, 0.01943783725514247, 0.020832017744289274, 0.020054067588293246, 0.02050401039719253, 0.019797890192266868, 0.020633922380559538, 0.020771440952858284, 0.02138031676088456, 0.020312552049123477, 0.02051720574388353, 0.02054043343878735, 0.019981482448472265, 0.021090381686630653, 0.01997853966979613, 0.020266506385497724, 0.0211375605732113, 0.020478146524300675, 0.019568593294438403, 0.020182648532410134, 0.019536713461598614, 0.0204307138909253, 0.020192204341905662, 0.021453267824387612, 0.019947855036133475, 0.02044088352452667, 0.020259943584728428, 0.02056077543938147, 0.020662809850483146, 0.02053547186009311, 0.020731903824929565, 0.02052042560803512, 0.020723125360699982, 0.02060920353278966, 0.020185481507921932, 0.019746765444203333, 0.020731903824929565, 0.02049815620977932, 0.02081949351398058, 0.02062593269392956, 0.02135079326376508, 0.020197289443508536, 0.020249027793606603, 0.020689518896441113, 0.020924982021725977, 0.020509784132426104, 0.02105146489667951, 0.021079759656875537, 0.02022785062778089, 0.020281837209034016, 0.019925049326053018, 0.0202180566142596, 0.020368546181755848, 0.020135340899770843, 0.01997916420409812, 0.020903693299969172, 0.020040390612070403, 0.020925822824773715, 0.02019282226787459, 0.02011777264896628, 0.019874170385499008, 0.020891549881190536, 0.0206018087631497, 0.020277587869213718, 0.020356837583478372, 0.0200775742506984, 0.020492339774203748, 0.020564477378667737, 0.020472284940940593, 0.021451058421809038, 0.020063830286463497, 0.02125778826159452, 0.02054059434617255, 0.020398399971824424, 0.020983401805877686, 0.02025948721596707, 0.020095400188709152, 0.020292934410446866, 0.0212935711706021, 0.02048166617531339, 0.020207775645343595, 0.02017674114429026, 0.020667813910844837], "moving_avg_accuracy_train": [0.03910604697305278, 0.07950652411521779, 0.12259921464043465, 0.16353285802965806, 0.2033972544606808, 0.24149086177116494, 0.2778580451469277, 0.31184136885644126, 0.3449837715437484, 0.37668832905161054, 0.4067870036157647, 0.4353261988971838, 0.4617368129325762, 0.48654327378702067, 0.5095175166833278, 0.5311499354018998, 0.5517511713283026, 0.5706804393179807, 0.5887397378384251, 0.6055789358603689, 0.6208783733063088, 0.6354917518302368, 0.6493529989374955, 0.6622000730459147, 0.6741436199529788, 0.6852136827050508, 0.6957230770545162, 0.7055093419023591, 0.7145122206677803, 0.7227985343614305, 0.7307515094309629, 0.7382021557435422, 0.7446775837415394, 0.7511748955039672, 0.7572201858365993, 0.7628051424109774, 0.7680315661255367, 0.7727770920007553, 0.7773435394313553, 0.7817159397390964, 0.7855882489005688, 0.7894593378970937, 0.7929665694820612, 0.796234721100208, 0.7993317704291407, 0.8023167524739897, 0.8051357337476779, 0.8077426074071017, 0.8101004194446307, 0.8124666269522256, 0.8146565233828336, 0.8165972753334945, 0.8185949600140616, 0.8204603415908669, 0.8220974044290579, 0.8237010053655818, 0.8251745091917959, 0.8266144507317802, 0.8279266741594328, 0.8293100352883861, 0.8305434706092152, 0.8317440990062578, 0.8328572526957483, 0.8339219060829657, 0.834912610165976, 0.8358066410871322, 0.8367158645637826, 0.8374319312427863, 0.8382437298705379, 0.8389766737843238, 0.8396688393412458, 0.84036157885558, 0.8409733465767958, 0.8415286599211464, 0.8421424463203663, 0.8426854453379697, 0.8431485678169081, 0.8436746960908189, 0.8441156594540054, 0.8445659688546734, 0.8450108108938553, 0.8454761647493295, 0.8458578889847602, 0.8462199698894864, 0.8466201753680073, 0.8469362906177514, 0.8472533103770357, 0.8475735053925344, 0.8478802820969594, 0.8481773435190465, 0.848421375213192, 0.8486224746450843, 0.8488545092611407, 0.8491679360632012, 0.8494013362553307, 0.849622986123476, 0.8498294464512354, 0.8500639446759438, 0.8503076533079711, 0.8504572005636911, 0.8506127194331249, 0.8507922499941959, 0.8509909938312935, 0.8511233963573096, 0.8513053376485812, 0.8514946974964491, 0.8516255217321309, 0.8516967245192353, 0.8518584632776292, 0.8520063893578119, 0.8521255358883005, 0.8522282256145775, 0.8523228994193988, 0.8524592230687287, 0.8525423868233637, 0.8526056084584877, 0.8527229617991468, 0.8528053283176449, 0.852918985714055, 0.8530143019243955, 0.8530861356208449, 0.8531810489309918, 0.8532106312898766, 0.8532907698843109, 0.8533582443216826, 0.8533980810248503, 0.8534454877041113, 0.8535346566916368, 0.8535870069946953, 0.8536480731603051, 0.8536774560724493, 0.853762029413617, 0.8538079184861441, 0.853828292312133, 0.8538675550948085, 0.8539261430873117, 0.8539742580317644, 0.8539687333567718, 0.8540335516623829, 0.8540360124683668, 0.8540196260032762, 0.8540583566073137, 0.8540792632580902, 0.8541120301366464, 0.8541415924249843, 0.8541425497499462, 0.8542224664019358, 0.8542548638589644, 0.8542817324702995, 0.8542593751954918, 0.8542973823684029, 0.8543478648656896, 0.854360783078733, 0.85433982133832, 0.8543558690529098, 0.8543307484174602, 0.8543127901431744, 0.8543594427629931, 0.8543781425839161, 0.8543879969763182, 0.8543852401854324, 0.8544060105617304, 0.8544131142051697, 0.8543846663009411, 0.8544054940656882, 0.8544265642027702, 0.8544130473404481, 0.8544287118524351, 0.854468386550128, 0.8544901428851945, 0.8545329750748496, 0.8545180456229201, 0.8545046091161836, 0.8544972026065586, 0.8545161133848008, 0.8545377833828378, 0.854554925183443, 0.8545261749766067, 0.8545468388154631, 0.8545212223942342, 0.8545191300032327, 0.8544660935775218, 0.8544788507122484, 0.8545018857799125, 0.8545017270503431, 0.8544899944985018, 0.8545026506411212, 0.8545256308647075, 0.8545439879171256, 0.8545535338178734, 0.8545481742356892, 0.854568963297447, 0.85453419503041, 0.8545470814174576, 0.854565654612229, 0.8545660944458565, 0.8545687793961121, 0.8545712319001609, 0.8545827757978617, 0.8545931292569736, 0.8545977610237366, 0.854557787835261, 0.8546008309763381, 0.854588416529498, 0.85458654412258, 0.854591870451601, 0.8545966280989012, 0.8545893202862426, 0.8545804181060402, 0.8545514437557536, 0.8545556298238384, 0.8545826127243911, 0.8546022470372695, 0.8545897270331549, 0.8546040356663566, 0.8546215276850384, 0.8546349093042238, 0.8546540363543754, 0.8546409516673505, 0.854629211497847, 0.8546395356357607, 0.8546256119206065, 0.8546200560233963, 0.8545917681789932, 0.8545570445726111, 0.8545536590637626, 0.8545668881474656, 0.8545369776930457, 0.8545030828376392, 0.8544981180558594, 0.8544866743058289, 0.8544973012700873, 0.8545649942581579, 0.8546073167569452, 0.854601229178473, 0.8545980755066575, 0.8546115492925088, 0.8546050384604802, 0.8546038290092735], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 995165243, "moving_var_accuracy_train": [0.0137635461887275, 0.027076978549686125, 0.04108210048503655, 0.05205395888657792, 0.061151093923207785, 0.06809609079221535, 0.07318962995317103, 0.07626446357096422, 0.07852378691685642, 0.07971801892609481, 0.0798995889281551, 0.07924000104113836, 0.0775936857405627, 0.07537256166761422, 0.07258564803077948, 0.06953873708424214, 0.06640456167107574, 0.06298896018359361, 0.05962530853268899, 0.0562148049896202, 0.052699979566118216, 0.04935193909645879, 0.0461459527291293, 0.043016783274541295, 0.03999893976157834, 0.037101962389433814, 0.034385792476823614, 0.03180915204619039, 0.02935770327615519, 0.027039899900205843, 0.024905158222294707, 0.02291425157434159, 0.021000206926722647, 0.019280121775294028, 0.017681019414616958, 0.016193643132594464, 0.01482011836293199, 0.013540786669130113, 0.012374379981445003, 0.01130900294336072, 0.010313055653202858, 0.009416618058053718, 0.008585662312763698, 0.007823223416480087, 0.00712722650574466, 0.006494694915442829, 0.005916745322691186, 0.005386232902907846, 0.004897643111055912, 0.004458269241671306, 0.004055603134895282, 0.0036839414846117006, 0.0033514640328972836, 0.003047634465451319, 0.0027669907915318574, 0.0025134355360512542, 0.0022816329041789357, 0.002072130498508193, 0.0018804148215740954, 0.0017095965315345777, 0.0015523291425971403, 0.0014100698052674937, 0.001280214824968581, 0.0011623947239859432, 0.0010549887028081862, 0.0009566834541192179, 0.0008684552946817259, 0.0007862245286125693, 0.0007135332288594873, 0.000647014767000342, 0.00058662512872401, 0.0005322816081640894, 0.0004824217850501736, 0.0004369549627388817, 0.00039665007015979835, 0.00035963869454188333, 0.00032560516696217707, 0.0002955359489114354, 0.00026773239220934634, 0.00024278415999538243, 0.00022028670395425636, 0.00020020702145607307, 0.00018149773983770176, 0.0001645278890880381, 0.00014951658000457766, 0.00013546428166420698, 0.00012282236724777624, 0.00011146285415455061, 0.00010116357625649648, 9.184142802727796e-05, 8.319324843427814e-05, 7.5237892424417e-05, 6.819866374941108e-05, 6.226292461671903e-05, 5.652691300222185e-05, 5.131637967843954e-05, 4.656837451304246e-05, 4.240644181826122e-05, 3.870034271235649e-05, 3.5031587876361466e-05, 3.174610415747497e-05, 2.8861574742953856e-05, 2.633090928371689e-05, 2.385559221540419e-05, 2.1767956695090095e-05, 1.991387539344234e-05, 1.8076522679873603e-05, 1.631449894390912e-05, 1.491848388321953e-05, 1.3623574621681887e-05, 1.2388980221061062e-05, 1.124498881790056e-05, 1.0201158099984638e-05, 9.348299526285955e-06, 8.47571546442239e-06, 7.664116694309872e-06, 7.021651283953712e-06, 6.380544345883716e-06, 5.858751945123881e-06, 5.354643370194623e-06, 4.865619752685344e-06, 4.460134605404314e-06, 4.021997188478668e-06, 3.677597218491917e-06, 3.3508126939305046e-06, 3.03001409081087e-06, 2.7472392208767855e-06, 2.5440752738158133e-06, 2.31433273450714e-06, 2.1164611502970285e-06, 1.9125852350019313e-06, 1.7857005618282802e-06, 1.6260827684421107e-06, 1.4672103266666855e-06, 1.334363388930901e-06, 1.2318200258278905e-06, 1.1294734541622437e-06, 1.016800807049985e-06, 9.52933441025662e-07, 8.576945970179113e-07, 7.743417834596232e-07, 7.104081423156207e-07, 6.433011205043015e-07, 5.88634023426635e-07, 5.376359811099613e-07, 4.838806312387091e-07, 4.929726095017425e-07, 4.531217055488799e-07, 4.143068354696414e-07, 3.773747815541272e-07, 3.5263821013299564e-07, 3.403107319104094e-07, 3.077815807734943e-07, 2.8095797374641205e-07, 2.551799386638137e-07, 2.3534136172600555e-07, 2.1470972209128463e-07, 2.1282695230575604e-07, 1.9469140679812275e-07, 1.7609624756483088e-07, 1.585550218722392e-07, 1.46582196469076e-07, 1.3237813257319267e-07, 1.2642386861090578e-07, 1.1768564380908644e-07, 1.0991263551805886e-07, 1.0056572206955153e-07, 9.271754228469839e-08, 9.761252278951208e-08, 9.211131355032165e-08, 9.941155043118663e-08, 9.147639220227754e-08, 8.395361040155831e-08, 7.605195682483012e-08, 7.166531894590079e-08, 6.872508638565955e-08, 6.449714969896159e-08, 6.548660426726644e-08, 6.278089196712918e-08, 6.240861209957853e-08, 5.620715379054834e-08, 7.590220048113613e-08, 6.977668081091724e-08, 6.757454181041354e-08, 6.081731438505775e-08, 5.597445790091975e-08, 5.18186136248448e-08, 5.138956834704801e-08, 4.928344387368749e-08, 4.517521747609447e-08, 4.0916221819182836e-08, 4.07142654361806e-08, 4.752233042740162e-08, 4.426462812492709e-08, 4.294283738858525e-08, 3.865029473230654e-08, 3.4850145879952765e-08, 3.141926427694284e-08, 2.9476692016384252e-08, 2.749376985499253e-08, 2.4937472239611468e-08, 3.682442718775709e-08, 4.981639241301621e-08, 4.6221819584839716e-08, 4.163119079535644e-08, 3.7723399743388346e-08, 3.415477663954944e-08, 3.1219936108281506e-08, 2.8811181808644343e-08, 3.3485680398533015e-08, 3.029482085276463e-08, 3.381803106760726e-08, 3.3905784140692394e-08, 3.19259602538748e-08, 3.0575997085389105e-08, 3.0272133834932215e-08, 2.885653003963347e-08, 2.926347346319898e-08, 2.7878007427719132e-08, 2.6330690904705564e-08, 2.4656912227188906e-08, 2.3936049597709232e-08, 2.182025658222494e-08, 2.6840050192777933e-08, 3.5007604735141845e-08, 3.1609999293090525e-08, 3.002407726439846e-08, 3.507338709038433e-08, 4.190579938855935e-08, 3.793706097279201e-08, 3.53219896083325e-08, 3.280618197164086e-08, 7.076662947986929e-08, 7.981071166436658e-08, 7.216316800283721e-08, 6.503636201583502e-08, 6.016661196078295e-08, 5.453146916805245e-08, 4.909148720124058e-08], "duration": 198694.632086, "accuracy_train": [0.3910604697305279, 0.4431108183947029, 0.5104334293673864, 0.5319356485326688, 0.5621768223398855, 0.5843333275655224, 0.6051626955287929, 0.6176912822420635, 0.6432653957295128, 0.6620293466223699, 0.6776750746931525, 0.6921789564299556, 0.6994323392511074, 0.7098014214770211, 0.7162857027500923, 0.7258417038690477, 0.7371622946659284, 0.7410438512250831, 0.7512734245224253, 0.7571317180578626, 0.7585733103197674, 0.7670121585455887, 0.7741042229028239, 0.7778237400216869, 0.781635542116556, 0.7848442474736989, 0.7903076261997047, 0.7935857255329457, 0.7955381295565707, 0.797375357604282, 0.8023282850567552, 0.8052579725567552, 0.8029564357235143, 0.8096507013658176, 0.8116277988302879, 0.8130697515803802, 0.8150693795565707, 0.8154868248777224, 0.8184415663067552, 0.821067542508767, 0.8204390313538206, 0.8242991388658176, 0.82453165374677, 0.825648085663529, 0.8272052143895349, 0.8291815908776301, 0.8305065652108711, 0.8312044703419158, 0.831320727782392, 0.8337624945205795, 0.8343655912583056, 0.8340640428894426, 0.8365741221391657, 0.8372487757821151, 0.8368309699727758, 0.8381334137942967, 0.8384360436277224, 0.8395739245916389, 0.8397366850083056, 0.8417602854489663, 0.8416443884966777, 0.8425497545796419, 0.8428756359011628, 0.8435037865679217, 0.8438289469130675, 0.8438529193775378, 0.844898875853636, 0.8438765313538206, 0.8455499175203026, 0.8455731690083979, 0.8458983293535437, 0.8465962344845883, 0.8464792560677371, 0.8465264800203026, 0.8476665239133444, 0.8475724364964008, 0.8473166701273532, 0.8484098505560169, 0.8480843297226835, 0.8486187534606865, 0.8490143892464931, 0.8496643494485974, 0.849293407103636, 0.8494786980320228, 0.8502220246746955, 0.8497813278654485, 0.8501064882105942, 0.8504552605320228, 0.8506412724367847, 0.8508508963178294, 0.8506176604605021, 0.8504323695321151, 0.8509428208056479, 0.851988777281746, 0.851501937984496, 0.8516178349367847, 0.8516875894010705, 0.8521744286983205, 0.8525010309962164, 0.8518031258651717, 0.8520123892580289, 0.8524080250438354, 0.8527796883651717, 0.8523150190914545, 0.852942809270026, 0.8531989361272609, 0.8528029398532669, 0.8523375496031746, 0.8533141121031746, 0.8533377240794574, 0.8531978546626985, 0.8531524331510705, 0.8531749636627908, 0.8536861359126985, 0.8532908606150794, 0.8531746031746033, 0.8537791418650794, 0.8535466269841271, 0.853941902281746, 0.8538721478174603, 0.853732638888889, 0.8540352687223146, 0.8534768725198413, 0.8540120172342194, 0.8539655142580289, 0.8537566113533592, 0.8538721478174603, 0.8543371775793651, 0.8540581597222223, 0.8541976686507937, 0.853941902281746, 0.8545231894841271, 0.854220920138889, 0.8540116567460319, 0.854220920138889, 0.8544534350198413, 0.8544072925318383, 0.8539190112818383, 0.8546169164128831, 0.8540581597222223, 0.8538721478174603, 0.8544069320436508, 0.8542674231150794, 0.8544069320436508, 0.854407653020026, 0.8541511656746033, 0.8549417162698413, 0.8545464409722223, 0.8545235499723146, 0.8540581597222223, 0.8546394469246033, 0.8548022073412699, 0.8544770469961241, 0.8541511656746033, 0.8545002984842194, 0.8541046626984128, 0.8541511656746033, 0.8547793163413622, 0.8545464409722223, 0.8544766865079365, 0.8543604290674603, 0.8545929439484128, 0.8544770469961241, 0.8541286351628831, 0.8545929439484128, 0.854616195436508, 0.8542913955795497, 0.8545696924603176, 0.8548254588293651, 0.8546859499007937, 0.854918464781746, 0.8543836805555556, 0.8543836805555556, 0.8544305440199336, 0.8546863103889812, 0.8547328133651717, 0.854709201388889, 0.8542674231150794, 0.8547328133651717, 0.8542906746031746, 0.8545002984842194, 0.8539887657461241, 0.8545936649247878, 0.854709201388889, 0.8545002984842194, 0.8543844015319306, 0.8546165559246955, 0.8547324528769842, 0.854709201388889, 0.8546394469246033, 0.8544999379960319, 0.8547560648532669, 0.8542212806270765, 0.854663058900886, 0.8547328133651717, 0.8545700529485051, 0.8545929439484128, 0.8545933044366003, 0.8546866708771688, 0.8546863103889812, 0.8546394469246033, 0.8541980291389812, 0.8549882192460319, 0.8544766865079365, 0.8545696924603176, 0.8546398074127908, 0.8546394469246033, 0.8545235499723146, 0.8545002984842194, 0.8542906746031746, 0.8545933044366003, 0.8548254588293651, 0.8547789558531746, 0.8544770469961241, 0.8547328133651717, 0.8547789558531746, 0.8547553438768919, 0.8548261798057402, 0.8545231894841271, 0.8545235499723146, 0.8547324528769842, 0.8545002984842194, 0.8545700529485051, 0.8543371775793651, 0.8542445321151717, 0.8545231894841271, 0.8546859499007937, 0.8542677836032669, 0.8541980291389812, 0.8544534350198413, 0.8543836805555556, 0.8545929439484128, 0.8551742311507937, 0.8549882192460319, 0.8545464409722223, 0.8545696924603176, 0.8547328133651717, 0.8545464409722223, 0.8545929439484128], "end": "2016-01-26 05:15:29.379000", "learning_rate_per_epoch": [0.0018318345537409186, 0.001753418822772801, 0.0016783599276095629, 0.0016065140953287482, 0.0015377437230199575, 0.0014719172613695264, 0.0014089086325839162, 0.0013485972303897142, 0.0012908675707876682, 0.0012356091756373644, 0.001182716223411262, 0.0011320874327793717, 0.0010836259461939335, 0.0010372389806434512, 0.000992837711237371, 0.0009503370965830982, 0.0009096558205783367, 0.0008707160013727844, 0.000833443074952811, 0.0007977657369337976, 0.0007636156515218318, 0.0007309273933060467, 0.0006996384472586215, 0.000669688917696476, 0.0006410214118659496, 0.0006135810981504619, 0.000587315415032208, 0.0005621740710921586, 0.0005381089868023992, 0.0005150740616954863, 0.0004930251743644476, 0.0004719201533589512, 0.00045171857345849276, 0.00043238175567239523, 0.0004138727090321481, 0.00039615598507225513, 0.0003791976487264037, 0.0003629652492236346, 0.00034742773277685046, 0.00033255532616749406, 0.0003183195658493787, 0.0003046932106371969, 0.00029165015439502895, 0.000279165426036343, 0.000267215131316334, 0.00025577639462426305, 0.00024482732987962663, 0.00023434695322066545, 0.00022431521210819483, 0.00021471291256602854, 0.00020552166097331792, 0.00019672384951263666, 0.00018830265616998076, 0.00018024194287136197, 0.0001725262845866382, 0.00016514092567376792, 0.0001580717071192339, 0.00015130509564187378, 0.00014482815458904952, 0.00013862847117707133, 0.00013269417104311287, 0.00012701390369329602, 0.00012157679884694517, 0.00011637243733275682, 0.00011139085836475715, 0.00010662253043847159, 0.00010205832222709432, 9.768949530553073e-05, 9.350768232252449e-05, 8.95048797247e-05, 8.567342592868954e-05, 8.2005986769218e-05, 7.849554094718769e-05, 7.513536547776312e-05, 7.191902841441333e-05, 6.884037429699674e-05, 6.589350959984586e-05, 6.307279545580968e-05, 6.037282582838088e-05, 5.7788434787653387e-05, 5.531467468244955e-05, 5.294680886436254e-05, 5.0680304411798716e-05, 4.851082121604122e-05, 4.6434208343271166e-05, 4.4446489482652396e-05, 4.2543859308352694e-05, 4.072267620358616e-05, 3.89794513466768e-05, 3.73108487110585e-05, 3.5713677789317444e-05, 3.4184875403298065e-05, 3.272151661803946e-05, 3.1320800189860165e-05, 2.9980044928379357e-05, 2.869668423954863e-05, 2.7468260668683797e-05, 2.629242226248607e-05, 2.516691711207386e-05, 2.408959153399337e-05, 2.3058384613250382e-05, 2.207131910836324e-05, 2.112650872732047e-05, 2.0222141756676137e-05, 1.935648833750747e-05, 1.852789137046784e-05, 1.7734764696797356e-05, 1.6975589460344054e-05, 1.6248912288574502e-05, 1.5553341654594988e-05, 1.4887546967656817e-05, 1.4250253116188105e-05, 1.3640239558299072e-05, 1.3056339412287343e-05, 1.2497433999669738e-05, 1.1962453754676972e-05, 1.1450374586274847e-05, 1.0960216059174854e-05, 1.0491039574844763e-05, 1.0041947462013923e-05, 9.612080248189159e-06, 9.200613931170665e-06, 8.806761798041407e-06, 8.429768968198914e-06, 8.06891421234468e-06, 7.723506314505357e-06, 7.392884526780108e-06, 7.076415840856498e-06, 6.773494533263147e-06, 6.483540346380323e-06, 6.205998033692595e-06, 5.940336905041477e-06, 5.686047643393977e-06, 5.442644123831997e-06, 5.2096597755735274e-06, 4.9866489462147e-06, 4.773184627993032e-06, 4.568858003040077e-06, 4.373277988634072e-06, 4.186070327705238e-06, 4.006876224593725e-06, 3.835353254544316e-06, 3.6711726352223195e-06, 3.5140201362082735e-06, 3.363594714755891e-06, 3.219608743165736e-06, 3.0817864171694964e-06, 2.9498637559299823e-06, 2.8235883746674517e-06, 2.7027185751649085e-06, 2.5870228910207516e-06, 2.4762796329014236e-06, 2.370277115915087e-06, 2.268812295369571e-06, 2.1716907667723717e-06, 2.078726765830652e-06, 1.9897422589565394e-06, 1.9045669432671275e-06, 1.8230377918371232e-06, 1.7449987126383348e-06, 1.6703002074791584e-06, 1.5987993720045779e-06, 1.5303592135751387e-06, 1.4648488786406233e-06, 1.402142856932187e-06, 1.3421210951491958e-06, 1.284668655898713e-06, 1.229675604008662e-06, 1.1770366654673126e-06, 1.1266510000496055e-06, 1.078422201317153e-06, 1.032257955557725e-06, 9.880699280984118e-07, 9.457734222451109e-07, 9.052874929693644e-07, 8.665346626912651e-07, 8.294407507491997e-07, 7.93934702869592e-07, 7.599485911669035e-07, 7.274173299265385e-07, 6.962786187614256e-07, 6.664728857685986e-07, 6.379430601555214e-07, 6.106345153966686e-07, 5.844949555466883e-07, 5.59474358396983e-07, 5.355248049454531e-07, 5.126004793964967e-07, 4.906574986307533e-07, 4.6965382693997526e-07, 4.495492760270281e-07, 4.3030533447563357e-07, 4.118851677503699e-07, 3.942535329315433e-07, 3.773766366066411e-07, 3.6122219171375036e-07, 3.4575927543301077e-07, 3.309583007649053e-07, 3.1679090284342237e-07, 3.032299673577654e-07, 2.9024954528722446e-07, 2.778247676360479e-07, 2.6593187385515193e-07, 2.5454806973357336e-07, 2.436515842418885e-07, 2.332215416345207e-07, 2.232379756605951e-07, 2.1368178693137452e-07, 2.0453467186598573e-07, 1.9577912269141962e-07, 1.8739837059911224e-07, 1.7937637153409014e-07, 1.716977777732609e-07, 1.6434788108199427e-07, 1.5731261271412222e-07, 1.5057850077937474e-07, 1.4413265603252512e-07, 1.3796274345168058e-07, 1.3205693960571807e-07, 1.26403946865139e-07, 1.2099295076950511e-07, 1.1581358450030166e-07, 1.1085592888093743e-07, 1.0611049816589002e-07, 1.0156820451356907e-07, 9.722035798631623e-08, 9.305863102326839e-08, 8.907505133493032e-08, 8.526200190317468e-08, 8.161217834867784e-08, 7.811858893091994e-08, 7.477455454818482e-08, 7.157366610499594e-08, 6.850979872297103e-08, 6.557708331911272e-08, 6.276991371123586e-08, 6.008291109083075e-08, 5.7510931128490483e-08, 5.5049049763056246e-08], "accuracy_valid": [0.3957696018448795, 0.44722444465361444, 0.5094317700489458, 0.5238566570971386, 0.558861422251506, 0.578270601939006, 0.5977415521460843, 0.6113840126129518, 0.6378335608057228, 0.6560132130082832, 0.6731339420180723, 0.6855454042733433, 0.6889839631965362, 0.6993099350527108, 0.7062782379518072, 0.7136127517884037, 0.7293201124811747, 0.7307040662650602, 0.7377738493034638, 0.744396531438253, 0.7437655897025602, 0.750377976750753, 0.7578036756400602, 0.757335984563253, 0.7626261883471386, 0.7645484280873494, 0.7671824995293675, 0.7700518872364458, 0.7694312405873494, 0.7700518872364458, 0.7753215008471386, 0.7742625776543675, 0.7732154202748494, 0.7803263836596386, 0.7807837796498494, 0.7842326336596386, 0.7843752941453314, 0.7839987881212349, 0.7850665356739458, 0.7891566265060241, 0.7871829113328314, 0.7892581066453314, 0.7904788097703314, 0.7915980327560241, 0.7919848338667168, 0.7942835796310241, 0.7932967220444277, 0.7929408061935241, 0.7957793086408133, 0.7960131541792168, 0.7971323771649097, 0.7964808452560241, 0.7983530802899097, 0.7973353374435241, 0.7992075724774097, 0.7999605845256024, 0.7993605280496988, 0.8007135965737951, 0.7997267389871988, 0.8037653543862951, 0.8022902155496988, 0.8040094950112951, 0.8035109186746988, 0.802147555064006, 0.8046198465737951, 0.8030123423381024, 0.8033888483621988, 0.8055964090737951, 0.8047316217996988, 0.8043757059487951, 0.8044874811746988, 0.8054743387612951, 0.8051081278237951, 0.8063288309487951, 0.8046095514871988, 0.8050978327371988, 0.8042433405496988, 0.8060743952371988, 0.8043654108621988, 0.8060743952371988, 0.8063288309487951, 0.8057184793862951, 0.8066847467996988, 0.8057081842996988, 0.8064406061746988, 0.8068068171121988, 0.8054537485881024, 0.8065626764871988, 0.8066847467996988, 0.8058199595256024, 0.8064406061746988, 0.8061861704631024, 0.8077833796121988, 0.8067965220256024, 0.8068068171121988, 0.8070509577371988, 0.8074274637612951, 0.8082819559487951, 0.8074068735881024, 0.8085158014871988, 0.8081495905496988, 0.8084040262612951, 0.8090040827371988, 0.8079054499246988, 0.8081598856362951, 0.8081495905496988, 0.8082716608621988, 0.8091261530496988, 0.8082716608621988, 0.8077833796121988, 0.8088820124246988, 0.8087599421121988, 0.8088820124246988, 0.8080275202371988, 0.8081495905496988, 0.8088717173381024, 0.8082716608621988, 0.8077833796121988, 0.8097365046121988, 0.8093702936746988, 0.8094923639871988, 0.8088820124246988, 0.8093702936746988, 0.8083937311746988, 0.8088820124246988, 0.8092482233621988, 0.8092482233621988, 0.8090040827371988, 0.8087599421121988, 0.8088820124246988, 0.8097365046121988, 0.8089937876506024, 0.8091261530496988, 0.8092379282756024, 0.8101027155496988, 0.8101130106362951, 0.8107233621987951, 0.8101130106362951, 0.8102350809487951, 0.8103571512612951, 0.8107233621987951, 0.8099806452371988, 0.8101027155496988, 0.8109880929969879, 0.8103571512612951, 0.8106012918862951, 0.8108557275978916, 0.8108557275978916, 0.8108557275978916, 0.8101027155496988, 0.8093599985881024, 0.8108454325112951, 0.8101233057228916, 0.8107130671121988, 0.8104792215737951, 0.8112219385353916, 0.8115778543862951, 0.8105909967996988, 0.8104689264871988, 0.8102247858621988, 0.8101130106362951, 0.8103571512612951, 0.8108454325112951, 0.8108557275978916, 0.8101233057228916, 0.8096247293862951, 0.8104792215737951, 0.8104792215737951, 0.8104792215737951, 0.8108557275978916, 0.8106012918862951, 0.8105909967996988, 0.8114660791603916, 0.8110895731362951, 0.8107130671121988, 0.8107233621987951, 0.8099806452371988, 0.8110998682228916, 0.8106012918862951, 0.8107336572853916, 0.8103571512612951, 0.8107233621987951, 0.8104792215737951, 0.8106012918862951, 0.8109777979103916, 0.8110998682228916, 0.8110998682228916, 0.8114557840737951, 0.8119543604103916, 0.8097365046121988, 0.8103571512612951, 0.8107233621987951, 0.8110998682228916, 0.8099909403237951, 0.8106115869728916, 0.8113440088478916, 0.8114660791603916, 0.8103571512612951, 0.8108454325112951, 0.8107336572853916, 0.8115881494728916, 0.8099909403237951, 0.8109777979103916, 0.8109777979103916, 0.8092482233621988, 0.8099909403237951, 0.8114557840737951, 0.8107233621987951, 0.8106012918862951, 0.8107233621987951, 0.8102350809487951, 0.8110998682228916, 0.8107233621987951, 0.8108557275978916, 0.8102350809487951, 0.8113337137612951, 0.8115778543862951, 0.8112219385353916, 0.8098688700112951, 0.8104792215737951, 0.8106012918862951, 0.8108454325112951, 0.8099909403237951, 0.8108454325112951, 0.8109675028237951, 0.8114660791603916, 0.8119440653237951, 0.8113440088478916, 0.8108557275978916, 0.8102350809487951, 0.8109777979103916, 0.8113337137612951, 0.8109777979103916, 0.8102350809487951, 0.8109777979103916, 0.8107233621987951, 0.8108454325112951, 0.8107233621987951, 0.8112219385353916], "accuracy_test": 0.8031867825255101, "start": "2016-01-23 22:03:54.747000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0, 211.0, 212.0, 213.0, 214.0, 215.0, 216.0, 217.0, 218.0, 219.0, 220.0, 221.0, 222.0, 223.0, 224.0, 225.0, 226.0, 227.0, 228.0, 229.0, 230.0, 231.0, 232.0, 233.0, 234.0, 235.0, 236.0, 237.0, 238.0], "accuracy_train_last": 0.8545929439484128, "batch_size_eval": 1024, "accuracy_train_std": [0.015863263723406056, 0.012471136755622527, 0.01628040722490533, 0.01610078200963555, 0.01727169240160209, 0.016575742185123005, 0.016543710782442208, 0.014738788403522841, 0.014519934975178322, 0.012605676799178721, 0.01418013433124635, 0.013760204627239127, 0.013316357871420624, 0.015049323969545147, 0.013155131550480167, 0.0139547094543782, 0.015642377101453254, 0.01395748103608708, 0.014390149537821723, 0.014613548890675843, 0.014670090399687497, 0.014628484238422735, 0.013294483378933976, 0.013615409669066226, 0.014252628045130252, 0.013299614821359912, 0.012798397855620676, 0.01356647858618821, 0.013320804577274744, 0.01422355329490168, 0.01201945948521616, 0.012384923116626041, 0.012439137825609636, 0.011782751667872658, 0.011986675878118025, 0.011806534608187006, 0.01219400859083025, 0.011629175094052806, 0.012112469629974948, 0.011732681068742475, 0.011793696382241836, 0.0123825013858273, 0.012419097453684465, 0.012235121924621953, 0.011766110106782269, 0.012145095829734191, 0.011584697315411672, 0.011672740463525134, 0.011918057977365554, 0.011434830498416634, 0.011494395503456645, 0.011866834109938254, 0.011777176903631491, 0.011593818397605775, 0.011272580428719975, 0.011307348514339672, 0.011512416086475151, 0.011868051447635449, 0.011773641252247707, 0.011606034850838184, 0.011699207911116734, 0.011017452125117361, 0.011803181785671748, 0.011854798405523535, 0.012015195156621217, 0.01160788064911918, 0.011560524676224936, 0.012060972478787638, 0.011739305509130708, 0.0112970193075775, 0.011765039009975352, 0.011384730390655596, 0.01078457090013908, 0.011122705890385874, 0.011786913734456414, 0.010882534969279878, 0.010986603813831363, 0.011434954087258918, 0.011433902175661708, 0.011602077281695496, 0.011817532424518672, 0.011313259671319988, 0.011023427531945712, 0.010920296315852989, 0.011631557918083606, 0.010816007482341699, 0.011488614187477143, 0.011488981343781787, 0.011274133938463183, 0.011277889015076075, 0.01107836204723929, 0.011760875306972089, 0.011160556309031492, 0.0113340583873961, 0.01164259217373049, 0.011829655230796583, 0.011624847498488206, 0.011742755503687148, 0.011725123701516446, 0.011723516130268606, 0.011617123490384292, 0.011601800655372679, 0.01159625541353864, 0.011974092365356867, 0.01146059086668859, 0.011933044779923367, 0.011633871405848697, 0.01144227335117534, 0.011371940228538175, 0.011690311241723175, 0.01109332100703493, 0.011757268072907804, 0.011213739789588756, 0.011485691211711436, 0.011318465744928674, 0.011444257621431952, 0.011446304523859177, 0.011696512863521248, 0.011769125034176268, 0.012118365888676355, 0.012157205825939942, 0.011933964988313893, 0.012026704696140022, 0.011890161383140611, 0.01158051982988604, 0.012090081597842639, 0.011546501597081392, 0.011694879585805087, 0.01178458780036, 0.012399088424424292, 0.01155298464085914, 0.011568985539119895, 0.01146033170080514, 0.011527569939883235, 0.011755703941409337, 0.011528656382580034, 0.011643971111255489, 0.01172447324866141, 0.011845465491102767, 0.011968131167490223, 0.01158184502830837, 0.011770104970753235, 0.011686948782887946, 0.012022058696292559, 0.011963125314043178, 0.011387902750855093, 0.011554396229332452, 0.011828021303528308, 0.011802095212275314, 0.011773021330494465, 0.011565145161009505, 0.011738314947404467, 0.0120402175480834, 0.011801122204675347, 0.011797323279435109, 0.011636920058056952, 0.011467382071352356, 0.011929743787136899, 0.011605153413368912, 0.011494192049229107, 0.011480598675736043, 0.011892809746455853, 0.011505973241805759, 0.011700683426879662, 0.011791184475829249, 0.012144807179513295, 0.012087314202547515, 0.011975747857034221, 0.01197799731568503, 0.011782316711336019, 0.012355810047930823, 0.011914678006190033, 0.011786300386093412, 0.011790426485480955, 0.011859083005029457, 0.012162612207156127, 0.011903661848640544, 0.011942558737183822, 0.012117723474200877, 0.011573159433268691, 0.011845343520837706, 0.012004189186758777, 0.012153186208618333, 0.011852046773205805, 0.012041167243390943, 0.011908720470996925, 0.01175714536175521, 0.01207649900505705, 0.011928433896438316, 0.011878951921375702, 0.011820461880782614, 0.011843365408045666, 0.011958638518023961, 0.011862312421049835, 0.011810262742296, 0.012148866714543825, 0.012192604341668405, 0.012080282449219258, 0.011950448158863928, 0.012016951939785626, 0.011936158956336661, 0.011684371406474957, 0.011923900741031435, 0.011850191616007812, 0.011945372649368607, 0.011886527022438333, 0.011899959766314408, 0.01198537047476919, 0.012020406186721184, 0.011705815116022674, 0.012142151997135978, 0.011705715048348995, 0.011736004193252004, 0.011647195402951862, 0.011844819170017465, 0.011671981980597391, 0.011271677675420014, 0.011882568745438468, 0.011899876475143482, 0.011629603983823216, 0.012168281238151857, 0.011762627318382289, 0.011928186539180525, 0.011789633098535982, 0.011737797229833829, 0.011460528258184026, 0.011859152988037067, 0.011858548659399087, 0.01198180119517852, 0.011875978115860582, 0.01140245221453558, 0.011885170230781351, 0.011886087535146425, 0.011642555635676054, 0.011681211204005455, 0.011819249789855817, 0.011860145263524446, 0.011774146347039821, 0.011923318861305836], "accuracy_test_std": 0.010380669076065937, "error_valid": [0.6042303981551205, 0.5527755553463856, 0.4905682299510542, 0.4761433429028614, 0.44113857774849397, 0.42172939806099397, 0.40225844785391573, 0.38861598738704817, 0.36216643919427716, 0.3439867869917168, 0.3268660579819277, 0.3144545957266567, 0.3110160368034638, 0.3006900649472892, 0.2937217620481928, 0.28638724821159633, 0.2706798875188253, 0.26929593373493976, 0.2622261506965362, 0.255603468561747, 0.25623441029743976, 0.24962202324924698, 0.24219632435993976, 0.24266401543674698, 0.23737381165286142, 0.23545157191265065, 0.23281750047063254, 0.2299481127635542, 0.23056875941265065, 0.2299481127635542, 0.22467849915286142, 0.22573742234563254, 0.22678457972515065, 0.21967361634036142, 0.21921622035015065, 0.21576736634036142, 0.21562470585466864, 0.2160012118787651, 0.2149334643260542, 0.21084337349397586, 0.21281708866716864, 0.21074189335466864, 0.20952119022966864, 0.20840196724397586, 0.2080151661332832, 0.20571642036897586, 0.2067032779555723, 0.20705919380647586, 0.20422069135918675, 0.2039868458207832, 0.2028676228350903, 0.20351915474397586, 0.2016469197100903, 0.20266466255647586, 0.2007924275225903, 0.20003941547439763, 0.20063947195030118, 0.19928640342620485, 0.20027326101280118, 0.19623464561370485, 0.19770978445030118, 0.19599050498870485, 0.19648908132530118, 0.19785244493599397, 0.19538015342620485, 0.19698765766189763, 0.19661115163780118, 0.19440359092620485, 0.19526837820030118, 0.19562429405120485, 0.19551251882530118, 0.19452566123870485, 0.19489187217620485, 0.19367116905120485, 0.19539044851280118, 0.19490216726280118, 0.19575665945030118, 0.19392560476280118, 0.19563458913780118, 0.19392560476280118, 0.19367116905120485, 0.19428152061370485, 0.19331525320030118, 0.19429181570030118, 0.19355939382530118, 0.19319318288780118, 0.19454625141189763, 0.19343732351280118, 0.19331525320030118, 0.19418004047439763, 0.19355939382530118, 0.19381382953689763, 0.19221662038780118, 0.19320347797439763, 0.19319318288780118, 0.19294904226280118, 0.19257253623870485, 0.19171804405120485, 0.19259312641189763, 0.19148419851280118, 0.19185040945030118, 0.19159597373870485, 0.19099591726280118, 0.19209455007530118, 0.19184011436370485, 0.19185040945030118, 0.19172833913780118, 0.19087384695030118, 0.19172833913780118, 0.19221662038780118, 0.19111798757530118, 0.19124005788780118, 0.19111798757530118, 0.19197247976280118, 0.19185040945030118, 0.19112828266189763, 0.19172833913780118, 0.19221662038780118, 0.19026349538780118, 0.19062970632530118, 0.19050763601280118, 0.19111798757530118, 0.19062970632530118, 0.19160626882530118, 0.19111798757530118, 0.19075177663780118, 0.19075177663780118, 0.19099591726280118, 0.19124005788780118, 0.19111798757530118, 0.19026349538780118, 0.19100621234939763, 0.19087384695030118, 0.19076207172439763, 0.18989728445030118, 0.18988698936370485, 0.18927663780120485, 0.18988698936370485, 0.18976491905120485, 0.18964284873870485, 0.18927663780120485, 0.19001935476280118, 0.18989728445030118, 0.18901190700301207, 0.18964284873870485, 0.18939870811370485, 0.1891442724021084, 0.1891442724021084, 0.1891442724021084, 0.18989728445030118, 0.19064000141189763, 0.18915456748870485, 0.1898766942771084, 0.18928693288780118, 0.18952077842620485, 0.1887780614646084, 0.18842214561370485, 0.18940900320030118, 0.18953107351280118, 0.18977521413780118, 0.18988698936370485, 0.18964284873870485, 0.18915456748870485, 0.1891442724021084, 0.1898766942771084, 0.19037527061370485, 0.18952077842620485, 0.18952077842620485, 0.18952077842620485, 0.1891442724021084, 0.18939870811370485, 0.18940900320030118, 0.1885339208396084, 0.18891042686370485, 0.18928693288780118, 0.18927663780120485, 0.19001935476280118, 0.1889001317771084, 0.18939870811370485, 0.1892663427146084, 0.18964284873870485, 0.18927663780120485, 0.18952077842620485, 0.18939870811370485, 0.1890222020896084, 0.1889001317771084, 0.1889001317771084, 0.18854421592620485, 0.1880456395896084, 0.19026349538780118, 0.18964284873870485, 0.18927663780120485, 0.1889001317771084, 0.19000905967620485, 0.1893884130271084, 0.1886559911521084, 0.1885339208396084, 0.18964284873870485, 0.18915456748870485, 0.1892663427146084, 0.1884118505271084, 0.19000905967620485, 0.1890222020896084, 0.1890222020896084, 0.19075177663780118, 0.19000905967620485, 0.18854421592620485, 0.18927663780120485, 0.18939870811370485, 0.18927663780120485, 0.18976491905120485, 0.1889001317771084, 0.18927663780120485, 0.1891442724021084, 0.18976491905120485, 0.18866628623870485, 0.18842214561370485, 0.1887780614646084, 0.19013112998870485, 0.18952077842620485, 0.18939870811370485, 0.18915456748870485, 0.19000905967620485, 0.18915456748870485, 0.18903249717620485, 0.1885339208396084, 0.18805593467620485, 0.1886559911521084, 0.1891442724021084, 0.18976491905120485, 0.1890222020896084, 0.18866628623870485, 0.1890222020896084, 0.18976491905120485, 0.1890222020896084, 0.18927663780120485, 0.18915456748870485, 0.18927663780120485, 0.1887780614646084], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-3, 3], "momentum": 0.6353791800629406, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.0019137570131662399, "patience_threshold": 1, "do_flip": true, "batch_size": 32, "optimization": "nesterov_momentum", "nb_data_augmentation": 4, "learning_rate_decay_method": "exp", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 9.531300979303621e-06, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.04280719133094532}, "accuracy_valid_max": 0.8119543604103916, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-3, 3)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8112219385353916, "loss_train": [1.9118794202804565, 1.6314531564712524, 1.5172393321990967, 1.439215064048767, 1.3765099048614502, 1.3251070976257324, 1.2808197736740112, 1.2404307126998901, 1.2065598964691162, 1.1734434366226196, 1.1451877355575562, 1.1199344396591187, 1.0960967540740967, 1.0746865272521973, 1.053890585899353, 1.0348865985870361, 1.0177947282791138, 1.001104474067688, 0.9841641187667847, 0.9694491028785706, 0.9571723341941833, 0.9449160695075989, 0.9324915409088135, 0.9219391942024231, 0.9116643667221069, 0.9010605216026306, 0.8940877914428711, 0.8846678733825684, 0.8772183656692505, 0.8695544004440308, 0.8617642521858215, 0.8545904755592346, 0.8493561148643494, 0.8430527448654175, 0.838252604007721, 0.8308286666870117, 0.826286256313324, 0.8203591704368591, 0.8171789050102234, 0.8111069202423096, 0.8089552521705627, 0.8035438656806946, 0.8002656102180481, 0.7972043752670288, 0.7932327389717102, 0.7888368964195251, 0.7869868278503418, 0.7836896777153015, 0.7795524001121521, 0.77703458070755, 0.7759671807289124, 0.7732364535331726, 0.7721248269081116, 0.7671910524368286, 0.7660239338874817, 0.7625094056129456, 0.7621192932128906, 0.7590800523757935, 0.7577298283576965, 0.756241500377655, 0.7556600570678711, 0.7537882328033447, 0.7524569034576416, 0.7505254149436951, 0.7494767308235168, 0.7457407712936401, 0.7468468546867371, 0.7446225881576538, 0.7431375980377197, 0.7421155571937561, 0.7414346933364868, 0.7392212748527527, 0.7393651008605957, 0.7385937571525574, 0.7359057664871216, 0.736666202545166, 0.7341580986976624, 0.7339733839035034, 0.7338207960128784, 0.7338319420814514, 0.7316752076148987, 0.7311025261878967, 0.7314602732658386, 0.7297330498695374, 0.7297532558441162, 0.7310085892677307, 0.7275410890579224, 0.7291510701179504, 0.7273080348968506, 0.7288164496421814, 0.7269616723060608, 0.7257025837898254, 0.725899875164032, 0.7249451279640198, 0.724485456943512, 0.7245278358459473, 0.7223821878433228, 0.7239879965782166, 0.7234265208244324, 0.7226819396018982, 0.7232268452644348, 0.7236505150794983, 0.7213539481163025, 0.7223395705223083, 0.7215380668640137, 0.7215394973754883, 0.7206842303276062, 0.7203221917152405, 0.7204416990280151, 0.7199376821517944, 0.720565915107727, 0.7196817994117737, 0.7196498513221741, 0.7191850543022156, 0.7203550338745117, 0.7193662524223328, 0.718761682510376, 0.7184737920761108, 0.7191920280456543, 0.7177221775054932, 0.7196047902107239, 0.7187711000442505, 0.718708872795105, 0.7176939249038696, 0.7177408337593079, 0.7186828851699829, 0.7187262177467346, 0.7178143858909607, 0.717775285243988, 0.7178652882575989, 0.7189183235168457, 0.7177082896232605, 0.7190715074539185, 0.7177594900131226, 0.7180687785148621, 0.7165043354034424, 0.7162036895751953, 0.7179713845252991, 0.7174887657165527, 0.7165122628211975, 0.7171244025230408, 0.7162870764732361, 0.7170547842979431, 0.7175255417823792, 0.7159876227378845, 0.7172002792358398, 0.7157229781150818, 0.7169733643531799, 0.7149240374565125, 0.7173367738723755, 0.7166932225227356, 0.7161653637886047, 0.7150317430496216, 0.7155994772911072, 0.7175328731536865, 0.7172687649726868, 0.7160645723342896, 0.7158201336860657, 0.7156659960746765, 0.7165658473968506, 0.7164605855941772, 0.7168303728103638, 0.7159736156463623, 0.715740978717804, 0.7160539627075195, 0.716639518737793, 0.7164307832717896, 0.7157536149024963, 0.7147459983825684, 0.717235267162323, 0.7161491513252258, 0.714878261089325, 0.715714156627655, 0.7154279947280884, 0.7149494886398315, 0.717441201210022, 0.7169786095619202, 0.7159128785133362, 0.7154095768928528, 0.7177031636238098, 0.716383159160614, 0.7151868939399719, 0.7164537310600281, 0.7142115831375122, 0.7154245972633362, 0.7163236737251282, 0.7164221405982971, 0.7168535590171814, 0.716841995716095, 0.7158787250518799, 0.7163495421409607, 0.7153835892677307, 0.7164813876152039, 0.7161754965782166, 0.7168266773223877, 0.716434895992279, 0.7162367105484009, 0.7158369421958923, 0.7161492109298706, 0.7154927849769592, 0.7168740034103394, 0.717947244644165, 0.7167065143585205, 0.7150646448135376, 0.7160167098045349, 0.7146209478378296, 0.7155550122261047, 0.7150219678878784, 0.7166734933853149, 0.7156355977058411, 0.715760350227356, 0.7161049246788025, 0.7158360481262207, 0.716210663318634, 0.7154768109321594, 0.7158817052841187, 0.7162299752235413, 0.7155643701553345, 0.7155887484550476, 0.7159883379936218, 0.7154038548469543, 0.7150653600692749, 0.7146013379096985, 0.7149420976638794, 0.7155953645706177, 0.7152861952781677, 0.7167825102806091, 0.7153694033622742, 0.7152721285820007, 0.716290295124054, 0.7157980799674988, 0.7167133092880249, 0.7154563665390015, 0.7140499353408813, 0.7157045006752014, 0.7156617045402527, 0.7157453894615173, 0.7155795693397522, 0.7159278988838196], "accuracy_train_first": 0.3910604697305279, "model": "residualv3", "loss_std": [0.2658401429653168, 0.1697164922952652, 0.1736440658569336, 0.17522649466991425, 0.17673014104366302, 0.17656227946281433, 0.1778150200843811, 0.17763474583625793, 0.17448453605175018, 0.17718443274497986, 0.17459715902805328, 0.17591188848018646, 0.17437031865119934, 0.17471052706241608, 0.1739984154701233, 0.17325319349765778, 0.17315956950187683, 0.17188777029514313, 0.17260248959064484, 0.1717115342617035, 0.1709010750055313, 0.1695394665002823, 0.16926464438438416, 0.16712801158428192, 0.16722364723682404, 0.16722235083580017, 0.16724087297916412, 0.1662815362215042, 0.16517828404903412, 0.16541482508182526, 0.16458863019943237, 0.16502869129180908, 0.16335803270339966, 0.16376058757305145, 0.1630225032567978, 0.16237381100654602, 0.16124503314495087, 0.16119849681854248, 0.16012690961360931, 0.16128696501255035, 0.1608007699251175, 0.16025647521018982, 0.15747769176959991, 0.16010719537734985, 0.15964752435684204, 0.15847407281398773, 0.15766310691833496, 0.1572834551334381, 0.15836478769779205, 0.1578299105167389, 0.1575632393360138, 0.1572926938533783, 0.15743839740753174, 0.15623751282691956, 0.1588437557220459, 0.15637783706188202, 0.15682385861873627, 0.15708142518997192, 0.15562397241592407, 0.15588702261447906, 0.15539149940013885, 0.15579412877559662, 0.15466539561748505, 0.15395058691501617, 0.15417920053005219, 0.1546356976032257, 0.1544824242591858, 0.1535559892654419, 0.15466736257076263, 0.15366601943969727, 0.15223973989486694, 0.15301448106765747, 0.15428850054740906, 0.15273897349834442, 0.15233202278614044, 0.15311607718467712, 0.1531662940979004, 0.15325988829135895, 0.15342679619789124, 0.15377016365528107, 0.15365062654018402, 0.15270107984542847, 0.15370038151741028, 0.15343929827213287, 0.15170960128307343, 0.1534474790096283, 0.1526281237602234, 0.1516098827123642, 0.15146242082118988, 0.15249629318714142, 0.15234234929084778, 0.15213996171951294, 0.15158498287200928, 0.1520872414112091, 0.1518266499042511, 0.15261203050613403, 0.1509312093257904, 0.1506281942129135, 0.15172848105430603, 0.1521679311990738, 0.1525006741285324, 0.1513264924287796, 0.15130066871643066, 0.15089093148708344, 0.15057630836963654, 0.15237973630428314, 0.1516052484512329, 0.15101151168346405, 0.15162287652492523, 0.1513657569885254, 0.15041165053844452, 0.15059930086135864, 0.15153692662715912, 0.15183469653129578, 0.15201014280319214, 0.14992539584636688, 0.1504206508398056, 0.1494433432817459, 0.1520787477493286, 0.15069957077503204, 0.151791051030159, 0.1507091522216797, 0.1505938321352005, 0.14986352622509003, 0.15147465467453003, 0.15205895900726318, 0.15116459131240845, 0.15103153884410858, 0.15191395580768585, 0.15174314379692078, 0.1519988775253296, 0.15133512020111084, 0.15213413536548615, 0.15152007341384888, 0.14986737072467804, 0.14985783398151398, 0.1510574370622635, 0.15007276833057404, 0.14924611151218414, 0.14965161681175232, 0.150171160697937, 0.15075381100177765, 0.15141651034355164, 0.15096794068813324, 0.15102583169937134, 0.1506301313638687, 0.15102270245552063, 0.15088988840579987, 0.15125997364521027, 0.15059636533260345, 0.15034808218479156, 0.1510816216468811, 0.14991958439350128, 0.1507520228624344, 0.1498573273420334, 0.1504932940006256, 0.1501314342021942, 0.15134721994400024, 0.1500476598739624, 0.1502930223941803, 0.15075987577438354, 0.1512240469455719, 0.15031778812408447, 0.15126974880695343, 0.15088675916194916, 0.15108460187911987, 0.15095104277133942, 0.1510530412197113, 0.15054775774478912, 0.15099084377288818, 0.15001548826694489, 0.15150468051433563, 0.15039198100566864, 0.1502620130777359, 0.15223228931427002, 0.1504087895154953, 0.1506427675485611, 0.15128915011882782, 0.15146122872829437, 0.15247932076454163, 0.1511460393667221, 0.15089534223079681, 0.1491418480873108, 0.15003955364227295, 0.15135787427425385, 0.15188103914260864, 0.15166178345680237, 0.15178312361240387, 0.15137121081352234, 0.15143685042858124, 0.15174412727355957, 0.15041181445121765, 0.1516426056623459, 0.15104670822620392, 0.14999952912330627, 0.15215882658958435, 0.14963461458683014, 0.1520407497882843, 0.1498388648033142, 0.15132929384708405, 0.15081197023391724, 0.15116530656814575, 0.1501825749874115, 0.15161475539207458, 0.15169432759284973, 0.15042628347873688, 0.14991730451583862, 0.1505410224199295, 0.15181565284729004, 0.15037673711776733, 0.1504921019077301, 0.15048320591449738, 0.15117591619491577, 0.15115468204021454, 0.15115386247634888, 0.15093488991260529, 0.15040279924869537, 0.15119552612304688, 0.15017926692962646, 0.15026429295539856, 0.1485702246427536, 0.14882826805114746, 0.1518879383802414, 0.15037935972213745, 0.14907297492027283, 0.15084663033485413, 0.15146906673908234, 0.15033020079135895, 0.1520700454711914, 0.1514224112033844, 0.15022848546504974, 0.15092477202415466, 0.15122325718402863, 0.1503988355398178, 0.14984209835529327, 0.14862513542175293, 0.15046319365501404, 0.1511097103357315, 0.15149663388729095]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:15 2016", "state": "available"}], "summary": "55f5f1b4cfc84c47350fc1925356b1cf"}