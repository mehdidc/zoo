{"content": {"hp_model": {"f0": 64, "f1": 16, "f2": 16, "f3": 32, "nonlin": "very_leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.6288635730743408, 1.2189908027648926, 1.045662522315979, 0.9183796644210815, 0.8301532864570618, 0.7608271241188049, 0.7073610424995422, 0.660551905632019, 0.6262818574905396, 0.5929790735244751, 0.5660296082496643, 0.5433762073516846, 0.5200269818305969, 0.4976591169834137, 0.48187920451164246, 0.4652562737464905, 0.4514956474304199, 0.43704625964164734, 0.42472121119499207, 0.4112052917480469, 0.39933469891548157, 0.3876718580722809, 0.3764260709285736, 0.367423415184021, 0.35663679242134094, 0.34887170791625977, 0.3406573235988617, 0.33097904920578003, 0.3247024118900299, 0.31537315249443054, 0.3115387558937073, 0.3016706109046936, 0.29659152030944824, 0.29032209515571594, 0.28357118368148804, 0.279447466135025, 0.2738552689552307, 0.2689301073551178, 0.2634996175765991, 0.25764200091362, 0.2525458037853241, 0.24988700449466705, 0.24593818187713623, 0.2411845475435257, 0.23651283979415894, 0.23280291259288788, 0.22839462757110596, 0.22500944137573242, 0.22292880713939667, 0.21999609470367432, 0.2187715321779251, 0.21557080745697021, 0.21405844390392303, 0.20914927124977112, 0.20553277432918549, 0.20456290245056152, 0.2034340649843216, 0.19963236153125763, 0.19771800935268402, 0.19383519887924194, 0.19479773938655853, 0.19173839688301086, 0.1890948861837387, 0.1889708936214447, 0.18644598126411438, 0.18397420644760132, 0.18294145166873932, 0.18084965646266937, 0.17978693544864655, 0.17802858352661133, 0.17664076387882233, 0.1772976666688919, 0.174101784825325, 0.1729031652212143, 0.17151999473571777, 0.1703738123178482, 0.1715717613697052, 0.17004001140594482, 0.16772538423538208, 0.16626742482185364, 0.1664137989282608, 0.16469167172908783, 0.16489939391613007, 0.1635838896036148, 0.16071538627147675, 0.16196446120738983, 0.16069161891937256, 0.1602843999862671, 0.15992873907089233, 0.1594950556755066, 0.15740002691745758, 0.15687215328216553, 0.1573246866464615, 0.15743190050125122, 0.15662039816379547, 0.15557081997394562, 0.1551092267036438, 0.15316039323806763, 0.15501630306243896, 0.15280619263648987, 0.15217013657093048, 0.1533491611480713, 0.15183958411216736, 0.1502048522233963, 0.14994646608829498, 0.14860984683036804, 0.1492614448070526, 0.1500159054994583, 0.14828112721443176, 0.14815081655979156, 0.14810842275619507, 0.14640077948570251, 0.14900143444538116, 0.14741197228431702, 0.14717571437358856, 0.14665038883686066, 0.14915500581264496, 0.14791278541088104, 0.14621397852897644, 0.14668601751327515, 0.14700567722320557, 0.14619240164756775, 0.14586329460144043, 0.14530080556869507, 0.1460118293762207, 0.14568063616752625, 0.14530159533023834, 0.14603166282176971, 0.1444224715232849, 0.14495180547237396, 0.14527146518230438, 0.14441728591918945, 0.1456797868013382, 0.14448313415050507, 0.1450633555650711, 0.14421385526657104, 0.1441815197467804, 0.1433766782283783, 0.14493073523044586], "moving_avg_accuracy_train": [0.05343294117647057, 0.10263082352941175, 0.1573818588235294, 0.21115190823529412, 0.2626673056470588, 0.31139586919999995, 0.356611576397647, 0.3997457128755294, 0.43959467099974114, 0.4774163803703553, 0.5118418011568492, 0.5434905622176348, 0.5734944471723419, 0.6010273553962842, 0.6270469727978322, 0.6506316872827548, 0.6724155773780087, 0.6929128431696197, 0.7121250882644223, 0.7294984617909213, 0.7451344979647704, 0.759693989344764, 0.773326943351464, 0.7861730725457294, 0.7974145888205683, 0.8079131299385114, 0.8180841698858368, 0.8269533999560766, 0.835782765842822, 0.8437574304350104, 0.851160510920921, 0.858035048064123, 0.8645491903165342, 0.870760153637822, 0.8763594323916869, 0.8816740773878123, 0.8862901990607959, 0.8909341203311869, 0.8951183553568917, 0.8991900492329672, 0.9030522207802587, 0.9067987634081152, 0.9100341811849506, 0.9133742924782203, 0.9165498044068688, 0.9194830592602996, 0.9224124003930931, 0.9249452780008426, 0.9275660443184054, 0.9300612045924472, 0.9323656723684965, 0.9344420463081176, 0.9366307828537764, 0.9386477045683987, 0.9406888164645001, 0.9426575818768737, 0.944575353100951, 0.9462801707320324, 0.9479274477764762, 0.9495182324105932, 0.9511193503460045, 0.9525485917819922, 0.9540278502508519, 0.9555474181669432, 0.9568773822326018, 0.9582272910681651, 0.9595575031378192, 0.9607264587063902, 0.9618985187181041, 0.9629439609639407, 0.9639225060440173, 0.965007902498439, 0.9659447593074186, 0.9668820480825591, 0.9678056079801854, 0.9686179883586374, 0.9693961895227736, 0.9700871588057904, 0.9707725605722702, 0.9714035398091608, 0.9720184799458919, 0.9726378084218908, 0.9732046158149958, 0.973707683645261, 0.9742969152807348, 0.9747872237526614, 0.975282619024454, 0.9757520041808322, 0.976287391998043, 0.9767127704452976, 0.977182669871356, 0.9776244028842205, 0.9780384331840337, 0.9784016486891598, 0.9787097191143614, 0.9790646295558665, 0.9793934607179269, 0.9797152911167224, 0.9800049384756384, 0.9803409152163098, 0.9805821178123258, 0.9808533177957992, 0.9810973977809252, 0.9813335403557739, 0.9815837157319612, 0.9817382853352358, 0.9819573979781828, 0.9821663640627175, 0.9823756100093869, 0.9825851078319776, 0.9827265970487798, 0.9828868785203725, 0.9830476024330411, 0.9832346068956193, 0.9833934991472338, 0.983567090408981, 0.9836880284269065, 0.9837992255842158, 0.9839204794963825, 0.9840107844879208, 0.9841179413332464, 0.984252029552863, 0.9843515324799297, 0.9844787321731132, 0.9846049766028607, 0.9847091848249275, 0.9848241486953759, 0.9848876161787795, 0.9849823839726664, 0.9850488514577527, 0.9851110251355067, 0.9851575696807796, 0.985208871536231, 0.9852668079120197, 0.9853448330031707, 0.9853585849969713, 0.9853803735560976, 0.9854188067887232, 0.9854816319922038], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.05275999999999999, 0.10197733333333331, 0.1564729333333333, 0.21018563999999995, 0.2609404093333333, 0.30915303506666664, 0.35339773156, 0.39505795840399993, 0.4330721625635999, 0.4696316129739066, 0.5026551183431827, 0.5324429398421977, 0.5607853125246446, 0.5864001146055134, 0.6109734364782955, 0.6329694261637994, 0.652979150214086, 0.6715879018593441, 0.6890957783400764, 0.7050928671727354, 0.7196235804554619, 0.7327545557432491, 0.7448257668355909, 0.7557698568186985, 0.7654728711368286, 0.7742722506898124, 0.7823383589541645, 0.7898245230587481, 0.7969220707528732, 0.8030565303442525, 0.8086975439764938, 0.8140277895788445, 0.81918501062096, 0.8235998428921973, 0.8276398586029776, 0.8314492060760131, 0.8345176188017451, 0.837399190254904, 0.8404059378960802, 0.8429653441064722, 0.8456154763624916, 0.8480939287262425, 0.8502845358536182, 0.852362748934923, 0.8544198073747641, 0.855831159970621, 0.857648043973559, 0.859203239576203, 0.8606962489519161, 0.8619332907233912, 0.863059961651052, 0.8641272988192802, 0.8649812356040187, 0.8656297787102835, 0.8665334675059219, 0.867253454088663, 0.8679147753464633, 0.8684966311451503, 0.8690736346973019, 0.869739604560905, 0.8704856441048145, 0.8710904130276664, 0.8715147050582331, 0.8722299012190765, 0.8728335777638355, 0.8732168866541187, 0.8737751979887067, 0.8741310115231694, 0.8744379103708525, 0.8748607860004339, 0.8749747074003905, 0.8752905699936848, 0.8754015129943163, 0.8755813616948848, 0.8758898921920629, 0.8761542363061899, 0.8764854793422375, 0.8766502647413471, 0.8768385716005458, 0.8767680477738246, 0.876851242996442, 0.8768461186967978, 0.8769481734937847, 0.8770533561444063, 0.8771080205299656, 0.8773038851436358, 0.8773734966292722, 0.8775828136330116, 0.8778511989363771, 0.8778260790427393, 0.8777768044717987, 0.8777991240246189, 0.8777392116221571, 0.877631957126608, 0.8776554280806139, 0.8778098852725525, 0.8778022300786306, 0.8778753404041009, 0.8779811396970241, 0.8780496923939883, 0.8781647231545895, 0.8783482508391305, 0.8784200924218841, 0.8784180831796957, 0.8785096081950595, 0.8785653140422203, 0.8783754493046649, 0.8783379043741985, 0.8782907806034453, 0.8783017025431008, 0.8782581989554574, 0.8780990457265784, 0.8781024744872539, 0.8781322270385286, 0.8781456710013423, 0.8781311039012081, 0.8780779935110873, 0.8781768608266453, 0.8781458414106474, 0.8781979239362494, 0.8782581315426244, 0.878232318388362, 0.8781290865495258, 0.8780628445612398, 0.8780032267717826, 0.877949570761271, 0.8779279470184773, 0.8778951523166295, 0.8780123037516333, 0.87797107337647, 0.8780272993721563, 0.8780512361016073, 0.8779927791581132, 0.8779401679089686, 0.8780261511180717, 0.8781168693395978, 0.8781185157389713, 0.8780799974984075, 0.8780853310819], "moving_var_accuracy_train": [0.025695712824913484, 0.0449100261945467, 0.06739810636709147, 0.08667925965407489, 0.10189585922309588, 0.11307652945418353, 0.12016901810521534, 0.12489709986192704, 0.126698845048001, 0.1269032958406377, 0.12487895262351842, 0.12140585405131091, 0.11736736665755751, 0.11245317930921402, 0.1073010457857991, 0.10157709002323709, 0.0956902218300523, 0.08990244079143454, 0.08423418996653605, 0.07852727793910373, 0.0728749207902626, 0.0674952378144333, 0.06241843094752916, 0.05766179517025806, 0.05303296084664945, 0.0487216390524308, 0.044780525629678505, 0.04101044224506028, 0.03761101733821244, 0.03442227308261233, 0.03147329618047895, 0.02875129991083043, 0.026258076363309228, 0.02397945331538374, 0.021863675286916708, 0.019931516821138605, 0.018130142352722835, 0.01651122216034086, 0.015017670349059792, 0.013665111533338053, 0.012432847701550622, 0.011315892166356672, 0.010278514303436978, 0.009351069964156127, 0.008506717851821418, 0.007733481922955853, 0.007037363085910755, 0.006391365998102229, 0.005814045143113451, 0.005288673051940514, 0.0048076008923241135, 0.004365642761725934, 0.003972193594550064, 0.0036115859939212947, 0.003287922634480821, 0.0029940147062733637, 0.002727713853857119, 0.00248110009686862, 0.002257411782132122, 0.002054445965688197, 0.0018720735769072383, 0.0017032507989576142, 0.0015526195696210882, 0.0014181393925235054, 0.0012922446930146436, 0.0011794205084921675, 0.001077403634995231, 0.000981961385587347, 0.000896128769068142, 0.0008163524375657477, 0.0007433351480728504, 0.0006796044024350082, 0.0006195432683162908, 0.000565495533716701, 0.0005166226463055648, 0.0004709000385886535, 0.0004292604081965555, 0.00039063131432755406, 0.000355796163128241, 0.0003237997599919006, 0.000294823146338575, 0.00026879294155536727, 0.00024480508298773725, 0.00022260226986559249, 0.00020346678816122259, 0.00018528373092388653, 0.00016896410610932883, 0.00015405059732364904, 0.0001412252986246448, 0.00012873129017267832, 0.00011784541039090124, 0.00010781702184369943, 9.857810946180055e-05, 8.990762804409613e-05, 8.177103172164196e-05, 7.47275813428814e-05, 6.822799260687125e-05, 6.233736659648429e-05, 5.685869026957899e-05, 5.2188744575071e-05, 4.749347834848797e-05, 4.340607539296243e-05, 3.9601643205918014e-05, 3.614334872623156e-05, 3.3092303323262565e-05, 2.999809885124414e-05, 2.7430382118812713e-05, 2.508034532730314e-05, 2.2966365590351083e-05, 2.1064733070348253e-05, 1.9138432549555224e-05, 1.7455800645822717e-05, 1.5942710166172085e-05, 1.46631751707725e-05, 1.3424078382303488e-05, 1.2352875879468029e-05, 1.1249222329138786e-05, 1.0235583366368034e-05, 9.344347630672692e-06, 8.48330779107587e-06, 7.738320317469443e-06, 7.126305141481882e-06, 6.502782119787149e-06, 5.998121765322218e-06, 5.541748493170442e-06, 5.085307825770449e-06, 4.695727266769748e-06, 4.262407633139025e-06, 3.916995282648571e-06, 3.5650570935470004e-06, 3.243341480041586e-06, 2.9385048842893365e-06, 2.668341319215254e-06, 2.4317168000494505e-06, 2.243336353686559e-06, 2.020704774319324e-06, 1.8229069686666342e-06, 1.6539102921304067e-06, 1.5240423186487962e-06], "duration": 69479.190331, "accuracy_train": [0.5343294117647058, 0.5454117647058824, 0.6501411764705882, 0.6950823529411765, 0.7263058823529411, 0.7499529411764706, 0.7635529411764705, 0.7879529411764706, 0.798235294117647, 0.8178117647058823, 0.8216705882352942, 0.8283294117647059, 0.8435294117647059, 0.8488235294117648, 0.8612235294117647, 0.8628941176470588, 0.8684705882352941, 0.8773882352941177, 0.885035294117647, 0.8858588235294118, 0.8858588235294118, 0.8907294117647059, 0.8960235294117647, 0.9017882352941177, 0.8985882352941177, 0.9024, 0.9096235294117647, 0.9067764705882353, 0.9152470588235294, 0.9155294117647059, 0.9177882352941177, 0.9199058823529411, 0.9231764705882353, 0.9266588235294118, 0.9267529411764706, 0.9295058823529412, 0.9278352941176471, 0.9327294117647059, 0.9327764705882353, 0.9358352941176471, 0.9378117647058823, 0.9405176470588236, 0.9391529411764706, 0.943435294117647, 0.9451294117647059, 0.9458823529411765, 0.9487764705882353, 0.9477411764705882, 0.9511529411764705, 0.9525176470588236, 0.9531058823529411, 0.9531294117647059, 0.9563294117647059, 0.9568, 0.9590588235294117, 0.9603764705882353, 0.961835294117647, 0.9616235294117647, 0.9627529411764706, 0.963835294117647, 0.9655294117647059, 0.9654117647058823, 0.9673411764705883, 0.9692235294117647, 0.9688470588235294, 0.9703764705882353, 0.9715294117647059, 0.9712470588235295, 0.9724470588235294, 0.9723529411764706, 0.9727294117647058, 0.9747764705882352, 0.9743764705882353, 0.9753176470588235, 0.9761176470588235, 0.9759294117647059, 0.9764, 0.9763058823529411, 0.9769411764705882, 0.9770823529411765, 0.9775529411764706, 0.9782117647058823, 0.9783058823529411, 0.9782352941176471, 0.9796, 0.9792, 0.9797411764705882, 0.9799764705882353, 0.9811058823529412, 0.9805411764705882, 0.9814117647058823, 0.9816, 0.981764705882353, 0.9816705882352941, 0.9814823529411765, 0.9822588235294117, 0.9823529411764705, 0.9826117647058824, 0.9826117647058824, 0.9833647058823529, 0.9827529411764706, 0.9832941176470589, 0.9832941176470589, 0.9834588235294117, 0.983835294117647, 0.9831294117647059, 0.9839294117647058, 0.9840470588235294, 0.9842588235294117, 0.9844705882352941, 0.984, 0.9843294117647059, 0.9844941176470589, 0.9849176470588236, 0.9848235294117647, 0.9851294117647059, 0.9847764705882353, 0.9848, 0.9850117647058824, 0.9848235294117647, 0.9850823529411765, 0.9854588235294117, 0.9852470588235294, 0.9856235294117647, 0.9857411764705882, 0.9856470588235294, 0.9858588235294118, 0.9854588235294117, 0.985835294117647, 0.9856470588235294, 0.9856705882352941, 0.9855764705882353, 0.9856705882352941, 0.9857882352941176, 0.9860470588235294, 0.9854823529411765, 0.9855764705882353, 0.985764705882353, 0.9860470588235294], "end": "2016-02-06 05:06:05.037000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0], "moving_var_accuracy_valid": [0.025052558399999994, 0.044348415663999996, 0.06664150787183999, 0.0859428508017904, 0.10053298521233023, 0.11139980221201949, 0.11787816050090219, 0.12171051495705382, 0.12254518092233818, 0.12232000355883742, 0.11990297036482507, 0.11589850211525722, 0.1115382627071679, 0.10628949920722988, 0.101095182617277, 0.0953400764157524, 0.08940957028329477, 0.08358518399511934, 0.07798539724538861, 0.07249001918092961, 0.06714129191937988, 0.06197896533551819, 0.05709249603708924, 0.05246120438340552, 0.04806242032678552, 0.04395304001876419, 0.04014329493967801, 0.03663334932271701, 0.03342339103987874, 0.030419736286195265, 0.027664151970767943, 0.02515344043732355, 0.02287746875348635, 0.020765138573986142, 0.01883552025907769, 0.017082568386702825, 0.015459047957931451, 0.013987874248495244, 0.012670451606045192, 0.011462361486788811, 0.010379334146879485, 0.009396685267265975, 0.008500205576817963, 0.007689055745637927, 0.006958233575898425, 0.0062803374636570715, 0.005682013324612549, 0.005135579692413647, 0.004642083416135985, 0.004191647525621754, 0.003783907259472705, 0.0034157694112015658, 0.0030807553423723775, 0.002776465281581292, 0.002506168634377423, 0.002260217197053626, 0.002038131589602432, 0.00183736543617638, 0.0016566252904515026, 0.0014949544041394007, 0.0013504681387351508, 0.0012187130339120626, 0.0010984619440656779, 0.0009932192995954759, 0.0008971771979721579, 0.0008087818095232728, 0.0007307090324879118, 0.000658777558680882, 0.0005937474849371765, 0.0005359821506263039, 0.00048250073833198617, 0.00043514858709937086, 0.0003917445035339359, 0.0003528611631764075, 0.00031843176646796797, 0.00028721749011723373, 0.0002594832386458811, 0.0002337793028311303, 0.0002107205078070085, 0.00018969321951752634, 0.00017078619057137116, 0.00015370780784025565, 0.0001384307636905224, 0.0001246872578313961, 0.00011224542580369376, 0.00010136614974531763, 9.127314660117845e-05, 8.254015441355073e-05, 7.4934415011759e-05, 6.744665259209042e-05, 6.072383918295381e-05, 5.465593872660124e-05, 4.9222650317659916e-05, 4.440391702723323e-05, 3.996848329564742e-05, 3.6186348183356715e-05, 3.256824078296688e-05, 2.9359522781883526e-05, 2.652431191714271e-05, 2.391417597577811e-05, 2.1641847061160643e-05, 1.9780804053981586e-05, 1.784917456569633e-05, 1.6064293442614243e-05, 1.453325555428884e-05, 1.310785827153099e-05, 1.2121510011480574e-05, 1.092204560656612e-05, 9.849826893839493e-06, 8.865917803348083e-06, 7.996359082253909e-06, 7.42469092639223e-06, 6.682327641350935e-06, 6.0220618059819735e-06, 5.421482286609028e-06, 4.881243861605012e-06, 4.418505897293566e-06, 4.064628022334902e-06, 3.666825057621046e-06, 3.3245558571166176e-06, 3.024724874193687e-06, 2.72824925717111e-06, 2.551335644399535e-06, 2.3356940890682255e-06, 2.1341132075394035e-06, 1.946612593961648e-06, 1.7561596108371826e-06, 1.5902230819769937e-06, 1.5547209022901523e-06, 1.4145483065861134e-06, 1.3015457392457888e-06, 1.1765478684725191e-06, 1.0896480098092857e-06, 1.0055947006574121e-06, 9.715732408207506e-07, 9.484840781905175e-07, 8.53660066049541e-07, 7.81646953149757e-07, 7.037382818506329e-07], "accuracy_test": 0.8675, "start": "2016-02-05 09:48:05.847000", "learning_rate_per_epoch": [0.004995100200176239, 0.004792208317667246, 0.004597557242959738, 0.004410812631249428, 0.004231653176248074, 0.004059771075844765, 0.003894870402291417, 0.003736667800694704, 0.003584891092032194, 0.0034392792731523514, 0.003299582051113248, 0.003165558911859989, 0.0030369795858860016, 0.002913622884079814, 0.0027952766977250576, 0.002681737532839179, 0.0025728102773427963, 0.0024683072697371244, 0.0023680489975959063, 0.002271863166242838, 0.0021795842330902815, 0.002091053407639265, 0.0020061186514794827, 0.0019246337469667196, 0.0018464586464688182, 0.0017714587738737464, 0.0016995053738355637, 0.0016304745804518461, 0.0015642476500943303, 0.0015007107285782695, 0.00143975461833179, 0.0013812744291499257, 0.0013251695781946182, 0.001271343557164073, 0.0012197039322927594, 0.0011701617622748017, 0.0011226319475099444, 0.0010770326480269432, 0.0010332855163142085, 0.0009913153480738401, 0.0009510499075986445, 0.0009124199859797955, 0.0008753591100685298, 0.0008398036006838083, 0.0008056922815740108, 0.0007729665376245975, 0.0007415700238198042, 0.000711448781657964, 0.000682551006320864, 0.000654826988466084, 0.0006282291142269969, 0.0006027115741744637, 0.0005782305379398167, 0.0005547438631765544, 0.000532211153768003, 0.0005105937016196549, 0.0004898543120361865, 0.0004699573037214577, 0.000450868479674682, 0.00043255501077510417, 0.00041498540667816997, 0.00039812945760786533, 0.0003819581470452249, 0.00036644370993599296, 0.0003515594289638102, 0.0003372797218617052, 0.0003235800249967724, 0.0003104367933701724, 0.0002978274133056402, 0.00028573020244948566, 0.00027412435156293213, 0.0002629899245221168, 0.0002523077419027686, 0.00024205945373978466, 0.00023222743766382337, 0.00022279478434938937, 0.00021374526841100305, 0.00020506331929937005, 0.0001967340213013813, 0.00018874304078053683, 0.0001810766407288611, 0.00017372163711115718, 0.00016666538431309164, 0.00015989573148544878, 0.00015340105164796114, 0.00014717018348164856, 0.00014119240222498775, 0.00013545741967391223, 0.0001299553841818124, 0.00012467683700378984, 0.00011961269046878442, 0.00011475424253148958, 0.00011009313311660662, 0.00010562135139480233, 0.0001013311994029209, 9.721530659589916e-05, 9.326659346697852e-05, 8.947827154770494e-05, 8.584382885601372e-05, 8.235700806835666e-05, 7.901181379565969e-05, 7.58024980314076e-05, 7.272353832377121e-05, 6.976963777560741e-05, 6.693571776850149e-05, 6.421691068680957e-05, 6.160853808978572e-05, 5.9106110711582005e-05, 5.670532846124843e-05, 5.4402062232838944e-05, 5.219235026743263e-05, 5.0072390877176076e-05, 4.803854244528338e-05, 4.608730523614213e-05, 4.421532139531337e-05, 4.241937494953163e-05, 4.0696377254789695e-05, 3.9043363358359784e-05, 3.745749199879356e-05, 3.5936034691985697e-05, 3.447637936915271e-05, 3.30760121869389e-05, 3.173252480337396e-05, 3.0443607101915404e-05, 2.9207043553469703e-05, 2.802070594043471e-05, 2.688255517568905e-05, 2.5790634026634507e-05, 2.4743065296206623e-05, 2.3738046365906484e-05, 2.277384919580072e-05, 2.1848816686542705e-05, 2.096135722240433e-05, 2.0109944671276025e-05, 1.9293114746687934e-05, 1.850946318882052e-05, 1.7757642126525752e-05, 1.7036358258337714e-05, 1.634437285247259e-05], "accuracy_train_first": 0.5343294117647058, "accuracy_train_last": 0.9860470588235294, "batch_size_eval": 1024, "accuracy_train_std": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "accuracy_test_std": 0, "error_valid": [0.47240000000000004, 0.4550666666666666, 0.35306666666666664, 0.3064, 0.28226666666666667, 0.25693333333333335, 0.24839999999999995, 0.22999999999999998, 0.2248, 0.20133333333333336, 0.20013333333333339, 0.19946666666666668, 0.18413333333333337, 0.1830666666666667, 0.16786666666666672, 0.1690666666666667, 0.16693333333333338, 0.16093333333333337, 0.15333333333333332, 0.15093333333333336, 0.14959999999999996, 0.14906666666666668, 0.1465333333333333, 0.14573333333333338, 0.1472, 0.1465333333333333, 0.14506666666666668, 0.14280000000000004, 0.1392, 0.14173333333333338, 0.1405333333333333, 0.138, 0.13439999999999996, 0.13666666666666671, 0.136, 0.13426666666666665, 0.1378666666666667, 0.13666666666666671, 0.13253333333333328, 0.134, 0.13053333333333328, 0.12960000000000005, 0.13, 0.12893333333333334, 0.12706666666666666, 0.13146666666666662, 0.126, 0.12680000000000002, 0.12586666666666668, 0.12693333333333334, 0.12680000000000002, 0.12626666666666664, 0.1273333333333333, 0.1285333333333334, 0.1253333333333333, 0.12626666666666664, 0.12613333333333332, 0.12626666666666664, 0.12573333333333336, 0.12426666666666664, 0.12280000000000002, 0.12346666666666661, 0.1246666666666667, 0.1213333333333333, 0.12173333333333336, 0.1233333333333333, 0.12119999999999997, 0.1226666666666667, 0.12280000000000002, 0.1213333333333333, 0.124, 0.12186666666666668, 0.12360000000000004, 0.12280000000000002, 0.1213333333333333, 0.12146666666666661, 0.12053333333333338, 0.12186666666666668, 0.12146666666666661, 0.12386666666666668, 0.12239999999999995, 0.12319999999999998, 0.12213333333333332, 0.122, 0.12239999999999995, 0.12093333333333334, 0.122, 0.12053333333333338, 0.11973333333333336, 0.12239999999999995, 0.1226666666666667, 0.122, 0.12280000000000002, 0.1233333333333333, 0.12213333333333332, 0.12080000000000002, 0.12226666666666663, 0.12146666666666661, 0.12106666666666666, 0.1213333333333333, 0.12080000000000002, 0.12, 0.12093333333333334, 0.12160000000000004, 0.1206666666666667, 0.12093333333333334, 0.1233333333333333, 0.122, 0.12213333333333332, 0.12160000000000004, 0.12213333333333332, 0.1233333333333333, 0.12186666666666668, 0.12160000000000004, 0.12173333333333336, 0.122, 0.12239999999999995, 0.12093333333333334, 0.12213333333333332, 0.1213333333333333, 0.12119999999999997, 0.122, 0.12280000000000002, 0.12253333333333338, 0.12253333333333338, 0.12253333333333338, 0.12226666666666663, 0.12239999999999995, 0.12093333333333334, 0.12239999999999995, 0.12146666666666661, 0.12173333333333336, 0.12253333333333338, 0.12253333333333338, 0.12119999999999997, 0.12106666666666666, 0.12186666666666668, 0.12226666666666663, 0.12186666666666668], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.040618193788951235, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 128, "valid_ratio": 0.15, "learning_rate": 0.005206581917739864, "optimization": "adam", "nb_data_augmentation": 3, "learning_rate_decay_method": "exp", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 2.9723313323031484e-08, "rotation_range": [0, 0], "momentum": 0.6493415175944558}, "accuracy_valid_max": 0.8802666666666666, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        y_pred = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            y_pred.extend((nnet.predict(X[mini_batch]) == y[mini_batch]).tolist())\n        return np.mean(y_pred)\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            acc = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = acc\n            status[\"accuracy_train_std\"] = 0\n            acc = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = acc\n            status[\"accuracy_valid_std\"] = 0\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n\n    # rescaling to [-1, 1]\n    X_min = X_train.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X_train.max(axis=(0, 2, 3))[None, :, None, None]\n    def preprocess(a):\n        return (a / 255.) * 2 - 1\n        # return 2 * ((a - X_min) / (X_max - X_min)) - 1\n    X_train = preprocess(X_train)\n    X_valid = preprocess(X_valid)\n\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = preprocess(X_test)\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    acc = evaluate(X_test, y_test, batch_size_eval)\n    light.set(\"accuracy_test\", acc)\n    light.set(\"accuracy_test_std\", 0)\n    print(\"Test accuracy : {}+-{}\".format(acc, 0))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8781333333333333, "accuracy_valid_std": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "accuracy_valid": [0.5276, 0.5449333333333334, 0.6469333333333334, 0.6936, 0.7177333333333333, 0.7430666666666667, 0.7516, 0.77, 0.7752, 0.7986666666666666, 0.7998666666666666, 0.8005333333333333, 0.8158666666666666, 0.8169333333333333, 0.8321333333333333, 0.8309333333333333, 0.8330666666666666, 0.8390666666666666, 0.8466666666666667, 0.8490666666666666, 0.8504, 0.8509333333333333, 0.8534666666666667, 0.8542666666666666, 0.8528, 0.8534666666666667, 0.8549333333333333, 0.8572, 0.8608, 0.8582666666666666, 0.8594666666666667, 0.862, 0.8656, 0.8633333333333333, 0.864, 0.8657333333333334, 0.8621333333333333, 0.8633333333333333, 0.8674666666666667, 0.866, 0.8694666666666667, 0.8704, 0.87, 0.8710666666666667, 0.8729333333333333, 0.8685333333333334, 0.874, 0.8732, 0.8741333333333333, 0.8730666666666667, 0.8732, 0.8737333333333334, 0.8726666666666667, 0.8714666666666666, 0.8746666666666667, 0.8737333333333334, 0.8738666666666667, 0.8737333333333334, 0.8742666666666666, 0.8757333333333334, 0.8772, 0.8765333333333334, 0.8753333333333333, 0.8786666666666667, 0.8782666666666666, 0.8766666666666667, 0.8788, 0.8773333333333333, 0.8772, 0.8786666666666667, 0.876, 0.8781333333333333, 0.8764, 0.8772, 0.8786666666666667, 0.8785333333333334, 0.8794666666666666, 0.8781333333333333, 0.8785333333333334, 0.8761333333333333, 0.8776, 0.8768, 0.8778666666666667, 0.878, 0.8776, 0.8790666666666667, 0.878, 0.8794666666666666, 0.8802666666666666, 0.8776, 0.8773333333333333, 0.878, 0.8772, 0.8766666666666667, 0.8778666666666667, 0.8792, 0.8777333333333334, 0.8785333333333334, 0.8789333333333333, 0.8786666666666667, 0.8792, 0.88, 0.8790666666666667, 0.8784, 0.8793333333333333, 0.8790666666666667, 0.8766666666666667, 0.878, 0.8778666666666667, 0.8784, 0.8778666666666667, 0.8766666666666667, 0.8781333333333333, 0.8784, 0.8782666666666666, 0.878, 0.8776, 0.8790666666666667, 0.8778666666666667, 0.8786666666666667, 0.8788, 0.878, 0.8772, 0.8774666666666666, 0.8774666666666666, 0.8774666666666666, 0.8777333333333334, 0.8776, 0.8790666666666667, 0.8776, 0.8785333333333334, 0.8782666666666666, 0.8774666666666666, 0.8774666666666666, 0.8788, 0.8789333333333333, 0.8781333333333333, 0.8777333333333334, 0.8781333333333333], "seed": 460312339, "model": "residualv3", "loss_std": [0.30163854360580444, 0.12359769642353058, 0.10806525498628616, 0.10459885001182556, 0.09976991266012192, 0.09597854316234589, 0.09549695998430252, 0.095926932990551, 0.0944179892539978, 0.0879339948296547, 0.08394134789705276, 0.08594156801700592, 0.0813886970281601, 0.07967035472393036, 0.07865417748689651, 0.07787463068962097, 0.07810048758983612, 0.07601505517959595, 0.07315097004175186, 0.07303079217672348, 0.07113945484161377, 0.07300516963005066, 0.07046661525964737, 0.06852152943611145, 0.06761249154806137, 0.06702335178852081, 0.06555057317018509, 0.06525135040283203, 0.06497136503458023, 0.06353278458118439, 0.06270340085029602, 0.06257349252700806, 0.05983639881014824, 0.06112365797162056, 0.05990377441048622, 0.059309497475624084, 0.0578639954328537, 0.05674929544329643, 0.05602623522281647, 0.05463795363903046, 0.0548286959528923, 0.053757332265377045, 0.05356156826019287, 0.05268195644021034, 0.05170965939760208, 0.051757849752902985, 0.05234921723604202, 0.05075164884328842, 0.0491960309445858, 0.05183997377753258, 0.050927624106407166, 0.04723943769931793, 0.047892648726701736, 0.047759294509887695, 0.04726080596446991, 0.04594094306230545, 0.04630027711391449, 0.04777121916413307, 0.04405524954199791, 0.044166382402181625, 0.046080127358436584, 0.04501001164317131, 0.043926749378442764, 0.044047605246305466, 0.04514028877019882, 0.04118524119257927, 0.044253282248973846, 0.043031081557273865, 0.04268866404891014, 0.04132017493247986, 0.042779162526130676, 0.04337388649582863, 0.0406431183218956, 0.040693968534469604, 0.03984885662794113, 0.038619425147771835, 0.0395519956946373, 0.03983210772275925, 0.040305543690919876, 0.03928693011403084, 0.037958379834890366, 0.03878537565469742, 0.04000958427786827, 0.038424767553806305, 0.03788222745060921, 0.03875356167554855, 0.03897136449813843, 0.0376800000667572, 0.036690037697553635, 0.03911808505654335, 0.038118816912174225, 0.0368894599378109, 0.03629273176193237, 0.037649404257535934, 0.037443291395902634, 0.03515729308128357, 0.03590671345591545, 0.03708209842443466, 0.03643462806940079, 0.03657509759068489, 0.035465702414512634, 0.036331430077552795, 0.03774048388004303, 0.03592885285615921, 0.03590349853038788, 0.035343971103429794, 0.03496614843606949, 0.03574971854686737, 0.03517656773328781, 0.036989323794841766, 0.034629207104444504, 0.035712018609046936, 0.03714641556143761, 0.036383286118507385, 0.03513437137007713, 0.03520116209983826, 0.036340560764074326, 0.03597237169742584, 0.03534173220396042, 0.03515119478106499, 0.03606708347797394, 0.034458037465810776, 0.03453626483678818, 0.035129912197589874, 0.03458614647388458, 0.03600572422146797, 0.03505078703165054, 0.035069264471530914, 0.03473082184791565, 0.034790027886629105, 0.036573924124240875, 0.03390710800886154, 0.036719802767038345, 0.03519119694828987, 0.03527027368545532, 0.03611581027507782, 0.03404616937041283, 0.034649863839149475, 0.035385794937610626]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:41 2016", "state": "available"}], "summary": "43293568231e86f4508b3140a39c5448"}