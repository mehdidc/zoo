{"content": {"hp_model": {"f0": 32, "f1": 32, "f2": 32, "f3": 32, "nonlin": "very_leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.13151743032513397, 0.12515478160414334, 0.12251466017566609, 0.12004524181609616, 0.11927796504775248, 0.11418409805702365, 0.11198687103673781, 0.10859753368387513, 0.10741455794159888, 0.10367953638506099, 0.1026714230791226, 0.09907424048765212, 0.10022353770173459, 0.09763994334781226, 0.09894986237778645, 0.09777246376988849, 0.09413122261408957, 0.09489574533938698, 0.09713119153373567, 0.09444512048598781, 0.09482552338537009, 0.09345161258888746, 0.09448892068614699, 0.09223787445370228, 0.0929927434970766, 0.09227189894310224, 0.09473322512763864, 0.09148535449495773, 0.09301422116588011, 0.09195174625540567, 0.09125580874254109, 0.09297739926753375, 0.09116322213671725, 0.08979921709024631, 0.09022985803380318, 0.09168836159703476, 0.09165752571197677, 0.0895259144048558, 0.08837008487360398, 0.0915727465231893, 0.09059912132616839, 0.08980755822042208, 0.09090098625097948, 0.08913700104682289, 0.08972480773011322, 0.09002801942549273, 0.09014442861643372, 0.0898706873709961, 0.09109942249252609, 0.09017855076340496, 0.08987783127327191, 0.09036347586234583, 0.09050231584463067, 0.09038518366797163, 0.0906106365559934, 0.09099756360936394, 0.09053778052558145, 0.0906106365559934, 0.09068304071022505, 0.09035321217511641, 0.0911416996417131, 0.08991145951863129, 0.09091599433727277, 0.0908766541534996, 0.08982940036693281, 0.09072108873577125, 0.09038518366797163, 0.09168165056818983, 0.09037176491259906, 0.08979723099267231, 0.08991145951863129, 0.09167931618213487, 0.09147570410509225, 0.09122296934041325, 0.08933037165700164, 0.09077857292030105, 0.09054802329185055, 0.0910700518141074, 0.09048802753256902, 0.09011830903019355, 0.09093757025071222, 0.09098805749603366, 0.09135162051918264, 0.09059055777049829, 0.09062037906910356, 0.09059055777049829, 0.0906177221239518, 0.08960148417228302, 0.09112095497326601, 0.0903736397113916, 0.09048802753256902, 0.09027106066650809, 0.09122296934041325, 0.09140578147698716, 0.0907027057653227, 0.08999235385531275, 0.09046407723227186, 0.08933127007722137, 0.08989955715325547, 0.09072197338371288, 0.0900562446511738, 0.09064841985177237, 0.09060620779463842, 0.09009426044545203, 0.09026078647061096, 0.09023697350117461, 0.09060620779463842, 0.09200099847062734, 0.09058022136649559], "moving_avg_accuracy_train": [0.04903990963855421, 0.10459337349397588, 0.16022496234939756, 0.21388902484939754, 0.26454925640060234, 0.31227778332078304, 0.3577701668862951, 0.40045935878200295, 0.44092923615681473, 0.4789005068182417, 0.5138606067388272, 0.5458165076613299, 0.5754945519253174, 0.6028825026062796, 0.6281646589721577, 0.6513421689785563, 0.6727478617192549, 0.6924036101858836, 0.7106444238660904, 0.7276800379553849, 0.743308589129726, 0.7576684305179582, 0.7707640686408611, 0.7829219426502689, 0.7942758327225914, 0.8048708398117781, 0.8145710675775882, 0.8235436483198294, 0.8318613467408585, 0.8395449409824353, 0.8467378489926255, 0.8533597154487846, 0.8594041091147495, 0.8649099519683348, 0.8700440508980073, 0.8749118220130259, 0.8794598905647353, 0.8836331597913942, 0.8874738159508091, 0.8909868823978968, 0.894313363585818, 0.8974719180404892, 0.9003028512364403, 0.9028765759019529, 0.9053223520466973, 0.9076129707576902, 0.9096909997361381, 0.9116977092504761, 0.9135908148314525, 0.9152240149748133, 0.9167503710074524, 0.9181899799910445, 0.9196338773232653, 0.9209404444102159, 0.9221304737643751, 0.9232391507855279, 0.9243004954961317, 0.9252604120609763, 0.9261219838066859, 0.9269138705163787, 0.927687750784018, 0.9283960088381463, 0.929014615785657, 0.9296254847793805, 0.9302223301267436, 0.9307947883791295, 0.9313429450833853, 0.9318198139786612, 0.9323384161651324, 0.9327675075305468, 0.9330948606931548, 0.9334200696539598, 0.933703345068082, 0.933986530892599, 0.9342743424117728, 0.9344816032007159, 0.9347175543264276, 0.9349393229901704, 0.9351318552995871, 0.9352886622395079, 0.9354627327625451, 0.9356805784621942, 0.9358225168509146, 0.9359596740513652, 0.9361301787847828, 0.9362671609063045, 0.9363927979783246, 0.9365129308310945, 0.9366563478383465, 0.9367783636569215, 0.936822289339422, 0.9368971198934316, 0.9369432889281848, 0.9370319043124747, 0.9370857733691791, 0.9371507276587673, 0.9372209523326496, 0.9372606229126377, 0.9372963264346269, 0.9373755228574293, 0.9374350338246985, 0.9374838873699395, 0.9375043239341503, 0.9375815459082052, 0.9376275140583485, 0.9376029968392607, 0.937578578179431, 0.9376366089157048, 0.9376323606747367], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 339804164, "moving_var_accuracy_train": [0.021644214636218063, 0.04725547928961705, 0.07038379446786344, 0.08926389945711226, 0.10343564105879624, 0.11359418749065037, 0.12086078140383039, 0.12517600720582445, 0.1273987052578327, 0.12763519129283954, 0.12587154944167148, 0.12247501093142367, 0.11815458664029811, 0.11309002655879369, 0.1075337107774927, 0.10161511243061384, 0.09557743432293522, 0.08949682692069211, 0.08354168978206708, 0.07779943013045472, 0.07221775162369022, 0.06685182186357788, 0.06171010131783436, 0.056869416289908645, 0.052342672038887234, 0.04811869241197775, 0.04415367293915732, 0.040462870491825925, 0.03703924040585205, 0.03386665494948957, 0.03094563078532814, 0.028245709745064178, 0.02574995102366017, 0.02344778467104954, 0.02134023695032157, 0.019419470015943293, 0.017663687362308403, 0.016054064210421173, 0.014581413546992713, 0.013234346915048326, 0.012010501517385834, 0.010899239561835361, 0.009881443250491265, 0.00895291545332695, 0.008111460296546065, 0.007347536673603816, 0.006651646846160855, 0.006022724109219182, 0.005452706336963782, 0.004931441787641867, 0.004459265473523046, 0.004031991192401494, 0.003647555628715295, 0.0032981641238180883, 0.002981093240210123, 0.0026940463988242022, 0.002434779832294323, 0.0021995948073680596, 0.0019863160794882997, 0.0017933282325883636, 0.0016193854253472998, 0.0014619615480537084, 0.0013192094642479138, 0.0011906469661705568, 0.0010747882888715228, 0.0009702588360408928, 0.0008759372343885879, 0.0007903901464392642, 0.0007137716658456525, 0.0006440515738599466, 0.0005806108573115769, 0.00052350161939411, 0.00047187366209691336, 0.00042540804378808894, 0.0003836127586444025, 0.0003456380960916625, 0.00031157534288601713, 0.0002808604406593797, 0.00025310801480496573, 0.0002280185090721351, 0.00020548936308783548, 0.00018536753751875238, 0.00016701210232260988, 0.0001504802009690682, 0.00013569382764922196, 0.0001222933217988491, 0.00011020605168375638, 9.931533363621258e-05, 8.956891621431338e-05, 8.074601533272476e-05, 7.26887789897006e-05, 6.547029759705101e-05, 5.894245205527632e-05, 5.3118881026744407e-05, 4.78331098015019e-05, 4.3087770358974774e-05, 3.882337686647419e-05, 3.495520297407607e-05, 3.1471155350010393e-05, 2.8380488475471585e-05, 2.5574313624952176e-05, 2.3038362282400482e-05, 2.0738284932571143e-05, 1.871812553880638e-05, 1.6865330622374146e-05, 1.5184207406422958e-05, 1.367115310431156e-05, 1.2334345891052678e-05, 1.1101073729909315e-05], "duration": 140287.906319, "accuracy_train": [0.4903990963855422, 0.6045745481927711, 0.6609092620481928, 0.6968655873493976, 0.7204913403614458, 0.7418345256024096, 0.7672016189759037, 0.7846620858433735, 0.8051581325301205, 0.8206419427710844, 0.8285015060240963, 0.8334196159638554, 0.8425969503012049, 0.8493740587349398, 0.8557040662650602, 0.8599397590361446, 0.8653990963855421, 0.8693053463855421, 0.8748117469879518, 0.8810005647590361, 0.8839655496987951, 0.8869070030120482, 0.8886248117469879, 0.8923428087349398, 0.896460843373494, 0.9002259036144579, 0.9018731174698795, 0.904296875, 0.9067206325301205, 0.9086972891566265, 0.9114740210843374, 0.9129565135542169, 0.9138036521084337, 0.9144625376506024, 0.9162509412650602, 0.9187217620481928, 0.9203925075301205, 0.9211925828313253, 0.9220397213855421, 0.9226044804216867, 0.9242516942771084, 0.9258989081325302, 0.92578125, 0.9260400978915663, 0.9273343373493976, 0.9282285391566265, 0.9283932605421686, 0.9297580948795181, 0.930628765060241, 0.9299228162650602, 0.9304875753012049, 0.9311464608433735, 0.932628953313253, 0.9326995481927711, 0.9328407379518072, 0.9332172439759037, 0.9338525978915663, 0.9338996611445783, 0.9338761295180723, 0.9340408509036144, 0.9346526731927711, 0.9347703313253012, 0.934582078313253, 0.9351233057228916, 0.9355939382530121, 0.9359469126506024, 0.9362763554216867, 0.9361116340361446, 0.9370058358433735, 0.9366293298192772, 0.9360410391566265, 0.9363469503012049, 0.9362528237951807, 0.936535203313253, 0.9368646460843374, 0.9363469503012049, 0.9368411144578314, 0.9369352409638554, 0.9368646460843374, 0.9366999246987951, 0.9370293674698795, 0.9376411897590361, 0.9370999623493976, 0.9371940888554217, 0.9376647213855421, 0.9375, 0.937523531626506, 0.9375941265060241, 0.9379471009036144, 0.9378765060240963, 0.9372176204819277, 0.9375705948795181, 0.9373588102409639, 0.9378294427710844, 0.9375705948795181, 0.9377353162650602, 0.9378529743975904, 0.9376176581325302, 0.9376176581325302, 0.9380882906626506, 0.9379706325301205, 0.9379235692771084, 0.9376882530120482, 0.9382765436746988, 0.9380412274096386, 0.9373823418674698, 0.9373588102409639, 0.9381588855421686, 0.9375941265060241], "end": "2016-01-22 18:28:47.230000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0], "accuracy_valid": [0.49011752136752135, 0.5954861111111112, 0.6487713675213675, 0.6801549145299145, 0.7017895299145299, 0.7215544871794872, 0.7375801282051282, 0.7541399572649573, 0.7709668803418803, 0.7781784188034188, 0.7853899572649573, 0.7916666666666666, 0.7952724358974359, 0.8010149572649573, 0.8043536324786325, 0.8050213675213675, 0.8143696581196581, 0.8134348290598291, 0.8162393162393162, 0.8169070512820513, 0.8199786324786325, 0.8205128205128205, 0.8230502136752137, 0.8246527777777778, 0.8257211538461539, 0.8257211538461539, 0.8259882478632479, 0.827857905982906, 0.8267895299145299, 0.8295940170940171, 0.8310630341880342, 0.827590811965812, 0.8317307692307693, 0.8311965811965812, 0.8348023504273504, 0.8322649572649573, 0.8353365384615384, 0.8352029914529915, 0.8340010683760684, 0.8336004273504274, 0.8348023504273504, 0.8362713675213675, 0.8350694444444444, 0.8341346153846154, 0.8353365384615384, 0.8356036324786325, 0.8358707264957265, 0.8370726495726496, 0.8362713675213675, 0.8362713675213675, 0.8360042735042735, 0.8362713675213675, 0.8357371794871795, 0.8365384615384616, 0.8370726495726496, 0.8373397435897436, 0.8373397435897436, 0.8370726495726496, 0.8381410256410257, 0.8373397435897436, 0.8368055555555556, 0.8364049145299145, 0.8362713675213675, 0.8369391025641025, 0.8365384615384616, 0.8361378205128205, 0.8365384615384616, 0.8380074786324786, 0.8376068376068376, 0.8373397435897436, 0.8364049145299145, 0.8366720085470085, 0.8344017094017094, 0.8366720085470085, 0.8361378205128205, 0.8357371794871795, 0.8362713675213675, 0.8370726495726496, 0.8366720085470085, 0.8366720085470085, 0.8365384615384616, 0.8372061965811965, 0.8360042735042735, 0.8368055555555556, 0.8372061965811965, 0.8368055555555556, 0.8360042735042735, 0.8372061965811965, 0.8365384615384616, 0.8353365384615384, 0.8366720085470085, 0.8362713675213675, 0.8366720085470085, 0.8366720085470085, 0.8370726495726496, 0.8374732905982906, 0.8376068376068376, 0.8373397435897436, 0.8374732905982906, 0.8373397435897436, 0.8370726495726496, 0.8376068376068376, 0.8358707264957265, 0.8376068376068376, 0.8373397435897436, 0.8358707264957265, 0.8358707264957265, 0.8373397435897436, 0.8366720085470085], "accuracy_test": 0.8318, "start": "2016-01-21 03:30:39.324000", "learning_rate_per_epoch": [0.005177383776754141, 0.004845478571951389, 0.00453485082834959, 0.004244136158376932, 0.003972058650106192, 0.00371742295101285, 0.0034791110083460808, 0.003256076481193304, 0.0030473400838673115, 0.0028519851621240377, 0.0026691537350416183, 0.0024980430025607347, 0.002337901620194316, 0.002188026439398527, 0.002047759247943759, 0.001916484092362225, 0.0017936246003955603, 0.0016786411870270967, 0.001571028959006071, 0.0014703153865411878, 0.001376058324240148, 0.0012878436828032136, 0.001205284264869988, 0.0011280174367129803, 0.0010557039640843868, 0.0009880262659862638, 0.0009246871341019869, 0.0008654084522277117, 0.000809929973911494, 0.0007580079836770892, 0.0007094145403243601, 0.0006639363127760589, 0.0006213735323399305, 0.0005815392942167819, 0.0005442586843855679, 0.0005093680229038, 0.00047671410720795393, 0.0004461535136215389, 0.0004175520734861493, 0.00039078417466953397, 0.0003657322668004781, 0.0003422863665036857, 0.0003203435044270009, 0.00029980731778778136, 0.0002805876429192722, 0.00026260007871314883, 0.0002457656373735517, 0.000230010409723036, 0.00021526518685277551, 0.00020146524184383452, 0.00018854996596928686, 0.0001764626504154876, 0.00016515020979568362, 0.00015456296387128532, 0.00014465443382505327, 0.00013538110943045467, 0.00012670227442868054, 0.00011857980280183256, 0.00011097804235760123, 0.00010386360372649506, 9.720524394651875e-05, 9.097373549593613e-05, 8.514170622220263e-05, 7.968355203047395e-05, 7.457529864041135e-05, 6.979452155064791e-05, 6.532022234750912e-05, 6.113275594543666e-05, 5.7213735999539495e-05, 5.354595123208128e-05, 5.0113296310883015e-05, 4.6900695451768115e-05, 4.3894044210901484e-05, 4.1080140363192186e-05, 3.844662569463253e-05, 3.598193870857358e-05, 3.367525278008543e-05, 3.151644341414794e-05, 2.9496026400011033e-05, 2.760513234534301e-05, 2.5835457563516684e-05, 2.417923133180011e-05, 2.262917951156851e-05, 2.1178497263463214e-05, 1.9820812667603604e-05, 1.855016489571426e-05, 1.7360975107294507e-05, 1.624801916477736e-05, 1.5206412172119599e-05, 1.4231578461476602e-05, 1.3319238860276528e-05, 1.2465386134863365e-05, 1.1666271348076407e-05, 1.091838475986151e-05, 1.0218443094345275e-05, 9.56337225943571e-06, 8.950295523391105e-06, 8.376521691388916e-06, 7.83953055361053e-06, 7.336963790294249e-06, 6.866614967293572e-06, 6.426418622140773e-06, 6.014442078594584e-06, 5.6288758969458286e-06, 5.268027052807156e-06, 4.930311206408078e-06, 4.6142449718900025e-06, 4.318440915085375e-06, 4.041599822812714e-06], "accuracy_train_last": 0.9375941265060241, "error_valid": [0.5098824786324787, 0.40451388888888884, 0.35122863247863245, 0.3198450854700855, 0.2982104700854701, 0.2784455128205128, 0.2624198717948718, 0.2458600427350427, 0.22903311965811968, 0.22182158119658124, 0.2146100427350427, 0.20833333333333337, 0.2047275641025641, 0.1989850427350427, 0.19564636752136755, 0.19497863247863245, 0.1856303418803419, 0.1865651709401709, 0.18376068376068377, 0.18309294871794868, 0.18002136752136755, 0.17948717948717952, 0.1769497863247863, 0.1753472222222222, 0.17427884615384615, 0.17427884615384615, 0.17401175213675213, 0.17214209401709402, 0.17321047008547008, 0.17040598290598286, 0.16893696581196582, 0.17240918803418803, 0.16826923076923073, 0.16880341880341876, 0.1651976495726496, 0.1677350427350427, 0.16466346153846156, 0.16479700854700852, 0.16599893162393164, 0.1663995726495726, 0.1651976495726496, 0.16372863247863245, 0.16493055555555558, 0.16586538461538458, 0.16466346153846156, 0.16439636752136755, 0.16412927350427353, 0.1629273504273504, 0.16372863247863245, 0.16372863247863245, 0.16399572649572647, 0.16372863247863245, 0.16426282051282048, 0.16346153846153844, 0.1629273504273504, 0.1626602564102564, 0.1626602564102564, 0.1629273504273504, 0.16185897435897434, 0.1626602564102564, 0.16319444444444442, 0.1635950854700855, 0.16372863247863245, 0.16306089743589747, 0.16346153846153844, 0.16386217948717952, 0.16346153846153844, 0.1619925213675214, 0.16239316239316237, 0.1626602564102564, 0.1635950854700855, 0.16332799145299148, 0.16559829059829057, 0.16332799145299148, 0.16386217948717952, 0.16426282051282048, 0.16372863247863245, 0.1629273504273504, 0.16332799145299148, 0.16332799145299148, 0.16346153846153844, 0.16279380341880345, 0.16399572649572647, 0.16319444444444442, 0.16279380341880345, 0.16319444444444442, 0.16399572649572647, 0.16279380341880345, 0.16346153846153844, 0.16466346153846156, 0.16332799145299148, 0.16372863247863245, 0.16332799145299148, 0.16332799145299148, 0.1629273504273504, 0.16252670940170943, 0.16239316239316237, 0.1626602564102564, 0.16252670940170943, 0.1626602564102564, 0.1629273504273504, 0.16239316239316237, 0.16412927350427353, 0.16239316239316237, 0.1626602564102564, 0.16412927350427353, 0.16412927350427353, 0.1626602564102564, 0.16332799145299148], "accuracy_train_std": [0.12396814616764953, 0.12405712806563055, 0.11985382527181497, 0.11388326053960224, 0.11309000041154367, 0.10906303189534788, 0.10475821053424285, 0.10147980849976639, 0.09788921307412575, 0.09500389970510394, 0.09389032869807781, 0.09158711882323009, 0.0900785222328673, 0.08834252424428438, 0.08749224556018827, 0.08668964334101588, 0.08512783720001439, 0.08322889798241935, 0.0815872135122595, 0.08011874012056323, 0.07880302550657557, 0.07853432775671276, 0.07833912433577803, 0.07585650025221716, 0.07576289188009992, 0.07469233319248836, 0.07311671315568705, 0.07237302748228266, 0.0719086464183577, 0.07035879197808319, 0.06985022355165675, 0.06921540235334225, 0.06848684536036914, 0.06813091060491934, 0.06780473519657662, 0.06689022848274254, 0.06649069585950328, 0.06555709695219018, 0.06553800475710911, 0.0656910589972856, 0.0648519630047014, 0.06393645985396144, 0.06411029155116087, 0.06366229639093712, 0.06331517223226876, 0.06296365003856741, 0.06267167533318109, 0.06306510586326812, 0.06221576925230168, 0.06241714139288192, 0.061891863924303084, 0.06222351200962318, 0.061274543531951546, 0.06141197307156689, 0.06142284632986863, 0.06130647645277726, 0.06109531285039061, 0.0609052274279296, 0.060843431012466476, 0.060622982570075076, 0.06063054514765831, 0.060478093035739984, 0.06029880917688586, 0.06001702935849107, 0.0600583122526328, 0.060276232586865146, 0.059843068349543484, 0.05972877058489433, 0.05966896209487037, 0.05966465594578729, 0.060351736415599384, 0.05988132256308437, 0.05973188551474549, 0.05951512146645166, 0.060158579000502366, 0.06012642784197882, 0.059827372933058294, 0.05975454266848881, 0.06006070940395785, 0.06007098380144816, 0.06014786846034361, 0.05928757690097726, 0.059841953333456974, 0.05989164160284531, 0.0597938907341225, 0.059411648486669806, 0.0593744999321787, 0.059732520532594036, 0.05971860885102116, 0.05960816928595826, 0.05973192723145367, 0.059818667420256655, 0.05997814158141595, 0.05968242144727355, 0.05942398269152974, 0.05995331996687573, 0.05934867944356578, 0.059498111054586, 0.05974478849403, 0.059396356482053894, 0.059310679204273103, 0.05973109289175441, 0.0595843831041309, 0.059369426271903794, 0.05924805146381209, 0.0597694002438508, 0.0598554107912903, 0.05920961255775883, 0.05965860917953928], "accuracy_test_std": 0.09318669432917985, "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-3, 3], "momentum": 0.5636677519180362, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.005532023715518529, "patience_threshold": 1, "do_flip": true, "batch_size": 16, "optimization": "nesterov_momentum", "nb_data_augmentation": 3, "learning_rate_decay_method": "exp", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 5.483651992157172e-06, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.06410673162852934}, "accuracy_valid_max": 0.8381410256410257, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import os\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-6, -3], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128, 256],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-3, 3)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_optimizer.learning_rate = learning_rate\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8366720085470085, "loss_train": [1.8627454042434692, 1.4782601594924927, 1.2879054546356201, 1.1673688888549805, 1.07394278049469, 0.9995841383934021, 0.9377633333206177, 0.8854265809059143, 0.8425813913345337, 0.8053578734397888, 0.7722159028053284, 0.7423463463783264, 0.7180916666984558, 0.6958896517753601, 0.6758325695991516, 0.6593804359436035, 0.6400742530822754, 0.6251810193061829, 0.6118444204330444, 0.5980535745620728, 0.5892035365104675, 0.5760865211486816, 0.565253734588623, 0.5555424690246582, 0.5447211265563965, 0.5368428826332092, 0.5294687747955322, 0.5217365026473999, 0.5163296461105347, 0.5090109705924988, 0.503360390663147, 0.4997827410697937, 0.491194486618042, 0.4863892197608948, 0.48094847798347473, 0.4789446294307709, 0.4727080166339874, 0.468265175819397, 0.46550509333610535, 0.4632028639316559, 0.4585597813129425, 0.45660674571990967, 0.45428842306137085, 0.4509056806564331, 0.45046666264533997, 0.44709545373916626, 0.44338345527648926, 0.44143620133399963, 0.4385605454444885, 0.4376954436302185, 0.43721768260002136, 0.4343138635158539, 0.43439096212387085, 0.4303863048553467, 0.43174251914024353, 0.430128812789917, 0.42684435844421387, 0.4268753230571747, 0.42682167887687683, 0.4252251088619232, 0.4238729774951935, 0.42359381914138794, 0.4240815341472626, 0.42328691482543945, 0.42097195982933044, 0.4229860305786133, 0.42133599519729614, 0.4204573929309845, 0.4213258922100067, 0.4202406406402588, 0.41924604773521423, 0.41834667325019836, 0.41790446639060974, 0.4159206748008728, 0.41671523451805115, 0.41552308201789856, 0.41859275102615356, 0.4163669943809509, 0.41428476572036743, 0.4152860641479492, 0.413391649723053, 0.41405028104782104, 0.4143151044845581, 0.41418081521987915, 0.4139707088470459, 0.4142792224884033, 0.4131850004196167, 0.41288846731185913, 0.41281235218048096, 0.41369473934173584, 0.41384050250053406, 0.41370856761932373, 0.41285666823387146, 0.4126664698123932, 0.4141325056552887, 0.41440629959106445, 0.4116538166999817, 0.41291290521621704, 0.41352492570877075, 0.41106536984443665, 0.41172584891319275, 0.4124777913093567, 0.4123670160770416, 0.4117235541343689, 0.4124516546726227, 0.41308900713920593, 0.41156163811683655, 0.4130959212779999, 0.41276034712791443], "accuracy_train_first": 0.4903990963855422, "model": "residualv4", "loss_std": [0.2603840231895447, 0.23643618822097778, 0.2421475648880005, 0.24531589448451996, 0.24201954901218414, 0.23979930579662323, 0.2356160432100296, 0.23167869448661804, 0.22897373139858246, 0.2219908982515335, 0.22046925127506256, 0.21643789112567902, 0.21196268498897552, 0.21229054033756256, 0.20820964872837067, 0.20687314867973328, 0.20145638287067413, 0.19937503337860107, 0.1973353922367096, 0.19581341743469238, 0.19383865594863892, 0.1909729540348053, 0.18723097443580627, 0.18453797698020935, 0.182979017496109, 0.1795692890882492, 0.17868578433990479, 0.17744405567646027, 0.17522677779197693, 0.17483660578727722, 0.17282874882221222, 0.1693619191646576, 0.16953979432582855, 0.16669537127017975, 0.16380149126052856, 0.1653604656457901, 0.16188529133796692, 0.16139152646064758, 0.1618388444185257, 0.16225296258926392, 0.16009066998958588, 0.1591717004776001, 0.15699350833892822, 0.1579602062702179, 0.15685616433620453, 0.1567324697971344, 0.15224964916706085, 0.15339791774749756, 0.15388691425323486, 0.15453270077705383, 0.15324977040290833, 0.15277284383773804, 0.15287558734416962, 0.1488947719335556, 0.15085738897323608, 0.14970819652080536, 0.1492006927728653, 0.1502479612827301, 0.14932003617286682, 0.1484832465648651, 0.14919771254062653, 0.14861664175987244, 0.14933393895626068, 0.14897072315216064, 0.14722725749015808, 0.14960913360118866, 0.14663481712341309, 0.1478416621685028, 0.14960020780563354, 0.1472700983285904, 0.14718234539031982, 0.1462966948747635, 0.1454627513885498, 0.14532208442687988, 0.14645381271839142, 0.14561787247657776, 0.14685608446598053, 0.14759798347949982, 0.1440919041633606, 0.145379438996315, 0.1438482254743576, 0.1438196748495102, 0.14474312961101532, 0.1467902660369873, 0.14626720547676086, 0.14586706459522247, 0.14428892731666565, 0.1440507024526596, 0.14599506556987762, 0.14546658098697662, 0.14552564918994904, 0.14578530192375183, 0.14480958878993988, 0.14425013959407806, 0.1455407440662384, 0.14471587538719177, 0.14307156205177307, 0.14402788877487183, 0.1457233726978302, 0.14457878470420837, 0.14354592561721802, 0.14460597932338715, 0.1436576396226883, 0.1451559066772461, 0.14482682943344116, 0.14445845782756805, 0.1456528604030609, 0.14467981457710266, 0.14409717917442322]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:07 2016", "state": "available"}], "summary": "948a0ff04b6e89d12d56c34573f1cdb7"}