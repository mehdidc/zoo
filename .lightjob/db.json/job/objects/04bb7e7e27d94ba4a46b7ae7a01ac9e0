{"content": {"hp_model": {"f0": 64, "f1": 64, "f2": 32, "f3": 32, "nonlin": "rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.5842111110687256, 1.233638882637024, 1.05398428440094, 0.9231602549552917, 0.8216403722763062, 0.7449710965156555, 0.6833634972572327, 0.6311419010162354, 0.5859735012054443, 0.545391857624054, 0.5079332590103149, 0.4719510078430176, 0.4372805655002594, 0.4076824188232422, 0.3807280361652374, 0.35734888911247253, 0.33407875895500183, 0.31754884123802185, 0.29268303513526917, 0.27905595302581787, 0.2603735625743866, 0.24338757991790771, 0.23521371185779572, 0.22079534828662872, 0.2125542014837265, 0.20245453715324402, 0.1899547427892685, 0.18459516763687134, 0.16902078688144684, 0.1679115891456604, 0.1671919971704483, 0.15800811350345612, 0.14833317697048187, 0.14656637609004974, 0.14485488831996918, 0.13456737995147705, 0.13169589638710022, 0.13209480047225952, 0.12407354265451431, 0.12146012485027313, 0.12743452191352844, 0.11605991423130035, 0.11513643711805344, 0.10882171243429184, 0.10936960577964783, 0.11559431999921799, 0.10791756957769394, 0.1069822758436203, 0.103553906083107, 0.1054166704416275, 0.10430825501680374, 0.10155818611383438, 0.09758172184228897, 0.09796984493732452, 0.10042571276426315, 0.09459732472896576, 0.09336651861667633, 0.094029001891613, 0.09485019743442535, 0.09598597139120102, 0.08578282594680786, 0.091420479118824, 0.09258707612752914, 0.09264809638261795, 0.08775608241558075, 0.08810397237539291, 0.08398391306400299, 0.09070491790771484, 0.08710265904664993, 0.08074764907360077, 0.08899302035570145, 0.0825251117348671, 0.08205172419548035, 0.08811644464731216, 0.08290701359510422, 0.08372896164655685, 0.07836254686117172, 0.08555181324481964, 0.08357371389865875, 0.07996512204408646, 0.07770941406488419, 0.08448375016450882, 0.08125180006027222, 0.08202145993709564, 0.0762285441160202, 0.08262504637241364, 0.07913690060377121, 0.07600663602352142, 0.08202065527439117, 0.08017439395189285, 0.05160755664110184, 0.03913269191980362, 0.03757966682314873, 0.036834314465522766, 0.036169663071632385, 0.03541041538119316, 0.03437165543437004, 0.03282437473535538, 0.030524373054504395, 0.027302514761686325, 0.023200849071145058, 0.01858799159526825, 0.015833141282200813, 0.01514357328414917, 0.013199251145124435, 0.013023740611970425, 0.013019142672419548, 0.013018535450100899, 0.013018493540585041, 0.013018492609262466, 0.013018492609262466, 0.013018492609262466, 0.013018492609262466, 0.013018492609262466, 0.013018492609262466, 0.013018492609262466, 0.013018492609262466, 0.013018492609262466, 0.013018492609262466, 0.013018492609262466, 0.013018493540585041, 0.013018492609262466, 0.013018492609262466, 0.013018492609262466, 0.013018492609262466, 0.013018492609262466, 0.013018492609262466, 0.013018492609262466, 0.013018492609262466, 0.013018492609262466, 0.013018492609262466, 0.013018492609262466, 0.013018492609262466, 0.013018492609262466, 0.013018492609262466, 0.013018492609262466, 0.013018492609262466, 0.013018492609262466, 0.013018492609262466, 0.013018492609262466, 0.013018492609262466, 0.013018492609262466, 0.013018492609262466, 0.013018492609262466, 0.013018492609262466, 0.013018492609262466, 0.013018492609262466, 0.013018492609262466, 0.013018492609262466, 0.013018492609262466, 0.013018492609262466], "moving_avg_accuracy_train": [0.0489178324854651, 0.10266345435642762, 0.15849218551731495, 0.21328971161305482, 0.26603882796097783, 0.3157795841287505, 0.36210847628919585, 0.4049158643157303, 0.44379400754685566, 0.47997219910598515, 0.5105076913354752, 0.538965656109735, 0.5668161496361277, 0.5936876216049622, 0.6178302378936169, 0.6388052803879393, 0.658336113350668, 0.6766580187826278, 0.6936145478498246, 0.7104381042543955, 0.7242192011113942, 0.7381565702040661, 0.7499166632874801, 0.7617493159244482, 0.7725402489845504, 0.782521986244641, 0.7926401863489513, 0.8036387689391392, 0.8137631408512884, 0.822507990450868, 0.8313615685975493, 0.8385253955879235, 0.8444287911066505, 0.8514414227068104, 0.8579921733278791, 0.8648527135451558, 0.8701020068074269, 0.8736219797089562, 0.8797518344154784, 0.8835110713954422, 0.8879709285762191, 0.8931984916686709, 0.8957738586793601, 0.8996958974722769, 0.9041553953727051, 0.9077364658045192, 0.9107805008812746, 0.9139549352777356, 0.9163399570750359, 0.9201814741259303, 0.9220648218741438, 0.9252175229070138, 0.92733659100091, 0.9304226748294827, 0.9322606099120567, 0.9334290837757329, 0.9355106069803855, 0.9369118645097833, 0.9382987345660493, 0.9386959131524031, 0.9397624361205699, 0.9410198537419017, 0.9429071669153767, 0.9438686405500664, 0.9457851143272856, 0.9462240613374788, 0.9472004008490337, 0.9494972669391488, 0.9496835091798039, 0.9513112845999556, 0.9529856179685869, 0.9525049844028559, 0.9540832713769299, 0.9541787192715366, 0.954866295186068, 0.9556479460234506, 0.956735117379485, 0.957834371191555, 0.9578589791593781, 0.9586412336982206, 0.9600173749844063, 0.9610628787419642, 0.9616017616238124, 0.9622190373578874, 0.9624978928102216, 0.9622257402839982, 0.9627342233223204, 0.9627920045592098, 0.9634647503069156, 0.9642143446572226, 0.9671628669617568, 0.9699583711132186, 0.9725579941578583, 0.9749581087670817, 0.9771740154868114, 0.9792334357012347, 0.98111481567993, 0.9828243337024224, 0.9843768508155227, 0.9857926813589705, 0.9870669288480735, 0.9881858858513706, 0.9880049042591276, 0.9888766038998906, 0.9896797347670536, 0.9904072028451194, 0.9910619241153786, 0.991651173258612, 0.9921814974875218, 0.9926587892935408, 0.993088351918958, 0.9934749582818332, 0.993822904008421, 0.9941360551623499, 0.9944178912008861, 0.9946715436355686, 0.9948998308267829, 0.9951052892988757, 0.9952902019237593, 0.9954566232861546, 0.9956064025123104, 0.9957412038158504, 0.9958625249890365, 0.995971714044904, 0.9960699841951848, 0.9961584273304374, 0.9962380261521648, 0.9963096650917196, 0.9963741401373187, 0.9964321676783581, 0.9964843924652933, 0.996531394773535, 0.9965736968509527, 0.9966117687206286, 0.9966460334033369, 0.9966768716177743, 0.9967046260107679, 0.9967296049644623, 0.9967520860227872, 0.9967723189752795, 0.9967905286325227, 0.9968069173240415, 0.9968216671464085, 0.9968349419865388, 0.9968468893426561, 0.9968576419631616, 0.9968673193216167, 0.9968760289442262, 0.9968838676045748, 0.9968909223988884, 0.9968972717137707], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.048889160156249986, 0.1016421398484563, 0.15540158926722514, 0.20710952427875562, 0.2571340528373258, 0.3042608854583372, 0.3472609723172474, 0.3873313265162456, 0.4225075029760668, 0.4554164735933698, 0.482683590588325, 0.508369544876631, 0.5322518286702179, 0.555006296829175, 0.5754364900472364, 0.5932824365338532, 0.6092848122328774, 0.62394123900093, 0.6374942630677044, 0.65063604416869, 0.6608012848195921, 0.670117811316925, 0.6786137250271, 0.6870971849792094, 0.6943996205663787, 0.7010309358064276, 0.707291941199806, 0.7148670286592682, 0.7208117311020763, 0.7259259116722151, 0.7312743326002495, 0.7358458298277999, 0.7392795250359085, 0.7425814882890948, 0.7463325932723088, 0.7498580135855147, 0.7525365806381078, 0.7541294198916916, 0.7572384270271459, 0.7599653502788741, 0.762339132457312, 0.7651642777112043, 0.7654149280180206, 0.7668784239210378, 0.7698549911204249, 0.7710577332546174, 0.7725889198783272, 0.7742873121054794, 0.7749247518286664, 0.7771505148329232, 0.7772014589727484, 0.7787855417087567, 0.7796221901451853, 0.7808409528701246, 0.7808545020672989, 0.7800416186903882, 0.7812378521526747, 0.7814498220673018, 0.7820200424678759, 0.7825017672779407, 0.7832781459906587, 0.7833134420373006, 0.7848387313990826, 0.7848462162693399, 0.7855793445482795, 0.7853662837286172, 0.7861785940794, 0.7875547341067011, 0.7875093157600369, 0.7876811063225875, 0.7886556479399522, 0.7877300657325534, 0.7886641198238312, 0.788046984243933, 0.7891173034682596, 0.7895373043431957, 0.7902804865594786, 0.790956410042085, 0.7905271435201806, 0.7903228796105571, 0.7915540282082364, 0.7920477394216747, 0.7921532240920826, 0.79172222844304, 0.791842027200769, 0.7904482341663096, 0.7914532977978713, 0.791720000915298, 0.7918522294570514, 0.792061684834012, 0.7936876829074632, 0.7953830147673192, 0.7969444050262801, 0.7984584900319351, 0.7998089595057748, 0.8011851324557997, 0.8024379541593915, 0.8036041738036932, 0.8046039138499053, 0.8054904433515864, 0.8064002422016687, 0.8070613992691525, 0.8065536897827493, 0.8070509586998057, 0.8074842346765871, 0.8079108041494405, 0.8082947166750085, 0.8086402379480196, 0.8089512070937297, 0.8092310793248687, 0.8094829643328939, 0.8097096608401165, 0.8099136876966169, 0.8100973118674673, 0.8102625736212326, 0.8104113091996213, 0.8105451712201712, 0.8106656470386662, 0.8107740752753115, 0.8108716606882924, 0.8109594875599752, 0.8110385317444897, 0.8111096715105528, 0.8111736973000095, 0.8112313205105206, 0.8112831813999806, 0.8113298562004946, 0.8113718635209571, 0.8114096701093735, 0.8114436960389482, 0.8114743193755654, 0.8115018803785209, 0.8115266852811808, 0.8115490096935747, 0.8115691016647293, 0.8115871844387684, 0.8116034589354036, 0.8116181059823753, 0.8116312883246498, 0.8116431524326968, 0.8116538301299392, 0.8116634400574573, 0.8116720889922235, 0.8116798730335132, 0.8116868786706739, 0.8116931837441186, 0.8116988583102187, 0.8117039654197089, 0.81170856181825, 0.8117126985769371, 0.8117164216597553], "moving_var_accuracy_train": [0.021536589015684227, 0.04538025694678417, 0.06889385625941744, 0.08902939042939534, 0.10516867486583628, 0.11691909279652897, 0.12454447975620364, 0.12858228400747185, 0.12932764579662376, 0.128174635117363, 0.12374891817690194, 0.11866272819104884, 0.11377730527891672, 0.10889825880297184, 0.10325422621402562, 0.09688837526137072, 0.09063261866119576, 0.08459058676299529, 0.07871924298875597, 0.07339460714076038, 0.06776441410192213, 0.06273622500675823, 0.05770730061005746, 0.05319667556489605, 0.04892500613517492, 0.04492922123022275, 0.041357700867358266, 0.038310650151561106, 0.03540211129594464, 0.03255015171702357, 0.03000060915931576, 0.027462431997716318, 0.025029839505799425, 0.022969448572855544, 0.021058714718864873, 0.019376446355634225, 0.017686797437850718, 0.016029629577113156, 0.014764842687909497, 0.013415545183162292, 0.012253003599502411, 0.011273649982522262, 0.010205977621427751, 0.009323821353923269, 0.00857042331224626, 0.007828797569960149, 0.007129313158900798, 0.006507075146647628, 0.0059075625927452425, 0.005449621612741526, 0.004936582440133682, 0.004532379910344242, 0.004119555965588941, 0.0037933155896028485, 0.0034443860789523727, 0.003112235451587985, 0.0028400065560927525, 0.0025736776044567233, 0.0023336205209877575, 0.002101678226354103, 0.0019017476448933398, 0.0017258027720739254, 0.0015852800539994842, 0.0014350719325513673, 0.0013246205849451525, 0.001193892596750455, 0.0010830824866518176, 0.001022254582509921, 0.0009203412998087669, 0.0008521540451939426, 0.0007921691707383566, 0.0007150313312850864, 0.0006659471061093615, 0.0005994343882036892, 0.0005437457951275142, 0.0004948700178989895, 0.00045602049012552606, 0.0004212936716031264, 0.0003791697544115372, 0.00034676007844224147, 0.00032912795415392066, 0.00030605286170213783, 0.0002780611283750657, 0.00025368427952445985, 0.00022901569484168243, 0.00020678072833528187, 0.00018842965050410633, 0.00016961673349572394, 0.00015672834171565802, 0.00014611253275420138, 0.0002097453335018075, 0.0002591043912991895, 0.00029401631193726933, 0.00031645963198021084, 0.000329005852097076, 0.00033427617146354757, 0.00033270486993531206, 0.00032573644976481835, 0.00031485558926656085, 0.0003014112154897401, 0.0002858834539121327, 0.0002685636914979687, 0.0002420021113787491, 0.00022464064261423346, 0.00020798175106092047, 0.00019194646419627083, 0.00017660975725221207, 0.00016207371250220114, 0.00014839753534190135, 0.00013560804902054674, 0.00012370796056088878, 0.00011268234482314124, 0.00010250370639868362, 9.313590856567908e-05, 8.453720168267088e-05, 7.666253753298716e-05, 6.946531915474095e-05, 6.289870589305941e-05, 5.691656941332545e-05, 5.1474177100746186e-05, 4.65286637399617e-05, 4.203933988889053e-05, 3.796787534357074e-05, 3.4278388058504756e-05, 3.093746245458006e-05, 2.7914115902681943e-05, 2.517972806419726e-05, 2.2707944496722164e-05, 2.0474563330595093e-05, 1.845741175720715e-05, 1.6636217436820405e-05, 1.4992478645958888e-05, 1.3509335973147627e-05, 1.217144758117841e-05, 1.0964869439390456e-05, 9.876941454678622e-06, 8.896180066184803e-06, 8.012177592715301e-06, 7.215508415294442e-06, 6.497641925064043e-06, 5.850862057109867e-06, 5.268193154286187e-06, 4.743331854196287e-06, 4.27058466120102e-06, 3.8448108489446505e-06, 3.4613703336798086e-06, 3.116076161711822e-06, 2.8051512632746333e-06, 2.5251891383117036e-06, 2.2731181555858056e-06, 2.0461691642224956e-06], "duration": 71800.455819, "accuracy_train": [0.4891783248546511, 0.5863740511950905, 0.6609507659653009, 0.7064674464747139, 0.740780875092285, 0.7634463896387044, 0.7790685057332041, 0.7901823565545404, 0.7936972966269842, 0.8055759231381506, 0.785327121400886, 0.7950873390780732, 0.8174705913736618, 0.835530869324474, 0.8351137844915099, 0.8275806628368402, 0.8341136100152271, 0.8415551676702658, 0.8462233094545959, 0.8618501118955334, 0.8482490728243817, 0.8635928920381136, 0.8557575010382059, 0.8682431896571613, 0.8696586465254706, 0.8723576215854559, 0.8837039872877446, 0.9026260122508305, 0.9048824880606312, 0.9012116368470838, 0.9110437719176817, 0.902999838501292, 0.8975593507751938, 0.9145551071082503, 0.9169489289174971, 0.926597575500646, 0.9173456461678663, 0.9053017358227206, 0.9349205267741787, 0.9173442042151162, 0.9281096432032114, 0.9402465595007383, 0.9189521617755629, 0.9349942466085271, 0.9442908764765596, 0.9399660996908453, 0.9381768165720746, 0.9425248448458842, 0.9378051532507383, 0.9547551275839794, 0.9390149516080657, 0.9535918322028424, 0.9464082038459765, 0.9581974292866371, 0.9488020256552234, 0.9439453485488187, 0.9542443158222591, 0.9495231822743633, 0.9507805650724437, 0.9422705204295865, 0.9493611428340717, 0.9523366123338871, 0.9598929854766519, 0.9525219032622739, 0.9630333783222591, 0.9501745844292175, 0.9559874564530271, 0.9701690617501846, 0.9513596893456996, 0.9659612633813216, 0.968054618286268, 0.9481792823112772, 0.9682878541435955, 0.9550377503229974, 0.9610544784168512, 0.962682803559893, 0.9665196595837948, 0.9677276555001846, 0.958080450869786, 0.9656815245478036, 0.9724026465600776, 0.9704724125599853, 0.9664517075604466, 0.9677745189645626, 0.9650075918812293, 0.9597763675479882, 0.9673105706672205, 0.9633120356912146, 0.969519462036268, 0.9709606938099853, 0.9936995677025655, 0.995117908476375, 0.9959546015596161, 0.9965591402500923, 0.997117175964378, 0.9977682176310447, 0.9980472354881875, 0.9982099959048542, 0.9983495048334257, 0.99853515625, 0.99853515625, 0.9982564988810447, 0.9863760699289406, 0.9967219006667589, 0.9969079125715209, 0.9969544155477114, 0.9969544155477114, 0.9969544155477114, 0.9969544155477114, 0.9969544155477114, 0.9969544155477114, 0.9969544155477114, 0.9969544155477114, 0.9969544155477114, 0.9969544155477114, 0.9969544155477114, 0.9969544155477114, 0.9969544155477114, 0.9969544155477114, 0.9969544155477114, 0.9969544155477114, 0.9969544155477114, 0.9969544155477114, 0.9969544155477114, 0.9969544155477114, 0.9969544155477114, 0.9969544155477114, 0.9969544155477114, 0.9969544155477114, 0.9969544155477114, 0.9969544155477114, 0.9969544155477114, 0.9969544155477114, 0.9969544155477114, 0.9969544155477114, 0.9969544155477114, 0.9969544155477114, 0.9969544155477114, 0.9969544155477114, 0.9969544155477114, 0.9969544155477114, 0.9969544155477114, 0.9969544155477114, 0.9969544155477114, 0.9969544155477114, 0.9969544155477114, 0.9969544155477114, 0.9969544155477114, 0.9969544155477114, 0.9969544155477114, 0.9969544155477114], "end": "2016-02-02 05:44:25.293000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0], "moving_var_accuracy_valid": [0.021511349827051156, 0.044406106642003035, 0.06597620159408521, 0.08344197632308664, 0.09761985980834284, 0.10784631900350797, 0.11370275433202161, 0.11678317846951797, 0.11624113113555849, 0.11436402114581722, 0.10961908005418466, 0.10459508627807473, 0.09926884896304408, 0.09400185645751262, 0.08835820596610726, 0.08238868562352522, 0.07645450131328721, 0.0707423487924241, 0.06532127406537279, 0.06034350435339151, 0.05523914297546913, 0.050496407671701785, 0.046096391852468256, 0.04213447450205281, 0.03840095714139036, 0.03495663050356746, 0.03181376915003393, 0.029148829785197044, 0.0265520021908791, 0.02413219555792705, 0.021976426459945284, 0.01996687109606426, 0.018076296351497524, 0.016366793368276306, 0.014856751128804514, 0.013482933311386943, 0.012199212473345386, 0.011002125458000665, 0.009988906240515353, 0.00905694061025116, 0.008201960125702068, 0.007453597124482172, 0.006708802842220719, 0.0060571989403219815, 0.005531218616921993, 0.004991116053002049, 0.004513105239991499, 0.004087755541407609, 0.003682636951873119, 0.0033589594452458735, 0.0030230868584697292, 0.002743362035653432, 0.002475325657543694, 0.0022411615350066377, 0.0020170470337326704, 0.0018212893448195217, 0.0016520391808042148, 0.0014872396439261565, 0.0013414420412806182, 0.0012093863662862444, 0.0010938726048076745, 0.000984496556625084, 0.0009069854696970634, 0.0008162874269369019, 0.0007394959779036388, 0.0006659549343291513, 0.000605298073850137, 0.0005618121188377824, 0.0005056494723899275, 0.0004553501331273676, 0.0004183627020904139, 0.0003842367536852524, 0.0003536651917256222, 0.00032172637946884647, 0.0002998639906996293, 0.0002714651982441901, 0.00024928955667916276, 0.00022847245400029498, 0.00020728363632171556, 0.00018693078639251605, 0.00018187924957937203, 0.0001658850814819083, 0.00014939671647493693, 0.000136128860072886, 0.00012264513974677841, 0.00012786455697826933, 0.0001241694774118364, 0.00011239270464625896, 0.00010131079366692098, 9.157455829466798e-05, 0.00010621193007900486, 0.00012145808810649268, 0.00013125373516284386, 0.0001387604422857068, 0.00014129830825509006, 0.0001442131453250041, 0.00014391789078141754, 0.00014176671603207517, 0.00013658536586886745, 0.00013000024029814007, 0.00012444982179682625, 0.00011593899762809675, 0.00010666501816854135, 9.822400373452116e-05, 9.009115600957254e-05, 8.271969404514861e-05, 7.577422408622573e-05, 6.927126622853268e-05, 6.321445589193234e-05, 5.759796649460402e-05, 5.2409184360554206e-05, 4.7630787681981357e-05, 4.3242351537344126e-05, 3.922157690869405e-05, 3.554522224314296e-05, 3.2189800469336505e-05, 2.9132091787314208e-05, 2.6349512414160983e-05, 2.382037131526323e-05, 2.1524040399176774e-05, 1.9441058393765398e-05, 1.7553184402338962e-05, 1.5843413758944652e-05, 1.4295966098490249e-05, 1.2896253398147678e-05, 1.163083402503314e-05, 1.0487357455557014e-05, 9.454503244753334e-06, 8.521916963427139e-06, 7.680145142035227e-06, 6.920570726541856e-06, 6.235350133842893e-06, 5.617352669222336e-06, 5.0601028167987285e-06, 4.5577257208627426e-06, 4.104896029229016e-06, 3.6967901594726777e-06, 3.329041967390327e-06, 2.997701737981877e-06, 2.6991983777214623e-06, 2.4303046589149125e-06, 2.188105349385555e-06, 1.9699680511003285e-06, 1.773516567679493e-06, 1.5966066214797938e-06, 1.437303744892097e-06, 1.2938631767067165e-06, 1.1647116021421467e-06, 1.0484305838438746e-06, 9.437415404114008e-07, 8.494921384813112e-07], "accuracy_test": 0.1313556281887755, "start": "2016-02-01 09:47:44.837000", "learning_rate_per_epoch": [0.004756789654493332, 0.004756789654493332, 0.004756789654493332, 0.004756789654493332, 0.004756789654493332, 0.004756789654493332, 0.004756789654493332, 0.004756789654493332, 0.004756789654493332, 0.004756789654493332, 0.004756789654493332, 0.004756789654493332, 0.004756789654493332, 0.004756789654493332, 0.004756789654493332, 0.004756789654493332, 0.004756789654493332, 0.004756789654493332, 0.004756789654493332, 0.004756789654493332, 0.004756789654493332, 0.004756789654493332, 0.004756789654493332, 0.004756789654493332, 0.004756789654493332, 0.004756789654493332, 0.004756789654493332, 0.004756789654493332, 0.004756789654493332, 0.004756789654493332, 0.004756789654493332, 0.004756789654493332, 0.004756789654493332, 0.004756789654493332, 0.004756789654493332, 0.004756789654493332, 0.004756789654493332, 0.004756789654493332, 0.004756789654493332, 0.004756789654493332, 0.004756789654493332, 0.004756789654493332, 0.004756789654493332, 0.004756789654493332, 0.004756789654493332, 0.004756789654493332, 0.004756789654493332, 0.004756789654493332, 0.004756789654493332, 0.004756789654493332, 0.004756789654493332, 0.004756789654493332, 0.004756789654493332, 0.004756789654493332, 0.004756789654493332, 0.004756789654493332, 0.004756789654493332, 0.004756789654493332, 0.004756789654493332, 0.004756789654493332, 0.004756789654493332, 0.004756789654493332, 0.004756789654493332, 0.004756789654493332, 0.004756789654493332, 0.004756789654493332, 0.004756789654493332, 0.004756789654493332, 0.004756789654493332, 0.004756789654493332, 0.004756789654493332, 0.004756789654493332, 0.004756789654493332, 0.004756789654493332, 0.004756789654493332, 0.004756789654493332, 0.004756789654493332, 0.004756789654493332, 0.004756789654493332, 0.004756789654493332, 0.004756789654493332, 0.004756789654493332, 0.004756789654493332, 0.004756789654493332, 0.004756789654493332, 0.004756789654493332, 0.004756789654493332, 0.004756789654493332, 0.004756789654493332, 0.0004756789712700993, 0.0004756789712700993, 0.0004756789712700993, 0.0004756789712700993, 0.0004756789712700993, 0.0004756789712700993, 0.0004756789712700993, 0.0004756789712700993, 0.0004756789712700993, 0.0004756789712700993, 0.0004756789712700993, 0.0004756789712700993, 0.0004756789712700993, 0.0004756789712700993, 4.756789712700993e-05, 4.756789621751523e-06, 4.7567897354383604e-07, 4.756789806492634e-08, 4.75678962885695e-09, 4.756789406812345e-10, 4.756789406812345e-11, 4.756789580284693e-12, 4.756789580284693e-13, 4.7567896480473285e-14, 4.7567896480473285e-15, 4.756789859805565e-16, 4.756789793631116e-17, 4.756789793631116e-18, 4.756790000426269e-19, 4.7567900004262694e-20, 4.756790161984983e-21, 4.7567903639333745e-22, 4.7567902377156297e-23, 4.75679023771563e-24, 4.756790336323243e-25, 4.7567902746934846e-26, 4.75679042876788e-27, 4.756790332471383e-28, 4.756790272286072e-29, 4.756790422749349e-30, 4.756790234670253e-31, 4.756790117120818e-32, 4.756789970184024e-33, 4.756789878348528e-34, 4.756789993142898e-35, 4.756789921396417e-36, 4.756789921396417e-37, 4.756790033500294e-38, 4.756790313759987e-39, 4.756791715058451e-40, 4.756847766997024e-41, 4.757408286382754e-42, 4.764414778704378e-43, 4.764414778704378e-44, 4.203895392974451e-45, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "accuracy_train_first": 0.4891783248546511, "accuracy_train_last": 0.9969544155477114, "batch_size_eval": 1024, "accuracy_train_std": [0.01726826660798569, 0.019327688647115985, 0.019386615954020043, 0.019434725245548033, 0.018204860942176937, 0.01831152407730936, 0.018839160414746328, 0.019123944627964927, 0.018476857432123076, 0.017885534512234614, 0.019551367604955974, 0.02206901056751358, 0.02409300866972828, 0.025008278282756893, 0.026315592966389068, 0.02519001606235267, 0.026100588158070627, 0.0252661812677549, 0.026741753122506106, 0.023402469378851185, 0.024546263869393483, 0.024895655089922746, 0.025879213763600882, 0.021177698159946832, 0.025632255462659546, 0.024871601783930025, 0.021449977736177347, 0.02300894900274166, 0.022382689463872764, 0.021318788723676256, 0.021461753559209908, 0.022070531501401928, 0.022190661274792453, 0.020462456097053446, 0.02092900862830045, 0.01978141575616814, 0.018638522554719317, 0.019706572974753724, 0.017369509742807848, 0.018548726739592396, 0.016912843876084445, 0.01661092969312001, 0.01638860559766405, 0.012847633596440148, 0.017184905111029703, 0.01684400982651704, 0.014194743117094226, 0.016745162756773343, 0.016409530763871003, 0.01427980714468721, 0.014377447323228817, 0.013300025727209567, 0.015418782426585934, 0.01215396984750386, 0.013290265144139921, 0.012242617387949309, 0.010892497813578098, 0.013029290273899485, 0.011607619502980782, 0.013847699796974998, 0.012094489276420326, 0.011239394705605739, 0.01084485826731006, 0.013052428537343409, 0.009595220146113427, 0.011263300248133951, 0.009704446339229266, 0.010939131701421622, 0.01217551183322046, 0.010200596757280545, 0.0074562347381256265, 0.009293544107719525, 0.00956188406850596, 0.010286873972308702, 0.010634512263854852, 0.010234314975519748, 0.008449681178819684, 0.008723854789710044, 0.01197698097340544, 0.009268253025350717, 0.01019912218770572, 0.008617869432759903, 0.008698324274880011, 0.008776800075356138, 0.010231860778169235, 0.011299100219438644, 0.009561767569180481, 0.0075728984800517065, 0.008366384756915457, 0.007275515594589517, 0.003337493932038827, 0.002610246898660846, 0.002200283853028357, 0.0020133686752702475, 0.0017952272287589093, 0.0015409934827066384, 0.0014453394796993594, 0.001443989073113749, 0.0014529123105559952, 0.001449259862560214, 0.001335089027153608, 0.0014536793883295372, 0.004567196324663884, 0.0018062646265233548, 0.0015937922830324216, 0.0015546305949053914, 0.0015546305949053914, 0.0015546305949053914, 0.0015546305949053914, 0.0015546305949053914, 0.0015546305949053914, 0.0015546305949053914, 0.0015546305949053914, 0.0015546305949053914, 0.0015546305949053914, 0.0015546305949053914, 0.0015546305949053914, 0.0015546305949053914, 0.0015546305949053914, 0.0015546305949053914, 0.0015546305949053914, 0.0015546305949053914, 0.0015546305949053914, 0.0015546305949053914, 0.0015546305949053914, 0.0015546305949053914, 0.0015546305949053914, 0.0015546305949053914, 0.0015546305949053914, 0.0015546305949053914, 0.0015546305949053914, 0.0015546305949053914, 0.0015546305949053914, 0.0015546305949053914, 0.0015546305949053914, 0.0015546305949053914, 0.0015546305949053914, 0.0015546305949053914, 0.0015546305949053914, 0.0015546305949053914, 0.0015546305949053914, 0.0015546305949053914, 0.0015546305949053914, 0.0015546305949053914, 0.0015546305949053914, 0.0015546305949053914, 0.0015546305949053914, 0.0015546305949053914, 0.0015546305949053914, 0.0015546305949053914, 0.0015546305949053914], "accuracy_test_std": 0.010189959231748794, "error_valid": [0.5111083984375, 0.42358104292168675, 0.3607633659638554, 0.3275190606174698, 0.29264519013554224, 0.27159762095256024, 0.26573824595256024, 0.2520354856927711, 0.26090690888554224, 0.24840279085090367, 0.27191235645707834, 0.26045686652861444, 0.2528076171875, 0.24020348974021077, 0.24069177099021077, 0.24610404508659633, 0.24669380647590367, 0.24415092008659633, 0.24052852033132532, 0.23108792592243976, 0.24771154932228923, 0.24603345020707834, 0.24492305158132532, 0.23655167545180722, 0.23987845914909633, 0.23928722703313254, 0.23635901025978923, 0.2169571842055723, 0.22568594691265065, 0.2280464631965362, 0.22058987904743976, 0.22301069512424698, 0.22981721809111444, 0.22770084243222888, 0.2199074618787651, 0.21841320359563254, 0.2233563158885542, 0.2315350268260542, 0.2147805087537651, 0.2154923404555723, 0.21629682793674698, 0.2094094150037651, 0.23232921922063254, 0.21995011295180722, 0.2033559040850903, 0.21811758753765065, 0.2136304005082832, 0.21042715785015065, 0.21933829066265065, 0.2028176181287651, 0.22234004376882532, 0.20695771366716864, 0.21284797392695776, 0.20819018260542166, 0.21902355515813254, 0.22727433170180722, 0.20799604668674698, 0.2166424487010542, 0.21284797392695776, 0.21316270943147586, 0.20973444559487953, 0.21636889354292166, 0.20143366434487953, 0.21508641989834332, 0.2078225009412651, 0.21655126364834332, 0.2065106127635542, 0.2000600056475903, 0.21289944935993976, 0.21077277861445776, 0.2025734775037651, 0.2206001741340362, 0.20292939335466864, 0.21750723597515065, 0.20124982351280118, 0.20668268778237953, 0.20303087349397586, 0.20296027861445776, 0.21333625517695776, 0.2115154955760542, 0.19736563441265065, 0.20350885965737953, 0.20689741387424698, 0.21215673239834332, 0.20707978397966864, 0.22209590314382532, 0.1995011295180723, 0.20587967102786142, 0.20695771366716864, 0.20605321677334332, 0.19167833443147586, 0.18935899849397586, 0.1890030826430723, 0.18791474491716864, 0.18803681522966864, 0.18642931099397586, 0.1862866505082832, 0.1858998493975903, 0.18639842573418675, 0.1865307911332832, 0.1854115681475903, 0.18698818712349397, 0.19801569559487953, 0.18847362104668675, 0.18861628153237953, 0.18825007059487953, 0.18825007059487953, 0.18825007059487953, 0.18825007059487953, 0.18825007059487953, 0.18825007059487953, 0.18825007059487953, 0.18825007059487953, 0.18825007059487953, 0.18825007059487953, 0.18825007059487953, 0.18825007059487953, 0.18825007059487953, 0.18825007059487953, 0.18825007059487953, 0.18825007059487953, 0.18825007059487953, 0.18825007059487953, 0.18825007059487953, 0.18825007059487953, 0.18825007059487953, 0.18825007059487953, 0.18825007059487953, 0.18825007059487953, 0.18825007059487953, 0.18825007059487953, 0.18825007059487953, 0.18825007059487953, 0.18825007059487953, 0.18825007059487953, 0.18825007059487953, 0.18825007059487953, 0.18825007059487953, 0.18825007059487953, 0.18825007059487953, 0.18825007059487953, 0.18825007059487953, 0.18825007059487953, 0.18825007059487953, 0.18825007059487953, 0.18825007059487953, 0.18825007059487953, 0.18825007059487953, 0.18825007059487953, 0.18825007059487953, 0.18825007059487953], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.037765334197608094, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 32, "valid_ratio": 0.15, "learning_rate": 0.004756789735219578, "optimization": "adam", "nb_data_augmentation": 0, "learning_rate_decay_method": "discrete", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 4.929649797961125e-08, "rotation_range": [0, 0], "momentum": 0.7952122813450159}, "accuracy_valid_max": 0.8145884318524097, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8117499294051205, "accuracy_valid_std": [0.0213392732988448, 0.016905026350246273, 0.012942616522432969, 0.02300152545437775, 0.01936494702728743, 0.016439794869872616, 0.01464963876567548, 0.019080757600395903, 0.022968556633768563, 0.01394858592277275, 0.013989370048541859, 0.0204465375062584, 0.02173502963716756, 0.02090377023445876, 0.020196799368988303, 0.019165231719838985, 0.016420405396265384, 0.02005975705698026, 0.019066775370003928, 0.014234738906059245, 0.012283389292490227, 0.017671095486364023, 0.021165524861934265, 0.015163905921408443, 0.01386296274131038, 0.018624771380934574, 0.018437691921678143, 0.014735015869728531, 0.015446423623218982, 0.013252850922083479, 0.013664545902550158, 0.011921807800492661, 0.013724765605395548, 0.013081860550668116, 0.01789689451435661, 0.010874727439573025, 0.014283796629491448, 0.015220424398969814, 0.014793880800276712, 0.01633839020316491, 0.012328827782894842, 0.012808533965757042, 0.014640976165694652, 0.015826548525786473, 0.0176992815686645, 0.012156406637756993, 0.015228727850406566, 0.011975002800028904, 0.013452593224222324, 0.00780279567680719, 0.013034661886236767, 0.012016010318330017, 0.015300406393522367, 0.013787682051104205, 0.010860049190026907, 0.019857221273968398, 0.012916076575322512, 0.012766625016346385, 0.011176167794840257, 0.015613818739776361, 0.016369381669739034, 0.019499262024539343, 0.01702079095385563, 0.012311710581650634, 0.01033158626010417, 0.01056069149851313, 0.01723175128300945, 0.014371141388215727, 0.006258604666376051, 0.014693873325637385, 0.01391802766427713, 0.01336599071945994, 0.008380036715294581, 0.010352556718138213, 0.018468084573469122, 0.013042969100629409, 0.017177491996989, 0.017819232888319415, 0.013819924906745154, 0.009552411978942386, 0.013866555020703953, 0.01412644881760853, 0.014336223332014396, 0.010882528080010053, 0.015379737866362715, 0.013765069935723245, 0.013676949457500875, 0.013465427677826866, 0.016198817877409663, 0.015447784071442497, 0.015133340717544762, 0.015828164446190524, 0.015255953308880592, 0.014961223665999357, 0.014392774753049314, 0.013142741251752186, 0.014561961562175863, 0.015035384704370144, 0.015007414649800172, 0.015007323558596217, 0.0152279716997337, 0.014834465010908316, 0.01569827343513995, 0.015824119688238606, 0.01561512200851706, 0.015701658250817516, 0.015701658250817516, 0.015701658250817516, 0.015701658250817516, 0.015701658250817516, 0.015701658250817516, 0.015701658250817516, 0.015701658250817516, 0.015701658250817516, 0.015701658250817516, 0.015701658250817516, 0.015701658250817516, 0.015701658250817516, 0.015701658250817516, 0.015701658250817516, 0.015701658250817516, 0.015701658250817516, 0.015701658250817516, 0.015701658250817516, 0.015701658250817516, 0.015701658250817516, 0.015701658250817516, 0.015701658250817516, 0.015701658250817516, 0.015701658250817516, 0.015701658250817516, 0.015701658250817516, 0.015701658250817516, 0.015701658250817516, 0.015701658250817516, 0.015701658250817516, 0.015701658250817516, 0.015701658250817516, 0.015701658250817516, 0.015701658250817516, 0.015701658250817516, 0.015701658250817516, 0.015701658250817516, 0.015701658250817516, 0.015701658250817516, 0.015701658250817516, 0.015701658250817516, 0.015701658250817516, 0.015701658250817516, 0.015701658250817516, 0.015701658250817516], "accuracy_valid": [0.4888916015625, 0.5764189570783133, 0.6392366340361446, 0.6724809393825302, 0.7073548098644578, 0.7284023790474398, 0.7342617540474398, 0.7479645143072289, 0.7390930911144578, 0.7515972091490963, 0.7280876435429217, 0.7395431334713856, 0.7471923828125, 0.7597965102597892, 0.7593082290097892, 0.7538959549134037, 0.7533061935240963, 0.7558490799134037, 0.7594714796686747, 0.7689120740775602, 0.7522884506777108, 0.7539665497929217, 0.7550769484186747, 0.7634483245481928, 0.7601215408509037, 0.7607127729668675, 0.7636409897402108, 0.7830428157944277, 0.7743140530873494, 0.7719535368034638, 0.7794101209525602, 0.776989304875753, 0.7701827819088856, 0.7722991575677711, 0.7800925381212349, 0.7815867964043675, 0.7766436841114458, 0.7684649731739458, 0.7852194912462349, 0.7845076595444277, 0.783703172063253, 0.7905905849962349, 0.7676707807793675, 0.7800498870481928, 0.7966440959149097, 0.7818824124623494, 0.7863695994917168, 0.7895728421498494, 0.7806617093373494, 0.7971823818712349, 0.7776599562311747, 0.7930422863328314, 0.7871520260730422, 0.7918098173945783, 0.7809764448418675, 0.7727256682981928, 0.792003953313253, 0.7833575512989458, 0.7871520260730422, 0.7868372905685241, 0.7902655544051205, 0.7836311064570783, 0.7985663356551205, 0.7849135801016567, 0.7921774990587349, 0.7834487363516567, 0.7934893872364458, 0.7999399943524097, 0.7871005506400602, 0.7892272213855422, 0.7974265224962349, 0.7793998258659638, 0.7970706066453314, 0.7824927640248494, 0.7987501764871988, 0.7933173122176205, 0.7969691265060241, 0.7970397213855422, 0.7866637448230422, 0.7884845044239458, 0.8026343655873494, 0.7964911403426205, 0.793102586125753, 0.7878432676016567, 0.7929202160203314, 0.7779040968561747, 0.8004988704819277, 0.7941203289721386, 0.7930422863328314, 0.7939467832266567, 0.8083216655685241, 0.8106410015060241, 0.8109969173569277, 0.8120852550828314, 0.8119631847703314, 0.8135706890060241, 0.8137133494917168, 0.8141001506024097, 0.8136015742658133, 0.8134692088667168, 0.8145884318524097, 0.813011812876506, 0.8019843044051205, 0.8115263789533133, 0.8113837184676205, 0.8117499294051205, 0.8117499294051205, 0.8117499294051205, 0.8117499294051205, 0.8117499294051205, 0.8117499294051205, 0.8117499294051205, 0.8117499294051205, 0.8117499294051205, 0.8117499294051205, 0.8117499294051205, 0.8117499294051205, 0.8117499294051205, 0.8117499294051205, 0.8117499294051205, 0.8117499294051205, 0.8117499294051205, 0.8117499294051205, 0.8117499294051205, 0.8117499294051205, 0.8117499294051205, 0.8117499294051205, 0.8117499294051205, 0.8117499294051205, 0.8117499294051205, 0.8117499294051205, 0.8117499294051205, 0.8117499294051205, 0.8117499294051205, 0.8117499294051205, 0.8117499294051205, 0.8117499294051205, 0.8117499294051205, 0.8117499294051205, 0.8117499294051205, 0.8117499294051205, 0.8117499294051205, 0.8117499294051205, 0.8117499294051205, 0.8117499294051205, 0.8117499294051205, 0.8117499294051205, 0.8117499294051205, 0.8117499294051205, 0.8117499294051205, 0.8117499294051205], "seed": 582626570, "model": "residualv3", "loss_std": [0.2777201533317566, 0.19814994931221008, 0.1957509070634842, 0.1920190006494522, 0.1904093474149704, 0.18885113298892975, 0.18476882576942444, 0.1804923564195633, 0.17500652372837067, 0.16846270859241486, 0.16163651645183563, 0.15606889128684998, 0.15314660966396332, 0.14853021502494812, 0.1453775316476822, 0.13900582492351532, 0.13127267360687256, 0.13133381307125092, 0.12792038917541504, 0.1240888237953186, 0.11636709421873093, 0.11485833674669266, 0.11273149400949478, 0.1100676953792572, 0.10890220105648041, 0.10536589473485947, 0.10578952729701996, 0.10636942088603973, 0.09470482170581818, 0.09912887960672379, 0.09699337929487228, 0.09358590096235275, 0.09200073778629303, 0.08738677203655243, 0.09149514138698578, 0.08295374363660812, 0.08958958089351654, 0.0826854482293129, 0.07983739674091339, 0.0842047780752182, 0.08568989485502243, 0.07612399756908417, 0.07810603082180023, 0.07720594108104706, 0.07473993301391602, 0.07825666666030884, 0.07439281046390533, 0.07391975075006485, 0.0696406438946724, 0.07246166467666626, 0.07265102118253708, 0.07711820304393768, 0.0680730789899826, 0.06824642419815063, 0.06965696811676025, 0.07318771630525589, 0.06517305970191956, 0.06753856688737869, 0.0687638372182846, 0.07083462923765182, 0.061477646231651306, 0.06596747040748596, 0.0654788464307785, 0.071040578186512, 0.06357062608003616, 0.06389189511537552, 0.059786856174468994, 0.06709476560354233, 0.06300336122512817, 0.057869866490364075, 0.06592106819152832, 0.06045901030302048, 0.061279911547899246, 0.06599254161119461, 0.06110575050115585, 0.06044178828597069, 0.0578262060880661, 0.0634697750210762, 0.058834485709667206, 0.05892855301499367, 0.055495571345090866, 0.06026454269886017, 0.057267867028713226, 0.06027701869606972, 0.05203637108206749, 0.05994895100593567, 0.05423460155725479, 0.05535084381699562, 0.06177534908056259, 0.05867437273263931, 0.03038862720131874, 0.004758097231388092, 0.0014826920814812183, 0.0008471959736198187, 0.0005220254533924162, 0.00037880774470977485, 0.000400381104554981, 0.000555298407562077, 0.0007965091499499977, 0.0010690188501030207, 0.0012873189989477396, 0.0013509803684428334, 0.013659704476594925, 0.009998038411140442, 0.0017268321244046092, 0.000319259095704183, 0.0003001494624186307, 0.0002974217932205647, 0.00029720584279857576, 0.0002971991489175707, 0.0002971985668409616, 0.00029719958547502756, 0.0002971991489175707, 0.0002971990033984184, 0.0002971990907099098, 0.00029719906160607934, 0.0002971990907099098, 0.00029719993472099304, 0.0002971992071252316, 0.0002971989451907575, 0.00029719987651333213, 0.00029719885787926614, 0.0002971993526443839, 0.00029719973099417984, 0.0002971983631141484, 0.0002971990907099098, 0.00029719879967160523, 0.0002971992362290621, 0.0002971983631141484, 0.0002971985668409616, 0.00029719932354055345, 0.00029720025486312807, 0.00029719879967160523, 0.00029719911981374025, 0.0002971978101413697, 0.00029719946905970573, 0.0002971987414639443, 0.00029719938174821436, 0.00029719871236011386, 0.00029719973099417984, 0.0002971994108520448, 0.0002971994108520448, 0.00029719926533289254, 0.00029719871236011386, 0.0002971994399558753, 0.0002971983631141484, 0.0002971983631141484, 0.0002971989451907575, 0.0002971995563711971, 0.0002972000220324844, 0.00029719859594479203]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:30 2016", "state": "available"}], "summary": "65634b408b53ca3a64c26746c7cf5657"}