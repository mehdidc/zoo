{"content": {"hp_model": {"f0": 32, "f1": 32, "f2": 64, "f3": 64, "nonlin": "rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.6870436668395996, 1.3055381774902344, 1.1008778810501099, 0.9343987107276917, 0.8165416121482849, 0.728226363658905, 0.6605358123779297, 0.6064271926879883, 0.5637481808662415, 0.5246374011039734, 0.49368003010749817, 0.4639896750450134, 0.43895798921585083, 0.41559383273124695, 0.395399808883667, 0.3743385076522827, 0.35511863231658936, 0.3374020755290985, 0.3242456018924713, 0.3098863959312439, 0.2955264747142792, 0.28082025051116943, 0.2696971893310547, 0.2625267207622528, 0.25085732340812683, 0.23989389836788177, 0.23243850469589233, 0.22522138059139252, 0.2148183137178421, 0.21399499475955963, 0.20185407996177673, 0.19462350010871887, 0.18754737079143524, 0.18651050329208374, 0.18032537400722504, 0.1754857897758484, 0.1714739352464676, 0.16241693496704102, 0.16093853116035461, 0.15845569968223572, 0.15414343774318695, 0.14867131412029266, 0.14790166914463043, 0.1400526911020279, 0.14173747599124908, 0.13372968137264252, 0.13638103008270264, 0.12992236018180847, 0.12987862527370453, 0.12346027791500092, 0.12423848360776901, 0.12060366570949554, 0.1203439012169838, 0.11878743767738342, 0.11101888120174408, 0.1135464683175087, 0.11030764877796173, 0.10657088458538055, 0.11205000430345535, 0.1036706492304802, 0.1040465235710144, 0.10193870216608047, 0.09936179965734482, 0.09878420829772949, 0.09915503114461899, 0.10016267746686935, 0.09418849647045135, 0.09386712312698364, 0.09213580191135406, 0.091712586581707, 0.09079480916261673, 0.08996174484491348, 0.08739608526229858, 0.09211631864309311, 0.08479001373052597, 0.08872418105602264, 0.08229481428861618, 0.08667881041765213, 0.08053955435752869, 0.0828004702925682, 0.08298991620540619, 0.07996520400047302, 0.08164344727993011, 0.07898078113794327, 0.07939878106117249, 0.08006185293197632, 0.07587339729070663, 0.07848534733057022, 0.07705944776535034, 0.07614262402057648, 0.0798700824379921, 0.07173001766204834, 0.0760888084769249, 0.07378103584051132, 0.07395396381616592, 0.07210028916597366, 0.07576979696750641, 0.07444305717945099, 0.06900956481695175, 0.07318653911352158, 0.07102824002504349, 0.06971018761396408, 0.07122629135847092, 0.06831888854503632, 0.07363655418157578, 0.06470861285924911, 0.07150623202323914, 0.06875544786453247, 0.06724192202091217, 0.06832819432020187, 0.06899394094944, 0.07009142637252808, 0.06710237264633179, 0.06496705114841461, 0.06818274408578873, 0.06550704687833786, 0.06755228340625763, 0.06536035984754562, 0.06453011184930801, 0.0674675926566124, 0.06528456509113312, 0.065067820250988, 0.06162228807806969, 0.06590687483549118, 0.06448885053396225, 0.0632028579711914, 0.0632629245519638, 0.0647502914071083, 0.06448519229888916, 0.0625593438744545, 0.06402251124382019, 0.06272873282432556, 0.062149982899427414, 0.061554569751024246, 0.06625726819038391, 0.0636192038655281, 0.06157906353473663, 0.06456214189529419, 0.06179007142782211, 0.06145479157567024, 0.06090247258543968, 0.061692167073488235, 0.06400032341480255, 0.06135726720094681, 0.061141449958086014, 0.06261372566223145, 0.060500916093587875, 0.061696864664554596, 0.061242133378982544, 0.0589786171913147, 0.06084325537085533, 0.06117790564894676, 0.06051486358046532, 0.06068450212478638, 0.06024918332695961, 0.05759815126657486, 0.060564979910850525, 0.06411873549222946, 0.06015310436487198, 0.05972304940223694, 0.05825677141547203, 0.057441551238298416, 0.06047983467578888, 0.059694357216358185, 0.061168305575847626, 0.0588027648627758, 0.05998581275343895, 0.056984372437000275, 0.062110573053359985, 0.05724469572305679, 0.05862058699131012, 0.06072685867547989, 0.06156997010111809, 0.05530068650841713, 0.05852659419178963, 0.06032839044928551, 0.058221425861120224, 0.059459201991558075, 0.059750720858573914, 0.05760147050023079, 0.05822376906871796, 0.059192415326833725, 0.055996861308813095, 0.057013265788555145, 0.06062261387705803, 0.05586419999599457, 0.0596478134393692, 0.059373605996370316, 0.058876655995845795, 0.05769731104373932, 0.058198604732751846, 0.05668892711400986, 0.060934245586395264, 0.05773421749472618, 0.05836651474237442, 0.05549890920519829, 0.059292204678058624, 0.06165491044521332, 0.05273013934493065, 0.061383236199617386, 0.056002430617809296, 0.05948043614625931, 0.05696883425116539, 0.05632017180323601, 0.057976555079221725, 0.06024850159883499, 0.05759338662028313, 0.05789806693792343, 0.057877108454704285, 0.05821090191602707, 0.058696214109659195, 0.05868309736251831, 0.057178840041160583, 0.059561874717473984, 0.05760017782449722, 0.053840309381484985, 0.059584200382232666, 0.05802808329463005, 0.0589325875043869, 0.05724737048149109, 0.05554502457380295, 0.0562211275100708, 0.059219084680080414, 0.055985622107982635, 0.0574970580637455, 0.05794244259595871, 0.05898454412817955, 0.05652161315083504, 0.057116053998470306, 0.06137966737151146, 0.056284528225660324, 0.059094373136758804, 0.056305378675460815, 0.05909593403339386, 0.05729597806930542, 0.05657314881682396, 0.05346732586622238, 0.06138888746500015, 0.055911436676979065, 0.05949815362691879, 0.05501939728856087, 0.05697185918688774, 0.059201765805482864, 0.055036988109350204, 0.05748990550637245, 0.058655668050050735, 0.0562613271176815, 0.059478484094142914, 0.055290114134550095, 0.057552870362997055, 0.05684269592165947], "moving_avg_accuracy_train": [0.047508235294117636, 0.10134094117647055, 0.1578703764705882, 0.21385980941176463, 0.2699561814117646, 0.32167350444705867, 0.372087330472941, 0.418549185660941, 0.46244250238896456, 0.5028406050912446, 0.5402718386997672, 0.5742281842415552, 0.6062688952291644, 0.6347902410003656, 0.6615135698415054, 0.6863551540338255, 0.7087784621598547, 0.7293900277085751, 0.7485216131730117, 0.76651651067924, 0.7821707419642572, 0.7970619030619491, 0.8108051245204602, 0.8233669650095906, 0.8342090920380433, 0.8451458298930625, 0.8554641880802268, 0.8646589457427923, 0.8734401099920425, 0.8816631578163676, 0.8889509596817897, 0.8955123343018461, 0.9017352185187203, 0.9074346378433189, 0.9124394093531046, 0.9169648801825001, 0.9218448627524853, 0.9258556705948838, 0.9296795153001013, 0.9324433284759734, 0.9360742897460231, 0.9385186254773031, 0.9412902923413375, 0.944307145460145, 0.9466293720906012, 0.9491876113521294, 0.9510735560992694, 0.952883847548166, 0.954903698087467, 0.9566745047493085, 0.9586235248626129, 0.9602388194351752, 0.961591408079893, 0.9629828555071978, 0.9642728052505957, 0.9658125835490655, 0.9669889722529825, 0.9681841926747431, 0.9692081263484453, 0.9703155490077184, 0.97144164116577, 0.9722339476374283, 0.973198788167803, 0.9739212622921992, 0.9740303125335675, 0.9749825753978578, 0.9750678472698366, 0.9756222390134413, 0.9746858974650383, 0.9749443665420638, 0.9754428710643281, 0.9758774074873071, 0.9763696667385764, 0.9770927000647188, 0.9772799006464822, 0.9773189694053634, 0.9775776607001212, 0.9782928358065797, 0.9788729639906276, 0.9792586087680354, 0.9799374537735848, 0.9806001789844615, 0.9813307493213095, 0.9815270861538844, 0.9814167304796725, 0.9818044691964111, 0.9820710811002993, 0.9824098553432106, 0.9820700462794778, 0.9822559828280006, 0.9823150904275535, 0.9822365225612687, 0.982283458540436, 0.9829539362158041, 0.9831220720059884, 0.982892217746566, 0.9833300547954388, 0.9833429316688361, 0.9834933443843055, 0.9828310687694043, 0.983338550127758, 0.9838258715855703, 0.9841397550152486, 0.9845657795137237, 0.9850056721505867, 0.9852792225825868, 0.9856454179713869, 0.9856549938213071, 0.9859083179685881, 0.9862892508776117, 0.9864391493192622, 0.986750528504983, 0.9868825344780141, 0.9871048692655068, 0.9871661470448384, 0.98742600292859, 0.987483402635731, 0.9872880035486284, 0.9873050855467067, 0.9876428122861536, 0.9876361781163617, 0.9876678544223727, 0.9876093042742531, 0.9879424914938866, 0.988143536462145, 0.9883150651688717, 0.988175323357867, 0.9882236733750214, 0.9881518942728134, 0.9885414107278849, 0.9886896225962729, 0.9887618368072338, 0.9887233001853339, 0.98865802899033, 0.9885522260912969, 0.9888734740704025, 0.9892378913692447, 0.9893188081146731, 0.989483397891441, 0.9890385875140616, 0.9895159052332436, 0.9892607852981545, 0.9896994126506921, 0.9897412360915052, 0.9897741713058841, 0.9893779306458839, 0.9897507258165896, 0.9895591826466954, 0.9897185584996729, 0.9897255261791175, 0.9896776794435587, 0.9893946173815558, 0.9896504497610472, 0.9897583459614131, 0.9900272172476247, 0.9901115543463916, 0.9903286342058701, 0.990660476667636, 0.990467370177343, 0.9905476919831381, 0.9908458639612948, 0.9912271599181065, 0.9913067968674723, 0.9915619995336662, 0.9918599172273584, 0.9917539255046226, 0.9918279447188663, 0.991976914952862, 0.9920309881634581, 0.991891418758877, 0.9920528651182834, 0.9919958139005727, 0.9919562325105155, 0.9917370798476992, 0.9917657248041057, 0.9920197405589892, 0.9917495312089726, 0.9908734016174872, 0.9913154732204443, 0.991294514133694, 0.9915438862497363, 0.9917094976247627, 0.9916067831564042, 0.9917025754289991, 0.9918617296508051, 0.9920449684504304, 0.9919510598406814, 0.9920547773860251, 0.9919528290591874, 0.9920210755650333, 0.9921836738908829, 0.9922053065017946, 0.992429481733968, 0.9923135923841007, 0.992044586086867, 0.991823656889945, 0.9918059970833034, 0.9917548091396788, 0.9919252105786521, 0.9922950424619633, 0.9926043617451787, 0.9927886314530138, 0.9930250624253595, 0.9931178503004706, 0.9933519476233648, 0.993440282272793, 0.9935786069866902, 0.9936466286409623, 0.9935878481298073, 0.9937020044932972, 0.993804745220438, 0.9933136824631001, 0.9935329024520843, 0.9930266710304052, 0.9932016509861883, 0.9932250152993342, 0.9933613372988125, 0.9935028506277548, 0.9936819773296852, 0.9935208384202462, 0.9936887545782215, 0.9937739967674581, 0.9936460088554182, 0.9938508197345823, 0.9939292671728889, 0.9939833992791294, 0.9941356475865105, 0.9941079651808007, 0.9940665804274265, 0.9939328635611544, 0.9940878124991566, 0.9942084430139468, 0.9938087751831404, 0.9940396623707086, 0.9941133431924614, 0.9941796559320388, 0.9942911021035409, 0.9943396389520103, 0.9943362632921033, 0.9943332251981871, 0.9941869615018978, 0.994012971234061, 0.9940234388165372, 0.9942846243466482, 0.994489103088454, 0.9942637221913733, 0.9942397029134125, 0.994314556151483, 0.9945113358304523, 0.9946178493062305, 0.9944219467285487], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.04651999999999999, 0.09905466666666665, 0.15406919999999996, 0.20902227999999995, 0.26408005199999995, 0.31367204679999994, 0.3613981754533333, 0.40580502457466666, 0.4473311887838667, 0.48553140323881333, 0.5214315962482653, 0.5530084366234388, 0.5827609262944282, 0.6091781669983187, 0.6337003502984868, 0.6558769819353049, 0.675675950408441, 0.6941483553675969, 0.7109201864975039, 0.7263481678477535, 0.7395933510629781, 0.7519940159566803, 0.7636346143610122, 0.7743244862582443, 0.7832653709657532, 0.7921388338691778, 0.8007516171489266, 0.8081697887673673, 0.815326143223964, 0.821966862234901, 0.8276368426780776, 0.8330198250769364, 0.8377178425692428, 0.8421860583123185, 0.8459941191477534, 0.849234707232978, 0.8528845698430135, 0.8558094461920456, 0.8584551682395077, 0.8605163180822236, 0.8635446862740012, 0.865270217646601, 0.8674231958819408, 0.8695608762937467, 0.8716314553310387, 0.8736149764646014, 0.8753068121514745, 0.8764827976029937, 0.877901184509361, 0.8797643993917583, 0.8814412927859159, 0.8822571635073242, 0.8830181138232585, 0.884249635774266, 0.8851580055301727, 0.8862155383104887, 0.8872073178127732, 0.8883532526981626, 0.8892379274283464, 0.8898874680188451, 0.8906053878836272, 0.8909448490952645, 0.8919970308524047, 0.8924906611004975, 0.8923349283237811, 0.8930081021580697, 0.8931206252755961, 0.8933285627480364, 0.8923557064732328, 0.8922668024925762, 0.8924934555766518, 0.89321744335232, 0.893655699017088, 0.8940634624487125, 0.894123782870508, 0.8943514045834572, 0.8947295974584448, 0.8953099710459337, 0.8961523072746737, 0.8966704098805396, 0.8969900355591524, 0.8979310320032371, 0.89848459546958, 0.8985828025892887, 0.8980445223303598, 0.8988534034306571, 0.898914729754258, 0.8997832567788322, 0.8995515977676156, 0.8993831046575208, 0.8993914608584354, 0.8993456481059251, 0.899144416628666, 0.8998033082991327, 0.8997963108025527, 0.8987100130556307, 0.8989190117500676, 0.8993071105750609, 0.8989897328508881, 0.898197426232466, 0.8982843502758862, 0.8988025819149642, 0.8991489903901345, 0.8995807580177878, 0.9000226822160091, 0.9002737473277416, 0.9005263725949675, 0.900047068668804, 0.9005490284685903, 0.9010807922883979, 0.9010793797262248, 0.9011714417536022, 0.9011876309115753, 0.9014555344870844, 0.9015766477050425, 0.9022589829345382, 0.9020330846410843, 0.9014831095103093, 0.901521465225945, 0.9020226520366839, 0.9023270534996821, 0.9020276814830472, 0.9022782466680758, 0.9023970886679349, 0.9024373798011415, 0.9025536418210274, 0.9023916109722581, 0.9027524498750323, 0.9023572048875291, 0.9026814843987762, 0.9026800026255651, 0.9029053356963419, 0.9033748021267077, 0.9033039885807036, 0.9027469230559666, 0.9030188974170367, 0.9037836743419997, 0.9043386402411331, 0.9045847762170198, 0.9041529652619844, 0.9050443354024527, 0.9050732351955407, 0.9053125783426533, 0.9048746538417214, 0.9044538551242159, 0.9039418029451277, 0.9045076226506149, 0.9044568603855534, 0.9048911743469981, 0.9051620569122982, 0.9048591845544018, 0.9046799327656283, 0.9046919394890655, 0.9047560788734923, 0.9045871376528097, 0.9046617572208622, 0.905115581498776, 0.9052973566822318, 0.9051142876806753, 0.9053095255792744, 0.9056452396880136, 0.9060673823858789, 0.905980644147291, 0.9063159130658952, 0.9063909884259723, 0.9063385562500417, 0.9060780339583708, 0.9066168972292004, 0.9071152075062804, 0.9068036867556524, 0.9068033180800872, 0.9069496529387451, 0.9068546876448706, 0.9066625522137169, 0.9067296303256785, 0.906470000626444, 0.9053696672304663, 0.9045660338407531, 0.9048960971233445, 0.9047131540776767, 0.905201838669909, 0.906001654802918, 0.9056281559892929, 0.9057453403903636, 0.9060508063513272, 0.9060857257161944, 0.9063171531445748, 0.9062987711634507, 0.9059088940471056, 0.9059580046423951, 0.9061488708448222, 0.90609398376034, 0.9066179187176393, 0.9065161268458753, 0.9060378474946211, 0.9058607294118257, 0.9055546564706431, 0.9052791908235789, 0.9057379384078876, 0.9060174779004322, 0.9068823967770556, 0.9072608237660167, 0.9074014080560817, 0.9073012672504736, 0.9075444738587596, 0.9074966931395503, 0.9073870238255952, 0.9076883214430357, 0.9075728226320654, 0.9073088737021922, 0.907737986331973, 0.9068975210321091, 0.9075144355955649, 0.9070163253693417, 0.9074880261657409, 0.9072192235491668, 0.9074973011942501, 0.9080009044081584, 0.9080408139673426, 0.907743399237275, 0.9077423926468807, 0.9076881533821927, 0.9075860047106401, 0.9074007375729094, 0.9079806638156185, 0.9077692641007233, 0.908112337690651, 0.9083677705882526, 0.9084643268627607, 0.9076578941764847, 0.9077854380921695, 0.9081402276162859, 0.9073528715213239, 0.9077509177025248, 0.907882492598939, 0.9083609100057117, 0.9084181523384739, 0.9086696704379599, 0.9091093700608306, 0.9090384330547476, 0.9087212564159395, 0.9086091307743455, 0.9087348843635776, 0.9092613959272198, 0.9094152563344977, 0.9089003973677147, 0.9088370242976098, 0.9086466552011822, 0.9086886563477307, 0.9088331240462909, 0.9086298116416618], "moving_var_accuracy_train": [0.020313291786851206, 0.04436360461170934, 0.0686874376425849, 0.0900320432879969, 0.10935006552325867, 0.12248719248836548, 0.13311245793064036, 0.13922954802517248, 0.14264620250313456, 0.1430696425703168, 0.14137255355838724, 0.13761259882552826, 0.13309080338829893, 0.1271029275308728, 0.12081986151695082, 0.11429181411191286, 0.1073878754265552, 0.10047261759322247, 0.0937195138953476, 0.08726190953214978, 0.0807412131930581, 0.07466281198328902, 0.0688964160094792, 0.0634269729368005, 0.05814224110963039, 0.05340452711285204, 0.04902229104267443, 0.04488095405466676, 0.04108683825935087, 0.03758672107310603, 0.034306057470062275, 0.03126291645519852, 0.028485143401468266, 0.025928979487059904, 0.023561511179140383, 0.021389679037275726, 0.019465039202498396, 0.017663314498186367, 0.016028579143334307, 0.014494469198441001, 0.01316367719629831, 0.01190108247117339, 0.010780113458902727, 0.009784014737676577, 0.008854147892617714, 0.008027634396428958, 0.007256882045089447, 0.006560688236750035, 0.0059413375788850645, 0.00537542562709916, 0.004872071179007831, 0.004408346650112389, 0.003983977449477525, 0.003603004838016352, 0.003257680087279147, 0.00295325033342718, 0.002670380313528793, 0.0024161992488852552, 0.0021840152855100007, 0.0019766512214754446, 0.0017903988512637298, 0.0016170087120426413, 0.0014636860960798627, 0.0013220152062156745, 0.0011899207131903895, 0.0010790898829357081, 0.0009712463362714945, 0.0008768878544927374, 0.0007970896885008545, 0.0007179819760247752, 0.0006484203392507585, 0.0005852777024517407, 0.0005289308047407085, 0.00048074271898304995, 0.000432983843605058, 0.00038969919655583684, 0.0003513315675741042, 0.00032080168971277475, 0.0002917504591308378, 0.0002639139102668314, 0.0002416699941141826, 0.00022145583704894983, 0.00020411385049779422, 0.00018404939881444486, 0.0001657540643064774, 0.00015053172968795283, 0.0001361182938848121, 0.00012353937638527165, 0.0001122246705448993, 0.00010131335509109852, 9.12134629569128e-05, 8.214767284773438e-05, 7.395273243822444e-05, 7.060332201290591e-05, 6.379741660708338e-05, 5.789317177154647e-05, 5.3829166126682725e-05, 4.844774183883086e-05, 4.380658351972149e-05, 4.337340607858335e-05, 4.1353901432413586e-05, 3.935585111837223e-05, 3.63069712733743e-05, 3.430974600574578e-05, 3.262032119286715e-05, 3.003175762320769e-05, 2.8235473425893296e-05, 2.5412751355419203e-05, 2.3449034332238385e-05, 2.2410119829608744e-05, 2.037133373193137e-05, 1.920681333443981e-05, 1.7442962193238852e-05, 1.6143560793479814e-05, 1.4562999410290223e-05, 1.3714425192143443e-05, 1.2372635210347954e-05, 1.1478998918477604e-05, 1.0333725178554976e-05, 1.0326886815536877e-05, 9.294594243862631e-06, 8.374165314738823e-06, 7.567601861868404e-06, 7.809965185625567e-06, 7.392740380421195e-06, 6.918265217461013e-06, 6.402188659401064e-06, 5.783009310890513e-06, 5.251078535425574e-06, 6.0914783008269656e-06, 5.680031292123777e-06, 5.158962193293802e-06, 4.656431615011491e-06, 4.2291314135855685e-06, 3.906966553221074e-06, 4.445072274613916e-06, 5.195764756410856e-06, 4.735115957986399e-06, 4.505412513736593e-06, 5.835577708782569e-06, 7.302509783310984e-06, 7.15803443649852e-06, 8.173776582395086e-06, 7.372141725968636e-06, 6.644690108487427e-06, 7.393281043374992e-06, 7.904739092751115e-06, 7.444464256874279e-06, 6.928623793797648e-06, 6.236198351429451e-06, 5.633182307219219e-06, 5.790981255005437e-06, 5.800934987071368e-06, 5.325615798844746e-06, 5.443680135902188e-06, 4.963326838367805e-06, 4.891107143051816e-06, 5.393071203624506e-06, 5.189375132601633e-06, 4.728501951717164e-06, 5.0558105135668085e-06, 5.858708922338682e-06, 5.329916423443475e-06, 5.3830803885916565e-06, 5.64356691966643e-06, 5.180318435296393e-06, 4.711596188461946e-06, 4.440165745166423e-06, 4.022464379587363e-06, 3.7955345098847454e-06, 3.650565401586538e-06, 3.3148024344083365e-06, 2.997422368917292e-06, 3.129931138600626e-06, 2.8243228264883643e-06, 3.1226065774010383e-06, 3.46746375518831e-06, 1.0029144929358443e-05, 1.0785076155692622e-05, 9.710522089980023e-06, 9.299147951317054e-06, 8.616077304028459e-06, 7.849421931717372e-06, 7.147065173945647e-06, 6.6603292534190455e-06, 6.296484447270478e-06, 5.746205445408209e-06, 5.2684008637763325e-06, 4.835101929503964e-06, 4.3935100065952275e-06, 4.192102946057584e-06, 3.7771043801455406e-06, 3.851684754611318e-06, 3.5873893518643698e-06, 3.879929908239885e-06, 3.931224307889274e-06, 3.540908696035901e-06, 3.2103996765848173e-06, 3.150689562563843e-06, 4.066601203529435e-06, 4.5210468538967e-06, 4.374540095537643e-06, 4.440182528142786e-06, 4.073650583237221e-06, 4.159499534189199e-06, 3.8137766733767588e-06, 3.6046025443116714e-06, 3.285784798929792e-06, 2.9883026554616586e-06, 2.8067574678425495e-06, 2.6210826341792064e-06, 4.529258055560214e-06, 4.508848882135884e-06, 6.3643962645786925e-06, 6.003518502453212e-06, 5.4080796723669e-06, 5.034524893006192e-06, 4.711306604120497e-06, 4.528953321808599e-06, 4.309749722844774e-06, 4.132537275543222e-06, 3.7846796254216564e-06, 3.5536398135345537e-06, 3.575803298196694e-06, 3.27360897356865e-06, 2.9726206405461038e-06, 2.8839745003953942e-06, 2.6024738906288296e-06, 2.357640781872524e-06, 2.2827985066158517e-06, 2.2706012164463835e-06, 2.174506584688722e-06, 3.3946653010536347e-06, 3.5349788113971563e-06, 3.2303407017047657e-06, 2.9468830464066125e-06, 2.7639769840480124e-06, 2.50878171657732e-06, 2.2580061006378526e-06, 2.0322885607058614e-06, 2.0215973243051325e-06, 2.0918911115920145e-06, 1.8836881329788889e-06, 2.309280249935203e-06, 2.4546562275959234e-06, 2.666359543756379e-06, 2.4049159208045883e-06, 2.2148513939708987e-06, 2.341866433071352e-06, 2.2097858744655947e-06, 2.3342076665006533e-06], "duration": 218158.538556, "accuracy_train": [0.47508235294117646, 0.585835294117647, 0.6666352941176471, 0.717764705882353, 0.7748235294117647, 0.7871294117647059, 0.8258117647058824, 0.8367058823529412, 0.8574823529411765, 0.8664235294117647, 0.8771529411764706, 0.879835294117647, 0.8946352941176471, 0.8914823529411765, 0.9020235294117647, 0.9099294117647059, 0.9105882352941177, 0.9148941176470589, 0.9207058823529412, 0.9284705882352942, 0.9230588235294117, 0.9310823529411765, 0.9344941176470588, 0.9364235294117647, 0.9317882352941177, 0.9435764705882352, 0.9483294117647059, 0.9474117647058824, 0.9524705882352941, 0.9556705882352942, 0.9545411764705882, 0.954564705882353, 0.9577411764705882, 0.9587294117647058, 0.9574823529411765, 0.9576941176470588, 0.965764705882353, 0.9619529411764706, 0.9640941176470588, 0.9573176470588235, 0.9687529411764706, 0.9605176470588235, 0.9662352941176471, 0.9714588235294118, 0.9675294117647059, 0.9722117647058823, 0.9680470588235294, 0.9691764705882353, 0.9730823529411765, 0.9726117647058824, 0.9761647058823529, 0.9747764705882352, 0.973764705882353, 0.9755058823529412, 0.9758823529411764, 0.9796705882352941, 0.9775764705882353, 0.9789411764705882, 0.9784235294117647, 0.9802823529411765, 0.9815764705882353, 0.9793647058823529, 0.9818823529411764, 0.9804235294117647, 0.9750117647058824, 0.9835529411764706, 0.975835294117647, 0.9806117647058824, 0.9662588235294117, 0.9772705882352941, 0.9799294117647059, 0.9797882352941176, 0.9808, 0.9836, 0.978964705882353, 0.9776705882352941, 0.9799058823529412, 0.9847294117647059, 0.9840941176470588, 0.9827294117647059, 0.9860470588235294, 0.9865647058823529, 0.9879058823529412, 0.9832941176470589, 0.9804235294117647, 0.9852941176470589, 0.9844705882352941, 0.9854588235294117, 0.9790117647058824, 0.9839294117647058, 0.9828470588235294, 0.9815294117647059, 0.9827058823529412, 0.9889882352941176, 0.984635294117647, 0.9808235294117648, 0.9872705882352941, 0.9834588235294117, 0.9848470588235294, 0.9768705882352942, 0.9879058823529412, 0.9882117647058823, 0.986964705882353, 0.9884, 0.988964705882353, 0.9877411764705882, 0.9889411764705882, 0.9857411764705882, 0.9881882352941177, 0.9897176470588235, 0.9877882352941176, 0.9895529411764706, 0.9880705882352941, 0.9891058823529412, 0.9877176470588235, 0.989764705882353, 0.988, 0.9855294117647059, 0.9874588235294117, 0.9906823529411765, 0.9875764705882353, 0.9879529411764706, 0.9870823529411765, 0.9909411764705882, 0.9899529411764706, 0.9898588235294118, 0.9869176470588236, 0.9886588235294118, 0.9875058823529411, 0.9920470588235294, 0.9900235294117647, 0.9894117647058823, 0.9883764705882353, 0.9880705882352941, 0.9876, 0.991764705882353, 0.9925176470588235, 0.9900470588235294, 0.990964705882353, 0.985035294117647, 0.9938117647058824, 0.986964705882353, 0.9936470588235294, 0.9901176470588235, 0.9900705882352941, 0.9858117647058824, 0.9931058823529412, 0.987835294117647, 0.9911529411764706, 0.9897882352941176, 0.9892470588235294, 0.9868470588235294, 0.9919529411764706, 0.9907294117647059, 0.9924470588235295, 0.9908705882352942, 0.9922823529411765, 0.9936470588235294, 0.9887294117647059, 0.9912705882352941, 0.9935294117647059, 0.9946588235294118, 0.9920235294117647, 0.9938588235294118, 0.9945411764705883, 0.9908, 0.9924941176470589, 0.9933176470588235, 0.9925176470588235, 0.9906352941176471, 0.9935058823529411, 0.9914823529411765, 0.9916, 0.989764705882353, 0.9920235294117647, 0.9943058823529412, 0.9893176470588235, 0.9829882352941176, 0.9952941176470588, 0.9911058823529412, 0.9937882352941176, 0.9932, 0.9906823529411765, 0.9925647058823529, 0.9932941176470588, 0.9936941176470588, 0.9911058823529412, 0.9929882352941176, 0.991035294117647, 0.9926352941176471, 0.9936470588235294, 0.9924, 0.9944470588235295, 0.9912705882352941, 0.9896235294117647, 0.989835294117647, 0.9916470588235294, 0.9912941176470588, 0.9934588235294117, 0.9956235294117647, 0.9953882352941177, 0.9944470588235295, 0.9951529411764706, 0.9939529411764706, 0.9954588235294117, 0.9942352941176471, 0.9948235294117647, 0.9942588235294118, 0.9930588235294118, 0.9947294117647059, 0.9947294117647059, 0.9888941176470588, 0.9955058823529411, 0.9884705882352941, 0.9947764705882353, 0.9934352941176471, 0.9945882352941177, 0.9947764705882353, 0.9952941176470588, 0.9920705882352941, 0.9952, 0.9945411764705883, 0.9924941176470589, 0.9956941176470588, 0.9946352941176471, 0.9944705882352941, 0.9955058823529411, 0.9938588235294118, 0.9936941176470588, 0.9927294117647059, 0.9954823529411765, 0.9952941176470588, 0.9902117647058823, 0.9961176470588236, 0.9947764705882353, 0.9947764705882353, 0.9952941176470588, 0.9947764705882353, 0.9943058823529412, 0.9943058823529412, 0.9928705882352942, 0.9924470588235295, 0.9941176470588236, 0.9966352941176471, 0.9963294117647059, 0.9922352941176471, 0.9940235294117648, 0.9949882352941176, 0.9962823529411765, 0.9955764705882353, 0.9926588235294118], "end": "2016-02-06 13:24:48.871000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0, 211.0, 212.0, 213.0, 214.0, 215.0, 216.0, 217.0, 218.0, 219.0, 220.0, 221.0, 222.0, 223.0, 224.0, 225.0, 226.0, 227.0, 228.0, 229.0, 230.0, 231.0, 232.0, 233.0, 234.0, 235.0, 236.0, 237.0, 238.0, 239.0, 240.0, 241.0, 242.0, 243.0, 244.0, 245.0, 246.0, 247.0, 248.0, 249.0, 250.0], "moving_var_accuracy_valid": [0.019476993599999996, 0.042368315055999986, 0.06537087345135999, 0.08601235511960159, 0.1046933439260773, 0.11635830306765862, 0.12522252296700348, 0.1304479849102669, 0.1329229872446052, 0.13276399597977992, 0.13108701110484508, 0.12695218162707325, 0.12222385923896693, 0.11628230877273583, 0.11006611515972566, 0.10348573056054913, 0.09666514987789639, 0.09006970259488217, 0.08359438121044502, 0.07737714656629337, 0.07121834581530785, 0.06548049964203015, 0.06015198145872556, 0.055165243563466096, 0.050368173981296185, 0.04604000167825265, 0.042103621832842685, 0.03838852308100417, 0.0350105914548798, 0.031906424650231795, 0.029005120289242656, 0.02636539675587621, 0.02392749939551074, 0.021714434023299697, 0.01967350256690708, 0.017800665010459273, 0.016140491983062565, 0.014603436899670448, 0.013206091816075247, 0.011923717682534873, 0.010813885039426123, 0.009759293662143948, 0.00882508213346618, 0.007983701018006733, 0.007223916594153117, 0.0065369341395234174, 0.005909001497493475, 0.005330547823783791, 0.0048155994341508, 0.004365283618017598, 0.00395406299931416, 0.0035646475046892073, 0.0032133941626701704, 0.002905704563245473, 0.0026225603274419407, 0.0023703696749307343, 0.0021421853466680255, 0.0019397853128551934, 0.0017528506259737052, 0.0015813626901846833, 0.0014278651015564546, 0.0012861156966286656, 0.001167467905016327, 0.0010529141519111844, 0.0009478410109997603, 0.000857135377000321, 0.0007715357923680895, 0.0006947713550632846, 0.0006338122635397794, 0.0005705021724457908, 0.000513914299785901, 0.00046724029450116326, 0.00042224487730035807, 0.0003815168287158543, 0.000343397892823839, 0.0003095244083393085, 0.00027985923616160024, 0.00025490481405493227, 0.00023580010554966993, 0.0002146359677865489, 0.0001940918161777517, 0.00018265190332999808, 0.00016714460559842493, 0.0001505169467838357, 0.0001380729628398249, 0.00013015426426560677, 0.00011717268610074377, 0.00011224447022241074, 0.00010150301627747016, 9.160822400306819e-05, 8.24480300376049e-05, 7.422211630847745e-05, 6.716435164458892e-05, 6.435516058082399e-05, 5.792008520736707e-05, 6.274846184133937e-05, 5.686673974569253e-05, 5.25356520527735e-05, 4.818864442570584e-05, 4.901952798149509e-05, 4.418557728726602e-05, 4.21840958442131e-05, 3.904567574481974e-05, 3.681891772894128e-05, 3.489469892880834e-05, 3.19725322488904e-05, 2.9349654754769723e-05, 2.8482279562013665e-05, 2.7901724371225508e-05, 2.7656506774610704e-05, 2.489087405513667e-05, 2.2478065401586755e-05, 2.0232617660950986e-05, 1.885530682679115e-05, 1.710179184818981e-05, 1.9581844952069976e-05, 1.8082930807731094e-05, 1.899689152719786e-05, 1.7110442822775444e-05, 1.7660092513824923e-05, 1.672802551852189e-05, 1.5861835405766014e-05, 1.4840698072725208e-05, 1.3483739053827249e-05, 1.2149975527180108e-05, 1.1056629689873529e-05, 1.0187252684462583e-05, 1.034036983981387e-05, 1.0712300257150342e-05, 1.058748504416715e-05, 9.528756300617074e-06, 9.032855605626794e-06, 1.0113158608227812e-05, 9.146973772084118e-06, 1.1025174384530516e-05, 1.058838742379238e-05, 1.479350238601582e-05, 1.6086036490222348e-05, 1.502267910883137e-05, 1.51985575059448e-05, 2.082956830121544e-05, 1.8754128253458693e-05, 1.7394281706740657e-05, 1.7380854352715462e-05, 1.723641296333222e-05, 1.787254857398036e-05, 1.8966661168641193e-05, 1.7093186319764627e-05, 1.7081525241739992e-05, 1.6033768995218503e-05, 1.5255977082296693e-05, 1.4019560208073565e-05, 1.261890163993548e-05, 1.1394036221653753e-05, 1.0511502823899803e-05, 9.510465260936701e-06, 1.0413027011858704e-05, 9.669104266556127e-06, 9.0038221738786e-06, 8.446500489935433e-06, 8.616186106201051e-06, 9.358407611829981e-06, 8.490278548946946e-06, 8.652897924090738e-06, 7.838334918898088e-06, 7.079243624663594e-06, 6.982166042314013e-06, 8.897312059925036e-06, 1.0242399044124067e-05, 1.0091565742358362e-05, 9.082410391417578e-06, 8.366894370001802e-06, 7.611370596367669e-06, 7.182477751872568e-06, 6.504725234624357e-06, 6.460920937683383e-06, 1.6711431084650178e-05, 2.0852727601743988e-05, 1.974793077620444e-05, 1.8074351120207836e-05, 1.8416229684354535e-05, 2.233195933551381e-05, 2.1354275675976908e-05, 1.934243776306794e-05, 1.824797906652816e-05, 1.6434155418259956e-05, 1.5272767767895537e-05, 1.3748532066176422e-05, 1.3741716352204715e-05, 1.2389251372111389e-05, 1.147819539996096e-05, 1.0357489188351495e-05, 1.179231082483864e-05, 1.0706334008769623e-05, 1.1694460848417862e-05, 1.0807352100854293e-05, 1.0569742698686188e-05, 1.0195700333230615e-05, 1.1070174414889863e-05, 1.0666437924429547e-05, 1.6332556100242642e-05, 1.5988163363985873e-05, 1.4567222511104985e-05, 1.3200753888525173e-05, 1.2413023588498343e-05, 1.1192268203801921e-05, 1.0181287609232027e-05, 9.980181136786598e-06, 9.102222801127782e-06, 8.819021859245875e-06, 9.59435851465766e-06, 1.4992359945670801e-05, 1.691837615853801e-05, 1.745956271989699e-05, 1.7716121219819672e-05, 1.6594802717931375e-05, 1.5631267036394014e-05, 1.6350686106283737e-05, 1.4729952451883826e-05, 1.405305690164604e-05, 1.264776033049943e-05, 1.1409461377954612e-05, 1.0362424400058813e-05, 9.635097170959205e-06, 1.1698417476707113e-05, 1.0930784284156269e-05, 1.0897001248693348e-05, 1.0394514810418175e-05, 9.438971356698264e-06, 1.4348077318478445e-05, 1.3059676640484668e-05, 1.288658943424077e-05, 1.7177297073279857e-05, 1.6885534227269757e-05, 1.5352788384840414e-05, 1.587745848228494e-05, 1.4319202795996936e-05, 1.3456634705718443e-05, 1.3850993060320161e-05, 1.2511182283776406e-05, 1.2165473237249138e-05, 1.1062075349049916e-05, 1.0098193500987777e-05, 1.1583303990730023e-05, 1.063803081600688e-05, 1.1959945535498919e-05, 1.0800096296079595e-05, 1.0046250202343734e-05, 9.05750204891182e-06, 8.339590087366348e-06, 7.877654483514257e-06], "accuracy_test": 0.1895, "start": "2016-02-04 00:48:50.332000", "learning_rate_per_epoch": [0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166, 0.0034929357934743166], "accuracy_train_first": 0.47508235294117646, "accuracy_train_last": 0.9926588235294118, "batch_size_eval": 1024, "accuracy_train_std": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "accuracy_test_std": 0, "error_valid": [0.5347999999999999, 0.42813333333333337, 0.3508, 0.2964, 0.24039999999999995, 0.24, 0.20906666666666662, 0.19453333333333334, 0.1789333333333334, 0.17066666666666663, 0.15546666666666664, 0.16279999999999994, 0.14946666666666664, 0.15306666666666668, 0.14559999999999995, 0.1445333333333333, 0.14613333333333334, 0.13959999999999995, 0.13813333333333333, 0.13480000000000003, 0.1412, 0.13639999999999997, 0.13160000000000005, 0.12946666666666662, 0.13626666666666665, 0.128, 0.12173333333333336, 0.12506666666666666, 0.12026666666666663, 0.11826666666666663, 0.1213333333333333, 0.11853333333333338, 0.12, 0.11760000000000004, 0.11973333333333336, 0.12160000000000004, 0.11426666666666663, 0.11786666666666668, 0.11773333333333336, 0.12093333333333334, 0.10919999999999996, 0.11919999999999997, 0.11319999999999997, 0.11119999999999997, 0.10973333333333335, 0.10853333333333337, 0.10946666666666671, 0.11293333333333333, 0.10933333333333328, 0.1034666666666667, 0.1034666666666667, 0.11040000000000005, 0.1101333333333333, 0.10466666666666669, 0.10666666666666669, 0.10426666666666662, 0.10386666666666666, 0.10133333333333339, 0.1028, 0.10426666666666662, 0.10293333333333332, 0.10599999999999998, 0.09853333333333336, 0.10306666666666664, 0.10906666666666665, 0.10093333333333332, 0.10586666666666666, 0.1048, 0.11639999999999995, 0.10853333333333337, 0.10546666666666671, 0.10026666666666662, 0.10240000000000005, 0.10226666666666662, 0.10533333333333328, 0.10360000000000003, 0.10186666666666666, 0.0994666666666667, 0.09626666666666661, 0.09866666666666668, 0.1001333333333333, 0.09360000000000002, 0.09653333333333336, 0.10053333333333336, 0.1068, 0.09386666666666665, 0.10053333333333336, 0.09240000000000004, 0.10253333333333337, 0.1021333333333333, 0.10053333333333336, 0.10106666666666664, 0.10266666666666668, 0.09426666666666672, 0.10026666666666662, 0.11106666666666665, 0.09919999999999995, 0.09719999999999995, 0.10386666666666666, 0.10893333333333333, 0.10093333333333332, 0.09653333333333336, 0.09773333333333334, 0.09653333333333336, 0.09599999999999997, 0.0974666666666667, 0.09719999999999995, 0.10426666666666662, 0.09493333333333331, 0.09413333333333329, 0.09893333333333332, 0.09799999999999998, 0.09866666666666668, 0.09613333333333329, 0.09733333333333338, 0.09160000000000001, 0.09999999999999998, 0.1034666666666667, 0.0981333333333333, 0.0934666666666667, 0.09493333333333331, 0.10066666666666668, 0.0954666666666667, 0.09653333333333336, 0.09719999999999995, 0.09640000000000004, 0.09906666666666664, 0.09399999999999997, 0.10119999999999996, 0.09440000000000004, 0.09733333333333338, 0.09506666666666663, 0.09240000000000004, 0.09733333333333338, 0.10226666666666662, 0.09453333333333336, 0.08933333333333338, 0.09066666666666667, 0.09319999999999995, 0.09973333333333334, 0.0869333333333333, 0.09466666666666668, 0.09253333333333336, 0.09906666666666664, 0.09933333333333338, 0.10066666666666668, 0.09040000000000004, 0.09599999999999997, 0.09119999999999995, 0.09240000000000004, 0.09786666666666666, 0.09693333333333332, 0.09519999999999995, 0.09466666666666668, 0.09693333333333332, 0.09466666666666668, 0.09079999999999999, 0.09306666666666663, 0.09653333333333336, 0.09293333333333331, 0.09133333333333338, 0.09013333333333329, 0.0948, 0.09066666666666667, 0.09293333333333331, 0.09413333333333329, 0.09626666666666661, 0.08853333333333335, 0.08840000000000003, 0.09599999999999997, 0.09319999999999995, 0.09173333333333333, 0.09399999999999997, 0.09506666666666663, 0.09266666666666667, 0.09586666666666666, 0.10453333333333337, 0.10266666666666668, 0.09213333333333329, 0.09693333333333332, 0.09040000000000004, 0.08679999999999999, 0.09773333333333334, 0.09319999999999995, 0.09119999999999995, 0.09360000000000002, 0.09160000000000001, 0.09386666666666665, 0.09760000000000002, 0.09360000000000002, 0.09213333333333329, 0.09440000000000004, 0.08866666666666667, 0.09440000000000004, 0.09826666666666661, 0.09573333333333334, 0.09719999999999995, 0.09719999999999995, 0.09013333333333329, 0.0914666666666667, 0.08533333333333337, 0.08933333333333338, 0.09133333333333338, 0.09360000000000002, 0.09026666666666672, 0.09293333333333331, 0.09360000000000002, 0.08960000000000001, 0.0934666666666667, 0.09506666666666663, 0.08840000000000003, 0.10066666666666668, 0.0869333333333333, 0.0974666666666667, 0.08826666666666672, 0.09519999999999995, 0.08999999999999997, 0.08746666666666669, 0.09160000000000001, 0.09493333333333331, 0.09226666666666672, 0.0928, 0.09333333333333338, 0.09426666666666672, 0.08679999999999999, 0.09413333333333329, 0.08879999999999999, 0.08933333333333338, 0.09066666666666667, 0.09960000000000002, 0.09106666666666663, 0.08866666666666667, 0.09973333333333334, 0.08866666666666667, 0.09093333333333331, 0.08733333333333337, 0.09106666666666663, 0.08906666666666663, 0.0869333333333333, 0.09160000000000001, 0.09413333333333329, 0.09240000000000004, 0.09013333333333329, 0.08599999999999997, 0.08919999999999995, 0.09573333333333334, 0.09173333333333333, 0.09306666666666663, 0.09093333333333331, 0.08986666666666665, 0.09319999999999995], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.06182533152424281, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 32, "valid_ratio": 0.15, "learning_rate": 0.0034929359045589993, "optimization": "adam", "nb_data_augmentation": 2, "learning_rate_decay_method": "none", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 1.7374855326245723e-08, "rotation_range": [0, 0], "momentum": 0.6311668653991287}, "accuracy_valid_max": 0.9146666666666666, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        y_pred = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            y_pred.extend((nnet.predict(X[mini_batch]) == y[mini_batch]).tolist())\n        return np.mean(y_pred)\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            acc = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = acc\n            status[\"accuracy_train_std\"] = 0\n            acc = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = acc\n            status[\"accuracy_valid_std\"] = 0\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n\n    # rescaling to [-1, 1]\n    X_min = X_train.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X_train.max(axis=(0, 2, 3))[None, :, None, None]\n    def preprocess(a):\n        return (a / 255.) * 2 - 1\n        # return 2 * ((a - X_min) / (X_max - X_min)) - 1\n    X_train = preprocess(X_train)\n    X_valid = preprocess(X_valid)\n\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = preprocess(X_test)\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    acc = evaluate(X_test, y_test, batch_size_eval)\n    light.set(\"accuracy_test\", acc)\n    light.set(\"accuracy_test_std\", 0)\n    print(\"Test accuracy : {}+-{}\".format(acc, 0))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.9068, "accuracy_valid_std": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "accuracy_valid": [0.4652, 0.5718666666666666, 0.6492, 0.7036, 0.7596, 0.76, 0.7909333333333334, 0.8054666666666667, 0.8210666666666666, 0.8293333333333334, 0.8445333333333334, 0.8372, 0.8505333333333334, 0.8469333333333333, 0.8544, 0.8554666666666667, 0.8538666666666667, 0.8604, 0.8618666666666667, 0.8652, 0.8588, 0.8636, 0.8684, 0.8705333333333334, 0.8637333333333334, 0.872, 0.8782666666666666, 0.8749333333333333, 0.8797333333333334, 0.8817333333333334, 0.8786666666666667, 0.8814666666666666, 0.88, 0.8824, 0.8802666666666666, 0.8784, 0.8857333333333334, 0.8821333333333333, 0.8822666666666666, 0.8790666666666667, 0.8908, 0.8808, 0.8868, 0.8888, 0.8902666666666667, 0.8914666666666666, 0.8905333333333333, 0.8870666666666667, 0.8906666666666667, 0.8965333333333333, 0.8965333333333333, 0.8896, 0.8898666666666667, 0.8953333333333333, 0.8933333333333333, 0.8957333333333334, 0.8961333333333333, 0.8986666666666666, 0.8972, 0.8957333333333334, 0.8970666666666667, 0.894, 0.9014666666666666, 0.8969333333333334, 0.8909333333333334, 0.8990666666666667, 0.8941333333333333, 0.8952, 0.8836, 0.8914666666666666, 0.8945333333333333, 0.8997333333333334, 0.8976, 0.8977333333333334, 0.8946666666666667, 0.8964, 0.8981333333333333, 0.9005333333333333, 0.9037333333333334, 0.9013333333333333, 0.8998666666666667, 0.9064, 0.9034666666666666, 0.8994666666666666, 0.8932, 0.9061333333333333, 0.8994666666666666, 0.9076, 0.8974666666666666, 0.8978666666666667, 0.8994666666666666, 0.8989333333333334, 0.8973333333333333, 0.9057333333333333, 0.8997333333333334, 0.8889333333333334, 0.9008, 0.9028, 0.8961333333333333, 0.8910666666666667, 0.8990666666666667, 0.9034666666666666, 0.9022666666666667, 0.9034666666666666, 0.904, 0.9025333333333333, 0.9028, 0.8957333333333334, 0.9050666666666667, 0.9058666666666667, 0.9010666666666667, 0.902, 0.9013333333333333, 0.9038666666666667, 0.9026666666666666, 0.9084, 0.9, 0.8965333333333333, 0.9018666666666667, 0.9065333333333333, 0.9050666666666667, 0.8993333333333333, 0.9045333333333333, 0.9034666666666666, 0.9028, 0.9036, 0.9009333333333334, 0.906, 0.8988, 0.9056, 0.9026666666666666, 0.9049333333333334, 0.9076, 0.9026666666666666, 0.8977333333333334, 0.9054666666666666, 0.9106666666666666, 0.9093333333333333, 0.9068, 0.9002666666666667, 0.9130666666666667, 0.9053333333333333, 0.9074666666666666, 0.9009333333333334, 0.9006666666666666, 0.8993333333333333, 0.9096, 0.904, 0.9088, 0.9076, 0.9021333333333333, 0.9030666666666667, 0.9048, 0.9053333333333333, 0.9030666666666667, 0.9053333333333333, 0.9092, 0.9069333333333334, 0.9034666666666666, 0.9070666666666667, 0.9086666666666666, 0.9098666666666667, 0.9052, 0.9093333333333333, 0.9070666666666667, 0.9058666666666667, 0.9037333333333334, 0.9114666666666666, 0.9116, 0.904, 0.9068, 0.9082666666666667, 0.906, 0.9049333333333334, 0.9073333333333333, 0.9041333333333333, 0.8954666666666666, 0.8973333333333333, 0.9078666666666667, 0.9030666666666667, 0.9096, 0.9132, 0.9022666666666667, 0.9068, 0.9088, 0.9064, 0.9084, 0.9061333333333333, 0.9024, 0.9064, 0.9078666666666667, 0.9056, 0.9113333333333333, 0.9056, 0.9017333333333334, 0.9042666666666667, 0.9028, 0.9028, 0.9098666666666667, 0.9085333333333333, 0.9146666666666666, 0.9106666666666666, 0.9086666666666666, 0.9064, 0.9097333333333333, 0.9070666666666667, 0.9064, 0.9104, 0.9065333333333333, 0.9049333333333334, 0.9116, 0.8993333333333333, 0.9130666666666667, 0.9025333333333333, 0.9117333333333333, 0.9048, 0.91, 0.9125333333333333, 0.9084, 0.9050666666666667, 0.9077333333333333, 0.9072, 0.9066666666666666, 0.9057333333333333, 0.9132, 0.9058666666666667, 0.9112, 0.9106666666666666, 0.9093333333333333, 0.9004, 0.9089333333333334, 0.9113333333333333, 0.9002666666666667, 0.9113333333333333, 0.9090666666666667, 0.9126666666666666, 0.9089333333333334, 0.9109333333333334, 0.9130666666666667, 0.9084, 0.9058666666666667, 0.9076, 0.9098666666666667, 0.914, 0.9108, 0.9042666666666667, 0.9082666666666667, 0.9069333333333334, 0.9090666666666667, 0.9101333333333333, 0.9068], "seed": 564939267, "model": "residualv3", "loss_std": [0.30658942461013794, 0.20192059874534607, 0.19597308337688446, 0.19033728539943695, 0.1835012584924698, 0.17943507432937622, 0.17754590511322021, 0.16983428597450256, 0.16336672008037567, 0.1596037894487381, 0.15436400473117828, 0.14992105960845947, 0.14689402282238007, 0.1442544311285019, 0.14211368560791016, 0.13742604851722717, 0.13204820454120636, 0.12816396355628967, 0.12400975823402405, 0.1239306852221489, 0.11626283079385757, 0.11455540359020233, 0.11262847483158112, 0.1088089719414711, 0.10853030532598495, 0.10438515990972519, 0.1009301170706749, 0.1009686067700386, 0.09472964704036713, 0.0961349681019783, 0.09133251756429672, 0.08930797874927521, 0.08652738481760025, 0.08498086035251617, 0.08560679107904434, 0.08172613382339478, 0.08130767941474915, 0.0790187269449234, 0.07696778327226639, 0.07753577083349228, 0.07532784342765808, 0.07614460587501526, 0.07545687258243561, 0.07129969447851181, 0.07319311052560806, 0.06850596517324448, 0.07372689992189407, 0.06620769202709198, 0.06861376762390137, 0.06379877775907516, 0.06390488147735596, 0.06627199798822403, 0.06516913324594498, 0.06261895596981049, 0.05765400826931, 0.06120512634515762, 0.05883238837122917, 0.060018375515937805, 0.061002932488918304, 0.057336948812007904, 0.0580468513071537, 0.05874929204583168, 0.0567035973072052, 0.05634381249547005, 0.05868132412433624, 0.057303015142679214, 0.05420650541782379, 0.05417288839817047, 0.05186678469181061, 0.05170987546443939, 0.053285449743270874, 0.05029205605387688, 0.05036501586437225, 0.054359305649995804, 0.048578765243291855, 0.051816970109939575, 0.04743872210383415, 0.04999058321118355, 0.045484524220228195, 0.04755575582385063, 0.047259457409381866, 0.044576793909072876, 0.0505816824734211, 0.04596970975399017, 0.04601577669382095, 0.0472930371761322, 0.045867957174777985, 0.04601827636361122, 0.04392930120229721, 0.04660489410161972, 0.04755787551403046, 0.043087173253297806, 0.044817376881837845, 0.04362291470170021, 0.04548477753996849, 0.043225787580013275, 0.04633817449212074, 0.04469475522637367, 0.041346244513988495, 0.04641485586762428, 0.040985357016325, 0.039744917303323746, 0.04138125106692314, 0.04205084592103958, 0.04547601938247681, 0.03677792474627495, 0.04565152898430824, 0.041479069739580154, 0.03821781650185585, 0.04163602739572525, 0.040603939443826675, 0.042869292199611664, 0.039698608219623566, 0.037957753986120224, 0.039871301501989365, 0.038723789155483246, 0.04123721644282341, 0.03704140707850456, 0.03745462745428085, 0.04137837514281273, 0.03775078430771828, 0.039453860372304916, 0.03470857813954353, 0.03841597959399223, 0.03817961737513542, 0.03718142956495285, 0.03556545823812485, 0.039491862058639526, 0.041424479335546494, 0.0348658487200737, 0.03857903182506561, 0.03650163486599922, 0.03599345311522484, 0.034011248499155045, 0.041653234511613846, 0.04037487879395485, 0.036595091223716736, 0.04062182828783989, 0.03584420680999756, 0.03739297017455101, 0.03397948294878006, 0.036461811512708664, 0.03754136711359024, 0.03442542254924774, 0.034291695803403854, 0.03639549762010574, 0.036521416157484055, 0.03717019408941269, 0.03610631823539734, 0.03218210116028786, 0.03477044403553009, 0.035072941333055496, 0.03570220619440079, 0.03581862896680832, 0.03398197889328003, 0.031139042228460312, 0.03492341563105583, 0.0396510548889637, 0.03541248291730881, 0.03215008229017258, 0.03224426507949829, 0.03270727023482323, 0.0348912812769413, 0.03299654647707939, 0.035230085253715515, 0.032723281532526016, 0.03331589326262474, 0.03075031377375126, 0.036542732268571854, 0.033556018024683, 0.03113570064306259, 0.03532477095723152, 0.0349954329431057, 0.030193788930773735, 0.03245753049850464, 0.03402426838874817, 0.03277937322854996, 0.035210397094488144, 0.03379420191049576, 0.03206774219870567, 0.03144437447190285, 0.03138691559433937, 0.030016863718628883, 0.030953500419855118, 0.03353444114327431, 0.029853573068976402, 0.03137718141078949, 0.031963102519512177, 0.031810056418180466, 0.03301633894443512, 0.0310429185628891, 0.028339417651295662, 0.03319291025400162, 0.03172935172915459, 0.03180970996618271, 0.028853431344032288, 0.03178710490465164, 0.03213131055235863, 0.025865504518151283, 0.0330825038254261, 0.028330184519290924, 0.03277454525232315, 0.03090559132397175, 0.029071712866425514, 0.03044271469116211, 0.03234897181391716, 0.029300488531589508, 0.03244262933731079, 0.031132323667407036, 0.02980129048228264, 0.03128755837678909, 0.030454307794570923, 0.030894767493009567, 0.037263527512550354, 0.03056599758565426, 0.025373943150043488, 0.03176425024867058, 0.029533574357628822, 0.03208460658788681, 0.03129365295171738, 0.03168891742825508, 0.0284254290163517, 0.03037836216390133, 0.027109824120998383, 0.0298222117125988, 0.03175651654601097, 0.030997416004538536, 0.03014550358057022, 0.026977844536304474, 0.04053688049316406, 0.0302263256162405, 0.03268574923276901, 0.0281429011374712, 0.0304641705006361, 0.03153113275766373, 0.029158003628253937, 0.02417602762579918, 0.034709345549345016, 0.028091924265027046, 0.031376611441373825, 0.025401391088962555, 0.028876185417175293, 0.029949601739645004, 0.026125475764274597, 0.02947116270661354, 0.03136216476559639, 0.03392873331904411, 0.0316147543489933, 0.02688632160425186, 0.02817416377365589, 0.030087901279330254]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:42 2016", "state": "available"}], "summary": "3b33a83fa570f62f186f2a7c813e714b"}