{"content": {"hp_model": {"f0": 64, "f1": 16, "f2": 16, "f3": 32, "nonlin": "leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.7305845022201538, 1.4041026830673218, 1.2597708702087402, 1.1565496921539307, 1.0790791511535645, 1.0229045152664185, 0.9769145846366882, 0.9355558753013611, 0.9018141031265259, 0.8746265769004822, 0.849561870098114, 0.8269627094268799, 0.8101649880409241, 0.7936119437217712, 0.7777838110923767, 0.7653684616088867, 0.7537834644317627, 0.7432442307472229, 0.732720673084259, 0.724262535572052, 0.715521514415741, 0.7094957828521729, 0.7020513415336609, 0.6969126462936401, 0.6890992522239685, 0.6855142712593079, 0.6827191114425659, 0.677286684513092, 0.6724620461463928, 0.669419527053833, 0.6664013862609863, 0.6632875800132751, 0.6621870398521423, 0.6578692197799683, 0.6565283536911011, 0.6538711786270142, 0.6521787047386169, 0.650497555732727, 0.6497154235839844, 0.6483375430107117, 0.6467961668968201, 0.644102156162262, 0.6430911421775818, 0.641800045967102, 0.6415146589279175, 0.6414116621017456, 0.6405342221260071, 0.6398788094520569, 0.6393603086471558, 0.6399456858634949, 0.6398013830184937, 0.6359285712242126, 0.6365635395050049, 0.6364787220954895, 0.6345155835151672, 0.6349425315856934, 0.6350571513175964, 0.634659469127655, 0.6332523822784424, 0.634725034236908, 0.633045494556427, 0.6320667862892151, 0.6328749656677246, 0.6334361433982849, 0.6324349641799927, 0.6325695514678955, 0.6315168142318726, 0.6332952976226807, 0.6329762935638428, 0.6300565600395203, 0.6311450004577637, 0.6324383616447449, 0.6320706009864807, 0.6301739811897278, 0.6320529580116272, 0.631476640701294, 0.6318342685699463, 0.631586492061615, 0.632224440574646, 0.6311724185943604, 0.6301170587539673, 0.630318284034729, 0.6289029121398926, 0.6309621930122375, 0.6314255595207214, 0.6304489374160767, 0.631397545337677, 0.6296702027320862, 0.6305371522903442, 0.6317578554153442, 0.6284291744232178, 0.6307748556137085, 0.6302928924560547, 0.629736065864563, 0.6290794014930725, 0.6297863125801086, 0.6303278207778931, 0.6299352049827576, 0.6298882961273193, 0.630254328250885, 0.6310874223709106, 0.6308120489120483, 0.6284974813461304, 0.631315290927887, 0.6295444369316101, 0.6294413208961487, 0.6312096118927002, 0.6321455240249634, 0.6319631338119507, 0.6299682855606079, 0.6292019486427307, 0.6297633051872253, 0.6307486295700073, 0.6311399936676025, 0.6307008266448975, 0.6296738386154175, 0.6304954886436462, 0.6303228735923767, 0.6299488544464111, 0.63096022605896, 0.6308205127716064, 0.6318206787109375, 0.6304187774658203, 0.6302670836448669, 0.6319100260734558, 0.6290448904037476, 0.6301360726356506, 0.6311091184616089, 0.6309335231781006], "moving_avg_accuracy_train": [0.0491947775355297, 0.10199964239571795, 0.15574017092907894, 0.20758856596357045, 0.256620655057505, 0.302860734312275, 0.3458319708605141, 0.38593826727413966, 0.4229220687940033, 0.457135296634518, 0.48883382948260184, 0.51797148174106, 0.5450464813844144, 0.5699578495919493, 0.5927596577252239, 0.6135926747415534, 0.6326887651312316, 0.6503100132605042, 0.666513258600659, 0.6815495473758368, 0.6953263839473155, 0.7078558893902547, 0.7193974752043669, 0.7298731138965548, 0.7395685447838005, 0.7483969193740343, 0.7566746643944566, 0.7640549885950072, 0.7710344630017025, 0.7775623836439003, 0.7833306995719151, 0.7887127379630533, 0.793623965781735, 0.7980045793376055, 0.8021726709724126, 0.8060098397544165, 0.8094937348856562, 0.8126290963084971, 0.8155324459926622, 0.8182894757393262, 0.8207406116256188, 0.8230675777101961, 0.8251966883696398, 0.8272153026572144, 0.8290552709553081, 0.830753059053345, 0.8322856825903784, 0.8337115467498991, 0.8349972577887335, 0.8362402119367243, 0.8373636291139913, 0.8383538503318835, 0.8392403270327298, 0.8400636606027587, 0.8407745420277172, 0.8415142085625331, 0.8421497896557999, 0.84274030568376, 0.8432950215970193, 0.8437919407701431, 0.8442299034795352, 0.8446147332739312, 0.8450191367114883, 0.8453366328779179, 0.845582815849124, 0.8458742070851325, 0.8461154968094358, 0.8463698599422611, 0.8465964976618131, 0.8467748949725051, 0.8469982315699851, 0.8472061739053269, 0.8473167002428765, 0.8474672551228433, 0.8475864424243278, 0.8477285161301693, 0.8478657551583022, 0.847977680588393, 0.8480969786171321, 0.8481694696108544, 0.848239289705186, 0.8482858877972366, 0.8483580530146059, 0.8484114120150094, 0.8484431590737059, 0.8484786708241426, 0.8485177149924206, 0.8485132551164712, 0.8485626836019171, 0.8486118195364375, 0.8486397297870203, 0.8486602708125635, 0.8487112737700668, 0.8487711633734958, 0.848799487379677, 0.8488133532411926, 0.8487863049867946, 0.8488038502852266, 0.8487708129288154, 0.848734067812798, 0.8487497532357449, 0.8486779477569009, 0.848634249165227, 0.8486204610208066, 0.8486452540717806, 0.8486512557271717, 0.8486449954241574, 0.8486114954145489, 0.8486279565285482, 0.8486473857799477, 0.8486856902990367, 0.8486666859435977, 0.8487170473879977, 0.8486972685212909, 0.8486748532924545, 0.8487104831579303, 0.8487030225070966, 0.8487032112701374, 0.848768593469997, 0.8487529605903283, 0.8488040312141121, 0.8488150814945558, 0.8488645182278982, 0.8488439792188773, 0.8488579740964544, 0.8488823033767775, 0.8489226567242695, 0.8488985929656022, 0.8488978979709061], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.04812908862010541, 0.0990017148672816, 0.1507727889095444, 0.20131698027461403, 0.24906814181040565, 0.2947706202686723, 0.33675028358066045, 0.3761494857384077, 0.4119240914755609, 0.4448780725764988, 0.4751459776211832, 0.5028581659095167, 0.5288021709488361, 0.5524437147255639, 0.5741015811106882, 0.5941195927097098, 0.6122111043536484, 0.628833202690874, 0.6439904627116961, 0.657874078338117, 0.670578910950465, 0.6824690384924667, 0.6933023716066988, 0.7030585484614656, 0.7117871909797768, 0.7200060916577781, 0.7273685401915485, 0.7340944591392611, 0.7404153113710428, 0.7460705458118753, 0.7512589425672841, 0.7560638064995616, 0.7602040490612019, 0.7641133728354281, 0.767804721687051, 0.7711279651621712, 0.774339640360939, 0.77716911288358, 0.7798021168813666, 0.7821484359255342, 0.7843333652527851, 0.7863964283886511, 0.7882084751205691, 0.7899837425369761, 0.7916048677655827, 0.7929296031275787, 0.7941330424759653, 0.7952527589832633, 0.7962371192859912, 0.7972715869507656, 0.7980906855504933, 0.7988156672589982, 0.7995525705067429, 0.8002656410633728, 0.8007476836494302, 0.8013900710167914, 0.8019183620137569, 0.8024803026384354, 0.8029138365218057, 0.8034372648519293, 0.8038106940990406, 0.8040857452651907, 0.8043353503320452, 0.8046810356960545, 0.8050277441087533, 0.8052909535551822, 0.8055400490882182, 0.8057286434828602, 0.806057069844288, 0.8061807256234135, 0.8063795240606957, 0.8064719639268401, 0.8065795738688699, 0.8068117296691064, 0.80693316165325, 0.8069824447913889, 0.8070644502181236, 0.8071982607497751, 0.807417375986921, 0.8074304447229428, 0.8075022122329527, 0.8074834127905309, 0.8075763565736014, 0.8075867637908647, 0.8075218585902422, 0.8075366860971818, 0.8075001732197679, 0.8076260030363454, 0.8076426231299247, 0.8076819952766461, 0.8077174302086954, 0.8076862274739703, 0.8078433094987871, 0.8078636425172819, 0.807867676185358, 0.8078977795664457, 0.8079594346858554, 0.8079559481543933, 0.8079263371962582, 0.8080339646776866, 0.8080209661297222, 0.8080458885303041, 0.8079950765033279, 0.8079880257901186, 0.808004035193411, 0.8078719592813742, 0.8078395696879507, 0.8078093895452099, 0.8078687061441527, 0.807862085435611, 0.8078917183830138, 0.8078441163395166, 0.8078521616426884, 0.8078339588443834, 0.8078908185134089, 0.8079552287554415, 0.8080477600497016, 0.8080832395981953, 0.8081406147629993, 0.8080569455589133, 0.8080670924939859, 0.8080151895793012, 0.8080173050810849, 0.8079693513990307, 0.8078895719914319, 0.8079896984707525, 0.8080075996233007, 0.8080959233394345, 0.8080767289252953], "moving_var_accuracy_train": [0.021781135230932322, 0.04469820548396379, 0.06622078459897232, 0.08379301074794926, 0.09705102152139417, 0.10658922373464144, 0.11254904589553977, 0.11577077641414417, 0.11650391294647576, 0.11538842628523095, 0.1128927565191973, 0.10924450587949106, 0.10491755574273068, 0.10001098656220005, 0.09468918999330006, 0.08912640237599416, 0.08349570815193158, 0.07794071280743894, 0.0725095479626743, 0.06729340298758169, 0.062272273722078615, 0.05745794290967254, 0.05291102244664593, 0.04860757125606471, 0.04459282655126244, 0.040835005677285575, 0.03736819467316516, 0.03412159787359575, 0.031147855653179603, 0.028416593819059126, 0.02587439565496173, 0.02354765312465872, 0.02140996924037579, 0.019441680292471092, 0.017653869154109332, 0.016020997017052663, 0.014528135042916683, 0.01316379595989156, 0.011923281318399277, 0.010799364103775261, 0.009773500297595376, 0.008844883208264796, 0.008001192897239726, 0.007237746840293756, 0.006544441506306287, 0.005915939715508181, 0.005345486158113786, 0.004829235339715058, 0.004361189281621977, 0.003938974768585847, 0.0035564358871148704, 0.0032096171409466572, 0.0028957279953222823, 0.002612256099297884, 0.0023555786609712553, 0.0021249447541186696, 0.0019160859486418668, 0.0017276157363911796, 0.0015576235504518694, 0.0014040835533882451, 0.0012654015000627835, 0.0011401941957923994, 0.001027646655475931, 0.0009257892242696154, 0.0008337557563404601, 0.0007511443603782177, 0.0006765539109198852, 0.0006094808252579632, 0.0005489950246354805, 0.0004943819525760919, 0.00044539267044044824, 0.0004012425635298501, 0.00036122825181849434, 0.0003253094275835809, 0.0002929063353407394, 0.00026379736624768926, 0.00023758714058050618, 0.00021394117223956461, 0.00019267514319255756, 0.00017345492337083953, 0.00015615330464390797, 0.000140557516619162, 0.00012654863532462752, 0.00011391939643848139, 0.00010253652767625619, 9.229422466840228e-05, 8.307852222525065e-05, 7.477084901716693e-05, 6.731575269201153e-05, 6.060590648336109e-05, 5.4552326673813396e-05, 4.910089141000531e-05, 4.421421398407158e-05, 3.982507346705425e-05, 3.584978636428422e-05, 3.2266538086895894e-05, 2.90464687508001e-05, 2.6144592413193705e-05, 2.3539956374142094e-05, 2.119811256868807e-05, 1.908051560425646e-05, 1.7218868284958913e-05, 1.551416755869152e-05, 1.39644618191614e-05, 1.2573547895634642e-05, 1.1316517284878086e-05, 1.0185218278934756e-05, 9.17679670683516e-06, 8.261555750618478e-06, 7.43879763784614e-06, 6.708122999705301e-06, 6.04056118946561e-06, 5.4593315462574695e-06, 4.916919223745549e-06, 4.429749283725076e-06, 3.998199741177008e-06, 3.598880718857061e-06, 3.2389929676547254e-06, 2.9535671594156297e-06, 2.6604099258146533e-06, 2.417842810756092e-06, 2.1771575079614483e-06, 1.9814376725975136e-06, 1.787090563361814e-06, 1.6101442164111938e-06, 1.4544570196994684e-06, 1.3236668516138607e-06, 1.1965117467832344e-06, 1.0768649192635589e-06], "duration": 99030.502263, "accuracy_train": [0.4919477753552971, 0.5772434261374123, 0.6394049277293282, 0.6742241212739941, 0.6979094569029162, 0.7190214476052049, 0.7325730997946659, 0.74689493499677, 0.755776282472776, 0.765054347199151, 0.7741206251153562, 0.7802103520671835, 0.7887214781746033, 0.7941601634597637, 0.7979759309246955, 0.8010898278885198, 0.8045535786383352, 0.8089012464239571, 0.8123424666620525, 0.8168761463524363, 0.8193179130906239, 0.8206214383767073, 0.8232717475313769, 0.8241538621262459, 0.8268274227690107, 0.8278522906861389, 0.8311743695782576, 0.830477906399963, 0.83384973266196, 0.8363136694236802, 0.8352455429240495, 0.8371510834832964, 0.8378250161498707, 0.8374301013404393, 0.8396854956856773, 0.840544358792451, 0.8408487910668143, 0.8408473491140642, 0.8416625931501477, 0.8431027434593023, 0.8428008346022517, 0.8440102724713916, 0.8443586843046327, 0.8453828312453857, 0.8456149856381506, 0.8460331519356773, 0.8460792944236802, 0.846544324185585, 0.8465686571382429, 0.8474267992686415, 0.8474743837093945, 0.8472658412929125, 0.847218617340347, 0.8474736627330195, 0.847172474852344, 0.8481712073758766, 0.8478700194952011, 0.8480549499354004, 0.8482874648163529, 0.8482642133282576, 0.8481715678640642, 0.8480782014234957, 0.8486587676495018, 0.8481940983757843, 0.8477984625899779, 0.84849672820921, 0.8482871043281653, 0.8486591281376891, 0.8486362371377814, 0.8483804707687338, 0.8490082609473052, 0.8490776549234034, 0.8483114372808231, 0.8488222490425433, 0.8486591281376891, 0.8490071794827427, 0.8491009064114986, 0.84898500945921, 0.8491706608757843, 0.8488218885543558, 0.8488676705541713, 0.8487052706256922, 0.8490075399709303, 0.8488916430186415, 0.8487288826019748, 0.8487982765780732, 0.8488691125069213, 0.8484731162329272, 0.8490075399709303, 0.8490540429471208, 0.8488909220422666, 0.848845140042451, 0.849170300387597, 0.8493101698043558, 0.8490544034353081, 0.848938145994832, 0.8485428706972129, 0.8489617579711147, 0.8484734767211147, 0.8484033617686415, 0.8488909220422666, 0.8480316984473052, 0.8482409618401624, 0.8484963677210224, 0.8488683915305463, 0.8487052706256922, 0.8485886526970285, 0.8483099953280732, 0.8487761065545404, 0.8488222490425433, 0.849030430970838, 0.8484956467446475, 0.849170300387597, 0.8485192587209303, 0.8484731162329272, 0.8490311519472129, 0.8486358766495938, 0.8487049101375047, 0.8493570332687338, 0.8486122646733113, 0.8492636668281653, 0.8489145340185493, 0.8493094488279809, 0.8486591281376891, 0.8489839279946475, 0.8491012668996861, 0.8492858368516981, 0.848682019137597, 0.8488916430186415], "end": "2016-02-01 19:15:41.550000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0], "moving_var_accuracy_valid": [0.020847682542617648, 0.042055131199919805, 0.06197181504733294, 0.07876707106933868, 0.0914119248145602, 0.10106918116815913, 0.10682289223743421, 0.11011127718969407, 0.11061855121156346, 0.10933037992401586, 0.10664265661376059, 0.10289007936993695, 0.09865889401026534, 0.09382330793856125, 0.08866254573190821, 0.08340277825412444, 0.07800822557077691, 0.07269405039189078, 0.06749232813475105, 0.062477888367035696, 0.05768281447570214, 0.05318690922481754, 0.048924468259611074, 0.044888668315043366, 0.04108550428545127, 0.037584906812099785, 0.034314266966601566, 0.03128998214116221, 0.02852056248347017, 0.025956341324350155, 0.023602982339938974, 0.0214504645626144, 0.019459692582575912, 0.01765126863566389, 0.0160087762791969, 0.014507294176031572, 0.013149398476669837, 0.01190651186181028, 0.010778255066100492, 0.009749976477003654, 0.008817944074789015, 0.007974455732833239, 0.0072065617797778085, 0.006514269771397833, 0.0058864952173194734, 0.00531364000960143, 0.004795310405028496, 0.004327063250036088, 0.0039030776118827585, 0.0035224009608396563, 0.003176199167400373, 0.0028633096369593356, 0.0025818659108322334, 0.002328255546317603, 0.0020975212771787983, 0.001891483103228626, 0.001704846615303036, 0.0015372039491637092, 0.0013851751188996098, 0.0012491234019606332, 0.001125466106387953, 0.0010136003740451628, 0.0009128010608452411, 0.0008225964400987295, 0.0007414186565997815, 0.000667900303854008, 0.0006016687107298136, 0.0005418219502680458, 0.0004886105301151678, 0.00043988709386905107, 0.0003962540718501384, 0.0003567055708247995, 0.00032113923283893256, 0.00028951037639529035, 0.00026069205029671875, 0.0002346447047163902, 0.00021124075825487647, 0.00019027782975481654, 0.0001716821501636808, 0.0001545154722740636, 0.0001391102802260945, 0.00012520243297480335, 0.00011275993659862595, 0.00010148491773030385, 9.137434012288424e-05, 8.223888480525423e-05, 7.402699503668223e-05, 6.676679381767319e-05, 6.0092600483501146e-05, 5.409729192858809e-05, 4.869886344541329e-05, 4.38377395967608e-05, 3.967603849976971e-05, 3.571215553456272e-05, 3.214108641540978e-05, 2.8935133695844993e-05, 2.6075832510005262e-05, 2.3468358662119457e-05, 2.112941407548262e-05, 1.912072574076195e-05, 1.721017382692841e-05, 1.54947465786925e-05, 1.396850867959211e-05, 1.2572105224643737e-05, 1.131720141112336e-05, 1.0342477688874393e-05, 9.31767169184625e-06, 8.394102091804345e-06, 7.586358012815255e-06, 6.828116715568088e-06, 6.153208048157251e-06, 5.558280834247467e-06, 5.003035292950861e-06, 4.5057138404509605e-06, 4.084239654061089e-06, 3.713153802163265e-06, 3.418896785703942e-06, 3.0883362923853992e-06, 2.809129848973396e-06, 2.591221685487628e-06, 2.33302615956116e-06, 2.12396875657998e-06, 1.911612159052155e-06, 1.7411469437499986e-06, 1.6243150342664035e-06, 1.5521113375901406e-06, 1.3997842651941255e-06, 1.3300155481598508e-06, 1.2003298231511978e-06], "accuracy_test": 0.804215162627551, "start": "2016-01-31 15:45:11.048000", "learning_rate_per_epoch": [0.00017718490562401712, 0.00016292688087560236, 0.00014981618733145297, 0.00013776050764136016, 0.00012667494593188167, 0.00011648144572973251, 0.00010710820788517594, 9.84892321866937e-05, 9.056382987182587e-05, 8.327617979375646e-05, 7.65749646234326e-05, 7.041299249976873e-05, 6.474687688751146e-05, 5.95367127971258e-05, 5.4745807574363425e-05, 5.034042624174617e-05, 4.6289544116007164e-05, 4.2564635805319995e-05, 3.9139467844506726e-05, 3.5989924072055146e-05, 3.309382373117842e-05, 3.0430772312683985e-05, 2.798201603582129e-05, 2.5730309062055312e-05, 2.365979707974475e-05, 2.1755899069830775e-05, 2.0005207261419855e-05, 1.8395392544334754e-05, 1.6915118976612575e-05, 1.5553963748971e-05, 1.430233987775864e-05, 1.3151434359315317e-05, 1.2093141776858829e-05, 1.1120009730802849e-05, 1.0225185178569518e-05, 9.402367140864953e-06, 8.64576122694416e-06, 7.95003870734945e-06, 7.3103010436170734e-06, 6.7220430537418e-06, 6.181121989357052e-06, 5.683728431904456e-06, 5.226360372034833e-06, 4.805796379514504e-06, 4.419075139594497e-06, 4.063473170390353e-06, 3.7364864056144143e-06, 3.4358122320554685e-06, 3.159333346047788e-06, 2.9051025194348767e-06, 2.6713296392699704e-06, 2.4563682927691843e-06, 2.258704853375093e-06, 2.076947339446633e-06, 1.9098158645647345e-06, 1.7561334288984654e-06, 1.6148177337527159e-06, 1.4848736782369087e-06, 1.3653861969942227e-06, 1.25551389373868e-06, 1.1544830158527475e-06, 1.0615819974191254e-06, 9.761567980604013e-07, 8.976056733445148e-07, 8.253755368059501e-07, 7.589577535327408e-07, 6.978846158745e-07, 6.417259896807082e-07, 5.900864721297694e-07, 5.426023790278123e-07, 4.989393005416787e-07, 4.587897706187505e-07, 4.2187107851532346e-07, 3.879232224335283e-07, 3.567071189536364e-07, 3.2800298299662245e-07, 3.01608650943308e-07, 2.773382732357277e-07, 2.55020921713367e-07, 2.3449945274478523e-07, 2.156293419375288e-07, 1.9827770358915586e-07, 1.8232233855997038e-07, 1.6765091004344868e-07, 1.541600767041018e-07, 1.4175485318901337e-07, 1.303478711633943e-07, 1.1985881087639427e-07, 1.1021379719977631e-07, 1.0134491645885646e-07, 9.318971194716141e-08, 8.569075760078704e-08, 7.879524588361164e-08, 7.245461119964602e-08, 6.662420304337502e-08, 6.126296625552641e-08, 5.6333149700549257e-08, 5.180003270766065e-08, 4.763169769717024e-08, 4.379878504323642e-08, 4.0274308332755027e-08, 3.7033444755252276e-08, 3.4053371678055555e-08, 3.131310322146419e-08, 2.8793344597488613e-08, 2.6476351777660057e-08, 2.434580537169495e-08, 2.238670404608456e-08, 2.0585250837257263e-08, 1.8928760781022902e-08, 1.7405568542017136e-08, 1.6004946701286826e-08, 1.4717032037481204e-08, 1.353275624893513e-08, 1.24437784521092e-08, 1.1442430114527724e-08, 1.0521660875895122e-08, 9.674985257390745e-09, 8.896441805461563e-09, 8.18054690654435e-09, 7.522260148107307e-09, 6.916945682888809e-09, 6.360340698563505e-09, 5.848525663765258e-09, 5.377896350466926e-09, 4.945138520895398e-09, 4.547204390803472e-09, 4.181291757276995e-09, 3.844824014720416e-09], "accuracy_train_first": 0.4919477753552971, "accuracy_train_last": 0.8488916430186415, "batch_size_eval": 1024, "accuracy_train_std": [0.019860847701594006, 0.015736565263928373, 0.016258902068854657, 0.01795052121111984, 0.020120813759244696, 0.0168207819482898, 0.01860166618265053, 0.018212996653497106, 0.017840338152410457, 0.01709704909671973, 0.01664416435926959, 0.017091720773718514, 0.016188840121522597, 0.01603383095993064, 0.015143963549313993, 0.01532503062394655, 0.01455121297026683, 0.014426975818146141, 0.01418514481463275, 0.013848552151724043, 0.01275030423305112, 0.012700478794186391, 0.012546708688779341, 0.012333628781650119, 0.011569377490099446, 0.012035959572633592, 0.012101993951928612, 0.01131470571534995, 0.011239588215555605, 0.011404379740230047, 0.011019009018801165, 0.011204649918634135, 0.010771941020144633, 0.011054436273980416, 0.010502991423546432, 0.010984036707560414, 0.010452621707842887, 0.011000711327124374, 0.01004472865934317, 0.010199127405333297, 0.009833475027002193, 0.009731580557573805, 0.0095051339011779, 0.009954441093999684, 0.010050890147694483, 0.00971103929542115, 0.010582784860763935, 0.010087429822436277, 0.010014168399802684, 0.010401321712043459, 0.010102043498122559, 0.01027188255168505, 0.010023089561917248, 0.010239174862375371, 0.010244372309109191, 0.009759803684504869, 0.010016073100738477, 0.01033130967609636, 0.01023132281819251, 0.01053773702417377, 0.009902558591703185, 0.010172012793741988, 0.010545684962552766, 0.01023914377896032, 0.010092810153032601, 0.009997210687107032, 0.010233834521731862, 0.010019965593865475, 0.01019380523563924, 0.010144901812037822, 0.0099409602926946, 0.010163198635076667, 0.010046316678747995, 0.009812940906684149, 0.009922043580167815, 0.010182975957048335, 0.00998975875946069, 0.010074535946959022, 0.009960063297287861, 0.0100284102450384, 0.010625724317611086, 0.009963094867198506, 0.010420974637976036, 0.010095685562242173, 0.010322046480446406, 0.010176216001383551, 0.010161615301770207, 0.0101627425495888, 0.010271739100205521, 0.010190031377472927, 0.010064368709829526, 0.00997231461477381, 0.009985300460498867, 0.009875965662906694, 0.010260907395927084, 0.010332271259185089, 0.010297875498934466, 0.009960861029852577, 0.009904449683217662, 0.010222673359385207, 0.010158684273084194, 0.00997094529628539, 0.01032685138128929, 0.010275289850650104, 0.010790914523690425, 0.01050231562873151, 0.010187530948601226, 0.010511653424958661, 0.010250114120980763, 0.010171974889000906, 0.009916602980135605, 0.010009060421451652, 0.01025676390834099, 0.010026324391682296, 0.010278269085254352, 0.010331301789413019, 0.01036420338525885, 0.010228522889804947, 0.010043343875888688, 0.010108141437054548, 0.010242413612120882, 0.010194842203263894, 0.010888800793830327, 0.010001820134393529, 0.00990461279643421, 0.009882018520953346, 0.010509212918238269, 0.010127118803703664, 0.009998505152079812], "accuracy_test_std": 0.008151151709770743, "error_valid": [0.5187091137989458, 0.44314464890813254, 0.3832875447100903, 0.34378529743975905, 0.3211714043674698, 0.2939070736069277, 0.2854327466114458, 0.26925769484186746, 0.26610445689006024, 0.25853609751506024, 0.2524428769766567, 0.2477321394954819, 0.23770178369728923, 0.23478239128388556, 0.23097762142319278, 0.22571830289909633, 0.22496529085090367, 0.22156791227409633, 0.21959419710090367, 0.21717338102409633, 0.21507759553840367, 0.2105198136295181, 0.20919763036521077, 0.20913585984563254, 0.20965502635542166, 0.20602380224021077, 0.2063694230045181, 0.20537227033132532, 0.20269701854292166, 0.20303234422063254, 0.2020454866340362, 0.20069241810993976, 0.2025337678840362, 0.2007027131965362, 0.19897313864834332, 0.19896284356174698, 0.19675528285015065, 0.19736563441265065, 0.1965008471385542, 0.19673469267695776, 0.19600227080195776, 0.1950360033885542, 0.19548310429216864, 0.19403885071536142, 0.19380500517695776, 0.19514777861445776, 0.1950360033885542, 0.1946697924510542, 0.19490363798945776, 0.1934182040662651, 0.19453742705195776, 0.19465949736445776, 0.1938153002635542, 0.19331672392695776, 0.1949139330760542, 0.19282844267695776, 0.1933270190135542, 0.19246223173945776, 0.19318435852786142, 0.19185188017695776, 0.19282844267695776, 0.19343879423945776, 0.1934182040662651, 0.19220779602786142, 0.19185188017695776, 0.19234016142695776, 0.19221809111445776, 0.19257400696536142, 0.19098709290286142, 0.19270637236445776, 0.1918312900037651, 0.19269607727786142, 0.19245193665286142, 0.1910988681287651, 0.19197395048945776, 0.19257400696536142, 0.1921975009412651, 0.19159744446536142, 0.1906105868787651, 0.19245193665286142, 0.19185188017695776, 0.1926857821912651, 0.1915871493787651, 0.1923195712537651, 0.19306228821536142, 0.19232986634036142, 0.19282844267695776, 0.19124152861445776, 0.19220779602786142, 0.19196365540286142, 0.19196365540286142, 0.1925945971385542, 0.19074295227786142, 0.1919533603162651, 0.19209602080195776, 0.1918312900037651, 0.19148566923945776, 0.1920754306287651, 0.19234016142695776, 0.19099738798945776, 0.19209602080195776, 0.19172980986445776, 0.19246223173945776, 0.1920754306287651, 0.19185188017695776, 0.19331672392695776, 0.19245193665286142, 0.19246223173945776, 0.19159744446536142, 0.1921975009412651, 0.19184158509036142, 0.19258430205195776, 0.1920754306287651, 0.19232986634036142, 0.19159744446536142, 0.1914650790662651, 0.19111945830195776, 0.19159744446536142, 0.1913430087537651, 0.19269607727786142, 0.19184158509036142, 0.19245193665286142, 0.19196365540286142, 0.19246223173945776, 0.19282844267695776, 0.19110916321536142, 0.1918312900037651, 0.19110916321536142, 0.19209602080195776], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.0804697632702005, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 16, "valid_ratio": 0.15, "learning_rate": 0.00019269068964665575, "optimization": "rmsprop", "nb_data_augmentation": 3, "learning_rate_decay_method": "exp", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 2.1401735384535168e-05, "rotation_range": [0, 0], "momentum": 0.6631842860175814}, "accuracy_valid_max": 0.8093894131212349, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8079039791980422, "accuracy_valid_std": [0.02227757039863152, 0.022129779792062532, 0.02715298933260373, 0.02180915917928488, 0.027469090879977058, 0.02511075107126833, 0.01947235428437662, 0.016884843646803795, 0.014767882369425923, 0.015049260514242052, 0.017547008362108846, 0.012949510604797671, 0.015140159104893393, 0.015186304921574861, 0.013159280715122294, 0.014614148633672523, 0.0147217604951112, 0.015920888802413633, 0.015082031881480384, 0.02051379578864196, 0.020704455429526855, 0.015091880057935417, 0.01692506964672315, 0.013352561920510791, 0.015192508124357998, 0.01807906791364358, 0.017445536919911103, 0.015983678672450676, 0.01584981579125411, 0.015131301799800883, 0.015107449398883043, 0.01438728021390681, 0.013971234118707527, 0.01426972150162331, 0.013841428476529092, 0.013641964190652936, 0.013766898918578196, 0.01333183463807644, 0.014537843092865953, 0.014152844646149477, 0.014316050480405592, 0.014978888834434143, 0.01417920483726439, 0.01435708959189819, 0.01567134491852355, 0.01386238510653066, 0.014939043403740013, 0.015664257113779598, 0.015719426758168197, 0.014126993989602581, 0.014624782153858816, 0.0142049294914427, 0.015301074569935983, 0.013398039173912104, 0.013167472759016046, 0.01219466299505911, 0.014961259520854552, 0.013888495679745943, 0.012618676241206021, 0.012490695682714272, 0.012503567494691826, 0.014045111232994394, 0.012863829226585062, 0.012718104575071337, 0.0127924537587371, 0.013341425686711239, 0.01230384711232186, 0.014351686515797553, 0.013675251353145646, 0.013107673085452364, 0.013136363779546264, 0.013310893567620594, 0.013659120353303283, 0.013319857368629853, 0.012423528495677127, 0.012713184106856734, 0.012717996151532867, 0.013831990531173954, 0.013353296057809957, 0.0137807630025861, 0.013114553878725028, 0.013983755364093088, 0.011924529745157167, 0.012563592657906594, 0.013346446617990292, 0.013065936941852216, 0.012617457075161095, 0.013386423145905641, 0.012624024643172829, 0.011757555074246327, 0.012815136614410548, 0.012995999851789243, 0.012641476533108096, 0.01290752212731877, 0.013577573812200263, 0.013181659464729122, 0.013502466140998742, 0.012785750748888523, 0.014021065884017532, 0.012448936225363628, 0.013533603279623597, 0.012763475365718344, 0.014076061984324881, 0.012512445141488457, 0.013295107935628537, 0.012890151794140978, 0.012299821914116998, 0.01419413192463625, 0.012690342883806863, 0.011773734494099705, 0.012759278577728633, 0.01252892754658532, 0.013025908999733724, 0.012761304598594411, 0.01216272093974048, 0.012381847087472881, 0.012675321235284578, 0.01252963234352313, 0.013676469853154506, 0.013157764693282697, 0.012861640416144514, 0.012173174507525652, 0.012861563642944572, 0.013654779771588391, 0.012823627871385194, 0.013604167514545802, 0.013100014466868683, 0.012930273791811452, 0.013212696503083021], "accuracy_valid": [0.4812908862010542, 0.5568553510918675, 0.6167124552899097, 0.656214702560241, 0.6788285956325302, 0.7060929263930723, 0.7145672533885542, 0.7307423051581325, 0.7338955431099398, 0.7414639024849398, 0.7475571230233433, 0.7522678605045181, 0.7622982163027108, 0.7652176087161144, 0.7690223785768072, 0.7742816971009037, 0.7750347091490963, 0.7784320877259037, 0.7804058028990963, 0.7828266189759037, 0.7849224044615963, 0.7894801863704819, 0.7908023696347892, 0.7908641401543675, 0.7903449736445783, 0.7939761977597892, 0.7936305769954819, 0.7946277296686747, 0.7973029814570783, 0.7969676557793675, 0.7979545133659638, 0.7993075818900602, 0.7974662321159638, 0.7992972868034638, 0.8010268613516567, 0.801037156438253, 0.8032447171498494, 0.8026343655873494, 0.8034991528614458, 0.8032653073230422, 0.8039977291980422, 0.8049639966114458, 0.8045168957078314, 0.8059611492846386, 0.8061949948230422, 0.8048522213855422, 0.8049639966114458, 0.8053302075489458, 0.8050963620105422, 0.8065817959337349, 0.8054625729480422, 0.8053405026355422, 0.8061846997364458, 0.8066832760730422, 0.8050860669239458, 0.8071715573230422, 0.8066729809864458, 0.8075377682605422, 0.8068156414721386, 0.8081481198230422, 0.8071715573230422, 0.8065612057605422, 0.8065817959337349, 0.8077922039721386, 0.8081481198230422, 0.8076598385730422, 0.8077819088855422, 0.8074259930346386, 0.8090129070971386, 0.8072936276355422, 0.8081687099962349, 0.8073039227221386, 0.8075480633471386, 0.8089011318712349, 0.8080260495105422, 0.8074259930346386, 0.8078024990587349, 0.8084025555346386, 0.8093894131212349, 0.8075480633471386, 0.8081481198230422, 0.8073142178087349, 0.8084128506212349, 0.8076804287462349, 0.8069377117846386, 0.8076701336596386, 0.8071715573230422, 0.8087584713855422, 0.8077922039721386, 0.8080363445971386, 0.8080363445971386, 0.8074054028614458, 0.8092570477221386, 0.8080466396837349, 0.8079039791980422, 0.8081687099962349, 0.8085143307605422, 0.8079245693712349, 0.8076598385730422, 0.8090026120105422, 0.8079039791980422, 0.8082701901355422, 0.8075377682605422, 0.8079245693712349, 0.8081481198230422, 0.8066832760730422, 0.8075480633471386, 0.8075377682605422, 0.8084025555346386, 0.8078024990587349, 0.8081584149096386, 0.8074156979480422, 0.8079245693712349, 0.8076701336596386, 0.8084025555346386, 0.8085349209337349, 0.8088805416980422, 0.8084025555346386, 0.8086569912462349, 0.8073039227221386, 0.8081584149096386, 0.8075480633471386, 0.8080363445971386, 0.8075377682605422, 0.8071715573230422, 0.8088908367846386, 0.8081687099962349, 0.8088908367846386, 0.8079039791980422], "seed": 181930039, "model": "residualv3", "loss_std": [0.3672071397304535, 0.25091466307640076, 0.2498403638601303, 0.2502363324165344, 0.2514563798904419, 0.24870333075523376, 0.24753837287425995, 0.24574138224124908, 0.24509339034557343, 0.24389493465423584, 0.24128548800945282, 0.23882843554019928, 0.2392570972442627, 0.23863141238689423, 0.23563553392887115, 0.23398683965206146, 0.23492200672626495, 0.23386617004871368, 0.23154574632644653, 0.232132226228714, 0.2303396761417389, 0.2309619039297104, 0.22968143224716187, 0.2293006181716919, 0.22858186066150665, 0.22842499613761902, 0.22664615511894226, 0.2264971137046814, 0.22462722659111023, 0.2244536280632019, 0.22426961362361908, 0.22406938672065735, 0.2255922108888626, 0.22200626134872437, 0.22268831729888916, 0.22377212345600128, 0.22331146895885468, 0.2225802093744278, 0.22268511354923248, 0.22171413898468018, 0.22214888036251068, 0.22156906127929688, 0.22151896357536316, 0.21980725228786469, 0.21997316181659698, 0.2223210632801056, 0.22142411768436432, 0.22041849792003632, 0.22081582248210907, 0.22215110063552856, 0.21989892423152924, 0.21888691186904907, 0.22063803672790527, 0.22048133611679077, 0.22124573588371277, 0.22159409523010254, 0.21989190578460693, 0.21981747448444366, 0.21905378997325897, 0.2205461859703064, 0.2198931872844696, 0.2211635410785675, 0.21914584934711456, 0.22012613713741302, 0.219013050198555, 0.2201489955186844, 0.21956507861614227, 0.2201007902622223, 0.2207174301147461, 0.21964283287525177, 0.2199854552745819, 0.21913355588912964, 0.21861054003238678, 0.21762286126613617, 0.21894970536231995, 0.21912309527397156, 0.22095584869384766, 0.2192467749118805, 0.21940447390079498, 0.21936960518360138, 0.21965083479881287, 0.21931007504463196, 0.2187771052122116, 0.219546377658844, 0.22012189030647278, 0.21902523934841156, 0.22111296653747559, 0.21940934658050537, 0.21812354028224945, 0.22116801142692566, 0.2180289626121521, 0.22045700252056122, 0.21887768805027008, 0.21959887444972992, 0.2188064455986023, 0.21952663362026215, 0.21990931034088135, 0.2183983474969864, 0.21823501586914062, 0.21938981115818024, 0.2207239270210266, 0.21926699578762054, 0.21800126135349274, 0.2196204662322998, 0.21768486499786377, 0.21883395314216614, 0.2203875482082367, 0.22036665678024292, 0.22010695934295654, 0.2181767374277115, 0.21886049211025238, 0.2179344743490219, 0.2184653878211975, 0.22124667465686798, 0.21747776865959167, 0.2186354547739029, 0.21958774328231812, 0.2195807248353958, 0.21779926121234894, 0.22013308107852936, 0.22081167995929718, 0.22037728130817413, 0.21938949823379517, 0.21970772743225098, 0.2200118899345398, 0.21874107420444489, 0.2179533988237381, 0.2195017784833908, 0.21886491775512695]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:28 2016", "state": "available"}], "summary": "8a426faa6b5f1d1fee32c74ad5e2b300"}