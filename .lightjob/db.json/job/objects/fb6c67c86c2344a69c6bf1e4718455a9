{"content": {"hp_model": {"f0": 16, "f1": 64, "f2": 64, "f3": 64, "nonlin": "leaky_rectify", "nbg1": 5, "nbg3": 4, "nbg2": 3, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.012426210863652937, 0.011942348177878613, 0.014245924234986615, 0.016048700822501948, 0.015522105711062328, 0.015132164056532069, 0.017464094188134938, 0.016368271662875498, 0.016177723609708313, 0.018249941473210875, 0.021867391413954596, 0.026447486240966503, 0.02440959989860866, 0.0275359410095738, 0.02697093333363783, 0.025272024665642354, 0.02677037421942441, 0.028170514412849415, 0.029765824430619977, 0.028033455023879775, 0.029158639008438982, 0.027865337805224686, 0.028599742207496916, 0.026808779066056217, 0.028120687968212807, 0.028280188903138017, 0.030537302310952485, 0.030014186287332413, 0.029781076359867438, 0.028186681960594193, 0.02737080168333359, 0.027764428619054368, 0.02903066643408034, 0.027445848261555477, 0.02536519742274361, 0.026636676658218707, 0.02765695901454513, 0.027227236995398925, 0.02825734858768109, 0.027036531137878326, 0.025927548642244235, 0.027029830873292804, 0.0261061403436702, 0.024847175210470567, 0.024670273138484036, 0.02542029646652161, 0.02316045125243314, 0.023959896446236542, 0.02323897042388488, 0.024371630379728563, 0.025356860857321094, 0.024270834989665757, 0.023585347453310056, 0.022671735272464928, 0.02225195914941686, 0.022014317466083882, 0.02320800028168815, 0.024760065466829385, 0.023346958068649528, 0.02345136074876412, 0.023381515066148537, 0.021986838377258813, 0.023257802153320708, 0.023216761453101007, 0.023214128263529137, 0.02392200301580084, 0.02391258627465838, 0.021690223878247362, 0.024383113679258862, 0.022748731166157297, 0.024018189460424003, 0.022142171878768654, 0.021762610450606148, 0.0238020311511837, 0.02344625217041646, 0.023322751556107704, 0.02245865367436089, 0.02259462902250341, 0.02083744440517575, 0.022060025828256054, 0.023675732639812118, 0.022007132361461675, 0.02341625694143331, 0.024434177608107358, 0.022591137393968817, 0.02393448789910156, 0.022332061340873368, 0.022185693369416077, 0.022387234596272034, 0.021991626882454676, 0.022858910467106107, 0.023000371380713626, 0.021525350703786902, 0.02187371247508404, 0.023273835117604776, 0.021716120714895447, 0.02185934929331886, 0.022567826592223422, 0.02132282935005507, 0.02105959541975481, 0.02208163068841906, 0.024960048991847218, 0.022307395491019327, 0.022708246996856637, 0.021175662752971704, 0.02176109660283384, 0.02146691712308469, 0.02327457178350239, 0.022866933683934443, 0.02247560819566035, 0.01971412784191535, 0.022725900108735334, 0.022002073879916387, 0.02199911576113756, 0.021978998163418507, 0.02322521375181352, 0.022475516335056584, 0.021253413463776856, 0.02367762250221645, 0.0235226171320997, 0.019929544884869387, 0.02085668040375351, 0.023933834311002884, 0.022569467416845967, 0.02192759182347306, 0.02117242356738123, 0.02222534907293093, 0.021733379956401004, 0.020697501019824337, 0.02123930616346279, 0.022666161639745303, 0.023819332407284427, 0.021826821922855073, 0.02139645444648862, 0.023170926846475745, 0.020844030173228726, 0.02151450975285209, 0.022363374086187295, 0.023081526562588297, 0.023475259533746965, 0.021338951820081123, 0.02284450657031865, 0.021279166078021727, 0.025407570824382546, 0.024490519044857387, 0.020396613127356387, 0.023444771244398362, 0.022766690122874845, 0.02333446658743308, 0.024677092481812556, 0.02263684292681532, 0.021211923368043582, 0.023773255136206106, 0.01985861459660058, 0.02107432139007143, 0.024086730123818526, 0.02282764410708409, 0.02322818576586534, 0.02273345114706218, 0.023953749166736826, 0.02169104414288325, 0.021538142075411975, 0.022399663091683886, 0.02067101073377068, 0.02123315161774618, 0.02217996736727101, 0.022177993090861426, 0.02100854152908182, 0.019727495123131388, 0.021693357380347795, 0.022558901177182115, 0.02365651040010666, 0.020206815742856838, 0.020338436346628342, 0.02172179129109213, 0.02249624335297546, 0.022475342717971303, 0.02247412240416867, 0.020945710412300953, 0.022556120370794148, 0.022477764815003867, 0.021390012219786404, 0.02184865738594555, 0.021902506524459627, 0.023346712783150037, 0.022855688050860266, 0.022192806141166324, 0.0223793600501904, 0.020849102422334637, 0.02155474007725299, 0.023402977793744995, 0.021028148964624634, 0.02280225460518948, 0.022555458919909833, 0.02050061708080049, 0.023961902280727043, 0.023192112254796598, 0.022908368077803738, 0.021548637278367144, 0.024553715902381108, 0.022217556662804346, 0.022562307102054837, 0.023053762269525508, 0.02408729658826782, 0.021470856683978898, 0.021681322827538835, 0.022561682455739192, 0.021436747453143116, 0.023015691570614952, 0.02207010070862166, 0.020359923715541343, 0.021637539104169396, 0.02225558393065417, 0.02143828420459071, 0.022532463900478368, 0.02250004176706935, 0.021496407265090056, 0.02308609967415585, 0.022024126870587003, 0.020821706563168733, 0.024936157538400047, 0.020416162555915832, 0.02365796746600146, 0.021069095210351784, 0.023077761620722555, 0.023706337407387968, 0.022015952531167463, 0.021989940186904363, 0.023563745065577985, 0.02183752452112907], "moving_avg_accuracy_train": [0.012381687776854925, 0.024108530736664815, 0.03570428396952288, 0.04815360816231773, 0.06070840671070389, 0.0728747175196538, 0.08461734508939384, 0.09593662482117982, 0.10695619960950296, 0.11774113347376491, 0.12818675498011672, 0.1384573839417765, 0.14854469063457745, 0.1580254453045084, 0.1671882037860085, 0.17595526741096063, 0.1844034801436095, 0.19241373659584138, 0.20021577220282238, 0.20748875636935263, 0.21441583862162925, 0.22092207481529874, 0.22710077492766884, 0.23301030525259298, 0.23853356773790033, 0.243732260411554, 0.24871789531424318, 0.2534583718980828, 0.2578596234056721, 0.26203924165296016, 0.2659684169362614, 0.26974649016742297, 0.27330715529450683, 0.2766464322957409, 0.27992149885875633, 0.2829596314225854, 0.2858404711538503, 0.28855173740363693, 0.2912219946629503, 0.29379023966417095, 0.2960853841236029, 0.29830218485852944, 0.3004390674997068, 0.30240876485295687, 0.30424648849109237, 0.30594468969043276, 0.30758224461743044, 0.3091537003017284, 0.31066566666759654, 0.31206596392663977, 0.313407611668112, 0.31460343284257064, 0.3158261202257647, 0.31685678440635356, 0.31790761505578835, 0.3187952339200416, 0.3197219740823933, 0.3206071574535006, 0.3213759567506016, 0.3221561957239357, 0.3228607359487459, 0.32355527602012274, 0.32416870029149547, 0.32472310728454046, 0.32527551595208126, 0.3257286140719433, 0.32616419601907715, 0.3265260649346113, 0.3269958701359637, 0.327448885702886, 0.32786121396191636, 0.32819525120936627, 0.32857490374277626, 0.32886075140259796, 0.32913897668454195, 0.3294312321168629, 0.3296756608154756, 0.3299258375299321, 0.33015568291938074, 0.33038579525797973, 0.33062312329724264, 0.3308972084504457, 0.3311205254537769, 0.33126109293654615, 0.3314456963424578, 0.3316514029863589, 0.3317389187646887, 0.33191297801755726, 0.3320928467844154, 0.33224542807934965, 0.3323386094662283, 0.3325015277739428, 0.3326388897044666, 0.3326998085217184, 0.3327778148477027, 0.33292482254944034, 0.33302675835120543, 0.3331324875144699, 0.3331811768340362, 0.3332435623633033, 0.3333066847860723, 0.33334950802488844, 0.33341373372318406, 0.333513317432584, 0.33356574039009157, 0.33354788898281923, 0.33351311337934164, 0.3336189991159737, 0.33371898262538036, 0.333720540031447, 0.33373596468740163, 0.3337312456872846, 0.33380837879551256, 0.3339475530572035, 0.3340216926677345, 0.33398603967195595, 0.3339982019007737, 0.33405100058528103, 0.3340682564179951, 0.3341465656852949, 0.3341473256103977, 0.33417827252633286, 0.3342153532482751, 0.33422772746109985, 0.33419475852289865, 0.33418136252018427, 0.3342366993843988, 0.3343678827705252, 0.33441157910495295, 0.3343741758952236, 0.3342893597326577, 0.33432931867564336, 0.3343094421040831, 0.334333441917069, 0.3343155142189944, 0.33429709019073656, 0.3342176574498098, 0.33423692088418466, 0.3342471743822372, 0.33429127976262735, 0.3344054514645207, 0.33440822459741526, 0.3344176598146302, 0.33442382636131407, 0.3344525556437873, 0.3344296920194695, 0.33441612625283074, 0.3344271325021324, 0.3344206899871997, 0.33441027747495994, 0.3344938761175064, 0.3345110222243789, 0.33452423671821085, 0.33449191588645993, 0.33450925801643705, 0.33452490198223517, 0.33452045245861484, 0.3345837330075577, 0.33459425462305337, 0.33459442348176127, 0.3345712879176845, 0.3346039443326344, 0.3346147699644319, 0.33459425004970705, 0.3345711678776544, 0.33451319154185466, 0.3344865894765396, 0.33446715372010005, 0.3344800326691033, 0.33439629262201576, 0.33450454123795187, 0.3345601844113605, 0.33447544068529456, 0.3344642394496831, 0.33437968147809055, 0.3343827425096374, 0.3343343081154012, 0.334323269243922, 0.3343691017822006, 0.3344661546380798, 0.33452563647147565, 0.33448620021796965, 0.3345436774933765, 0.3345232913793286, 0.3345236171647993, 0.3344797325443419, 0.33447271637162607, 0.33455708261975325, 0.3345795338204487, 0.3345090590975031, 0.3344549324420902, 0.334527090141495, 0.33454792634121594, 0.3344945632590508, 0.3344930755101114, 0.33453348106818115, 0.33451178944784327, 0.3345550820562151, 0.3346312117358834, 0.3346299739832991, 0.3345777067321637, 0.3345422919501895, 0.3345104186464127, 0.33464452913849896, 0.33463498419922455, 0.33469382306935375, 0.33462590636319356, 0.3346647266776402, 0.33473915644158525, 0.33472018482082094, 0.334696062818067, 0.33466051026918764, 0.3347284222763682, 0.33469424803045894, 0.33463551732578883, 0.3345501797058899, 0.33452209582652465, 0.33454575660655217, 0.33448567110024363, 0.3345338646433662, 0.33454243369767117, 0.33466858424055634, 0.3346985225184664, 0.3346812530923857, 0.3346936484434462, 0.33471635790581067, 0.3346554883112429, 0.33460997022255123, 0.33460155602606206, 0.3345381436289745], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 323804964, "moving_var_accuracy_train": [0.001379755729831668, 0.002479449769084887, 0.0034416582295124423, 0.0044923634622769625, 0.0054617338153643, 0.0062477325021276136, 0.006863962972089423, 0.007330701517698268, 0.007690510622567464, 0.007968292746419256, 0.008153462549662968, 0.008287488668109435, 0.008374523608130131, 0.008346033629319878, 0.008267035553300602, 0.00813208463940617, 0.007961226860850367, 0.007742582050640033, 0.007516169682089427, 0.00724061940205988, 0.006948417678622145, 0.006634555895430263, 0.0063146873215946565, 0.005997521527385974, 0.005672327230981807, 0.00534833215751953, 0.0050372079402137915, 0.004735736210369798, 0.004436501722830331, 0.00415007442878487, 0.0038740127515685323, 0.003615076012471858, 0.0033676734365497506, 0.0031312630309135127, 0.0029146712767517966, 0.0027062763943552035, 0.002510341892934793, 0.002325466385736402, 0.0021570922116410005, 0.0020007459319535546, 0.0018480805315651487, 0.0017075003278939711, 0.0015778467019040587, 0.0014549794006842556, 0.0013398765141472869, 0.001231843848553528, 0.0011327937389485853, 0.0010417396217631371, 0.0009581400402104718, 0.0008799735279125804, 0.0008081763430811004, 0.0007402286033045433, 0.0006796604229072857, 0.0006212547984948981, 0.000569067524129532, 0.0005192515769501823, 0.00047505604521180485, 0.0004346023870949882, 0.0003964616196184967, 0.0003622944133562318, 0.000330532364375989, 0.0003018206011351228, 0.00027502514505199307, 0.00025028893457222846, 0.0002280064391387733, 0.00020705347638089836, 0.00018805571343682867, 0.00017042868410141454, 0.00015537226803623348, 0.0001416820491674753, 0.00012904397558948267, 0.00011714380597468975, 0.00010672664979234243, 9.67893647747379e-05, 8.780711206487932e-05, 7.979511999788159e-05, 7.235331649644296e-05, 6.568128034290489e-05, 5.958861243607163e-05, 5.4106316387844085e-05, 4.920260613304319e-05, 4.4958449560595766e-05, 4.091143896032771e-05, 3.699812801920363e-05, 3.36050209745509e-05, 3.062535588720144e-05, 2.76317514015913e-05, 2.5141245873014506e-05, 2.291829624533271e-05, 2.0835996084873742e-05, 1.8830541414131966e-05, 1.7186368647615975e-05, 1.563754648246932e-05, 1.410719175488056e-05, 1.2751237461434636e-05, 1.1670615094622893e-05, 1.059707175429402e-05, 9.637972482546059e-06, 8.695511082849914e-06, 7.860987562922355e-06, 7.1107487689361334e-06, 6.416178360086885e-06, 5.811684986972186e-06, 5.319768724875625e-06, 4.812525350652621e-06, 4.33414087026179e-06, 3.91161086661067e-06, 3.6213558829487087e-06, 3.349190614033335e-06, 3.014293382252909e-06, 2.715005324129476e-06, 2.4437052123754713e-06, 2.252880338602112e-06, 2.201917580796763e-06, 2.031195959364332e-06, 1.839516588399812e-06, 1.6568962078481553e-06, 1.5162958968346986e-06, 1.3673461810151269e-06, 1.2858026350188786e-06, 1.1572275688924483e-06, 1.050124216456265e-06, 9.5748661426843e-07, 8.631160431288797e-07, 7.865869967910016e-07, 7.095433731104139e-07, 6.661485526692161e-07, 7.544154245625453e-07, 6.961582088880692e-07, 6.391333888817689e-07, 6.399640828852722e-07, 5.90338128717491e-07, 5.3486001871866e-07, 4.865579360570317e-07, 4.4079476367560167e-07, 3.9977029066326426e-07, 4.165793045771452e-07, 3.782610932546659e-07, 3.4138119193002384e-07, 3.2475063395126783e-07, 4.095921681752884e-07, 3.687021637522172e-07, 3.326331572920291e-07, 2.997120782448687e-07, 2.771692154632298e-07, 2.541570017694251e-07, 2.303975718129476e-07, 2.0844805234486173e-07, 1.879768010982971e-07, 1.7015490468874806e-07, 2.1603801154036298e-07, 1.9708011121426344e-07, 1.7894370571794963e-07, 1.704510606318135e-07, 1.5611269981791898e-07, 1.4270403282917274e-07, 1.2861181389028737e-07, 1.517904833718601e-07, 1.3760777456841138e-07, 1.238472537309395e-07, 1.1627981728422509e-07, 1.1424980849221857e-07, 1.0387957637732548e-07, 9.728122084242824e-08, 9.234817875820175e-08, 1.1336466049729253e-07, 1.0839722335879051e-07, 1.0095723867828712e-07, 9.235432075732071e-08, 1.4623044805756882e-07, 2.370672689205686e-07, 2.4122600675130135e-07, 2.8173689804397753e-07, 2.546924173525799e-07, 2.9357363065589836e-07, 2.6430059681748277e-07, 2.589835520409501e-07, 2.3418190698866668e-07, 2.2966931037526892e-07, 2.9147569084658534e-07, 2.941709182990344e-07, 2.787507892844348e-07, 2.8060844504968885e-07, 2.562879433584775e-07, 2.3066010424818548e-07, 2.24926833037588e-07, 2.0287718985003663e-07, 2.466484452725214e-07, 2.2652010845925466e-07, 2.4856827678162723e-07, 2.500787025391617e-07, 2.719314345358701e-07, 2.4864561605157766e-07, 2.4940962128990476e-07, 2.244885797330748e-07, 2.1673320386608185e-07, 1.9929462101541236e-07, 1.9623340837059283e-07, 2.2877162067106178e-07, 2.0590824688709465e-07, 2.099042120696188e-07, 2.0020165190318637e-07, 1.8932465415569627e-07, 3.3226280552881786e-07, 2.9985647776770537e-07, 3.0102894373368985e-07, 3.124401601411564e-07, 2.9475929545064176e-07, 3.151414737538898e-07, 2.868666279283152e-07, 2.634168042872289e-07, 2.48450977444919e-07, 2.6511424617402405e-07, 2.4911373330780926e-07, 2.552460210164541e-07, 2.9526400324493485e-07, 2.728359414422811e-07, 2.505908399016357e-07, 2.580241685267159e-07, 2.5312531006239284e-07, 2.2847363728128354e-07, 3.4885190878512965e-07, 3.2203342226458335e-07, 2.9251417773252804e-07, 2.646455625104764e-07, 2.428224833873911e-07, 2.518862029342679e-07, 2.453446502241159e-07, 2.2144737352472785e-07, 2.3549282511177323e-07], "duration": 205735.580922, "accuracy_train": [0.12381687776854927, 0.12965011737495385, 0.14006606306524547, 0.16019752589747138, 0.1737015936461794, 0.18237151480020303, 0.19030099321705427, 0.1978101424072536, 0.20613237270441123, 0.21480553825212254, 0.22219734853728315, 0.23089304459671464, 0.2393304508697859, 0.24335223733388706, 0.24965303011950907, 0.2548588400355297, 0.26043739473744926, 0.26450604466592836, 0.27043409266565155, 0.27294561386812477, 0.2767595788921189, 0.2794782005583241, 0.28270907593899963, 0.2861960781769103, 0.2882429301056663, 0.29052049447443706, 0.2935886094384459, 0.29612266115263935, 0.2974708869739756, 0.29965580587855295, 0.30133099448597267, 0.30374914924787744, 0.30535314143826137, 0.30669992530684753, 0.30939709792589515, 0.3103028244970469, 0.3117680287352344, 0.3129531336517165, 0.31525430999677007, 0.3169044446751569, 0.3167416842584902, 0.31825339147286824, 0.3196710112703027, 0.32013604103220744, 0.32078600123431156, 0.32122850048449614, 0.32232023896040973, 0.32329680146040973, 0.32427336396040973, 0.3246686392580288, 0.3254824413413621, 0.32536582341269843, 0.3268303066745109, 0.32613276203165376, 0.32736509090070137, 0.32678380369832044, 0.3280626355435585, 0.32857380779346623, 0.3282951504245109, 0.3291783464839424, 0.32920159797203763, 0.32980613666251385, 0.3296895187338501, 0.32971277022194534, 0.33024719395994834, 0.32980649715070137, 0.33008443354328165, 0.3297828851744186, 0.3312241169481358, 0.3315260258051864, 0.33157216829318936, 0.3312015864364157, 0.33199177654346623, 0.33143338034099296, 0.33164300422203763, 0.33206153100775193, 0.33187551910299, 0.33217742796004057, 0.3322242914244186, 0.332456806305371, 0.3327590756506091, 0.3333639748292728, 0.33313037848375787, 0.3325262002814692, 0.33310712699566264, 0.3335027627814692, 0.3325265607696567, 0.33347951129337394, 0.3337116656861388, 0.33361865973375787, 0.3331772419481358, 0.33396779254337394, 0.3338751470791805, 0.33324807787698413, 0.33347987178156147, 0.33424789186507936, 0.3339441805670912, 0.3340840499838501, 0.33361938071013286, 0.33380503212670726, 0.33387478659099296, 0.33373491717423404, 0.3339917650078442, 0.3344095708171834, 0.33403754700765964, 0.33338722631736806, 0.33320013294804357, 0.33457197074566264, 0.33461883421004057, 0.3337345566860465, 0.33387478659099296, 0.33368877468623104, 0.3345025767695644, 0.33520012141242156, 0.33468894916251385, 0.33366516270994834, 0.33410766196013286, 0.33452618874584716, 0.33422355891242156, 0.33485134909099296, 0.3341541649363234, 0.334456794769749, 0.3345490797457549, 0.33433909537652273, 0.3338980380790882, 0.3340607984957549, 0.33473473116232927, 0.33554853324566264, 0.3348048461148025, 0.33403754700765964, 0.3335260142695644, 0.33468894916251385, 0.33413055296004057, 0.3345494402339424, 0.3341541649363234, 0.3341312739364157, 0.3335027627814692, 0.3344102917935585, 0.33433945586471026, 0.3346882281861388, 0.33543299678156147, 0.33443318279346623, 0.3345025767695644, 0.3344793252814692, 0.3347111191860465, 0.3342239194006091, 0.3342940343530823, 0.33452618874584716, 0.3343627073528055, 0.3343165648648025, 0.3352462639004245, 0.33466533718623104, 0.33464316716269843, 0.33420102840070137, 0.33466533718623104, 0.3346656976744186, 0.33448040674603174, 0.33515325794804357, 0.33468894916251385, 0.33459594321013286, 0.33436306784099296, 0.3348978520671834, 0.3347122006506091, 0.3344095708171834, 0.3343634283291805, 0.3339914045196567, 0.3342471708887043, 0.3342922319121447, 0.33459594321013286, 0.3336426321982281, 0.3354787787813769, 0.33506097297203763, 0.33371274715070137, 0.3343634283291805, 0.33361865973375787, 0.3344102917935585, 0.33389839856727577, 0.3342239194006091, 0.33478159462670726, 0.33533963034099296, 0.33506097297203763, 0.3341312739364157, 0.33506097297203763, 0.3343398163528977, 0.3345265492340347, 0.33408477096022515, 0.3344095708171834, 0.3353163788528977, 0.33478159462670726, 0.33387478659099296, 0.33396779254337394, 0.3351765094361388, 0.3347354521387043, 0.3340142955195644, 0.3344796857696567, 0.33489713109080843, 0.3343165648648025, 0.33494471553156147, 0.3353163788528977, 0.33461883421004057, 0.33410730147194534, 0.33422355891242156, 0.33422355891242156, 0.33585152356727577, 0.3345490797457549, 0.3352233729005168, 0.33401465600775193, 0.33501410950765964, 0.3354090243170912, 0.3345494402339424, 0.33447896479328165, 0.3343405373292728, 0.33533963034099296, 0.33438667981727577, 0.33410694098375787, 0.33378214112679955, 0.334269340912237, 0.33475870362679955, 0.33394490154346623, 0.3349676065314692, 0.3346195551864157, 0.33580393912652273, 0.3349679670196567, 0.33452582825765964, 0.33480520660299, 0.3349207430670912, 0.33410766196013286, 0.3342003074243263, 0.33452582825765964, 0.3339674320551864], "end": "2016-01-28 02:07:47.623000", "learning_rate_per_epoch": [0.0001614150678506121, 0.00015408964827656746, 0.00014709666720591486, 0.0001404210488544777, 0.00013404838682617992, 0.0001279649295611307, 0.00012215755123179406, 0.00011661372991511598, 0.00011132150393677875, 0.0001062694500433281, 0.00010144667612621561, 9.684277029009536e-05, 9.2447800852824e-05, 8.825228724163026e-05, 8.424717816524208e-05, 8.042382978601381e-05, 7.677399116801098e-05, 7.328978972509503e-05, 6.996371666900814e-05, 6.678858335362747e-05, 6.375755037879571e-05, 6.08640730206389e-05, 5.810190850752406e-05, 5.546509783016518e-05, 5.2947951189707965e-05, 5.054504072177224e-05, 4.825117866857909e-05, 4.6061417378950864e-05, 4.3971034756395966e-05, 4.197551606921479e-05, 4.007056122645736e-05, 3.825205931207165e-05, 3.651608494692482e-05, 3.485889101284556e-05, 3.327690501464531e-05, 3.176671452820301e-05, 3.0325059924507514e-05, 2.8948832550668158e-05, 2.7635061996988952e-05, 2.638091245898977e-05, 2.5183679099427536e-05, 2.404078077233862e-05, 2.2949749109102413e-05, 2.1908232156420127e-05, 2.091398164338898e-05, 1.996485298150219e-05, 1.9058797988691367e-05, 1.8193861251347698e-05, 1.736817830533255e-05, 1.6579966541030444e-05, 1.582752520334907e-05, 1.510923175374046e-05, 1.4423536413232796e-05, 1.3768959433946293e-05, 1.3144089280103799e-05, 1.2547577171062585e-05, 1.1978136171819642e-05, 1.1434538464527577e-05, 1.0915609891526401e-05, 1.0420231774332933e-05, 9.94733545667259e-06, 9.495900485489983e-06, 9.064952791959513e-06, 8.653561963001266e-06, 8.260841241281014e-06, 7.885942977736704e-06, 7.528059086325811e-06, 7.1864164965518285e-06, 6.86027851770632e-06, 6.5489416556374636e-06, 6.251734248508001e-06, 5.9680146478058305e-06, 5.69717121834401e-06, 5.438619155029301e-06, 5.191800937609514e-06, 4.956184056936763e-06, 4.731259650725406e-06, 4.5165429582993966e-06, 4.311570592108183e-06, 4.115900537726702e-06, 3.929110334865982e-06, 3.7507973047468113e-06, 3.580576731110341e-06, 3.4180811780970544e-06, 3.262960035499418e-06, 3.11487860926718e-06, 2.9735174393863417e-06, 2.8385716177581344e-06, 2.7097501060779905e-06, 2.586774826340843e-06, 2.4693804334674496e-06, 2.3573136331833666e-06, 2.250332727271598e-06, 2.1482069314515684e-06, 2.0507159206317738e-06, 1.9576491467887536e-06, 1.8688060663407668e-06, 1.7839948895925772e-06, 1.7030326944222907e-06, 1.625744744160329e-06, 1.5519643739025923e-06, 1.4815323083894327e-06, 1.414296662005654e-06, 1.3501122566594859e-06, 1.288840735469421e-06, 1.230349880643189e-06, 1.174513499790919e-06, 1.1212110848646262e-06, 1.0703276984713739e-06, 1.021753519125923e-06, 9.753837275638944e-07, 9.311183362115116e-07, 8.888618481250887e-07, 8.48523029617354e-07, 8.100149102574505e-07, 7.732543849670037e-07, 7.381621571767027e-07, 7.04662511452625e-07, 6.726831429659796e-07, 6.421551006496884e-07, 6.130125029812916e-07, 5.851924242961104e-07, 5.586348947872466e-07, 5.332826162884885e-07, 5.09080905430892e-07, 4.859775231125241e-07, 4.6392264607675315e-07, 4.4286866796028335e-07, 4.2277017087144486e-07, 4.035837832816469e-07, 3.852681231819588e-07, 3.677836843962723e-07, 3.510927228944638e-07, 3.3515925679239444e-07, 3.19948867399944e-07, 3.054287844861392e-07, 2.915676589054783e-07, 2.7833559101964056e-07, 2.657040170106484e-07, 2.5364570888086746e-07, 2.4213463234445953e-07, 2.3114594682738243e-07, 2.2065596283482591e-07, 2.1064204247522866e-07, 2.0108257103856886e-07, 1.9195692857465474e-07, 1.8324543304970575e-07, 1.749292977137884e-07, 1.6699056004654267e-07, 1.5941211017889145e-07, 1.5217759141705756e-07, 1.4527138603170897e-07, 1.3867860104710417e-07, 1.3238501139767322e-07, 1.2637704571716313e-07, 1.206417294952189e-07, 1.151666992882383e-07, 1.0994014587595302e-07, 1.0495078583971917e-07, 1.0018785445709e-07, 9.56410772801064e-08, 9.130064171358754e-08, 8.715718990970345e-08, 8.320177613541091e-08, 7.942586677245345e-08, 7.582131900107925e-08, 7.238035237833174e-08, 6.909554883804958e-08, 6.59598171637299e-08, 6.296639298852824e-08, 6.010881747897656e-08, 5.7380926676842137e-08, 5.4776833735559194e-08, 5.229092181480155e-08, 4.9917826316914216e-08, 4.765242778148604e-08, 4.548984122720867e-08, 4.342539838830817e-08, 4.145464416183131e-08, 3.957332594950458e-08, 3.777738655230678e-08, 3.606295351232802e-08, 3.4426324901914995e-08, 3.286396932367097e-08, 3.137251880502845e-08, 2.994875458739443e-08, 2.858960357343676e-08, 2.7292134774370425e-08, 2.6053548651816527e-08, 2.4871171788731772e-08, 2.374245511305162e-08, 2.2664963239549252e-08, 2.1636369140765055e-08, 2.0654455923363457e-08, 1.9717104393635054e-08, 1.882229305749661e-08, 1.7968089238706852e-08, 1.7152652631580168e-08, 1.637422109013187e-08, 1.5631117733505562e-08, 1.492173851147527e-08, 1.4244552204445426e-08, 1.3598098647094048e-08, 1.2980982511123784e-08, 1.2391872417083505e-08, 1.1829498269833039e-08, 1.1292645929472656e-08, 1.0780157211343067e-08, 1.0290926333311745e-08, 9.823898139416087e-09, 9.378064547149734e-09, 8.952464547462569e-09, 8.546178875690202e-09, 8.15833178791081e-09, 7.788085731874617e-09, 7.434642679271519e-09, 7.097239684838996e-09, 6.7751488863621034e-09, 6.4676752842274254e-09, 6.174155853244656e-09, 5.893956878111339e-09, 5.626474397502079e-09, 5.371130651354861e-09, 5.12737496904947e-09, 4.894681548961444e-09, 4.672548570283652e-09, 4.460496416669457e-09, 4.258067676232713e-09, 4.064825809280137e-09, 3.88035381604368e-09], "accuracy_valid": [0.1275796545557229, 0.13260512754141568, 0.14264577842620482, 0.16472138554216867, 0.1784344408885542, 0.18725438864834337, 0.19521984422063254, 0.19851574265813254, 0.20742687547063254, 0.21553499152861444, 0.22344014730798192, 0.22898478680346385, 0.23695024237575302, 0.24419357115963855, 0.2515486751694277, 0.25532255977033136, 0.2572962749435241, 0.2593817653426205, 0.2687105845256024, 0.27150790662650603, 0.27654367469879515, 0.2792189264871988, 0.2843664697853916, 0.2842238092996988, 0.28779473362198793, 0.28901543674698793, 0.2945600762424699, 0.2947939217808735, 0.2955263436558735, 0.2972147378576807, 0.2989237222326807, 0.3030844079442771, 0.30567876976656627, 0.3052919686558735, 0.3071024331701807, 0.3076010095067771, 0.31019537132906627, 0.31007330101656627, 0.31290150837725905, 0.31435605704066266, 0.31422369164156627, 0.31509877400225905, 0.31631947712725905, 0.31690923851656627, 0.31789609610316266, 0.31876088337725905, 0.31948301016566266, 0.31948301016566266, 0.32082578360316266, 0.32046986775225905, 0.3209684440888554, 0.3224332878388554, 0.3226774284638554, 0.32376576618975905, 0.32191412132906627, 0.32265683829066266, 0.3241422722138554, 0.32439670792545183, 0.32413197712725905, 0.3251188347138554, 0.3243864128388554, 0.32474232868975905, 0.32388783650225905, 0.32388783650225905, 0.32498646931475905, 0.3252409050263554, 0.3260953972138554, 0.32460996329066266, 0.3265836784638554, 0.3268278190888554, 0.32632924275225905, 0.32706166462725905, 0.32607480704066266, 0.32584096150225905, 0.32754994587725905, 0.3271940300263554, 0.32742787556475905, 0.32705136954066266, 0.32765142601656627, 0.32889271931475905, 0.32803822712725905, 0.32877064900225905, 0.3287809440888554, 0.3281705925263554, 0.32816029743975905, 0.3291471550263554, 0.32901478962725905, 0.32851621329066266, 0.32889271931475905, 0.32999135212725905, 0.32779408650225905, 0.3300016472138554, 0.32912656485316266, 0.32924863516566266, 0.3286588737763554, 0.32938100056475905, 0.32962514118975905, 0.3306119987763554, 0.32850591820406627, 0.32973691641566266, 0.32889271931475905, 0.33112087019954817, 0.32962514118975905, 0.33047963337725905, 0.32887212914156627, 0.32888242422816266, 0.32900449454066266, 0.3307340690888554, 0.3306119987763554, 0.3302457878388554, 0.3286176934299699, 0.3298795769013554, 0.33023549275225905, 0.33060170368975905, 0.32962514118975905, 0.3297575065888554, 0.3291471550263554, 0.33022519766566266, 0.3297575065888554, 0.3302457878388554, 0.32960455101656627, 0.3289839043674699, 0.33025608292545183, 0.32999135212725905, 0.33035756306475905, 0.33010312735316266, 0.3308561394013554, 0.33121205525225905, 0.33083554922816266, 0.33059140860316266, 0.3308561394013554, 0.33136501082454817, 0.33096791462725905, 0.33010312735316266, 0.33074436417545183, 0.32997076195406627, 0.32899419945406627, 0.3313444206513554, 0.33147678605045183, 0.33172092667545183, 0.33060170368975905, 0.33074436417545183, 0.32888242422816266, 0.33014430769954817, 0.33136501082454817, 0.32960455101656627, 0.33172092667545183, 0.3311002800263554, 0.32901478962725905, 0.33123264542545183, 0.33098850480045183, 0.33096791462725905, 0.3309782097138554, 0.3291059746799699, 0.32924863516566266, 0.33185329207454817, 0.3313444206513554, 0.3303678581513554, 0.3313444206513554, 0.33025608292545183, 0.33023549275225905, 0.33035756306475905, 0.3318327019013554, 0.33010312735316266, 0.32985898672816266, 0.32912656485316266, 0.33011342243975905, 0.32888242422816266, 0.32997076195406627, 0.32986928181475905, 0.32999135212725905, 0.33246364363704817, 0.33022519766566266, 0.33107968985316266, 0.33011342243975905, 0.3313444206513554, 0.32925893025225905, 0.3309782097138554, 0.33121205525225905, 0.33269748917545183, 0.3304899284638554, 0.3303678581513554, 0.33096791462725905, 0.32937070547816266, 0.33184299698795183, 0.3303678581513554, 0.33072377400225905, 0.3313444206513554, 0.32973691641566266, 0.33035756306475905, 0.3298795769013554, 0.32998105704066266, 0.3314664909638554, 0.32986928181475905, 0.32887212914156627, 0.33258571394954817, 0.33062229386295183, 0.32950307087725905, 0.33022519766566266, 0.33136501082454817, 0.3312223503388554, 0.32950307087725905, 0.3304899284638554, 0.33209743269954817, 0.33145619587725905, 0.3313444206513554, 0.33011342243975905, 0.33023549275225905, 0.33196506730045183, 0.3311002800263554, 0.32998105704066266, 0.33035756306475905, 0.3309782097138554, 0.33035756306475905, 0.32999135212725905, 0.3301237175263554, 0.32962514118975905, 0.33172092667545183, 0.32999135212725905, 0.33133412556475905, 0.33112087019954817, 0.33182240681475905, 0.33159885636295183, 0.33060170368975905, 0.33196506730045183, 0.33123264542545183, 0.32985898672816266, 0.33010312735316266, 0.33074436417545183, 0.32962514118975905], "accuracy_test": 0.32918327487244897, "start": "2016-01-25 16:58:52.042000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0, 211.0, 212.0, 213.0, 214.0, 215.0, 216.0, 217.0, 218.0, 219.0, 220.0, 221.0, 222.0, 223.0, 224.0, 225.0, 226.0, 227.0, 228.0, 229.0], "accuracy_train_last": 0.3339674320551864, "batch_size_eval": 1024, "accuracy_train_std": [0.010984523296901865, 0.012489457389710426, 0.013495601453196393, 0.012870389829338158, 0.013070992576697445, 0.012913272859620671, 0.012100462239579838, 0.01297493306109417, 0.012606946865294438, 0.013438135234000068, 0.014456229233541172, 0.016106008002208884, 0.015328300711644864, 0.0161027222966798, 0.016159474019619535, 0.016150256370935118, 0.015694794290779704, 0.01560382270744993, 0.015728163908078352, 0.015749198841315173, 0.016023168014897084, 0.015860252511819193, 0.015453527064262885, 0.01661623163806296, 0.016920899739526256, 0.01713770583329922, 0.017611804141362045, 0.017762284885929458, 0.017301123176590927, 0.017410939788060013, 0.017161835797249798, 0.017129644777516273, 0.017005317298724728, 0.017512800817650907, 0.01647564902630947, 0.01700535729123653, 0.017270290933444225, 0.017391883748191114, 0.0161991422415803, 0.01667015946970008, 0.01656150116584306, 0.01664908090770954, 0.01577865689649568, 0.01574462338305977, 0.016641736860440914, 0.016284055106907315, 0.016548319706759705, 0.016029266484523925, 0.01565322074980397, 0.016241424070988525, 0.01621314829236274, 0.01570122718588319, 0.01562262315483981, 0.016100067981040054, 0.015470279397957027, 0.015790754636343364, 0.015481474472775298, 0.015551922845539879, 0.01553191704857033, 0.01523925476452681, 0.015315663459054165, 0.015191049205374584, 0.015425797782803308, 0.015285150325255849, 0.015458286628782588, 0.015489283077021656, 0.015402557161058898, 0.016040533725571758, 0.01628004543948013, 0.016052017606768547, 0.015461685169161491, 0.015755025751514575, 0.015492959308220332, 0.015856064815474406, 0.01594967350838308, 0.01596148389662196, 0.01637505683964616, 0.015435776363628083, 0.015566378753113608, 0.015970774061100316, 0.015369162926377003, 0.015372117848183861, 0.016249573746256105, 0.015765554483576732, 0.016095995639357732, 0.015585520939303656, 0.01612652926114941, 0.016411539688001384, 0.015767201904355495, 0.016187432107173796, 0.015535559949610731, 0.016026692835136594, 0.015489539595677696, 0.01580960801484481, 0.01604563673304717, 0.015256913942364288, 0.015427421720395422, 0.01599873958738324, 0.015487823164897873, 0.015806710259291232, 0.015465363468261557, 0.015492289103833013, 0.015323257055750442, 0.015615542786529638, 0.015441416192438766, 0.015807786567792994, 0.015952029103071776, 0.01574347230899379, 0.015585875882992574, 0.01610794068131935, 0.015620219327850566, 0.015309446063133123, 0.015531075410817234, 0.015349258290569363, 0.015139375721560516, 0.015845341530689352, 0.015680295888856256, 0.015861520764919923, 0.016130133986915082, 0.015597981614868571, 0.01563780903439642, 0.015327830422843936, 0.015096467491975659, 0.015420302130594829, 0.015499975152690942, 0.01566396542384533, 0.014732734200242686, 0.015602548890803519, 0.015707834615909688, 0.015315914066741139, 0.014872799609490095, 0.015393710759848776, 0.01556531090671876, 0.0155096562402655, 0.015237739935622179, 0.015490570398068206, 0.015515432186714752, 0.015214708507508504, 0.015250909884592512, 0.015762490137381735, 0.015762081483608793, 0.015743737720442175, 0.015699752185562327, 0.01563746487619016, 0.015599118514975868, 0.015826696946680766, 0.015254031965494986, 0.015038501450338035, 0.015500668162519467, 0.015065740788714248, 0.015228877165714996, 0.015421332824111894, 0.015585954609232953, 0.014854446026317492, 0.01604339540401913, 0.015741161250353068, 0.015146547537369659, 0.01530024632725636, 0.015520046853952503, 0.015400635251771043, 0.015729741848567562, 0.015422564403594224, 0.015849777088193774, 0.01579769674403124, 0.015334250789428048, 0.015334491843462986, 0.01484694969062739, 0.015499644378502014, 0.015218204869422458, 0.015573880802271204, 0.015747692774883214, 0.015067827571585233, 0.015567847450805685, 0.015455197132787653, 0.01598984457236278, 0.015057190421404004, 0.015459605683609996, 0.01618279997331313, 0.016366674972499492, 0.015246994967756482, 0.015676402953281758, 0.015734902789301147, 0.015580511782877487, 0.015242768941106082, 0.015864966206564263, 0.0149797393403522, 0.015943630540449324, 0.015684333310930037, 0.015488499057069985, 0.015810938496013133, 0.01538182880134405, 0.015297710089283878, 0.015768976570334516, 0.015967057159645383, 0.015809990102250367, 0.016124888133549203, 0.015160394733051872, 0.016236668182358485, 0.015468586817509216, 0.015596070610216268, 0.01572010343087395, 0.01524578109296732, 0.015553943912591952, 0.01533187612265865, 0.015737153904206852, 0.015610711014062827, 0.015810538812416605, 0.015901750297063956, 0.015789428892554077, 0.015295899535553142, 0.016228142170048754, 0.015612024718462625, 0.015412888067056894, 0.015369895886740469, 0.015577123387721721, 0.015391543358221335, 0.015925180628696172, 0.01562972189884541, 0.015324073868224685, 0.015726666895718812, 0.015319725599600853, 0.015640150608484723, 0.015098266346248616, 0.015397395321519711, 0.01578061505714461, 0.015751829498325334, 0.01538204844531885, 0.015181204004950467, 0.015364917679833655, 0.01594211445363706], "accuracy_test_std": 0.01305783267282877, "error_valid": [0.8724203454442772, 0.8673948724585843, 0.8573542215737951, 0.8352786144578314, 0.8215655591114458, 0.8127456113516567, 0.8047801557793675, 0.8014842573418675, 0.7925731245293675, 0.7844650084713856, 0.7765598526920181, 0.7710152131965362, 0.763049757624247, 0.7558064288403614, 0.7484513248305723, 0.7446774402296686, 0.7427037250564759, 0.7406182346573795, 0.7312894154743976, 0.728492093373494, 0.7234563253012049, 0.7207810735128012, 0.7156335302146084, 0.7157761907003012, 0.7122052663780121, 0.7109845632530121, 0.7054399237575302, 0.7052060782191265, 0.7044736563441265, 0.7027852621423193, 0.7010762777673193, 0.6969155920557228, 0.6943212302334337, 0.6947080313441265, 0.6928975668298193, 0.6923989904932228, 0.6898046286709337, 0.6899266989834337, 0.687098491622741, 0.6856439429593373, 0.6857763083584337, 0.684901225997741, 0.683680522872741, 0.6830907614834337, 0.6821039038968373, 0.681239116622741, 0.6805169898343373, 0.6805169898343373, 0.6791742163968373, 0.679530132247741, 0.6790315559111446, 0.6775667121611446, 0.6773225715361446, 0.676234233810241, 0.6780858786709337, 0.6773431617093373, 0.6758577277861446, 0.6756032920745482, 0.675868022872741, 0.6748811652861446, 0.6756135871611446, 0.675257671310241, 0.676112163497741, 0.676112163497741, 0.675013530685241, 0.6747590949736446, 0.6739046027861446, 0.6753900367093373, 0.6734163215361446, 0.6731721809111446, 0.673670757247741, 0.672938335372741, 0.6739251929593373, 0.674159038497741, 0.672450054122741, 0.6728059699736446, 0.672572124435241, 0.6729486304593373, 0.6723485739834337, 0.671107280685241, 0.671961772872741, 0.671229350997741, 0.6712190559111446, 0.6718294074736446, 0.671839702560241, 0.6708528449736446, 0.670985210372741, 0.6714837867093373, 0.671107280685241, 0.670008647872741, 0.672205913497741, 0.6699983527861446, 0.6708734351468373, 0.6707513648343373, 0.6713411262236446, 0.670618999435241, 0.670374858810241, 0.6693880012236446, 0.6714940817959337, 0.6702630835843373, 0.671107280685241, 0.6688791298004518, 0.670374858810241, 0.669520366622741, 0.6711278708584337, 0.6711175757718373, 0.6709955054593373, 0.6692659309111446, 0.6693880012236446, 0.6697542121611446, 0.6713823065700302, 0.6701204230986446, 0.669764507247741, 0.669398296310241, 0.670374858810241, 0.6702424934111446, 0.6708528449736446, 0.6697748023343373, 0.6702424934111446, 0.6697542121611446, 0.6703954489834337, 0.6710160956325302, 0.6697439170745482, 0.670008647872741, 0.669642436935241, 0.6698968726468373, 0.6691438605986446, 0.668787944747741, 0.6691644507718373, 0.6694085913968373, 0.6691438605986446, 0.6686349891754518, 0.669032085372741, 0.6698968726468373, 0.6692556358245482, 0.6700292380459337, 0.6710058005459337, 0.6686555793486446, 0.6685232139495482, 0.6682790733245482, 0.669398296310241, 0.6692556358245482, 0.6711175757718373, 0.6698556923004518, 0.6686349891754518, 0.6703954489834337, 0.6682790733245482, 0.6688997199736446, 0.670985210372741, 0.6687673545745482, 0.6690114951995482, 0.669032085372741, 0.6690217902861446, 0.6708940253200302, 0.6707513648343373, 0.6681467079254518, 0.6686555793486446, 0.6696321418486446, 0.6686555793486446, 0.6697439170745482, 0.669764507247741, 0.669642436935241, 0.6681672980986446, 0.6698968726468373, 0.6701410132718373, 0.6708734351468373, 0.669886577560241, 0.6711175757718373, 0.6700292380459337, 0.670130718185241, 0.670008647872741, 0.6675363563629518, 0.6697748023343373, 0.6689203101468373, 0.669886577560241, 0.6686555793486446, 0.670741069747741, 0.6690217902861446, 0.668787944747741, 0.6673025108245482, 0.6695100715361446, 0.6696321418486446, 0.669032085372741, 0.6706292945218373, 0.6681570030120482, 0.6696321418486446, 0.669276225997741, 0.6686555793486446, 0.6702630835843373, 0.669642436935241, 0.6701204230986446, 0.6700189429593373, 0.6685335090361446, 0.670130718185241, 0.6711278708584337, 0.6674142860504518, 0.6693777061370482, 0.670496929122741, 0.6697748023343373, 0.6686349891754518, 0.6687776496611446, 0.670496929122741, 0.6695100715361446, 0.6679025673004518, 0.668543804122741, 0.6686555793486446, 0.669886577560241, 0.669764507247741, 0.6680349326995482, 0.6688997199736446, 0.6700189429593373, 0.669642436935241, 0.6690217902861446, 0.669642436935241, 0.670008647872741, 0.6698762824736446, 0.670374858810241, 0.6682790733245482, 0.670008647872741, 0.668665874435241, 0.6688791298004518, 0.668177593185241, 0.6684011436370482, 0.669398296310241, 0.6680349326995482, 0.6687673545745482, 0.6701410132718373, 0.6698968726468373, 0.6692556358245482, 0.670374858810241], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "momentum": 0.583570243496081, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.00016908875010526465, "patience_threshold": 1, "do_flip": true, "batch_size": 32, "l2_decay": 2.9274790135765435e-05, "optimization": "adadelta", "nb_data_augmentation": 1, "learning_rate_decay_method": "exp", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.045382532985372995}, "accuracy_valid_max": 0.33269748917545183, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        #nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        nb_data_augmentation=make_constant_param(1),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.32962514118975905, "loss_train": [2.568315267562866, 2.52327036857605, 2.491773843765259, 2.4671790599823, 2.4466660022735596, 2.426469326019287, 2.4073617458343506, 2.3890719413757324, 2.370792865753174, 2.3522088527679443, 2.333911895751953, 2.3155391216278076, 2.298610210418701, 2.283078193664551, 2.2682604789733887, 2.2532806396484375, 2.240009069442749, 2.2287096977233887, 2.2176506519317627, 2.2069780826568604, 2.197890281677246, 2.1891088485717773, 2.180720567703247, 2.173307418823242, 2.1659300327301025, 2.1591479778289795, 2.1537487506866455, 2.1485342979431152, 2.142821788787842, 2.1373956203460693, 2.1327810287475586, 2.128412961959839, 2.1246142387390137, 2.1214616298675537, 2.1176023483276367, 2.114161968231201, 2.111182928085327, 2.108715057373047, 2.1050198078155518, 2.1029884815216064, 2.099961280822754, 2.097621440887451, 2.09596586227417, 2.0938146114349365, 2.091179609298706, 2.089486837387085, 2.0874791145324707, 2.08648419380188, 2.084726572036743, 2.0828332901000977, 2.081263542175293, 2.0803542137145996, 2.079029083251953, 2.0777149200439453, 2.0765748023986816, 2.0752317905426025, 2.074446439743042, 2.0728871822357178, 2.0723085403442383, 2.0714268684387207, 2.070413827896118, 2.070087432861328, 2.069347381591797, 2.0686628818511963, 2.0680339336395264, 2.066138744354248, 2.066255569458008, 2.065763235092163, 2.06565260887146, 2.064584732055664, 2.0641212463378906, 2.0630807876586914, 2.0631062984466553, 2.0625998973846436, 2.061969518661499, 2.061480760574341, 2.0610086917877197, 2.0616798400878906, 2.0614867210388184, 2.060258388519287, 2.0593929290771484, 2.059536933898926, 2.059990882873535, 2.0586934089660645, 2.058776617050171, 2.0589821338653564, 2.058171510696411, 2.0586516857147217, 2.0581881999969482, 2.058418035507202, 2.0570011138916016, 2.057673692703247, 2.057816982269287, 2.0573036670684814, 2.0571815967559814, 2.056185483932495, 2.0562918186187744, 2.0563769340515137, 2.056342840194702, 2.056227684020996, 2.055612325668335, 2.0561368465423584, 2.055886745452881, 2.054725408554077, 2.0549168586730957, 2.054905414581299, 2.0552234649658203, 2.0556349754333496, 2.0549421310424805, 2.0552449226379395, 2.0549609661102295, 2.0554778575897217, 2.055119514465332, 2.055483341217041, 2.0549402236938477, 2.054506301879883, 2.053678274154663, 2.054739475250244, 2.0546114444732666, 2.0545339584350586, 2.0543878078460693, 2.0552115440368652, 2.0539610385894775, 2.0549211502075195, 2.054264545440674, 2.0545403957366943, 2.0543813705444336, 2.0539329051971436, 2.0546793937683105, 2.0548148155212402, 2.0539348125457764, 2.054173469543457, 2.0546371936798096, 2.054370880126953, 2.0547218322753906, 2.0543010234832764, 2.0549168586730957, 2.054265260696411, 2.0544252395629883, 2.053203582763672, 2.0544986724853516, 2.054666519165039, 2.054105520248413, 2.0537211894989014, 2.054917573928833, 2.0547733306884766, 2.0541305541992188, 2.054614305496216, 2.054154396057129, 2.053856134414673, 2.054234027862549, 2.0548512935638428, 2.053283929824829, 2.054643392562866, 2.054534912109375, 2.0547943115234375, 2.0538103580474854, 2.053711414337158, 2.054354190826416, 2.0539491176605225, 2.0536723136901855, 2.053953170776367, 2.0546631813049316, 2.0544850826263428, 2.0543854236602783, 2.0536680221557617, 2.054504871368408, 2.054494619369507, 2.05391788482666, 2.0544912815093994, 2.053251028060913, 2.0548698902130127, 2.0538671016693115, 2.0545661449432373, 2.053844928741455, 2.0544238090515137, 2.0543019771575928, 2.054492712020874, 2.054464101791382, 2.053969144821167, 2.0543596744537354, 2.054105281829834, 2.05511474609375, 2.0543434619903564, 2.0545454025268555, 2.0547218322753906, 2.0544378757476807, 2.0541958808898926, 2.054368495941162, 2.0548150539398193, 2.054569959640503, 2.0545458793640137, 2.054368734359741, 2.0544474124908447, 2.0539093017578125, 2.0543391704559326, 2.053635597229004, 2.054610013961792, 2.0540342330932617, 2.0540170669555664, 2.055140256881714, 2.054180383682251, 2.054715871810913, 2.055082082748413, 2.0545427799224854, 2.054023265838623, 2.0543839931488037, 2.0545167922973633, 2.053793430328369, 2.054248094558716, 2.053600549697876, 2.054027557373047, 2.0538690090179443, 2.054203510284424, 2.054194450378418, 2.0543429851531982, 2.054928779602051, 2.0551133155822754, 2.0543103218078613, 2.054527521133423, 2.054788589477539, 2.0542795658111572, 2.054872989654541, 2.054168701171875, 2.053809881210327, 2.05405330657959, 2.05472469329834, 2.053710699081421, 2.054615020751953, 2.0545997619628906], "accuracy_train_first": 0.12381687776854927, "model": "residualv5", "loss_std": [0.09421190619468689, 0.0814073383808136, 0.07493698596954346, 0.07119309157133102, 0.06969646364450455, 0.06876671314239502, 0.06986486911773682, 0.06947208940982819, 0.07048190385103226, 0.0712272971868515, 0.07231167703866959, 0.07266981899738312, 0.07318945229053497, 0.0738803893327713, 0.07592805474996567, 0.0762074664235115, 0.07726041227579117, 0.07858288288116455, 0.07887186110019684, 0.08083935081958771, 0.0820498839020729, 0.08247265219688416, 0.08413802087306976, 0.08558396995067596, 0.08583544194698334, 0.08690039068460464, 0.0874071940779686, 0.08763916790485382, 0.08860175311565399, 0.08940918743610382, 0.08937519043684006, 0.09008428454399109, 0.09104838967323303, 0.09182596951723099, 0.0913901999592781, 0.09251830726861954, 0.09227975457906723, 0.09306062757968903, 0.09308132529258728, 0.09378831833600998, 0.09399963170289993, 0.0952502116560936, 0.09480803459882736, 0.09575265645980835, 0.09444494545459747, 0.09533049911260605, 0.09518937766551971, 0.09617317467927933, 0.09628940373659134, 0.09646085649728775, 0.09619844704866409, 0.0963672548532486, 0.09611819684505463, 0.0969575047492981, 0.09706657379865646, 0.09757735580205917, 0.09839688986539841, 0.09746882319450378, 0.09759700298309326, 0.09754740446805954, 0.09762850403785706, 0.09763258695602417, 0.09808670729398727, 0.09845460951328278, 0.09798143804073334, 0.09812285751104355, 0.09796611219644547, 0.09895630925893784, 0.0986948311328888, 0.09818208962678909, 0.09844660758972168, 0.09884695708751678, 0.09874599426984787, 0.09847717732191086, 0.09875033050775528, 0.09883170574903488, 0.09908923506736755, 0.09891042858362198, 0.09877181053161621, 0.09902478754520416, 0.09876547008752823, 0.09893433749675751, 0.09843558818101883, 0.0986790806055069, 0.09914116561412811, 0.0989636555314064, 0.09828142076730728, 0.09889078140258789, 0.09997693449258804, 0.10017865151166916, 0.09967981278896332, 0.09953905642032623, 0.09875913709402084, 0.099964439868927, 0.09989739954471588, 0.09939084947109222, 0.10004735738039017, 0.09981873631477356, 0.09931934624910355, 0.0995236486196518, 0.09959107637405396, 0.09955236315727234, 0.09956482797861099, 0.09959474951028824, 0.09938227385282516, 0.09974028170108795, 0.09891191869974136, 0.09928043931722641, 0.09876669198274612, 0.09940258413553238, 0.09967701137065887, 0.09872432053089142, 0.09979655593633652, 0.09944790601730347, 0.09919600933790207, 0.09906523674726486, 0.09951335936784744, 0.09863846004009247, 0.09947234392166138, 0.09947918355464935, 0.09977946430444717, 0.09946857392787933, 0.09962311387062073, 0.09939418733119965, 0.09962204098701477, 0.09997183829545975, 0.09885042160749435, 0.09932658076286316, 0.09930688142776489, 0.09928000718355179, 0.09937452524900436, 0.09933893382549286, 0.09994657337665558, 0.0994279757142067, 0.09991039335727692, 0.09931054711341858, 0.09975741803646088, 0.09962209314107895, 0.09883280098438263, 0.09912797808647156, 0.10006178915500641, 0.0993255078792572, 0.09899155050516129, 0.09983676671981812, 0.0997031182050705, 0.09954887628555298, 0.09896215051412582, 0.09893163293600082, 0.09937433898448944, 0.09936004877090454, 0.09998534619808197, 0.10001438856124878, 0.09977181255817413, 0.09977508336305618, 0.09997478127479553, 0.09937432408332825, 0.0988372340798378, 0.0989578366279602, 0.09951230883598328, 0.09941816329956055, 0.09969296306371689, 0.09916731715202332, 0.09929025173187256, 0.09969784319400787, 0.09961195290088654, 0.09972681105136871, 0.09932903200387955, 0.09938737750053406, 0.09980697929859161, 0.0992828905582428, 0.09992358833551407, 0.09936880320310593, 0.09859056025743484, 0.09983617067337036, 0.09978576749563217, 0.09989132732152939, 0.0995359793305397, 0.0994470864534378, 0.10032004117965698, 0.09950225800275803, 0.1000109612941742, 0.09921549260616302, 0.10053100436925888, 0.10066478699445724, 0.09961646050214767, 0.09955663979053497, 0.09973752498626709, 0.09943516552448273, 0.10058096796274185, 0.10006601363420486, 0.10010398179292679, 0.09966479241847992, 0.0998748317360878, 0.10004207491874695, 0.09961008280515671, 0.09904705733060837, 0.09945507347583771, 0.09949462115764618, 0.09948202967643738, 0.09851342439651489, 0.09966137260198593, 0.09902012348175049, 0.09960626065731049, 0.09976319968700409, 0.09981167316436768, 0.09942004829645157, 0.10017525404691696, 0.09965009987354279, 0.09967661648988724, 0.09992894530296326, 0.09955663979053497, 0.0996936559677124, 0.10009371489286423, 0.10013025254011154, 0.09978783875703812, 0.09922080487012863, 0.10016157478094101, 0.1000727042555809, 0.09994059801101685, 0.09945551306009293, 0.10020940750837326, 0.09970293194055557, 0.0987095832824707, 0.0984041690826416, 0.09989003092050552, 0.09985004365444183, 0.09969951957464218, 0.0998694896697998, 0.09976267069578171, 0.0997212752699852]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:19 2016", "state": "available"}], "summary": "8969ba3fed82dc8f5fe349c34def18e5"}