{"content": {"hp_model": {"f0": 16, "f1": 64, "f2": 16, "f3": 16, "nonlin": "very_leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.6094774007797241, 1.234848976135254, 0.991865873336792, 0.8673182129859924, 0.7880776524543762, 0.7312046885490417, 0.6899619698524475, 0.6552332639694214, 0.6291307210922241, 0.6064568758010864, 0.5868836641311646, 0.5676209926605225, 0.5550378561019897, 0.5421571135520935, 0.5305036902427673, 0.5201190710067749, 0.508430540561676, 0.5002304911613464, 0.49131327867507935, 0.48279082775115967, 0.47819191217422485, 0.47082436084747314, 0.465270072221756, 0.45728635787963867, 0.4528043568134308, 0.44847071170806885, 0.4440438747406006, 0.43890973925590515, 0.4321947991847992, 0.4306827485561371, 0.42749616503715515, 0.4230821132659912, 0.41876789927482605, 0.41558972001075745, 0.41141417622566223, 0.4078749120235443, 0.4050530195236206, 0.4017127454280853, 0.39921504259109497, 0.39706307649612427, 0.3944278657436371, 0.3898480534553528, 0.3893469274044037, 0.38612547516822815, 0.38327690958976746, 0.38072827458381653, 0.3787989020347595, 0.37647557258605957, 0.37438488006591797, 0.3721368610858917, 0.37138378620147705, 0.3685932457447052, 0.36676040291786194, 0.36339887976646423, 0.3644389510154724, 0.3608993589878082, 0.3581114709377289, 0.3557732105255127, 0.3550439476966858, 0.3530598282814026, 0.35190117359161377, 0.34943315386772156, 0.3492211401462555, 0.347717821598053, 0.34517183899879456, 0.3456622362136841, 0.34385791420936584, 0.34165889024734497, 0.33917906880378723, 0.3394206464290619, 0.33735302090644836, 0.33579695224761963, 0.33407726883888245, 0.3354647755622864, 0.331856906414032, 0.33159151673316956, 0.3307546377182007, 0.3296318054199219, 0.32985877990722656, 0.32735663652420044, 0.32456174492836, 0.325077086687088, 0.32507678866386414, 0.32177627086639404, 0.3213144540786743, 0.31999671459198, 0.320159912109375, 0.31706473231315613, 0.31643301248550415, 0.3172336220741272, 0.31639695167541504, 0.3144740164279938, 0.3149469494819641, 0.31094804406166077, 0.312439888715744, 0.311353862285614, 0.3097880780696869, 0.31076183915138245, 0.3068625032901764, 0.30872344970703125, 0.30647149682044983, 0.3045331537723541, 0.3025910556316376, 0.3028968870639801, 0.30147573351860046, 0.3012491762638092, 0.2999717891216278, 0.30231186747550964, 0.30039912462234497, 0.29934394359588623, 0.2976512610912323, 0.29558464884757996, 0.2963995039463043, 0.2957925498485565, 0.29539600014686584, 0.29662764072418213, 0.294514536857605, 0.2912527620792389, 0.29123935103416443, 0.28969842195510864, 0.29173544049263, 0.2902427613735199, 0.2911968231201172, 0.2894226908683777, 0.2879848778247833, 0.2881726324558258, 0.2883062958717346, 0.2871028184890747, 0.28528255224227905, 0.2859272062778473, 0.28726285696029663, 0.28446725010871887, 0.28351348638534546, 0.281760573387146, 0.2837921977043152, 0.2829248905181885, 0.2831336557865143, 0.2800953686237335, 0.281319260597229, 0.2785320281982422, 0.2794252336025238, 0.27807262539863586, 0.2774159908294678, 0.2783787250518799, 0.27826738357543945, 0.27660271525382996, 0.2754303812980652, 0.27530181407928467, 0.2764627933502197, 0.2742791175842285, 0.2753862142562866, 0.27285802364349365, 0.274399071931839, 0.273555725812912, 0.27136707305908203, 0.27189528942108154, 0.2721962332725525, 0.2691221833229065, 0.2694007456302643, 0.26973170042037964, 0.2696984112262726, 0.26735928654670715, 0.26952600479125977, 0.26890310645103455, 0.26679909229278564, 0.2677035331726074, 0.26563555002212524, 0.2665032148361206, 0.2644500136375427, 0.26530522108078003, 0.2668772339820862, 0.2661396563053131, 0.2639709413051605, 0.26456183195114136, 0.2639443278312683, 0.26375705003738403, 0.2634550929069519, 0.2617569863796234, 0.2635190188884735, 0.2613392770290375, 0.26114606857299805, 0.25998902320861816, 0.26046988368034363, 0.25960972905158997, 0.2590078115463257, 0.2598881125450134, 0.2601431608200073, 0.2603214979171753, 0.2603456974029541, 0.2551783323287964, 0.25925710797309875, 0.25786909461021423, 0.25752612948417664], "moving_avg_accuracy_train": [0.05185431518779991, 0.10724128123269654, 0.16556226676414262, 0.22269636847804763, 0.2771785960751487, 0.3282977907600388, 0.3756652419811925, 0.41946739049440585, 0.4599309908229645, 0.496831753924592, 0.5306585330529431, 0.5618139856088983, 0.5900841907878571, 0.61598532161794, 0.6397287809947672, 0.6614698100969792, 0.681276226616351, 0.6995205643183188, 0.7162241724536703, 0.7314223611456879, 0.745591373416132, 0.758513076127352, 0.7705681828972303, 0.781594418202007, 0.7915737614500972, 0.8007991668031033, 0.8093787603779609, 0.81713774117156, 0.8243485803226764, 0.8310149387217488, 0.8371634708047233, 0.8427158229675142, 0.8478314504056743, 0.8526913175178846, 0.8572000565498263, 0.8613462052356983, 0.8651987188887158, 0.8686962081109553, 0.8720461642597619, 0.875144794102012, 0.8780846595838373, 0.8808096296258227, 0.8833156171350472, 0.8857012142266827, 0.8880085426817368, 0.8901155094210843, 0.8921069663924125, 0.8939527921380458, 0.8957465687912586, 0.897516788798207, 0.899140105592528, 0.9006406903348164, 0.9020470562231233, 0.9034221035654658, 0.9047270033914128, 0.906015309477613, 0.9072026506920887, 0.9082712217362982, 0.909437512722506, 0.9103709532184354, 0.9113389328492958, 0.9122635568908705, 0.9131934468759252, 0.9140744535922178, 0.9148231818095003, 0.915722612688397, 0.9165017293496054, 0.91730298784114, 0.9181264991287775, 0.9189164153150138, 0.9195761505599982, 0.9202373415959604, 0.9208952285950023, 0.9215872722441307, 0.9221915824355076, 0.9228469966529664, 0.9234879866736702, 0.9240254222601791, 0.9245463166689896, 0.9251220424333383, 0.9257611033593474, 0.926280454621327, 0.926880332141614, 0.9273830195289199, 0.9279378168227518, 0.9284231474455246, 0.9288110447833826, 0.9292183171565117, 0.9296196674268332, 0.9300182292463499, 0.9303280346612774, 0.9307092742287876, 0.9311871403240428, 0.9315962213728494, 0.9320039218465371, 0.9324358843418853, 0.932822469634164, 0.9332587160031581, 0.9336698668280915, 0.9339887492967219, 0.9343501843292129, 0.9347638675620354, 0.9351431218691856, 0.9354751141015638, 0.935759956217847, 0.9361418361093975, 0.9364715771189357, 0.9367149377025386, 0.9370525087682481, 0.9374237159940442, 0.9378298460615372, 0.938114055011585, 0.93835360307378, 0.9386969713678233, 0.9390246761205758, 0.9392080393040148, 0.9395030582095306, 0.939761635826885, 0.9401176246182276, 0.9404565796720933, 0.940678005961067, 0.9410353636913723, 0.9413638889974381, 0.9417154013931447, 0.9419458041409656, 0.9421973083925668, 0.9424771045928081, 0.9427823635468255, 0.9430547354078129, 0.9432952558339014, 0.9435861650281044, 0.9438456942028964, 0.9441071361971047, 0.9443146403526342, 0.9445431025759072, 0.9447627055185288, 0.945034752928793, 0.9452587053075638, 0.9454439503579721, 0.94555951762953, 0.9457402941334653, 0.9459238832774739, 0.9461146901439863, 0.9463421477976386, 0.9466468771335539, 0.9468049842418578, 0.9470797420238366, 0.9472294398752551, 0.9474199715129603, 0.9476331584701914, 0.9478715297078898, 0.9480164175039889, 0.9482234382847362, 0.9483911918457513, 0.9485700718363792, 0.948777530755316, 0.948845733290711, 0.9489931100297001, 0.9491838778150283, 0.9493719169611279, 0.9495294903997511, 0.9495178106242648, 0.9496863352846604, 0.9498776071064158, 0.9501031220221585, 0.9502457036749169, 0.9504158798409708, 0.9505574126463717, 0.9506499870367272, 0.9508472362797138, 0.9509758603757642, 0.9510799963181619, 0.9512085598496439, 0.9514079363363019, 0.9515850500254847, 0.9517537529409872, 0.9518893095232728, 0.9520135995473206, 0.9520766684927824, 0.9522170998520221, 0.9523342235289184, 0.952523340195268, 0.9526773052021347, 0.9527019414166481, 0.9528705623358913, 0.9530362360072486, 0.9532853237102797, 0.9534211830370646, 0.9535527930752279, 0.9536502436726516, 0.9537984391281993, 0.9538690720691537, 0.9541302433160036], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.05128585631588854, 0.10708491681570027, 0.16517995958443144, 0.2221572631835937, 0.2759495729156861, 0.3260778131015271, 0.3722247969457419, 0.41426977771803514, 0.4530502018193491, 0.48836056308507986, 0.5202354854569182, 0.549510912108892, 0.575997191474057, 0.6000678060051152, 0.6221108065604772, 0.6422404167929837, 0.6604842838580377, 0.6770827516506979, 0.6920935853429323, 0.7055423005096932, 0.7179736749862089, 0.7292636862997115, 0.7398041439592735, 0.7493017333754697, 0.7579492791173655, 0.7655133732312314, 0.7728683153226413, 0.7792334745659796, 0.7852795006974841, 0.7906334159797688, 0.7954916493535539, 0.7997581670706835, 0.8040436631930579, 0.8074082104185563, 0.8109510572513241, 0.8141182938642941, 0.8169443927534671, 0.8195234733388131, 0.822023633299736, 0.8241792095405456, 0.8263511617510242, 0.8284269595442952, 0.8303491532265374, 0.8320750095059167, 0.8337168179020871, 0.8351822384273904, 0.8365886251362327, 0.8378502551395522, 0.8390844079011994, 0.8402042638919529, 0.8413149380769293, 0.8424986798208176, 0.8434165335066575, 0.8444745354176635, 0.8454043820923881, 0.8461537358635709, 0.8467559415787951, 0.8474311745575872, 0.8479544645284098, 0.8485597028459002, 0.8490301456354818, 0.8496163535869938, 0.8499323031774662, 0.8502553379199604, 0.8504697384747264, 0.8509964068524044, 0.8514184917413357, 0.8518970539000333, 0.8523409963827709, 0.8525818532109848, 0.8527497962313774, 0.8530962574497306, 0.8530753941765196, 0.8531919240830393, 0.8533212150614071, 0.8536095048880977, 0.853735717897029, 0.8538472505877478, 0.8541307354781447, 0.8543380732631615, 0.8544880561759267, 0.8545711241464365, 0.8547191275073953, 0.8549042471832371, 0.8551207125251543, 0.8550784501958317, 0.8551035082730105, 0.8551006169713118, 0.855232292143533, 0.8553793318956707, 0.855474017070185, 0.8554971690623382, 0.8557031703413454, 0.8557278210688825, 0.8557235336438466, 0.8557461480411336, 0.8559160738996708, 0.8562552011670832, 0.8562867134769562, 0.8563750802034322, 0.85652991146208, 0.8565970469160227, 0.8565577535572517, 0.8566200457843579, 0.8566862568026842, 0.8567133436600664, 0.8568078754932315, 0.8567963274017396, 0.8568713833381469, 0.8569898208232328, 0.8570811190025812, 0.8571032817164045, 0.8572219139175049, 0.8572310266484954, 0.8572504056289771, 0.85744080416623, 0.8574208213117306, 0.8574588714282985, 0.8574920870245499, 0.8575087445212667, 0.8573772518933116, 0.8575162856930618, 0.857580380956587, 0.8576279186798289, 0.8577418858009274, 0.8579574080171449, 0.8577291323886231, 0.8577189968229536, 0.8576773717547396, 0.8576388796846873, 0.8578138153702096, 0.85761622407227, 0.8576815030204648, 0.85769142594884, 0.8579130236589259, 0.8578997945234549, 0.8579367164265311, 0.8581418740854594, 0.8584171127405429, 0.8584155393618199, 0.8584304483868578, 0.858379742827163, 0.8586423721306665, 0.8588400583927505, 0.8588704621449664, 0.8589354761243704, 0.8589573676120839, 0.8588794137010262, 0.8587217469450049, 0.8586663255919954, 0.8586052688516964, 0.858670329080608, 0.8585213637553786, 0.858436123087672, 0.8584723582939651, 0.8584164322348999, 0.8583936013702201, 0.858606016694418, 0.8586242330313769, 0.8586904853682995, 0.8587999701051894, 0.8586990758337517, 0.8587913764582078, 0.8588276779125376, 0.8587240128603651, 0.8587639621485004, 0.8587612363967527, 0.8586886295586588, 0.858572396262055, 0.858581767611, 0.8586909466010295, 0.8586508123136675, 0.8586666081060206, 0.8585689020205691, 0.8585186171460725, 0.8585058638181369, 0.8585818940590642, 0.8587235634633987, 0.8585713337172095, 0.8585696337980487, 0.8586556121068734, 0.8587176970275866, 0.8586779762235478, 0.8586645825450936, 0.8587634210243945, 0.858703832263446, 0.8587132965521617, 0.8586984298581654, 0.8585354769325897, 0.8586228119106409, 0.8585406629673178, 0.8584656994096673, 0.8585976627424204], "moving_var_accuracy_train": [0.024199830032361275, 0.04938929109805194, 0.07506239816847907, 0.09693490855952477, 0.11395623581905323, 0.12607916082423315, 0.13366432365850522, 0.13756554522201708, 0.1385447172637594, 0.13694524239472536, 0.13354897703103705, 0.12893003934363073, 0.12322987591701152, 0.11694470552980399, 0.11032400174543501, 0.10354565268870061, 0.09672173463788017, 0.09004526389774312, 0.08355183223062326, 0.07727551346322449, 0.07135481029538202, 0.06572206287445816, 0.06045778698011084, 0.05550620906706651, 0.05085187378532869, 0.04653265934214129, 0.04254187824111481, 0.03882950646360231, 0.035414521628611535, 0.03227303247449432, 0.029385969248023204, 0.02672482985407772, 0.024287873665444444, 0.022071651074035095, 0.020047444515554975, 0.018197415004327696, 0.016511250256915102, 0.014970217108960727, 0.013574195253855002, 0.012303189290563048, 0.0111506556429678, 0.010102419234238482, 0.009148697071382146, 0.008285047025596511, 0.007504456204432384, 0.006793964363555597, 0.006150261035017904, 0.005565898585665297, 0.005038267439233269, 0.004562643805166944, 0.004130095841382774, 0.003737352048363594, 0.0033814176286333726, 0.0030602926625131846, 0.0027695882682636738, 0.0025075670345829706, 0.0022694983435610073, 0.0020528251058936106, 0.0018597847072848348, 0.0016816480369913223, 0.0015219160943840363, 0.0013774188515099535, 0.0012474592248177038, 0.0011296988578433087, 0.0010217743175491728, 0.0009268776689474728, 0.0008396531069986765, 0.0007614659328311137, 0.0006914228771158009, 0.0006278962976357241, 0.0005690239232134233, 0.0005160560931664114, 0.0004683458215813447, 0.00042582155913390096, 0.0003865261204871286, 0.0003517396186064404, 0.0003202634706055722, 0.00029083665663183093, 0.0002641949698348176, 0.00024075861425295022, 0.00022035834263201974, 0.00020075003996869624, 0.00018391371332593783, 0.00016779659347755218, 0.00015378713446498576, 0.00014052833333909654, 0.00012782967910764294, 0.00011653954827010625, 0.00010633533179847997, 9.713146233442052e-05, 8.828213065704452e-05, 8.076201006185785e-05, 7.47410131006225e-05, 6.87730375309938e-05, 6.339171086410131e-05, 5.873186415417758e-05, 5.420371143261638e-05, 5.0496138339499374e-05, 4.6967929513139784e-05, 4.3186310821024696e-05, 4.004339728332792e-05, 3.757926190906159e-05, 3.511584018358247e-05, 3.259622574645949e-05, 3.0066818452692074e-05, 2.8372626871558283e-05, 2.6513926384744372e-05, 2.439555310913391e-05, 2.2981585817859356e-05, 2.19235804764224e-05, 2.121569711427658e-05, 1.9821099948434527e-05, 1.8355439420504106e-05, 1.7581011546640776e-05, 1.678942403676622e-05, 1.5413080146457223e-05, 1.4655097523317385e-05, 1.379134922875595e-05, 1.3552766481933916e-05, 1.3231504590610397e-05, 1.2349620544587583e-05, 1.2263999416809244e-05, 1.2008959365659032e-05, 1.1920112108111356e-05, 1.1205869733131447e-05, 1.0654572256979252e-05, 1.0293688254306548e-05, 1.0102966689946413e-05, 9.760347896871647e-06, 9.304963785476214e-06, 9.136120840375518e-06, 8.828707289451946e-06, 8.561003807527432e-06, 8.092425197832625e-06, 7.752937565214684e-06, 7.411672880365578e-06, 7.3365937332118415e-06, 7.054326371504399e-06, 6.657735292660769e-06, 6.112163911692735e-06, 5.795068819898599e-06, 5.5189067020889916e-06, 5.2946813746549605e-06, 5.230846095034636e-06, 5.543501199037972e-06, 5.214131798400121e-06, 5.372145167381615e-06, 5.036615671117216e-06, 4.859674848705099e-06, 4.78274547243571e-06, 4.81585854784928e-06, 4.52320495419066e-06, 4.456602891722554e-06, 4.2642139176496134e-06, 4.125774985308023e-06, 4.100550314195175e-06, 3.7323595552843455e-06, 3.5546027285114037e-06, 3.5266735869317144e-06, 3.4922347124314124e-06, 3.366475738224368e-06, 3.031055918800638e-06, 2.983555377373666e-06, 3.0144640278151745e-06, 3.1707304200355697e-06, 3.036623127361628e-06, 2.9936001620607246e-06, 2.8745239608964872e-06, 2.664201724553963e-06, 2.747946926827472e-06, 2.622049656907794e-06, 2.457443341708617e-06, 2.3604562421820638e-06, 2.482169468852594e-06, 2.5162758520303577e-06, 2.5207943301186567e-06, 2.434095180115254e-06, 2.32971775280388e-06, 2.1325452044583926e-06, 2.096779383933656e-06, 2.0105630467477974e-06, 2.1313927634936484e-06, 2.1316004971994712e-06, 1.9239029350694636e-06, 1.9874097712205667e-06, 2.035698682527712e-06, 2.390530968486896e-06, 2.3175976817081843e-06, 2.241728732845331e-06, 2.103025430004897e-06, 2.09037992440931e-06, 1.9262430430993593e-06, 2.347512520419394e-06], "duration": 150626.455649, "accuracy_train": [0.5185431518779993, 0.6057239756367663, 0.6904511365471576, 0.736903283903193, 0.7675186444490587, 0.7883705429240495, 0.8019723029715762, 0.813686727113326, 0.8241033937799926, 0.8289386218392396, 0.8350995452081026, 0.8422130586124953, 0.8445160373984865, 0.8490954990886858, 0.8534199153862125, 0.8571390720168882, 0.8595339752906977, 0.8637196036360282, 0.8665566456718347, 0.8682060593738464, 0.8731124838501293, 0.8748084005283315, 0.8790641438261352, 0.8808305359449982, 0.8813878506829088, 0.8838278149801587, 0.8865951025516795, 0.8869685683139534, 0.8892461326827242, 0.8910121643133997, 0.8925002595514949, 0.8926869924326319, 0.893872097349114, 0.8964301215277777, 0.8977787078373015, 0.8986615434085455, 0.8998713417658729, 0.900173611111111, 0.9021957695990217, 0.9030324626822629, 0.9045434489202658, 0.9053343600036915, 0.9058695047180694, 0.9071715880514026, 0.9087744987772242, 0.9090782100752122, 0.910030079134367, 0.9105652238487449, 0.9118905586701735, 0.913448768860742, 0.9137499567414176, 0.9141459530154117, 0.9147043492178849, 0.9157975296465486, 0.9164711018249354, 0.9176100642534146, 0.9178887216223699, 0.9178883611341824, 0.9199341315983758, 0.9187719176818014, 0.9200507495270396, 0.9205851732650425, 0.9215624567414176, 0.922003514038852, 0.9215617357650425, 0.9238174905984681, 0.9235137793004798, 0.9245143142649501, 0.9255381007175157, 0.9260256609911407, 0.9255137677648578, 0.9261880609196198, 0.9268162115863787, 0.9278156650862864, 0.9276303741578996, 0.928745724610096, 0.9292568968600037, 0.9288623425387597, 0.9292343663482835, 0.9303035743124769, 0.9315126516934293, 0.9309546159791436, 0.9322792298241971, 0.9319072060146733, 0.9329309924672389, 0.9327911230504798, 0.9323021208241048, 0.9328837685146733, 0.9332318198597268, 0.9336052856220007, 0.9331162833956257, 0.9341404303363787, 0.93548793518134, 0.9352779508121077, 0.9356732261097268, 0.9363235468000184, 0.9363017372646733, 0.9371849333241048, 0.9373702242524916, 0.9368586915143964, 0.9376030996216316, 0.9384870166574382, 0.9385564106335363, 0.9384630441929678, 0.9383235352643964, 0.9395787551333518, 0.9394392462047803, 0.9389051829549648, 0.9400906483596345, 0.9407645810262089, 0.9414850166689737, 0.9406719355620154, 0.9405095356335363, 0.9417872860142118, 0.9419740188953488, 0.9408583079549648, 0.9421582283591732, 0.942088834383075, 0.9433215237403102, 0.9435071751568845, 0.9426708425618309, 0.9442515832641197, 0.9443206167520304, 0.9448790129545036, 0.9440194288713547, 0.9444608466569768, 0.9449952703949798, 0.9455296941329827, 0.9455060821567, 0.945459939668697, 0.9462043477759321, 0.9461814567760245, 0.9464601141449798, 0.9461821777523993, 0.9465992625853636, 0.9467391320021227, 0.9474831796211702, 0.9472742767165007, 0.9471111558116464, 0.9465996230735512, 0.9473672826688816, 0.9475761855735512, 0.9478319519425988, 0.9483892666805095, 0.9493894411567922, 0.9482279482165927, 0.9495525620616464, 0.9485767205380213, 0.949134756252307, 0.9495518410852714, 0.9500168708471761, 0.9493204076688816, 0.9500866253114618, 0.9499009738948875, 0.9501799917520304, 0.9506446610257475, 0.9494595561092655, 0.9503195006806018, 0.9509007878829827, 0.9510642692760245, 0.9509476513473607, 0.9494126926448875, 0.9512030572282208, 0.951599053502215, 0.9521327562638427, 0.9515289385497416, 0.9519474653354559, 0.9518312078949798, 0.9514831565499261, 0.9526224794665927, 0.9521334772402179, 0.9520172197997416, 0.9523656316329827, 0.9532023247162238, 0.9531790732281286, 0.9532720791805095, 0.9531093187638427, 0.9531322097637505, 0.952644289001938, 0.9534809820851791, 0.9533883366209857, 0.9542253901924143, 0.954062990263935, 0.9529236673472684, 0.9543881506090809, 0.9545272990494648, 0.95552711303756, 0.9546439169781286, 0.954737283418697, 0.9545272990494648, 0.9551321982281286, 0.9545047685377446, 0.9564807845376523], "end": "2016-02-02 09:35:34.424000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0], "moving_var_accuracy_valid": [0.02367215152248569, 0.04932675274419198, 0.07476938341847912, 0.0965101632055112, 0.11290166016178038, 0.12422705832276665, 0.13097024955175396, 0.13378324826986115, 0.13394021508417497, 0.13176758809020525, 0.12773492536708123, 0.12267488828126902, 0.11672110640462822, 0.1102635461192905, 0.10361023636871464, 0.09689602360485686, 0.09020196941375744, 0.08366135466995747, 0.07732314535618501, 0.07121864227729652, 0.06548762969194515, 0.06008604592188178, 0.05507735255875077, 0.05038145514534346, 0.046016330057032734, 0.04192963672920024, 0.03822352961479217, 0.03476581392305044, 0.03161822241859091, 0.02871437985638076, 0.026055363754370123, 0.023613655939808333, 0.021417579638961473, 0.01937770327735881, 0.017552898822747007, 0.015887891430335144, 0.014370983801684094, 0.012993750331507067, 0.011750632496828178, 0.010617387827514842, 0.009598105432404783, 0.008677075317471245, 0.007842621242692586, 0.007085166337496987, 0.006400909517034909, 0.005780145681175241, 0.00521993242523099, 0.004712264575095375, 0.00425474631493757, 0.003840558380404053, 0.0034676049166702046, 0.0031334556256492, 0.0028276921615817694, 0.0025549972578168224, 0.002307279065581608, 0.0020816049386929215, 0.001876708310334668, 0.0016931409354820383, 0.0015262913334759063, 0.0013769590209169425, 0.0012412549665896727, 0.0011202222277924491, 0.0010090984223066807, 0.0009091277430797382, 0.0008186286771527201, 0.0007392622256578616, 0.0006669394039732524, 0.0006023066592335643, 0.0005438497576620212, 0.0004899868900010944, 0.0004412420447238719, 0.00039819815863389, 0.0003583822602560227, 0.0003226662472024419, 0.0002905500678959833, 0.0002622430603239447, 0.00023616212180416142, 0.00021265786549363604, 0.00019211535209202253, 0.00017329071749668158, 0.00015616409961410717, 0.0001406097922422179, 0.0001267459579716919, 0.00011437978582397665, 0.0001033635224398412, 9.304324513617508e-05, 8.37445717876447e-05, 7.537018984550986e-05, 6.79892160197743e-05, 6.138488061617522e-05, 5.5327080095012844e-05, 4.979919621817751e-05, 4.5201205338932994e-05, 4.06865537303527e-05, 3.661806379543837e-05, 3.296086011457643e-05, 2.9924647279715365e-05, 2.7967248283267455e-05, 2.5179460686002503e-05, 2.2731792722535112e-05, 2.067436791817161e-05, 1.8647495648939155e-05, 1.6796641796436755e-05, 1.515190051081368e-05, 1.3676165550262508e-05, 1.2315152275821845e-05, 1.1164063455573553e-05, 1.0048857335770129e-05, 9.094672144502995e-06, 8.311451870913973e-06, 7.555324901793567e-06, 6.8042130845702925e-06, 6.250454168354866e-06, 5.626156128314323e-06, 5.066920419443513e-06, 4.88649280439152e-06, 4.401437354217907e-06, 3.974323921133603e-06, 3.5868210115293153e-06, 3.2306361601481807e-06, 3.063185344991929e-06, 2.9308403877494505e-06, 2.6747301742317245e-06, 2.4275956729877686e-06, 2.3017326479121773e-06, 2.4896078142706736e-06, 2.7096348960364824e-06, 2.439595973655809e-06, 2.2112301930246358e-06, 2.0034419288343573e-06, 2.0785201825731277e-06, 2.2220490535090516e-06, 2.038196217854776e-06, 1.8352627766371537e-06, 2.09368640501121e-06, 1.8858928547378563e-06, 1.7095726116049784e-06, 1.9174223355966135e-06, 2.4074869573069154e-06, 2.166760541261677e-06, 1.9520849983837334e-06, 1.7800159826010354e-06, 2.2227817438699334e-06, 2.352222293433465e-06, 2.125319557429383e-06, 1.950828959347893e-06, 1.7600591985219244e-06, 1.6387445889125802e-06, 1.6985993836098242e-06, 1.5563831825734966e-06, 1.43429619413953e-06, 1.328962075199948e-06, 1.3957818807664727e-06, 1.3215974355693233e-06, 1.2012546035882794e-06, 1.1092786599725138e-06, 1.0030420294134898e-06, 1.3088202560588985e-06, 1.1809247448427757e-06, 1.1023366196877814e-06, 1.0999851262257261e-06, 1.0816034996834879e-06, 1.0501177971901093e-06, 9.569661777492431e-07, 9.579875473515354e-07, 8.765523032189383e-07, 7.889639404003514e-07, 7.575133228023212e-07, 8.033536036766342e-07, 7.2380864293843e-07, 7.587082454193691e-07, 6.973342700759164e-07, 6.298464065728953e-07, 6.52780078123828e-07, 6.10259187739764e-07, 5.506970953266761e-07, 5.476527636131545e-07, 6.735194683724033e-07, 8.147325821588148e-07, 7.332853314693099e-07, 7.264872246174894e-07, 6.885293385754966e-07, 6.338760851793295e-07, 5.72102992264201e-07, 6.02814097952367e-07, 5.744900720395082e-07, 5.178472196835932e-07, 4.680516650286461e-07, 6.60229402108488e-07, 6.628530474185798e-07, 6.573037826786721e-07, 6.421492191913202e-07, 7.346631879940571e-07], "accuracy_test": 0.8290318080357142, "start": "2016-01-31 15:45:07.968000", "learning_rate_per_epoch": [0.0035429070703685284, 0.0017714535351842642, 0.0011809690622612834, 0.0008857267675921321, 0.0007085814140737057, 0.0005904845311306417, 0.0005061295814812183, 0.00044286338379606605, 0.00039365634438581765, 0.00035429070703685284, 0.0003220824582967907, 0.00029524226556532085, 0.00027253132429905236, 0.00025306479074060917, 0.00023619380954187363, 0.00022143169189803302, 0.0002084063016809523, 0.00019682817219290882, 0.00018646879470907152, 0.00017714535351842642, 0.00016870985564310104, 0.00016104122914839536, 0.0001540394441690296, 0.00014762113278266042, 0.00014171628572512418, 0.00013626566214952618, 0.00013121878146193922, 0.00012653239537030458, 0.00012216920731589198, 0.00011809690477093682, 0.0001142873297794722, 0.00011071584594901651, 0.00010736082185758278, 0.00010420315084047616, 0.00010122591629624367, 9.841408609645441e-05, 9.575424337526783e-05, 9.323439735453576e-05, 9.084377234103158e-05, 8.857267675921321e-05, 8.641237218398601e-05, 8.435492782155052e-05, 8.2393191405572e-05, 8.052061457419768e-05, 7.8731267421972e-05, 7.70197220845148e-05, 7.538100180681795e-05, 7.381056639133021e-05, 7.230422488646582e-05, 7.085814286256209e-05, 6.946876965230331e-05, 6.813283107476309e-05, 6.684730760753155e-05, 6.560939073096961e-05, 6.441649020416662e-05, 6.326619768515229e-05, 6.215626490302384e-05, 6.108460365794599e-05, 6.004927490721457e-05, 5.904845238546841e-05, 5.808044443256222e-05, 5.71436648897361e-05, 5.623662218567915e-05, 5.5357922974508256e-05, 5.450626485981047e-05, 5.368041092879139e-05, 5.287921158014797e-05, 5.210157542023808e-05, 5.134648017701693e-05, 5.0612958148121834e-05, 4.990009983885102e-05, 4.9207043048227206e-05, 4.8532972868997604e-05, 4.787712168763392e-05, 4.723876190837473e-05, 4.661719867726788e-05, 4.6011780796106905e-05, 4.542188617051579e-05, 4.484692544792779e-05, 4.4286338379606605e-05, 4.3739593820646405e-05, 4.3206186091993004e-05, 4.268562770448625e-05, 4.217746391077526e-05, 4.1681261791381985e-05, 4.1196595702786e-05, 4.072306910529733e-05, 4.026030728709884e-05, 3.980794645030983e-05, 3.9365633710986e-05, 3.8933045289013535e-05, 3.85098610422574e-05, 3.80957753804978e-05, 3.7690500903408974e-05, 3.729375748662278e-05, 3.6905283195665106e-05, 3.652481609606184e-05, 3.615211244323291e-05, 3.578693940653466e-05, 3.5429071431281045e-05, 3.507828660076484e-05, 3.4734384826151654e-05, 3.4397155104670674e-05, 3.4066415537381545e-05, 3.374197331140749e-05, 3.3423653803765774e-05, 3.311128239147365e-05, 3.2804695365484804e-05, 3.250373629271053e-05, 3.220824510208331e-05, 3.191808355040848e-05, 3.1633098842576146e-05, 3.1353160011349246e-05, 3.107813245151192e-05, 3.080788883380592e-05, 3.0542301828972995e-05, 3.0281256840680726e-05, 3.0024637453607284e-05, 2.9772329071420245e-05, 2.9524226192734204e-05, 2.9280223316163756e-05, 2.904022221628111e-05, 2.8804122848669067e-05, 2.857183244486805e-05, 2.8343256417429075e-05, 2.8118311092839576e-05, 2.7896905521629378e-05, 2.7678961487254128e-05, 2.7464397135190666e-05, 2.7253132429905236e-05, 2.7045092792832293e-05, 2.6840205464395694e-05, 2.6638399504008703e-05, 2.6439605790073983e-05, 2.6243757019983605e-05, 2.605078771011904e-05, 2.5860636014840566e-05, 2.5673240088508464e-05, 2.5488539904472418e-05, 2.5306479074060917e-05, 2.512700120860245e-05, 2.495004991942551e-05, 2.4775574274826795e-05, 2.4603521524113603e-05, 2.443384255457204e-05, 2.4266486434498802e-05, 2.4101409508148208e-05, 2.393856084381696e-05, 2.3777900423738174e-05, 2.3619380954187363e-05, 2.3462960598408245e-05, 2.330859933863394e-05, 2.3156255338108167e-05, 2.3005890398053452e-05, 2.2857464500702918e-05, 2.2710943085257895e-05, 2.256628795294091e-05, 2.2423462723963894e-05, 2.228243465651758e-05, 2.2143169189803302e-05, 2.2005633582011797e-05, 2.1869796910323203e-05, 2.1735626432928257e-05, 2.1603093045996502e-05, 2.1472164007718675e-05, 2.1342813852243125e-05, 2.1215013475739397e-05, 2.108873195538763e-05, 2.096394746331498e-05, 2.0840630895690992e-05, 2.0718754967674613e-05, 2.0598297851393e-05, 2.0479232262005098e-05, 2.0361534552648664e-05, 2.0245182895450853e-05, 2.013015364354942e-05, 2.0016424969071522e-05, 1.9903973225154914e-05, 1.9792776583926752e-05, 1.9682816855493e-05, 1.957407221198082e-05, 1.9466522644506767e-05, 1.9360148144187406e-05, 1.92549305211287e-05, 1.915084976644721e-05, 1.90478876902489e-05, 1.8946027921629138e-05, 1.8845250451704487e-05, 1.8745540728559718e-05, 1.864687874331139e-05, 1.854925176303368e-05, 1.8452641597832553e-05, 1.8357031876803376e-05], "accuracy_train_first": 0.5185431518779993, "accuracy_train_last": 0.9564807845376523, "batch_size_eval": 1024, "accuracy_train_std": [0.01890600133943865, 0.01819824626742757, 0.019553313660901763, 0.01605165143977905, 0.01680944005381363, 0.016450676624015156, 0.016536952595012738, 0.01713168003654439, 0.014982593418380078, 0.01544454850886673, 0.014657417190009446, 0.014927060290717357, 0.014178126278537382, 0.014719970480656932, 0.013916326006028315, 0.012800231980112473, 0.012895720785137626, 0.01282470707804734, 0.01245233551464936, 0.013739433672324431, 0.011552806790807486, 0.012471140100506133, 0.012128484232105921, 0.012792423497393693, 0.012404959125752377, 0.013531125313721934, 0.012420468123160295, 0.011969482496236616, 0.01249470544538599, 0.011880670471484405, 0.011654650015191361, 0.011316532301515907, 0.01164444404164529, 0.01123600880098409, 0.010809963878972994, 0.011168288367278635, 0.010881743350988972, 0.009935857809884922, 0.011383914847922028, 0.010568834937761593, 0.01036335050379586, 0.010718176126801739, 0.011373441428682693, 0.010959606118368888, 0.010327160354666999, 0.010753738386032071, 0.010459751743533315, 0.010436448337568375, 0.00938228088103113, 0.00885709422962079, 0.010153063592515926, 0.009009990833036371, 0.009965080156262818, 0.009584598720704433, 0.010184463048299112, 0.009716833006586335, 0.00907308228281235, 0.009854971267920565, 0.0099784188388305, 0.009882438078026768, 0.010286060542804376, 0.009539396728207014, 0.010062159581893652, 0.009720414400885356, 0.010032404179534661, 0.009844329936242416, 0.009815788071689332, 0.01013620292246708, 0.009317277720729988, 0.009393321108643144, 0.009978386117788155, 0.009482341031731383, 0.00960777475372029, 0.009768969849704986, 0.009292783477678231, 0.009183400897948297, 0.00966122288679789, 0.00907181860362593, 0.00922931957562283, 0.009083142223407215, 0.00926139466241951, 0.0090078409242436, 0.009029348033213017, 0.008847499343701383, 0.009546783659681816, 0.008948601758233908, 0.009003110678540469, 0.0088283198199463, 0.009430684312982737, 0.009013807240026645, 0.009097067465017053, 0.00878666785155856, 0.008981549926118815, 0.00933004302189548, 0.008628935070347355, 0.009058628887378873, 0.008567582459058922, 0.00879064573460052, 0.009030916157483837, 0.008833431410788337, 0.008805408674087795, 0.009336134822078908, 0.009039937490157378, 0.008989203523668592, 0.00936889305379084, 0.0091814304818351, 0.009147593843444067, 0.009069014970083628, 0.00909376614930271, 0.008690960848307979, 0.00848760262785004, 0.009266263593668438, 0.008816467828828022, 0.00842972552652875, 0.008221762624245796, 0.009328525541413706, 0.009535190022820246, 0.008943481108417355, 0.00871579551936186, 0.008661183253538205, 0.008713132746759597, 0.008620187960572023, 0.00787037687176729, 0.008716402624192389, 0.008102365261962114, 0.007969538972065907, 0.009001982085488629, 0.007886589582928745, 0.00850489791820658, 0.008744182365271297, 0.007945399478712574, 0.008679665219719622, 0.008335935826464498, 0.008000717602559734, 0.008899947544291232, 0.00871984262982845, 0.008399016278955384, 0.008067871401215971, 0.008438170809766983, 0.008315537175806752, 0.008577443222744103, 0.008333658029684643, 0.00777419772001685, 0.007936944977104623, 0.007860729588354798, 0.007506364337145132, 0.008018323788067992, 0.008581940660812913, 0.008214009879263554, 0.008985450805114049, 0.008215632745165251, 0.008479614744498836, 0.008118157808061402, 0.0084788328343377, 0.007409567811407346, 0.008086809850933005, 0.0078989997838359, 0.008350842991655808, 0.008298692356429565, 0.007654116859683642, 0.008079353768848297, 0.008007581664339574, 0.008456160403048071, 0.007878474088743018, 0.007455046389796361, 0.008193216361979436, 0.008139166468388205, 0.007724571929454241, 0.007997602137409415, 0.008133571855684136, 0.0076676620594866955, 0.008381100495234583, 0.00837424499625673, 0.007937565123867249, 0.007979128066544595, 0.007877710272469418, 0.00799493180458255, 0.007799490276821026, 0.007395695436238898, 0.007845095217627501, 0.008187108688246524, 0.007316753598622635, 0.00726444690257442, 0.00816224982208009, 0.00798746239660426, 0.007698985355159694, 0.007818222136923842, 0.007837756019671833, 0.0077238806368530935, 0.007690132398789075, 0.007908666126548941, 0.0074890053281009466, 0.007251880515005602], "accuracy_test_std": 0.006111143989293679, "error_valid": [0.48714143684111444, 0.39072353868599397, 0.31196465549698793, 0.2650470044239458, 0.2399196394954819, 0.22276802522590367, 0.21245234845632532, 0.20732539533132532, 0.19792598126882532, 0.19384618552334332, 0.1928902131965362, 0.18701024802334332, 0.18562629423945776, 0.18329666321536142, 0.1795021884412651, 0.17659309111445776, 0.17532091255647586, 0.17353103821536142, 0.17280891142695776, 0.17341926298945776, 0.17014395472515065, 0.1691262118787651, 0.16533173710466864, 0.1652199618787651, 0.1642228092055723, 0.16640977974397586, 0.16093720585466864, 0.16348009224397586, 0.16030626411897586, 0.16118134647966864, 0.16078425028237953, 0.16184317347515065, 0.1573868717055723, 0.16231086455195776, 0.1571633212537651, 0.15737657661897586, 0.15762071724397586, 0.1572648013930723, 0.15547492705195776, 0.15642060429216864, 0.15410126835466864, 0.1528908603162651, 0.1523511036332832, 0.15239228397966864, 0.15150690653237953, 0.15162897684487953, 0.15075389448418675, 0.1507950748305723, 0.14980821724397586, 0.1497170321912651, 0.1486889942582832, 0.14684764448418675, 0.1483227833207832, 0.1460034473832832, 0.1462269978350903, 0.1471020801957832, 0.14782420698418675, 0.1464917286332832, 0.14733592573418675, 0.14599315229668675, 0.1467358692582832, 0.14510777484939763, 0.1472241505082832, 0.1468373493975903, 0.14760065653237953, 0.14426357774849397, 0.1447827442582832, 0.14379588667168675, 0.1436635212725903, 0.1452504353350903, 0.1457387165850903, 0.1437855915850903, 0.14711237528237953, 0.1457593067582832, 0.1455151661332832, 0.14379588667168675, 0.1451283650225903, 0.1451489551957832, 0.1433179005082832, 0.14379588667168675, 0.14416209760918675, 0.14468126411897586, 0.14394884224397586, 0.14342967573418675, 0.1429310993975903, 0.1453019107680723, 0.14467096903237953, 0.14492540474397586, 0.14358263130647586, 0.1432973103350903, 0.14367381635918675, 0.1442944630082832, 0.1424428181475903, 0.1440503223832832, 0.14431505318147586, 0.1440503223832832, 0.14255459337349397, 0.14069265342620485, 0.14342967573418675, 0.1428296192582832, 0.1420766072100903, 0.14279873399849397, 0.14379588667168675, 0.14281932417168675, 0.14271784403237953, 0.14304287462349397, 0.1423413380082832, 0.14330760542168675, 0.14245311323418675, 0.14194424181099397, 0.1420971973832832, 0.14269725385918675, 0.1417103962725903, 0.1426869587725903, 0.14257518354668675, 0.14084560899849397, 0.1427590243787651, 0.1421986775225903, 0.14220897260918675, 0.1423413380082832, 0.1438061817582832, 0.14123241010918675, 0.14184276167168675, 0.14194424181099397, 0.14123241010918675, 0.14010289203689763, 0.1443253482680723, 0.1423722232680723, 0.14269725385918675, 0.1427075489457832, 0.1406117634600903, 0.14416209760918675, 0.1417309864457832, 0.1422192676957832, 0.14009259695030118, 0.1422192676957832, 0.1417309864457832, 0.14001170698418675, 0.13910573936370485, 0.14159862104668675, 0.14143537038780118, 0.1420766072100903, 0.13899396413780118, 0.13938076524849397, 0.1408559040850903, 0.14047939806099397, 0.14084560899849397, 0.14182217149849397, 0.14269725385918675, 0.1418324665850903, 0.14194424181099397, 0.14074412885918675, 0.14281932417168675, 0.14233104292168675, 0.14120152484939763, 0.14208690229668675, 0.14181187641189763, 0.13948224538780118, 0.14121181993599397, 0.14071324359939763, 0.14021466726280118, 0.14220897260918675, 0.14037791792168675, 0.14084560899849397, 0.14220897260918675, 0.1408764942582832, 0.14126329536897586, 0.14196483198418675, 0.14247370340737953, 0.14133389024849397, 0.14032644248870485, 0.1417103962725903, 0.14119122976280118, 0.14231045274849397, 0.14193394672439763, 0.1416089161332832, 0.1407338337725903, 0.1400014118975903, 0.14279873399849397, 0.14144566547439763, 0.14057058311370485, 0.14072353868599397, 0.14167951101280118, 0.14145596056099397, 0.14034703266189763, 0.1418324665850903, 0.14120152484939763, 0.14143537038780118, 0.1429310993975903, 0.14059117328689763, 0.1421986775225903, 0.14220897260918675, 0.14021466726280118], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.05513178102722739, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 64, "valid_ratio": 0.15, "learning_rate": 0.003542907113123785, "optimization": "adam", "nb_data_augmentation": 2, "learning_rate_decay_method": "lin", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 1.1296387255941194e-06, "rotation_range": [0, 0], "momentum": 0.9479333525197327}, "accuracy_valid_max": 0.8610060358621988, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8597853327371988, "accuracy_valid_std": [0.016151025362261887, 0.018786516739533625, 0.01930122672816879, 0.015526159141309224, 0.01201723146381279, 0.014002206358405175, 0.011186143529513574, 0.01173636628275554, 0.015331279556620975, 0.013253513550096244, 0.0135344477798018, 0.012710972333562212, 0.008046940554505003, 0.009452803580792793, 0.009539407916808445, 0.011084768365340112, 0.00849462676139915, 0.01108567024705025, 0.01280350794509252, 0.011437954202648454, 0.016007593714992566, 0.012044730805765869, 0.011908981407395207, 0.013259498550452786, 0.011257883620408023, 0.009413599173306489, 0.013764348331994353, 0.00993947498407295, 0.011332044614979261, 0.01274023050809572, 0.010544231655311176, 0.018251696654435164, 0.012777478093161028, 0.015621684352900988, 0.01581755839133033, 0.01242288915429575, 0.012570203597366169, 0.013649013742133845, 0.018721747885788457, 0.015279129440871432, 0.016042283965021904, 0.017767436708898902, 0.0132714500230565, 0.016973448241591776, 0.014436947048728508, 0.01343877462305903, 0.012744721417425619, 0.016387587924037553, 0.015422063003899436, 0.01879966345893078, 0.014124116306531787, 0.013768974614013768, 0.013985951677691446, 0.015153564072711616, 0.013386241443035954, 0.013853855404215232, 0.013699746651472994, 0.015286478949319877, 0.01345821960276512, 0.014220784819312193, 0.015056530297117843, 0.012267589797100617, 0.015111613195130272, 0.01367046875098717, 0.015103431537176496, 0.01250915644662054, 0.016522137696549526, 0.01540922255115197, 0.0136455724840945, 0.013702141299880777, 0.013421334061081782, 0.013428288101028871, 0.014716235190759869, 0.015135972150610256, 0.01570240123371232, 0.01486584197658924, 0.012924694647322146, 0.015587803264689037, 0.0157470196261257, 0.015540181964045885, 0.014762813892105216, 0.017674669667585092, 0.018223732162880323, 0.014612515920609087, 0.013455714752670522, 0.01879703743978987, 0.015748199793362137, 0.017677655747045405, 0.018139469127634523, 0.014153909553909804, 0.015212115277605933, 0.015889751966079002, 0.013618152477855009, 0.014723607776774825, 0.017426056148555174, 0.015209475970246293, 0.012527370292482477, 0.010315652446154695, 0.014029836812769586, 0.01576238070680613, 0.013848370867582751, 0.012695606573773076, 0.013990028173243864, 0.014401307358643907, 0.01652600035850613, 0.011833438529555613, 0.016136317912828668, 0.013757932771292844, 0.014943059868956414, 0.012525628932463487, 0.01570400183017092, 0.01441524425259991, 0.014056814538917767, 0.013757733727979556, 0.015464976335155308, 0.013431247550663994, 0.02081076779580835, 0.013366102058994658, 0.014583688669904315, 0.015270921313604404, 0.01507378616808826, 0.01482669172700842, 0.015143527260498871, 0.011860814338605091, 0.014567134002146404, 0.01164588037873936, 0.017882569228093975, 0.01837979995283557, 0.014046666842409482, 0.015213553692176583, 0.013898528995372405, 0.013871909963990812, 0.01600074036278343, 0.0157186454301379, 0.011086015213989553, 0.015241189660065853, 0.015775650059004224, 0.015617422411348638, 0.01083242081139185, 0.014754730627895942, 0.01094461842674487, 0.013490819617167468, 0.011585852435047568, 0.013437555173255653, 0.014319109459032812, 0.013022205413203657, 0.01254080573943979, 0.01270318403578851, 0.014072103791507177, 0.013811503667981999, 0.012891458596246094, 0.014628282874854124, 0.014434380068244615, 0.01567737772589796, 0.011931541188105708, 0.014295226755287388, 0.011337106616169022, 0.011867248007310247, 0.013398010113908712, 0.011875072169839237, 0.01213318998267677, 0.014697681290874332, 0.015493438495377295, 0.013333259406352984, 0.01571675063781698, 0.016173879946595755, 0.01839710264702987, 0.015010620362149518, 0.01652626108314728, 0.012617532933628042, 0.010294197293608252, 0.01373945222906547, 0.011014211248368825, 0.012628797097044096, 0.01178282424914378, 0.01635859681407462, 0.013996401405747406, 0.014319880521270416, 0.0126485704085688, 0.011877158325464952, 0.009957034423754037, 0.013195426362137951, 0.011477244674120644, 0.013358341268976774, 0.011947217227201276, 0.014169375917553526, 0.012305347663894847, 0.01064647222657821, 0.013917343288056737, 0.012059632060663565, 0.014321911413346149, 0.014394455456923668, 0.011403866075170098], "accuracy_valid": [0.5128585631588856, 0.609276461314006, 0.6880353445030121, 0.7349529955760542, 0.7600803605045181, 0.7772319747740963, 0.7875476515436747, 0.7926746046686747, 0.8020740187311747, 0.8061538144766567, 0.8071097868034638, 0.8129897519766567, 0.8143737057605422, 0.8167033367846386, 0.8204978115587349, 0.8234069088855422, 0.8246790874435241, 0.8264689617846386, 0.8271910885730422, 0.8265807370105422, 0.8298560452748494, 0.8308737881212349, 0.8346682628953314, 0.8347800381212349, 0.8357771907944277, 0.8335902202560241, 0.8390627941453314, 0.8365199077560241, 0.8396937358810241, 0.8388186535203314, 0.8392157497176205, 0.8381568265248494, 0.8426131282944277, 0.8376891354480422, 0.8428366787462349, 0.8426234233810241, 0.8423792827560241, 0.8427351986069277, 0.8445250729480422, 0.8435793957078314, 0.8458987316453314, 0.8471091396837349, 0.8476488963667168, 0.8476077160203314, 0.8484930934676205, 0.8483710231551205, 0.8492461055158133, 0.8492049251694277, 0.8501917827560241, 0.8502829678087349, 0.8513110057417168, 0.8531523555158133, 0.8516772166792168, 0.8539965526167168, 0.8537730021649097, 0.8528979198042168, 0.8521757930158133, 0.8535082713667168, 0.8526640742658133, 0.8540068477033133, 0.8532641307417168, 0.8548922251506024, 0.8527758494917168, 0.8531626506024097, 0.8523993434676205, 0.855736422251506, 0.8552172557417168, 0.8562041133283133, 0.8563364787274097, 0.8547495646649097, 0.8542612834149097, 0.8562144084149097, 0.8528876247176205, 0.8542406932417168, 0.8544848338667168, 0.8562041133283133, 0.8548716349774097, 0.8548510448042168, 0.8566820994917168, 0.8562041133283133, 0.8558379023908133, 0.8553187358810241, 0.8560511577560241, 0.8565703242658133, 0.8570689006024097, 0.8546980892319277, 0.8553290309676205, 0.8550745952560241, 0.8564173686935241, 0.8567026896649097, 0.8563261836408133, 0.8557055369917168, 0.8575571818524097, 0.8559496776167168, 0.8556849468185241, 0.8559496776167168, 0.857445406626506, 0.8593073465737951, 0.8565703242658133, 0.8571703807417168, 0.8579233927899097, 0.857201266001506, 0.8562041133283133, 0.8571806758283133, 0.8572821559676205, 0.856957125376506, 0.8576586619917168, 0.8566923945783133, 0.8575468867658133, 0.858055758189006, 0.8579028026167168, 0.8573027461408133, 0.8582896037274097, 0.8573130412274097, 0.8574248164533133, 0.859154391001506, 0.8572409756212349, 0.8578013224774097, 0.8577910273908133, 0.8576586619917168, 0.8561938182417168, 0.8587675898908133, 0.8581572383283133, 0.858055758189006, 0.8587675898908133, 0.8598971079631024, 0.8556746517319277, 0.8576277767319277, 0.8573027461408133, 0.8572924510542168, 0.8593882365399097, 0.8558379023908133, 0.8582690135542168, 0.8577807323042168, 0.8599074030496988, 0.8577807323042168, 0.8582690135542168, 0.8599882930158133, 0.8608942606362951, 0.8584013789533133, 0.8585646296121988, 0.8579233927899097, 0.8610060358621988, 0.860619234751506, 0.8591440959149097, 0.859520601939006, 0.859154391001506, 0.858177828501506, 0.8573027461408133, 0.8581675334149097, 0.858055758189006, 0.8592558711408133, 0.8571806758283133, 0.8576689570783133, 0.8587984751506024, 0.8579130977033133, 0.8581881235881024, 0.8605177546121988, 0.858788180064006, 0.8592867564006024, 0.8597853327371988, 0.8577910273908133, 0.8596220820783133, 0.859154391001506, 0.8577910273908133, 0.8591235057417168, 0.8587367046310241, 0.8580351680158133, 0.8575262965926205, 0.858666109751506, 0.8596735575112951, 0.8582896037274097, 0.8588087702371988, 0.857689547251506, 0.8580660532756024, 0.8583910838667168, 0.8592661662274097, 0.8599985881024097, 0.857201266001506, 0.8585543345256024, 0.8594294168862951, 0.859276461314006, 0.8583204889871988, 0.858544039439006, 0.8596529673381024, 0.8581675334149097, 0.8587984751506024, 0.8585646296121988, 0.8570689006024097, 0.8594088267131024, 0.8578013224774097, 0.8577910273908133, 0.8597853327371988], "seed": 961082240, "model": "residualv3", "loss_std": [0.23093195259571075, 0.15145990252494812, 0.13568486273288727, 0.12718966603279114, 0.1264503300189972, 0.12446189671754837, 0.12524820864200592, 0.11937139928340912, 0.11594682931900024, 0.11713913828134537, 0.11579754948616028, 0.11310145258903503, 0.11064673215150833, 0.1090475469827652, 0.11053420603275299, 0.10793834179639816, 0.10602118819952011, 0.10462617874145508, 0.1039348617196083, 0.10307032614946365, 0.10293692350387573, 0.1035756766796112, 0.10103175789117813, 0.0992312878370285, 0.09986011683940887, 0.09863513708114624, 0.09851279109716415, 0.09913691878318787, 0.09591611474752426, 0.09661321341991425, 0.09602975845336914, 0.09776268899440765, 0.09527193754911423, 0.09529151022434235, 0.09377360343933105, 0.09285160899162292, 0.09531377255916595, 0.092958964407444, 0.09340813755989075, 0.09394524991512299, 0.09186408668756485, 0.09000428020954132, 0.09146973490715027, 0.0920032411813736, 0.09134874492883682, 0.09121116250753403, 0.09138594567775726, 0.09065112471580505, 0.08832395821809769, 0.08923108130693436, 0.08957018703222275, 0.09041909128427505, 0.08910491317510605, 0.08729273080825806, 0.08999428153038025, 0.08781018853187561, 0.08838149160146713, 0.08800146728754044, 0.0873335525393486, 0.08708813041448593, 0.0850721225142479, 0.08564452081918716, 0.08674286305904388, 0.08471633493900299, 0.0843588262796402, 0.08677992969751358, 0.08418038487434387, 0.0846782848238945, 0.08462145179510117, 0.08413287252187729, 0.08317416906356812, 0.0861739069223404, 0.08469929546117783, 0.08257820457220078, 0.08199858665466309, 0.08387389779090881, 0.08620870858430862, 0.08130670338869095, 0.08162553608417511, 0.08351331949234009, 0.08094409853219986, 0.08204879611730576, 0.08350899070501328, 0.08345381170511246, 0.08278132975101471, 0.08254479616880417, 0.08292150497436523, 0.08125191181898117, 0.08048352599143982, 0.08167410641908646, 0.08017507940530777, 0.08073929697275162, 0.08039938658475876, 0.07948116213083267, 0.0795264020562172, 0.08066954463720322, 0.08098261058330536, 0.07873807102441788, 0.0793442502617836, 0.07996044307947159, 0.07789754867553711, 0.08013825863599777, 0.07926282286643982, 0.07800478488206863, 0.07812966406345367, 0.07852213829755783, 0.07852431386709213, 0.07812410593032837, 0.07716445624828339, 0.07901943475008011, 0.07778300344944, 0.07738497853279114, 0.07804366201162338, 0.0773932933807373, 0.07761414349079132, 0.07797149568796158, 0.07662205398082733, 0.07475733757019043, 0.0763583555817604, 0.07767663896083832, 0.07751302421092987, 0.07615184783935547, 0.07809693366289139, 0.077488474547863, 0.07519639283418655, 0.07463941723108292, 0.07607174664735794, 0.07632797211408615, 0.07512392848730087, 0.07458902895450592, 0.07620332390069962, 0.07550477981567383, 0.07622849196195602, 0.07497404515743256, 0.07475556433200836, 0.07350128889083862, 0.07657766342163086, 0.07646023482084274, 0.07517609745264053, 0.07299936562776566, 0.07443231344223022, 0.07588713616132736, 0.0746569037437439, 0.07255533337593079, 0.07526490837335587, 0.07533319294452667, 0.07384353131055832, 0.0737096294760704, 0.07437459379434586, 0.07423749566078186, 0.07483300566673279, 0.07443826645612717, 0.0729675367474556, 0.07297645509243011, 0.0723436251282692, 0.07325950264930725, 0.07306934893131256, 0.0730290338397026, 0.07555519789457321, 0.07273513078689575, 0.07289211452007294, 0.0719032809138298, 0.07370328158140182, 0.07306763529777527, 0.07209492474794388, 0.07342405617237091, 0.07147014141082764, 0.07392208278179169, 0.07300079613924026, 0.07230542600154877, 0.07356642186641693, 0.07263082265853882, 0.07162566483020782, 0.0731668621301651, 0.07199812680482864, 0.07089634984731674, 0.0723290666937828, 0.07295337319374084, 0.07317173480987549, 0.07160370796918869, 0.07197249680757523, 0.072374626994133, 0.0732722282409668, 0.07155570387840271, 0.07074394822120667, 0.07313784211874008, 0.07155252248048782, 0.07181824743747711, 0.07115627825260162, 0.0708535686135292, 0.07087383419275284, 0.07009787112474442, 0.06988289952278137]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:31 2016", "state": "available"}], "summary": "ecb7748be353202386e6346eac7eb2d5"}