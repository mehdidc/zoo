{"content": {"hp_model": {"f0": 16, "f1": 32, "f2": 32, "f3": 64, "nonlin": "very_leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.5693472623825073, 1.1672133207321167, 0.9914909601211548, 0.8712977170944214, 0.779748260974884, 0.7037881016731262, 0.63869708776474, 0.5806294083595276, 0.5295782089233398, 0.49118635058403015, 0.4651561379432678, 0.42939794063568115, 0.39178889989852905, 0.3607477843761444, 0.3279346823692322, 0.30808401107788086, 0.28538697957992554, 0.26054754853248596, 0.24644775688648224, 0.2285546362400055, 0.21485260128974915, 0.20414109528064728, 0.19769947230815887, 0.1936706155538559, 0.19169005751609802, 0.19050423800945282, 0.18959452211856842, 0.18880076706409454, 0.18808405101299286, 0.18742771446704865, 0.1868220567703247, 0.18626059591770172, 0.18573857843875885, 0.18525229394435883, 0.18479852378368378, 0.18437470495700836, 0.18397840857505798, 0.18360771238803864, 0.1832606941461563, 0.1829356700181961, 0.18263117969036102, 0.18234583735466003, 0.1820783168077469, 0.18182750046253204, 0.18159224092960358, 0.1813715547323227, 0.18116453289985657, 0.18097026646137238, 0.18078795075416565, 0.1806168407201767, 0.1804562360048294, 0.18030546605587006, 0.1801639199256897, 0.18003104627132416, 0.17990627884864807, 0.17978914082050323, 0.17967914044857025, 0.17957589030265808, 0.17947891354560852, 0.17938785254955292, 0.179302379488945, 0.17922213673591614, 0.17914678156375885, 0.17907604575157166, 0.1790096014738083, 0.17894722521305084, 0.17888867855072021, 0.1788337379693985, 0.17878217995166779, 0.17873381078243256, 0.1786884218454361, 0.17864581942558289, 0.1786058396100998, 0.1785682886838913, 0.17853298783302307, 0.17849986255168915, 0.1784687042236328, 0.17843939363956451, 0.1784118413925171, 0.17838598787784576, 0.1783616989850998, 0.1783389449119568, 0.1783176064491272, 0.17829759418964386, 0.17827880382537842, 0.1782611608505249], "moving_avg_accuracy_train": [0.06085411764705881, 0.12429341176470586, 0.18661465882352937, 0.24552966352941172, 0.30076022658823526, 0.3501171451058823, 0.39537366588941175, 0.43714218165341173, 0.47736443407630585, 0.5123997553745576, 0.5437644857194548, 0.5773856842063328, 0.6102753510798171, 0.6392031100894825, 0.6666827990805342, 0.6939298132901279, 0.7190780084317034, 0.7430972664120625, 0.7655334221237974, 0.7867659622643588, 0.8066823072143935, 0.8255223117870718, 0.842777139431894, 0.8584217784298811, 0.8725301888221871, 0.8852395228811449, 0.8966826294165599, 0.9069837782396097, 0.9162548121803546, 0.9246010956682015, 0.9321198096307931, 0.9388866521971256, 0.9449791634480014, 0.9504600706326131, 0.95539524003994, 0.9598368925065343, 0.9638367326676456, 0.9674365888126457, 0.9706764593431458, 0.9735923428205959, 0.976216637950301, 0.9785785035670356, 0.9807041826220967, 0.9826172937716517, 0.9843390938062513, 0.9858887138373909, 0.9872833718654165, 0.9885385640906396, 0.9896705900345167, 0.9906894133840062, 0.9916063543985467, 0.9924316013116332, 0.9931743235334111, 0.9938427735330111, 0.9944443785326511, 0.9949858230323272, 0.9954754760232121, 0.9959161637150085, 0.9963127826376252, 0.9966697396679803, 0.9969910009952999, 0.9972801361898875, 0.9975403578650164, 0.9977745573726323, 0.9979853369294867, 0.9981750385306556, 0.9983481229128841, 0.9985038988568897, 0.9986440972064948, 0.9987702757211394, 0.9988838363843195, 0.9989860409811817, 0.9990780251183576, 0.999160810841816, 0.9992353179929284, 0.9993023744289297, 0.9993627252213307, 0.9994170409344917, 0.9994659250763366, 0.999509920803997, 0.9995495169588914, 0.9995851534982964, 0.9996172263837608, 0.9996460919806788, 0.999672071017905, 0.9996954521514085], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.05917333333333332, 0.11994933333333332, 0.17846106666666664, 0.2338682933333333, 0.284961464, 0.32963865093333333, 0.36934145250666667, 0.4043406405893333, 0.43698657653039996, 0.46491458554402665, 0.48894312698962394, 0.5146888142906616, 0.5394599328615954, 0.5613939395754359, 0.5805478789512257, 0.5994664243894365, 0.6166264486171595, 0.6321904704221103, 0.6467447567132326, 0.6601102810419094, 0.6726059196043851, 0.6846653276439465, 0.6958521282128852, 0.7061735820582634, 0.7155028905191037, 0.72393926813386, 0.7314520079871407, 0.7382268071884267, 0.744284126469584, 0.7497757138226255, 0.7546781424403629, 0.7591703281963267, 0.7632266287100273, 0.7668906325056912, 0.7701882359217888, 0.7731827456629432, 0.7758511377633156, 0.7782393573203173, 0.7803754215882857, 0.7823245460961238, 0.7840920914865114, 0.7856562156711936, 0.7870639274374076, 0.7883308680270003, 0.7894577812243002, 0.7904853364352036, 0.7914234694583498, 0.7922677891791815, 0.79302767692793, 0.7937115759018036, 0.79432708497829, 0.7949077098137942, 0.7954302721657482, 0.7959005782825068, 0.7963238537875894, 0.7967048017421638, 0.7970476549012808, 0.7973562227444861, 0.7976339338033709, 0.7978838737563672, 0.7980954863807305, 0.7982859377426574, 0.7984573439683916, 0.7986116095715524, 0.7987637819477305, 0.7989007370862908, 0.7990239967109951, 0.7991349303732289, 0.7992347706692393, 0.7993246269356487, 0.7994054975754172, 0.7994782811512089, 0.7995437863694212, 0.7996027410658124, 0.7996558002925644, 0.7997035535966412, 0.7997465315703105, 0.7997852117466129, 0.7998200239052848, 0.7998513548480897, 0.799879552696614, 0.7998915974269526, 0.7999024376842573, 0.7999121939158316, 0.7999209745242484, 0.7999288770718235], "moving_var_accuracy_train": [0.03332901271141868, 0.06621700778358476, 0.09455074751992856, 0.11633447278338296, 0.1321547613671968, 0.140864233880496, 0.14521118455331666, 0.14639154628013265, 0.14631285796185847, 0.14272883581191825, 0.13730966901719943, 0.13375216700472595, 0.13011252198769233, 0.12463260696081457, 0.1189655460281375, 0.11375058947536393, 0.10806741599773637, 0.10245299718330612, 0.09673812721306645, 0.09112170133914478, 0.08557947837074925, 0.08021604248436118, 0.07487399992939775, 0.06958939250085401, 0.06442187844494816, 0.059433435150453, 0.054668593820035395, 0.05015675744170346, 0.04591465033050711, 0.041950129329991946, 0.03826389593385418, 0.034849617765326524, 0.031698724228872294, 0.02879921489808201, 0.026138496481984953, 0.023702201323492482, 0.021475969682973182, 0.019445003393058123, 0.01759497390324194, 0.015911997901004348, 0.01438278043525406, 0.012994708074452273, 0.011735903870013177, 0.010595253431446827, 0.009562409446534467, 0.008627780402049201, 0.007782508000980508, 0.007018436768582801, 0.006328126436363021, 0.005704655801883904, 0.005141757249112834, 0.00463371081640958, 0.004175304461457126, 0.003761795443929102, 0.0033888732567165193, 0.0030526243903609327, 0.0027495197917881825, 0.002476315663384672, 0.002230099856174204, 0.0020082366354504636, 0.0018083418514692976, 0.0016282600587691112, 0.001466043490774062, 0.001319932786380964, 0.0011883393599371574, 0.0010698293042208163, 0.0009631159976290776, 0.0008670227931687478, 0.000780497414046961, 0.0007025909618002863, 0.0006324479298382551, 0.0005692971488710074, 0.0005124435837173347, 0.0004612609066296779, 0.0004151847778068123, 0.00037370676911661384, 0.00033636887216824347, 0.00030275853672168487, 0.0002725041899834317, 0.00024527119160155984, 0.00022075818314074567, 0.00019869379449313798, 0.00017883367307366236, 0.00016095780477046504, 0.00014486809848679536, 0.00013038620873475105], "duration": 22479.541537, "accuracy_train": [0.6085411764705883, 0.6952470588235294, 0.7475058823529411, 0.7757647058823529, 0.7978352941176471, 0.7943294117647058, 0.8026823529411765, 0.8130588235294117, 0.8393647058823529, 0.8277176470588236, 0.8260470588235294, 0.8799764705882352, 0.9062823529411764, 0.8995529411764706, 0.914, 0.9391529411764706, 0.9454117647058824, 0.9592705882352941, 0.9674588235294118, 0.9778588235294118, 0.9859294117647058, 0.9950823529411764, 0.9980705882352942, 0.9992235294117647, 0.9995058823529411, 0.9996235294117647, 0.9996705882352941, 0.9996941176470588, 0.9996941176470588, 0.9997176470588235, 0.9997882352941176, 0.9997882352941176, 0.9998117647058824, 0.9997882352941176, 0.9998117647058824, 0.9998117647058824, 0.999835294117647, 0.999835294117647, 0.999835294117647, 0.999835294117647, 0.999835294117647, 0.999835294117647, 0.999835294117647, 0.999835294117647, 0.999835294117647, 0.999835294117647, 0.999835294117647, 0.999835294117647, 0.9998588235294118, 0.9998588235294118, 0.9998588235294118, 0.9998588235294118, 0.9998588235294118, 0.9998588235294118, 0.9998588235294118, 0.9998588235294118, 0.9998823529411764, 0.9998823529411764, 0.9998823529411764, 0.9998823529411764, 0.9998823529411764, 0.9998823529411764, 0.9998823529411764, 0.9998823529411764, 0.9998823529411764, 0.9998823529411764, 0.9999058823529412, 0.9999058823529412, 0.9999058823529412, 0.9999058823529412, 0.9999058823529412, 0.9999058823529412, 0.9999058823529412, 0.9999058823529412, 0.9999058823529412, 0.9999058823529412, 0.9999058823529412, 0.9999058823529412, 0.9999058823529412, 0.9999058823529412, 0.9999058823529412, 0.9999058823529412, 0.9999058823529412, 0.9999058823529412, 0.9999058823529412, 0.9999058823529412], "end": "2016-02-05 23:24:52.618000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0], "moving_var_accuracy_valid": [0.031513350399999994, 0.06160551494399999, 0.08625756988863997, 0.10526145980179838, 0.11822992262057669, 0.12437138964900311, 0.1261210627590461, 0.1245334449811544, 0.12167191468425301, 0.11652448640301463, 0.11006837499873827, 0.10502710123028972, 0.10004686594455811, 0.0943720852048075, 0.0882367372268296, 0.08263426575762545, 0.07702103706532727, 0.07149908233149938, 0.06625561934334499, 0.06123779257423454, 0.056519282164567364, 0.05217621784849239, 0.04808489662636601, 0.04423519864906986, 0.040595002751380435, 0.03717605468157124, 0.03396642055434176, 0.03098285963686728, 0.028214793725045473, 0.025664732137445707, 0.023314563180869236, 0.02116472445857706, 0.01919633417743639, 0.01739752507402451, 0.01575564026123078, 0.014260780032416525, 0.012898784876786838, 0.011660238722980165, 0.010535279785694168, 0.009515943584248246, 0.008592467176187146, 0.0077552388187544005, 0.006997549808629596, 0.006312241073884651, 0.005692446366684425, 0.005132704557419074, 0.004627354943799225, 0.00417103533153817, 0.003759128662900636, 0.0033874252568687614, 0.003052092393991019, 0.002749917281388357, 0.0024773831959546376, 0.002231635566950318, 0.002010084469634113, 0.0018103821147675513, 0.0016304018378892444, 0.001468218581125063, 0.0013220908339025986, 0.0011904439803332727, 0.0010718026014250547, 0.0009649487867738878, 0.0008687183289444832, 0.0007820606769369022, 0.0007040630171318571, 0.0006338255258084741, 0.0005705797096433668, 0.0005136324951757796, 0.0004623589584205687, 0.00041619572991602916, 0.0003746350174678153, 0.0003372191927611789, 0.0003035358918875786, 0.00027321358360486, 0.0002459175627782658, 0.00022134632990289155, 0.0001992283208685888, 0.0001793189541860789, 0.00016139796574499369, 0.00014526700382228768, 0.00013074745950801154, 0.00011767401923697077, 0.00010590767491387959, 9.531776407898241e-05, 8.578668156284171e-05, 7.720857545888115e-05], "accuracy_test": 0.7519, "start": "2016-02-05 17:10:13.076000", "learning_rate_per_epoch": [0.006562413647770882, 0.006169718690216541, 0.005800522398203611, 0.00545341894030571, 0.00512708630412817, 0.00482028117403388, 0.004531835205852985, 0.0042606499046087265, 0.004005692433565855, 0.0037659916561096907, 0.0035406346432864666, 0.003328762948513031, 0.0031295695807784796, 0.002942295977845788, 0.0027662289794534445, 0.002600697800517082, 0.0024450719356536865, 0.0022987588308751583, 0.00216120108962059, 0.002031874842941761, 0.0019102874211966991, 0.001795975724235177, 0.0016885044751688838, 0.001587464357726276, 0.0014924705028533936, 0.0014031609753146768, 0.001319195725955069, 0.0012402549618855119, 0.001166038098745048, 0.00109626236371696, 0.0010306619806215167, 0.0009689871221780777, 0.0009110029204748571, 0.0008564884774386883, 0.0008052361663430929, 0.0007570508169010282, 0.0007117489003576338, 0.0006691578309983015, 0.0006291153840720654, 0.000591469113714993, 0.0005560755962505937, 0.0005228000227361917, 0.0004915156750939786, 0.000462103373138234, 0.0004344510962255299, 0.0004084535175934434, 0.0003840116551145911, 0.00036103237653151155, 0.0003394281957298517, 0.00031911680707708, 0.0003000208525918424, 0.00028206760180182755, 0.0002651886607054621, 0.0002493197680450976, 0.00023440046061296016, 0.00022037392773199826, 0.00020718674932140857, 0.00019478869216982275, 0.00018313253531232476, 0.00017217388085555285, 0.00016187099390663207, 0.0001521846279501915, 0.00014307789388112724, 0.0001345161144854501, 0.00012646666436921805, 0.00011889889719896019, 0.0001117839856306091, 0.00010509482672205195, 9.880594734568149e-05, 9.289339504903182e-05, 8.733465074328706e-05, 8.210854139178991e-05, 7.71951672504656e-05, 7.257580728037283e-05, 6.82328682160005e-05, 6.414981180569157e-05, 6.031108569004573e-05, 5.6702068832237273e-05, 5.330901694833301e-05, 5.011900429963134e-05, 4.7119880036916584e-05, 4.4300224544713274e-05, 4.164929850958288e-05, 3.915700290235691e-05, 3.681384623632766e-05, 3.461090454948135e-05], "accuracy_train_first": 0.6085411764705883, "accuracy_train_last": 0.9999058823529412, "batch_size_eval": 1024, "accuracy_train_std": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "accuracy_test_std": 0, "error_valid": [0.40826666666666667, 0.3330666666666666, 0.2949333333333334, 0.26746666666666663, 0.2552, 0.26826666666666665, 0.2733333333333333, 0.2806666666666666, 0.2692, 0.2837333333333333, 0.29479999999999995, 0.25360000000000005, 0.23760000000000003, 0.24119999999999997, 0.24706666666666666, 0.23026666666666662, 0.22893333333333332, 0.22773333333333334, 0.2222666666666666, 0.21960000000000002, 0.2149333333333333, 0.20679999999999998, 0.20346666666666668, 0.2009333333333333, 0.20053333333333334, 0.20013333333333339, 0.2009333333333333, 0.20079999999999998, 0.20120000000000005, 0.20079999999999998, 0.20120000000000005, 0.20040000000000002, 0.2002666666666667, 0.20013333333333339, 0.20013333333333339, 0.19986666666666664, 0.20013333333333339, 0.2002666666666667, 0.20040000000000002, 0.20013333333333339, 0.19999999999999996, 0.2002666666666667, 0.2002666666666667, 0.2002666666666667, 0.20040000000000002, 0.2002666666666667, 0.20013333333333339, 0.20013333333333339, 0.20013333333333339, 0.20013333333333339, 0.20013333333333339, 0.19986666666666664, 0.19986666666666664, 0.19986666666666664, 0.19986666666666664, 0.19986666666666664, 0.19986666666666664, 0.19986666666666664, 0.19986666666666664, 0.19986666666666664, 0.19999999999999996, 0.19999999999999996, 0.19999999999999996, 0.19999999999999996, 0.19986666666666664, 0.19986666666666664, 0.19986666666666664, 0.19986666666666664, 0.19986666666666664, 0.19986666666666664, 0.19986666666666664, 0.19986666666666664, 0.19986666666666664, 0.19986666666666664, 0.19986666666666664, 0.19986666666666664, 0.19986666666666664, 0.19986666666666664, 0.19986666666666664, 0.19986666666666664, 0.19986666666666664, 0.19999999999999996, 0.19999999999999996, 0.19999999999999996, 0.19999999999999996, 0.19999999999999996], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.059840027719103586, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 64, "valid_ratio": 0.15, "learning_rate": 0.0069801032317714905, "optimization": "nesterov_momentum", "nb_data_augmentation": 0, "learning_rate_decay_method": "exp", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 4.453520161800312e-05, "rotation_range": [0, 0], "momentum": 0.9561677667762636}, "accuracy_valid_max": 0.8001333333333334, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        y_pred = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            y_pred.extend((nnet.predict(X[mini_batch]) == y[mini_batch]).tolist())\n        return np.mean(y_pred)\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            acc = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = acc\n            status[\"accuracy_train_std\"] = 0\n            acc = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = acc\n            status[\"accuracy_valid_std\"] = 0\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n\n    # rescaling to [-1, 1]\n    X_min = X_train.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X_train.max(axis=(0, 2, 3))[None, :, None, None]\n    def preprocess(a):\n        return (a / 255.) * 2 - 1\n        # return 2 * ((a - X_min) / (X_max - X_min)) - 1\n    X_train = preprocess(X_train)\n    X_valid = preprocess(X_valid)\n\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = preprocess(X_test)\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    acc = evaluate(X_test, y_test, batch_size_eval)\n    light.set(\"accuracy_test\", acc)\n    light.set(\"accuracy_test_std\", 0)\n    print(\"Test accuracy : {}+-{}\".format(acc, 0))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8, "accuracy_valid_std": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "accuracy_valid": [0.5917333333333333, 0.6669333333333334, 0.7050666666666666, 0.7325333333333334, 0.7448, 0.7317333333333333, 0.7266666666666667, 0.7193333333333334, 0.7308, 0.7162666666666667, 0.7052, 0.7464, 0.7624, 0.7588, 0.7529333333333333, 0.7697333333333334, 0.7710666666666667, 0.7722666666666667, 0.7777333333333334, 0.7804, 0.7850666666666667, 0.7932, 0.7965333333333333, 0.7990666666666667, 0.7994666666666667, 0.7998666666666666, 0.7990666666666667, 0.7992, 0.7988, 0.7992, 0.7988, 0.7996, 0.7997333333333333, 0.7998666666666666, 0.7998666666666666, 0.8001333333333334, 0.7998666666666666, 0.7997333333333333, 0.7996, 0.7998666666666666, 0.8, 0.7997333333333333, 0.7997333333333333, 0.7997333333333333, 0.7996, 0.7997333333333333, 0.7998666666666666, 0.7998666666666666, 0.7998666666666666, 0.7998666666666666, 0.7998666666666666, 0.8001333333333334, 0.8001333333333334, 0.8001333333333334, 0.8001333333333334, 0.8001333333333334, 0.8001333333333334, 0.8001333333333334, 0.8001333333333334, 0.8001333333333334, 0.8, 0.8, 0.8, 0.8, 0.8001333333333334, 0.8001333333333334, 0.8001333333333334, 0.8001333333333334, 0.8001333333333334, 0.8001333333333334, 0.8001333333333334, 0.8001333333333334, 0.8001333333333334, 0.8001333333333334, 0.8001333333333334, 0.8001333333333334, 0.8001333333333334, 0.8001333333333334, 0.8001333333333334, 0.8001333333333334, 0.8001333333333334, 0.8, 0.8, 0.8, 0.8, 0.8], "seed": 321348668, "model": "residualv3", "loss_std": [0.3136894106864929, 0.15366873145103455, 0.1414635330438614, 0.13184432685375214, 0.12414252012968063, 0.11923781782388687, 0.11295441538095474, 0.10673745721578598, 0.09638712555170059, 0.08898640424013138, 0.08748968690633774, 0.08268299698829651, 0.07004447281360626, 0.06344816088676453, 0.05464085936546326, 0.04955190792679787, 0.04192688688635826, 0.033483944833278656, 0.030245469883084297, 0.022065984085202217, 0.01506747305393219, 0.008907536044716835, 0.005047500599175692, 0.00209297938272357, 0.000967548112384975, 0.0006402830476872623, 0.000536063511390239, 0.0004743376630358398, 0.00043095191358588636, 0.0003980006149504334, 0.00037156022153794765, 0.0003498176811262965, 0.00033146768691949546, 0.0003156556049361825, 0.0003018677234649658, 0.00028977106558158994, 0.0002790658618323505, 0.00026956311194226146, 0.0002611087984405458, 0.0002534942759666592, 0.00024664649390615523, 0.00024044368183240294, 0.00023484253324568272, 0.00022975431056693196, 0.00022514602460432798, 0.00022094958694651723, 0.00021713424939662218, 0.00021364868734963238, 0.00021047495829407126, 0.0002075688389595598, 0.00020490743918344378, 0.00020246249914634973, 0.00020022103853989393, 0.00019815802806988358, 0.00019626153516583145, 0.00019451731350272894, 0.00019291309581603855, 0.00019143482495564967, 0.00019007224182132632, 0.00018881152209360152, 0.00018764680135063827, 0.00018657096370588988, 0.00018557917792350054, 0.0001846573723014444, 0.00018380704568699002, 0.00018301662930753082, 0.00018228022963739932, 0.00018159407773055136, 0.00018095374980475754, 0.00018035715038422495, 0.00017979988479055464, 0.00017928185116034, 0.00017880290397442877, 0.00017836129700299352, 0.0001779533486114815, 0.00017757626483216882, 0.00017723001656122506, 0.00017690968525130302, 0.00017661273886915296, 0.00017633581592235714, 0.00017607655900064856, 0.00017583579756319523, 0.00017560787091497332, 0.0001753938413457945, 0.00017518995446152985, 0.00017499277601018548]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:41 2016", "state": "available"}], "summary": "3e5b6fb5d0d7e42390ef7f3f8827d411"}