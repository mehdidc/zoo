{"content": {"hp_model": {"f0": 64, "f1": 32, "f2": 64, "f3": 64, "nonlin": "very_leaky_rectify", "nbg1": 5, "nbg3": 3, "nbg2": 2, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.015078376283844364, 0.023541315680722614, 0.016796417539676565, 0.0200474074596889, 0.012043710457956365, 0.010315859927309951, 0.013832796173828639, 0.01631767023445537, 0.014642938667214165, 0.017681460670518668, 0.010068876866607495, 0.016922489919421156, 0.008832691962311177, 0.014141787748945527, 0.014120722283514114, 0.014150731710193364, 0.015383592450524827, 0.012940605432660091, 0.0067723471667585945, 0.013236142480646682, 0.014860338585180835, 0.00940438689141645, 0.009943926400867126, 0.013837751204822783, 0.014074168977581743, 0.01287288648160101, 0.009031993681452499, 0.013886174524740858, 0.010843896534061953, 0.010208245706812774, 0.011779192108600702, 0.012671067936785537, 0.012519635194086507, 0.012596852630284313, 0.012715611705493422, 0.012842634438947946, 0.012893509478215078, 0.013088910339001572, 0.013016507223814578, 0.01291727015241933, 0.012713373642784381, 0.012759418182105983, 0.012703237416944051, 0.012590886698430904, 0.012982450006643196, 0.013182048773015756, 0.01328388966792584, 0.013154890881982318, 0.013200258999490157, 0.013221194194868292, 0.013059668461289444, 0.013002996963776525, 0.012818329004123363, 0.012689669513192699, 0.013021319717992667, 0.012727190839712372, 0.0126802718336379, 0.012552049493021447, 0.012411703278522252, 0.012466281693566285, 0.012466281693566285, 0.012547156910808824, 0.012537509215314841, 0.012393804983637562, 0.012297594631489009, 0.012144749936918147, 0.012008584561718124, 0.0123511438111816, 0.0123511438111816, 0.012291103681801597, 0.012291103681801597, 0.012291103681801597, 0.012291103681801597, 0.012291103681801597, 0.0123511438111816, 0.0123511438111816, 0.012454892434763271, 0.012646807083079824, 0.012646807083079824, 0.012646807083079824, 0.012804133433711208, 0.012912321300294105, 0.012912321300294105, 0.012852583903229759, 0.012735701329691238, 0.0126260001578232, 0.012514146766328742, 0.01233215509572535, 0.012466752159742594, 0.012466752159742594, 0.012466752159742594, 0.012466752159742594, 0.012466752159742594, 0.012466752159742594, 0.012466752159742594, 0.012466752159742594, 0.012575372745670119, 0.012575372745670119, 0.012418849242862191, 0.012355418745360551, 0.01243508763916084], "moving_avg_accuracy_train": [0.0289930916043743, 0.07945861523509595, 0.1432330682303202, 0.2058930953174026, 0.2687730592145106, 0.3292379676133807, 0.384851367465184, 0.4357822614841695, 0.4826408064286374, 0.5262458245941438, 0.5655624926538324, 0.602891174117039, 0.6368056770161048, 0.6691980411216649, 0.6985558179607259, 0.7238479750385458, 0.7489335958740231, 0.7711013563378389, 0.7915010223778738, 0.8101701108008468, 0.8273906729719618, 0.8432379512473939, 0.8578771397536068, 0.8704688691556455, 0.8826291064960333, 0.8937058535845251, 0.9037632816189297, 0.9128219422963224, 0.9213235092274045, 0.9290981523522831, 0.9361534598849118, 0.9425171875571349, 0.9482468676109452, 0.9534035796593746, 0.958044620502961, 0.9622238824109982, 0.9659875432770412, 0.9693748380564798, 0.9724210782091652, 0.9751650194953916, 0.9776392169506144, 0.9798659946603149, 0.9818700945990453, 0.9836737845439026, 0.9852971054942743, 0.9867580943496088, 0.9880729843194098, 0.9892563852922308, 0.9903214461677696, 0.9912800009557546, 0.9921427002649411, 0.9929191296432089, 0.99361791608365, 0.9942468238800469, 0.9948128408968041, 0.9953222562118856, 0.995780729995459, 0.996193356400675, 0.9965647201653693, 0.9968989475535943, 0.9971997522029968, 0.9974704763874591, 0.9977141281534752, 0.9979334147428895, 0.9981307726733625, 0.9983083948107881, 0.9984682547344712, 0.998612128665786, 0.9987416152039693, 0.9988581530883343, 0.9989630371842627, 0.9990574328705983, 0.9991423889883004, 0.9992211746430418, 0.9992920817323091, 0.9993558981126496, 0.999413332854956, 0.9994650241230318, 0.9995138714131095, 0.9995578339741795, 0.9995974002791425, 0.9996330099536092, 0.9996650586606292, 0.9996939024969471, 0.9997198619496334, 0.999743225457051, 0.9997642526137268, 0.9997831770547351, 0.9998002090516425, 0.9998155378488591, 0.9998293337663541, 0.9998417500920995, 0.9998529247852704, 0.9998629820091243, 0.9998720335105927, 0.9998801798619144, 0.9998875115781038, 0.9998941101226744, 0.9999000488127878, 0.99990539363389, 0.9999102039728819], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 506858074, "moving_var_accuracy_train": [0.0075653942470167565, 0.029729776500221426, 0.0633614265437599, 0.09236179484036902, 0.11871062409364658, 0.13974360801343566, 0.15360489939978111, 0.1615900131499612, 0.1651925209437795, 0.16578584733232896, 0.16311946608493771, 0.15934839361447786, 0.15376529581504672, 0.14783215350466664, 0.14080584970258886, 0.13248250361917205, 0.12489784861156683, 0.11683075018624021, 0.10889299253852076, 0.10114050704757177, 0.09369538619621774, 0.08658607363524681, 0.07985621883280608, 0.07329756179353299, 0.06729864796375075, 0.0616730321019374, 0.05641609561974872, 0.05151302005718712, 0.04701220781402142, 0.04285499271409227, 0.03901748972210277, 0.03548021401886846, 0.0322276557186529, 0.029244215259141355, 0.02651364707423375, 0.024019478437674116, 0.02174501688193796, 0.01967377908704948, 0.017789917389955025, 0.01607868857499984, 0.014525914594926732, 0.013117949986149829, 0.01184230273661462, 0.010687352139707776, 0.009642333463908238, 0.008697310513436118, 0.007843139882786657, 0.007071429835270253, 0.0063744960437606595, 0.005745315884918714, 0.00517748254730948, 0.004665159875793468, 0.004203038610618219, 0.0037862944747037164, 0.003410548404602673, 0.003071829099811562, 0.0027665379737224225, 0.002491416521302714, 0.0022435160685839945, 0.0020201698332489525, 0.0018189672008579766, 0.0016377301050286537, 0.0014744913901735327, 0.0013274750306308525, 0.0011950780789422523, 0.0010758542176613599, 0.0009684987926520235, 0.0008718352107598289, 0.0007848025905559824, 0.0007064445612068146, 0.0006358991111483418, 0.0005723893949438967, 0.0005152154133269221, 0.00046374973660876715, 0.00041742001328566564, 0.000375714664730697, 0.0003381728870042416, 0.00030437964618857505, 0.000273963156089449, 0.0002465842348414866, 0.00022193990078973376, 0.00019975732315100103, 0.00017979083491249584, 0.00016181923912328812, 0.00014564338024961322, 0.0001310839549059616, 0.00011797953868722626, 0.00010618480802871092, 9.556893802610772e-05, 8.601415897171395e-05, 7.741445602059833e-05, 6.967439790484367e-05, 6.270808197826651e-05, 5.643818411020469e-05, 5.079510306629374e-05, 4.571619002702307e-05, 4.114505481088132e-05, 3.703094119690723e-05, 3.332816448957889e-05, 2.999560514463453e-05, 2.6996252884422035e-05], "duration": 22980.622009, "accuracy_train": [0.28993091604374305, 0.533648327911591, 0.7172031451873385, 0.7698333391011444, 0.8346927342884828, 0.8734221432032114, 0.8853719661314139, 0.8941603076550388, 0.9043677109288483, 0.9186909880837025, 0.91941250519103, 0.9388493072858989, 0.9420362031076966, 0.9607293180717055, 0.9627758095122739, 0.9514773887389257, 0.9747041833933187, 0.9706112005121816, 0.9750980167381875, 0.9781919066076044, 0.9823757325119971, 0.9858634557262828, 0.9896298363095238, 0.9837944337739941, 0.9920712425595238, 0.9933965773809523, 0.9942801339285714, 0.9943498883928571, 0.9978376116071429, 0.9990699404761905, 0.9996512276785714, 0.9997907366071429, 0.9998139880952381, 0.9998139880952381, 0.9998139880952381, 0.9998372395833334, 0.9998604910714286, 0.9998604910714286, 0.9998372395833334, 0.9998604910714286, 0.9999069940476191, 0.9999069940476191, 0.9999069940476191, 0.9999069940476191, 0.9999069940476191, 0.9999069940476191, 0.9999069940476191, 0.9999069940476191, 0.9999069940476191, 0.9999069940476191, 0.9999069940476191, 0.9999069940476191, 0.9999069940476191, 0.9999069940476191, 0.9999069940476191, 0.9999069940476191, 0.9999069940476191, 0.9999069940476191, 0.9999069940476191, 0.9999069940476191, 0.9999069940476191, 0.9999069940476191, 0.9999069940476191, 0.9999069940476191, 0.9999069940476191, 0.9999069940476191, 0.9999069940476191, 0.9999069940476191, 0.9999069940476191, 0.9999069940476191, 0.9999069940476191, 0.9999069940476191, 0.9999069940476191, 0.9999302455357143, 0.9999302455357143, 0.9999302455357143, 0.9999302455357143, 0.9999302455357143, 0.9999534970238095, 0.9999534970238095, 0.9999534970238095, 0.9999534970238095, 0.9999534970238095, 0.9999534970238095, 0.9999534970238095, 0.9999534970238095, 0.9999534970238095, 0.9999534970238095, 0.9999534970238095, 0.9999534970238095, 0.9999534970238095, 0.9999534970238095, 0.9999534970238095, 0.9999534970238095, 0.9999534970238095, 0.9999534970238095, 0.9999534970238095, 0.9999534970238095, 0.9999534970238095, 0.9999534970238095, 0.9999534970238095], "end": "2016-01-25 23:14:24.162000", "learning_rate_per_epoch": [0.0010900181951001287, 0.000770759244915098, 0.0006293223123066127, 0.0005450090975500643, 0.0004874709702562541, 0.0004449980624485761, 0.00041198814869858325, 0.000385379622457549, 0.00036333940806798637, 0.00034469400998204947, 0.00032865285174921155, 0.00031466115615330637, 0.00030231665004976094, 0.0002913196221925318, 0.00028144149109721184, 0.00027250454877503216, 0.00026436825282871723, 0.00025691973860375583, 0.00025006732903420925, 0.00024373548512812704, 0.0002378614735789597, 0.00023239266010932624, 0.00022728450130671263, 0.00022249903122428805, 0.00021800363902002573, 0.00021377015218604356, 0.00020977409440092742, 0.00020599407434929162, 0.0002024112909566611, 0.000199009184143506, 0.0001957730419235304, 0.0001926898112287745, 0.00018974780687130988, 0.0001869365805760026, 0.00018424670270178467, 0.00018166970403399318, 0.00017919788660947233, 0.00017682429461274296, 0.00017454259796068072, 0.00017234700499102473, 0.00017023224791046232, 0.00016819345182739198, 0.00016622622206341475, 0.00016432642587460577, 0.00016249032341875136, 0.00016071440768428147, 0.0001589954918017611, 0.00015733057807665318, 0.00015571688709314913, 0.00015415185771416873, 0.00015263307432178408, 0.00015115832502488047, 0.000149725514347665, 0.0001483326923334971, 0.00014697802544105798, 0.0001456598110962659, 0.00014437643403653055, 0.00014312639541458338, 0.00014190828369464725, 0.00014072074554860592, 0.00013956252951174974, 0.00013843244232703, 0.00013732937804888934, 0.00013625227438751608, 0.00013520011270884424, 0.0001341719616902992, 0.00013316691911313683, 0.00013218412641435862, 0.0001312227686867118, 0.00013028208923060447, 0.00012936136045027524, 0.00012845986930187792, 0.00012757697550114244, 0.00012671203876379877, 0.00012586446246132255, 0.00012503366451710463, 0.00012421910651028156, 0.0001234202500199899, 0.00012263662938494235, 0.00012186774256406352, 0.00012111313117202371, 0.00012037237320328131, 0.00011964503937633708, 0.00011893073678947985, 0.00011822907254099846, 0.0001175396828330122, 0.00011686221841955557, 0.00011619633005466312, 0.00011554169759619981, 0.00011489800817798823, 0.00011426495620980859, 0.00011364225065335631, 0.00011302962229819968, 0.00011242679465794936, 0.00011183350579813123, 0.00011124951561214402, 0.00011067457671742886, 0.00011010846355929971, 0.00010955095058307052, 0.00010900181951001287, 0.00010846086661331356], "accuracy_valid": [0.2912215267319277, 0.5345591349774097, 0.6976715455572289, 0.7349662321159638, 0.7748935193900602, 0.7899699383471386, 0.7840296733810241, 0.7850165309676205, 0.7801028332078314, 0.7844370646649097, 0.7771422604480422, 0.7880991740399097, 0.781139695500753, 0.7890654414533133, 0.7914862575301205, 0.7823809887989458, 0.7978545039533133, 0.7929613963667168, 0.7901023037462349, 0.7887801204819277, 0.7996752635542168, 0.7931643566453314, 0.8005091655685241, 0.7940497340926205, 0.8011401073042168, 0.7977324336408133, 0.7982707195971386, 0.8003062052899097, 0.8024519954819277, 0.8026858410203314, 0.8037535885730422, 0.8066126811935241, 0.8066126811935241, 0.8069891872176205, 0.8077216090926205, 0.8080878200301205, 0.8084540309676205, 0.8083319606551205, 0.8088202419051205, 0.8089423122176205, 0.8090643825301205, 0.8089423122176205, 0.8089423122176205, 0.8090643825301205, 0.8090643825301205, 0.8094408885542168, 0.8093188182417168, 0.8094408885542168, 0.8096850291792168, 0.8098070994917168, 0.8099291698042168, 0.8098070994917168, 0.8098070994917168, 0.8095629588667168, 0.8098070994917168, 0.8095629588667168, 0.8095629588667168, 0.8094408885542168, 0.8093188182417168, 0.8094408885542168, 0.8094408885542168, 0.8091967479292168, 0.8089526073042168, 0.8084540309676205, 0.8086981715926205, 0.8088202419051205, 0.8089423122176205, 0.8089423122176205, 0.8089423122176205, 0.8088202419051205, 0.8088202419051205, 0.8088202419051205, 0.8088202419051205, 0.8088202419051205, 0.8089423122176205, 0.8089423122176205, 0.8088202419051205, 0.8089423122176205, 0.8089423122176205, 0.8089423122176205, 0.8088202419051205, 0.8086981715926205, 0.8086981715926205, 0.8085761012801205, 0.8086981715926205, 0.8088202419051205, 0.8089423122176205, 0.8085658061935241, 0.8085658061935241, 0.8085658061935241, 0.8085658061935241, 0.8085658061935241, 0.8085658061935241, 0.8085658061935241, 0.8085658061935241, 0.8085658061935241, 0.8084437358810241, 0.8084437358810241, 0.8085658061935241, 0.8084437358810241, 0.8083216655685241], "accuracy_test": 0.7926717952806122, "start": "2016-01-25 16:51:23.540000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0], "accuracy_train_last": 0.9999534970238095, "batch_size_eval": 1024, "accuracy_train_std": [0.014231661596160588, 0.022394311983375294, 0.023183003498378183, 0.025863455246415937, 0.02691709638687425, 0.02732425883343306, 0.027445104244239033, 0.027005941189463725, 0.025805467434288368, 0.024454699073161917, 0.02216972296261455, 0.019004201246644998, 0.015807376754386877, 0.013575419966287899, 0.013776228203725012, 0.014355515451178098, 0.009296403569444018, 0.010667802711436164, 0.00979722950092424, 0.006867836104237817, 0.008245756267656328, 0.006308838648460384, 0.005189198747977426, 0.006549075638488453, 0.004066884010712043, 0.0034678251836871574, 0.0032358854644930807, 0.0030847461042710503, 0.0018024028065739858, 0.0012233023100786742, 0.0005959824041097608, 0.0005013918027540881, 0.0004877273289481731, 0.0004877273289481731, 0.0004877273289481731, 0.0004217437952756942, 0.0004027275873253528, 0.0004027275873253528, 0.0004217437952756942, 0.0004027275873253528, 0.0002866635976083043, 0.0002866635976083043, 0.0002866635976083043, 0.0002866635976083043, 0.0002866635976083043, 0.0002866635976083043, 0.0002866635976083043, 0.0002866635976083043, 0.0002866635976083043, 0.0002866635976083043, 0.0002866635976083043, 0.0002866635976083043, 0.0002866635976083043, 0.0002866635976083043, 0.0002866635976083043, 0.0002866635976083043, 0.0002866635976083043, 0.0002866635976083043, 0.0002866635976083043, 0.0002866635976083043, 0.0002866635976083043, 0.0002866635976083043, 0.0002866635976083043, 0.0002866635976083043, 0.0002866635976083043, 0.0002866635976083043, 0.0002866635976083043, 0.0002866635976083043, 0.0002866635976083043, 0.0002866635976083043, 0.0002866635976083043, 0.0002866635976083043, 0.0002866635976083043, 0.00025150329767466447, 0.00025150329767466447, 0.00025150329767466447, 0.00025150329767466447, 0.00025150329767466447, 0.00020796763183591793, 0.00020796763183591793, 0.00020796763183591793, 0.00020796763183591793, 0.00020796763183591793, 0.00020796763183591793, 0.00020796763183591793, 0.00020796763183591793, 0.00020796763183591793, 0.00020796763183591793, 0.00020796763183591793, 0.00020796763183591793, 0.00020796763183591793, 0.00020796763183591793, 0.00020796763183591793, 0.00020796763183591793, 0.00020796763183591793, 0.00020796763183591793, 0.00020796763183591793, 0.00020796763183591793, 0.00020796763183591793, 0.00020796763183591793, 0.00020796763183591793], "accuracy_test_std": 0.007300700105186275, "error_valid": [0.7087784732680723, 0.4654408650225903, 0.3023284544427711, 0.2650337678840362, 0.22510648060993976, 0.21003006165286142, 0.21597032661897586, 0.21498346903237953, 0.21989716679216864, 0.2155629353350903, 0.22285773955195776, 0.2119008259600903, 0.21886030449924698, 0.21093455854668675, 0.20851374246987953, 0.2176190112010542, 0.20214549604668675, 0.2070386036332832, 0.2098976962537651, 0.2112198795180723, 0.2003247364457832, 0.20683564335466864, 0.19949083443147586, 0.20595026590737953, 0.1988598926957832, 0.20226756635918675, 0.20172928040286142, 0.1996937947100903, 0.1975480045180723, 0.19731415897966864, 0.19624641142695776, 0.19338731880647586, 0.19338731880647586, 0.19301081278237953, 0.19227839090737953, 0.19191217996987953, 0.19154596903237953, 0.19166803934487953, 0.19117975809487953, 0.19105768778237953, 0.19093561746987953, 0.19105768778237953, 0.19105768778237953, 0.19093561746987953, 0.19093561746987953, 0.1905591114457832, 0.1906811817582832, 0.1905591114457832, 0.1903149708207832, 0.1901929005082832, 0.1900708301957832, 0.1901929005082832, 0.1901929005082832, 0.1904370411332832, 0.1901929005082832, 0.1904370411332832, 0.1904370411332832, 0.1905591114457832, 0.1906811817582832, 0.1905591114457832, 0.1905591114457832, 0.1908032520707832, 0.1910473926957832, 0.19154596903237953, 0.19130182840737953, 0.19117975809487953, 0.19105768778237953, 0.19105768778237953, 0.19105768778237953, 0.19117975809487953, 0.19117975809487953, 0.19117975809487953, 0.19117975809487953, 0.19117975809487953, 0.19105768778237953, 0.19105768778237953, 0.19117975809487953, 0.19105768778237953, 0.19105768778237953, 0.19105768778237953, 0.19117975809487953, 0.19130182840737953, 0.19130182840737953, 0.19142389871987953, 0.19130182840737953, 0.19117975809487953, 0.19105768778237953, 0.19143419380647586, 0.19143419380647586, 0.19143419380647586, 0.19143419380647586, 0.19143419380647586, 0.19143419380647586, 0.19143419380647586, 0.19143419380647586, 0.19143419380647586, 0.19155626411897586, 0.19155626411897586, 0.19143419380647586, 0.19155626411897586, 0.19167833443147586], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "momentum": 0.5602249761795977, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.0010900181946467168, "patience_threshold": 1, "do_flip": true, "batch_size": 128, "l2_decay": 4.8668391647430505e-08, "optimization": "rmsprop", "nb_data_augmentation": 0, "learning_rate_decay_method": "sqrt", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.03386221102970329}, "accuracy_valid_max": 0.8099291698042168, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8083216655685241, "loss_train": [1.3444125652313232, 0.9135624170303345, 0.6909664273262024, 0.5591657757759094, 0.46228736639022827, 0.37957993149757385, 0.30471664667129517, 0.23585037887096405, 0.17368485033512115, 0.12465237826108932, 0.09180095046758652, 0.06872059404850006, 0.054923105984926224, 0.043723464012145996, 0.036370135843753815, 0.031719598919153214, 0.02662336826324463, 0.02343442104756832, 0.019537894055247307, 0.016724811866879463, 0.01449526846408844, 0.01269506011158228, 0.012574845924973488, 0.009586409665644169, 0.00902813021093607, 0.006812826730310917, 0.0060021886602044106, 0.0032808869145810604, 0.0031833830289542675, 0.0018213107250630856, 0.0008451985195279121, 0.0005585112958215177, 0.0004912551958113909, 0.0004590437456499785, 0.00043746421579271555, 0.00042122413287870586, 0.00040831902879290283, 0.00039767034468241036, 0.0003886546764988452, 0.0003808889305219054, 0.0003741069813258946, 0.0003681004454847425, 0.00036273631849326193, 0.0003579080803319812, 0.00035352265695109963, 0.00034953447175212204, 0.0003458797582425177, 0.00034251404576934874, 0.0003394047380425036, 0.00033651708508841693, 0.0003338296082802117, 0.00033131931559182703, 0.00032896961783990264, 0.00032675726106390357, 0.0003246763371862471, 0.00032271468080580235, 0.0003208610287401825, 0.0003191047871951014, 0.000317436148179695, 0.00031584949465468526, 0.00031434078118763864, 0.00031290409970097244, 0.0003115307481493801, 0.0003102158079855144, 0.0003089565725531429, 0.0003077543224208057, 0.000306598813040182, 0.00030548847280442715, 0.00030442021670751274, 0.0003033937537111342, 0.0003024070174433291, 0.00030145581695251167, 0.00030053683440200984, 0.0002996488765347749, 0.00029879267094656825, 0.00029796434682793915, 0.0002971625654026866, 0.0002963850856758654, 0.00029563443968072534, 0.0002949053596239537, 0.0002941986022051424, 0.00029351154807955027, 0.0002928457106463611, 0.0002921978884842247, 0.00029156883829273283, 0.0002909584727603942, 0.0002903637068811804, 0.0002897850063163787, 0.0002892213233280927, 0.00028867239598184824, 0.00028813662356697023, 0.00028761461726389825, 0.00028710640617646277, 0.0002866092836484313, 0.0002861250250134617, 0.00028565144748426974, 0.0002851898898370564, 0.0002847377909347415, 0.0002842969261109829, 0.0002838652872014791, 0.00028344354359433055], "accuracy_train_first": 0.28993091604374305, "model": "residualv5", "loss_std": [0.2963019013404846, 0.1198386698961258, 0.10401467233896255, 0.09310104697942734, 0.08792054653167725, 0.08132758736610413, 0.07408703863620758, 0.06484273821115494, 0.05434633046388626, 0.041612692177295685, 0.033268798142671585, 0.02622278779745102, 0.023394815623760223, 0.018416021019220352, 0.016205301508307457, 0.0157211571931839, 0.014475020579993725, 0.012603888288140297, 0.011311364360153675, 0.009643377736210823, 0.009878735058009624, 0.007886496372520924, 0.009085876867175102, 0.006663432344794273, 0.006640374660491943, 0.005826829466968775, 0.005596165545284748, 0.002687779488041997, 0.003618096001446247, 0.0017827213741838932, 0.00045958574628457427, 0.00010414356802357361, 6.538285379065201e-05, 5.399108704295941e-05, 4.710693610832095e-05, 4.225230077281594e-05, 3.8620746636297554e-05, 3.5664503229781985e-05, 3.325332363601774e-05, 3.1229210435412824e-05, 2.9496453862520866e-05, 2.7990661692456342e-05, 2.665983083716128e-05, 2.5493887733318843e-05, 2.443930679874029e-05, 2.3493370463256724e-05, 2.2633696062257513e-05, 2.185111770813819e-05, 2.1133349946467206e-05, 2.0470226445468143e-05, 1.9860057363985106e-05, 1.9290595446364023e-05, 1.8763639673125e-05, 1.8267981431563385e-05, 1.7804894014261663e-05, 1.7369191482430324e-05, 1.6960391803877428e-05, 1.6575118934269994e-05, 1.6210748071898706e-05, 1.5864965462242253e-05, 1.553838410472963e-05, 1.5227202311507426e-05, 1.4932758858776651e-05, 1.4651525816589128e-05, 1.4383410416485276e-05, 1.4129304872767534e-05, 1.3885071894037537e-05, 1.3651223525812384e-05, 1.342673749604728e-05, 1.3213404599810019e-05, 1.3007636880502105e-05, 1.2809831787308212e-05, 1.2620802408491727e-05, 1.2438772500900086e-05, 1.2263662028999534e-05, 1.2093292752979323e-05, 1.192888248624513e-05, 1.1771663594117854e-05, 1.161961608886486e-05, 1.147228067566175e-05, 1.1330113011354115e-05, 1.1191622434125748e-05, 1.1057775736844633e-05, 1.0928080882877111e-05, 1.0801337339216843e-05, 1.0679440492822323e-05, 1.056171822710894e-05, 1.0447362910781521e-05, 1.0334650141885504e-05, 1.022521155391587e-05, 1.0118471436726395e-05, 1.0016181477112696e-05, 9.915789632941596e-06, 9.818223588808905e-06, 9.721668902784586e-06, 9.62836838880321e-06, 9.536372090224177e-06, 9.447298907616641e-06, 9.360029253002722e-06, 9.275094271288253e-06, 9.19240392249776e-06]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:14 2016", "state": "available"}], "summary": "4854991390f483fa50c027ad92957823"}