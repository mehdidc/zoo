{"content": {"hp_model": {"f0": 64, "f1": 32, "f2": 64, "f3": 64, "nonlin": "rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.5629593133926392, 1.190176248550415, 0.9925422072410583, 0.8446182012557983, 0.7372767925262451, 0.6603540182113647, 0.5975249409675598, 0.5484778881072998, 0.502648651599884, 0.4647897779941559, 0.4278543293476105, 0.39898815751075745, 0.37121573090553284, 0.34678369760513306, 0.3190248906612396, 0.301147997379303, 0.2824321389198303, 0.26369667053222656, 0.24750079214572906, 0.2332528531551361, 0.2197650969028473, 0.20952971279621124, 0.19923700392246246, 0.18649660050868988, 0.17727600038051605, 0.16844090819358826, 0.15926861763000488, 0.15315444767475128, 0.14915834367275238, 0.14139051735401154, 0.13515610992908478, 0.12771075963974, 0.12378141283988953, 0.1197655200958252, 0.11383436620235443, 0.1121077835559845, 0.10873504728078842, 0.10310132056474686, 0.09828954190015793, 0.09388326108455658, 0.09353958815336227, 0.09071642905473709, 0.08548039942979813, 0.08188759535551071, 0.08074431121349335, 0.07821200788021088, 0.07611728459596634, 0.07458130270242691, 0.07077480107545853, 0.06853050738573074, 0.06757953017950058, 0.06518348306417465, 0.06326017528772354, 0.061937835067510605, 0.06062043458223343, 0.059594105929136276, 0.05678439512848854, 0.05474964156746864, 0.05594570189714432, 0.05521726235747337, 0.05282362177968025, 0.05147475749254227, 0.04969192296266556, 0.049236781895160675, 0.04966270923614502, 0.04768820479512215, 0.046447087079286575, 0.046073269098997116, 0.04462478682398796, 0.04277869313955307, 0.04400176182389259, 0.04236093908548355, 0.04279478266835213, 0.041190650314092636, 0.04030928760766983, 0.03880682587623596, 0.03921617940068245, 0.03868018090724945, 0.03841957449913025, 0.038162536919116974, 0.038092177361249924, 0.037754788994789124, 0.036834076046943665, 0.036070968955755234, 0.036876924335956573, 0.03444104269146919, 0.0349862203001976, 0.03410514444112778, 0.03523598238825798, 0.036136142909526825, 0.03451626002788544, 0.03310558944940567, 0.03309515491127968, 0.03445511311292648, 0.03395488113164902, 0.03331675007939339, 0.03215496242046356, 0.03183094039559364, 0.031774044036865234, 0.03243470937013626, 0.03190317749977112, 0.032133959233760834, 0.032066460698843, 0.030580714344978333, 0.030789606273174286, 0.032239433377981186, 0.029928943142294884, 0.03132885321974754, 0.03172316029667854, 0.03098606877028942, 0.029972780495882034, 0.03111261874437332, 0.030663199722766876, 0.03088214062154293, 0.03028506226837635, 0.03088262304663658, 0.030460882931947708, 0.03065972402691841, 0.030079688876867294, 0.03007889911532402, 0.02993159554898739, 0.029513416811823845, 0.030339868739247322, 0.030206890776753426, 0.03012411668896675, 0.02955707721412182, 0.029700584709644318, 0.029738372191786766, 0.02937665581703186, 0.028961623087525368, 0.02909061498939991, 0.029465893283486366, 0.02887829765677452, 0.02894509583711624], "moving_avg_accuracy_train": [0.052645118124769276, 0.11146500596968437, 0.17187492393699239, 0.22955596150130347, 0.2859115697566271, 0.33910060078276394, 0.3891212390133653, 0.43608789158628125, 0.48001985561725263, 0.5200466882022143, 0.5566244031894403, 0.5905670156171279, 0.6209920978663233, 0.6497648224881443, 0.6761626149370966, 0.7005066377387911, 0.722283616631717, 0.7439241980460185, 0.7639609019378913, 0.7819033988322804, 0.7994023412026515, 0.8151956032121851, 0.8295049061707745, 0.843080715329906, 0.8555267721076389, 0.8667445713469027, 0.8768987193824782, 0.8868744340882871, 0.8957362838342203, 0.9040793942151025, 0.9118323341828964, 0.9185030965598726, 0.92486016926938, 0.9305349956829273, 0.935979486032501, 0.9407167669304506, 0.9453105269183764, 0.9493286534670334, 0.9530751396453393, 0.956758511097472, 0.9599271331270289, 0.9626347337274397, 0.9654667774677909, 0.9679644635602975, 0.9702705458126104, 0.9723320689468348, 0.9742316175950176, 0.9760458070259921, 0.9777111656460212, 0.9790681543266664, 0.9804963463344759, 0.9818282221176949, 0.9829501804118779, 0.983966918323071, 0.9849029448312494, 0.9859080930564578, 0.9867964504174787, 0.9875983332400257, 0.9883804456005469, 0.9891354999988257, 0.9898383004453717, 0.990368514299644, 0.9909270869768224, 0.991408912095816, 0.9918750707374249, 0.9923597176815395, 0.9927586975502903, 0.9931712578547851, 0.993519346689554, 0.9938581672289319, 0.9941607805655626, 0.9944563840566253, 0.9947247523473912, 0.9949593083626521, 0.9951866848180536, 0.9953796978838673, 0.9955603850895282, 0.995727653872242, 0.9958991221159703, 0.9960883207674686, 0.9962539492561978, 0.9963937143008161, 0.9965334537338297, 0.9966499186283039, 0.9967477976357207, 0.9968242269495295, 0.9968930133319575, 0.9969665468201903, 0.9970443527036473, 0.9971236785939969, 0.997188096448883, 0.9972600234111375, 0.9973084816354999, 0.997338143144569, 0.9973764642467787, 0.997457456214958, 0.9975024472006051, 0.9975475893853065, 0.9975765916074902, 0.9976050187562651, 0.9976189774461147, 0.9976547917550747, 0.9977033006748053, 0.9977399832561343, 0.9977892736209971, 0.997826659502945, 0.9978765828383648, 0.9978843114592901, 0.9978749911764564, 0.9978921795588107, 0.9978960233588821, 0.9979180839694224, 0.9979239876260515, 0.9979316260658273, 0.9979454761080542, 0.9979625914436773, 0.9979663695016905, 0.9979697697539024, 0.9979960814689883, 0.9980151117149466, 0.9980159628946424, 0.99800975350994, 0.998013465658946, 0.9980261071882895, 0.9980374845646988, 0.9980430739058479, 0.9980434540152631, 0.9980577470065939, 0.9980566598059345, 0.998051031027722, 0.9980506154249498, 0.998059541977693, 0.9980582752799236, 0.9980524849543122], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.051635742187499986, 0.10966135048004516, 0.16966263001223642, 0.2261163653637989, 0.2806939054558075, 0.33125412122611536, 0.378967935148308, 0.4231056272321218, 0.46419379615424095, 0.5011557936095999, 0.5351144506473597, 0.5653791256880454, 0.592164643559753, 0.6175148148871361, 0.6399503745317057, 0.6606358070051164, 0.6781081767469993, 0.6958852673460193, 0.7122264457601373, 0.7268287905948916, 0.7408853987812609, 0.7534844294980144, 0.7648500302229118, 0.7757678121686176, 0.7855805793798432, 0.7943754487761961, 0.8020394840472964, 0.8094692246956541, 0.8164723445830164, 0.8229888490648504, 0.8288038454648412, 0.8336670132526944, 0.8386450973189912, 0.8431497870411584, 0.8475468341747684, 0.8514075498536771, 0.8551406006382642, 0.8583141523496637, 0.8612781531538539, 0.8641451844122637, 0.8666980099563536, 0.8685784548662152, 0.8707408995245486, 0.8726565086025907, 0.8744833605661269, 0.8761376753472401, 0.8775390504141727, 0.8790353100941409, 0.880382973314772, 0.8815368940744093, 0.8830891475792424, 0.8842552716485018, 0.8853769959896757, 0.8861821168914612, 0.8869117261737006, 0.887742361491195, 0.8884644897057803, 0.8895945973522956, 0.8901304724721112, 0.8910054440972645, 0.8916759957907006, 0.892201102583995, 0.8926279590989389, 0.8930100709450691, 0.8934505983479266, 0.893811481425408, 0.8940762705475509, 0.8943756159137296, 0.8947304759620404, 0.8951903044012279, 0.8954270745070388, 0.8958263615969976, 0.8960768862053701, 0.8963216248721071, 0.89650629808708, 0.896698977060375, 0.896782820882952, 0.8968481323093406, 0.8970432489541595, 0.8973541607869062, 0.8976197153878089, 0.8977854723411214, 0.8979967182640122, 0.8984086251744332, 0.8984365150101525, 0.898373078117571, 0.8986486632839765, 0.8986016621664222, 0.8986946680130331, 0.8988760295249828, 0.8991033785679664, 0.8990994436667421, 0.8991447303806401, 0.8992210800082387, 0.8991789018831678, 0.8992040357441734, 0.8990669353041687, 0.8990523786807549, 0.8991644365581614, 0.8991666028891675, 0.899156345555823, 0.899147113955813, 0.8992974969220541, 0.8994440191142613, 0.8995514750247477, 0.8996380373302548, 0.8998756643201209, 0.9000030498835907, 0.9000322476719635, 0.9000707327127491, 0.9001409608345464, 0.9003028519028237, 0.9003030989979329, 0.9003775930796908, 0.9004791998297036, 0.9005462318422152, 0.9004447807212467, 0.900316853618625, 0.9002515768599252, 0.9001816502545049, 0.8999712024259671, 0.8999293132639427, 0.899902790540711, 0.8998667130585525, 0.8997711491510406, 0.8998103004727588, 0.8998556846762359, 0.8998731459055249, 0.8998278258556351, 0.8997636532568939, 0.8998401752617767, 0.8998235958474213, 0.8998463249769111, 0.8998779587160423], "moving_var_accuracy_train": [0.0249435761613382, 0.05358723139999991, 0.08107273195935191, 0.10290937761387592, 0.12120203108893589, 0.13454348517353643, 0.14360771489715313, 0.14909974149258276, 0.15155992451589154, 0.15082325800536334, 0.1477822953080076, 0.14337297422295273, 0.13736684746949013, 0.1310809898620094, 0.1242444818914101, 0.11715371671779391, 0.10970647633334102, 0.10295066157554804, 0.09626882094364857, 0.08953933760253019, 0.08334132069901129, 0.07725203275322619, 0.07136963483834988, 0.06589139470344132, 0.060696394196928155, 0.05575930595518718, 0.0511113358606239, 0.04689583622958676, 0.04291304403490356, 0.03924820704886145, 0.03586435904727325, 0.03267841477875665, 0.029774284661786662, 0.027086689089023057, 0.024644802456620155, 0.02238229868371284, 0.02033399249278157, 0.018445901312152617, 0.01672763660909549, 0.015176977975475419, 0.01374964166802362, 0.012440657410323365, 0.011268775915016392, 0.010198044245865063, 0.009226101959468446, 0.008341740662218083, 0.0075400411615975955, 0.00681565859506097, 0.006159053509554619, 0.00555972092311375, 0.005022106422502915, 0.004535860818169952, 0.004093603850077929, 0.003693547268890654, 0.003332077852617701, 0.003007962973947687, 0.0027142692857608382, 0.002448629501734619, 0.002209271849261478, 0.0019934756286345695, 0.0017985734219800999, 0.001621246220363451, 0.0014619296292483189, 0.0013178260653311262, 0.0011879991937103333, 0.001071313218283257, 0.0009656145608759471, 0.0008705849588319559, 0.0007846169554807767, 0.0007071884541538382, 0.0006372937822220148, 0.00057435083681517, 0.0005175639469890512, 0.0004663027010088016, 0.0004201377313801598, 0.00037845924463431697, 0.00034090715096749116, 0.00030706824548177723, 0.00027662603316106453, 0.0002492855950125167, 0.00022460393067777392, 0.00020231934601927097, 0.00018226315539959459, 0.00016415891650443904, 0.00014782924755483146, 0.000133098895759432, 0.00011983159028115658, 0.00010789709581806611, 9.716187003576445e-05, 8.750231640410576e-05, 7.878943170394823e-05, 7.095704992464594e-05, 6.388247872775658e-05, 5.750214910106321e-05, 5.176515075282803e-05, 4.66476729677312e-05, 4.200112337006353e-05, 3.781935138461371e-05, 3.404498640617664e-05, 3.064776069064615e-05, 2.758473822678241e-05, 2.4837808386640684e-05, 2.2375205585617508e-05, 2.014979553301238e-05, 1.8156681840325873e-05, 1.6353592993814487e-05, 1.4740664749207971e-05, 1.3267135858519857e-05, 1.1941204081716786e-05, 1.0749742637936747e-05, 9.674901347333967e-06, 8.711791247437293e-06, 7.840925801147924e-06, 7.057358332893009e-06, 6.353348912630863e-06, 5.720650433789216e-06, 5.148713853911457e-06, 4.633946523956253e-06, 4.176782628717504e-06, 3.7623637181968573e-06, 3.3861338669390426e-06, 3.0478674883705726e-06, 2.7432047599856997e-06, 2.4703225583644107e-06, 2.2244553047735657e-06, 2.0022909409065455e-06, 1.8020631471643988e-06, 1.6236954388586087e-06, 1.4613365330202119e-06, 1.3154880280156798e-06, 1.1839407797450902e-06, 1.0662638518654611e-06, 9.596519073880635e-07, 8.639884674854325e-07], "duration": 97354.784905, "accuracy_train": [0.5264511812476929, 0.6408439965739203, 0.7155641856427648, 0.7486852995801033, 0.7931120440545404, 0.8178018800179956, 0.839306983088778, 0.8587877647425249, 0.8754075318959949, 0.8802881814668696, 0.885823838074474, 0.8960505274663161, 0.8948178381090809, 0.908719344084533, 0.913742746977667, 0.9196028429540422, 0.918276426668051, 0.9386894307747323, 0.9442912369647471, 0.9433858708817828, 0.9568928225359912, 0.9573349612979882, 0.9582886327980805, 0.9652629977620893, 0.9675412831072352, 0.9677047645002769, 0.9682860517026578, 0.9766558664405685, 0.9754929315476191, 0.9791673876430418, 0.9816087938930418, 0.9785399579526578, 0.9820738236549464, 0.9816084334048542, 0.9849798991786637, 0.9833522950119971, 0.9866543668097084, 0.9854917924049464, 0.9867935152500923, 0.9899088541666666, 0.9884447313930418, 0.987003139131137, 0.9909551711309523, 0.9904436383928571, 0.9910252860834257, 0.9908857771548542, 0.9913275554286637, 0.9923735119047619, 0.9926993932262828, 0.9912810524524732, 0.9933500744047619, 0.9938151041666666, 0.9930478050595238, 0.9931175595238095, 0.9933271834048542, 0.9949544270833334, 0.9947916666666666, 0.9948152786429494, 0.9954194568452381, 0.9959309895833334, 0.9961635044642857, 0.9951404389880952, 0.9959542410714286, 0.9957453381667589, 0.9960704985119048, 0.9967215401785714, 0.9963495163690477, 0.9968843005952381, 0.9966521462024732, 0.9969075520833334, 0.9968843005952381, 0.9971168154761905, 0.9971400669642857, 0.9970703125, 0.9972330729166666, 0.9971168154761905, 0.9971865699404762, 0.9972330729166666, 0.9974423363095238, 0.9977911086309523, 0.9977446056547619, 0.9976515997023809, 0.9977911086309523, 0.9976981026785714, 0.9976287087024732, 0.9975120907738095, 0.9975120907738095, 0.9976283482142857, 0.9977446056547619, 0.9978376116071429, 0.9977678571428571, 0.9979073660714286, 0.9977446056547619, 0.9976050967261905, 0.9977213541666666, 0.9981863839285714, 0.9979073660714286, 0.9979538690476191, 0.9978376116071429, 0.9978608630952381, 0.9977446056547619, 0.9979771205357143, 0.9981398809523809, 0.9980701264880952, 0.9982328869047619, 0.9981631324404762, 0.9983258928571429, 0.9979538690476191, 0.9977911086309523, 0.998046875, 0.9979306175595238, 0.9981166294642857, 0.9979771205357143, 0.9980003720238095, 0.9980701264880952, 0.9981166294642857, 0.9980003720238095, 0.9980003720238095, 0.9982328869047619, 0.9981863839285714, 0.9980236235119048, 0.9979538690476191, 0.998046875, 0.9981398809523809, 0.9981398809523809, 0.9980933779761905, 0.998046875, 0.9981863839285714, 0.998046875, 0.9980003720238095, 0.998046875, 0.9981398809523809, 0.998046875, 0.9980003720238095], "end": "2016-02-04 14:41:03.153000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0], "moving_var_accuracy_valid": [0.0239962488412857, 0.05189936491663612, 0.07911081033447388, 0.09988294741732481, 0.11670302361804566, 0.12803974002490182, 0.1357252383734263, 0.13968593729845366, 0.14091148219696167, 0.13911603728027522, 0.1355831470425215, 0.1302683873361341, 0.12369872431142076, 0.11711253255722778, 0.10993146833059034, 0.10278930554703963, 0.09525792833190928, 0.08857636005021023, 0.08212203105284752, 0.07582888421962046, 0.07002428990100434, 0.06445048108591918, 0.059168024895867345, 0.054324004069806434, 0.04975821726590113, 0.04547854308860118, 0.041459325709471084, 0.037810202553440704, 0.03447057549150756, 0.031405701418312656, 0.028569458924668552, 0.025925366640597018, 0.023555861865277382, 0.02138290574418663, 0.019418621381224663, 0.01761090537308254, 0.015975235849217045, 0.01446835513847968, 0.013100587331536876, 0.011864507412513478, 0.010736708935589152, 0.009694862699561458, 0.0087674619317085, 0.007923741761796546, 0.007161404078486973, 0.006469894487193362, 0.00584057970717801, 0.005276670873729298, 0.004765349551762544, 0.004300798394661987, 0.0038924039736851898, 0.0035154021844208275, 0.0031751863554569833, 0.0028635016969097114, 0.0025819424945613093, 0.0023299578403811996, 0.0021016552787677814, 0.0019029840405254154, 0.0017152700957692108, 0.0015506332642957013, 0.0013996166940282628, 0.001262136658924712, 0.0011375628513913911, 0.0010251206514188293, 0.0009243551658109622, 0.0008330917785903781, 0.0007504136202441874, 0.0006761787270540422, 0.0006096941852336221, 0.0005506277464516308, 0.0004960695125535193, 0.000447897432920037, 0.0004036725528426352, 0.00036384437069333795, 0.0003277668713909604, 0.0002953243109326146, 0.000265855147918612, 0.00023930802336850294, 0.00021571985557742131, 0.00019501786552935638, 0.00017615075219096633, 0.00015878295528001264, 0.0001433062833114536, 0.0001305026607059819, 0.00011745939522181177, 0.00010574967385369418, 9.585823112380976e-05, 8.629228995689096e-05, 7.774091174873597e-05, 7.026284855601252e-05, 6.370175198652152e-05, 5.733171613889818e-05, 5.161700250310954e-05, 4.650776564350865e-05, 4.187300002726822e-05, 3.7691385423262786e-05, 3.4091415656781824e-05, 3.068418114867056e-05, 2.772877574480303e-05, 2.4955940407232982e-05, 2.246129328249574e-05, 2.021593095619487e-05, 1.8397873189394295e-05, 1.6751304645737818e-05, 1.518009513545043e-05, 1.3729522916517818e-05, 1.2864769901681415e-05, 1.1724336647538034e-05, 1.0559575580397028e-05, 9.516947907635733e-06, 8.609641018692847e-06, 7.98455537871522e-06, 7.1861003903476355e-06, 6.517434665265469e-06, 5.958606583572492e-06, 5.403185541527404e-06, 4.95549795688655e-06, 4.607236253464425e-06, 4.1848621251552096e-06, 3.8103834839500604e-06, 3.827939732381684e-06, 3.460938076199491e-06, 3.121175362208128e-06, 2.8207720884573513e-06, 2.6208870233820616e-06, 2.3725937549744114e-06, 2.1538719128042608e-06, 1.9412287722784023e-06, 1.765591057348724e-06, 1.6260950534766766e-06, 1.5161861032106938e-06, 1.3670413857129333e-06, 1.234986767087916e-06, 1.1204943314418983e-06], "accuracy_test": 0.8960459183673469, "start": "2016-02-03 11:38:28.368000", "learning_rate_per_epoch": [0.002098081400617957, 0.0020046222489327192, 0.0019153262255713344, 0.0018300078809261322, 0.001748490147292614, 0.0016706035239621997, 0.0015961864264681935, 0.0015250842552632093, 0.0014571492793038487, 0.0013922405196353793, 0.0013302230508998036, 0.001270968234166503, 0.0012143529020249844, 0.0011602594750002027, 0.0011085756123065948, 0.0010591940954327583, 0.0010120122460648417, 0.0009669321007095277, 0.0009238600614480674, 0.0008827066631056368, 0.000843386456836015, 0.0008058177772909403, 0.0007699225679971278, 0.0007356263231486082, 0.0007028577965684235, 0.0006715489435009658, 0.0006416347459889948, 0.0006130530964583158, 0.0005857446230947971, 0.000559652631636709, 0.00053472287254408, 0.0005109035992063582, 0.0004881453642155975, 0.00046640090295113623, 0.0004456250462681055, 0.00042577466228976846, 0.0004068085108883679, 0.00038868721458129585, 0.0003713731130119413, 0.0003548302920535207, 0.0003390243509784341, 0.0003239224897697568, 0.00030949333449825644, 0.00029570693732239306, 0.0002825346600729972, 0.0002699491451494396, 0.00025792425731197, 0.00024643499637022614, 0.00023545754083897918, 0.00022496907331515104, 0.00021494780958164483, 0.00020537295495159924, 0.00019622460240498185, 0.00018748376169241965, 0.00017913228657562286, 0.00017115283117163926, 0.00016352882084902376, 0.00015624442312400788, 0.00014928450400475413, 0.00014263461343944073, 0.00013628094166051596, 0.00013021030463278294, 0.00012441008584573865, 0.00011886823631357402, 0.0001135732454713434, 0.00010851411934709176, 0.00010368035145802423, 9.906190825859085e-05, 9.46491927606985e-05, 9.043303725775331e-05, 8.640469604870304e-05, 8.255579450633377e-05, 7.887834362918511e-05, 7.536470366176218e-05, 7.200757681857795e-05, 6.879999273223802e-05, 6.573529390152544e-05, 6.280711386352777e-05, 6.000936991767958e-05, 5.7336248573847115e-05, 5.478220191434957e-05, 5.2341925766086206e-05, 5.001035242457874e-05, 4.778263974003494e-05, 4.5654160203412175e-05, 4.362049367045984e-05, 4.167741644778289e-05, 3.982089401688427e-05, 3.8047070120228454e-05, 3.6352263123262674e-05, 3.473295146250166e-05, 3.3185770007548854e-05, 3.17075100610964e-05, 3.0295097531052306e-05, 2.8945600206498057e-05, 2.765621684375219e-05, 2.6424269890412688e-05, 2.524720002838876e-05, 2.4122562535922043e-05, 2.304802183061838e-05, 2.2021346012479626e-05, 2.1040405044914223e-05, 2.01031598408008e-05, 1.920766408147756e-05, 1.8352056940784678e-05, 1.7534563085064292e-05, 1.6753485397202894e-05, 1.600720133865252e-05, 1.529415931145195e-05, 1.4612880477216095e-05, 1.3961948752694298e-05, 1.3340013538254425e-05, 1.2745782441925257e-05, 1.217802127939649e-05, 1.1635550436039921e-05, 1.1117243957414757e-05, 1.0622025911288802e-05, 1.0148867659154348e-05, 9.69678603723878e-06, 9.264842447009869e-06, 8.852139217196964e-06, 8.457820513285697e-06, 8.08106688054977e-06, 7.721095244050957e-06, 7.377158908639103e-06, 7.048543011478614e-06, 6.73456543154316e-06, 6.4345740611315705e-06, 6.14794589637313e-06, 5.874085218238179e-06, 5.612424047285458e-06, 5.3624185056833085e-06, 5.1235492719570175e-06, 4.895320671494119e-06], "accuracy_train_first": 0.5264511812476929, "accuracy_train_last": 0.9980003720238095, "batch_size_eval": 1024, "accuracy_train_std": [0.01730342599036177, 0.019947447319439263, 0.02111927022609534, 0.02183150801417206, 0.02090241043403035, 0.020170027070344206, 0.01997399897245223, 0.020032178685903262, 0.020181477503809175, 0.019694561218867132, 0.018108244545271754, 0.019233170853545516, 0.018985029478401545, 0.020520937391511566, 0.01680637684715754, 0.017233248393862692, 0.015861538286434804, 0.014591252608099941, 0.015204599522534482, 0.014198310524119807, 0.01184469034974622, 0.01120507232789061, 0.011963862665719076, 0.011819311916501117, 0.010968707870440039, 0.010053542730982419, 0.010681712640745978, 0.007760444511379989, 0.009217330968254856, 0.007988406897043523, 0.006628088424989104, 0.007330361479692494, 0.006637981602339628, 0.006779829815881129, 0.005883197923640808, 0.0066034065212368966, 0.005047298681031193, 0.00497909423078335, 0.006070331577992477, 0.004652622186860403, 0.004010422875966032, 0.0044942959754865, 0.0041599375045622685, 0.004338817393996648, 0.0045072740811125354, 0.003810801876541482, 0.0035036506279439443, 0.0037231433968095413, 0.0037053230036122125, 0.00406296273514988, 0.0034575205162650866, 0.0029348499489378088, 0.0035544369688585336, 0.003196212849226113, 0.0036038534765402215, 0.0028663530704150274, 0.0031440276681334394, 0.0028870559162815315, 0.002348860677999401, 0.0023626303749188027, 0.002395806074057898, 0.002489647048266477, 0.0023496661266108223, 0.0023524347204165745, 0.002146831439804151, 0.0019039263482081726, 0.0019421604880730645, 0.0017343802268388686, 0.0018598095329785166, 0.00205417389914014, 0.0018361316055059182, 0.0022947228034738334, 0.0021030710702680437, 0.0017572957243016355, 0.0018810726906790252, 0.0019756925024412204, 0.0019084642395805808, 0.001690336808842619, 0.0014909988413208143, 0.0015522871773134784, 0.0016016545851278386, 0.0017451012459040918, 0.0017320407892916381, 0.001620446901416102, 0.0016250391663336294, 0.0017599088048889753, 0.0017727639617872856, 0.0015680538783563397, 0.0015585436221046216, 0.0015880953605402887, 0.0015845168375558797, 0.0014698177668600362, 0.001483911354688323, 0.0017599088048889753, 0.001853713860490583, 0.0016169399786750104, 0.0016585337146695508, 0.001715575404175662, 0.0017119475571242527, 0.0014173875236354323, 0.0014685298246501235, 0.0013204306979632333, 0.001519018133469222, 0.0015439058129681946, 0.001449073330625268, 0.0015396980218697269, 0.0013857574800735992, 0.0016888969305556992, 0.0017837089298745042, 0.0014764237226922937, 0.0014485135906968489, 0.001267792585585527, 0.001354386707515876, 0.0012777745223245987, 0.001389069664364197, 0.0013875119743150756, 0.0014756911879299137, 0.0014445893384495043, 0.001334886541518538, 0.0014542871810929957, 0.0014053211715273043, 0.0013943140206998652, 0.0014609633623888587, 0.0013943140206998652, 0.0013445714559896726, 0.001444589338449504, 0.0015218627392069913, 0.0015154548676339587, 0.0014917238590351043, 0.0015797328637893442, 0.0016368783395459487, 0.001426512430262088, 0.0015367105321668813, 0.0014128029697696531], "accuracy_test_std": 0.007296594321948106, "error_valid": [0.483642578125, 0.36810817488704817, 0.29032585419804224, 0.2658000164721386, 0.22810823371611444, 0.21370393684111444, 0.19160773955195776, 0.1796551440135542, 0.16601268354668675, 0.16618622929216864, 0.15925763601280118, 0.1622387989457832, 0.16676569559487953, 0.15433364316641573, 0.15812958866716864, 0.15319530073418675, 0.1646404955760542, 0.14412091726280118, 0.14070294851280118, 0.1417501058923193, 0.13260512754141573, 0.13312429405120485, 0.13285956325301207, 0.12597215032003017, 0.1261045157191265, 0.1264707266566265, 0.12898419851280118, 0.12366310946912651, 0.12049957643072284, 0.11836261059864461, 0.11886118693524095, 0.12256447665662651, 0.11655214608433728, 0.11630800545933728, 0.11287974162274095, 0.11384600903614461, 0.11126194230045183, 0.11312388224774095, 0.11204583960843373, 0.11005153426204817, 0.11032656014683728, 0.11449754094503017, 0.10979709855045183, 0.11010300969503017, 0.10907497176204817, 0.10897349162274095, 0.10984857398343373, 0.10749835278614461, 0.10748805769954817, 0.10807781908885539, 0.10294057087725905, 0.10524961172816272, 0.10452748493975905, 0.10657179499246983, 0.10652179028614461, 0.10478192065135539, 0.10503635636295183, 0.10023443382906627, 0.10504665144954817, 0.10111981127635539, 0.10228903896837349, 0.10307293627635539, 0.10353033226656627, 0.10355092243975905, 0.10258465502635539, 0.10294057087725905, 0.10354062735316272, 0.10293027579066272, 0.10207578360316272, 0.10067123964608427, 0.10244199454066272, 0.10058005459337349, 0.10166839231927716, 0.10147572712725905, 0.10183164297816272, 0.10156691217996983, 0.10246258471385539, 0.10256406485316272, 0.10120070124246983, 0.09984763271837349, 0.09999029320406627, 0.10072271507906627, 0.10010206842996983, 0.09788421263177716, 0.10131247646837349, 0.10219785391566272, 0.09887107021837349, 0.10182134789156627, 0.10046827936746983, 0.09949171686746983, 0.09885048004518071, 0.10093597044427716, 0.10044768919427716, 0.10009177334337349, 0.10120070124246983, 0.10056975950677716, 0.10216696865587349, 0.10107863092996983, 0.09982704254518071, 0.10081390013177716, 0.10093597044427716, 0.10093597044427716, 0.09934905638177716, 0.09923728115587349, 0.09948142178087349, 0.09958290192018071, 0.09798569277108427, 0.09885048004518071, 0.09970497223268071, 0.09958290192018071, 0.09922698606927716, 0.09824012848268071, 0.09969467714608427, 0.09895196018448793, 0.09860633942018071, 0.09885048004518071, 0.10046827936746983, 0.10083449030496983, 0.10033591396837349, 0.10044768919427716, 0.10192282803087349, 0.10044768919427716, 0.10033591396837349, 0.10045798428087349, 0.10108892601656627, 0.09983733763177716, 0.09973585749246983, 0.09996970303087349, 0.10058005459337349, 0.10081390013177716, 0.09947112669427716, 0.10032561888177716, 0.09994911285768071, 0.09983733763177716], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.04454504348027943, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 32, "valid_ratio": 0.15, "learning_rate": 0.002195897795635352, "optimization": "adam", "nb_data_augmentation": 1, "learning_rate_decay_method": "exp", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 2.2392922230214073e-07, "rotation_range": [0, 0], "momentum": 0.853517450221749}, "accuracy_valid_max": 0.9021157873682228, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.9001626623682228, "accuracy_valid_std": [0.017254715316488888, 0.01575394806680803, 0.012586970478066024, 0.01769357733288853, 0.010534743383542024, 0.017612077845116408, 0.018400154343535806, 0.01587581869058647, 0.013387378075537373, 0.013734440237076026, 0.010972861935648661, 0.014273991314553109, 0.015824027830679172, 0.011385840219002304, 0.017689480327726007, 0.014082765450279236, 0.018211001349427356, 0.011208233198793245, 0.013011960091302188, 0.00919192241545468, 0.010788588244089627, 0.014313990725250499, 0.013022727715909784, 0.009770778160770634, 0.01029275743725044, 0.009739865713937928, 0.01680003218889021, 0.009704012630173104, 0.012635133141401647, 0.0070661311317778, 0.007883377495432711, 0.011347527597057655, 0.008989366225950228, 0.009782517986584388, 0.009594229742600236, 0.009262974151262, 0.007706192133104424, 0.009551545131807046, 0.01085778992616812, 0.007274310981340578, 0.009798444027014824, 0.01101339503790698, 0.00651937129082464, 0.0123775676501923, 0.007743319839206727, 0.010551009742768966, 0.012453106982712767, 0.010065641203945071, 0.008463335489246297, 0.007103116447175879, 0.007311235167148863, 0.0059133043062267006, 0.008297845531528165, 0.007911148463397891, 0.010410602399312236, 0.007286582343069185, 0.008618704663962197, 0.00807237355640517, 0.009427561955902728, 0.00996314125720374, 0.005249747624612308, 0.008646814028100757, 0.0056305033174071754, 0.0077678897990043, 0.007271685328088343, 0.007815596855045959, 0.00677060305912046, 0.00519823213454267, 0.006187480936088008, 0.0052298469152084835, 0.005119723494315584, 0.004702735358974971, 0.004237910536525092, 0.00727746684565752, 0.006737216134500896, 0.006445563730816895, 0.00855911361360537, 0.006431169839326469, 0.004453934387225809, 0.005359404620628875, 0.006539040425101044, 0.006814175945018639, 0.005444014066977564, 0.004425028893532048, 0.0037993865583185573, 0.006703703103358235, 0.006242093137480442, 0.005755323050419379, 0.004676596262489267, 0.004997860709308791, 0.004848707201765085, 0.006151759003594888, 0.0050310401125385614, 0.0065461737841571955, 0.0052408898153808335, 0.004018843140150209, 0.004828207009374741, 0.004740898042501973, 0.004601899713320033, 0.004489154797491052, 0.0040480200986618324, 0.004221016251304836, 0.004457207169541842, 0.006246017850603999, 0.005352046647045134, 0.004422129493130425, 0.0052223579709966585, 0.004774380147322154, 0.0038287348270010578, 0.0035554311215475172, 0.004461776499380046, 0.003642849206239423, 0.00639291365030642, 0.0042588320672480974, 0.0037123413261113836, 0.0037698324821519146, 0.004599488761267267, 0.005168191019418951, 0.004967478944987489, 0.004119067169325761, 0.005045243068900468, 0.003787361875408811, 0.0048949557027198245, 0.0044802705851874295, 0.0055301254539889075, 0.003699136590757537, 0.005092163744549846, 0.004075538012768449, 0.0035801390017138313, 0.00472210525165958, 0.0036222119535967087, 0.004156403376130793, 0.004257840541984749, 0.004038095576778549], "accuracy_valid": [0.516357421875, 0.6318918251129518, 0.7096741458019578, 0.7341999835278614, 0.7718917662838856, 0.7862960631588856, 0.8083922604480422, 0.8203448559864458, 0.8339873164533133, 0.8338137707078314, 0.8407423639871988, 0.8377612010542168, 0.8332343044051205, 0.8456663568335843, 0.8418704113328314, 0.8468046992658133, 0.8353595044239458, 0.8558790827371988, 0.8592970514871988, 0.8582498941076807, 0.8673948724585843, 0.8668757059487951, 0.8671404367469879, 0.8740278496799698, 0.8738954842808735, 0.8735292733433735, 0.8710158014871988, 0.8763368905308735, 0.8795004235692772, 0.8816373894013554, 0.881138813064759, 0.8774355233433735, 0.8834478539156627, 0.8836919945406627, 0.887120258377259, 0.8861539909638554, 0.8887380576995482, 0.886876117752259, 0.8879541603915663, 0.8899484657379518, 0.8896734398531627, 0.8855024590549698, 0.8902029014495482, 0.8898969903049698, 0.8909250282379518, 0.891026508377259, 0.8901514260165663, 0.8925016472138554, 0.8925119423004518, 0.8919221809111446, 0.897059429122741, 0.8947503882718373, 0.895472515060241, 0.8934282050075302, 0.8934782097138554, 0.8952180793486446, 0.8949636436370482, 0.8997655661709337, 0.8949533485504518, 0.8988801887236446, 0.8977109610316265, 0.8969270637236446, 0.8964696677334337, 0.896449077560241, 0.8974153449736446, 0.897059429122741, 0.8964593726468373, 0.8970697242093373, 0.8979242163968373, 0.8993287603539157, 0.8975580054593373, 0.8994199454066265, 0.8983316076807228, 0.898524272872741, 0.8981683570218373, 0.8984330878200302, 0.8975374152861446, 0.8974359351468373, 0.8987992987575302, 0.9001523672816265, 0.9000097067959337, 0.8992772849209337, 0.8998979315700302, 0.9021157873682228, 0.8986875235316265, 0.8978021460843373, 0.9011289297816265, 0.8981786521084337, 0.8995317206325302, 0.9005082831325302, 0.9011495199548193, 0.8990640295557228, 0.8995523108057228, 0.8999082266566265, 0.8987992987575302, 0.8994302404932228, 0.8978330313441265, 0.8989213690700302, 0.9001729574548193, 0.8991860998682228, 0.8990640295557228, 0.8990640295557228, 0.9006509436182228, 0.9007627188441265, 0.9005185782191265, 0.9004170980798193, 0.9020143072289157, 0.9011495199548193, 0.9002950277673193, 0.9004170980798193, 0.9007730139307228, 0.9017598715173193, 0.9003053228539157, 0.9010480398155121, 0.9013936605798193, 0.9011495199548193, 0.8995317206325302, 0.8991655096950302, 0.8996640860316265, 0.8995523108057228, 0.8980771719691265, 0.8995523108057228, 0.8996640860316265, 0.8995420157191265, 0.8989110739834337, 0.9001626623682228, 0.9002641425075302, 0.9000302969691265, 0.8994199454066265, 0.8991860998682228, 0.9005288733057228, 0.8996743811182228, 0.9000508871423193, 0.9001626623682228], "seed": 492501998, "model": "residualv3", "loss_std": [0.2668949067592621, 0.20311866700649261, 0.19338417053222656, 0.18600288033485413, 0.18082641065120697, 0.174704447388649, 0.16773326694965363, 0.16051414608955383, 0.15264733135700226, 0.1457333117723465, 0.14051072299480438, 0.1318606287240982, 0.12990060448646545, 0.12380748242139816, 0.11763249337673187, 0.11247656494379044, 0.108099564909935, 0.10355320572853088, 0.10096914321184158, 0.0918111652135849, 0.09011298418045044, 0.0895920842885971, 0.0855279192328453, 0.08105316758155823, 0.07936801016330719, 0.07677551358938217, 0.07432686537504196, 0.07005836814641953, 0.06822286546230316, 0.0694350004196167, 0.06707759201526642, 0.06482841819524765, 0.06300678104162216, 0.06050381809473038, 0.05994146317243576, 0.05571534112095833, 0.05684385448694229, 0.05620022863149643, 0.05450928956270218, 0.05177731066942215, 0.05112553387880325, 0.05071878805756569, 0.04797264561057091, 0.04535554721951485, 0.04733562842011452, 0.043657660484313965, 0.04571467638015747, 0.04497319832444191, 0.04201788455247879, 0.040736451745033264, 0.04174243286252022, 0.04004807770252228, 0.038979142904281616, 0.03811024874448776, 0.038874316960573196, 0.0382409505546093, 0.03581548109650612, 0.03484939783811569, 0.03489245846867561, 0.03597142919898033, 0.03201288729906082, 0.032263703644275665, 0.03166782110929489, 0.0331009142100811, 0.032564785331487656, 0.030909311026334763, 0.030530832707881927, 0.03169453516602516, 0.02924257516860962, 0.027088098227977753, 0.028322957456111908, 0.028143078088760376, 0.028639504685997963, 0.02639196068048477, 0.025325242429971695, 0.025618387386202812, 0.025386307388544083, 0.02699453942477703, 0.02562054805457592, 0.02494843676686287, 0.025310300290584564, 0.024084709584712982, 0.024254504591226578, 0.02396344766020775, 0.024466196075081825, 0.022492215037345886, 0.0234694704413414, 0.021997634321451187, 0.023966917768120766, 0.024342838674783707, 0.023663772270083427, 0.020537715405225754, 0.022477591410279274, 0.02304082363843918, 0.02320142462849617, 0.023089053109288216, 0.021254850551486015, 0.021710798144340515, 0.02136753872036934, 0.021780356764793396, 0.02079204097390175, 0.022253958508372307, 0.02214094251394272, 0.01968713477253914, 0.020508456975221634, 0.021569984033703804, 0.019184241071343422, 0.020254703238606453, 0.0213630273938179, 0.020604897290468216, 0.019332818686962128, 0.020718025043606758, 0.02086026780307293, 0.02153654582798481, 0.020810546353459358, 0.022251352667808533, 0.02025846391916275, 0.020483117550611496, 0.02118435502052307, 0.02098487876355648, 0.019505128264427185, 0.01897340454161167, 0.021879062056541443, 0.02095233090221882, 0.019581926986575127, 0.020182162523269653, 0.02029445767402649, 0.01993582770228386, 0.020405974239110947, 0.01950070448219776, 0.01881840080022812, 0.019003262743353844, 0.01952710747718811, 0.01984013430774212]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:38 2016", "state": "available"}], "summary": "8f183acee1f4ac2115fefaec35ebd58f"}