{"content": {"hp_model": {"f0": 16, "f1": 32, "f2": 32, "f3": 16, "nonlin": "rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.8231595754623413, 1.449855089187622, 1.3230862617492676, 1.2265669107437134, 1.1543059349060059, 1.0983279943466187, 1.0556888580322266, 1.0173991918563843, 0.9822482466697693, 0.9548214673995972, 0.9269937872886658, 0.9030776619911194, 0.8805229067802429, 0.8596255779266357, 0.8387380242347717, 0.8212785124778748, 0.8033987283706665, 0.7889260053634644, 0.772599458694458, 0.7580262422561646, 0.7447822690010071, 0.7335842251777649, 0.7192113399505615, 0.7102483510971069, 0.6995927691459656, 0.6911805868148804, 0.682869017124176, 0.672395646572113, 0.6643840074539185, 0.6581861972808838, 0.6458894610404968, 0.6422266960144043, 0.6340530514717102, 0.628265917301178, 0.6243657469749451, 0.6157789826393127, 0.6122341752052307, 0.6067882776260376, 0.6016457080841064, 0.596444845199585, 0.5910542011260986, 0.584576427936554, 0.5805515050888062, 0.5786244869232178, 0.5741842985153198, 0.5709104537963867, 0.5676050186157227, 0.5621909499168396, 0.5594722032546997, 0.5561308264732361, 0.5531841516494751, 0.5510087609291077, 0.5470106601715088, 0.5453684329986572, 0.5415244698524475, 0.5380966067314148, 0.5363042950630188, 0.5343629121780396, 0.532196044921875, 0.5295732021331787, 0.5287002921104431, 0.5257301330566406, 0.5230157375335693, 0.5201254487037659, 0.5191920399665833, 0.5176210403442383, 0.5145038962364197, 0.5140892863273621, 0.5121645927429199, 0.509817361831665, 0.5099630951881409, 0.5063921213150024, 0.5065531134605408, 0.5045338869094849, 0.5047641396522522, 0.5029845833778381, 0.5019806623458862, 0.4993964433670044, 0.4982730448246002, 0.4977443516254425, 0.4968354403972626, 0.49547916650772095, 0.49467507004737854, 0.49455684423446655, 0.49275848269462585, 0.4924224615097046, 0.4904707074165344, 0.4894964396953583, 0.4884614646434784, 0.4880261719226837, 0.4857625961303711, 0.4854813516139984, 0.48420003056526184, 0.485544353723526, 0.48287975788116455, 0.4837212562561035, 0.482262521982193, 0.48222121596336365, 0.48226043581962585, 0.4818231761455536, 0.4803597331047058, 0.478727251291275, 0.47983303666114807, 0.4788200259208679, 0.47722724080085754, 0.4766657054424286, 0.47696205973625183, 0.4756290018558502, 0.47694578766822815, 0.4760039448738098, 0.4766320288181305, 0.47426530718803406, 0.4744275212287903, 0.47450366616249084, 0.47437387704849243, 0.47331488132476807, 0.4729439318180084, 0.4715060889720917, 0.4714358150959015, 0.47368860244750977, 0.4698883593082428, 0.47169190645217896, 0.47044846415519714, 0.47059550881385803, 0.47044625878334045, 0.4704481363296509, 0.4693291187286377, 0.4682619571685791, 0.4701966643333435, 0.4678971469402313, 0.46983715891838074, 0.4690132141113281, 0.4678483009338379, 0.4664841592311859, 0.46744948625564575, 0.46718481183052063, 0.4675132632255554, 0.466535747051239, 0.4669567048549652, 0.46673423051834106, 0.46597176790237427, 0.4666860103607178, 0.4670921862125397, 0.4655376076698303, 0.46527719497680664, 0.4663718342781067, 0.46512895822525024, 0.46533524990081787, 0.46710458397865295, 0.4650949537754059, 0.465032696723938, 0.46593594551086426, 0.46633046865463257, 0.4646703898906708, 0.4641418159008026, 0.46501991152763367, 0.46439245343208313, 0.4637396037578583, 0.46556320786476135, 0.4641271233558655, 0.4648357629776001, 0.4636891782283783, 0.46450692415237427, 0.4633379280567169, 0.4642280340194702, 0.4623653292655945, 0.46383383870124817, 0.4641187787055969, 0.4629896283149719, 0.4636220335960388, 0.46170568466186523, 0.4629178047180176, 0.4628637135028839, 0.4631502330303192, 0.46204861998558044], "moving_avg_accuracy_train": [0.046884705882352926, 0.09576329411764703, 0.14389519999999997, 0.19000450352941173, 0.23438287670588231, 0.2755257655058823, 0.3131214242494117, 0.3492422230009411, 0.3836497654067293, 0.4147977300425269, 0.44357442762650945, 0.47043110251091735, 0.49460210990688447, 0.5182524871514902, 0.5393872384363412, 0.5594555734162364, 0.5786041337216716, 0.5967860732906809, 0.6131498189027893, 0.628178366424275, 0.6427746474289064, 0.6568101238624864, 0.6692185232409437, 0.6803907885639082, 0.6920528861781057, 0.7022993622661775, 0.7120788378042656, 0.7208474246120744, 0.7289273880332199, 0.7365828845240155, 0.7446751843069082, 0.751475901170335, 0.7579941934062426, 0.7639688917126771, 0.7697108260708211, 0.7751020964049155, 0.7799989455879534, 0.7843966980879815, 0.788389969455654, 0.7929109725100886, 0.7968245811414327, 0.8002009465567013, 0.8036302636657371, 0.8064154725932811, 0.8096280429810119, 0.8122158269182047, 0.8153660089322666, 0.8180199962743341, 0.8206627025292537, 0.8230270205116224, 0.8252490243428131, 0.8273194160261788, 0.8292651214823844, 0.8312115505106166, 0.833076277812496, 0.8350392382665406, 0.8366882556163571, 0.8383817829958978, 0.8400988988139552, 0.841505479520795, 0.8430184609804803, 0.8443660266471381, 0.8458400122177184, 0.8469901286430055, 0.8483240569551755, 0.8495363571420109, 0.8504744861336921, 0.8514764492850287, 0.8526558631800553, 0.8537008650973439, 0.8546625432934919, 0.8556033477876721, 0.8567418365383166, 0.857582947002132, 0.8584975934783894, 0.8593584223658446, 0.860276697776319, 0.8611149103516282, 0.8620269487282302, 0.8628124891495248, 0.8636865343522193, 0.8643272926817033, 0.8650404457664741, 0.8655764011898267, 0.8662093493061381, 0.8668566496696419, 0.867627455290913, 0.8682670627029982, 0.8688850623150515, 0.8694694972600169, 0.8701272534163681, 0.8707004104276724, 0.8712727223260817, 0.871778391269944, 0.8722781992017732, 0.8726527322227724, 0.8731780472357893, 0.8737190660416222, 0.8742177476727542, 0.874732443493714, 0.8752074344384603, 0.8755925733475555, 0.8759721395422116, 0.8763561020585787, 0.8766993153821326, 0.8770787956086252, 0.8773685631065863, 0.8777046479723982, 0.8779459478810407, 0.8784148825047013, 0.8787428060189371, 0.879052054828808, 0.8793444964047507, 0.8796476938230992, 0.8799723362054951, 0.880306867290828, 0.8805444158558628, 0.8808335036820413, 0.8810560356667783, 0.8812727850412768, 0.8815196241842079, 0.88181236764814, 0.8820052485303849, 0.88228237073617, 0.8825058983684354, 0.8827117791198271, 0.8829017776784327, 0.8832445410870601, 0.8833694987430599, 0.8834913723981657, 0.8837139998642315, 0.8838296587013377, 0.8839737516547334, 0.8840304941363188, 0.8842439153109223, 0.8844171708386537, 0.8846295714018471, 0.8847407319087212, 0.884831364600202, 0.8849152869637111, 0.8850566994438106, 0.8851675000876649, 0.8853425147847808, 0.8854553221298321, 0.8855333193286136, 0.8856082226898699, 0.8857462239502947, 0.8858563074376181, 0.8858965590467974, 0.8859210207891766, 0.8859759775337883, 0.886100732721586, 0.8862200712141333, 0.8863604170338963, 0.8864373165069773, 0.8864194672092207, 0.8864739910765339, 0.8864665919688804, 0.88652816806611, 0.8865600571418519, 0.8865981690747256, 0.886627763931959, 0.8866661640093513, 0.8867219005495927, 0.8868520634358098, 0.88687273944517, 0.8869407596183, 0.8869525660094112, 0.8869843682319994, 0.8870624019970348, 0.887104397091449, 0.8871586632646571, 0.8872216204676031, 0.8872735760679016, 0.8873297478728761], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.04695999999999999, 0.09651733333333332, 0.1450789333333333, 0.19069103999999995, 0.23467526933333327, 0.2751144090666666, 0.3119496348266666, 0.3471680046773333, 0.3805178708762666, 0.41051941712197326, 0.4383874754097759, 0.4640020612021316, 0.4870818550819184, 0.5098536695737266, 0.5300016359496873, 0.5490014723547185, 0.56699465845258, 0.5839218592739887, 0.5994630066799231, 0.6135567060119308, 0.6270543687440711, 0.640108931869664, 0.6511647053493642, 0.6611949014810945, 0.6715287446663183, 0.6808958701996866, 0.689526283179718, 0.6975603215284129, 0.7045642893755715, 0.711214527104681, 0.7181464077275462, 0.723985100288125, 0.7296799235926459, 0.734778597900048, 0.7396474047767099, 0.7439493309657055, 0.7478877312024683, 0.7514056247488882, 0.754571728940666, 0.7580878893799328, 0.7610791004419395, 0.7636111903977456, 0.766330071357971, 0.7682703975555072, 0.7706166911332898, 0.7724616886866275, 0.774668853151298, 0.7766686345028349, 0.778548437719218, 0.7803335939472962, 0.7818469012192333, 0.7834355444306433, 0.7848786566542457, 0.7862174576554879, 0.7871023785566057, 0.7884454740342784, 0.7895475932975172, 0.7908195006344322, 0.7919642172376556, 0.7926211288472235, 0.7935323492958345, 0.7942324476995843, 0.795329202929626, 0.7961162826366633, 0.7969579877063304, 0.797808855602364, 0.798481303375461, 0.7992065063712481, 0.7999658557341233, 0.800569270160711, 0.8011523431446399, 0.8017704421635092, 0.8025933979471583, 0.8032407248191092, 0.8039433190038651, 0.8042956537701452, 0.8048794217264641, 0.8052714795538176, 0.8059309982651025, 0.806484565105259, 0.8068494419280665, 0.8072711644019266, 0.8077173812950673, 0.8079056431655607, 0.8083017455156712, 0.8086182376307707, 0.808969747201027, 0.8092594391475909, 0.8095868285661652, 0.8098548123762154, 0.8101093311385938, 0.8105517313580678, 0.8109498915555943, 0.8114549024000348, 0.8118694121600313, 0.8120958042773615, 0.8124995571829586, 0.8127562681313294, 0.8130139746515298, 0.8133392438530435, 0.8136319861344058, 0.8137754541876319, 0.814051242102202, 0.8143527845586485, 0.814690839436117, 0.8148350888258387, 0.8150449132765881, 0.815247088615596, 0.8153623797540364, 0.8156128084452994, 0.8158515276007694, 0.8161730415073591, 0.8162090706899566, 0.816388163620961, 0.8164826805921982, 0.8165544125329784, 0.8167389712796806, 0.8168650741517126, 0.816925233403208, 0.8170060433962205, 0.8172254390565985, 0.817356228484272, 0.8174739389691782, 0.8176332117389271, 0.8178832238983677, 0.8179349015085309, 0.8179014113576778, 0.8179779368885767, 0.818060143199719, 0.8181474622130803, 0.8181460493251056, 0.8181847777259283, 0.8182862999533356, 0.8182843366246686, 0.8182959029622018, 0.8182663126659816, 0.8182930147327168, 0.8183303799261118, 0.8183240086001672, 0.8182382744068171, 0.818201113632802, 0.8181543356028551, 0.8182189020425695, 0.8182103451716459, 0.818295977321148, 0.8182930462556999, 0.8182904082967966, 0.8184080341337835, 0.8182872307204052, 0.818231840981698, 0.8182086568835282, 0.818334457861842, 0.8183276787423244, 0.8182549108680919, 0.8183094197812827, 0.8183584778031544, 0.8183759633561724, 0.8183117003538884, 0.8184271969851662, 0.8183578106199829, 0.8183886962246513, 0.8183631599355194, 0.8182735106086342, 0.8184194928811042, 0.8185375435929938, 0.8185904559003612, 0.8185447436436584, 0.8185436026126259, 0.8185425756846967, 0.8186616514495604, 0.818782152971271, 0.8187039376741438, 0.8187268772400628, 0.8188141895160566, 0.8188794372311176], "moving_var_accuracy_train": [0.019783580811072655, 0.03930727022084428, 0.056226666473568984, 0.0697386106739189, 0.08048970965863778, 0.08767537438205648, 0.09162873895109012, 0.09420827397801751, 0.0954423573498708, 0.09462988292345983, 0.09261977954567377, 0.08984933046372746, 0.08612253580417778, 0.08254434531806946, 0.0783100101931151, 0.0741036517935912, 0.06999329287017007, 0.06596920992157303, 0.06178223846353589, 0.05763672978263231, 0.05379051957686452, 0.05018441900763627, 0.04655169248309032, 0.04301989884680171, 0.039941949648989165, 0.03689266713410109, 0.03406414369689156, 0.03134972235865711, 0.028802322402774843, 0.02644954980118262, 0.024393962663050182, 0.02237081414545365, 0.020516125933962532, 0.018785786519242488, 0.017203936158877355, 0.015745134705327193, 0.014386433422087242, 0.013121852123342055, 0.011953182856950703, 0.010941819788819494, 0.009985484802611523, 0.009089534913107161, 0.008286423364305372, 0.007527597526805469, 0.00686772325059004, 0.006241220556881381, 0.0057064113216887145, 0.005199163028826531, 0.004742101793091996, 0.004318201609478566, 0.003930817157763146, 0.0035763141374897823, 0.0032527546512415803, 0.0029615764597749248, 0.002696713684990806, 0.0024617212401890094, 0.0022400224401500717, 0.0020418325110023523, 0.0018641856404957198, 0.0016955733000098329, 0.00154661798608501, 0.0014082995865101044, 0.0012870233290196045, 0.0011702259062430782, 0.0010692175982968497, 0.0009755228841541749, 0.0008858913697840536, 0.000806337604215377, 0.0007382229980158743, 0.0006742289592785184, 0.0006151294879271842, 0.0005615825570008931, 0.0005170897110189016, 0.00047174794122806923, 0.00043210235069403275, 0.0003955613529859254, 0.00036359428525266965, 0.0003335582596200623, 0.0003076887596616075, 0.0002824735474768364, 0.00026110178787633364, 0.00023868675021992782, 0.00021939536109879862, 0.0002000410589313082, 0.0001836425628996573, 0.00016904928645502135, 0.00015749162956156732, 0.0001454243453797592, 0.0001343192225262635, 0.00012396137811770798, 0.00011545902875689897, 0.0001068697065176753, 9.913060404745475e-05, 9.151885336979177e-05, 8.461523975128632e-05, 7.741619063052647e-05, 7.215817433358214e-05, 6.7576669034607e-05, 6.305715245420164e-05, 5.9135643301803525e-05, 5.525262654994155e-05, 5.10623517086384e-05, 4.725275100290683e-05, 4.385432082839058e-05, 4.052904721473569e-05, 3.777218967395217e-05, 3.4750657532428155e-05, 3.229216911243593e-05, 2.95869830143904e-05, 2.8607381844361027e-05, 2.6714448140623455e-05, 2.4903716764220793e-05, 2.3183043765857517e-05, 2.169209745971033e-05, 2.0471421801768878e-05, 1.9431479045077722e-05, 1.7996195027320992e-05, 1.6948721465789994e-05, 1.569953367729011e-05, 1.4552402931671117e-05, 1.3645528700850848e-05, 1.3052264451840703e-05, 1.208186531927651e-05, 1.1564849239801331e-05, 1.0858045737296576e-05, 1.0153723117709616e-05, 9.46324587638822e-06, 9.574302077393766e-06, 8.757401611791254e-06, 8.015340140891707e-06, 7.65987302462437e-06, 7.0142784215688905e-06, 6.499715592376518e-06, 5.878721416087185e-06, 5.700786654400363e-06, 5.400865289965363e-06, 5.266804754172934e-06, 4.851334203352244e-06, 4.440129345902556e-06, 4.059503079185058e-06, 3.833530177017523e-06, 3.560668203422387e-06, 3.4802726809391503e-06, 3.2467748867230114e-06, 2.9768494652105446e-06, 2.7296591404369046e-06, 2.628092357302644e-06, 2.474348489204064e-06, 2.2414953686574146e-06, 2.0227312233536553e-06, 1.8476402950231859e-06, 1.8029509774626933e-06, 1.7508309619475423e-06, 1.7530204078776964e-06, 1.6309401277310728e-06, 1.4707134918315844e-06, 1.3503978116095419e-06, 1.2158507515951857e-06, 1.1283902181859302e-06, 1.0247034147324158e-06, 9.353057481054514e-07, 8.496578734668534e-07, 7.779631796138101e-07, 7.281259189151046e-07, 8.07794719559127e-07, 7.30862723870782e-07, 6.994171470574928e-07, 6.307299501913746e-07, 5.767593874262379e-07, 5.73886865053938e-07, 5.323704701422411e-07, 5.056367811198276e-07, 4.907455876329658e-07, 4.6596548849108247e-07, 4.4776638470886277e-07], "duration": 146890.949928, "accuracy_train": [0.4688470588235294, 0.5356705882352941, 0.5770823529411765, 0.6049882352941176, 0.6337882352941177, 0.6458117647058823, 0.6514823529411765, 0.6743294117647058, 0.6933176470588235, 0.6951294117647059, 0.702564705882353, 0.7121411764705883, 0.7121411764705883, 0.7311058823529412, 0.7296, 0.7400705882352941, 0.7509411764705882, 0.7604235294117647, 0.7604235294117647, 0.7634352941176471, 0.7741411764705882, 0.7831294117647059, 0.7808941176470588, 0.7809411764705882, 0.7970117647058823, 0.7945176470588236, 0.8000941176470588, 0.7997647058823529, 0.8016470588235294, 0.8054823529411764, 0.8175058823529412, 0.8126823529411765, 0.8166588235294118, 0.8177411764705882, 0.8213882352941176, 0.8236235294117648, 0.8240705882352941, 0.8239764705882353, 0.8243294117647059, 0.8336, 0.8320470588235294, 0.8305882352941176, 0.8344941176470588, 0.8314823529411765, 0.8385411764705882, 0.8355058823529412, 0.8437176470588236, 0.8419058823529412, 0.8444470588235294, 0.8443058823529411, 0.8452470588235295, 0.8459529411764706, 0.8467764705882352, 0.8487294117647058, 0.8498588235294118, 0.8527058823529412, 0.8515294117647059, 0.8536235294117647, 0.8555529411764706, 0.8541647058823529, 0.856635294117647, 0.8564941176470589, 0.8591058823529412, 0.8573411764705883, 0.8603294117647059, 0.8604470588235295, 0.8589176470588236, 0.8604941176470589, 0.8632705882352941, 0.8631058823529412, 0.8633176470588235, 0.8640705882352941, 0.8669882352941176, 0.8651529411764706, 0.8667294117647059, 0.8671058823529412, 0.8685411764705883, 0.8686588235294118, 0.8702352941176471, 0.8698823529411764, 0.8715529411764706, 0.8700941176470588, 0.8714588235294117, 0.8704, 0.8719058823529412, 0.8726823529411765, 0.8745647058823529, 0.8740235294117648, 0.8744470588235295, 0.8747294117647059, 0.8760470588235294, 0.8758588235294118, 0.8764235294117647, 0.8763294117647059, 0.8767764705882353, 0.8760235294117648, 0.8779058823529412, 0.8785882352941177, 0.8787058823529412, 0.8793647058823529, 0.8794823529411765, 0.8790588235294118, 0.8793882352941177, 0.8798117647058824, 0.8797882352941176, 0.8804941176470589, 0.8799764705882352, 0.8807294117647059, 0.8801176470588236, 0.8826352941176471, 0.8816941176470589, 0.881835294117647, 0.8819764705882353, 0.8823764705882353, 0.8828941176470588, 0.8833176470588235, 0.8826823529411765, 0.8834352941176471, 0.8830588235294118, 0.8832235294117647, 0.8837411764705883, 0.8844470588235294, 0.8837411764705883, 0.8847764705882353, 0.8845176470588235, 0.8845647058823529, 0.8846117647058823, 0.8863294117647059, 0.8844941176470589, 0.8845882352941177, 0.8857176470588235, 0.8848705882352941, 0.8852705882352941, 0.8845411764705883, 0.886164705882353, 0.8859764705882353, 0.8865411764705883, 0.8857411764705883, 0.8856470588235295, 0.8856705882352941, 0.8863294117647059, 0.886164705882353, 0.8869176470588235, 0.8864705882352941, 0.886235294117647, 0.8862823529411765, 0.8869882352941176, 0.8868470588235294, 0.8862588235294118, 0.8861411764705882, 0.8864705882352941, 0.8872235294117647, 0.8872941176470588, 0.8876235294117647, 0.8871294117647058, 0.8862588235294118, 0.886964705882353, 0.8864, 0.8870823529411764, 0.8868470588235294, 0.8869411764705882, 0.8868941176470588, 0.8870117647058824, 0.8872235294117647, 0.8880235294117647, 0.8870588235294118, 0.8875529411764705, 0.8870588235294118, 0.8872705882352941, 0.8877647058823529, 0.8874823529411765, 0.8876470588235295, 0.8877882352941177, 0.8877411764705883, 0.8878352941176471], "end": "2016-02-06 01:23:28.224000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0], "moving_var_accuracy_valid": [0.019847174399999997, 0.03996582054399999, 0.057193299440639986, 0.0701981479677184, 0.0805898450413719, 0.08724877673858325, 0.09073540377583643, 0.09282486557269792, 0.09355230119480894, 0.09229790606952754, 0.0900577735171661, 0.08695695921327506, 0.08305535526174858, 0.07941681955281764, 0.07512860253931745, 0.07086468633614727, 0.06669201041610302, 0.06260158052332741, 0.05851516783523153, 0.05445134229945768, 0.05064589016258739, 0.047115095711929456, 0.04350365728584651, 0.04005873506723064, 0.03701395639529876, 0.03410224812258978, 0.03136237956418384, 0.028807053557460157, 0.02636784829215044, 0.02412909441961844, 0.022148643698383486, 0.02024059230589775, 0.0185084121875354, 0.01689153928601852, 0.015415732881036735, 0.014040718713353162, 0.012776245809842244, 0.0116100014038935, 0.01053921920528289, 0.009596567742866581, 0.008717437061137166, 0.007903396670922096, 0.007179587826912773, 0.006495512835997104, 0.005895507394375689, 0.005336592798684519, 0.004846777693583009, 0.0043980920533103, 0.003990085789170191, 0.0036197582550809896, 0.0032783933196665685, 0.0029732680729783433, 0.0026946844216897033, 0.0024413474726090742, 0.0022042604903592844, 0.0020000695904826616, 0.0018109946332680142, 0.0016444549044044956, 0.001491802798879305, 0.0013465063147564387, 0.0012193285876344962, 0.0011018069688454447, 0.001002452120272512, 0.0009077823584323317, 0.0008233803294078265, 0.00074755808205555, 0.0006768719479178821, 0.000613918027591983, 0.0005577157279268753, 0.0005052211358661149, 0.0004577587892207932, 0.0004154213278728591, 0.00037997450108214647, 0.00034574833968627867, 0.000315616253013724, 0.0002851718858001189, 0.0002597217624615288, 0.00023513297027527886, 0.00021553435762256505, 0.00019673884807899534, 0.00017826317913349424, 0.00016203750982477232, 0.0001476257444838121, 0.0001331821528223655, 0.00012127601118599718, 0.00011004991539767913, 0.00010015695465974693, 9.089655200890842e-05, 8.277155129056701e-05, 7.514073406355127e-05, 6.820967886082012e-05, 6.315017256245349e-05, 5.826193919225763e-05, 5.473106885005505e-05, 5.0804327035240905e-05, 4.618517484882011e-05, 4.3033805042941665e-05, 3.932352913776847e-05, 3.5988890078975796e-05, 3.3342201552158407e-05, 3.077926378661753e-05, 2.7886585148624142e-05, 2.5782457398168276e-05, 2.4022562335709395e-05, 2.264883600376054e-05, 2.0571223381300067e-05, 1.8910337744360884e-05, 1.7387177779251272e-05, 1.5768088420752103e-05, 1.475571034334631e-05, 1.3793020825706724e-05, 1.3344059472311368e-05, 1.2021336443067972e-05, 1.1107871300182669e-05, 1.007748529083115e-05, 9.116046003700856e-06, 8.5109987821892e-06, 7.803016312982589e-06, 7.05528690154872e-06, 6.408530506130018e-06, 6.200887557651109e-06, 5.734751671406408e-06, 5.285978328577292e-06, 4.985690832370791e-06, 5.049676467947223e-06, 4.568743999682153e-06, 4.121963911551401e-06, 3.7624729323104685e-06, 3.4470465374040897e-06, 3.170963374513412e-06, 2.8538850033339324e-06, 2.581995504273145e-06, 2.416556817765242e-06, 2.1749358279238057e-06, 1.9586462666067997e-06, 1.7706619106197034e-06, 1.6000127228691004e-06, 1.452576869679193e-06, 1.3076845268598938e-06, 1.2430692413583541e-06, 1.1311906253510927e-06, 1.0377652195873743e-06, 9.715081238652806e-07, 8.750162918387834e-07, 8.535104479100236e-07, 7.682367234209722e-07, 6.914756805234554e-07, 7.46850650213175e-07, 8.035067673466e-07, 7.507682989983952e-07, 6.805289907700794e-07, 7.549090669955707e-07, 6.798317684489095e-07, 6.595050632867856e-07, 6.202955515132958e-07, 5.799262019516697e-07, 5.246852828355758e-07, 5.093843557148878e-07, 5.78501166672136e-07, 5.639812590650609e-07, 5.161684183401351e-07, 4.7042049506972166e-07, 4.957114618615999e-07, 6.379377305547504e-07, 6.995676926980058e-07, 6.548083338665782e-07, 6.081339941956663e-07, 5.473323123424541e-07, 4.926085723369558e-07, 5.709590551040537e-07, 6.445487002048365e-07, 6.351525245264803e-07, 5.763732852347793e-07, 5.873468585641318e-07, 5.669275515938665e-07], "accuracy_test": 0.8069, "start": "2016-02-04 08:35:17.274000", "learning_rate_per_epoch": [9.896264964481816e-05, 9.589459659764543e-05, 9.292165486840531e-05, 9.004088497022167e-05, 8.724942745175213e-05, 8.454451017314568e-05, 8.19234483060427e-05, 7.938364433357492e-05, 7.692258077440783e-05, 7.453781290678307e-05, 7.222698332043365e-05, 6.998779281275347e-05, 6.781802221667022e-05, 6.571551784873009e-05, 6.367819878505543e-05, 6.170403503347188e-05, 5.979107663733885e-05, 5.793742457171902e-05, 5.6141238019336015e-05, 5.440073800855316e-05, 5.2714196499437094e-05, 5.1079943659715354e-05, 4.949635695083998e-05, 4.796186476596631e-05, 4.647494279197417e-05, 4.50341212854255e-05, 4.36379668826703e-05, 4.2285097151761875e-05, 4.097416967852041e-05, 3.970388206653297e-05, 3.8472975575132295e-05, 3.7280231481418014e-05, 3.612446380429901e-05, 3.5004526580451056e-05, 3.3919310226337984e-05, 3.2867737900232896e-05, 3.184876550221816e-05, 3.08613853121642e-05, 2.9904615075793117e-05, 2.8977507099625655e-05, 2.8079142794013023e-05, 2.7208629035158083e-05, 2.6365103622083552e-05, 2.5547728000674397e-05, 2.475569272064604e-05, 2.398821379756555e-05, 2.324452725588344e-05, 2.2523896404891275e-05, 2.182560638175346e-05, 2.1148965970496647e-05, 2.0493302145041525e-05, 1.985796552617103e-05, 1.924232674355153e-05, 1.8645772797754034e-05, 1.8067714336211793e-05, 1.7507576558273286e-05, 1.696480467217043e-05, 1.6438858438050374e-05, 1.5929217624943703e-05, 1.5435378372785635e-05, 1.4956848644942511e-05, 1.4493154594674706e-05, 1.4043836017663125e-05, 1.3608447261503898e-05, 1.3186556316213682e-05, 1.2777744814229663e-05, 1.238160712091485e-05, 1.1997750334558077e-05, 1.1625794286374003e-05, 1.1265369721513707e-05, 1.0916119208559394e-05, 1.0577696230029687e-05, 1.0249765182379633e-05, 9.932000466505997e-06, 9.624087397241965e-06, 9.325720384367742e-06, 9.036602932610549e-06, 8.756448551139329e-06, 8.48497984407004e-06, 8.221927600970957e-06, 7.967029887367971e-06, 7.72003477322869e-06, 7.480697149730986e-06, 7.248779638757696e-06, 7.024051683401922e-06, 6.806290912209079e-06, 6.595281320187496e-06, 6.390813268808415e-06, 6.192684395500692e-06, 6.0006977946613915e-06, 5.814663381897844e-06, 5.63439652978559e-06, 5.45971806786838e-06, 5.290455192152876e-06, 5.126439646119252e-06, 4.96750908496324e-06, 4.8135057113540825e-06, 4.664276730181882e-06, 4.51967389381025e-06, 4.379554411571007e-06, 4.243778676027432e-06, 4.112212536711013e-06, 3.9847250263846945e-06, 3.861190180032281e-06, 3.7414849884953583e-06, 3.6254909900890198e-06, 3.513092906359816e-06, 3.404179551580455e-06, 3.2986426958814263e-06, 3.1963777473720256e-06, 3.09728307001933e-06, 3.001260665769223e-06, 2.908215037678019e-06, 2.8180540994071634e-06, 2.7306882657285314e-06, 2.6460311346454546e-06, 2.563998577898019e-06, 2.484509195710416e-06, 2.4074840894172667e-06, 2.332846861463622e-06, 2.2605236154049635e-06, 2.1904425011598505e-06, 2.122534169757273e-06, 2.0567310912156245e-06, 1.9929680092900526e-06, 1.931181714098784e-06, 1.8713109284362872e-06, 1.8132963077732711e-06, 1.757080212883011e-06, 1.7026069372150232e-06, 1.64982247952139e-06, 1.5986744301699218e-06, 1.5491120848309947e-06, 1.501086330790713e-06, 1.4545494195772335e-06, 1.409455308021279e-06, 1.3657592035087873e-06, 1.3234176776677486e-06, 1.2823888937418815e-06, 1.2426320381564437e-06, 1.2041077752655838e-06, 1.1667777926049894e-06, 1.130605141952401e-06, 1.0955538982670987e-06, 1.0615893870635773e-06, 1.0286778433510335e-06, 9.967866390070412e-07, 9.658840554038761e-07, 9.359395676256099e-07, 9.069234465641784e-07, 8.788068726062193e-07, 8.515619924764906e-07, 8.251617487076146e-07, 7.995799364834966e-07, 7.747912604827434e-07, 7.507710506615695e-07, 7.27495546470891e-07, 7.049416126392316e-07, 6.83086909702979e-07, 6.619097234761284e-07, 6.413890787371201e-07, 6.21504625542002e-07, 6.022366392244294e-07, 5.83566020395665e-07, 5.654742381011602e-07, 5.479433298205549e-07, 5.309559014676779e-07, 5.144951273905463e-07, 4.98544693527947e-07, 4.830887405660178e-07, 4.6811194920337584e-07, 4.535994833076984e-07, 4.3953693307230424e-07, 4.2591034343786305e-07, 4.127062140923954e-07], "accuracy_train_first": 0.4688470588235294, "accuracy_train_last": 0.8878352941176471, "batch_size_eval": 1024, "accuracy_train_std": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "accuracy_test_std": 0, "error_valid": [0.5304, 0.4574666666666667, 0.4178666666666667, 0.39880000000000004, 0.3694666666666667, 0.36093333333333333, 0.35653333333333337, 0.33586666666666665, 0.31933333333333336, 0.3194666666666667, 0.31079999999999997, 0.30546666666666666, 0.3052, 0.2852, 0.2886666666666666, 0.28, 0.2710666666666667, 0.2637333333333334, 0.2606666666666667, 0.25960000000000005, 0.2514666666666666, 0.24239999999999995, 0.2493333333333333, 0.24853333333333338, 0.2354666666666667, 0.2348, 0.2328, 0.2301333333333333, 0.23240000000000005, 0.22893333333333332, 0.2194666666666667, 0.2234666666666667, 0.21906666666666663, 0.21933333333333338, 0.21653333333333336, 0.21733333333333338, 0.21666666666666667, 0.2169333333333333, 0.2169333333333333, 0.2102666666666667, 0.21199999999999997, 0.2136, 0.20920000000000005, 0.21426666666666672, 0.2082666666666667, 0.2109333333333333, 0.2054666666666667, 0.20533333333333337, 0.20453333333333334, 0.2036, 0.20453333333333334, 0.2022666666666667, 0.2021333333333334, 0.20173333333333332, 0.2049333333333333, 0.19946666666666668, 0.20053333333333334, 0.19773333333333332, 0.19773333333333332, 0.20146666666666668, 0.1982666666666667, 0.19946666666666668, 0.19479999999999997, 0.19679999999999997, 0.19546666666666668, 0.19453333333333334, 0.19546666666666668, 0.1942666666666667, 0.19320000000000004, 0.19399999999999995, 0.1936, 0.19266666666666665, 0.18999999999999995, 0.1909333333333333, 0.1897333333333333, 0.19253333333333333, 0.18986666666666663, 0.19120000000000004, 0.18813333333333337, 0.18853333333333333, 0.18986666666666663, 0.1889333333333333, 0.1882666666666667, 0.1904, 0.18813333333333337, 0.18853333333333333, 0.18786666666666663, 0.18813333333333337, 0.18746666666666667, 0.1877333333333333, 0.1876, 0.18546666666666667, 0.18546666666666667, 0.18400000000000005, 0.1844, 0.18586666666666662, 0.18386666666666662, 0.18493333333333328, 0.18466666666666665, 0.1837333333333333, 0.1837333333333333, 0.18493333333333328, 0.18346666666666667, 0.18293333333333328, 0.1822666666666667, 0.18386666666666662, 0.1830666666666667, 0.18293333333333328, 0.18359999999999999, 0.18213333333333337, 0.18200000000000005, 0.18093333333333328, 0.18346666666666667, 0.18200000000000005, 0.18266666666666664, 0.18279999999999996, 0.18159999999999998, 0.18200000000000005, 0.18253333333333333, 0.1822666666666667, 0.18079999999999996, 0.18146666666666667, 0.18146666666666667, 0.18093333333333328, 0.17986666666666662, 0.18159999999999998, 0.1824, 0.18133333333333335, 0.18120000000000003, 0.1810666666666667, 0.18186666666666662, 0.18146666666666667, 0.18079999999999996, 0.1817333333333333, 0.18159999999999998, 0.18200000000000005, 0.18146666666666667, 0.18133333333333335, 0.1817333333333333, 0.18253333333333333, 0.18213333333333337, 0.1822666666666667, 0.18120000000000003, 0.18186666666666662, 0.18093333333333328, 0.1817333333333333, 0.1817333333333333, 0.18053333333333332, 0.18279999999999996, 0.1822666666666667, 0.18200000000000005, 0.18053333333333332, 0.1817333333333333, 0.1824, 0.18120000000000003, 0.18120000000000003, 0.18146666666666667, 0.1822666666666667, 0.18053333333333332, 0.1822666666666667, 0.18133333333333335, 0.18186666666666662, 0.18253333333333333, 0.1802666666666667, 0.1804, 0.18093333333333328, 0.18186666666666662, 0.18146666666666667, 0.18146666666666667, 0.1802666666666667, 0.18013333333333337, 0.18200000000000005, 0.1810666666666667, 0.1804, 0.18053333333333332], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.0310021512862811, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 32, "valid_ratio": 0.15, "learning_rate": 0.00010212886214969702, "optimization": "adam", "nb_data_augmentation": 3, "learning_rate_decay_method": "exp", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 1.178942785372837e-07, "rotation_range": [0, 0], "momentum": 0.6309727394624134}, "accuracy_valid_max": 0.8201333333333334, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        y_pred = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            y_pred.extend((nnet.predict(X[mini_batch]) == y[mini_batch]).tolist())\n        return np.mean(y_pred)\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            acc = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = acc\n            status[\"accuracy_train_std\"] = 0\n            acc = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = acc\n            status[\"accuracy_valid_std\"] = 0\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n\n    # rescaling to [-1, 1]\n    X_min = X_train.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X_train.max(axis=(0, 2, 3))[None, :, None, None]\n    def preprocess(a):\n        return (a / 255.) * 2 - 1\n        # return 2 * ((a - X_min) / (X_max - X_min)) - 1\n    X_train = preprocess(X_train)\n    X_valid = preprocess(X_valid)\n\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = preprocess(X_test)\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    acc = evaluate(X_test, y_test, batch_size_eval)\n    light.set(\"accuracy_test\", acc)\n    light.set(\"accuracy_test_std\", 0)\n    print(\"Test accuracy : {}+-{}\".format(acc, 0))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8194666666666667, "accuracy_valid_std": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "accuracy_valid": [0.4696, 0.5425333333333333, 0.5821333333333333, 0.6012, 0.6305333333333333, 0.6390666666666667, 0.6434666666666666, 0.6641333333333334, 0.6806666666666666, 0.6805333333333333, 0.6892, 0.6945333333333333, 0.6948, 0.7148, 0.7113333333333334, 0.72, 0.7289333333333333, 0.7362666666666666, 0.7393333333333333, 0.7404, 0.7485333333333334, 0.7576, 0.7506666666666667, 0.7514666666666666, 0.7645333333333333, 0.7652, 0.7672, 0.7698666666666667, 0.7676, 0.7710666666666667, 0.7805333333333333, 0.7765333333333333, 0.7809333333333334, 0.7806666666666666, 0.7834666666666666, 0.7826666666666666, 0.7833333333333333, 0.7830666666666667, 0.7830666666666667, 0.7897333333333333, 0.788, 0.7864, 0.7908, 0.7857333333333333, 0.7917333333333333, 0.7890666666666667, 0.7945333333333333, 0.7946666666666666, 0.7954666666666667, 0.7964, 0.7954666666666667, 0.7977333333333333, 0.7978666666666666, 0.7982666666666667, 0.7950666666666667, 0.8005333333333333, 0.7994666666666667, 0.8022666666666667, 0.8022666666666667, 0.7985333333333333, 0.8017333333333333, 0.8005333333333333, 0.8052, 0.8032, 0.8045333333333333, 0.8054666666666667, 0.8045333333333333, 0.8057333333333333, 0.8068, 0.806, 0.8064, 0.8073333333333333, 0.81, 0.8090666666666667, 0.8102666666666667, 0.8074666666666667, 0.8101333333333334, 0.8088, 0.8118666666666666, 0.8114666666666667, 0.8101333333333334, 0.8110666666666667, 0.8117333333333333, 0.8096, 0.8118666666666666, 0.8114666666666667, 0.8121333333333334, 0.8118666666666666, 0.8125333333333333, 0.8122666666666667, 0.8124, 0.8145333333333333, 0.8145333333333333, 0.816, 0.8156, 0.8141333333333334, 0.8161333333333334, 0.8150666666666667, 0.8153333333333334, 0.8162666666666667, 0.8162666666666667, 0.8150666666666667, 0.8165333333333333, 0.8170666666666667, 0.8177333333333333, 0.8161333333333334, 0.8169333333333333, 0.8170666666666667, 0.8164, 0.8178666666666666, 0.818, 0.8190666666666667, 0.8165333333333333, 0.818, 0.8173333333333334, 0.8172, 0.8184, 0.818, 0.8174666666666667, 0.8177333333333333, 0.8192, 0.8185333333333333, 0.8185333333333333, 0.8190666666666667, 0.8201333333333334, 0.8184, 0.8176, 0.8186666666666667, 0.8188, 0.8189333333333333, 0.8181333333333334, 0.8185333333333333, 0.8192, 0.8182666666666667, 0.8184, 0.818, 0.8185333333333333, 0.8186666666666667, 0.8182666666666667, 0.8174666666666667, 0.8178666666666666, 0.8177333333333333, 0.8188, 0.8181333333333334, 0.8190666666666667, 0.8182666666666667, 0.8182666666666667, 0.8194666666666667, 0.8172, 0.8177333333333333, 0.818, 0.8194666666666667, 0.8182666666666667, 0.8176, 0.8188, 0.8188, 0.8185333333333333, 0.8177333333333333, 0.8194666666666667, 0.8177333333333333, 0.8186666666666667, 0.8181333333333334, 0.8174666666666667, 0.8197333333333333, 0.8196, 0.8190666666666667, 0.8181333333333334, 0.8185333333333333, 0.8185333333333333, 0.8197333333333333, 0.8198666666666666, 0.818, 0.8189333333333333, 0.8196, 0.8194666666666667], "seed": 727603298, "model": "residualv3", "loss_std": [0.38069549202919006, 0.1748999059200287, 0.17338107526302338, 0.17496000230312347, 0.17544783651828766, 0.17649592459201813, 0.1768542230129242, 0.17634299397468567, 0.17558078467845917, 0.17536373436450958, 0.17414291203022003, 0.1732594221830368, 0.17157362401485443, 0.17239804565906525, 0.16701878607273102, 0.1685236543416977, 0.16668906807899475, 0.16761116683483124, 0.16692222654819489, 0.16625471413135529, 0.1638219654560089, 0.16180215775966644, 0.16168619692325592, 0.16081924736499786, 0.16026510298252106, 0.16130942106246948, 0.15968741476535797, 0.15877114236354828, 0.15788426995277405, 0.15829651057720184, 0.15663419663906097, 0.15531928837299347, 0.15482905507087708, 0.15470629930496216, 0.1533593386411667, 0.15345151722431183, 0.1536804735660553, 0.15412181615829468, 0.1536189168691635, 0.15135394036769867, 0.1516137272119522, 0.1509389728307724, 0.1504998803138733, 0.15053358674049377, 0.1495901495218277, 0.14951229095458984, 0.14885278046131134, 0.14979754388332367, 0.1486530750989914, 0.14614664018154144, 0.14809772372245789, 0.14740628004074097, 0.14562074840068817, 0.14617645740509033, 0.14620953798294067, 0.1461799144744873, 0.1441572904586792, 0.14407040178775787, 0.14400021731853485, 0.14468272030353546, 0.14472714066505432, 0.14297746121883392, 0.1431170105934143, 0.14140534400939941, 0.14288842678070068, 0.1421028971672058, 0.1420440971851349, 0.1445140838623047, 0.14117789268493652, 0.13948482275009155, 0.141709566116333, 0.14096325635910034, 0.13980986177921295, 0.14065760374069214, 0.14121337234973907, 0.14063715934753418, 0.14067545533180237, 0.1390637904405594, 0.139467254281044, 0.14098486304283142, 0.1392940729856491, 0.13881535828113556, 0.1406383514404297, 0.14052435755729675, 0.1394798457622528, 0.1380377560853958, 0.13847653567790985, 0.13794705271720886, 0.1370033174753189, 0.13838230073451996, 0.13804838061332703, 0.13799218833446503, 0.1390335112810135, 0.13763917982578278, 0.1385195404291153, 0.13770058751106262, 0.13780930638313293, 0.1363183706998825, 0.13748420774936676, 0.13714535534381866, 0.13886398077011108, 0.13719645142555237, 0.13632532954216003, 0.13884569704532623, 0.13605642318725586, 0.13608309626579285, 0.13699212670326233, 0.13625840842723846, 0.13625670969486237, 0.13673242926597595, 0.1368611752986908, 0.13490062952041626, 0.13500875234603882, 0.1361602544784546, 0.13610783219337463, 0.13576488196849823, 0.13716252148151398, 0.13468708097934723, 0.13443197309970856, 0.13631929457187653, 0.13545198738574982, 0.1354491263628006, 0.13547563552856445, 0.13532797992229462, 0.1357639729976654, 0.13605313003063202, 0.13412830233573914, 0.13391001522541046, 0.13595399260520935, 0.13531237840652466, 0.13490375876426697, 0.13516229391098022, 0.13365449011325836, 0.13340267539024353, 0.13423529267311096, 0.1350756734609604, 0.13515037298202515, 0.13401475548744202, 0.13279485702514648, 0.1355944275856018, 0.1340688019990921, 0.13554739952087402, 0.13623549044132233, 0.13333337008953094, 0.13387362658977509, 0.13530462980270386, 0.13396544754505157, 0.13289065659046173, 0.135555699467659, 0.13355566561222076, 0.13382385671138763, 0.13576656579971313, 0.13258087635040283, 0.1343301236629486, 0.1333675980567932, 0.1361260861158371, 0.13545580208301544, 0.13392643630504608, 0.1352967619895935, 0.13550423085689545, 0.13359877467155457, 0.13677465915679932, 0.1337549388408661, 0.13480216264724731, 0.1352672129869461, 0.13256676495075226, 0.135052889585495, 0.13393820822238922, 0.1334587186574936, 0.13369503617286682, 0.13403351604938507, 0.13516227900981903, 0.1330299973487854, 0.13499689102172852, 0.13472361862659454]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:41 2016", "state": "available"}], "summary": "04c70141098643c37e0c4fa2a0e9d6e2"}