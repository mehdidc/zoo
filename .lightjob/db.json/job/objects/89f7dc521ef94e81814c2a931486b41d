{"content": {"hp_model": {"f0": 16, "f1": 64, "f2": 16, "f3": 16, "nonlin": "very_leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.5743505954742432, 1.2798173427581787, 1.114789605140686, 0.9983618259429932, 0.9108807444572449, 0.844243586063385, 0.7912338376045227, 0.749151885509491, 0.7143597602844238, 0.6789941191673279, 0.6507343053817749, 0.6235880851745605, 0.602648913860321, 0.584296464920044, 0.5646651983261108, 0.546714723110199, 0.5308276414871216, 0.5124506950378418, 0.4997046887874603, 0.4889164865016937, 0.47280827164649963, 0.4616720378398895, 0.4497019350528717, 0.43959879875183105, 0.4309966564178467, 0.4185200333595276, 0.41219475865364075, 0.4009011685848236, 0.39279112219810486, 0.38451144099235535, 0.37535586953163147, 0.365996390581131, 0.3603335916996002, 0.35402679443359375, 0.3463018238544464, 0.3425135016441345, 0.33431941270828247, 0.3280498683452606, 0.3213891386985779, 0.31889092922210693, 0.30835384130477905, 0.30466946959495544, 0.30043089389801025, 0.29730820655822754, 0.2881098985671997, 0.2845219075679779, 0.2785305976867676, 0.2754562497138977, 0.2712400555610657, 0.26899123191833496, 0.26000410318374634, 0.2575908303260803, 0.2570633590221405, 0.25204893946647644, 0.24674354493618011, 0.24389046430587769, 0.24187494814395905, 0.2364184409379959, 0.23620858788490295, 0.23064377903938293, 0.22995470464229584, 0.22447575628757477, 0.2238665074110031, 0.2173713594675064, 0.21489261090755463, 0.21247035264968872, 0.21555067598819733, 0.2056933492422104, 0.2058977335691452, 0.20296132564544678, 0.1989528387784958, 0.19853195548057556, 0.19844694435596466, 0.19536836445331573, 0.1911494880914688, 0.1887560337781906, 0.18867643177509308, 0.18563105165958405, 0.18491464853286743, 0.18183116614818573, 0.1793259233236313, 0.17861169576644897, 0.17758820950984955, 0.17591650784015656, 0.17345553636550903, 0.17392079532146454, 0.1693560779094696, 0.16945353150367737, 0.16407693922519684, 0.16929025948047638, 0.1417405754327774, 0.1173257902264595, 0.1119694784283638, 0.10906365513801575, 0.10596512258052826, 0.10385984927415848, 0.10218647122383118, 0.09988871961832047, 0.09909739345312119, 0.09854594618082047, 0.09645701944828033, 0.09669157862663269, 0.09497290849685669, 0.09499679505825043, 0.09347334504127502, 0.09218503534793854, 0.09119874238967896, 0.0909917950630188, 0.08908215165138245, 0.08805230259895325, 0.08701743930578232, 0.08669674396514893, 0.0865170955657959, 0.08637240529060364, 0.0862206295132637, 0.08690168708562851, 0.08622988313436508, 0.08664848655462265, 0.08770092576742172, 0.08655761182308197, 0.08702670782804489, 0.08638153970241547, 0.0860024243593216, 0.08796662092208862, 0.08634231984615326, 0.0860406756401062, 0.08674880117177963, 0.08601143211126328, 0.08677336573600769, 0.08718890696763992, 0.0861843079328537, 0.08560726046562195, 0.08535749465227127, 0.08598919212818146, 0.08725324273109436, 0.08638451993465424, 0.08654334396123886, 0.08604676276445389, 0.08642954379320145, 0.0865253135561943, 0.08783908188343048, 0.08642127364873886], "moving_avg_accuracy_train": [0.052421759643779986, 0.10637498846437798, 0.15984081490517715, 0.2125023540599621, 0.26361224563232505, 0.3104828265092919, 0.35385435614495364, 0.3927237914468481, 0.43022856499342393, 0.46279256523895973, 0.4957151229800139, 0.5255477128933912, 0.5547169294974168, 0.5807064826255636, 0.6049387482611246, 0.6279193378937608, 0.6477558748846541, 0.6680797785310633, 0.6877219509294334, 0.7039074128939836, 0.7191182506870418, 0.7347214219269386, 0.7500311937534861, 0.763942485830703, 0.7763905690871029, 0.7882772214749504, 0.7980453653931161, 0.8082734926884759, 0.8174021133924042, 0.8251390355664528, 0.8329298382551933, 0.8405970002976142, 0.8466302016786593, 0.8523809895061331, 0.8568964644354109, 0.8617555927646181, 0.8671123101585143, 0.8720749375486706, 0.8751859927855902, 0.8796714590928101, 0.8834016754193633, 0.8867261017370154, 0.8901598997455306, 0.8922344802651635, 0.8956428880029974, 0.8980664247956286, 0.9011729089887309, 0.9029783395161222, 0.9055495625562505, 0.9086099279137668, 0.9111689802843502, 0.9133557618309429, 0.9161051152228763, 0.9179681633339866, 0.9195843806673007, 0.9213206797614233, 0.9226249690377266, 0.9245543585542197, 0.9261699013809683, 0.9269357179750606, 0.9286828595692581, 0.9301507995028824, 0.9313371228610106, 0.9332090990785087, 0.9346265576587992, 0.9357651586989363, 0.9371921143302886, 0.9380927608937529, 0.9399658637091857, 0.9411983243228554, 0.9410939553918619, 0.942029884032493, 0.9431116380388045, 0.944029449121875, 0.9440231479204664, 0.945026591422532, 0.9468665453005353, 0.9471460156955002, 0.9477626595117014, 0.9489267918855588, 0.9492514618399063, 0.9499319286011907, 0.9505629498768228, 0.9501800994547089, 0.9515744930425991, 0.9516040578002625, 0.9519515366178737, 0.9517505899109129, 0.9526369090805821, 0.9532044426499602, 0.956788925393316, 0.9601637693861457, 0.9633127721713683, 0.9662538315233068, 0.9689217112793371, 0.9713436573014127, 0.97352111962129, 0.9755645410663223, 0.9773920306716226, 0.9790297598211455, 0.9805315817926116, 0.9818901970133596, 0.9831571645882324, 0.9842508963806089, 0.9852701682747094, 0.9862293656579713, 0.9870856678564783, 0.9878539786375065, 0.9885175565547174, 0.9892473462111688, 0.9899133853995757, 0.9905058812715322, 0.9910507533003405, 0.99153183753103, 0.9919741139338886, 0.9923582118036042, 0.9927085141351485, 0.9930307977287857, 0.9933185278142497, 0.9935681482471104, 0.9937951678343133, 0.9939878597187484, 0.9941937984492545, 0.9943559278674335, 0.9945134700878423, 0.9946575832350196, 0.9947849599186698, 0.9949088995291929, 0.9950157948810447, 0.9951282767393781, 0.9952062589237829, 0.9952741177409377, 0.9953375158251865, 0.995410850142677, 0.9954651892355522, 0.9954955292774824, 0.9955205101664101, 0.9955755450497783, 0.9956157758495716, 0.9956333823789094, 0.9956701185457805, 0.9956892662519259], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.0510434805628765, 0.10455538462443521, 0.15622775231786518, 0.20732699209925637, 0.2566094119593608, 0.3022534171113163, 0.34483566317315156, 0.38232842992247795, 0.41886762429845303, 0.45033276557719204, 0.482087745561265, 0.509958190225771, 0.5370344250435554, 0.561010352362242, 0.5839061638880659, 0.6050881837473767, 0.6234907629159372, 0.6421131309955784, 0.6597461385379633, 0.6741855636525405, 0.6875085770820906, 0.7014575617119839, 0.7145131656687976, 0.7262886528010896, 0.736677012671583, 0.7466654202146658, 0.7546651879635304, 0.7625719572413491, 0.7694540569803165, 0.7757884011410952, 0.781617411177588, 0.7874647532676605, 0.7925504327319336, 0.7971153372185295, 0.8004098511246885, 0.803730976563801, 0.8076100732315925, 0.8112620106561742, 0.813339228735888, 0.8161985240475402, 0.8185796658540061, 0.8207471075423254, 0.8225848532545837, 0.8240608664701645, 0.8262580366002565, 0.8277633864605622, 0.8301783011089338, 0.8313842802977391, 0.832625264447935, 0.83457634634275, 0.8360118487096044, 0.8367452188555867, 0.8386596347524076, 0.8396665122504349, 0.8403315028269728, 0.8414091570905857, 0.8420403375788162, 0.8429196057787659, 0.8439093481846995, 0.8440087183361091, 0.8452253163820163, 0.8462225983733327, 0.8470316144207886, 0.8486448121654567, 0.8496032613423599, 0.850223783993892, 0.8512217075052708, 0.8514474224381021, 0.853237627012816, 0.85397799387667, 0.8539799968233402, 0.8545728845186418, 0.8552376722621842, 0.8553750554971857, 0.8551773466342142, 0.8558447823397687, 0.8574352735146773, 0.8573478961537969, 0.8581411032910528, 0.8585854055184234, 0.8589223304221534, 0.8592539478599832, 0.8597568345593012, 0.8587698853371813, 0.8595520823738698, 0.8596467376530491, 0.8595376444129701, 0.8590823680646701, 0.8592585568512001, 0.8594487473821946, 0.8621498628548185, 0.8648382439450897, 0.867012616792674, 0.8688718961055, 0.870595105120703, 0.8721083426319761, 0.8735699716594411, 0.8749841235428192, 0.8761703815104499, 0.8773122853774772, 0.878192484974142, 0.8788350917101616, 0.8797481751596273, 0.8803115435905773, 0.8810627158034322, 0.881554635817592, 0.8820614875125647, 0.882393524708221, 0.8827900144343116, 0.8832556889603834, 0.883699210096348, 0.8840495509937162, 0.8843404437388476, 0.8845656261157159, 0.8848171183798973, 0.8850200768638202, 0.8851640593882815, 0.8853312942627063, 0.8855306337746888, 0.8856479746705633, 0.88574240395426, 0.8859260760682467, 0.886114765524675, 0.8861503086917105, 0.8862189186357925, 0.8863417027417162, 0.8864155873432975, 0.8865685622121303, 0.8866807960229203, 0.8867329783276312, 0.8868409775581211, 0.8867418348569023, 0.8867634992157151, 0.8868694758660562, 0.8869638253427037, 0.8870121187779363, 0.8870189617758957, 0.8870495345365592, 0.8870770500211563, 0.8871760856534533, 0.887166531963861, 0.8871457266119779], "moving_var_accuracy_train": [0.024732367957352164, 0.04845768926312721, 0.06933927170979344, 0.08736448389417266, 0.10213802465360367, 0.1116958843539419, 0.11745610216498152, 0.11930798895467679, 0.1200366624082285, 0.11757672317532694, 0.11557410413171168, 0.11202654450699821, 0.10848147883193146, 0.10371244279494522, 0.09862602279594222, 0.09351638801512062, 0.0877061430117283, 0.08265307824541215, 0.07786010484961661, 0.07243181697570805, 0.06727096155543781, 0.0627349959745679, 0.05857099839753966, 0.05445561498310444, 0.05040464647561839, 0.04663581437296223, 0.04283098265611999, 0.03948941568223876, 0.036290459557620586, 0.033200153284404094, 0.030426407414777656, 0.027912835037362544, 0.025449147203764497, 0.02320187652911762, 0.021065194500738287, 0.01917117520374179, 0.01751230747451025, 0.01598272576258099, 0.014471561168507376, 0.013205479723595481, 0.012010162375821894, 0.010908612431313185, 0.009923869907051412, 0.008970217875338237, 0.00817775127757035, 0.007412837925080453, 0.00675840632895036, 0.006111901910558449, 0.0055602124108013835, 0.00508848369481462, 0.004638574066651658, 0.004217754781779156, 0.0038640098002648616, 0.003508847354617181, 0.003181472045372009, 0.002890457451733068, 0.0026167222412062778, 0.002388552912242832, 0.0021731874286440777, 0.0019611469612817525, 0.0017925047989051533, 0.001632647947853196, 0.0014820494210582404, 0.0013653831335823214, 0.0012469275196656422, 0.001133902478656489, 0.0010388380521554703, 0.0009422547250304439, 0.0008796058799420387, 0.0008053159244260621, 0.0007248823678472666, 0.000660277792845722, 0.0006047817391326872, 0.0005518849598772812, 0.0004966968212358058, 0.00045608922886876424, 0.0004409491784405042, 0.0003975571939114105, 0.000361223720884802, 0.0003372981864510908, 0.0003045170630192853, 0.00027823267183627243, 0.00025399309530534895, 0.00022991295578622857, 0.00022442066150914767, 0.00020198646203229412, 0.00018287448958726138, 0.000164950456839881, 0.00015552546619059993, 0.00014287176874287956, 0.00024422124070532983, 0.0003223052644182467, 0.0003793207048484816, 0.00041923710536825604, 0.00044137163636515927, 0.0004500268755332699, 0.0004476962673703073, 0.0004405067814514384, 0.00042651356762361776, 0.0004080016217660278, 0.00038750068269523025, 0.00036536313228814236, 0.0003432736805813439, 0.00031971255562610474, 0.0002970915368104219, 0.0002756629197098864, 0.00025469590883540986, 0.00023453903105806432, 0.0002150481488221483, 0.0001983366704239057, 0.000182495477185959, 0.0001674053916919317, 0.00015333682227273857, 0.0001400861183786274, 0.00012783798228949465, 0.00011638196462222586, 0.00010584817367137194, 9.619815673678438e-05, 8.732343848183602e-05, 7.915188787816669e-05, 7.170054012711424e-05, 6.486465757534695e-05, 5.8759888664314497e-05, 5.3120473332034666e-05, 4.803180195973303e-05, 4.3415539156464066e-05, 3.9220008616656997e-05, 3.54362569985011e-05, 3.199547084487875e-05, 2.8909793276477937e-05, 2.607354493859098e-05, 2.3507633816322662e-05, 2.1193044288468155e-05, 1.9122141158717626e-05, 1.723650167597632e-05, 1.5521136171677625e-05, 1.3974638957814403e-05, 1.2604434607519147e-05, 1.1358557802035314e-05, 1.0225491930709681e-05, 9.215088651246052e-06, 8.296879497977147e-06], "duration": 129562.775537, "accuracy_train": [0.5242175964378, 0.59195404784976, 0.6410332528723699, 0.686456206453027, 0.7236012697835917, 0.7323180544019934, 0.7441981228659099, 0.7425487091638981, 0.7677715269126062, 0.7558685674487818, 0.7920181426495018, 0.7940410221137875, 0.8172398789336471, 0.8146124607788853, 0.8230291389811739, 0.8347446445874861, 0.8262847078026948, 0.8509949113487449, 0.8645015025147655, 0.8495765705749354, 0.8560157908245662, 0.8751499630860096, 0.8878191401924143, 0.8891441145256552, 0.8884233183947029, 0.8952570929655776, 0.8859586606566077, 0.9003266383467147, 0.8995596997277593, 0.8947713351328904, 0.9030470624538575, 0.9096014586794019, 0.9009290141080657, 0.904138079953396, 0.897535738798911, 0.9054877477274824, 0.9153227667035806, 0.9167385840600776, 0.9031854899178663, 0.9200406558577889, 0.9169736223583426, 0.9166459385958842, 0.9210640818221669, 0.9109057049418604, 0.9263185576435032, 0.9198782559293098, 0.9291312667266519, 0.9192272142626431, 0.9286905699174051, 0.9361532161314139, 0.9342004516196014, 0.9330367957502769, 0.9408492957502769, 0.9347355963339794, 0.9341303366671282, 0.9369473716085271, 0.9343635725244556, 0.9419188642026578, 0.9407097868217055, 0.93382806732189, 0.9444071339170359, 0.9433622589055003, 0.9420140330841639, 0.9500568850359912, 0.9473836848814139, 0.9460125680601699, 0.9500347150124585, 0.9461985799649317, 0.9568237890480805, 0.9522904698458842, 0.9401546350129198, 0.9504532417981728, 0.9528474240956073, 0.9522897488695091, 0.9439664371077889, 0.9540575829411223, 0.9634261302025655, 0.9496612492501846, 0.9533124538575121, 0.9594039832502769, 0.952173491429033, 0.9560561294527501, 0.9562421413575121, 0.9467344456556847, 0.9641240353336102, 0.9518701406192323, 0.955078845976375, 0.9499420695482651, 0.9606137816076044, 0.9583122447743633, 0.989049270083518, 0.9905373653216132, 0.9916537972383721, 0.992723365690753, 0.9929326290836102, 0.9931411715000923, 0.9931182805001846, 0.9939553340716132, 0.9938394371193245, 0.9937693221668512, 0.9940479795358066, 0.9941177340000923, 0.9945598727620893, 0.9940944825119971, 0.9944436153216132, 0.9948621421073275, 0.9947923876430418, 0.9947687756667589, 0.9944897578096161, 0.9958154531192323, 0.9959077380952381, 0.99583834411914, 0.9959546015596161, 0.9958615956072352, 0.9959546015596161, 0.9958150926310447, 0.9958612351190477, 0.9959313500715209, 0.9959080985834257, 0.9958147321428571, 0.99583834411914, 0.9957220866786637, 0.9960472470238095, 0.9958150926310447, 0.9959313500715209, 0.9959546015596161, 0.9959313500715209, 0.9960243560239018, 0.9959778530477114, 0.996140613464378, 0.9959080985834257, 0.9958848470953304, 0.9959080985834257, 0.9960708590000923, 0.9959542410714286, 0.9957685896548542, 0.9957453381667589, 0.9960708590000923, 0.9959778530477114, 0.9957918411429494, 0.9960007440476191, 0.9958615956072352], "end": "2016-02-02 21:47:03.384000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0], "moving_var_accuracy_valid": [0.023448932171754762, 0.04687575384122047, 0.06621848070450365, 0.08309682339017829, 0.09664595321656905, 0.10573173475171779, 0.11147779039358215, 0.1129813793808985, 0.11369925597361627, 0.11123982641747371, 0.1091912525598262, 0.10526298247601923, 0.10133478665558748, 0.09637491380714723, 0.091455386095267, 0.08634794917362246, 0.08076104853675645, 0.07580607701912356, 0.07102377591211945, 0.06579787129966283, 0.060815608351292315, 0.05648521506600809, 0.0523707327115018, 0.04838161831517657, 0.04451471867084882, 0.040961161370984336, 0.037441011790207895, 0.03425956361490101, 0.031259876924764785, 0.02849500447581301, 0.025951300250281525, 0.023663892910918378, 0.021530280840346503, 0.019564797933057536, 0.017706002536652665, 0.016034671150628278, 0.014566630554188095, 0.013229997321346831, 0.011945831103768362, 0.010824828120504651, 0.009793373835176682, 0.008856316682909393, 0.008001080798344773, 0.007220580253623417, 0.006541970237486191, 0.005908167917554872, 0.005369837440629658, 0.004845943168801179, 0.004375209226870399, 0.00397194878922583, 0.0035932999137104514, 0.0032388104082785698, 0.002947914261484716, 0.0026622470560005483, 0.0024000022626024507, 0.0021704540847491544, 0.001956994175552746, 0.0017682527711044567, 0.0016002438042649408, 0.0014403082936813672, 0.0013095984615609764, 0.0011875897577367158, 0.0010747213446484147, 0.0009906708728541942, 0.0008998714089911312, 0.0008133497033415977, 0.0007409773950185027, 0.00066733818059478, 0.0006294478543092406, 0.0005714363567161527, 0.0005142927571506958, 0.00046602712380878575, 0.0004234018961235856, 0.00038123157389056224, 0.0003434602156519837, 0.0003131234278762265, 0.0003045780446857652, 0.0002741889534459385, 0.00025243265616468717, 0.00022896603077143696, 0.00020709109321107413, 0.00018737171501562218, 0.00017091059880521806, 0.00016258615782808366, 0.00015183403188311366, 0.00013673126529169087, 0.00012316525077780022, 0.00011271421467991288, 0.00010172217560841172, 9.187550999029011e-05, 0.00014835218215929582, 0.0001985634999221144, 0.00022125822545271385, 0.00023024467897536598, 0.00023394525486852195, 0.00023115971927138495, 0.00022727098206960216, 0.00022254231380600047, 0.000212952954117307, 0.0002033931586793633, 0.00019002660478114903, 0.00017474043505763312, 0.0001647698840230646, 0.0001511493515216769, 0.00014111275360979744, 0.0001291793459517965, 0.00011857349912288741, 0.00010770838750429161, 9.83523856799207e-05, 9.046882199001889e-05, 8.319233877344361e-05, 7.597775359541817e-05, 6.914154553840709e-05, 6.268375491023472e-05, 5.698461464969917e-05, 5.165688250049587e-05, 4.667777295659867e-05, 4.226170318995393e-05, 3.839315904029491e-05, 3.467776310886707e-05, 3.129023880455559e-05, 2.8464833933206958e-05, 2.593878393859129e-05, 2.335627539523839e-05, 2.1063013775556926e-05, 1.9092395828008733e-05, 1.7232286654365443e-05, 1.57196697833789e-05, 1.4261070659600995e-05, 1.2859470529965455e-05, 1.16784979810466e-05, 1.059911165978646e-05, 9.543424593792739e-06, 8.690161588171224e-06, 7.901261843046829e-06, 7.132125961721279e-06, 6.419334805138801e-06, 5.78581356787618e-06, 5.214046128122078e-06, 4.780914023490106e-06, 4.303644078004532e-06, 3.877175434206887e-06], "accuracy_test": 0.8478555484693878, "start": "2016-02-01 09:47:40.609000", "learning_rate_per_epoch": [0.0029452238231897354, 0.0029452238231897354, 0.0029452238231897354, 0.0029452238231897354, 0.0029452238231897354, 0.0029452238231897354, 0.0029452238231897354, 0.0029452238231897354, 0.0029452238231897354, 0.0029452238231897354, 0.0029452238231897354, 0.0029452238231897354, 0.0029452238231897354, 0.0029452238231897354, 0.0029452238231897354, 0.0029452238231897354, 0.0029452238231897354, 0.0029452238231897354, 0.0029452238231897354, 0.0029452238231897354, 0.0029452238231897354, 0.0029452238231897354, 0.0029452238231897354, 0.0029452238231897354, 0.0029452238231897354, 0.0029452238231897354, 0.0029452238231897354, 0.0029452238231897354, 0.0029452238231897354, 0.0029452238231897354, 0.0029452238231897354, 0.0029452238231897354, 0.0029452238231897354, 0.0029452238231897354, 0.0029452238231897354, 0.0029452238231897354, 0.0029452238231897354, 0.0029452238231897354, 0.0029452238231897354, 0.0029452238231897354, 0.0029452238231897354, 0.0029452238231897354, 0.0029452238231897354, 0.0029452238231897354, 0.0029452238231897354, 0.0029452238231897354, 0.0029452238231897354, 0.0029452238231897354, 0.0029452238231897354, 0.0029452238231897354, 0.0029452238231897354, 0.0029452238231897354, 0.0029452238231897354, 0.0029452238231897354, 0.0029452238231897354, 0.0029452238231897354, 0.0029452238231897354, 0.0029452238231897354, 0.0029452238231897354, 0.0029452238231897354, 0.0029452238231897354, 0.0029452238231897354, 0.0029452238231897354, 0.0029452238231897354, 0.0029452238231897354, 0.0029452238231897354, 0.0029452238231897354, 0.0029452238231897354, 0.0029452238231897354, 0.0029452238231897354, 0.0029452238231897354, 0.0029452238231897354, 0.0029452238231897354, 0.0029452238231897354, 0.0029452238231897354, 0.0029452238231897354, 0.0029452238231897354, 0.0029452238231897354, 0.0029452238231897354, 0.0029452238231897354, 0.0029452238231897354, 0.0029452238231897354, 0.0029452238231897354, 0.0029452238231897354, 0.0029452238231897354, 0.0029452238231897354, 0.0029452238231897354, 0.0029452238231897354, 0.0029452238231897354, 0.00029452238231897354, 0.00029452238231897354, 0.00029452238231897354, 0.00029452238231897354, 0.00029452238231897354, 0.00029452238231897354, 0.00029452238231897354, 0.00029452238231897354, 0.00029452238231897354, 0.00029452238231897354, 0.00029452238231897354, 0.00029452238231897354, 0.00029452238231897354, 0.00029452238231897354, 0.00029452238231897354, 0.00029452238231897354, 0.00029452238231897354, 0.00029452238231897354, 0.00029452238231897354, 2.9452237868099473e-05, 2.9452237413352123e-06, 2.945223798178631e-07, 2.945223798178631e-08, 2.945223842587552e-09, 2.945223787076401e-10, 2.945223925854279e-11, 2.945223925854279e-12, 2.9452238174340617e-13, 2.94522368190879e-14, 2.945223766612085e-15, 2.945223819551644e-16, 2.945223885726093e-17, 2.9452239270851237e-18, 2.9452238753863354e-19, 2.945223746139365e-20, 2.9452238269187214e-21, 2.9452239278929173e-22, 2.945223864784045e-23, 2.9452238253409996e-24, 2.945223776037193e-25, 2.9452238992967094e-26, 2.9452239378153083e-27, 2.9452240341118056e-28, 2.945224154482427e-29, 2.9452242297140655e-30, 2.9452241356745175e-31, 2.9452240181250824e-32, 2.945224054859281e-33, 2.945224054859281e-34, 2.945224054859281e-35, 2.9452240907325215e-36, 2.94522400104942e-37, 2.9452238889455427e-38], "accuracy_train_first": 0.5242175964378, "accuracy_train_last": 0.9958615956072352, "batch_size_eval": 1024, "accuracy_train_std": [0.013837639589633745, 0.019217585441212204, 0.017474883919311526, 0.019890599129340752, 0.02003796591238853, 0.018982468555475383, 0.017309400117961657, 0.017794590476719593, 0.018454259432584892, 0.018309720414223955, 0.01930278329296038, 0.018204709656909276, 0.019710708978793692, 0.02173343609679279, 0.020027715523033066, 0.022006814033519673, 0.021361496410994097, 0.02074598053462518, 0.020376536936601942, 0.020878809393017946, 0.02138184481245152, 0.019703802894991077, 0.01933845366728888, 0.017534258137786737, 0.019933011402252076, 0.018954570866323087, 0.020649480746175033, 0.019551938623913686, 0.01824214185497472, 0.01792384900268296, 0.018749475355541934, 0.019310929120276633, 0.020513173359320912, 0.019493919550453296, 0.0196588483656995, 0.017603281826750843, 0.017130044832203522, 0.019098487264983185, 0.018919854464833852, 0.018745318909118163, 0.01684784845321197, 0.018771944511581616, 0.018742157666352163, 0.01860244741621133, 0.018299463263096383, 0.016555202473509893, 0.01584721010481806, 0.017775389320863266, 0.017633026867354046, 0.015990210256035364, 0.015142505495940494, 0.01651799450760133, 0.016143454735799905, 0.01697707568571492, 0.016912758362066863, 0.01549689557157509, 0.017059346003094965, 0.01535043836450106, 0.014697696846018604, 0.016749019781106118, 0.013852424877029203, 0.015501301904184346, 0.01457031494754226, 0.013981714712194355, 0.013639628575844233, 0.013939589120120707, 0.012602980078427427, 0.014508688575382213, 0.012186442677643548, 0.013458218614939534, 0.014508257526424177, 0.01319980349098544, 0.01294993123303063, 0.013089106508017546, 0.013010946779338211, 0.013498960836996674, 0.01090215071632144, 0.013874123870117253, 0.01298481715563449, 0.012181983972176673, 0.012601084816369483, 0.012957837608561736, 0.01248888290427344, 0.014191103569923154, 0.011536140507690154, 0.012879435458678328, 0.013657422864293053, 0.013063967570941715, 0.01259839706919276, 0.01216808752030393, 0.004863054717155853, 0.004715632295776452, 0.0038495815744094115, 0.0035676272246016557, 0.003209794109194206, 0.003926924577128181, 0.0030140940344544685, 0.003141425326705118, 0.0032910752890621103, 0.003514763802961047, 0.003094575062371362, 0.0031862387362507393, 0.0028125779310002115, 0.0030735337449634744, 0.002733552623983298, 0.0030144840301089015, 0.0029964224633712735, 0.0028341438989654914, 0.002660619944471642, 0.0021341755016913025, 0.0022374649783704257, 0.002184644737031094, 0.0022002838530283566, 0.0021293748575961194, 0.002169103465913804, 0.002197356242344761, 0.002263410253652894, 0.0021831440611099254, 0.0021232648056896783, 0.002356558679188975, 0.0022156064215233928, 0.002183550036206608, 0.00208746053640843, 0.0021022980025670436, 0.0021411365535190627, 0.002050723325144849, 0.002193520242497331, 0.002092596148385241, 0.0021121477661323043, 0.0021520571146312587, 0.002112543578815456, 0.0021049845286719043, 0.002058099866130008, 0.002115201241407235, 0.0022610204205995572, 0.0020847456698909667, 0.0023232972099504123, 0.00219940464515714, 0.0020686990924814646, 0.002136610301928255, 0.002233111458992368, 0.0021400117632500307], "accuracy_test_std": 0.0070359642462125655, "error_valid": [0.4895651943712349, 0.4138374788215362, 0.3787209384412651, 0.33277984986822284, 0.2998488092996988, 0.28695053652108427, 0.27192412227033136, 0.28023666933358427, 0.2522796263177711, 0.2664809629141567, 0.23211743458207834, 0.23920780779367468, 0.21927946159638556, 0.22320630176957834, 0.2100315323795181, 0.20427363751882532, 0.2108860245670181, 0.19028555628765065, 0.1815567935805723, 0.1958596103162651, 0.19258430205195776, 0.17300157661897586, 0.16798639871987953, 0.1677319630082832, 0.16982774849397586, 0.1634389118975903, 0.17333690229668675, 0.1662671192582832, 0.16860704536897586, 0.16720250141189763, 0.16592149849397586, 0.15990916792168675, 0.1616784520896084, 0.1618005224021084, 0.16993952371987953, 0.16637889448418675, 0.1574780567582832, 0.1558705525225903, 0.16796580854668675, 0.1580678181475903, 0.15999005788780118, 0.15974591726280118, 0.1608754353350903, 0.1626550145896084, 0.15396743222891573, 0.15868846479668675, 0.14808746705572284, 0.15776190700301207, 0.15620587820030118, 0.14786391660391573, 0.15106862998870485, 0.1566544498305723, 0.14411062217620485, 0.1512715902673193, 0.15368358198418675, 0.14889195453689763, 0.1522790380271084, 0.14916698042168675, 0.14718297016189763, 0.15509695030120485, 0.1438253012048193, 0.1448018637048193, 0.1456872411521084, 0.13683640813253017, 0.14177069606551207, 0.1441915121423193, 0.1397969808923193, 0.14652114316641573, 0.13065053181475905, 0.1393587043486446, 0.1460019766566265, 0.1400911262236446, 0.13877923804593373, 0.14338849538780118, 0.14660203313253017, 0.13814829631024095, 0.1282503059111446, 0.1434385000941265, 0.1347200324736446, 0.13741587443524095, 0.13804534544427716, 0.13776149519954817, 0.13571718514683728, 0.15011265766189763, 0.13340814429593373, 0.13950136483433728, 0.14144419474774095, 0.14501511907003017, 0.13915574407003017, 0.1388395378388554, 0.11354009789156627, 0.11096632624246983, 0.11341802757906627, 0.11439459007906627, 0.11389601374246983, 0.11427251976656627, 0.11327536709337349, 0.11228850950677716, 0.11315329678087349, 0.11241057981927716, 0.11388571865587349, 0.11538144766566272, 0.11203407379518071, 0.11461814053087349, 0.11217673428087349, 0.11401808405496983, 0.11337684723268071, 0.11461814053087349, 0.11364157803087349, 0.11255324030496983, 0.11230909967996983, 0.11279738092996983, 0.11304152155496983, 0.11340773249246983, 0.11291945124246983, 0.11315329678087349, 0.11354009789156627, 0.11316359186746983, 0.11267531061746983, 0.11329595726656627, 0.11340773249246983, 0.11242087490587349, 0.11218702936746983, 0.11352980280496983, 0.11316359186746983, 0.11255324030496983, 0.11291945124246983, 0.11205466396837349, 0.11230909967996983, 0.11279738092996983, 0.11218702936746983, 0.11415044945406627, 0.11304152155496983, 0.11217673428087349, 0.11218702936746983, 0.11255324030496983, 0.11291945124246983, 0.11267531061746983, 0.11267531061746983, 0.11193259365587349, 0.11291945124246983, 0.11304152155496983], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.016132608151528904, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 32, "valid_ratio": 0.15, "learning_rate": 0.0029452237250373993, "optimization": "nesterov_momentum", "nb_data_augmentation": 2, "learning_rate_decay_method": "discrete", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 6.861295389258028e-06, "rotation_range": [0, 0], "momentum": 0.9261954691523886}, "accuracy_valid_max": 0.8890336737575302, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8869584784450302, "accuracy_valid_std": [0.029353504341187366, 0.02646974232584469, 0.029013699722012253, 0.027752841893441342, 0.01712961164561091, 0.0270106756479619, 0.02304023833832683, 0.028477966375250487, 0.015133756143363387, 0.01453143403448702, 0.017736483900908356, 0.015833834988731386, 0.02117544946832311, 0.020073859939913546, 0.014398950871240716, 0.015492454423973, 0.015242945895737258, 0.012920333259930371, 0.009947352239682327, 0.013100692833066648, 0.007014900406540223, 0.009077785766104055, 0.012161265753292051, 0.009525236896351017, 0.011245114078914204, 0.007822901125297288, 0.008801150274596494, 0.013153178002640844, 0.012706263599892473, 0.009125748688086223, 0.01219038190186145, 0.01014063288792115, 0.010807704759804753, 0.009590727046812927, 0.008977382976320222, 0.011897016830402863, 0.010406031959329412, 0.013415226245055348, 0.009183832931132938, 0.011033273358526226, 0.010975806603677692, 0.01089305500814918, 0.009768187692892017, 0.006947540713512241, 0.009389598147800104, 0.009820771715813284, 0.0076873265847993795, 0.008645291080254061, 0.01256736435827871, 0.009273121327983108, 0.01099854028037903, 0.0194732633064209, 0.010385449409123941, 0.010964804406986063, 0.011517067191698557, 0.012384961755389971, 0.008678256403916508, 0.015105542449345637, 0.012569026452809655, 0.005256738361669572, 0.009357110205813061, 0.006385433788381388, 0.01224655391451711, 0.006995587367252226, 0.01056581811836403, 0.007013945693345699, 0.011727053027949054, 0.011730423523685904, 0.01012298625701298, 0.009283157570429376, 0.009042155706133424, 0.0096549232955741, 0.00693047958620841, 0.015169258611812203, 0.009625747658155269, 0.01194670602319188, 0.008726333253897985, 0.009411204468274759, 0.006251687778421052, 0.008912438020197262, 0.015469856357385296, 0.011612705596607077, 0.008870316055652206, 0.013268761163014823, 0.006025799365508143, 0.010689340089851436, 0.011016563701435354, 0.008216006057907921, 0.006439209981627195, 0.010801455131895309, 0.006774368140655449, 0.006493281340833976, 0.005528323387602236, 0.006330116706030001, 0.006342673042290542, 0.005644769998450034, 0.005907730124590997, 0.0072642890859829674, 0.005186320025548284, 0.006015717182627147, 0.007115418579887785, 0.006994966465729092, 0.008221931864484786, 0.007818453593037738, 0.007433588384386385, 0.006615134112923606, 0.007572631878922554, 0.006894806565647484, 0.0070478815780066015, 0.007235744224896797, 0.007530556947628893, 0.007223228237363835, 0.007636239270489406, 0.007584138519025366, 0.008079113173242896, 0.007960901833959656, 0.0075088268847800935, 0.007862114593691912, 0.008210967742156334, 0.007869745512544272, 0.007937429331660233, 0.00831200254868681, 0.008545591649565315, 0.007775191871495957, 0.0068055593629577395, 0.007972557162284932, 0.007623615434491696, 0.007721582754965522, 0.007886248490937735, 0.0076090125553509086, 0.007442053600156666, 0.007470822244918313, 0.007790785325445709, 0.00853822060630901, 0.00840493610029873, 0.0077450235002469395, 0.0075608093466647975, 0.007854803967273427, 0.00829762030568652, 0.008724505110713711, 0.008676730374301861, 0.0070687693361440145], "accuracy_valid": [0.5104348056287651, 0.5861625211784638, 0.6212790615587349, 0.6672201501317772, 0.7001511907003012, 0.7130494634789157, 0.7280758777296686, 0.7197633306664157, 0.7477203736822289, 0.7335190370858433, 0.7678825654179217, 0.7607921922063253, 0.7807205384036144, 0.7767936982304217, 0.7899684676204819, 0.7957263624811747, 0.7891139754329819, 0.8097144437123494, 0.8184432064194277, 0.8041403896837349, 0.8074156979480422, 0.8269984233810241, 0.8320136012801205, 0.8322680369917168, 0.8301722515060241, 0.8365610881024097, 0.8266630977033133, 0.8337328807417168, 0.8313929546310241, 0.8327974985881024, 0.8340785015060241, 0.8400908320783133, 0.8383215479103916, 0.8381994775978916, 0.8300604762801205, 0.8336211055158133, 0.8425219432417168, 0.8441294474774097, 0.8320341914533133, 0.8419321818524097, 0.8400099421121988, 0.8402540827371988, 0.8391245646649097, 0.8373449854103916, 0.8460325677710843, 0.8413115352033133, 0.8519125329442772, 0.8422380929969879, 0.8437941217996988, 0.8521360833960843, 0.8489313700112951, 0.8433455501694277, 0.8558893778237951, 0.8487284097326807, 0.8463164180158133, 0.8511080454631024, 0.8477209619728916, 0.8508330195783133, 0.8528170298381024, 0.8449030496987951, 0.8561746987951807, 0.8551981362951807, 0.8543127588478916, 0.8631635918674698, 0.8582293039344879, 0.8558084878576807, 0.8602030191076807, 0.8534788568335843, 0.869349468185241, 0.8606412956513554, 0.8539980233433735, 0.8599088737763554, 0.8612207619540663, 0.8566115046121988, 0.8533979668674698, 0.861851703689759, 0.8717496940888554, 0.8565614999058735, 0.8652799675263554, 0.862584125564759, 0.8619546545557228, 0.8622385048004518, 0.8642828148531627, 0.8498873423381024, 0.8665918557040663, 0.8604986351656627, 0.858555805252259, 0.8549848809299698, 0.8608442559299698, 0.8611604621611446, 0.8864599021084337, 0.8890336737575302, 0.8865819724209337, 0.8856054099209337, 0.8861039862575302, 0.8857274802334337, 0.8867246329066265, 0.8877114904932228, 0.8868467032191265, 0.8875894201807228, 0.8861142813441265, 0.8846185523343373, 0.8879659262048193, 0.8853818594691265, 0.8878232657191265, 0.8859819159450302, 0.8866231527673193, 0.8853818594691265, 0.8863584219691265, 0.8874467596950302, 0.8876909003200302, 0.8872026190700302, 0.8869584784450302, 0.8865922675075302, 0.8870805487575302, 0.8868467032191265, 0.8864599021084337, 0.8868364081325302, 0.8873246893825302, 0.8867040427334337, 0.8865922675075302, 0.8875791250941265, 0.8878129706325302, 0.8864701971950302, 0.8868364081325302, 0.8874467596950302, 0.8870805487575302, 0.8879453360316265, 0.8876909003200302, 0.8872026190700302, 0.8878129706325302, 0.8858495505459337, 0.8869584784450302, 0.8878232657191265, 0.8878129706325302, 0.8874467596950302, 0.8870805487575302, 0.8873246893825302, 0.8873246893825302, 0.8880674063441265, 0.8870805487575302, 0.8869584784450302], "seed": 980527187, "model": "residualv3", "loss_std": [0.25850725173950195, 0.18906064331531525, 0.1810532808303833, 0.17859791219234467, 0.18042004108428955, 0.17859295010566711, 0.1733369082212448, 0.1714203655719757, 0.16780610382556915, 0.16502733528614044, 0.16205193102359772, 0.16121256351470947, 0.1582878977060318, 0.15763789415359497, 0.1554253101348877, 0.15342406928539276, 0.15274566411972046, 0.14859047532081604, 0.1473631113767624, 0.1451958417892456, 0.14453455805778503, 0.14460359513759613, 0.14107103645801544, 0.13694512844085693, 0.13775749504566193, 0.13505002856254578, 0.13439802825450897, 0.130186527967453, 0.13083741068840027, 0.1270483136177063, 0.12774524092674255, 0.12408717721700668, 0.12301024049520493, 0.1197245866060257, 0.12094517797231674, 0.11971426755189896, 0.11398162692785263, 0.11557748168706894, 0.11243066936731339, 0.11409956216812134, 0.10867337137460709, 0.10632394999265671, 0.10810612142086029, 0.10473216325044632, 0.10212167352437973, 0.10245456546545029, 0.10198403894901276, 0.10250517725944519, 0.097284696996212, 0.09768644720315933, 0.09723407030105591, 0.09482806921005249, 0.09427974373102188, 0.09313400089740753, 0.09314883500337601, 0.09118571877479553, 0.08942646533250809, 0.08677133917808533, 0.08687751740217209, 0.08913111686706543, 0.08630164712667465, 0.08693055808544159, 0.08314106613397598, 0.08277682214975357, 0.0806402787566185, 0.08000298589468002, 0.08360075950622559, 0.0785621777176857, 0.0784204751253128, 0.07604270428419113, 0.07614438235759735, 0.07628266513347626, 0.07653816789388657, 0.07831311970949173, 0.07398353517055511, 0.07445438951253891, 0.0735950767993927, 0.07005807012319565, 0.07144174724817276, 0.07294239103794098, 0.06951958686113358, 0.07018809020519257, 0.06955510377883911, 0.06678974628448486, 0.06844784319400787, 0.06881673634052277, 0.06436999142169952, 0.06815677136182785, 0.0635155662894249, 0.06493217498064041, 0.0652816891670227, 0.046930596232414246, 0.046491947025060654, 0.0445546954870224, 0.04237966984510422, 0.0415726900100708, 0.040035322308540344, 0.040383029729127884, 0.03921816498041153, 0.03912220895290375, 0.03870472311973572, 0.03803830221295357, 0.037935420870780945, 0.03881986066699028, 0.03602677583694458, 0.035521239042282104, 0.03654492646455765, 0.035762932151556015, 0.035031821578741074, 0.033369045704603195, 0.0330645926296711, 0.033016156405210495, 0.033505648374557495, 0.03325950726866722, 0.03198821842670441, 0.033512309193611145, 0.032658644020557404, 0.03419426456093788, 0.03408994525671005, 0.032606035470962524, 0.03318173065781593, 0.03202301263809204, 0.031901225447654724, 0.03390098363161087, 0.03301616013050079, 0.032971300184726715, 0.034086473286151886, 0.032677002251148224, 0.03329458087682724, 0.03370240703225136, 0.033429715782403946, 0.031437043100595474, 0.03254397585988045, 0.03163057938218117, 0.033323995769023895, 0.033076174557209015, 0.03359531983733177, 0.03241223841905594, 0.03340794891119003, 0.03343069553375244, 0.034034233540296555, 0.032829876989126205]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:31 2016", "state": "available"}], "summary": "9039e8925cd272500166bcb043cb941a"}