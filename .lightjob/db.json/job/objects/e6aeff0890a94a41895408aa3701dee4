{"content": {"hp_model": {"f0": 16, "f1": 16, "f2": 16, "f3": 32, "nonlin": "leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.6630598306655884, 1.3834798336029053, 1.2497118711471558, 1.1597206592559814, 1.0882434844970703, 1.0253387689590454, 0.9680862426757812, 0.9217803478240967, 0.8802845478057861, 0.8441154360771179, 0.8137646317481995, 0.785393238067627, 0.7603425979614258, 0.7375001907348633, 0.7169597148895264, 0.6982496976852417, 0.6815868020057678, 0.6644927859306335, 0.6489061117172241, 0.6362778544425964, 0.6226157546043396, 0.609360933303833, 0.5949068665504456, 0.5852939486503601, 0.5726981163024902, 0.5617179870605469, 0.5524373054504395, 0.5440763235092163, 0.5338839292526245, 0.523747980594635, 0.5154677629470825, 0.508059561252594, 0.5015040040016174, 0.4938015639781952, 0.485952228307724, 0.4797235429286957, 0.47249794006347656, 0.4645477831363678, 0.45775631070137024, 0.45209184288978577, 0.4452917277812958, 0.44062817096710205, 0.43555790185928345, 0.4275464713573456, 0.4226601719856262, 0.41809940338134766, 0.4150475263595581, 0.40955835580825806, 0.4039432406425476, 0.3973335027694702, 0.3958421051502228, 0.3899627923965454, 0.3848809003829956, 0.3822484016418457, 0.37793251872062683, 0.3723565638065338, 0.3711855411529541, 0.365823358297348, 0.3624120056629181, 0.3567735254764557, 0.35201215744018555, 0.34913185238838196, 0.3477080166339874, 0.3436529040336609, 0.33770424127578735, 0.33500224351882935, 0.33371037244796753, 0.33033275604248047, 0.32707393169403076, 0.32479867339134216, 0.3196845054626465, 0.3164641261100769, 0.31356215476989746, 0.31272801756858826, 0.30745017528533936, 0.3055225908756256, 0.30285879969596863, 0.3004007935523987, 0.29955607652664185, 0.29508107900619507, 0.29119056463241577, 0.29166296124458313, 0.28890252113342285, 0.2858145534992218, 0.2814178168773651, 0.2813587784767151, 0.2774123549461365, 0.2752828001976013, 0.27328038215637207, 0.2702438533306122, 0.26962947845458984, 0.26713767647743225, 0.2633986175060272, 0.2615857422351837, 0.26060521602630615, 0.2575812339782715, 0.2546898424625397, 0.2558833360671997, 0.2527664005756378, 0.25102105736732483, 0.24904955923557281, 0.24775926768779755, 0.24692660570144653, 0.2432805746793747, 0.24006976187229156, 0.24251163005828857, 0.23932254314422607, 0.23693592846393585, 0.2364639937877655, 0.23180949687957764, 0.23153327405452728, 0.22988572716712952, 0.2297038733959198, 0.22941617667675018, 0.22464174032211304, 0.222753643989563, 0.22371186316013336, 0.22200514376163483, 0.21952565014362335, 0.21660292148590088, 0.21767154335975647, 0.21651387214660645, 0.21389532089233398, 0.2126130610704422, 0.21070756018161774, 0.2107917219400406, 0.20676369965076447, 0.2081473469734192, 0.20612412691116333, 0.2041957527399063, 0.20671407878398895, 0.20152172446250916, 0.2026803344488144, 0.19996537268161774, 0.19889326393604279, 0.1975877732038498, 0.19432774186134338, 0.1964990198612213, 0.1955229640007019, 0.19203785061836243, 0.1932728886604309, 0.1912011355161667, 0.19079500436782837, 0.1884012222290039, 0.1858965903520584, 0.18779827654361725, 0.18556299805641174, 0.1843128800392151, 0.18321137130260468, 0.18126872181892395, 0.18276813626289368, 0.18060821294784546, 0.17841951549053192, 0.17916414141654968, 0.17746371030807495, 0.17608095705509186, 0.17644819617271423, 0.17380648851394653, 0.17296047508716583, 0.17396007478237152, 0.16921545565128326, 0.17256344854831696, 0.17138758301734924, 0.16918593645095825, 0.16851802170276642, 0.16755351424217224, 0.16768740117549896, 0.1649375557899475, 0.16360552608966827, 0.16295601427555084, 0.16342280805110931, 0.16217419505119324, 0.1612706184387207, 0.16157080233097076, 0.16093957424163818, 0.15835246443748474, 0.15691635012626648, 0.15962159633636475, 0.15652811527252197, 0.15683196485042572, 0.15535955131053925, 0.15411750972270966, 0.15595018863677979, 0.1523425132036209, 0.1530478298664093, 0.151544451713562, 0.15256506204605103, 0.15141667425632477, 0.14882540702819824, 0.1494511216878891, 0.15035516023635864, 0.14821210503578186, 0.1486440747976303, 0.1470099687576294, 0.14628180861473083, 0.14672869443893433, 0.1453329473733902, 0.14269815385341644, 0.14202918112277985, 0.1414438784122467, 0.14295002818107605, 0.1402616798877716, 0.14172698557376862, 0.14130352437496185, 0.14021316170692444, 0.13999943435192108, 0.1395529955625534, 0.13962434232234955, 0.13630108535289764, 0.13599029183387756, 0.13796398043632507, 0.13571815192699432, 0.13598506152629852, 0.1364338994026184, 0.13438941538333893, 0.13283410668373108, 0.1321868747472763, 0.13409458100795746, 0.13189412653446198, 0.13098913431167603, 0.13110433518886566, 0.12950357794761658, 0.12852592766284943, 0.13049821555614471, 0.12885747849941254, 0.1274576485157013, 0.12838123738765717, 0.1287156492471695, 0.12668828666210175, 0.12723250687122345, 0.12349638342857361, 0.12675949931144714, 0.12510420382022858, 0.12478434294462204, 0.12457509338855743, 0.12530101835727692, 0.12171441316604614, 0.12206081300973892, 0.12438788264989853, 0.12355916947126389, 0.12104713171720505, 0.12090791016817093, 0.11912550777196884, 0.12136285752058029, 0.117314912378788, 0.12003850191831589, 0.11993682384490967, 0.11811354756355286, 0.11821797490119934, 0.11933620274066925, 0.11973965167999268], "moving_avg_accuracy_train": [0.048199999999999986, 0.0995941176470588, 0.15038058823529407, 0.19896605882352936, 0.2459188647058823, 0.2913246252941176, 0.3340674568823529, 0.3744724759, 0.4121122871335294, 0.44707517606723524, 0.4793605996369823, 0.5096763043791664, 0.5376192621765439, 0.5639491006647719, 0.5882083082453535, 0.6108910068325828, 0.6314654355610892, 0.6504083037696862, 0.6676121792750704, 0.6836980201710928, 0.69873057109516, 0.7126057492797617, 0.7257051743517856, 0.7378923039754306, 0.7486136618131817, 0.7588205309259812, 0.7684349484216184, 0.7771138065206331, 0.7852353670450404, 0.7932294773993598, 0.8001300590711885, 0.8065311708111285, 0.8126404066711921, 0.8182681307099552, 0.823377788227195, 0.8283435388162401, 0.8334597731699103, 0.8377985017352721, 0.841813945679392, 0.8457101981702764, 0.849755648941484, 0.8532506722826297, 0.8567561932896609, 0.8600358680783419, 0.8629899283293312, 0.8656862296140452, 0.8683858419467583, 0.8708978459873765, 0.8732268849180507, 0.8752430199556573, 0.8773893061953857, 0.8796715520464353, 0.881810279194733, 0.8836527806870245, 0.8849557379124396, 0.8871142817682545, 0.8885840300620172, 0.8902973917616979, 0.8914911819972928, 0.893059710856387, 0.894861975064866, 0.8963004834407323, 0.8979339645084239, 0.8989523327634639, 0.899650040663588, 0.9009673895384057, 0.9019318270551534, 0.9029951149378733, 0.9041144269734978, 0.9050794548643832, 0.9059526858485332, 0.9069668290283857, 0.9081242637726059, 0.9089941903365217, 0.9105441830675755, 0.911565058878465, 0.9125732588729714, 0.9133535800444978, 0.9144346926282834, 0.9149041645419256, 0.9154961010289095, 0.9166217850436656, 0.9170866653628285, 0.9175379988265456, 0.9185253754144793, 0.9194728378730314, 0.920189083497493, 0.9206925280889202, 0.9212515105741459, 0.9219993006932019, 0.9225476059179993, 0.9233469629732582, 0.9238993254994619, 0.9244482164789274, 0.9253469242427994, 0.9262592906420489, 0.9267368909896088, 0.9268467313024126, 0.9274232346427596, 0.9281067935314248, 0.9286443494723999, 0.9293705027604541, 0.9300075701314675, 0.9301079895889091, 0.9306642494535475, 0.9313507656846634, 0.9320651008809029, 0.9323480025575185, 0.9328073199488255, 0.9332865879539429, 0.9330591056291369, 0.9336684891838702, 0.934080463794895, 0.9344159468271701, 0.9352825874385707, 0.935801387518243, 0.9367953664134776, 0.9370852415368357, 0.9377531879713874, 0.9384555162330722, 0.9383723175509414, 0.9390150857958472, 0.9396900478044978, 0.9401492783181656, 0.940628468133408, 0.9408338566141848, 0.9407622356586487, 0.940782482681019, 0.9410501167658584, 0.9416698109716255, 0.9421734181097571, 0.9427301939458401, 0.9427583510218444, 0.9424707512137775, 0.9428142643276939, 0.9430740143655127, 0.9437124952819027, 0.9441365398713595, 0.9444358270606941, 0.9445757737663895, 0.9450593728603387, 0.9455204943978343, 0.9459684449580508, 0.9462257181093044, 0.9464666757101386, 0.9471023610803013, 0.947090948501683, 0.9472053830632793, 0.9477271976981279, 0.9478274191047857, 0.9476729124884249, 0.9476067977101706, 0.9476978826450358, 0.9482645649687675, 0.9484898731777731, 0.9486926505658781, 0.9489786796269374, 0.9492690469583613, 0.9496433187331135, 0.9500789868598021, 0.9502287352326454, 0.9507493911211455, 0.9508720990678545, 0.9511637126904808, 0.9509132237743739, 0.9510666072792895, 0.9513458289043017, 0.9511359518962245, 0.9508152978830725, 0.9510890622124124, 0.9514883912852888, 0.9521324933332305, 0.9526298322352016, 0.9526115548940344, 0.9527339288163956, 0.9529405359347561, 0.9532347176353981, 0.953419481165976, 0.9538540036376136, 0.9539038973914994, 0.9539346841229377, 0.9542235686518203, 0.9546623882572266, 0.9548573259020922, 0.9551104168412947, 0.9552158457454005, 0.9553130847002722, 0.9554476585831861, 0.9560487750778087, 0.9558815446288513, 0.9559380960483191, 0.9559819335023108, 0.9561907989756091, 0.9561576014309894, 0.9560806648173023, 0.9566420101002778, 0.9567683973255441, 0.9569103811224015, 0.9574781665395731, 0.9575044675326746, 0.9580199031323483, 0.9579261481132312, 0.9580770627136728, 0.958227003501129, 0.9587219502098396, 0.9588332846006203, 0.9586628973170289, 0.9588554311147378, 0.9589487115326758, 0.9592279580264671, 0.9594816328120557, 0.9595099401190854, 0.9595330637542356, 0.9595562279670473, 0.9596500169350485, 0.9599250152415436, 0.9599019254820951, 0.9604482035221208, 0.9608669125816734, 0.960900221323506, 0.9613066697793907, 0.9613312969190987, 0.9614287554624829, 0.9612741152103522, 0.9611608213363758, 0.96098356273215, 0.961049912341288, 0.9610884505189239, 0.9614948995846786, 0.9616842331556225, 0.9621628686635897, 0.9626124641501719, 0.9627841589116253, 0.963126919491051, 0.9631577569537106, 0.9633855106701043, 0.963632841956035, 0.9635424989369021, 0.9632306019843884, 0.9633263653153613, 0.9636384346661782, 0.9636745911995603, 0.9638906614913689, 0.9644686541657614, 0.9646359063962442, 0.9645111392860315, 0.9644764959456635, 0.9646453169393325, 0.9645854911277523, 0.9646587067208594, 0.9644516595781852], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.04787999999999999, 0.0989453333333333, 0.1491974666666666, 0.19693105333333327, 0.24294461466666661, 0.28746348653333326, 0.32887713787999995, 0.36774942409199995, 0.4039744816827999, 0.4376703668478532, 0.46832999682973453, 0.4970969971467611, 0.5231339640987517, 0.5480605676888766, 0.5705745109199889, 0.5918503931613233, 0.6108786871785243, 0.6283374851273386, 0.644397069947938, 0.659544029619811, 0.6733362933244965, 0.6861493306587134, 0.6982010642595088, 0.7092076245002246, 0.7185535287168687, 0.7276315091785152, 0.7357616915939971, 0.7431455224345973, 0.7502176368578042, 0.7568758731720238, 0.762814952521488, 0.7683467906026725, 0.773538778209072, 0.7781449003881649, 0.782197077016015, 0.7863907026477469, 0.7904982990496389, 0.7936484691446749, 0.7967102888968741, 0.79961259334052, 0.8029046673398014, 0.8056942006058212, 0.8080714472119057, 0.8106243024907152, 0.8127485389083103, 0.814820351684146, 0.8167649831823981, 0.8184884848641583, 0.8201596363777424, 0.8216370060733015, 0.822819972132638, 0.8241513082527075, 0.8252428440941034, 0.8261985596846931, 0.8263787037162238, 0.8276741666779347, 0.828253416676808, 0.8291614083424605, 0.8297786008415479, 0.8305874074240598, 0.8317420000149871, 0.8327011333468217, 0.8338310200121395, 0.8344479180109255, 0.8349631262098329, 0.8354534802555164, 0.835801465563298, 0.8366613190069682, 0.837341853772938, 0.8378210017289776, 0.8385055682227465, 0.8390283447338052, 0.8397521769270913, 0.8398436259010488, 0.8402592633109438, 0.8412866703131827, 0.8414913366151977, 0.8417022029536779, 0.8420653159916435, 0.8422187843924791, 0.8424102392865646, 0.8430225486912415, 0.843626960488784, 0.843557597773239, 0.8440551713292485, 0.8440629875296569, 0.8443766887766913, 0.8438590198990221, 0.8438997845757866, 0.844349806118208, 0.8444214921730538, 0.8450593429557484, 0.8453800753268401, 0.8456287344608228, 0.8456391943480738, 0.8462486082465998, 0.8462370807552732, 0.8458933726797458, 0.8461440354117713, 0.8465029652039275, 0.8468926686835349, 0.8472300684818481, 0.8472670616336633, 0.8469670221369636, 0.8472169865899339, 0.8472819545976072, 0.8477004258045131, 0.8476370498907284, 0.8477933449016556, 0.8479606770781567, 0.847524609370341, 0.8479721484333069, 0.8479482669233096, 0.8480601068976453, 0.8485607628745474, 0.8486646865870926, 0.84922488459505, 0.8494490628022116, 0.8499441565219904, 0.8501630742031248, 0.8498267667828123, 0.8502307567711977, 0.8503543477607447, 0.8500389129846703, 0.8501550216862033, 0.8501261861842496, 0.8498735675658247, 0.8497262108092423, 0.8499135897283181, 0.8501355640888195, 0.8504686743466042, 0.8508618069119438, 0.8510556262207495, 0.8507900635986746, 0.8507377239054739, 0.8507572848482599, 0.8508815563634339, 0.8512334007270904, 0.8513633939877147, 0.8512003879222766, 0.8511336824633823, 0.8512736475503774, 0.8516262827953397, 0.8517836545158057, 0.8521519557308919, 0.8527767601578027, 0.852885750808689, 0.8531171757278201, 0.8534721248217048, 0.8534582456728677, 0.8531524211055809, 0.8529171789950227, 0.8528521277621871, 0.8530202483193017, 0.8531315568207049, 0.8534717344719678, 0.8536578943581044, 0.854012104922294, 0.8538108944300645, 0.854069804987058, 0.8541294911550189, 0.854396542039517, 0.8540502211688986, 0.8540451990520088, 0.8540673458134747, 0.8539272778987939, 0.8539078834422478, 0.853557095098023, 0.8532413855882207, 0.8532505803627319, 0.8533255223264586, 0.8539796367604795, 0.8544216730844316, 0.8544461724426552, 0.8544282218650563, 0.8543853996785506, 0.8543068597106956, 0.8545828404062927, 0.8548845563656634, 0.8552094340624303, 0.8549684906561873, 0.8551383082572352, 0.8552644774315117, 0.8553380296883606, 0.8553908933861911, 0.855491804047572, 0.8552492903094814, 0.8550576946118666, 0.8552452584840133, 0.8549473993022787, 0.8552659927053842, 0.855192726768179, 0.8551267874246944, 0.855094108682225, 0.8548380311473358, 0.8550475613659355, 0.8553694718960085, 0.8554991913730744, 0.8558826055691002, 0.8561343450121902, 0.8566809105109712, 0.8567061527932074, 0.85676887084722, 0.8569986504291647, 0.8572987853862482, 0.85739557351429, 0.8568560161628609, 0.8567170812132415, 0.8566453730919174, 0.8567941691160589, 0.857194752204453, 0.8568352769840076, 0.8568984159522736, 0.8570885743570462, 0.8566063835880083, 0.8569457452292075, 0.8570778373729534, 0.8572767203023247, 0.8574023816054256, 0.8572354767782163, 0.857485262433728, 0.8572834028570219, 0.8570083959046529, 0.856960889647521, 0.8566781340161023, 0.8565436539478254, 0.8566359552197095, 0.8565723596977386, 0.8565951237279648, 0.8565222780218349, 0.8569500502196513, 0.8573883785310195, 0.8573695406779176, 0.8575659199434592, 0.8576493279491133, 0.857684395154202, 0.8578226223054485, 0.8578803600749036, 0.8576923240674134, 0.857683091660672, 0.8579814491612715, 0.858089970911811, 0.8582543071539633, 0.8584155431052337, 0.858400655461377, 0.8584939232485727, 0.8585511975903821, 0.8586294111646773, 0.8588464700482095, 0.8586818230433886, 0.8583736407390498], "moving_var_accuracy_train": [0.020909159999999996, 0.0425904419584775, 0.06154478811591693, 0.07663524087484772, 0.08881281060939619, 0.09848667739982185, 0.10508055652946188, 0.10926559093286348, 0.11108983034683867, 0.11098247973547068, 0.10926536893762542, 0.10661020963000045, 0.10297646868119457, 0.09891816536642065, 0.09432293120171832, 0.08952118141833837, 0.08437882733404405, 0.0791704349043541, 0.07391715140556156, 0.0688542247609947, 0.0640026005704573, 0.05933502564030158, 0.05494587751122952, 0.05078802491627829, 0.046743750049616416, 0.04300699663842715, 0.039538230188608164, 0.03626231037087283, 0.03322971704195004, 0.030481897540968382, 0.027862270033557752, 0.02544481111376675, 0.023236234867535063, 0.02119765288148981, 0.019312864992832194, 0.0176035066037624, 0.016078738629041224, 0.014640285856212088, 0.013321371381206199, 0.01212586129434008, 0.011060566212386453, 0.010064446284544188, 0.0091685997538664, 0.008348546178955345, 0.007592229808758088, 0.00689843719344383, 0.006274184634821879, 0.005703557650040434, 0.005182021686101751, 0.004700402721900364, 0.004271821351315954, 0.0038915170313060595, 0.003543532712509242, 0.003219732747000182, 0.002913038750081519, 0.002663668679270652, 0.0024167432517667544, 0.0022014894014154712, 0.00199416667741334, 0.0018168925547083095, 0.0016644367057319566, 0.0015166167922856996, 0.001388969456643688, 0.0012594061761051773, 0.001137846725319721, 0.0010396807253095993, 0.000944083910292033, 0.0008598507493566815, 0.0007851414093188567, 0.0007150087778586536, 0.000650370691237903, 0.0005945899996172862, 0.0005471878963397114, 0.0004992800567451988, 0.00047097434826755285, 0.0004332566002321314, 0.0003990791452692234, 0.0003646513409188918, 0.0003387054465963769, 0.0003068185368360295, 0.00027929018239403224, 0.00026276564466432576, 0.00023843410359819772, 0.00021642401029761625, 0.00020355582200545054, 0.00019127940579819538, 0.000176768535369419, 0.00016137278994221249, 0.00014804766371709233, 0.0001382756079048032, 0.00012715379469018453, 0.00012018916053729549, 0.00011091618372675201, 0.00010253609712012497, 9.955156821170613e-05, 9.708812340885055e-05, 8.943222989586879e-05, 8.05975909551335e-05, 7.552903677250117e-05, 7.21814078837102e-05, 6.756396460243844e-05, 6.555325552196089e-05, 6.265062348665424e-05, 5.647631774488438e-05, 5.361351130346482e-05, 5.24939009933877e-05, 5.1836983847328256e-05, 4.7373585690282686e-05, 4.4534979314867844e-05, 4.21487617699444e-05, 3.839961946584249e-05, 3.790179237027363e-05, 3.56391208544072e-05, 3.3088148553467615e-05, 3.65389272420807e-05, 3.530741622188492e-05, 4.066862099724103e-05, 3.7358007181793894e-05, 3.7637578418487775e-05, 3.831320546108963e-05, 3.45441831013553e-05, 3.480812394115397e-05, 3.542747496513308e-05, 3.3782761450772636e-05, 3.247109121698259e-05, 2.9603641947606713e-05, 2.668944380429322e-05, 2.4024188901097715e-05, 2.2266422041297736e-05, 2.3495968015119883e-05, 2.34289525598015e-05, 2.3876051288635824e-05, 2.149558154813421e-05, 2.0090446239721247e-05, 1.914341295064176e-05, 1.783630239489963e-05, 1.972159308075647e-05, 1.9367758097308984e-05, 1.8237137682876566e-05, 1.658968963850361e-05, 1.703553342767048e-05, 1.724567773598373e-05, 1.732704730197043e-05, 1.6190047840977696e-05, 1.5093588145478152e-05, 1.7221092339479247e-05, 1.5500155328087805e-05, 1.4067997215268899e-05, 1.5111812112021015e-05, 1.3691029873991132e-05, 1.2536777537085757e-05, 1.1322440258509668e-05, 1.026486442089344e-05, 1.2128537683073778e-05, 1.1372558016174223e-05, 1.0605370236697145e-05, 1.0281146826961334e-05, 1.001185082868939e-05, 1.0271379998205215e-05, 1.095250244789628e-05, 1.0059073379629641e-05, 1.149290902973576e-05, 1.0479133288431982e-05, 1.0196566503699933e-05, 9.741612127161653e-06, 8.979189410667093e-06, 8.782952912470592e-06, 8.30109284789857e-06, 8.396354528462193e-06, 8.23124124778583e-06, 8.843290499006227e-06, 1.1692768482569976e-05, 1.2749605485037032e-05, 1.1477651487334614e-05, 1.0464664730467878e-05, 9.802376769635902e-06, 9.601024949606098e-06, 8.948160514729771e-06, 9.752632468480026e-06, 8.79977370172328e-06, 7.928326737044834e-06, 7.886582502590407e-06, 8.830988067131054e-06, 8.289895428889598e-06, 8.037401097558492e-06, 7.333698272191231e-06, 6.685427174073001e-06, 6.179875626328559e-06, 8.813957424661596e-06, 8.184255889721704e-06, 7.3946128681439515e-06, 6.6724470826817575e-06, 6.3978254478388214e-06, 5.767961595773953e-06, 5.244438618927405e-06, 7.5559714975058815e-06, 6.94413792415001e-06, 6.431158718865293e-06, 8.689465366553497e-06, 7.82674451004128e-06, 9.435134775736216e-06, 8.570731330649567e-06, 7.918635147222607e-06, 7.329111790187349e-06, 8.800950811339981e-06, 8.032413849340648e-06, 7.490458902093629e-06, 7.075036381226068e-06, 6.445843870439618e-06, 6.5030669220479545e-06, 6.4319183014339015e-06, 5.795938203971989e-06, 5.2211567060978615e-06, 4.703870262284762e-06, 4.312650570724774e-06, 4.562002130829135e-06, 4.110600150668718e-06, 6.385317408731578e-06, 7.324641156821395e-06, 6.602162291681531e-06, 7.42874918813282e-06, 6.69133273341131e-06, 6.107682969177393e-06, 5.712137140470886e-06, 5.256442943349009e-06, 5.013584163962873e-06, 4.551846183261399e-06, 4.110028285154729e-06, 5.185833044114805e-06, 4.989874549480673e-06, 6.5527146399154235e-06, 7.716668089919743e-06, 7.210313100922629e-06, 7.546645124104563e-06, 6.80053915362365e-06, 6.58733103624133e-06, 6.47915281761862e-06, 5.904694285811222e-06, 6.1897422381163505e-06, 5.653303554335995e-06, 5.964458716375427e-06, 5.379778498893836e-06, 5.2619779880250145e-06, 7.742459974085325e-06, 7.219973754089677e-06, 6.638077864798052e-06, 5.985071527604833e-06, 5.643069125974651e-06, 5.110974362958381e-06, 4.648121634328793e-06, 4.569126144501786e-06], "duration": 263680.45326, "accuracy_train": [0.482, 0.5621411764705883, 0.6074588235294117, 0.636235294117647, 0.6684941176470588, 0.6999764705882353, 0.7187529411764706, 0.7381176470588235, 0.7508705882352941, 0.7617411764705883, 0.7699294117647059, 0.7825176470588235, 0.7891058823529412, 0.8009176470588235, 0.8065411764705882, 0.8150352941176471, 0.816635294117647, 0.8208941176470588, 0.8224470588235294, 0.8284705882352941, 0.8340235294117647, 0.8374823529411765, 0.8436, 0.8475764705882353, 0.8451058823529412, 0.8506823529411764, 0.854964705882353, 0.8552235294117647, 0.8583294117647059, 0.8651764705882353, 0.8622352941176471, 0.8641411764705882, 0.8676235294117647, 0.8689176470588236, 0.8693647058823529, 0.873035294117647, 0.8795058823529411, 0.8768470588235294, 0.8779529411764706, 0.8807764705882353, 0.886164705882353, 0.8847058823529412, 0.8883058823529412, 0.8895529411764705, 0.8895764705882353, 0.8899529411764706, 0.8926823529411765, 0.8935058823529411, 0.8941882352941176, 0.8933882352941176, 0.8967058823529411, 0.9002117647058824, 0.9010588235294118, 0.900235294117647, 0.8966823529411765, 0.9065411764705882, 0.9018117647058823, 0.9057176470588235, 0.902235294117647, 0.9071764705882353, 0.9110823529411765, 0.9092470588235294, 0.9126352941176471, 0.9081176470588235, 0.9059294117647059, 0.9128235294117647, 0.9106117647058823, 0.912564705882353, 0.9141882352941176, 0.9137647058823529, 0.9138117647058823, 0.9160941176470588, 0.9185411764705882, 0.9168235294117647, 0.9244941176470588, 0.9207529411764706, 0.9216470588235294, 0.9203764705882352, 0.9241647058823529, 0.9191294117647059, 0.9208235294117647, 0.9267529411764706, 0.9212705882352941, 0.9216, 0.9274117647058824, 0.928, 0.9266352941176471, 0.9252235294117647, 0.9262823529411764, 0.9287294117647059, 0.9274823529411764, 0.9305411764705882, 0.9288705882352941, 0.9293882352941176, 0.933435294117647, 0.9344705882352942, 0.9310352941176471, 0.9278352941176471, 0.9326117647058824, 0.9342588235294118, 0.9334823529411764, 0.9359058823529411, 0.9357411764705882, 0.9310117647058823, 0.9356705882352941, 0.9375294117647058, 0.9384941176470588, 0.9348941176470589, 0.9369411764705883, 0.9376, 0.9310117647058823, 0.9391529411764706, 0.9377882352941177, 0.937435294117647, 0.9430823529411765, 0.9404705882352942, 0.9457411764705882, 0.9396941176470588, 0.943764705882353, 0.9447764705882353, 0.9376235294117647, 0.9448, 0.945764705882353, 0.9442823529411765, 0.9449411764705883, 0.9426823529411764, 0.9401176470588235, 0.9409647058823529, 0.9434588235294118, 0.9472470588235294, 0.9467058823529412, 0.9477411764705882, 0.9430117647058823, 0.9398823529411765, 0.9459058823529412, 0.9454117647058824, 0.9494588235294118, 0.9479529411764706, 0.9471294117647059, 0.9458352941176471, 0.9494117647058824, 0.9496705882352942, 0.95, 0.9485411764705882, 0.948635294117647, 0.9528235294117647, 0.9469882352941177, 0.9482352941176471, 0.9524235294117647, 0.9487294117647059, 0.9462823529411765, 0.9470117647058823, 0.9485176470588236, 0.9533647058823529, 0.9505176470588236, 0.9505176470588236, 0.9515529411764706, 0.9518823529411765, 0.9530117647058823, 0.954, 0.9515764705882352, 0.955435294117647, 0.9519764705882353, 0.9537882352941176, 0.9486588235294118, 0.9524470588235294, 0.9538588235294118, 0.9492470588235294, 0.9479294117647059, 0.9535529411764706, 0.9550823529411765, 0.9579294117647059, 0.9571058823529411, 0.9524470588235294, 0.953835294117647, 0.9548, 0.9558823529411765, 0.9550823529411765, 0.957764705882353, 0.9543529411764706, 0.9542117647058823, 0.9568235294117647, 0.9586117647058824, 0.9566117647058824, 0.9573882352941177, 0.9561647058823529, 0.9561882352941177, 0.9566588235294118, 0.9614588235294118, 0.9543764705882353, 0.9564470588235294, 0.9563764705882353, 0.9580705882352941, 0.9558588235294118, 0.9553882352941176, 0.9616941176470588, 0.9579058823529412, 0.9581882352941177, 0.9625882352941176, 0.9577411764705882, 0.9626588235294118, 0.9570823529411765, 0.959435294117647, 0.9595764705882353, 0.9631764705882353, 0.959835294117647, 0.9571294117647059, 0.9605882352941176, 0.9597882352941176, 0.9617411764705882, 0.961764705882353, 0.959764705882353, 0.9597411764705882, 0.959764705882353, 0.9604941176470588, 0.9624, 0.9596941176470588, 0.9653647058823529, 0.964635294117647, 0.9612, 0.964964705882353, 0.9615529411764706, 0.9623058823529411, 0.9598823529411765, 0.9601411764705883, 0.9593882352941177, 0.9616470588235294, 0.961435294117647, 0.9651529411764705, 0.9633882352941177, 0.9664705882352941, 0.9666588235294118, 0.9643294117647059, 0.9662117647058823, 0.9634352941176471, 0.9654352941176471, 0.9658588235294118, 0.9627294117647058, 0.9604235294117647, 0.9641882352941177, 0.9664470588235294, 0.964, 0.965835294117647, 0.9696705882352942, 0.9661411764705883, 0.9633882352941177, 0.9641647058823529, 0.9661647058823529, 0.9640470588235294, 0.9653176470588235, 0.9625882352941176], "end": "2016-02-07 09:49:57.881000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0, 211.0, 212.0, 213.0, 214.0, 215.0, 216.0, 217.0, 218.0, 219.0, 220.0, 221.0, 222.0, 223.0, 224.0, 225.0, 226.0, 227.0, 228.0, 229.0, 230.0, 231.0, 232.0, 233.0, 234.0, 235.0, 236.0, 237.0, 238.0, 239.0, 240.0, 241.0, 242.0, 243.0, 244.0, 245.0, 246.0, 247.0, 248.0, 249.0, 250.0], "moving_var_accuracy_valid": [0.020632449599999998, 0.04203821905599999, 0.060561889291359985, 0.0750121580268016, 0.0865661726633093, 0.09574692496750453, 0.10160804713152347, 0.10504673413649994, 0.10635235389996094, 0.105935832603473, 0.10380236553875859, 0.10086999175004097, 0.0968843054075683, 0.09278789496566449, 0.08807100422742178, 0.08333787229100378, 0.07826276882074884, 0.07317977857103164, 0.06818299309541875, 0.06342956727158897, 0.05879864938732635, 0.054396349780146076, 0.050263913347192324, 0.04632782132746564, 0.04248115252535927, 0.03897472483618166, 0.03567215114754458, 0.03259562465373353, 0.02978619541009456, 0.027206564866429035, 0.024803362351459232, 0.0225984372093213, 0.020581204106134215, 0.018714030949279366, 0.016990409073161086, 0.015449646629297042, 0.014056533100174863, 0.012740191934806315, 0.011550545403080297, 0.010471301202524728, 0.009521710843222952, 0.00863957322148074, 0.00782647761216793, 0.007102483481622043, 0.006432846556680376, 0.00582819357461538, 0.005279408542129789, 0.004778201810340082, 0.004325516355738266, 0.003912608311120646, 0.003533942158286461, 0.003196500045239231, 0.0028875730951527746, 0.0026070363162483626, 0.0023466247514723915, 0.002127066294891637, 0.0019173794404532248, 0.0017330615361919525, 0.0015631837218011238, 0.0014127528624122423, 0.001283475332630237, 0.0011634072301013392, 0.0010585563019793727, 0.0009561257400495915, 0.0008629021214386256, 0.0007787759331058255, 0.0007019881837651302, 0.0006384434968899408, 0.0005787672953101895, 0.0005229568106531623, 0.0004748788111473656, 0.0004298505875572613, 0.0003915809261978721, 0.0003524980998116259, 0.00031880307993900224, 0.0002964228582793488, 0.00026715756710803866, 0.00024084199191157132, 0.0002179444524254793, 0.00019636198013342685, 0.0001770556769083075, 0.0001627244144809785, 0.0001497397956219578, 0.0001348091167365321, 0.00012355642005563774, 0.0001112013278869734, 0.00010096687134979394, 9.328201381697951e-05, 8.396876826512695e-05, 7.73945659364038e-05, 6.97013593568977e-05, 6.63929060100652e-05, 6.067943869385436e-05, 5.516797710868596e-05, 4.965216408098911e-05, 4.802941537033963e-05, 4.3227669780812245e-05, 3.996811997337524e-05, 3.653679422307573e-05, 3.404259016204381e-05, 3.200515036400139e-05, 2.982918294271717e-05, 2.6858581087976452e-05, 2.498293627539685e-05, 2.3046982697595873e-05, 2.0780272006025594e-05, 2.0278308164506945e-05, 1.82866259060886e-05, 1.6677816489446213e-05, 1.5262035356134985e-05, 1.544722723271798e-05, 1.570512542536971e-05, 1.4139745821510524e-05, 1.2838344858094167e-05, 1.3810418037155224e-05, 1.2526577475702421e-05, 1.409831600120717e-05, 1.3140787218182474e-05, 1.403276861864435e-05, 1.306081631679883e-05, 1.2772658813734055e-05, 1.296426412880162e-05, 1.1805310310196079e-05, 1.152027116079087e-05, 1.0489575119856828e-05, 9.448100983427422e-06, 9.0776363824591e-06, 8.365298867607727e-06, 7.844766714672999e-06, 7.50374359368625e-06, 7.752031228890245e-06, 8.367807031375553e-06, 7.869119648430981e-06, 7.716919239777774e-06, 6.969882307159184e-06, 6.27633775078734e-06, 5.787694661061408e-06, 6.323075301087663e-06, 5.8428520012485265e-06, 5.497705597450268e-06, 4.987981601921962e-06, 4.6654954719277025e-06, 5.318110468641268e-06, 5.0091921473990735e-06, 5.729084997964217e-06, 8.669601645154007e-06, 7.909552138464381e-06, 7.600614363371453e-06, 7.974452660280173e-06, 7.178741071204148e-06, 7.302624957688938e-06, 7.07041211713844e-06, 6.401455871465491e-06, 6.015690979839742e-06, 5.525628124217327e-06, 6.014552821563961e-06, 5.72499706826492e-06, 6.281683475489796e-06, 6.017886087589579e-06, 6.01940956753505e-06, 5.449530558594221e-06, 5.546423076935814e-06, 6.071224078075001e-06, 5.46432866519e-06, 4.922310110061799e-06, 4.606650285562862e-06, 4.149370561509026e-06, 4.84190566735375e-06, 5.254767551834836e-06, 4.730051691556171e-06, 4.307593203745629e-06, 7.727625118520223e-06, 8.713427611905587e-06, 7.8474868176953e-06, 7.065638145050962e-06, 6.375577987459973e-06, 5.793536927670048e-06, 5.8996713339833795e-06, 6.128996881836056e-06, 6.46600685436209e-06, 6.341889694033836e-06, 5.9672428832615435e-06, 5.513786539773875e-06, 5.0110972961844886e-06, 4.535138701500976e-06, 4.173271485573846e-06, 4.28526055548024e-06, 4.187114702032743e-06, 4.085025087041328e-06, 4.475003407630041e-06, 4.9410188753878985e-06, 4.495228065839954e-06, 4.084837232428535e-06, 3.6859646110701386e-06, 3.907549484837318e-06, 3.911920748911733e-06, 4.453366178368148e-06, 4.159473845103384e-06, 5.066584472020673e-06, 5.1302807496838954e-06, 7.305857274834851e-06, 6.581006102663801e-06, 5.958307481089556e-06, 5.837664639488586e-06, 6.06462710771154e-06, 5.542475872508994e-06, 7.608327504587736e-06, 7.021221036160723e-06, 6.365377424519206e-06, 5.928101993270411e-06, 6.7794930903096754e-06, 7.264545688306426e-06, 6.5739698832989325e-06, 6.242014865120129e-06, 7.710384818316927e-06, 7.975843248141744e-06, 7.3352939332820855e-06, 6.957754316311753e-06, 6.404095752553645e-06, 6.014401169409943e-06, 5.974496915763648e-06, 5.743772822559086e-06, 5.850054954963984e-06, 5.285361059667777e-06, 5.476381677592266e-06, 5.091507508706876e-06, 4.659032480959049e-06, 4.229528746595966e-06, 3.8112396815855995e-06, 3.4778741855409902e-06, 4.776988246009848e-06, 6.028474798330941e-06, 5.428821100883265e-06, 5.233022334206743e-06, 4.772332159450794e-06, 4.306166323360289e-06, 4.047510399099671e-06, 3.6727622093846155e-06, 3.6237038494624224e-06, 3.262100600524317e-06, 3.7370453239475276e-06, 3.4693335246142162e-06, 3.3654577765152327e-06, 3.2628852867021453e-06, 2.9385915354883687e-06, 2.72302230309485e-06, 2.4802432248526603e-06, 2.28727517120355e-06, 2.482578684365705e-06, 2.478298541697736e-06, 3.085255681896435e-06], "accuracy_test": 0.6881, "start": "2016-02-04 08:35:17.428000", "learning_rate_per_epoch": [0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853, 0.0001621346891624853], "accuracy_train_first": 0.482, "accuracy_train_last": 0.9625882352941176, "batch_size_eval": 1024, "accuracy_train_std": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "accuracy_test_std": 0, "error_valid": [0.5212, 0.4414666666666667, 0.3985333333333333, 0.3734666666666666, 0.3429333333333333, 0.3118666666666666, 0.2984, 0.2824, 0.27, 0.25906666666666667, 0.25573333333333337, 0.244, 0.24253333333333338, 0.22760000000000002, 0.2268, 0.21666666666666667, 0.21786666666666665, 0.21453333333333335, 0.21106666666666662, 0.2041333333333334, 0.20253333333333334, 0.19853333333333334, 0.19333333333333336, 0.1917333333333333, 0.19733333333333336, 0.19066666666666665, 0.19106666666666672, 0.1904, 0.18613333333333337, 0.18320000000000003, 0.1837333333333333, 0.18186666666666662, 0.1797333333333333, 0.1804, 0.18133333333333335, 0.17586666666666662, 0.17253333333333332, 0.17800000000000005, 0.1757333333333333, 0.17426666666666668, 0.16746666666666665, 0.16920000000000002, 0.17053333333333331, 0.1664, 0.16813333333333336, 0.1665333333333333, 0.1657333333333333, 0.16600000000000004, 0.16479999999999995, 0.1650666666666667, 0.1665333333333333, 0.16386666666666672, 0.16493333333333338, 0.1652, 0.17200000000000004, 0.16066666666666662, 0.1665333333333333, 0.16266666666666663, 0.16466666666666663, 0.16213333333333335, 0.1578666666666667, 0.15866666666666662, 0.15600000000000003, 0.16000000000000003, 0.1604, 0.16013333333333335, 0.1610666666666667, 0.15559999999999996, 0.1565333333333333, 0.1578666666666667, 0.15533333333333332, 0.15626666666666666, 0.1537333333333334, 0.15933333333333333, 0.15600000000000003, 0.14946666666666664, 0.15666666666666662, 0.15639999999999998, 0.15466666666666662, 0.15639999999999998, 0.1558666666666667, 0.15146666666666664, 0.15093333333333336, 0.1570666666666667, 0.15146666666666664, 0.1558666666666667, 0.15280000000000005, 0.16080000000000005, 0.15573333333333328, 0.15159999999999996, 0.15493333333333337, 0.1492, 0.1517333333333334, 0.15213333333333334, 0.15426666666666666, 0.14826666666666666, 0.1538666666666667, 0.1572, 0.15159999999999996, 0.15026666666666666, 0.14959999999999996, 0.14973333333333338, 0.15239999999999998, 0.15573333333333328, 0.1505333333333333, 0.15213333333333334, 0.1485333333333333, 0.15293333333333337, 0.15080000000000005, 0.1505333333333333, 0.15639999999999998, 0.14800000000000002, 0.15226666666666666, 0.15093333333333336, 0.14693333333333336, 0.15039999999999998, 0.14573333333333338, 0.1485333333333333, 0.14559999999999995, 0.1478666666666667, 0.1532, 0.14613333333333334, 0.1485333333333333, 0.15280000000000005, 0.14880000000000004, 0.15013333333333334, 0.15239999999999998, 0.15159999999999996, 0.14839999999999998, 0.1478666666666667, 0.1465333333333333, 0.14559999999999995, 0.1472, 0.15159999999999996, 0.14973333333333338, 0.14906666666666668, 0.14800000000000002, 0.14559999999999995, 0.14746666666666663, 0.15026666666666666, 0.14946666666666664, 0.14746666666666663, 0.1452, 0.14680000000000004, 0.1445333333333333, 0.14159999999999995, 0.14613333333333334, 0.14480000000000004, 0.1433333333333333, 0.1466666666666666, 0.14959999999999996, 0.1492, 0.14773333333333338, 0.14546666666666663, 0.1458666666666667, 0.14346666666666663, 0.14466666666666672, 0.14280000000000004, 0.14800000000000002, 0.14359999999999995, 0.14533333333333331, 0.1432, 0.14906666666666668, 0.14600000000000002, 0.14573333333333338, 0.14733333333333332, 0.14626666666666666, 0.14959999999999996, 0.14959999999999996, 0.1466666666666666, 0.14600000000000002, 0.14013333333333333, 0.14159999999999995, 0.14533333333333331, 0.14573333333333338, 0.14600000000000002, 0.14639999999999997, 0.14293333333333336, 0.14239999999999997, 0.1418666666666667, 0.1472, 0.1433333333333333, 0.14359999999999995, 0.14400000000000002, 0.14413333333333334, 0.14359999999999995, 0.14693333333333336, 0.1466666666666666, 0.14306666666666668, 0.14773333333333338, 0.1418666666666667, 0.14546666666666663, 0.14546666666666663, 0.1452, 0.14746666666666663, 0.14306666666666668, 0.14173333333333338, 0.1433333333333333, 0.14066666666666672, 0.14159999999999995, 0.13839999999999997, 0.14306666666666668, 0.14266666666666672, 0.14093333333333335, 0.14, 0.14173333333333338, 0.14800000000000002, 0.1445333333333333, 0.14400000000000002, 0.1418666666666667, 0.1392, 0.14639999999999997, 0.1425333333333333, 0.1412, 0.14773333333333338, 0.14, 0.14173333333333338, 0.14093333333333335, 0.14146666666666663, 0.14426666666666665, 0.14026666666666665, 0.1445333333333333, 0.14546666666666663, 0.14346666666666663, 0.1458666666666667, 0.14466666666666672, 0.1425333333333333, 0.14400000000000002, 0.1432, 0.14413333333333334, 0.1392, 0.13866666666666672, 0.14280000000000004, 0.14066666666666672, 0.14159999999999995, 0.14200000000000002, 0.14093333333333335, 0.14159999999999995, 0.14400000000000002, 0.14239999999999997, 0.1393333333333333, 0.14093333333333335, 0.14026666666666665, 0.14013333333333333, 0.14173333333333338, 0.14066666666666672, 0.14093333333333335, 0.14066666666666672, 0.1392, 0.14280000000000004, 0.14439999999999997], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.05279553695111855, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 16, "valid_ratio": 0.15, "learning_rate": 0.00016213468958693037, "optimization": "adam", "nb_data_augmentation": 4, "learning_rate_decay_method": "none", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 4.5223587693473477e-08, "rotation_range": [0, 0], "momentum": 0.8840911036531354}, "accuracy_valid_max": 0.8616, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        y_pred = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            y_pred.extend((nnet.predict(X[mini_batch]) == y[mini_batch]).tolist())\n        return np.mean(y_pred)\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            acc = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = acc\n            status[\"accuracy_train_std\"] = 0\n            acc = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = acc\n            status[\"accuracy_valid_std\"] = 0\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n\n    # rescaling to [-1, 1]\n    X_min = X_train.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X_train.max(axis=(0, 2, 3))[None, :, None, None]\n    def preprocess(a):\n        return (a / 255.) * 2 - 1\n        # return 2 * ((a - X_min) / (X_max - X_min)) - 1\n    X_train = preprocess(X_train)\n    X_valid = preprocess(X_valid)\n\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = preprocess(X_test)\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    acc = evaluate(X_test, y_test, batch_size_eval)\n    light.set(\"accuracy_test\", acc)\n    light.set(\"accuracy_test_std\", 0)\n    print(\"Test accuracy : {}+-{}\".format(acc, 0))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8556, "accuracy_valid_std": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "accuracy_valid": [0.4788, 0.5585333333333333, 0.6014666666666667, 0.6265333333333334, 0.6570666666666667, 0.6881333333333334, 0.7016, 0.7176, 0.73, 0.7409333333333333, 0.7442666666666666, 0.756, 0.7574666666666666, 0.7724, 0.7732, 0.7833333333333333, 0.7821333333333333, 0.7854666666666666, 0.7889333333333334, 0.7958666666666666, 0.7974666666666667, 0.8014666666666667, 0.8066666666666666, 0.8082666666666667, 0.8026666666666666, 0.8093333333333333, 0.8089333333333333, 0.8096, 0.8138666666666666, 0.8168, 0.8162666666666667, 0.8181333333333334, 0.8202666666666667, 0.8196, 0.8186666666666667, 0.8241333333333334, 0.8274666666666667, 0.822, 0.8242666666666667, 0.8257333333333333, 0.8325333333333333, 0.8308, 0.8294666666666667, 0.8336, 0.8318666666666666, 0.8334666666666667, 0.8342666666666667, 0.834, 0.8352, 0.8349333333333333, 0.8334666666666667, 0.8361333333333333, 0.8350666666666666, 0.8348, 0.828, 0.8393333333333334, 0.8334666666666667, 0.8373333333333334, 0.8353333333333334, 0.8378666666666666, 0.8421333333333333, 0.8413333333333334, 0.844, 0.84, 0.8396, 0.8398666666666667, 0.8389333333333333, 0.8444, 0.8434666666666667, 0.8421333333333333, 0.8446666666666667, 0.8437333333333333, 0.8462666666666666, 0.8406666666666667, 0.844, 0.8505333333333334, 0.8433333333333334, 0.8436, 0.8453333333333334, 0.8436, 0.8441333333333333, 0.8485333333333334, 0.8490666666666666, 0.8429333333333333, 0.8485333333333334, 0.8441333333333333, 0.8472, 0.8392, 0.8442666666666667, 0.8484, 0.8450666666666666, 0.8508, 0.8482666666666666, 0.8478666666666667, 0.8457333333333333, 0.8517333333333333, 0.8461333333333333, 0.8428, 0.8484, 0.8497333333333333, 0.8504, 0.8502666666666666, 0.8476, 0.8442666666666667, 0.8494666666666667, 0.8478666666666667, 0.8514666666666667, 0.8470666666666666, 0.8492, 0.8494666666666667, 0.8436, 0.852, 0.8477333333333333, 0.8490666666666666, 0.8530666666666666, 0.8496, 0.8542666666666666, 0.8514666666666667, 0.8544, 0.8521333333333333, 0.8468, 0.8538666666666667, 0.8514666666666667, 0.8472, 0.8512, 0.8498666666666667, 0.8476, 0.8484, 0.8516, 0.8521333333333333, 0.8534666666666667, 0.8544, 0.8528, 0.8484, 0.8502666666666666, 0.8509333333333333, 0.852, 0.8544, 0.8525333333333334, 0.8497333333333333, 0.8505333333333334, 0.8525333333333334, 0.8548, 0.8532, 0.8554666666666667, 0.8584, 0.8538666666666667, 0.8552, 0.8566666666666667, 0.8533333333333334, 0.8504, 0.8508, 0.8522666666666666, 0.8545333333333334, 0.8541333333333333, 0.8565333333333334, 0.8553333333333333, 0.8572, 0.852, 0.8564, 0.8546666666666667, 0.8568, 0.8509333333333333, 0.854, 0.8542666666666666, 0.8526666666666667, 0.8537333333333333, 0.8504, 0.8504, 0.8533333333333334, 0.854, 0.8598666666666667, 0.8584, 0.8546666666666667, 0.8542666666666666, 0.854, 0.8536, 0.8570666666666666, 0.8576, 0.8581333333333333, 0.8528, 0.8566666666666667, 0.8564, 0.856, 0.8558666666666667, 0.8564, 0.8530666666666666, 0.8533333333333334, 0.8569333333333333, 0.8522666666666666, 0.8581333333333333, 0.8545333333333334, 0.8545333333333334, 0.8548, 0.8525333333333334, 0.8569333333333333, 0.8582666666666666, 0.8566666666666667, 0.8593333333333333, 0.8584, 0.8616, 0.8569333333333333, 0.8573333333333333, 0.8590666666666666, 0.86, 0.8582666666666666, 0.852, 0.8554666666666667, 0.856, 0.8581333333333333, 0.8608, 0.8536, 0.8574666666666667, 0.8588, 0.8522666666666666, 0.86, 0.8582666666666666, 0.8590666666666666, 0.8585333333333334, 0.8557333333333333, 0.8597333333333333, 0.8554666666666667, 0.8545333333333334, 0.8565333333333334, 0.8541333333333333, 0.8553333333333333, 0.8574666666666667, 0.856, 0.8568, 0.8558666666666667, 0.8608, 0.8613333333333333, 0.8572, 0.8593333333333333, 0.8584, 0.858, 0.8590666666666666, 0.8584, 0.856, 0.8576, 0.8606666666666667, 0.8590666666666666, 0.8597333333333333, 0.8598666666666667, 0.8582666666666666, 0.8593333333333333, 0.8590666666666666, 0.8593333333333333, 0.8608, 0.8572, 0.8556], "seed": 866133414, "model": "residualv3", "loss_std": [0.282298743724823, 0.24243153631687164, 0.24689215421676636, 0.2479914128780365, 0.24740563333034515, 0.2439655214548111, 0.24244330823421478, 0.24061445891857147, 0.2400338500738144, 0.2384403795003891, 0.2367166429758072, 0.2354864925146103, 0.2355765551328659, 0.23252978920936584, 0.22963248193264008, 0.22888118028640747, 0.22890454530715942, 0.22521449625492096, 0.22195672988891602, 0.22237537801265717, 0.22018256783485413, 0.2195781171321869, 0.21801301836967468, 0.2154160737991333, 0.21240867674350739, 0.21139732003211975, 0.20919260382652283, 0.2087751030921936, 0.20536774396896362, 0.20561684668064117, 0.2030421942472458, 0.20244427025318146, 0.20016397535800934, 0.19803017377853394, 0.19688189029693604, 0.1959923952817917, 0.19500523805618286, 0.1925812065601349, 0.1916237324476242, 0.19184410572052002, 0.18805277347564697, 0.1866116225719452, 0.18644240498542786, 0.18408304452896118, 0.1843585968017578, 0.18361471593379974, 0.1824176013469696, 0.1794697791337967, 0.17691877484321594, 0.1768251359462738, 0.17540119588375092, 0.17447954416275024, 0.1747112274169922, 0.17231890559196472, 0.1705743670463562, 0.1724444031715393, 0.17070016264915466, 0.16826777160167694, 0.1671704649925232, 0.16540421545505524, 0.16364970803260803, 0.16520842909812927, 0.16349777579307556, 0.16124506294727325, 0.15984265506267548, 0.16082094609737396, 0.16081452369689941, 0.15828067064285278, 0.15695065259933472, 0.15520289540290833, 0.15378886461257935, 0.1534162312746048, 0.15450157225131989, 0.1494302600622177, 0.1512504518032074, 0.1489628553390503, 0.14887532591819763, 0.14790111780166626, 0.14958330988883972, 0.1462620198726654, 0.14493541419506073, 0.14606556296348572, 0.1455029994249344, 0.14149713516235352, 0.14036032557487488, 0.1409938633441925, 0.13919785618782043, 0.13963210582733154, 0.14055171608924866, 0.13769420981407166, 0.13913685083389282, 0.13628557324409485, 0.13438044488430023, 0.13398927450180054, 0.13382749259471893, 0.13270314037799835, 0.13167092204093933, 0.133879616856575, 0.13226673007011414, 0.12901906669139862, 0.12866997718811035, 0.13055849075317383, 0.12918245792388916, 0.12557722628116608, 0.12586714327335358, 0.12875518202781677, 0.1286240816116333, 0.1257355809211731, 0.12465794384479523, 0.12202280759811401, 0.12136594206094742, 0.12387657165527344, 0.12421411275863647, 0.1192474439740181, 0.12167803943157196, 0.11896359920501709, 0.11880682408809662, 0.1188526451587677, 0.11786500364542007, 0.11493965983390808, 0.11902926117181778, 0.11840479075908661, 0.11598397046327591, 0.11507820338010788, 0.11320000886917114, 0.11380530148744583, 0.1123872920870781, 0.11271124333143234, 0.11149808019399643, 0.11142175644636154, 0.11248288303613663, 0.10926520824432373, 0.11220991611480713, 0.10886538028717041, 0.10989733040332794, 0.10896424204111099, 0.10858635604381561, 0.10848027467727661, 0.10664138942956924, 0.10552892833948135, 0.10624534636735916, 0.10648481547832489, 0.10571451485157013, 0.10434967279434204, 0.10394138097763062, 0.10479550808668137, 0.10257193446159363, 0.10350612550973892, 0.10319677740335464, 0.1026836484670639, 0.10301023721694946, 0.10257339477539062, 0.10073792189359665, 0.10190527141094208, 0.09935569018125534, 0.09785785526037216, 0.09760138392448425, 0.09787365049123764, 0.0980014055967331, 0.09713350236415863, 0.09474257379770279, 0.09681359678506851, 0.09918195009231567, 0.0944155901670456, 0.09708154201507568, 0.09735853970050812, 0.09674094617366791, 0.09324874728918076, 0.09399870038032532, 0.09182767570018768, 0.09649235755205154, 0.09347333014011383, 0.09410606324672699, 0.0923488587141037, 0.0936335027217865, 0.09146064519882202, 0.09080082923173904, 0.09457637369632721, 0.09051775187253952, 0.09152016043663025, 0.09117584675550461, 0.08871223032474518, 0.08850546926259995, 0.09039584547281265, 0.08835902810096741, 0.08770021796226501, 0.09133780002593994, 0.08593069761991501, 0.08670946210622787, 0.08742854744195938, 0.08735314011573792, 0.08702633529901505, 0.08815184235572815, 0.08710572123527527, 0.08579232543706894, 0.0869661495089531, 0.08636906743049622, 0.08586818724870682, 0.08370571583509445, 0.08511114865541458, 0.08392821997404099, 0.0837908685207367, 0.08452294021844864, 0.08294238150119781, 0.08505379408597946, 0.08198229223489761, 0.08265361189842224, 0.08319227397441864, 0.08258112519979477, 0.08199043571949005, 0.08224794268608093, 0.08293371647596359, 0.0822048932313919, 0.08053405582904816, 0.08030819147825241, 0.07996851205825806, 0.07946345955133438, 0.08143161237239838, 0.07892701774835587, 0.08065979182720184, 0.07811872661113739, 0.07988095283508301, 0.08014895766973495, 0.08000271767377853, 0.07989474385976791, 0.07737570255994797, 0.0778387263417244, 0.07977698743343353, 0.07824522256851196, 0.0780634805560112, 0.07496677339076996, 0.07744641602039337, 0.07655603438615799, 0.07654288411140442, 0.07794849574565887, 0.0775546059012413, 0.07463404536247253, 0.07522554695606232, 0.0773744061589241, 0.07467765361070633, 0.07494319975376129, 0.07658786326646805, 0.07556108385324478, 0.07428045570850372, 0.07354555279016495, 0.07468031346797943, 0.07311081886291504, 0.07379447668790817, 0.07272032648324966, 0.07405786216259003, 0.07419511675834656]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:44 2016", "state": "available"}], "summary": "a99bfa663ef7d23a964848b3b77e028b"}