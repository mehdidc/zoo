{"content": {"hp_model": {"f0": 64, "f1": 64, "f2": 64, "f3": 32, "nonlin": "very_leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.016815692457224588, 0.015169627710463516, 0.017660512086954763, 0.020603497814415846, 0.0165653050255215, 0.016745273905433074, 0.017970723679102123, 0.01450222748085138, 0.015381107095801616, 0.01442097067682977, 0.01306318193628474, 0.016525877847916922, 0.01355612144915386, 0.01764901402239823, 0.017758679805176963, 0.0186937767971385, 0.01421500148944147, 0.015789387057449256, 0.01197775527489883, 0.012456392416102868, 0.011165972870326325, 0.012369961390886836, 0.014339140434082274, 0.012523355190570596, 0.01061787032880016, 0.007538867948236155, 0.008817281434784717, 0.012598423358190333, 0.012526248524859696, 0.009166978609328186, 0.016708154888623795, 0.012854254636120192, 0.011561028078016336, 0.01036808166681838, 0.014141314502686719, 0.012618382774430193, 0.010921159436974623, 0.01453196832987679, 0.012431116159901827, 0.013284641928579368, 0.009013403960675369, 0.012634149689235902, 0.015196196263064654, 0.009076651849284424, 0.011384662781580025, 0.010242278976402595, 0.012904055781379497, 0.012764398609466291, 0.00855421442198997, 0.017876335365614594, 0.014964405449877772, 0.010424900211051266, 0.01146413900738898, 0.011571622004376607, 0.013601666880470606, 0.011783076387284055, 0.012357596397813031, 0.00926596965534443, 0.02019027347588823, 0.01551620052109282, 0.009076394473215533, 0.011372490470117892, 0.014899010112151539, 0.010743733725706365, 0.012231720072829418, 0.008921618950892678, 0.012751374749944264, 0.012408655200928015, 0.014308827455438239, 0.01517133234918221, 0.013827567730430878, 0.013655110762706335, 0.01391268611831156, 0.009627775260589843, 0.01436092789168372, 0.007683907093707558, 0.014367945275445312, 0.011354671882610525, 0.012302021452309488, 0.007492985996032645, 0.011072342069746149, 0.011929041246257495, 0.014347145837753742, 0.012521563176240579, 0.005814813341918077, 0.013104068208431217, 0.01080709922406718, 0.008962552616412933, 0.012372721805049142, 0.015265152473568086, 0.012400373784638487, 0.011911563643870107, 0.011472851607022312, 0.015369487838099062, 0.009989849295592228, 0.01025779032534956, 0.0077605812739435925, 0.015206551150998617, 0.015037982000050718, 0.01397497532560483, 0.012983592250873493, 0.012901258159165293, 0.007840574435489358, 0.009578316779337261, 0.009267476030236773, 0.007750543614065333, 0.011801388925167232, 0.010676567831393745, 0.008803037935389228, 0.009493928235497596, 0.014806956938230077, 0.0100534759527429, 0.00915954203866198, 0.009712062269193794, 0.013465787498400945, 0.011127753769296698, 0.009315045210953431, 0.013643532235052245, 0.009640140929770369, 0.008631677456849677, 0.01133425657853885, 0.012911422790119538, 0.01260798086569791, 0.009066528727524595, 0.01447435580056474, 0.011945654306270505, 0.008997150818385171, 0.0160336672075525, 0.0128308256132534, 0.011048123606963414, 0.010841101892181227, 0.012123315982272736, 0.008436973671317955, 0.009898563266609105, 0.012804155394944042, 0.011420477160258541, 0.009116254307038966, 0.011607432363822692, 0.010890801399581, 0.013729823665726124, 0.010748366238376977, 0.013461122578496966, 0.011589744774476848, 0.012646176293112255, 0.011056781760646242, 0.012223636332234307, 0.015086836986058162, 0.01267813732137345, 0.014478620542119819, 0.011719802699892241, 0.009126054564615672, 0.011663872285569879, 0.009788502113586128, 0.012941359596609777, 0.012620791074331348, 0.01160777151459439, 0.009138333659431059, 0.011592346249542449, 0.011159875033418065, 0.008463468005067394, 0.010883846291484728, 0.007510810244558343, 0.011298383312364215, 0.008568656462410179, 0.009201205310847, 0.016217847287154433, 0.006507693905572858, 0.011302895955857904, 0.011682858256031048, 0.009654807693027343, 0.010317652122391812, 0.009641920532922474, 0.010530997708387448, 0.00999314586290967, 0.012201243365844364, 0.006295695365748822, 0.00764814515601001, 0.012972413062459921, 0.01196083886311387, 0.006841846805044154, 0.010697156213126092, 0.008466528709260962, 0.011102314762887927, 0.011988305626178176, 0.012854800841933054, 0.010686088870302025, 0.010513516225238962, 0.013206920242937731, 0.010683921988205896, 0.010386464495349667, 0.011631019321021934, 0.014120652126100017, 0.007559321556387803, 0.012916953745916398, 0.017544169016834674, 0.012541726317108717, 0.010843164752032336, 0.01438243494824676, 0.009905694074424723, 0.009520367974859518, 0.008164038379055234, 0.011922833228766912, 0.01041789920176646, 0.010603195339510893, 0.011026830853175722, 0.012277500602673358, 0.00852851232604924, 0.010469342266769203, 0.010572275171488999, 0.010726790921586762, 0.01302344780984175, 0.012247998261458335, 0.009809991550257092, 0.01070216481322262, 0.016111677546550573, 0.013732670880523467, 0.01169816158339539, 0.010545314427811248, 0.0076811049319264295, 0.009466137871616313, 0.009553178217178356, 0.008497247197183014, 0.012296959296585173, 0.009166359548330732, 0.01011049593331416, 0.012903007079409017, 0.00706332657715179, 0.011072922356437096], "moving_avg_accuracy_train": [0.055731726132798066, 0.11821075653492985, 0.1808322760214793, 0.23970931108544433, 0.29560951265962415, 0.3478743511511829, 0.396126469520976, 0.4404949892240095, 0.481981748924486, 0.5196731471275061, 0.5544696975114239, 0.5868884970997519, 0.6167349514399336, 0.6437896755996502, 0.6688644098207852, 0.6923430208066838, 0.7138737323380493, 0.7334955854389158, 0.7514550532308493, 0.7677974304578289, 0.7828149229002336, 0.7967353140888924, 0.8095517683181538, 0.8213004187173276, 0.8318627225278115, 0.8411806030989506, 0.8505941590450541, 0.8587898108834888, 0.8669517257380617, 0.873760231585721, 0.880334315420043, 0.8863767291507408, 0.8915241497607221, 0.8970495773061062, 0.9016088379505879, 0.9061560957091466, 0.9097556961442305, 0.913565178238233, 0.9175910951227892, 0.9210749113903183, 0.9242871119906275, 0.9274640497856586, 0.9307372063381005, 0.9332064277781646, 0.9357797163980041, 0.9381189276439548, 0.9408077940676822, 0.9423838890264549, 0.9441858798476466, 0.9459984058867376, 0.948171366896901, 0.949754899850068, 0.9515219484805558, 0.9532239714884895, 0.9546394987063349, 0.9562272601452253, 0.95729846091411, 0.9581856314512981, 0.9595559994442727, 0.9607383215594154, 0.9618697326320638, 0.9629415891665226, 0.9636875518641652, 0.9632150171753954, 0.9640570141543305, 0.9643635523222308, 0.9654998138305024, 0.9662040119474983, 0.967051523699177, 0.9675027864328491, 0.9682624176098392, 0.9691250139810075, 0.9701501416376779, 0.9713029823096428, 0.9720987594870395, 0.9720802840205246, 0.972586670387529, 0.9729657242559374, 0.973365145653018, 0.973963898944859, 0.9748261167873148, 0.975441641528849, 0.9758561409164772, 0.976508136124848, 0.9768950050635814, 0.9774989534774889, 0.9776960598773867, 0.9779989776241903, 0.9783739101439326, 0.978471823035501, 0.9782437964974547, 0.978805871720356, 0.9794302138637966, 0.9789857288953755, 0.9791434036987136, 0.979403965708641, 0.9800732382961379, 0.980482704420151, 0.9810672464781359, 0.9815980927743977, 0.9822502766505663, 0.9827859807188523, 0.9830402497969762, 0.9831621711708685, 0.9832207831823807, 0.9835083740225037, 0.983681175272662, 0.9836297942025756, 0.984118227319232, 0.9839486641849463, 0.9844448099307651, 0.9848983165484306, 0.9854365726912067, 0.9859233283685146, 0.9863567581804726, 0.9867771800922149, 0.9869276231318123, 0.9871629672174405, 0.9873515614552295, 0.9875933758823348, 0.9878854857262719, 0.9882761956727015, 0.9884419308661826, 0.9879725308593447, 0.9881639101389048, 0.9879222750024137, 0.9879257285652953, 0.9880567560052311, 0.9882792042511458, 0.9885120318534398, 0.9888517129312002, 0.9892017479238406, 0.9894004498791034, 0.9893606816019258, 0.9894318469977041, 0.9892586585776957, 0.9893097632925544, 0.9894580640835463, 0.9897193819311441, 0.989766339086867, 0.9900782817401035, 0.9903613192280072, 0.990432366211168, 0.9904916581983938, 0.990575211872602, 0.9907132973437028, 0.9906352502724463, 0.9908463509142678, 0.9910735438728595, 0.9910455026546396, 0.9912759958784706, 0.9912905605751843, 0.9913338236391128, 0.9916401525097438, 0.9918972112540168, 0.9918775201500712, 0.9919247941767309, 0.9918743708971622, 0.9918685895729499, 0.9920911788668546, 0.9920450434575594, 0.9921732935011076, 0.9923933141879109, 0.9924309696358141, 0.9924299462579655, 0.9924499515571874, 0.9926354030895916, 0.9929022187699275, 0.9930028800024769, 0.9931702050224858, 0.9929883373095506, 0.992996681330995, 0.9931971062038478, 0.9933193598691773, 0.9934247739191736, 0.9933824267355895, 0.9933164124846496, 0.9935337644647746, 0.9934829515218963, 0.99338595845304, 0.9934614611565548, 0.9935270884409084, 0.9933118575349404, 0.9933320273612268, 0.9931246768191795, 0.9931867441074996, 0.9930403167205593, 0.9931550338949411, 0.9932002227292841, 0.9932525544730593, 0.9934925322471911, 0.9934272052867762, 0.9936380922354887, 0.9937604611738539, 0.9940030906517066, 0.9940447458722502, 0.9942590910755383, 0.994091531595384, 0.9942267574156352, 0.9942228305205094, 0.9943122662184585, 0.9943696150049737, 0.9943839904830662, 0.9945131498050069, 0.9943643262304679, 0.9944791759360018, 0.994624357300735, 0.9946551833254602, 0.994643255022676, 0.9947302478978078, 0.9948270705782651, 0.9949560997180669, 0.9949630520962971, 0.9947715634414385, 0.9948108482425512, 0.9949368493183053, 0.9950642011793411, 0.9951532051685499, 0.9952309836100283, 0.9952754796680915, 0.995299214029863, 0.9953716921804481, 0.9954485843088411, 0.9955038363315376, 0.99547221899245, 0.995464653677738, 0.9953021320218967, 0.9953069251066209], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 225314854, "moving_var_accuracy_train": [0.027954227679670866, 0.060291468071618344, 0.08955541358969515, 0.11179841955203594, 0.12874207042113778, 0.14045238346196254, 0.14736154746031901, 0.15034248258003333, 0.1507985953960357, 0.14850450934291987, 0.14455125767621269, 0.13955493900932495, 0.13361674263851742, 0.1268426912688908, 0.11981710280834676, 0.11279659909195651, 0.10568908303418277, 0.09858532880277236, 0.09162967827282059, 0.08487037008639867, 0.07841305879107792, 0.07231574852957773, 0.06656252716771673, 0.06114855152676314, 0.0560377567301515, 0.05121538714217846, 0.04689138374791439, 0.04280676375463451, 0.03912563906601074, 0.03563027692630817, 0.03245621643802359, 0.02953919166745756, 0.026823735951136348, 0.02441613550205631, 0.022161603670469557, 0.02013154128152756, 0.018235001263005108, 0.016542110521125333, 0.015033771529864992, 0.013639627158951598, 0.012368528537326085, 0.011222512087374945, 0.01019668286298859, 0.00923188806737038, 0.008368295589522302, 0.007580713213848717, 0.006887711916265684, 0.006221297402510731, 0.005628392200536591, 0.005095120236264379, 0.004628104048603156, 0.0041878618332667344, 0.003797177797702642, 0.003443531958808199, 0.0031172122186675297, 0.0028281798742822205, 0.00255568912663933, 0.002307203858033889, 0.0020933846481560247, 0.0018966271535960217, 0.001718485257374221, 0.0015569766195109576, 0.001406287100676331, 0.0012676679918975147, 0.001147281822920587, 0.0010333993314639452, 0.0009416792102541668, 0.0008519743441205754, 0.00077324139523162, 0.0006977499982016698, 0.000633168354107001, 0.0005765481711922733, 0.0005283513344852808, 0.0004874775755711823, 0.0004444291698586538, 0.0003999893249585549, 0.00036229823683689053, 0.00032736154966959996, 0.00029606123177465293, 0.0002696816581376022, 0.0002494042687944839, 0.00022787367828200336, 0.0002066325981349005, 0.00018979521808705605, 0.00017216270446016057, 0.00015822921719409855, 0.0001427559538706149, 0.00012930619093551055, 0.0001176407413912022, 0.00010596294966109947, 9.583461961346959e-05, 8.90945146579181e-05, 8.369329120081013e-05, 7.710206406509911e-05, 6.961560975105843e-05, 6.326508182510958e-05, 6.0969905809972106e-05, 5.638187778940305e-05, 5.381889476844176e-05, 5.097318540389054e-05, 4.970396113850989e-05, 4.731637466366181e-05, 4.316661207410591e-05, 3.8983734259401316e-05, 3.511627914450282e-05, 3.234902765195617e-05, 2.9382867335266905e-05, 2.6468340731009245e-05, 2.596860884292881e-05, 2.363051286721504e-05, 2.3482906990340156e-05, 2.298563056170346e-05, 2.329454458265781e-05, 2.30974699289153e-05, 2.2478475553069804e-05, 2.182141925262052e-05, 1.984297530082782e-05, 1.8357159318507037e-05, 1.6841553465401236e-05, 1.568366607326762e-05, 1.4883252914265304e-05, 1.4768815982990368e-05, 1.353914777391515e-05, 1.4168260294298357e-05, 1.3081068522673281e-05, 1.2298449523089464e-05, 1.106871191464971e-05, 1.0116354433330295e-05, 9.550067988991956e-06, 9.08293942160278e-06, 9.213094590739287e-06, 9.394505596320328e-06, 8.810397239915158e-06, 7.943591158750705e-06, 7.194812664882118e-06, 6.745279457819303e-06, 6.0942567389644815e-06, 5.682769186547394e-06, 5.729075425150757e-06, 5.176012652897858e-06, 5.534185357782285e-06, 5.7017587980333104e-06, 5.177011982576362e-06, 4.690950642061346e-06, 4.28468652611848e-06, 4.027826249468859e-06, 3.6798657325075425e-06, 3.7129504880536255e-06, 3.806205203151024e-06, 3.4326614721092145e-06, 3.5675394609862586e-06, 3.2126946884008837e-06, 2.9082704538652095e-06, 3.4619798013172104e-06, 3.7104946032501232e-06, 3.3429347990964126e-06, 3.028754821556229e-06, 2.748761903502696e-06, 2.47418652653926e-06, 2.6726818177343406e-06, 2.4245699198784954e-06, 2.3301455909217763e-06, 2.5328129554218963e-06, 2.292293054690865e-06, 2.063073174941769e-06, 1.8603677654202142e-06, 1.983861426717747e-06, 2.426190749503341e-06, 2.274765828198519e-06, 2.2992682062673526e-06, 2.3670241707150396e-06, 2.13094835788831e-06, 2.279384689022352e-06, 2.185959848298615e-06, 2.067372960898287e-06, 1.8767752204259467e-06, 1.7283186303277645e-06, 1.9806637166730113e-06, 1.8058349414813292e-06, 1.7099203459886464e-06, 1.59023423553211e-06, 1.46997327604374e-06, 1.7398950343932628e-06, 1.5695669279857182e-06, 1.7995584607732333e-06, 1.6542737492106536e-06, 1.6818151911054455e-06, 1.632073942878356e-06, 1.4872448253341191e-06, 1.363167845459668e-06, 1.7451550496092913e-06, 1.6090480504618521e-06, 1.8484029916513333e-06, 1.798330106175667e-06, 2.148318667265799e-06, 1.949103217126067e-06, 2.1676876909669733e-06, 2.203604536376871e-06, 2.1478182849026642e-06, 1.933175240960353e-06, 1.811846413473035e-06, 1.6602617219585999e-06, 1.4960954390962361e-06, 1.49662506918424e-06, 1.5462986693131253e-06, 1.5103828961327622e-06, 1.5490432645115286e-06, 1.4026911322636764e-06, 1.2637025787030936e-06, 1.2054421637460697e-06, 1.1692696304299075e-06, 1.2021793376487644e-06, 1.0823964239513936e-06, 1.304167926012157e-06, 1.1876407937971242e-06, 1.2117631542381528e-06, 1.2365533073979641e-06, 1.1841933675138043e-06, 1.1202194043916299e-06, 1.0260165566009793e-06, 9.2848478029919e-07, 8.82914043079443e-07, 8.478342334507354e-07, 7.905258842142047e-07, 7.204702009716575e-07, 6.489382867547095e-07, 8.217640556359479e-07, 7.397944130229251e-07], "duration": 336436.259204, "accuracy_train": [0.5573172613279808, 0.6805220301541159, 0.7444259514004246, 0.7696026266611297, 0.7987113268272426, 0.8182578975752122, 0.830395534849114, 0.8398116665513106, 0.8553625862287744, 0.858895730954688, 0.867638650966685, 0.8786576933947029, 0.8853530405015688, 0.8872821930370985, 0.8945370178110004, 0.9036505196797711, 0.9076501361203396, 0.9100922633467147, 0.9130902633582503, 0.914878825500646, 0.9179723548818751, 0.9220188347868217, 0.9248998563815062, 0.927038272309893, 0.9269234568221669, 0.9250415282392026, 0.9353161625599853, 0.9325506774294019, 0.9404089594292175, 0.9350367842146549, 0.9395010699289406, 0.940758452727021, 0.9378509352505537, 0.9467784252145626, 0.9426421837509228, 0.9470814155361758, 0.9421521000599853, 0.9478505170842562, 0.9538243470837948, 0.9524292577980805, 0.953196917393411, 0.9560564899409376, 0.9601956153100776, 0.9554294207387413, 0.9589393139765596, 0.9591718288575121, 0.9650075918812293, 0.956568743655408, 0.9604037972383721, 0.9623111402385567, 0.9677280159883721, 0.9640066964285714, 0.9674253861549464, 0.968542178559893, 0.9673792436669435, 0.9705171130952381, 0.9669392678340717, 0.9661701662859912, 0.9718893113810447, 0.9713792205956996, 0.9720524322858989, 0.9725882979766519, 0.9704012161429494, 0.9589622049764673, 0.9716349869647471, 0.9671223958333334, 0.9757261674049464, 0.9725417950004615, 0.9746791294642857, 0.9715641510358989, 0.9750990982027501, 0.9768883813215209, 0.9793762905477114, 0.9816785483573275, 0.9792607540836102, 0.97191400482189, 0.9771441476905685, 0.9763772090716132, 0.9769599382267442, 0.9793526785714286, 0.9825860773694168, 0.9809813642026578, 0.9795866354051311, 0.9823760930001846, 0.9803768255121816, 0.9829344892026578, 0.9794700174764673, 0.9807252373454227, 0.9817483028216132, 0.9793530390596161, 0.9761915576550388, 0.9838645487264673, 0.9850492931547619, 0.9749853641795865, 0.980562476928756, 0.9817490237979882, 0.9860966915836102, 0.984167899536268, 0.986328125, 0.986375709440753, 0.9881199315360835, 0.9876073173334257, 0.9853286715000923, 0.9842594635358989, 0.9837482912859912, 0.9860966915836102, 0.9852363865240864, 0.9831673645717978, 0.98851412536914, 0.982422595976375, 0.9889101216431341, 0.9889798761074198, 0.9902808779761905, 0.9903041294642857, 0.9902576264880952, 0.9905609772978959, 0.9882816104881875, 0.9892810639880952, 0.9890489095953304, 0.9897697057262828, 0.9905144743217055, 0.9917925851905685, 0.9899335476075121, 0.9837479307978036, 0.9898863236549464, 0.9857475587739941, 0.9879568106312293, 0.9892360029646549, 0.990281238464378, 0.9906074802740864, 0.9919088426310447, 0.9923520628576044, 0.9911887674764673, 0.9890027671073275, 0.9900723355597084, 0.9876999627976191, 0.9897697057262828, 0.9907927712024732, 0.9920712425595238, 0.9901889534883721, 0.9928857656192323, 0.99290865661914, 0.9910717890596161, 0.9910252860834257, 0.9913271949404762, 0.9919560665836102, 0.989932826631137, 0.9927462566906607, 0.9931182805001846, 0.9907931316906607, 0.9933504348929494, 0.9914216428456073, 0.9917231912144703, 0.9943971123454227, 0.9942107399524732, 0.9917003002145626, 0.9923502604166666, 0.9914205613810447, 0.9918165576550388, 0.9940944825119971, 0.9916298247739018, 0.9933275438930418, 0.99437350036914, 0.9927698686669435, 0.9924207358573275, 0.9926299992501846, 0.9943044668812293, 0.9953035598929494, 0.9939088310954227, 0.9946761302025655, 0.9913515278931341, 0.9930717775239941, 0.9950009300595238, 0.9944196428571429, 0.99437350036914, 0.9930013020833334, 0.9927222842261905, 0.9954899322858989, 0.9930256350359912, 0.9925130208333334, 0.9941409854881875, 0.9941177340000923, 0.9913747793812293, 0.9935135557978036, 0.991258521940753, 0.9937453497023809, 0.9917224702380952, 0.994187488464378, 0.9936069222383721, 0.9937235401670359, 0.995652332214378, 0.9928392626430418, 0.9955360747739018, 0.99486178161914, 0.9961867559523809, 0.9944196428571429, 0.9961881979051311, 0.9925834962739941, 0.9954437897978959, 0.994187488464378, 0.9951171875, 0.9948857540836102, 0.9945133697858989, 0.9956755837024732, 0.9930249140596161, 0.9955128232858066, 0.9959309895833334, 0.9949326175479882, 0.9945359002976191, 0.9955131837739941, 0.9956984747023809, 0.9961173619762828, 0.9950256235003692, 0.9930481655477114, 0.9951644114525655, 0.9960708590000923, 0.9962103679286637, 0.9959542410714286, 0.9959309895833334, 0.9956759441906607, 0.9955128232858066, 0.9960239955357143, 0.996140613464378, 0.9960011045358066, 0.9951876629406607, 0.9953965658453304, 0.9938394371193245, 0.99535006286914], "end": "2016-01-27 08:56:34.752000", "learning_rate_per_epoch": [0.0030296160839498043, 0.003016969421878457, 0.0030043756123632193, 0.0029918344225734472, 0.0029793456196784973, 0.002966908738017082, 0.002954523777589202, 0.002942190505564213, 0.002929908921942115, 0.0029176785610616207, 0.002905499190092087, 0.0028933705762028694, 0.0028812927193939686, 0.002869265154004097, 0.0028572878800332546, 0.002845360664650798, 0.002833483275026083, 0.0028216554783284664, 0.0028098770417273045, 0.0027981477323919535, 0.00278646731749177, 0.0027748355641961098, 0.0027632524725049734, 0.002751717809587717, 0.0027402311097830534, 0.0027287923730909824, 0.0027174013666808605, 0.0027060580905526876, 0.0026947620790451765, 0.002683513332158327, 0.002672311384230852, 0.0026611562352627516, 0.002650047652423382, 0.002638985402882099, 0.0026279694866389036, 0.002616999438032508, 0.002606075257062912, 0.0025951967108994722, 0.002584363566711545, 0.0025735755916684866, 0.0025628325529396534, 0.0025521344505250454, 0.002541481051594019, 0.0025308721233159304, 0.002520307432860136, 0.0025097867473959923, 0.002499310066923499, 0.002488877158612013, 0.00247848778963089, 0.0024681417271494865, 0.002457838971167803, 0.0024475790560245514, 0.0024373619817197323, 0.002427187515422702, 0.00241705565713346, 0.0024069661740213633, 0.002396918600425124, 0.0023869131691753864, 0.0023769494146108627, 0.002367027336731553, 0.00235714646987617, 0.0023473070468753576, 0.002337508602067828, 0.002327751135453582, 0.002318034414201975, 0.0023083582054823637, 0.0022987222764641047, 0.0022891266271471977, 0.0022795710247009993, 0.0022700552362948656, 0.0022605792619287968, 0.002251142868772149, 0.002241745823994279, 0.0022323881275951862, 0.0022230693139135838, 0.0022137893829494715, 0.0022045483347028494, 0.00219534570351243, 0.002186181489378214, 0.0021770556923002005, 0.0021679678466171026, 0.0021589179523289204, 0.0021499060094356537, 0.0021409315522760153, 0.002131994580850005, 0.0021230948623269796, 0.0021142323967069387, 0.002105406951159239, 0.002096618292853236, 0.002087866421788931, 0.0020791508723050356, 0.002070471877232194, 0.0020618289709091187, 0.0020532221533358097, 0.002044651424512267, 0.0020361163187772036, 0.002027616836130619, 0.0020191529765725136, 0.0020107242744416, 0.002002330729737878, 0.0019939723424613476, 0.0019856488797813654, 0.0019773601088672876, 0.0019691060297191143, 0.0019608864095062017, 0.0019527010153979063, 0.001944549847394228, 0.0019364326726645231, 0.00192834937479347, 0.0019202998373657465, 0.0019122838275507092, 0.0019043013453483582, 0.0018963521579280496, 0.0018884361488744617, 0.0018805532017722726, 0.0018727032002061605, 0.0018648859113454819, 0.0018571012187749147, 0.0018493490060791373, 0.0018416291568428278, 0.0018339415546506643, 0.001826286083087325, 0.0018186625093221664, 0.0018110708333551884, 0.0018035108223557472, 0.0017959823599085212, 0.0017884853295981884, 0.001781019615009427, 0.0017735850997269154, 0.0017661815509200096, 0.0017588089685887098, 0.0017514671199023724, 0.0017441558884456754, 0.001736875157803297, 0.0017296248115599155, 0.001722404733300209, 0.0017152148066088557, 0.0017080549150705338, 0.0017009249422699213, 0.0016938247717916965, 0.0016867541708052158, 0.0016797131393104792, 0.0016727014444768429, 0.001665719086304307, 0.0016587658319622278, 0.0016518415650352836, 0.0016449461691081524, 0.0016380796441808343, 0.0016312417574226856, 0.0016244323924183846, 0.0016176514327526093, 0.001610898762010038, 0.0016041743801906705, 0.0015974780544638634, 0.0015908096684142947, 0.0015841691056266427, 0.0015775562496855855, 0.0015709709841758013, 0.0015644131926819682, 0.0015578827587887645, 0.00155137968249619, 0.0015449037309736013, 0.0015384547878056765, 0.0015320327365770936, 0.0015256375772878528, 0.0015192690771073103, 0.0015129271196201444, 0.001506611704826355, 0.0015003225998952985, 0.0014940598048269749, 0.0014878230867907405, 0.0014816124457865953, 0.0014754276489838958, 0.0014692686963826418, 0.0014631354715675116, 0.0014570278581231833, 0.001450945739634335, 0.001444888999685645, 0.0014388575218617916, 0.0014328513061627746, 0.0014268701197579503, 0.001420913846231997, 0.0014149824855849147, 0.0014090759214013815, 0.0014031939208507538, 0.0013973364839330316, 0.001391503494232893, 0.001385694951750338, 0.0013799106236547232, 0.0013741503935307264, 0.0013684142613783479, 0.0013627019943669438, 0.0013570135924965143, 0.0013513489393517375, 0.0013457079185172915, 0.0013400905299931765, 0.0013344965409487486, 0.001328925951384008, 0.0013233785284683108, 0.0013178542722016573, 0.0013123530661687255, 0.0013068749103695154, 0.0013014195719733834, 0.0012959870509803295, 0.00129057711455971, 0.001285189762711525, 0.0012798249954357743, 0.0012744825799018145, 0.0012691625161096454, 0.0012638645712286234, 0.0012585887452587485, 0.001253334921784699, 0.0012481031008064747, 0.001242893049493432, 0.001237704767845571, 0.0012325381394475698, 0.0012273931642994285, 0.0012222696095705032, 0.0012171674752607942, 0.0012120866449549794, 0.0012070270022377372, 0.0012019884306937456, 0.0011969709303230047, 0.0011919743847101927, 0.0011869986774399877, 0.0011820436920970678, 0.0011771094286814332, 0.001172195770777762], "accuracy_valid": [0.5533270778426205, 0.6620446630271084, 0.7250755953501506, 0.7486984069088856, 0.7726447783320783, 0.788097703313253, 0.7965411450489458, 0.8058596691453314, 0.8188197124435241, 0.8165209666792168, 0.8225127070783133, 0.835594820689006, 0.8393187005835843, 0.835961031626506, 0.8356860057417168, 0.843529391001506, 0.8521463784826807, 0.8482401284826807, 0.8505697595067772, 0.8505285791603916, 0.8520346032567772, 0.8505285791603916, 0.8560526284826807, 0.8566835702183735, 0.8519125329442772, 0.8527361398719879, 0.8636724632906627, 0.861659038497741, 0.8649446418486446, 0.860508930252259, 0.8684037909450302, 0.8639166039156627, 0.8592264566076807, 0.8680978798004518, 0.8626047157379518, 0.8661653449736446, 0.8631635918674698, 0.868494975997741, 0.8732660132718373, 0.8665624411709337, 0.8703054405120482, 0.8710275673004518, 0.8759309699736446, 0.8662771201995482, 0.8746087867093373, 0.872523296310241, 0.878138530685241, 0.8758706701807228, 0.8722688605986446, 0.8788312429405121, 0.8768472326807228, 0.8755853492093373, 0.8791665686182228, 0.8818418204066265, 0.8862775320030121, 0.8821168462914157, 0.8785562170557228, 0.8814859045557228, 0.8873467502823795, 0.8835008000753012, 0.8789018378200302, 0.8842626364834337, 0.8811299887048193, 0.8695833137236446, 0.8789018378200302, 0.879603374435241, 0.8884439123682228, 0.8813844244164157, 0.8859216161521084, 0.8824933523155121, 0.8893601750753012, 0.8843347020896084, 0.8918015813253012, 0.8875894201807228, 0.8864098974021084, 0.8796239646084337, 0.8838052404932228, 0.8852494940700302, 0.8829713384789157, 0.8845170721950302, 0.8902852621423193, 0.8863996023155121, 0.8878850362387049, 0.8907941335655121, 0.8868467032191265, 0.8933987904743976, 0.8829095679593373, 0.8889321936182228, 0.8894410650414157, 0.8872849797628012, 0.8846700277673193, 0.8945989034262049, 0.8911397543298193, 0.8828698583396084, 0.8828183829066265, 0.8887895331325302, 0.8935399802334337, 0.893653226185994, 0.8916795110128012, 0.8948636342243976, 0.8935605704066265, 0.8945989034262049, 0.8890336737575302, 0.8916177404932228, 0.8889527837914157, 0.8900308264307228, 0.8953107351280121, 0.8895734304405121, 0.8934179099209337, 0.8884336172816265, 0.8937547063253012, 0.8896440253200302, 0.8963078878012049, 0.8987595891378012, 0.8953210302146084, 0.8977624364646084, 0.8901117163968373, 0.8927369634789157, 0.8944356527673193, 0.8980065770896084, 0.8967652837914157, 0.8981492375753012, 0.8969182393637049, 0.8873246893825302, 0.8992890507341867, 0.8959622670368976, 0.8947106786521084, 0.8979462772966867, 0.8964299581137049, 0.8959004965173193, 0.8974065206137049, 0.9018422322100903, 0.8966226233057228, 0.8906205878200302, 0.8944665380271084, 0.8935914556664157, 0.8991258000753012, 0.8945268378200302, 0.8991155049887049, 0.8964505482868976, 0.9051175404743976, 0.9047513295368976, 0.8951886648155121, 0.8948327489646084, 0.8994611257530121, 0.8968167592243976, 0.895118069935994, 0.8991360951618976, 0.9012524708207832, 0.8980565817959337, 0.8973756353539157, 0.8971829701618976, 0.8939576666039157, 0.9018422322100903, 0.9016995717243976, 0.9025540639118976, 0.8977830266378012, 0.8944665380271084, 0.8979050969503012, 0.9000817724021084, 0.8971726750753012, 0.8992169851280121, 0.8997052663780121, 0.9019231221762049, 0.8959210866905121, 0.9015980915850903, 0.9032658956137049, 0.9038762471762049, 0.900122952748494, 0.9054837514118976, 0.8980065770896084, 0.9000920674887049, 0.9019437123493976, 0.9014245458396084, 0.9068677051957832, 0.8994508306664157, 0.8979962820030121, 0.9080884083207832, 0.899512601185994, 0.8950357092432228, 0.8997258565512049, 0.9020554875753012, 0.8982404226280121, 0.898291898060994, 0.898169827748494, 0.9006921239646084, 0.8967961690512049, 0.8972947453878012, 0.8980771719691265, 0.9015672063253012, 0.9019643025225903, 0.9002759083207832, 0.9024114034262049, 0.9007230092243976, 0.9054028614457832, 0.8994302404932228, 0.9025540639118976, 0.897315335560994, 0.8998273366905121, 0.902320218373494, 0.8998273366905121, 0.900489163685994, 0.902930569935994, 0.904639554310994, 0.8957681311182228, 0.900489163685994, 0.9042424581137049, 0.9021878529743976, 0.9007230092243976, 0.9055043415850903, 0.9035512165850903, 0.9055146366716867, 0.904761624623494, 0.8983830831137049, 0.8996758518448795, 0.9050469455948795, 0.901709866810994, 0.9042939335466867, 0.9027982045368976, 0.9016789815512049, 0.9017804616905121, 0.9016789815512049, 0.9065014942582832, 0.9028790945030121, 0.9031541203878012, 0.9009877400225903, 0.8990743246423193, 0.9057175969503012], "accuracy_test": 0.6599350286989796, "start": "2016-01-23 11:29:18.493000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0, 211.0, 212.0, 213.0, 214.0, 215.0, 216.0, 217.0, 218.0, 219.0, 220.0, 221.0, 222.0, 223.0, 224.0, 225.0, 226.0, 227.0], "accuracy_train_last": 0.99535006286914, "batch_size_eval": 1024, "accuracy_train_std": [0.01594174120141923, 0.02149424073752669, 0.021780486738969408, 0.020375959291088885, 0.01967178346604126, 0.020959653327932475, 0.018053642524554447, 0.018807824784942438, 0.018456876272259833, 0.019708265031415328, 0.017922186441634295, 0.019177216071784523, 0.017912195368962918, 0.018226305302359122, 0.017894194036804114, 0.017587594187558106, 0.01602335774776262, 0.015803200960636934, 0.015738101397420528, 0.016575367475080725, 0.01605071079508306, 0.014113142830790569, 0.015783830761318646, 0.01632214208518692, 0.01531414728673367, 0.015806531550819087, 0.013294940804034983, 0.01300896141874568, 0.012903929968664764, 0.01305311756828731, 0.013873506767963367, 0.01217980531214976, 0.012339091536887727, 0.01225762741635975, 0.011607930959924547, 0.01223567444314195, 0.014441792311846018, 0.010689744248989424, 0.010304555792484715, 0.010057238649567953, 0.010823763732647103, 0.010665215880463848, 0.010885995588602615, 0.010580882067026695, 0.011171306174406412, 0.010912760436925052, 0.009708036390849346, 0.010598657554281224, 0.009725574268244722, 0.010120233843695545, 0.009203530040835123, 0.00939003221480544, 0.009190001895299131, 0.00860378258434124, 0.008918155421467438, 0.008815472924905346, 0.006973379733325287, 0.008998806400968428, 0.007942073029628098, 0.007528276848581969, 0.008153026246660356, 0.0076087069444612, 0.00849191579310454, 0.009691320453349533, 0.006767272707151044, 0.008130576952668939, 0.005891138140834566, 0.008301765663396595, 0.007592563295396642, 0.0068458345906498705, 0.00712332099276635, 0.007003301391153256, 0.006767497659002171, 0.005404882754628197, 0.006632000374727476, 0.006965146821662292, 0.006932878149858287, 0.007144097892839679, 0.0064677582194177345, 0.005680600745396856, 0.00513896721151331, 0.006522725821912543, 0.005170170196589418, 0.005574959633519457, 0.005747735804615408, 0.005010300550412974, 0.00605449223306025, 0.006611524217850366, 0.005629900389927247, 0.006011889766031216, 0.006957127430603696, 0.00547774686552105, 0.0047864756860757545, 0.004365155719863263, 0.006206774102879486, 0.005052631695683161, 0.004576485610188344, 0.004282964817182691, 0.005206879912533907, 0.004410382472774517, 0.00370442713508352, 0.003827885101058842, 0.004820520308470595, 0.004991255946595194, 0.0051158912033244216, 0.004591346178226928, 0.004652431418675199, 0.004675589339298286, 0.004688086038891467, 0.005160599794969806, 0.004043755258525811, 0.003731030014365358, 0.003584652593117888, 0.003931702196161259, 0.0039781794472316745, 0.003152477732874258, 0.0041930777923167885, 0.0037731226484658685, 0.00440190709114638, 0.003778423950379806, 0.0029945353409413624, 0.0032259586632053684, 0.0035762866365698517, 0.005421435835829743, 0.003610320301939033, 0.00468531253028295, 0.0034753650787277914, 0.0034274388861016882, 0.0034761065254230203, 0.0034829323053028774, 0.003693416430734939, 0.0028650454809331953, 0.00329194619774487, 0.0034182023759250064, 0.00375877413796958, 0.005233237999380345, 0.0035746127734574756, 0.003310025878010211, 0.0035094345309160785, 0.003554049292939396, 0.0025518015672197887, 0.003169021952288877, 0.003333432476745739, 0.002901257868327759, 0.0031116190465732573, 0.0028584045397125005, 0.003078471869357882, 0.00308263960927022, 0.0027211173310152537, 0.0033782883102866134, 0.0025776949773817564, 0.003003067249450425, 0.0033988314237658585, 0.0023452404069797873, 0.0025701179518053704, 0.0029938361938639645, 0.003435638496185621, 0.00339390870376582, 0.0028754290056145705, 0.0024205325300921403, 0.002916052542771302, 0.0031608702060087032, 0.0022438480018083023, 0.0029306632604696333, 0.0027860908593712514, 0.0027892367011070306, 0.002591726214047934, 0.0020692085394575393, 0.002342968924111131, 0.0021965977956962955, 0.003595739567188971, 0.003314466918835849, 0.002739041088057807, 0.0027644836212243023, 0.002990046361884065, 0.0025476034899899946, 0.003030285366339503, 0.002021362447276103, 0.002791781864048675, 0.0031799332212200074, 0.002335035152692036, 0.002791171443088383, 0.0030440947727397067, 0.002802158674847693, 0.003103496550442621, 0.002737856552430236, 0.003431150808351324, 0.0026702951951700364, 0.002206715454889529, 0.0023055315063458645, 0.0021547749603037783, 0.002572913646683148, 0.002046046227011214, 0.0019889548957129975, 0.0018918197178950883, 0.002533025266256756, 0.0016855042610862247, 0.002796217786975673, 0.0019368474781462698, 0.0030809017655583897, 0.002530462771654165, 0.002283044180833336, 0.0019531604962533104, 0.0018477506635807257, 0.002482003240784634, 0.0019309083039051346, 0.001807195642840685, 0.0019538891081110786, 0.0025309968386430064, 0.0019999780437861304, 0.0025039380601689924, 0.0017380520009916635, 0.002028754081713311, 0.002672573875485232, 0.002574782911108341, 0.002049779967590336, 0.001932457385109819, 0.0021582591863601734, 0.0021194595499854857, 0.0019665255443620977, 0.0021846984505574396, 0.002218659869413179, 0.002130850463843719, 0.002097116475893233, 0.002037586424402036, 0.0022484976133245885, 0.0026230211788302845, 0.002547031170339349], "accuracy_test_std": 0.009564688198423744, "error_valid": [0.4466729221573795, 0.3379553369728916, 0.27492440464984935, 0.25130159309111444, 0.22735522166792166, 0.21190229668674698, 0.2034588549510542, 0.19414033085466864, 0.18118028755647586, 0.1834790333207832, 0.17748729292168675, 0.16440517931099397, 0.16068129941641573, 0.16403896837349397, 0.1643139942582832, 0.15647060899849397, 0.1478536215173193, 0.1517598715173193, 0.14943024049322284, 0.1494714208396084, 0.14796539674322284, 0.1494714208396084, 0.1439473715173193, 0.1433164297816265, 0.14808746705572284, 0.14726386012801207, 0.13632753670933728, 0.13834096150225905, 0.1350553581513554, 0.13949106974774095, 0.13159620905496983, 0.13608339608433728, 0.1407735433923193, 0.13190212019954817, 0.13739528426204817, 0.1338346550263554, 0.13683640813253017, 0.13150502400225905, 0.12673398672816272, 0.13343755882906627, 0.12969455948795183, 0.12897243269954817, 0.12406903002635539, 0.13372287980045183, 0.12539121329066272, 0.12747670368975905, 0.12186146931475905, 0.12412932981927716, 0.1277311394013554, 0.12116875705948793, 0.12315276731927716, 0.12441465079066272, 0.12083343138177716, 0.11815817959337349, 0.11372246799698793, 0.11788315370858427, 0.12144378294427716, 0.11851409544427716, 0.11265324971762047, 0.11649919992469882, 0.12109816217996983, 0.11573736351656627, 0.11887001129518071, 0.1304166862763554, 0.12109816217996983, 0.12039662556475905, 0.11155608763177716, 0.11861557558358427, 0.1140783838478916, 0.11750664768448793, 0.11063982492469882, 0.1156652979103916, 0.10819841867469882, 0.11241057981927716, 0.1135901025978916, 0.12037603539156627, 0.11619475950677716, 0.11475050592996983, 0.11702866152108427, 0.11548292780496983, 0.10971473785768071, 0.11360039768448793, 0.11211496376129515, 0.10920586643448793, 0.11315329678087349, 0.10660120952560237, 0.11709043204066272, 0.11106780638177716, 0.11055893495858427, 0.11271502023719882, 0.11532997223268071, 0.10540109657379515, 0.10886024567018071, 0.1171301416603916, 0.11718161709337349, 0.11121046686746983, 0.10646001976656627, 0.10634677381400603, 0.10832048898719882, 0.10513636577560237, 0.10643942959337349, 0.10540109657379515, 0.11096632624246983, 0.10838225950677716, 0.11104721620858427, 0.10996917356927716, 0.10468926487198793, 0.11042656955948793, 0.10658209007906627, 0.11156638271837349, 0.10624529367469882, 0.11035597467996983, 0.10369211219879515, 0.10124041086219882, 0.1046789697853916, 0.1022375635353916, 0.10988828360316272, 0.10726303652108427, 0.10556434723268071, 0.1019934229103916, 0.10323471620858427, 0.10185076242469882, 0.10308176063629515, 0.11267531061746983, 0.10071094926581325, 0.10403773296310237, 0.1052893213478916, 0.10205372270331325, 0.10357004188629515, 0.10409950348268071, 0.10259347938629515, 0.0981577677899097, 0.10337737669427716, 0.10937941217996983, 0.1055334619728916, 0.10640854433358427, 0.10087419992469882, 0.10547316217996983, 0.10088449501129515, 0.10354945171310237, 0.09488245952560237, 0.09524867046310237, 0.10481133518448793, 0.1051672510353916, 0.10053887424698793, 0.10318324077560237, 0.10488193006400603, 0.10086390483810237, 0.09874752917921681, 0.10194341820406627, 0.10262436464608427, 0.10281702983810237, 0.10604233339608427, 0.0981577677899097, 0.09830042827560237, 0.09744593608810237, 0.10221697336219882, 0.1055334619728916, 0.10209490304969882, 0.0999182275978916, 0.10282732492469882, 0.10078301487198793, 0.10029473362198793, 0.09807687782379515, 0.10407891330948793, 0.0984019084149097, 0.09673410438629515, 0.09612375282379515, 0.09987704725150603, 0.09451624858810237, 0.1019934229103916, 0.09990793251129515, 0.09805628765060237, 0.0985754541603916, 0.09313229480421681, 0.10054916933358427, 0.10200371799698793, 0.09191159167921681, 0.10048739881400603, 0.10496429075677716, 0.10027414344879515, 0.09794451242469882, 0.10175957737198793, 0.10170810193900603, 0.10183017225150603, 0.0993078760353916, 0.10320383094879515, 0.10270525461219882, 0.10192282803087349, 0.09843279367469882, 0.0980356974774097, 0.09972409167921681, 0.09758859657379515, 0.09927699077560237, 0.09459713855421681, 0.10056975950677716, 0.09744593608810237, 0.10268466443900603, 0.10017266330948793, 0.09767978162650603, 0.10017266330948793, 0.09951083631400603, 0.09706943006400603, 0.09536044568900603, 0.10423186888177716, 0.09951083631400603, 0.09575754188629515, 0.09781214702560237, 0.09927699077560237, 0.0944956584149097, 0.0964487834149097, 0.09448536332831325, 0.09523837537650603, 0.10161691688629515, 0.10032414815512047, 0.09495305440512047, 0.09829013318900603, 0.09570606645331325, 0.09720179546310237, 0.09832101844879515, 0.09821953830948793, 0.09832101844879515, 0.09349850574171681, 0.09712090549698793, 0.09684587961219882, 0.0990122599774097, 0.10092567535768071, 0.09428240304969882], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-3, 3], "momentum": 0.5165522437345351, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.003042315812184113, "patience_threshold": 1, "do_flip": true, "batch_size": 16, "optimization": "adam", "nb_data_augmentation": 3, "learning_rate_decay_method": "exp", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 9.593382181892827e-10, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.004174333775513561}, "accuracy_valid_max": 0.9080884083207832, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import os\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-3, 3)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.9057175969503012, "loss_train": [1.5569062232971191, 1.0973209142684937, 0.905646562576294, 0.7913291454315186, 0.7139474749565125, 0.6527262926101685, 0.6073920726776123, 0.5712234377861023, 0.5362507104873657, 0.5074876546859741, 0.48000627756118774, 0.45673349499702454, 0.43534931540489197, 0.413811594247818, 0.39545938372612, 0.3774205148220062, 0.35811614990234375, 0.3450847566127777, 0.3269975185394287, 0.3151351809501648, 0.3027069568634033, 0.2871766686439514, 0.2749449908733368, 0.2661101222038269, 0.2548609972000122, 0.24405537545681, 0.2341325432062149, 0.22327248752117157, 0.21576111018657684, 0.20821429789066315, 0.20286925137043, 0.19471411406993866, 0.1856648176908493, 0.18072447180747986, 0.17436639964580536, 0.16784575581550598, 0.1623505800962448, 0.1576743721961975, 0.15235643088817596, 0.14575156569480896, 0.14262071251869202, 0.13915477693080902, 0.13104248046875, 0.12876848876476288, 0.127681165933609, 0.12261570245027542, 0.11937806010246277, 0.11211928725242615, 0.11384318023920059, 0.10915768891572952, 0.10588284581899643, 0.10352227836847305, 0.09917990118265152, 0.09987189620733261, 0.09456857293844223, 0.09311480820178986, 0.09072355180978775, 0.08679007738828659, 0.08722632378339767, 0.08476115763187408, 0.08232660591602325, 0.07937601208686829, 0.08045963197946548, 0.07584548741579056, 0.07350420206785202, 0.07183685153722763, 0.07392416149377823, 0.07162381708621979, 0.0690135657787323, 0.06709611415863037, 0.06470412015914917, 0.06381760537624359, 0.06226857379078865, 0.06086142361164093, 0.060273267328739166, 0.05906899645924568, 0.0581108033657074, 0.057281289249658585, 0.05415564030408859, 0.05316006392240524, 0.05338963493704796, 0.05258636921644211, 0.050921376794576645, 0.050730619579553604, 0.05106524005532265, 0.05014442279934883, 0.04577184095978737, 0.04767145216464996, 0.0460980050265789, 0.04479368403553963, 0.04410462826490402, 0.04302152246236801, 0.04545183852314949, 0.04247827082872391, 0.041598472744226456, 0.040419768542051315, 0.04012686014175415, 0.04067893698811531, 0.040388986468315125, 0.038673773407936096, 0.03914785012602806, 0.034766893833875656, 0.038829874247312546, 0.03673518821597099, 0.034017354249954224, 0.0379771888256073, 0.03342374414205551, 0.03447144106030464, 0.03186908736824989, 0.03353138267993927, 0.03336893022060394, 0.030852174386382103, 0.032050829380750656, 0.03125138580799103, 0.03089337982237339, 0.032195426523685455, 0.028441326692700386, 0.02815133146941662, 0.029620381072163582, 0.028782175853848457, 0.0275722723454237, 0.0278213731944561, 0.025660591199994087, 0.02882484905421734, 0.025517428293824196, 0.02723526395857334, 0.027621928602457047, 0.02615625225007534, 0.02589043416082859, 0.022735172882676125, 0.025854894891381264, 0.024185318499803543, 0.02379900962114334, 0.022307926788926125, 0.02455572411417961, 0.02357647754251957, 0.02267879620194435, 0.023815521970391273, 0.020899036899209023, 0.022701848298311234, 0.021430110558867455, 0.022167498245835304, 0.02266405150294304, 0.021399492397904396, 0.020526167005300522, 0.020357606932520866, 0.020992012694478035, 0.020063558593392372, 0.02045106515288353, 0.01862163655459881, 0.019247127696871758, 0.019501730799674988, 0.018167195841670036, 0.020457798615098, 0.01906539313495159, 0.018037129193544388, 0.018107350915670395, 0.01729978434741497, 0.017609164118766785, 0.01753060147166252, 0.01710715889930725, 0.017324943095445633, 0.016490155830979347, 0.016918445006012917, 0.016399148851633072, 0.01731688529253006, 0.015856599435210228, 0.016869768500328064, 0.01746748574078083, 0.015144645236432552, 0.016111671924591064, 0.014966621063649654, 0.016447700560092926, 0.014512675814330578, 0.014674111269414425, 0.01520268153399229, 0.013616901822388172, 0.015457283705472946, 0.013756079599261284, 0.013190637342631817, 0.013958064839243889, 0.014985564164817333, 0.014794471673667431, 0.013732369989156723, 0.013315403833985329, 0.013535094447433949, 0.012773308902978897, 0.013755599968135357, 0.012160589918494225, 0.013093867339193821, 0.013231751509010792, 0.011641599237918854, 0.013516906648874283, 0.011639025062322617, 0.01229338999837637, 0.010979881510138512, 0.012682609260082245, 0.012185420840978622, 0.011611506342887878, 0.010531719774007797, 0.011959998868405819, 0.011036898009479046, 0.01099618710577488, 0.01197084691375494, 0.010983465239405632, 0.011425087228417397, 0.010322095826268196, 0.010537592694163322, 0.010463650338351727, 0.01107869017869234, 0.010675736702978611, 0.010493815876543522, 0.01143462210893631, 0.009108414873480797, 0.010542195290327072, 0.010238287970423698, 0.0100388890132308, 0.010176228359341621, 0.009120259433984756, 0.00968932919204235, 0.00963346567004919, 0.009167743846774101, 0.009091708809137344, 0.008920553140342236, 0.008921870961785316, 0.009081190451979637, 0.009298435412347317, 0.008462523110210896], "accuracy_train_first": 0.5573172613279808, "model": "residualv3", "loss_std": [0.3447401821613312, 0.27214178442955017, 0.26034507155418396, 0.2526688277721405, 0.24586820602416992, 0.2405257374048233, 0.23638436198234558, 0.2294890433549881, 0.2251175343990326, 0.21817974746227264, 0.2132408171892166, 0.21038897335529327, 0.20604108273983002, 0.20048387348651886, 0.19612017273902893, 0.19205985963344574, 0.18787606060504913, 0.18155892193317413, 0.1770249456167221, 0.1772361695766449, 0.17013829946517944, 0.16551753878593445, 0.16160212457180023, 0.1568150371313095, 0.1526273936033249, 0.14923547208309174, 0.14534124732017517, 0.14164422452449799, 0.1357754021883011, 0.1337723433971405, 0.13155494630336761, 0.12794724106788635, 0.12415696680545807, 0.11860644072294235, 0.1174013614654541, 0.11571015417575836, 0.11370778828859329, 0.11184883862733841, 0.10563509911298752, 0.10386370867490768, 0.10188987106084824, 0.09926649183034897, 0.09575273841619492, 0.09586947411298752, 0.09769812971353531, 0.09371310472488403, 0.09212040901184082, 0.08639110624790192, 0.08990791440010071, 0.08745073527097702, 0.08444173634052277, 0.08147699385881424, 0.0812963917851448, 0.08420632779598236, 0.08002656698226929, 0.07787803560495377, 0.07518624514341354, 0.07465101033449173, 0.07541627436876297, 0.07353117316961288, 0.07145439088344574, 0.0689757838845253, 0.073968306183815, 0.07107338309288025, 0.0663645789027214, 0.06546643376350403, 0.06871898472309113, 0.06671616435050964, 0.06724516302347183, 0.06509228050708771, 0.06090620905160904, 0.06145831570029259, 0.060186244547367096, 0.058835335075855255, 0.058751288801431656, 0.059250228106975555, 0.059959497302770615, 0.058047786355018616, 0.05960420146584511, 0.05440785363316536, 0.05954247713088989, 0.05574818328022957, 0.055483873933553696, 0.05353936553001404, 0.05602547898888588, 0.055305417627096176, 0.05006991699337959, 0.05322292447090149, 0.049661602824926376, 0.049786780029535294, 0.050845999270677567, 0.047159213572740555, 0.052152231335639954, 0.04793751612305641, 0.04743388667702675, 0.046215008944272995, 0.047370001673698425, 0.048080772161483765, 0.0496017262339592, 0.04663274437189102, 0.04615218564867973, 0.04277419671416283, 0.04695615917444229, 0.04677535220980644, 0.04027588665485382, 0.047691624611616135, 0.04269840568304062, 0.04531789943575859, 0.04050731286406517, 0.04292818531394005, 0.04250724986195564, 0.038275931030511856, 0.04220622032880783, 0.040411122143268585, 0.042150985449552536, 0.042871054261922836, 0.03747496008872986, 0.03810681030154228, 0.039971739053726196, 0.04049522802233696, 0.03826659172773361, 0.03923541307449341, 0.03590193763375282, 0.04081030562520027, 0.03615494444966316, 0.03892058506608009, 0.0425788052380085, 0.037890903651714325, 0.036024704575538635, 0.03297363966703415, 0.03871092200279236, 0.03473005071282387, 0.03368749842047691, 0.033826183527708054, 0.037904344499111176, 0.03558178246021271, 0.03597235307097435, 0.03675113990902901, 0.03128194436430931, 0.03396407514810562, 0.03303241729736328, 0.034583497792482376, 0.03720270097255707, 0.0338708758354187, 0.03267369419336319, 0.030918443575501442, 0.03497765213251114, 0.03182925656437874, 0.03325863182544708, 0.029799388721585274, 0.029879067093133926, 0.03066411055624485, 0.02882392145693302, 0.03473101928830147, 0.03242945298552513, 0.02962837927043438, 0.029102575033903122, 0.028869884088635445, 0.02948283776640892, 0.029767097905278206, 0.028820527717471123, 0.027582217007875443, 0.03081621788442135, 0.0300226379185915, 0.02874508686363697, 0.031337011605501175, 0.02664775215089321, 0.030080074444413185, 0.03104426898062229, 0.026272671297192574, 0.029159246012568474, 0.026646416634321213, 0.030625909566879272, 0.02727481722831726, 0.027414893731474876, 0.02664807438850403, 0.02613609842956066, 0.030434824526309967, 0.025855833664536476, 0.023264963179826736, 0.0255330391228199, 0.02756424993276596, 0.028084728866815567, 0.026527734473347664, 0.025625213980674744, 0.025057552382349968, 0.024490896612405777, 0.028344284743070602, 0.022883595898747444, 0.024271154776215553, 0.027141010388731956, 0.02559364028275013, 0.025823509320616722, 0.024440303444862366, 0.024540996178984642, 0.02188475988805294, 0.02424800954759121, 0.0256210770457983, 0.02252570353448391, 0.02010970562696457, 0.0227385051548481, 0.021201878786087036, 0.028679020702838898, 0.025391336530447006, 0.02219810150563717, 0.023603148758411407, 0.020810825750231743, 0.022731536999344826, 0.021626587957143784, 0.022704867646098137, 0.02290336787700653, 0.02569357119500637, 0.026872865855693817, 0.01989215984940529, 0.022207312285900116, 0.02233787067234516, 0.019832929596304893, 0.023723114281892776, 0.018795598298311234, 0.019508998841047287, 0.02111421525478363, 0.019074993208050728, 0.019662730395793915, 0.019344577565789223, 0.018931079655885696, 0.020009690895676613, 0.022061457857489586, 0.018822772428393364]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:17 2016", "state": "available"}], "summary": "5461e6f7ab648e2033cdb5a0901aa704"}