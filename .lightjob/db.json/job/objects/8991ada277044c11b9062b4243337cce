{"content": {"hp_model": {"f0": 16, "f1": 32, "f2": 32, "f3": 64, "nonlin": "leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.3927834033966064, 0.9747105240821838, 0.8120108246803284, 0.7107665538787842, 0.6313749551773071, 0.563162088394165, 0.5004776120185852, 0.4430539608001709, 0.39216792583465576, 0.3457218110561371, 0.30932900309562683, 0.2792438268661499, 0.2465212494134903, 0.22338204085826874, 0.19912652671337128, 0.18547601997852325, 0.17018644511699677, 0.15609756112098694, 0.14662843942642212, 0.13508304953575134, 0.12477920949459076, 0.11602390557527542, 0.11119675636291504, 0.10294488072395325, 0.09686639904975891, 0.09386139363050461, 0.09005355834960938, 0.08072412759065628, 0.07912057638168335, 0.07516548782587051, 0.06948848813772202, 0.07292209565639496, 0.06496065855026245, 0.06456995755434036, 0.05925792455673218, 0.060388628393411636, 0.05828281491994858, 0.05660092830657959, 0.05282841995358467, 0.04846523702144623, 0.051990631967782974, 0.04600938409566879, 0.048140525817871094, 0.04955592378973961, 0.04955363646149635, 0.04108280688524246, 0.037907615303993225, 0.04207185283303261, 0.03818333148956299, 0.04297399893403053, 0.03779905289411545, 0.037772249430418015, 0.03885302320122719, 0.03619391471147537, 0.036024823784828186, 0.03277470916509628, 0.035480525344610214, 0.02904273383319378, 0.03209034726023674, 0.0277527067810297, 0.028654731810092926, 0.03175051137804985, 0.0332065112888813, 0.030199207365512848, 0.02982066012918949, 0.028917202726006508, 0.02624230459332466, 0.026058755815029144, 0.02762371487915516, 0.02384999208152294, 0.027084944769740105, 0.025790326297283173, 0.027756739407777786, 0.02783145196735859, 0.024863846600055695, 0.02295655757188797, 0.02210875414311886, 0.020109159871935844, 0.02065873146057129, 0.02153397910296917, 0.02079043537378311, 0.022999120876193047, 0.02194441482424736, 0.02553045190870762, 0.024078773334622383, 0.01137485820800066, 0.004572147503495216, 0.00421634316444397, 0.004145350772887468, 0.004104956518858671, 0.004078207537531853, 0.004058673977851868, 0.004045249428600073, 0.004043694119900465, 0.004043543245643377, 0.004043533932417631, 0.004043533932417631, 0.004043533932417631, 0.004043533932417631, 0.004043533932417631, 0.004043533932417631, 0.004043533932417631, 0.004043533932417631, 0.004043533932417631, 0.004043533932417631, 0.004043533932417631, 0.004043533932417631, 0.004043533466756344, 0.004043533932417631, 0.004043533466756344, 0.004043533932417631, 0.004043533932417631, 0.004043533932417631, 0.004043533932417631, 0.004043533932417631, 0.004043533932417631, 0.004043533932417631, 0.004043533932417631, 0.004043533932417631, 0.004043533932417631, 0.004043533932417631, 0.004043533932417631, 0.004043533932417631, 0.004043533932417631, 0.004043533932417631, 0.004043533932417631, 0.004043533932417631, 0.004043533932417631, 0.004043533932417631, 0.004043533932417631, 0.004043533932417631, 0.004043533932417631, 0.004043533932417631, 0.004043533932417631, 0.004043533932417631, 0.004043533932417631, 0.004043533932417631, 0.004043533932417631, 0.004043533932417631, 0.004043533932417631, 0.004043533932417631, 0.004043533932417631], "moving_avg_accuracy_train": [0.05676243395856404, 0.11559301027823918, 0.17749977793927646, 0.23726079863995012, 0.2933166306322674, 0.3451196113490333, 0.39181448175367406, 0.4361948443248921, 0.4763138098508746, 0.5131907554219426, 0.5462846032370757, 0.5784587064169673, 0.6081688556396116, 0.6327484674282252, 0.6593594757057054, 0.6819703137340644, 0.7033708189285797, 0.7254097903822057, 0.7448402166999744, 0.7612581400800414, 0.7768828783399406, 0.7912590378631356, 0.8062808787185256, 0.8191076050943198, 0.8318863488992105, 0.8426477489045461, 0.852344742799852, 0.8616508912127423, 0.869712529695058, 0.8771796649684462, 0.8851394270787907, 0.8921752576959393, 0.8983006391049629, 0.9039040550301993, 0.9090100886248631, 0.9145261335933569, 0.9193231633507156, 0.9240031412489865, 0.9287057638050586, 0.931949935861476, 0.9351977169384883, 0.938857539738687, 0.9417282392243513, 0.9446467622876582, 0.9474642033934716, 0.9497998294446284, 0.9523273230251748, 0.9528608552287019, 0.9544825741308962, 0.955602757563137, 0.9580196744401751, 0.959908906325938, 0.9615208233195438, 0.9622833406149888, 0.9633742541713839, 0.9647629053161687, 0.966577738556008, 0.9681133961730447, 0.9681772007510152, 0.9695831030342563, 0.9700508529986879, 0.9713369996167316, 0.9719735900931814, 0.9724837425041291, 0.9736054389382399, 0.9742290270753775, 0.9737140008905418, 0.9745941609943632, 0.975621073019927, 0.9768220586489051, 0.9774820937804616, 0.9781063523333862, 0.9784519822405515, 0.9791489517617437, 0.9792344286093788, 0.979536933255593, 0.9803834991931382, 0.9813081329047767, 0.9817288240036032, 0.9819098804413751, 0.9822168823151038, 0.9822537297228977, 0.9824147395256172, 0.9823317837647314, 0.982657085223991, 0.9840402792313537, 0.9853386322605993, 0.986528076326206, 0.9876311280685853, 0.9886355003807743, 0.9895533863546015, 0.9903818088798557, 0.991129714301394, 0.9918005040319688, 0.9924042147894863, 0.9929475544712519, 0.993436560184841, 0.9938766653270713, 0.9942727599550785, 0.9946292451202849, 0.9949500817689708, 0.9952388347527881, 0.9954987124382236, 0.9957326023551156, 0.9959431032803184, 0.996132554113001, 0.9963030598624152, 0.996456515036888, 0.9965946246939136, 0.9967189233852365, 0.9968307922074272, 0.9969314741473988, 0.9970220878933732, 0.9971036402647502, 0.9971770373989894, 0.9972430948198048, 0.9973025464985387, 0.9973560530093991, 0.9974042088691735, 0.9974475491429704, 0.9974865553893877, 0.9975216610111632, 0.9975532560707613, 0.9975816916243995, 0.9976072836226738, 0.9976303164211208, 0.997651045939723, 0.997669702506465, 0.9976864934165328, 0.9977016052355938, 0.9977152058727488, 0.9977274464461882, 0.9977384629622836, 0.9977483778267696, 0.997757301204807, 0.9977653322450406, 0.9977725601812508, 0.9977790653238401, 0.9977849199521704, 0.9977901891176677, 0.9977949313666152, 0.9977991993906681], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.056048804593373476, 0.11314894342996984, 0.17293882953689754, 0.22927158203124995, 0.28079304840455566, 0.32817055126364825, 0.3700015512483527, 0.4087673505662885, 0.44341654736206926, 0.47454890782729303, 0.5023818382512655, 0.5286024681968468, 0.5514928802627043, 0.5708054533527742, 0.5905131645592889, 0.6081006788168389, 0.623416746336134, 0.6391147695564362, 0.6526264618402504, 0.6640901546057736, 0.6744705722683138, 0.6838576582549614, 0.6938564756843597, 0.7022041736072642, 0.7107761720033149, 0.7174788164746702, 0.7229587915666609, 0.7294542986581123, 0.7339493926063071, 0.7373946265384475, 0.742391692028428, 0.7466824138741545, 0.7498503217713776, 0.7531429510211977, 0.7562610377903128, 0.7591668840771701, 0.7619643687680977, 0.7646234418216343, 0.7676565207381154, 0.7694432618307196, 0.7707062963404188, 0.7724738220621751, 0.7738154541161232, 0.7756240089492398, 0.7769322664692254, 0.7773294777458721, 0.7780186167559235, 0.7775503570664004, 0.7786570083420646, 0.7785328890684757, 0.7800722194669746, 0.781542036535714, 0.7823420285711485, 0.7823519545732205, 0.7836202712111544, 0.7840924284838643, 0.7851714021735049, 0.7858424502562298, 0.7850242008537243, 0.7853772906986982, 0.7851569326755152, 0.7862068161681294, 0.7870968532071899, 0.7874390198254769, 0.7872901620823871, 0.7871358940857448, 0.7868271839599263, 0.7870111530168704, 0.7876467694075779, 0.7886134201211876, 0.7891691114769153, 0.7899133743220701, 0.7899941848567306, 0.7903395870600636, 0.790233350963244, 0.7901925965803984, 0.7908030373647532, 0.7916555508346032, 0.791474635481941, 0.7915265377564125, 0.7913626417462081, 0.7909842312519336, 0.7913038710032463, 0.791312844077997, 0.7914591681509955, 0.7926509595907905, 0.7937825480255367, 0.7949230479293083, 0.7960359765701125, 0.7970620264093362, 0.7979854712646375, 0.7988531927281587, 0.7996097279828278, 0.80029060971203, 0.800903403268312, 0.8014549174689658, 0.8019512802495542, 0.8023980067520837, 0.8028000606043604, 0.8031619090714093, 0.8034875726917533, 0.8037806699500629, 0.8040444574825416, 0.8042818662617724, 0.8044955341630802, 0.8046878352742571, 0.8048609062743164, 0.8050166701743697, 0.8051568576844177, 0.805283026443461, 0.8053965783265998, 0.8054987750214249, 0.8055907520467673, 0.8056735313695755, 0.805748032760103, 0.8058150840115776, 0.8058754301379049, 0.8059297416515994, 0.8059786220139245, 0.806022614340017, 0.8060622074335002, 0.8060978412176352, 0.8061299116233567, 0.806158774988506, 0.8061847520171403, 0.8062081313429112, 0.8062291727361051, 0.8062481099899795, 0.8062651535184665, 0.8062804926941048, 0.8062942979521793, 0.8063067226844464, 0.8063179049434867, 0.806327968976623, 0.8063370266064457, 0.8063451784732861, 0.8063525151534425, 0.8063591181655833, 0.8063650608765099, 0.806370409316344, 0.8063752229121945, 0.80637955514846], "moving_var_accuracy_train": [0.028997765180103104, 0.0572473190530389, 0.08601461808787388, 0.10955557263576363, 0.1268803220753454, 0.1383442291680854, 0.14413350455023263, 0.1474467033327843, 0.14718781555338067, 0.14470821602990616, 0.14009421929581628, 0.1354013536051074, 0.12980545494608284, 0.12226232529258513, 0.11640940461722368, 0.10936971412260346, 0.10255457731356742, 0.09667056594681435, 0.09040138255414507, 0.08378717817175438, 0.07760565236579216, 0.07170515279294008, 0.06656553883780807, 0.0613897091397027, 0.05672040486481172, 0.05209063394900406, 0.047727855769554045, 0.04373450977714333, 0.03994596893460497, 0.03645319502386418, 0.03337809583715723, 0.03048581246570036, 0.027774913895784438, 0.025280006936486736, 0.022986650454466573, 0.020961826177869926, 0.019072747010519794, 0.017362592047622562, 0.015825364773004218, 0.01433755016668855, 0.012998727887337485, 0.01181940382496343, 0.010711631682300028, 0.00971712850590951, 0.0088168574247811, 0.007984268023760567, 0.00724333523558184, 0.006521563621533461, 0.005893077009159728, 0.005315062606540555, 0.004836129730601104, 0.004384639531604641, 0.003969560065992656, 0.003577836953026066, 0.0032307640892112006, 0.002925042848307291, 0.002662181140672395, 0.0024171872254560214, 0.0021755051421279495, 0.001975743678985356, 0.0017801384213498516, 0.0016170121373228147, 0.0014589581505028923, 0.0013154046347941641, 0.001195187997327422, 0.0010791689570776887, 0.0009736393291095172, 0.0008832475324737958, 0.0008044137140006418, 0.000736953640929686, 0.0006671790942107169, 0.0006039684734577418, 0.0005446467664065113, 0.0004945539883870978, 0.00044516434617172275, 0.00040147149310338113, 0.00036777440877254865, 0.00033869149540158045, 0.0003064151748671091, 0.00027606869028332586, 0.00024931007260925057, 0.00022439128493147563, 0.0002021854738474739, 0.00018202886138710383, 0.00016477836460296132, 0.00016551955910070474, 0.00016413908848759493, 0.0001604581743056992, 0.00015536286519242544, 0.0001489054523466114, 0.00014159753906049097, 0.00013361434007757778, 0.00012528716874591665, 0.00011680808163512765, 0.00010840747358029504, 0.00010022368831029645, 9.235345877057183e-05, 8.486134571947167e-05, 7.778722973654968e-05, 7.11522418200051e-05, 6.496344303426396e-05, 5.9217503301807626e-05, 5.390358067411258e-05, 4.900556304571472e-05, 4.4503802496744076e-05, 4.037644680910631e-05, 3.660045202344534e-05, 3.3152343236253015e-05, 3.0008777408900985e-05, 2.7146951149992234e-05, 2.4544887735397895e-05, 2.218163063918605e-05, 2.0037365233903083e-05, 1.809348581400764e-05, 1.6332621486437717e-05, 1.4738631583396929e-05, 1.3296578943995649e-05, 1.1992687569936194e-05, 1.0814289694418063e-05, 9.749766138971404e-06, 8.788482910410332e-06, 7.920726261491512e-06, 7.13763786546135e-06, 6.431151305311593e-06, 5.7939307281615e-06, 5.219312243584013e-06, 4.701248435698927e-06, 4.234256199472419e-06, 3.8133679914733184e-06, 3.4340864960039806e-06, 3.0923426423827576e-06, 2.7844568628876136e-06, 2.5071034492407892e-06, 2.2572778451566786e-06, 2.032266700721384e-06, 1.8296205091143475e-06, 1.6471286457596453e-06, 1.4827966331046337e-06, 1.3348254598501416e-06, 1.2015927908104637e-06, 1.0816359120551397e-06, 9.736362651134603e-07], "duration": 49146.671577, "accuracy_train": [0.5676243395856405, 0.6450681971553156, 0.7346606868886121, 0.7751099849460132, 0.797819118563123, 0.8113464377999261, 0.8120683153954411, 0.8356181074658545, 0.8373844995847176, 0.8450832655615541, 0.8441292335732743, 0.8680256350359912, 0.875560198643411, 0.8539649735257475, 0.8988585502030271, 0.8854678559892949, 0.8959753656792175, 0.9237605334648394, 0.919714053559893, 0.909019450500646, 0.917505522679033, 0.92064447357189, 0.9414774464170359, 0.9345481424764673, 0.9468950431432264, 0.9395003489525655, 0.9396176878576044, 0.945406226928756, 0.9422672760358989, 0.9443838824289406, 0.95677728607189, 0.9554977332502769, 0.9534290717861758, 0.9543347983573275, 0.9549643909768365, 0.9641705383098007, 0.9624964311669435, 0.9661229423334257, 0.9710293668097084, 0.9611474843692323, 0.9644277466315985, 0.9717959449404762, 0.9675645345953304, 0.9709134698574198, 0.9728211733457919, 0.9708204639050388, 0.9750747652500923, 0.9576626450604466, 0.969078044250646, 0.9656844084533037, 0.979771926333518, 0.9769119932978036, 0.9760280762619971, 0.9691459962739941, 0.9731924761789406, 0.9772607656192323, 0.9829112377145626, 0.981934314726375, 0.9687514419527501, 0.9822362235834257, 0.9742606026785714, 0.9829123191791252, 0.9777029043812293, 0.9770751142026578, 0.9837007068452381, 0.9798413203096161, 0.969078765227021, 0.982515601928756, 0.98486328125, 0.9876309293097084, 0.9834224099644703, 0.9837246793097084, 0.9815626514050388, 0.9854216774524732, 0.9800037202380952, 0.9822594750715209, 0.9880025926310447, 0.9896298363095238, 0.9855150438930418, 0.9835393883813216, 0.9849798991786637, 0.9825853563930418, 0.9838638277500923, 0.9815851819167589, 0.9855847983573275, 0.9964890252976191, 0.9970238095238095, 0.9972330729166666, 0.99755859375, 0.9976748511904762, 0.9978143601190477, 0.9978376116071429, 0.9978608630952381, 0.9978376116071429, 0.9978376116071429, 0.9978376116071429, 0.9978376116071429, 0.9978376116071429, 0.9978376116071429, 0.9978376116071429, 0.9978376116071429, 0.9978376116071429, 0.9978376116071429, 0.9978376116071429, 0.9978376116071429, 0.9978376116071429, 0.9978376116071429, 0.9978376116071429, 0.9978376116071429, 0.9978376116071429, 0.9978376116071429, 0.9978376116071429, 0.9978376116071429, 0.9978376116071429, 0.9978376116071429, 0.9978376116071429, 0.9978376116071429, 0.9978376116071429, 0.9978376116071429, 0.9978376116071429, 0.9978376116071429, 0.9978376116071429, 0.9978376116071429, 0.9978376116071429, 0.9978376116071429, 0.9978376116071429, 0.9978376116071429, 0.9978376116071429, 0.9978376116071429, 0.9978376116071429, 0.9978376116071429, 0.9978376116071429, 0.9978376116071429, 0.9978376116071429, 0.9978376116071429, 0.9978376116071429, 0.9978376116071429, 0.9978376116071429, 0.9978376116071429, 0.9978376116071429, 0.9978376116071429, 0.9978376116071429], "end": "2016-01-30 05:34:10.960000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0], "moving_var_accuracy_valid": [0.028273216467115482, 0.05478972751683117, 0.08148422909126254, 0.101896217214446, 0.11559674896830245, 0.12423872406594218, 0.12756334469683106, 0.1283320949979742, 0.1263039870455115, 0.12239660315419001, 0.11712899098264226, 0.11160378479706606, 0.10515914499826225, 0.09800000981266967, 0.09169555376039719, 0.0853098843041932, 0.07889013319207433, 0.07321897127009325, 0.06754016659843629, 0.06196889620499343, 0.05674178422213306, 0.051860662249806184, 0.04757438317470354, 0.04344410140274175, 0.03976100367098463, 0.03618923229007068, 0.03284058020414317, 0.02993624669510471, 0.027124474852022115, 0.024518854098444447, 0.022291704660200393, 0.020228226839796904, 0.018295724919824813, 0.01656372509423327, 0.014994854770707522, 0.01357136477742235, 0.012284661585043882, 0.011119831452075894, 0.010090644416290727, 0.009110311968249655, 0.008213638076978908, 0.007420391593874649, 0.006694552223600815, 0.006054534836500239, 0.005464485192497607, 0.004919456664432504, 0.004431785211165824, 0.003990580094280732, 0.0036025441782660214, 0.0032424284107861055, 0.002939511412389177, 0.002665003531090276, 0.002404263063292078, 0.0021638376436925242, 0.001961931523169813, 0.0017677447632643872, 0.0016014479449443799, 0.0014453559002138997, 0.0013068460989548154, 0.0011772835410069475, 0.0010599922058316833, 0.0009639132833810881, 0.0008746514484210764, 0.0007882400055309986, 0.000709615432626999, 0.0006388680768973915, 0.0005758389866836979, 0.0005185596895405443, 0.00047033979435171457, 0.0004317155373356419, 0.0003913231195475513, 0.0003571761522368982, 0.00032151731009581736, 0.00029043930322484104, 0.0002614969478767641, 0.00023536220136657774, 0.0002151797227907526, 0.00020020276345815826, 0.00018047706039580217, 0.00016245359897107986, 0.00014644999619342047, 0.00013309374709367158, 0.00012070389851987741, 0.00010863423331252401, 9.796350659032142e-05, 0.00010095045745500777, 0.00010237984318037032, 0.00010384851913686116, 0.00010461115865887356, 0.00010362504724612474, 0.0001009372961285545, 9.762003135999719e-05, 9.300913854801214e-05, 8.788062405566273e-05, 8.247220513368236e-05, 7.696249584201865e-05, 7.14836303473974e-05, 6.613134842521793e-05, 6.097303928386991e-05, 5.605414417343366e-05, 5.1403240898630385e-05, 4.703607083422481e-05, 4.295871851142286e-05, 3.917011301638318e-05, 3.566398746318795e-05, 3.243040617310804e-05, 2.945694769535072e-05, 2.672961445885396e-05, 2.423352585472959e-05, 2.1953440271083054e-05, 1.9874142515454136e-05, 1.7980725743807014e-05, 1.625879112814392e-05, 1.469458376189079e-05, 1.3275079500416327e-05, 1.1988034383293528e-05, 1.0822005839628425e-05, 9.766352920343622e-06, 8.811221236698465e-06, 7.947517035823867e-06, 7.166873849705632e-06, 6.461614363881031e-06, 5.824709525801153e-06, 5.249736417850696e-06, 4.730836030215648e-06, 4.262671763055601e-06, 3.840389248797869e-06, 3.459577900176823e-06, 3.1162344469287197e-06, 2.8067286150192064e-06, 2.527771019871805e-06, 2.2763832836317845e-06, 2.049870341523805e-06, 1.8457948702381357e-06, 1.661953749136357e-06, 1.4963564506195684e-06, 1.3472052474390564e-06, 1.2128771206191204e-06, 1.091907250875623e-06, 9.829739780659758e-07, 8.84885116604489e-07, 7.965655193835795e-07], "accuracy_test": 0.8040198501275511, "start": "2016-01-29 15:55:04.288000", "learning_rate_per_epoch": [0.0015392848290503025, 0.0015392848290503025, 0.0015392848290503025, 0.0015392848290503025, 0.0015392848290503025, 0.0015392848290503025, 0.0015392848290503025, 0.0015392848290503025, 0.0015392848290503025, 0.0015392848290503025, 0.0015392848290503025, 0.0015392848290503025, 0.0015392848290503025, 0.0015392848290503025, 0.0015392848290503025, 0.0015392848290503025, 0.0015392848290503025, 0.0015392848290503025, 0.0015392848290503025, 0.0015392848290503025, 0.0015392848290503025, 0.0015392848290503025, 0.0015392848290503025, 0.0015392848290503025, 0.0015392848290503025, 0.0015392848290503025, 0.0015392848290503025, 0.0015392848290503025, 0.0015392848290503025, 0.0015392848290503025, 0.0015392848290503025, 0.0015392848290503025, 0.0015392848290503025, 0.0015392848290503025, 0.0015392848290503025, 0.0015392848290503025, 0.0015392848290503025, 0.0015392848290503025, 0.0015392848290503025, 0.0015392848290503025, 0.0015392848290503025, 0.0015392848290503025, 0.0015392848290503025, 0.0015392848290503025, 0.0015392848290503025, 0.0015392848290503025, 0.0015392848290503025, 0.0015392848290503025, 0.0015392848290503025, 0.0015392848290503025, 0.0015392848290503025, 0.0015392848290503025, 0.0015392848290503025, 0.0015392848290503025, 0.0015392848290503025, 0.0015392848290503025, 0.0015392848290503025, 0.0015392848290503025, 0.0015392848290503025, 0.0015392848290503025, 0.0015392848290503025, 0.0015392848290503025, 0.0015392848290503025, 0.0015392848290503025, 0.0015392848290503025, 0.0015392848290503025, 0.0015392848290503025, 0.0015392848290503025, 0.0015392848290503025, 0.0015392848290503025, 0.0015392848290503025, 0.0015392848290503025, 0.0015392848290503025, 0.0015392848290503025, 0.0015392848290503025, 0.0015392848290503025, 0.0015392848290503025, 0.0015392848290503025, 0.0015392848290503025, 0.0015392848290503025, 0.0015392848290503025, 0.0015392848290503025, 0.0015392848290503025, 0.0015392848290503025, 0.0001539284858154133, 0.0001539284858154133, 0.0001539284858154133, 0.0001539284858154133, 0.0001539284858154133, 0.0001539284858154133, 0.0001539284858154133, 1.539284858154133e-05, 1.5392848808915005e-06, 1.5392848240480816e-07, 1.539284788520945e-08, 1.5392848107254054e-09, 1.5392848107254054e-10, 1.539284845419875e-11, 1.5392848671039183e-12, 1.539284921314027e-13, 1.5392848535513912e-14, 1.5392848111997438e-15, 1.5392847847299642e-16, 1.5392848509044132e-17, 1.5392848922634438e-18, 1.5392848922634438e-19, 1.5392848922634438e-20, 1.5392848720686047e-21, 1.5392848468250557e-22, 1.539284878379492e-23, 1.5392848586579693e-24, 1.5392848093541627e-25, 1.539284870983921e-26, 1.539284870983921e-27, 1.5392848228356723e-28, 1.5392847626503615e-29, 1.539284743842452e-30, 1.539284696822678e-31, 1.5392847262100367e-32, 1.5392846894758382e-33, 1.5392847124347122e-34, 1.5392847411333048e-35, 1.539284705260064e-36, 1.5392846604185132e-37, 1.5392846043665747e-38, 1.5392843241068818e-39, 1.5392843241068818e-40, 1.5393263630608116e-41, 1.538625713828649e-42, 1.5414283107572988e-43, 1.5414283107572988e-44, 1.401298464324817e-45, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "accuracy_train_first": 0.5676243395856405, "accuracy_train_last": 0.9978376116071429, "batch_size_eval": 1024, "accuracy_train_std": [0.028469763052978193, 0.02430414794125942, 0.02506202437072161, 0.02680322804690785, 0.02892729406667766, 0.031917037247688, 0.03344266049822581, 0.03273902764305879, 0.03342261452296531, 0.03189580813636159, 0.03081764899025036, 0.031810692585848656, 0.03211991155727129, 0.02931490456546059, 0.029064109383620907, 0.027352637444333906, 0.026432800091438318, 0.021794492279981155, 0.02228550229689323, 0.025183660762295477, 0.02125167196457485, 0.021511400154925038, 0.01913024812152514, 0.018912645320896447, 0.017167500799172935, 0.01845990956385329, 0.0169064185074575, 0.015799026574996423, 0.01754180478051731, 0.01600261882611727, 0.013511022334368012, 0.014625143378220044, 0.013567530661346366, 0.014126258211548176, 0.01141783704765586, 0.011109856777634835, 0.01231318755568674, 0.009175815661073413, 0.008772157916414346, 0.012904015514025176, 0.010628675823040364, 0.009703116493528232, 0.008907161080357093, 0.009130288843966414, 0.008631066248495068, 0.00807916077961975, 0.009236509746362118, 0.01062840174521596, 0.00874396739835114, 0.009767789219623806, 0.007491439463027961, 0.00877466314543396, 0.008060133435712803, 0.008058340367352099, 0.007631395239412279, 0.006904958837220707, 0.005126835491638513, 0.005846573012789898, 0.00863657991633512, 0.00563209863930659, 0.008010027971228014, 0.005082263203563558, 0.0070797194603161116, 0.006176514713752132, 0.00521771889321047, 0.006213382747777686, 0.006884207060758218, 0.004971484425567761, 0.0058758217218564885, 0.004560292793439637, 0.006802275908790689, 0.005528626544388768, 0.004821049815981326, 0.005068069585230509, 0.006548339011502695, 0.00558220068709555, 0.004617244723720398, 0.004928876749343789, 0.004771104309262473, 0.005890767937579769, 0.004209318055164647, 0.004785801917011246, 0.005338074012976984, 0.005535903598477641, 0.004440247162261437, 0.0018948179624143728, 0.0017174651518433212, 0.001490454846590155, 0.001449259862560214, 0.0014128029697696531, 0.0013943140206998652, 0.0014060903676284275, 0.0013684839071393378, 0.0014060903676284275, 0.0014060903676284275, 0.0014060903676284275, 0.0014060903676284275, 0.0014060903676284275, 0.0014060903676284275, 0.0014060903676284275, 0.0014060903676284275, 0.0014060903676284275, 0.0014060903676284275, 0.0014060903676284275, 0.0014060903676284275, 0.0014060903676284275, 0.0014060903676284275, 0.0014060903676284275, 0.0014060903676284275, 0.0014060903676284275, 0.0014060903676284275, 0.0014060903676284275, 0.0014060903676284275, 0.0014060903676284275, 0.0014060903676284275, 0.0014060903676284275, 0.0014060903676284275, 0.0014060903676284275, 0.0014060903676284275, 0.0014060903676284275, 0.0014060903676284275, 0.0014060903676284275, 0.0014060903676284275, 0.0014060903676284275, 0.0014060903676284275, 0.0014060903676284275, 0.0014060903676284275, 0.0014060903676284275, 0.0014060903676284275, 0.0014060903676284275, 0.0014060903676284275, 0.0014060903676284275, 0.0014060903676284275, 0.0014060903676284275, 0.0014060903676284275, 0.0014060903676284275, 0.0014060903676284275, 0.0014060903676284275, 0.0014060903676284275, 0.0014060903676284275, 0.0014060903676284275, 0.0014060903676284275], "accuracy_test_std": 0.007802438353813144, "error_valid": [0.4395119540662651, 0.3729498070406627, 0.288952195500753, 0.26373364551957834, 0.2555137542356928, 0.2454319230045181, 0.2535194488893072, 0.24234045557228923, 0.24474068147590367, 0.24525984798569278, 0.2471217879329819, 0.23541186229292166, 0.24249341114457834, 0.25538138883659633, 0.23211743458207834, 0.23361169286521077, 0.23873864599021077, 0.21960302146084332, 0.22576830760542166, 0.2327366105045181, 0.23210566876882532, 0.23165856786521077, 0.2161541674510542, 0.22266654508659633, 0.21207584243222888, 0.22219738328313254, 0.22772143260542166, 0.21208613751882532, 0.22559476185993976, 0.23159826807228923, 0.21263471856174698, 0.21470108951430722, 0.22163850715361444, 0.21722338573042166, 0.21567618128765065, 0.21468049934111444, 0.2128582690135542, 0.2114449006965362, 0.2050457690135542, 0.21447606833584332, 0.21792639307228923, 0.2116184464420181, 0.21410985739834332, 0.20809899755271077, 0.21129341585090367, 0.21909562076430722, 0.21577913215361444, 0.22666398013930722, 0.21138313017695776, 0.22258418439382532, 0.2060738069465362, 0.20522960984563254, 0.21045804310993976, 0.21755871140813254, 0.20496487904743976, 0.21165815606174698, 0.20511783461972888, 0.20811811699924698, 0.22234004376882532, 0.2114449006965362, 0.21682628953313254, 0.20434423239834332, 0.2048928134412651, 0.20948148060993976, 0.21404955760542166, 0.2142525178840362, 0.21595120717243976, 0.21133312547063254, 0.2066326830760542, 0.20268672345632532, 0.2058296663215362, 0.2033882600715362, 0.20927852033132532, 0.20655179310993976, 0.21072277390813254, 0.21017419286521077, 0.2037029955760542, 0.20067182793674698, 0.2101536026920181, 0.20800634177334332, 0.21011242234563254, 0.2124214631965362, 0.20581937123493976, 0.20860639824924698, 0.2072239151920181, 0.1966229174510542, 0.19603315606174698, 0.19481245293674698, 0.19394766566265065, 0.19370352503765065, 0.19370352503765065, 0.19333731410015065, 0.19358145472515065, 0.19358145472515065, 0.19358145472515065, 0.19358145472515065, 0.19358145472515065, 0.19358145472515065, 0.19358145472515065, 0.19358145472515065, 0.19358145472515065, 0.19358145472515065, 0.19358145472515065, 0.19358145472515065, 0.19358145472515065, 0.19358145472515065, 0.19358145472515065, 0.19358145472515065, 0.19358145472515065, 0.19358145472515065, 0.19358145472515065, 0.19358145472515065, 0.19358145472515065, 0.19358145472515065, 0.19358145472515065, 0.19358145472515065, 0.19358145472515065, 0.19358145472515065, 0.19358145472515065, 0.19358145472515065, 0.19358145472515065, 0.19358145472515065, 0.19358145472515065, 0.19358145472515065, 0.19358145472515065, 0.19358145472515065, 0.19358145472515065, 0.19358145472515065, 0.19358145472515065, 0.19358145472515065, 0.19358145472515065, 0.19358145472515065, 0.19358145472515065, 0.19358145472515065, 0.19358145472515065, 0.19358145472515065, 0.19358145472515065, 0.19358145472515065, 0.19358145472515065, 0.19358145472515065, 0.19358145472515065, 0.19358145472515065], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.030598628301553733, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 32, "valid_ratio": 0.15, "learning_rate": 0.001539284820869836, "optimization": "rmsprop", "nb_data_augmentation": 0, "learning_rate_decay_method": "discrete", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 6.028079541479544e-08, "rotation_range": [0, 0], "momentum": 0.7100964561360463}, "accuracy_valid_max": 0.8066626858998494, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8064185452748494, "accuracy_valid_std": [0.011430299853630636, 0.015007307199595176, 0.01439770450179981, 0.008885325198789432, 0.007611758137274661, 0.009048238860640772, 0.009450146435755825, 0.011776945799300973, 0.005772060837000571, 0.010407200088079606, 0.012135220001472104, 0.009031630613234889, 0.01291771230475599, 0.008049864996368557, 0.015203169356474141, 0.006953803266918146, 0.0070148782053835335, 0.006487131045799246, 0.011466257388545216, 0.005457149046436276, 0.010658923290676924, 0.005721671668957292, 0.008082246502235501, 0.012134723045475037, 0.009341336539510693, 0.007326592791412638, 0.011549254506078799, 0.00736355913670389, 0.008712115478839265, 0.013341057404771506, 0.012118061903800244, 0.015756954883382458, 0.011454487528404977, 0.010734068453046749, 0.010240818724898217, 0.013312706308178011, 0.010688243676520144, 0.007280829454171508, 0.008401144432937376, 0.011899896860352125, 0.016929073490704525, 0.01158165548667531, 0.007576279178344061, 0.014557782006237033, 0.015261048710993113, 0.01217405368001227, 0.010543831318256728, 0.012347192280823554, 0.00956050057207532, 0.006974392206842006, 0.01092052236074134, 0.009602769665383372, 0.007789152212156537, 0.0066240973497718805, 0.010269197725152805, 0.007561683572997319, 0.009262910518397479, 0.00747189111802527, 0.012905988934641606, 0.005805047168930224, 0.006913469122276718, 0.009797708450073958, 0.003971405344990823, 0.011352349575537095, 0.0083490964442301, 0.007825143223907887, 0.007316125725113107, 0.007989989867128778, 0.009218033071527153, 0.012666441365826448, 0.009518261929666713, 0.011884250317281847, 0.01116567773883133, 0.007287690852727115, 0.009527457186691046, 0.014952451318651486, 0.012540406528361135, 0.00709256066015363, 0.010607839778217403, 0.006476453503664327, 0.0099538179267722, 0.008872561153054074, 0.011054643710590844, 0.01087138930250814, 0.011806962730219027, 0.007838124675727118, 0.008433059441274825, 0.009114181258342173, 0.008504422685350881, 0.00917798247124248, 0.009255586081769409, 0.009177914006897003, 0.008991518931168055, 0.008991518931168055, 0.008991518931168055, 0.008991518931168055, 0.008991518931168055, 0.008991518931168055, 0.008991518931168055, 0.008991518931168055, 0.008991518931168055, 0.008991518931168055, 0.008991518931168055, 0.008991518931168055, 0.008991518931168055, 0.008991518931168055, 0.008991518931168055, 0.008991518931168055, 0.008991518931168055, 0.008991518931168055, 0.008991518931168055, 0.008991518931168055, 0.008991518931168055, 0.008991518931168055, 0.008991518931168055, 0.008991518931168055, 0.008991518931168055, 0.008991518931168055, 0.008991518931168055, 0.008991518931168055, 0.008991518931168055, 0.008991518931168055, 0.008991518931168055, 0.008991518931168055, 0.008991518931168055, 0.008991518931168055, 0.008991518931168055, 0.008991518931168055, 0.008991518931168055, 0.008991518931168055, 0.008991518931168055, 0.008991518931168055, 0.008991518931168055, 0.008991518931168055, 0.008991518931168055, 0.008991518931168055, 0.008991518931168055, 0.008991518931168055, 0.008991518931168055, 0.008991518931168055, 0.008991518931168055, 0.008991518931168055], "accuracy_valid": [0.5604880459337349, 0.6270501929593373, 0.711047804499247, 0.7362663544804217, 0.7444862457643072, 0.7545680769954819, 0.7464805511106928, 0.7576595444277108, 0.7552593185240963, 0.7547401520143072, 0.7528782120670181, 0.7645881377070783, 0.7575065888554217, 0.7446186111634037, 0.7678825654179217, 0.7663883071347892, 0.7612613540097892, 0.7803969785391567, 0.7742316923945783, 0.7672633894954819, 0.7678943312311747, 0.7683414321347892, 0.7838458325489458, 0.7773334549134037, 0.7879241575677711, 0.7778026167168675, 0.7722785673945783, 0.7879138624811747, 0.7744052381400602, 0.7684017319277108, 0.787365281438253, 0.7852989104856928, 0.7783614928463856, 0.7827766142695783, 0.7843238187123494, 0.7853195006588856, 0.7871417309864458, 0.7885550993034638, 0.7949542309864458, 0.7855239316641567, 0.7820736069277108, 0.7883815535579819, 0.7858901426016567, 0.7919010024472892, 0.7887065841490963, 0.7809043792356928, 0.7842208678463856, 0.7733360198606928, 0.7886168698230422, 0.7774158156061747, 0.7939261930534638, 0.7947703901543675, 0.7895419568900602, 0.7824412885918675, 0.7950351209525602, 0.788341843938253, 0.7948821653802711, 0.791881883000753, 0.7776599562311747, 0.7885550993034638, 0.7831737104668675, 0.7956557676016567, 0.7951071865587349, 0.7905185193900602, 0.7859504423945783, 0.7857474821159638, 0.7840487928275602, 0.7886668745293675, 0.7933673169239458, 0.7973132765436747, 0.7941703336784638, 0.7966117399284638, 0.7907214796686747, 0.7934482068900602, 0.7892772260918675, 0.7898258071347892, 0.7962970044239458, 0.799328172063253, 0.7898463973079819, 0.7919936582266567, 0.7898875776543675, 0.7875785368034638, 0.7941806287650602, 0.791393601750753, 0.7927760848079819, 0.8033770825489458, 0.803966843938253, 0.805187547063253, 0.8060523343373494, 0.8062964749623494, 0.8062964749623494, 0.8066626858998494, 0.8064185452748494, 0.8064185452748494, 0.8064185452748494, 0.8064185452748494, 0.8064185452748494, 0.8064185452748494, 0.8064185452748494, 0.8064185452748494, 0.8064185452748494, 0.8064185452748494, 0.8064185452748494, 0.8064185452748494, 0.8064185452748494, 0.8064185452748494, 0.8064185452748494, 0.8064185452748494, 0.8064185452748494, 0.8064185452748494, 0.8064185452748494, 0.8064185452748494, 0.8064185452748494, 0.8064185452748494, 0.8064185452748494, 0.8064185452748494, 0.8064185452748494, 0.8064185452748494, 0.8064185452748494, 0.8064185452748494, 0.8064185452748494, 0.8064185452748494, 0.8064185452748494, 0.8064185452748494, 0.8064185452748494, 0.8064185452748494, 0.8064185452748494, 0.8064185452748494, 0.8064185452748494, 0.8064185452748494, 0.8064185452748494, 0.8064185452748494, 0.8064185452748494, 0.8064185452748494, 0.8064185452748494, 0.8064185452748494, 0.8064185452748494, 0.8064185452748494, 0.8064185452748494, 0.8064185452748494, 0.8064185452748494, 0.8064185452748494], "seed": 830731955, "model": "residualv3", "loss_std": [0.34889787435531616, 0.20160368084907532, 0.1911601573228836, 0.18486352264881134, 0.1799425482749939, 0.17371471226215363, 0.16816605627536774, 0.16188079118728638, 0.15478485822677612, 0.14606736600399017, 0.13959191739559174, 0.1319875568151474, 0.12716038525104523, 0.114452064037323, 0.10523983836174011, 0.10913415253162384, 0.10200505703687668, 0.09681106358766556, 0.09453391283750534, 0.09420941770076752, 0.08570723980665207, 0.08431866765022278, 0.0835314467549324, 0.08304427564144135, 0.08015978336334229, 0.07878343760967255, 0.08210659772157669, 0.07177938520908356, 0.07228358834981918, 0.07134891301393509, 0.0653466135263443, 0.07186021655797958, 0.06377676874399185, 0.06515543907880783, 0.0631999671459198, 0.06534907966852188, 0.06525912135839462, 0.06448987871408463, 0.05975720286369324, 0.05807080492377281, 0.059882752597332, 0.058370545506477356, 0.0558675080537796, 0.062060900032520294, 0.060769468545913696, 0.05233171954751015, 0.04848721623420715, 0.05667576566338539, 0.05079580843448639, 0.058826152235269547, 0.05140669271349907, 0.05529376119375229, 0.05530877783894539, 0.05374590680003166, 0.05255299061536789, 0.04698564112186432, 0.05225222930312157, 0.04259723797440529, 0.052504248917102814, 0.04284849017858505, 0.047905437648296356, 0.04874520003795624, 0.051999449729919434, 0.04917633533477783, 0.04669690504670143, 0.04562648385763168, 0.04690742492675781, 0.04068337753415108, 0.04823281988501549, 0.03878461942076683, 0.0456714890897274, 0.043566349893808365, 0.04619896784424782, 0.047411613166332245, 0.045718248933553696, 0.039427999407052994, 0.039694901555776596, 0.03484392538666725, 0.03810834139585495, 0.03883878141641617, 0.039411723613739014, 0.04116449132561684, 0.04162197560071945, 0.05034470930695534, 0.045088864862918854, 0.02343244105577469, 0.0031247709412127733, 0.0004573525511659682, 0.0003236279299017042, 0.0002561631554272026, 0.00021516768902074546, 0.00018635830201674253, 0.00016807846259325743, 0.00016559903451707214, 0.0001653493964113295, 0.0001653303042985499, 0.00016532983863726258, 0.00016532976587768644, 0.00016532980953343213, 0.00016532970767002553, 0.00016532970767002553, 0.00016532983863726258, 0.00016532982408534735, 0.00016532967856619507, 0.0001653298531891778, 0.00016532970767002553, 0.00016532964946236461, 0.00016532967856619507, 0.00016532976587768644, 0.00016532950394321233, 0.00016532962035853416, 0.0001653296931181103, 0.0001653298531891778, 0.0001653296931181103, 0.00016532957670278847, 0.0001653296931181103, 0.00016532973677385598, 0.0001653296931181103, 0.00016532982408534735, 0.00016532991139683872, 0.00016532976587768644, 0.00016532970767002553, 0.00016532973677385598, 0.00016532980953343213, 0.00016532973677385598, 0.0001653295330470428, 0.00016532991139683872, 0.00016532966401427984, 0.00016532972222194076, 0.0001653297513257712, 0.0001653297949815169, 0.00016532967856619507, 0.00016532980953343213, 0.0001653297949815169, 0.00016532980953343213, 0.00016532982408534735, 0.00016532973677385598, 0.00016532962035853416, 0.00016532998415641487, 0.0001653297513257712, 0.0001653296931181103, 0.00016532972222194076]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:22 2016", "state": "available"}], "summary": "a2d16412021f5486a5918ef3fc3333d3"}