{"content": {"hp_model": {"f0": 64, "f1": 16, "f2": 64, "f3": 64, "nonlin": "leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.017376960772471813, 0.013014821753184623, 0.01065069456910127, 0.00987247745199585, 0.014421723844310562, 0.008470909585310589, 0.009932668520376298, 0.009694255012979644, 0.009694255012979644, 0.014858519584670838, 0.016246363223086955, 0.02663721459669463, 0.024537092781608247, 0.017373524112706203, 0.018084813701079568, 0.017181372749381627, 0.020926691656999234, 0.025909143921139105, 0.01975715487568121, 0.011654998303007823, 0.02191335136394847, 0.013987231258019464, 0.021180198373710622, 0.017739911272617953, 0.01939775504102501, 0.019343072805372653, 0.027607328605696712, 0.01956531805539285, 0.01990863299726022, 0.01733728708082999, 0.012776522612225108, 0.016567519452234402, 0.017154480824611292, 0.016933660278990067, 0.022363044454171543, 0.018446724979283024, 0.014917065069682694, 0.017556124102048716, 0.017715880228165375, 0.023744273304165387, 0.02374515656622286, 0.01767660286958615, 0.015363486255504148, 0.015707040930406115, 0.01275352660046396, 0.018173881055105392, 0.023348408558907855, 0.020345712086317844, 0.020700680144519878, 0.019971918632954944, 0.022786670883142228, 0.01840674848826504, 0.021510281541934583, 0.018601106484973182, 0.021719677521808433, 0.019500182992201413, 0.0194988032161058, 0.025156534152996, 0.022029097023226498, 0.021063029720068596, 0.020491107453548457, 0.023607400436536946, 0.02792595389217905, 0.024211471374357023, 0.024660176193505055, 0.01983868191665594, 0.023284895757059776, 0.02303613387799251, 0.023994630051323084, 0.02227260286169585, 0.023985470118608673, 0.023323740553326696, 0.023903895853988593, 0.02488234036530908, 0.023905035805788465, 0.023041900344750375, 0.021030751671060274, 0.0229640989539285, 0.022498020538357515, 0.020801924089018525, 0.022066944639266162, 0.02232323080232303, 0.021290151300829945, 0.020075836641946173, 0.024273445866680163, 0.020942879676783547, 0.021618067194374122, 0.024148925233248398, 0.021851198640940473, 0.021919814641567587, 0.02150209207536343, 0.02260134250058012, 0.021752583826432203, 0.02041607795683717, 0.02020769515123005, 0.021191513963652903, 0.020464399097559195, 0.022298027818930644, 0.02240754879553742, 0.022399374938081074, 0.02290090555300348, 0.022337184004323076, 0.02114976150814721, 0.022486501384960663, 0.022281790128656546, 0.022864725035181215, 0.022889598176445015, 0.023175151775985597, 0.023134837495025453, 0.023333188862259043, 0.02347667685186642, 0.023874239089260035, 0.024066057090192022, 0.02282179360065275, 0.02317713485864911, 0.02317713485864911, 0.022255023667522543, 0.022906992129932007, 0.02307403754599477, 0.023888432455049048, 0.023170879050544168, 0.024056536285273624, 0.02304866241581147, 0.022808132931687528, 0.022412291471120956, 0.022443282419178586, 0.023237665061345274, 0.02240450398149516, 0.02231544039109632, 0.02242824256877444, 0.022453903066175684, 0.022584685748229607, 0.02230055181418254, 0.022327026486320477], "moving_avg_accuracy_train": [0.011305143878045402, 0.023151699557608894, 0.03098618266801633, 0.03798134179831695, 0.0434749528715768, 0.04922134312797763, 0.05433489354086277, 0.05888597168746863, 0.06298426716822343, 0.08477517014524567, 0.12230694074838444, 0.1604676827719679, 0.20363427159148578, 0.2554629118176731, 0.30391311164388846, 0.3493915683539459, 0.39346531194883005, 0.436309835167476, 0.4769295192748517, 0.5130478178953086, 0.5479511906369701, 0.5817077598115529, 0.6143182734840982, 0.6439209967655333, 0.6707283530402073, 0.6953109191469931, 0.7188138976918729, 0.7413802328096365, 0.761645720539424, 0.7801335225164894, 0.79686089995061, 0.8135918277377103, 0.8281499359961468, 0.8423448370763034, 0.8552715269163382, 0.8673239303628089, 0.8785942705479658, 0.8889956321836454, 0.8989195436676618, 0.9079487202532767, 0.915837886099396, 0.923284510435885, 0.9301213309696775, 0.9364139783786621, 0.9423587040527007, 0.9477368589450497, 0.9525655726041161, 0.9569276909389426, 0.9608419716962389, 0.964576412919472, 0.9679490357644296, 0.9710262490034628, 0.9738329432995451, 0.9763798945053048, 0.9786698254416791, 0.980744714177273, 0.9826121140393076, 0.984311375105615, 0.9858569861069583, 0.9872503611569767, 0.9884997484043744, 0.9896334975222703, 0.9906538717283766, 0.9915838342579198, 0.9924161502368898, 0.9931675597667722, 0.9938438283436664, 0.9944524700628712, 0.9950048979077746, 0.9954974326705686, 0.9959453642547023, 0.9963461775316129, 0.996704584332023, 0.9970294756012016, 0.9973195525946529, 0.997585272186378, 0.9978267449677403, 0.9980440704709662, 0.9982396634238696, 0.9984133719326731, 0.9985720347394058, 0.9987148312654652, 0.9988433481389187, 0.9989590133250269, 0.9990631119925242, 0.9991568007932717, 0.9992411207139446, 0.9993170086425501, 0.9993853077782952, 0.9994467770004656, 0.999502099300419, 0.9995518893703771, 0.9995967004333394, 0.9996370303900054, 0.9996733273510049, 0.9997059946159044, 0.999735395154314, 0.9997618556388825, 0.9997856700749943, 0.9998071030674949, 0.9998263927607454, 0.9998437534846708, 0.9998593781362037, 0.9998734403225834, 0.9998860962903251, 0.9998974866612926, 0.9999077379951634, 0.999916964195647, 0.9999252677760823, 0.9999327409984741, 0.9999394668986267, 0.999945520208764, 0.9999509681878876, 0.9999558713690989, 0.999960284232189, 0.9999642558089701, 0.9999678302280731, 0.9999710472052658, 0.9999739424847391, 0.9999765482362653, 0.9999788934126387, 0.9999810040713748, 0.9999829036642374, 0.9999846132978136, 0.9999861519680323, 0.9999875367712291, 0.9999887830941062, 0.9999899047846956, 0.9999909143062261, 0.9999918228756035, 0.9999926405880432, 0.9999933765292388, 0.9999940388763149, 0.9999946349886834], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 765264571, "moving_var_accuracy_train": [0.0011502565029297672, 0.0022982987858577724, 0.00262088103773733, 0.00279918319528765, 0.002790882739377074, 0.002808983474249092, 0.0027634207072502436, 0.0026734894471938404, 0.002557304735102633, 0.0065751653345783425, 0.0185953530425802, 0.02984199782443659, 0.043627987554812855, 0.06344106032859165, 0.07822375106453426, 0.08901598618063794, 0.09759684143278244, 0.10435803581800204, 0.10877186086904875, 0.10963545823927236, 0.10963612127403533, 0.10892806280857734, 0.107606266945605, 0.1047325312821395, 0.10072698730786132, 0.09609301158462566, 0.09145522043049285, 0.08689285371326846, 0.08189977827827714, 0.07678598984793807, 0.07162563726555604, 0.06698238904055492, 0.06219159678107863, 0.057785894053049604, 0.05351119843972879, 0.04946742245528439, 0.04566386532075842, 0.04207117370356826, 0.038750412505494776, 0.03560910552327322, 0.03260834541067408, 0.029846580795685782, 0.027282601751218778, 0.02491071827882111, 0.02273770432099516, 0.02072425483931052, 0.018861677635790768, 0.017146762559514955, 0.015569980648185915, 0.014138497044615379, 0.012827018603842808, 0.01162953991532486, 0.010537483719637322, 0.00954211799167428, 0.008635100245747127, 0.007810336690558269, 0.007060687661704984, 0.006380606289077697, 0.0057640458804771874, 0.005205114738699594, 0.004698651981275271, 0.004240355266708711, 0.0038256902117222236, 0.0034509046633071925, 0.0031120489459761113, 0.0028059255979128836, 0.002529449090814446, 0.0022798381844142106, 0.0020546009546872105, 0.0018513241736515342, 0.0016679975406229607, 0.0015026436481071959, 0.0013535353822076983, 0.0012191318330180253, 0.0010979759516753904, 0.0009888138186206908, 0.0008904572186958709, 0.0008018365701954556, 0.0007219972226049392, 0.0006500690721587219, 0.0005852887299190124, 0.0005269433745578029, 0.000474397686382883, 0.0004270783236620915, 0.00038446802008905483, 0.00034610021640261903, 0.0003115541834035576, 0.00028045059586257425, 0.00025244751922380847, 0.00022723677348889584, 0.0002045406411518555, 0.00018410888849626784, 0.00016571607192891535, 0.000149159103284666, 0.00013425505018059957, 0.00012083914951430374, 0.00010876301408780231, 9.789301409421453e-05, 8.810881683109897e-05, 7.930206950649684e-05, 7.137521138623846e-05, 6.424040280023156e-05, 5.781855968782813e-05, 5.203848342481729e-05, 4.6836076644010864e-05, 4.215363664456677e-05, 3.793921878872526e-05, 3.414606301483102e-05, 3.0732077258380333e-05, 2.7659372174018553e-05, 2.489384209621246e-05, 2.240478766966378e-05, 2.0164576026986183e-05, 1.8148334794961475e-05, 1.6333676575711195e-05, 1.4700450878939229e-05, 1.323052077929262e-05, 1.1907561841843682e-05, 1.0716881101448378e-05, 9.64525410077268e-06, 8.680778189365416e-06, 7.81274046435158e-06, 7.0314988939938115e-06, 6.328375310217116e-06, 5.695559086749781e-06, 5.126020437193847e-06, 4.613432373360888e-06, 4.152100459732804e-06, 3.7368995859630077e-06, 3.363217056851529e-06, 3.0269013690490813e-06, 2.724216106629165e-06, 2.451798444299092e-06, 2.2066217980187858e-06], "duration": 48353.440992, "accuracy_train": [0.11305143878045404, 0.12977070067368032, 0.10149653066168328, 0.10093777397102252, 0.09291745253091548, 0.10093885543558508, 0.10035684725682908, 0.09984567500692137, 0.09986892649501662, 0.2808932969384459, 0.46009287617663347, 0.5039143609842193, 0.5921335709671466, 0.7219206738533591, 0.7399649100798266, 0.7586976787444628, 0.7901290043027871, 0.8219105441352897, 0.842506676241233, 0.8381125054794205, 0.8620815453119232, 0.8855168823827981, 0.9078128965370063, 0.9103455062984496, 0.9119945595122739, 0.9165540141080657, 0.9303407045957919, 0.9444772488695091, 0.9440351101075121, 0.9465237403100776, 0.9474072968576966, 0.9641701778216132, 0.9591729103220746, 0.9700989467977114, 0.9716117354766519, 0.9757955613810447, 0.980027332214378, 0.9826078869047619, 0.9882347470238095, 0.9892113095238095, 0.9868403787144703, 0.9903041294642857, 0.9916527157738095, 0.9930478050595238, 0.9958612351190477, 0.9961402529761905, 0.9960239955357143, 0.9961867559523809, 0.9960704985119048, 0.9981863839285714, 0.9983026413690477, 0.9987211681547619, 0.9990931919642857, 0.9993024553571429, 0.9992792038690477, 0.9994187127976191, 0.9994187127976191, 0.9996047247023809, 0.9997674851190477, 0.9997907366071429, 0.9997442336309523, 0.9998372395833334, 0.9998372395833334, 0.9999534970238095, 0.9999069940476191, 0.9999302455357143, 0.9999302455357143, 0.9999302455357143, 0.9999767485119048, 0.9999302455357143, 0.9999767485119048, 0.9999534970238095, 0.9999302455357143, 0.9999534970238095, 0.9999302455357143, 0.9999767485119048, 1.0, 1.0, 1.0, 0.9999767485119048, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "end": "2016-01-22 00:26:44.442000", "learning_rate_per_epoch": [0.0018842644058167934, 0.0017416286282241344, 0.00160979013890028, 0.001487931702286005, 0.0013752976665273309, 0.0012711898889392614, 0.0011749628465622663, 0.0010860200272873044, 0.0010038100881502032, 0.0009278233046643436, 0.0008575886022299528, 0.0007926705293357372, 0.0007326666382141411, 0.0006772049819119275, 0.0006259416695684195, 0.0005785588873550296, 0.0005347629194147885, 0.0004942822270095348, 0.0004568658769130707, 0.0004222818824928254, 0.0003903158358298242, 0.00036076956894248724, 0.0003334599023219198, 0.00030821753898635507, 0.0002848859876394272, 0.00026332057313993573, 0.00024338763614650816, 0.00022496358724310994, 0.00020793420844711363, 0.00019219392561353743, 0.0001776451535988599, 0.00016419769963249564, 0.00015176819579210132, 0.00014027958968654275, 0.00012966064969077706, 0.00011984555021626875, 0.00011077343515353277, 0.00010238806862616912, 9.463745664106682e-05, 8.747355605009943e-05, 8.085194713203236e-05, 7.473158620996401e-05, 6.90745291649364e-05, 6.384569860529155e-05, 5.901268377783708e-05, 5.454551865113899e-05, 5.041651093051769e-05, 4.660006379708648e-05, 4.307251583668403e-05, 3.981199552072212e-05, 3.6798293876927346e-05, 3.401272260816768e-05, 3.143801586702466e-05, 2.9058210202492774e-05, 2.6858551791519858e-05, 2.482540367054753e-05, 2.2946160243009217e-05, 2.120917270076461e-05, 1.960367262654472e-05, 1.8119706510333344e-05, 1.6748073903727345e-05, 1.548027285025455e-05, 1.4308441677712835e-05, 1.322531625191914e-05, 1.2224181773490272e-05, 1.1298830941086635e-05, 1.0443527571624145e-05, 9.652969310991466e-06, 8.922254892240744e-06, 8.24685503175715e-06, 7.622581506439019e-06, 7.045564416330308e-06, 6.512226718768943e-06, 6.01926194576663e-06, 5.5636137403780594e-06, 5.142457212059526e-06, 4.7531816562695894e-06, 4.393373728817096e-06, 4.060802439198596e-06, 3.7534064176725224e-06, 3.4692795907176333e-06, 3.206660721843946e-06, 2.9639218155352864e-06, 2.7395578854338964e-06, 2.532177859393414e-06, 2.3404961666528834e-06, 2.1633243250107625e-06, 1.9995641196146607e-06, 1.848200327003724e-06, 1.708294576019398e-06, 1.5789794360898668e-06, 1.4594533013223554e-06, 1.3489750472217565e-06, 1.2468598242776352e-06, 1.1524746241775574e-06, 1.0652341870809323e-06, 9.845977047007182e-07, 9.100652391680342e-07, 8.411747671743797e-07, 7.774992241138534e-07, 7.186437755990482e-07, 6.642436005677155e-07, 6.13961447015754e-07, 5.67485585634131e-07, 5.245278771326412e-07, 4.848219532505027e-07, 4.4812171040575777e-07, 4.1419963281441596e-07, 3.828453998266923e-07, 3.5386463537179225e-07, 3.270776574026968e-07, 3.023184262929135e-07, 2.7943340796809935e-07, 2.582807496764872e-07, 2.387293136507651e-07, 2.2065789551106718e-07, 2.0395445687881875e-07, 1.8851544325571012e-07, 1.7424513032437972e-07, 1.610550555142254e-07, 1.4886344956721587e-07, 1.37594724947121e-07, 1.2717903530301555e-07, 1.1755179230021895e-07, 1.0865331034892733e-07, 1.0042843001656365e-07, 9.282616275640976e-08, 8.579937116337533e-08, 7.930449896775826e-08, 7.330127971272304e-08, 6.775249516977055e-08, 6.262374085963529e-08, 5.7883227100319345e-08, 5.3501562291558e-08], "accuracy_valid": [0.11095456042921686, 0.12853562688253012, 0.1049245811370482, 0.10368328783885541, 0.09130124011671686, 0.0953207360692771, 0.10368328783885541, 0.10270672533885541, 0.10270672533885541, 0.2783203125, 0.4486657567771084, 0.48863422439759036, 0.574474656438253, 0.6989304875753012, 0.703955960560994, 0.7214531955948795, 0.7486248705760542, 0.767932570124247, 0.7873329254518072, 0.777587890625, 0.7920730774661144, 0.8076995481927711, 0.8228259718561747, 0.8232333631400602, 0.8200183546686747, 0.8200389448418675, 0.819091796875, 0.8362037015248494, 0.8325210019766567, 0.8326739575489458, 0.8313826595444277, 0.8486048686935241, 0.8393172298569277, 0.8516875117658133, 0.8459899166980422, 0.8493269954819277, 0.8534067912274097, 0.8517786968185241, 0.8565497340926205, 0.8579631024096386, 0.8568747646837349, 0.8608324901167168, 0.8610060358621988, 0.8626841349774097, 0.8633253717996988, 0.8650034709149097, 0.8640872082078314, 0.8621649684676205, 0.8641077983810241, 0.8681567088667168, 0.8635989269578314, 0.8674448771649097, 0.8681464137801205, 0.8681772990399097, 0.8712084666792168, 0.8704966349774097, 0.8669360057417168, 0.8678713878953314, 0.8702216090926205, 0.8694891872176205, 0.8682684840926205, 0.8681258236069277, 0.8678405026355422, 0.8686141048569277, 0.8687361751694277, 0.8697539180158133, 0.8694788921310241, 0.8688685405685241, 0.8691023861069277, 0.8685126247176205, 0.8695906673569277, 0.8672713314194277, 0.8716761577560241, 0.8698348079819277, 0.8691023861069277, 0.8693568218185241, 0.8707201854292168, 0.8685023296310241, 0.8698451030685241, 0.8713305369917168, 0.8702216090926205, 0.8716864528426205, 0.8714526073042168, 0.8728159709149097, 0.8706893001694277, 0.8712084666792168, 0.8716967479292168, 0.8695906673569277, 0.8698553981551205, 0.8692450465926205, 0.8710863963667168, 0.8692347515060241, 0.8712084666792168, 0.8702421992658133, 0.8704863398908133, 0.8702319041792168, 0.8713408320783133, 0.8702216090926205, 0.8704657497176205, 0.8702216090926205, 0.8699671733810241, 0.8703436794051205, 0.8705981151167168, 0.8708319606551205, 0.8699774684676205, 0.8688685405685241, 0.8692347515060241, 0.8699671733810241, 0.8693568218185241, 0.8692347515060241, 0.8699671733810241, 0.8693465267319277, 0.8689803157944277, 0.8692347515060241, 0.8694788921310241, 0.8694788921310241, 0.8699774684676205, 0.8693568218185241, 0.8694788921310241, 0.8688582454819277, 0.8693568218185241, 0.8692244564194277, 0.8696009624435241, 0.8687464702560241, 0.8697333278426205, 0.8698553981551205, 0.8693568218185241, 0.8699774684676205, 0.8698553981551205, 0.8697333278426205, 0.8698553981551205, 0.8699774684676205, 0.8696112575301205, 0.8697333278426205], "accuracy_test": 0.87689696485623, "start": "2016-01-21 11:00:51.001000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0], "accuracy_train_last": 1.0, "batch_size_eval": 1024, "accuracy_train_std": [0.008309653028640708, 0.011987415971110388, 0.011354444358691563, 0.01122761169545851, 0.010453178276873724, 0.008165309132710446, 0.011457630948053403, 0.011552087231457793, 0.011574171259265388, 0.014014143173504487, 0.02086525760695689, 0.021417282210053425, 0.024773838329543454, 0.025735150658407147, 0.02270867354230044, 0.024785578183718567, 0.023401747909773705, 0.023533808563709697, 0.024482414738347856, 0.02342941501233727, 0.023413132154223176, 0.020778190204730394, 0.020312562550783047, 0.020314854749372106, 0.02079947881177164, 0.01984536491636068, 0.016969763440975096, 0.015271066226567997, 0.014876919625022272, 0.014881869593395988, 0.013970012330892524, 0.010977416033718566, 0.011414988917124702, 0.009701229754134882, 0.009584618544745809, 0.008817149592785803, 0.008317829191696302, 0.007716692155019174, 0.005275987575774864, 0.00562589923171005, 0.005700396278360154, 0.004795503160793166, 0.004127843171213194, 0.00380138720377558, 0.0033000734745554632, 0.0028427737566179175, 0.0025160001111706803, 0.002564418839092678, 0.0031857932887469358, 0.001994213322395049, 0.0013655177511184917, 0.001176672333415178, 0.0010524990976335821, 0.000859990792824913, 0.0010233296635354069, 0.0008539977526137908, 0.0008539977526137908, 0.0006758948034667333, 0.0004673491267262319, 0.00045385094159093966, 0.0005245774819883965, 0.0003639433557128564, 0.0004217437952756942, 0.00020796763183591793, 0.0002866635976083043, 0.00025150329767466447, 0.00025150329767466447, 0.0003296467373223081, 0.0001488821669790004, 0.00025150329767466447, 0.0001488821669790004, 0.00020796763183591793, 0.00025150329767466447, 0.00020796763183591793, 0.0003296467373223081, 0.0001488821669790004, 0.0, 0.0, 0.0, 0.0001488821669790004, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "accuracy_test_std": 0.05227474083270581, "error_valid": [0.8890454395707832, 0.8714643731174698, 0.8950754188629518, 0.8963167121611446, 0.9086987598832832, 0.9046792639307228, 0.8963167121611446, 0.8972932746611446, 0.8972932746611446, 0.7216796875, 0.5513342432228916, 0.5113657756024097, 0.425525343561747, 0.3010695124246988, 0.29604403943900603, 0.2785468044051205, 0.2513751294239458, 0.23206742987575302, 0.21266707454819278, 0.222412109375, 0.20792692253388556, 0.19230045180722888, 0.17717402814382532, 0.17676663685993976, 0.17998164533132532, 0.17996105515813254, 0.180908203125, 0.16379629847515065, 0.16747899802334332, 0.1673260424510542, 0.1686173404555723, 0.15139513130647586, 0.1606827701430723, 0.14831248823418675, 0.15401008330195776, 0.1506730045180723, 0.1465932087725903, 0.14822130318147586, 0.14345026590737953, 0.14203689759036142, 0.1431252353162651, 0.1391675098832832, 0.13899396413780118, 0.1373158650225903, 0.13667462820030118, 0.1349965290850903, 0.13591279179216864, 0.13783503153237953, 0.13589220161897586, 0.1318432911332832, 0.13640107304216864, 0.1325551228350903, 0.13185358621987953, 0.1318227009600903, 0.1287915333207832, 0.1295033650225903, 0.1330639942582832, 0.13212861210466864, 0.12977839090737953, 0.13051081278237953, 0.13173151590737953, 0.1318741763930723, 0.13215949736445776, 0.1313858951430723, 0.1312638248305723, 0.13024608198418675, 0.13052110786897586, 0.13113145943147586, 0.1308976138930723, 0.13148737528237953, 0.1304093326430723, 0.1327286685805723, 0.12832384224397586, 0.1301651920180723, 0.1308976138930723, 0.13064317818147586, 0.1292798145707832, 0.13149767036897586, 0.13015489693147586, 0.1286694630082832, 0.12977839090737953, 0.12831354715737953, 0.1285473926957832, 0.1271840290850903, 0.1293106998305723, 0.1287915333207832, 0.1283032520707832, 0.1304093326430723, 0.13014460184487953, 0.13075495340737953, 0.1289136036332832, 0.13076524849397586, 0.1287915333207832, 0.12975780073418675, 0.12951366010918675, 0.1297680958207832, 0.12865916792168675, 0.12977839090737953, 0.12953425028237953, 0.12977839090737953, 0.13003282661897586, 0.12965632059487953, 0.1294018848832832, 0.12916803934487953, 0.13002253153237953, 0.13113145943147586, 0.13076524849397586, 0.13003282661897586, 0.13064317818147586, 0.13076524849397586, 0.13003282661897586, 0.1306534732680723, 0.1310196842055723, 0.13076524849397586, 0.13052110786897586, 0.13052110786897586, 0.13002253153237953, 0.13064317818147586, 0.13052110786897586, 0.1311417545180723, 0.13064317818147586, 0.1307755435805723, 0.13039903755647586, 0.13125352974397586, 0.13026667215737953, 0.13014460184487953, 0.13064317818147586, 0.13002253153237953, 0.13014460184487953, 0.13026667215737953, 0.13014460184487953, 0.13002253153237953, 0.13038874246987953, 0.13026667215737953], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-3, 3], "momentum": 0.584954998836739, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.0020385818345298842, "patience_threshold": 1, "do_flip": true, "batch_size": 32, "optimization": "rmsprop", "nb_data_augmentation": 2, "learning_rate_decay_method": "exp", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 5.778947956891252e-06, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.07569836760269522}, "accuracy_valid_max": 0.8728159709149097, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import os\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-6, -3], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128, 256],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-3, 3)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8697333278426205, "loss_train": [157313.953125, 7662.4560546875, 3835.09814453125, 613.3423461914062, 398.1602783203125, 272.50933837890625, 27.999773025512695, 14.711019515991211, 3.083913803100586, 2.444575548171997, 1.8357335329055786, 1.4634108543395996, 1.2003847360610962, 1.0455870628356934, 0.9431244134902954, 0.8674046397209167, 0.7994582056999207, 0.7459083199501038, 0.6986499428749084, 0.6569582223892212, 0.6196370124816895, 0.587730348110199, 0.553539514541626, 0.5287493467330933, 0.500063955783844, 0.47998854517936707, 0.4551165997982025, 0.4347309172153473, 0.4172188639640808, 0.40137702226638794, 0.3862975239753723, 0.3724157512187958, 0.35843101143836975, 0.34804242849349976, 0.33687978982925415, 0.32741159200668335, 0.3170958161354065, 0.30957186222076416, 0.3015926778316498, 0.2959303557872772, 0.2880561947822571, 0.2832847237586975, 0.27639663219451904, 0.27120909094810486, 0.2664754092693329, 0.2627772390842438, 0.25752606987953186, 0.2542518973350525, 0.2507404685020447, 0.24859140813350677, 0.24571454524993896, 0.24254272878170013, 0.24018992483615875, 0.23821593821048737, 0.23427827656269073, 0.23303410410881042, 0.23101848363876343, 0.23033717274665833, 0.2292436808347702, 0.22770462930202484, 0.22591105103492737, 0.2243606001138687, 0.22218719124794006, 0.2227374017238617, 0.22070370614528656, 0.22084292769432068, 0.21939562261104584, 0.21910156309604645, 0.21804927289485931, 0.21846739947795868, 0.21703554689884186, 0.21646368503570557, 0.21546217799186707, 0.21552002429962158, 0.21594931185245514, 0.21508896350860596, 0.21469363570213318, 0.21497894823551178, 0.21438756585121155, 0.2135237753391266, 0.2141091674566269, 0.2141292840242386, 0.21377551555633545, 0.2133418470621109, 0.21339033544063568, 0.21299491822719574, 0.212794691324234, 0.21221987903118134, 0.2118544578552246, 0.2116837501525879, 0.21255458891391754, 0.2122315913438797, 0.2109764963388443, 0.2115340530872345, 0.21178673207759857, 0.21254894137382507, 0.21134352684020996, 0.21248413622379303, 0.21137604117393494, 0.21128952503204346, 0.21148082613945007, 0.2099204957485199, 0.21139578521251678, 0.21169067919254303, 0.21126706898212433, 0.21128639578819275, 0.21099334955215454, 0.21133512258529663, 0.2108408510684967, 0.2109563797712326, 0.21201932430267334, 0.21134334802627563, 0.2109069526195526, 0.21015436947345734, 0.2103748619556427, 0.2101835161447525, 0.21212653815746307, 0.21099746227264404, 0.21014991402626038, 0.2102237194776535, 0.21070300042629242, 0.21028181910514832, 0.20989613234996796, 0.21123184263706207, 0.21118584275245667, 0.20990368723869324, 0.2112603336572647, 0.21069060266017914, 0.2116124927997589, 0.2115004062652588, 0.21034027636051178, 0.20966999232769012, 0.21014755964279175, 0.20994630455970764], "accuracy_train_first": 0.11305143878045404, "model": "residualv2", "loss_std": [3293792.25, 69757.5703125, 79559.171875, 6468.25390625, 3807.633544921875, 3328.22607421875, 232.69969177246094, 168.19314575195312, 6.721982479095459, 0.9466127157211304, 0.2032388150691986, 0.22069129347801208, 0.2046203315258026, 0.19114179909229279, 0.1868186593055725, 0.1815367341041565, 0.17240490019321442, 0.16516391932964325, 0.1582890897989273, 0.1480443775653839, 0.14629718661308289, 0.1386396586894989, 0.13299007713794708, 0.12539440393447876, 0.1178494393825531, 0.11452976614236832, 0.10989771038293839, 0.10486426204442978, 0.09997151792049408, 0.09235003590583801, 0.0903240218758583, 0.08655787259340286, 0.08088494837284088, 0.07784944772720337, 0.07410081475973129, 0.07147297263145447, 0.06836849451065063, 0.0643688216805458, 0.061192672699689865, 0.061112046241760254, 0.05545894056558609, 0.05685843899846077, 0.051225483417510986, 0.05085501819849014, 0.048195309937000275, 0.046687494963407516, 0.045229412615299225, 0.0456007644534111, 0.041132427752017975, 0.04079313948750496, 0.04048098623752594, 0.04027421772480011, 0.03755686432123184, 0.0390058271586895, 0.034643370658159256, 0.03364567831158638, 0.03272297978401184, 0.03341494873166084, 0.031770817935466766, 0.032003194093704224, 0.02984820492565632, 0.029356330633163452, 0.028523927554488182, 0.029557840898633003, 0.02736668847501278, 0.027349112555384636, 0.026457028463482857, 0.026387907564640045, 0.026676680892705917, 0.026202362030744553, 0.025931376963853836, 0.026416849344968796, 0.02447853609919548, 0.023988815024495125, 0.02561432681977749, 0.024292312562465668, 0.024530204012989998, 0.02398744970560074, 0.02451857551932335, 0.02476104535162449, 0.0231799203902483, 0.024269651621580124, 0.024792494252324104, 0.023939883336424828, 0.023601803928613663, 0.02277929149568081, 0.02375706285238266, 0.02202211506664753, 0.02262997068464756, 0.022578241303563118, 0.02312517911195755, 0.02202494628727436, 0.022261571139097214, 0.021944915875792503, 0.022321240976452827, 0.024208541959524155, 0.021920105442404747, 0.023896275088191032, 0.02206076867878437, 0.022666573524475098, 0.022413058206439018, 0.020287036895751953, 0.022775525227189064, 0.023901890963315964, 0.02229178696870804, 0.022458793595433235, 0.02203245833516121, 0.021521447226405144, 0.021595153957605362, 0.021761519834399223, 0.02402975969016552, 0.02235313691198826, 0.021239548921585083, 0.02206381782889366, 0.021293045952916145, 0.021028347313404083, 0.022238772362470627, 0.021829161792993546, 0.021982384845614433, 0.02163158357143402, 0.0215747207403183, 0.021341603249311447, 0.021695388481020927, 0.02313755266368389, 0.0228810403496027, 0.021865643560886383, 0.022821886464953423, 0.021337803453207016, 0.022924158722162247, 0.022202474996447563, 0.02162824757397175, 0.02153034135699272, 0.021913131698966026, 0.020379643887281418]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:07 2016", "state": "available"}], "summary": "6d09b1cbc6a583a57b7b912c9ccc8205"}