{"content": {"hp_model": {"f0": 32, "f1": 32, "f2": 16, "f3": 16, "nonlin": "rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.7789500951766968, 1.4885387420654297, 1.3641496896743774, 1.2740098237991333, 1.2003686428070068, 1.1435390710830688, 1.094221591949463, 1.0541088581085205, 1.0179723501205444, 0.9835803508758545, 0.9554772973060608, 0.9290635585784912, 0.9072744846343994, 0.8842024207115173, 0.8651934266090393, 0.8473252654075623, 0.8310335874557495, 0.812562108039856, 0.7979212403297424, 0.7863253951072693, 0.7728400230407715, 0.7606798410415649, 0.748272180557251, 0.7397838830947876, 0.7278140187263489, 0.7203338146209717, 0.7098886370658875, 0.7022889852523804, 0.6962217092514038, 0.6885010004043579, 0.6807292103767395, 0.6747828722000122, 0.6656631827354431, 0.660007119178772, 0.6560519337654114, 0.6524195075035095, 0.6451060771942139, 0.6401551961898804, 0.6370836496353149, 0.6312757134437561, 0.6269057393074036, 0.6260950565338135, 0.619611918926239, 0.616145133972168, 0.6137557625770569, 0.6100949645042419, 0.6076155304908752, 0.6043397188186646, 0.6025375723838806, 0.5996591448783875, 0.5963303446769714, 0.5930052995681763, 0.592915415763855, 0.5882487297058105, 0.5893123149871826, 0.5846640467643738, 0.581999659538269, 0.582947313785553, 0.5797039270401001, 0.5763307213783264, 0.5758448839187622, 0.5757980346679688, 0.5733516216278076, 0.5708643198013306, 0.5699792504310608, 0.5681735873222351, 0.5645570158958435, 0.5664322972297668, 0.5656415224075317, 0.5634783506393433, 0.5612828731536865, 0.5607898235321045, 0.5616505742073059, 0.559815526008606, 0.5601447820663452, 0.5575727224349976, 0.5575078129768372, 0.556896448135376, 0.5557887554168701, 0.5554054379463196, 0.5530160665512085, 0.5509636402130127, 0.5529800057411194, 0.5516104698181152, 0.5526930093765259, 0.5525097250938416, 0.5498629808425903, 0.5499823689460754, 0.5487507581710815, 0.5485936999320984, 0.5519847273826599, 0.5488268136978149, 0.5472155809402466, 0.547383725643158, 0.5475822687149048, 0.5448445677757263, 0.5445986986160278, 0.5434009432792664, 0.5447372794151306, 0.546100378036499, 0.5434962511062622, 0.5456675291061401, 0.5441161394119263, 0.5466839075088501, 0.5442396402359009, 0.5427933931350708, 0.544037938117981, 0.5430912971496582, 0.544675350189209, 0.5424889922142029, 0.5431837439537048, 0.5414693355560303, 0.5433308482170105, 0.5413157343864441, 0.5412471890449524, 0.5423865914344788, 0.5439941883087158, 0.5425158143043518, 0.5377650856971741, 0.5394582748413086, 0.5396801233291626, 0.5447713732719421, 0.5401130318641663, 0.5419354438781738, 0.5407061576843262, 0.539226233959198, 0.5408769249916077, 0.5395976901054382, 0.5410681962966919, 0.5415730476379395, 0.5414872765541077, 0.5386778116226196, 0.5399792790412903, 0.540185809135437, 0.540164053440094, 0.53935307264328, 0.5412598848342896, 0.5405972599983215, 0.5409197807312012, 0.539183497428894, 0.5394358038902283, 0.5404725670814514, 0.5406461954116821, 0.538704514503479, 0.5397186875343323], "moving_avg_accuracy_train": [0.04127330195644148, 0.08287586300872091, 0.12574841781734492, 0.16643514698545353, 0.20661700748301373, 0.2461582219305872, 0.28311733297511615, 0.3189096098923978, 0.3518642373929144, 0.38355478912889057, 0.41299232222458404, 0.4417038253403501, 0.46719308067430143, 0.4911214545236302, 0.513863310634344, 0.5348587538649295, 0.5547541062723642, 0.5737295279402551, 0.5913931646972631, 0.6079088192154475, 0.6228379403508426, 0.6372596519797488, 0.6500253148041069, 0.6628047607888585, 0.6743179960656389, 0.6852471720266277, 0.6946137503319937, 0.7039573100936909, 0.712582788767323, 0.7206409413747638, 0.7280582921892992, 0.7350616457580678, 0.7420668949592544, 0.7483458623593238, 0.754238856642033, 0.7596540144440534, 0.7647973737277766, 0.7696286489807371, 0.7742651512096014, 0.7786867220405608, 0.782933599999157, 0.7864489426166739, 0.7901264646640689, 0.793487531975809, 0.7967030466122997, 0.7997155563256083, 0.802426959262861, 0.8049113276361319, 0.807640154670876, 0.8099681797688032, 0.8119796249021572, 0.813968998029328, 0.8159688053830951, 0.8178591325609632, 0.8196697811127296, 0.8212853778676434, 0.8229510034887325, 0.8244593671429508, 0.8260609629591098, 0.827476822556748, 0.8287325671017839, 0.8300604108899443, 0.8312414112599753, 0.8322323401263643, 0.833344921047744, 0.8342905484520136, 0.8352415584658469, 0.8363695098890112, 0.8373101532614979, 0.8382358955027162, 0.8391156025448218, 0.8398050323350977, 0.8406161092510895, 0.8413925814516727, 0.8420542040512452, 0.8426333883491938, 0.8432221195816425, 0.8438170097598754, 0.8443431463738656, 0.8449074222276658, 0.8454640625722672, 0.846020842453837, 0.8464987289079734, 0.8470171102738207, 0.8474302832269204, 0.8478996869882538, 0.8482780085948917, 0.8486510861730179, 0.8489937953909412, 0.8493208709263672, 0.8496035410665655, 0.8498556911415719, 0.8501871858078687, 0.8504158125920689, 0.8506819584692591, 0.8509355487980437, 0.8511265056153601, 0.8513123536926207, 0.8515423599311936, 0.8517168495113947, 0.8519296216073666, 0.8521443319330179, 0.8522516488666079, 0.852427253117544, 0.8525411551648243, 0.8526947842323673, 0.8528492182883665, 0.8530185800685646, 0.853187209614772, 0.8533180498670729, 0.8534195300524772, 0.8535620154931505, 0.8537088175314139, 0.853766570652765, 0.8539254692584002, 0.854001156834452, 0.8540576138600322, 0.854138688166397, 0.854276759208792, 0.8543335217338338, 0.8544496761242194, 0.8545031338993944, 0.854551209848233, 0.8546433063271878, 0.8546959662237233, 0.8548015248996622, 0.8548337126413312, 0.8549022091385953, 0.854942965695666, 0.8549610093577347, 0.8549865492488347, 0.8550281723901195, 0.8550796201589518, 0.8551096110604154, 0.8551365307740951, 0.8551654448628446, 0.8551542651617668, 0.8551976458045971, 0.8552320380855253, 0.8552514014431318, 0.8552106997447397, 0.8552531232757106, 0.8553378434785937, 0.8553744920337889, 0.855379609996569], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.040922675075301194, 0.08217129259224396, 0.12523398966961594, 0.16567498735175074, 0.2060963816888648, 0.24536487910921023, 0.28176647948895184, 0.31703653828892714, 0.3491803642228356, 0.3798626195927207, 0.4085193946251655, 0.4364081895847122, 0.461200870249735, 0.4842009651242344, 0.5059996833237839, 0.525903409456767, 0.5449551054086806, 0.5629246504024511, 0.5797828936641638, 0.5955087470405938, 0.6094259634509922, 0.6232373145362393, 0.6353857392855522, 0.6474353089469819, 0.65805607704513, 0.6682679180416713, 0.6765704031106217, 0.6849165454520445, 0.692843112621825, 0.7001702765573081, 0.7065989732050412, 0.712796750724522, 0.7184357856483048, 0.7235475381734593, 0.7285407994634177, 0.7328983982633108, 0.7371152649505339, 0.7407008664204654, 0.7441842553996538, 0.7478981834929715, 0.7508958333759786, 0.7535092985605947, 0.7562621902406799, 0.7587906798950758, 0.7611019121691224, 0.763134222599424, 0.7650731652679454, 0.7668649827772954, 0.7687879418602586, 0.7703843276911755, 0.7718444594928411, 0.7731951992080901, 0.7745817633893142, 0.7758948243433045, 0.7772474776393957, 0.7785869359183778, 0.7798545130343714, 0.7807847243815367, 0.781659565196395, 0.7825923767961078, 0.7834197002045995, 0.784042220959742, 0.7848751623615087, 0.7856380461630085, 0.7863693516747197, 0.786858687215079, 0.7875045497153332, 0.7882231918352909, 0.7890083651216413, 0.7895369160812393, 0.7900034934396063, 0.7903501708746367, 0.7910192729983929, 0.7916733815607525, 0.792274286298126, 0.792553605362214, 0.7929911865146221, 0.7933208858695604, 0.7937163010476646, 0.7941861560238469, 0.7945744634259803, 0.7952291158691503, 0.7956707891843438, 0.7960540291194484, 0.796413211109612, 0.7967364749007593, 0.7971494826252918, 0.7976086978134405, 0.7978246199654548, 0.7979192346349485, 0.7981274876586525, 0.7983759505362361, 0.7986626612996305, 0.7988484883078452, 0.799236488686398, 0.7992540401660262, 0.7993308716539416, 0.7994366410868154, 0.7996183123038116, 0.7995701788332197, 0.7996764316106657, 0.7997486745565269, 0.7998635508414615, 0.7998672242305835, 0.7999448019769528, 0.8000777161222545, 0.8002583740092761, 0.800294777760457, 0.8002909200427698, 0.8003495127617608, 0.8003656251151028, 0.8004411613893606, 0.800535617116012, 0.8006307752839288, 0.8005953768312137, 0.80066117447377, 0.8006705347184111, 0.8007145505236785, 0.8005578227397594, 0.8005520745866419, 0.8005346942175862, 0.8005211109027552, 0.8004447622371784, 0.800538857879048, 0.800598100385571, 0.8007867254938512, 0.8006482237840746, 0.8006710861289352, 0.8007048987792195, 0.8005491361697463, 0.8005574932135398, 0.8006504637717038, 0.8006375105327111, 0.800600409046458, 0.8006667329761495, 0.800589058643143, 0.8006300445333467, 0.8006659023258704, 0.8007002333564611, 0.8007687818864024, 0.8006931096936207, 0.8005517625326171, 0.8005975075425331, 0.8004901346591382, 0.8005786635501522], "moving_var_accuracy_train": [0.015331369089488369, 0.029375189955517272, 0.04298017456233178, 0.05358084647968979, 0.06275399904912933, 0.07055016790411735, 0.075788934116522, 0.07973982448718017, 0.0815399093017424, 0.0824245379955434, 0.08198119938882949, 0.08120223315044632, 0.07892932907271599, 0.07618949984110358, 0.07322527803123689, 0.06987002795615187, 0.06644545058728084, 0.06304150517582115, 0.05954539122959078, 0.056045753704107915, 0.05244708625457482, 0.04907424952588328, 0.045633483899401736, 0.04253996366655625, 0.039478958578747135, 0.0366060847055487, 0.033735071337348814, 0.031147283184797606, 0.02870214480746137, 0.026416334737718483, 0.024269855101899638, 0.02228429224259232, 0.020497524665669616, 0.018802601083602863, 0.01723488740978697, 0.015775314074995314, 0.01443586996998893, 0.013202353958118864, 0.012075592938571347, 0.01104398624223293, 0.01010191136956653, 0.009202938936076654, 0.008404362558150677, 0.007665597263602035, 0.006992093346639205, 0.0063745609449302925, 0.0058032702034305494, 0.005278491959014474, 0.004817661235982981, 0.004384672420093893, 0.003982618381804949, 0.003619974992576437, 0.0032939705583884217, 0.002996733534104058, 0.0027265662142957744, 0.0024774009687365895, 0.0022546296502495866, 0.002049643133444929, 0.0018677648025254771, 0.0016990302478749498, 0.001543319272348939, 0.0014048558672458526, 0.0012769231373873885, 0.0011580682838128358, 0.0010534019821911175, 0.0009561096846613555, 0.0008686384966129216, 0.0007932251166687946, 0.0007218658945897458, 0.000657392293405353, 0.00059861802438419, 0.0005430340428672515, 0.0004946512504534206, 0.0004506123071125849, 0.0004094907765797118, 0.0003715607889806522, 0.00033752415025913143, 0.0003069567841506404, 0.000278752483364807, 0.000253742900180964, 0.00023115724642200973, 0.00021083155630849742, 0.00019180377984507135, 0.00017504187502468243, 0.000159074094524772, 0.00014514974409268058, 0.00013192291482585418, 0.0001199833052569733, 0.00010904202120372293, 9.910062473621843e-05, 8.990968393603397e-05, 8.149093248536228e-05, 7.43308376608754e-05, 6.736818575287139e-05, 6.12688698290928e-05, 5.572075533986165e-05, 5.047686036059173e-05, 4.5740029894925666e-05, 4.1642152733475266e-05, 3.775195698251636e-05, 3.438420896768361e-05, 3.136069278638681e-05, 2.8328275825864675e-05, 2.577297991979967e-05, 2.3312445015191492e-05, 2.1193617527219635e-05, 1.928890467336882e-05, 1.761816491935861e-05, 1.6112271742109762e-05, 1.4655117112498338e-05, 1.3282289453515499e-05, 1.2136779415398672e-05, 1.1117059019803348e-05, 1.00353719250551e-05, 9.2590736344051e-06, 8.384723753481991e-06, 7.5749379397701365e-06, 6.876601534165926e-06, 6.360513895481703e-06, 5.753460364175618e-06, 5.299540909410546e-06, 4.795306422009388e-06, 4.336577451519126e-06, 3.9792555592901295e-06, 3.606287585689335e-06, 3.345942533714056e-06, 3.020672736766442e-06, 2.7608313943268065e-06, 2.4996981273924393e-06, 2.252658478320837e-06, 2.03326320482531e-06, 1.8455292573565474e-06, 1.6847981878812605e-06, 1.5244134566285381e-06, 1.3784941498270829e-06, 1.2481689555983423e-06, 1.1244769314841975e-06, 1.0289661598871312e-06, 9.367150047854446e-07, 8.464179608670965e-07, 7.766858190484676e-07, 7.152150409639225e-07, 7.082911518563243e-07, 6.495500860517939e-07, 5.848308193337904e-07], "duration": 63503.525187, "accuracy_train": [0.4127330195644149, 0.4572989124792359, 0.5116014110949612, 0.5326157094984312, 0.5682537519610558, 0.6020291519587486, 0.6157493323758767, 0.6410401021479328, 0.6484558848975637, 0.6687697547526762, 0.6779301200858251, 0.7001073533822444, 0.6965963786798633, 0.7064768191675895, 0.7185400156307677, 0.7238177429401993, 0.7338122779392765, 0.7445083229512736, 0.750365895510336, 0.7565497098791066, 0.7572000305693983, 0.767055056639904, 0.7649162802233297, 0.7778197746516242, 0.777937113556663, 0.783609755675526, 0.7789129550802879, 0.7880493479489663, 0.790212096830011, 0.7931643148417312, 0.794814449520118, 0.7980918278769842, 0.8051141377699336, 0.8048565689599483, 0.8072758051864157, 0.808390434662237, 0.8110876072812846, 0.8131101262573828, 0.8159936712693798, 0.8184808595191952, 0.8211555016265227, 0.8180870261743264, 0.8232241630906239, 0.8237371377814692, 0.8256426783407161, 0.8268281437453857, 0.8268295856981359, 0.8272706429955703, 0.8321995979835732, 0.8309204056501477, 0.830082631102344, 0.8318733561738648, 0.8339670715669989, 0.8348720771617755, 0.8359656180786268, 0.8358257486618678, 0.8379416340785345, 0.8380346400309154, 0.8404753253045404, 0.8402195589354927, 0.8400342680071059, 0.8420110049833887, 0.8418704145902547, 0.8411506999238648, 0.8433581493401624, 0.8428011950904393, 0.843800648590347, 0.8465210726974898, 0.8457759436138795, 0.8465675756736802, 0.8470329659237725, 0.8460099004475821, 0.8479158014950166, 0.8483808312569213, 0.8480088074473975, 0.8478460470307309, 0.8485207006736802, 0.8491710213639718, 0.8490783758997784, 0.8499859049118678, 0.8504738256736802, 0.8510318613879659, 0.8507997069952011, 0.8516825425664452, 0.8511488398048173, 0.8521243208402547, 0.8516829030546327, 0.8520087843761536, 0.8520781783522517, 0.8522645507452011, 0.8521475723283499, 0.8521250418166297, 0.8531706378045404, 0.8524734536498707, 0.8530772713639718, 0.8532178617571059, 0.852845116971207, 0.8529849863879659, 0.8536124160783499, 0.8532872557332041, 0.8538445704711147, 0.8540767248638795, 0.8532175012689184, 0.8540076913759689, 0.853566273590347, 0.8540774458402547, 0.8542391247923589, 0.854542836090347, 0.8547048755306386, 0.8544956121377814, 0.8543328517211147, 0.85484438445921, 0.8550300358757843, 0.8542863487449243, 0.8553555567091177, 0.8546823450189184, 0.8545657270902547, 0.8548683569236802, 0.855519398590347, 0.85484438445921, 0.8554950656376891, 0.8549842538759689, 0.8549838933877814, 0.8554721746377814, 0.8551699052925433, 0.8557515529831118, 0.8551234023163529, 0.8555186776139718, 0.8553097747093023, 0.8551234023163529, 0.8552164082687338, 0.8554027806616832, 0.8555426500784422, 0.855379529173588, 0.8553788081972129, 0.8554256716615909, 0.8550536478520672, 0.8555880715900701, 0.8555415686138795, 0.8554256716615909, 0.85484438445921, 0.8556349350544481, 0.8561003253045404, 0.8557043290305463, 0.8554256716615909], "end": "2016-01-31 11:37:40.543000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0], "moving_var_accuracy_valid": [0.015071988017868104, 0.02887782523961263, 0.042679605631848874, 0.05313091371040186, 0.06252282442076998, 0.07014867598555807, 0.07505949697885991, 0.07874934071075737, 0.08017343655070631, 0.08062870004688127, 0.0799567268395345, 0.0789611181142418, 0.07659709943383776, 0.07369842876857774, 0.07060524292801029, 0.06711014346100039, 0.06366583318269782, 0.06020539078997632, 0.05674265500381843, 0.05329411168318936, 0.0497079007283952, 0.0464538914247553, 0.04313676029728721, 0.04012981342878932, 0.037132038520862165, 0.034357369937623713, 0.03154201426874263, 0.02901473566971798, 0.026678736306619558, 0.02449404865799461, 0.022416597057492316, 0.020520649367371923, 0.018754772864479505, 0.017114465702937363, 0.01562741305743179, 0.014235569757396087, 0.012972050463576789, 0.011790554258329687, 0.0107207048215197, 0.009772773696308745, 0.0088763694700677, 0.008050204325501728, 0.007313389606372091, 0.006639589985126365, 0.006023707138235082, 0.005458508995577589, 0.0049464935840661485, 0.004480739715540854, 0.004065945688699526, 0.003682287149319944, 0.003333246298292066, 0.003016342148468019, 0.0027320109756791013, 0.0024743270397312354, 0.0022433613742129503, 0.0020351725731218586, 0.0018461160815145875, 0.001669292111716684, 0.0015092510186070945, 0.0013661571540714158, 0.0012357016148644181, 0.0011156192421932247, 0.0010103014403828972, 0.0009145092215959237, 0.0008278715691994639, 0.0007472394557190466, 0.0006762697554702527, 0.0006132907983924225, 0.0005575101923595652, 0.0005042734681756356, 0.00045580537124013894, 0.000411306501311758, 0.0003742051300487179, 0.00034063533914601416, 0.0003098215837619945, 0.000279541597641862, 0.00025331073326216114, 0.0002289579749177662, 0.00020746935589366616, 0.00018870929359208926, 0.0001711954079798445, 0.00015793299557399662, 0.00014389537387278224, 0.00013082769211623612, 0.00011890602822313335, 0.00010795592070882188, 9.869550706265128e-05, 9.072386365762273e-05, 8.207107867343515e-05, 7.394453822724222e-05, 6.69404083014545e-05, 6.0801971685142886e-05, 5.546160207324473e-05, 5.0226226958758383e-05, 4.6558502906696834e-05, 4.1905425105961425e-05, 3.776801029318301e-05, 3.4091893820238936e-05, 3.097974431797887e-05, 2.7902621365102066e-05, 2.521396610302697e-05, 2.2739540881764538e-05, 2.0584355841151587e-05, 1.85260417011252e-05, 1.6727602291598425e-05, 1.5213837592630115e-05, 1.3986189282654844e-05, 1.259949745228967e-05, 1.1339681644932496e-05, 1.0236611440908172e-05, 9.21528676818931e-06, 8.345109649929113e-06, 7.590895643611202e-06, 6.913301771541753e-06, 6.23324904847923e-06, 5.648888111525059e-06, 5.084787827990219e-06, 4.593745565211168e-06, 4.355443392959645e-06, 3.920196425042042e-06, 3.5308954775944693e-06, 3.1794664878111805e-06, 2.913981907648183e-06, 2.7022696252528696e-06, 2.4636297339396376e-06, 2.537481643809302e-06, 2.4563779919279976e-06, 2.2154443740479266e-06, 2.004189594516303e-06, 2.0221285496536496e-06, 1.820544256316974e-06, 1.7162815528533738e-06, 1.5461634751716462e-06, 1.4039358101941415e-06, 1.303132002022293e-06, 1.2271185198921436e-06, 1.1195252566650904e-06, 1.0191447625606721e-06, 9.278378632573637e-07, 8.773441855455639e-07, 8.411462938346924e-07, 9.368428437653369e-07, 8.619920127787435e-07, 8.79553236297651e-07, 8.621341935652306e-07], "accuracy_test": 0.7884885204081632, "start": "2016-01-30 17:59:17.018000", "learning_rate_per_epoch": [0.0017254806589335203, 0.001647769589908421, 0.0015735584311187267, 0.0015026895562186837, 0.0014350124401971698, 0.0013703833101317286, 0.0013086649123579264, 0.0012497261632233858, 0.0011934417998418212, 0.001139692380093038, 0.0010883637005463243, 0.0010393466800451279, 0.0009925372432917356, 0.0009478360298089683, 0.0009051480446942151, 0.0008643826004117727, 0.000825453142169863, 0.0007882769568823278, 0.0007527750567533076, 0.0007188720628619194, 0.0006864959723316133, 0.0006555780419148505, 0.0006260525551624596, 0.0005978568224236369, 0.0005709309480153024, 0.0005452177720144391, 0.0005206626374274492, 0.0004972133901901543, 0.0004748202336486429, 0.0004534356121439487, 0.0004330140945967287, 0.0004135123162996024, 0.0003948888333979994, 0.00037710409378632903, 0.0003601203497964889, 0.00034390148357488215, 0.000328413094393909, 0.00031362223671749234, 0.0002994975366163999, 0.00028600895893760026, 0.0002731278946157545, 0.0002608269569464028, 0.000249080010689795, 0.00023786211386322975, 0.00022714944498147815, 0.00021691924484912306, 0.00020714978745672852, 0.00019782032177317888, 0.0001889110280899331, 0.0001804029889171943, 0.00017227811622433364, 0.0001645191659918055, 0.00015710966545157135, 0.0001500338694313541, 0.00014327674580272287, 0.00013682394637726247, 0.0001306617632508278, 0.00012477711425162852, 0.00011915748473256826, 0.00011379094939911738, 0.00010866611410165206, 0.00010377208673162386, 9.909846994560212e-05, 9.463533933740109e-05, 9.03732143342495e-05, 8.63030509208329e-05, 8.241619070759043e-05, 7.870439003454521e-05, 7.515975448768586e-05, 7.17747607268393e-05, 6.854222010588273e-05, 6.545526412082836e-05, 6.25073371338658e-05, 5.969217454548925e-05, 5.700379915651865e-05, 5.4436499340226874e-05, 5.198482540436089e-05, 4.964356776326895e-05, 4.740775329992175e-05, 4.5272634451976046e-05, 4.3233678297838196e-05, 4.128654836677015e-05, 3.9427111914847046e-05, 3.7651421735063195e-05, 3.5955701605416834e-05, 3.433635356486775e-05, 3.278993608546443e-05, 3.131316407234408e-05, 2.990290340676438e-05, 2.8556156394188292e-05, 2.727006358327344e-05, 2.60418928519357e-05, 2.4869035769370385e-05, 2.3749000320094638e-05, 2.2679409084958024e-05, 2.1657990146195516e-05, 2.068257163045928e-05, 1.9751083527808078e-05, 1.8861548596760258e-05, 1.8012075088336132e-05, 1.7200860384036787e-05, 1.642618008190766e-05, 1.5686389815527946e-05, 1.497991706855828e-05, 1.4305262084235437e-05, 1.3660991498909425e-05, 1.3045737432548776e-05, 1.2458192031772342e-05, 1.189710837934399e-05, 1.1361294127709698e-05, 1.0849611498997547e-05, 1.036097455653362e-05, 9.89434374787379e-06, 9.448728633287828e-06, 9.02318333828589e-06, 8.616803825134411e-06, 8.22872607386671e-06, 7.858126082282979e-06, 7.504217137466185e-06, 7.166247542045312e-06, 6.84349879520596e-06, 6.535286047437694e-06, 6.240954462555237e-06, 5.959878762951121e-06, 5.691461865353631e-06, 5.435133516584756e-06, 5.1903498388128355e-06, 4.956590601068456e-06, 4.7333592192444485e-06, 4.520181391853839e-06, 4.316604645282496e-06, 4.122196514799725e-06, 3.936543635063572e-06, 3.759252194868168e-06, 3.5899456634069793e-06, 3.4282641081517795e-06, 3.2738641948526492e-06, 3.1264180506695993e-06, 2.9856125820515445e-06, 2.8511485652416013e-06, 2.7227404189034132e-06, 2.6001155220001237e-06, 2.483013304299675e-06, 2.371185019001132e-06, 2.264393287987332e-06], "accuracy_train_first": 0.4127330195644149, "accuracy_train_last": 0.8554256716615909, "batch_size_eval": 1024, "accuracy_train_std": [0.014792855158575193, 0.015586875511104716, 0.015685159266562502, 0.015981244723078738, 0.01677287849864085, 0.014341802011155957, 0.015149608015588368, 0.014658935208014885, 0.01274865480812655, 0.015198714838197236, 0.013989741269374797, 0.015733390785793052, 0.015438315175392268, 0.013280915094517485, 0.015375101908039698, 0.015090840983163606, 0.01490898981191788, 0.014768018163933291, 0.015383535577712602, 0.016198678899402193, 0.01515810842866878, 0.01629548223461617, 0.015445141463248093, 0.015685285140245333, 0.013961154845661194, 0.014195179066531345, 0.015347095309822176, 0.015517905067039197, 0.015343732699372056, 0.014077699973510174, 0.01666552163836163, 0.015532862280018355, 0.014534013906510677, 0.014653003593051755, 0.013703201165502254, 0.015501827983717582, 0.014238969678592927, 0.014587487964791831, 0.013825782919941838, 0.01481600533789971, 0.014614711643319577, 0.013855706128711967, 0.013388107728552265, 0.012637473934355839, 0.01271484313885446, 0.012767121761255561, 0.01358731738426133, 0.013176783423918807, 0.013016813983274691, 0.013521953014242387, 0.012288304776798627, 0.012723060678358251, 0.01295876982342112, 0.013067342745035434, 0.013305275478462367, 0.012795348275453236, 0.011547925768628111, 0.012878361853818715, 0.013034494971206147, 0.0118982431981727, 0.012602596305526244, 0.012837561156207775, 0.01204837454326051, 0.011244699147744841, 0.01119765533521228, 0.011058921665015523, 0.0112593726981132, 0.01074859383915382, 0.010870582412008346, 0.010808164731733062, 0.010766564898774679, 0.010919164605569633, 0.011426733841552031, 0.011840367189519446, 0.010718732338599197, 0.011719446666128666, 0.010978346970433775, 0.010921025969402464, 0.010830850316600167, 0.011472191328032325, 0.011094884090022586, 0.010724192324432278, 0.010458390409289432, 0.010957089084546567, 0.010775166742025149, 0.010399688997831991, 0.010873234593579752, 0.010780040272573293, 0.010634561534019957, 0.010957684883661943, 0.010862974059297896, 0.010620712673004008, 0.01061258246227811, 0.010925413153097226, 0.010406608723996805, 0.010725232275172056, 0.010648298228894255, 0.010431487462524971, 0.01057381607489806, 0.010594667357437134, 0.010273418978302343, 0.010863540533096402, 0.01038402092777777, 0.010359860241292802, 0.010575558284563354, 0.010097651661550508, 0.010435093706406159, 0.010622307823944823, 0.00998591137877897, 0.010372916843800366, 0.010425192934517597, 0.010416101323859122, 0.009942286529975114, 0.010443963978462948, 0.010501959738636452, 0.011085194126429946, 0.010594035024420339, 0.010681867359890683, 0.009928022205886844, 0.010269006358403766, 0.010424414050849973, 0.010438111977348088, 0.010218928434685047, 0.010153562029988059, 0.010737306874740548, 0.010466153291162844, 0.009977318395719609, 0.010337389663801074, 0.010181781004215264, 0.010047620977765397, 0.009945917509337019, 0.01023840379035734, 0.009636565239316244, 0.00989652589423203, 0.010516228718158058, 0.01005233256815793, 0.010397096372446785, 0.01029443988160666, 0.010031235525616666, 0.010333026466591774, 0.009906613371707257, 0.010288834701161594, 0.010234660140986524, 0.009714762384363678, 0.010002514817872368], "accuracy_test_std": 0.011435091783334028, "error_valid": [0.5907732492469879, 0.5465911497552711, 0.4872017366340362, 0.4703560335090362, 0.4301110692771084, 0.4012186441076807, 0.3906191170933735, 0.36553293251129515, 0.36152520237198793, 0.34399708207831325, 0.33356963008283136, 0.31259265577936746, 0.31566500376506024, 0.3087981810052711, 0.2978118528802711, 0.29496305534638556, 0.28357963102409633, 0.27534944465361444, 0.26849291698042166, 0.2629585725715362, 0.26531908885542166, 0.2524605256965362, 0.25527843797063254, 0.24411856410015065, 0.2463570100715362, 0.23982551298945776, 0.24870723126882532, 0.23996817347515065, 0.23581778285015065, 0.23388524802334332, 0.23554275696536142, 0.23142325160015065, 0.23081290003765065, 0.23044668910015065, 0.22651984892695776, 0.22788321253765065, 0.22493293486445776, 0.22702872035015065, 0.22446524378765065, 0.21867646366716864, 0.22212531767695776, 0.22296951477786142, 0.2189617846385542, 0.21845291321536142, 0.21809699736445776, 0.21857498352786142, 0.21747635071536142, 0.2170086596385542, 0.2139054263930723, 0.2152481998305723, 0.21501435429216864, 0.21464814335466864, 0.21293915897966864, 0.2122876270707832, 0.2105786426957832, 0.2093579395707832, 0.20873729292168675, 0.21084337349397586, 0.21046686746987953, 0.20901231880647586, 0.20913438911897586, 0.21035509224397586, 0.2076283650225903, 0.20749599962349397, 0.20704889871987953, 0.20873729292168675, 0.20668268778237953, 0.2053090290850903, 0.20392507530120485, 0.20570612528237953, 0.2057973103350903, 0.2065297322100903, 0.20295880788780118, 0.20243964137801207, 0.20231757106551207, 0.20493252306099397, 0.20307058311370485, 0.20371181993599397, 0.20272496234939763, 0.20158514919051207, 0.2019307699548193, 0.1988790121423193, 0.20035415097891573, 0.2004968114646084, 0.20035415097891573, 0.20035415097891573, 0.19913344785391573, 0.19825836549322284, 0.20023208066641573, 0.2012292333396084, 0.19999823512801207, 0.19938788356551207, 0.1987569418298193, 0.19947906861822284, 0.1972715079066265, 0.2005879965173193, 0.1999776449548193, 0.1996114340173193, 0.19874664674322284, 0.2008630224021084, 0.1993672933923193, 0.19960113893072284, 0.1991025625941265, 0.2000997152673193, 0.19935699830572284, 0.19872605657003017, 0.19811570500753017, 0.19937758847891573, 0.19974379941641573, 0.1991231527673193, 0.1994893637048193, 0.1988790121423193, 0.1986142813441265, 0.1985128012048193, 0.19972320924322284, 0.19874664674322284, 0.1992452230798193, 0.19888930722891573, 0.20085272731551207, 0.19949965879141573, 0.19962172910391573, 0.19960113893072284, 0.20024237575301207, 0.1986142813441265, 0.19886871705572284, 0.1975156485316265, 0.20059829160391573, 0.1991231527673193, 0.19899078736822284, 0.20085272731551207, 0.1993672933923193, 0.1985128012048193, 0.19947906861822284, 0.1997335043298193, 0.1987363516566265, 0.20011001035391573, 0.1990010824548193, 0.19901137754141573, 0.19899078736822284, 0.1986142813441265, 0.19998794004141573, 0.20072036191641573, 0.19899078736822284, 0.20047622129141573, 0.19862457643072284], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.045037314527258725, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 64, "valid_ratio": 0.15, "learning_rate": 0.0018068565658134552, "optimization": "nesterov_momentum", "nb_data_augmentation": 2, "learning_rate_decay_method": "exp", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 3.010382003300215e-08, "rotation_range": [0, 0], "momentum": 0.8901573686684748}, "accuracy_valid_max": 0.8027284920933735, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8013754235692772, "accuracy_valid_std": [0.02276096953926102, 0.01850448659054479, 0.01787835335051154, 0.01392115739990366, 0.013689219079511561, 0.022990347867396493, 0.016469893442025787, 0.01569957320635134, 0.01397244909909842, 0.013533533035420011, 0.01619881787740965, 0.019261861206674034, 0.019335912227182724, 0.019877684190634595, 0.020290735912668917, 0.018743767719835682, 0.020963674823705234, 0.020337773176842443, 0.0216107065557921, 0.023298896450087307, 0.019965511266360494, 0.0183008681759588, 0.02000118689675789, 0.01806443346012053, 0.016315409699730006, 0.01887626345206958, 0.014063535177130621, 0.017157253797330473, 0.015224393980791125, 0.015154405080127298, 0.0202915314159228, 0.01628512502586599, 0.012593266505522227, 0.015193088363543581, 0.015948108427445623, 0.014831489864550226, 0.016261730934268734, 0.014215094005939309, 0.013667527628072513, 0.015016703986287008, 0.015209126682760385, 0.013720547554242877, 0.016058128310036782, 0.013517531510184177, 0.013894259780432357, 0.014228521381253504, 0.015160050340381048, 0.013956883225374441, 0.01607040048784139, 0.016044467040114502, 0.016011697293562277, 0.013538463708665367, 0.015095076931713063, 0.017537311468050307, 0.017160110901902226, 0.016887540815254315, 0.016072653224120395, 0.01559266482178392, 0.01652262686253265, 0.014515656879176943, 0.014009220078890211, 0.016467321219993343, 0.01740494065630225, 0.01812603916201989, 0.01654790318203009, 0.018615720666426156, 0.0165838239789459, 0.016873234538425094, 0.020671037155626622, 0.016243522238200133, 0.018956370574453228, 0.01757493216755042, 0.018918516201924553, 0.022336389162737606, 0.022109674008294018, 0.018158442972372166, 0.02048666628322862, 0.01850478302771479, 0.01979536977067476, 0.022070760877726886, 0.02397222660939599, 0.022636436628310653, 0.022750876346793864, 0.02032678063728627, 0.022907530026612458, 0.0231559716642819, 0.02212597451513511, 0.02372120425671834, 0.021937807980708532, 0.021316660359925424, 0.021353929826895497, 0.021641881378921604, 0.02348468169926148, 0.024500338062738172, 0.024420890678314346, 0.023390743716624612, 0.02333945839374771, 0.023412224823097058, 0.02383117582721591, 0.020681495047082308, 0.023024869049651064, 0.024540062501947183, 0.024996689315109594, 0.023902975338075245, 0.024146011500712737, 0.025629487130444908, 0.025440015153158944, 0.02263689419138369, 0.0220253949493892, 0.023051021972086028, 0.023211973338192346, 0.022898235569200575, 0.024754799201316253, 0.02326950828567487, 0.02447705410230102, 0.02419851061865576, 0.023524705535488185, 0.021961442001564205, 0.021685472254890325, 0.022511607271279047, 0.0224593856333102, 0.02447683039266224, 0.022057754231057672, 0.024783675977790243, 0.023885742500730986, 0.024734886941443768, 0.023230112782932015, 0.02371379407097894, 0.024329770112794098, 0.022351402464881487, 0.02383886673921035, 0.023228488268165, 0.024495471961351455, 0.02384627239165311, 0.025217853805885874, 0.02278791911972379, 0.022818890132042895, 0.022817463617516947, 0.024197115780722023, 0.025113372472252805, 0.02292014844397196, 0.02326613406906092, 0.02451525279794741, 0.023224211384849803, 0.023970600953390937], "accuracy_valid": [0.40922675075301207, 0.4534088502447289, 0.5127982633659638, 0.5296439664909638, 0.5698889307228916, 0.5987813558923193, 0.6093808829066265, 0.6344670674887049, 0.6384747976280121, 0.6560029179216867, 0.6664303699171686, 0.6874073442206325, 0.6843349962349398, 0.6912018189947289, 0.7021881471197289, 0.7050369446536144, 0.7164203689759037, 0.7246505553463856, 0.7315070830195783, 0.7370414274284638, 0.7346809111445783, 0.7475394743034638, 0.7447215620293675, 0.7558814358998494, 0.7536429899284638, 0.7601744870105422, 0.7512927687311747, 0.7600318265248494, 0.7641822171498494, 0.7661147519766567, 0.7644572430346386, 0.7685767483998494, 0.7691870999623494, 0.7695533108998494, 0.7734801510730422, 0.7721167874623494, 0.7750670651355422, 0.7729712796498494, 0.7755347562123494, 0.7813235363328314, 0.7778746823230422, 0.7770304852221386, 0.7810382153614458, 0.7815470867846386, 0.7819030026355422, 0.7814250164721386, 0.7825236492846386, 0.7829913403614458, 0.7860945736069277, 0.7847518001694277, 0.7849856457078314, 0.7853518566453314, 0.7870608410203314, 0.7877123729292168, 0.7894213573042168, 0.7906420604292168, 0.7912627070783133, 0.7891566265060241, 0.7895331325301205, 0.7909876811935241, 0.7908656108810241, 0.7896449077560241, 0.7923716349774097, 0.792504000376506, 0.7929511012801205, 0.7912627070783133, 0.7933173122176205, 0.7946909709149097, 0.7960749246987951, 0.7942938747176205, 0.7942026896649097, 0.7934702677899097, 0.7970411921121988, 0.7975603586219879, 0.7976824289344879, 0.795067476939006, 0.7969294168862951, 0.796288180064006, 0.7972750376506024, 0.7984148508094879, 0.7980692300451807, 0.8011209878576807, 0.7996458490210843, 0.7995031885353916, 0.7996458490210843, 0.7996458490210843, 0.8008665521460843, 0.8017416345067772, 0.7997679193335843, 0.7987707666603916, 0.8000017648719879, 0.8006121164344879, 0.8012430581701807, 0.8005209313817772, 0.8027284920933735, 0.7994120034826807, 0.8000223550451807, 0.8003885659826807, 0.8012533532567772, 0.7991369775978916, 0.8006327066076807, 0.8003988610692772, 0.8008974374058735, 0.7999002847326807, 0.8006430016942772, 0.8012739434299698, 0.8018842949924698, 0.8006224115210843, 0.8002562005835843, 0.8008768472326807, 0.8005106362951807, 0.8011209878576807, 0.8013857186558735, 0.8014871987951807, 0.8002767907567772, 0.8012533532567772, 0.8007547769201807, 0.8011106927710843, 0.7991472726844879, 0.8005003412085843, 0.8003782708960843, 0.8003988610692772, 0.7997576242469879, 0.8013857186558735, 0.8011312829442772, 0.8024843514683735, 0.7994017083960843, 0.8008768472326807, 0.8010092126317772, 0.7991472726844879, 0.8006327066076807, 0.8014871987951807, 0.8005209313817772, 0.8002664956701807, 0.8012636483433735, 0.7998899896460843, 0.8009989175451807, 0.8009886224585843, 0.8010092126317772, 0.8013857186558735, 0.8000120599585843, 0.7992796380835843, 0.8010092126317772, 0.7995237787085843, 0.8013754235692772], "seed": 399521321, "model": "residualv3", "loss_std": [0.24668601155281067, 0.12576989829540253, 0.1260768622159958, 0.1279391497373581, 0.12781719863414764, 0.1262812614440918, 0.12771403789520264, 0.1272508203983307, 0.1274334192276001, 0.12706834077835083, 0.12516610324382782, 0.1262665092945099, 0.1244330108165741, 0.12389229238033295, 0.1214902326464653, 0.1212349683046341, 0.12230782955884933, 0.11984175443649292, 0.11999492347240448, 0.12146186083555222, 0.11937912553548813, 0.11848041415214539, 0.11819441616535187, 0.11743051558732986, 0.11661186069250107, 0.11649548262357712, 0.11435000598430634, 0.11516545712947845, 0.11559636890888214, 0.11442666500806808, 0.11275270581245422, 0.11129130423069, 0.11347554624080658, 0.11450701206922531, 0.11146528273820877, 0.11114932596683502, 0.11259201914072037, 0.10997828841209412, 0.10889583081007004, 0.10938680171966553, 0.10948402434587479, 0.1131092756986618, 0.11124901473522186, 0.10958877205848694, 0.10943218320608139, 0.10834191739559174, 0.1075613796710968, 0.10896298289299011, 0.10818161815404892, 0.10788369923830032, 0.10922839492559433, 0.10660193115472794, 0.10647072643041611, 0.10895504802465439, 0.107608363032341, 0.10644568502902985, 0.10789963603019714, 0.10740099102258682, 0.1074296310544014, 0.10682526975870132, 0.10589735209941864, 0.10665783286094666, 0.10933983325958252, 0.10654313117265701, 0.1044418215751648, 0.10480108112096786, 0.10652007162570953, 0.10597066581249237, 0.10528922080993652, 0.10635550320148468, 0.10590394586324692, 0.10427071154117584, 0.10449294000864029, 0.10454699397087097, 0.1071968674659729, 0.1052163615822792, 0.10558924823999405, 0.10543721914291382, 0.10377317667007446, 0.1064063236117363, 0.10526078939437866, 0.10668308287858963, 0.10459395498037338, 0.10667459666728973, 0.10495699942111969, 0.10559063404798508, 0.10484618693590164, 0.10436558723449707, 0.10323092341423035, 0.10406698286533356, 0.1047801673412323, 0.10333480685949326, 0.10281259566545486, 0.1067982092499733, 0.10580113530158997, 0.10570903867483139, 0.1040211170911789, 0.10304123908281326, 0.10487516224384308, 0.10561370104551315, 0.10407483577728271, 0.10593581944704056, 0.1041758731007576, 0.10416149348020554, 0.1043875440955162, 0.10536029934883118, 0.10558536648750305, 0.10476724803447723, 0.10469095408916473, 0.10329851508140564, 0.10690819472074509, 0.10339406877756119, 0.1051124781370163, 0.10572002828121185, 0.10406257957220078, 0.10476571321487427, 0.10343082249164581, 0.1056504026055336, 0.10341961681842804, 0.10348442941904068, 0.1037619337439537, 0.10396473854780197, 0.10633504390716553, 0.10498891770839691, 0.10421881079673767, 0.10294599086046219, 0.10474486649036407, 0.10171864926815033, 0.10329371690750122, 0.1034080982208252, 0.10325215756893158, 0.10424859076738358, 0.10441308468580246, 0.10442771762609482, 0.10497104376554489, 0.10489246994256973, 0.10305878520011902, 0.10585930198431015, 0.10568245500326157, 0.10291964560747147, 0.1035730168223381, 0.10366465896368027, 0.1060168519616127, 0.103115014731884, 0.10570138692855835]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:26 2016", "state": "available"}], "summary": "6695d88326d68b5c03ae44466a6e5eaf"}