{"content": {"hp_model": {"f0": 16, "f1": 32, "f2": 16, "f3": 64, "nonlin": "very_leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.6670634746551514, 1.1955152750015259, 0.9800832867622375, 0.8462992906570435, 0.7517729997634888, 0.6809414625167847, 0.6236572265625, 0.5743058323860168, 0.5385781526565552, 0.5043457746505737, 0.47526636719703674, 0.4484086036682129, 0.42685243487358093, 0.40806451439857483, 0.38816115260124207, 0.36901363730430603, 0.35648855566978455, 0.33884984254837036, 0.32884472608566284, 0.3127692937850952, 0.3024676740169525, 0.29326069355010986, 0.28043538331985474, 0.27196335792541504, 0.2625589370727539, 0.2532118856906891, 0.24733836948871613, 0.2388174831867218, 0.2301282435655594, 0.22357001900672913, 0.21506735682487488, 0.21219444274902344, 0.20793747901916504, 0.19890864193439484, 0.19395066797733307, 0.18905211985111237, 0.18266747891902924, 0.18044275045394897, 0.17324700951576233, 0.17124201357364655, 0.16728180646896362, 0.1632923036813736, 0.16003358364105225, 0.1568106859922409, 0.15194253623485565, 0.14860765635967255, 0.14751194417476654, 0.1431155949831009, 0.13989433646202087, 0.13726726174354553, 0.13230647146701813, 0.13015243411064148, 0.12636613845825195, 0.1259423792362213, 0.12272554636001587, 0.12164805829524994, 0.11756588518619537, 0.11725693196058273, 0.11372867971658707, 0.1144237369298935, 0.11064761877059937, 0.1106170192360878, 0.10520300269126892, 0.10460329800844193, 0.10260602831840515, 0.1017359048128128, 0.10202475637197495, 0.0969676673412323, 0.09597491472959518, 0.09365872293710709, 0.09306974709033966, 0.0919000580906868, 0.0904134139418602, 0.09045670181512833, 0.08733021467924118, 0.0865119993686676, 0.08630223572254181, 0.08381688594818115, 0.08573197573423386, 0.08231767266988754, 0.08058054000139236, 0.07680854201316833, 0.0805596187710762, 0.07684079557657242, 0.0771903544664383, 0.07580525428056717, 0.07722677290439606, 0.07248388975858688, 0.07075601071119308, 0.07044604420661926, 0.06919962167739868, 0.06745011359453201, 0.06739380955696106, 0.0692807286977768, 0.06945554912090302, 0.06531305611133575, 0.04781210422515869, 0.035374078899621964, 0.03097444772720337, 0.029234955087304115, 0.028421616181731224, 0.027680909261107445, 0.026774203404784203, 0.024984542280435562, 0.02513894997537136, 0.02342788316309452, 0.02422269433736801, 0.02304558828473091, 0.022829513996839523, 0.021786704659461975, 0.02200859598815441, 0.02229960821568966, 0.02223896235227585, 0.02254646085202694, 0.02253812737762928, 0.02226911671459675, 0.021563075482845306, 0.022065861150622368, 0.022334689274430275, 0.021898729726672173, 0.022757919505238533, 0.021943269297480583, 0.021423107013106346, 0.022616811096668243, 0.02127809263765812, 0.0218452587723732, 0.022042864933609962, 0.022710010409355164, 0.022181997075676918, 0.022267313674092293, 0.02232774905860424, 0.02285437472164631, 0.022135531529784203, 0.021357866004109383, 0.021590055897831917, 0.022312618792057037, 0.022339150309562683, 0.022345324978232384, 0.023103389889001846, 0.022676974534988403, 0.022022003307938576, 0.02274349145591259, 0.021848829463124275, 0.02216789685189724, 0.021859226748347282, 0.02225763164460659, 0.021459024399518967, 0.022425735369324684, 0.022054603323340416, 0.022704502567648888, 0.02252495102584362, 0.0226521547883749, 0.022680550813674927, 0.022076725959777832, 0.022132325917482376, 0.021764924749732018], "moving_avg_accuracy_train": [0.03356876816860464, 0.08294787052417865, 0.13440724494739753, 0.1876876388600498, 0.2307605153971714, 0.2769810540717861, 0.3182941223564679, 0.3663108068989219, 0.403121407872005, 0.44188813685339995, 0.48144557769579155, 0.5176752809254279, 0.5404653977779756, 0.5722857563886516, 0.6035460899702091, 0.6294860625473391, 0.6560843442701799, 0.6787791315981914, 0.7004900672409746, 0.7180839039611832, 0.7319378349071191, 0.7490188912154565, 0.7656792533570616, 0.7810130870595152, 0.7957947501893425, 0.8079289134479092, 0.8201632252627251, 0.8307555430615264, 0.8406048493185428, 0.8497809030367901, 0.8583042741522233, 0.8653036285407109, 0.8717075349915031, 0.878007907830485, 0.8850989814058252, 0.8921552047295745, 0.8969460995044023, 0.9001021616481388, 0.9050373243131514, 0.9099415671783018, 0.9141530978105085, 0.9191827075973332, 0.9130227742084008, 0.9177422753209218, 0.9216457403471998, 0.9232571394863909, 0.9272089704401604, 0.9305517406568956, 0.9345018470221769, 0.9379918025354446, 0.9400679885378986, 0.9430385322805466, 0.9446193098549097, 0.9467767927444649, 0.9453405547594702, 0.9474189016633221, 0.9492102506708087, 0.9503623737038108, 0.9530800786322485, 0.954435626422632, 0.9563205398958635, 0.957858887951543, 0.958978442383825, 0.9597650801383366, 0.961124059735245, 0.9594785205714289, 0.9609568369285995, 0.9591395748454202, 0.9607308768835342, 0.9625908040011516, 0.9637231229296632, 0.9656093462760011, 0.966474507965077, 0.9639732697354758, 0.9652136997179084, 0.9667996946639839, 0.9679038944309281, 0.9691394896973683, 0.9707073267014686, 0.971051208799225, 0.9724440417883501, 0.9735163380178762, 0.9740163748625449, 0.9739759097703934, 0.9738488464327043, 0.9747643501073094, 0.975430265393044, 0.9754948049240146, 0.9759411182554412, 0.9766520089965729, 0.977412682352868, 0.9781764515795136, 0.9789474410941813, 0.9798878334800105, 0.9805691010617806, 0.9804359772639728, 0.9822365945673375, 0.9839269046046514, 0.985462134531091, 0.98687639354822, 0.9881422512172076, 0.9893001243097724, 0.9903398849442714, 0.9912756695153204, 0.9921248510756931, 0.9928821390336, 0.9935706736421448, 0.9941903547898351, 0.9947527181203755, 0.9952681457130998, 0.9957320305465518, 0.996147201747849, 0.9965185306802069, 0.9968550518681385, 0.9971602460860866, 0.9974256202870018, 0.9976714325142539, 0.9978856880723525, 0.9980808432234506, 0.9982564828594389, 0.9984145585318284, 0.9985614769345978, 0.9986913783482809, 0.9988106147694051, 0.9989156023996074, 0.9990100912667895, 0.9990951312472534, 0.9991739923784804, 0.9992426422477751, 0.9993090774277595, 0.999361893643317, 0.9994140785349377, 0.9994633700862059, 0.9995054073335377, 0.9995409157073267, 0.999577523541356, 0.9996058202943632, 0.9996312873720697, 0.9996588580396247, 0.9996766961939956, 0.9996974008305484, 0.9997113847058269, 0.9997286204911966, 0.9997418075492198, 0.9997536759014407, 0.9997666825672491, 0.9997783885664766, 0.9997889239657813, 0.9997984058251557, 0.999804614349783, 0.9998148523195666, 0.9998217413435624, 0.9998279414651585, 0.9998288712769761, 0.9998343584052309, 0.9998392968206602], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.03379038615399096, 0.08192193088761293, 0.13334314258989077, 0.1855824999705854, 0.22853077411697564, 0.2737692523811064, 0.3135429119039144, 0.3583869687933422, 0.39331331450625195, 0.4297276414705364, 0.4656561268528502, 0.4988331664171886, 0.5198226966316444, 0.5484162458690823, 0.5757394132625958, 0.598740261328279, 0.621802129161942, 0.6407602880925098, 0.6594360182723401, 0.6743214944759796, 0.6862178406514992, 0.7006062222489999, 0.7144245239228199, 0.7270085093129174, 0.7393116881726648, 0.7489033803305489, 0.7588655872429007, 0.7672597549399208, 0.7746161048412601, 0.7816243562264864, 0.7878545693237173, 0.7924749035246288, 0.7974593114684762, 0.8020419053592791, 0.8073320848817096, 0.8123363575682375, 0.8160701305087029, 0.8187997314920947, 0.822086597574813, 0.8256042414694401, 0.8279226882750563, 0.831815048589117, 0.8265805858047535, 0.8297241320812058, 0.8325471466780551, 0.8332934262214694, 0.8362580877408887, 0.8387502371275679, 0.8415526359957599, 0.8440087593502502, 0.8452131462634933, 0.8470742264507736, 0.8483494551140546, 0.8497677746158268, 0.8495550043549218, 0.850704225540288, 0.8517955888013948, 0.8522579138632733, 0.854178559788693, 0.854966170206661, 0.8564256784683744, 0.8581147124193532, 0.8594049683988034, 0.86006159246442, 0.8606301990782943, 0.8591367563071366, 0.8601975900420104, 0.85873828966921, 0.8594442270332378, 0.8612983618412091, 0.8623668796198142, 0.8636763724748509, 0.8641011686328627, 0.8627236136577541, 0.8636851978397949, 0.8649666921747913, 0.8656154307603995, 0.8668921548153836, 0.868493896129779, 0.8689028661271174, 0.8701274903295413, 0.8710750787401113, 0.8722951487557838, 0.8719424869957928, 0.871645534512328, 0.8729347472979174, 0.8733787949231708, 0.8735963618258086, 0.873827763623273, 0.8746647608866986, 0.8754077633371854, 0.8764244394695814, 0.8768602852440088, 0.8777052361059031, 0.8782592018590176, 0.8776875232487936, 0.879296153501098, 0.8810033569104009, 0.8826263187061831, 0.8839293224247967, 0.88498907396432, 0.8858950517335507, 0.8868172064811293, 0.8877733341010887, 0.8887151801431635, 0.8896279947719195, 0.8902765704829806, 0.8909213237791855, 0.8915748439332699, 0.8921131544382863, 0.8925376282452107, 0.8930305474613522, 0.8933500454260603, 0.8937861369866169, 0.894080963141118, 0.8943208631090093, 0.8945856012051113, 0.8948350430141936, 0.8950727771822772, 0.8951748156349831, 0.8953785725409878, 0.895561953756392, 0.8956781687252559, 0.8957949692284832, 0.8959621543462976, 0.8959895211311708, 0.8960873934250567, 0.8962497501857136, 0.8962727714491453, 0.8963311411886434, 0.8963338163205321, 0.8964349096978916, 0.8965381007687651, 0.8966431797638013, 0.8967255438280838, 0.8967742279147783, 0.8968434871639631, 0.8968071347295699, 0.8967978020924563, 0.896801609750304, 0.8968671013072765, 0.896889422614802, 0.8968097965242555, 0.8966770978865137, 0.896780484201025, 0.8968114672191755, 0.8967406661768513, 0.8968122520911692, 0.896642686802986, 0.8966253848960308, 0.8966474637821807, 0.8967293994446253, 0.8967166628134158, 0.8966929928140772, 0.8966340392122629, 0.8966684892066993], "moving_var_accuracy_train": [0.010141759767217722, 0.031072245535476245, 0.05179762592618993, 0.07216706671295747, 0.08164781428030084, 0.09271007661061474, 0.09879999544940635, 0.10967041385451155, 0.11089855556505629, 0.11333443349180297, 0.11608411027661651, 0.11628902181392262, 0.10933462446790534, 0.10751397902012304, 0.105557457218783, 0.10105765109262504, 0.09731910329883102, 0.09222267331572097, 0.08724268852251459, 0.08130430748509936, 0.07490125936048228, 0.07003699578591144, 0.06553140520752512, 0.061094402790903155, 0.05695144059576612, 0.052581437798059454, 0.04867039948849282, 0.04481313430680077, 0.04120490037982122, 0.037842209998400524, 0.034711819695103084, 0.03168155638229353, 0.028882490904538673, 0.02635149409527643, 0.024168894605806783, 0.022200117733577725, 0.020186680014911196, 0.018257658567716224, 0.016651095185715812, 0.015202450049867634, 0.013841837957275012, 0.012685326933017022, 0.011758297253920065, 0.010782930745287836, 0.009841771023661438, 0.008880963385967364, 0.00813341975835517, 0.007420644797016678, 0.006819010379988331, 0.006246727447350789, 0.005660849637466786, 0.00517418184486297, 0.00467925338003316, 0.004253220633798354, 0.0038464635863643924, 0.0035006929604027063, 0.0031795040457620427, 0.002873500128534406, 0.002652623396383455, 0.002403898645053232, 0.002195484869762034, 0.001997235015449545, 0.0018087921330461716, 0.0016334821103529613, 0.001486755329220983, 0.0013624499885557585, 0.0012458737629670838, 0.0011510083599810244, 0.001058697703571472, 0.0009839618931599629, 0.0008971050192467584, 0.0008394150639325122, 0.000762210100273461, 0.0007422948243770767, 0.0006819133408112262, 0.0006363604264508978, 0.0005836976979336843, 0.0005390681891023615, 0.0005072843860349635, 0.0004576202415058835, 0.000429318070975652, 0.0003967346367127881, 0.00035931150465574484, 0.0003233950910033158, 0.0002912008877290463, 0.00026962412176008177, 0.00024665269809404783, 0.0002220249164441642, 0.0002016151851080291, 0.00018600195740966818, 0.00017260937726349654, 0.000160598530421285, 0.0001498885008647041, 0.00014285869133216336, 0.00013274995186068474, 0.00011963445418450123, 0.00013685101282463592, 0.0001488802437423684, 0.00015520459771145408, 0.00015768529504808467, 0.00015633832628648693, 0.0001527705245442117, 0.00014722339168327334, 0.00014038228738566706, 0.00013283404254939385, 0.00012471200375517133, 0.00011650752254412922, 0.00010831281281294107, 0.00010032780417147438, 9.268601418440159e-05, 8.535411501432187e-05, 7.837000765036877e-05, 7.177397346938684e-05, 6.561579471179081e-05, 5.9892506836632066e-05, 5.4537067351571074e-05, 4.962717347601419e-05, 4.507760512599734e-05, 4.0912614410398674e-05, 3.7098996504929656e-05, 3.3613988118249066e-05, 3.0446854460075363e-05, 2.755403840955958e-05, 2.4926590485706394e-05, 2.2533133059595303e-05, 2.0360173067828015e-05, 1.838924194554094e-05, 1.6606289453152537e-05, 1.4988075748825055e-05, 1.3528990872198577e-05, 1.2201197758610954e-05, 1.1005587348971023e-05, 9.926895527311692e-06, 8.950110146049636e-06, 8.066446732926988e-06, 7.271863261245096e-06, 6.551883291197374e-06, 5.902532110499836e-06, 5.319120174834673e-06, 4.790071955113435e-06, 4.314922897375154e-06, 3.88519054654788e-06, 3.499345142568884e-06, 3.1509757148057606e-06, 2.837145863385134e-06, 2.554953837245662e-06, 2.3006917272823178e-06, 2.0716215063006756e-06, 1.8652685065853447e-06, 1.6790885679292397e-06, 1.5121230553639268e-06, 1.3613378776920567e-06, 1.225550063493114e-06, 1.103002838093946e-06, 9.929735314729102e-07, 8.938956698481897e-07], "duration": 64861.053143, "accuracy_train": [0.3356876816860465, 0.5273597917243448, 0.5975416147563676, 0.6672111840739203, 0.6184164042312661, 0.6929659021433185, 0.6901117369186046, 0.7984609677810077, 0.7344168166297527, 0.7907886976859542, 0.8374625452773165, 0.8437426099921558, 0.7455764494509044, 0.858668983884736, 0.8848890922042267, 0.8629458157415099, 0.8954688797757475, 0.8830322175502953, 0.8958884880260245, 0.8764284344430602, 0.8566232134205426, 0.9027483979904946, 0.9156225126315062, 0.9190175903815985, 0.9288297183577889, 0.9171363827750092, 0.9302720315960686, 0.9260864032507383, 0.9292486056316908, 0.9323653865010151, 0.9350146141911223, 0.9282978180370985, 0.9293426930486341, 0.9347112633813216, 0.9489186435838871, 0.9556612146433187, 0.9400641524778516, 0.9285067209417681, 0.9494537882982651, 0.9540797529646549, 0.9520568735003692, 0.964449195678756, 0.8575833737080103, 0.9602177853336102, 0.9567769255837025, 0.9377597317391103, 0.9627754490240864, 0.9606366726075121, 0.9700528043097084, 0.9694014021548542, 0.9587536625599853, 0.969773425964378, 0.9588463080241787, 0.9661941387504615, 0.9324144128945183, 0.9661240237979882, 0.9653323917381875, 0.9607314810008305, 0.9775394229881875, 0.9666355565360835, 0.9732847611549464, 0.9717040204526578, 0.9690544322743633, 0.9668448199289406, 0.9733548761074198, 0.9446686680970838, 0.9742616841431341, 0.942784216096807, 0.9750525952265596, 0.9793301480597084, 0.973913993286268, 0.9825853563930418, 0.9742609631667589, 0.9414621256690661, 0.9763775695598007, 0.9810736491786637, 0.9778416923334257, 0.9802598470953304, 0.9848178597383721, 0.974146147679033, 0.9849795386904762, 0.9831670040836102, 0.9785167064645626, 0.97361172394103, 0.9727052763935032, 0.983003883178756, 0.9814235029646549, 0.9760756607027501, 0.9799579382382798, 0.9830500256667589, 0.9842587425595238, 0.9850503746193245, 0.9858863467261905, 0.9883513649524732, 0.9867005092977114, 0.9792378630837025, 0.9984421502976191, 0.9991396949404762, 0.9992792038690477, 0.9996047247023809, 0.9995349702380952, 0.9997209821428571, 0.9996977306547619, 0.9996977306547619, 0.9997674851190477, 0.9996977306547619, 0.9997674851190477, 0.9997674851190477, 0.9998139880952381, 0.9999069940476191, 0.9999069940476191, 0.9998837425595238, 0.9998604910714286, 0.9998837425595238, 0.9999069940476191, 0.9998139880952381, 0.9998837425595238, 0.9998139880952381, 0.9998372395833334, 0.9998372395833334, 0.9998372395833334, 0.9998837425595238, 0.9998604910714286, 0.9998837425595238, 0.9998604910714286, 0.9998604910714286, 0.9998604910714286, 0.9998837425595238, 0.9998604910714286, 0.9999069940476191, 0.9998372395833334, 0.9998837425595238, 0.9999069940476191, 0.9998837425595238, 0.9998604910714286, 0.9999069940476191, 0.9998604910714286, 0.9998604910714286, 0.9999069940476191, 0.9998372395833334, 0.9998837425595238, 0.9998372395833334, 0.9998837425595238, 0.9998604910714286, 0.9998604910714286, 0.9998837425595238, 0.9998837425595238, 0.9998837425595238, 0.9998837425595238, 0.9998604910714286, 0.9999069940476191, 0.9998837425595238, 0.9998837425595238, 0.9998372395833334, 0.9998837425595238, 0.9998837425595238], "end": "2016-02-01 00:25:18.478000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0], "moving_var_accuracy_valid": [0.010276111767922418, 0.03009831097713206, 0.05088574899579315, 0.07035772823214527, 0.07992294367831218, 0.09034932855136904, 0.09555189162275846, 0.10409560740520278, 0.10466469328840171, 0.10613225283391785, 0.10713673210733031, 0.10632950248488039, 0.09966159564400434, 0.09705375560154803, 0.09406737932911884, 0.08942199250187273, 0.08526644098348155, 0.07997450299546342, 0.07511609877566552, 0.06959868551438103, 0.06391252443389311, 0.05938450171546147, 0.055164560694253566, 0.0510733148195119, 0.047328297228055136, 0.043423472531314164, 0.039974335377263324, 0.03661106030146827, 0.033436997226159844, 0.030535337790850613, 0.027831144008897715, 0.025240157001160946, 0.02293974020400105, 0.020834767684513165, 0.019003164910477738, 0.01732823312552012, 0.015720879356306675, 0.014215847914432807, 0.01289149452080104, 0.011713709435845604, 0.010590715252575291, 0.00966799794664803, 0.008947794557751207, 0.008141952050705864, 0.007399481548361495, 0.0066645457919376145, 0.006077194174066385, 0.005525372033749476, 0.005043515785122523, 0.004593457084002524, 0.004147166306133392, 0.003763622250691434, 0.0034018958989151735, 0.0030798109809056238, 0.0027722373234703914, 0.002506899975119404, 0.002266929641516705, 0.002042160377530604, 0.0018711442667150235, 0.0016896128115779475, 0.0015398230097142403, 0.001411516229930849, 0.0012853474513703275, 0.0011606931027052171, 0.0010475336137667696, 0.0009628535941866028, 0.000876696548685357, 0.0008081929120193206, 0.0007318587488747633, 0.0006896132169624676, 0.0006309274674549753, 0.0005832676645460062, 0.0005265649640741607, 0.0004909873870517614, 0.0004502104455989461, 0.0004199694506147027, 0.000381760261325345, 0.00035825445400598696, 0.0003455191857495572, 0.000312472575303108, 0.0002947226577072562, 0.0002733317060991492, 0.00025939567307752437, 0.00023457543862241172, 0.00021191152175709387, 0.00020567899604013117, 0.00018688570107755706, 0.00016862314918391174, 0.00015224275539234804, 0.00014332355962395256, 0.0001339596774344213, 0.00012986638291463088, 0.00011858939847494375, 0.00011315593625859266, 0.00010460224513334689, 9.70833685205015e-05, 0.00011066425326611058, 0.00012582871926611694, 0.0001369518922546223, 0.00013853707124564832, 0.00013479102405078243, 0.00012869908311076547, 0.00012348249920602578, 0.00011936186951626542, 0.00011540934826738758, 0.0001113674883588863, 0.00010401659359980301, 9.735629555652715e-05, 9.146446332702573e-05, 8.492602079262208e-05, 7.805502082824382e-05, 7.243624292819414e-05, 6.61113291804486e-05, 6.121177890510211e-05, 5.5872903166992295e-05, 5.080358080164093e-05, 4.6353999057226685e-05, 4.2278590096568e-05, 3.855938889898108e-05, 3.4797156621558705e-05, 3.169109285010432e-05, 2.88246415965621e-05, 2.6063730707798045e-05, 2.358013885500588e-05, 2.147368274207261e-05, 1.9333054936094044e-05, 1.74859603156791e-05, 1.597460174369045e-05, 1.4381911376451312e-05, 1.2974383477207887e-05, 1.1677009536462694e-05, 1.0601287421329859e-05, 9.636994253169033e-06, 8.772669184632381e-06, 7.956456817935365e-06, 7.182142398817453e-06, 6.507099751314511e-06, 5.8682832715599305e-06, 5.282238827443387e-06, 4.754145429023618e-06, 4.3173331824335416e-06, 3.890084031116963e-06, 3.558138456666699e-06, 3.3608049671268555e-06, 3.1209230406681982e-06, 2.817470263324818e-06, 2.580838325340045e-06, 2.368875380964588e-06, 2.390759325477966e-06, 2.1543775967887564e-06, 1.943327132032477e-06, 1.8094154938513185e-06, 1.6299339404372902e-06, 1.4719829662117482e-06, 1.3560644140925397e-06, 1.2311391917332721e-06], "accuracy_test": 0.8940788424744899, "start": "2016-01-31 06:24:17.425000", "learning_rate_per_epoch": [0.0028760621789842844, 0.0028760621789842844, 0.0028760621789842844, 0.0028760621789842844, 0.0028760621789842844, 0.0028760621789842844, 0.0028760621789842844, 0.0028760621789842844, 0.0028760621789842844, 0.0028760621789842844, 0.0028760621789842844, 0.0028760621789842844, 0.0028760621789842844, 0.0028760621789842844, 0.0028760621789842844, 0.0028760621789842844, 0.0028760621789842844, 0.0028760621789842844, 0.0028760621789842844, 0.0028760621789842844, 0.0028760621789842844, 0.0028760621789842844, 0.0028760621789842844, 0.0028760621789842844, 0.0028760621789842844, 0.0028760621789842844, 0.0028760621789842844, 0.0028760621789842844, 0.0028760621789842844, 0.0028760621789842844, 0.0028760621789842844, 0.0028760621789842844, 0.0028760621789842844, 0.0028760621789842844, 0.0028760621789842844, 0.0028760621789842844, 0.0028760621789842844, 0.0028760621789842844, 0.0028760621789842844, 0.0028760621789842844, 0.0028760621789842844, 0.0028760621789842844, 0.0028760621789842844, 0.0028760621789842844, 0.0028760621789842844, 0.0028760621789842844, 0.0028760621789842844, 0.0028760621789842844, 0.0028760621789842844, 0.0028760621789842844, 0.0028760621789842844, 0.0028760621789842844, 0.0028760621789842844, 0.0028760621789842844, 0.0028760621789842844, 0.0028760621789842844, 0.0028760621789842844, 0.0028760621789842844, 0.0028760621789842844, 0.0028760621789842844, 0.0028760621789842844, 0.0028760621789842844, 0.0028760621789842844, 0.0028760621789842844, 0.0028760621789842844, 0.0028760621789842844, 0.0028760621789842844, 0.0028760621789842844, 0.0028760621789842844, 0.0028760621789842844, 0.0028760621789842844, 0.0028760621789842844, 0.0028760621789842844, 0.0028760621789842844, 0.0028760621789842844, 0.0028760621789842844, 0.0028760621789842844, 0.0028760621789842844, 0.0028760621789842844, 0.0028760621789842844, 0.0028760621789842844, 0.0028760621789842844, 0.0028760621789842844, 0.0028760621789842844, 0.0028760621789842844, 0.0028760621789842844, 0.0028760621789842844, 0.0028760621789842844, 0.0028760621789842844, 0.0028760621789842844, 0.0028760621789842844, 0.0028760621789842844, 0.0028760621789842844, 0.0028760621789842844, 0.0028760621789842844, 0.00028760620625689626, 0.00028760620625689626, 0.00028760620625689626, 0.00028760620625689626, 0.00028760620625689626, 0.00028760620625689626, 0.00028760620625689626, 0.00028760620625689626, 0.00028760620625689626, 0.00028760620625689626, 0.00028760620625689626, 0.00028760620625689626, 2.8760619898093864e-05, 2.8760618988599163e-06, 2.8760618420164974e-07, 2.8760618775436342e-08, 2.876061833134713e-09, 2.8760618886458644e-10, 2.8760618886458644e-11, 2.8760618019096906e-12, 2.876061747699582e-13, 2.876061747699582e-14, 2.8760618324028767e-15, 2.8760617265237583e-16, 2.8760616603493093e-17, 2.87606170170834e-18, 2.876061753407128e-19, 2.876061753407128e-20, 2.8760617534071282e-21, 2.8760616524329324e-22, 2.87606158932406e-23, 2.8760616287671052e-24, 2.876061628767105e-25, 2.8760617520266216e-26, 2.876061674989424e-27, 2.8760617231376725e-28, 2.876061602767051e-29, 2.8760615275354125e-30, 2.8760615745551865e-31, 2.8760614570057515e-32, 2.8760614570057515e-33, 2.8760613651702553e-34, 2.87606130777307e-35, 2.8760613795195516e-36, 2.87606128983645e-37, 2.876061401940327e-38, 2.876061401940327e-39, 2.8760670071341843e-40, 2.8760249681802546e-41, 2.8754644487945246e-42, 2.872661851865875e-43, 2.802596928649634e-44, 2.802596928649634e-45, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "accuracy_train_first": 0.3356876816860465, "accuracy_train_last": 0.9998837425595238, "batch_size_eval": 1024, "accuracy_train_std": [0.013311628821384851, 0.01630599198945895, 0.01803909797106832, 0.01763294955438574, 0.0180815187129494, 0.018837443024344636, 0.0181438662051935, 0.01726759431502201, 0.016885694300727186, 0.01906544620963559, 0.02145228678162152, 0.01961228510477522, 0.016622349938981717, 0.01769930839732436, 0.018154357339884748, 0.019525491364428157, 0.01811025739053765, 0.017697098112460542, 0.015895616932782877, 0.018336702778973663, 0.01700431450830664, 0.014934466442757859, 0.01798040370918276, 0.016835006371042176, 0.018098236908940152, 0.01641650780806046, 0.014812635127593305, 0.015899998006537016, 0.015612223511944707, 0.01413107628507233, 0.01403882648341562, 0.0142114074731777, 0.015586330941369556, 0.01583463490586283, 0.013011951796842527, 0.011766718487213758, 0.011755649082659185, 0.014378358220721832, 0.012958918779840544, 0.011859180848533872, 0.0125285230874029, 0.01061046298698873, 0.013467006724638311, 0.012125708103021049, 0.011440914904470004, 0.011894133113578382, 0.010564637411651966, 0.01073265790740782, 0.010014713020756188, 0.009401028698845413, 0.011529882141514465, 0.009842981776839227, 0.010901006303845728, 0.008855629425208664, 0.011562392441927975, 0.00923538442663694, 0.009361449842009869, 0.009042831362457177, 0.007965980137352594, 0.009476613648490558, 0.00865511078667651, 0.008221920042819044, 0.008404286162690384, 0.008982160043225428, 0.00832270935977209, 0.011523125284910871, 0.008128070539891905, 0.009659429795062804, 0.00812114829006305, 0.006933067904869206, 0.006841444916532312, 0.006383669291596424, 0.008551261668164223, 0.009415788602153191, 0.006928598512808004, 0.006110615180475469, 0.007080066884542555, 0.006536469763267954, 0.005539243885564794, 0.007046986089406245, 0.0055281849336732075, 0.005281927284395546, 0.006697502265088354, 0.006574818241363689, 0.00820232871328917, 0.005222014754025473, 0.005216350174492335, 0.007725623534781197, 0.007031608227581847, 0.006158134119141465, 0.0050752731238669, 0.005100798919254428, 0.0049300283240457455, 0.004035184395942805, 0.005106184632826625, 0.0064104248503955505, 0.0013318455628366524, 0.0009338302468388766, 0.0007400404791127022, 0.0005662107357647842, 0.0006116511550393372, 0.0004411659682154546, 0.0004992306211305778, 0.0005428114550748117, 0.00041593526367183587, 0.0004514622358386253, 0.00041593526367183587, 0.00041593526367183587, 0.00043870819996542985, 0.0002866635976083043, 0.0002866635976083043, 0.00031625442961159415, 0.0004027275873253528, 0.00031625442961159415, 0.0002866635976083043, 0.00043870819996542985, 0.00031625442961159415, 0.000383473365477833, 0.0004217437952756942, 0.0004217437952756942, 0.0004217437952756942, 0.00031625442961159415, 0.0003417256895623853, 0.00031625442961159415, 0.0004027275873253528, 0.0004027275873253528, 0.0004027275873253528, 0.0003813527591810065, 0.0003417256895623853, 0.0002866635976083043, 0.0004217437952756942, 0.00031625442961159415, 0.0002866635976083043, 0.00031625442961159415, 0.0003417256895623853, 0.0002866635976083043, 0.0004027275873253528, 0.0003417256895623853, 0.0002866635976083043, 0.0004217437952756942, 0.00031625442961159415, 0.0004217437952756942, 0.00031625442961159415, 0.0004027275873253528, 0.0003417256895623853, 0.00031625442961159415, 0.00031625442961159415, 0.0003813527591810065, 0.00031625442961159415, 0.0003417256895623853, 0.0002866635976083043, 0.00031625442961159415, 0.00031625442961159415, 0.0004217437952756942, 0.00031625442961159415, 0.00031625442961159415], "accuracy_test_std": 0.0063030330038061026, "error_valid": [0.6620961384600903, 0.4848941665097892, 0.4038659520896084, 0.3442632836031627, 0.38493475856551207, 0.3190844432417168, 0.32849415239081325, 0.23801651920180722, 0.29234957407756024, 0.24254341585090367, 0.21098750470632532, 0.2025734775037651, 0.291271531438253, 0.19424181099397586, 0.1783520801957832, 0.1942521060805723, 0.1706410603350903, 0.18861628153237953, 0.17248241010918675, 0.1917092196912651, 0.20671504376882532, 0.16989834337349397, 0.16121076101280118, 0.15973562217620485, 0.1499597020896084, 0.16477139024849397, 0.15147455054593373, 0.15719273578689763, 0.15917674604668675, 0.15530138130647586, 0.15607351280120485, 0.16594208866716864, 0.15768101703689763, 0.15671474962349397, 0.14505629941641573, 0.14262518825301207, 0.1503259130271084, 0.15663385965737953, 0.14833160768072284, 0.14273696347891573, 0.15121129047439763, 0.13315370858433728, 0.2205295792545181, 0.14198395143072284, 0.14204572195030118, 0.15999005788780118, 0.13705995858433728, 0.1388204183923193, 0.13322577419051207, 0.13388613045933728, 0.1439473715173193, 0.13617605186370485, 0.14017348691641573, 0.13746734986822284, 0.15235992799322284, 0.13895278379141573, 0.1383821418486446, 0.1435811605798193, 0.12853562688253017, 0.1379453360316265, 0.13043874717620485, 0.12668398202183728, 0.1289827277861446, 0.13402879094503017, 0.13425234139683728, 0.1543042286332832, 0.1302549063441265, 0.15439541368599397, 0.13420233669051207, 0.12201442488704817, 0.12801646037274095, 0.12453819182981929, 0.13207566594503017, 0.14967438111822284, 0.12766054452183728, 0.12349985881024095, 0.1285459219691265, 0.12161732868975905, 0.11709043204066272, 0.12741640389683728, 0.11885089184864461, 0.12039662556475905, 0.11672422110316272, 0.1312314688441265, 0.1310270378388554, 0.11546233763177716, 0.12262477644954817, 0.12444553605045183, 0.12408962019954817, 0.11780226374246983, 0.11790521460843373, 0.11442547533885539, 0.11921710278614461, 0.11469020613704817, 0.11675510636295183, 0.12745758424322284, 0.10622617422816272, 0.10363181240587349, 0.10276702513177716, 0.10434364410768071, 0.10547316217996983, 0.10595114834337349, 0.10488340079066272, 0.10362151731927716, 0.10280820547816272, 0.10215667356927716, 0.10388624811746983, 0.10327589655496983, 0.10254347467996983, 0.10304205101656627, 0.10364210749246983, 0.10253317959337349, 0.10377447289156627, 0.10228903896837349, 0.10326560146837349, 0.10352003717996983, 0.10303175592996983, 0.10291998070406627, 0.10278761530496983, 0.10390683829066272, 0.10278761530496983, 0.10278761530496983, 0.10327589655496983, 0.10315382624246983, 0.10253317959337349, 0.10376417780496983, 0.10303175592996983, 0.10228903896837349, 0.10352003717996983, 0.10314353115587349, 0.10364210749246983, 0.10265524990587349, 0.10253317959337349, 0.10241110928087349, 0.10253317959337349, 0.10278761530496983, 0.10253317959337349, 0.10352003717996983, 0.10328619164156627, 0.10316412132906627, 0.10254347467996983, 0.10290968561746983, 0.10390683829066272, 0.10451718985316272, 0.10228903896837349, 0.10290968561746983, 0.10389654320406627, 0.10254347467996983, 0.10488340079066272, 0.10353033226656627, 0.10315382624246983, 0.10253317959337349, 0.10339796686746983, 0.10352003717996983, 0.10389654320406627, 0.10302146084337349], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.07246589690932967, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 128, "valid_ratio": 0.15, "learning_rate": 0.0028760621796125423, "optimization": "rmsprop", "nb_data_augmentation": 2, "learning_rate_decay_method": "discrete", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 5.554389329551404e-08, "rotation_range": [0, 0], "momentum": 0.6819986190063926}, "accuracy_valid_max": 0.8978433264307228, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8969785391566265, "accuracy_valid_std": [0.014390597841616507, 0.020056575398202006, 0.013936824248123734, 0.014812051345495778, 0.00951755560658205, 0.004249352333844426, 0.005388469310802867, 0.008151639117544388, 0.008252061428780617, 0.009977565295025677, 0.008449479649599004, 0.011601020922591082, 0.003070244668718699, 0.010085788329142156, 0.008803727998419802, 0.009862233593288826, 0.006664534677443085, 0.010936907955585976, 0.005976375720644621, 0.0058930275503595335, 0.011831897781834185, 0.006829494655535594, 0.007416936064572152, 0.008336891857109256, 0.0081201692985083, 0.007115661314097182, 0.00765878403445207, 0.008368124887360459, 0.011204351666094296, 0.014027770728239915, 0.008160993173483755, 0.013347963491769232, 0.008312934121810982, 0.008415287675907908, 0.005317558412934673, 0.00880887823650302, 0.006797845614363505, 0.012486456409082636, 0.005175072283780026, 0.006399145034772861, 0.009548970054875639, 0.005511702721915638, 0.010220570860809722, 0.009311853300330552, 0.010302638739112923, 0.00778622291273777, 0.007892012975113716, 0.011027303983954847, 0.010930423790237422, 0.007110686420425237, 0.010702104987876744, 0.011025082525535542, 0.007026941580941258, 0.007550478469245334, 0.007329832382404043, 0.010279827529508597, 0.011210060245388589, 0.010001290340265756, 0.008264145916433534, 0.007060288839602589, 0.014592556409116173, 0.011439480127802526, 0.00756712633347627, 0.013340630580603674, 0.006266593356351783, 0.013350971043200873, 0.011162790036569087, 0.009971080623221612, 0.011582926906548531, 0.008786383971455326, 0.013417205046982393, 0.010526595949160706, 0.006540019484645641, 0.005403389489958093, 0.010393380331810228, 0.007409812233191866, 0.013117670762477064, 0.006342684637273735, 0.009415943994356014, 0.00912547088974686, 0.010284195672621364, 0.0075145646934520315, 0.00906293020005852, 0.010025772531793415, 0.0110414568419924, 0.010060415258694184, 0.008023668802047346, 0.009295311626047272, 0.006698765670639534, 0.006561711218470621, 0.012892808299179314, 0.007137810273350704, 0.011518023581713598, 0.009855165826627895, 0.011241795491592781, 0.011152973878196776, 0.00679146584433669, 0.008643380229966136, 0.009528253013849198, 0.0064872589168826576, 0.00640577743086695, 0.006310458778575138, 0.008869091472500975, 0.008507791548737754, 0.008939347990300241, 0.008399225037406227, 0.00941895974371926, 0.007563480197018666, 0.007785252763857031, 0.007751692788380179, 0.007636791748051384, 0.007855964873357227, 0.008033526348750614, 0.008193737502079374, 0.0073993957990977615, 0.008245757637130036, 0.0084600890341836, 0.008372037439461709, 0.008022661580376069, 0.008388382800042903, 0.007842325923093446, 0.007887796372962995, 0.008484585116255855, 0.008157839049136202, 0.007733617229900257, 0.008051789767029544, 0.008041080006589181, 0.008208273437945614, 0.008202271836081977, 0.007305054000086321, 0.007636791748051383, 0.008305087949523896, 0.007886255210784504, 0.007877656665683045, 0.008168413202853749, 0.008540852866131021, 0.00799136729873785, 0.007876043075856634, 0.007989784253893958, 0.00818439906632889, 0.00871027839162696, 0.008055033350809383, 0.007905514081304521, 0.008287364880159466, 0.007698670089218681, 0.007799387568131544, 0.0082411478900589, 0.008070942547191947, 0.008356990853221733, 0.008184651325155182, 0.008416761394707562, 0.008182994123892366, 0.007605649573156358, 0.007630029965021385, 0.00781040202242349, 0.007886619445340836], "accuracy_valid": [0.33790386153990964, 0.5151058334902108, 0.5961340479103916, 0.6557367163968373, 0.6150652414344879, 0.6809155567582832, 0.6715058476091867, 0.7619834807981928, 0.7076504259224398, 0.7574565841490963, 0.7890124952936747, 0.7974265224962349, 0.708728468561747, 0.8057581890060241, 0.8216479198042168, 0.8057478939194277, 0.8293589396649097, 0.8113837184676205, 0.8275175898908133, 0.8082907803087349, 0.7932849562311747, 0.830101656626506, 0.8387892389871988, 0.8402643778237951, 0.8500402979103916, 0.835228609751506, 0.8485254494540663, 0.8428072642131024, 0.8408232539533133, 0.8446986186935241, 0.8439264871987951, 0.8340579113328314, 0.8423189829631024, 0.843285250376506, 0.8549437005835843, 0.8573748117469879, 0.8496740869728916, 0.8433661403426205, 0.8516683923192772, 0.8572630365210843, 0.8487887095256024, 0.8668462914156627, 0.7794704207454819, 0.8580160485692772, 0.8579542780496988, 0.8400099421121988, 0.8629400414156627, 0.8611795816076807, 0.8667742258094879, 0.8661138695406627, 0.8560526284826807, 0.8638239481362951, 0.8598265130835843, 0.8625326501317772, 0.8476400720067772, 0.8610472162085843, 0.8616178581513554, 0.8564188394201807, 0.8714643731174698, 0.8620546639683735, 0.8695612528237951, 0.8733160179781627, 0.8710172722138554, 0.8659712090549698, 0.8657476586031627, 0.8456957713667168, 0.8697450936558735, 0.845604586314006, 0.8657976633094879, 0.8779855751129518, 0.871983539627259, 0.8754618081701807, 0.8679243340549698, 0.8503256188817772, 0.8723394554781627, 0.876500141189759, 0.8714540780308735, 0.878382671310241, 0.8829095679593373, 0.8725835961031627, 0.8811491081513554, 0.879603374435241, 0.8832757788968373, 0.8687685311558735, 0.8689729621611446, 0.8845376623682228, 0.8773752235504518, 0.8755544639495482, 0.8759103798004518, 0.8821977362575302, 0.8820947853915663, 0.8855745246611446, 0.8807828972138554, 0.8853097938629518, 0.8832448936370482, 0.8725424157567772, 0.8937738257718373, 0.8963681875941265, 0.8972329748682228, 0.8956563558923193, 0.8945268378200302, 0.8940488516566265, 0.8951165992093373, 0.8963784826807228, 0.8971917945218373, 0.8978433264307228, 0.8961137518825302, 0.8967241034450302, 0.8974565253200302, 0.8969579489834337, 0.8963578925075302, 0.8974668204066265, 0.8962255271084337, 0.8977109610316265, 0.8967343985316265, 0.8964799628200302, 0.8969682440700302, 0.8970800192959337, 0.8972123846950302, 0.8960931617093373, 0.8972123846950302, 0.8972123846950302, 0.8967241034450302, 0.8968461737575302, 0.8974668204066265, 0.8962358221950302, 0.8969682440700302, 0.8977109610316265, 0.8964799628200302, 0.8968564688441265, 0.8963578925075302, 0.8973447500941265, 0.8974668204066265, 0.8975888907191265, 0.8974668204066265, 0.8972123846950302, 0.8974668204066265, 0.8964799628200302, 0.8967138083584337, 0.8968358786709337, 0.8974565253200302, 0.8970903143825302, 0.8960931617093373, 0.8954828101468373, 0.8977109610316265, 0.8970903143825302, 0.8961034567959337, 0.8974565253200302, 0.8951165992093373, 0.8964696677334337, 0.8968461737575302, 0.8974668204066265, 0.8966020331325302, 0.8964799628200302, 0.8961034567959337, 0.8969785391566265], "seed": 748676305, "model": "residualv3", "loss_std": [0.45798584818840027, 0.1393645852804184, 0.11527993530035019, 0.1062583327293396, 0.09895583242177963, 0.09909170866012573, 0.09633702784776688, 0.09249185025691986, 0.08672738820314407, 0.08755052834749222, 0.08477453887462616, 0.08015662431716919, 0.07825416326522827, 0.07506117969751358, 0.07555651664733887, 0.07288700342178345, 0.06922600418329239, 0.07049315422773361, 0.07088156044483185, 0.06653134524822235, 0.06087525933980942, 0.06106259301304817, 0.061933644115924835, 0.05947795882821083, 0.05826146528124809, 0.05470754951238632, 0.05657202750444412, 0.05409938469529152, 0.053235605359077454, 0.04983256012201309, 0.05050138756632805, 0.05144548416137695, 0.05064469948410988, 0.04700726270675659, 0.04607986658811569, 0.04689384251832962, 0.04536852240562439, 0.043989397585392, 0.04404834657907486, 0.044850464910268784, 0.04102081060409546, 0.04075461998581886, 0.04113928601145744, 0.04180554300546646, 0.040396831929683685, 0.03859442099928856, 0.03896687552332878, 0.03749869391322136, 0.03668016195297241, 0.036057330667972565, 0.035263873636722565, 0.03362192213535309, 0.03492864593863487, 0.03606124222278595, 0.03317832201719284, 0.03361263498663902, 0.03423016890883446, 0.033975280821323395, 0.03182240203022957, 0.03283338621258736, 0.03286389634013176, 0.034063685685396194, 0.029834596440196037, 0.03130248188972473, 0.030827241018414497, 0.030006060376763344, 0.031007368117570877, 0.02827271819114685, 0.02849549986422062, 0.02897786721587181, 0.028991661965847015, 0.028555218130350113, 0.028377093374729156, 0.02942156046628952, 0.027996031567454338, 0.02759268879890442, 0.02822929620742798, 0.028114670887589455, 0.028941961005330086, 0.025074923411011696, 0.026530694216489792, 0.023906297981739044, 0.02594168856739998, 0.024218568578362465, 0.02595691941678524, 0.025255730375647545, 0.025714479386806488, 0.024092024192214012, 0.02400645986199379, 0.02252950333058834, 0.024191951379179955, 0.02468763291835785, 0.023252081125974655, 0.022225206717848778, 0.02271507866680622, 0.023134320974349976, 0.021686067804694176, 0.014353460632264614, 0.012316021136939526, 0.011470568366348743, 0.01118962187319994, 0.01178149413317442, 0.010485127568244934, 0.010233545675873756, 0.009914063848555088, 0.009687243960797787, 0.010409852489829063, 0.00944830197840929, 0.009385711513459682, 0.00883764773607254, 0.009409721940755844, 0.009541487321257591, 0.00901373103260994, 0.00941528845578432, 0.009678026661276817, 0.009343164041638374, 0.009113551117479801, 0.009791134856641293, 0.010007737204432487, 0.009983023628592491, 0.009896467439830303, 0.009847717359662056, 0.009190482087433338, 0.009794436395168304, 0.00938412081450224, 0.009543206542730331, 0.009702436625957489, 0.009734027087688446, 0.009432998485863209, 0.00919314380735159, 0.009157119318842888, 0.010348811745643616, 0.009344383142888546, 0.008634556084871292, 0.009566215798258781, 0.010019136592745781, 0.009582259692251682, 0.009397365152835846, 0.009354175999760628, 0.009753393009305, 0.009272430092096329, 0.010829250328242779, 0.008965355344116688, 0.009182642214000225, 0.009181536734104156, 0.010011492297053337, 0.009082146920263767, 0.009756888262927532, 0.009471161291003227, 0.009919026866555214, 0.010109932161867619, 0.010026932694017887, 0.009928430430591106, 0.009572261944413185, 0.009346255101263523, 0.010013891384005547]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:27 2016", "state": "available"}], "summary": "390ecd8034cd9cdb668e9b2213cfadcb"}