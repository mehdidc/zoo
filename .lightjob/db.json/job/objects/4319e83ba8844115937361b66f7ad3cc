{"content": {"hp_model": {"f0": 64, "f1": 64, "f2": 64, "f3": 64, "nonlin": "leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.464726209640503, 1.0325725078582764, 0.7661158442497253, 0.6357192397117615, 0.5439318418502808, 0.4688139259815216, 0.4029649496078491, 0.34363535046577454, 0.2893659472465515, 0.2401498705148697, 0.19690749049186707, 0.1593702733516693, 0.12807410955429077, 0.1035502552986145, 0.08518311381340027, 0.07225949317216873, 0.06412948668003082, 0.059149209409952164, 0.05615196377038956, 0.05435546860098839, 0.05326187610626221, 0.0525568425655365, 0.05206466466188431, 0.05169462040066719, 0.05140690505504608, 0.05117487162351608, 0.050982046872377396, 0.050817303359508514, 0.05067438259720802, 0.05054789036512375, 0.050434719771146774, 0.05033190920948982, 0.0502377450466156, 0.050150930881500244, 0.05007020756602287, 0.049994729459285736, 0.049923837184906006, 0.04985694959759712, 0.04979357495903969, 0.04973330721259117, 0.04967576265335083, 0.04962075501680374, 0.049568090587854385, 0.0495174340903759, 0.04946866258978844, 0.04942164570093155, 0.04937626048922539, 0.04933232069015503, 0.0492897592484951, 0.049248527735471725, 0.04920845106244087, 0.049169547855854034, 0.049131669104099274, 0.049094825983047485, 0.0490589514374733, 0.049023907631635666, 0.048989761620759964, 0.048956383019685745, 0.04892374202609062, 0.04889184236526489, 0.04886062443256378, 0.04883002117276192, 0.048800066113471985, 0.04877075180411339, 0.04874200373888016, 0.048713769763708115, 0.04868609458208084, 0.04865894466638565, 0.048632264137268066, 0.048606064170598984, 0.04858030006289482, 0.04855499789118767, 0.04853013530373573, 0.04850565642118454, 0.04848159849643707, 0.04845789447426796, 0.04843456670641899, 0.04841161519289017, 0.04838898405432701, 0.048366691917181015, 0.04834471270442009, 0.04832307994365692, 0.04830174893140793, 0.04828070476651192, 0.04825994372367859, 0.04823948070406914, 0.04821930453181267, 0.048199381679296494, 0.04817970097064972, 0.048160284757614136, 0.04814111068844795, 0.04812218248844147, 0.048103488981723785, 0.04808502271771431, 0.04806677624583244, 0.04804873466491699, 0.04803090915083885, 0.048013295978307724, 0.047995880246162415, 0.047978661954402924, 0.047961633652448654, 0.0479448027908802, 0.047928161919116974, 0.047911688685417175, 0.0478954054415226, 0.04787927493453026, 0.04786332696676254, 0.04784754291176796, 0.04783192276954651, 0.04781645908951759, 0.04780115187168121, 0.04778599366545677, 0.04777098074555397, 0.04775611311197281, 0.04774140566587448, 0.0477268360555172, 0.04771240055561066, 0.047698091715574265, 0.047683924436569214, 0.04766988754272461, 0.047655969858169556, 0.04764218255877495, 0.047628529369831085, 0.047614987939596176, 0.047601569443941116, 0.04758826643228531, 0.047575078904628754, 0.04756200313568115, 0.0475490428507328, 0.04753618687391281, 0.04752344265580177, 0.04751080647110939, 0.04749826714396477, 0.0474858321249485, 0.04747350513935089, 0.047461267560720444, 0.04744913429021835, 0.047437094151973724, 0.04742515832185745, 0.047413311898708344, 0.0474015474319458, 0.04738987609744072, 0.0473782978951931, 0.047366801649332047, 0.047355394810438156, 0.04734406620264053, 0.04733283072710037, 0.04732167348265648, 0.04731059446930885, 0.04729960486292839, 0.0472886823117733, 0.047277841717004776, 0.04726707935333252, 0.04725639149546623, 0.04724578186869621, 0.047235239297151566, 0.04722476378083229, 0.04721437394618988, 0.047204047441482544, 0.04719379544258118, 0.04718360677361488, 0.04717349261045456, 0.04716344177722931, 0.04715345799922943, 0.04714353755116463, 0.047133687883615494, 0.04712389409542084, 0.04711416736245155, 0.04710450395941734, 0.04709490388631821, 0.04708535596728325, 0.047075867652893066, 0.047066446393728256, 0.047057073563337326, 0.04704776406288147, 0.04703851044178009, 0.04702931270003319, 0.047020167112350464, 0.047011084854602814, 0.047002047300338745, 0.046993061900138855, 0.046984124928712845], "moving_avg_accuracy_train": [0.047237398774916936, 0.10402884121793095, 0.17079802459682997, 0.2359210104411798, 0.29718386336993, 0.35435475597004484, 0.40743830838153056, 0.45648761500184926, 0.5016270825529747, 0.5430965242203886, 0.5811095188198521, 0.6159908207676933, 0.6480001569552742, 0.6777083200157453, 0.7052245195237223, 0.7310912556654346, 0.7550270101572614, 0.7773898946320299, 0.7981000669616932, 0.8170763686357712, 0.8343875550233938, 0.8501187574448732, 0.8644186737015855, 0.8773420767552457, 0.8890266179261589, 0.8996008337002189, 0.9091594805754443, 0.9177994651440995, 0.9256149787856511, 0.932676842848762, 0.9390534468448474, 0.9448133167806099, 0.9500158009132724, 0.9547003617814782, 0.9589257671581015, 0.9627286319970625, 0.9661605109473655, 0.9692492020026382, 0.9720406496964312, 0.974552952620845, 0.9768233258480553, 0.9788620114549257, 0.9806991536499186, 0.9823525816254121, 0.9838429919521657, 0.9851866863950537, 0.9864006616912718, 0.9874932394578682, 0.9884765594478048, 0.9893638725875573, 0.9901624544133346, 0.9908835032053437, 0.9915301219693424, 0.9921144040057508, 0.9926449081361374, 0.9931223618534852, 0.9935520701990984, 0.9939411328589597, 0.9942912892528348, 0.9946064300073225, 0.9948923818351709, 0.9951497384802345, 0.9953813594607916, 0.9955944686409122, 0.9957862669030206, 0.9959612104877278, 0.9961209848627737, 0.996264781800315, 0.9963941990441022, 0.9965106745635107, 0.9966155025309783, 0.9967098477016991, 0.9967947583553479, 0.9968758282412509, 0.9969511162873731, 0.997018875528883, 0.997079858846242, 0.9971347438318651, 0.9971864654677355, 0.9972330149400188, 0.9972749094650738, 0.9973102893888137, 0.9973421313201797, 0.997370789058409, 0.9973965810228154, 0.9974174686419717, 0.9974362674992123, 0.9974555116195384, 0.9974728313278318, 0.9974907442141056, 0.9975068658117519, 0.9975213752496336, 0.9975367588925366, 0.9975529293199588, 0.9975698078534483, 0.997584998533589, 0.9975986701457156, 0.9976109745966295, 0.9976220486024521, 0.9976343403565018, 0.9976454029351466, 0.9976553592559269, 0.9976643199446291, 0.9976723845644612, 0.9976819678711195, 0.9976929179959215, 0.9977027731082433, 0.9977116427093329, 0.9977219504991232, 0.9977312275099344, 0.9977395768196645, 0.9977470911984216, 0.9977538541393031, 0.9977599407860963, 0.9977654187682102, 0.9977703489521127, 0.9977747861176249, 0.997778779566586, 0.9977846988194604, 0.9977900261470474, 0.9977948207418756, 0.9977991358772211, 0.997803019499032, 0.9978065147586619, 0.9978096604923288, 0.997812491652629, 0.9978150396968991, 0.9978173329367422, 0.997819396852601, 0.997821254376874, 0.9978229261487196, 0.9978244307433807, 0.9978257848785757, 0.9978270036002511, 0.997828100449759, 0.9978314127631256, 0.9978343938451556, 0.9978370768189826, 0.9978394914954268, 0.9978416647042266, 0.9978436205921465, 0.9978453808912744, 0.9978469651604894, 0.9978530052515834, 0.9978607664823774, 0.9978700767389016, 0.9978807811185828, 0.9978904150602959, 0.9978990856078378, 0.9979068891006255, 0.9979139122441345, 0.9979202330732926, 0.9979259218195347, 0.9979310416911527, 0.9979356495756089, 0.9979397966716195, 0.997943529058029, 0.9979468882057976, 0.9979499114387893, 0.9979526323484817, 0.9979550811672051, 0.9979572851040561, 0.9979592686472218, 0.997961053836071, 0.9979626605060354, 0.9979641065090032, 0.9979654079116744, 0.9979665791740784, 0.9979676333102421, 0.9979685820327893, 0.9979694358830817, 0.9979702043483449, 0.9979708959670819, 0.997971518423945, 0.997972078635122, 0.9979725828251813, 0.9979730365962347, 0.9979734449901827, 0.9979738125447359, 0.9979741433438338, 0.9979744410630218, 0.9979747090102911], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.04750050004706324, 0.10271785873964606, 0.16578268101703686, 0.22607311820524278, 0.28167228464149563, 0.33283869934376176, 0.37897701032053016, 0.4205269337707813, 0.4577499369298478, 0.49079589109077865, 0.5200580870908875, 0.5459494628226873, 0.569093009575057, 0.5901307506920995, 0.609298710308507, 0.6267696005257738, 0.6426409156049735, 0.6571601212959822, 0.6710127744525286, 0.6835534044809203, 0.6948755630915632, 0.7051631620911418, 0.714494213869603, 0.7229664321663776, 0.7305670145709747, 0.7373342965476122, 0.7434502938977456, 0.7489679280527753, 0.7539836564259617, 0.7585710541493293, 0.7627119191316103, 0.7664885552493228, 0.769899734786514, 0.7730928961911457, 0.7760643977053143, 0.778763163130566, 0.7812042590445426, 0.7834256594296215, 0.7854249197761924, 0.7871876329943563, 0.7887740748907038, 0.7902018725974166, 0.791499097564708, 0.7927032211290204, 0.7938235534306515, 0.7948440595333695, 0.7957625150258156, 0.7965891249690172, 0.7973452809491486, 0.7980380283625168, 0.7986737080657983, 0.7992336127675016, 0.7997375269990346, 0.8001788427761642, 0.8005760269755811, 0.800909078692556, 0.8012088252378335, 0.8014541830660833, 0.8016627980802582, 0.8018383445617655, 0.8020339869975317, 0.8021978581584713, 0.8023575492345669, 0.802501271203053, 0.8026306209746905, 0.8027226217066642, 0.8028054223654405, 0.8028677359270893, 0.8029238181325732, 0.8029987061800087, 0.8030783124539507, 0.8031499581004984, 0.803239882753551, 0.8033330219725483, 0.803429054300896, 0.8035276904276587, 0.8036408770042452, 0.8037803955255828, 0.8039059621947865, 0.8040189721970699, 0.8041206811991249, 0.8042122193009744, 0.804294603592639, 0.8043687494551371, 0.8044354807313855, 0.8044955388800089, 0.80454959121377, 0.804598238314155, 0.8046542277357516, 0.8047046182151885, 0.8047621766779316, 0.8048261863256505, 0.8048837950085975, 0.8049356428232498, 0.8049700988251869, 0.8050011092269302, 0.8050412256197492, 0.8050895374045364, 0.8051330180108448, 0.8051721505565224, 0.8052195768788821, 0.8052866746315059, 0.8053470626088675, 0.8054014117884927, 0.8054381190189055, 0.805471155526277, 0.8055008883829113, 0.8055398549851323, 0.8055749249271311, 0.80560648787493, 0.805647101559199, 0.8056714468437911, 0.8056933575999241, 0.8057130772804437, 0.8057308249929114, 0.8057467979341324, 0.8057733806124812, 0.8057973050229951, 0.8058188369924576, 0.8058504227962239, 0.8058788500196136, 0.8059166415519142, 0.8059506539309849, 0.8059812650721484, 0.8060088150991955, 0.8060214030922881, 0.8060205252548214, 0.8060075281698513, 0.8060080378246283, 0.8060084965139275, 0.8060089093342968, 0.8060469314750388, 0.8060811514017067, 0.8061119493357077, 0.8061396674763087, 0.8061401997403495, 0.8061406787779861, 0.8061411099118592, 0.8061414979323449, 0.8061418471507822, 0.8061421614473756, 0.8061424443143097, 0.8061304918633004, 0.8061319416886421, 0.8061332465314497, 0.8061344208899763, 0.8061354778126504, 0.806148636074307, 0.806160478509798, 0.8061711367017399, 0.8061807290744876, 0.8061893622099605, 0.8061971320318861, 0.8062041248716192, 0.806210418427379, 0.8062160826275627, 0.8062211804077282, 0.806225768409877, 0.8062176905805609, 0.8062104205341765, 0.8062038774924305, 0.8061979887548592, 0.8061926888910449, 0.8061879190136121, 0.8061836261239226, 0.8061797625232021, 0.8061762852825536, 0.8061609487347199, 0.8061593528729196, 0.8061579165972994, 0.8061566239492413, 0.8061554605659889, 0.8061666205523117, 0.8061766645400023, 0.8061857041289237, 0.806206046790203, 0.8062243551853544, 0.8062408327409907, 0.8062556625410633, 0.8062690093611287, 0.8062810214991876, 0.8062918324234405], "moving_var_accuracy_train": [0.02008234658718472, 0.04710152334128982, 0.08251448564892624, 0.11243206665158414, 0.13496709432715284, 0.1508869835406823, 0.16115905701622066, 0.16669566163400495, 0.16836423924779648, 0.1670052466528802, 0.1633096118133618, 0.1579289976622139, 0.15135747632451876, 0.14416490326391473, 0.1365626840557884, 0.12892820799783425, 0.12119167028588788, 0.11357339067558568, 0.1060762527493463, 0.09870952770144222, 0.09153566949862109, 0.08460933911538901, 0.077988793648391, 0.07169304340193808, 0.06575249558311717, 0.06018357237793287, 0.0549875247109069, 0.05016061623993562, 0.04569429489727358, 0.04157369472395892, 0.03778227495826906, 0.034302632377534274, 0.031115961710136295, 0.028201871533873994, 0.02554237083585777, 0.023118289781122642, 0.020912460941176172, 0.01890707495897285, 0.017086497085120205, 0.015434652370464345, 0.013937578484735418, 0.012581226787294819, 0.011353479931566947, 0.010242736355041556, 0.009238454626016248, 0.008330858796217254, 0.00751103654077398, 0.006770676422281128, 0.006102311043876498, 0.005499165860960646, 0.004954988871256738, 0.004464169186375185, 0.0040215153101712635, 0.003622436248636762, 0.0032627255354643007, 0.0029385046403877547, 0.002646316019709585, 0.0023830467455183122, 0.002145845556468026, 0.001932154824077475, 0.0017396752577003762, 0.001566303823915164, 0.0014101562760313563, 0.0012695493881320855, 0.0011429255284790077, 0.0010289084229515787, 0.0009262473313147128, 0.0008338086962164581, 0.0007505785660017176, 0.0006756428083211393, 0.0006081774276138959, 0.0005474397939536516, 0.0004927607029302141, 0.0004435437835747956, 0.0003992404198263162, 0.0003593576996769747, 0.00032345540039424223, 0.00029113697160963964, 0.00026204735059722964, 0.00023586211721783536, 0.00021229170185711806, 0.00019107379732244086, 0.00017197554276753482, 0.00015478537988442513, 0.0001393128289248341, 0.00012538547266605684, 0.00011285010597275313, 0.00010156842840098195, 9.14142853115421e-05, 8.227574462383977e-05, 7.405050931465181e-05, 6.66473530972754e-05, 5.9984747695768574e-05, 5.398862627069888e-05, 4.8592327607663814e-05, 4.373517165776564e-05, 3.936333670879232e-05, 3.542836563352371e-05, 3.188663277261594e-05, 2.869932928031292e-05, 2.5830497778098073e-05, 2.3248340155199585e-05, 2.0924228785157796e-05, 1.8832391249479333e-05, 1.694997868242997e-05, 1.5256059961285599e-05, 1.3731328074306926e-05, 1.2358903295287643e-05, 1.1123969220532115e-05, 1.0012346864845225e-05, 9.011739577117422e-06, 8.111073812398621e-06, 7.300378067483041e-06, 6.570673686157405e-06, 5.913876392134025e-06, 5.322707513340436e-06, 4.79061395794644e-06, 4.311696090863235e-06, 3.880841819768236e-06, 3.493013061564386e-06, 3.1439186486640564e-06, 2.829694367335099e-06, 2.546860673266922e-06, 2.29228455749915e-06, 2.0631451625119598e-06, 1.8569027854785709e-06, 1.6712709396971375e-06, 1.5041911762682269e-06, 1.3538103963794548e-06, 1.2184604103093305e-06, 1.0966395226683325e-06, 9.869959446473468e-07, 8.883128533217485e-07, 7.994949355322742e-07, 7.195562696886342e-07, 6.47699385498322e-07, 5.830094285991165e-07, 5.247732708762126e-07, 4.7234841974956767e-07, 4.2515608330300206e-07, 3.826749044506989e-07, 3.4443530188280673e-07, 3.100143608750402e-07, 2.7934126909134134e-07, 2.519492725131532e-07, 2.2753447315074947e-07, 2.0581227953492232e-07, 1.8606636707782234e-07, 1.6813633592211777e-07, 1.518707528270876e-07, 1.3712759844709473e-07, 1.2377441453358393e-07, 1.1168822958449653e-07, 1.0075532439450666e-07, 9.087088534750803e-08, 8.193858246064294e-08, 7.387010058936585e-08, 6.658464539400659e-08, 6.000844029410319e-08, 5.407422641068567e-08, 4.8720774187871144e-08, 4.3892412807869386e-08, 3.953858151849861e-08, 3.56134054596961e-08, 3.20752974090948e-08, 2.8886585989433657e-08, 2.6013170230701435e-08, 2.3424199908202278e-08, 2.1091780744844474e-08, 1.8990703340604537e-08, 1.7098194549442193e-08, 1.5393689944245505e-08, 1.3858625978116484e-08, 1.2476250453224256e-08, 1.1231449936966555e-08, 1.0110592811812276e-08, 9.101386704150313e-09, 8.192749104285837e-09, 7.374690061003163e-09, 6.6382059072909915e-09, 5.975183046996299e-09, 5.378310903948553e-09], "duration": 92159.878874, "accuracy_train": [0.47237398774916944, 0.6151518232050572, 0.7717206750069213, 0.8220278830403286, 0.8485495397286821, 0.868892789371078, 0.8851902800849022, 0.8979313745847176, 0.9078822905131044, 0.9163214992271133, 0.9232264702150241, 0.9299225382982651, 0.9360841826435032, 0.9450817875599853, 0.952870315095515, 0.9638918809408453, 0.9704488005837025, 0.9786558549049464, 0.9844916179286637, 0.9878630837024732, 0.9901882325119971, 0.9916995792381875, 0.9931179200119971, 0.9936527042381875, 0.994187488464378, 0.9947687756667589, 0.9951873024524732, 0.9955593262619971, 0.9959546015596161, 0.9962336194167589, 0.9964428828096161, 0.9966521462024732, 0.9968381581072352, 0.9968614095953304, 0.9969544155477114, 0.9969544155477114, 0.9970474215000923, 0.9970474215000923, 0.9971636789405685, 0.9971636789405685, 0.9972566848929494, 0.9972101819167589, 0.9972334334048542, 0.9972334334048542, 0.9972566848929494, 0.9972799363810447, 0.9973264393572352, 0.9973264393572352, 0.9973264393572352, 0.9973496908453304, 0.9973496908453304, 0.9973729423334257, 0.9973496908453304, 0.9973729423334257, 0.9974194453096161, 0.9974194453096161, 0.9974194453096161, 0.9974426967977114, 0.9974426967977114, 0.9974426967977114, 0.9974659482858066, 0.9974659482858066, 0.9974659482858066, 0.9975124512619971, 0.9975124512619971, 0.9975357027500923, 0.9975589542381875, 0.9975589542381875, 0.9975589542381875, 0.9975589542381875, 0.9975589542381875, 0.9975589542381875, 0.9975589542381875, 0.997605457214378, 0.9976287087024732, 0.9976287087024732, 0.9976287087024732, 0.9976287087024732, 0.9976519601905685, 0.9976519601905685, 0.9976519601905685, 0.9976287087024732, 0.9976287087024732, 0.9976287087024732, 0.9976287087024732, 0.997605457214378, 0.997605457214378, 0.9976287087024732, 0.9976287087024732, 0.9976519601905685, 0.9976519601905685, 0.9976519601905685, 0.9976752116786637, 0.9976984631667589, 0.9977217146548542, 0.9977217146548542, 0.9977217146548542, 0.9977217146548542, 0.9977217146548542, 0.9977449661429494, 0.9977449661429494, 0.9977449661429494, 0.9977449661429494, 0.9977449661429494, 0.9977682176310447, 0.99779146911914, 0.99779146911914, 0.99779146911914, 0.9978147206072352, 0.9978147206072352, 0.9978147206072352, 0.9978147206072352, 0.9978147206072352, 0.9978147206072352, 0.9978147206072352, 0.9978147206072352, 0.9978147206072352, 0.9978147206072352, 0.9978379720953304, 0.9978379720953304, 0.9978379720953304, 0.9978379720953304, 0.9978379720953304, 0.9978379720953304, 0.9978379720953304, 0.9978379720953304, 0.9978379720953304, 0.9978379720953304, 0.9978379720953304, 0.9978379720953304, 0.9978379720953304, 0.9978379720953304, 0.9978379720953304, 0.9978379720953304, 0.9978379720953304, 0.9978612235834257, 0.9978612235834257, 0.9978612235834257, 0.9978612235834257, 0.9978612235834257, 0.9978612235834257, 0.9978612235834257, 0.9978612235834257, 0.9979073660714286, 0.9979306175595238, 0.9979538690476191, 0.9979771205357143, 0.9979771205357143, 0.9979771205357143, 0.9979771205357143, 0.9979771205357143, 0.9979771205357143, 0.9979771205357143, 0.9979771205357143, 0.9979771205357143, 0.9979771205357143, 0.9979771205357143, 0.9979771205357143, 0.9979771205357143, 0.9979771205357143, 0.9979771205357143, 0.9979771205357143, 0.9979771205357143, 0.9979771205357143, 0.9979771205357143, 0.9979771205357143, 0.9979771205357143, 0.9979771205357143, 0.9979771205357143, 0.9979771205357143, 0.9979771205357143, 0.9979771205357143, 0.9979771205357143, 0.9979771205357143, 0.9979771205357143, 0.9979771205357143, 0.9979771205357143, 0.9979771205357143, 0.9979771205357143, 0.9979771205357143, 0.9979771205357143, 0.9979771205357143], "end": "2016-02-03 10:11:54.045000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0], "moving_var_accuracy_valid": [0.020306677542489503, 0.04571662009710872, 0.07693950436730784, 0.10195998527768212, 0.11958539252556918, 0.13118887121437073, 0.13722867775103456, 0.13904337522442672, 0.13760900537960288, 0.1336764206192997, 0.12801526359010884, 0.12124700726666504, 0.11394292034651068, 0.10653190727362892, 0.0991854126289721, 0.09201395941092898, 0.08507965125092518, 0.07846895213091303, 0.07234912091310189, 0.0665296154353727, 0.06103037537227642, 0.055879850073633955, 0.05107548181190146, 0.046613939976525146, 0.04247246565487428, 0.03863738403754876, 0.03511029444607543, 0.03187326358148663, 0.028912355003360243, 0.02621051746387545, 0.02374378658270123, 0.0214977747477216, 0.01945272258546401, 0.017599216844721877, 0.01591876355148804, 0.014392437209724045, 0.013006824032102736, 0.01175055320592992, 0.010611471262737268, 0.00957828855746895, 0.008643110882736435, 0.00779714725108444, 0.007032577659517875, 0.006342369115589277, 0.005719428504225052, 0.005156858548153709, 0.00464876473776278, 0.0041900378199703, 0.0037761799847698665, 0.003402881077101436, 0.0030662297675577684, 0.002762428230276897, 0.0024884707732238803, 0.0022413765324377846, 0.002018658676788403, 0.001817791120125183, 0.001636820640035317, 0.0014736803802067363, 0.001326704024203315, 0.00119431097088751, 0.0010752243574628118, 0.0009679436055330199, 0.0008713787561377791, 0.0007844267845620309, 0.0007061346883766318, 0.0006355973967511219, 0.0005720993606178539, 0.0005149243713757567, 0.0004634602411621284, 0.0004171646910227538, 0.00037550525635013664, 0.0003380009286031461, 0.00030427361373187113, 0.0002739243265857229, 0.0002466148937999414, 0.0002220409661894722, 0.0001999521703805994, 0.00018013214210270534, 0.00016226083078816917, 0.0001461496890548971, 0.00013162782263929874, 0.00011854045339218085, 0.00010674749259658048, 9.612222181725278e-05, 8.655007720459508e-05, 7.792753231508032e-05, 7.016107397663754e-05, 6.316626544235665e-05, 5.687785223609743e-05, 5.121291981624861e-05, 4.6121444624324e-05, 4.1546175276901464e-05, 3.7421426592369305e-05, 3.3703477696090345e-05, 3.0343814871106695e-05, 2.731808818914259e-05, 2.4600763294983624e-05, 2.2161693222429116e-05, 1.996253896831073e-05, 1.798006727666052e-05, 1.6202303853467664e-05, 1.4622592443785424e-05, 1.3193153569695145e-05, 1.1900422712659126e-05, 1.0722507228274405e-05, 9.660079202820726e-06, 8.702027667411401e-06, 7.845490465468048e-06, 7.072010526407448e-06, 6.373775450830528e-06, 5.751243147896626e-06, 5.181453069043817e-06, 4.667628493248287e-06, 4.204365436121628e-06, 3.786763724189983e-06, 3.410383565432203e-06, 3.0757049579827494e-06, 2.7732858589504272e-06, 2.500129904435807e-06, 2.2590958809882884e-06, 2.0404592561562693e-06, 1.8492671297633509e-06, 1.6747519941574102e-06, 1.5157101724116891e-06, 1.370970191083239e-06, 1.2352992901057845e-06, 1.1117762964827686e-06, 1.00211898479397e-06, 9.019094240464978e-07, 8.11720375204707e-07, 7.305498714701521e-07, 6.70506033002577e-07, 6.13994460132664e-07, 5.611316287679735e-07, 5.119331237565191e-07, 4.6074236112594953e-07, 4.1467019030687135e-07, 3.7320484416393283e-07, 3.3588571478661593e-07, 3.0229824088960617e-07, 2.720693058417835e-07, 2.4486309538092696e-07, 2.216625356089997e-07, 1.9951519998979098e-07, 1.7957900352358162e-07, 1.616335152327669e-07, 1.4548021747934044e-07, 1.3249045437982908e-07, 1.205035984470686e-07, 1.0947561210159136e-07, 9.935617342580824e-08, 9.009133533607196e-08, 8.162553299726871e-08, 7.390307796533318e-08, 6.686924976571099e-08, 6.047107326263794e-08, 5.465785219990746e-08, 4.938151485337856e-08, 4.5030625306169076e-08, 4.1003244945435245e-08, 3.728822300849675e-08, 3.387149577930728e-08, 3.073714320942131e-08, 2.7868194464995334e-08, 2.5247235135473983e-08, 2.2856858316679416e-08, 2.0679993307761614e-08, 2.0728881272077228e-08, 1.8678914118840705e-08, 1.682958869587346e-08, 1.5161668277308962e-08, 1.3657682594906742e-08, 1.3412821987940342e-08, 1.2979474987690122e-08, 1.2416954999741741e-08, 1.4899674311091137e-08, 1.64264828771532e-08, 1.7227423147145986e-08, 1.748398756417451e-08, 1.7338827260468688e-08, 1.6903567681116936e-08, 1.626509566182742e-08], "accuracy_test": 0.7861069036989796, "start": "2016-02-02 08:35:54.166000", "learning_rate_per_epoch": [0.0009479781147092581, 0.00047398905735462904, 0.00031599271460436285, 0.00023699452867731452, 0.00018959562294185162, 0.00015799635730218142, 0.0001354254491161555, 0.00011849726433865726, 0.00010533090244280174, 9.479781147092581e-05, 8.617982530267909e-05, 7.899817865109071e-05, 7.29213934391737e-05, 6.771272455807775e-05, 6.3198538555298e-05, 5.924863216932863e-05, 5.576341936830431e-05, 5.266545122140087e-05, 4.989358421880752e-05, 4.7398905735462904e-05, 4.5141816372051835e-05, 4.3089912651339546e-05, 4.1216440877178684e-05, 3.9499089325545356e-05, 3.7919126043561846e-05, 3.646069671958685e-05, 3.511030081426725e-05, 3.3856362279038876e-05, 3.268890213803388e-05, 3.1599269277649e-05, 3.0579940357711166e-05, 2.9624316084664315e-05, 2.8726610253215767e-05, 2.7881709684152156e-05, 2.708508873183746e-05, 2.6332725610700436e-05, 2.5621029635658488e-05, 2.494679210940376e-05, 2.4307131752721034e-05, 2.3699452867731452e-05, 2.3121418053051457e-05, 2.2570908186025918e-05, 2.204600241384469e-05, 2.1544956325669773e-05, 2.1066180124762468e-05, 2.0608220438589342e-05, 2.0169747585896403e-05, 1.9749544662772678e-05, 1.9346492990734987e-05, 1.8959563021780923e-05, 1.858780706243124e-05, 1.8230348359793425e-05, 1.7886379282572307e-05, 1.7555150407133624e-05, 1.723596506053582e-05, 1.6928181139519438e-05, 1.6631194739602506e-05, 1.634445106901694e-05, 1.606742625881452e-05, 1.57996346388245e-05, 1.55406250996748e-05, 1.5289970178855583e-05, 1.5047271517687477e-05, 1.4812158042332157e-05, 1.458427868783474e-05, 1.4363305126607884e-05, 1.4148927220958285e-05, 1.3940854842076078e-05, 1.3738813322561327e-05, 1.354254436591873e-05, 1.335180422756821e-05, 1.3166362805350218e-05, 1.298600182053633e-05, 1.2810514817829244e-05, 1.263970807485748e-05, 1.247339605470188e-05, 1.23114041343797e-05, 1.2153565876360517e-05, 1.1999723028566223e-05, 1.1849726433865726e-05, 1.1703433301590849e-05, 1.1560709026525728e-05, 1.142142264143331e-05, 1.1285454093012959e-05, 1.1152683327964041e-05, 1.1023001206922345e-05, 1.089630040951306e-05, 1.0772478162834886e-05, 1.065143987943884e-05, 1.0533090062381234e-05, 1.04173423096654e-05, 1.0304110219294671e-05, 1.0193312846240588e-05, 1.0084873792948201e-05, 9.978716661862563e-06, 9.874772331386339e-06, 9.77296986093279e-06, 9.673246495367493e-06, 9.575536751071922e-06, 9.479781510890462e-06, 9.385921657667495e-06, 9.29390353121562e-06, 9.203670742863324e-06, 9.115174179896712e-06, 9.028362910612486e-06, 8.943189641286153e-06, 8.859608897182625e-06, 8.777575203566812e-06, 8.697046723682433e-06, 8.61798253026791e-06, 8.540343515051063e-06, 8.464090569759719e-06, 8.389186405111104e-06, 8.315597369801253e-06, 8.243287993536796e-06, 8.17222553450847e-06, 8.102377250907011e-06, 8.03371312940726e-06, 7.966202247189358e-06, 7.89981731941225e-06, 7.834530151740182e-06, 7.7703125498374e-06, 7.707139047852252e-06, 7.644985089427792e-06, 7.583824753965018e-06, 7.523635758843739e-06, 7.464394457201706e-06, 7.406079021166079e-06, 7.3486676228640135e-06, 7.29213934391737e-06, 7.236474175442709e-06, 7.181652563303942e-06, 7.127654953364981e-06, 7.0744636104791425e-06, 7.022059890005039e-06, 6.970427421038039e-06, 6.919548468431458e-06, 6.869406661280664e-06, 6.819986538175726e-06, 6.771272182959365e-06, 6.7232490437163506e-06, 6.675902113784105e-06, 6.6292177507421e-06, 6.583181402675109e-06, 6.537779881909955e-06, 6.493000910268165e-06, 6.448830845329212e-06, 6.405257408914622e-06, 6.362269232340623e-06, 6.31985403742874e-06, 6.278000910242554e-06, 6.23669802735094e-06, 6.195935384312179e-06, 6.15570206718985e-06, 6.115987616794882e-06, 6.0767829381802585e-06, 6.038077117409557e-06, 5.999861514283111e-06, 5.9621265791065525e-06, 5.924863216932863e-06, 5.888062787562376e-06, 5.851716650795424e-06, 5.8158166211796924e-06, 5.780354513262864e-06, 5.745321686845273e-06, 5.710711320716655e-06, 5.6765156841720454e-06, 5.642727046506479e-06, 5.6093381317623425e-06, 5.576341663982021e-06, 5.543731731449952e-06, 5.5115006034611724e-06, 5.479642368300119e-06, 5.44815020475653e-06, 5.417017746367492e-06, 5.386239081417443e-06, 5.355808752938174e-06, 5.32571993971942e-06, 5.295967184792971e-06, 5.266545031190617e-06, 5.237448021944147e-06, 5.2086711548327e-06], "accuracy_train_first": 0.47237398774916944, "accuracy_train_last": 0.9979771205357143, "batch_size_eval": 1024, "accuracy_train_std": [0.01903532015216319, 0.021930939653987618, 0.02546529637453701, 0.025164365185369502, 0.0278205777308501, 0.027174573070437764, 0.026258632202228096, 0.02568899107466216, 0.025090570425553158, 0.023930938156686666, 0.02230341391859578, 0.021204682234054806, 0.019937178755524126, 0.01770990206230846, 0.01661936940247921, 0.013888207526531032, 0.012390166412769988, 0.009347750362478773, 0.00787450315342586, 0.00656934129064349, 0.005123695852528584, 0.004263937168661201, 0.0035914769279583795, 0.0031845941540302866, 0.002898627638134384, 0.00264345818924095, 0.002443157655595891, 0.002356137398023891, 0.002061766043706016, 0.0020171826399589836, 0.0020423159207216225, 0.001989581661682651, 0.0019277183604777498, 0.001982602354964478, 0.0019902061244856765, 0.0019902061244856765, 0.0020273301930547375, 0.0020273301930547375, 0.0018185559711811176, 0.0018185559711811176, 0.0018113885402334942, 0.0018404145685606982, 0.001782055562048346, 0.001730338052371096, 0.0017345460696024067, 0.0016853773599765673, 0.0016098983621951424, 0.0016098983621951424, 0.0016098983621951424, 0.0016270958907775624, 0.0016270958907775624, 0.0016437846804172897, 0.001668436461434257, 0.0016437846804172897, 0.00163453943754911, 0.00163453943754911, 0.00163453943754911, 0.0016224168068436728, 0.0016224168068436728, 0.0016224168068436728, 0.001537727817439398, 0.001537727817439398, 0.001537727817439398, 0.0014642389080793434, 0.0014642389080793434, 0.0014492024180802706, 0.0014493831503418083, 0.0014493831503418083, 0.0014493831503418083, 0.0014493831503418083, 0.0014493831503418083, 0.0014493831503418083, 0.0014493831503418083, 0.0014486253698430698, 0.0014632869173788737, 0.0014632869173788737, 0.0014632869173788737, 0.0014632869173788737, 0.001461987488629114, 0.001461987488629114, 0.001461987488629114, 0.001509121529443604, 0.001509121529443604, 0.001509121529443604, 0.001509121529443604, 0.0015398031816660992, 0.0015398031816660992, 0.0015536045116722073, 0.0015536045116722073, 0.0015228459106725511, 0.0015228459106725511, 0.0015228459106725511, 0.001536095791699442, 0.001504260763114573, 0.0014713698862685932, 0.0014713698862685932, 0.0014713698862685932, 0.0014713698862685932, 0.0014713698862685932, 0.001453062297452795, 0.001453062297452795, 0.001453062297452795, 0.001453062297452795, 0.001453062297452795, 0.0014341441611435896, 0.001414590981947934, 0.001414590981947934, 0.001414590981947934, 0.0013943760447158093, 0.0013943760447158093, 0.0013943760447158093, 0.0013943760447158093, 0.0013943760447158093, 0.0013943760447158093, 0.0013943760447158093, 0.0013943760447158093, 0.0013943760447158093, 0.0013943760447158093, 0.0013399977884970104, 0.0013399977884970104, 0.0013399977884970104, 0.0013399977884970104, 0.0013399977884970104, 0.0013399977884970104, 0.0013399977884970104, 0.0013399977884970104, 0.0013399977884970104, 0.0013399977884970104, 0.0013399977884970104, 0.0013399977884970104, 0.0013399977884970104, 0.0013399977884970104, 0.0013399977884970104, 0.0013399977884970104, 0.0013399977884970104, 0.0013004750637998325, 0.0013004750637998325, 0.0013004750637998325, 0.0013004750637998325, 0.0013004750637998325, 0.0013004750637998325, 0.0013004750637998325, 0.0013004750637998325, 0.0013405445782551687, 0.0012997976989744871, 0.0012752333706103132, 0.0013031209327754884, 0.0013031209327754884, 0.0013031209327754884, 0.0013031209327754884, 0.0013031209327754884, 0.0013031209327754884, 0.0013031209327754884, 0.0013031209327754884, 0.0013031209327754884, 0.0013031209327754884, 0.0013031209327754884, 0.0013031209327754884, 0.0013031209327754884, 0.0013031209327754884, 0.0013031209327754884, 0.0013031209327754884, 0.0013031209327754884, 0.0013031209327754884, 0.0013031209327754884, 0.0013031209327754884, 0.0013031209327754884, 0.0013031209327754884, 0.0013031209327754884, 0.0013031209327754884, 0.0013031209327754884, 0.0013031209327754884, 0.0013031209327754884, 0.0013031209327754884, 0.0013031209327754884, 0.0013031209327754884, 0.0013031209327754884, 0.0013031209327754884, 0.0013031209327754884, 0.0013031209327754884, 0.0013031209327754884, 0.0013031209327754884], "accuracy_test_std": 0.012786288535581603, "error_valid": [0.5249949995293675, 0.4003259130271084, 0.2666339184864458, 0.23131294710090367, 0.21793521743222888, 0.20666356833584332, 0.2057781908885542, 0.20552375517695776, 0.2072430346385542, 0.21179052146084332, 0.21658214890813254, 0.22102815559111444, 0.22261506965361444, 0.2205295792545181, 0.21818965314382532, 0.21599238751882532, 0.21451724868222888, 0.21216702748493976, 0.2043133471385542, 0.2035809252635542, 0.20322500941265065, 0.20224844691265065, 0.20152632012424698, 0.20078360316265065, 0.20102774378765065, 0.20176016566265065, 0.2015057299510542, 0.20137336455195776, 0.20087478821536142, 0.20014236634036142, 0.20002029602786142, 0.1995217196912651, 0.1993996493787651, 0.19816865116716864, 0.19719208866716864, 0.19694794804216864, 0.19682587772966864, 0.19658173710466864, 0.19658173710466864, 0.19694794804216864, 0.19694794804216864, 0.19694794804216864, 0.19682587772966864, 0.19645966679216864, 0.19609345585466864, 0.19597138554216864, 0.19597138554216864, 0.19597138554216864, 0.19584931522966864, 0.19572724491716864, 0.19560517460466864, 0.19572724491716864, 0.19572724491716864, 0.19584931522966864, 0.19584931522966864, 0.19609345585466864, 0.19609345585466864, 0.19633759647966864, 0.19645966679216864, 0.19658173710466864, 0.1962052310805723, 0.1963273013930723, 0.1962052310805723, 0.1962052310805723, 0.1962052310805723, 0.1964493717055723, 0.1964493717055723, 0.1965714420180723, 0.1965714420180723, 0.1963273013930723, 0.1962052310805723, 0.1962052310805723, 0.19595079536897586, 0.19582872505647586, 0.19570665474397586, 0.19558458443147586, 0.19534044380647586, 0.19496393778237953, 0.19496393778237953, 0.19496393778237953, 0.19496393778237953, 0.19496393778237953, 0.19496393778237953, 0.19496393778237953, 0.19496393778237953, 0.19496393778237953, 0.19496393778237953, 0.19496393778237953, 0.19484186746987953, 0.19484186746987953, 0.19471979715737953, 0.19459772684487953, 0.19459772684487953, 0.19459772684487953, 0.19471979715737953, 0.19471979715737953, 0.19459772684487953, 0.19447565653237953, 0.19447565653237953, 0.19447565653237953, 0.19435358621987953, 0.19410944559487953, 0.19410944559487953, 0.19410944559487953, 0.19423151590737953, 0.19423151590737953, 0.19423151590737953, 0.19410944559487953, 0.19410944559487953, 0.19410944559487953, 0.19398737528237953, 0.19410944559487953, 0.19410944559487953, 0.19410944559487953, 0.19410944559487953, 0.19410944559487953, 0.19398737528237953, 0.19398737528237953, 0.19398737528237953, 0.19386530496987953, 0.19386530496987953, 0.19374323465737953, 0.19374323465737953, 0.19374323465737953, 0.19374323465737953, 0.19386530496987953, 0.19398737528237953, 0.19410944559487953, 0.19398737528237953, 0.19398737528237953, 0.19398737528237953, 0.1936108692582832, 0.1936108692582832, 0.1936108692582832, 0.1936108692582832, 0.1938550098832832, 0.1938550098832832, 0.1938550098832832, 0.1938550098832832, 0.1938550098832832, 0.1938550098832832, 0.1938550098832832, 0.1939770801957832, 0.1938550098832832, 0.1938550098832832, 0.1938550098832832, 0.1938550098832832, 0.1937329395707832, 0.1937329395707832, 0.1937329395707832, 0.1937329395707832, 0.1937329395707832, 0.1937329395707832, 0.1937329395707832, 0.1937329395707832, 0.1937329395707832, 0.1937329395707832, 0.1937329395707832, 0.1938550098832832, 0.1938550098832832, 0.1938550098832832, 0.1938550098832832, 0.1938550098832832, 0.1938550098832832, 0.1938550098832832, 0.1938550098832832, 0.1938550098832832, 0.1939770801957832, 0.1938550098832832, 0.1938550098832832, 0.1938550098832832, 0.1938550098832832, 0.1937329395707832, 0.1937329395707832, 0.1937329395707832, 0.1936108692582832, 0.1936108692582832, 0.1936108692582832, 0.1936108692582832, 0.1936108692582832, 0.1936108692582832, 0.1936108692582832], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.015544868582811767, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 32, "valid_ratio": 0.15, "learning_rate": 0.0009479781139072401, "optimization": "rmsprop", "nb_data_augmentation": 0, "learning_rate_decay_method": "lin", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 6.755032001259992e-06, "rotation_range": [0, 0], "momentum": 0.7129965064978775}, "accuracy_valid_max": 0.8063891307417168, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8063891307417168, "accuracy_valid_std": [0.015855166726550445, 0.017088600295238697, 0.016295120133037404, 0.014059453609557404, 0.011230952176461138, 0.01307325706977801, 0.01093989245366431, 0.011893195226513805, 0.01240443052454667, 0.01325044502642223, 0.014137251030228656, 0.01582243356599898, 0.01419926743664547, 0.012818078330051077, 0.013985344400548593, 0.01257061098192732, 0.012841742085615766, 0.013629045461116311, 0.01326096673084846, 0.012300745228215402, 0.012065297933715202, 0.011465671823635185, 0.011325744943004172, 0.010859389904848983, 0.01016240022900813, 0.01019326937903449, 0.010697788595958619, 0.010421589325337188, 0.010307145795283983, 0.010012933474334261, 0.009133591816207032, 0.009014092677031663, 0.009297254018762052, 0.009743270505387147, 0.009723645196734831, 0.009706458997780401, 0.009459055183837588, 0.009240384337983075, 0.009005174528559387, 0.00911096444169434, 0.008804887605102221, 0.008804887605102221, 0.008545340426974043, 0.008241206813895017, 0.008319833880508483, 0.008351827677650655, 0.008351827677650655, 0.008351827677650655, 0.008223989823837416, 0.008107009802923953, 0.008031105819359235, 0.008077547324931123, 0.00763758024459034, 0.007511879618631274, 0.007511879618631274, 0.007694547126179109, 0.007694547126179109, 0.007850232332900864, 0.007749172642226915, 0.007503175065480763, 0.007756249158697056, 0.007916335110788014, 0.007878245340467717, 0.007878245340467717, 0.007878245340467717, 0.0077242405787933425, 0.0077242405787933425, 0.007493516128712178, 0.007493516128712178, 0.007562058864288159, 0.0075538043053463115, 0.007522175371876499, 0.007944999347895953, 0.008153686167928022, 0.008369635468093474, 0.008324521659906948, 0.008610417035766756, 0.009033413243468208, 0.009033413243468208, 0.009033413243468208, 0.009033413243468208, 0.009033413243468208, 0.009033413243468208, 0.009033413243468208, 0.009033413243468208, 0.009033413243468208, 0.009033413243468208, 0.009033413243468208, 0.008954956911306336, 0.008954956911306336, 0.009020684773739126, 0.008978702764641699, 0.008978702764641699, 0.008978702764641699, 0.009007459975379705, 0.009007459975379705, 0.008978702764641699, 0.008948188036754368, 0.008948188036754368, 0.008948188036754368, 0.008767589649039157, 0.008668950980046887, 0.008668950980046887, 0.008668950980046887, 0.008705581672484012, 0.008705581672484012, 0.008705581672484012, 0.008641404627504302, 0.008641404627504302, 0.008641404627504302, 0.00871292023863664, 0.008899653192073411, 0.008899653192073411, 0.008899653192073411, 0.008899653192073411, 0.008899653192073411, 0.00871292023863664, 0.00871292023863664, 0.00871292023863664, 0.008916864230325764, 0.008916864230325764, 0.008740734167222498, 0.008740734167222498, 0.008740734167222498, 0.008740734167222498, 0.008795720578221493, 0.008862143276598565, 0.008939747349701061, 0.008781063260240857, 0.008781063260240857, 0.008781063260240857, 0.009263885629476151, 0.009263885629476151, 0.009263885629476151, 0.009263885629476151, 0.00944528724110392, 0.00944528724110392, 0.00944528724110392, 0.00944528724110392, 0.00944528724110392, 0.00944528724110392, 0.00944528724110392, 0.009519835400874862, 0.00944528724110392, 0.00944528724110392, 0.00944528724110392, 0.00944528724110392, 0.009644430646337677, 0.009644430646337677, 0.009644430646337677, 0.009644430646337677, 0.009644430646337677, 0.009644430646337677, 0.009644430646337677, 0.009644430646337677, 0.009644430646337677, 0.009644430646337677, 0.009644430646337677, 0.009731242386576158, 0.009731242386576158, 0.009731242386576158, 0.009731242386576158, 0.009731242386576158, 0.009731242386576158, 0.009731242386576158, 0.009731242386576158, 0.009731242386576158, 0.009852135184592851, 0.009792301633518253, 0.009792301633518253, 0.009792301633518253, 0.009792301633518253, 0.00999645843716477, 0.00999645843716477, 0.00999645843716477, 0.009946486756971061, 0.009946486756971061, 0.009946486756971061, 0.009946486756971061, 0.009946486756971061, 0.009946486756971061, 0.009946486756971061], "accuracy_valid": [0.47500500047063254, 0.5996740869728916, 0.7333660815135542, 0.7686870528990963, 0.7820647825677711, 0.7933364316641567, 0.7942218091114458, 0.7944762448230422, 0.7927569653614458, 0.7882094785391567, 0.7834178510918675, 0.7789718444088856, 0.7773849303463856, 0.7794704207454819, 0.7818103468561747, 0.7840076124811747, 0.7854827513177711, 0.7878329725150602, 0.7956866528614458, 0.7964190747364458, 0.7967749905873494, 0.7977515530873494, 0.798473679875753, 0.7992163968373494, 0.7989722562123494, 0.7982398343373494, 0.7984942700489458, 0.7986266354480422, 0.7991252117846386, 0.7998576336596386, 0.7999797039721386, 0.8004782803087349, 0.8006003506212349, 0.8018313488328314, 0.8028079113328314, 0.8030520519578314, 0.8031741222703314, 0.8034182628953314, 0.8034182628953314, 0.8030520519578314, 0.8030520519578314, 0.8030520519578314, 0.8031741222703314, 0.8035403332078314, 0.8039065441453314, 0.8040286144578314, 0.8040286144578314, 0.8040286144578314, 0.8041506847703314, 0.8042727550828314, 0.8043948253953314, 0.8042727550828314, 0.8042727550828314, 0.8041506847703314, 0.8041506847703314, 0.8039065441453314, 0.8039065441453314, 0.8036624035203314, 0.8035403332078314, 0.8034182628953314, 0.8037947689194277, 0.8036726986069277, 0.8037947689194277, 0.8037947689194277, 0.8037947689194277, 0.8035506282944277, 0.8035506282944277, 0.8034285579819277, 0.8034285579819277, 0.8036726986069277, 0.8037947689194277, 0.8037947689194277, 0.8040492046310241, 0.8041712749435241, 0.8042933452560241, 0.8044154155685241, 0.8046595561935241, 0.8050360622176205, 0.8050360622176205, 0.8050360622176205, 0.8050360622176205, 0.8050360622176205, 0.8050360622176205, 0.8050360622176205, 0.8050360622176205, 0.8050360622176205, 0.8050360622176205, 0.8050360622176205, 0.8051581325301205, 0.8051581325301205, 0.8052802028426205, 0.8054022731551205, 0.8054022731551205, 0.8054022731551205, 0.8052802028426205, 0.8052802028426205, 0.8054022731551205, 0.8055243434676205, 0.8055243434676205, 0.8055243434676205, 0.8056464137801205, 0.8058905544051205, 0.8058905544051205, 0.8058905544051205, 0.8057684840926205, 0.8057684840926205, 0.8057684840926205, 0.8058905544051205, 0.8058905544051205, 0.8058905544051205, 0.8060126247176205, 0.8058905544051205, 0.8058905544051205, 0.8058905544051205, 0.8058905544051205, 0.8058905544051205, 0.8060126247176205, 0.8060126247176205, 0.8060126247176205, 0.8061346950301205, 0.8061346950301205, 0.8062567653426205, 0.8062567653426205, 0.8062567653426205, 0.8062567653426205, 0.8061346950301205, 0.8060126247176205, 0.8058905544051205, 0.8060126247176205, 0.8060126247176205, 0.8060126247176205, 0.8063891307417168, 0.8063891307417168, 0.8063891307417168, 0.8063891307417168, 0.8061449901167168, 0.8061449901167168, 0.8061449901167168, 0.8061449901167168, 0.8061449901167168, 0.8061449901167168, 0.8061449901167168, 0.8060229198042168, 0.8061449901167168, 0.8061449901167168, 0.8061449901167168, 0.8061449901167168, 0.8062670604292168, 0.8062670604292168, 0.8062670604292168, 0.8062670604292168, 0.8062670604292168, 0.8062670604292168, 0.8062670604292168, 0.8062670604292168, 0.8062670604292168, 0.8062670604292168, 0.8062670604292168, 0.8061449901167168, 0.8061449901167168, 0.8061449901167168, 0.8061449901167168, 0.8061449901167168, 0.8061449901167168, 0.8061449901167168, 0.8061449901167168, 0.8061449901167168, 0.8060229198042168, 0.8061449901167168, 0.8061449901167168, 0.8061449901167168, 0.8061449901167168, 0.8062670604292168, 0.8062670604292168, 0.8062670604292168, 0.8063891307417168, 0.8063891307417168, 0.8063891307417168, 0.8063891307417168, 0.8063891307417168, 0.8063891307417168, 0.8063891307417168], "seed": 989334445, "model": "residualv3", "loss_std": [0.34187278151512146, 0.21110482513904572, 0.19012993574142456, 0.17643100023269653, 0.16462913155555725, 0.15378665924072266, 0.1423524022102356, 0.12954609096050262, 0.11586809158325195, 0.1007864847779274, 0.08605209738016129, 0.07114812731742859, 0.05667467042803764, 0.04320741817355156, 0.0313633531332016, 0.02111820876598358, 0.012953094206750393, 0.007160727400332689, 0.0037473656702786684, 0.0022198245860636234, 0.0014331311685964465, 0.0010743554448708892, 0.0008549349731765687, 0.0007005034713074565, 0.0005949625046923757, 0.0005177931161597371, 0.0004585569549817592, 0.0004119924851693213, 0.00037478739977814257, 0.0003440857690293342, 0.00031860475428402424, 0.00029685848858207464, 0.00027824577409774065, 0.0002621505700517446, 0.0002480580878909677, 0.00023555924417451024, 0.0002244234347017482, 0.00021456403192132711, 0.00020567771571222693, 0.00019764072203543037, 0.00019030773546546698, 0.00018363751587457955, 0.00017758119793143123, 0.0001720212458167225, 0.00016688001051079482, 0.00016212460468523204, 0.0001577379007358104, 0.00015364147839136422, 0.00014983171422500163, 0.00014626714983023703, 0.00014289948740042746, 0.00013977305206935853, 0.00013682815188076347, 0.00013404618948698044, 0.0001314387918682769, 0.0001289311476284638, 0.0001266190520254895, 0.0001243969309143722, 0.0001222783757839352, 0.00012027947377646342, 0.00011837585043394938, 0.00011653771798592061, 0.00011479254317237064, 0.00011314616858726367, 0.00011156466644024476, 0.00011003632243955508, 0.00010859767644433305, 0.00010721723083406687, 0.00010588119766907766, 0.00010460461635375395, 0.0001033518637996167, 0.00010216159716947004, 0.00010104058310389519, 9.99377152766101e-05, 9.888784552458674e-05, 9.785978181753308e-05, 9.687493002275005e-05, 9.592609421815723e-05, 9.500101441517472e-05, 9.410755592398345e-05, 9.323639096692204e-05, 9.240288636647165e-05, 9.159350884146988e-05, 9.080736344913021e-05, 9.004395542433485e-05, 8.930356125347316e-05, 8.86015041032806e-05, 8.790403080638498e-05, 8.72185846674256e-05, 8.656305726617575e-05, 8.592130325268954e-05, 8.529819024261087e-05, 8.46894909045659e-05, 8.410393638769165e-05, 8.353073644684628e-05, 8.296363375848159e-05, 8.241629257099703e-05, 8.188197534764186e-05, 8.135620009852573e-05, 8.084730507107452e-05, 8.034538041101769e-05, 7.986691343830898e-05, 7.939236093079671e-05, 7.892482972238213e-05, 7.847187225706875e-05, 7.802214531693608e-05, 7.758718857076019e-05, 7.716410618741065e-05, 7.675125380046666e-05, 7.634161011083052e-05, 7.594443013658747e-05, 7.55558066884987e-05, 7.517297490267083e-05, 7.479349005734548e-05, 7.442541391355917e-05, 7.406465010717511e-05, 7.37140653654933e-05, 7.336577255045995e-05, 7.30290194042027e-05, 7.269780326168984e-05, 7.236557576106861e-05, 7.204608846222982e-05, 7.173395715653896e-05, 7.142467075027525e-05, 7.112127786967903e-05, 7.082398951752111e-05, 7.053151784930378e-05, 7.02471224940382e-05, 6.996850424911827e-05, 6.968958768993616e-05, 6.941832543816417e-05, 6.915061385370791e-05, 6.888615462230518e-05, 6.862702866783366e-05, 6.837613182142377e-05, 6.812428910052404e-05, 6.787766324123368e-05, 6.763725832570344e-05, 6.740189564879984e-05, 6.716931966366246e-05, 6.694079638691619e-05, 6.671376468148082e-05, 6.648784619756043e-05, 6.626352114835754e-05, 6.604292866541073e-05, 6.582724017789587e-05, 6.561535701621324e-05, 6.540601316373795e-05, 6.519747694255784e-05, 6.499955634353682e-05, 6.479755393229425e-05, 6.460143049480394e-05, 6.440479046432301e-05, 6.421347643481568e-05, 6.402374128811061e-05, 6.383426807587966e-05, 6.364856380969286e-05, 6.346921145450324e-05, 6.329110328806564e-05, 6.311665492830798e-05, 6.294201739365235e-05, 6.277280044741929e-05, 6.260382360778749e-05, 6.243636744329706e-05, 6.227230187505484e-05, 6.211059371707961e-05, 6.1951664974913e-05, 6.179232150316238e-05, 6.163642683532089e-05, 6.148144166218117e-05, 6.132670387160033e-05, 6.117264274507761e-05, 6.102538827690296e-05, 6.087596557335928e-05, 6.0730053519364446e-05, 6.058622966520488e-05, 6.044342808309011e-05, 6.030144868418574e-05, 6.0162332374602556e-05, 6.0021458921255544e-05, 5.988402699586004e-05, 5.974625673843548e-05]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:33 2016", "state": "available"}], "summary": "87cd0b5e80c0f357aeda6a156f4717a3"}