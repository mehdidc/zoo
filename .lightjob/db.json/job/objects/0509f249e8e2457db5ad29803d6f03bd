{"content": {"hp_model": {"f0": 16, "f1": 32, "f2": 32, "f3": 32, "nonlin": "leaky_rectify", "nbg1": 5, "nbg3": 4, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.012003610752335461, 0.017790396408859776, 0.014014408456872572, 0.012735875245096714, 0.012139750132521237, 0.012798205902370912, 0.013470132295693518, 0.013994296071383697, 0.011858570266387987, 0.012978303945272782, 0.013211865490232209, 0.01261839280246711, 0.012517254607591509, 0.0105160090709658, 0.010287218879253518, 0.01057991649640428, 0.010873587256742735, 0.012635716208759178, 0.01294685813253618, 0.010411562991153943, 0.008421151079828606, 0.012736682290194364, 0.01137688046300757, 0.010443559362104948, 0.012310694352289235, 0.010930974013434548, 0.01104301140609907, 0.008980516614954616, 0.012722856699806802, 0.012839112415524163, 0.012565180626577103, 0.00948112263541335, 0.01055149283205503, 0.009614956702589321, 0.01015742051647131, 0.014352083422384008, 0.009435790584801138, 0.008908719464793909, 0.011456035891333477, 0.015347612062295927, 0.013489404755872899, 0.011857731913299883, 0.012138310814069393, 0.01314795075074004, 0.015508628420956115, 0.010514821350385903, 0.013179815652805495, 0.01528346809863175, 0.01319409794338328, 0.010862346615585715, 0.01132708681310068, 0.011640196271627596, 0.011626996715016926, 0.014062072495888266, 0.014088379136057594, 0.013880274607118399, 0.011464406173074752, 0.013965318326680083, 0.011030909174049088, 0.01172907662188418, 0.010805748029528847, 0.012980826434087758, 0.01142699520138063, 0.011391953526767438, 0.010051275663554776, 0.012889486009077565, 0.012820123403842296, 0.012546743506563467, 0.012709710028643236, 0.013479619807922909, 0.010133327476304932, 0.011875284554103856, 0.01082307966450983, 0.009965603454033177, 0.009591176300406156, 0.010826952244170537, 0.012540802979761116, 0.012944484178502831, 0.012005558093489233, 0.009772601814561424, 0.011446288951994775, 0.007827796833862878, 0.008918216258726153, 0.009875429023362508, 0.010248062918677114, 0.010467213484183193, 0.010645110904864325, 0.010159718002223395, 0.010324231178510699, 0.010040991901452662, 0.010481302099123171, 0.010350027116675535, 0.0105690882229893, 0.011227756457219204, 0.013142494708081893, 0.011865080352043648, 0.009470735940506321, 0.011122948382067122, 0.00902532392589256, 0.012891819084531098, 0.0102700444368536, 0.010395711914800185, 0.011593391023316158, 0.011435728534091763, 0.011289107651009607, 0.010608821759825068, 0.01258573148397288, 0.009746545389723913, 0.011732624261514765, 0.009411570360633743, 0.011044954695219366, 0.013079505807799352, 0.01364290749639213, 0.013607044440114615, 0.011957195492332007, 0.012048533069303143, 0.0137053544253021, 0.012918521385090156, 0.013027138424064477, 0.010691278369794884, 0.009692659087004172, 0.008369097381625026, 0.008522309593438929, 0.008862094461254725, 0.012749657961949027, 0.008959602574392461, 0.01000840533731105, 0.009260243966778688, 0.00918995964279883, 0.0075157923999973, 0.00878133224818068, 0.009216060956103484, 0.00996129657163241, 0.010022811713169981, 0.009270884923082581, 0.010206986995793959, 0.010947437916150953, 0.012012202442075579, 0.010735026395380895, 0.01013491302287482, 0.009610305533503241, 0.011443636978908323, 0.009622395133161408, 0.009588967718730327, 0.012645474057563073, 0.013113530587761076, 0.010982644193525144, 0.010148697262739639, 0.011451176728385753, 0.011179526856286462, 0.010467593814353405, 0.009280552773814383, 0.008162333002501753, 0.009415079630687554, 0.010746007407841353, 0.011206314459976488, 0.008274746683972508, 0.011155799748926427, 0.013600353569296472, 0.012945261425702358, 0.012128665248371534, 0.011458982112351749, 0.010806346134849284, 0.013793928220133549, 0.010484332838336195, 0.012165867971996955, 0.011063469399744617, 0.010736289180952941, 0.00984075053458824, 0.009804341657994892, 0.010618928278478934, 0.010869670100287422, 0.01158398392018801, 0.010758563632358003, 0.011565041834040543, 0.0114609998159012, 0.014060821801609287, 0.012790792540747635, 0.011664986123281368, 0.011661976857286224, 0.011734705142563476, 0.014023042952539939, 0.011675447094713371, 0.011415858373320204, 0.01161869804789925, 0.010130669258945962, 0.010639322429466986, 0.013313531026680972, 0.012184577232863916, 0.010526681429756777, 0.012896184664067787, 0.011252072044830938, 0.011383519235067242, 0.010009723702935815, 0.011106905022836169, 0.008749639406457765, 0.011721257054981725, 0.009379024765769472, 0.011837926640804897, 0.012538215513789423, 0.01036956696758597, 0.010557080423152537, 0.012051142121877674, 0.009644981794610798, 0.01112166220585183, 0.009805517052551817, 0.010070993090750865, 0.01122716076496847, 0.013018607773856702, 0.0118783806343915], "moving_avg_accuracy_train": [0.05928046197282206, 0.12205308296707731, 0.18609007103780223, 0.2468873110501165, 0.30348340310401956, 0.35576356962270583, 0.40358995799427616, 0.44796129540129004, 0.48854163809491885, 0.5258264511334336, 0.5600547508740492, 0.591225196906061, 0.6197947092729484, 0.6458933893008031, 0.6699609110377124, 0.6918518342842549, 0.7118420197073428, 0.7301446844309607, 0.7467658922060264, 0.7621108457595102, 0.776146735245713, 0.7891046287142665, 0.8009062778133547, 0.8117299057536876, 0.8217409242107109, 0.830941466975594, 0.839291782025912, 0.8469651756902459, 0.854003799519108, 0.8605267898721991, 0.8666742820447706, 0.8721929659607716, 0.8772224523565733, 0.8819188341223463, 0.8864385104127233, 0.8905015327276249, 0.8943559625574833, 0.8979969743674419, 0.9013319416190051, 0.9045846003144782, 0.90734222122849, 0.9099472408403678, 0.9124825288398951, 0.9147525901977845, 0.9169374794972659, 0.9189411182965703, 0.9209280799718966, 0.9228697692523001, 0.9246777074248922, 0.9263583662516628, 0.9278313956171758, 0.9291547608485091, 0.9304713836412423, 0.9316563081058832, 0.9328715856966884, 0.9340164888022225, 0.9350165665162232, 0.9360887695683661, 0.9371048695402854, 0.93803331040787, 0.9390153555148774, 0.9400038638564313, 0.9408957744150019, 0.941700819066525, 0.9424370570945809, 0.9430996713198312, 0.9437959334237285, 0.9443783554410363, 0.944965386372108, 0.945526266293406, 0.9461101132820978, 0.9466473094624244, 0.9472887519484909, 0.9478149690097788, 0.948330308897053, 0.9487918977932466, 0.9492305432390973, 0.9496416362308483, 0.9499813929889004, 0.9503847582235098, 0.9508012653572773, 0.9511458587943254, 0.9516049105579346, 0.9520854504118402, 0.9524551572624982, 0.9527924355792531, 0.953158873228646, 0.9534747522690612, 0.9537892342911399, 0.9540652926645822, 0.9544276774923469, 0.954711935109945, 0.9550724347110305, 0.9554294003865221, 0.9557738849337412, 0.9561258458024472, 0.9564378881890261, 0.9567280629810039, 0.9569635715592416, 0.9572081534606264, 0.9574213017254442, 0.9575643070387801, 0.9577976435172111, 0.958044740582295, 0.9582624776432516, 0.9585561693457499, 0.9587065956351506, 0.958921034355135, 0.9591420030864728, 0.9593221656077443, 0.9594889621745076, 0.9596460545310233, 0.9598525057697354, 0.9600453233798235, 0.9602746267515126, 0.9604647958420036, 0.9606497907698465, 0.9608465491882475, 0.9609073022266947, 0.9610829237482112, 0.9612758963985377, 0.9614008518052878, 0.9615621037475441, 0.9617537695205841, 0.9617750979948824, 0.9618802880788844, 0.9620260763794769, 0.9621643694428951, 0.962232993579724, 0.9623831109576321, 0.9624530763822639, 0.9625415498036997, 0.9626538000639628, 0.9628128819208002, 0.9629490440967065, 0.9630995639383739, 0.9632559581351604, 0.9633943517146398, 0.9634886790016477, 0.9635340820790116, 0.9636446632641059, 0.9637279102890242, 0.9637934959673938, 0.9639083266493551, 0.9639674964357392, 0.9640184240946754, 0.9641479643448609, 0.9643622428688466, 0.9644551121416242, 0.9646130992490287, 0.9646832801302351, 0.9647951989506833, 0.9649168161795537, 0.9649171338867644, 0.9650033782315689, 0.9650786369442647, 0.9651371052392715, 0.9652803714595305, 0.9654442603875438, 0.9655452213977467, 0.9656500371997863, 0.9657281314287741, 0.9658076807812825, 0.9659583302580639, 0.9659497916097953, 0.9660072109930203, 0.9660100963617415, 0.9661149276435722, 0.9661697482674578, 0.9662376519706123, 0.9662895007570321, 0.9663942573362293, 0.96652109034084, 0.9665677746806948, 0.9666632690091831, 0.9667098305703357, 0.9668445977324789, 0.9669589487807979, 0.9670362880873804, 0.9671337592002001, 0.9671819917207947, 0.9671951740548059, 0.9673162480518263, 0.9674136970515532, 0.9674153345965363, 0.9675469446227168, 0.9676538039510505, 0.9677080886191606, 0.967805736896641, 0.967933183924954, 0.9679433627004633, 0.9680222420138888, 0.9681419533745154, 0.9681567597443359, 0.9681631460795646, 0.9682385761479185, 0.9681948921153987, 0.9682324145433014, 0.9682940144164908, 0.9683238416166375, 0.9683925748241599, 0.9684149432299868, 0.9685536213356979, 0.9685342724046474, 0.9685564940429202, 0.9686087932589678, 0.9687257251641526, 0.9688123987371614, 0.9688206504885837, 0.968900156677959], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 159763026, "moving_var_accuracy_train": [0.03162755854540083, 0.06392842020925649, 0.09444220075886231, 0.1182647202210106, 0.13526630692087438, 0.14633861852978103, 0.15229102749881743, 0.15478126499851932, 0.1541240164168586, 0.15122303032502574, 0.14664491582072403, 0.14072479459116272, 0.13399826846398205, 0.1267287115103509, 0.11926905078232525, 0.11165505838936687, 0.10408602016967514, 0.09669230597657412, 0.08950945631003382, 0.0826777190750576, 0.07618300291057195, 0.07007586564779619, 0.06432178937613868, 0.05894396873464164, 0.05395155627609922, 0.04931825053300529, 0.045013975332660876, 0.04104250653234534, 0.03738413590774883, 0.03402866694529262, 0.03096592519086981, 0.02814343552126538, 0.02555675356978884, 0.023199582228018926, 0.0210634712691452, 0.01910569749521317, 0.017328837409511574, 0.015715266371562737, 0.014243837793527461, 0.012914672111477845, 0.011691645158278616, 0.010583555787055169, 0.009583049375514576, 0.008671123045080364, 0.007846974411831223, 0.007098408086590804, 0.006424099428224665, 0.005815620900756905, 0.005263476574604455, 0.004762550443972028, 0.004305823739179797, 0.0038910030250813355, 0.0035175041827783, 0.003178390178382613, 0.002873843257148771, 0.002598256159523451, 0.002347431942477474, 0.002123035322694948, 0.0019200239228018647, 0.0017357795525230898, 0.0015708813106005551, 0.0014225875182123959, 0.0012874883063915639, 0.0011645723477709206, 0.0010529935308994291, 0.0009516456963130226, 0.0008608441549376317, 0.0007778126781000731, 0.00070313285811638, 0.0006356508488797781, 0.0005751536597476417, 0.0005202355113982949, 0.0004719149964248463, 0.00042721563634267554, 0.0003868842495031505, 0.00035011340333463814, 0.0003168337514456642, 0.0002866713533318992, 0.00025904312989048805, 0.00023460314851386278, 0.0002127041373947896, 0.0001925024253870209, 0.00017514873954337307, 0.00015971213254976113, 0.0001449710676935961, 0.0001314977708908142, 0.00011955648275976605, 0.00010849885059735217, 9.853905601751357e-05, 8.93710244456907e-05, 8.161582687166844e-05, 7.418146572296449e-05, 6.793295881211401e-05, 6.228648337221536e-05, 5.712586146444766e-05, 5.252816339590537e-05, 4.815168111551117e-05, 4.4094325693054254e-05, 4.018407173756089e-05, 3.670404732216989e-05, 3.344253223510631e-05, 3.028233368837659e-05, 2.774411352903798e-05, 2.5519214812292283e-05, 2.339397818048897e-05, 2.1830873707487753e-05, 1.9851438953624585e-05, 1.8280150739919016e-05, 1.6891580287988215e-05, 1.5494549065827558e-05, 1.4195484011401347e-05, 1.2998037686541955e-05, 1.2081832943579219e-05, 1.1208257326062116e-05, 1.0560651919867872e-05, 9.830065274684816e-06, 9.155066857164495e-06, 8.587985048353374e-06, 7.762404928643259e-06, 7.263750705157088e-06, 6.872521628607792e-06, 6.325794148831881e-06, 5.92723443388179e-06, 5.665132907488704e-06, 5.1027137510828245e-06, 4.692026959925637e-06, 4.414112321240058e-06, 4.144825831622177e-06, 3.772726697859674e-06, 3.5982710724234568e-06, 3.282500410976243e-06, 3.024698286583717e-06, 2.8356295462875688e-06, 2.7798299262322907e-06, 2.6687081769369544e-06, 2.6057433638637027e-06, 2.5653013305736613e-06, 2.4811462430867052e-06, 2.313110352446256e-06, 2.1003522721085874e-06, 2.0003708313696847e-06, 1.8627043526524683e-06, 1.7151472482520717e-06, 1.6623072931040163e-06, 1.5275861363803442e-06, 1.3981701607448337e-06, 1.4093792324333635e-06, 1.6816788817633049e-06, 1.5911333100230982e-06, 1.6566593139753829e-06, 1.535321587360001e-06, 1.4945218299587565e-06, 1.4781864001858861e-06, 1.3303686686081435e-06, 1.2642745848442316e-06, 1.188821990889508e-06, 1.1007066654896922e-06, 1.1753628877461231e-06, 1.2995628254998172e-06, 1.2613446731803471e-06, 1.2340873770772742e-06, 1.1655670167803025e-06, 1.1059632104627227e-06, 1.1996242731067767e-06, 1.0803180224243855e-06, 1.001959090311376e-06, 9.018381094541556e-07, 9.105606773610554e-07, 8.46552316853886e-07, 8.033953012874124e-07, 7.472504410374855e-07, 7.712908648994702e-07, 8.389412779368163e-07, 7.746619984321702e-07, 7.792682995498116e-07, 7.208532803875107e-07, 8.122276442780571e-07, 8.486903401153663e-07, 8.176536211877096e-07, 8.213938195777847e-07, 7.601918220061968e-07, 6.857366051754572e-07, 7.490931594482143e-07, 7.596506114334103e-07, 6.837096842722145e-07, 7.712295067663934e-07, 7.968768005571414e-07, 7.43710547227904e-07, 7.551561673590978e-07, 8.258252558555416e-07, 7.44175197507826e-07, 7.257551925352462e-07, 7.82156962049433e-07, 7.059143231298473e-07, 6.356899583157366e-07, 6.233282193911491e-07, 5.781700497267477e-07, 5.330244381155158e-07, 5.138728936963251e-07, 4.7049256114405833e-07, 4.659615893763659e-07, 4.2386854065186575e-07, 5.545662396194502e-07, 5.024790458526459e-07, 4.5667535213512607e-07, 4.356246889143382e-07, 5.151198540742337e-07, 5.312186429899585e-07, 4.787096013047791e-07, 4.877297485151267e-07], "duration": 181682.904033, "accuracy_train": [0.5928046197282207, 0.6870066719153747, 0.7624229636743264, 0.794062471160945, 0.8128482315891473, 0.8262850682908823, 0.8340274533384091, 0.847303332064415, 0.8537647223375784, 0.8613897684800664, 0.8681094485395902, 0.8717592111941677, 0.8769203205749354, 0.8807815095514949, 0.8865686066698967, 0.8888701435031378, 0.8917536885151348, 0.8948686669435216, 0.8963567621816169, 0.9002154277408637, 0.9024697406215393, 0.9057256699312477, 0.9071211197051495, 0.909142557216685, 0.9118400903239202, 0.9137463518595422, 0.9144446174787744, 0.9160257186692506, 0.9173514139788667, 0.9192337030500184, 0.9220017115979143, 0.9218611212047803, 0.9224878299187893, 0.9241862700143041, 0.9271155970261166, 0.9270687335617387, 0.9290458310262089, 0.930766080657069, 0.931346646883075, 0.9338585285737356, 0.9321608094545959, 0.9333924173472684, 0.9353001208356404, 0.9351831424187893, 0.9366014831925988, 0.9369738674903102, 0.9388107350498339, 0.9403449727759321, 0.9409491509782208, 0.9414842956925988, 0.9410886599067922, 0.9410650479305095, 0.9423209887758398, 0.9423206282876523, 0.943809084013935, 0.9443206167520304, 0.9440172659422297, 0.9457385970376523, 0.94624976928756, 0.9463892782161315, 0.9478537614779439, 0.9489004389304172, 0.9489229694421374, 0.9489462209302326, 0.9490631993470838, 0.9490631993470838, 0.950062292358804, 0.949620153596807, 0.9502486647517534, 0.9505741855850868, 0.9513647361803249, 0.9514820750853636, 0.9530617343230897, 0.9525509225613695, 0.9529683678825213, 0.9529461978589886, 0.9531783522517534, 0.9533414731566077, 0.9530392038113695, 0.9540150453349945, 0.954549829561185, 0.9542471997277593, 0.9557363764304172, 0.9564103090969915, 0.9557825189184201, 0.955827940430048, 0.956456812073182, 0.9563176636327981, 0.9566195724898486, 0.9565498180255629, 0.9576891409422297, 0.9572702536683279, 0.9583169311208011, 0.9586420914659468, 0.9588742458587117, 0.9592934936208011, 0.9592462696682356, 0.959339636108804, 0.9590831487633813, 0.9594093905730897, 0.959339636108804, 0.958851354858804, 0.9598976718230897, 0.960268614168051, 0.9602221111918604, 0.9611993946682356, 0.9600604322397563, 0.9608509828349945, 0.9611307216685124, 0.9609436282991879, 0.9609901312753784, 0.961059885739664, 0.9617105669181433, 0.9617806818706165, 0.9623383570967147, 0.9621763176564231, 0.9623147451204319, 0.9626173749538575, 0.9614540795727206, 0.9626635174418604, 0.9630126502514765, 0.9625254504660392, 0.9630133712278516, 0.9634787614779439, 0.9619670542635659, 0.9628269988349022, 0.9633381710848099, 0.9634090070136582, 0.962850610811185, 0.963734167358804, 0.9630827652039498, 0.9633378105966224, 0.9636640524063308, 0.9642446186323367, 0.9641745036798633, 0.9644542425133813, 0.9646635059062385, 0.9646398939299556, 0.9643376245847176, 0.9639427097752861, 0.9646398939299556, 0.964477133513289, 0.9643837670727206, 0.9649418027870063, 0.9645000245131967, 0.9644767730251015, 0.9653138265965301, 0.9662907495847176, 0.9652909355966224, 0.9660349832156699, 0.9653149080610927, 0.9658024683347176, 0.9660113712393872, 0.9649199932516611, 0.9657795773348099, 0.9657559653585271, 0.9656633198943337, 0.9665697674418604, 0.966919260739664, 0.9664538704895718, 0.9665933794181433, 0.966430979489664, 0.9665236249538575, 0.9673141755490956, 0.9658729437753784, 0.9665239854420451, 0.9660360646802326, 0.967058409180048, 0.966663133882429, 0.9668487852990033, 0.9667561398348099, 0.9673370665490033, 0.9676625873823367, 0.9669879337393872, 0.9675227179655776, 0.9671288846207088, 0.9680575021917681, 0.9679881082156699, 0.9677323418466224, 0.9680109992155776, 0.9676160844061462, 0.9673138150609081, 0.9684059140250092, 0.9682907380490956, 0.9674300725013842, 0.9687314348583426, 0.9686155379060539, 0.9681966506321521, 0.9686845713939645, 0.9690802071797711, 0.968034971680048, 0.9687321558347176, 0.969219355620155, 0.9682900170727206, 0.9682206230966224, 0.9689174467631044, 0.9678017358227206, 0.968570116394426, 0.9688484132751938, 0.9685922864179586, 0.9690111736918604, 0.968616258882429, 0.9698017242870985, 0.9683601320251938, 0.9687564887873754, 0.969079486203396, 0.9697781123108158, 0.9695924608942414, 0.9688949162513842, 0.9696157123823367], "end": "2016-01-27 19:26:52.869000", "learning_rate_per_epoch": [0.008830984123051167, 0.004415492061525583, 0.0029436612967401743, 0.0022077460307627916, 0.001766196801327169, 0.0014718306483700871, 0.0012615692103281617, 0.0011038730153813958, 0.0009812205098569393, 0.0008830984006635845, 0.0008028167649172246, 0.0007359153241850436, 0.0006793064530938864, 0.0006307846051640809, 0.0005887322477065027, 0.0005519365076906979, 0.0005194696714170277, 0.0004906102549284697, 0.00046478863805532455, 0.00044154920033179224, 0.0004205230507068336, 0.0004014083824586123, 0.0003839558339677751, 0.0003679576620925218, 0.0003532393602654338, 0.0003396532265469432, 0.000327073474181816, 0.00031539230258204043, 0.0003045166959054768, 0.00029436612385325134, 0.0002848704461939633, 0.00027596825384534895, 0.00026760558830574155, 0.00025973483570851386, 0.00025231382460333407, 0.00024530512746423483, 0.00023867524578236043, 0.00023239431902766228, 0.0002264354843646288, 0.00022077460016589612, 0.00021538985311053693, 0.0002102615253534168, 0.00020537171803880483, 0.00020070419122930616, 0.00019624408741947263, 0.00019197791698388755, 0.00018789328169077635, 0.0001839788310462609, 0.0001802241604309529, 0.0001766196801327169, 0.00017315655713900924, 0.0001698266132734716, 0.00016662233974784613, 0.000163536737090908, 0.0001605633442522958, 0.00015769615129102021, 0.0001549295411678031, 0.0001522583479527384, 0.0001496776967542246, 0.00014718306192662567, 0.00014477023796644062, 0.00014243522309698164, 0.0001401743502356112, 0.00013798412692267448, 0.00013586129352916032, 0.00013380279415287077, 0.0001318057329626754, 0.00012986741785425693, 0.00012798527313861996, 0.00012615691230166703, 0.0001243800506927073, 0.00012265256373211741, 0.00012097238504793495, 0.00011933762289118022, 0.00011774645827244967, 0.00011619715951383114, 0.00011468810407677665, 0.0001132177421823144, 0.00011178461136296391, 0.00011038730008294806, 0.00010902449866989627, 0.00010769492655526847, 0.00010639739775797352, 0.0001051307626767084, 0.00010389392991783097, 0.00010268585901940241, 0.00010150556772714481, 0.00010035209561465308, 9.922454046318308e-05, 9.812204370973632e-05, 9.70437831711024e-05, 9.598895849194378e-05, 9.495682024862617e-05, 9.394664084538817e-05, 9.295772906625643e-05, 9.198941552313045e-05, 9.104107448365539e-05, 9.011208021547645e-05, 8.92018579179421e-05, 8.830984006635845e-05, 8.743548823986202e-05, 8.657827856950462e-05, 8.573770901421085e-05, 8.49133066367358e-05, 8.410461305174977e-05, 8.331116987392306e-05, 8.253256237367168e-05, 8.1768368545454e-05, 8.101820276351646e-05, 8.02816721261479e-05, 7.955841283546761e-05, 7.884807564551011e-05, 7.81503040343523e-05, 7.746477058390155e-05, 7.679116970393807e-05, 7.61291739763692e-05, 7.5478499638848e-05, 7.48388483771123e-05, 7.420995098073035e-05, 7.359153096331283e-05, 7.298334094230086e-05, 7.238511898322031e-05, 7.179661770351231e-05, 7.121761154849082e-05, 7.06478749634698e-05, 7.00871751178056e-05, 6.953530828468502e-05, 6.899206346133724e-05, 6.845724419690669e-05, 6.793064676458016e-05, 6.741208926541731e-05, 6.690139707643539e-05, 6.639837374677882e-05, 6.59028664813377e-05, 6.541470065712929e-05, 6.493370892712846e-05, 6.445973849622533e-05, 6.399263656930998e-05, 6.353225762723014e-05, 6.307845615083352e-05, 6.263109389692545e-05, 6.219002534635365e-05, 6.175513408379629e-05, 6.132628186605871e-05, 6.090333772590384e-05, 6.0486192523967475e-05, 6.0074722568970174e-05, 5.966881144559011e-05, 5.926835001446307e-05, 5.8873229136224836e-05, 5.84833396715112e-05, 5.809857975691557e-05, 5.7718851167010143e-05, 5.734405203838833e-05, 5.697409142157994e-05, 5.66088710911572e-05, 5.624830737360753e-05, 5.589230568148196e-05, 5.5540782341267914e-05, 5.519365004147403e-05, 5.4850832384545356e-05, 5.451224933494814e-05, 5.417781721916981e-05, 5.384746327763423e-05, 5.3521114750765264e-05, 5.319869887898676e-05, 5.288014290272258e-05, 5.25653813383542e-05, 5.225434506428428e-05, 5.194696495891549e-05, 5.164318281458691e-05, 5.134292950970121e-05, 5.104615047457628e-05, 5.0752783863572404e-05, 5.046276783104986e-05, 5.017604780732654e-05, 4.989256558474153e-05, 4.961227023159154e-05, 4.933510717819445e-05, 4.906102185486816e-05, 4.8789966967888176e-05, 4.85218915855512e-05, 4.825674477615394e-05, 4.799447924597189e-05, 4.7735047701280564e-05, 4.7478410124313086e-05, 4.7224515583366156e-05, 4.697332042269409e-05, 4.672478462453e-05, 4.647886453312822e-05, 4.623552013072185e-05, 4.599470776156522e-05, 4.5756394683849066e-05, 4.5520537241827697e-05, 4.528709905571304e-05, 4.505604010773823e-05, 4.482733129407279e-05, 4.460092895897105e-05, 4.4376803998602554e-05, 4.4154920033179224e-05, 4.39352443208918e-05, 4.371774411993101e-05, 4.35023830505088e-05, 4.328913928475231e-05, 4.307797280489467e-05, 4.2868854507105425e-05, 4.266175892553292e-05, 4.24566533183679e-05, 4.2253512219758704e-05, 4.205230652587488e-05], "accuracy_valid": [0.5798781061746988, 0.6765827960278614, 0.7422389754329819, 0.7696856762989458, 0.7831545910203314, 0.7891360363328314, 0.7895331325301205, 0.800804781626506, 0.8002856151167168, 0.8039889048381024, 0.8096041392131024, 0.8143854715737951, 0.8114557840737951, 0.8143854715737951, 0.8127985575112951, 0.8132765436746988, 0.8174166392131024, 0.8181799463478916, 0.8177122552710843, 0.8203669168862951, 0.8208551981362951, 0.8223612222326807, 0.8210081537085843, 0.8207434229103916, 0.8227171380835843, 0.8236834054969879, 0.8240393213478916, 0.821800875376506, 0.8238157708960843, 0.8245584878576807, 0.8263998376317772, 0.8232863092996988, 0.8231951242469879, 0.8235510400978916, 0.8251482492469879, 0.8259321465549698, 0.8245378976844879, 0.8242731668862951, 0.8255247552710843, 0.8287500588290663, 0.8263998376317772, 0.8240496164344879, 0.8270101891942772, 0.8252909097326807, 0.8292486351656627, 0.8268778237951807, 0.8277426110692772, 0.8307031838290663, 0.8288309487951807, 0.8252806146460843, 0.8277220208960843, 0.8267351633094879, 0.8271116693335843, 0.8278749764683735, 0.8266439782567772, 0.8279970467808735, 0.8284647378576807, 0.8271116693335843, 0.8279455713478916, 0.8271116693335843, 0.8302854974585843, 0.8300722420933735, 0.8298178063817772, 0.8287088784826807, 0.8283220773719879, 0.8273661050451807, 0.8273455148719879, 0.8293192300451807, 0.8308046639683735, 0.8277323159826807, 0.8277117258094879, 0.8272337396460843, 0.8280882318335843, 0.8278337961219879, 0.8289221338478916, 0.8294310052710843, 0.8318930016942772, 0.8310488045933735, 0.8296854409826807, 0.8294104150978916, 0.8318827066076807, 0.8276705454631024, 0.8280573465737951, 0.8312620599585843, 0.8327269037085843, 0.8311296945594879, 0.8294001200112951, 0.8288103586219879, 0.8305296380835843, 0.8286676981362951, 0.8308752588478916, 0.8298986963478916, 0.8318827066076807, 0.8302854974585843, 0.8293398202183735, 0.8295427804969879, 0.8301325418862951, 0.8300413568335843, 0.8291559793862951, 0.8285662179969879, 0.8275587702371988, 0.8290544992469879, 0.8303060876317772, 0.8302752023719879, 0.8311296945594879, 0.8299089914344879, 0.8299398766942772, 0.8319841867469879, 0.8311502847326807, 0.8319944818335843, 0.8300310617469879, 0.8308046639683735, 0.8332563653049698, 0.8317812264683735, 0.8321371423192772, 0.8312826501317772, 0.8320253670933735, 0.8306722985692772, 0.8323915780308735, 0.8334799157567772, 0.8306517083960843, 0.8308752588478916, 0.8324724679969879, 0.8335916909826807, 0.8323915780308735, 0.8311399896460843, 0.8323709878576807, 0.8332048898719879, 0.8316179758094879, 0.8321062570594879, 0.8296442606362951, 0.8337137612951807, 0.8326151284826807, 0.8316282708960843, 0.8327269037085843, 0.8334593255835843, 0.8324827630835843, 0.8360433923192772, 0.8342020425451807, 0.8329607492469879, 0.8336019860692772, 0.8342123376317772, 0.8333475503576807, 0.8344461831701807, 0.8342226327183735, 0.8343652932040663, 0.8338461266942772, 0.8321165521460843, 0.8321165521460843, 0.8323812829442772, 0.8323606927710843, 0.8345579583960843, 0.8336828760353916, 0.8337034662085843, 0.8333578454442772, 0.8347109139683735, 0.8326151284826807, 0.8349550545933735, 0.8363287132906627, 0.8375391213290663, 0.8365625588290663, 0.8365522637424698, 0.8337034662085843, 0.8354742211031627, 0.8348226891942772, 0.8353315606174698, 0.8341005624058735, 0.8358095467808735, 0.8348123941076807, 0.8348123941076807, 0.8355654061558735, 0.8344564782567772, 0.8343549981174698, 0.8343344079442772, 0.8349550545933735, 0.8339784920933735, 0.8346094338290663, 0.8337240563817772, 0.8347109139683735, 0.8350771249058735, 0.8343241128576807, 0.8353315606174698, 0.8351991952183735, 0.8353109704442772, 0.8337240563817772, 0.8346903237951807, 0.8365316735692772, 0.8370508400790663, 0.8342226327183735, 0.8356874764683735, 0.8360742775790663, 0.8359316170933735, 0.8367861092808735, 0.8337137612951807, 0.8334799157567772, 0.8346800287085843, 0.8337137612951807, 0.8355654061558735, 0.8356874764683735, 0.8331342949924698, 0.8365419686558735, 0.8346903237951807, 0.8359522072665663, 0.8348123941076807, 0.8361757577183735, 0.8344358880835843, 0.8335916909826807, 0.8354330407567772, 0.8349653496799698, 0.8361757577183735], "accuracy_test": 0.8232342155612244, "start": "2016-01-25 16:58:49.965000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0, 209.0], "accuracy_train_last": 0.9696157123823367, "batch_size_eval": 1024, "accuracy_train_std": [0.02248967386428689, 0.018836696523837602, 0.01898149742263871, 0.017491651549388858, 0.01759267939057737, 0.017331475113719078, 0.017250830145205547, 0.014167258591752515, 0.014439627048008134, 0.014679245526018797, 0.013975515243459698, 0.013198120526768707, 0.014065805222530982, 0.011971448257328637, 0.011607999238525913, 0.012550544957525, 0.011262037694400205, 0.011274584719385371, 0.010608692209298899, 0.012269588893193992, 0.01099599935986121, 0.010499820408570771, 0.010107578934520417, 0.010187987919010995, 0.009600445994530724, 0.010581162515503723, 0.009746471313293174, 0.009549589685413997, 0.009028392070381851, 0.009053123339650845, 0.00896843611366611, 0.00873168644301979, 0.009190460297702643, 0.008278834001323985, 0.007929183083577285, 0.008415526216784174, 0.00866960801542814, 0.0076509331481407665, 0.008278782648873375, 0.007314161254737616, 0.007268347208779513, 0.007976024267648655, 0.007441661080970791, 0.006899688357248565, 0.007827612349505313, 0.007796286226175236, 0.006994181158713257, 0.006958582993144473, 0.007406423187616433, 0.00677394596542131, 0.006786915366793063, 0.007232498342783066, 0.006676602511829764, 0.007154064992260027, 0.006227142895376581, 0.007495023224949071, 0.007075559836063835, 0.006927051432851269, 0.006518544633630288, 0.006910198273493442, 0.006989141133193578, 0.006618354739407076, 0.006762727136175059, 0.00666052505069806, 0.006101371297055088, 0.006643005135689834, 0.007108956444981914, 0.006894581754041329, 0.0077506684186335675, 0.006891225102099083, 0.007156465629839528, 0.006025537282615397, 0.006226849653386283, 0.006406484081960901, 0.006156750444694937, 0.006205023495076948, 0.006452940265783291, 0.006042512550728715, 0.005945419363348801, 0.00651603139402954, 0.006325919726547136, 0.0061394276068644504, 0.005426264533216983, 0.00533108148690842, 0.00587668824016451, 0.006113964000442109, 0.005997430602892514, 0.005740262325933022, 0.0061137986244052525, 0.0052425477751387286, 0.00635133993350876, 0.0059287714063489825, 0.005725752461796437, 0.0061009891637694175, 0.0058102486679028715, 0.006071082226767742, 0.0062171177426644485, 0.005880396134861872, 0.006319141517558993, 0.0056129014112434325, 0.00658014944652572, 0.005883829977687514, 0.005360921747959206, 0.0064847918685782355, 0.006472814423631907, 0.0058578471644691495, 0.006028289419917123, 0.0059820612792421774, 0.006217457517701236, 0.006302297956228737, 0.00550184605339857, 0.006041721434864312, 0.005529839802070702, 0.005832032623600305, 0.005866357964582912, 0.00532699554765584, 0.00540831434383248, 0.005661797015507092, 0.005870030175475147, 0.006093981607646877, 0.005807571316469336, 0.006209713946423745, 0.005205856362273825, 0.005335789921442491, 0.0058236693981092566, 0.005802911279836312, 0.005908034287343549, 0.00572263461954608, 0.005953797357036543, 0.005674029333943259, 0.006107099254047428, 0.006407236832666175, 0.005371798844695689, 0.005287018224204866, 0.005884114267776022, 0.0056990011984170376, 0.005225631158571616, 0.005836105575978311, 0.005453039563599387, 0.006171341823769606, 0.005666336258859567, 0.005623922582557582, 0.005832577937771764, 0.005542935099539583, 0.005321864052268325, 0.0055316427728697316, 0.005361125107231485, 0.004842263466269971, 0.005422713105772549, 0.00539818767065713, 0.005032461359399408, 0.005693024036509344, 0.00540686279833522, 0.005380221446380815, 0.0053477425222644755, 0.005402071185127533, 0.005560048155206427, 0.005804169830683124, 0.005294257766059238, 0.005497226118825379, 0.005458588916037614, 0.005387289121077037, 0.0052498004036473115, 0.005014202821600173, 0.005488136040421066, 0.005256336831079772, 0.005982989132338118, 0.0054352679754875455, 0.005328152141689028, 0.005339922132235853, 0.005081032179068422, 0.0049015250534312485, 0.005232780723069027, 0.005595829617197109, 0.005541609141684653, 0.005002502163648662, 0.004902916043696216, 0.0050887110668803096, 0.004730163798937403, 0.004905389170283525, 0.004895257107314439, 0.00507469384132507, 0.005110584702362513, 0.005053680069459992, 0.005108389442103851, 0.005431930320532029, 0.004903664778955749, 0.005907567648008507, 0.005162697432875187, 0.00472603889556071, 0.005467556786260979, 0.005206815137547063, 0.005152762719594964, 0.004756708035308518, 0.005201736988586047, 0.004817797277019462, 0.005178856525855106, 0.004768924201773423, 0.0046065785025603285, 0.005335438909499888, 0.004582913184456396, 0.005081687044953528, 0.00494122061040999, 0.004791961575041618, 0.004520212815658867, 0.005003754439737946, 0.0050636894682846816, 0.004484617347164101, 0.004840918303863911, 0.004205697253591204], "accuracy_test_std": 0.009790733613162215, "error_valid": [0.4201218938253012, 0.3234172039721386, 0.2577610245670181, 0.2303143237010542, 0.21684540897966864, 0.21086396366716864, 0.21046686746987953, 0.19919521837349397, 0.1997143848832832, 0.19601109516189763, 0.19039586078689763, 0.18561452842620485, 0.18854421592620485, 0.18561452842620485, 0.18720144248870485, 0.18672345632530118, 0.18258336078689763, 0.1818200536521084, 0.18228774472891573, 0.17963308311370485, 0.17914480186370485, 0.1776387777673193, 0.17899184629141573, 0.1792565770896084, 0.17728286191641573, 0.17631659450301207, 0.1759606786521084, 0.17819912462349397, 0.17618422910391573, 0.1754415121423193, 0.17360016236822284, 0.17671369070030118, 0.17680487575301207, 0.1764489599021084, 0.17485175075301207, 0.17406785344503017, 0.17546210231551207, 0.17572683311370485, 0.17447524472891573, 0.17124994117093373, 0.17360016236822284, 0.17595038356551207, 0.17298981080572284, 0.1747090902673193, 0.17075136483433728, 0.1731221762048193, 0.17225738893072284, 0.16929681617093373, 0.1711690512048193, 0.17471938535391573, 0.17227797910391573, 0.17326483669051207, 0.17288833066641573, 0.1721250235316265, 0.17335602174322284, 0.1720029532191265, 0.1715352621423193, 0.17288833066641573, 0.1720544286521084, 0.17288833066641573, 0.16971450254141573, 0.1699277579066265, 0.17018219361822284, 0.1712911215173193, 0.17167792262801207, 0.1726338949548193, 0.17265448512801207, 0.1706807699548193, 0.1691953360316265, 0.1722676840173193, 0.17228827419051207, 0.17276626035391573, 0.17191176816641573, 0.17216620387801207, 0.1710778661521084, 0.17056899472891573, 0.16810699830572284, 0.1689511954066265, 0.1703145590173193, 0.1705895849021084, 0.1681172933923193, 0.17232945453689763, 0.17194265342620485, 0.16873794004141573, 0.16727309629141573, 0.16887030544051207, 0.17059987998870485, 0.17118964137801207, 0.16947036191641573, 0.17133230186370485, 0.1691247411521084, 0.1701013036521084, 0.1681172933923193, 0.16971450254141573, 0.1706601797816265, 0.17045721950301207, 0.16986745811370485, 0.16995864316641573, 0.17084402061370485, 0.17143378200301207, 0.17244122976280118, 0.17094550075301207, 0.16969391236822284, 0.16972479762801207, 0.16887030544051207, 0.17009100856551207, 0.17006012330572284, 0.16801581325301207, 0.1688497152673193, 0.16800551816641573, 0.16996893825301207, 0.1691953360316265, 0.16674363469503017, 0.1682187735316265, 0.16786285768072284, 0.16871734986822284, 0.1679746329066265, 0.16932770143072284, 0.1676084219691265, 0.16652008424322284, 0.16934829160391573, 0.1691247411521084, 0.16752753200301207, 0.1664083090173193, 0.1676084219691265, 0.16886001035391573, 0.1676290121423193, 0.16679511012801207, 0.16838202419051207, 0.16789374294051207, 0.17035573936370485, 0.1662862387048193, 0.1673848715173193, 0.16837172910391573, 0.16727309629141573, 0.16654067441641573, 0.16751723691641573, 0.16395660768072284, 0.1657979574548193, 0.16703925075301207, 0.16639801393072284, 0.16578766236822284, 0.1666524496423193, 0.1655538168298193, 0.1657773672816265, 0.16563470679593373, 0.16615387330572284, 0.16788344785391573, 0.16788344785391573, 0.16761871705572284, 0.16763930722891573, 0.16544204160391573, 0.1663171239646084, 0.16629653379141573, 0.16664215455572284, 0.1652890860316265, 0.1673848715173193, 0.1650449454066265, 0.16367128670933728, 0.16246087867093373, 0.16343744117093373, 0.16344773625753017, 0.16629653379141573, 0.16452577889683728, 0.16517731080572284, 0.16466843938253017, 0.1658994375941265, 0.1641904532191265, 0.1651876058923193, 0.1651876058923193, 0.1644345938441265, 0.16554352174322284, 0.16564500188253017, 0.16566559205572284, 0.1650449454066265, 0.1660215079066265, 0.16539056617093373, 0.16627594361822284, 0.1652890860316265, 0.1649228750941265, 0.1656758871423193, 0.16466843938253017, 0.1648008047816265, 0.16468902955572284, 0.16627594361822284, 0.1653096762048193, 0.16346832643072284, 0.16294915992093373, 0.1657773672816265, 0.1643125235316265, 0.16392572242093373, 0.1640683829066265, 0.1632138907191265, 0.1662862387048193, 0.16652008424322284, 0.16531997129141573, 0.1662862387048193, 0.1644345938441265, 0.1643125235316265, 0.16686570500753017, 0.1634580313441265, 0.1653096762048193, 0.16404779273343373, 0.1651876058923193, 0.1638242422816265, 0.16556411191641573, 0.1664083090173193, 0.16456695924322284, 0.16503465032003017, 0.1638242422816265], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "momentum": 0.8014008484174229, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.008830984109620701, "patience_threshold": 1, "do_flip": true, "batch_size": 16, "l2_decay": 1.0251442146297229e-06, "optimization": "nesterov_momentum", "nb_data_augmentation": 1, "learning_rate_decay_method": "lin", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.01216244116424925}, "accuracy_valid_max": 0.8375391213290663, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        #nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        nb_data_augmentation=make_constant_param(1),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8361757577183735, "loss_train": [1.5621033906936646, 1.13883376121521, 0.9168437123298645, 0.8141828179359436, 0.7440037727355957, 0.6994677782058716, 0.6624965667724609, 0.6337998509407043, 0.6104179620742798, 0.5875214338302612, 0.5703250765800476, 0.5537465214729309, 0.5381652116775513, 0.5235913395881653, 0.5116276741027832, 0.5010963082313538, 0.49378126859664917, 0.4813651740550995, 0.4715867340564728, 0.46215012669563293, 0.4565555453300476, 0.4480181634426117, 0.44141390919685364, 0.4360959529876709, 0.4316173791885376, 0.42403632402420044, 0.4198096692562103, 0.4139416813850403, 0.4081622362136841, 0.4039253890514374, 0.3980026841163635, 0.39794668555259705, 0.3920155167579651, 0.38703933358192444, 0.38188886642456055, 0.37926891446113586, 0.3753899037837982, 0.3713596761226654, 0.36983147263526917, 0.3666824698448181, 0.3671063184738159, 0.3618490695953369, 0.359772652387619, 0.3566519618034363, 0.35352256894111633, 0.35016554594039917, 0.34708085656166077, 0.34412622451782227, 0.34291714429855347, 0.3410276472568512, 0.3404059112071991, 0.33663520216941833, 0.3349143862724304, 0.33374619483947754, 0.3302696943283081, 0.3286612331867218, 0.32841721177101135, 0.3248453438282013, 0.32067546248435974, 0.3193618059158325, 0.3234885632991791, 0.3190079629421234, 0.3193202614784241, 0.31473690271377563, 0.3151531219482422, 0.31304699182510376, 0.312459796667099, 0.31050562858581543, 0.3095497488975525, 0.307666540145874, 0.30531319975852966, 0.3057655394077301, 0.30623647570610046, 0.30486202239990234, 0.30040448904037476, 0.30018243193626404, 0.2992715537548065, 0.2998703718185425, 0.30090606212615967, 0.2971745431423187, 0.29476457834243774, 0.29446837306022644, 0.29107239842414856, 0.29319319128990173, 0.2905450165271759, 0.2919254004955292, 0.28826072812080383, 0.28917017579078674, 0.28645312786102295, 0.28509703278541565, 0.28436824679374695, 0.28432944416999817, 0.28375929594039917, 0.28110387921333313, 0.2828660011291504, 0.2815087139606476, 0.2820785939693451, 0.281099408864975, 0.2801615595817566, 0.279365211725235, 0.28005552291870117, 0.27595603466033936, 0.2742789089679718, 0.27350276708602905, 0.27582576870918274, 0.2748464345932007, 0.2738054692745209, 0.270433247089386, 0.2697980999946594, 0.2726173400878906, 0.2701299786567688, 0.2702149748802185, 0.27315130829811096, 0.2687093913555145, 0.2664766013622284, 0.26688021421432495, 0.2684129774570465, 0.2687950134277344, 0.2675609290599823, 0.26654157042503357, 0.268064945936203, 0.26784807443618774, 0.2633364498615265, 0.2649015486240387, 0.2611537575721741, 0.263316810131073, 0.2641078233718872, 0.261564165353775, 0.26088327169418335, 0.2599075436592102, 0.26272693276405334, 0.2602190673351288, 0.25791504979133606, 0.25813978910446167, 0.25797873735427856, 0.2587500214576721, 0.26114621758461, 0.2569226920604706, 0.2572145462036133, 0.25522381067276, 0.25691771507263184, 0.2543121576309204, 0.25494372844696045, 0.256320983171463, 0.25296923518180847, 0.25220686197280884, 0.25424402952194214, 0.25167545676231384, 0.252774178981781, 0.2523479163646698, 0.25503289699554443, 0.25038281083106995, 0.2516995966434479, 0.2502972483634949, 0.24819210171699524, 0.24815796315670013, 0.2508237063884735, 0.24935784935951233, 0.2504161596298218, 0.24665053188800812, 0.24788497388362885, 0.24910305440425873, 0.24672120809555054, 0.24791958928108215, 0.24473227560520172, 0.24492953717708588, 0.2462722659111023, 0.24507978558540344, 0.24586813151836395, 0.2453366369009018, 0.2456267923116684, 0.24511931836605072, 0.24178147315979004, 0.2443396896123886, 0.24597668647766113, 0.24424119293689728, 0.242239847779274, 0.2421729862689972, 0.23967690765857697, 0.24329334497451782, 0.23965434730052948, 0.24344734847545624, 0.24359606206417084, 0.24084536731243134, 0.2388666272163391, 0.23861318826675415, 0.2396748811006546, 0.24098384380340576, 0.23801960051059723, 0.23962505161762238, 0.23824408650398254, 0.23639772832393646, 0.239299476146698, 0.23949161171913147, 0.23995688557624817, 0.23626908659934998, 0.23584213852882385, 0.2385382056236267, 0.2385014146566391, 0.233326718211174, 0.23360872268676758, 0.23512548208236694, 0.23420436680316925, 0.23636828362941742, 0.23200522363185883, 0.23486043512821198, 0.23464137315750122, 0.2344536930322647, 0.23326274752616882, 0.2342471480369568], "accuracy_train_first": 0.5928046197282207, "model": "residualv5", "loss_std": [0.3067086935043335, 0.26154524087905884, 0.2491448074579239, 0.24217210710048676, 0.23241744935512543, 0.2311631590127945, 0.22390882670879364, 0.22106389701366425, 0.21749290823936462, 0.2144141048192978, 0.21394895017147064, 0.20644159615039825, 0.20206835865974426, 0.20229610800743103, 0.1980237513780594, 0.1974848210811615, 0.19232863187789917, 0.1940588802099228, 0.18911314010620117, 0.1833460032939911, 0.1866965889930725, 0.18132591247558594, 0.1818905919790268, 0.18181158602237701, 0.17851193249225616, 0.17788486182689667, 0.17674395442008972, 0.17523889243602753, 0.17188629508018494, 0.17232055962085724, 0.16710855066776276, 0.16945889592170715, 0.16881951689720154, 0.16632594168186188, 0.16567228734493256, 0.16469775140285492, 0.1632445752620697, 0.16154001653194427, 0.16093377768993378, 0.16149938106536865, 0.16279292106628418, 0.15913598239421844, 0.15945462882518768, 0.15807987749576569, 0.15546758472919464, 0.1573459953069687, 0.15532620251178741, 0.15448515117168427, 0.15122151374816895, 0.15365995466709137, 0.15316708385944366, 0.1489698737859726, 0.1478685587644577, 0.1491239070892334, 0.1497863084077835, 0.1492248773574829, 0.14740686118602753, 0.14326342940330505, 0.14732827246189117, 0.14515964686870575, 0.14818252623081207, 0.14639639854431152, 0.14534050226211548, 0.1447441279888153, 0.14384594559669495, 0.14167916774749756, 0.14423534274101257, 0.1429114192724228, 0.14357741177082062, 0.14223268628120422, 0.1410340815782547, 0.13871154189109802, 0.14105647802352905, 0.1401759535074234, 0.13899770379066467, 0.13817185163497925, 0.13812263309955597, 0.13935662806034088, 0.14162707328796387, 0.13886868953704834, 0.13652604818344116, 0.13838407397270203, 0.13587553799152374, 0.13863001763820648, 0.13768091797828674, 0.13642841577529907, 0.1355224996805191, 0.1351870745420456, 0.1365930289030075, 0.1333555430173874, 0.13485266268253326, 0.1363488733768463, 0.13413821160793304, 0.13463127613067627, 0.13379482924938202, 0.1343439668416977, 0.13465099036693573, 0.13191092014312744, 0.13087715208530426, 0.13321110606193542, 0.13103443384170532, 0.12767083942890167, 0.1282322108745575, 0.12870247662067413, 0.13148139417171478, 0.12999899685382843, 0.13105370104312897, 0.13209131360054016, 0.12816303968429565, 0.1310729831457138, 0.1290217638015747, 0.12969788908958435, 0.12949314713478088, 0.1304146945476532, 0.12957091629505157, 0.13113753497600555, 0.12888304889202118, 0.1317668855190277, 0.1299249678850174, 0.12953948974609375, 0.13092994689941406, 0.12878555059432983, 0.12810134887695312, 0.1262398660182953, 0.12748435139656067, 0.12703433632850647, 0.128338024020195, 0.1271684467792511, 0.1278519630432129, 0.1252637505531311, 0.12900444865226746, 0.12908916175365448, 0.12416622787714005, 0.12294255197048187, 0.12606178224086761, 0.12673582136631012, 0.12941932678222656, 0.12449874728918076, 0.12397781759500504, 0.12521299719810486, 0.12573006749153137, 0.12420785427093506, 0.12591534852981567, 0.12433890253305435, 0.12567362189292908, 0.12291911989450455, 0.12722072005271912, 0.1258821189403534, 0.12454571574926376, 0.12640203535556793, 0.12634608149528503, 0.12235841900110245, 0.12287715077400208, 0.1266290247440338, 0.12573517858982086, 0.1232173964381218, 0.12500092387199402, 0.12364451587200165, 0.12344803661108017, 0.12512646615505219, 0.12005136162042618, 0.12220919132232666, 0.12278620898723602, 0.1245342269539833, 0.12138097733259201, 0.12037347257137299, 0.12398352473974228, 0.123066246509552, 0.12438373267650604, 0.12249845266342163, 0.12490050494670868, 0.12194620072841644, 0.11807787418365479, 0.12175703793764114, 0.12344289571046829, 0.11954497545957565, 0.1216500848531723, 0.12160845100879669, 0.12222478538751602, 0.12233270704746246, 0.11986137926578522, 0.12265603989362717, 0.12237999588251114, 0.12199782580137253, 0.11934919655323029, 0.11893793940544128, 0.11924628168344498, 0.12128892540931702, 0.11844799667596817, 0.11944904923439026, 0.11946837604045868, 0.12219776958227158, 0.12048501521348953, 0.11794908344745636, 0.1215202659368515, 0.12081803381443024, 0.11939026415348053, 0.12204182147979736, 0.12113644927740097, 0.11679034680128098, 0.11974551528692245, 0.12086943536996841, 0.11722616106271744, 0.11923988163471222, 0.1185489073395729, 0.11797335743904114, 0.1179773136973381, 0.12212850153446198, 0.12087846547365189, 0.11935253441333771]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:18 2016", "state": "available"}], "summary": "36f931240423bff456f95424cfd2bc92"}