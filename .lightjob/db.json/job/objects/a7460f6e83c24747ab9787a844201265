{"content": {"hp_model": {"f0": 64, "f1": 16, "f2": 32, "f3": 16, "nonlin": "very_leaky_rectify", "nbg1": 4, "nbg3": 8, "nbg2": 4, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.66047203540802, 1.2277742624282837, 1.0176414251327515, 0.8962023258209229, 0.8134360909461975, 0.7443373799324036, 0.6884999871253967, 0.6389040946960449, 0.5943788886070251, 0.5520864725112915, 0.5152645111083984, 0.48339569568634033, 0.4496864676475525, 0.41865602135658264, 0.39549708366394043, 0.3767346441745758, 0.3505862057209015, 0.3332924544811249, 0.31780770421028137, 0.30177581310272217, 0.29102057218551636, 0.2888723909854889, 0.26155462861061096, 0.25521883368492126, 0.24899518489837646, 0.23889759182929993, 0.22609321773052216, 0.22318477928638458, 0.21474123001098633, 0.21003592014312744, 0.2091323286294937, 0.20918987691402435, 0.20049282908439636, 0.19282034039497375, 0.1938779503107071, 0.19122059643268585, 0.18608807027339935, 0.18533751368522644, 0.18447160720825195, 0.1835186928510666, 0.17510268092155457, 0.17083965241909027, 0.17953670024871826, 0.17729483544826508, 0.17710623145103455, 0.16873598098754883, 0.1683453619480133, 0.1694403439760208, 0.1637284755706787, 0.16898606717586517, 0.16738052666187286, 0.16273336112499237, 0.16057857871055603, 0.1630449742078781, 0.1637050062417984, 0.16474390029907227, 0.15319468080997467, 0.1581922024488449, 0.1655944585800171, 0.15110965073108673, 0.16214287281036377, 0.15859322249889374, 0.15054728090763092, 0.1535661220550537, 0.1537984013557434, 0.15230397880077362, 0.1554696261882782, 0.15435414016246796, 0.1495083123445511, 0.14410120248794556, 0.15842121839523315, 0.14943653345108032, 0.14518649876117706, 0.14486026763916016, 0.15206021070480347, 0.14589107036590576, 0.14645074307918549, 0.14139677584171295, 0.14301234483718872, 0.15085172653198242, 0.14212660491466522, 0.1428421139717102, 0.1434188336133957, 0.13723224401474, 0.14246360957622528, 0.13940201699733734, 0.14124339818954468, 0.14738833904266357, 0.13596700131893158, 0.12981544435024261, 0.12710866332054138, 0.12532839179039001, 0.12358208000659943, 0.121407151222229, 0.11854173243045807, 0.11478130519390106, 0.10992728918790817, 0.10383346676826477, 0.1959712952375412, 0.1162857636809349, 0.10114086419343948, 0.09862326830625534, 0.09758052974939346, 0.09661813825368881, 0.09547702223062515, 0.09402275085449219, 0.09212231636047363, 0.08963140845298767, 0.0863957479596138, 0.16635505855083466, 0.1375102996826172, 0.0884941816329956, 0.08369772881269455, 0.08220566064119339, 0.08139301836490631, 0.08057506382465363, 0.07959847152233124, 0.07835653424263, 0.07673882693052292, 0.07462449371814728, 0.07297519594430923, 0.20017920434474945, 0.08668939024209976, 0.08423850685358047, 0.08611710369586945, 0.09155699610710144, 0.0990128368139267, 0.0809846818447113, 0.07621507346630096, 0.10236687958240509, 0.09002979844808578, 0.08109219372272491, 0.08781975507736206, 0.09318888187408447, 0.0792996734380722, 0.07389224320650101, 0.07204355299472809, 0.07112789154052734, 0.07045222818851471, 0.06972219049930573], "moving_avg_accuracy_train": [0.05490117647058822, 0.1132981176470588, 0.17060359999999997, 0.22769618117647056, 0.28273597482352936, 0.33335414204705877, 0.3795599043129411, 0.42238979623458817, 0.4622261107287764, 0.4988717349500164, 0.532716326160897, 0.5642823406036308, 0.5930917536020913, 0.6196790488301175, 0.6440405557118116, 0.6666670883759246, 0.6880592030677439, 0.707980341584499, 0.727027013308402, 0.7445737237422676, 0.7609751748974526, 0.7762917750547661, 0.7902461269610542, 0.8036803377943605, 0.8158134804855127, 0.8279591912604909, 0.8387468015462065, 0.8479662390386447, 0.8569954974877214, 0.8653924183271845, 0.8731943529650543, 0.8796796235509018, 0.8864928376663999, 0.8920717891938775, 0.8978481396862544, 0.9037739139529231, 0.9086388754988072, 0.913083223243044, 0.9179654891540338, 0.9219477637680421, 0.9258706344500615, 0.9292482768874083, 0.9326081550810205, 0.9352132219258595, 0.9378471938509206, 0.9398671803481815, 0.9417722270192457, 0.943404416082027, 0.9461086803561772, 0.947980165261736, 0.9496127369708565, 0.9512585220973002, 0.953186787534629, 0.9545904617223426, 0.9560420037854025, 0.9576236857598034, 0.9587036701249996, 0.9591744795830879, 0.9604382080953673, 0.9621261519917129, 0.963490007380777, 0.9646657125250522, 0.9662014942137234, 0.9670048742041157, 0.9673773279601747, 0.9680372422229807, 0.9684970474124472, 0.9690567544359084, 0.9698193142864352, 0.969809147563674, 0.970355291630836, 0.9708797624677524, 0.971450609750389, 0.9716114311282913, 0.9722149938978151, 0.9731652592139158, 0.9736416744689947, 0.9741457423162129, 0.9741170504375327, 0.9743194630408383, 0.9747110461485191, 0.9745646474160201, 0.9743952414979475, 0.9751768938187411, 0.9755180279662787, 0.9763450486990626, 0.975134073240921, 0.9760794894462407, 0.9772197757957342, 0.9788036805691019, 0.9803797831004271, 0.9819112165550903, 0.9833130360760518, 0.984598203056682, 0.9857854415745433, 0.986846897417089, 0.9877927959106743, 0.9886182222019598, 0.9874646352758815, 0.987746407042411, 0.9883435310440523, 0.9890197661749411, 0.9896754366162706, 0.9902773047193494, 0.9908236918944733, 0.9913224991756141, 0.9917643669051115, 0.9921573419793063, 0.9924780783696109, 0.9862749764150027, 0.9861933611264436, 0.986717554425564, 0.9874599166300664, 0.9881868661435304, 0.9888905324703539, 0.9895332439292008, 0.990125801889222, 0.9906685158179468, 0.9911616642361522, 0.9916337331066546, 0.9912515362665774, 0.9901169708752137, 0.9901405679053393, 0.9902841581736289, 0.9898086835327367, 0.98782310929711, 0.9873419748379872, 0.9875371891188943, 0.9872517055011225, 0.9858865349510103, 0.985676704985321, 0.9851513874279654, 0.983346836920463, 0.9835721532284167, 0.9842008202585162, 0.9852419147032527, 0.9863294879388098, 0.9873883038508112, 0.9883506499363183, 0.9892308790603335], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.05405333333333332, 0.11111466666666664, 0.1660965333333333, 0.21979354666666662, 0.2708275253333333, 0.3171181061333333, 0.35891296218666663, 0.3968883326346666, 0.4310261660378666, 0.4623235494340799, 0.4907845278240052, 0.516799408374938, 0.5398528008707775, 0.5602408541170331, 0.5788034353719964, 0.5952164251681301, 0.6105481159846504, 0.6241733043861853, 0.6366493072809001, 0.6475310432194767, 0.6578446055641957, 0.6673001450077761, 0.6756234638403318, 0.6834877841229653, 0.6901523390440021, 0.6968571051396019, 0.702784727958975, 0.7080662551630774, 0.712872962980103, 0.7171190000154261, 0.7206871000138835, 0.7236183900124951, 0.7270165510112456, 0.729114895910121, 0.7314167396524422, 0.7338617323538648, 0.735702225785145, 0.7377453365399638, 0.7396908028859674, 0.7410817225973707, 0.742506883670967, 0.7433628619705369, 0.7442799091068165, 0.7445585848628016, 0.7453427263765214, 0.745795120405536, 0.746842275031649, 0.7474247141951509, 0.7482155761089692, 0.7487673518314055, 0.7489839499815983, 0.7494055549834384, 0.7502516661517613, 0.7504664995365852, 0.7509265162495933, 0.7513271979579673, 0.7510744781621705, 0.7509670303459535, 0.7510969939780248, 0.7514006279135557, 0.7515005651222001, 0.7516305086099802, 0.7522007910823155, 0.752260711974084, 0.7526213074433422, 0.7530925100323413, 0.7538232590291072, 0.7538142664595298, 0.7535928398135768, 0.7534735558322191, 0.7531395335823305, 0.7534522468907642, 0.7534670222016877, 0.753360319981519, 0.7535576213167005, 0.7539618591850303, 0.7538723399331939, 0.7535517726065412, 0.7539299286792204, 0.7535102691446317, 0.7532525755635019, 0.753300651340485, 0.7535572528731032, 0.7534281942524595, 0.7535653748272136, 0.7542888373444923, 0.7541132869433763, 0.7544086249157054, 0.7547144290908014, 0.7555229861817213, 0.7562373542302159, 0.7566802854738609, 0.7570655902598081, 0.757465697900494, 0.757812461443778, 0.7579912152994003, 0.7582187604361269, 0.7584102177258476, 0.7582358626199295, 0.7582522763579366, 0.7582670487221429, 0.7582670105165952, 0.758373642798269, 0.7584162785184421, 0.7585346506665979, 0.758681185599938, 0.7589197337066109, 0.7591877603359498, 0.7593756509690215, 0.7579714192054527, 0.7582009439515742, 0.7585141828897501, 0.7587827646007751, 0.7587711548073642, 0.7588007059932945, 0.7590139687272984, 0.7592592385212353, 0.7594933146691119, 0.759637316535534, 0.7597135848819806, 0.7591422263937826, 0.7587213370877377, 0.7587558700456306, 0.7586002830410675, 0.7582069214036274, 0.7581328959299314, 0.7573462730036049, 0.7570783123699111, 0.75641048113292, 0.756009433019628, 0.7560884897176652, 0.7559063074125654, 0.7547956766713089, 0.755236109004178, 0.7553124981037602, 0.7555145816267176, 0.7557231234640458, 0.7559908111176412, 0.7562983966725438, 0.7564952236719562], "moving_var_accuracy_train": [0.027127252600692032, 0.05510635198953632, 0.07915098155991196, 0.10057194883244772, 0.11777916391160029, 0.12906103719806294, 0.1353696856773978, 0.13834231388783771, 0.1387904700713728, 0.1369975390353145, 0.13360689231986767, 0.12921392249807084, 0.12376237074410651, 0.11774809207757587, 0.11131463002773977, 0.10479080684856733, 0.09843032930260216, 0.09215896221057561, 0.0862080473233412, 0.08035822601445645, 0.0747434718129741, 0.06938050879508785, 0.0641949733496998, 0.05939977820115335, 0.05478471874511275, 0.05063391148266626, 0.04661787315548793, 0.042721068089031915, 0.03918270885339073, 0.03589901248430955, 0.032856942892720925, 0.029949777214593763, 0.027372578472386985, 0.024915442926461792, 0.022724194658912664, 0.020767808398957012, 0.018904038216647694, 0.01719140443682826, 0.015686792676975946, 0.014260840009190732, 0.01297325623776228, 0.011778606829897146, 0.010702345180190623, 0.009693188021566284, 0.008786309492327747, 0.00794440165213702, 0.007182624312293714, 0.006488338251294309, 0.005905321833544887, 0.0053463117519560064, 0.004835668190229191, 0.004376478849348084, 0.003972294832784447, 0.0035927980605332823, 0.0032524810237274425, 0.002949748382168002, 0.0026652708400128145, 0.002400738709923961, 0.002175037926706297, 0.0019831765254105615, 0.0018015997865700158, 0.0016338803511894919, 0.0014917199446278643, 0.0013483567248457435, 0.0012147695485647913, 0.001097211975216606, 0.0009893935650052896, 0.0008932736560737664, 0.0008091797681971088, 0.0007282627216376633, 0.0006581209095527633, 0.000594784445526469, 0.0005382388005546636, 0.0004846476921395107, 0.00043946151507635693, 0.00040364240110757956, 0.0003653209044542696, 0.0003310755735602351, 0.00029797542521933134, 0.00026854662045519063, 0.00024307199438166073, 0.0002189576882433904, 0.00019732020470475354, 0.00018308700738969467, 0.00016582565921027142, 0.00015539876292133391, 0.0001530570406711908, 0.00014579564281560098, 0.00014291835516361357, 0.0001512053086271268, 0.00015844167046765847, 0.00016370509925544693, 0.00016502047105404225, 0.00016338331146155527, 0.00015973079800003922, 0.00015389791475110424, 0.00014656063891749482, 0.00013803653208685352, 0.00013620974404233782, 0.00012330332759382244, 0.0001141820084944647, 0.00010687945321525309, 0.00010006064144242576, 9.331478421971656e-05, 8.667015630400363e-05, 8.0242419007076e-05, 7.397540091970921e-05, 6.79677255081835e-05, 6.209679944595618e-05, 0.0004021933842347313, 0.0003620339953091974, 0.00032830360331186174, 0.00030043315776473884, 0.0002751459423443942, 0.00025208766480550146, 0.00023059660049895008, 0.00021069706487291426, 0.0001922782040615107, 0.00017523914191676533, 0.00015972086889156576, 0.0001450634518234943, 0.00014214225428666548, 0.00012793304023647571, 0.00011532529969915545, 0.00010582745493642492, 0.0001307272548494401, 0.0001197379426742943, 0.00010810712594609576, 9.802992141563079e-05, 0.00010500014495211179, 9.489638798741103e-05, 8.78903760132647e-05, 0.00010840896121908355, 9.802497204484417e-05, 9.177947495296699e-05, 9.235642624342327e-05, 9.376612350338185e-05, 9.447933137260938e-05, 9.336638812996632e-05, 9.10029791138513e-05], "duration": 33577.457117, "accuracy_train": [0.5490117647058823, 0.6388705882352941, 0.6863529411764706, 0.7415294117647059, 0.7780941176470588, 0.7889176470588235, 0.7954117647058824, 0.8078588235294117, 0.8207529411764706, 0.8286823529411764, 0.8373176470588235, 0.8483764705882353, 0.8523764705882353, 0.858964705882353, 0.8632941176470589, 0.8703058823529412, 0.8805882352941177, 0.8872705882352941, 0.8984470588235294, 0.9024941176470588, 0.9085882352941177, 0.9141411764705882, 0.9158352941176471, 0.9245882352941176, 0.9250117647058823, 0.9372705882352941, 0.9358352941176471, 0.9309411764705883, 0.9382588235294118, 0.9409647058823529, 0.9434117647058824, 0.9380470588235295, 0.9478117647058824, 0.9422823529411765, 0.9498352941176471, 0.9571058823529411, 0.9524235294117647, 0.9530823529411765, 0.9619058823529412, 0.9577882352941176, 0.9611764705882353, 0.9596470588235294, 0.9628470588235294, 0.9586588235294118, 0.9615529411764706, 0.9580470588235294, 0.9589176470588235, 0.9580941176470589, 0.9704470588235294, 0.9648235294117647, 0.9643058823529411, 0.9660705882352941, 0.9705411764705882, 0.9672235294117647, 0.9691058823529412, 0.9718588235294118, 0.9684235294117647, 0.9634117647058823, 0.9718117647058824, 0.9773176470588235, 0.975764705882353, 0.9752470588235295, 0.9800235294117647, 0.9742352941176471, 0.9707294117647058, 0.9739764705882353, 0.972635294117647, 0.9740941176470588, 0.9766823529411764, 0.9697176470588236, 0.9752705882352941, 0.9756, 0.9765882352941176, 0.9730588235294118, 0.9776470588235294, 0.9817176470588236, 0.9779294117647059, 0.9786823529411764, 0.9738588235294118, 0.9761411764705883, 0.9782352941176471, 0.9732470588235295, 0.9728705882352942, 0.9822117647058823, 0.9785882352941176, 0.9837882352941176, 0.9642352941176471, 0.9845882352941177, 0.9874823529411765, 0.9930588235294118, 0.9945647058823529, 0.9956941176470588, 0.9959294117647058, 0.996164705882353, 0.9964705882352941, 0.9964, 0.9963058823529412, 0.9960470588235294, 0.9770823529411765, 0.9902823529411765, 0.9937176470588235, 0.9951058823529412, 0.9955764705882353, 0.9956941176470588, 0.9957411764705882, 0.9958117647058824, 0.9957411764705882, 0.9956941176470588, 0.9953647058823529, 0.9304470588235294, 0.9854588235294117, 0.9914352941176471, 0.9941411764705882, 0.9947294117647059, 0.9952235294117647, 0.9953176470588235, 0.9954588235294117, 0.9955529411764706, 0.9956, 0.9958823529411764, 0.9878117647058824, 0.9799058823529412, 0.9903529411764705, 0.9915764705882353, 0.9855294117647059, 0.9699529411764706, 0.9830117647058824, 0.9892941176470588, 0.9846823529411765, 0.9736, 0.9837882352941176, 0.9804235294117647, 0.9671058823529411, 0.9856, 0.9898588235294118, 0.9946117647058823, 0.9961176470588236, 0.9969176470588236, 0.9970117647058824, 0.9971529411764706], "end": "2016-02-07 15:06:25.070000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0], "moving_var_accuracy_valid": [0.026295865599999992, 0.052970240896, 0.07488026776576, 0.0933425641574656, 0.10744851054866714, 0.11598902033101238, 0.12011140823058084, 0.12107942625348823, 0.11946000865332115, 0.11632974365503518, 0.11198701490773784, 0.10687927950767777, 0.10097448170701508, 0.09461808797286295, 0.0882574039812007, 0.08185613968951254, 0.07578607241020166, 0.0698782769999774, 0.06429130513404004, 0.05892788421396823, 0.05399242190671706, 0.049397844751567305, 0.045081559003905966, 0.041130030905085935, 0.037416774445236986, 0.03407968199628363, 0.030987944207254047, 0.028140200552997718, 0.02553412045804224, 0.02314296788678603, 0.020943253136498353, 0.01892625997235216, 0.017137561458677807, 0.015463432774641756, 0.013964775858704152, 0.012622100176623819, 0.011390376903596707, 0.010288907927245143, 0.009294080688251523, 0.008382084538218502, 0.007562155841167899, 0.006812534546695122, 0.006138849871077039, 0.005525663825562099, 0.004978631344227738, 0.004482610153022357, 0.004044217933019031, 0.0036428492581297538, 0.003284193495417333, 0.002958514253906432, 0.0026630850613437915, 0.0023983763122076025, 0.0021649818179692876, 0.001948899016621473, 0.0017559136533455472, 0.001581767200493822, 0.0014241652861011276, 0.0012818526627899032, 0.0011538194114218635, 0.001039267212380931, 0.0009354303781538827, 0.0008420393081286422, 0.0007607623762000539, 0.0006847184531994814, 0.0006174168697115796, 0.0005576734696593369, 0.0005067120695598721, 0.0004560415904006533, 0.0004108786991964298, 0.0003699188872906637, 0.000333931136332383, 0.0003014181292185881, 0.0002712782810850453, 0.0002442529212506413, 0.00022017797947735654, 0.00019963085581734845, 0.00017973989350365767, 0.00016269077485154735, 0.00014770871550413012, 0.000134522871078458, 0.00012166823780641188, 0.0001095222155487635, 9.91625931127651e-05, 8.939623894955059e-05, 8.062598164540419e-05, 7.727396560602808e-05, 6.98239305354128e-05, 6.362655814296635e-05, 5.810554807022573e-05, 5.817887438669422e-05, 5.6953882326413864e-05, 5.302418687314533e-05, 4.905790618849542e-05, 4.559289068686268e-05, 4.2115806212733847e-05, 3.819180205955869e-05, 3.483861295683432e-05, 3.168465470523544e-05, 2.8789786561349268e-05, 2.5913232602372616e-05, 2.3323873346833556e-05, 2.0991486025287178e-05, 1.8994671414213128e-05, 1.7111564514503928e-05, 1.55265157521847e-05, 1.416711655716743e-05, 1.3262551694225554e-05, 1.2582840991116017e-05, 1.1642282901969125e-05, 2.8224856224112688e-05, 2.5876505083440195e-05, 2.4171922266602304e-05, 2.240395525941606e-05, 2.0164772819201834e-05, 1.8156154990590603e-05, 1.6749868434964835e-05, 1.561629703782862e-05, 1.4547792121087939e-05, 1.3279641746776708e-05, 1.2004029318126318e-05, 1.3741681084637466e-05, 1.3961843247660538e-05, 1.2576391649521977e-05, 1.1536618328469934e-05, 1.1775556895908821e-05, 1.0647319143121305e-05, 1.5151567882811008e-05, 1.4282637205416246e-05, 1.686836053478434e-05, 1.6629080783881578e-05, 1.5022422359034334e-05, 1.3818893653754376e-05, 2.3538510079195343e-05, 2.2930484829804537e-05, 2.0689953997638893e-05, 1.8988498350132528e-05, 1.748105579636545e-05, 1.6377860335715683e-05, 1.559155416440645e-05, 1.4381066557244565e-05], "accuracy_test": 0.1006, "start": "2016-02-07 05:46:47.612000", "learning_rate_per_epoch": [0.0022322828881442547, 0.0015784624265506864, 0.0012888092314824462, 0.0011161414440721273, 0.0009983072523027658, 0.0009113256819546223, 0.0008437236538156867, 0.0007892312132753432, 0.0007440943154506385, 0.0007059098570607603, 0.0006730586173944175, 0.0006444046157412231, 0.0006191239226609468, 0.0005966027383692563, 0.0005763730150647461, 0.0005580707220360637, 0.0005414081388153136, 0.0005261541227810085, 0.0005121208378113806, 0.0004991536261513829, 0.00048712408170104027, 0.00047592431656084955, 0.00046546317753382027, 0.00045566284097731113, 0.0004464566009119153, 0.0004377867153380066, 0.0004296030674595386, 0.00042186182690784335, 0.0004145245475228876, 0.0004075572360306978, 0.0004009298572782427, 0.0003946156066376716, 0.0003885905898641795, 0.00038283338653855026, 0.00037732470082119107, 0.00037204715772531927, 0.00036698507028631866, 0.00036212411941960454, 0.00035745135392062366, 0.00035295492853038013, 0.00034862401662394404, 0.00034444875200279057, 0.00034041996696032584, 0.00033652930869720876, 0.00033276909380219877, 0.0003291321627330035, 0.00032561193802393973, 0.00032220230787061155, 0.00031889756792224944, 0.0003156924794893712, 0.00031258215312846005, 0.0003095619613304734, 0.0003066276549361646, 0.00030377524672076106, 0.00030100098229013383, 0.00029830136918462813, 0.0002956730895675719, 0.0002931131166405976, 0.00029061848181299865, 0.00028818650753237307, 0.0002858145453501493, 0.0002835002087522298, 0.00028124122763983905, 0.00027903536101803184, 0.0002768806298263371, 0.00027477502590045333, 0.00027271677390672266, 0.0002707040694076568, 0.0002687352825887501, 0.000266808841843158, 0.00026492326287552714, 0.00026307706139050424, 0.0002612689568195492, 0.0002594976394902915, 0.00025776182883419096, 0.0002560604189056903, 0.00025439224555157125, 0.00025275626103393734, 0.0002511514467187226, 0.00024957681307569146, 0.00024803142878226936, 0.00024651442072354257, 0.00024502488668076694, 0.00024356204085052013, 0.0002421250828774646, 0.00024071327061392367, 0.00023932586191222072, 0.00023796215828042477, 0.0002366215194342658, 0.0002353032905375585, 0.00023400684585794806, 0.00023273158876691014, 0.00023147696629166603, 0.00023024241090752184, 0.00022902739874552935, 0.00022783142048865557, 0.00022665399592369795, 0.00022549463028553873, 0.00022435288701672107, 0.00022322830045595765, 0.00022212046314962208, 0.00022102895309217274, 0.00021995337738189846, 0.0002188933576690033, 0.00021784851560369134, 0.00021681848738808185, 0.00021580293832812458, 0.0002148015337297693, 0.0002138139243470505, 0.00021283983369357884, 0.0002118789270753041, 0.00021093091345392168, 0.00020999551634304225, 0.0002090724592562765, 0.00020816146570723504, 0.0002072622737614438, 0.00020637463603634387, 0.0002054983051493764, 0.0002046330482698977, 0.0002037786180153489, 0.00020293481065891683, 0.00020210140792187303, 0.00020127817697357386, 0.00020046492863912135, 0.0001996614591917023, 0.00019886757945641875, 0.0001980830857064575, 0.0001973078033188358, 0.0001965415576705709, 0.0001957841741386801, 0.0001950354635482654, 0.00019429529493208975, 0.00019356347911525518, 0.00019283988513052464, 0.00019212432380300015, 0.00019141669326927513, 0.0001907168043544516, 0.00019002454064320773, 0.00018933977116830647, 0.00018866235041059554], "accuracy_train_first": 0.5490117647058823, "accuracy_train_last": 0.9971529411764706, "batch_size_eval": 1024, "accuracy_train_std": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "accuracy_test_std": 0, "error_valid": [0.4594666666666667, 0.3753333333333333, 0.3390666666666666, 0.2969333333333334, 0.2698666666666667, 0.26626666666666665, 0.26493333333333335, 0.2613333333333333, 0.2617333333333334, 0.256, 0.25306666666666666, 0.24906666666666666, 0.2526666666666667, 0.25626666666666664, 0.2541333333333333, 0.25706666666666667, 0.2514666666666666, 0.2532, 0.25106666666666666, 0.2545333333333333, 0.2493333333333333, 0.24760000000000004, 0.24946666666666661, 0.24573333333333336, 0.24986666666666668, 0.24280000000000002, 0.24386666666666668, 0.24439999999999995, 0.24386666666666668, 0.2446666666666667, 0.24719999999999998, 0.25, 0.24239999999999995, 0.252, 0.24786666666666668, 0.2441333333333333, 0.24773333333333336, 0.24386666666666668, 0.24280000000000002, 0.24639999999999995, 0.2446666666666667, 0.24893333333333334, 0.2474666666666666, 0.25293333333333334, 0.24760000000000004, 0.2501333333333333, 0.24373333333333336, 0.2473333333333333, 0.2446666666666667, 0.24626666666666663, 0.24906666666666666, 0.24680000000000002, 0.2421333333333333, 0.24760000000000004, 0.24493333333333334, 0.24506666666666665, 0.2512, 0.25, 0.24773333333333336, 0.24586666666666668, 0.24760000000000004, 0.24719999999999998, 0.2426666666666667, 0.24719999999999998, 0.2441333333333333, 0.2426666666666667, 0.23960000000000004, 0.24626666666666663, 0.24839999999999995, 0.24760000000000004, 0.24986666666666668, 0.24373333333333336, 0.24639999999999995, 0.24760000000000004, 0.2446666666666667, 0.24239999999999995, 0.24693333333333334, 0.2493333333333333, 0.2426666666666667, 0.25026666666666664, 0.24906666666666666, 0.24626666666666663, 0.2441333333333333, 0.24773333333333336, 0.24519999999999997, 0.23919999999999997, 0.2474666666666666, 0.24293333333333333, 0.24253333333333338, 0.23719999999999997, 0.23733333333333329, 0.2393333333333333, 0.23946666666666672, 0.23893333333333333, 0.23906666666666665, 0.24039999999999995, 0.23973333333333335, 0.23986666666666667, 0.2433333333333333, 0.24160000000000004, 0.24160000000000004, 0.24173333333333336, 0.2406666666666667, 0.24119999999999997, 0.24039999999999995, 0.24, 0.23893333333333333, 0.23839999999999995, 0.23893333333333333, 0.2546666666666667, 0.23973333333333335, 0.2386666666666667, 0.2388, 0.2413333333333333, 0.24093333333333333, 0.23906666666666665, 0.23853333333333337, 0.23839999999999995, 0.23906666666666665, 0.23960000000000004, 0.246, 0.24506666666666665, 0.24093333333333333, 0.24280000000000002, 0.2453333333333333, 0.24253333333333338, 0.24973333333333336, 0.2453333333333333, 0.24960000000000004, 0.24760000000000004, 0.24319999999999997, 0.24573333333333336, 0.2552, 0.24080000000000001, 0.244, 0.2426666666666667, 0.24239999999999995, 0.24160000000000004, 0.24093333333333333, 0.24173333333333336], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.05909348625323244, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 64, "valid_ratio": 0.15, "learning_rate": 0.002232282978046564, "optimization": "adam", "nb_data_augmentation": 0, "learning_rate_decay_method": "sqrt", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 4.306404928099453e-05, "rotation_range": [0, 0], "momentum": 0.5079287161524388}, "accuracy_valid_max": 0.7628, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        y_pred = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            y_pred.extend((nnet.predict(X[mini_batch]) == y[mini_batch]).tolist())\n        return np.mean(y_pred)\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            acc = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = acc\n            status[\"accuracy_train_std\"] = 0\n            acc = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = acc\n            status[\"accuracy_valid_std\"] = 0\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n\n    # rescaling to [-1, 1]\n    X_min = X_train.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X_train.max(axis=(0, 2, 3))[None, :, None, None]\n    def preprocess(a):\n        return (a / 255.) * 2 - 1\n        # return 2 * ((a - X_min) / (X_max - X_min)) - 1\n    X_train = preprocess(X_train)\n    X_valid = preprocess(X_valid)\n\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = preprocess(X_test)\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    acc = evaluate(X_test, y_test, batch_size_eval)\n    light.set(\"accuracy_test\", acc)\n    light.set(\"accuracy_test_std\", 0)\n    print(\"Test accuracy : {}+-{}\".format(acc, 0))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.7582666666666666, "accuracy_valid_std": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "accuracy_valid": [0.5405333333333333, 0.6246666666666667, 0.6609333333333334, 0.7030666666666666, 0.7301333333333333, 0.7337333333333333, 0.7350666666666666, 0.7386666666666667, 0.7382666666666666, 0.744, 0.7469333333333333, 0.7509333333333333, 0.7473333333333333, 0.7437333333333334, 0.7458666666666667, 0.7429333333333333, 0.7485333333333334, 0.7468, 0.7489333333333333, 0.7454666666666667, 0.7506666666666667, 0.7524, 0.7505333333333334, 0.7542666666666666, 0.7501333333333333, 0.7572, 0.7561333333333333, 0.7556, 0.7561333333333333, 0.7553333333333333, 0.7528, 0.75, 0.7576, 0.748, 0.7521333333333333, 0.7558666666666667, 0.7522666666666666, 0.7561333333333333, 0.7572, 0.7536, 0.7553333333333333, 0.7510666666666667, 0.7525333333333334, 0.7470666666666667, 0.7524, 0.7498666666666667, 0.7562666666666666, 0.7526666666666667, 0.7553333333333333, 0.7537333333333334, 0.7509333333333333, 0.7532, 0.7578666666666667, 0.7524, 0.7550666666666667, 0.7549333333333333, 0.7488, 0.75, 0.7522666666666666, 0.7541333333333333, 0.7524, 0.7528, 0.7573333333333333, 0.7528, 0.7558666666666667, 0.7573333333333333, 0.7604, 0.7537333333333334, 0.7516, 0.7524, 0.7501333333333333, 0.7562666666666666, 0.7536, 0.7524, 0.7553333333333333, 0.7576, 0.7530666666666667, 0.7506666666666667, 0.7573333333333333, 0.7497333333333334, 0.7509333333333333, 0.7537333333333334, 0.7558666666666667, 0.7522666666666666, 0.7548, 0.7608, 0.7525333333333334, 0.7570666666666667, 0.7574666666666666, 0.7628, 0.7626666666666667, 0.7606666666666667, 0.7605333333333333, 0.7610666666666667, 0.7609333333333334, 0.7596, 0.7602666666666666, 0.7601333333333333, 0.7566666666666667, 0.7584, 0.7584, 0.7582666666666666, 0.7593333333333333, 0.7588, 0.7596, 0.76, 0.7610666666666667, 0.7616, 0.7610666666666667, 0.7453333333333333, 0.7602666666666666, 0.7613333333333333, 0.7612, 0.7586666666666667, 0.7590666666666667, 0.7609333333333334, 0.7614666666666666, 0.7616, 0.7609333333333334, 0.7604, 0.754, 0.7549333333333333, 0.7590666666666667, 0.7572, 0.7546666666666667, 0.7574666666666666, 0.7502666666666666, 0.7546666666666667, 0.7504, 0.7524, 0.7568, 0.7542666666666666, 0.7448, 0.7592, 0.756, 0.7573333333333333, 0.7576, 0.7584, 0.7590666666666667, 0.7582666666666666], "seed": 636776181, "model": "residualv5", "loss_std": [0.27294495701789856, 0.15590311586856842, 0.13788367807865143, 0.13211475312709808, 0.12608273327350616, 0.11990678310394287, 0.11420290917158127, 0.10858342051506042, 0.10592766106128693, 0.10226230323314667, 0.09727156162261963, 0.095355324447155, 0.09020489454269409, 0.08753398805856705, 0.08162787556648254, 0.07945219427347183, 0.07272559404373169, 0.06960216164588928, 0.06720125675201416, 0.06419064849615097, 0.06302708387374878, 0.06896417587995529, 0.05217753350734711, 0.0527767650783062, 0.048148445785045624, 0.04850323870778084, 0.04398036375641823, 0.04356006905436516, 0.04019416496157646, 0.03762013837695122, 0.03914114832878113, 0.038883455097675323, 0.0378456674516201, 0.032579123973846436, 0.0328858345746994, 0.03261513635516167, 0.030306922271847725, 0.0302011389285326, 0.030249672010540962, 0.031808990985155106, 0.023722169920802116, 0.02192043699324131, 0.029679778963327408, 0.02755899168550968, 0.02818867564201355, 0.022167222574353218, 0.02430250681936741, 0.02320016361773014, 0.021946368739008904, 0.02600347250699997, 0.02347688004374504, 0.02059561386704445, 0.020573100075125694, 0.022866014391183853, 0.021044164896011353, 0.024887874722480774, 0.015432620421051979, 0.021195078268647194, 0.027583446353673935, 0.013571642339229584, 0.025982659310102463, 0.021808892488479614, 0.015547394752502441, 0.017757907509803772, 0.018883805721998215, 0.01739964261651039, 0.02208905853331089, 0.021538471803069115, 0.017093300819396973, 0.014150398783385754, 0.024049337953329086, 0.017733324319124222, 0.01353382971137762, 0.016366472467780113, 0.022056708112359047, 0.01661771349608898, 0.017423326149582863, 0.012167650274932384, 0.015433087944984436, 0.021533329039812088, 0.014417723752558231, 0.01623552106320858, 0.01628394052386284, 0.010171047411859035, 0.016515815630555153, 0.014590630307793617, 0.020526472479104996, 0.024049699306488037, 0.013563428074121475, 0.004286044742912054, 0.0015715527115389705, 0.0006001103902235627, 0.0005872189067304134, 0.0007416388252750039, 0.0009646045509725809, 0.0012482003076002002, 0.0015882013831287622, 0.001952582155354321, 0.16374383866786957, 0.01873633824288845, 0.003169415285810828, 0.0008587292395532131, 0.0005452159093692899, 0.00045807615970261395, 0.00046160040074028075, 0.0005282203783281147, 0.0006537510198540986, 0.0008351800497621298, 0.0010656805243343115, 0.185692697763443, 0.05523798614740372, 0.0059228213503956795, 0.0016112099401652813, 0.000602294341661036, 0.00044493688619695604, 0.00039944055606611073, 0.0004101527447346598, 0.00046859640860930085, 0.0005735437152907252, 0.0007282079895958304, 0.02436625398695469, 0.1581740528345108, 0.0397879034280777, 0.04777314513921738, 0.016471974551677704, 0.02388550341129303, 0.026262834668159485, 0.010605431161820889, 0.006786311976611614, 0.03156839683651924, 0.01623258739709854, 0.010529184713959694, 0.01873183622956276, 0.022570239380002022, 0.009124081581830978, 0.0033510916400700808, 0.001101551577448845, 0.0004280532884877175, 0.0002893665514420718, 0.0003015225229319185]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:44 2016", "state": "available"}], "summary": "fd53cf12568348fdfbf8cb6eaa09325e"}