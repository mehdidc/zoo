{"content": {"hp_model": {"f0": 64, "f1": 64, "f2": 16, "f3": 64, "nonlin": "rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.6515339612960815, 1.2424347400665283, 1.0689122676849365, 0.9662806987762451, 0.9014474749565125, 0.8501881957054138, 0.8086764216423035, 0.7800387740135193, 0.7543362975120544, 0.727581799030304, 0.707432210445404, 0.6895062327384949, 0.6732252836227417, 0.6635794639587402, 0.6495428085327148, 0.6393102407455444, 0.6302471160888672, 0.6209124326705933, 0.6094631552696228, 0.6019371747970581, 0.5937768816947937, 0.5881595611572266, 0.5832069516181946, 0.5753858089447021, 0.5727953314781189, 0.5659853219985962, 0.5618643164634705, 0.5545063614845276, 0.5520106554031372, 0.5486651659011841, 0.5426816344261169, 0.5408838391304016, 0.5383269190788269, 0.534511387348175, 0.5316829681396484, 0.5283259749412537, 0.5264195799827576, 0.522127628326416, 0.5211794972419739, 0.5185047388076782, 0.5150088667869568, 0.5126239657402039, 0.5077318549156189, 0.5070911645889282, 0.509067952632904, 0.5044915080070496, 0.5017054080963135, 0.5008091330528259, 0.49971911311149597, 0.5004401803016663, 0.49887171387672424, 0.49608519673347473, 0.4954131245613098, 0.4915868937969208, 0.4924490451812744, 0.48746734857559204, 0.4898749589920044, 0.48519375920295715, 0.4868123233318329, 0.48510807752609253, 0.47906729578971863, 0.48065489530563354, 0.48239660263061523, 0.4792240858078003, 0.4796219766139984, 0.4761323928833008, 0.47434383630752563, 0.4751249849796295, 0.4727083444595337, 0.4707900285720825, 0.4710735082626343, 0.4696839451789856, 0.4685620665550232, 0.46969038248062134, 0.46551674604415894, 0.4667697250843048, 0.4641009569168091, 0.46886950731277466, 0.463045209646225, 0.4607587456703186, 0.46285581588745117, 0.462313711643219, 0.4628680646419525, 0.4589115381240845, 0.45844143629074097, 0.4580395817756653, 0.45823240280151367, 0.4569889008998871, 0.4572749435901642, 0.4551507234573364, 0.4546051025390625, 0.4526657462120056, 0.45051541924476624, 0.4533355236053467, 0.4545934200286865, 0.45048171281814575, 0.45342719554901123, 0.44989830255508423, 0.4491046667098999, 0.45361921191215515, 0.4474862515926361, 0.44850659370422363, 0.44461768865585327, 0.44749006628990173, 0.4460470676422119, 0.4439479112625122, 0.4476483166217804, 0.44246822595596313, 0.4447033405303955, 0.4457734525203705, 0.4428621828556061, 0.4419315457344055, 0.44129472970962524, 0.4452914595603943, 0.44108861684799194, 0.44339483976364136, 0.43931764364242554, 0.4380699396133423, 0.4410095512866974, 0.4386434555053711, 0.4390297830104828, 0.4414435029029846, 0.4367437958717346, 0.43494826555252075, 0.43879491090774536, 0.4338901937007904, 0.4345985949039459, 0.43455255031585693, 0.4358862340450287, 0.433452308177948, 0.4317317306995392, 0.43559423089027405, 0.4312469959259033, 0.4346582591533661, 0.4328900873661041, 0.4299401044845581, 0.4333791732788086, 0.4322107434272766, 0.4305676221847534, 0.4319920539855957, 0.4323394298553467, 0.4286787509918213, 0.4357028305530548, 0.4297151565551758, 0.4307483732700348, 0.42974036931991577, 0.4286143183708191, 0.4303615987300873, 0.42766332626342773, 0.4291926622390747, 0.42852088809013367, 0.426074355840683, 0.4261925518512726, 0.424276739358902, 0.42782503366470337, 0.4256187081336975, 0.4264976978302002, 0.42454400658607483, 0.42386919260025024, 0.4263898432254791, 0.42331889271736145, 0.4259389638900757, 0.4257550835609436, 0.4248928725719452, 0.4233283996582031, 0.424185186624527, 0.4218771159648895, 0.42428985238075256, 0.4204986095428467, 0.4223407804965973, 0.42507249116897583, 0.4214618504047394, 0.42122507095336914, 0.4217565059661865, 0.42437538504600525, 0.4185330867767334, 0.42144495248794556, 0.4191097319126129, 0.4183725416660309, 0.41968443989753723, 0.41775327920913696, 0.4188368320465088, 0.4195545017719269, 0.4180910885334015, 0.41888928413391113, 0.4160063564777374, 0.41666901111602783, 0.4177936017513275, 0.41494858264923096, 0.41849321126937866, 0.4172132611274719, 0.4176114797592163, 0.41564473509788513, 0.41664984822273254, 0.4141295552253723, 0.4175448417663574, 0.4147190451622009, 0.4166111946105957, 0.41801688075065613, 0.4148220121860504, 0.4161739647388458, 0.4133252203464508, 0.4141067862510681, 0.4132385849952698, 0.4155789315700531, 0.4109284579753876, 0.4172445833683014, 0.4113939106464386, 0.41643041372299194, 0.41203367710113525, 0.4133642911911011, 0.4147285223007202, 0.40700119733810425, 0.41178202629089355, 0.4095487892627716, 0.41029977798461914, 0.4105038642883301, 0.4140608608722687, 0.41259416937828064, 0.4109972417354584, 0.40671461820602417, 0.41036999225616455, 0.4137706756591797], "moving_avg_accuracy_train": [0.05759631523394241, 0.12275507423172755, 0.18518701261997045, 0.24548851282761164, 0.3020078493274732, 0.35349595876303525, 0.40159514256211926, 0.4450727810836851, 0.48360520065550294, 0.5204067704985905, 0.554097808766884, 0.5855495492368641, 0.6148581826991135, 0.6407872351437186, 0.6654623436187801, 0.6853730547107134, 0.7055013337207698, 0.724237527464326, 0.7416093094228122, 0.7570973567128123, 0.7698856145642793, 0.7831621597258376, 0.7938903832950589, 0.8042131382645102, 0.8137522644643604, 0.8225398741371104, 0.8307112123163303, 0.8382166955455224, 0.8455737718958244, 0.851867186482497, 0.8578660810390739, 0.86325592974003, 0.8674160801791868, 0.8720811186982744, 0.8771329108809109, 0.8801358413286725, 0.8840147516875458, 0.8865130766641401, 0.889823909807296, 0.8921341970230873, 0.8945599387387372, 0.8975335126827853, 0.8992310218277126, 0.9014818292402718, 0.9031587114925089, 0.9045144456980938, 0.9069971983355103, 0.9078993293925093, 0.908695079448598, 0.9093532699741146, 0.9109893809208246, 0.9123479484811969, 0.9134868818307517, 0.914814623776414, 0.9151814780631321, 0.9155209835652353, 0.9162682807421189, 0.9176313453001053, 0.9186976681344359, 0.9196665511341152, 0.9201224883921969, 0.9211442739637377, 0.921905879005533, 0.9225866011478923, 0.9231480617533873, 0.9232905809864096, 0.9233421904830529, 0.924627943345508, 0.924255317200326, 0.925359004489845, 0.9254478401635071, 0.9250186928269737, 0.9258462239514837, 0.9260817583254384, 0.9266703773203216, 0.9272977185680791, 0.9280599993886891, 0.9278415331915146, 0.928107582178334, 0.9288981225831474, 0.9289261954415732, 0.9283770411117478, 0.9282502478244471, 0.9284057790301438, 0.9287341302176609, 0.9289994554007213, 0.9288219743309334, 0.929196989545496, 0.9288765942719635, 0.9291275649031373, 0.929832383077137, 0.9302691899313834, 0.9299856256989962, 0.9305304493244178, 0.9304558154754017, 0.9298886298707832, 0.9296245565027986, 0.9295217130537462, 0.9302754720674469, 0.9304843193155287, 0.9314952042732799, 0.9317424054221793, 0.9320904084430843, 0.932687243267842, 0.9329919518268089, 0.9324104987191557, 0.9323661715770297, 0.9319937087717169, 0.9315166942183732, 0.931094572859705, 0.9311283959297237, 0.93208428196775, 0.9310774849079259, 0.931422297613608, 0.9310746119356267, 0.9317916636504251, 0.9318139784592474, 0.9320036534550078, 0.9325371202142966, 0.9332731148131604, 0.932682326841442, 0.9324481285680583, 0.932611735129165, 0.9331471366900764, 0.9332663469782485, 0.9337269507101947, 0.9339649990523351, 0.9340234215412047, 0.9339202889085866, 0.9338877431641842, 0.9334771275894601, 0.9340491867424281, 0.9326855360349166, 0.9320371764029088, 0.932983528553131, 0.9333190624526168, 0.9342116668085917, 0.9350638028051503, 0.9352399932091591, 0.9353964917656887, 0.9359348327665377, 0.9353010512363771, 0.9355326439544245, 0.9358619490899437, 0.9361327470750062, 0.935720881443733, 0.9364731771529496, 0.9360180760651002, 0.936026867676475, 0.934981631911273, 0.9348500715083056, 0.934650214839664, 0.9343052222235917, 0.9354129614964799, 0.9362821518040116, 0.9352740584974569, 0.9356990126917772, 0.9359303367940465, 0.9349086689611258, 0.9351772829043451, 0.9349283930056141, 0.9347208483825167, 0.9356359985133496, 0.9344090326251929, 0.9354575627329209, 0.9342854986084844, 0.9328075359107958, 0.9325537691864105, 0.9333716960987495, 0.9330081070305873, 0.9329343541382983, 0.933681706520934, 0.9352006057343353, 0.9351655502942535, 0.9355038072053505, 0.9360407893551089, 0.9359497254851203, 0.9363280753687788, 0.9368337118783664, 0.9378863479810429, 0.9385290178329477, 0.9389144693972905, 0.9394054989825706, 0.9383689193570156, 0.9381170860511129, 0.9376556681436761, 0.937521626886479, 0.9373916531109449, 0.9376327856784494, 0.9372476274963557, 0.9374682132955389, 0.9367370414791817, 0.9375807445848626, 0.9381865094120906, 0.937753134547155, 0.9382719059138681, 0.9385297530439652, 0.9385178190313277, 0.9391415556544039, 0.9394215756092201, 0.9397804783161553, 0.9392828934667288, 0.9384630793415398, 0.9385435908633659, 0.9390880564413427, 0.9396338790329504, 0.9390766214475771, 0.9393306187885614, 0.940212511113286, 0.9401276323949068, 0.9392814009995006, 0.9391707262638455, 0.9391432707124886, 0.9381700441972568, 0.9382659804382916, 0.9382336323194809, 0.9380604318839982, 0.9386532854575493, 0.9390519950427929, 0.9389807532373878, 0.939562846737477, 0.940019301572081, 0.9391700244637379], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.056542086314006014, 0.12007394813629516, 0.17992199265813247, 0.23799474656438246, 0.29252587949454056, 0.34118268301722504, 0.3864570340208489, 0.4271408557505411, 0.46272163910432734, 0.4970425015963795, 0.5280390821031573, 0.5574039368352361, 0.5845301658926764, 0.6078623467337552, 0.6302489400912531, 0.6484416900956821, 0.6670266967732373, 0.6843635543455371, 0.6999362821188146, 0.7137909866911952, 0.7248645012091992, 0.7363536017283395, 0.7457497327546773, 0.7546823248971313, 0.7631631699676591, 0.7703554478974746, 0.7778060900429681, 0.783944996993114, 0.7908394068382304, 0.795873383134829, 0.8011771228051564, 0.8057450279945202, 0.8088398705452188, 0.8130596285490553, 0.817221562672689, 0.8190516819137033, 0.8221250999422125, 0.8242086119265304, 0.8265170487854587, 0.8285468433421538, 0.8300318615681793, 0.832220811141783, 0.8337412646190957, 0.8354576466756349, 0.8367733983860834, 0.8374835597240564, 0.8396700564436689, 0.840357047646064, 0.8410171083652678, 0.8412174494865723, 0.8430061431674332, 0.8439689948239579, 0.8449230695508995, 0.8454083193070294, 0.8453038166779078, 0.8453208041742736, 0.8459382084142257, 0.8468244915825923, 0.8477137727048301, 0.8484601500465458, 0.8487524421766803, 0.8491793440433496, 0.8497110696070116, 0.8503869941316268, 0.8507226534816418, 0.8507032460495167, 0.8503795740706945, 0.8510954229042125, 0.85053927975724, 0.8516409605446937, 0.8513222028750135, 0.8512469585381899, 0.8517498805778196, 0.8517254066860769, 0.8525152212979662, 0.8529686772837569, 0.8535934257074896, 0.8535840278374184, 0.8540394369418542, 0.8550414196878044, 0.8549675240951535, 0.8544809785286351, 0.8541335372081512, 0.8538737861793542, 0.8543175740241447, 0.854135016546429, 0.8534040438356867, 0.8533697564984283, 0.853368459500694, 0.8537649766906847, 0.8550751671805018, 0.8550599615034908, 0.854567113649452, 0.8549720127890249, 0.8551574345805291, 0.8543761367173557, 0.8537176787308611, 0.8535065730377449, 0.8541853361500096, 0.8548237255395267, 0.855865231703571, 0.8557730785915723, 0.855926192419162, 0.8561503265187368, 0.8563714608002215, 0.8556824568982867, 0.8556838824716357, 0.8553748421631018, 0.8546318091892615, 0.8543181129277149, 0.8544386183235729, 0.8553425892284144, 0.8541052347201061, 0.8546783920651135, 0.8544454867347316, 0.8552347894825687, 0.8555108563739202, 0.855660630817477, 0.8564537250681842, 0.8573922369269531, 0.8569806028897548, 0.8570820884403877, 0.8568376585402495, 0.8579473556003812, 0.8579195157481743, 0.8583442080927846, 0.8585259711596357, 0.8587404450621209, 0.858480781909448, 0.8584840191364249, 0.857891876635433, 0.8580710742316788, 0.8569882643894597, 0.8566739447276221, 0.8574704233252666, 0.8576063170337791, 0.8577468583819825, 0.8584145730050041, 0.8584875252959947, 0.8593498749877808, 0.8596925665646805, 0.8594160809925498, 0.8594958043127225, 0.8596062354119473, 0.8597401854776803, 0.8594090803805899, 0.860062204722049, 0.8599653933707025, 0.8594386629568251, 0.8577103698915642, 0.8575160636535373, 0.8572588273465419, 0.8569439244688154, 0.8580887345351116, 0.8589461061399588, 0.8575916581295021, 0.8576045355676815, 0.8579905722688199, 0.8572443729579771, 0.8574761138907185, 0.8574864267768726, 0.8571467049387937, 0.8581115160431825, 0.8575547119068613, 0.8586121172221992, 0.857304304643428, 0.8558281275205761, 0.8550631505648287, 0.8555903739590236, 0.8548379948368412, 0.8547600276667866, 0.8557245134166742, 0.8575610240949616, 0.8572361975702546, 0.857332419680699, 0.8577486094238489, 0.8572219188975032, 0.8574509047656595, 0.8578990736546809, 0.8589961674187309, 0.8595277736154873, 0.859997100687297, 0.8603503708990643, 0.8600701695584048, 0.8600855135306517, 0.8589231829983696, 0.8588752677011682, 0.8583193015485212, 0.8582573556274793, 0.8576502288749723, 0.8576969588583335, 0.85689261265247, 0.8575613321383524, 0.8578935954794871, 0.8578253920403487, 0.858410981601374, 0.858207649348616, 0.8578313968384532, 0.8585864019211742, 0.8587693891763459, 0.8592413125045697, 0.8589956862898808, 0.8583401700422932, 0.858352467985353, 0.8595274692101611, 0.8597007694465245, 0.8590113659770227, 0.859546599861248, 0.8607892643291895, 0.8609330117949904, 0.8602943710541209, 0.8601499290070672, 0.8605824841108786, 0.8597581400672605, 0.859917491723185, 0.8596021885693154, 0.8595879999269923, 0.8602039657946093, 0.8605069878898471, 0.8602761309683322, 0.8604478072163786, 0.8605779017771202, 0.8602240602063359], "moving_var_accuracy_train": [0.029856019756749005, 0.06508139264825713, 0.09365297576165163, 0.11701411653111582, 0.1340626234634655, 0.14451558983635465, 0.15088581419196184, 0.15280997823547318, 0.15089170663445342, 0.14799173585724895, 0.14340833680788456, 0.13797041093441498, 0.13190433379979383, 0.12476474226589017, 0.11776801684360563, 0.10955914290492301, 0.1022495571575908, 0.09518400604579635, 0.0883816147169354, 0.08170236972495735, 0.07500398860234209, 0.06908998960494998, 0.06321684367301575, 0.05785419273714816, 0.052887727821343364, 0.04829395379305451, 0.04406549532250166, 0.04016593629678464, 0.036636481818923726, 0.03332929724146905, 0.030320248140430484, 0.02754967754756021, 0.02495047145789194, 0.022651287571563886, 0.02061584425271643, 0.01863541814891163, 0.01690728984417004, 0.015272735508951106, 0.013844116502972376, 0.012507741695850175, 0.011309925532104555, 0.010258512256900592, 0.009258594866884545, 0.00837833058627197, 0.007565804934435584, 0.006825766578117761, 0.006198666466233373, 0.005586124383606055, 0.005033210908611336, 0.004533788750661122, 0.004104501606864508, 0.0037106627985229214, 0.0033512710412431806, 0.0030320100251873044, 0.0027300202612777253, 0.002458055611023578, 0.0022172761275564244, 0.0020122700197039286, 0.0018212764172166687, 0.0016475973838986106, 0.0014847085545585128, 0.0013456341108905412, 0.0012162910799586792, 0.0010988324156786945, 0.0009917863162145304, 0.0008927904901791088, 0.0008035354130224918, 0.0007380603155300471, 0.0006655039361737015, 0.0006099166732537414, 0.0005489960319206025, 0.0004957539356566259, 0.0004523418119492598, 0.00040760691872616164, 0.0003699644777437819, 0.0003365100433396443, 0.000308088687450908, 0.00027770936601958845, 0.00025057546798811815, 0.00023114250837408815, 0.00020803535030510108, 0.00018994594957628537, 0.00017109604345799738, 0.0001542041487157068, 0.00013975406436523172, 0.00012641223500360298, 0.00011405450727444016, 0.0001039147842473772, 9.444718400435784e-05, 8.556934192332854e-05, 8.148332565659782e-05, 7.50521951421881e-05, 6.827065369297304e-05, 6.411508336903277e-05, 5.77537069349002e-05, 5.4873631832187374e-05, 5.0013881342077014e-05, 4.510768418298639e-05, 4.5710289621303e-05, 4.153181521645486e-05, 4.657562927508028e-05, 4.24680420197269e-05, 3.931119274078524e-05, 3.858597973909805e-05, 3.5563007518358005e-05, 3.504949621411834e-05, 3.156223065246802e-05, 2.9654564459294482e-05, 2.8736993970280358e-05, 2.7466972546247668e-05, 2.4730571292212375e-05, 3.0480977222231134e-05, 3.655564237704177e-05, 3.397014035733608e-05, 3.166109429766241e-05, 3.3122453323153496e-05, 2.9814689547073164e-05, 2.715701002851615e-05, 2.7002590075059537e-05, 2.917752351356384e-05, 2.94010450099517e-05, 2.6954579990259968e-05, 2.4500025952768413e-05, 2.462991684032968e-05, 2.2294824991551467e-05, 2.1974744673341952e-05, 2.028727332476978e-05, 1.828926467714415e-05, 1.6556065268626304e-05, 1.4909991771072034e-05, 1.4936438945818631e-05, 1.6388060121686807e-05, 3.148514337838843e-05, 3.2119960952304155e-05, 3.6968206387145895e-05, 3.4284632727768135e-05, 3.802685228174037e-05, 4.075938886324619e-05, 3.696283750310457e-05, 3.3486979936556885e-05, 3.274658124165712e-05, 3.308703436924574e-05, 3.0261047615794466e-05, 2.8210919704728664e-05, 2.6049811672680877e-05, 2.497153018942963e-05, 2.756791667743783e-05, 2.6675178011149476e-05, 2.400835584190962e-05, 3.144018050143429e-05, 2.84519357079515e-05, 2.5966226329160845e-05, 2.4440782842544746e-05, 3.304048122857917e-05, 3.65358592220861e-05, 4.202854233236145e-05, 3.9450962704559696e-05, 3.598746399672004e-05, 4.1782964044472026e-05, 3.8254048694451144e-05, 3.498615946021861e-05, 3.1875216449386554e-05, 3.622519266211857e-05, 4.61516810122076e-05, 5.1431251392296265e-05, 5.865173505918482e-05, 7.244592517509684e-05, 6.578091061123404e-05, 6.522385945546664e-05, 5.9891246604303025e-05, 5.3951077345961754e-05, 5.3582789865846316e-05, 6.898800426349995e-05, 6.210026379206387e-05, 5.691999705400137e-05, 5.382314581103375e-05, 4.8515464885685996e-05, 4.4952256107297105e-05, 4.2758045015018825e-05, 4.845462539543819e-05, 4.7326383702822107e-05, 4.3930901508628214e-05, 4.170780184034923e-05, 4.7207497537355333e-05, 4.305752790927692e-05, 4.066793348608001e-05, 3.6762843665150725e-05, 3.323859793957519e-05, 3.0438042381620066e-05, 2.872935957056183e-05, 2.6294346466717203e-05, 2.8476421845360526e-05, 3.2035294035645584e-05, 3.213432386524049e-05, 3.061121544073744e-05, 2.997220747495578e-05, 2.7573353009954152e-05, 2.4817299494877445e-05, 2.5836995920087777e-05, 2.3958996903936178e-05, 2.27223975909508e-05, 2.2678473973265074e-05, 2.6459483374672224e-05, 2.3871873983525775e-05, 2.4152671475587845e-05, 2.441870504161325e-05, 2.477165868555648e-05, 2.2875124660044535e-05, 2.75872188457122e-05, 2.489333653264421e-05, 2.8848971050519114e-05, 2.6074314019478184e-05, 2.3473666883233145e-05, 2.965082884446328e-05, 2.6768579821111858e-05, 2.4101139446115992e-05, 2.1961011019166653e-05, 2.2928188154300647e-05, 2.2066093339156667e-05, 1.9905162558777494e-05, 2.0964141888514584e-05, 2.074288684396421e-05, 2.5160042620369998e-05], "duration": 259731.948988, "accuracy_train": [0.5759631523394242, 0.709183905211794, 0.7470744581141565, 0.7882020146963824, 0.8106818778262275, 0.8168889436830934, 0.8344877967538759, 0.8363715277777777, 0.8303969768018641, 0.8516208990863787, 0.8573171531815246, 0.868615213466685, 0.8786358838593578, 0.8741487071451642, 0.8875383198943337, 0.8645694545381136, 0.8866558448112772, 0.8928632711563308, 0.8979553470491879, 0.8964897823228128, 0.8849799352274824, 0.9026510661798633, 0.890444395418051, 0.8971179329895718, 0.8996044002630121, 0.9016283611918604, 0.9042532559293098, 0.9057660446082503, 0.9117874590485419, 0.9085079177625508, 0.9118561320482651, 0.9117645680486341, 0.9048574341315985, 0.9140664653700628, 0.92259904052464, 0.9071622153585271, 0.9189249449174051, 0.9089980014534883, 0.9196214080956996, 0.9129267819652085, 0.9163916141795865, 0.9242956781792175, 0.9145086041320598, 0.9217390959533037, 0.9182506517626431, 0.9167160535483574, 0.9293419720722591, 0.9160185089055003, 0.915856829953396, 0.9152769847037652, 0.9257143794412146, 0.9245750565245479, 0.9237372819767442, 0.9267643012873754, 0.9184831666435955, 0.9185765330841639, 0.9229939553340717, 0.9298989263219823, 0.928294573643411, 0.9283864981312293, 0.9242259237149317, 0.9303403441076044, 0.9287603243816908, 0.9287131004291252, 0.9282012072028424, 0.9245732540836102, 0.9238066759528424, 0.9361997191076044, 0.9209016818936876, 0.935292190095515, 0.9262473612264673, 0.9211563667981728, 0.9332940040720746, 0.92820156769103, 0.931967948274271, 0.9329437897978959, 0.9349205267741787, 0.9258753374169435, 0.9305020230597084, 0.9360129862264673, 0.9291788511674051, 0.9234346521433187, 0.9271091082387413, 0.9298055598814139, 0.9316892909053157, 0.9313873820482651, 0.9272246447028424, 0.9325721264765596, 0.9259930368101699, 0.9313863005837025, 0.9361757466431341, 0.9342004516196014, 0.9274335476075121, 0.9354338619532114, 0.9297841108342562, 0.9247839594292175, 0.9272478961909376, 0.9285961220122739, 0.937059303190753, 0.9323639445482651, 0.9405931688930418, 0.9339672157622739, 0.9352224356312293, 0.9380587566906607, 0.9357343288575121, 0.9271774207502769, 0.9319672272978959, 0.9286415435239018, 0.9272235632382798, 0.9272954806316908, 0.931432803559893, 0.9406872563099853, 0.9220163113695091, 0.9345256119647471, 0.9279454408337948, 0.9382451290836102, 0.932014811738649, 0.9337107284168512, 0.9373383210478959, 0.9398970662029347, 0.9273652350959765, 0.9303403441076044, 0.9340841941791252, 0.9379657507382798, 0.9343392395717978, 0.9378723842977114, 0.9361074341315985, 0.93454922394103, 0.9329920952150241, 0.9335948314645626, 0.9297815874169435, 0.93919771911914, 0.9204126796673128, 0.9262019397148394, 0.9415006979051311, 0.9363388675479882, 0.9422451060123662, 0.9427330267741787, 0.9368257068452381, 0.9368049787744556, 0.9407799017741787, 0.9295970174649317, 0.9376169784168512, 0.9388256953096161, 0.9385699289405685, 0.9320140907622739, 0.9432438385358989, 0.9319221662744556, 0.9361059921788483, 0.9255745100244556, 0.9336660278815985, 0.93285150482189, 0.9312002886789406, 0.9453826149524732, 0.9441048645717978, 0.9262012187384644, 0.9395236004406607, 0.9380122537144703, 0.9257136584648394, 0.9375948083933187, 0.9326883839170359, 0.93285294677464, 0.9438723496908453, 0.9233663396317828, 0.9448943337024732, 0.9237369214885567, 0.9195058716315985, 0.9302698686669435, 0.9407330383098007, 0.9297358054171282, 0.9322705781076966, 0.9404078779646549, 0.9488706986549464, 0.934850051333518, 0.9385481194052234, 0.9408736287029347, 0.9351301506552234, 0.9397332243217055, 0.9413844404646549, 0.9473600729051311, 0.9443130465000923, 0.942383533476375, 0.9438247652500923, 0.929039702727021, 0.9358505862979882, 0.9335029069767442, 0.9363152555717055, 0.936221889131137, 0.9398029787859912, 0.9337812038575121, 0.9394534854881875, 0.9301564951319674, 0.9451740725359912, 0.9436383928571429, 0.9338527607627353, 0.9429408482142857, 0.9408503772148394, 0.9384104129175894, 0.9447551852620893, 0.9419417552025655, 0.9430106026785714, 0.93480462982189, 0.9310847522148394, 0.9392681945598007, 0.9439882466431341, 0.9445462823574198, 0.9340613031792175, 0.9416165948574198, 0.9481495420358066, 0.9393637239294942, 0.9316653184408453, 0.9381746536429494, 0.9388961707502769, 0.9294110055601699, 0.9391294066076044, 0.9379424992501846, 0.9365016279646549, 0.9439889676195091, 0.9426403813099853, 0.9383395769887413, 0.9448016882382798, 0.944127395083518, 0.931526530488649], "end": "2016-02-01 16:03:56.237000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0, 211.0, 212.0, 213.0, 214.0, 215.0, 216.0, 217.0, 218.0, 219.0, 220.0, 221.0, 222.0], "moving_var_accuracy_valid": [0.02877306772266456, 0.06222243814985609, 0.08823629023266091, 0.1097646639256977, 0.1255509976609471, 0.1343032586562585, 0.1393208345198267, 0.14028531122264443, 0.13765070939700166, 0.13448693287708668, 0.1296853316173964, 0.1244774506965816, 0.11865219635281388, 0.11168649268273971, 0.10502827947385149, 0.09750423690097917, 0.09086243546972422, 0.08448129159709167, 0.07821575109008802, 0.07212175153017071, 0.06601318089117764, 0.06059985767871007, 0.05533445741521602, 0.05051913249514529, 0.04611454184362343, 0.04196864741563848, 0.03827139128949647, 0.03478342776742978, 0.03173288097469873, 0.02878766113342131, 0.026162061910494614, 0.02373364753981631, 0.021446485239557212, 0.019462093934099974, 0.01767177980153517, 0.01593474584930863, 0.014426284349779455, 0.013022725114500677, 0.011768412529635537, 0.010628651870153485, 0.00958563419532279, 0.00867019427791253, 0.007823980859111327, 0.007068096479476283, 0.0063768676546005895, 0.005743719851274094, 0.005212374777290568, 0.004695384911771027, 0.004229767541971247, 0.0038071520168580925, 0.003455231640927847, 0.0031180522266473147, 0.002814439331243881, 0.0025351146040519103, 0.002281701430842159, 0.0020535338849332384, 0.0018516111883995128, 0.0016735195502503308, 0.0015132849834546137, 0.001366970197335193, 0.0012310420898057211, 0.0011095780876590408, 0.001001164867568602, 0.0009051602464785272, 0.0008156582266239472, 0.0007340957937973477, 0.0006616290863664848, 0.0006000781337018766, 0.0005428539771310087, 0.0004994918844349096, 0.00045045715405923827, 0.0004054623940453307, 0.0003671925298423063, 0.000330478667600469, 0.0003030450649308066, 0.00027459115941717095, 0.00025064483881206273, 0.00022558114981051334, 0.00020488961190108978, 0.0001934363755196163, 0.00017414188299517378, 0.00015885823399034452, 0.0001440588498319271, 0.00013026020022138452, 0.00011900670905990014, 0.0001074059832479393, 0.00010147427485779482, 9.13374279654818e-05, 8.220370030876173e-05, 7.539836321550968e-05, 8.330791897042243e-05, 7.497920798690046e-05, 6.96673782532867e-05, 6.417613024699967e-05, 5.806794838918113e-05, 5.7754990709256795e-05, 5.588159391813783e-05, 5.0694525049318814e-05, 4.97715468075279e-05, 4.846226124060741e-05, 5.3378650924227404e-05, 4.811721559626402e-05, 4.3516488634430296e-05, 3.9616964622316995e-05, 3.609537149411528e-05, 3.67583717366362e-05, 3.308255285330694e-05, 3.0633850778665714e-05, 3.253934770272476e-05, 3.0171061033027305e-05, 2.728464888360267e-05, 3.1910654566442035e-05, 4.2499004722875886e-05, 4.1205688329812313e-05, 3.757332353311353e-05, 3.942298062949057e-05, 3.616659892304613e-05, 3.275183048622618e-05, 3.5137633852145656e-05, 3.955111104838033e-05, 3.7120983168764036e-05, 3.35015787047731e-05, 3.068913461902977e-05, 3.8703069244510786e-05, 3.483973783639776e-05, 3.2979036340893243e-05, 2.997847301904383e-05, 2.7394617210764627e-05, 2.5261980065392575e-05, 2.2735876375599814e-05, 2.3617983411368158e-05, 2.1545191076733996e-05, 2.9942966358719376e-05, 2.7837841371206458e-05, 3.076346064263629e-05, 2.7853318478492184e-05, 2.524575346563647e-05, 2.6733763479245102e-05, 2.410828546216753e-05, 2.8390279834262483e-05, 2.6608189502737836e-05, 2.463536899683204e-05, 2.222903436716311e-05, 2.011588617953064e-05, 1.8265781142566425e-05, 1.742587829618244e-05, 1.9522433115221402e-05, 1.7654541743445037e-05, 1.8386091929232496e-05, 4.34304550111673e-05, 3.942720373727609e-05, 3.608001802227769e-05, 3.336449062165364e-05, 4.182335235052575e-05, 4.425679173465863e-05, 5.634187727846434e-05, 5.070918200634446e-05, 4.6979482817342956e-05, 4.72928552391295e-05, 4.304690445438746e-05, 3.874317120953615e-05, 3.5907552433991476e-05, 4.069454139495792e-05, 3.9415364871482394e-05, 4.553678239247906e-05, 5.637646782395893e-05, 7.035071112384293e-05, 6.858234769687933e-05, 6.422579349366836e-05, 6.289788323576627e-05, 5.666280482864656e-05, 5.936861920140832e-05, 8.378670052444016e-05, 7.63576409123752e-05, 6.880520507198309e-05, 6.348360968551406e-05, 5.963187491184376e-05, 5.4140598170997056e-05, 5.053423653167742e-05, 5.631334542256592e-05, 5.3225457216177854e-05, 4.988532259756128e-05, 4.601998892050407e-05, 4.21246051502198e-05, 3.791426357255664e-05, 4.6281947611776136e-05, 4.1674415731951625e-05, 4.028885942475777e-05, 3.6294509156485536e-05, 3.59824842833238e-05, 3.240388907709586e-05, 3.4986255539368854e-05, 3.5512301742622846e-05, 3.2954661919118146e-05, 2.9701061109199172e-05, 2.9817191204116257e-05, 2.7207568128808977e-05, 2.5760904878562424e-05, 2.8315108465116377e-05, 2.5784956638602328e-05, 2.5210865624238868e-05, 2.3232769197896547e-05, 2.4776806235767307e-05, 2.23004867668221e-05, 3.249608899484407e-05, 2.9516776842672167e-05, 3.084259345225561e-05, 3.0336611904437648e-05, 4.120088553295221e-05, 3.7266766984974716e-05, 3.721084824956261e-05, 3.367753496921982e-05, 3.1993716732797065e-05, 3.491023297975317e-05, 3.164774623399094e-05, 2.937771632015352e-05, 2.6441756546276926e-05, 2.721230644227217e-05, 2.5317477309866048e-05, 2.3265383842781173e-05, 2.1204100065792314e-05, 1.9236011411824163e-05, 1.8439244985577696e-05], "accuracy_test": 0.09950573979591837, "start": "2016-01-29 15:55:04.288000", "learning_rate_per_epoch": [0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697, 0.0012809487525373697], "accuracy_train_first": 0.5759631523394242, "accuracy_train_last": 0.931526530488649, "batch_size_eval": 1024, "accuracy_train_std": [0.015863876308757876, 0.018539414651402155, 0.020963647810875587, 0.019054001853229544, 0.019595066162172967, 0.020849889942844348, 0.01963344193781076, 0.019014916022665838, 0.018457752433506436, 0.020045381311537158, 0.01879550770427981, 0.019444511522625104, 0.02000874041657138, 0.019227608241383816, 0.020377385607948346, 0.020382884174994866, 0.01905749002306065, 0.0191357787336533, 0.017805124923896076, 0.0187297176274995, 0.020081586958707775, 0.01772614571828022, 0.02033924235195692, 0.018780986774801556, 0.01906044969312782, 0.01796846774674065, 0.019977391882700996, 0.01750013087565227, 0.017784348031492602, 0.019873698046424415, 0.018569784739539692, 0.01801888990481728, 0.019101250182842876, 0.01700862931854153, 0.016584617467957637, 0.0166132554471478, 0.01785599122504293, 0.019675028404113663, 0.01860979297761579, 0.01663797531094806, 0.01637546318333619, 0.01687173196266181, 0.017376805378488736, 0.01644477405479175, 0.016075812505531806, 0.016508138853854423, 0.01677086679710035, 0.018260723607997107, 0.01626981933041321, 0.015934430799502058, 0.015801084013192832, 0.01594636224447349, 0.016790550136708736, 0.014451682289031619, 0.016742680200582817, 0.017425142424043385, 0.017833808222081915, 0.016317517930134064, 0.015786722023561697, 0.017345098305476165, 0.0160351883679596, 0.017000823262418367, 0.016353550218754054, 0.016492715108485118, 0.015973042773088857, 0.018102659766759708, 0.016591662883284307, 0.016171949985088706, 0.016178745928910776, 0.01592278016652821, 0.015892246966737092, 0.01711269198217693, 0.015096369243233975, 0.016021019378112913, 0.015100741077573348, 0.015782005153519022, 0.015479670927177959, 0.016785274942009487, 0.017109429974418004, 0.01604657019832593, 0.014699897687970491, 0.017537213095673028, 0.016704383008175783, 0.015592999759021052, 0.016245340543879737, 0.01704317236826176, 0.01638714341536547, 0.016759116696468145, 0.01681525915028512, 0.017037511024704993, 0.017031726864580127, 0.01655514278111173, 0.01748175752461206, 0.015167703277528612, 0.01542148623656597, 0.015857719873531045, 0.016999258934944545, 0.01727747320804824, 0.015269156117008152, 0.01635915723054762, 0.014725244152329066, 0.015913919285552438, 0.01677229956277107, 0.015574034009559097, 0.015350584278081983, 0.017290729510649545, 0.016445770016216523, 0.016330584100633146, 0.016468944312747022, 0.01686408845288591, 0.016044289192420725, 0.016494528203858858, 0.016950751526674985, 0.016241188888473933, 0.017215911075218734, 0.015378143238405476, 0.01459146366315761, 0.015068963261113667, 0.015532626567639695, 0.014307820109993456, 0.015493952838735964, 0.015645947352072035, 0.016305428754329086, 0.015437991438813609, 0.015114553726082058, 0.014892723476352565, 0.01546979569643487, 0.014305907847560684, 0.013841039773705852, 0.015862803904313296, 0.016124023967681173, 0.015546319311971197, 0.016009132166491264, 0.016535864268268443, 0.014859329959264457, 0.015449316148166754, 0.013827741222737479, 0.01412758908220443, 0.01527148462960676, 0.01576935753259088, 0.015281294569832843, 0.015011498089492589, 0.016128506774412812, 0.014988248771251394, 0.015337160382697008, 0.01565081049605635, 0.014545585347262939, 0.015498549513279955, 0.014612598818515137, 0.016044144213635283, 0.014499257924010099, 0.01504963917919583, 0.014257362232604051, 0.014086166048069407, 0.013241363257573704, 0.017599213347604496, 0.014747602034175676, 0.015457098124315118, 0.016189348564047887, 0.014788810183906098, 0.016736655534411674, 0.015169164180719077, 0.013515374108901537, 0.01594959948309268, 0.012941513706602613, 0.017712074321698957, 0.01644431470279137, 0.0158358957540466, 0.015254442862025073, 0.015421323268173186, 0.014950933587707254, 0.014988273806552205, 0.013305439172472348, 0.01640513003252696, 0.01391958248504399, 0.014402578782639806, 0.014686396901323777, 0.015133160302499223, 0.014115935439946346, 0.014433974150542768, 0.01432292928832953, 0.014694412437819248, 0.014830372908585335, 0.015277157399348068, 0.014084293777235744, 0.015269319333937699, 0.0161990705876969, 0.016644439819503573, 0.015411974598210936, 0.01633677622526562, 0.014920176616301859, 0.013516029227778852, 0.015378780935952942, 0.014209836447432332, 0.01422057618866715, 0.016996440195702907, 0.013893729957892119, 0.013006436767201163, 0.013973756872837405, 0.014421799426041075, 0.01601514497515135, 0.01417772444619449, 0.015237408697838375, 0.014760934377019868, 0.014168109535237394, 0.013316659429703902, 0.01510338385740485, 0.014768909416919784, 0.013740794440815376, 0.013142361478478179, 0.01645640410614249, 0.016323552421603413, 0.014025661341931407, 0.0156520312423169, 0.014156109026405165, 0.01628959065156367, 0.01506420492725519, 0.014240499470120328, 0.013898918303140719, 0.013867957906436822, 0.015711759237939427, 0.015108094212740565, 0.015793555006202616], "accuracy_test_std": 0.008598738409044708, "error_valid": [0.43457913685993976, 0.30813929546310237, 0.28144560664533136, 0.23935046827936746, 0.2166939241340362, 0.22090608527861444, 0.2060738069465362, 0.20670474868222888, 0.21705131071159633, 0.19406973597515065, 0.19299169333584332, 0.1783123705760542, 0.17133377259036142, 0.1821480256965362, 0.1682717196912651, 0.18782355986445776, 0.1657082431287651, 0.1596047275037651, 0.15990916792168675, 0.16151667215737953, 0.1754738681287651, 0.16024449359939763, 0.1696850880082832, 0.1649243458207832, 0.1605092243975903, 0.16491405073418675, 0.1551381306475903, 0.1608048404555723, 0.14711090455572284, 0.1588208301957832, 0.15108922016189763, 0.15314382530120485, 0.16330654649849397, 0.14896254941641573, 0.1453210302146084, 0.16447724491716864, 0.15021413780120485, 0.1570397802146084, 0.15270701948418675, 0.1531850056475903, 0.1566029743975903, 0.1480786426957832, 0.1525746540850903, 0.14909491481551207, 0.15138483621987953, 0.15612498823418675, 0.1406514730798193, 0.15346003153237953, 0.15304234516189763, 0.15697948042168675, 0.1408956137048193, 0.1473653402673193, 0.1464902579066265, 0.15022443288780118, 0.15563670698418675, 0.15452630835843373, 0.14850515342620485, 0.1451989599021084, 0.14428269719503017, 0.14482245387801207, 0.1486169286521084, 0.1469785391566265, 0.14550340032003017, 0.14352968514683728, 0.14625641236822284, 0.1494714208396084, 0.15253347373870485, 0.1424619375941265, 0.15446600856551207, 0.13844391236822284, 0.1515466161521084, 0.14943024049322284, 0.14372382106551207, 0.1484948583396084, 0.14037644719503017, 0.1429502188441265, 0.14078383847891573, 0.14650055299322284, 0.14186188111822284, 0.1359407355986446, 0.14569753623870485, 0.14989793157003017, 0.14899343467620485, 0.1484639730798193, 0.14168833537274095, 0.14750800075301207, 0.15317471056099397, 0.14693882953689763, 0.14664321347891573, 0.14266636859939763, 0.1331331184111446, 0.1450768895896084, 0.14986851703689763, 0.1413838949548193, 0.14317376929593373, 0.15265554405120485, 0.1522084431475903, 0.14839337820030118, 0.1397057958396084, 0.1394307699548193, 0.13476121282003017, 0.14505629941641573, 0.14269578313253017, 0.1418324665850903, 0.14163833066641573, 0.1505185782191265, 0.14430328736822284, 0.14740652061370485, 0.15205548757530118, 0.14850515342620485, 0.14447683311370485, 0.13652167262801207, 0.15703095585466864, 0.1401631918298193, 0.14765066123870485, 0.13766148578689763, 0.14200454160391573, 0.14299139919051207, 0.13640842667545183, 0.1341611563441265, 0.14672410344503017, 0.14200454160391573, 0.14536221056099397, 0.13206537085843373, 0.14233104292168675, 0.13783356080572284, 0.13983816123870485, 0.13932928981551207, 0.1438561864646084, 0.1414868458207832, 0.14743740587349397, 0.1403161474021084, 0.15275702419051207, 0.14615493222891573, 0.13536126929593373, 0.1411706395896084, 0.14098826948418675, 0.13557599538780118, 0.1408559040850903, 0.1328889777861446, 0.13722320924322284, 0.1430722891566265, 0.13978668580572284, 0.13939988469503017, 0.13905426393072284, 0.14357086549322284, 0.1340596762048193, 0.14090590879141573, 0.1453019107680723, 0.1578442676957832, 0.14423269248870485, 0.14505629941641573, 0.14589020143072284, 0.13160797486822284, 0.13333754941641573, 0.1545983739646084, 0.14227956748870485, 0.13853509742093373, 0.1494714208396084, 0.1404382177146084, 0.14242075724774095, 0.14591079160391573, 0.1332051840173193, 0.14745652532003017, 0.13187123493975905, 0.15446600856551207, 0.1574574665850903, 0.15182164203689763, 0.13966461549322284, 0.15193341726280118, 0.14594167686370485, 0.13559511483433728, 0.12591037980045183, 0.1456872411521084, 0.14180158132530118, 0.13850568288780118, 0.1475182958396084, 0.14048822242093373, 0.1380674063441265, 0.1311299887048193, 0.13568777061370485, 0.13577895566641573, 0.13647019719503017, 0.14245164250753017, 0.1397763907191265, 0.15153779179216864, 0.1415559699736446, 0.14668439382530118, 0.14230015766189763, 0.1478139118975903, 0.14188247129141573, 0.15034650320030118, 0.13642019248870485, 0.13911603445030118, 0.14278843891189763, 0.13631871234939763, 0.14362234092620485, 0.14555487575301207, 0.13461855233433728, 0.1395837255271084, 0.13651137754141573, 0.1432149496423193, 0.14755947618599397, 0.1415368505271084, 0.12989751976656627, 0.13873952842620485, 0.14719326524849397, 0.13563629518072284, 0.12802675545933728, 0.13777326101280118, 0.14545339561370485, 0.14115004941641573, 0.1355245199548193, 0.14766095632530118, 0.13864834337349397, 0.14323553981551207, 0.14053969785391573, 0.13425234139683728, 0.13676581325301207, 0.14180158132530118, 0.13800710655120485, 0.13825124717620485, 0.14296051393072284], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.06129906103367386, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 32, "valid_ratio": 0.15, "learning_rate": 0.0012809487242219767, "optimization": "adam", "nb_data_augmentation": 3, "learning_rate_decay_method": "none", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 4.341059946354908e-05, "rotation_range": [0, 0], "momentum": 0.5911446342712243}, "accuracy_valid_max": 0.8740896201995482, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8570394860692772, "accuracy_valid_std": [0.018213530932567997, 0.01475826054275753, 0.016880579958347297, 0.018231243848205028, 0.010607483848609834, 0.013163792980047988, 0.008791822263306772, 0.009060038626639108, 0.01640546894528789, 0.008859163095126259, 0.010525604652317595, 0.012106147713207666, 0.012560515397305207, 0.014689402787113833, 0.013568817647014498, 0.009146395039606578, 0.01503936105788459, 0.015290998923342672, 0.010245887105541527, 0.012790897387837924, 0.012068139416998205, 0.010653170746222188, 0.008902658086477291, 0.009061669584153567, 0.011402771309006248, 0.008316871597335206, 0.012791295207085423, 0.014270701295833887, 0.007842880431027389, 0.014626342216606154, 0.010969783183381201, 0.010323550770092465, 0.01497901331162867, 0.007864134689034248, 0.008944246018191087, 0.012614183412075831, 0.01180665211034263, 0.010796469553817254, 0.011923667004582078, 0.01214872351487924, 0.015167583402774095, 0.014566417755218003, 0.010965021402835357, 0.012408934801903346, 0.016104254613507268, 0.01496169461556588, 0.009997124866343203, 0.015432395217964603, 0.01451994904150636, 0.011812352071929677, 0.01205852077364281, 0.013991927230215386, 0.008413535532063017, 0.009870848435900649, 0.015574507751962003, 0.014333480766062945, 0.007826120588875303, 0.011747082980045271, 0.008553287884959153, 0.011793232050289598, 0.014506506692892319, 0.013082662455518842, 0.009730713351582177, 0.013660159145124641, 0.010505690202231736, 0.0094023672484916, 0.0083871422679036, 0.013412552903589163, 0.013726049047998177, 0.012992168451423215, 0.011317537771036994, 0.010250913627087842, 0.014088131638025733, 0.013967449399978283, 0.014216494461748283, 0.009692313067553191, 0.010408012400682466, 0.012956855691978918, 0.01200536278802722, 0.009900742111123143, 0.01643502964947591, 0.01126552570610387, 0.014555501972270736, 0.013402891714074484, 0.011493943923882729, 0.01227068278738482, 0.013458921619750853, 0.015276249037723264, 0.011855766129588186, 0.014567024789201542, 0.010128265236226085, 0.01399852056196739, 0.012378001733010012, 0.014073825940831067, 0.013278207584120549, 0.009229523074491132, 0.013998735727971506, 0.011570835019197498, 0.015077490794780405, 0.013595019185391612, 0.010204348708549787, 0.010715520131512026, 0.012596130129273821, 0.01442782692733366, 0.009663209915871869, 0.008804556322039593, 0.015426816318690993, 0.011728454752489206, 0.013171440379134973, 0.013792483995479684, 0.0140385492588281, 0.012878247972856433, 0.0194776596927449, 0.00775580140928761, 0.015158878820479345, 0.01826436967244516, 0.012969164535370609, 0.011018892095987488, 0.011563003656489034, 0.010824552888704365, 0.009340608848329449, 0.012201939347427035, 0.014478381731869395, 0.011728339762053877, 0.01873929519281267, 0.013449945402314858, 0.012893302794160393, 0.014637865051795245, 0.013621622179732827, 0.019023278684799635, 0.014387296150329876, 0.013515233468538269, 0.010141686447978386, 0.013656158808722771, 0.010924164671988727, 0.015213619591666686, 0.015525740074189004, 0.016152035665919984, 0.014235614138798062, 0.009349861760830323, 0.012500374317448102, 0.009481628639607704, 0.01361498398978689, 0.010354202178801324, 0.012545011129129376, 0.009796555081411779, 0.01202404952547275, 0.013554797022888029, 0.01941473810907365, 0.0156809546521215, 0.011952977555674218, 0.01261618909979278, 0.012402348996116174, 0.010053274567388753, 0.012286988048184873, 0.010984467311151106, 0.012717491608508578, 0.008220382564009813, 0.009223158340182913, 0.012058587412495325, 0.01164046013982285, 0.011401904374631629, 0.01211666124665893, 0.011115424664850014, 0.011685124596080941, 0.011104780518917298, 0.013609217867508683, 0.010876558801961466, 0.01134734450750936, 0.007701835744242134, 0.01578656200723424, 0.007249459791199532, 0.01092062535708584, 0.00887079069924475, 0.011788340867670964, 0.01616003658174267, 0.012417474587955986, 0.00828165527386327, 0.009695276316819799, 0.013247221572737021, 0.01669182874439695, 0.013465070417887217, 0.010924422075179206, 0.011764130253665843, 0.008825249071789578, 0.017333336224900325, 0.01062550889060399, 0.01274842968900235, 0.013302706186468777, 0.017068535559186446, 0.012249232214850988, 0.00960902297212298, 0.013216911664038749, 0.012266625987228613, 0.012181329824400997, 0.015660989141152572, 0.011437299105219333, 0.012394132476697154, 0.010677913089710044, 0.012049673460550152, 0.009430093688973, 0.01270992880310746, 0.013953729879798269, 0.012965229526107234, 0.014174605320657663, 0.01407093715364646, 0.014993484893550726, 0.011220402588569177, 0.008677719507402804, 0.012742717882488293, 0.011911088863828771, 0.013199902674116555, 0.016723741910546645, 0.013523099280457349, 0.019058374933735755, 0.01020915595060107, 0.015582450190913458, 0.009614444104238203, 0.011726639948342428, 0.012476148482184346, 0.012483215969238801, 0.012702002952261343, 0.010759464712254703], "accuracy_valid": [0.5654208631400602, 0.6918607045368976, 0.7185543933546686, 0.7606495317206325, 0.7833060758659638, 0.7790939147213856, 0.7939261930534638, 0.7932952513177711, 0.7829486892884037, 0.8059302640248494, 0.8070083066641567, 0.8216876294239458, 0.8286662274096386, 0.8178519743034638, 0.8317282803087349, 0.8121764401355422, 0.8342917568712349, 0.8403952724962349, 0.8400908320783133, 0.8384833278426205, 0.8245261318712349, 0.8397555064006024, 0.8303149119917168, 0.8350756541792168, 0.8394907756024097, 0.8350859492658133, 0.8448618693524097, 0.8391951595444277, 0.8528890954442772, 0.8411791698042168, 0.8489107798381024, 0.8468561746987951, 0.836693453501506, 0.8510374505835843, 0.8546789697853916, 0.8355227550828314, 0.8497858621987951, 0.8429602197853916, 0.8472929805158133, 0.8468149943524097, 0.8433970256024097, 0.8519213573042168, 0.8474253459149097, 0.8509050851844879, 0.8486151637801205, 0.8438750117658133, 0.8593485269201807, 0.8465399684676205, 0.8469576548381024, 0.8430205195783133, 0.8591043862951807, 0.8526346597326807, 0.8535097420933735, 0.8497755671121988, 0.8443632930158133, 0.8454736916415663, 0.8514948465737951, 0.8548010400978916, 0.8557173028049698, 0.8551775461219879, 0.8513830713478916, 0.8530214608433735, 0.8544965996799698, 0.8564703148531627, 0.8537435876317772, 0.8505285791603916, 0.8474665262612951, 0.8575380624058735, 0.8455339914344879, 0.8615560876317772, 0.8484533838478916, 0.8505697595067772, 0.8562761789344879, 0.8515051416603916, 0.8596235528049698, 0.8570497811558735, 0.8592161615210843, 0.8534994470067772, 0.8581381188817772, 0.8640592644013554, 0.8543024637612951, 0.8501020684299698, 0.8510065653237951, 0.8515360269201807, 0.858311664627259, 0.8524919992469879, 0.846825289439006, 0.8530611704631024, 0.8533567865210843, 0.8573336314006024, 0.8668668815888554, 0.8549231104103916, 0.8501314829631024, 0.8586161050451807, 0.8568262307040663, 0.8473444559487951, 0.8477915568524097, 0.8516066217996988, 0.8602942041603916, 0.8605692300451807, 0.8652387871799698, 0.8549437005835843, 0.8573042168674698, 0.8581675334149097, 0.8583616693335843, 0.8494814217808735, 0.8556967126317772, 0.8525934793862951, 0.8479445124246988, 0.8514948465737951, 0.8555231668862951, 0.8634783273719879, 0.8429690441453314, 0.8598368081701807, 0.8523493387612951, 0.8623385142131024, 0.8579954583960843, 0.8570086008094879, 0.8635915733245482, 0.8658388436558735, 0.8532758965549698, 0.8579954583960843, 0.854637789439006, 0.8679346291415663, 0.8576689570783133, 0.8621664391942772, 0.8601618387612951, 0.8606707101844879, 0.8561438135353916, 0.8585131541792168, 0.852562594126506, 0.8596838525978916, 0.8472429758094879, 0.8538450677710843, 0.8646387307040663, 0.8588293604103916, 0.8590117305158133, 0.8644240046121988, 0.8591440959149097, 0.8671110222138554, 0.8627767907567772, 0.8569277108433735, 0.8602133141942772, 0.8606001153049698, 0.8609457360692772, 0.8564291345067772, 0.8659403237951807, 0.8590940912085843, 0.8546980892319277, 0.8421557323042168, 0.8557673075112951, 0.8549437005835843, 0.8541097985692772, 0.8683920251317772, 0.8666624505835843, 0.8454016260353916, 0.8577204325112951, 0.8614649025790663, 0.8505285791603916, 0.8595617822853916, 0.857579242752259, 0.8540892083960843, 0.8667948159826807, 0.8525434746799698, 0.868128765060241, 0.8455339914344879, 0.8425425334149097, 0.8481783579631024, 0.8603353845067772, 0.8480665827371988, 0.8540583231362951, 0.8644048851656627, 0.8740896201995482, 0.8543127588478916, 0.8581984186746988, 0.8614943171121988, 0.8524817041603916, 0.8595117775790663, 0.8619325936558735, 0.8688700112951807, 0.8643122293862951, 0.8642210443335843, 0.8635298028049698, 0.8575483574924698, 0.8602236092808735, 0.8484622082078314, 0.8584440300263554, 0.8533156061746988, 0.8576998423381024, 0.8521860881024097, 0.8581175287085843, 0.8496534967996988, 0.8635798075112951, 0.8608839655496988, 0.8572115610881024, 0.8636812876506024, 0.8563776590737951, 0.8544451242469879, 0.8653814476656627, 0.8604162744728916, 0.8634886224585843, 0.8567850503576807, 0.852440523814006, 0.8584631494728916, 0.8701024802334337, 0.8612604715737951, 0.852806734751506, 0.8643637048192772, 0.8719732445406627, 0.8622267389871988, 0.8545466043862951, 0.8588499505835843, 0.8644754800451807, 0.8523390436746988, 0.861351656626506, 0.8567644601844879, 0.8594603021460843, 0.8657476586031627, 0.8632341867469879, 0.8581984186746988, 0.8619928934487951, 0.8617487528237951, 0.8570394860692772], "seed": 297624995, "model": "residualv3", "loss_std": [0.29132774472236633, 0.19199326634407043, 0.17951613664627075, 0.16974999010562897, 0.16610372066497803, 0.16184531152248383, 0.15907183289527893, 0.1552310436964035, 0.1518343687057495, 0.14884814620018005, 0.14837408065795898, 0.14273689687252045, 0.1427766978740692, 0.1423550397157669, 0.14155490696430206, 0.13982461392879486, 0.1366465985774994, 0.1340295821428299, 0.13369184732437134, 0.1309957504272461, 0.12946829199790955, 0.12571905553340912, 0.12726181745529175, 0.12588322162628174, 0.12357961386442184, 0.12266640365123749, 0.12338871508836746, 0.12272235751152039, 0.12088863551616669, 0.11824493855237961, 0.11757203191518784, 0.11638564616441727, 0.11694018542766571, 0.11808802932500839, 0.11333576589822769, 0.11567800492048264, 0.11206837743520737, 0.1125774085521698, 0.1131017729640007, 0.10840258002281189, 0.11094071716070175, 0.10921522974967957, 0.10951919108629227, 0.10897429287433624, 0.1093103438615799, 0.10663211345672607, 0.10521174967288971, 0.1071254089474678, 0.10804668068885803, 0.10657258331775665, 0.10458215326070786, 0.10754787921905518, 0.10337116569280624, 0.1011892557144165, 0.10469701141119003, 0.10341918468475342, 0.10401469469070435, 0.09999065846204758, 0.10281536728143692, 0.10022807866334915, 0.09834595024585724, 0.10042240470647812, 0.10142547637224197, 0.096486397087574, 0.10406654328107834, 0.09960127621889114, 0.10049054026603699, 0.10107969492673874, 0.09955372661352158, 0.09854508936405182, 0.09570290893316269, 0.09817972779273987, 0.09891724586486816, 0.09784449636936188, 0.09582215547561646, 0.09569615870714188, 0.09747176617383957, 0.09517916291952133, 0.09556597471237183, 0.09674821048974991, 0.09161508083343506, 0.09368021041154861, 0.09338384866714478, 0.09216922521591187, 0.09198585152626038, 0.09489906579256058, 0.0916532427072525, 0.09433984756469727, 0.0966489166021347, 0.0916842594742775, 0.09236419200897217, 0.08998008817434311, 0.09133338928222656, 0.09460078924894333, 0.09557551890611649, 0.09066001325845718, 0.09616672247648239, 0.08983815461397171, 0.0930156260728836, 0.0933549627661705, 0.09402526915073395, 0.09157636016607285, 0.09247426688671112, 0.09396857023239136, 0.09189671277999878, 0.08852981775999069, 0.09114744514226913, 0.08950282633304596, 0.08973034471273422, 0.0897567868232727, 0.09147930890321732, 0.08881682902574539, 0.08865202963352203, 0.09088148921728134, 0.09051372855901718, 0.09162488579750061, 0.09061829000711441, 0.08889541774988174, 0.08837395906448364, 0.09044007956981659, 0.08733265846967697, 0.09114943444728851, 0.08876664936542511, 0.08838378638029099, 0.08868102729320526, 0.08707938343286514, 0.08911842107772827, 0.08707874268293381, 0.08976341784000397, 0.08952085673809052, 0.08996892720460892, 0.09035983681678772, 0.08721321821212769, 0.08840981870889664, 0.08884599804878235, 0.08796301484107971, 0.08859573304653168, 0.08870302885770798, 0.08663532137870789, 0.08708766847848892, 0.08667271584272385, 0.0879315659403801, 0.08771295100450516, 0.08545338362455368, 0.0858921930193901, 0.09028825163841248, 0.08708953857421875, 0.08838515728712082, 0.08647359162569046, 0.09074730426073074, 0.0867006778717041, 0.0880693569779396, 0.08471477031707764, 0.08630283921957016, 0.08669772744178772, 0.08593270927667618, 0.08452261984348297, 0.08565916866064072, 0.08855153620243073, 0.08829433470964432, 0.08635887503623962, 0.08522550016641617, 0.0873711109161377, 0.0862685814499855, 0.08575943112373352, 0.08738575875759125, 0.08363319933414459, 0.08358818292617798, 0.08346150815486908, 0.08541400730609894, 0.08740458637475967, 0.08517449349164963, 0.0860285609960556, 0.0873446986079216, 0.08660197257995605, 0.08440040051937103, 0.08526486158370972, 0.0856264978647232, 0.08439213782548904, 0.08543028682470322, 0.08337552100419998, 0.08609979599714279, 0.08645865321159363, 0.08229091763496399, 0.08792050927877426, 0.08258526772260666, 0.08295732736587524, 0.08468060940504074, 0.08502398431301117, 0.08365759998559952, 0.08609634637832642, 0.08468533307313919, 0.0851568654179573, 0.08326687663793564, 0.08236446231603622, 0.08672728389501572, 0.08087556064128876, 0.08532040566205978, 0.08236826956272125, 0.08442018181085587, 0.08543944358825684, 0.08189644664525986, 0.08367691934108734, 0.08356551080942154, 0.08378437906503677, 0.08254910260438919, 0.0821678638458252, 0.08029285073280334, 0.08485197275876999, 0.08136576414108276, 0.08194655179977417, 0.08308520913124084, 0.08048327267169952, 0.08117393404245377, 0.08032413572072983, 0.08407757431268692, 0.07983127981424332, 0.0833229348063469, 0.08243124186992645, 0.08461568504571915, 0.08107338100671768, 0.08207700401544571, 0.0838325023651123]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:28 2016", "state": "available"}], "summary": "459b9102a8a68cb8aa418a8916e6e239"}