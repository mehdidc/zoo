{"content": {"hp_model": {"f0": 16, "f1": 64, "f2": 16, "f3": 32, "nonlin": "very_leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.604913592338562, 1.238123893737793, 1.1021405458450317, 1.0160157680511475, 0.9537795782089233, 0.9077728390693665, 0.868415355682373, 0.8383644223213196, 0.812660276889801, 0.7905582785606384, 0.7697461247444153, 0.7509449124336243, 0.7352744340896606, 0.7186549305915833, 0.7052652835845947, 0.6943544745445251, 0.6799858808517456, 0.6663306355476379, 0.6522065997123718, 0.6442686319351196, 0.6345417499542236, 0.6250522136688232, 0.6160957217216492, 0.6051819920539856, 0.5975629687309265, 0.5889506340026855, 0.581034243106842, 0.5704333782196045, 0.5639168620109558, 0.5552546977996826, 0.5512674450874329, 0.5404114723205566, 0.5342100858688354, 0.5280807614326477, 0.5203863382339478, 0.5189625024795532, 0.5073664784431458, 0.49846917390823364, 0.49287787079811096, 0.48919302225112915, 0.4864252805709839, 0.47282174229621887, 0.4753914475440979, 0.46653488278388977, 0.46605798602104187, 0.45609158277511597, 0.44996678829193115, 0.44345465302467346, 0.43495973944664, 0.43387845158576965, 0.42719128727912903, 0.43343624472618103, 0.4210573136806488, 0.41308891773223877, 0.417568176984787, 0.3985522389411926, 0.40275144577026367, 0.39566153287887573, 0.39191436767578125, 0.3909892737865448, 0.3812097907066345, 0.3888104557991028, 0.3759940266609192, 0.37344589829444885, 0.3693062365055084, 0.36411258578300476, 0.3580217957496643, 0.361463725566864, 0.36065080761909485, 0.3441397249698639, 0.34567931294441223, 0.3482082784175873, 0.3333933353424072, 0.3342258036136627, 0.33537644147872925, 0.32380902767181396, 0.32390403747558594, 0.32023856043815613, 0.32165291905403137, 0.313001811504364, 0.30976182222366333, 0.3106841742992401, 0.30532780289649963, 0.3025312125682831, 0.3024786114692688, 0.2911953330039978, 0.29717588424682617, 0.28627467155456543, 0.28306934237480164, 0.2763894498348236, 0.2842593789100647, 0.29037898778915405, 0.2668328881263733, 0.26472318172454834, 0.2729185223579407, 0.2652256190776825, 0.2650910019874573, 0.2636447548866272, 0.25378522276878357, 0.2583203911781311, 0.24278591573238373, 0.24995486438274384, 0.24624702334403992, 0.24360430240631104, 0.24610187113285065, 0.2411949187517166, 0.23637470602989197, 0.23215532302856445, 0.22945621609687805, 0.23038481175899506, 0.22709210216999054, 0.2289443463087082, 0.22495950758457184, 0.2190241515636444, 0.21568584442138672, 0.21902576088905334, 0.2225043773651123, 0.20306052267551422, 0.2066444754600525, 0.20902594923973083, 0.21285180747509003, 0.20167997479438782, 0.19456009566783905, 0.20497950911521912, 0.1985877901315689, 0.19416402280330658, 0.19556747376918793, 0.19038639962673187, 0.19275428354740143, 0.18425273895263672, 0.18795447051525116, 0.18611212074756622, 0.17909592390060425, 0.17751359939575195, 0.182134211063385, 0.1716805100440979, 0.1774502843618393, 0.1819579005241394, 0.16787627339363098, 0.16799217462539673, 0.16799265146255493, 0.16914811730384827, 0.15789784491062164, 0.16672645509243011, 0.16726036369800568, 0.16217266023159027, 0.1508692055940628, 0.15814091265201569, 0.16210506856441498, 0.15100893378257751, 0.152398943901062, 0.14807380735874176, 0.15513475239276886, 0.14689235389232635, 0.1425110101699829, 0.15073391795158386, 0.1459333449602127, 0.14164212346076965, 0.13461370766162872, 0.13001790642738342, 0.1633850336074829, 0.13635031878948212, 0.12774880230426788, 0.12338144332170486, 0.11889773607254028, 0.11458496749401093, 0.10965250432491302, 0.10356936603784561, 0.09606469422578812, 0.08722172677516937, 0.2058267891407013, 0.09456964582204819, 0.08638141304254532, 0.08374281227588654, 0.08174459636211395, 0.07950776815414429, 0.076736219227314, 0.07324788719415665, 0.06893225014209747, 0.18647119402885437, 0.09474824368953705, 0.07201890647411346, 0.06751333177089691, 0.06576716154813766, 0.06438560038805008, 0.06283432990312576, 0.06093030795454979, 0.05854622274637222, 0.055626172572374344, 0.18976759910583496, 0.07163435965776443, 0.05995125323534012, 0.05638747662305832, 0.05507805198431015, 0.05406876280903816, 0.052968498319387436, 0.051636952906847, 0.04997427389025688, 0.04792363569140434, 0.1565089374780655, 0.0865778774023056, 0.05417292192578316, 0.04908221215009689, 0.04758407920598984, 0.046750906854867935, 0.04598671942949295, 0.04512964189052582, 0.04409276694059372, 0.04280472919344902, 0.041238829493522644, 0.15658918023109436, 0.06125237047672272, 0.045482467859983444, 0.04240984842181206, 0.04142662137746811, 0.04080378636717796, 0.04020571708679199], "moving_avg_accuracy_train": [0.05680282665997599, 0.115562186735765, 0.1713726126669781, 0.2248566636061219, 0.2760024398903971, 0.32370531239383615, 0.3671583704920771, 0.40679629275788415, 0.4440047326208462, 0.4785638976593337, 0.5101760653927082, 0.5390992018051723, 0.5639210192930753, 0.5876064115094395, 0.6090185235077203, 0.6285776346120977, 0.643252084375325, 0.6611755543880969, 0.6766555636352876, 0.692110292086266, 0.7039157927029901, 0.7161077493603858, 0.7271522294769497, 0.7380247904497512, 0.7490280767644568, 0.7568364358584985, 0.764914890256222, 0.7743938498509393, 0.783496864044509, 0.7904482357449862, 0.7971253222099395, 0.8032208026319873, 0.8086044284642111, 0.8153049081214814, 0.8219304337129877, 0.8257733396657014, 0.8296434703136109, 0.8340122893490641, 0.8379789595178398, 0.8428720804888133, 0.847945424073376, 0.8527646221779893, 0.854363632199716, 0.8585996428953959, 0.8617283145762329, 0.8659064399984546, 0.8679860668261747, 0.8711292629305636, 0.8733651904292662, 0.8750613770376408, 0.8774180591589966, 0.8800272461717606, 0.8826034511642191, 0.8839922284753535, 0.8857510472541101, 0.8883687835216855, 0.8908431845565139, 0.8940908497664217, 0.8973973259112727, 0.897478560466694, 0.9002416884950986, 0.9026539547634828, 0.9049622863312468, 0.9062562095934248, 0.9092854017281669, 0.9116211217470722, 0.9124538827093621, 0.9153840687682062, 0.9160776641581353, 0.9179712510149778, 0.9185967903826107, 0.9205825505919962, 0.9212304839614142, 0.9233948333308228, 0.9256170432251769, 0.927663390911011, 0.9296911517818424, 0.9306094006275138, 0.9321264658826842, 0.933570771525405, 0.9345754608515006, 0.9354842954937869, 0.9366440074980257, 0.9374598116208699, 0.9394612053838013, 0.9407463848811816, 0.9415309505216625, 0.9432484632814195, 0.944768648128296, 0.9456741098773896, 0.9458846309563728, 0.9474666477714497, 0.9482859963121804, 0.949253563682162, 0.9492504067533145, 0.9501752277947143, 0.951728146570014, 0.9528444304618314, 0.9535376962680846, 0.9541754782401134, 0.9542007468958916, 0.9536982213480522, 0.9547244906561225, 0.955197090213157, 0.9559944536240119, 0.9559147348962084, 0.956854283578053, 0.9571511062238469, 0.9578416039324884, 0.9586002356500277, 0.9597664188552815, 0.9600649967233524, 0.9611777097736547, 0.9620885067641741, 0.9629848458198996, 0.9630176008558499, 0.9636676067297979, 0.9646804754461222, 0.9654363083693948, 0.9662421881336919, 0.965642253246587, 0.9670900977874229, 0.9675493091004119, 0.9683019268130175, 0.9685003020996006, 0.9692300082718387, 0.9691030605315872, 0.9699467328260659, 0.9709524676160876, 0.9710368513973452, 0.9718103774921529, 0.9725367779120038, 0.9736276302172412, 0.9729656612788966, 0.9739717725688734, 0.9739705367917756, 0.9742018673757026, 0.9749077188441124, 0.9756592065573386, 0.9766703308789949, 0.976694605267332, 0.9780532325382179, 0.9790389040010812, 0.9785425087271729, 0.9792677721759303, 0.9802389104714417, 0.9811478121695449, 0.9816101480276181, 0.9816566237368072, 0.9815053924262493, 0.9824969093217288, 0.9822406870645745, 0.9822892851343721, 0.9828908428185631, 0.982997441906954, 0.983272453593658, 0.9830106844247684, 0.9832424831323008, 0.9843602351536037, 0.9849128079549192, 0.9850473642129988, 0.9856939484762227, 0.9866316220809813, 0.9877569073800353, 0.9888998364336984, 0.9899540492189, 0.9909051658743909, 0.9917611708643328, 0.9925152993136138, 0.9931103095608239, 0.9929133969083129, 0.9933732662948626, 0.9939313079689477, 0.9944405209220529, 0.9949243892167524, 0.9953645209796009, 0.9957466886733075, 0.9960650629607387, 0.9963236980337125, 0.9938641635708728, 0.9939778402197379, 0.9943940442930023, 0.994856983613702, 0.9952992056392366, 0.9957065060574558, 0.9960754015826626, 0.9963981069601107, 0.9966652903117187, 0.9968429763103087, 0.9952939814316772, 0.9951322149099565, 0.9954237169677796, 0.995767412979335, 0.9961023160266396, 0.9964083790668328, 0.9966838358030067, 0.9969363971631823, 0.9971590520897212, 0.9973013128033681, 0.9946021827861818, 0.9945467624611443, 0.9948154295555153, 0.9952269297547256, 0.9956251817197292, 0.9959836084882325, 0.9963108428775045, 0.9966007035302302, 0.9968569278200643, 0.9970666033416292, 0.9971832316979425, 0.9956862417865001, 0.9956573102412019, 0.9959404805932814, 0.9962674135232482, 0.9965778931530662, 0.9968549996710929, 0.9970997452396979], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.05463881894766565, 0.11183406967714606, 0.16604122888036515, 0.2177757437464702, 0.26684145462219966, 0.3126964893181122, 0.35398631657229496, 0.3912162852539208, 0.4257558542530016, 0.45738872374110506, 0.4858593357890578, 0.5115317147572153, 0.5330915633304395, 0.5536949517074858, 0.5723437464934391, 0.588749243832047, 0.6013951684699267, 0.615837524034455, 0.6285844439295788, 0.6411420681077806, 0.6503991785974543, 0.66017306674298, 0.6686327696695857, 0.6760908994232596, 0.6837075660227259, 0.6897935229934955, 0.6954163391335285, 0.7016783102653563, 0.7070027314507936, 0.7107550538441179, 0.7144362902707, 0.7176863088810547, 0.7207548685520606, 0.724130012344445, 0.7278013778739312, 0.7299001992826977, 0.7316650092207684, 0.7332767227188723, 0.7349174298238677, 0.7367135080481828, 0.7398061467953224, 0.7422436067581095, 0.7419632643428407, 0.7445512234883758, 0.7447469506312551, 0.7466678280926025, 0.7474343213563844, 0.7484771396913785, 0.7486954613491232, 0.7487353184521628, 0.7489990054040098, 0.748931147879422, 0.7499432653486335, 0.7498145143973545, 0.7493872857079955, 0.7501737724515785, 0.7508358709217822, 0.7522862517324653, 0.7535467372990532, 0.753015870515684, 0.7534893564121576, 0.7543315622901438, 0.755267505505783, 0.7554750887748582, 0.7569843911266645, 0.7583051126408806, 0.7575609330315366, 0.7590244603985636, 0.759100488803361, 0.7598617736956154, 0.7596283142289153, 0.7600794394137045, 0.7604376534636744, 0.7613031854629395, 0.762280565288257, 0.763214182799341, 0.7639648713059279, 0.7644410604272177, 0.7646743181363784, 0.7649930838472135, 0.765225114882673, 0.7653851146895864, 0.7656511848283084, 0.7656475368368179, 0.7661263578425186, 0.7669927789106764, 0.7674816481393376, 0.7683091669191539, 0.7686724273262596, 0.768906852985953, 0.768602052241198, 0.7698537575498343, 0.7705804017496701, 0.7709933294305014, 0.7709356592321801, 0.7704106232882542, 0.7709961316954981, 0.7715872129442465, 0.7723806812676682, 0.7728262480712478, 0.7726666171929634, 0.7717274333539381, 0.7722342069142522, 0.7721724602627366, 0.7726103170696708, 0.772072535786273, 0.7719343004681879, 0.7716339897737787, 0.7721103980723798, 0.7725809341781689, 0.7729322039945388, 0.7726084336609734, 0.7726913402948761, 0.7725869688312771, 0.7727514411876072, 0.7722249910635754, 0.7726770083822933, 0.7730979429450429, 0.7730393899438368, 0.7730721414615013, 0.7725370058639204, 0.7733585947165343, 0.7738822690833598, 0.7744003451211835, 0.7741077186004055, 0.7748494493288439, 0.7746951649287155, 0.7753477069825307, 0.776195460521853, 0.7760702668793062, 0.7764795534914509, 0.777111318586583, 0.7780899457640693, 0.7787468656266684, 0.7792435257789865, 0.7793171024179553, 0.7794708296290965, 0.7796590417527832, 0.7797887230443724, 0.780493432724122, 0.7805529114584868, 0.7817011041699424, 0.7822584033915024, 0.78205387682306, 0.7821526236475311, 0.7828122848049919, 0.7829115215447487, 0.7830395147215992, 0.7835229785355838, 0.783363922398893, 0.7841354167836272, 0.7835997930269513, 0.7839692823801296, 0.7841735754335324, 0.7837084075080255, 0.7843925072222079, 0.7836449804269449, 0.783369008363166, 0.7841929403111114, 0.7846973979272142, 0.7844178113253812, 0.7843931165068492, 0.78489682302258, 0.7855576784179877, 0.7864332099926047, 0.7874531220035099, 0.7884086934157343, 0.7891089867718265, 0.7897921969519481, 0.7902738382789671, 0.7901396149838265, 0.7907288808480192, 0.7911137652594522, 0.7915211963859918, 0.7918756773686276, 0.7920349893380901, 0.7922048431904257, 0.792237700362347, 0.7920210721747568, 0.7906826168303986, 0.7906051719301148, 0.790930214554498, 0.7910253813991235, 0.7910266118491961, 0.7909432995441711, 0.7906831539835792, 0.7904612300102967, 0.7904201898405923, 0.7899560075941083, 0.7892615998881614, 0.7893527297619205, 0.789961708009373, 0.7902381452186015, 0.7904248740419973, 0.7905196877955536, 0.7905795766025946, 0.7906456835601815, 0.7905963460494193, 0.7902182344113449, 0.7887894491385086, 0.7889186755819619, 0.7891926412786603, 0.7897464452042582, 0.7901960406122962, 0.7905141977521207, 0.7907150899592129, 0.7908226507580958, 0.7909072484458405, 0.7906933590681239, 0.7904652670430885, 0.7891226712969875, 0.7884239419120177, 0.7884604422050027, 0.7885299135624392, 0.788519195596632, 0.7884342482225862, 0.7884310377734451], "moving_var_accuracy_train": [0.029039050049069516, 0.05720910661280863, 0.07952142873513857, 0.09731417920537311, 0.11112577517032597, 0.12049327405900764, 0.12543746097590883, 0.1270341988122694, 0.12679099090436344, 0.12486091480734388, 0.12136878566583652, 0.11676083747865895, 0.11062985734141789, 0.1046158518472632, 0.0982805735245792, 0.09189554561686172, 0.08464404633785688, 0.07907089869975985, 0.07332048500642184, 0.06813807418922099, 0.06257859537360215, 0.057658530100464275, 0.052990501959824424, 0.04875536500280756, 0.04496947929003729, 0.041021265606707075, 0.03750649187514124, 0.03456449876261168, 0.03185383269302551, 0.029103343540386708, 0.02659426053929219, 0.02426922841954308, 0.022103156421501277, 0.020296908628088587, 0.018662296069553077, 0.016928977798050398, 0.015370881219332345, 0.014005572315279958, 0.012746625333802662, 0.01168744649595162, 0.010750351182499675, 0.009884338097593284, 0.008918915785280198, 0.008188518286277412, 0.00745776373602791, 0.006869097950819453, 0.006221111785420665, 0.0056879177426344026, 0.005164120314386052, 0.004673601724041311, 0.004256227107227247, 0.003891875108312711, 0.0035624190869499523, 0.003223535500034252, 0.0029290229414993896, 0.002697793535848663, 0.0024831181265942397, 0.0023297322777756277, 0.002195154110466283, 0.0019756980908966055, 0.0018468421703191319, 0.001714529210233477, 0.0015910318408507494, 0.0014469967934413234, 0.001384881158999839, 0.0012954933351602925, 0.0011721854190270878, 0.0011322407901793808, 0.0010233463822458206, 0.0009532827846808961, 0.0008614762017169334, 0.000810817774027855, 0.0007335143554859172, 0.0007023225936730605, 0.0006765342856368395, 0.0006465687067350229, 0.0006189181634049932, 0.0005646149755476861, 0.0005288668608889237, 0.0004947543439063898, 0.0004543635152934835, 0.0004163609874273117, 0.00038682927607956263, 0.0003541361757732522, 0.000354772751144636, 0.00033416065309455273, 0.00030628447698310724, 0.0003022046800041487, 0.00029278286972179036, 0.0002708833315612565, 0.00024419387052739667, 0.00024229947830333486, 0.000224111518753778, 0.0002101260464174795, 0.00018911353147152928, 0.00017789982395191912, 0.0001818138520608322, 0.00017484727439892692, 0.00016168810426211316, 0.00014918018643050578, 0.00013426791433213876, 0.00012311391023500691, 0.00012028157744569159, 0.00011026357277290511, 0.00010495931117634521, 9.452057573877424e-05, 9.301328369490097e-05, 8.450488847291521e-05, 8.034548339637647e-05, 7.749063380244819e-05, 8.198141983614721e-05, 7.458561654224875e-05, 7.827022787884055e-05, 7.79091655124085e-05, 7.734906228653668e-05, 6.962381208930393e-05, 6.646399960587633e-05, 6.90507269738663e-05, 6.728720494760501e-05, 6.640346420337713e-05, 6.300241460192934e-05, 7.556845747159238e-05, 6.990948699422674e-05, 6.801643908675238e-05, 6.15689699670194e-05, 6.020431285053992e-05, 5.43289231242804e-05, 5.5302077276092264e-05, 5.8875391759222726e-05, 5.305193818615445e-05, 5.31318279416758e-05, 5.25675632771427e-05, 5.802043571600461e-05, 5.616221802240232e-05, 5.9656335570529405e-05, 5.369071575778178e-05, 4.880326873354329e-05, 4.840697851929554e-05, 4.864888471553637e-05, 5.298534778858735e-05, 4.76921162230909e-05, 5.953571715153433e-05, 6.232607953070923e-05, 5.8311145989265827e-05, 5.721409502127091e-05, 5.998067182022366e-05, 6.141752530953433e-05, 5.719956278952337e-05, 5.1499046434472704e-05, 4.655497997466266e-05, 5.0747433763388876e-05, 4.626353899260178e-05, 4.1658441044834104e-05, 4.074944176703461e-05, 3.677676788114331e-05, 3.3779773943443034e-05, 3.101850442912916e-05, 2.8400229753539425e-05, 3.680453300832728e-05, 3.587211001427771e-05, 3.244784749214517e-05, 3.296570362797022e-05, 3.758221936672354e-05, 4.5220400468453745e-05, 5.2454941816974164e-05, 5.7211729003619235e-05, 5.963216213442805e-05, 6.026364680623352e-05, 5.9355669587744995e-05, 5.660643737753513e-05, 5.1294764974251736e-05, 4.806860715099663e-05, 4.6064441026039044e-05, 4.379167740792649e-05, 4.1519666406672236e-05, 3.9111143484018766e-05, 3.651449845063387e-05, 3.377530828764614e-05, 3.0999806367630735e-05, 8.234361369593215e-05, 7.422555375081373e-05, 6.836203085114852e-05, 6.34546430978834e-05, 5.886922166690611e-05, 5.447534217634933e-05, 5.0252563135372825e-05, 4.616455566754041e-05, 4.2190582591174624e-05, 3.825567515891161e-05, 5.602457384925886e-05, 5.0657632132279484e-05, 4.635662996648745e-05, 4.2784109505070345e-05, 3.951513901440844e-05, 3.640669637411824e-05, 3.344891445823841e-05, 3.0678108178297924e-05, 2.805647430727656e-05, 2.5432969872374903e-05, 8.8457398532217e-05, 7.963930139084068e-05, 7.232500932013636e-05, 6.661650011367439e-05, 6.138229175097019e-05, 5.640029031129038e-05, 5.17240023898612e-05, 4.73077749328624e-05, 4.316785541988515e-05, 3.924674429698864e-05, 3.544448942875678e-05, 5.206884964052566e-05, 4.6869497985293174e-05, 4.290421722143539e-05, 3.957576176556192e-05, 3.648576399379326e-05, 3.3528279795409946e-05, 3.071455535603501e-05], "duration": 74774.715782, "accuracy_train": [0.56802826659976, 0.6443964274178663, 0.6736664460478959, 0.7062131220584165, 0.7363144264488741, 0.7530311649247877, 0.7582358933762459, 0.7635375931501477, 0.7788806913875047, 0.7895963830057217, 0.7946855749930787, 0.7994074295173496, 0.7873173766842008, 0.8007749414567183, 0.8017275314922481, 0.8046096345514949, 0.7753221322443706, 0.8224867845030455, 0.8159756468600037, 0.8312028481450721, 0.8101652982535069, 0.8258353592769472, 0.8265525505260245, 0.8358778392049648, 0.848057653596807, 0.8271116677048725, 0.8376209798357327, 0.859704486203396, 0.8654239917866371, 0.8530105810492802, 0.8572191003945183, 0.8580801264304172, 0.8570570609542267, 0.875609225036914, 0.8815601640365448, 0.8603594932401256, 0.8644746461447952, 0.8733316606681433, 0.8736789910368217, 0.8869101692275747, 0.8936055163344407, 0.8961374051195091, 0.8687547223952565, 0.8967237391565154, 0.8898863597037652, 0.9035095687984496, 0.8867027082756552, 0.8994180278700628, 0.8934885379175894, 0.8903270565130121, 0.8986281982511997, 0.9035099292866371, 0.9057892960963455, 0.8964912242755629, 0.9015804162629198, 0.9119284099298633, 0.9131127938699704, 0.9233198366555924, 0.9271556112149317, 0.8982096714654854, 0.9251098407507383, 0.9243643511789406, 0.9257372704411223, 0.9179015189530271, 0.9365481309408453, 0.9326426019172205, 0.9199487313699704, 0.9417557432978036, 0.9223200226674971, 0.9350135327265596, 0.9242266446913067, 0.9384543924764673, 0.9270618842861758, 0.9428739776555003, 0.9456169322743633, 0.946080520083518, 0.9479409996193245, 0.9388736402385567, 0.9457800531792175, 0.946569522309893, 0.9436176647863603, 0.9436638072743633, 0.9470814155361758, 0.9448020487264673, 0.9574737492501846, 0.9523130003576044, 0.9485920412859912, 0.9587060781192323, 0.9584503117501846, 0.9538232656192323, 0.9477793206672205, 0.9617047991071429, 0.955660133178756, 0.9579616700119971, 0.9492219943936876, 0.9584986171673128, 0.9657044155477114, 0.9628909854881875, 0.9597770885243633, 0.9599155159883721, 0.9544281647978959, 0.9491754914174971, 0.963960914428756, 0.9594504862264673, 0.9631707243217055, 0.9551972663459765, 0.9653102217146549, 0.9598225100359912, 0.9640560833102622, 0.9654279211078812, 0.9702620677025655, 0.9627521975359912, 0.971192127226375, 0.9702856796788483, 0.9710518973214286, 0.9633123961794019, 0.9695176595953304, 0.9737962938930418, 0.9722388046788483, 0.9734951060123662, 0.9602428392626431, 0.9801206986549464, 0.9716822109173128, 0.9750754862264673, 0.9702856796788483, 0.9757973638219823, 0.9679605308693245, 0.977539783476375, 0.9800040807262828, 0.9717963054286637, 0.9787721123454227, 0.9790743816906607, 0.983445300964378, 0.9670079408337948, 0.9830267741786637, 0.9739594147978959, 0.9762838426310447, 0.9812603820598007, 0.982422595976375, 0.9857704497739018, 0.9769130747623662, 0.9902808779761905, 0.9879099471668512, 0.9740749512619971, 0.9857951432147471, 0.9889791551310447, 0.9893279274524732, 0.9857711707502769, 0.9820749051195091, 0.9801443106312293, 0.9914205613810447, 0.9799346867501846, 0.9827266677625508, 0.9883048619762828, 0.9839568337024732, 0.9857475587739941, 0.9806547619047619, 0.9853286715000923, 0.9944200033453304, 0.9898859631667589, 0.9862583705357143, 0.9915132068452381, 0.9950706845238095, 0.9978844750715209, 0.9991861979166666, 0.9994419642857143, 0.9994652157738095, 0.9994652157738095, 0.9993024553571429, 0.9984654017857143, 0.9911411830357143, 0.9975120907738095, 0.9989536830357143, 0.9990234375, 0.9992792038690477, 0.9993257068452381, 0.9991861979166666, 0.9989304315476191, 0.9986514136904762, 0.9717283534053157, 0.9950009300595238, 0.9981398809523809, 0.9990234375, 0.9992792038690477, 0.9993722098214286, 0.9993954613095238, 0.9993024553571429, 0.9990699404761905, 0.9984421502976191, 0.9813530275239941, 0.9936763162144703, 0.9980472354881875, 0.9988606770833334, 0.9991164434523809, 0.9991629464285714, 0.9991629464285714, 0.9992094494047619, 0.9991629464285714, 0.9985816592261905, 0.9703100126315062, 0.9940479795358066, 0.9972334334048542, 0.9989304315476191, 0.9992094494047619, 0.9992094494047619, 0.9992559523809523, 0.9992094494047619, 0.9991629464285714, 0.9989536830357143, 0.9982328869047619, 0.982213332583518, 0.995396926333518, 0.9984890137619971, 0.9992098098929494, 0.9993722098214286, 0.9993489583333334, 0.9993024553571429], "end": "2016-01-30 12:41:18.863000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0, 211.0, 212.0, 213.0, 214.0, 215.0, 216.0], "moving_var_accuracy_valid": [0.026868604823962094, 0.053623414695639054, 0.07470681820602344, 0.0913242766413023, 0.10385884483083818, 0.11239711821042447, 0.11650105490150431, 0.11732558452366751, 0.11632986251108114, 0.1137026221484355, 0.10962754168705724, 0.10459642689531376, 0.09832022784028563, 0.09230870156979577, 0.08620782933553357, 0.08000930948832385, 0.07344765322901385, 0.06798012261438284, 0.06264446605425888, 0.05779926477384161, 0.0527905851480197, 0.048371286638548956, 0.04417825713715184, 0.04026104471824039, 0.03675706273720316, 0.03341470631373337, 0.030357780234461573, 0.027674912753118026, 0.025162566626445622, 0.022773029273892045, 0.0206176898611584, 0.018650984463751425, 0.016870630543467, 0.015286091849693734, 0.013878792988384257, 0.0125305591512989, 0.011305534223226625, 0.010198359384503695, 0.009202750724292764, 0.008311508724754215, 0.007566437582061574, 0.006863264723487124, 0.006177645577966601, 0.005620158813020575, 0.005058487713748654, 0.0045858468743674, 0.004132549794241467, 0.0037290820455355192, 0.0033566028200981307, 0.0030209568353862823, 0.002719486929124823, 0.0024475796780051295, 0.0022120411461479627, 0.0019909862228002636, 0.0017935303196973396, 0.0016197443403080916, 0.0014617152757354958, 0.0013344761886259296, 0.0012153279845355219, 0.0010963315619571324, 0.0009887161058088553, 0.000896228291896198, 0.0008144893700326884, 0.0007334282503518195, 0.0006805873676191511, 0.000628227378720255, 0.0005703888704669013, 0.0005326271946065456, 0.0004794164980109152, 0.0004366908403943949, 0.00039351228625828243, 0.0003559926830236139, 0.0003215482704716152, 0.0002961357542002201, 0.00027511962068663695, 0.00025545243353099624, 0.00023497898928319277, 0.00021352189506798573, 0.0001926593879911335, 0.00017430795339765768, 0.00015736170367063902, 0.00014185593274748613, 0.00012830747934121383, 0.00011547685117766968, 0.00010599259205940519, 0.00010214950205959355, 9.408548995822113e-05, 9.084002694093603e-05, 8.294364735717775e-05, 7.514388113076379e-05, 6.846562446371585e-05, 7.571995763435704e-05, 7.290006800931706e-05, 6.714464463475486e-05, 6.0460112837249046e-05, 5.689506623525101e-05, 5.4290940466305076e-05, 5.200623980327207e-05, 5.247194364540808e-05, 4.90115172689362e-05, 4.4339703697759253e-05, 4.7844329879359355e-05, 4.5371271864323564e-05, 4.086845851865169e-05, 3.8507079917193864e-05, 3.725925030443134e-05, 3.370530630248309e-05, 3.114645429082336e-05, 3.0074492664524536e-05, 2.9059681439732926e-05, 2.726422765079225e-05, 2.5481249945786008e-05, 2.299498654071303e-05, 2.0793528508366007e-05, 1.8957636061500465e-05, 1.955622005318829e-05, 1.9439474955656555e-05, 1.909020061514565e-05, 1.7212036639183258e-05, 1.550048693244889e-05, 1.6527769209387893e-05, 2.0950066473105213e-05, 2.132317340802541e-05, 2.1606481095926358e-05, 2.0216505512297367e-05, 2.314633522265506e-05, 2.104593478549628e-05, 2.277364149492281e-05, 2.6964451916330808e-05, 2.440906775790482e-05, 2.347580076004214e-05, 2.472036490288242e-05, 3.0867728785227005e-05, 3.16648492595992e-05, 3.0718406095744996e-05, 2.769528718238803e-05, 2.513844696315653e-05, 2.2943416498364894e-05, 2.080042998502246e-05, 2.3189928581114017e-05, 2.090277520157749e-05, 3.067761620517634e-05, 3.0405096385820946e-05, 2.7741066802028306e-05, 2.505471853991363e-05, 2.646562226988431e-05, 2.3907691417553972e-05, 2.1664362555680957e-05, 2.1601561635006235e-05, 1.9669095163076974e-05, 2.3059017917856794e-05, 2.3335151404512282e-05, 2.223033770307055e-05, 2.038292479778089e-05, 2.0292063108285987e-05, 2.2474788567957178e-05, 2.5256476497886328e-05, 2.341627406797475e-05, 2.7184421354783888e-05, 2.6756276597301804e-05, 2.4784166948891623e-05, 2.2311238760563465e-05, 2.236359717041359e-05, 2.4057806136127005e-05, 2.8551025365874506e-05, 3.5057907419185216e-05, 3.9770167192009905e-05, 4.020684753409133e-05, 4.038714813267794e-05, 3.843623863044417e-05, 3.475475780402557e-05, 3.440439035194744e-05, 3.229717540822995e-05, 3.056145897326778e-05, 2.863622397939466e-05, 2.60010243139815e-05, 2.3660574862962436e-05, 2.130423372038618e-05, 1.9596160293274764e-05, 3.375970864351692e-05, 3.0437717192384996e-05, 2.834481984213915e-05, 2.559184841276901e-05, 2.303267719755854e-05, 2.079187793931994e-05, 1.9321771559648566e-05, 1.7832846652941635e-05, 1.6064720647411737e-05, 1.6397435004228326e-05, 1.9097510062512365e-05, 1.7262500941283387e-05, 1.887394139998823e-05, 1.7674305035803085e-05, 1.622068341360405e-05, 1.4679521903014542e-05, 1.3243849735592257e-05, 1.1958795930605622e-05, 1.0784824047258881e-05, 1.0993057340159184e-05, 2.8266597809008293e-05, 2.5590233291295923e-05, 2.3706724788873226e-05, 2.4096341402053954e-05, 2.350593154020859e-05, 2.206635407678011e-05, 2.0222937778935297e-05, 1.8304767930148788e-05, 1.653870205607957e-05, 1.5296569843571487e-05, 1.4235146606176551e-05, 2.903470198259587e-05, 3.052523656511915e-05, 2.7484703351099134e-05, 2.477966944152582e-05, 2.2302736370492632e-05, 2.0137407240658754e-05, 1.812375927944607e-05], "accuracy_test": 0.09998804209183673, "start": "2016-01-29 15:55:04.147000", "learning_rate_per_epoch": [0.004171296488493681, 0.004117431119084358, 0.004064260981976986, 0.0040117776952683926, 0.003959971945732832, 0.003908835351467133, 0.0038583590649068356, 0.0038085347041487694, 0.003759353654459119, 0.003710807766765356, 0.0036628886591643095, 0.003615588415414095, 0.0035688988864421844, 0.0035228123888373375, 0.00347732100635767, 0.003432417055591941, 0.00338809285312891, 0.003344341181218624, 0.0033011543564498425, 0.0032585253939032555, 0.0032164468429982662, 0.003174911718815565, 0.003133912803605199, 0.0030934433452785015, 0.003053496591746807, 0.003014065558090806, 0.002975143725052476, 0.0029367245733737946, 0.0028988015837967396, 0.0028613682370632887, 0.0028244182467460632, 0.002787945559248328, 0.0027519436553120613, 0.002716406714171171, 0.002681328682228923, 0.002646703738719225, 0.0026125258300453424, 0.0025787893682718277, 0.0025454885326325893, 0.0025126177351921797, 0.002480171388015151, 0.0024481439031660557, 0.0024165301583707333, 0.002385324565693736, 0.0023545220028609037, 0.002324117114767432, 0.0022941050119698048, 0.0022644803393632174, 0.0022352382075041533, 0.0022063737269490957, 0.002177882008254528, 0.0021497581619769335, 0.002121997531503439, 0.0020945954602211714, 0.002067547058686614, 0.002040848135948181, 0.002014493802562356, 0.001988479867577553, 0.001962801907211542, 0.0019374554976820946, 0.0019124364480376244, 0.0018877404509112239, 0.0018633633153513074, 0.001839300966821611, 0.0018155493307858706, 0.0017921044491231441, 0.0017689623637124896, 0.0017461191164329648, 0.0017235708655789495, 0.0017013137694448233, 0.0016793441027402878, 0.0016576581401750445, 0.001636252156458795, 0.0016151226591318846, 0.0015942659229040146, 0.0015736785717308521, 0.0015533571131527424, 0.0015332980547100306, 0.0015134980203583837, 0.0014939536340534687, 0.0014746616361662745, 0.00145561876706779, 0.001436821767129004, 0.0014182674931362271, 0.001399952918291092, 0.0013818747829645872, 0.0013640300603583455, 0.0013464158400893211, 0.0013290290953591466, 0.0013118667993694544, 0.0012949261581525207, 0.0012782042613252997, 0.0012616983149200678, 0.001245405524969101, 0.0012293230975046754, 0.001213448354974389, 0.00119777861982584, 0.0011823112145066261, 0.0011670435778796673, 0.0011519731488078833, 0.001137097249738872, 0.0011224134359508753, 0.0011079192627221346, 0.0010936122853308916, 0.001079490059055388, 0.0010655501391738653, 0.001051790313795209, 0.0010382081381976604, 0.0010248014004901052, 0.0010115677723661065, 0.0009985050419345498, 0.0009856109973043203, 0.0009728834265843034, 0.0009603202342987061, 0.0009479192667640746, 0.000935678428504616, 0.0009235956822521985, 0.000911668932531029, 0.0008998962002806365, 0.0008882755064405501, 0.0008768048719502985, 0.0008654823759570718, 0.0008543060976080596, 0.000843274116050452, 0.0008323845686390996, 0.0008216356509365141, 0.0008110255585052073, 0.0008005524869076908, 0.0007902146317064762, 0.0007800103048793972, 0.0007699377601966262, 0.0007599952514283359, 0.0007501811487600207, 0.0007404937641695142, 0.0007309314678423107, 0.0007214926881715655, 0.0007121757953427732, 0.000702979217749089, 0.0006939013837836683, 0.0006849407800473273, 0.0006760958931408823, 0.0006673652096651495, 0.000658747274428606, 0.0006502406322397292, 0.000641843827906996, 0.0006335554644465446, 0.0006253741448745131, 0.0006172984722070396, 0.0006093270494602621, 0.0006014585960656404, 0.0005936917150393128, 0.0005860251258127391, 0.0005784575478173792, 0.000570987700484693, 0.0005636143032461405, 0.0005563361337408423, 0.0005491519696079195, 0.0005420605302788317, 0.0005350607098080218, 0.0005281512858346105, 0.0005213310942053795, 0.0005145989707671106, 0.0005079537513665855, 0.0005013943300582469, 0.0004949196591041982, 0.0004885285743512213, 0.0004822200280614197, 0.0004759929433930665, 0.0004698462726082653, 0.00046377896796911955, 0.00045779001084156334, 0.0004518783825915307, 0.00044604309368878603, 0.00044028315460309386, 0.0004345976049080491, 0.0004289854841772467, 0.00042344583198428154, 0.00041797771700657904, 0.0004125802079215646, 0.000407252402510494, 0.0004019933985546231, 0.0003968022938352078, 0.00039167824434116483, 0.00038662034785375, 0.00038162776036188006, 0.00037669966695830226, 0.0003718351945281029, 0.0003670335572678596, 0.0003622939111664891, 0.0003576154704205692, 0.0003529974492266774, 0.0003484390617813915, 0.0003439395222812891, 0.0003394981031306088, 0.0003351140476297587, 0.000330786599079147, 0.0003265150298830122, 0.0003222986124455929, 0.00031813664827495813, 0.000314028438879177, 0.000309973256662488, 0.0003059704613406211, 0.0003020193544216454, 0.00029811926651746035, 0.00029426952823996544, 0.00029046949930489063, 0.00028671856853179634, 0.00028301606653258204, 0.00027936138212680817, 0.00027575387503020465, 0.00027219296316616237, 0.00026867803535424173, 0.00026520848041400313, 0.0002617837453726679, 0.00025840321904979646, 0.0002550663484726101, 0.0002517725806683302], "accuracy_train_first": 0.56802826659976, "accuracy_train_last": 0.9993024553571429, "batch_size_eval": 1024, "accuracy_train_std": [0.023599917722574432, 0.02341244008123921, 0.02216098637746368, 0.018960368878709985, 0.022179290623452433, 0.022876884862467677, 0.023327319273199185, 0.02456679970142872, 0.02473721850869738, 0.0264030046431803, 0.02743992196822656, 0.025709250120574043, 0.027092882545296366, 0.029050864153663526, 0.02971632121513081, 0.029242270993526057, 0.02776529960245163, 0.0306520217019186, 0.03252330202601455, 0.030510348830252144, 0.029517696733980716, 0.029586190671254026, 0.03219149024882466, 0.030048439037191385, 0.02955625336268196, 0.03185917067892977, 0.032673995356975655, 0.03198641003170816, 0.031128065909951347, 0.029849874678544358, 0.03024216775199163, 0.029290189507830258, 0.029213815832243327, 0.02808150350424429, 0.027820980256984363, 0.02496115457339486, 0.026986077388982305, 0.026591475853751694, 0.026964478477244855, 0.025256323530427064, 0.024571455427920944, 0.027324888121743326, 0.025844757780819867, 0.022881405028001595, 0.026595341979315408, 0.024798428348866114, 0.025224736658278537, 0.025728058563754156, 0.026859665622843643, 0.02561316334751707, 0.022866641642160245, 0.024546208917985105, 0.022613815313225934, 0.020855925919968404, 0.022775148228591766, 0.020890395499705738, 0.02160577869189055, 0.020378628671123465, 0.019526182262060012, 0.023534759012732522, 0.02021566720081739, 0.02182569215306651, 0.02128549126244945, 0.021358238752260214, 0.01910579125836753, 0.0190745912625666, 0.02101323719938999, 0.018447355673863353, 0.01756752936389694, 0.019254733339680762, 0.018600381159983453, 0.017642776256219628, 0.02008097490761379, 0.01652964006721454, 0.015611270575393575, 0.018113945436420528, 0.014980562716816892, 0.0162016617145542, 0.01465579909273242, 0.01574686335693222, 0.016805216485578013, 0.015323813416822395, 0.015774930900708282, 0.015531114852738219, 0.014171663679389355, 0.013636836504978555, 0.01621406343080339, 0.01259921226877614, 0.01341772933750519, 0.014326554602173823, 0.013988904684345148, 0.012528841649142832, 0.013466624488628263, 0.014005918794845293, 0.01347068716762082, 0.01125288663313785, 0.011068493327465647, 0.011645976014664371, 0.01114870540219353, 0.012974101568173907, 0.012760533864506903, 0.01244551767978004, 0.011843700597766934, 0.010676228568802742, 0.011060378650978094, 0.01156842614637426, 0.009931503390730343, 0.012006267659154569, 0.010297166023529068, 0.010705077065914012, 0.009981848908209714, 0.010006941023814124, 0.00872174005099864, 0.00853286943827862, 0.01034116304629602, 0.009783937576520399, 0.009818931811780042, 0.009047802427559665, 0.00864629389248713, 0.0076000672837983115, 0.010200863196597453, 0.007395422417650745, 0.009135237950164845, 0.008050535811949085, 0.009360262778701736, 0.007729896343623276, 0.010116210050324507, 0.007694153428066372, 0.007834339432211923, 0.00931488443822985, 0.007711899996060182, 0.006160434919698468, 0.00641856824434086, 0.009466643715184051, 0.006516892343010899, 0.008397001632712472, 0.008235919740447638, 0.00700929687925021, 0.0065732456309069084, 0.005774818801694043, 0.007479540901879973, 0.005083096511138285, 0.004827686986521511, 0.009020048491451046, 0.0046764320261460665, 0.00438511617596114, 0.004875978299168385, 0.004264957852367068, 0.005934354870529448, 0.006658703689255081, 0.003979052304842011, 0.006226544480483151, 0.005737566823289008, 0.004982441918915552, 0.006376624723095115, 0.005125075068874363, 0.006682204221237373, 0.005037045760226921, 0.0034778542683599426, 0.004721821016928461, 0.005607466471123609, 0.003706333880105779, 0.0023725634335610302, 0.001621818697738581, 0.0007949833053050018, 0.0006081053213644912, 0.0006103238815293289, 0.0006806771376629673, 0.0009358543432616307, 0.0014318084373456709, 0.003925922706398237, 0.0018232784523916803, 0.0010524990976335821, 0.0009765625, 0.0007990532123065282, 0.0008111404177135607, 0.000974622949922558, 0.0011438219750975487, 0.0014909988413208146, 0.010609326104708074, 0.0031550992800724255, 0.0014423421148875393, 0.0009995435853889824, 0.0008539977526137908, 0.00079225841877794, 0.000766946730955666, 0.0008860006921157761, 0.0009518921824060044, 0.001187648138869246, 0.007211836412822748, 0.002985846797380023, 0.001460965185849282, 0.0011259579625003022, 0.0008200889178082402, 0.0008932930018740023, 0.0008932930018740023, 0.0008318705273436718, 0.0008932930018740023, 0.0015689155885265382, 0.011770517128778366, 0.003615688128624154, 0.0019523075843297882, 0.0009952071502526366, 0.0008318705273436718, 0.0008318705273436718, 0.0008737115991469464, 0.0010272843207949462, 0.0010776255790789124, 0.001135520409884187, 0.0015402246244121274, 0.007276715173304989, 0.0027399192766320224, 0.0011902447220911933, 0.000884308678559734, 0.0007326986261434492, 0.0007278867114257128, 0.0006834513791247706], "accuracy_test_std": 0.007350288587607382, "error_valid": [0.4536118105233433, 0.3734086737575302, 0.3460943382906627, 0.31661362245858427, 0.2915671474962349, 0.2746081984186747, 0.27440523814006024, 0.2737139966114458, 0.2633880247552711, 0.2579154508659638, 0.25790515577936746, 0.25741687452936746, 0.27286979951054224, 0.26087455289909633, 0.2598171004329819, 0.2636012801204819, 0.2847915097891567, 0.2541812758847892, 0.2566932770143072, 0.24583931428840367, 0.2662868269954819, 0.2518619399472892, 0.2552299039909638, 0.2567859327936747, 0.24774243458207834, 0.25543286426957834, 0.2539783156061747, 0.24196394954819278, 0.24507747788027112, 0.2554740446159638, 0.25243258189006024, 0.253063523625753, 0.25162809440888556, 0.24549369352409633, 0.23915633236069278, 0.25121040803840367, 0.25245170133659633, 0.2522178557981928, 0.2503162062311747, 0.2471217879329819, 0.23236010448042166, 0.23581925357680722, 0.26055981739457834, 0.23215714420180722, 0.25349150508283136, 0.23604427475527112, 0.24566723926957834, 0.24213749529367468, 0.24933964373117468, 0.2509059676204819, 0.24862781202936746, 0.25167956984186746, 0.2409476774284638, 0.2513442441641567, 0.2544577724962349, 0.24274784685617468, 0.24320524284638556, 0.23466032097138556, 0.23510889260165668, 0.2517619305346386, 0.24224927051957834, 0.2380885848079819, 0.2363090055534638, 0.2426566618034638, 0.22943188770707834, 0.22980839373117468, 0.24913668345256024, 0.22780379329819278, 0.2402152555534638, 0.23328666227409633, 0.24247282097138556, 0.23586043392319278, 0.23633842008659633, 0.23090702654367468, 0.22892301628388556, 0.22838325960090367, 0.22927893213478923, 0.23127323748117468, 0.23322636248117468, 0.23213802475527112, 0.23268660579819278, 0.23317488704819278, 0.23195418392319278, 0.23438529508659633, 0.22956425310617468, 0.22520943147590367, 0.22811852880271077, 0.2242431640625, 0.22805822900978923, 0.22898331607680722, 0.23414115446159633, 0.21888089467243976, 0.22287980045180722, 0.2252903214420181, 0.22958337255271077, 0.23431470020707834, 0.22373429263930722, 0.2230930558170181, 0.2204781038215362, 0.2231636506965362, 0.22877006071159633, 0.23672522119728923, 0.22320483104292166, 0.22838325960090367, 0.22344897166792166, 0.23276749576430722, 0.22930981739457834, 0.23106880647590367, 0.22360192724021077, 0.22318424086972888, 0.22390636765813254, 0.23030549934111444, 0.2265625, 0.22835237434111444, 0.22576830760542166, 0.23251306005271077, 0.22325483574924698, 0.22311364599021077, 0.2274875870670181, 0.2266330948795181, 0.23227921451430722, 0.21924710560993976, 0.22140466161521077, 0.22093697053840367, 0.22852592008659633, 0.21847497411521077, 0.22669339467243976, 0.21877941453313254, 0.21617475762424698, 0.22505647590361444, 0.21983686699924698, 0.21720279555722888, 0.2131024096385542, 0.21534085560993976, 0.21628653285015065, 0.22002070783132532, 0.21914562547063254, 0.2186470491340362, 0.21904414533132532, 0.21316418015813254, 0.21891177993222888, 0.20796516142695776, 0.21272590361445776, 0.21978686229292166, 0.21695865493222888, 0.21125076477786142, 0.21619534779743976, 0.21580854668674698, 0.2121258471385542, 0.21806758283132532, 0.2089211337537651, 0.22122082078313254, 0.2127053134412651, 0.21398778708584332, 0.2204781038215362, 0.20945059535015065, 0.22308276073042166, 0.21911474021084332, 0.20839167215737953, 0.21076248352786142, 0.21809846809111444, 0.21582913685993976, 0.21056981833584332, 0.20849462302334332, 0.20568700583584332, 0.20336766989834332, 0.20299116387424698, 0.20458837302334332, 0.20405891142695776, 0.20539138977786142, 0.21106839467243976, 0.20396772637424698, 0.20542227503765065, 0.20481192347515065, 0.20493399378765065, 0.20653120293674698, 0.2062664721385542, 0.20746658509036142, 0.2099285815135542, 0.22136348126882532, 0.21009183217243976, 0.2061444018260542, 0.20811811699924698, 0.20896231410015065, 0.2098065112010542, 0.21165815606174698, 0.21153608574924698, 0.20994917168674698, 0.21422163262424698, 0.21698806946536142, 0.20982710137424698, 0.2045574877635542, 0.20727391989834332, 0.20789456654743976, 0.20862698842243976, 0.2088814241340362, 0.2087593538215362, 0.20984769154743976, 0.21318477033132532, 0.2240696183170181, 0.20991828642695776, 0.2083416674510542, 0.20526931946536142, 0.20575760071536142, 0.20662238798945776, 0.20747688017695776, 0.20820930205195776, 0.20833137236445776, 0.21123164533132532, 0.21158756118222888, 0.22296069041792166, 0.21786462255271077, 0.21121105515813254, 0.21084484422063254, 0.21157726609563254, 0.21233027814382532, 0.21159785626882532], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.012913384205386402, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 64, "valid_ratio": 0.15, "learning_rate": 0.004225866829758497, "optimization": "adam", "nb_data_augmentation": 0, "learning_rate_decay_method": "exp", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 4.8698879958074276e-05, "rotation_range": [0, 0], "momentum": 0.8335665793660967}, "accuracy_valid_max": 0.797008836125753, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.7884021437311747, "accuracy_valid_std": [0.009628846297727119, 0.009046586353820142, 0.009884175595703395, 0.01239626415515524, 0.011643161657376971, 0.011689770681349413, 0.009659477852919627, 0.016641337044742785, 0.013915044665945954, 0.01452474962570635, 0.013651576608125325, 0.012667612360858803, 0.013246236290656235, 0.009274199853827419, 0.009254008350756803, 0.00900332492386088, 0.010517380110491687, 0.006904991170393581, 0.010536263493643066, 0.011237341754701074, 0.013401786498293593, 0.010566165011128628, 0.01662262191451884, 0.010917523651879554, 0.019404863092265755, 0.009988154639126738, 0.012498365533608008, 0.011452938956147875, 0.016881311864416357, 0.016384537890033527, 0.015999875975811442, 0.016587164750826035, 0.01693911832221606, 0.014693807376878764, 0.013050347525956381, 0.01252621329802009, 0.01627555950187263, 0.015304287459798576, 0.013480033806951083, 0.014967776954338666, 0.010908601200488654, 0.010715284053422657, 0.012905142278150442, 0.011847198289518204, 0.02339672403523397, 0.016758992075582336, 0.013447673127892662, 0.018578984512658814, 0.015653529571170784, 0.010669494155843446, 0.01645648777433961, 0.016540055103176084, 0.019442280980627585, 0.019563978805798705, 0.0226684541395297, 0.014591178631636689, 0.013632226865360553, 0.013777504507587881, 0.021920400000694455, 0.02509132625255535, 0.0182519809021794, 0.017126904291095656, 0.019098991985077043, 0.017807079709913874, 0.018002604025924494, 0.020294827744984496, 0.0158816709988529, 0.01599903155145373, 0.019903610209818855, 0.01859408018316914, 0.012922974103815456, 0.01413040954426583, 0.012123118291770598, 0.01997824328389758, 0.0164886632684891, 0.013503670758862977, 0.017803404115685945, 0.022885052669171135, 0.018989582403787866, 0.018738342600117062, 0.015174226395782394, 0.013634191703454956, 0.013030295224972859, 0.012183625231140747, 0.01976150687595691, 0.01387142977146947, 0.01671307241319887, 0.014368562500847441, 0.017290933873153353, 0.013692767845709362, 0.013622754654096576, 0.015989150867690403, 0.01271495787567708, 0.0113729236402077, 0.016576130771560872, 0.021599575723724604, 0.01393038990898499, 0.014048647621567389, 0.011437077736593007, 0.010466346349983554, 0.014392626870568059, 0.019109770552845542, 0.012801031706955436, 0.011100509733637617, 0.012890366583245648, 0.015152375848938334, 0.01793876117301423, 0.012501903272199138, 0.013710827860052151, 0.008511507626305058, 0.01519546781717904, 0.01393738358850233, 0.014705293307007419, 0.01283605528717812, 0.01365832164919353, 0.017868825797680576, 0.013992120159423362, 0.017064345401229017, 0.00788395518022995, 0.013661170703588502, 0.014096069675013272, 0.010028464377517598, 0.015251693761513597, 0.011559970555826736, 0.01508237113326542, 0.012899273152576577, 0.010062715546361976, 0.013905211491224409, 0.015459786615544051, 0.014113740371614778, 0.008860940388203127, 0.012786905912206693, 0.012098812141863974, 0.011770624119404936, 0.01415637100591045, 0.014627326515275037, 0.013118658775459376, 0.016199052889455282, 0.015307577393243602, 0.013759873056907776, 0.01301725433167104, 0.013949382583078951, 0.01058895738850939, 0.013776407599298926, 0.012083070251654579, 0.01474890089792157, 0.013896599041496621, 0.010317487864779701, 0.014166068473406603, 0.010268161461441813, 0.01519058871376824, 0.018678827937505568, 0.013873525063389411, 0.010095019346693079, 0.017391610739261515, 0.012258537562027006, 0.014756314990788486, 0.01018394008459524, 0.019013385790161166, 0.013842324827505317, 0.01264855287999215, 0.015942772202022188, 0.010718823367125629, 0.01640605961605088, 0.01620061456471635, 0.01497536022659522, 0.015047133081430903, 0.015017213526411109, 0.01632461002317476, 0.015454582626709502, 0.011408077616640769, 0.011479852706850455, 0.010912593182799375, 0.012306014116606721, 0.0114812255490302, 0.012970006762457357, 0.013811646575011265, 0.014816492469044436, 0.013285881030447377, 0.01456709183164339, 0.011554191018532164, 0.011013599292590189, 0.012247632245652305, 0.01199220153584719, 0.012541895333863158, 0.012833777216117715, 0.012937166342504827, 0.013705904905133116, 0.014626178283628001, 0.014333761754002545, 0.01793616703088718, 0.016906791743816218, 0.017108449262584945, 0.017277840309382623, 0.017397791945597547, 0.01618849855509225, 0.016648334138622354, 0.016558097732896522, 0.019551230145112553, 0.013586406245896281, 0.013869668143548416, 0.014209427476787412, 0.01473398804382106, 0.01550829228707781, 0.015675325398179927, 0.01573883623875149, 0.015949594317037723, 0.017056456778118723, 0.018471586745233085, 0.018462326672021214, 0.017490961223457585, 0.013223543744901698, 0.014604480989491606, 0.016503506010273974, 0.015477082034726895, 0.015362023088257822, 0.014738722288471078], "accuracy_valid": [0.5463881894766567, 0.6265913262424698, 0.6539056617093373, 0.6833863775414157, 0.7084328525037651, 0.7253918015813253, 0.7255947618599398, 0.7262860033885542, 0.7366119752447289, 0.7420845491340362, 0.7420948442206325, 0.7425831254706325, 0.7271302004894578, 0.7391254471009037, 0.7401828995670181, 0.7363987198795181, 0.7152084902108433, 0.7458187241152108, 0.7433067229856928, 0.7541606857115963, 0.7337131730045181, 0.7481380600527108, 0.7447700960090362, 0.7432140672063253, 0.7522575654179217, 0.7445671357304217, 0.7460216843938253, 0.7580360504518072, 0.7549225221197289, 0.7445259553840362, 0.7475674181099398, 0.746936476374247, 0.7483719055911144, 0.7545063064759037, 0.7608436676393072, 0.7487895919615963, 0.7475482986634037, 0.7477821442018072, 0.7496837937688253, 0.7528782120670181, 0.7676398955195783, 0.7641807464231928, 0.7394401826054217, 0.7678428557981928, 0.7465084949171686, 0.7639557252447289, 0.7543327607304217, 0.7578625047063253, 0.7506603562688253, 0.7490940323795181, 0.7513721879706325, 0.7483204301581325, 0.7590523225715362, 0.7486557558358433, 0.7455422275037651, 0.7572521531438253, 0.7567947571536144, 0.7653396790286144, 0.7648911073983433, 0.7482380694653614, 0.7577507294804217, 0.7619114151920181, 0.7636909944465362, 0.7573433381965362, 0.7705681122929217, 0.7701916062688253, 0.7508633165474398, 0.7721962067018072, 0.7597847444465362, 0.7667133377259037, 0.7575271790286144, 0.7641395660768072, 0.7636615799134037, 0.7690929734563253, 0.7710769837161144, 0.7716167403990963, 0.7707210678652108, 0.7687267625188253, 0.7667736375188253, 0.7678619752447289, 0.7673133942018072, 0.7668251129518072, 0.7680458160768072, 0.7656147049134037, 0.7704357468938253, 0.7747905685240963, 0.7718814711972892, 0.7757568359375, 0.7719417709902108, 0.7710166839231928, 0.7658588455384037, 0.7811191053275602, 0.7771201995481928, 0.7747096785579819, 0.7704166274472892, 0.7656852997929217, 0.7762657073606928, 0.7769069441829819, 0.7795218961784638, 0.7768363493034638, 0.7712299392884037, 0.7632747788027108, 0.7767951689570783, 0.7716167403990963, 0.7765510283320783, 0.7672325042356928, 0.7706901826054217, 0.7689311935240963, 0.7763980727597892, 0.7768157591302711, 0.7760936323418675, 0.7696945006588856, 0.7734375, 0.7716476256588856, 0.7742316923945783, 0.7674869399472892, 0.776745164250753, 0.7768863540097892, 0.7725124129329819, 0.7733669051204819, 0.7677207854856928, 0.7807528943900602, 0.7785953383847892, 0.7790630294615963, 0.7714740799134037, 0.7815250258847892, 0.7733066053275602, 0.7812205854668675, 0.783825242375753, 0.7749435240963856, 0.780163133000753, 0.7827972044427711, 0.7868975903614458, 0.7846591443900602, 0.7837134671498494, 0.7799792921686747, 0.7808543745293675, 0.7813529508659638, 0.7809558546686747, 0.7868358198418675, 0.7810882200677711, 0.7920348385730422, 0.7872740963855422, 0.7802131377070783, 0.7830413450677711, 0.7887492352221386, 0.7838046522025602, 0.784191453313253, 0.7878741528614458, 0.7819324171686747, 0.7910788662462349, 0.7787791792168675, 0.7872946865587349, 0.7860122129141567, 0.7795218961784638, 0.7905494046498494, 0.7769172392695783, 0.7808852597891567, 0.7916083278426205, 0.7892375164721386, 0.7819015319088856, 0.7841708631400602, 0.7894301816641567, 0.7915053769766567, 0.7943129941641567, 0.7966323301016567, 0.797008836125753, 0.7954116269766567, 0.7959410885730422, 0.7946086102221386, 0.7889316053275602, 0.796032273625753, 0.7945777249623494, 0.7951880765248494, 0.7950660062123494, 0.793468797063253, 0.7937335278614458, 0.7925334149096386, 0.7900714184864458, 0.7786365187311747, 0.7899081678275602, 0.7938555981739458, 0.791881883000753, 0.7910376858998494, 0.7901934887989458, 0.788341843938253, 0.788463914250753, 0.790050828313253, 0.785778367375753, 0.7830119305346386, 0.790172898625753, 0.7954425122364458, 0.7927260801016567, 0.7921054334525602, 0.7913730115775602, 0.7911185758659638, 0.7912406461784638, 0.7901523084525602, 0.7868152296686747, 0.7759303816829819, 0.7900817135730422, 0.7916583325489458, 0.7947306805346386, 0.7942423992846386, 0.7933776120105422, 0.7925231198230422, 0.7917906979480422, 0.7916686276355422, 0.7887683546686747, 0.7884124388177711, 0.7770393095820783, 0.7821353774472892, 0.7887889448418675, 0.7891551557793675, 0.7884227339043675, 0.7876697218561747, 0.7884021437311747], "seed": 758401436, "model": "residualv3", "loss_std": [0.2774064838886261, 0.1462101936340332, 0.1393669694662094, 0.13524843752384186, 0.13403159379959106, 0.1320880800485611, 0.1292189508676529, 0.1275745928287506, 0.12437241524457932, 0.12302251905202866, 0.12293769419193268, 0.1207984909415245, 0.11772380769252777, 0.11379711329936981, 0.11230835318565369, 0.11400235444307327, 0.10873731970787048, 0.10731200128793716, 0.10493025928735733, 0.1033315509557724, 0.1029621809720993, 0.10069247335195541, 0.09834355860948563, 0.09400268644094467, 0.09614579379558563, 0.09461439400911331, 0.09293107688426971, 0.0918274000287056, 0.08918969333171844, 0.08623099327087402, 0.08217049390077591, 0.08471442013978958, 0.08300019055604935, 0.08089625090360641, 0.08085577934980392, 0.07860326021909714, 0.07633144408464432, 0.07440309971570969, 0.07642918080091476, 0.0758826732635498, 0.07393395900726318, 0.06761696934700012, 0.07072202861309052, 0.06698418408632278, 0.07390402257442474, 0.06694190949201584, 0.06439101696014404, 0.06470713019371033, 0.05969491973519325, 0.06580248475074768, 0.061041854321956635, 0.06605906784534454, 0.061962272971868515, 0.058445315808057785, 0.060070015490055084, 0.05434536188840866, 0.058810897171497345, 0.054320428520441055, 0.05367866903543472, 0.05410067364573479, 0.05405574291944504, 0.053156156092882156, 0.05403364449739456, 0.0525505468249321, 0.0520680733025074, 0.05334709957242012, 0.049006231129169464, 0.05123613402247429, 0.05444266274571419, 0.047902196645736694, 0.04960862174630165, 0.052330464124679565, 0.04506859928369522, 0.047571662813425064, 0.04925777018070221, 0.04365513101220131, 0.044626545161008835, 0.04319310560822487, 0.044963352382183075, 0.04511560499668121, 0.04035545140504837, 0.04441258683800697, 0.04577203840017319, 0.039991993457078934, 0.04385486617684364, 0.03926302120089531, 0.042522579431533813, 0.03792119771242142, 0.038717709481716156, 0.034542519599199295, 0.04193580523133278, 0.04625675082206726, 0.033553775399923325, 0.03353748470544815, 0.0413464680314064, 0.037764064967632294, 0.038525015115737915, 0.03914336487650871, 0.031227441504597664, 0.03621876612305641, 0.02816576324403286, 0.033721067011356354, 0.03425092622637749, 0.03342379629611969, 0.03499387577176094, 0.03657792881131172, 0.033281076699495316, 0.02813378907740116, 0.02866636961698532, 0.030587108805775642, 0.031032241880893707, 0.03240647539496422, 0.03187866508960724, 0.027492117136716843, 0.028420191258192062, 0.029469894245266914, 0.0333305224776268, 0.021497607231140137, 0.025182189419865608, 0.028415622189641, 0.03201453760266304, 0.027036413550376892, 0.0220810454338789, 0.03189075365662575, 0.02587326616048813, 0.024048523977398872, 0.026591241359710693, 0.023477725684642792, 0.02432958595454693, 0.022085832431912422, 0.025975672528147697, 0.02423718571662903, 0.02010522224009037, 0.023536337539553642, 0.026041682809591293, 0.019168170168995857, 0.02379533089697361, 0.029078079387545586, 0.017337223514914513, 0.019320949912071228, 0.020770348608493805, 0.022079097107052803, 0.012770437635481358, 0.023835795000195503, 0.022367151454091072, 0.02037809044122696, 0.011587756685912609, 0.0202762633562088, 0.022840779274702072, 0.014189032837748528, 0.01662846840918064, 0.016476664692163467, 0.020176678895950317, 0.016564898192882538, 0.012914509512484074, 0.02145419269800186, 0.0168489720672369, 0.014746202155947685, 0.00887987855821848, 0.006578564178198576, 0.039563972502946854, 0.013767756521701813, 0.006444531958550215, 0.0036889328621327877, 0.0016533170128241181, 0.0014281381154432893, 0.0015741067472845316, 0.0019545278046280146, 0.0023792521096765995, 0.002685925457626581, 0.1455497294664383, 0.008489021100103855, 0.0017362991347908974, 0.000815271632745862, 0.0006732958136126399, 0.0007420487818308175, 0.0009040180593729019, 0.001124256756156683, 0.001369205303490162, 0.18413889408111572, 0.02453562058508396, 0.004586260300129652, 0.001198215875774622, 0.0006460160366259515, 0.000548045092727989, 0.0005651336396113038, 0.0006540328613482416, 0.0007914765155874193, 0.000928117020521313, 0.17575453221797943, 0.01475704275071621, 0.004099720157682896, 0.00094168062787503, 0.000500813010148704, 0.0004111136368010193, 0.0004071740841027349, 0.00045907776802778244, 0.0005515113589353859, 0.0006569084362126887, 0.17098766565322876, 0.036094117909669876, 0.0057978336699306965, 0.0014476324431598186, 0.0006229814607650042, 0.000431343971285969, 0.000365560787031427, 0.00035260184085927904, 0.00038205916644074023, 0.00044509756844490767, 0.0005156192928552628, 0.15616565942764282, 0.01786552183330059, 0.0030175186693668365, 0.0009068477083928883, 0.0004879852931480855, 0.0003663297975435853, 0.00031026083161123097]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:24 2016", "state": "available"}], "summary": "6f3d33eacb58eda1d7cda2080d59cb57"}