{"content": {"hp_model": {"f0": 16, "f1": 32, "f2": 64, "f3": 64, "nonlin": "very_leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.0061694118855560795, 0.012709696668898181, 0.012646792117575929, 0.011461898796997797, 0.012295267724463764, 0.013063488922998153, 0.0132460037573153, 0.013331954050600725, 0.013133612430348746, 0.009410578373227204, 0.008535511510212025, 0.00979332000115618, 0.00870451221118491, 0.009579349202045707, 0.012374516146081333, 0.013114461515314117, 0.013781795136872952, 0.016813760871823423, 0.014825123639503644, 0.01569990992923032, 0.01286280785048509, 0.013325682147913543, 0.01556656334489592, 0.01273524521506818, 0.012528787445666812, 0.013079389713302813, 0.013123274672628542, 0.014247228366242272, 0.013035145772859835, 0.013898642683086593, 0.013140527459654681, 0.012464774138916333, 0.014003726882897407, 0.014417604078028856, 0.013222074435521982, 0.014937069411962644, 0.014754219499705073, 0.014388607086191729, 0.014548033192180034, 0.014649977547460922, 0.014438906646039936, 0.014483642936362553, 0.014049244387404196, 0.014802987345314286, 0.013883439027802978, 0.014525436059775411, 0.013148496929645289, 0.014205797729436009, 0.013990919951233312, 0.013158372027095135, 0.013167976487096575, 0.013733685209295804, 0.012898397211215978, 0.01255112846615203, 0.012846048911697452, 0.013131937625088165, 0.013496331174629539, 0.013183571354377609, 0.013213874500298003, 0.013121491150580797, 0.012741128697758154, 0.012779288384625805, 0.013670980131001181, 0.013396092014303697, 0.013219690918186101, 0.012221430916675024, 0.013233797864305157, 0.012455902717018845, 0.013059328921024784, 0.013118754406945028, 0.011680627771894528, 0.013412934220112733, 0.013182301386509747, 0.012146968473434007, 0.01272692706838244, 0.013724272384177639, 0.013340654577158495, 0.012916347117966983, 0.012520827261096169, 0.012946321825411644, 0.012905747002539825, 0.01363957507359354, 0.012747055196187237, 0.012677285599812273, 0.012571658592217061, 0.012213304856422948, 0.012551242898089936, 0.01349268585586225, 0.01281756017601365, 0.012640127458665397, 0.013371409920625793, 0.013416700761138437, 0.013186948151828146, 0.013070326448965172, 0.011420788529401513, 0.012269051060679343, 0.012372719007880104, 0.01289597533921487, 0.013575691280290696, 0.013719350390848054, 0.012591671342391052, 0.012566677254832976, 0.013127978332348262, 0.012961050002533553, 0.013078778132973666, 0.01238287206328018, 0.014523361841212211, 0.012436046828778202, 0.01283883544380659, 0.01255374497424517, 0.011499735400956734, 0.013004205683363657, 0.01249892010996002, 0.013137055746007436, 0.013076972911155416, 0.011843885132345288, 0.013261653011301822, 0.012414778223365055, 0.013068863498911822, 0.01356327534236283, 0.013034116662553446, 0.012673435759664654, 0.012730858598197274, 0.01292507311942219, 0.013619703888079145, 0.014090315980428751, 0.012851076472793925, 0.012848079516206002, 0.013392942376951905, 0.01315650867517623, 0.012459130903071517, 0.012622901269186158, 0.01290643985313698, 0.012993696136448445, 0.01304325939909787, 0.011227753374805078, 0.012307168260884061, 0.013625602255509045, 0.012074210382809675, 0.011182113024153737, 0.012588663403823051, 0.013766751226232722, 0.013609787572803742, 0.012876867515544307, 0.012212525569726249, 0.012689404961916827, 0.012352917472522132, 0.01389297661935357, 0.013724069621696078, 0.01237630979003568, 0.012339816649227361, 0.01379733098678062, 0.012819689021686695, 0.011244745908314114, 0.011860285458657756, 0.012834056656540738, 0.013142426241195387, 0.01366770321754915, 0.012912210487140148, 0.011964056083845834, 0.012217588893981194, 0.011949843603687131, 0.01262512256068535, 0.0135515292055053, 0.01285916965321337, 0.012978197362179762, 0.013075380265237978, 0.013599059536359962, 0.013364553903124462, 0.012734088422903406, 0.01285735454258977, 0.013279734614283771, 0.01161500834480526, 0.012401006350509397, 0.01332277101014863, 0.012765745310511532, 0.012785682401768372, 0.013025962552845898, 0.013120215173676885, 0.012495569259799753, 0.011526131161019281, 0.012939531692791547, 0.012815521191319233, 0.012222608886741535, 0.013229951293705012, 0.01287374408044707, 0.011944983320978906, 0.013953196695651862], "moving_avg_accuracy_train": [0.03456142646617755, 0.07251072272113786, 0.11129831785397054, 0.14968489116501127, 0.18688737006023032, 0.22248992048730068, 0.2561290163227068, 0.28780143676500236, 0.3174948383023726, 0.3449907409443114, 0.37057392664939115, 0.39430313362915487, 0.4163312437216196, 0.43664475195720037, 0.4555196420715578, 0.47272567926021225, 0.4886969606847355, 0.5034198141905976, 0.516909764526798, 0.5293669040186549, 0.5409247046363077, 0.5515406388826714, 0.5612879310067704, 0.5702580955184504, 0.5785079188396675, 0.5860442948739824, 0.5930154424560747, 0.5993615188442344, 0.6053403076090357, 0.6107514444318807, 0.6157307495664888, 0.6202772283543028, 0.6245410842264214, 0.6285179913422622, 0.6320856541001086, 0.6354778761404946, 0.6386865888494425, 0.6415651657410764, 0.6441627882923379, 0.6465844260432536, 0.6488708208154972, 0.6507704659914687, 0.6526754591498432, 0.6544108793316658, 0.6559495060072108, 0.6574343235116484, 0.6588333301370431, 0.6600738349094223, 0.6612089264438584, 0.6622514351641366, 0.6632175587492827, 0.6641637998866283, 0.6650573056376297, 0.6658265475325692, 0.6665235515844526, 0.6672182124489864, 0.6678155414901711, 0.66839495425699, 0.6688490685292884, 0.669332140087443, 0.6696854882326301, 0.6700686778276026, 0.6705111326154404, 0.6708465629066372, 0.6711437998710953, 0.6714508767176882, 0.6717945670486417, 0.6720760586584231, 0.6722875123798363, 0.6724081023136411, 0.6725631722790747, 0.6727399015800986, 0.6728548161724579, 0.6729512278103339, 0.6731146921463179, 0.6732385585606082, 0.6733314731918121, 0.673398820318229, 0.6736104592581669, 0.6737196622422339, 0.6738179088790754, 0.6739086199522236, 0.6739647553787897, 0.6740291560579189, 0.6741429562893824, 0.6741477562965184, 0.674270586794589, 0.6743765199940522, 0.6743742396723879, 0.6743164198602801, 0.6743434010400882, 0.6743490468626204, 0.6745237918683573, 0.6745183380056726, 0.674585581239989, 0.6746088977699214, 0.6746624347301939, 0.6746083114468202, 0.6745758404846317, 0.6746233465293764, 0.6746404892839231, 0.6746838555975481, 0.6747368722214866, 0.6747612996461172, 0.6747786340306656, 0.674745442900578, 0.674757351464433, 0.6747286137397782, 0.6746539216625889, 0.674698233838338, 0.6746870336203403, 0.6747815490717521, 0.6748666490268416, 0.6748665090757078, 0.6749175003446781, 0.6749192146593704, 0.674916179342612, 0.6749715402289489, 0.6750236901754615, 0.6750264833487607, 0.6749940118261308, 0.6749881110414967, 0.6749548985496117, 0.6749459336462009, 0.6749517800771696, 0.6749640173114699, 0.6749679832782742, 0.6749902619853305, 0.6749986510288148, 0.6750038760191412, 0.6750481060401968, 0.6749531625746508, 0.6749211918782784, 0.6749806657598488, 0.674985400177081, 0.6749942754013902, 0.675023225491373, 0.6750190536378337, 0.6749758074887052, 0.6749810277330518, 0.674971702962469, 0.6749750445594483, 0.6749849192967021, 0.67493574993763, 0.674900798109703, 0.6749018574990834, 0.6748377067828591, 0.6749101794715906, 0.6749243237152769, 0.674941631734576, 0.6749618952983831, 0.674973157059381, 0.6749949183883266, 0.6749656394105589, 0.6749718404139013, 0.6749402549847758, 0.6749932083068962, 0.6748896955753668, 0.6748918291693621, 0.6748380179301667, 0.6748476804863103, 0.6748377395475446, 0.6747893733193497, 0.6748434278663369, 0.6747944207086254, 0.6747922029940752, 0.6748273733831136, 0.6748799530725339, 0.6749063124049077, 0.6748765934302439, 0.6749381659589895, 0.6749494034074797, 0.6749270731742436, 0.6748279569536262, 0.674794411731224, 0.6747665822286903, 0.6747671483621337, 0.674756032138185, 0.6748158180497357, 0.6748277005939223, 0.6748407200324998, 0.674889639908172, 0.6749011157129435, 0.6749533687134469, 0.6749933849186526, 0.6749689816831088, 0.6749214421342147, 0.6749553143532868], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 367564898, "moving_var_accuracy_train": [0.010750429794392984, 0.02263672859117439, 0.033913353557753864, 0.04378377929705363, 0.05186162129089124, 0.058083333534010906, 0.062459299098222565, 0.0652416491384622, 0.06665276707835147, 0.06679171232937113, 0.06600303561381939, 0.06447040942743366, 0.06239050719290202, 0.05986520402514488, 0.057085036914092016, 0.05404096266431907, 0.050932602870959055, 0.047790204322058806, 0.04464899273051133, 0.04158071637633643, 0.038624889534759524, 0.03577668312059163, 0.033054102142305176, 0.030472866590374074, 0.028038116194818326, 0.02574547724891186, 0.023608301611522484, 0.021609925620089444, 0.019770646293927538, 0.01805710527997473, 0.016474536068589073, 0.015013116686042553, 0.013675429219522103, 0.012450228409442113, 0.011319759526481428, 0.010291348107174808, 0.009354875831694405, 0.008493964092814398, 0.007705296469802359, 0.006987545787392064, 0.006335839618143739, 0.005734733522480691, 0.0051939211606337, 0.004701634193437626, 0.004252777122514154, 0.0038473415574560993, 0.0034802223775515758, 0.0031460498086090766, 0.002843040722872106, 0.0025685180704716013, 0.0023200668164604183, 0.0020961184854244234, 0.001893691809625632, 0.00170964822649944, 0.0015430557356845723, 0.0013930931455665473, 0.0012569950488608775, 0.0011343170163639652, 0.0010227412926783147, 0.0009225673865831644, 0.000831434342130212, 0.0007496124163084475, 0.0006764130708311277, 0.0006097843850702865, 0.0005496010948806202, 0.0004954896510999794, 0.0004470037933823003, 0.0004030165517814664, 0.00036311731069001493, 0.000326936457010229, 0.0002944592315568223, 0.00026529440761370354, 0.0002388838151241671, 0.00021507909044701185, 0.00019381166670455873, 0.00017456858603140512, 0.00015718942558649033, 0.0001415113037467707, 0.000127763292740176, 0.0001150942910917207, 0.00010367173359740476, 9.337861672678956e-05, 8.406911572915246e-05, 7.569953118348786e-05, 6.824613249926951e-05, 6.142172660995911e-05, 5.541533993026958e-05, 4.9974802521979236e-05, 4.497736906858335e-05, 4.0509720337774585e-05, 3.646530016057165e-05, 3.281905702232308e-05, 2.9811973673360525e-05, 2.6831044007588133e-05, 2.418863447988137e-05, 2.177466397700605e-05, 1.9622993434342452e-05, 1.768705805913663e-05, 1.592784152369198e-05, 1.4355368789908306e-05, 1.2922476777218503e-05, 1.164715483391348e-05, 1.0507736212246597e-05, 9.462332882688686e-06, 8.518803922408892e-06, 7.676838390216433e-06, 6.910430876232605e-06, 6.2268204999743735e-06, 5.654348607530663e-06, 5.106585867054273e-06, 4.597056284297612e-06, 4.217749190868181e-06, 3.861152292987423e-06, 3.47503723996556e-06, 3.1509345015698917e-06, 2.835867501286684e-06, 2.552363669488429e-06, 2.3247107521635455e-06, 2.116716229238631e-06, 1.905114822668481e-06, 1.7240929384387478e-06, 1.5519970179285582e-06, 1.4067249426905975e-06, 1.2667757738600196e-06, 1.1404058232696618e-06, 1.0277129900725986e-06, 9.25083251099573e-07, 8.3704199308252e-07, 7.539711782294998e-07, 6.788197651217422e-07, 6.285444414727803e-07, 6.468183521745854e-07, 5.91335645795964e-07, 5.640363645178815e-07, 5.078344604248475e-07, 4.577599408412175e-07, 4.1952691614720446e-07, 3.777308637900637e-07, 3.5678984214106584e-07, 3.21356116486296e-07, 2.9000306695545865e-07, 2.6110325669326617e-07, 2.358705249464159e-07, 2.3404210529592304e-07, 2.2163256724521474e-07, 1.994794112734266e-07, 2.1656929967492926e-07, 2.421829852150976e-07, 2.1976522335869553e-07, 2.0048480881137363e-07, 1.8413183609370645e-07, 1.6686009783127555e-07, 1.5443608698546544e-07, 1.46707805139005e-07, 1.3238309660717728e-07, 1.281235409438118e-07, 1.4054767576173115e-07, 2.2292687848369483e-07, 2.0067516064535426e-07, 2.0666848975446644e-07, 1.8684192570006158e-07, 1.690471335019576e-07, 1.7319604841992135e-07, 1.8217349002778978e-07, 1.8557145458772196e-07, 1.6705857344938714e-07, 1.614853224904566e-07, 1.7021840389720155e-07, 1.594498931361983e-07, 1.5145386091824475e-07, 1.7042906149160983e-07, 1.5452267757956102e-07, 1.4355816366895735e-07, 2.1761857400739738e-07, 2.059842541207253e-07, 1.9235615961007434e-07, 1.7312342821274704e-07, 1.5692321930537002e-07, 1.7340009435429665e-07, 1.5733083862599373e-07, 1.4312330679125305e-07, 1.503493642341146e-07, 1.3649967466710138e-07, 1.4742309175482266e-07, 1.4709245269092046e-07, 1.3774286856685506e-07, 1.4430866009168249e-07, 1.4020373910631056e-07], "duration": 266336.727154, "accuracy_train": [0.3456142646617756, 0.41405438901578073, 0.46038667404946476, 0.495164050964378, 0.5217096801172019, 0.5429128743309339, 0.5588808788413621, 0.5728532207456626, 0.5847354521387044, 0.5924538647217608, 0.600822597995109, 0.6078659964470284, 0.6145842345538022, 0.6194663260774271, 0.6253936531007752, 0.6275800139581026, 0.6324384935054448, 0.6359254957433554, 0.6383193175526024, 0.6414811594453672, 0.6449449101951827, 0.6470840470999446, 0.6490135601236618, 0.6509895761235696, 0.6527563287306202, 0.6538716791828165, 0.6557557706949059, 0.6564762063376707, 0.6591494064922481, 0.6594516758374861, 0.6605444957779623, 0.661195537444629, 0.6629157870754891, 0.6643101553848284, 0.6641946189207273, 0.6660078745039683, 0.6675650032299741, 0.6674723577657807, 0.6675413912536914, 0.668379165801495, 0.6694483737656884, 0.6678672725752123, 0.6698203975752123, 0.6700296609680694, 0.669797146087117, 0.6707976810515873, 0.6714243897655962, 0.6712383778608343, 0.6714247502537837, 0.6716340136466409, 0.6719126710155962, 0.6726799701227391, 0.6730988573966409, 0.6727497245870248, 0.6727965880514027, 0.6734701602297896, 0.6731915028608343, 0.673609669158361, 0.6729360969799741, 0.6736797841108343, 0.6728656215393134, 0.6735173841823551, 0.67449322570598, 0.6738654355274086, 0.6738189325512182, 0.6742145683370248, 0.674887780027224, 0.6746094831464562, 0.6741905958725545, 0.6734934117178848, 0.6739588019679771, 0.6743304652893134, 0.6738890475036914, 0.6738189325512182, 0.6745858711701734, 0.6743533562892211, 0.6741677048726468, 0.67400494445598, 0.675515209717608, 0.6747024890988372, 0.6747021286106497, 0.6747250196105574, 0.6744699742178848, 0.6746087621700811, 0.6751671583725545, 0.674190956360742, 0.675376061277224, 0.6753299187892211, 0.6743537167774086, 0.6737960415513105, 0.674586231658361, 0.6743998592654116, 0.676096496919989, 0.6744692532415097, 0.6751907703488372, 0.6748187465393134, 0.6751442673726468, 0.6741212018964562, 0.6742836018249354, 0.6750509009320782, 0.6747947740748431, 0.6750741524201734, 0.6752140218369325, 0.6749811464677925, 0.674934643491602, 0.6744467227297896, 0.6748645285391288, 0.6744699742178848, 0.6739816929678848, 0.6750970434200811, 0.674586231658361, 0.6756321881344591, 0.6756325486226468, 0.6748652495155039, 0.6753764217654116, 0.674934643491602, 0.6748888614917866, 0.67546978820598, 0.6754930396940753, 0.6750516219084532, 0.6747017681224621, 0.6749350039797896, 0.6746559861226468, 0.6748652495155039, 0.6750043979558877, 0.6750741524201734, 0.6750036769795128, 0.6751907703488372, 0.6750741524201734, 0.6750509009320782, 0.6754461762296973, 0.6740986713847361, 0.6746334556109266, 0.675515930693983, 0.6750280099321705, 0.6750741524201734, 0.6752837763012182, 0.67498150695598, 0.6745865921465486, 0.6750280099321705, 0.674887780027224, 0.6750051189322628, 0.6750737919319859, 0.67449322570598, 0.674586231658361, 0.6749113920035068, 0.6742603503368402, 0.6755624336701734, 0.6750516219084532, 0.6750974039082687, 0.6751442673726468, 0.675074512908361, 0.6751907703488372, 0.6747021286106497, 0.675027649443983, 0.6746559861226468, 0.67546978820598, 0.673958080991602, 0.6749110315153193, 0.6743537167774086, 0.674934643491602, 0.6747482710986527, 0.6743540772655962, 0.6753299187892211, 0.6743533562892211, 0.6747722435631229, 0.6751439068844591, 0.6753531702773163, 0.6751435463962716, 0.6746091226582687, 0.6754923187177002, 0.6750505404438907, 0.67472610107512, 0.6739359109680694, 0.674492504729605, 0.6745161167058877, 0.6747722435631229, 0.6746559861226468, 0.6753538912536914, 0.674934643491602, 0.6749578949796973, 0.6753299187892211, 0.6750043979558877, 0.6754236457179771, 0.6753535307655039, 0.6747493525632152, 0.6744935861941675, 0.6752601643249354], "end": "2016-01-26 12:46:20.760000", "learning_rate_per_epoch": [0.004598710220307112, 0.004258970730006695, 0.003944330383092165, 0.0036529346834868193, 0.0033830662723630667, 0.003133135149255395, 0.002901668194681406, 0.002687301253899932, 0.0024887712206691504, 0.002304907888174057, 0.0021346278954297304, 0.0019769277423620224, 0.0018308779690414667, 0.0016956179169937968, 0.0015703504905104637, 0.0014543375000357628, 0.0013468952383846045, 0.001247390522621572, 0.0011552368523553014, 0.0010698912665247917, 0.0009908507345244288, 0.0009176495368592441, 0.0008498562383465469, 0.0007870713016018271, 0.0007289247587323189, 0.0006750738830305636, 0.000625201384536922, 0.000579013314563781, 0.0005362374940887094, 0.0004966218257322907, 0.0004599328385666013, 0.0004259543202351779, 0.0003944860363844782, 0.000365342537406832, 0.000338352081598714, 0.0003133556165266782, 0.0002902058186009526, 0.0002687662490643561, 0.0002489105681888759, 0.00023052177857607603, 0.00021349149756133556, 0.00019771936058532447, 0.00018311242456547916, 0.00016958460037130862, 0.00015705617261119187, 0.00014545331941917539, 0.00013470764679368585, 0.00012475583935156465, 0.00011553923832252622, 0.00010700352868298069, 9.909841901389882e-05, 9.177731408271939e-05, 8.499706746079028e-05, 7.871772686485201e-05, 7.290228677447885e-05, 6.7516477429308e-05, 6.252855382626876e-05, 5.7909124734578654e-05, 5.36309635208454e-05, 4.9668862629914656e-05, 4.5999469875823706e-05, 4.2601161112543195e-05, 3.9453909266740084e-05, 3.653916792245582e-05, 3.383975854376331e-05, 3.1339775887317955e-05, 2.902448431996163e-05, 2.6880239602178335e-05, 2.489440521458164e-05, 2.3055277779349126e-05, 2.135201975761447e-05, 1.9774593965848908e-05, 1.8313703549210913e-05, 1.6960739230853505e-05, 1.5707728380220942e-05, 1.454728680982953e-05, 1.3472575119521935e-05, 1.2477260497689713e-05, 1.1555476703506429e-05, 1.0701791325118393e-05, 9.911173947330099e-06, 9.178965228784364e-06, 8.500849617121276e-06, 7.872831702115946e-06, 7.291209385584807e-06, 6.75255569149158e-06, 6.253696483327076e-06, 5.791691364720464e-06, 5.363817763281986e-06, 4.967554559698328e-06, 4.600565716827987e-06, 4.260689365764847e-06, 3.945921889680903e-06, 3.6544086015055655e-06, 3.384431465747184e-06, 3.1343993214250077e-06, 2.9028390144958394e-06, 2.6883856207859935e-06, 2.4897753974073566e-06, 2.3058380520524224e-06, 2.1354894670366775e-06, 1.9777255602093646e-06, 1.8316169416721095e-06, 1.6963023199423333e-06, 1.5709844092270941e-06, 1.4549245861417148e-06, 1.347438910670462e-06, 1.2478940334403887e-06, 1.1557032166820136e-06, 1.070323150997865e-06, 9.912507721310249e-07, 9.180200208902534e-07, 8.501993420395593e-07, 7.873890695009322e-07, 7.292190389307507e-07, 6.753464276698651e-07, 6.254537652239378e-07, 5.792470574306208e-07, 5.364539674701518e-07, 4.968222810930456e-07, 4.6011848553462187e-07, 4.2612626316440583e-07, 3.9464526935262256e-07, 3.6549002402352926e-07, 3.3848866110020026e-07, 3.134821042749536e-07, 2.903229585626832e-07, 2.6887474291470426e-07, 2.4901103756747034e-07, 2.3061481613240176e-07, 2.1357766399887623e-07, 1.9779916726747615e-07, 1.8318633010494523e-07, 1.6965304894256406e-07, 1.5711957246367092e-07, 1.455120326454562e-07, 1.3476201843332092e-07, 1.2480619204779941e-07, 1.1558586976434526e-07, 1.0704671637995489e-07, 9.913841125808176e-08, 9.181434990068738e-08, 8.503136683657431e-08, 7.874949403685605e-08, 7.293170511957214e-08, 6.754372350314952e-08, 6.255378792729971e-08, 5.793249258090327e-08, 5.3652605913612206e-08, 4.9688907211020705e-08, 4.601803382797698e-08, 4.261835329089081e-08, 3.946983184732744e-08, 3.65539136737425e-08, 3.3853414294071627e-08, 3.1352421103747474e-08, 2.90361938937167e-08, 2.689108313802535e-08, 2.4904446505047417e-08, 2.306457780321125e-08, 2.136063237401231e-08, 1.978257024859431e-08, 1.832109042254615e-08, 1.6967581117910413e-08, 1.5714064716121356e-08, 1.4553155125440753e-08, 1.3478009819323233e-08, 1.2482293421101076e-08, 1.1560137735955323e-08, 1.0706108000135828e-08, 9.91517179471657e-09, 9.182667071172546e-09, 8.50427817056243e-09, 7.876006513640732e-09, 7.294149728664934e-09, 6.755279002845782e-09, 6.256218654243639e-09, 5.794027035932459e-09, 5.365980992877439e-09, 4.969557654277423e-09, 4.602421110888599e-09, 4.262407315991368e-09, 3.947512983160095e-09, 3.6558820415422133e-09, 3.385796087940207e-09, 3.1356632845813692e-09, 2.9040094773336023e-09, 2.689469535965827e-09], "accuracy_valid": [0.3485960443335843, 0.41400808311370485, 0.46558499623493976, 0.5002338455384037, 0.522594773625753, 0.5409773861069277, 0.5548433970256024, 0.5642942865210843, 0.5747011483433735, 0.5793089349585843, 0.5854124505835843, 0.5901937829442772, 0.5963281838290663, 0.6006109398531627, 0.6070100715361446, 0.6097265036709337, 0.6120561346950302, 0.6145284262048193, 0.6160947500941265, 0.6160947500941265, 0.619837749435241, 0.6227880271084337, 0.6252706137048193, 0.6244970114834337, 0.6258294898343373, 0.6286474021084337, 0.6297460349209337, 0.6299901755459337, 0.6328080878200302, 0.6323095114834337, 0.6345273672816265, 0.6343950018825302, 0.6366231527673193, 0.6373555746423193, 0.6368364081325302, 0.6397969808923193, 0.6410176840173193, 0.6407838384789157, 0.6412721197289157, 0.6406514730798193, 0.6415059652673193, 0.6421163168298193, 0.6433473150414157, 0.6427163733057228, 0.6437135259789157, 0.6437032308923193, 0.6451680746423193, 0.6440694418298193, 0.6448018637048193, 0.6450460043298193, 0.6459107916039157, 0.6455342855798193, 0.6457784262048193, 0.6446797933923193, 0.6465211431664157, 0.6466329183923193, 0.6460225668298193, 0.6470094244164157, 0.6463887777673193, 0.6465211431664157, 0.6465108480798193, 0.6468873541039157, 0.6471314947289157, 0.6471211996423193, 0.6470094244164157, 0.6472432699548193, 0.6479756918298193, 0.6468770590173193, 0.6473756353539157, 0.6474977056664157, 0.6482198324548193, 0.6467549887048193, 0.6467446936182228, 0.6485860433923193, 0.6482301275414157, 0.6483521978539157, 0.6480977621423193, 0.6468770590173193, 0.6484742681664157, 0.6482404226280121, 0.6481080572289157, 0.6473447500941265, 0.6477418462914157, 0.6476094808923193, 0.6478433264307228, 0.6467343985316265, 0.6468770590173193, 0.6467549887048193, 0.6476094808923193, 0.6473653402673193, 0.6483419027673193, 0.6474771154932228, 0.6482301275414157, 0.6478536215173193, 0.6476094808923193, 0.6477315512048193, 0.6476094808923193, 0.6486066335655121, 0.6474771154932228, 0.6484639730798193, 0.6474771154932228, 0.6488404791039157, 0.6480874670557228, 0.6481080572289157, 0.6484639730798193, 0.6473653402673193, 0.6479859869164157, 0.6474874105798193, 0.6482301275414157, 0.6474771154932228, 0.6474874105798193, 0.6485757483057228, 0.6476197759789157, 0.6480874670557228, 0.6490743246423193, 0.6477315512048193, 0.6482095373682228, 0.6484639730798193, 0.6477315512048193, 0.6477109610316265, 0.6479756918298193, 0.6472535650414157, 0.6472329748682228, 0.6471211996423193, 0.6468667639307228, 0.6477212561182228, 0.6471211996423193, 0.6477315512048193, 0.6466123282191265, 0.6474771154932228, 0.6476094808923193, 0.6488301840173193, 0.6474874105798193, 0.6487081137048193, 0.6479756918298193, 0.6468770590173193, 0.6485757483057228, 0.6493184652673193, 0.6480977621423193, 0.6484639730798193, 0.6471211996423193, 0.6483419027673193, 0.6478433264307228, 0.6480977621423193, 0.6479756918298193, 0.6482301275414157, 0.6468667639307228, 0.6479756918298193, 0.6470094244164157, 0.6484639730798193, 0.6474874105798193, 0.6484639730798193, 0.6488301840173193, 0.6489522543298193, 0.6470094244164157, 0.6474668204066265, 0.6484639730798193, 0.6477418462914157, 0.6485757483057228, 0.6477315512048193, 0.6476094808923193, 0.6479653967432228, 0.6484742681664157, 0.6482198324548193, 0.6479756918298193, 0.6473550451807228, 0.6476094808923193, 0.6474874105798193, 0.6487081137048193, 0.6473653402673193, 0.6477315512048193, 0.6474874105798193, 0.6474977056664157, 0.6483521978539157, 0.6480977621423193, 0.6477418462914157, 0.6482095373682228, 0.6478639166039157, 0.6474771154932228, 0.6478433264307228, 0.6487184087914157, 0.6482198324548193, 0.6480874670557228, 0.6481080572289157, 0.6473653402673193, 0.6473653402673193, 0.6484639730798193, 0.6481080572289157], "accuracy_test": 0.6448682108626198, "start": "2016-01-23 10:47:24.032000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0], "accuracy_train_last": 0.6752601643249354, "batch_size_eval": 1024, "accuracy_train_std": [0.012957475975792493, 0.01417308332248144, 0.015541020969175433, 0.014996179777872898, 0.015435438796032043, 0.01574629101175008, 0.01494957804916787, 0.015802862816885, 0.015223894467466676, 0.01429983239667469, 0.013979224915310999, 0.01420884554466041, 0.014288876789103193, 0.013473972358325951, 0.014086541883605658, 0.013651482851839888, 0.011536489625157255, 0.012730680128461824, 0.012436253151437376, 0.013048346067963134, 0.012757383216049337, 0.012266053033998452, 0.012789922572476266, 0.012215729380469957, 0.01250974250646404, 0.012938119097472358, 0.0121279960387832, 0.012652486323633815, 0.013192316556967376, 0.012940146093746191, 0.013104247434179933, 0.012778840395926442, 0.01315260243043235, 0.013209774927601463, 0.01328157194122521, 0.013563024814959622, 0.013384664966144771, 0.012333857787626573, 0.013449427426082109, 0.01294505055087419, 0.012669826629063323, 0.013035496637943891, 0.012968660487582403, 0.013032360165489957, 0.012585482914826036, 0.012158413869274274, 0.012630258296639666, 0.013018975094713687, 0.012758589680086488, 0.013020516950455027, 0.012988230633856786, 0.012465955207970031, 0.01256035976360597, 0.012522148802919306, 0.012773284809817884, 0.01258819199486055, 0.012180546997206868, 0.012528109846558116, 0.0127357373806157, 0.013171044776796417, 0.01262677365795015, 0.012884446264964764, 0.012568480627201674, 0.012337292616021671, 0.012567105576309976, 0.01288996185577409, 0.012945454879090485, 0.012433701088293928, 0.013067181847592183, 0.012999684900287793, 0.012967961317321686, 0.012996034309912132, 0.013219060980424593, 0.012962015371168787, 0.012815841893707096, 0.012999792111374812, 0.013092674472841279, 0.012670237550378109, 0.012522123453941353, 0.01292195558071916, 0.012814641864323906, 0.013012830681118205, 0.012359283355708989, 0.012827940501985016, 0.012457657043632196, 0.013089448924661783, 0.013016234435463061, 0.01288471927834302, 0.01298025407306691, 0.012825088820001268, 0.012981001820013757, 0.012832566184649459, 0.012810510615538779, 0.013323493427269286, 0.012625897387035508, 0.012514403623942258, 0.012398588000485055, 0.012950817508335412, 0.013057316538351047, 0.01257387368768866, 0.012753100967446094, 0.012717942473965859, 0.012532199227046894, 0.01286865306131996, 0.012909724290702685, 0.01277688942125702, 0.013268672188113824, 0.01296539626292144, 0.013030125089148902, 0.012480663352014087, 0.01257044115259014, 0.013003122732033663, 0.012643313315878701, 0.012534208058403601, 0.012951296775357665, 0.012923787593559678, 0.012803375881663737, 0.012773478321274756, 0.01279799859505502, 0.012973061174521221, 0.012999573798384382, 0.012529146992810481, 0.013094317914636397, 0.012441475212142778, 0.012491358429663988, 0.012884676505973674, 0.012688289559462254, 0.012711927935481645, 0.012996973673724769, 0.012770972414401754, 0.012318042944066228, 0.013341613923434657, 0.012699565199269883, 0.012805056399737311, 0.012650483077981443, 0.012505491568673158, 0.012870323788779336, 0.013238833252775953, 0.012589857255607486, 0.012876419574843714, 0.012695729998225387, 0.012497833105932565, 0.012981462805802198, 0.012690736524685735, 0.012651465472094616, 0.013114973077984185, 0.012711824761266224, 0.012915401388843199, 0.012677394636490022, 0.012732595974563488, 0.01227157215194684, 0.012715819776301908, 0.013095539679693588, 0.01245157048120069, 0.012843203435041629, 0.012594017028312544, 0.01264843207816903, 0.012804801930661247, 0.01295317552290329, 0.0128889687927971, 0.013028778512378195, 0.013005097758787956, 0.012971085898334444, 0.012810489539783082, 0.012910403895385154, 0.012813798210485814, 0.01286450904845568, 0.012592298824388878, 0.01267088745840041, 0.012694276018323512, 0.012747028567831758, 0.012831505494059012, 0.013133457492914795, 0.012928201655194767, 0.012895379100721983, 0.01294404384398106, 0.012732023550696601, 0.013104718235264875, 0.013231628254370973, 0.01294309968488975, 0.01238247404162563, 0.012748301261536038, 0.012918504456797405, 0.012462260592482786, 0.012910699114036786, 0.012960409710031859, 0.013238508997281503, 0.013270191406799433], "accuracy_test_std": 0.08490414396232206, "error_valid": [0.6514039556664157, 0.5859919168862951, 0.5344150037650602, 0.49976615446159633, 0.477405226374247, 0.4590226138930723, 0.44515660297439763, 0.43570571347891573, 0.4252988516566265, 0.42069106504141573, 0.41458754941641573, 0.40980621705572284, 0.40367181617093373, 0.3993890601468373, 0.3929899284638554, 0.39027349632906627, 0.3879438653049698, 0.3854715737951807, 0.3839052499058735, 0.3839052499058735, 0.38016225056475905, 0.37721197289156627, 0.3747293862951807, 0.37550298851656627, 0.3741705101656627, 0.37135259789156627, 0.37025396507906627, 0.37000982445406627, 0.3671919121799698, 0.36769048851656627, 0.3654726327183735, 0.3656049981174698, 0.3633768472326807, 0.3626444253576807, 0.3631635918674698, 0.3602030191076807, 0.3589823159826807, 0.35921616152108427, 0.35872788027108427, 0.3593485269201807, 0.3584940347326807, 0.3578836831701807, 0.35665268495858427, 0.35728362669427716, 0.35628647402108427, 0.3562967691076807, 0.3548319253576807, 0.3559305581701807, 0.3551981362951807, 0.3549539956701807, 0.35408920839608427, 0.3544657144201807, 0.3542215737951807, 0.3553202066076807, 0.35347885683358427, 0.3533670816076807, 0.3539774331701807, 0.35299057558358427, 0.3536112222326807, 0.35347885683358427, 0.3534891519201807, 0.35311264589608427, 0.35286850527108427, 0.3528788003576807, 0.35299057558358427, 0.3527567300451807, 0.3520243081701807, 0.3531229409826807, 0.35262436464608427, 0.35250229433358427, 0.3517801675451807, 0.3532450112951807, 0.35325530638177716, 0.3514139566076807, 0.35176987245858427, 0.35164780214608427, 0.3519022378576807, 0.3531229409826807, 0.35152573183358427, 0.35175957737198793, 0.35189194277108427, 0.3526552499058735, 0.35225815370858427, 0.3523905191076807, 0.35215667356927716, 0.3532656014683735, 0.3531229409826807, 0.3532450112951807, 0.3523905191076807, 0.3526346597326807, 0.3516580972326807, 0.35252288450677716, 0.35176987245858427, 0.3521463784826807, 0.3523905191076807, 0.3522684487951807, 0.3523905191076807, 0.35139336643448793, 0.35252288450677716, 0.3515360269201807, 0.35252288450677716, 0.35115952089608427, 0.35191253294427716, 0.35189194277108427, 0.3515360269201807, 0.3526346597326807, 0.35201401308358427, 0.3525125894201807, 0.35176987245858427, 0.35252288450677716, 0.3525125894201807, 0.35142425169427716, 0.35238022402108427, 0.35191253294427716, 0.3509256753576807, 0.3522684487951807, 0.35179046263177716, 0.3515360269201807, 0.3522684487951807, 0.3522890389683735, 0.3520243081701807, 0.35274643495858427, 0.35276702513177716, 0.3528788003576807, 0.35313323606927716, 0.35227874388177716, 0.3528788003576807, 0.3522684487951807, 0.3533876717808735, 0.35252288450677716, 0.3523905191076807, 0.3511698159826807, 0.3525125894201807, 0.3512918862951807, 0.3520243081701807, 0.3531229409826807, 0.35142425169427716, 0.3506815347326807, 0.3519022378576807, 0.3515360269201807, 0.3528788003576807, 0.3516580972326807, 0.35215667356927716, 0.3519022378576807, 0.3520243081701807, 0.35176987245858427, 0.35313323606927716, 0.3520243081701807, 0.35299057558358427, 0.3515360269201807, 0.3525125894201807, 0.3515360269201807, 0.3511698159826807, 0.3510477456701807, 0.35299057558358427, 0.3525331795933735, 0.3515360269201807, 0.35225815370858427, 0.35142425169427716, 0.3522684487951807, 0.3523905191076807, 0.35203460325677716, 0.35152573183358427, 0.3517801675451807, 0.3520243081701807, 0.35264495481927716, 0.3523905191076807, 0.3525125894201807, 0.3512918862951807, 0.3526346597326807, 0.3522684487951807, 0.3525125894201807, 0.35250229433358427, 0.35164780214608427, 0.3519022378576807, 0.35225815370858427, 0.35179046263177716, 0.35213608339608427, 0.35252288450677716, 0.35215667356927716, 0.35128159120858427, 0.3517801675451807, 0.35191253294427716, 0.35189194277108427, 0.3526346597326807, 0.3526346597326807, 0.3515360269201807, 0.35189194277108427], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-3, 3], "momentum": 0.9618641470914548, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.004965550661198018, "patience_threshold": 1, "do_flip": true, "batch_size": 32, "optimization": "adadelta", "nb_data_augmentation": 4, "learning_rate_decay_method": "exp", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 6.063567403988598e-10, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.07387712188495738}, "accuracy_valid_max": 0.6493184652673193, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import os\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-3, 3)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.6481080572289157, "loss_train": [2.0907464027404785, 1.7663116455078125, 1.6216187477111816, 1.5267019271850586, 1.4563263654708862, 1.3988040685653687, 1.3551744222640991, 1.3187419176101685, 1.2904597520828247, 1.2649638652801514, 1.2423290014266968, 1.2214276790618896, 1.2043644189834595, 1.1885948181152344, 1.1748014688491821, 1.1632158756256104, 1.1522339582443237, 1.143533706665039, 1.1349068880081177, 1.1249183416366577, 1.1190221309661865, 1.111634373664856, 1.104506015777588, 1.1008195877075195, 1.0955299139022827, 1.0906310081481934, 1.0857387781143188, 1.0808873176574707, 1.0781238079071045, 1.0748995542526245, 1.0728862285614014, 1.0687545537948608, 1.0653965473175049, 1.0641248226165771, 1.0612839460372925, 1.0593318939208984, 1.0564382076263428, 1.0558083057403564, 1.053568959236145, 1.0515663623809814, 1.0524952411651611, 1.0491124391555786, 1.0494256019592285, 1.0484644174575806, 1.0472536087036133, 1.0457477569580078, 1.045780897140503, 1.0439611673355103, 1.0430418252944946, 1.0428467988967896, 1.0416322946548462, 1.0420336723327637, 1.0413240194320679, 1.0400826930999756, 1.0407956838607788, 1.0396621227264404, 1.0405378341674805, 1.0392143726348877, 1.0391147136688232, 1.0392298698425293, 1.0380533933639526, 1.0378104448318481, 1.0370838642120361, 1.0381083488464355, 1.037404179573059, 1.0353630781173706, 1.0373071432113647, 1.036097764968872, 1.0362998247146606, 1.0371685028076172, 1.037182331085205, 1.0361818075180054, 1.0343621969223022, 1.037265658378601, 1.0361074209213257, 1.0364108085632324, 1.0370491743087769, 1.0358556509017944, 1.036076545715332, 1.035992980003357, 1.0349866151809692, 1.034840703010559, 1.0355697870254517, 1.034135103225708, 1.03536057472229, 1.0364235639572144, 1.0345280170440674, 1.0353212356567383, 1.0350462198257446, 1.0341838598251343, 1.0353928804397583, 1.0343443155288696, 1.0335562229156494, 1.0357365608215332, 1.0355024337768555, 1.0338984727859497, 1.0352641344070435, 1.0343133211135864, 1.0341297388076782, 1.0353434085845947, 1.0343455076217651, 1.0345925092697144, 1.033974289894104, 1.0349032878875732, 1.0344350337982178, 1.0340983867645264, 1.0345252752304077, 1.0344815254211426, 1.0340185165405273, 1.0360690355300903, 1.0356489419937134, 1.0335729122161865, 1.0348458290100098, 1.0347461700439453, 1.0340262651443481, 1.0347697734832764, 1.0371558666229248, 1.0343445539474487, 1.0360243320465088, 1.0357093811035156, 1.0342769622802734, 1.034679889678955, 1.034259557723999, 1.0347076654434204, 1.0337755680084229, 1.035353183746338, 1.0355110168457031, 1.0357195138931274, 1.0357768535614014, 1.0350754261016846, 1.03533935546875, 1.0354119539260864, 1.0344889163970947, 1.0352387428283691, 1.0359221696853638, 1.034072995185852, 1.035601258277893, 1.0359220504760742, 1.0349018573760986, 1.03433096408844, 1.0349218845367432, 1.0355525016784668, 1.033592700958252, 1.0350302457809448, 1.0350710153579712, 1.0360504388809204, 1.0349701642990112, 1.0349171161651611, 1.0353294610977173, 1.034890055656433, 1.0348564386367798, 1.0362728834152222, 1.035603642463684, 1.0343599319458008, 1.0346547365188599, 1.035618782043457, 1.0346662998199463, 1.0337224006652832, 1.0363006591796875, 1.0350563526153564, 1.0346417427062988, 1.0359891653060913, 1.035291075706482, 1.034422755241394, 1.0344353914260864, 1.034644365310669, 1.0356996059417725, 1.0361047983169556, 1.0347135066986084, 1.033982753753662, 1.0348782539367676, 1.034787654876709, 1.0341553688049316, 1.0339411497116089, 1.034247875213623, 1.0340008735656738, 1.0348730087280273, 1.0349147319793701, 1.0348825454711914, 1.0353302955627441, 1.0350468158721924, 1.0344657897949219, 1.0342215299606323, 1.0347959995269775, 1.035912036895752, 1.0352320671081543, 1.0341145992279053, 1.035017967224121], "accuracy_train_first": 0.3456142646617756, "model": "residualv4", "loss_std": [0.16896380484104156, 0.12782639265060425, 0.13784916698932648, 0.14562244713306427, 0.151773601770401, 0.1537136435508728, 0.15571078658103943, 0.15782400965690613, 0.15985898673534393, 0.16064147651195526, 0.16184104979038239, 0.16078266501426697, 0.162632018327713, 0.1610601246356964, 0.16250790655612946, 0.1624700278043747, 0.1627766638994217, 0.1625726819038391, 0.1622716337442398, 0.1634184569120407, 0.16284054517745972, 0.16336573660373688, 0.16306528449058533, 0.16331727802753448, 0.1644480973482132, 0.16134555637836456, 0.1638709008693695, 0.1636662632226944, 0.1628994643688202, 0.16433975100517273, 0.16320601105690002, 0.16375672817230225, 0.16395244002342224, 0.16213253140449524, 0.16316749155521393, 0.1626780480146408, 0.16379983723163605, 0.16226382553577423, 0.16273076832294464, 0.16249430179595947, 0.16296939551830292, 0.1620839536190033, 0.16352957487106323, 0.16412967443466187, 0.16340090334415436, 0.16419599950313568, 0.16493327915668488, 0.16354647278785706, 0.16299019753932953, 0.16406026482582092, 0.16327492892742157, 0.1641053557395935, 0.16495154798030853, 0.16335482895374298, 0.16363844275474548, 0.16302980482578278, 0.16345618665218353, 0.16285012662410736, 0.16244003176689148, 0.16358640789985657, 0.16360630095005035, 0.16327865421772003, 0.16297338902950287, 0.1624215692281723, 0.1631082147359848, 0.1631924957036972, 0.1627645343542099, 0.16283269226551056, 0.16364556550979614, 0.16372734308242798, 0.16301566362380981, 0.16290488839149475, 0.1619296669960022, 0.16342350840568542, 0.16333860158920288, 0.16369082033634186, 0.16303080320358276, 0.1633625626564026, 0.16356050968170166, 0.16315585374832153, 0.16320529580116272, 0.16355887055397034, 0.16459758579730988, 0.16344194114208221, 0.16314885020256042, 0.1621563881635666, 0.1623854637145996, 0.1630605161190033, 0.16311386227607727, 0.16440333425998688, 0.16439016163349152, 0.16449172794818878, 0.1629732847213745, 0.16473697125911713, 0.16368046402931213, 0.16275888681411743, 0.1628699153661728, 0.16338150203227997, 0.16448545455932617, 0.16370896995067596, 0.1632627248764038, 0.1628151535987854, 0.16307026147842407, 0.1634748876094818, 0.16314749419689178, 0.16248220205307007, 0.16309863328933716, 0.16321417689323425, 0.163413867354393, 0.16422897577285767, 0.1618773639202118, 0.16356094181537628, 0.16355085372924805, 0.16358377039432526, 0.16270144283771515, 0.16301777958869934, 0.16441071033477783, 0.16242855787277222, 0.16376948356628418, 0.16301876306533813, 0.1625634729862213, 0.16315190494060516, 0.16284674406051636, 0.16282306611537933, 0.163216695189476, 0.16405415534973145, 0.16461868584156036, 0.16310109198093414, 0.16356948018074036, 0.16310814023017883, 0.1634039282798767, 0.16440775990486145, 0.16315218806266785, 0.16266801953315735, 0.1626460999250412, 0.1627824753522873, 0.16368471086025238, 0.16471810638904572, 0.16320231556892395, 0.16211073100566864, 0.16256201267242432, 0.1628054976463318, 0.1628463864326477, 0.1622888743877411, 0.1623428612947464, 0.16516231000423431, 0.16353675723075867, 0.1642022579908371, 0.16303403675556183, 0.1628149300813675, 0.1636829972267151, 0.16339710354804993, 0.1648528277873993, 0.16234444081783295, 0.16242094337940216, 0.16378368437290192, 0.16353334486484528, 0.1635701209306717, 0.16136868298053741, 0.16333822906017303, 0.16233815252780914, 0.1625724732875824, 0.16367965936660767, 0.16320213675498962, 0.1626458615064621, 0.16326725482940674, 0.1639137715101242, 0.1640068143606186, 0.1632620096206665, 0.16212688386440277, 0.16153018176555634, 0.16453073918819427, 0.16337037086486816, 0.16252577304840088, 0.16293102502822876, 0.162113755941391, 0.1630219668149948, 0.16371187567710876, 0.1645718514919281, 0.1633075624704361, 0.16362550854682922, 0.16335532069206238, 0.16479520499706268, 0.16229210793972015, 0.16370148956775665, 0.1645531803369522, 0.16231144964694977, 0.163757786154747]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:15 2016", "state": "available"}], "summary": "700279f68d407db728edfa93710b209e"}