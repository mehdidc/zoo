{"content": {"hp_model": {"f0": 64, "f1": 64, "f2": 32, "f3": 32, "nonlin": "rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.016827363115973173, 0.01569803719548707, 0.014926246446855517, 0.010564832139648185, 0.01174922794483981, 0.012381284908767824, 0.009723853408863089, 0.014931574310096405, 0.011074686960016828, 0.0115730380690318, 0.01774465056723132, 0.011822435291620416, 0.012814736665813465, 0.012135023217887009, 0.01164721238739893, 0.009841410803827107, 0.011278415785340489, 0.009269839612767276, 0.015107394705230315, 0.012746804053778527, 0.011191545204031393, 0.009012553311339955, 0.009928064994197568, 0.006933056625710212, 0.01144836897522591, 0.009723596591018138, 0.008012775820404624, 0.010981656541001281, 0.01041056209136901, 0.006746744977971806, 0.009949493985638195, 0.005634089307191787, 0.005002151952778132, 0.009306979540264965, 0.01056144193096567, 0.006136979667974073, 0.006675228765845552, 0.00922362842828199, 0.00922008382585489, 0.012923096290044298, 0.009220825133847284, 0.010825206802498438, 0.007272912099160187, 0.01359177305378907, 0.008989156281034763, 0.00830592042727471, 0.006351106544814076, 0.009355438155158416, 0.007397532277545218, 0.006707304184455033, 0.008347633970863142, 0.008861329001497128, 0.0056144038037066035, 0.00877746019513298, 0.010295323170555292, 0.006732236611619517, 0.0058405938737579564, 0.006196280843896325, 0.010829303729938084, 0.00963108616061331, 0.008468145110748337, 0.010479430043695505, 0.005499591925216378, 0.007392834699160142, 0.007125048638878033, 0.007718797213043682, 0.009490660081911136, 0.008985445041116024, 0.0059821262125926465, 0.00805142870609052, 0.00705586397842946, 0.005451659567368913, 0.011604685250059575, 0.007400597015915303, 0.007493915471525405, 0.007552933749155061, 0.010714369466664668, 0.007736576792137528, 0.01177423300002176, 0.00907660800054402, 0.006714546135958586, 0.006433179472024199, 0.009088161610062715, 0.007851846819250406, 0.011881813790116554, 0.012220640466255555, 0.011664208311746034, 0.01056228292951894, 0.007972382029560843, 0.01353544500209267, 0.008783709551224005, 0.0075070693331284405, 0.011040060662363516, 0.009555958824240662, 0.00845009401877285, 0.00642240111940945, 0.0056731347260338474, 0.015660867322157147, 0.004008850163784784, 0.0064230226457474135, 0.008237955793246717, 0.010180715814643615, 0.010727213668476633, 0.006949735925797944, 0.007794731401101358, 0.013628497907934498, 0.007395549395003908, 0.01592247015251769, 0.005407211909764468, 0.008730746785493815, 0.006391755037827486, 0.01420548345229167, 0.006395904464714159, 0.009216265497473968, 0.006344238002872422, 0.0031769874562723095, 0.009989157154322494, 0.008848028345720519, 0.006424647827154849, 0.0062672148032235035, 0.005560999333050262, 0.007058102728432898, 0.010314203002063314, 0.011470571235132666, 0.008622209501247367, 0.006373628944129762, 0.0067253125078526425, 0.008638366321513116, 0.008308787559315453, 0.004788179840316302, 0.00929608893432332, 0.005442108366667813, 0.010268806466002044, 0.009720427463321501, 0.007173779349876654, 0.007466518715008987, 0.010445972408622035, 0.006746164658842471, 0.004344039344375654, 0.005774116308742506, 0.008438187780324343, 0.009060308404220202, 0.008825268679480898, 0.007618003915135235, 0.010338650530019913, 0.009465058249625012, 0.00792361558245597, 0.006885491581288161, 0.01067348386494125, 0.006921380123118597, 0.007702643417653844, 0.007011859581386136, 0.002593774799206445, 0.004466464648627944, 0.009303122126164028, 0.009113402675445918, 0.012371526045052135, 0.010368849376926477, 0.006197154894403103, 0.008237463723088046, 0.011954572902286497, 0.005415174607262027, 0.013239155501985766, 0.013453793304165672, 0.009857817148380908, 0.013079316946996144, 0.011438149646649699, 0.0065900312415216695, 0.014674501989816438, 0.012503861666813405, 0.00979789003144802, 0.007452142653222717, 0.005329127456546813, 0.011240749789348822, 0.009999928575228275, 0.009914345440832138, 0.010184577998585666, 0.0067052205769891895, 0.007394456615751895, 0.010513677523122905, 0.010442175419314451, 0.010750904824306896, 0.008497977745430494, 0.006609806066894825, 0.007320983027263429, 0.008346027273737289, 0.00568274906248583, 0.010654239503626333, 0.006936736338576278, 0.005609047597372638, 0.011100459362409595, 0.007911133698904211, 0.004898333015302704, 0.006547066538823826, 0.007733934814530487, 0.005792887244519954, 0.008782475724449264, 0.00772044352758863, 0.004338961451931138, 0.008734550891361582, 0.007955182102000838, 0.005561524995082026, 0.006944368072479024, 0.011320334313207997, 0.005052745375399566, 0.01072144036744997, 0.00918567585959031, 0.006468369533585163, 0.010465360197178666, 0.009845048633620745, 0.007445414505394664, 0.00615343034356787, 0.011799215220383847, 0.009581026313026072, 0.008835154231770906, 0.014704122110671718, 0.01037245018013682, 0.007221531921072141, 0.012163396985527548, 0.009832795687815215, 0.010252157661737473, 0.005487943948027719, 0.012091685891240076, 0.008499496041240798, 0.0071027660889428695, 0.005585747621674038, 0.0076772156874293914, 0.005992146694317681, 0.011952420903041015, 0.00659638055553669, 0.007266170258259709, 0.01157323534359134, 0.013858420902655929, 0.008588344043695212, 0.010618710728551955, 0.00836193407260845, 0.012120715148760869, 0.0035296578760638155, 0.012022420308187362, 0.007627439561013549, 0.007029528797959063, 0.011033826588460582, 0.01168393390597848, 0.007905388629591096, 0.007765487860141987, 0.004782889693008927, 0.007525627536585894, 0.009901209630590225, 0.0076472659641862555, 0.00946384422762678, 0.012497211565570873, 0.013241513299688817, 0.006151031122141303, 0.012285686670216296, 0.013300166843996254, 0.010667331912318521, 0.0061405203360689765, 0.01005178696631151, 0.004815125831843973, 0.008009343364778189, 0.00744195129046329, 0.01177056972463411, 0.010352781532403457, 0.01010893062898659, 0.011796938161250943, 0.006198395248086306, 0.010019442762840327, 0.007802410479746632, 0.012318019464508357, 0.013379888047862339, 0.008809676612367095, 0.009039603259859232, 0.011751274594199546, 0.009067317776283819, 0.009485328972559413, 0.00905699746275297, 0.006871187090608738, 0.012229023344764743, 0.008529956847014039, 0.011176141183041544, 0.011154412063888368, 0.007835185250480098, 0.010144836237324257, 0.008500847784725813, 0.013911233853772931, 0.009407677198295718, 0.006772344771314412, 0.007506003164091298, 0.008088923877000484, 0.008736596541700867], "moving_avg_accuracy_train": [0.05804292404946474, 0.12215160258628643, 0.18178983106081575, 0.24042781656630668, 0.2957660298282345, 0.3478091795793535, 0.39559413362792417, 0.44002540380841343, 0.481629345149379, 0.5193821010990959, 0.5544102963740146, 0.5868050975321096, 0.616237039185091, 0.6426424057550001, 0.6674484336999412, 0.6902924391325496, 0.7113123513885453, 0.7309415516617431, 0.7484985859623923, 0.7644580269520242, 0.7801397029735992, 0.7930047867263962, 0.8062873717419257, 0.8182581184928439, 0.8298966017305381, 0.8397295676706719, 0.8495767258560779, 0.8583995325467252, 0.8662818217016134, 0.8737782047826979, 0.880687637874703, 0.8869572088336798, 0.8928789487003579, 0.8987710563969981, 0.9035950808156685, 0.9084807876139005, 0.912652456395423, 0.9159882873178039, 0.9196160362265274, 0.9225765238967872, 0.9256617065880886, 0.9287383512555072, 0.9311632094323743, 0.9323714525868205, 0.9348050244400802, 0.9368022878056421, 0.9388252561227155, 0.9408739003378711, 0.9434313566207599, 0.9458400601694167, 0.9477567772917792, 0.9488866567043048, 0.9501429664053214, 0.9517921893695789, 0.953320595767154, 0.9544102042702189, 0.9557977169158254, 0.9567371974563857, 0.9577781505893463, 0.9581570087435439, 0.95934197405136, 0.9607525288033854, 0.9617617195599978, 0.9621630006540164, 0.9630752139064903, 0.9630661637575356, 0.963597453147286, 0.9637641157552226, 0.9646394503844807, 0.9656457434412707, 0.9664653766864294, 0.9670799579154424, 0.9671423664762884, 0.9682634523358117, 0.9687934850034394, 0.9689171278340755, 0.9688703323114191, 0.9688327584921912, 0.969989454294181, 0.970532862621915, 0.9705081443276083, 0.9712763403115141, 0.9719119131256008, 0.9721329032856783, 0.9724619667142624, 0.9720955284869115, 0.9722539792834769, 0.9728128226861092, 0.9736086423520222, 0.9734809231311241, 0.9736451378847337, 0.9738091351070116, 0.9743774763951293, 0.9747541249234827, 0.9751233715823433, 0.975676546663404, 0.9762581816911389, 0.976046870143472, 0.9763543797422661, 0.9762939918037999, 0.976244220859162, 0.9758854598220645, 0.9768740309148858, 0.9774335377186537, 0.9772188031039958, 0.9777275205983673, 0.9779737778016351, 0.9782930294857572, 0.978785005145524, 0.9788301827928856, 0.9794265171921684, 0.9793982069908088, 0.9802004807857755, 0.9805854527215021, 0.9806946901874471, 0.980874528310406, 0.981196673693651, 0.981272762945733, 0.9815039676404546, 0.9818632225871418, 0.9821052078796457, 0.9819835589655276, 0.98239243893207, 0.9827348903138722, 0.982922188819399, 0.9826513043493731, 0.9825283800156261, 0.9827457743414907, 0.9831226466466366, 0.9833804875617532, 0.9832288948317868, 0.9836017410617403, 0.9839209545293942, 0.9837384944931215, 0.9840812349985897, 0.9839339722868444, 0.9843083182867498, 0.9843568391366463, 0.9846447206241906, 0.9848224337546472, 0.9850777427220673, 0.9852633429653644, 0.9857186295390753, 0.9858819577304243, 0.9860033404169148, 0.9862892961442802, 0.985811981372737, 0.9857590000878533, 0.9858043228838391, 0.9861357570014169, 0.9863480532501032, 0.9863903103501113, 0.9864236914424995, 0.9863234900434968, 0.9866448601236801, 0.9863134226589774, 0.9863312410323838, 0.9862170331862975, 0.9864073230212761, 0.9864506646394143, 0.9863733786064437, 0.9860248033196273, 0.9863063957543681, 0.9861760712456071, 0.9865051717103321, 0.9864805636905079, 0.9868560529679133, 0.9871032404163693, 0.9873025297295219, 0.9876840699113317, 0.9878926334928267, 0.9879292781411908, 0.9871042423651855, 0.987554547554885, 0.987794664562501, 0.987954930249108, 0.9878945562718162, 0.9877985112089572, 0.9882606976249755, 0.987446625630335, 0.9871069470327869, 0.9873197805283362, 0.9876368526612261, 0.9878966769927411, 0.9882118990994377, 0.9880398698287982, 0.9883709274887755, 0.9881782729839456, 0.988428061012932, 0.9887040595616481, 0.9888269002197781, 0.9891258299144854, 0.9884554344230368, 0.9889216469331141, 0.9889878516219548, 0.9893566445847594, 0.9892816932584355, 0.9893979238206964, 0.9891281823683978, 0.9893667208649006, 0.9895698518653431, 0.9898270384788272, 0.9899166002559445, 0.9898113381458632, 0.9900954573074674, 0.9901675498945962, 0.9900952494432502, 0.9898533956298776, 0.9899241177478607, 0.9902737249087982, 0.9904162742929183, 0.9900725635302932, 0.9902585526379966, 0.9905491396730157, 0.9902619328854853, 0.9905126183171747, 0.9906382538068859, 0.9907931784261973, 0.9906163903454823, 0.9904270901871338, 0.9905147755136585, 0.9906728194646921, 0.9906243968182413, 0.9906249582149979, 0.9906277525720695, 0.9908720829696244, 0.9912128880655191, 0.9915219738494526, 0.9915513601323737, 0.9913941210310503, 0.99149903556485, 0.991716763629812, 0.9915011315001734, 0.9915906946894417, 0.991648050071688, 0.991720668352633, 0.9914372164352361, 0.9912565144714836, 0.9916123908886301, 0.9919280654152617, 0.9916494864773253, 0.9917383092570107, 0.9917971792241667, 0.9915990461231785, 0.9908907365989928, 0.9907437202307602, 0.9907090977981696, 0.990838408925514, 0.9908292309044096, 0.9909302526794632, 0.9909351417710592, 0.9909953094761055, 0.9909447926653997, 0.991138889960783, 0.9911600816563806, 0.9914373177979131, 0.9916402913002831, 0.9913811891786065, 0.9916781311976691, 0.9917290680779023, 0.9919261540891874, 0.9919429880850305, 0.9916652420289268, 0.9918569767546056, 0.992127266355354, 0.9921635527031611, 0.9921056738078912, 0.992337106761635, 0.9923384942247757, 0.9921188177558787, 0.9922558943136243, 0.9925513963251374, 0.9927219449366713, 0.9924616342965941, 0.9925621040514584, 0.9928036615034553, 0.9928583388424047, 0.9929261496379354, 0.9925524125753508, 0.9926810789809294, 0.9926155171388072, 0.9925099724558881, 0.9922406321293653, 0.9923818393402475, 0.9924740485978987, 0.9928267181428707, 0.9926884997131351, 0.9926617233275543, 0.9927306305329127, 0.9928995317653357], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 312226994, "moving_var_accuracy_train": [0.030320829289907397, 0.06427805033455454, 0.08986070996132053, 0.11182035906246798, 0.12819918377942463, 0.1397556703256387, 0.146330719793891, 0.1494648877431666, 0.15009639038477252, 0.14791418658248529, 0.14416553810219734, 0.13919379257063025, 0.13307056601874773, 0.12603869987009444, 0.11897288108473181, 0.11177223023410397, 0.10457153761194139, 0.09758213338103507, 0.09059816512383913, 0.08383068242176911, 0.07766084884520295, 0.07138435738038099, 0.06583376522459579, 0.06054007770210777, 0.055705158560563794, 0.05100482767712588, 0.04677704362836898, 0.04279991652663686, 0.03907909921486459, 0.03567695112706346, 0.03253891840523319, 0.029638794244796665, 0.02699051784775445, 0.024603918460958253, 0.022352967519189786, 0.020332501945535417, 0.018455877134786438, 0.01671043933279221, 0.01515784045881569, 0.013720936798145964, 0.012434508288479716, 0.011276249141317739, 0.01020154366178724, 0.009194527959290912, 0.008328375611046619, 0.0075314395985047, 0.006815127245961171, 0.006171387009447667, 0.005613113552252886, 0.005104018872095402, 0.004626681225630281, 0.004175502750448897, 0.003772157301987822, 0.003419420999261547, 0.0030985031343807303, 0.0027993380411522207, 0.0025367309591124575, 0.0022910014763760354, 0.002071653579563615, 0.0018657800231162713, 0.0016918393058311947, 0.001540562357624226, 0.00139567231571089, 0.0012575543227875525, 0.0011392880876706993, 0.0010253600160503942, 0.0009253644301863062, 0.0008330779749916336, 0.0007566660739110762, 0.0006901130979652628, 0.00062714797607786, 0.0005678325692535705, 0.0005110843657844152, 0.00047128743074578055, 0.00042668709932997597, 0.0003841559773430878, 0.0003457600879972452, 0.00031119678532454305, 0.00029211861339715643, 0.0002655643855532966, 0.00023901344594462787, 0.00022042322697736654, 0.0002020164794976845, 0.00018225436140557544, 0.0001650034699253026, 0.00014971161570294925, 0.00013496641402704424, 0.00012428052616233147, 0.00011755243401198163, 0.00010594400040526468, 9.559229873246588e-05, 8.62751246594529e-05, 8.055471857152002e-05, 7.377602373956535e-05, 6.762550922132672e-05, 6.361698233195171e-05, 6.029997784815021e-05, 5.467185319493146e-05, 5.0055727255593394e-05, 4.508297485804381e-05, 4.059697169461088e-05, 3.76956598608029e-05, 4.2721549124778505e-05, 4.126682498346456e-05, 3.755514107770862e-05, 3.612876837165485e-05, 3.3061675025940615e-05, 3.067280226368032e-05, 2.9783882485539487e-05, 2.68238634153757e-05, 2.7342009515750706e-05, 2.4615021771684882e-05, 2.7946308773329384e-05, 2.6485508417670426e-05, 2.394435299159819e-05, 2.184099344666272e-05, 2.0590892933511716e-05, 1.8583909808702024e-05, 1.7206619325583786e-05, 1.6647534443499047e-05, 1.5509792935243314e-05, 1.409199976647407e-05, 1.4187445233184681e-05, 1.3824157249950547e-05, 1.2757468096508573e-05, 1.2142126851768639e-05, 1.1063907693035685e-05, 1.0382859559994764e-05, 1.0622868213468918e-05, 1.015891882969602e-05, 9.349850148734369e-06, 9.665993934575662e-06, 9.616469682503066e-06, 8.954447697782664e-06, 9.116242414801891e-06, 8.399794929756828e-06, 8.82102978558804e-06, 7.9601152629013e-06, 7.909985494447664e-06, 7.4032245556328504e-06, 7.2495461196756705e-06, 6.834618560515776e-06, 8.016729482276725e-06, 7.455141416853137e-06, 6.842231084384937e-06, 6.893944078063925e-06, 8.255014190457477e-06, 7.454775920342896e-06, 6.727785730832344e-06, 7.043644326399674e-06, 6.744907168616151e-06, 6.086487414264419e-06, 5.487867348799295e-06, 5.02944349717812e-06, 5.456007703393855e-06, 5.899064070132196e-06, 5.312015112996639e-06, 4.898204490666002e-06, 4.734276033265034e-06, 4.277754892704044e-06, 3.903737581464618e-06, 4.6069063985302905e-06, 4.859864452406863e-06, 4.526738305420138e-06, 5.0488285178179965e-06, 4.549395657793246e-06, 5.363385869032264e-06, 5.376961994196675e-06, 5.196711867808756e-06, 5.987196874046354e-06, 5.779966094376316e-06, 5.214054957222215e-06, 1.081880574669919e-05, 1.1561898046862972e-05, 1.0924613838294709e-05, 1.0063318267197887e-05, 9.089791594684322e-06, 8.263834322112168e-06, 9.359997438267307e-06, 1.4388416606560859e-05, 1.3988008892594879e-05, 1.2996890874784954e-05, 1.2602014424404585e-05, 1.194939113118855e-05, 1.164873680702266e-05, 1.0750209755931527e-05, 1.0661581348405296e-05, 9.929465037646174e-06, 9.498065068705726e-06, 9.233835351875323e-06, 8.44626026229639e-06, 8.405864897467e-06, 1.161014944231105e-05, 1.2405321439052866e-05, 1.1204236842567998e-05, 1.130788740303841e-05, 1.0227657974593903e-05, 9.326478069565933e-06, 9.048674322402267e-06, 8.65591241898631e-06, 8.16168100715421e-06, 7.940817493837796e-06, 7.218927551737657e-06, 6.596755802932876e-06, 6.663593504555418e-06, 6.044010224170268e-06, 5.486655399136649e-06, 5.464429262608978e-06, 4.9630008980961935e-06, 5.566727311095257e-06, 5.192937522203125e-06, 5.736877565082199e-06, 5.474517342232857e-06, 5.687033032300258e-06, 5.860719378302201e-06, 5.840236111424466e-06, 5.398270986756538e-06, 5.0744586270999594e-06, 4.848298993735883e-06, 4.6859800439192936e-06, 4.286580487917073e-06, 4.082723453250046e-06, 3.6955538821286914e-06, 3.3260013304126867e-06, 2.9934714732544127e-06, 3.2314004144532678e-06, 3.9535933934981085e-06, 4.418040250616397e-06, 3.984008208170033e-06, 3.8081246022178583e-06, 3.5263756766178255e-06, 3.6003877014050398e-06, 3.6588238692569696e-06, 3.3651355661785743e-06, 3.0582287684142965e-06, 2.7998666241193983e-06, 3.24298486699142e-06, 3.2125651776282652e-06, 4.031140878395263e-06, 4.5248804514327195e-06, 4.770848428244776e-06, 4.364768961139567e-06, 3.959483122322229e-06, 3.916845341454426e-06, 8.040482245778442e-06, 7.43095833395538e-06, 6.698650916106261e-06, 6.179278133391247e-06, 5.562108444694652e-06, 5.097746191540172e-06, 4.588186701335857e-06, 4.161949405776968e-06, 3.768721998674225e-06, 3.7309136394826395e-06, 3.361864067195084e-06, 3.717416564021965e-06, 3.716459091598902e-06, 3.949018367554434e-06, 4.347687594963602e-06, 3.936269927378158e-06, 3.892228997238683e-06, 3.5055565482592466e-06, 3.849286738563425e-06, 3.7952179099870214e-06, 4.073204333442761e-06, 3.67773419143313e-06, 3.340110470948865e-06, 3.4881503325617444e-06, 3.1393526247912707e-06, 3.259737121195273e-06, 3.1028732532258455e-06, 3.578478877178231e-06, 3.482412449525591e-06, 3.744025868609848e-06, 3.460470826531498e-06, 3.6395737674160046e-06, 3.3025228932256988e-06, 3.0136553398176455e-06, 3.969404333380175e-06, 3.7214592953628678e-06, 3.387998562108563e-06, 3.149455826730169e-06, 3.487408147479506e-06, 3.3181226203777872e-06, 3.062833283109179e-06, 3.8759322263553306e-06, 3.660278012586894e-06, 3.300702984751131e-06, 3.013366512828594e-06, 2.9687784983718737e-06], "duration": 274196.434956, "accuracy_train": [0.5804292404946475, 0.6991297094176818, 0.7185338873315799, 0.7681696861157253, 0.793809949185585, 0.8161975273394242, 0.8256587200650609, 0.8399068354328165, 0.8560648172180694, 0.8591569046465486, 0.8696640538482835, 0.8783583079549648, 0.8811245140619232, 0.8802907048841824, 0.8907026852044113, 0.8958884880260245, 0.9004915616925065, 0.9076043541205242, 0.9065118946682356, 0.9080929958587117, 0.921274787167774, 0.9087905405015688, 0.9258306368816908, 0.9259948392511074, 0.934642950869786, 0.9282262611318751, 0.9382011495247323, 0.9378047927625508, 0.9372224240956073, 0.9412456525124585, 0.9428725357027501, 0.9433833474644703, 0.9461746075004615, 0.9518000256667589, 0.9470113005837025, 0.9524521487979882, 0.9501974754291252, 0.9460107656192323, 0.9522657764050388, 0.9492209129291252, 0.9534283508098007, 0.9564281532622739, 0.9529869330241787, 0.9432456409768365, 0.9567071711194168, 0.9547776580956996, 0.957031970976375, 0.959311698274271, 0.9664484631667589, 0.9675183921073275, 0.9650072313930418, 0.9590555714170359, 0.9614497537144703, 0.9666351960478959, 0.9670762533453304, 0.9642166807978036, 0.9682853307262828, 0.9651925223214286, 0.9671467287859912, 0.9615667321313216, 0.9700066618217055, 0.9734475215716132, 0.9708444363695091, 0.9657745305001846, 0.971285133178756, 0.9629847124169435, 0.9683790576550388, 0.9652640792266519, 0.9725174620478036, 0.9747023809523809, 0.9738420758928571, 0.9726111889765596, 0.9677040435239018, 0.9783532250715209, 0.9735637790120893, 0.9700299133098007, 0.9684491726075121, 0.96849459411914, 0.9803997165120893, 0.9754235375715209, 0.9702856796788483, 0.9781901041666666, 0.9776320684523809, 0.974121814726375, 0.9754235375715209, 0.968797584440753, 0.9736800364525655, 0.9778424133098007, 0.9807710193452381, 0.9723314501430418, 0.9751230706672205, 0.9752851101075121, 0.9794925479881875, 0.9781439616786637, 0.9784465915120893, 0.9806551223929494, 0.981492896940753, 0.9741450662144703, 0.9791219661314139, 0.9757505003576044, 0.9757962823574198, 0.9726566104881875, 0.9857711707502769, 0.9824690989525655, 0.9752861915720746, 0.9823059780477114, 0.9801900926310447, 0.9811662946428571, 0.9832127860834257, 0.97923678161914, 0.9847935267857143, 0.9791434151785714, 0.9874209449404762, 0.9840502001430418, 0.9816778273809523, 0.9824930714170359, 0.9840959821428571, 0.9819575662144703, 0.9835848098929494, 0.9850965171073275, 0.9842830755121816, 0.9808887187384644, 0.9860723586309523, 0.9858169527500923, 0.98460787536914, 0.98021334411914, 0.9814220610119048, 0.984702323274271, 0.9865144973929494, 0.9857010557978036, 0.9818645602620893, 0.9869573571313216, 0.9867938757382798, 0.9820963541666666, 0.9871658995478036, 0.982608607881137, 0.9876774322858989, 0.9847935267857143, 0.9872356540120893, 0.986421851928756, 0.9873755234288483, 0.9869337451550388, 0.9898162087024732, 0.9873519114525655, 0.9870957845953304, 0.9888628976905685, 0.9815161484288483, 0.9852821685239018, 0.9862122280477114, 0.9891186640596161, 0.9882587194882798, 0.9867706242501846, 0.9867241212739941, 0.9854216774524732, 0.9895371908453304, 0.9833304854766519, 0.9864916063930418, 0.9851891625715209, 0.9881199315360835, 0.9868407392026578, 0.9856778043097084, 0.9828876257382798, 0.9888407276670359, 0.9850031506667589, 0.9894670758928571, 0.9862590915120893, 0.9902354564645626, 0.9893279274524732, 0.9890961335478959, 0.9911179315476191, 0.9897697057262828, 0.9882590799764673, 0.979678920381137, 0.9916072942621816, 0.9899557176310447, 0.9893973214285714, 0.9873511904761905, 0.9869341056432264, 0.99242037536914, 0.9801199776785714, 0.9840498396548542, 0.9892352819882798, 0.9904905018572352, 0.990235095976375, 0.9910488980597084, 0.9864916063930418, 0.9913504464285714, 0.9864443824404762, 0.9906761532738095, 0.9911880465000923, 0.9899324661429494, 0.9918161971668512, 0.982421875, 0.9931175595238095, 0.9895836938215209, 0.99267578125, 0.9886071313215209, 0.9904439988810447, 0.9867005092977114, 0.9915135673334257, 0.9913980308693245, 0.9921417180001846, 0.99072265625, 0.9888639791551311, 0.9926525297619048, 0.990816383178756, 0.989444545381137, 0.9876767113095238, 0.9905606168097084, 0.9934201893572352, 0.99169921875, 0.9869791666666666, 0.9919324546073275, 0.9931644229881875, 0.9876770717977114, 0.9927687872023809, 0.9917689732142857, 0.9921875, 0.9890252976190477, 0.9887233887619971, 0.9913039434523809, 0.9920952150239941, 0.9901885930001846, 0.9906300107858066, 0.9906529017857143, 0.9930710565476191, 0.9942801339285714, 0.9943037459048542, 0.9918158366786637, 0.98997896911914, 0.9924432663690477, 0.9936763162144703, 0.9895604423334257, 0.9923967633928571, 0.9921642485119048, 0.992374232881137, 0.9888861491786637, 0.9896301967977114, 0.9948152786429494, 0.9947691361549464, 0.9891422760358989, 0.9925377142741787, 0.9923270089285714, 0.9898158482142857, 0.9845159508813216, 0.9894205729166666, 0.9903974959048542, 0.9920022090716132, 0.9907466287144703, 0.9918394486549464, 0.9909791435954227, 0.9915368188215209, 0.9904901413690477, 0.9928857656192323, 0.9913508069167589, 0.9939324430717055, 0.9934670528216132, 0.989049270083518, 0.9943506093692323, 0.9921875, 0.993699928190753, 0.9920944940476191, 0.9891655275239941, 0.9935825892857143, 0.9945598727620893, 0.9924901298334257, 0.9915847637504615, 0.9944200033453304, 0.9923509813930418, 0.9901417295358066, 0.9934895833333334, 0.995210914428756, 0.9942568824404762, 0.9901188385358989, 0.9934663318452381, 0.9949776785714286, 0.9933504348929494, 0.9935364467977114, 0.9891887790120893, 0.993839076631137, 0.9920254605597084, 0.9915600703096161, 0.9898165691906607, 0.9936527042381875, 0.9933039319167589, 0.9960007440476191, 0.991444533845515, 0.9924207358573275, 0.993350795381137, 0.9944196428571429], "end": "2016-01-27 12:07:54.205000", "learning_rate_per_epoch": [0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102, 0.0003238953067921102], "accuracy_valid": [0.5707831325301205, 0.6823598103350903, 0.7030117540474398, 0.7453098526920181, 0.76220703125, 0.786877000188253, 0.790783250188253, 0.8051684276167168, 0.815941500376506, 0.8152296686746988, 0.8250776543674698, 0.8367964043674698, 0.8328695641942772, 0.8332151849585843, 0.8407129494540663, 0.8407835443335843, 0.8448736351656627, 0.8471723809299698, 0.8491975715361446, 0.8474577019013554, 0.8585763954254518, 0.8503667992281627, 0.8606618858245482, 0.8564600197665663, 0.863368022872741, 0.857752788497741, 0.8693288780120482, 0.8656976538968373, 0.8648431617093373, 0.8690641472138554, 0.8702142554593373, 0.8597559182040663, 0.8668771766754518, 0.8746293768825302, 0.8718011695218373, 0.870936382247741, 0.8760839255459337, 0.865199077560241, 0.8708349021084337, 0.8741822759789157, 0.865269672439759, 0.8748941076807228, 0.8732763083584337, 0.8619340643825302, 0.8746396719691265, 0.8709466773343373, 0.8711702277861446, 0.8727571418486446, 0.8804887518825302, 0.8805902320218373, 0.8786576971950302, 0.8740190253200302, 0.8738557746611446, 0.8752397284450302, 0.882044780685241, 0.8761545204254518, 0.8787385871611446, 0.8765619117093373, 0.8812726491905121, 0.8727777320218373, 0.8761957007718373, 0.8852906744164157, 0.883021343185241, 0.8776811346950302, 0.8833169592432228, 0.8727468467620482, 0.8741410956325302, 0.8728689170745482, 0.8813226538968373, 0.8815667945218373, 0.8795018942959337, 0.8823198065700302, 0.875575054122741, 0.8854730445218373, 0.8822080313441265, 0.8786576971950302, 0.8822492116905121, 0.8715570288968373, 0.8868672933923193, 0.8822080313441265, 0.8795827842620482, 0.8788812476468373, 0.8884542074548193, 0.8844758918486446, 0.8817506353539157, 0.8793092291039157, 0.8826257177146084, 0.8828389730798193, 0.8853921545557228, 0.8859525014118976, 0.8814241340361446, 0.8799901755459337, 0.8812520590173193, 0.8779561605798193, 0.8847920980798193, 0.883509624435241, 0.880091655685241, 0.8787091726280121, 0.8807123023343373, 0.8751985480986446, 0.8796239646084337, 0.8747617422816265, 0.8813535391566265, 0.8840493811182228, 0.8821065512048193, 0.8861863469503012, 0.8827874976468373, 0.8868069935993976, 0.8886468726468373, 0.8807534826807228, 0.8907632483057228, 0.884497952748494, 0.8869584784450302, 0.8900411215173193, 0.8868467032191265, 0.8887895331325302, 0.8866231527673193, 0.8810991034450302, 0.8869687735316265, 0.8863481268825302, 0.8853715643825302, 0.8846288474209337, 0.8920457219503012, 0.8918015813253012, 0.8860333913780121, 0.8791253882718373, 0.8853715643825302, 0.8871217291039157, 0.8887086431664157, 0.8900205313441265, 0.8886071630271084, 0.8880777014307228, 0.8868981786521084, 0.8870908438441265, 0.8927472585655121, 0.8851068335843373, 0.8866731574736446, 0.8876806052334337, 0.8902646719691265, 0.8877217855798193, 0.8930928793298193, 0.8893189947289157, 0.8955651708396084, 0.8900617116905121, 0.8921472020896084, 0.8976712514118976, 0.8915265554405121, 0.8927472585655121, 0.8935208607868976, 0.8912824148155121, 0.8894204748682228, 0.8901426016566265, 0.8901426016566265, 0.8897763907191265, 0.8969285344503012, 0.8861142813441265, 0.8893601750753012, 0.8886174581137049, 0.8900514166039157, 0.8893395849021084, 0.8878953313253012, 0.8808343726468373, 0.8961152226091867, 0.8909779743975903, 0.8946091985128012, 0.8939076618975903, 0.8976918415850903, 0.8953107351280121, 0.8939282520707832, 0.8942429875753012, 0.8959519719503012, 0.8896440253200302, 0.8792165733245482, 0.8974477009600903, 0.8958401967243976, 0.8938664815512049, 0.8910588643637049, 0.8943238775414157, 0.8966123282191265, 0.8818418204066265, 0.8888513036521084, 0.8953519154743976, 0.8992581654743976, 0.8938561864646084, 0.8963167121611446, 0.8932355398155121, 0.8955445806664157, 0.8924016378012049, 0.8967652837914157, 0.8936929358057228, 0.8911912297628012, 0.8987492940512049, 0.878260600997741, 0.9004582784262049, 0.8933473150414157, 0.8945680181664157, 0.8891866293298193, 0.8961652273155121, 0.8906205878200302, 0.8937341161521084, 0.8959313817771084, 0.8946797933923193, 0.8902029014495482, 0.8908250188253012, 0.8976094808923193, 0.8965726185993976, 0.8968064641378012, 0.8897866858057228, 0.8939782567771084, 0.8997464467243976, 0.9009465596762049, 0.8883012518825302, 0.8974271107868976, 0.8973859304405121, 0.8891969244164157, 0.8991875705948795, 0.8942326924887049, 0.8975182958396084, 0.8958299016378012, 0.8959622670368976, 0.8931237645896084, 0.8973756353539157, 0.8949857045368976, 0.8961652273155121, 0.8907735433923193, 0.8985860433923193, 0.8981492375753012, 0.8950562994164157, 0.8978242069841867, 0.8921266119164157, 0.8975285909262049, 0.9012421757341867, 0.8941518025225903, 0.8979050969503012, 0.8974271107868976, 0.8959107916039157, 0.8897366810993976, 0.8938150061182228, 0.9014966114457832, 0.9047513295368976, 0.8942223974021084, 0.8995228962725903, 0.899390530873494, 0.8951680746423193, 0.8896440253200302, 0.8937032308923193, 0.8922589773155121, 0.9018216420368976, 0.8959416768637049, 0.8964505482868976, 0.8943959431475903, 0.8985669239457832, 0.8962667074548193, 0.894629788685994, 0.8952607304216867, 0.9009980351091867, 0.8946900884789157, 0.8953519154743976, 0.8976197759789157, 0.8954533956137049, 0.8993493505271084, 0.8944871282003012, 0.8931443547628012, 0.8971829701618976, 0.8983124882341867, 0.8916074454066265, 0.8963284779743976, 0.8994817159262049, 0.894751858998494, 0.8938870717243976, 0.8977418462914157, 0.8975182958396084, 0.8920457219503012, 0.8949651143637049, 0.8977624364646084, 0.903052640248494, 0.8988610692771084, 0.9000214726091867, 0.8923707525414157, 0.8992581654743976, 0.8953416203878012, 0.8897866858057228, 0.8912927099021084, 0.8959210866905121, 0.8972035603350903, 0.8984036732868976, 0.8897660956325302, 0.8958093114646084, 0.8959313817771084, 0.9005185782191265], "accuracy_test": 0.6703663105867347, "start": "2016-01-24 07:57:57.770000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0, 211.0, 212.0, 213.0, 214.0, 215.0, 216.0, 217.0, 218.0, 219.0, 220.0, 221.0, 222.0, 223.0, 224.0, 225.0, 226.0, 227.0, 228.0, 229.0, 230.0, 231.0, 232.0, 233.0, 234.0, 235.0, 236.0, 237.0, 238.0, 239.0, 240.0, 241.0, 242.0, 243.0, 244.0, 245.0, 246.0, 247.0, 248.0, 249.0, 250.0, 251.0, 252.0, 253.0, 254.0, 255.0, 256.0, 257.0, 258.0, 259.0, 260.0, 261.0, 262.0, 263.0, 264.0, 265.0, 266.0, 267.0, 268.0, 269.0, 270.0, 271.0, 272.0, 273.0, 274.0, 275.0, 276.0, 277.0, 278.0, 279.0, 280.0, 281.0, 282.0, 283.0, 284.0, 285.0, 286.0, 287.0, 288.0, 289.0], "accuracy_train_last": 0.9944196428571429, "batch_size_eval": 1024, "accuracy_train_std": [0.016002797483572105, 0.01471950185478627, 0.019279552385418997, 0.01912869945488652, 0.017004677363712542, 0.018013058380036837, 0.017126790481963614, 0.019247279376433033, 0.01777061626328751, 0.017691221741251744, 0.016839064781071028, 0.018078659124026845, 0.01756164787999888, 0.014705282956583385, 0.019434071716375078, 0.01718997253594906, 0.01667745799862457, 0.017894997348379862, 0.017124507105937527, 0.018117652265410122, 0.018075519856646894, 0.01682673810511682, 0.017303938822477768, 0.016898693372425277, 0.015291141614174615, 0.014811156481941222, 0.015773983402577914, 0.01643093144753039, 0.01617871502078293, 0.014670744285451478, 0.015022480875939812, 0.014412727596179826, 0.014624706277209408, 0.013429836593361357, 0.013706592486801126, 0.012275225094003351, 0.012963681534421397, 0.014012868076823877, 0.013650596449795493, 0.01281764327768201, 0.01191365891416138, 0.01152774256958563, 0.012929359420257103, 0.01279152431352741, 0.011638883687959043, 0.01163768175434913, 0.012957740052913731, 0.0119833948735411, 0.010964627433284687, 0.011169073370780505, 0.011276179569293452, 0.011731335534250308, 0.012747678816916186, 0.010514073410333275, 0.011150374989516725, 0.011119902640542492, 0.011076685330328826, 0.011469678858882528, 0.010222542290349832, 0.010367361312533067, 0.009377606400429941, 0.009274180149299076, 0.009886054482598439, 0.009113542521811796, 0.008167305209584627, 0.010717416231775584, 0.008473856692955377, 0.009432391315126553, 0.008807954732598537, 0.008811792501296321, 0.009526784750013548, 0.008278724638586414, 0.01004952542409991, 0.007574108396029459, 0.008414505452858498, 0.008972229375603078, 0.00924485112097927, 0.009891600086357678, 0.007223204913059227, 0.007999971086349965, 0.008753545459774244, 0.0075604171997401695, 0.008208796191821514, 0.00797060347432407, 0.008714538018096919, 0.009561578587729503, 0.008715550478988976, 0.007908592942089633, 0.007177783709863503, 0.008623243728131827, 0.008412851646142461, 0.0075540978198961025, 0.007923019339331653, 0.007705002629593762, 0.006999899366561433, 0.0070017177822913265, 0.00715582039678525, 0.00851596923417948, 0.006313896741970978, 0.007292588791097318, 0.008740904678917904, 0.008361161094164257, 0.005196152388407583, 0.006174063938891016, 0.006969886474582049, 0.0075807697128610334, 0.007596080876224886, 0.00706233091762263, 0.006391232792214369, 0.006777084679632961, 0.006304198397151229, 0.007756558423907225, 0.005106494758701684, 0.0057064364396568696, 0.006431380100707667, 0.006113544438103079, 0.006217675731860223, 0.0067412373775715875, 0.006531777008696725, 0.006188844386855463, 0.005956536184965848, 0.006310388618583502, 0.006038989360427851, 0.005451584971690344, 0.005793211372541757, 0.007205740555347389, 0.006991896834893459, 0.004952574911497723, 0.005562861537273267, 0.005294030442628146, 0.006226103936052157, 0.005115172262616528, 0.004729426160376759, 0.00653941643899983, 0.005266954950463524, 0.006262727283018393, 0.004823016053530731, 0.006849684761726923, 0.004681851382549583, 0.005198662420538514, 0.0052706645571267445, 0.0049534361392159585, 0.004033646455237731, 0.004832471351559988, 0.005214330347998806, 0.004822947052614708, 0.006057519176449464, 0.0056123348017727585, 0.005476344602052444, 0.0049690729494668625, 0.005218803554844449, 0.005775451896511839, 0.004889121295789502, 0.005930991436692101, 0.0049723055146598805, 0.0056540473396184295, 0.004851190600104539, 0.005565716487999746, 0.004718096669644798, 0.0045435869663014395, 0.0050877647053196, 0.005543384162099203, 0.004828367210583643, 0.0054290310381092406, 0.004216732850996087, 0.005343355469872998, 0.00448625497090702, 0.003920937104449497, 0.003698176390520489, 0.003513360633840049, 0.004621788109360012, 0.0040420572806772114, 0.005661334965184501, 0.0035606849557420516, 0.0043694737223293565, 0.004922510815953502, 0.004609897049854692, 0.004916586877279445, 0.0034618579698135277, 0.0067214498359433, 0.005629678248976879, 0.0042230318979460816, 0.0035924179205323666, 0.004445576736756944, 0.004320680632942875, 0.005891151301681325, 0.0037434173730465227, 0.005324184618788906, 0.004066884010712042, 0.00408023252817265, 0.004696100210532998, 0.0042465296113793575, 0.0048501511802575975, 0.003287270355853966, 0.004932785344382252, 0.0036368330326399323, 0.004730783617620349, 0.004228137786783955, 0.004717883449057402, 0.0033599585405445637, 0.0035105177609473168, 0.0037525468552674778, 0.003404656260271298, 0.004410121099804782, 0.003490202491808183, 0.0035289537259306403, 0.004269784650500288, 0.005202309375082236, 0.0036591961785520616, 0.0033770879780737036, 0.004347779587680642, 0.004927560334430657, 0.003397593828545988, 0.003096075248960506, 0.004383496887239197, 0.0034233424210096614, 0.0037589825851862386, 0.0034295747893400845, 0.004324526762486113, 0.004306326499001787, 0.003864226638865742, 0.003817686611710639, 0.004083254680771889, 0.003976405656497492, 0.003830784100933588, 0.0033816329915732596, 0.0028236919611719243, 0.0027288572982452444, 0.004143485632636386, 0.0042309895076970624, 0.003585331211845648, 0.002853639611401961, 0.004124654585489355, 0.0038802129033738437, 0.004129414535608993, 0.0037963323385967313, 0.005263688437445047, 0.0036420496415597707, 0.002320181628482007, 0.002801835603934541, 0.003960649367748792, 0.003695984552853398, 0.0030105065680133022, 0.003976208419476507, 0.005241913163471604, 0.00447982022913049, 0.003906472225697426, 0.003742186272976863, 0.0035621327559919856, 0.0033216594663576823, 0.0033422089530802074, 0.0038990944457503746, 0.00402116319921406, 0.0028700849500552807, 0.003563850180037095, 0.003128679829897641, 0.003242898904534534, 0.004768756643857134, 0.002499690759383455, 0.0037338733567979583, 0.003323487394118218, 0.0035774059834294838, 0.00498875944157532, 0.003350151349391346, 0.0026718107253859186, 0.0030554632415050564, 0.0033481540679653112, 0.003046286333458616, 0.003103095891964655, 0.004933402122629941, 0.0029425766405424807, 0.002437571837621866, 0.0027473185371840137, 0.004056766855875328, 0.003235300653328389, 0.002517611176272166, 0.003398399942106347, 0.0028592737289016763, 0.004406721866261332, 0.003277639726648751, 0.003103171522876033, 0.0036597030532202614, 0.0038438010748693243, 0.0029858813160207076, 0.0032434230281971166, 0.0026083154133891666, 0.004377390100473295, 0.0033282812159958638, 0.0033984861510493263, 0.0026726065932851585], "accuracy_test_std": 0.013699562960040395, "error_valid": [0.4292168674698795, 0.3176401896649097, 0.29698824595256024, 0.2546901473079819, 0.23779296875, 0.21312299981174698, 0.20921674981174698, 0.1948315723832832, 0.18405849962349397, 0.18477033132530118, 0.17492234563253017, 0.16320359563253017, 0.16713043580572284, 0.16678481504141573, 0.15928705054593373, 0.15921645566641573, 0.15512636483433728, 0.15282761907003017, 0.1508024284638554, 0.1525422980986446, 0.14142360457454817, 0.14963320077183728, 0.13933811417545183, 0.14353998023343373, 0.13663197712725905, 0.14224721150225905, 0.13067112198795183, 0.13430234610316272, 0.13515683829066272, 0.1309358527861446, 0.12978574454066272, 0.14024408179593373, 0.13312282332454817, 0.12537062311746983, 0.12819883047816272, 0.12906361775225905, 0.12391607445406627, 0.13480092243975905, 0.12916509789156627, 0.12581772402108427, 0.13473032756024095, 0.12510589231927716, 0.12672369164156627, 0.13806593561746983, 0.1253603280308735, 0.12905332266566272, 0.1288297722138554, 0.1272428581513554, 0.11951124811746983, 0.11940976797816272, 0.12134230280496983, 0.12598097467996983, 0.1261442253388554, 0.12476027155496983, 0.11795521931475905, 0.12384547957454817, 0.12126141283885539, 0.12343808829066272, 0.11872735080948793, 0.12722226797816272, 0.12380429922816272, 0.11470932558358427, 0.11697865681475905, 0.12231886530496983, 0.11668304075677716, 0.12725315323795183, 0.12585890436746983, 0.12713108292545183, 0.11867734610316272, 0.11843320547816272, 0.12049810570406627, 0.11768019342996983, 0.12442494587725905, 0.11452695547816272, 0.11779196865587349, 0.12134230280496983, 0.11775078830948793, 0.12844297110316272, 0.11313270660768071, 0.11779196865587349, 0.12041721573795183, 0.12111875235316272, 0.11154579254518071, 0.11552410815135539, 0.11824936464608427, 0.12069077089608427, 0.1173742822853916, 0.11716102692018071, 0.11460784544427716, 0.11404749858810237, 0.11857586596385539, 0.12000982445406627, 0.11874794098268071, 0.12204383942018071, 0.11520790192018071, 0.11649037556475905, 0.11990834431475905, 0.12129082737198793, 0.11928769766566272, 0.12480145190135539, 0.12037603539156627, 0.1252382577183735, 0.11864646084337349, 0.11595061888177716, 0.11789344879518071, 0.11381365304969882, 0.11721250235316272, 0.11319300640060237, 0.11135312735316272, 0.11924651731927716, 0.10923675169427716, 0.11550204725150603, 0.11304152155496983, 0.10995887848268071, 0.11315329678087349, 0.11121046686746983, 0.11337684723268071, 0.11890089655496983, 0.11303122646837349, 0.11365187311746983, 0.11462843561746983, 0.11537115257906627, 0.10795427804969882, 0.10819841867469882, 0.11396660862198793, 0.12087461172816272, 0.11462843561746983, 0.11287827089608427, 0.11129135683358427, 0.10997946865587349, 0.1113928369728916, 0.11192229856927716, 0.1131018213478916, 0.11290915615587349, 0.10725274143448793, 0.11489316641566272, 0.11332684252635539, 0.11231939476656627, 0.10973532803087349, 0.11227821442018071, 0.10690712067018071, 0.11068100527108427, 0.1044348291603916, 0.10993828830948793, 0.1078527979103916, 0.10232874858810237, 0.10847344455948793, 0.10725274143448793, 0.10647913921310237, 0.10871758518448793, 0.11057952513177716, 0.10985739834337349, 0.10985739834337349, 0.11022360928087349, 0.10307146554969882, 0.11388571865587349, 0.11063982492469882, 0.11138254188629515, 0.10994858339608427, 0.1106604150978916, 0.11210466867469882, 0.11916562735316272, 0.10388477739081325, 0.1090220256024097, 0.10539080148719882, 0.1060923381024097, 0.1023081584149097, 0.10468926487198793, 0.10607174792921681, 0.10575701242469882, 0.10404802804969882, 0.11035597467996983, 0.12078342667545183, 0.1025522990399097, 0.10415980327560237, 0.10613351844879515, 0.10894113563629515, 0.10567612245858427, 0.10338767178087349, 0.11815817959337349, 0.1111486963478916, 0.10464808452560237, 0.10074183452560237, 0.1061438135353916, 0.10368328783885539, 0.10676446018448793, 0.10445541933358427, 0.10759836219879515, 0.10323471620858427, 0.10630706419427716, 0.10880877023719882, 0.10125070594879515, 0.12173939900225905, 0.09954172157379515, 0.10665268495858427, 0.10543198183358427, 0.11081337067018071, 0.10383477268448793, 0.10937941217996983, 0.1062658838478916, 0.1040686182228916, 0.10532020660768071, 0.10979709855045183, 0.10917498117469882, 0.10239051910768071, 0.10342738140060237, 0.10319353586219882, 0.11021331419427716, 0.1060217432228916, 0.10025355327560237, 0.09905344032379515, 0.11169874811746983, 0.10257288921310237, 0.10261406955948793, 0.11080307558358427, 0.10081242940512047, 0.10576730751129515, 0.1024817041603916, 0.10417009836219882, 0.10403773296310237, 0.1068762354103916, 0.10262436464608427, 0.10501429546310237, 0.10383477268448793, 0.10922645660768071, 0.10141395660768071, 0.10185076242469882, 0.10494370058358427, 0.10217579301581325, 0.10787338808358427, 0.10247140907379515, 0.09875782426581325, 0.1058481974774097, 0.10209490304969882, 0.10257288921310237, 0.10408920839608427, 0.11026331890060237, 0.10618499388177716, 0.09850338855421681, 0.09524867046310237, 0.1057776025978916, 0.1004771037274097, 0.10060946912650603, 0.10483192535768071, 0.11035597467996983, 0.10629676910768071, 0.10774102268448793, 0.09817835796310237, 0.10405832313629515, 0.10354945171310237, 0.1056040568524097, 0.10143307605421681, 0.10373329254518071, 0.10537021131400603, 0.10473926957831325, 0.09900196489081325, 0.10530991152108427, 0.10464808452560237, 0.10238022402108427, 0.10454660438629515, 0.1006506494728916, 0.10551287179969882, 0.10685564523719882, 0.10281702983810237, 0.10168751176581325, 0.10839255459337349, 0.10367152202560237, 0.10051828407379515, 0.10524814100150603, 0.10611292827560237, 0.10225815370858427, 0.1024817041603916, 0.10795427804969882, 0.10503488563629515, 0.1022375635353916, 0.09694735975150603, 0.1011389307228916, 0.09997852739081325, 0.10762924745858427, 0.10074183452560237, 0.10465837961219882, 0.11021331419427716, 0.1087072900978916, 0.10407891330948793, 0.1027964396649097, 0.10159632671310237, 0.11023390436746983, 0.1041906885353916, 0.1040686182228916, 0.09948142178087349], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-3, 3], "momentum": 0.5926662912708968, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.00032389530067667295, "patience_threshold": 1, "do_flip": true, "batch_size": 32, "optimization": "adam", "nb_data_augmentation": 2, "learning_rate_decay_method": "none", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 1.576718354629743e-07, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.056083387809810537}, "accuracy_valid_max": 0.9047513295368976, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-3, 3)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.9005185782191265, "loss_train": [1.5173437595367432, 1.0705012083053589, 0.8752854466438293, 0.7613524198532104, 0.6863947510719299, 0.630090057849884, 0.5838245749473572, 0.5469852089881897, 0.5112605094909668, 0.4836840033531189, 0.45791444182395935, 0.43351495265960693, 0.41252270340919495, 0.392872154712677, 0.37584492564201355, 0.35857051610946655, 0.3430390954017639, 0.32757651805877686, 0.3139455318450928, 0.3018835484981537, 0.292152464389801, 0.2770332396030426, 0.2660222053527832, 0.2600417733192444, 0.24913901090621948, 0.23933488130569458, 0.23532716929912567, 0.22450953722000122, 0.21845993399620056, 0.2111193835735321, 0.2032097578048706, 0.19685930013656616, 0.19025181233882904, 0.186000257730484, 0.1813317984342575, 0.17456631362438202, 0.1676083505153656, 0.16338498890399933, 0.16226407885551453, 0.1565520167350769, 0.1553695648908615, 0.14913685619831085, 0.14875632524490356, 0.1414601355791092, 0.14150843024253845, 0.13490186631679535, 0.13408781588077545, 0.12978112697601318, 0.1291189342737198, 0.12527333199977875, 0.1214919239282608, 0.11877062171697617, 0.11745214462280273, 0.11560513079166412, 0.11247839033603668, 0.11032216995954514, 0.10757932811975479, 0.1065521240234375, 0.10281971096992493, 0.10241545736789703, 0.1033463403582573, 0.09975774586200714, 0.09589502215385437, 0.09668347984552383, 0.0938495546579361, 0.09225782006978989, 0.09156545251607895, 0.0908358246088028, 0.09023146331310272, 0.0864529013633728, 0.08885874599218369, 0.08473680913448334, 0.08376605808734894, 0.08313670009374619, 0.08122818171977997, 0.08146263659000397, 0.07667012512683868, 0.08089274913072586, 0.0758100301027298, 0.07628576457500458, 0.07487290352582932, 0.0741516649723053, 0.07214213907718658, 0.07199458032846451, 0.07431352138519287, 0.06935485452413559, 0.0679626390337944, 0.07248173654079437, 0.06844891607761383, 0.06677085161209106, 0.06659407168626785, 0.06892244517803192, 0.06574267894029617, 0.06348513811826706, 0.06376414000988007, 0.06476248800754547, 0.0661194920539856, 0.06141262874007225, 0.06374680250883102, 0.059068839997053146, 0.060925185680389404, 0.05904389172792435, 0.05947563052177429, 0.06187748908996582, 0.05852852389216423, 0.062209226191043854, 0.054514795541763306, 0.055311985313892365, 0.06035971641540527, 0.05208441987633705, 0.05999519303441048, 0.05501673370599747, 0.05662880465388298, 0.05228129401803017, 0.053955502808094025, 0.05617870017886162, 0.05287676677107811, 0.05417831242084503, 0.05244574323296547, 0.05068347603082657, 0.0566263422369957, 0.05145252123475075, 0.050474707037210464, 0.055186592042446136, 0.04725309833884239, 0.05041379854083061, 0.051228251308202744, 0.04693466052412987, 0.051296018064022064, 0.04735643416643143, 0.05116705223917961, 0.04926107078790665, 0.04915730655193329, 0.047192852944135666, 0.04458422586321831, 0.04817316681146622, 0.043880853801965714, 0.051065895706415176, 0.04300002381205559, 0.04643240198493004, 0.045061953365802765, 0.04499734193086624, 0.04559515789151192, 0.04490182176232338, 0.04162019491195679, 0.04627294838428497, 0.0452168770134449, 0.04367837682366371, 0.04421868547797203, 0.04430098459124565, 0.04262378811836243, 0.04218955710530281, 0.04412667453289032, 0.04482917860150337, 0.04279215633869171, 0.04220797121524811, 0.040357064455747604, 0.044234760105609894, 0.043593354523181915, 0.04132263362407684, 0.038347337394952774, 0.04548387974500656, 0.039703402668237686, 0.04295867308974266, 0.04191828519105911, 0.040107548236846924, 0.04197758436203003, 0.03881332650780678, 0.04116188362240791, 0.04020501300692558, 0.03872733935713768, 0.04144986346364021, 0.0393587201833725, 0.040156010538339615, 0.03840716555714607, 0.038479484617710114, 0.03908173367381096, 0.04114671051502228, 0.036915890872478485, 0.040051043033599854, 0.03753820061683655, 0.03754048049449921, 0.04189511388540268, 0.03899514302611351, 0.03990335762500763, 0.036545656621456146, 0.03814262896776199, 0.03676779568195343, 0.0372786745429039, 0.03787814825773239, 0.03692203760147095, 0.0372345857322216, 0.034886155277490616, 0.038192737847566605, 0.03667191043496132, 0.03562817722558975, 0.03774665668606758, 0.03745076432824135, 0.0368339829146862, 0.03680548444390297, 0.03773688152432442, 0.03319547325372696, 0.03593099117279053, 0.03475915268063545, 0.03724563121795654, 0.03772155940532684, 0.032824087888002396, 0.03923877328634262, 0.03418203070759773, 0.03564631566405296, 0.03695108741521835, 0.03356574848294258, 0.03210526704788208, 0.038304176181554794, 0.03331545367836952, 0.03513981029391289, 0.034699369221925735, 0.03330568969249725, 0.03550412505865097, 0.035298287868499756, 0.03341129422187805, 0.03719458356499672, 0.03280310705304146, 0.03175460547208786, 0.03574632108211517, 0.03305579721927643, 0.03388916328549385, 0.03513422980904579, 0.03255438059568405, 0.03632610663771629, 0.03163661062717438, 0.03522275760769844, 0.033203382045030594, 0.03395320102572441, 0.02913341484963894, 0.03432011231780052, 0.0325443334877491, 0.03470635414123535, 0.03176981955766678, 0.03153638914227486, 0.03438698872923851, 0.033454976975917816, 0.03248574957251549, 0.03442464396357536, 0.03146304190158844, 0.032469864934682846, 0.03546888753771782, 0.030844511464238167, 0.031393762677907944, 0.03462187573313713, 0.031779635697603226, 0.0329902321100235, 0.030826807022094727, 0.03399796411395073, 0.03197832405567169, 0.03162609785795212, 0.03297663480043411, 0.030437178909778595, 0.032864585518836975, 0.030199509114027023, 0.03278747946023941, 0.03030342049896717, 0.03044864907860756, 0.03461179509758949, 0.02962818183004856, 0.030451582744717598, 0.03154577314853668, 0.0326533168554306, 0.030692975968122482, 0.03274812176823616, 0.031551770865917206, 0.028170710429549217, 0.0338493175804615, 0.03096996247768402, 0.031321484595537186, 0.03011360764503479, 0.03238751366734505, 0.030273661017417908, 0.03085700236260891, 0.032982807606458664, 0.02993088960647583, 0.030000869184732437, 0.0317377932369709, 0.03148101642727852, 0.031022941693663597, 0.029506541788578033, 0.0298143420368433, 0.030691681429743767, 0.030163872987031937, 0.030897894874215126], "accuracy_train_first": 0.5804292404946475, "model": "residualv3", "loss_std": [0.34796783328056335, 0.19723975658416748, 0.18514318764209747, 0.17685727775096893, 0.1710539609193802, 0.1661294400691986, 0.16103920340538025, 0.15702509880065918, 0.14932087063789368, 0.14877036213874817, 0.1426173746585846, 0.13898637890815735, 0.13502919673919678, 0.13281938433647156, 0.129409059882164, 0.12472718209028244, 0.12151265144348145, 0.11719410866498947, 0.11610889434814453, 0.11073771864175797, 0.11254667490720749, 0.11053052544593811, 0.10403794795274734, 0.10192641615867615, 0.09821084141731262, 0.0966179370880127, 0.09830387681722641, 0.09374632686376572, 0.0914539247751236, 0.0886765867471695, 0.08961374312639236, 0.08612783253192902, 0.08170696347951889, 0.08173394203186035, 0.08167223632335663, 0.07902637124061584, 0.08041652292013168, 0.07702980935573578, 0.07505450397729874, 0.07371151447296143, 0.07508431375026703, 0.07162638008594513, 0.07149970531463623, 0.06694146245718002, 0.07023698836565018, 0.06642455607652664, 0.06558653712272644, 0.0647997334599495, 0.06485438346862793, 0.06312981992959976, 0.06285016238689423, 0.060495611280202866, 0.05872727185487747, 0.05920874699950218, 0.05769440531730652, 0.05770496651530266, 0.05586254596710205, 0.054898668080568314, 0.05540258437395096, 0.054828252643346786, 0.05482398718595505, 0.053463105112314224, 0.05155962333083153, 0.05271225422620773, 0.05077097564935684, 0.05066687613725662, 0.05123360827565193, 0.049165599048137665, 0.04967186227440834, 0.048420682549476624, 0.049429040402173996, 0.049295730888843536, 0.04817548766732216, 0.046742361038923264, 0.048484500497579575, 0.05037633329629898, 0.04434531554579735, 0.04904694855213165, 0.043455932289361954, 0.0470222570002079, 0.04373762011528015, 0.04277954623103142, 0.04311184585094452, 0.04448072984814644, 0.04492128640413284, 0.04199093207716942, 0.04172208160161972, 0.04316665604710579, 0.04156108945608139, 0.04147997125983238, 0.04239507392048836, 0.04465353861451149, 0.04264689236879349, 0.03929121419787407, 0.04060295224189758, 0.041755128651857376, 0.0421854592859745, 0.03861025720834732, 0.04200782626867294, 0.037492234259843826, 0.038266487419605255, 0.03656676411628723, 0.036372508853673935, 0.03945179283618927, 0.03833400830626488, 0.039439842104911804, 0.03437640890479088, 0.03499384969472885, 0.04140615090727806, 0.034590788185596466, 0.040001414716243744, 0.03528823330998421, 0.03777795284986496, 0.03393645957112312, 0.0362064465880394, 0.03845856338739395, 0.036388929933309555, 0.03804835304617882, 0.03510582074522972, 0.03414415568113327, 0.039924897253513336, 0.034749388694763184, 0.03397928550839424, 0.03717569261789322, 0.033625226467847824, 0.03492653742432594, 0.035261787474155426, 0.03381457179784775, 0.0371505543589592, 0.031261082738637924, 0.03468017280101776, 0.03237123414874077, 0.033378396183252335, 0.03489159420132637, 0.03141385316848755, 0.033563900738954544, 0.031199948862195015, 0.036844734102487564, 0.029858652502298355, 0.031484633684158325, 0.032815009355545044, 0.031694039702415466, 0.031878042966127396, 0.031539350748062134, 0.028628451749682426, 0.03406074643135071, 0.0322783887386322, 0.030859054997563362, 0.03048720583319664, 0.03190937265753746, 0.030860934406518936, 0.03050546906888485, 0.03092768043279648, 0.03235745429992676, 0.031829237937927246, 0.029095394536852837, 0.02801024541258812, 0.030975989997386932, 0.030737536028027534, 0.030350124463438988, 0.027219258248806, 0.03352065756917, 0.028793804347515106, 0.031941644847393036, 0.03198391944169998, 0.028190715238451958, 0.028784550726413727, 0.029088586568832397, 0.03024616651237011, 0.03143106400966644, 0.02789192833006382, 0.030973926186561584, 0.029342856258153915, 0.03090202435851097, 0.025813642889261246, 0.027547460049390793, 0.029611770063638687, 0.030498061329126358, 0.02620765194296837, 0.029581574723124504, 0.02791200391948223, 0.027758289128541946, 0.029546011239290237, 0.028402812778949738, 0.03030313178896904, 0.025903841480612755, 0.027450721710920334, 0.027517961338162422, 0.026415126398205757, 0.027417587116360664, 0.02755044959485531, 0.02541101910173893, 0.025341855362057686, 0.029637020081281662, 0.025733105838298798, 0.026224825531244278, 0.02758808434009552, 0.02667653188109398, 0.028068119660019875, 0.02697448618710041, 0.028999201953411102, 0.02243855968117714, 0.025733526796102524, 0.025059308856725693, 0.02674608863890171, 0.028076911345124245, 0.024462832137942314, 0.029035625979304314, 0.025202875956892967, 0.026391932740807533, 0.02689850516617298, 0.024232828989624977, 0.02393026277422905, 0.02933654561638832, 0.024269355461001396, 0.02548673003911972, 0.02765519917011261, 0.023270023986697197, 0.02589115872979164, 0.026666851714253426, 0.02432187832891941, 0.02722117304801941, 0.025311337783932686, 0.020986909046769142, 0.027025165036320686, 0.023743826895952225, 0.025926735252141953, 0.02548738196492195, 0.023827841505408287, 0.027964962646365166, 0.021928614005446434, 0.026543041691184044, 0.025203999131917953, 0.024920092895627022, 0.01965251937508583, 0.024799726903438568, 0.024369895458221436, 0.025632135570049286, 0.0231911800801754, 0.023132357746362686, 0.02546692080795765, 0.02413126826286316, 0.024587910622358322, 0.02561427652835846, 0.02303249016404152, 0.023593993857502937, 0.02946954220533371, 0.02189997397363186, 0.021879572421312332, 0.02709469385445118, 0.023829421028494835, 0.023910388350486755, 0.021459782496094704, 0.024778179824352264, 0.02301562763750553, 0.024139344692230225, 0.024339715018868446, 0.02138179913163185, 0.02531278505921364, 0.02082125097513199, 0.02317879907786846, 0.021142365410923958, 0.023047348484396935, 0.025732768699526787, 0.02115103416144848, 0.021392859518527985, 0.023522168397903442, 0.022975455969572067, 0.022142469882965088, 0.0228282380849123, 0.02281889319419861, 0.0180978886783123, 0.02580377273261547, 0.02353743463754654, 0.023888880386948586, 0.02137574553489685, 0.025130918249487877, 0.020656278356909752, 0.021196488291025162, 0.027332646772265434, 0.02062249556183815, 0.0204815324395895, 0.02361295185983181, 0.02269657887518406, 0.023462915793061256, 0.020187493413686752, 0.0212160162627697, 0.0211423859000206, 0.021587897092103958, 0.022190535441040993]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:17 2016", "state": "available"}], "summary": "dda846f9e568530f93eaa99fbb57e7ec"}