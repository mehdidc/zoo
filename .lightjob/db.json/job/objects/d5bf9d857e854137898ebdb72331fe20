{"content": {"hp_model": {"f0": 32, "f1": 16, "f2": 32, "f3": 32, "nonlin": "very_leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.019035475009118844, 0.019998306560348483, 0.012282476384846001, 0.011222063047820467, 0.007693997952419059, 0.007304945033949246, 0.00883217082126679, 0.009087617036042515, 0.005814380518549305, 0.011951480542761315, 0.007374318492696697, 0.01091097909985108, 0.009327453275737715, 0.012092097322720641, 0.009509088034457876, 0.011421892741861252, 0.012870191747282312, 0.015026214880102843, 0.01195424277628827, 0.01071496913996538, 0.012810590778486232, 0.012969761855131295, 0.015241059092668225, 0.012798998454666383, 0.014730634614636869, 0.013473915596566624, 0.014618557475949302, 0.01703707781182614, 0.017334310312715123, 0.01663894186542355, 0.013897212064359322, 0.016164829877928683, 0.016039587136254335, 0.01954916156126426, 0.017904571893311915, 0.015023260717590884, 0.016256264672283825, 0.017034501142904612, 0.02002915111276047, 0.014862829160056611, 0.018235552981944823, 0.016215606985061273, 0.015554699930520161, 0.018183333286788873, 0.014084395916297144, 0.015370785367244044, 0.013574791345466616, 0.01506090593593456, 0.013541665754774647, 0.011664701299518762, 0.013982819428616431, 0.014422174166128052, 0.013703942772760956, 0.013367960628759248, 0.013776592242089719, 0.013690416112812255, 0.010770000159356679, 0.010849022112161629, 0.013841123819618235, 0.01221724046866149, 0.016099560371975085, 0.012191128449357429, 0.013469353459077746, 0.014405296045488968, 0.013773539116324974, 0.01182390876245723, 0.013288201806988474, 0.013923126128747505, 0.012072775075686788, 0.011842030115637706, 0.013032650477707654, 0.011872486460190871, 0.012041970378311936, 0.01295300052272947, 0.012081585330051284, 0.011970458748626022, 0.011767821726515922, 0.012315674989330316, 0.012441946883364372, 0.01189138882948731, 0.011032051135664397, 0.01203256684948423, 0.011431830300908892, 0.012171401132869412, 0.011694976825600474, 0.0118020828279781, 0.011960495979408868, 0.01274636080929246, 0.012242780906884703, 0.013796321337353796, 0.011597475643190793, 0.008938295972155568, 0.013600683340328658, 0.0117068833048105, 0.010642124327235627, 0.009562431623524624, 0.0091049501592236, 0.010014507627648965, 0.00924020701676871, 0.011233972061530243, 0.010949669595818437, 0.01135025834015737, 0.01263373442328217, 0.011029567845629704, 0.01102860871540215, 0.01130391352082829, 0.012026051113195801, 0.011574666254461876, 0.009534456620554821, 0.010108717509761632, 0.010894587068630992, 0.01002160428363642, 0.010741540614506068, 0.011464844737709078, 0.010159599095067, 0.012765976510261299, 0.011381723073665804, 0.009436261885299077, 0.00910132131082722, 0.009431823598563663, 0.009609233667877028, 0.01103327953399575, 0.009174896001161862, 0.009040708806262747, 0.008958412168693947, 0.009654168829573453, 0.011680975815455455, 0.011629404512280202, 0.010199980301018617, 0.009859697867353802, 0.009293573645601202, 0.009205588074453801, 0.009176121379086745, 0.008535426361943859, 0.009182094109199285, 0.013520244168800237, 0.010059061175260356, 0.011565165929335914, 0.012216672574289454, 0.008253971150972766, 0.008365982287265, 0.00938317336724149, 0.00872221409207674, 0.01105319479402564, 0.01101629509948007, 0.009382670698462976, 0.011282118076711199, 0.008783094875957236, 0.0068314642109850695, 0.010445865560479583, 0.01117285544539129, 0.011600629366177605, 0.012329442528470791, 0.009276336476048117, 0.010127323800592546, 0.008090717312321695, 0.00946953361060251, 0.00972993052707897, 0.012557856807902102, 0.013057778103551533, 0.011306644945159063, 0.011808822156542857, 0.012396520741559102, 0.012596739986388931, 0.008917042280948867, 0.00883437089022561, 0.010000558868818852, 0.012931995622170449, 0.009620534575164937, 0.01228293909873372, 0.011268153563606631, 0.011650897182991693, 0.011090578771877312, 0.008801294293186376, 0.009741370305996748, 0.013223470054304324, 0.010614468021333664, 0.009615852924763075, 0.011239773751301552, 0.010714421652999895, 0.01271747502534567, 0.012500540432603758, 0.01046629561338188], "moving_avg_accuracy_train": [0.03987803242663343, 0.08137515068406237, 0.13787320368736156, 0.19638391620668372, 0.2511631198702531, 0.3025523867984179, 0.3510458467562192, 0.3967103708384415, 0.43793646989224044, 0.4755676957227838, 0.5109237860619118, 0.5432648844075478, 0.5733716508578968, 0.6009512634691355, 0.6263608530286912, 0.6498431066250929, 0.6710514314773031, 0.6908968863073782, 0.7094692190925226, 0.7260913126467716, 0.7411511782444052, 0.7556071068739145, 0.7687221824833577, 0.7808603917163341, 0.7923683563283848, 0.8030299747780029, 0.8121720634136208, 0.8208835020404204, 0.8293887893640638, 0.8369480366100587, 0.8438166435422144, 0.8500215691716123, 0.8557175012344714, 0.8612693423231875, 0.8664891414911087, 0.8718261964207631, 0.8765180829098699, 0.8804035260262288, 0.8845864158273988, 0.8882371564544227, 0.8918948107794492, 0.8953678449862035, 0.8981961730175757, 0.9013949269148307, 0.9039459954890361, 0.9063884055320022, 0.9091887160147009, 0.9118927543027195, 0.9140193249238042, 0.9161123109899325, 0.9181635173101899, 0.920300246599612, 0.9223650649398354, 0.9242023309114757, 0.9261348881430949, 0.9279207286765614, 0.9297327063983756, 0.9310425437146566, 0.9324422143385767, 0.9336298743358282, 0.9350613473523653, 0.9363637321065622, 0.9378730610115391, 0.938866408662923, 0.940120639370551, 0.9411751503919676, 0.9422729837862334, 0.9429425245029865, 0.9440844375254176, 0.9451145204432339, 0.9456370912740489, 0.9466048787205643, 0.9472829361200564, 0.9481815783296178, 0.9490089575086992, 0.9496907476543778, 0.9507785449473564, 0.9507508812229696, 0.9516351170555453, 0.9523937629727298, 0.9535089859279485, 0.9539988647495593, 0.9547651683758861, 0.9553455596455326, 0.9560678385370147, 0.9568992150976727, 0.9574199138583004, 0.9579954635392847, 0.958683013871172, 0.9591368317508506, 0.9597614706330283, 0.9603562698079591, 0.96108911856775, 0.9619022143706281, 0.9623408876479432, 0.9629309339998894, 0.9636479876214028, 0.9641887041843364, 0.9642942409791272, 0.9644238489848506, 0.9649009303042966, 0.9655070508501404, 0.9661037486640282, 0.9658595266965272, 0.966639468816234, 0.9672089553394647, 0.9677702131889162, 0.9682684779534502, 0.9686727023653311, 0.9692993182491376, 0.9696075061755158, 0.9699848567080657, 0.9704150448444757, 0.9706464652458254, 0.9709105471784687, 0.971264478358324, 0.9716015815618512, 0.971898143193872, 0.9720579476221777, 0.9725621336243102, 0.9730019861821911, 0.9731490986104836, 0.9733581936578425, 0.9737649071397418, 0.9738496783651365, 0.9743235368656015, 0.9747290471279154, 0.974752209488998, 0.9750125459413532, 0.9750655592389676, 0.9744902034235057, 0.9749185745597911, 0.9752807849967153, 0.9756534576102314, 0.9761328419444925, 0.976617838365584, 0.9770333727564618, 0.9771050122653764, 0.9775322831353227, 0.977775064938531, 0.9781075008530851, 0.9782764127452129, 0.9784750806195934, 0.9786840004946035, 0.9789696485832937, 0.9791942158286004, 0.979431095435063, 0.9796628161737181, 0.9799365411028118, 0.9800875624378056, 0.9802770682083753, 0.9804242277185179, 0.9807125283455217, 0.9809185565360249, 0.9812295038943734, 0.981362872141887, 0.981455001778935, 0.9817145576641644, 0.9818551520084899, 0.9818864279148299, 0.9820540130614699, 0.9818329961280158, 0.9820455240807365, 0.9812650312310792, 0.9815831117008561, 0.9818137247475018, 0.9819862911108839, 0.9822323176903178, 0.9824373934725041, 0.9826638864526809, 0.9828607186395926, 0.9831634256435274, 0.9832451997446878, 0.983135073630961, 0.9834591011631214, 0.9836554669385129, 0.9837367929375372, 0.9840262251759448, 0.9842658239000447, 0.9844790655052875, 0.9846478035595484, 0.9847065897583647, 0.9849875061159077], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 397846231, "moving_var_accuracy_train": [0.014312317231976651, 0.028379182921818364, 0.05426953456810897, 0.07965411242696693, 0.09869555157040352, 0.11259370721209072, 0.12249887741899151, 0.1290162285139951, 0.13141092685133834, 0.13101481658378872, 0.1291638130420271, 0.1256609515176433, 0.12125261284074171, 0.11597306684274153, 0.11018658533273308, 0.10413067290515132, 0.09776574300196808, 0.09153374739848426, 0.08548475656437518, 0.07942292685507353, 0.07352183013593529, 0.06805041197521496, 0.06279341765186616, 0.057840100997131196, 0.05324799014302796, 0.04894622210041231, 0.04480379995196453, 0.041006422423304545, 0.037556839393093656, 0.03431543542411902, 0.03130849173240325, 0.028524152477760165, 0.025963730008566503, 0.02364476346297905, 0.021525503846862023, 0.01962931086007515, 0.0178645039635076, 0.016213923581050987, 0.014750000326744478, 0.013394951458202254, 0.012175862228834496, 0.011066833705362612, 0.010032145289903767, 0.009121018999370227, 0.00826748865726589, 0.007494428092901135, 0.006815560932806628, 0.006199811247093604, 0.005620530845842386, 0.005097903077315211, 0.004625979795898065, 0.004204472324814729, 0.003822396365336361, 0.0034705366450576516, 0.003157095977633237, 0.0028700894175686586, 0.00261262984519095, 0.0023668079248279537, 0.002147758833044342, 0.0019456777761615528, 0.0017695520335190632, 0.0016078626845988362, 0.0014675790798295402, 0.0013297018278551759, 0.0012108894970812708, 0.0010998084888217466, 0.001000674783393658, 0.0009046418679968054, 0.0008259133693543051, 0.0007528716697770672, 0.000680042225258329, 0.0006204675156071906, 0.0005625586205795256, 0.0005135707789088208, 0.00046837470777173556, 0.0004257207772192623, 0.0003937984260528404, 0.0003544254709823789, 0.0003260197809526384, 0.0002985976955063207, 0.0002799314261143094, 0.00025409811484164386, 0.0002339732945869743, 0.00021360765136121411, 0.00019694206739881922, 0.0001834685435294423, 0.00016756183397037113, 0.00015378696749086408, 0.00014266279987168307, 0.00013025007589575907, 0.0001207366319043378, 0.00011184704324038972, 0.00010549594465889586, 0.0001008964732549267, 9.253873412750804e-05, 8.641825299176225e-05, 8.240392075771637e-05, 7.679489829482075e-05, 6.921565060083111e-05, 6.244526965707616e-05, 5.82492019596469e-05, 5.573072080852884e-05, 5.336208325756186e-05, 4.856267425649628e-05, 4.918119422168179e-05, 4.7181908900787066e-05, 4.529881137284634e-05, 4.300334021574678e-05, 4.0173582570616174e-05, 3.9690051506101855e-05, 3.657586453717945e-05, 3.419981890320294e-05, 3.244539350725483e-05, 2.9682852775977304e-05, 2.7342220902717505e-05, 2.5735404333109156e-05, 2.4184611028252664e-05, 2.2557689139709305e-05, 2.0531757323493315e-05, 2.0766413313861683e-05, 2.0431004436544964e-05, 1.8582682591913534e-05, 1.711790098219201e-05, 1.6894853591201436e-05, 1.5270043677975368e-05, 1.5763916216343847e-05, 1.5667471750287046e-05, 1.4105553029996622e-05, 1.3304973342821373e-05, 1.1999769696054801e-05, 1.3779101555921944e-05, 1.4052707873952262e-05, 1.3828204692108292e-05, 1.3695348114680911e-05, 1.439409736262746e-05, 1.5071681382609258e-05, 1.5118532714368968e-05, 1.3652869416069727e-05, 1.3930626041204805e-05, 1.3068050472805706e-05, 1.2755868161093756e-05, 1.1737062390704178e-05, 1.091857647043159e-05, 1.0219546450956279e-05, 9.931945281012303e-06, 9.392624781892659e-06, 8.95836983532425e-06, 8.545783358297253e-06, 8.365533053733563e-06, 7.734246740970076e-06, 7.284034000586232e-06, 6.750533893356127e-06, 6.823535767798005e-06, 6.52321072855643e-06, 6.7410839926755385e-06, 6.22705939841156e-06, 5.680744288773637e-06, 5.718993177911397e-06, 5.324994787027211e-06, 4.801298949180993e-06, 4.5739320866318545e-06, 4.556175241829796e-06, 4.507070893835935e-06, 9.538885599747384e-06, 9.49557370705412e-06, 9.02465773189746e-06, 8.390204306645628e-06, 8.095945576072818e-06, 7.664855706419756e-06, 7.360061766401612e-06, 6.9727417780013916e-06, 7.1001513722818735e-06, 6.450319267638962e-06, 5.914437189196184e-06, 6.2679380446585165e-06, 5.9881798998984205e-06, 5.44888717296429e-06, 5.657937641334532e-06, 5.608811814513635e-06, 5.457178472921215e-06, 5.167713404230702e-06, 4.682044418349056e-06, 4.924065975931266e-06], "duration": 118884.295907, "accuracy_train": [0.3987803242663344, 0.45484921500092285, 0.6463556807170543, 0.7229803288805832, 0.7441759528423773, 0.7650557891519011, 0.7874869863764304, 0.8076910875784422, 0.8089713613764304, 0.8142487281976744, 0.8291285991140642, 0.8343347695182725, 0.8443325489110374, 0.8491677769702842, 0.8550471590646919, 0.8611833889927095, 0.8619263551471945, 0.8695059797780547, 0.8766202141588224, 0.8756901546350129, 0.8766899686231081, 0.8857104645394979, 0.8867578629683462, 0.890104274813123, 0.8959400378368402, 0.8989845408245662, 0.8944508611341824, 0.8992864496816169, 0.9059363752768549, 0.9049812618240125, 0.9056341059316169, 0.9058658998361941, 0.906980889800203, 0.9112359121216316, 0.9134673340023993, 0.9198596907876523, 0.9187450613118309, 0.9153725140734589, 0.9222324240379292, 0.9210938220976375, 0.924813699704688, 0.9266251528469915, 0.9236511252999261, 0.9301837119901256, 0.9269056126568845, 0.928370095918697, 0.9343915103589886, 0.9362290988948875, 0.9331584605135659, 0.9349491855850868, 0.9366243741925065, 0.9395308102044113, 0.9409484300018457, 0.9407377246562385, 0.943527903227667, 0.9439932934777593, 0.9460405058947029, 0.942831079561185, 0.9450392499538575, 0.9443188143110927, 0.9479446045011997, 0.9480851948943337, 0.9514570211563308, 0.9478065375253784, 0.9514087157392026, 0.9506657495847176, 0.9521534843346253, 0.9489683909537652, 0.9543616547272978, 0.9543852667035806, 0.9503402287513842, 0.9553149657392026, 0.9533854527154854, 0.9562693582156699, 0.9564553701204319, 0.9558268589654854, 0.9605687205841639, 0.9505019077034883, 0.9595932395487264, 0.9592215762273901, 0.9635459925249169, 0.9584077741440569, 0.9616619010128276, 0.9605690810723514, 0.9625683485603543, 0.9643816041435955, 0.9621062027039498, 0.9631754106681433, 0.964870966858158, 0.9632211926679586, 0.9653832205726283, 0.9657094623823367, 0.9676847574058692, 0.9692200765965301, 0.9662889471437799, 0.9682413511674051, 0.9701014702150241, 0.9690551532507383, 0.9652440721322444, 0.9655903210363603, 0.9691946621793098, 0.9709621357627353, 0.971474028989018, 0.963661528989018, 0.9736589478935955, 0.9723343340485419, 0.9728215338339794, 0.9727528608342562, 0.9723107220722591, 0.974938861203396, 0.9723811975129198, 0.9733810115010151, 0.9742867380721669, 0.9727292488579733, 0.9732872845722591, 0.974449858977021, 0.9746355103935955, 0.9745671978820598, 0.9734961874769288, 0.9770998076435032, 0.9769606592031194, 0.9744731104651162, 0.9752400490840717, 0.9774253284768365, 0.9746126193936876, 0.978588263369786, 0.9783786394887413, 0.9749606707387413, 0.9773555740125508, 0.9755426789174971, 0.9693120010843485, 0.9787739147863603, 0.978540678929033, 0.9790075111318751, 0.9804473009528424, 0.980982806155408, 0.9807731822743633, 0.9777497678456073, 0.9813777209648394, 0.9799601011674051, 0.9810994240840717, 0.9797966197743633, 0.980263091489018, 0.9805642793696937, 0.9815404813815062, 0.9812153210363603, 0.9815630118932264, 0.9817483028216132, 0.9824000654646549, 0.9814467544527501, 0.9819826201435032, 0.9817486633098007, 0.9833072339885567, 0.9827728102505537, 0.9840280301195091, 0.9825631863695091, 0.9822841685123662, 0.9840505606312293, 0.9831205011074198, 0.98216791107189, 0.9835622793812293, 0.9798438437269288, 0.9839582756552234, 0.9742405955841639, 0.9844458359288483, 0.9838892421673128, 0.9835393883813216, 0.9844465569052234, 0.9842830755121816, 0.984702323274271, 0.9846322083217978, 0.9858877886789406, 0.9839811666551311, 0.9821439386074198, 0.9863753489525655, 0.9854227589170359, 0.984468726928756, 0.9866311153216132, 0.9864222124169435, 0.9863982399524732, 0.9861664460478959, 0.9852356655477114, 0.9875157533337948], "end": "2016-01-23 14:01:12.585000", "learning_rate_per_epoch": [0.0008346705581061542, 0.0005902012344449759, 0.000481897295685485, 0.0004173352790530771, 0.0003732760378625244, 0.0003407528274692595, 0.00031547583057545125, 0.0002951006172224879, 0.0002782235387712717, 0.00026394601445645094, 0.0002516626555006951, 0.0002409486478427425, 0.00023149597109295428, 0.0002230750978924334, 0.00021551101235672832, 0.00020866763952653855, 0.0002024373534368351, 0.00019673374481499195, 0.00019148657156620175, 0.0001866380189312622, 0.00018214005103800446, 0.0001779523736331612, 0.00017404084792360663, 0.00017037641373462975, 0.00016693411453161389, 0.00016369236982427537, 0.00016063242219388485, 0.00015773791528772563, 0.00015499444270972162, 0.00015238930063787848, 0.00014991126954555511, 0.00014755030861124396, 0.00014529749751091003, 0.0001431448181392625, 0.00014108508185017854, 0.00013911176938563585, 0.00013721900177188218, 0.00013540145300794393, 0.00013365426275413483, 0.00013197300722822547, 0.00013035364099778235, 0.00012879246787633747, 0.00012728606816381216, 0.00012583132775034755, 0.0001244253508048132, 0.0001230654597748071, 0.00012174921721452847, 0.00012047432392137125, 0.0001192386553157121, 0.00011804024688899517, 0.00011687725782394409, 0.00011574798554647714, 0.00011465082206996158, 0.00011358428309904411, 0.0001125469570979476, 0.0001115375489462167, 0.00011055482173105702, 0.00010959761857520789, 0.0001086648553609848, 0.00010775550617836416, 0.0001068686178768985, 0.00010600327368592843, 0.00010515860776649788, 0.00010433381976326928, 0.00010352814570069313, 0.0001027408434310928, 0.00010197124356636778, 0.00010121867671841756, 0.00010048253170680255, 9.9762219178956e-05, 9.905717161018401e-05, 9.836687240749598e-05, 9.769080497790128e-05, 9.702848183223978e-05, 9.6379459137097e-05, 9.574328578310087e-05, 9.511953976470977e-05, 9.450783545617014e-05, 9.390777995577082e-05, 9.33190094656311e-05, 9.274117473978549e-05, 9.217394836014137e-05, 9.161699563264847e-05, 9.107002551900223e-05, 9.053273970494047e-05, 9.000484715215862e-05, 8.948607865022495e-05, 8.89761868165806e-05, 8.847490244079381e-05, 8.798200724413618e-05, 8.749725384404883e-05, 8.702042396180332e-05, 8.655130659462884e-05, 8.60896980157122e-05, 8.56353944982402e-05, 8.518820686731488e-05, 8.474796049995348e-05, 8.431445894530043e-05, 8.388754940824583e-05, 8.346705726580694e-05, 8.305282972287387e-05, 8.264469943242148e-05, 8.224253542721272e-05, 8.184618491213769e-05, 8.145550964400172e-05, 8.107037137961015e-05, 8.069065370364115e-05, 8.031621109694242e-05, 7.994694169610739e-05, 7.958271453389898e-05, 7.92234277469106e-05, 7.886895764386281e-05, 7.851920236134902e-05, 7.817406003596261e-05, 7.783343608025461e-05, 7.749722135486081e-05, 7.716532127233222e-05, 7.683765579713508e-05, 7.651413034182042e-05, 7.619465031893924e-05, 7.58791429689154e-05, 7.556752098025754e-05, 7.525971159338951e-05, 7.495563477277756e-05, 7.465520320693031e-05, 7.435836596414447e-05, 7.406503573292866e-05, 7.377515430562198e-05, 7.348864892264828e-05, 7.320545410038903e-05, 7.292550435522571e-05, 7.264874875545502e-05, 7.237512181745842e-05, 7.21045580576174e-05, 7.183700654422864e-05, 7.157240906963125e-05, 7.131072197807953e-05, 7.105187978595495e-05, 7.079583156155422e-05, 7.054254092508927e-05, 7.029194239294156e-05, 7.004399958532304e-05, 6.979866157053038e-05, 6.955588469281793e-05, 6.931561802048236e-05, 6.907783244969323e-05, 6.884246977278963e-05, 6.860950088594109e-05, 6.837888213340193e-05, 6.815056985942647e-05, 6.792452768422663e-05, 6.770072650397196e-05, 6.747911538695917e-05, 6.725967250531539e-05, 6.704235420329496e-05, 6.682713137706742e-05, 6.661396764684469e-05, 6.64028266328387e-05, 6.619368650717661e-05, 6.598650361411273e-05, 6.578125612577423e-05, 6.557791493833065e-05, 6.537644367199391e-05, 6.517682049889117e-05, 6.497901631519198e-05, 6.478300201706588e-05, 6.458874850068241e-05, 6.439623393816873e-05, 6.420542922569439e-05, 6.401631253538653e-05, 6.382885476341471e-05, 6.364303408190608e-05, 6.34588286629878e-05, 6.327621667878702e-05, 6.30951690254733e-05, 6.291566387517378e-05, 6.273767940001562e-05, 6.256120832404122e-05, 6.238620699150488e-05, 6.22126754024066e-05, 6.204057717695832e-05, 6.186989776324481e-05, 6.170062260935083e-05], "accuracy_valid": [0.39829189806099397, 0.4539794921875, 0.6317712255271084, 0.7074577607304217, 0.7211296357304217, 0.7383209596197289, 0.7561844055911144, 0.7745670180722892, 0.7767848738704819, 0.7749126388365963, 0.7907420698418675, 0.7931525908320783, 0.8027667309864458, 0.8013827772025602, 0.8038550687123494, 0.8091143872364458, 0.8099997646837349, 0.813610398625753, 0.8164694912462349, 0.8141501553087349, 0.8150355327560241, 0.8216170345444277, 0.8212302334337349, 0.8205081066453314, 0.8234274990587349, 0.8257468349962349, 0.8207625423569277, 0.8226641919239458, 0.8278220303087349, 0.8237628247364458, 0.8273337490587349, 0.8247702724962349, 0.8214537838855422, 0.8235083890248494, 0.8264586666980422, 0.8291648037462349, 0.8305178722703314, 0.8242511059864458, 0.8282897213855422, 0.8287074077560241, 0.8316268001694277, 0.8334887401167168, 0.8279543957078314, 0.8315047298569277, 0.8304266872176205, 0.8312708843185241, 0.8339770213667168, 0.8342108669051205, 0.8330210490399097, 0.8344755977033133, 0.8340887965926205, 0.8383112528237951, 0.8365610881024097, 0.835350680064006, 0.8364184276167168, 0.8370493693524097, 0.8364596079631024, 0.8354521602033133, 0.8370287791792168, 0.8365919733621988, 0.8378832713667168, 0.8376597209149097, 0.8416174463478916, 0.8376494258283133, 0.839867281626506, 0.8392466349774097, 0.8378935664533133, 0.835106539439006, 0.8413733057228916, 0.842186617564006, 0.8363978374435241, 0.841087984751506, 0.841087984751506, 0.8407114787274097, 0.8401217173381024, 0.8407423639871988, 0.8439058970256024, 0.8357374811746988, 0.8426954889871988, 0.8412203501506024, 0.8434176157756024, 0.8408541392131024, 0.8414439006024097, 0.8394907756024097, 0.8409762095256024, 0.842308687876506, 0.8407423639871988, 0.8427660838667168, 0.8420748423381024, 0.8405585231551205, 0.8414439006024097, 0.8412203501506024, 0.8458281367658133, 0.8439058970256024, 0.8389921992658133, 0.8425631235881024, 0.8428072642131024, 0.8417189264871988, 0.841576266001506, 0.8393084054969879, 0.8424513483621988, 0.843895601939006, 0.8442412227033133, 0.8397760965737951, 0.8447603892131024, 0.843407320689006, 0.8440485575112951, 0.8433970256024097, 0.8426954889871988, 0.8467135142131024, 0.8425425334149097, 0.8441603327371988, 0.843407320689006, 0.8400908320783133, 0.8436720514871988, 0.845726656626506, 0.843895601939006, 0.8448927546121988, 0.8437838267131024, 0.8466120340737951, 0.8451471903237951, 0.842674898814006, 0.8434382059487951, 0.8421969126506024, 0.8431837702371988, 0.8445265436746988, 0.8442721079631024, 0.8435396860881024, 0.8439367822853916, 0.8442926981362951, 0.8386362834149097, 0.8446897943335843, 0.8472532708960843, 0.8432955454631024, 0.8444147684487951, 0.8453501506024097, 0.8471003153237951, 0.8399687617658133, 0.8451266001506024, 0.8424410532756024, 0.8419630671121988, 0.8435499811746988, 0.8470091302710843, 0.8437838267131024, 0.8452486704631024, 0.8443132883094879, 0.8457369517131024, 0.8455031061746988, 0.8442926981362951, 0.8420748423381024, 0.842674898814006, 0.8421660273908133, 0.8463267131024097, 0.8445265436746988, 0.8455942912274097, 0.8419630671121988, 0.8440279673381024, 0.8447706842996988, 0.8424101680158133, 0.8420336619917168, 0.844628023814006, 0.8398878717996988, 0.8420233669051205, 0.8382597773908133, 0.8426646037274097, 0.8456148814006024, 0.8429087443524097, 0.8424101680158133, 0.8456251764871988, 0.844139742564006, 0.845360445689006, 0.8456148814006024, 0.8436617564006024, 0.8449030496987951, 0.8452486704631024, 0.8433867305158133, 0.8449839396649097, 0.8455236963478916, 0.8457369517131024, 0.839623141001506, 0.8423998729292168, 0.8433867305158133, 0.8442412227033133], "accuracy_test": 0.8290150316455697, "start": "2016-01-22 04:59:48.289000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0], "accuracy_train_last": 0.9875157533337948, "batch_size_eval": 1024, "accuracy_train_std": [0.01529251560125423, 0.015347720462062595, 0.014474002975305671, 0.01380978596633481, 0.01603014470745477, 0.014379147451119735, 0.014254160777621396, 0.01237945045830559, 0.01230251069893672, 0.013350767014857162, 0.011184843007793618, 0.012517174707558787, 0.012136393932820271, 0.011680710278773798, 0.013755026697553173, 0.01411622668352833, 0.01366500029162798, 0.013440336630560818, 0.013262523932281929, 0.01341795425214866, 0.012120002715186205, 0.012772750484138853, 0.011519022352544754, 0.011307079065256555, 0.012915760271921059, 0.01252066599405361, 0.012483613453735666, 0.012821145708208624, 0.011588816916293671, 0.012744426398490603, 0.011743190764972126, 0.011925621446789995, 0.012756327011332417, 0.011021905152237407, 0.010743863290351801, 0.012494278658275657, 0.010981512038895, 0.012216377416964673, 0.0117180971833191, 0.01056943613203038, 0.011208790468084127, 0.011405622290338903, 0.01082480985450109, 0.011638424931260728, 0.011841962239529763, 0.011548733172577132, 0.011626770767652452, 0.010143841427638052, 0.012126870117733194, 0.010935808223496752, 0.010999102020739475, 0.010694659496636314, 0.011337890887358208, 0.010896723812711057, 0.011010158245641678, 0.010345970720515896, 0.00929345607725424, 0.010827525512276188, 0.011206362651192453, 0.010842315860887278, 0.010907922663120106, 0.00978234558954063, 0.00934613673092362, 0.010721798306878392, 0.009423940962501, 0.009719098317480529, 0.009804102802470785, 0.009141771377409512, 0.009134425386550484, 0.009285247148195107, 0.009536105667166351, 0.00939560130101197, 0.009621472981112402, 0.009920188323420034, 0.00903236769941951, 0.009825215317030345, 0.009116455237367366, 0.010613471136486996, 0.007740093401230918, 0.008220396429170723, 0.008237388641651088, 0.009122279967166705, 0.008598551142710267, 0.008654003082881072, 0.008076909813646883, 0.0089396442618832, 0.008326153963672327, 0.008744340081599166, 0.008189622010539853, 0.0089396507815309, 0.007230229459444659, 0.008216011855043113, 0.007337440186214942, 0.007317320500642703, 0.00807687664551024, 0.008349997077604995, 0.007881292137180028, 0.008463832420238931, 0.009020421091694512, 0.008904867505618792, 0.0074833539173287285, 0.007316330389826908, 0.007261093353309169, 0.008805107729531057, 0.00637480953979356, 0.0072485563243434295, 0.007243709633746796, 0.0071287686528773075, 0.0072557755518123475, 0.005906872013030662, 0.006534919105239176, 0.007128527524364936, 0.006898614626139408, 0.007521637909357659, 0.0069549345258867355, 0.005875109487967546, 0.00646632072128728, 0.006531368216902247, 0.006785799460748202, 0.006769963480421981, 0.0069020804832320365, 0.005942938450861781, 0.0065266359297010265, 0.006180725535500212, 0.006637498245862095, 0.005997775242625617, 0.005660136040725621, 0.006922059091458737, 0.005929643065106292, 0.006396458719728951, 0.006466433319627225, 0.006135398741767839, 0.006401689579390359, 0.005587595306608848, 0.00575879410481956, 0.005326895390241366, 0.004985015358873497, 0.005976228668392891, 0.0057064576212003644, 0.0055193646883335366, 0.005813644269551054, 0.005751031815711015, 0.005170033929051604, 0.005700092341122591, 0.005513805365134718, 0.0058686053448155015, 0.005966435137375128, 0.005056185809272772, 0.005500015275335514, 0.006052602186405276, 0.004927674001583679, 0.005416682061769933, 0.005384235796256509, 0.004861143179959428, 0.005413485826644647, 0.005119733182075662, 0.005370906005307797, 0.005658882035481471, 0.00510475720461835, 0.005086448090225533, 0.005308035884071649, 0.0057038702551751, 0.005063814691532748, 0.006964704915763163, 0.005572789435602804, 0.0055346490369916675, 0.00575427577549142, 0.0047453462438784635, 0.004466436598026601, 0.005007290185941776, 0.004786727995850821, 0.005032149027394407, 0.0052786992982817894, 0.005800399970876207, 0.00479014299958325, 0.005279739250511292, 0.005115277029218473, 0.004207316758731002, 0.0051418379055331144, 0.004724834240847228, 0.004912010763384705, 0.0052434087987862955, 0.004618765509772278], "accuracy_test_std": 0.034313527046476174, "error_valid": [0.601708101939006, 0.5460205078125, 0.3682287744728916, 0.29254223926957834, 0.27887036426957834, 0.2616790403802711, 0.24381559440888556, 0.22543298192771077, 0.2232151261295181, 0.22508736116340367, 0.20925793015813254, 0.20684740916792166, 0.1972332690135542, 0.19861722279743976, 0.19614493128765065, 0.1908856127635542, 0.1900002353162651, 0.18638960137424698, 0.1835305087537651, 0.1858498446912651, 0.18496446724397586, 0.1783829654555723, 0.1787697665662651, 0.17949189335466864, 0.1765725009412651, 0.1742531650037651, 0.1792374576430723, 0.1773358080760542, 0.1721779696912651, 0.1762371752635542, 0.1726662509412651, 0.1752297275037651, 0.17854621611445776, 0.17649161097515065, 0.17354133330195776, 0.1708351962537651, 0.16948212772966864, 0.1757488940135542, 0.17171027861445776, 0.17129259224397586, 0.1683731998305723, 0.1665112598832832, 0.17204560429216864, 0.1684952701430723, 0.16957331278237953, 0.16872911568147586, 0.1660229786332832, 0.16578913309487953, 0.1669789509600903, 0.16552440229668675, 0.16591120340737953, 0.16168874717620485, 0.1634389118975903, 0.16464931993599397, 0.1635815723832832, 0.1629506306475903, 0.16354039203689763, 0.16454783979668675, 0.1629712208207832, 0.16340802663780118, 0.1621167286332832, 0.1623402790850903, 0.1583825536521084, 0.16235057417168675, 0.16013271837349397, 0.1607533650225903, 0.16210643354668675, 0.16489346056099397, 0.1586266942771084, 0.15781338243599397, 0.16360216255647586, 0.15891201524849397, 0.15891201524849397, 0.1592885212725903, 0.15987828266189763, 0.15925763601280118, 0.15609410297439763, 0.16426251882530118, 0.15730451101280118, 0.15877964984939763, 0.15658238422439763, 0.15914586078689763, 0.1585560993975903, 0.1605092243975903, 0.15902379047439763, 0.15769131212349397, 0.15925763601280118, 0.1572339161332832, 0.15792515766189763, 0.15944147684487953, 0.1585560993975903, 0.15877964984939763, 0.15417186323418675, 0.15609410297439763, 0.16100780073418675, 0.15743687641189763, 0.15719273578689763, 0.15828107351280118, 0.15842373399849397, 0.16069159450301207, 0.15754865163780118, 0.15610439806099397, 0.15575877729668675, 0.16022390342620485, 0.15523961078689763, 0.15659267931099397, 0.15595144248870485, 0.1566029743975903, 0.15730451101280118, 0.15328648578689763, 0.1574574665850903, 0.15583966726280118, 0.15659267931099397, 0.15990916792168675, 0.15632794851280118, 0.15427334337349397, 0.15610439806099397, 0.15510724538780118, 0.15621617328689763, 0.15338796592620485, 0.15485280967620485, 0.15732510118599397, 0.15656179405120485, 0.15780308734939763, 0.15681622976280118, 0.15547345632530118, 0.15572789203689763, 0.15646031391189763, 0.1560632177146084, 0.15570730186370485, 0.1613637165850903, 0.15531020566641573, 0.15274672910391573, 0.15670445453689763, 0.15558523155120485, 0.1546498493975903, 0.15289968467620485, 0.16003123823418675, 0.15487339984939763, 0.15755894672439763, 0.15803693288780118, 0.15645001882530118, 0.15299086972891573, 0.15621617328689763, 0.15475132953689763, 0.15568671169051207, 0.15426304828689763, 0.15449689382530118, 0.15570730186370485, 0.15792515766189763, 0.15732510118599397, 0.15783397260918675, 0.1536732868975903, 0.15547345632530118, 0.1544057087725903, 0.15803693288780118, 0.15597203266189763, 0.15522931570030118, 0.15758983198418675, 0.1579663380082832, 0.15537197618599397, 0.16011212820030118, 0.15797663309487953, 0.16174022260918675, 0.1573353962725903, 0.15438511859939763, 0.1570912556475903, 0.15758983198418675, 0.15437482351280118, 0.15586025743599397, 0.15463955431099397, 0.15438511859939763, 0.15633824359939763, 0.15509695030120485, 0.15475132953689763, 0.15661326948418675, 0.1550160603350903, 0.1544763036521084, 0.15426304828689763, 0.16037685899849397, 0.1576001270707832, 0.15661326948418675, 0.15575877729668675], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-3, 3], "momentum": 0.834528727611497, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.0008346705855418204, "patience_threshold": 1, "do_flip": true, "batch_size": 128, "optimization": "rmsprop", "nb_data_augmentation": 4, "learning_rate_decay_method": "sqrt", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 1.0656159821523425e-10, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.05917992127992808}, "accuracy_valid_max": 0.8472532708960843, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import os\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-3, 3)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8442412227033133, "loss_train": [1.5827163457870483, 1.1457208395004272, 0.9421170353889465, 0.8265500068664551, 0.7485103011131287, 0.6952642202377319, 0.6525025963783264, 0.6197834014892578, 0.5903224945068359, 0.5683580040931702, 0.5479850172996521, 0.5292751789093018, 0.512630045413971, 0.49655628204345703, 0.48202192783355713, 0.4689720869064331, 0.4582239091396332, 0.4462648630142212, 0.4354550540447235, 0.4268360733985901, 0.4166842997074127, 0.40919196605682373, 0.3996942341327667, 0.39338958263397217, 0.3856508731842041, 0.3767543137073517, 0.3712540864944458, 0.3652409017086029, 0.3579033613204956, 0.3524906635284424, 0.3483191728591919, 0.3422466218471527, 0.33694958686828613, 0.3328109681606293, 0.3261197805404663, 0.3232917785644531, 0.3163027763366699, 0.3126543164253235, 0.3082334101200104, 0.3035833537578583, 0.29876092076301575, 0.2957283556461334, 0.2911752164363861, 0.2878866195678711, 0.28510189056396484, 0.28147387504577637, 0.2769457995891571, 0.2742250859737396, 0.2703256607055664, 0.26646116375923157, 0.2623862028121948, 0.2589128613471985, 0.2579697370529175, 0.2552967965602875, 0.2517417073249817, 0.24889352917671204, 0.24541030824184418, 0.24254459142684937, 0.24175919592380524, 0.23976890742778778, 0.2363920658826828, 0.23212815821170807, 0.2296079844236374, 0.22814840078353882, 0.22620613873004913, 0.22277644276618958, 0.2196136862039566, 0.21785421669483185, 0.21644917130470276, 0.21314792335033417, 0.21100673079490662, 0.2096540629863739, 0.20651134848594666, 0.20319439470767975, 0.20315885543823242, 0.2007182389497757, 0.19862079620361328, 0.19869475066661835, 0.19734768569469452, 0.19458040595054626, 0.19167841970920563, 0.1911531537771225, 0.1870357245206833, 0.1862889677286148, 0.18481773138046265, 0.18339625000953674, 0.1818106472492218, 0.18053266406059265, 0.1777912825345993, 0.1764460653066635, 0.17628175020217896, 0.17428462207317352, 0.17183923721313477, 0.171738862991333, 0.16898371279239655, 0.16734230518341064, 0.16610755026340485, 0.16477979719638824, 0.16453857719898224, 0.16237853467464447, 0.16235002875328064, 0.1596681773662567, 0.15805336833000183, 0.15636195242404938, 0.15578976273536682, 0.15543033182621002, 0.15181471407413483, 0.15261989831924438, 0.15088976919651031, 0.14881686866283417, 0.14822930097579956, 0.1465630978345871, 0.14551521837711334, 0.1441539078950882, 0.14288516342639923, 0.14177706837654114, 0.14021286368370056, 0.13939973711967468, 0.13779892027378082, 0.1375093162059784, 0.13632069528102875, 0.13565512001514435, 0.1342291682958603, 0.1336870938539505, 0.13401548564434052, 0.13106919825077057, 0.13161267340183258, 0.13072963058948517, 0.12749026715755463, 0.12864872813224792, 0.12713144719600677, 0.12579166889190674, 0.12489728629589081, 0.12321600317955017, 0.12191326916217804, 0.12277556955814362, 0.1212812066078186, 0.12067253142595291, 0.11957155913114548, 0.11822404712438583, 0.11805450171232224, 0.11691169440746307, 0.11629366874694824, 0.11512285470962524, 0.11401895433664322, 0.11294576525688171, 0.11168338358402252, 0.11143500357866287, 0.11137692630290985, 0.11060754954814911, 0.11013945192098618, 0.10970592498779297, 0.10775876045227051, 0.10866755247116089, 0.10749910771846771, 0.10549696534872055, 0.10559915751218796, 0.10416508466005325, 0.10484495759010315, 0.10301946103572845, 0.10179594159126282, 0.10095097869634628, 0.10045662522315979, 0.1008416935801506, 0.10035720467567444, 0.0988239124417305, 0.0987304151058197, 0.09728918969631195, 0.09660571068525314, 0.09619336575269699, 0.09715767949819565, 0.09626024216413498, 0.09507464617490768, 0.09396829456090927, 0.09346769005060196, 0.09183209389448166, 0.09166466444730759, 0.09249116480350494, 0.08967303484678268, 0.09128919988870621, 0.08886972814798355, 0.09002421796321869, 0.08892130106687546], "accuracy_train_first": 0.3987803242663344, "model": "residualv4", "loss_std": [0.2573792338371277, 0.11058459430932999, 0.09095821529626846, 0.08442916721105576, 0.08083412051200867, 0.0779547467827797, 0.0768372118473053, 0.07619573175907135, 0.07435570657253265, 0.07430699467658997, 0.07499207556247711, 0.0723094716668129, 0.07203444838523865, 0.06885536760091782, 0.06782901287078857, 0.06694352626800537, 0.06812949478626251, 0.06807059049606323, 0.06605836749076843, 0.06529174000024796, 0.06329945474863052, 0.06389728933572769, 0.06323564797639847, 0.06286310404539108, 0.06237402185797691, 0.0618429109454155, 0.062113549560308456, 0.06060921028256416, 0.0603720024228096, 0.06033369526267052, 0.05980855971574783, 0.05856097862124443, 0.05798976123332977, 0.05739009380340576, 0.057529911398887634, 0.05684496462345123, 0.05545696243643761, 0.05633367598056793, 0.05430959537625313, 0.05654947832226753, 0.05516296625137329, 0.053806062787771225, 0.05286633223295212, 0.05285993218421936, 0.0515788197517395, 0.05146266892552376, 0.05144455283880234, 0.05299766734242439, 0.05192198231816292, 0.051413942128419876, 0.05071832984685898, 0.04980406537652016, 0.04928019270300865, 0.04952394217252731, 0.04873174428939819, 0.04716609790921211, 0.04845748469233513, 0.04800056666135788, 0.04649990797042847, 0.04799225926399231, 0.04623129218816757, 0.04584241658449173, 0.04598506540060043, 0.04610130563378334, 0.045978426933288574, 0.04517243057489395, 0.04521739110350609, 0.043385665863752365, 0.04467149078845978, 0.04483110457658768, 0.043902356177568436, 0.045526985079050064, 0.04317883029580116, 0.04114096984267235, 0.04278681427240372, 0.04171309620141983, 0.040999263525009155, 0.041803937405347824, 0.041016314178705215, 0.040700286626815796, 0.041826266795396805, 0.04065360501408577, 0.03888436779379845, 0.04079994559288025, 0.03875478729605675, 0.0386660061776638, 0.04046156629920006, 0.039561208337545395, 0.038541216403245926, 0.037736885249614716, 0.03844742849469185, 0.038128335028886795, 0.037619397044181824, 0.03783508762717247, 0.03707021474838257, 0.0369318425655365, 0.0367208756506443, 0.035660699009895325, 0.03594319149851799, 0.036152586340904236, 0.03660578280687332, 0.03614993765950203, 0.03548716753721237, 0.03377184644341469, 0.03569607809185982, 0.03619581460952759, 0.033683355897665024, 0.03332336246967316, 0.03536262735724449, 0.03366324305534363, 0.03422938659787178, 0.034088969230651855, 0.03512570634484291, 0.03414285182952881, 0.03302057087421417, 0.03146270290017128, 0.031724829226732254, 0.032428860664367676, 0.03163916990160942, 0.03181545436382294, 0.03400728479027748, 0.03225478529930115, 0.03175755962729454, 0.032819025218486786, 0.029922591522336006, 0.03124297596514225, 0.029850149527192116, 0.03051774948835373, 0.030109073966741562, 0.030940746888518333, 0.030571715906262398, 0.029158590361475945, 0.028100134804844856, 0.02945832908153534, 0.030168181285262108, 0.0297341700643301, 0.029184585437178612, 0.02872019074857235, 0.028446009382605553, 0.028649985790252686, 0.029474908486008644, 0.02774328924715519, 0.028250666335225105, 0.028044121339917183, 0.02715637907385826, 0.028103647753596306, 0.027126658707857132, 0.027088142931461334, 0.027868282049894333, 0.02760118618607521, 0.027757329866290092, 0.02722341939806938, 0.027007609605789185, 0.027227025479078293, 0.02809062786400318, 0.02641480416059494, 0.027408070862293243, 0.02666471339762211, 0.02587699517607689, 0.025829492136836052, 0.027302300557494164, 0.026324138045310974, 0.025623179972171783, 0.026294391602277756, 0.024847790598869324, 0.025212548673152924, 0.025429902598261833, 0.024145467206835747, 0.024756092578172684, 0.02534482628107071, 0.024898799136281013, 0.024957381188869476, 0.024230867624282837, 0.02506871335208416, 0.024696480482816696, 0.0246572308242321, 0.024051669985055923, 0.023892592638731003, 0.022362004965543747, 0.02208283729851246, 0.02283470705151558, 0.023270167410373688, 0.023085836321115494]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:09 2016", "state": "available"}], "summary": "f8f453027533c356ab35f03e6c19c78d"}