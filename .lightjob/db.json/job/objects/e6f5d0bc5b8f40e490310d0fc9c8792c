{"content": {"hp_model": {"f0": 32, "f1": 32, "f2": 32, "f3": 32, "nonlin": "very_leaky_rectify", "nbg1": 3, "nbg3": 5, "nbg2": 3, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.018705990744293903, 0.019768444799298904, 0.01757293134449554, 0.015463130262820039, 0.019058714906018875, 0.016669976510961304, 0.018578798408203142, 0.018019253627194344, 0.013983282879581218, 0.017891586478158743, 0.0110751869515276, 0.01490941232324173, 0.014471332104520217, 0.017133748095386186, 0.013028571692698703, 0.014764781221944369, 0.01218877775685922, 0.00948173255191495, 0.010823873555824877, 0.015075962638970647, 0.011526128158399023, 0.012341381532203302, 0.012718800504624152, 0.008804366292350941, 0.0120114827708937, 0.012238245788349478, 0.008886926397899288, 0.01040223785802007, 0.009352968660798495, 0.011391251825427794, 0.00910241318272128, 0.011202710110283629, 0.0072664417427872295, 0.011052040240711675, 0.01437620486665977, 0.009043525358648746, 0.012315556963500465, 0.01225274011344167, 0.008002843658009594, 0.008198194891338991, 0.008591807390292187, 0.011480509992131136, 0.015614078418655513, 0.010682227692037654, 0.016526769955537934, 0.012128797933264436, 0.010944641451168748, 0.012598970439934756, 0.014609844171420944, 0.01006324443067804, 0.010003362372041668, 0.012683687806823284, 0.011602030051711733, 0.011166295694761354, 0.010571201810510637, 0.01502837859399147, 0.011762825000414142, 0.014010906884925404, 0.009448847171076304, 0.019160761446547513, 0.016067510148838973, 0.014519027856251861, 0.01142027762575156, 0.010853025944197408, 0.012956385659665686, 0.011539881713757813, 0.01399580138217499, 0.01889340960512963, 0.016656346511984974, 0.01278182391883186, 0.01509959439336675, 0.01286355312336251, 0.014722738047232441, 0.012253324871519361, 0.00818752340903531, 0.014301811539987549, 0.01926126512837251, 0.017829535246024677, 0.014274183765163592, 0.016414356820137373, 0.01434374456967558, 0.014307490689913195, 0.015551032137685002, 0.012480978087379356, 0.016616392391756416, 0.014699208012023119, 0.013750460146985522, 0.014393297815020172, 0.014559601958230004, 0.016294872701207667, 0.01445426918280292, 0.013509573995246478, 0.016302807431171185, 0.012907552626762934, 0.016507006982781583, 0.010502965177875957, 0.013614008164272998, 0.016958939320330033, 0.011505877209119653, 0.02204670571231096, 0.01853596796823657, 0.01982803897170832, 0.017573116961929088, 0.01590755621913326, 0.016202871624440902, 0.015132604671908932, 0.01769513277141203, 0.01803599531520795, 0.01898266109466565, 0.01837241199318752, 0.01582106759937862, 0.019528324939777107, 0.0151928962342884, 0.013004103221436235, 0.016905429587091007, 0.017976585977890217, 0.01835972854970795, 0.01404643277666877, 0.013619058441575432, 0.014128515245825172, 0.020073252360296796, 0.015960983506689265, 0.01746272032077168, 0.018596978884139883, 0.01562834532353077, 0.010784499115242992, 0.016705554094725766, 0.017703083860139586, 0.019292607947033084, 0.015509601285238098, 0.01619815875725461, 0.014626823061288003, 0.017473868663738578, 0.010174533781158058, 0.01779916535537798, 0.0206751172789914, 0.019958746103794302, 0.011918244332018798, 0.017910896345710546, 0.01731525306203275, 0.014786721132962125, 0.021540532284651456, 0.011545647431223117, 0.015019653317834811, 0.014708994200933955, 0.014087296376838455, 0.008696333843122633, 0.016544638940125887, 0.01660928534108699, 0.014966628972070658, 0.016984535164684438, 0.010494795031118052, 0.009481850264590913, 0.016352632849223444, 0.014204938551723806, 0.016623023478801294, 0.015379192798681954, 0.010871545788144844, 0.018298466333938085, 0.012718859092263371, 0.015715025329703987, 0.017077187016909898, 0.018757595829165172, 0.016708361829072524, 0.021124684994567183, 0.0151937470231298, 0.018010130174148216, 0.015250735931606868, 0.01389670675243572, 0.012959807549180387, 0.012759940053309054, 0.009787915622226352, 0.01712632231532194, 0.01687345502959106, 0.015735147652113726, 0.01853704228073898, 0.017522132396552214], "moving_avg_accuracy_train": [0.05834011051125876, 0.12421057052302509, 0.1891738183918189, 0.2522809814723491, 0.31117388334818546, 0.36578657011026616, 0.41587949321990214, 0.46199996014352834, 0.5045567340973371, 0.5436901978807559, 0.5799912210917113, 0.612864538074456, 0.6428200778243655, 0.6703730486433501, 0.6956170788542273, 0.7189225714463793, 0.7402182771685741, 0.7594681897733299, 0.7769951467223073, 0.793455398972834, 0.808534656913775, 0.821952421092737, 0.8344677898811654, 0.8458223025943225, 0.8560855779123634, 0.8657317879378952, 0.8743760664334652, 0.8824604755247071, 0.8899551158413763, 0.8965304120680083, 0.9028132270350724, 0.9083887054459062, 0.9136996408144754, 0.9184398830187883, 0.9228107326990985, 0.9270654400447295, 0.9309411075343502, 0.9344710249047616, 0.9378618281797894, 0.9408089194308857, 0.9436775403961581, 0.946129235126845, 0.9483171231451683, 0.9501884579652029, 0.9521843013413478, 0.9542061358632208, 0.956128021382888, 0.957757881147054, 0.958922377443109, 0.9600866815500347, 0.961427523996268, 0.962487761774059, 0.9634977432966808, 0.9645114665099253, 0.9654470688899405, 0.9662286932117252, 0.9670505934953423, 0.9678973326934732, 0.9685430684336774, 0.9691685165736984, 0.9696941093723086, 0.970455569489895, 0.9710223370552558, 0.9716649613462234, 0.972064250700942, 0.9722980891332934, 0.9727806221819426, 0.9731402448220909, 0.9735384181065855, 0.9739874188173832, 0.9741009119047295, 0.9745262874166836, 0.9749114505262518, 0.9752743012688925, 0.9756869334920402, 0.9760001737726349, 0.9761194738037786, 0.9763941463996374, 0.9766668562751776, 0.9769356547977152, 0.977177429272724, 0.9773183684871551, 0.9775010173515718, 0.9774561018878708, 0.9775853417359978, 0.9775040920481399, 0.9778286398731341, 0.9779509610037147, 0.9782982151998088, 0.9784432972131889, 0.978766930474059, 0.9790000356397853, 0.979333027127025, 0.9793444010131597, 0.9795523112083092, 0.979764934923211, 0.9799888123011373, 0.9799044528329746, 0.9799006810223607, 0.9801947612451523, 0.9804478077016171, 0.9806406362314739, 0.9806026654643158, 0.9809009880536355, 0.9808463547971368, 0.98089709416746, 0.981031079206694, 0.9810609849384333, 0.9812158914279787, 0.9813947266518751, 0.9815604007486385, 0.981428056283317, 0.981397374016927, 0.9816092142557381, 0.9818300613563733, 0.9819916574148112, 0.9820184752293009, 0.9819706758445215, 0.9819369209446392, 0.9820017644894795, 0.9821020484560447, 0.9821527043985538, 0.9822238713837169, 0.9821600745346586, 0.9822352989991052, 0.9822633292920704, 0.9823397458783673, 0.9821946792131957, 0.9821733291109515, 0.9823262110796551, 0.9823382468157742, 0.9824188694913858, 0.9824798402042073, 0.9827346045457283, 0.9829173173792691, 0.9830259914068461, 0.9830913180459695, 0.9831827001533328, 0.9832299586713605, 0.9832609376911754, 0.9834143407959043, 0.9835570899365981, 0.9836668548262901, 0.9837401747365645, 0.9838712307736592, 0.9838288180368249, 0.9838092117153314, 0.9838195038605204, 0.9837682408245053, 0.9837989421492622, 0.983668282978402, 0.9838624759580167, 0.9839371240455853, 0.9840810011862926, 0.9840338874498616, 0.9840030026846651, 0.9839846511865014, 0.9839540397500217, 0.984040529895313, 0.9841856921950951, 0.9841466744994413, 0.9841440746078673, 0.9841719616399747, 0.9842877407724426, 0.9844268552726254, 0.9844961826537239, 0.9845446264038553, 0.9845394337027924, 0.9847113994837221, 0.9848941065210919, 0.9849167448261625, 0.9849650571352592, 0.9849550958396458, 0.9849088561950038, 0.9848276408874266, 0.9848476612094443, 0.9847377602659179], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 261590114, "moving_var_accuracy_train": [0.030632116450192967, 0.06661916232462904, 0.09793925825512725, 0.12398795871826822, 0.14280452786865336, 0.15536698508014632, 0.16241399508314283, 0.1653164727981083, 0.16508453660249772, 0.16235893483144126, 0.15798291992375807, 0.15191052265641436, 0.1447954796461486, 0.13714842749010012, 0.12916893429267925, 0.12114035472807701, 0.11310788299512621, 0.10513212691323019, 0.09738366220092895, 0.0900837551181948, 0.08312183578682023, 0.07642997976819835, 0.07019669189457182, 0.06433734733569384, 0.05885162598440964, 0.05380390769667872, 0.049096028883391624, 0.04477464502824346, 0.0408027072267053, 0.03711154718824642, 0.03375565634461506, 0.030659864345740626, 0.027847732221568642, 0.025265188064811723, 0.02291060820068136, 0.020782470191985918, 0.01883941035919826, 0.017067612173055823, 0.015464328877399689, 0.013996064111240318, 0.012670518576297892, 0.011457563982140404, 0.010354889269752867, 0.00935091738885565, 0.008451676167008999, 0.007643298883812635, 0.00691221179098773, 0.006244898597546582, 0.005632613202403657, 0.0050815523186439285, 0.00458957781297012, 0.004140736968982207, 0.003735843836168323, 0.0033715081653291267, 0.003042235515117626, 0.002743510392829514, 0.002475239034232451, 0.002234167836236069, 0.0020145038244280557, 0.0018165741103679514, 0.0016374029294407148, 0.001478881030092716, 0.0013338839563417496, 0.0012042122545216493, 0.00108522591696861, 0.0009771954489837498, 0.0008815714473727237, 0.0007945782586252164, 0.000716547310443061, 0.0006467069941434265, 0.0005821522208569622, 0.0005255654977067984, 0.0004743441035248692, 0.0004280946391252958, 0.00038681756337698424, 0.00034901888229976985, 0.0003142450865466708, 0.0002834995832062459, 0.00025581896097157537, 0.0002308873386858835, 0.0002083246988881874, 0.00018767100375884899, 0.0001692041488520183, 0.00015230189055673172, 0.00013722202794615336, 0.00012355923875753094, 0.0001121512964981532, 0.00010107082897921663, 9.204901537163907e-05, 8.30335529499331e-05, 7.56728440428126e-05, 6.859460180312573e-05, 6.273309159797971e-05, 5.6460946725754e-05, 5.1203891896402596e-05, 4.6490382304010223e-05, 4.2292433796733256e-05, 3.812723909587836e-05, 3.431464322528828e-05, 3.166152749969397e-05, 2.9071667331889306e-05, 2.6499146176040796e-05, 2.3862207570863885e-05, 2.2276954119462774e-05, 2.007612184195732e-05, 1.809168001106878e-05, 1.644407992660892e-05, 1.4807721109065763e-05, 1.3542913182688641e-05, 1.2476460200174725e-05, 1.1475845337202577e-05, 1.0485896320993615e-05, 9.445779302131703e-06, 8.905087952934776e-06, 8.45354013437175e-06, 7.843205695858788e-06, 7.065357882838957e-06, 6.379385125222726e-06, 5.751701152094961e-06, 5.214373204652515e-06, 4.783447749737649e-06, 4.328197195367379e-06, 3.9409601338254516e-06, 3.583494461990793e-06, 3.276073496253306e-06, 2.9555374225413446e-06, 2.712539132238578e-06, 2.630684255110626e-06, 2.3717182713921297e-06, 2.3449025114453285e-06, 2.111715990796135e-06, 1.9590445341214596e-06, 1.796596931107041e-06, 2.201081065391779e-06, 2.281428774717426e-06, 2.159576295673887e-06, 1.982026794118981e-06, 1.8589803206223173e-06, 1.6931825962956953e-06, 1.5325016336843302e-06, 1.591044083180091e-06, 1.6153355293814749e-06, 1.5622369555251684e-06, 1.4543955431563056e-06, 1.463537152571518e-06, 1.3333729995263857e-06, 1.2034953701562995e-06, 1.0840991874139745e-06, 9.993403584259707e-07, 9.078894646598499e-07, 9.70746888562611e-07, 1.2130704196906438e-06, 1.1419144105204005e-06, 1.2140286540311963e-06, 1.1126031260725095e-06, 1.0099276319564855e-06, 9.119658661245429e-07, 8.292028199021819e-07, 8.136074450045507e-07, 9.218955400063717e-07, 8.434074111729575e-07, 7.591275049814248e-07, 6.902139335210751e-07, 7.41835807804398e-07, 8.418278244740467e-07, 8.009016139564137e-07, 7.419326249020022e-07, 6.679820397107593e-07, 8.673339040365195e-07, 1.0810372671728024e-06, 9.775459761637417e-07, 9.007980914395431e-07, 8.116113289882654e-07, 7.496931387189546e-07, 7.340871605107198e-07, 6.642857641029079e-07, 7.065611441848881e-07], "duration": 106651.573505, "accuracy_train": [0.5834011051125877, 0.7170447106289222, 0.7738430492109634, 0.8202454491971208, 0.8412100002307125, 0.8573007509689923, 0.8667158012066261, 0.8770841624561646, 0.8875676996816169, 0.8958913719315246, 0.9067004299903102, 0.9087243909191584, 0.9124199355735512, 0.9183497860142118, 0.9228133507521227, 0.9286720047757475, 0.9318796286683279, 0.9327174032161315, 0.9347377592631044, 0.9415976692275747, 0.9442479783822444, 0.942712298703396, 0.947106108977021, 0.9480129170127353, 0.9484550557747323, 0.9525476781676817, 0.9521745728935955, 0.9552201573458842, 0.957406878691399, 0.9557080781076966, 0.959358561738649, 0.958568011143411, 0.9614980591315985, 0.9611020628576044, 0.96214837982189, 0.965357806155408, 0.9658221149409376, 0.9662402812384644, 0.9683790576550388, 0.967332740690753, 0.9694951290836102, 0.9681944877030271, 0.9680081153100776, 0.967030471345515, 0.9701468917266519, 0.9724026465600776, 0.973424991059893, 0.9724266190245479, 0.9694028441076044, 0.9705654185123662, 0.9734951060123662, 0.9720299017741787, 0.9725875770002769, 0.9736349754291252, 0.9738674903100776, 0.9732633121077889, 0.9744476960478959, 0.9755179854766519, 0.974354690095515, 0.9747975498338871, 0.9744244445598007, 0.9773087105481728, 0.9761232451435032, 0.9774485799649317, 0.975657854893411, 0.9744026350244556, 0.977123419619786, 0.9763768485834257, 0.9771219776670359, 0.9780284252145626, 0.9751223496908453, 0.978354667024271, 0.9783779185123662, 0.9785399579526578, 0.9794006235003692, 0.9788193362979882, 0.9771931740840717, 0.9788661997623662, 0.9791212451550388, 0.9793548415005537, 0.9793533995478036, 0.9785868214170359, 0.9791448571313216, 0.9770518627145626, 0.97874850036914, 0.9767728448574198, 0.9807495702980805, 0.9790518511789406, 0.9814235029646549, 0.9797490353336102, 0.98167962982189, 0.9810979821313216, 0.9823299505121816, 0.9794467659883721, 0.9814235029646549, 0.9816785483573275, 0.9820037087024732, 0.9791452176195091, 0.9798667347268365, 0.9828414832502769, 0.9827252258098007, 0.9823760930001846, 0.980260928559893, 0.9835858913575121, 0.980354655488649, 0.9813537485003692, 0.9822369445598007, 0.9813301365240864, 0.9826100498338871, 0.9830042436669435, 0.9830514676195091, 0.9802369560954227, 0.9811212336194168, 0.9835157764050388, 0.9838176852620893, 0.983446021940753, 0.9822598355597084, 0.9815404813815062, 0.9816331268456996, 0.9825853563930418, 0.9830046041551311, 0.982608607881137, 0.9828643742501846, 0.9815859028931341, 0.9829123191791252, 0.982515601928756, 0.9830274951550388, 0.9808890792266519, 0.981981178190753, 0.9837021487979882, 0.9824465684408453, 0.98314447357189, 0.9830285766196014, 0.9850274836194168, 0.984561732881137, 0.9840040576550388, 0.9836792577980805, 0.9840051391196014, 0.9836552853336102, 0.9835397488695091, 0.9847949687384644, 0.9848418322028424, 0.984654738833518, 0.984400053929033, 0.9850507351075121, 0.9834471034053157, 0.98363275482189, 0.9839121331672205, 0.9833068735003692, 0.9840752540720746, 0.9824923504406607, 0.9856102127745479, 0.9846089568337025, 0.9853758954526578, 0.9836098638219823, 0.9837250397978959, 0.9838194877030271, 0.9836785368217055, 0.9848189412029347, 0.9854921528931341, 0.9837955152385567, 0.9841206755837025, 0.9844229449289406, 0.9853297529646549, 0.985678885774271, 0.9851201290836102, 0.9849806201550388, 0.9844926993932264, 0.9862590915120893, 0.9865384698574198, 0.9851204895717978, 0.9853998679171282, 0.9848654441791252, 0.9844926993932264, 0.9840967031192323, 0.9850278441076044, 0.9837486517741787], "end": "2016-01-24 21:11:50.495000", "learning_rate_per_epoch": [0.0031337763648480177, 0.0022159144282341003, 0.001809286535717547, 0.0015668881824240088, 0.0014014673652127385, 0.0012793587520718575, 0.0011844560503959656, 0.0011079572141170502, 0.001044592121616006, 0.0009909870568662882, 0.000944869068916887, 0.0009046432678587735, 0.0008691531256772578, 0.000837536936160177, 0.0008091375348158181, 0.0007834440912120044, 0.000760052353143692, 0.0007386381621472538, 0.000718937604688108, 0.0007007336826063693, 0.0006838460685685277, 0.0006681233644485474, 0.0006534375133924186, 0.0006396793760359287, 0.000626755238045007, 0.0006145840743556619, 0.000603095511905849, 0.0005922280251979828, 0.0005819276557303965, 0.0005721466732211411, 0.0005628428189083934, 0.0005539786070585251, 0.0005455204518511891, 0.0005374382017180324, 0.0005297048483043909, 0.000522296060808003, 0.0005151896621100605, 0.0005083656287752092, 0.0005018058000132442, 0.0004954935284331441, 0.0004894136218354106, 0.00048355216858908534, 0.0004778963921125978, 0.0004724345344584435, 0.00046715576900169253, 0.0004620501131284982, 0.00045710825361311436, 0.00045232163392938673, 0.00044768230873160064, 0.00044318288564682007, 0.0004388164379633963, 0.0004345765628386289, 0.0004304572648834437, 0.00042645292705856264, 0.00042255831067450345, 0.0004187684680800885, 0.0004150788008701056, 0.0004114849725738168, 0.000407982908654958, 0.00040456876740790904, 0.0004012389399576932, 0.00039798999205231667, 0.0003948186931665987, 0.0003917220456060022, 0.00038869710988365114, 0.000385741179343313, 0.00038285169284790754, 0.000380026176571846, 0.0003772623313125223, 0.0003745579160749912, 0.00037191080627962947, 0.0003693190810736269, 0.0003667807613965124, 0.0003642941010184586, 0.00036185732460580766, 0.000359468802344054, 0.00035712693352252245, 0.0003548302920535207, 0.0003525773645378649, 0.00035036684130318463, 0.00034819735446944833, 0.00034606768167577684, 0.00034397662966512144, 0.00034192303428426385, 0.00033990576048381627, 0.0003379237896297127, 0.0003359760739840567, 0.0003340616822242737, 0.0003321796248201281, 0.0003303290286567062, 0.00032850902061909437, 0.0003267187566962093, 0.0003249574510846287, 0.00032322434708476067, 0.0003215186588931829, 0.00031983968801796436, 0.00031818676507100463, 0.0003165591915603727, 0.000314956356305629, 0.0003133776190225035, 0.000311822397634387, 0.00031029008096084, 0.0003087801451329142, 0.00030729203717783093, 0.000305825233226642, 0.0003043792676180601, 0.00030295358737930655, 0.0003015477559529245, 0.00030016133678145707, 0.0002987938350997865, 0.0002974448725581169, 0.0002961140125989914, 0.000294800876872614, 0.00029350502882152796, 0.00029222614830359817, 0.00029096382786519825, 0.00028971771826036274, 0.0002884874993469566, 0.0002872727927751839, 0.00028607333661057055, 0.0002848887525033206, 0.0002837187785189599, 0.00028256309451535344, 0.0002814214094541967, 0.0002802934614010155, 0.0002791789884213358, 0.00027807767037302256, 0.00027698930352926254, 0.00027591362595558167, 0.00027485034661367536, 0.0002737992908805609, 0.00027276022592559457, 0.00027173286071047187, 0.0002707170497160405, 0.00026971250190399587, 0.0002687191008590162, 0.0002677365846466273, 0.00026676474954001606, 0.0002658034209161997, 0.00026485242415219545, 0.00026391155552119017, 0.00026298066950403154, 0.0002620595332700759, 0.0002611480304040015, 0.000260245957178995, 0.00025935316807590425, 0.0002584695175755769, 0.0002575948310550302, 0.00025672896299511194, 0.00025587176787666976, 0.0002550231001805514, 0.0002541828143876046, 0.0002533507940825075, 0.0002525268937461078, 0.0002517109678592533, 0.0002509029000066221, 0.00025010257377289236, 0.00024930984363891184, 0.00024852462229318917, 0.00024774676421657205, 0.00024697615299373865, 0.0002462127013131976, 0.0002454562927596271, 0.0002447068109177053, 0.0002439641539240256, 0.00024322820536326617, 0.00024249889247585088, 0.00024177608429454267, 0.0002410597080597654, 0.00024034966190811247, 0.0002396458585280925, 0.0002389481960562989, 0.00023825660173315555, 0.0002375709591433406, 0.00023689122463110834, 0.00023621726722922176, 0.00023554904328193516], "accuracy_valid": [0.5733863187123494, 0.6953198536332832, 0.7523605162838856, 0.7904567488704819, 0.8034064970820783, 0.8121043745293675, 0.8244849515248494, 0.825329148625753, 0.8350859492658133, 0.8348712231739458, 0.8447912744728916, 0.8415247905685241, 0.8438853068524097, 0.8480254023908133, 0.8510271554969879, 0.8519728327371988, 0.8534273814006024, 0.8523905191076807, 0.8562158791415663, 0.8563570689006024, 0.8575174722326807, 0.8549128153237951, 0.8583204889871988, 0.8584837396460843, 0.8566115046121988, 0.8613119470067772, 0.8599279932228916, 0.8615252023719879, 0.8626341302710843, 0.8599279932228916, 0.8608133706701807, 0.8604059793862951, 0.8657373635165663, 0.8615046121987951, 0.8598971079631024, 0.8676493081701807, 0.8650755365210843, 0.8635798075112951, 0.8668257012424698, 0.8650858316076807, 0.8669168862951807, 0.8658991434487951, 0.8624605845256024, 0.8647093255835843, 0.8625620646649097, 0.8665300851844879, 0.8675169427710843, 0.8664080148719879, 0.8649225809487951, 0.8630106362951807, 0.8669271813817772, 0.8651976068335843, 0.8627459054969879, 0.8668962961219879, 0.8692465173192772, 0.8645357798381024, 0.8678831537085843, 0.8671507318335843, 0.8659712090549698, 0.8663462443524097, 0.8668551157756024, 0.8656344126506024, 0.8676493081701807, 0.8702127847326807, 0.8690935617469879, 0.8640886789344879, 0.8693274072853916, 0.8641283885542168, 0.8677096079631024, 0.8708128412085843, 0.8634371470256024, 0.8734380882906627, 0.8696833231362951, 0.8682287744728916, 0.8702333749058735, 0.8743734704442772, 0.8696421427899097, 0.8685641001506024, 0.8745970208960843, 0.8702833796121988, 0.8667433405496988, 0.8715658532567772, 0.8723894601844879, 0.8684729150978916, 0.8684523249246988, 0.8686258706701807, 0.8716570383094879, 0.8723997552710843, 0.8731218820594879, 0.8701613092996988, 0.8742308099585843, 0.8677507883094879, 0.8699068735881024, 0.8710672769201807, 0.8690729715737951, 0.8705789956701807, 0.8741293298192772, 0.867211031626506, 0.8721864999058735, 0.8681464137801205, 0.8696627329631024, 0.8697642131024097, 0.8716364481362951, 0.8735895731362951, 0.8717585184487951, 0.8732439523719879, 0.8737013483621988, 0.8721041392131024, 0.869042086314006, 0.8699068735881024, 0.8710466867469879, 0.8732027720256024, 0.8695612528237951, 0.8727556711219879, 0.8737219385353916, 0.8700289439006024, 0.868431734751506, 0.8717894037085843, 0.8696936182228916, 0.8720232492469879, 0.8717482233621988, 0.8718908838478916, 0.8721247293862951, 0.8729586314006024, 0.8725115304969879, 0.8720644295933735, 0.8774046380835843, 0.8708834360881024, 0.871849703501506, 0.8701716043862951, 0.8706495905496988, 0.8709040262612951, 0.8727247858621988, 0.8759912697665663, 0.8733248423381024, 0.8703539744917168, 0.8706084102033133, 0.8733969079442772, 0.8716158579631024, 0.8690626764871988, 0.8739660791603916, 0.8701098338667168, 0.8757368340549698, 0.8700598291603916, 0.8760927499058735, 0.8725012354103916, 0.8732954278049698, 0.8729792215737951, 0.8723482798381024, 0.8699171686746988, 0.8746779108621988, 0.8712202324924698, 0.8762251153049698, 0.8731012918862951, 0.8723894601844879, 0.8697848032756024, 0.8698053934487951, 0.8752382577183735, 0.8721041392131024, 0.8751058923192772, 0.8735998682228916, 0.8743322900978916, 0.872215914439006, 0.8748205713478916, 0.8717070430158133, 0.8715143778237951, 0.8711481668862951, 0.8720026590737951, 0.8732542474585843, 0.8743631753576807, 0.8715555581701807, 0.8748823418674698, 0.8727453760353916, 0.8746985010353916, 0.8685949854103916, 0.8715143778237951, 0.8697950983621988], "accuracy_test": 0.8453005420918368, "start": "2016-01-23 15:34:18.921000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0], "accuracy_train_last": 0.9837486517741787, "batch_size_eval": 1024, "accuracy_train_std": [0.0197652612168563, 0.022019472064318237, 0.019740259332810053, 0.019079807835663567, 0.020381560822598226, 0.018352621230736627, 0.018499824280940717, 0.017135015397623278, 0.016595874554986154, 0.01583319153010668, 0.01564237387496093, 0.015493436389096024, 0.015302145003029293, 0.013435753907260978, 0.014407427788829178, 0.01401325082678, 0.013876729548188374, 0.012551083714878574, 0.013600550141988738, 0.012353009118961412, 0.011715160514680722, 0.012473257782710752, 0.01197362928618676, 0.013129203919871492, 0.011804910393488398, 0.010386707657569653, 0.010743425409938284, 0.009947694820419104, 0.008969668987109077, 0.010620866590733878, 0.009486601348414602, 0.009383757955384195, 0.009300952631513301, 0.009423008290669693, 0.008577358084624174, 0.00865518540629752, 0.00779486977187211, 0.008181041167717387, 0.008333352746228378, 0.008160720643359935, 0.007229780683670649, 0.00744388267027643, 0.007143717946473506, 0.008403685913879175, 0.0066087502949831265, 0.007312821249710667, 0.007070182672879689, 0.006783704221254249, 0.008527660729433229, 0.00737946002899612, 0.007320042414573043, 0.006879324879364634, 0.008028712956719464, 0.007793292668794324, 0.006675230660559135, 0.007296762544917114, 0.006832135512235856, 0.0065216750150134915, 0.006168977426841419, 0.00731887492849727, 0.006977886339209245, 0.00577293613833254, 0.006312116111189062, 0.006149805318156491, 0.005803302113181389, 0.006442165892005301, 0.005553262775500273, 0.007499854225534861, 0.007230148450675762, 0.006094975415672321, 0.006485923165299174, 0.006094395165614001, 0.006837650150889937, 0.005864148251768404, 0.00518775949261343, 0.006261538777537576, 0.005796026646865415, 0.006306301689719791, 0.0058774400260660645, 0.005401346040238893, 0.007052092033297784, 0.0061772739066827355, 0.005957952302477263, 0.006361288480268964, 0.00578573213625749, 0.0065764554746205375, 0.00525048782944633, 0.0055531269224347, 0.005706930045118446, 0.005131086116356008, 0.004617472775936467, 0.0050722533979653955, 0.0055748973724376195, 0.005431643142122361, 0.005190166737862103, 0.005570393172722887, 0.0062943030253072565, 0.005095376739633587, 0.004777873828183817, 0.0046964148578676305, 0.004988772612536799, 0.00469982856107976, 0.00584499983023104, 0.004324404818144435, 0.006019791706460877, 0.00497266416685685, 0.004730733197288975, 0.005301152888644591, 0.00456947944301789, 0.004986761067325809, 0.0054347414382994175, 0.006077421900429974, 0.0049077274791188555, 0.004339726924206509, 0.004729422615807223, 0.0047863559235304335, 0.005384210227550085, 0.005234958210655708, 0.005180315222108499, 0.004747693701933647, 0.004848487246326275, 0.005023398223729757, 0.004593468222555702, 0.0043369056337764075, 0.004720930033456034, 0.00587965066959997, 0.00489898170796275, 0.004919356189056389, 0.004792612826870436, 0.003981265134623857, 0.004511095903789298, 0.004244051594575946, 0.004582749820784978, 0.004620577885100251, 0.004720155116690074, 0.005546571542826761, 0.004574576933186004, 0.004879854642864882, 0.004908600179272266, 0.0050118094495627055, 0.005009581514599823, 0.004185023410447266, 0.004913636974008939, 0.0042830832233921754, 0.0044494403390065865, 0.004317210461939884, 0.005195252341066154, 0.004328983230042648, 0.004280013370269352, 0.00452935987158708, 0.004663144833389451, 0.0042402251279260224, 0.004247620626689917, 0.004025943863989823, 0.004525551408929735, 0.005246690076998164, 0.005426516994057452, 0.004872501711586877, 0.0044371415916418635, 0.004309667762247621, 0.004457753086392845, 0.004259796771714313, 0.004507766425151285, 0.0045252525464924455, 0.004325691371067853, 0.004273986672564627, 0.0051162119104797305, 0.003633353740334901, 0.0041306104655196765, 0.004584938015008376, 0.00436867327892382, 0.00466077772074481, 0.0038511023302927397, 0.004544117261131316, 0.00402459277485667, 0.004077370707021685, 0.004507322463810847], "accuracy_test_std": 0.009484910441972395, "error_valid": [0.42661368128765065, 0.3046801463667168, 0.24763948371611444, 0.2095432511295181, 0.19659350291792166, 0.18789562547063254, 0.17551504847515065, 0.17467085137424698, 0.16491405073418675, 0.1651287768260542, 0.1552087255271084, 0.15847520943147586, 0.1561146931475903, 0.15197459760918675, 0.14897284450301207, 0.14802716726280118, 0.14657261859939763, 0.1476094808923193, 0.14378412085843373, 0.14364293109939763, 0.1424825277673193, 0.14508718467620485, 0.14167951101280118, 0.14151626035391573, 0.14338849538780118, 0.13868805299322284, 0.1400720067771084, 0.13847479762801207, 0.13736586972891573, 0.1400720067771084, 0.1391866293298193, 0.13959402061370485, 0.13426263648343373, 0.13849538780120485, 0.14010289203689763, 0.1323506918298193, 0.13492446347891573, 0.13642019248870485, 0.13317429875753017, 0.1349141683923193, 0.1330831137048193, 0.13410085655120485, 0.13753941547439763, 0.13529067441641573, 0.1374379353350903, 0.13346991481551207, 0.13248305722891573, 0.13359198512801207, 0.13507741905120485, 0.1369893637048193, 0.13307281861822284, 0.13480239316641573, 0.13725409450301207, 0.13310370387801207, 0.13075348268072284, 0.13546422016189763, 0.13211684629141573, 0.13284926816641573, 0.13402879094503017, 0.1336537556475903, 0.13314488422439763, 0.13436558734939763, 0.1323506918298193, 0.1297872152673193, 0.13090643825301207, 0.13591132106551207, 0.1306725927146084, 0.1358716114457832, 0.13229039203689763, 0.12918715879141573, 0.13656285297439763, 0.12656191170933728, 0.13031667686370485, 0.1317712255271084, 0.1297666250941265, 0.12562652955572284, 0.1303578572100903, 0.13143589984939763, 0.12540297910391573, 0.12971662038780118, 0.13325665945030118, 0.12843414674322284, 0.12761053981551207, 0.1315270849021084, 0.13154767507530118, 0.1313741293298193, 0.12834296169051207, 0.12760024472891573, 0.12687811794051207, 0.12983869070030118, 0.12576919004141573, 0.13224921169051207, 0.13009312641189763, 0.1289327230798193, 0.13092702842620485, 0.1294210043298193, 0.12587067018072284, 0.13278896837349397, 0.1278135000941265, 0.13185358621987953, 0.13033726703689763, 0.1302357868975903, 0.12836355186370485, 0.12641042686370485, 0.12824148155120485, 0.12675604762801207, 0.12629865163780118, 0.12789586078689763, 0.13095791368599397, 0.13009312641189763, 0.12895331325301207, 0.12679722797439763, 0.13043874717620485, 0.12724432887801207, 0.1262780614646084, 0.12997105609939763, 0.13156826524849397, 0.12821059629141573, 0.1303063817771084, 0.12797675075301207, 0.12825177663780118, 0.1281091161521084, 0.12787527061370485, 0.12704136859939763, 0.12748846950301207, 0.1279355704066265, 0.12259536191641573, 0.12911656391189763, 0.12815029649849397, 0.12982839561370485, 0.12935040945030118, 0.12909597373870485, 0.12727521413780118, 0.12400873023343373, 0.12667515766189763, 0.1296460255082832, 0.12939158979668675, 0.12660309205572284, 0.12838414203689763, 0.13093732351280118, 0.1260339208396084, 0.1298901661332832, 0.12426316594503017, 0.1299401708396084, 0.12390725009412651, 0.1274987645896084, 0.12670457219503017, 0.12702077842620485, 0.12765172016189763, 0.13008283132530118, 0.12532208913780118, 0.12877976750753017, 0.12377488469503017, 0.12689870811370485, 0.12761053981551207, 0.13021519672439763, 0.13019460655120485, 0.12476174228162651, 0.12789586078689763, 0.12489410768072284, 0.1264001317771084, 0.1256677099021084, 0.12778408556099397, 0.1251794286521084, 0.12829295698418675, 0.12848562217620485, 0.12885183311370485, 0.12799734092620485, 0.12674575254141573, 0.1256368246423193, 0.1284444418298193, 0.12511765813253017, 0.1272546239646084, 0.1253014989646084, 0.1314050145896084, 0.12848562217620485, 0.13020490163780118], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-3, 3], "momentum": 0.5802557329839566, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.0031337762492693764, "patience_threshold": 1, "do_flip": true, "batch_size": 16, "optimization": "adam", "nb_data_augmentation": 1, "learning_rate_decay_method": "sqrt", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 1.2861588528389085e-07, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.08047452308364197}, "accuracy_valid_max": 0.8774046380835843, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-3, 3)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8697950983621988, "loss_train": [1.4692578315734863, 1.020334005355835, 0.8045470118522644, 0.6925528645515442, 0.6216148138046265, 0.5680911540985107, 0.5291925668716431, 0.49518021941185, 0.4626695513725281, 0.441501259803772, 0.42038750648498535, 0.3986719846725464, 0.3766839802265167, 0.36304065585136414, 0.34813156723976135, 0.33507904410362244, 0.3212531805038452, 0.31110790371894836, 0.29843130707740784, 0.28943800926208496, 0.2787398099899292, 0.26981812715530396, 0.2628300189971924, 0.25328412652015686, 0.2471320629119873, 0.239101380109787, 0.23502972722053528, 0.22861164808273315, 0.22390654683113098, 0.21874894201755524, 0.21283769607543945, 0.20765620470046997, 0.20238859951496124, 0.19939586520195007, 0.19846786558628082, 0.19165998697280884, 0.1882084161043167, 0.1833989918231964, 0.1818981170654297, 0.17631570994853973, 0.17407463490962982, 0.17116360366344452, 0.16587120294570923, 0.16776633262634277, 0.16375716030597687, 0.16163639724254608, 0.15571868419647217, 0.15312935411930084, 0.15240491926670074, 0.14809823036193848, 0.14888930320739746, 0.14526128768920898, 0.1424255520105362, 0.14223073422908783, 0.13882148265838623, 0.1408265233039856, 0.13693355023860931, 0.13362757861614227, 0.13354945182800293, 0.1311797797679901, 0.13112863898277283, 0.12718647718429565, 0.12438543140888214, 0.12625424563884735, 0.1230509877204895, 0.11908436566591263, 0.11841415613889694, 0.11928407102823257, 0.11559917032718658, 0.11610398441553116, 0.11528618633747101, 0.11556535959243774, 0.11377759277820587, 0.1108304113149643, 0.11170191317796707, 0.10708683729171753, 0.10878685116767883, 0.1068786010146141, 0.10297691822052002, 0.10604728758335114, 0.10200931876897812, 0.10315391421318054, 0.10015356540679932, 0.0980755016207695, 0.0988481268286705, 0.0962129682302475, 0.09635618329048157, 0.09578213095664978, 0.09427641332149506, 0.09378119558095932, 0.09458453953266144, 0.09192358702421188, 0.09161678701639175, 0.09027864784002304, 0.08777420222759247, 0.08773620426654816, 0.08859797567129135, 0.08787315338850021, 0.0852808803319931, 0.0847434550523758, 0.08426862955093384, 0.08330057561397552, 0.08276444673538208, 0.08306311815977097, 0.08154790848493576, 0.08182740211486816, 0.08096802979707718, 0.07942333817481995, 0.08000265806913376, 0.07801760733127594, 0.0769311934709549, 0.07576532661914825, 0.0769587829709053, 0.07466018944978714, 0.07340863347053528, 0.07545291632413864, 0.07260724157094955, 0.07311549037694931, 0.07263102382421494, 0.07245901972055435, 0.07179837673902512, 0.0702386349439621, 0.07113675028085709, 0.07047448307275772, 0.07184535264968872, 0.07009942829608917, 0.0677112564444542, 0.06664124876260757, 0.0691523477435112, 0.06815441697835922, 0.06707809120416641, 0.06447745859622955, 0.06573286652565002, 0.06435166299343109, 0.0668683871626854, 0.06451337784528732, 0.0658414363861084, 0.06567846238613129, 0.06230362877249718, 0.06270945072174072, 0.06485510617494583, 0.06163680553436279, 0.061522483825683594, 0.061302442103624344, 0.060976117849349976, 0.06315501034259796, 0.06077518314123154, 0.0609288215637207, 0.059485744684934616, 0.059468790888786316, 0.060536939650774, 0.05928383395075798, 0.057328734546899796, 0.05764709785580635, 0.05670967698097229, 0.05568413436412811, 0.05675804242491722, 0.05609431117773056, 0.05489802733063698, 0.05649825930595398, 0.05519780516624451, 0.052893441170454025, 0.056617241352796555, 0.05422321334481239, 0.05571365728974342, 0.05406828224658966, 0.05346730723977089, 0.051865845918655396, 0.05338029935956001, 0.05333631858229637, 0.05163334310054779, 0.05219591408967972, 0.05313077196478844, 0.05146991088986397, 0.05073826014995575, 0.052336785942316055, 0.050137732177972794], "accuracy_train_first": 0.5834011051125877, "model": "residualv5", "loss_std": [0.3323821425437927, 0.2701312303543091, 0.25464096665382385, 0.24332594871520996, 0.2323901504278183, 0.22657571732997894, 0.2154904305934906, 0.20978812873363495, 0.20364437997341156, 0.20060135424137115, 0.19470424950122833, 0.18986716866493225, 0.183027982711792, 0.17856432497501373, 0.17560584843158722, 0.16913169622421265, 0.16280695796012878, 0.16053783893585205, 0.1585739552974701, 0.15427851676940918, 0.1490747034549713, 0.14447815716266632, 0.1438782960176468, 0.13971929252147675, 0.13660655915737152, 0.13339798152446747, 0.13236217200756073, 0.128621906042099, 0.13113349676132202, 0.12909573316574097, 0.12178057432174683, 0.12235365808010101, 0.12077146023511887, 0.11732244491577148, 0.12175558507442474, 0.11464749276638031, 0.11445879936218262, 0.1107783317565918, 0.11361183971166611, 0.10953854024410248, 0.10727039724588394, 0.1069977805018425, 0.10468211770057678, 0.10507634282112122, 0.10678409785032272, 0.10366524755954742, 0.10076955705881119, 0.09669727832078934, 0.1003967747092247, 0.09878597408533096, 0.09863078594207764, 0.09752582758665085, 0.0943911224603653, 0.09754989296197891, 0.09384029358625412, 0.09739451110363007, 0.09426356106996536, 0.09311478585004807, 0.0919542983174324, 0.0930250808596611, 0.09088815003633499, 0.09128276258707047, 0.08745842427015305, 0.08930622041225433, 0.08752896636724472, 0.08557498455047607, 0.08612743765115738, 0.08775827288627625, 0.08582008630037308, 0.08282973617315292, 0.08282677084207535, 0.08477149903774261, 0.08288392424583435, 0.08411448448896408, 0.08316836506128311, 0.08197559416294098, 0.08307063579559326, 0.08237707614898682, 0.07759413868188858, 0.08105826377868652, 0.07768065482378006, 0.07792670279741287, 0.0773126408457756, 0.07773696631193161, 0.07539393752813339, 0.0753994956612587, 0.07456529140472412, 0.0762830451130867, 0.07412215322256088, 0.07398001849651337, 0.07442110031843185, 0.07240621000528336, 0.07024256885051727, 0.07382015138864517, 0.07027781009674072, 0.07256132364273071, 0.07296687364578247, 0.0713995173573494, 0.0709439218044281, 0.06944113224744797, 0.06899017840623856, 0.06640298664569855, 0.06752048432826996, 0.07003677636384964, 0.07122766226530075, 0.06710461527109146, 0.06777480989694595, 0.06986232101917267, 0.06700605899095535, 0.06720499694347382, 0.06405746191740036, 0.06386873871088028, 0.06573987752199173, 0.06522156298160553, 0.06394344568252563, 0.06554765999317169, 0.06151998043060303, 0.06497866660356522, 0.06315049529075623, 0.06309163570404053, 0.0634019747376442, 0.06276850402355194, 0.0640513151884079, 0.06504303961992264, 0.06313151866197586, 0.061611369252204895, 0.05966414511203766, 0.05913948640227318, 0.0618746280670166, 0.06105605885386467, 0.06046683341264725, 0.0580475777387619, 0.06033335253596306, 0.059136729687452316, 0.060199420899152756, 0.0584673248231411, 0.06410983949899673, 0.06250763684511185, 0.057991523295640945, 0.05662451684474945, 0.05953653156757355, 0.05629195272922516, 0.05645640194416046, 0.05739854276180267, 0.05661231279373169, 0.058891572058200836, 0.05635311082005501, 0.0561208575963974, 0.05562705174088478, 0.05748750641942024, 0.055047061294317245, 0.05501759424805641, 0.05303611606359482, 0.0530744343996048, 0.054083649069070816, 0.05103306099772453, 0.05358585715293884, 0.05255177617073059, 0.0520239993929863, 0.05392725393176079, 0.051826346665620804, 0.050375740975141525, 0.05458112061023712, 0.053163353353738785, 0.05405639857053757, 0.05441092327237129, 0.05109039694070816, 0.05019614100456238, 0.05122697353363037, 0.0509067103266716, 0.05117495357990265, 0.049346912652254105, 0.05356408283114433, 0.04976488649845123, 0.04921676218509674, 0.054740481078624725, 0.050122249871492386]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:12 2016", "state": "available"}], "summary": "0ce6fa1a5ad767a90d01f51481943af6"}