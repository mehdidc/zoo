{"content": {"hp_model": {"f0": 16, "f1": 16, "f2": 32, "f3": 32, "nonlin": "leaky_rectify", "nbg1": 7, "nbg3": 1, "nbg2": 1, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.631880760192871, 1.2626752853393555, 1.1464921236038208, 1.0825542211532593, 1.0366427898406982, 0.997214674949646, 0.96489018201828, 0.9348165392875671, 0.9118665456771851, 0.8907000422477722, 0.8770260810852051, 0.863181471824646, 0.850609302520752, 0.8415576815605164, 0.8321352601051331, 0.8256521821022034, 0.8192091584205627, 0.8134276866912842, 0.810291588306427, 0.8039908409118652, 0.8010727167129517, 0.7965421080589294, 0.7943443059921265, 0.7908319234848022, 0.7900879383087158, 0.786300778388977, 0.7827032804489136, 0.7817871570587158, 0.7790653109550476, 0.7753356099128723, 0.7717841863632202, 0.7715606689453125, 0.7671242952346802, 0.764468252658844, 0.7643688321113586, 0.762565016746521, 0.7625905871391296, 0.7592672109603882, 0.7596531510353088, 0.7578558921813965, 0.7579658627510071, 0.7573878169059753, 0.7544870376586914, 0.753024697303772, 0.7513380646705627, 0.7511499524116516, 0.7488152384757996, 0.7472724318504333, 0.7476120591163635, 0.7461405992507935, 0.7446380257606506, 0.7428150773048401, 0.7468782663345337, 0.7440159320831299, 0.7423413991928101, 0.7402560710906982, 0.743502140045166, 0.7418515682220459, 0.7400957942008972, 0.7402012944221497, 0.7384012341499329, 0.7393999099731445, 0.7381963729858398, 0.7377755045890808, 0.7328949570655823, 0.7355268597602844, 0.7329595685005188, 0.7328425049781799, 0.7308876514434814, 0.7368513941764832, 0.728925347328186, 0.7328545451164246, 0.7316238880157471, 0.7326935529708862, 0.728821873664856, 0.7348683476448059, 0.7291936278343201, 0.7346280217170715, 0.7314831614494324, 0.7313717603683472, 0.7300254702568054, 0.7318853735923767, 0.7288787364959717, 0.728659987449646, 0.7294780611991882, 0.6341896057128906, 0.5659745931625366, 0.5355306267738342, 0.5132342576980591, 0.4948495626449585, 0.47822636365890503, 0.4631216824054718, 0.4493058919906616, 0.43609827756881714, 0.42363783717155457, 0.41189414262771606, 0.40067312121391296, 0.3900110125541687, 0.37966668605804443, 0.36977913975715637, 0.36008790135383606, 0.35091206431388855, 0.3418254554271698, 0.33319804072380066, 0.32498446106910706, 0.3170209228992462, 0.3091708719730377, 0.3017626404762268, 0.2945568561553955, 0.2876092195510864, 0.28109437227249146, 0.2747403085231781, 0.2686035633087158, 0.26260238885879517, 0.25681182742118835, 0.2673124372959137, 0.2570461332798004, 0.25545284152030945, 0.2552928924560547, 0.25528058409690857, 0.25528034567832947, 0.25528034567832947, 0.25528034567832947, 0.25528034567832947, 0.25528034567832947, 0.25528034567832947, 0.25528034567832947, 0.25528034567832947, 0.25528034567832947, 0.25528034567832947, 0.25528034567832947, 0.25528034567832947, 0.25528034567832947, 0.25528034567832947, 0.25528034567832947, 0.25528034567832947], "moving_avg_accuracy_train": [0.05556235294117646, 0.11383905882352939, 0.16859868235294112, 0.21730822588235288, 0.2644503444705882, 0.3078194276705882, 0.3482963084329411, 0.38608785406023527, 0.4211425980659764, 0.45335539708290823, 0.48343397502167623, 0.5105211657548028, 0.5352408138852048, 0.5579308501437431, 0.5792907063058393, 0.5987028121458436, 0.616291354460671, 0.6325375131322509, 0.6477519971131435, 0.6616120915194762, 0.6736485294263521, 0.6840295588366581, 0.694090132364757, 0.702509354422399, 0.7116160660389826, 0.7197485770821432, 0.7285407781974583, 0.7352184650835948, 0.7420448538693529, 0.7478615449530059, 0.7540448022224112, 0.758565027882523, 0.7621226427413295, 0.7666162608201377, 0.7709099288557709, 0.7753765830290174, 0.7789212776672921, 0.7817820910770336, 0.7850203525575655, 0.7881371408312208, 0.7906057796892751, 0.7938487311321123, 0.7961109168424305, 0.7974880604523051, 0.7996498426423687, 0.8014001524957789, 0.8026342548932598, 0.8040531823451103, 0.805697275875305, 0.8080757835818922, 0.8094964405178207, 0.8096267964660386, 0.8106899991723759, 0.8115174698433736, 0.8111210169766833, 0.8115877388084266, 0.8091536708099368, 0.8088430096112961, 0.8090410615913429, 0.8083675436675029, 0.809408436359576, 0.8094393574295008, 0.8097118922747859, 0.8104865854002484, 0.8097932209778707, 0.8095362518212601, 0.8099332148744282, 0.8105210698575736, 0.809963080518875, 0.8104961842316933, 0.8113124481614652, 0.8106423798159069, 0.8110252006578457, 0.81253679823912, 0.8121490007681491, 0.8121505712795696, 0.8127143376810244, 0.8128193745011573, 0.8129774370510415, 0.8128749874635844, 0.8123804298936965, 0.8114153280807974, 0.8119137952727177, 0.8115671216277989, 0.811125703582666, 0.8194272508714582, 0.8275433493137241, 0.8353396026176457, 0.842615054120587, 0.8494406075320577, 0.8557953703082638, 0.8617499509244962, 0.8672502499496937, 0.872268754366489, 0.8768889377533695, 0.8812400439780326, 0.8852454513449353, 0.8889514944457358, 0.8923269332364564, 0.8954612987363402, 0.8983316394509415, 0.9009925931529063, 0.9035074514846745, 0.9059049416303246, 0.9081191533496451, 0.9101707674264453, 0.9120831024485067, 0.9139242039683618, 0.9155976659244668, 0.9172261346261377, 0.918654109398818, 0.919962816105995, 0.921124181554219, 0.9221176457517383, 0.9230776458824468, 0.9254098812942021, 0.9275912461059583, 0.929559180318892, 0.9313279681693557, 0.9329198772347731, 0.9343525953936488, 0.9356420417366369, 0.9368025434453262, 0.9378469949831465, 0.9387870013671848, 0.9396330071128193, 0.9403944122838903, 0.9410796769378542, 0.9416964151264218, 0.9422514794961325, 0.9427510374288722, 0.943200639568338, 0.9436052814938571, 0.9439694592268244, 0.9442972191864949, 0.9445922031501983], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.055813333333333326, 0.11385866666666664, 0.16781946666666664, 0.2152508533333333, 0.2605524346666666, 0.30241719119999994, 0.3416954720799999, 0.3780059248719999, 0.41080533238479994, 0.4413914658129866, 0.4695323192316879, 0.4949657539751858, 0.5177358452443339, 0.5386822607199006, 0.5580673679812438, 0.5759406311831194, 0.5917732347314741, 0.6057959112583267, 0.6191629867991606, 0.6308600214525779, 0.6412806859739868, 0.6501526173765881, 0.659084022305596, 0.6659622867417031, 0.6734327247341995, 0.6794894522607795, 0.6865138403680349, 0.6917024563312314, 0.6972655440314416, 0.7015389896282974, 0.7059317573321343, 0.7089385815989209, 0.7110847234390288, 0.7137495844284594, 0.7168946259856135, 0.7197651633870521, 0.7224819803816802, 0.7243271156768456, 0.726534404109161, 0.7284542970315784, 0.7302888673284206, 0.7320999805955786, 0.733183315869354, 0.7338116509490853, 0.7348704858541767, 0.7354501039354256, 0.7356517602085497, 0.7364065841876947, 0.7375792591022585, 0.7390213331920328, 0.7393058665394961, 0.7386152798855465, 0.7386737518969919, 0.7387130433739594, 0.7373084057032301, 0.7371642317995738, 0.734634475286283, 0.7337976944243213, 0.7336579249818892, 0.732198799150367, 0.7328322525686637, 0.732655693978464, 0.733043457913951, 0.7332057787892226, 0.732631867576967, 0.7323553474859369, 0.7328931460706766, 0.7332971647969422, 0.7325407816505813, 0.7326200368188565, 0.7337980331369708, 0.732871563156607, 0.732691073507613, 0.733235299490185, 0.7327117695411665, 0.7325339259203831, 0.7331071999950114, 0.733143146662177, 0.7334021653292926, 0.7333552821296967, 0.7333130872500604, 0.7323551118583876, 0.7327462673392156, 0.7328849739386274, 0.7329831432114313, 0.738071495556955, 0.7425710126679261, 0.7465939114011335, 0.7501478535943535, 0.7532664015682515, 0.7560197614114263, 0.758471118603617, 0.760570673409922, 0.7624869394022632, 0.7641582454620369, 0.7657557542491666, 0.7670601788242499, 0.768074160941825, 0.7689067448476425, 0.769602737029545, 0.7701357966599238, 0.7707222169939315, 0.7711166619612051, 0.7713916624317512, 0.7716258295219094, 0.7716899132363851, 0.7715075885794133, 0.7712234963881386, 0.7708344800826581, 0.7702043654077255, 0.769623928866953, 0.7690215359802577, 0.7685060490488986, 0.7678287774773421, 0.7673792330629412, 0.7675079764233137, 0.7676105121143156, 0.7677161275695508, 0.7678111814792623, 0.7678967299980027, 0.7679737236648692, 0.7680430179650489, 0.7681053828352107, 0.7681615112183563, 0.7682120267631873, 0.7682574907535352, 0.7682984083448483, 0.7683352341770301, 0.7683683774259937, 0.7683982063500611, 0.7684250523817217, 0.7684492138102161, 0.7684709590958612, 0.7684905298529418, 0.7685081435343142, 0.7685239958475494], "moving_var_accuracy_train": [0.02778457557923875, 0.0555716880577993, 0.07700206657376552, 0.09065543659398191, 0.10159130703946889, 0.10836007273399871, 0.11226946634684648, 0.11389632800026052, 0.11356621089600688, 0.11154856959095336, 0.1085362002892249, 0.10428602337661737, 0.09935697007217364, 0.09405481277368034, 0.08875552259370129, 0.08327143901262292, 0.0777285064982049, 0.07233109489262378, 0.06718131010860492, 0.062192099050316536, 0.0572767716826596, 0.05251898645895238, 0.0481780240704857, 0.04399817136394009, 0.040344743995754655, 0.03690550921898334, 0.03391068350115434, 0.03092093867038242, 0.028248241058033106, 0.025727922008693626, 0.023499223841961143, 0.02133319341793003, 0.019313783687489438, 0.017564138749684224, 0.015973645141517775, 0.014555839622896416, 0.013213339401314297, 0.011965663741471072, 0.010863474404070636, 0.009864556285948725, 0.008932948257657317, 0.008134304038436985, 0.007366930992284996, 0.006647306613756463, 0.006024635672516303, 0.005449744366511176, 0.004918477008407272, 0.00444474950358908, 0.004024601945054429, 0.0036730574407416354, 0.0033239160918338857, 0.002991677416709619, 0.002702683274991524, 0.0024385773168946238, 0.0021961341590847243, 0.0019784812065902855, 0.0018339552691227026, 0.0016514283356335005, 0.0014866385233513549, 0.0013420573085598253, 0.0012176026960715462, 0.0010958510314774794, 0.0009869344055067832, 0.0008936423099038559, 0.0008086048669134439, 0.000728338678549142, 0.0006569230276844528, 0.0005943408862468877, 0.0005377089665411099, 0.0004864958660045856, 0.00044384286063154624, 0.00040349949885786456, 0.0003644685151452821, 0.00034858600886018164, 0.00031508088988058575, 0.00028357282309108227, 0.0002580760337806577, 0.00023236772500484452, 0.00020935580643144334, 0.00018851468905003017, 0.00017186450485442756, 0.00016306084795233424, 0.00014899098902988864, 0.00013517353367163153, 0.00012340982931958831, 0.0007313100328801111, 0.0012510185149130664, 0.0016729507536319514, 0.001982045429413615, 0.0022031345008278083, 0.0023462681402217065, 0.0024307545990364357, 0.0024599587434320823, 0.0024406313483214225, 0.002388683064244941, 0.0023202038862251604, 0.0022325730911761845, 0.0021329285812434914, 0.00202217800638825, 0.00190837842953118, 0.0017916902889391414, 0.0016762473314812227, 0.0015655432101928759, 0.0014607205201599967, 0.0013587730699857823, 0.0012607778458683268, 0.0011676132884109157, 0.0010813588528275429, 0.0009984272418115654, 0.0009224517104413065, 0.0008485585469598794, 0.0007791171114725826, 0.000713344327664282, 0.0006508926349036272, 0.000594097773671908, 0.000583641894447327, 0.0005681028769803101, 0.0005461474748801894, 0.0005196902215317041, 0.0004905287696315558, 0.000459950024573348, 0.00042891906895902083, 0.00039814804000595484, 0.0003681511471390565, 0.00033928854044344556, 0.0003118012178939197, 0.0002858387366153309, 0.0002614811517675484, 0.00023875633052993148, 0.00021765356556764005, 0.00019813423216434445, 0.00018014008770221937, 0.000163599694722988, 0.00014843335404139153, 0.00013455685795772128, 0.00012188431201152897], "duration": 67604.848663, "accuracy_train": [0.5556235294117647, 0.6383294117647059, 0.661435294117647, 0.6556941176470589, 0.6887294117647059, 0.6981411764705883, 0.7125882352941176, 0.7262117647058823, 0.736635294117647, 0.7432705882352941, 0.7541411764705882, 0.7543058823529412, 0.7577176470588235, 0.7621411764705882, 0.7715294117647059, 0.7734117647058824, 0.7745882352941177, 0.7787529411764705, 0.7846823529411765, 0.7863529411764706, 0.7819764705882353, 0.7774588235294118, 0.7846352941176471, 0.7782823529411764, 0.7935764705882353, 0.7929411764705883, 0.8076705882352941, 0.7953176470588236, 0.8034823529411764, 0.8002117647058824, 0.8096941176470588, 0.7992470588235294, 0.7941411764705882, 0.8070588235294117, 0.8095529411764706, 0.8155764705882353, 0.8108235294117647, 0.8075294117647058, 0.8141647058823529, 0.8161882352941177, 0.8128235294117647, 0.8230352941176471, 0.8164705882352942, 0.8098823529411765, 0.8191058823529411, 0.8171529411764706, 0.8137411764705882, 0.8168235294117647, 0.8204941176470588, 0.8294823529411764, 0.8222823529411765, 0.8108, 0.8202588235294118, 0.8189647058823529, 0.8075529411764706, 0.8157882352941176, 0.7872470588235294, 0.8060470588235294, 0.8108235294117647, 0.8023058823529412, 0.8187764705882353, 0.8097176470588235, 0.8121647058823529, 0.8174588235294118, 0.8035529411764706, 0.8072235294117647, 0.8135058823529412, 0.8158117647058823, 0.8049411764705883, 0.8152941176470588, 0.8186588235294118, 0.8046117647058824, 0.8144705882352942, 0.8261411764705883, 0.8086588235294118, 0.8121647058823529, 0.8177882352941176, 0.813764705882353, 0.8144, 0.8119529411764705, 0.8079294117647059, 0.8027294117647059, 0.8164, 0.8084470588235294, 0.8071529411764706, 0.8941411764705882, 0.9005882352941177, 0.9055058823529412, 0.9080941176470588, 0.9108705882352941, 0.9129882352941177, 0.9153411764705882, 0.9167529411764705, 0.917435294117647, 0.9184705882352941, 0.9204, 0.9212941176470588, 0.9223058823529412, 0.9227058823529412, 0.9236705882352941, 0.9241647058823529, 0.9249411764705883, 0.9261411764705882, 0.9274823529411764, 0.9280470588235294, 0.9286352941176471, 0.9292941176470588, 0.9304941176470588, 0.9306588235294118, 0.9318823529411765, 0.9315058823529412, 0.9317411764705882, 0.9315764705882353, 0.9310588235294117, 0.9317176470588235, 0.9464, 0.9472235294117647, 0.9472705882352941, 0.9472470588235294, 0.9472470588235294, 0.9472470588235294, 0.9472470588235294, 0.9472470588235294, 0.9472470588235294, 0.9472470588235294, 0.9472470588235294, 0.9472470588235294, 0.9472470588235294, 0.9472470588235294, 0.9472470588235294, 0.9472470588235294, 0.9472470588235294, 0.9472470588235294, 0.9472470588235294, 0.9472470588235294, 0.9472470588235294], "end": "2016-02-07 18:04:14.861000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0], "moving_var_accuracy_valid": [0.028036153600000002, 0.055555884736, 0.07620620769216, 0.0888332148930496, 0.09841999284545018, 0.10435191411726265, 0.10780177284552996, 0.10888763639861751, 0.10768108295747225, 0.10533257868450657, 0.10192648949625144, 0.09755557697229218, 0.09246631278271097, 0.08716845239591539, 0.0818336486081279, 0.0765253655846668, 0.07112887104227449, 0.06578570305083772, 0.06081524112238322, 0.0559651025872941, 0.05134590457017443, 0.046919714614469164, 0.0429456730990754, 0.039076900484044996, 0.035671477429838096, 0.03243448522183538, 0.029635114954184003, 0.026913899079287443, 0.024501040674200766, 0.02221529764220427, 0.020167435550882698, 0.018232060925336458, 0.01645030815598357, 0.014869190697222107, 0.013471293205065932, 0.012198323749316863, 0.011044921225625878, 0.009971069821380475, 0.009017811939253328, 0.008149204644829926, 0.007364575013913434, 0.0066576386939203695, 0.006002437362366989, 0.005405746870882079, 0.0048752623660000315, 0.004390759743481025, 0.003952049756405335, 0.003561972613920233, 0.003218151850625436, 0.0029150528646864735, 0.0026242762112501945, 0.0023661407794646947, 0.0021295574723033273, 0.0019166156194544552, 0.0017427111203832952, 0.0015686270833754252, 0.0014693613871867137, 0.0013287270683665497, 0.0011960301810032348, 0.0010955885966328512, 0.0009896411060679314, 0.0008909575518830981, 0.0008032150445217672, 0.0007231306726685308, 0.0006537819721176518, 0.0005890919451525758, 0.00053278579649705, 0.0004809762970279051, 0.0004380277065020037, 0.0003942814682870883, 0.00036734239938779816, 0.00033833327906965504, 0.0003047931397832354, 0.0002769794630858704, 0.00025174826924495716, 0.00022685809750154132, 0.00020713007623315616, 0.00018642869807576333, 0.00016838964429741627, 0.00015157046217731382, 0.00013642943963039017, 0.00013104594732680537, 0.00011931837608576018, 0.00010755969416366756, 9.689045960240648e-05, 0.00032022337997191945, 0.0004704119300620288, 0.0005690241650146007, 0.0006257962945278839, 0.0006507447382646241, 0.0006538991782722324, 0.0006425916291983546, 0.0006180056397406234, 0.0005892537539471891, 0.000555467754061396, 0.0005228892875798641, 0.00048591407007060996, 0.000446576100676406, 0.00040815725425080224, 0.0003717011748811456, 0.00033708843051888767, 0.0003064745867402374, 0.0002772274095560802, 0.0002501852959296777, 0.0002256602743717285, 0.0002031312074367047, 0.00018311726721789335, 0.00016553191585439312, 0.00015034072744232128, 0.0001388800552301765, 0.00012802420890793494, 0.00011848768272661135, 0.00010903045544156843, 0.00010225568093215942, 9.384892446361475e-05, 8.461320569281343e-05, 7.624650723489534e-05, 6.872224813086648e-05, 6.193134052954299e-05, 5.5804073418116845e-05, 5.027701829894296e-05, 4.5292531769385286e-05, 4.0798282985719416e-05, 3.674680824569833e-05, 3.3095093803554694e-05, 2.9804187192964442e-05, 2.683883671717783e-05, 2.4167158322703015e-05, 2.176032876499952e-05, 1.9592303770898682e-05, 1.7639559778552093e-05, 1.588085777233894e-05, 1.4297027712135115e-05, 1.2870772071715958e-05, 1.158648704048779e-05, 1.043009999895319e-05], "accuracy_test": 0.1496, "start": "2016-02-06 23:17:30.012000", "learning_rate_per_epoch": [0.006477434653788805, 0.006477434653788805, 0.006477434653788805, 0.006477434653788805, 0.006477434653788805, 0.006477434653788805, 0.006477434653788805, 0.006477434653788805, 0.006477434653788805, 0.006477434653788805, 0.006477434653788805, 0.006477434653788805, 0.006477434653788805, 0.006477434653788805, 0.006477434653788805, 0.006477434653788805, 0.006477434653788805, 0.006477434653788805, 0.006477434653788805, 0.006477434653788805, 0.006477434653788805, 0.006477434653788805, 0.006477434653788805, 0.006477434653788805, 0.006477434653788805, 0.006477434653788805, 0.006477434653788805, 0.006477434653788805, 0.006477434653788805, 0.006477434653788805, 0.006477434653788805, 0.006477434653788805, 0.006477434653788805, 0.006477434653788805, 0.006477434653788805, 0.006477434653788805, 0.006477434653788805, 0.006477434653788805, 0.006477434653788805, 0.006477434653788805, 0.006477434653788805, 0.006477434653788805, 0.006477434653788805, 0.006477434653788805, 0.006477434653788805, 0.006477434653788805, 0.006477434653788805, 0.006477434653788805, 0.006477434653788805, 0.006477434653788805, 0.006477434653788805, 0.006477434653788805, 0.006477434653788805, 0.006477434653788805, 0.006477434653788805, 0.006477434653788805, 0.006477434653788805, 0.006477434653788805, 0.006477434653788805, 0.006477434653788805, 0.006477434653788805, 0.006477434653788805, 0.006477434653788805, 0.006477434653788805, 0.006477434653788805, 0.006477434653788805, 0.006477434653788805, 0.006477434653788805, 0.006477434653788805, 0.006477434653788805, 0.006477434653788805, 0.006477434653788805, 0.006477434653788805, 0.006477434653788805, 0.006477434653788805, 0.006477434653788805, 0.006477434653788805, 0.006477434653788805, 0.006477434653788805, 0.006477434653788805, 0.006477434653788805, 0.006477434653788805, 0.006477434653788805, 0.006477434653788805, 0.0006477434653788805, 0.0006477434653788805, 0.0006477434653788805, 0.0006477434653788805, 0.0006477434653788805, 0.0006477434653788805, 0.0006477434653788805, 0.0006477434653788805, 0.0006477434653788805, 0.0006477434653788805, 0.0006477434653788805, 0.0006477434653788805, 0.0006477434653788805, 0.0006477434653788805, 0.0006477434653788805, 0.0006477434653788805, 0.0006477434653788805, 0.0006477434653788805, 0.0006477434653788805, 0.0006477434653788805, 0.0006477434653788805, 0.0006477434653788805, 0.0006477434653788805, 0.0006477434653788805, 0.0006477434653788805, 0.0006477434653788805, 0.0006477434653788805, 0.0006477434653788805, 0.0006477434653788805, 0.0006477434653788805, 6.4774343627505e-05, 6.477434453699971e-06, 6.477434340013133e-07, 6.477434055796039e-08, 6.4774341446138806e-09, 6.477434144613881e-10, 6.477433867058124e-11, 6.477434040530472e-12, 6.477434148950689e-13, 6.477434420001232e-14, 6.477434589407822e-15, 6.477434589407822e-16, 6.477434324710026e-17, 6.4774342419919646e-18, 6.477434241991965e-19, 6.477434241991965e-20, 6.477434080433251e-21, 6.477434080433251e-22, 6.477434080433251e-23, 6.47743392266107e-24, 6.477434021268683e-25, 6.477434267787716e-26], "accuracy_train_first": 0.5556235294117647, "accuracy_train_last": 0.9472470588235294, "batch_size_eval": 1024, "accuracy_train_std": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "accuracy_test_std": 0, "error_valid": [0.44186666666666663, 0.36373333333333335, 0.34653333333333336, 0.35786666666666667, 0.3317333333333333, 0.3208, 0.30479999999999996, 0.2952, 0.29400000000000004, 0.2833333333333333, 0.2772, 0.27613333333333334, 0.2773333333333333, 0.27280000000000004, 0.26746666666666663, 0.2632, 0.2657333333333334, 0.268, 0.2605333333333333, 0.2638666666666667, 0.26493333333333335, 0.27, 0.2605333333333333, 0.27213333333333334, 0.2593333333333333, 0.266, 0.25026666666666664, 0.26160000000000005, 0.2526666666666667, 0.26, 0.2545333333333333, 0.264, 0.26959999999999995, 0.26226666666666665, 0.2548, 0.25439999999999996, 0.25306666666666666, 0.25906666666666667, 0.25360000000000005, 0.25426666666666664, 0.2532, 0.25160000000000005, 0.25706666666666667, 0.2605333333333333, 0.25560000000000005, 0.2593333333333333, 0.2625333333333333, 0.25680000000000003, 0.2518666666666667, 0.248, 0.2581333333333333, 0.26759999999999995, 0.26080000000000003, 0.26093333333333335, 0.2753333333333333, 0.26413333333333333, 0.28813333333333335, 0.2737333333333334, 0.26759999999999995, 0.28093333333333337, 0.2614666666666666, 0.26893333333333336, 0.2634666666666666, 0.2653333333333333, 0.2725333333333333, 0.27013333333333334, 0.26226666666666665, 0.26306666666666667, 0.27426666666666666, 0.2666666666666667, 0.25560000000000005, 0.27546666666666664, 0.26893333333333336, 0.2618666666666667, 0.272, 0.2690666666666667, 0.2617333333333334, 0.2665333333333333, 0.26426666666666665, 0.2670666666666667, 0.2670666666666667, 0.27626666666666666, 0.2637333333333334, 0.2658666666666667, 0.26613333333333333, 0.2161333333333333, 0.2169333333333333, 0.21719999999999995, 0.21786666666666665, 0.21866666666666668, 0.21919999999999995, 0.2194666666666667, 0.22053333333333336, 0.22026666666666672, 0.2208, 0.21986666666666665, 0.22119999999999995, 0.2228, 0.22360000000000002, 0.2241333333333333, 0.22506666666666664, 0.22399999999999998, 0.22533333333333339, 0.2261333333333333, 0.22626666666666662, 0.22773333333333334, 0.2301333333333333, 0.23133333333333328, 0.2326666666666667, 0.2354666666666667, 0.23560000000000003, 0.23640000000000005, 0.2361333333333333, 0.23826666666666663, 0.2366666666666667, 0.23133333333333328, 0.2314666666666667, 0.23133333333333328, 0.23133333333333328, 0.23133333333333328, 0.23133333333333328, 0.23133333333333328, 0.23133333333333328, 0.23133333333333328, 0.23133333333333328, 0.23133333333333328, 0.23133333333333328, 0.23133333333333328, 0.23133333333333328, 0.23133333333333328, 0.23133333333333328, 0.23133333333333328, 0.23133333333333328, 0.23133333333333328, 0.23133333333333328, 0.23133333333333328], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.07639682677952409, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 16, "valid_ratio": 0.15, "learning_rate": 0.006477434739194714, "optimization": "adam", "nb_data_augmentation": 0, "learning_rate_decay_method": "discrete", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 1.2351558175419131e-05, "rotation_range": [0, 0], "momentum": 0.5809627289911531}, "accuracy_valid_max": 0.7838666666666667, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        y_pred = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            y_pred.extend((nnet.predict(X[mini_batch]) == y[mini_batch]).tolist())\n        return np.mean(y_pred)\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            acc = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = acc\n            status[\"accuracy_train_std\"] = 0\n            acc = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = acc\n            status[\"accuracy_valid_std\"] = 0\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n\n    # rescaling to [-1, 1]\n    X_min = X_train.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X_train.max(axis=(0, 2, 3))[None, :, None, None]\n    def preprocess(a):\n        return (a / 255.) * 2 - 1\n        # return 2 * ((a - X_min) / (X_max - X_min)) - 1\n    X_train = preprocess(X_train)\n    X_valid = preprocess(X_valid)\n\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = preprocess(X_test)\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    acc = evaluate(X_test, y_test, batch_size_eval)\n    light.set(\"accuracy_test\", acc)\n    light.set(\"accuracy_test_std\", 0)\n    print(\"Test accuracy : {}+-{}\".format(acc, 0))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.7686666666666667, "accuracy_valid_std": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "accuracy_valid": [0.5581333333333334, 0.6362666666666666, 0.6534666666666666, 0.6421333333333333, 0.6682666666666667, 0.6792, 0.6952, 0.7048, 0.706, 0.7166666666666667, 0.7228, 0.7238666666666667, 0.7226666666666667, 0.7272, 0.7325333333333334, 0.7368, 0.7342666666666666, 0.732, 0.7394666666666667, 0.7361333333333333, 0.7350666666666666, 0.73, 0.7394666666666667, 0.7278666666666667, 0.7406666666666667, 0.734, 0.7497333333333334, 0.7384, 0.7473333333333333, 0.74, 0.7454666666666667, 0.736, 0.7304, 0.7377333333333334, 0.7452, 0.7456, 0.7469333333333333, 0.7409333333333333, 0.7464, 0.7457333333333334, 0.7468, 0.7484, 0.7429333333333333, 0.7394666666666667, 0.7444, 0.7406666666666667, 0.7374666666666667, 0.7432, 0.7481333333333333, 0.752, 0.7418666666666667, 0.7324, 0.7392, 0.7390666666666666, 0.7246666666666667, 0.7358666666666667, 0.7118666666666666, 0.7262666666666666, 0.7324, 0.7190666666666666, 0.7385333333333334, 0.7310666666666666, 0.7365333333333334, 0.7346666666666667, 0.7274666666666667, 0.7298666666666667, 0.7377333333333334, 0.7369333333333333, 0.7257333333333333, 0.7333333333333333, 0.7444, 0.7245333333333334, 0.7310666666666666, 0.7381333333333333, 0.728, 0.7309333333333333, 0.7382666666666666, 0.7334666666666667, 0.7357333333333334, 0.7329333333333333, 0.7329333333333333, 0.7237333333333333, 0.7362666666666666, 0.7341333333333333, 0.7338666666666667, 0.7838666666666667, 0.7830666666666667, 0.7828, 0.7821333333333333, 0.7813333333333333, 0.7808, 0.7805333333333333, 0.7794666666666666, 0.7797333333333333, 0.7792, 0.7801333333333333, 0.7788, 0.7772, 0.7764, 0.7758666666666667, 0.7749333333333334, 0.776, 0.7746666666666666, 0.7738666666666667, 0.7737333333333334, 0.7722666666666667, 0.7698666666666667, 0.7686666666666667, 0.7673333333333333, 0.7645333333333333, 0.7644, 0.7636, 0.7638666666666667, 0.7617333333333334, 0.7633333333333333, 0.7686666666666667, 0.7685333333333333, 0.7686666666666667, 0.7686666666666667, 0.7686666666666667, 0.7686666666666667, 0.7686666666666667, 0.7686666666666667, 0.7686666666666667, 0.7686666666666667, 0.7686666666666667, 0.7686666666666667, 0.7686666666666667, 0.7686666666666667, 0.7686666666666667, 0.7686666666666667, 0.7686666666666667, 0.7686666666666667, 0.7686666666666667, 0.7686666666666667, 0.7686666666666667], "seed": 929518653, "model": "residualv5", "loss_std": [0.29592621326446533, 0.274479478597641, 0.2724611163139343, 0.2687336206436157, 0.2670783996582031, 0.2658020853996277, 0.2641848027706146, 0.2624782919883728, 0.26267102360725403, 0.2624286711215973, 0.260631263256073, 0.2575926184654236, 0.25685206055641174, 0.2548430860042572, 0.25191524624824524, 0.25121212005615234, 0.24941596388816833, 0.25033921003341675, 0.24593673646450043, 0.2460169494152069, 0.24609433114528656, 0.24297823011875153, 0.2418697327375412, 0.23977363109588623, 0.23824812471866608, 0.23973649740219116, 0.23988524079322815, 0.23758167028427124, 0.23723646998405457, 0.23383864760398865, 0.23424704372882843, 0.23267622292041779, 0.23152875900268555, 0.23125164210796356, 0.23136873543262482, 0.23120976984500885, 0.22983059287071228, 0.22870197892189026, 0.22845682501792908, 0.22760817408561707, 0.22872309386730194, 0.2255650907754898, 0.22516293823719025, 0.22551141679286957, 0.2257760912179947, 0.2235131412744522, 0.2220083624124527, 0.22376619279384613, 0.2256271094083786, 0.22349220514297485, 0.22181488573551178, 0.21981951594352722, 0.22382435202598572, 0.2234657108783722, 0.22313661873340607, 0.22060203552246094, 0.22440992295742035, 0.22430910170078278, 0.22128041088581085, 0.22079548239707947, 0.21765665709972382, 0.2228434532880783, 0.21962548792362213, 0.2197546809911728, 0.218882754445076, 0.22031593322753906, 0.21922536194324493, 0.22197166085243225, 0.2130359709262848, 0.22164784371852875, 0.21783044934272766, 0.21851025521755219, 0.21464982628822327, 0.21651770174503326, 0.21753977239131927, 0.21708886325359344, 0.2114037573337555, 0.21746741235256195, 0.21290740370750427, 0.2141886204481125, 0.2150590568780899, 0.21399028599262238, 0.21444135904312134, 0.2139158695936203, 0.21116971969604492, 0.19999295473098755, 0.16311638057231903, 0.15000289678573608, 0.1411559283733368, 0.1345212757587433, 0.12858934700489044, 0.12338374555110931, 0.11880957335233688, 0.11433614045381546, 0.10996333509683609, 0.10594910383224487, 0.1020311713218689, 0.09816703200340271, 0.09457924962043762, 0.0909971222281456, 0.08724705129861832, 0.0835871621966362, 0.08014878630638123, 0.076534703373909, 0.07322043180465698, 0.06982948631048203, 0.06625953316688538, 0.06294970959424973, 0.059474531561136246, 0.05606324225664139, 0.05267808213829994, 0.04938535764813423, 0.046077895909547806, 0.04302563518285751, 0.03980625793337822, 0.05642164498567581, 0.046782854944467545, 0.04508063569664955, 0.04491274058818817, 0.04489897936582565, 0.04489869251847267, 0.04489868879318237, 0.044898685067892075, 0.04489868879318237, 0.04489869251847267, 0.04489869996905327, 0.04489868879318237, 0.04489867389202118, 0.04489868879318237, 0.04489867389202118, 0.04489868879318237, 0.04489868879318237, 0.04489868879318237, 0.04489868879318237, 0.04489868879318237, 0.04489868879318237]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:44 2016", "state": "available"}], "summary": "a5c1eccb4b0d041499cf78c6fd3308db"}