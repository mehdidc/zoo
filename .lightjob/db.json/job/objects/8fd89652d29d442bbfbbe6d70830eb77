{"content": {"hp_model": {"f0": 64, "f1": 16, "f2": 16, "f3": 16, "nonlin": "rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.016669683583211917, 0.008101698930795638, 0.009085973119491822, 0.01750358692067241, 0.016530270907927965, 0.016190864181101456, 0.01832662419144865, 0.01940262048899493, 0.015920227958037252, 0.01805718851011646, 0.016134912495151223, 0.016496518750868862, 0.01656746357293657, 0.01746650705048949, 0.016245757958463832, 0.016522191634469892, 0.013100795530206013, 0.017487934916899733, 0.01232043442200845, 0.01601632909815106, 0.011921963561689085, 0.015695711123271915, 0.014050432916701223, 0.017263692960182424, 0.01623379522754763, 0.019495769217812795, 0.01432225175518978, 0.014635449700109326, 0.014331887994547066, 0.012436802370068388, 0.010738536239507511, 0.01302994155588298, 0.013375532371997326, 0.00958464336219447, 0.014813633155171958, 0.014134707514837731, 0.015815888665045665, 0.014533096321528074, 0.014746456868210583, 0.017316645624165326, 0.017113260546697644, 0.018321936561810272, 0.018837881825274586, 0.01601219225949594, 0.015198987022844392, 0.015647080460681417, 0.014915951395951475, 0.015069337117257052, 0.014738976252351451, 0.014137704752312035, 0.016628199744676737, 0.01674157856924044, 0.014527827417470376, 0.013315983433769975, 0.015862566928138727, 0.01617425527352043, 0.012685891634359424, 0.012980135804338093, 0.009547747105967902, 0.01113931738763628, 0.015094316880167715, 0.011176811974577448, 0.009968255021451338, 0.011482441309449518, 0.013423636575889793, 0.009176467769745478, 0.012716884735703869, 0.013616019397317476, 0.016721798345444207, 0.015303353204587216, 0.011980316549204392, 0.010442423989490728, 0.008464739003704072, 0.012264410577295494, 0.011267552328251614, 0.011278246629238455, 0.011871308276310872, 0.01158323017410483, 0.010808215500971302, 0.010859057558158794, 0.010814604930750735, 0.010911781562335158, 0.01097013258320798, 0.01131588663827777, 0.011356423277038983, 0.011749612340068626, 0.011894466078797501, 0.012417698946486956, 0.012027451801604905, 0.011896579015716485, 0.01160373171537681, 0.011459103958505364, 0.011371337044579112, 0.011181045199693502, 0.010662264651556178, 0.010216925316104814, 0.009987602287030141, 0.009724041485537467, 0.009126458433921266, 0.0091232388334287, 0.009031776824189949, 0.00895281740072753, 0.009441747618186915, 0.010035866166981027, 0.011646708723284929, 0.011653758875242731, 0.010862481526434185, 0.012037980159896985, 0.012455535690562155, 0.01384180016606933, 0.014073900250800778, 0.014013330555571005, 0.015662329087567976, 0.009329132428820556, 0.009247015931912874, 0.008701779443897193, 0.009693388688955171, 0.00958217040930192, 0.010117852035630965, 0.010549747743180796, 0.010806948710498178, 0.010606413133008127, 0.010931634027057945, 0.011266234662139014, 0.010934614425769497, 0.01105288715977263, 0.01091071553117732, 0.011259476021259502, 0.011708674661292238, 0.011388272507478231], "moving_avg_accuracy_train": [0.0461989584775286, 0.10028690714112677, 0.15401523873690703, 0.20525552586935325, 0.25320611040635665, 0.2977472088872234, 0.3394240946222552, 0.3804623990419972, 0.41820588061020425, 0.4537647669285674, 0.4860140501448099, 0.514378423265711, 0.5412356232171945, 0.5658138600199214, 0.5890434772709746, 0.6100223205564741, 0.6281733630313269, 0.644674170531258, 0.6585625019669658, 0.6733189266791377, 0.6878479172938006, 0.7020865472029403, 0.7156194606639401, 0.728192105025287, 0.740325721038539, 0.7514273731064275, 0.7622162778627375, 0.7721890339588927, 0.7791817392920086, 0.7866373880083873, 0.7937567340924231, 0.7979161592550708, 0.8036798357287424, 0.8085557188098456, 0.8146084235934512, 0.8221785385176977, 0.8300122020328069, 0.837659738001084, 0.844565988153541, 0.8507885166395435, 0.8565492635936216, 0.8631495549402857, 0.868913322135672, 0.8747258892507316, 0.8797874277423712, 0.8852426810229513, 0.8899411448731757, 0.8935071670253008, 0.8974137711168829, 0.901792381056459, 0.9051285192139638, 0.9090703676259468, 0.9125995381527117, 0.9151876731732654, 0.9187701057048222, 0.9219803080415474, 0.925118317116038, 0.9276772780746999, 0.929838617006571, 0.9315699804524163, 0.9342209553965158, 0.9359582244748045, 0.9385911909023702, 0.9409866175681778, 0.9435050084887778, 0.9461878340518602, 0.9488789976693301, 0.9513870393821867, 0.9533258036344904, 0.9559751382996496, 0.95872222666376, 0.9608039811914593, 0.9624589962782933, 0.9646298145064532, 0.9667184095427496, 0.9690072631194362, 0.9714811799241869, 0.9738123006960725, 0.9759730523598078, 0.977959581535741, 0.9797683841333666, 0.981424208256944, 0.9829167751169732, 0.9842693858862375, 0.9854844104297658, 0.9865825828165603, 0.987577913411104, 0.9884830115414314, 0.9893046113539734, 0.9900440511852613, 0.9907165224798489, 0.9913240717937872, 0.9918638907299032, 0.9923474026235981, 0.992784888476733, 0.9931763005957448, 0.9935308966516649, 0.9938523582508026, 0.9941439988388361, 0.9944018250704472, 0.994629218381278, 0.9948245717657878, 0.9950050401094656, 0.9951488604282993, 0.9952829490128688, 0.9953850275485051, 0.9954722479329587, 0.9955437708325385, 0.9955941905493031, 0.9956256174015341, 0.9956213494852086, 0.99559658202123, 0.9955184877322207, 0.9946996131019019, 0.9946019697595966, 0.9946512384824648, 0.9947141815235225, 0.9947848172021503, 0.994860015056963, 0.994934668572723, 0.9950111573321451, 0.995084647513244, 0.995150788676233, 0.9952196163181612, 0.9952955120887537, 0.9953731188775251, 0.9954452901362288, 0.9955172197154908, 0.9956028826761123, 0.9956799793406717], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 407335380, "moving_var_accuracy_train": [0.019209093879675707, 0.043617740207432434, 0.06523656873128428, 0.08234301508689568, 0.09480204059516889, 0.10317702162059242, 0.10849198469967127, 0.11280006809653087, 0.11434119489488406, 0.1142869849712161, 0.11221843288574732, 0.10823742856004784, 0.10390546840714876, 0.09895172908541212, 0.0939130922355448, 0.0884827898023682, 0.08259965390844656, 0.07679017835094996, 0.07084713226646758, 0.06572218767239484, 0.061049793019683996, 0.056769460952920625, 0.05274077257831475, 0.04888933779661544, 0.04532542575496731, 0.04190210328719865, 0.03875949715104535, 0.03577865021332142, 0.03264086654287136, 0.02987706016862258, 0.0273455199497388, 0.02476667531391792, 0.02258898748096453, 0.020544056855253377, 0.018819368286505392, 0.017453191217551567, 0.01626016865240798, 0.015160515044642007, 0.014073730160692634, 0.013014835891455382, 0.01201202815153013, 0.011202899949124752, 0.010381599064755782, 0.009647512586483857, 0.00891333387495662, 0.008289838582658485, 0.007659534787359412, 0.007008029934528489, 0.0064445809408309376, 0.005972672871774429, 0.005475573944850617, 0.005067860070493039, 0.0046731694649066085, 0.004266138504377504, 0.003955029059528163, 0.003652274744959793, 0.0033756711790280777, 0.003097038591816876, 0.0028293772064409838, 0.00257341806023137, 0.002379325267596421, 0.0021685556754901785, 0.0020140927178193533, 0.001864326066238779, 0.0017349740950755499, 0.0016262546625853483, 0.0015288104508707585, 0.0014325418648845397, 0.0013231169398301812, 0.0012539760133592963, 0.0011964968623454405, 0.0011158504933332656, 0.0010289171184387769, 0.0009684374726123034, 0.0009108537883818493, 0.0008669180658032661, 0.0008353086384343922, 0.0008006848910689965, 0.0007626360317331114, 0.0007218891120613053, 0.0006791461023897689, 0.0006359072739047778, 0.0005923663489992146, 0.0005495957171374609, 0.0005079227071961012, 0.00046798427979655367, 0.0004301019987488103, 0.00039446462250362864, 0.00036109339652098694, 0.00032990499824574226, 0.00030098445719956663, 0.0002742080569994125, 0.0002494098916535751, 0.00022657295625031656, 0.0002056382054705229, 0.00018645321594565456, 0.00016893953961695635, 0.0001529756236927421, 0.0001384435494167643, 0.0001251974637664486, 0.0001131430868500994, 0.00010217224466864403, 9.224813960940782e-05, 8.320948420545216e-05, 7.505035352151357e-05, 6.763909841630116e-05, 6.094365513384926e-05, 5.489532934694295e-05, 4.942867574279626e-05, 4.449469699188699e-05, 4.004539122868613e-05, 3.6046372951264896e-05, 3.249662411792122e-05, 3.528196264774649e-05, 3.1839574383640915e-05, 2.867746360875451e-05, 2.5845373685637373e-05, 2.330574090893055e-05, 2.1026059274353324e-05, 1.8973611673655892e-05, 1.7128905279151622e-05, 1.5464622011698e-05, 1.395753169150205e-05, 1.2604413720992413e-05, 1.1395813860837666e-05, 1.0310437797724525e-05, 9.32627223319802e-06, 8.440209789233418e-06, 7.66223209571199e-06, 6.9495039473163365e-06], "duration": 18008.4204, "accuracy_train": [0.46198958477528607, 0.5870784451135105, 0.6375702230989295, 0.6664181100613695, 0.6847613712393872, 0.698617095215024, 0.7145160662375416, 0.7498071388196751, 0.7578972147240679, 0.7737947437938354, 0.776257599090993, 0.7696577813538206, 0.7829504227805463, 0.7870179912444629, 0.7981100325304541, 0.7988319101259689, 0.7915327453050018, 0.7931814380306386, 0.7835574848883352, 0.8061267490886858, 0.8186088328257659, 0.8302342163851975, 0.8374156818129384, 0.8413459042774087, 0.8495282651578073, 0.8513422417174235, 0.8593164206695275, 0.8619438388242894, 0.8421160872900517, 0.8537382264557956, 0.8578308488487449, 0.8353509857189, 0.8555529239917867, 0.8524386665397747, 0.8690827666459026, 0.8903095728359173, 0.9005151736687893, 0.9064875617155776, 0.9067222395256552, 0.9067912730135659, 0.9083959861803249, 0.9225521770602622, 0.9207872268941492, 0.927038993286268, 0.9253412741671282, 0.9343399605481728, 0.9322273195251938, 0.925601366394426, 0.9325732079411223, 0.9411998705126431, 0.9351537626315062, 0.9445470033337948, 0.9443620728935955, 0.9384808883582503, 0.9510119984888336, 0.9508721290720746, 0.9533603987864526, 0.9507079267026578, 0.949290667393411, 0.9471522514650241, 0.958079729893411, 0.9515936461794019, 0.9622878887504615, 0.9625454575604466, 0.9661705267741787, 0.9703332641196014, 0.9730994702265596, 0.9739594147978959, 0.9707746819052234, 0.9798191502860835, 0.983446021940753, 0.979539771940753, 0.9773541320598007, 0.984167178559893, 0.9855157648694168, 0.9896069453096161, 0.9937464311669435, 0.9947923876430418, 0.9954198173334257, 0.99583834411914, 0.9960476075119971, 0.99632662536914, 0.9963498768572352, 0.9964428828096161, 0.9964196313215209, 0.9964661342977114, 0.9965358887619971, 0.996628894714378, 0.9966990096668512, 0.9966990096668512, 0.996768764131137, 0.9967920156192323, 0.9967222611549464, 0.9966990096668512, 0.9967222611549464, 0.9966990096668512, 0.9967222611549464, 0.9967455126430418, 0.996768764131137, 0.9967222611549464, 0.996675758178756, 0.996582752226375, 0.9966292552025655, 0.9964432432978036, 0.9964897462739941, 0.9963037343692323, 0.9962572313930418, 0.996187476928756, 0.9960479680001846, 0.9959084590716132, 0.9955829382382798, 0.9953736748454227, 0.994815639131137, 0.987329741429033, 0.9937231796788483, 0.9950946569882798, 0.9952806688930418, 0.9954205383098007, 0.9955367957502769, 0.9956065502145626, 0.9956995561669435, 0.9957460591431341, 0.9957460591431341, 0.995839065095515, 0.9959785740240864, 0.9960715799764673, 0.9960948314645626, 0.9961645859288483, 0.9963738493217055, 0.9963738493217055], "end": "2016-01-24 14:59:31.149000", "learning_rate_per_epoch": [0.0048610069788992405, 0.004718531388789415, 0.004580231849104166, 0.004445985890924931, 0.004315674304962158, 0.004189182538539171, 0.004066397901624441, 0.003947212360799313, 0.003831519978120923, 0.0037192185409367085, 0.0036102086305618286, 0.003504393855109811, 0.0034016803838312626, 0.003301977412775159, 0.003205196699127555, 0.0031112527940422297, 0.003020062344148755, 0.0029315445572137833, 0.0028456212021410465, 0.0027622163761407137, 0.002681256039068103, 0.0026026687119156122, 0.0025263847783207893, 0.0024523367173969746, 0.0023804588709026575, 0.0023106879089027643, 0.002242961898446083, 0.0021772210020571947, 0.002113406779244542, 0.0020514631178230047, 0.0019913348369300365, 0.0019329689675942063, 0.001876313821412623, 0.001821319223381579, 0.0017679365118965507, 0.0017161184223368764, 0.0016658192034810781, 0.0016169942682608962, 0.001569600310176611, 0.0015235955361276865, 0.0014789390843361616, 0.001435591490007937, 0.0013935144525021315, 0.0013526707189157605, 0.001313024084083736, 0.0012745395069941878, 0.001237182877957821, 0.0012009211350232363, 0.0011657222639769316, 0.0011315550655126572, 0.001098389271646738, 0.0010661955457180738, 0.0010349454823881388, 0.001004611374810338, 0.0009751663310453296, 0.0009465843322686851, 0.0009188400581479073, 0.0008919089450500906, 0.0008657671860419214, 0.0008403916144743562, 0.0008157598203979433, 0.0007918499759398401, 0.000768640951719135, 0.0007461121422238648, 0.0007242436986416578, 0.0007030161796137691, 0.0006824108422733843, 0.0006624094676226377, 0.0006429943023249507, 0.0006241482333280146, 0.0006058544968254864, 0.0005880969692952931, 0.000570859934668988, 0.0005541280843317509, 0.0005378866335377097, 0.0005221212049946189, 0.0005068178870715201, 0.0004919631173834205, 0.0004775437409989536, 0.0004635469813365489, 0.0004499604692682624, 0.00043677218491211534, 0.00042397042852826416, 0.00041154390783049166, 0.0003994815924670547, 0.0003877728304360062, 0.00037640726077370346, 0.0003653748135548085, 0.00035466570989228785, 0.0003442704910412431, 0.00033417996019124985, 0.00032438518246635795, 0.00031487748492509127, 0.0003056484565604478, 0.00029668991919606924, 0.0002879939565900713, 0.0002795528853312135, 0.0002713592257350683, 0.0002634057018440217, 0.00025568529963493347, 0.00024819117970764637, 0.00024091672094073147, 0.0002338554768357426, 0.00022700119006913155, 0.00022034780704416335, 0.00021388943423517048, 0.00020762035273946822, 0.00020153501827735454, 0.00019562804664019495, 0.00018989421369042248, 0.0001843284408096224, 0.00017892579489853233, 0.00017368150292895734, 0.0001685909228399396, 0.0001636495435377583, 0.00015885299944784492, 0.0001541970414109528, 0.00014967753668315709, 0.0001452904980396852, 0.00014103205467108637, 0.00013689842307940125, 0.00013288595073390752, 0.00012899107241537422, 0.00012521035387180746, 0.0001215404481627047, 0.00011797811021097004, 0.00011452018225099891, 0.00011116360838059336, 0.00010790541273308918, 0.00010474271402927116], "accuracy_valid": [0.44823924604668675, 0.5767351633094879, 0.6270090126129518, 0.6514333701995482, 0.6674260518637049, 0.6759812688253012, 0.6868455266378012, 0.7114640201430723, 0.7190735598644578, 0.7301716632153614, 0.7312408814947289, 0.720813429499247, 0.7297260330384037, 0.7320247788027108, 0.7380665239081325, 0.7298672227974398, 0.7227356692394578, 0.7175469455948795, 0.7117081607680723, 0.7208428440323795, 0.7310776308358433, 0.7373032167733433, 0.7386768754706325, 0.7382400696536144, 0.7464393707643072, 0.7423507506588856, 0.7458599044615963, 0.7474571136106928, 0.7316482727786144, 0.732666015625, 0.7381488846009037, 0.7260433334902108, 0.7264904343938253, 0.7297863328313253, 0.735339796686747, 0.7458187241152108, 0.7453907426581325, 0.7552799086972892, 0.7517707548945783, 0.7513633636106928, 0.7527164321347892, 0.7587890625, 0.7525531814759037, 0.7579242752259037, 0.7589317229856928, 0.7585861022213856, 0.7578742705195783, 0.7574874694088856, 0.7535091538027108, 0.7605289321347892, 0.7554416886295181, 0.7578227950865963, 0.7573756941829819, 0.7590640883847892, 0.7637130553463856, 0.7590037885918675, 0.7614643142884037, 0.7589934935052711, 0.7535606292356928, 0.7498279249811747, 0.7586581678275602, 0.7530414627259037, 0.7620246611445783, 0.7592376341302711, 0.7594405944088856, 0.7626144225338856, 0.7638763060052711, 0.7668265836784638, 0.7637748258659638, 0.7667045133659638, 0.76123046875, 0.7611392836972892, 0.7616378600338856, 0.7632144790097892, 0.7689620787838856, 0.7712814147213856, 0.7704475127070783, 0.7708240187311747, 0.7706916533320783, 0.7714240752070783, 0.7705592879329819, 0.7703151473079819, 0.7700710066829819, 0.7701827819088856, 0.7706813582454819, 0.7704372176204819, 0.7703151473079819, 0.7700710066829819, 0.7696945006588856, 0.7698268660579819, 0.7699489363704819, 0.7700710066829819, 0.7697047957454819, 0.7697047957454819, 0.7693385848079819, 0.7693385848079819, 0.7686061629329819, 0.7694709502070783, 0.7691047392695783, 0.7697253859186747, 0.7703460325677711, 0.7701121870293675, 0.7697459760918675, 0.7704886930534638, 0.7707534238516567, 0.7706313535391567, 0.7691562147025602, 0.770153367375753, 0.769298875188253, 0.770275437688253, 0.7702857327748494, 0.7683017225150602, 0.767345750188253, 0.7595729598079819, 0.7679149214043675, 0.7657073606927711, 0.7656970656061747, 0.7663177122552711, 0.7665721479668675, 0.7669486539909638, 0.7671927946159638, 0.7666942182793675, 0.7662059370293675, 0.7665721479668675, 0.7670604292168675, 0.7669383589043675, 0.7673045698418675, 0.7680369917168675, 0.7681693571159638, 0.7679252164909638], "accuracy_test": 0.3780552455357143, "start": "2016-01-24 09:59:22.728000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0], "accuracy_train_last": 0.9963738493217055, "batch_size_eval": 1024, "accuracy_train_std": [0.01544689814093013, 0.015706170010318616, 0.017275252700882752, 0.017307676490366394, 0.01844013834189883, 0.016595408998098738, 0.018545108799716718, 0.01843686961443425, 0.019060971393071214, 0.021053628366501428, 0.020522901891752997, 0.01975639848296711, 0.023918187534221855, 0.02369712748293225, 0.0206715551639939, 0.02223056924544099, 0.021000535352389837, 0.02315786844850584, 0.022851704915325056, 0.02516319589618452, 0.025932972178891192, 0.025453325190665906, 0.02624069968742811, 0.024823112652558352, 0.025083827550003424, 0.0237783202228814, 0.02497854746308711, 0.02460603210812514, 0.02248727923882761, 0.025215789195992867, 0.023318256370908237, 0.02262254911339867, 0.023327481990007597, 0.02161134133650812, 0.023211009462302815, 0.02282516916879172, 0.023157969050203637, 0.024228780311483036, 0.021254329411093266, 0.022325907272986528, 0.02132708468750758, 0.021282990593754318, 0.02087407996668888, 0.02170008456809933, 0.022503716561899256, 0.02070390607614773, 0.020510303379990276, 0.019081255834253658, 0.020178464885240114, 0.0194170152207687, 0.018955574719897627, 0.017435669045724005, 0.017573703977232194, 0.01796560661485233, 0.015921779685543182, 0.015211813908615812, 0.014322002156822038, 0.014153833337864723, 0.014344742748629761, 0.013630187683210958, 0.013128405204426756, 0.013411228654008981, 0.011218176836306472, 0.011317936325597298, 0.011194473516495442, 0.01040648871916227, 0.010227668283774175, 0.010132230001409974, 0.009390924983725861, 0.008104807802469588, 0.007438691402694923, 0.007989669670812848, 0.00844932496709102, 0.006382605412291482, 0.006308517143631087, 0.005411968822419101, 0.0034761301381875296, 0.003506237464159386, 0.0031860914163066687, 0.002999347838587043, 0.0028189662033817104, 0.0025823082610201727, 0.0025312396332063973, 0.0025558821529377343, 0.002545605835299499, 0.002420179295904938, 0.0024036939641530067, 0.0023171016494830526, 0.002213955247775283, 0.002193347148487829, 0.002259972994018532, 0.00229780458655931, 0.0023711234396158348, 0.0023337854638727736, 0.0022932339314617536, 0.002284620160541747, 0.0022833109297074545, 0.0022717909233479095, 0.0022998109018867474, 0.002437235689197438, 0.002448756941072549, 0.002459798332486649, 0.0024127397168333476, 0.002390283418725504, 0.002476472887977303, 0.0026405011989314078, 0.002643787784790826, 0.002510707630942485, 0.002736983957493369, 0.0028451058547169857, 0.002796894408820189, 0.002787363905888286, 0.002918265572066566, 0.004574143121311067, 0.002755253571799114, 0.0021471721466956367, 0.002203771327009482, 0.0023092804427901647, 0.0024304765381518555, 0.002440767108209633, 0.0024296254323188373, 0.002322202152098284, 0.0022527164482925538, 0.0022144278300169126, 0.002174448021008609, 0.002208446825171598, 0.0021930818785351656, 0.002238068441490605, 0.0022267058581984175, 0.00225709064217483], "accuracy_test_std": 0.012798140336717063, "error_valid": [0.5517607539533133, 0.42326483669051207, 0.37299098738704817, 0.34856662980045183, 0.33257394813629515, 0.3240187311746988, 0.3131544733621988, 0.2885359798569277, 0.28092644013554224, 0.2698283367846386, 0.2687591185052711, 0.279186570500753, 0.27027396696159633, 0.2679752211972892, 0.26193347609186746, 0.27013277720256024, 0.27726433076054224, 0.2824530544051205, 0.2882918392319277, 0.2791571559676205, 0.2689223691641567, 0.2626967832266567, 0.26132312452936746, 0.26175993034638556, 0.2535606292356928, 0.25764924934111444, 0.25414009553840367, 0.2525428863893072, 0.26835172722138556, 0.267333984375, 0.26185111539909633, 0.2739566665097892, 0.2735095656061747, 0.2702136671686747, 0.264660203313253, 0.2541812758847892, 0.25460925734186746, 0.24472009130271077, 0.24822924510542166, 0.24863663638930722, 0.24728356786521077, 0.2412109375, 0.24744681852409633, 0.24207572477409633, 0.24106827701430722, 0.24141389777861444, 0.24212572948042166, 0.24251253059111444, 0.24649084619728923, 0.23947106786521077, 0.2445583113704819, 0.24217720491340367, 0.2426243058170181, 0.24093591161521077, 0.23628694465361444, 0.24099621140813254, 0.23853568571159633, 0.24100650649472888, 0.24643937076430722, 0.2501720750188253, 0.24134183217243976, 0.24695853727409633, 0.23797533885542166, 0.24076236586972888, 0.24055940559111444, 0.23738557746611444, 0.23612369399472888, 0.2331734163215362, 0.2362251741340362, 0.2332954866340362, 0.23876953125, 0.23886071630271077, 0.23836213996611444, 0.23678552099021077, 0.23103792121611444, 0.22871858527861444, 0.22955248729292166, 0.22917598126882532, 0.22930834666792166, 0.22857592479292166, 0.2294407120670181, 0.2296848526920181, 0.2299289933170181, 0.22981721809111444, 0.2293186417545181, 0.2295627823795181, 0.2296848526920181, 0.2299289933170181, 0.23030549934111444, 0.2301731339420181, 0.2300510636295181, 0.2299289933170181, 0.2302952042545181, 0.2302952042545181, 0.2306614151920181, 0.2306614151920181, 0.2313938370670181, 0.23052904979292166, 0.23089526073042166, 0.23027461408132532, 0.22965396743222888, 0.22988781297063254, 0.23025402390813254, 0.2295113069465362, 0.22924657614834332, 0.22936864646084332, 0.23084378529743976, 0.22984663262424698, 0.23070112481174698, 0.22972456231174698, 0.22971426722515065, 0.23169827748493976, 0.23265424981174698, 0.2404270401920181, 0.23208507859563254, 0.23429263930722888, 0.23430293439382532, 0.23368228774472888, 0.23342785203313254, 0.2330513460090362, 0.2328072053840362, 0.23330578172063254, 0.23379406297063254, 0.23342785203313254, 0.23293957078313254, 0.23306164109563254, 0.23269543015813254, 0.23196300828313254, 0.2318306428840362, 0.2320747835090362], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-3, 3], "momentum": 0.5032151146852581, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.005007785043005017, "patience_threshold": 1, "do_flip": true, "batch_size": 128, "optimization": "adam", "nb_data_augmentation": 0, "learning_rate_decay_method": "exp", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 5.939602783025384e-07, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.029309896054101694}, "accuracy_valid_max": 0.7714240752070783, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-3, 3)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.7679252164909638, "loss_train": [1.4505884647369385, 1.12679922580719, 0.9962666034698486, 0.9006738066673279, 0.8259279131889343, 0.7620686292648315, 0.7131918668746948, 0.6666161417961121, 0.6294350028038025, 0.5937017798423767, 0.5602235198020935, 0.5340243577957153, 0.5103916525840759, 0.4873681366443634, 0.4654826819896698, 0.44281160831451416, 0.421434611082077, 0.4014662206172943, 0.3856913149356842, 0.37125200033187866, 0.3579587936401367, 0.3392762541770935, 0.3233075439929962, 0.30849725008010864, 0.2905175983905792, 0.2774384319782257, 0.2605551481246948, 0.25104570388793945, 0.23586583137512207, 0.2261105626821518, 0.21532107889652252, 0.2073962241411209, 0.1942669302225113, 0.18571659922599792, 0.1771065890789032, 0.1678713709115982, 0.15584972500801086, 0.14915041625499725, 0.14285075664520264, 0.1338028609752655, 0.12568596005439758, 0.12013198435306549, 0.11152887344360352, 0.10755746066570282, 0.09672225266695023, 0.0903024673461914, 0.08726093173027039, 0.08118502050638199, 0.07620759308338165, 0.07552816718816757, 0.06729196757078171, 0.06243166700005531, 0.05756460502743721, 0.05756310746073723, 0.05523766949772835, 0.04951102286577225, 0.04814792796969414, 0.04663462191820145, 0.043388061225414276, 0.03943381831049919, 0.037654731422662735, 0.03447658196091652, 0.034543346613645554, 0.035070326179265976, 0.03230336681008339, 0.03335922583937645, 0.0290887039154768, 0.02843526378273964, 0.026430722326040268, 0.02480158396065235, 0.022270577028393745, 0.02397315204143524, 0.025329366326332092, 0.023218797519803047, 0.02065444365143776, 0.018724773079156876, 0.01743239536881447, 0.016358569264411926, 0.015863675624132156, 0.015594917349517345, 0.015441338531672955, 0.015335282310843468, 0.015250538475811481, 0.015177657827734947, 0.015112423337996006, 0.01505250297486782, 0.01499547902494669, 0.014940247870981693, 0.0148859154433012, 0.01483127661049366, 0.014775816351175308, 0.014718631282448769, 0.014659025706350803, 0.014596182852983475, 0.01452912949025631, 0.01445706095546484, 0.01437907200306654, 0.014294160529971123, 0.014201324433088303, 0.01409957092255354, 0.013987910933792591, 0.013865151442587376, 0.013730601407587528, 0.013583417050540447, 0.013422837480902672, 0.013248384930193424, 0.01306027453392744, 0.012859092094004154, 0.012646549381315708, 0.012425106018781662, 0.012198216281831264, 0.01196877658367157, 0.0117475101724267, 0.02759328857064247, 0.014375945553183556, 0.012147700414061546, 0.011843004263937473, 0.011792280711233616, 0.011768098920583725, 0.011750738136470318, 0.011736760847270489, 0.011724727228283882, 0.011713975109159946, 0.011704056523740292, 0.011694622226059437, 0.01168547011911869, 0.011676434427499771, 0.011667356826364994, 0.011658105999231339, 0.01164857018738985], "accuracy_train_first": 0.46198958477528607, "model": "residualv3", "loss_std": [0.2368595153093338, 0.11553191393613815, 0.11128350347280502, 0.10864657163619995, 0.10241729021072388, 0.09585399925708771, 0.09408999979496002, 0.09287600964307785, 0.09123824536800385, 0.08995260298252106, 0.08592101186513901, 0.08437556028366089, 0.08453575521707535, 0.08300070464611053, 0.08039616793394089, 0.07680503278970718, 0.07495521754026413, 0.07060258090496063, 0.07284348458051682, 0.07202765345573425, 0.06962461024522781, 0.06603828817605972, 0.06603029370307922, 0.06363970041275024, 0.062476083636283875, 0.06253021210432053, 0.06146688386797905, 0.05855591595172882, 0.054483961313962936, 0.05608367919921875, 0.051069941371679306, 0.050477366894483566, 0.048297662287950516, 0.04757441207766533, 0.04475335404276848, 0.042228348553180695, 0.041745301336050034, 0.04079590365290642, 0.03866010159254074, 0.03557439148426056, 0.0365409180521965, 0.03555973246693611, 0.034624386578798294, 0.03322365880012512, 0.0288699883967638, 0.027316411957144737, 0.02708248794078827, 0.026956522837281227, 0.02451244369149208, 0.0253889299929142, 0.022794736549258232, 0.022069457918405533, 0.020720163360238075, 0.020236393436789513, 0.01951034553349018, 0.017242636531591415, 0.01601463556289673, 0.015912286937236786, 0.015453838743269444, 0.013459690846502781, 0.013241826556622982, 0.011195641942322254, 0.011942381970584393, 0.012642761692404747, 0.009943533688783646, 0.01134402398020029, 0.008721878752112389, 0.008724416606128216, 0.008407498709857464, 0.007074710913002491, 0.004757816903293133, 0.007352620363235474, 0.007798758335411549, 0.006169237196445465, 0.004298773128539324, 0.002568824216723442, 0.0016472360584884882, 0.0008427139255218208, 0.00048606659402139485, 0.0002791968290694058, 0.0002092477516271174, 0.00017190768267028034, 0.00014605636533815414, 0.00012596095621120185, 0.00010993280011462048, 9.68576132436283e-05, 8.570707723265514e-05, 7.596745854243636e-05, 6.777641829103231e-05, 6.0650174418697134e-05, 5.440269524115138e-05, 4.9088783271145076e-05, 4.4517342757899314e-05, 4.083422390976921e-05, 3.786326851695776e-05, 3.572705463739112e-05, 3.444523099460639e-05, 3.394794839550741e-05, 3.426051989663392e-05, 3.5392156860325485e-05, 3.72198446711991e-05, 3.956359068979509e-05, 4.2469429899938405e-05, 4.5698441681452096e-05, 4.9283782573184e-05, 5.30462057213299e-05, 5.675781721947715e-05, 6.024416143191047e-05, 6.314645725069568e-05, 6.525738717755303e-05, 6.65130719426088e-05, 6.644754466833547e-05, 6.117558950791135e-05, 0.023149758577346802, 0.006484282668679953, 0.0009449372300878167, 0.00014243554323911667, 8.516620437148958e-05, 6.825799209764227e-05, 5.754360245191492e-05, 4.9893613322637975e-05, 4.397194425109774e-05, 3.91956273233518e-05, 3.517556979204528e-05, 3.1758514523971826e-05, 2.8794751415262e-05, 2.6229148716083728e-05, 2.395665978838224e-05, 2.195521301473491e-05, 2.0186789697618224e-05]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:10 2016", "state": "available"}], "summary": "a734abd880742601796c5ef0aa2c1f03"}