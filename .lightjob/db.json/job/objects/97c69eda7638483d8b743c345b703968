{"content": {"hp_model": {"f0": 64, "f1": 32, "f2": 64, "f3": 64, "nonlin": "very_leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.6774210929870605, 1.1909968852996826, 0.9302710890769958, 0.8125966191291809, 0.7463505864143372, 0.7014214396476746, 0.6662510633468628, 0.64162677526474, 0.6212096214294434, 0.6028300523757935, 0.5881151556968689, 0.5734873414039612, 0.5625239014625549, 0.5528272390365601, 0.5425362586975098, 0.535253643989563, 0.5293254852294922, 0.5210419297218323, 0.5126990079879761, 0.5084202885627747, 0.5020727515220642, 0.4972895681858063, 0.49279600381851196, 0.48807016015052795, 0.4844435453414917, 0.4792253077030182, 0.4746919870376587, 0.4722740352153778, 0.46944814920425415, 0.4636249840259552, 0.4615550935268402, 0.4598202705383301, 0.4560956060886383, 0.45433562994003296, 0.45306533575057983, 0.44988536834716797, 0.445664644241333, 0.4445857107639313, 0.4415337145328522, 0.4391961395740509, 0.4381873309612274, 0.4339286983013153, 0.4335458278656006, 0.4315101206302643, 0.4291672706604004, 0.42753738164901733, 0.42744559049606323, 0.4240988492965698, 0.4229568541049957, 0.42075052857398987, 0.4194706976413727, 0.4186709225177765, 0.4160095751285553, 0.4129541218280792, 0.412020206451416, 0.4117376506328583, 0.4114534854888916, 0.40795832872390747, 0.4075908362865448, 0.40511882305145264, 0.4045535624027252, 0.40285512804985046, 0.4030769467353821, 0.4026584029197693, 0.4006955325603485, 0.399236261844635, 0.39872926473617554, 0.39754626154899597, 0.3971831500530243, 0.3939490020275116, 0.3944738805294037, 0.3930051028728485, 0.390349805355072, 0.3897879123687744, 0.390838086605072, 0.3890243470668793, 0.3878806233406067, 0.38647428154945374, 0.3861393332481384, 0.38422656059265137, 0.38594743609428406, 0.38372039794921875, 0.38215455412864685, 0.3805156648159027, 0.38191717863082886, 0.3797151446342468, 0.37953218817710876, 0.3802664875984192, 0.37811383605003357, 0.377674400806427, 0.37563756108283997, 0.37718236446380615, 0.37497082352638245, 0.3734964430332184, 0.37402769923210144, 0.3737741708755493, 0.3716304898262024, 0.37093910574913025, 0.370304673910141, 0.36965200304985046, 0.36884692311286926, 0.3674306273460388, 0.3669188916683197, 0.3678516149520874, 0.366914302110672, 0.3657881021499634, 0.3646959662437439, 0.36437365412712097, 0.36400559544563293, 0.3640969693660736, 0.36329108476638794, 0.3626089096069336, 0.3614640235900879, 0.3612121343612671, 0.3612724840641022, 0.36162763833999634, 0.35910969972610474, 0.3589697480201721, 0.3591403365135193, 0.35925084352493286, 0.35744839906692505, 0.3559597432613373, 0.3566323220729828, 0.35467562079429626, 0.3572554588317871, 0.3556910455226898, 0.35546356439590454, 0.3552328944206238, 0.3522186279296875, 0.35254278779029846, 0.3516148626804352, 0.3543434739112854, 0.3521776795387268, 0.3517507314682007, 0.35157278180122375, 0.350755900144577, 0.3512209951877594, 0.34857141971588135, 0.3495590090751648, 0.3496856093406677, 0.3475285768508911, 0.3490687906742096, 0.3479331433773041, 0.3484991490840912, 0.34611043334007263, 0.34704825282096863, 0.3468005061149597, 0.3441247045993805, 0.3451104462146759, 0.34509149193763733, 0.3432047665119171, 0.3444037139415741, 0.34239059686660767, 0.34282007813453674, 0.34401077032089233, 0.3422289192676544, 0.3402479588985443, 0.34167778491973877, 0.34053555130958557, 0.3408869802951813, 0.3411477506160736, 0.3404507637023926, 0.34150728583335876, 0.3395848572254181, 0.33993369340896606, 0.33960476517677307, 0.3392038345336914, 0.33933690190315247, 0.3394334018230438, 0.3374159634113312, 0.3367042541503906, 0.3376384973526001, 0.33566200733184814, 0.3354671001434326, 0.33521971106529236, 0.3361113965511322, 0.3373073637485504, 0.3350987732410431, 0.3334975242614746, 0.3325766623020172, 0.3344193398952484, 0.3340299129486084, 0.33372142910957336, 0.3350035548210144, 0.33233579993247986, 0.33375123143196106, 0.33210650086402893, 0.33282941579818726, 0.33124613761901855, 0.3320506513118744, 0.3309682011604309, 0.3302561342716217, 0.33061483502388, 0.3301834762096405, 0.3286903500556946, 0.32865920662879944, 0.3294309675693512, 0.3306320309638977, 0.32981163263320923, 0.3292413651943207, 0.32733994722366333, 0.3282618820667267, 0.32482874393463135, 0.3289921283721924, 0.3267362415790558, 0.32890939712524414, 0.32664430141448975, 0.32760119438171387, 0.32624614238739014, 0.32500016689300537, 0.32549381256103516, 0.3266787528991699, 0.32499995827674866, 0.32518261671066284, 0.325911283493042, 0.32476991415023804, 0.3253616392612457, 0.3227499723434448, 0.32457244396209717, 0.32323235273361206, 0.3231775164604187, 0.3222385346889496, 0.32363542914390564, 0.323993444442749, 0.32262033224105835, 0.32105356454849243, 0.3225494623184204, 0.3213612735271454, 0.32066529989242554, 0.31998834013938904, 0.31997695565223694, 0.32063010334968567, 0.31834477186203003, 0.32110580801963806, 0.3197619616985321, 0.3184298574924469, 0.3196066915988922, 0.3192217946052551, 0.31811070442199707, 0.31798121333122253, 0.3168126046657562, 0.3171529471874237, 0.3183706998825073, 0.31732767820358276], "moving_avg_accuracy_train": [0.028075294117647052, 0.06032423529411764, 0.1203929882352941, 0.18317486588235293, 0.2426973792941176, 0.2979099943058823, 0.34974252428705876, 0.39746474244658814, 0.4411135623195763, 0.4811645590287952, 0.5176292795965038, 0.5508945869309712, 0.5816545400025799, 0.6099126154140866, 0.6356131185785603, 0.658755336132469, 0.6801056848721633, 0.6998809987378881, 0.7176693694523345, 0.733947138389454, 0.7488230127858028, 0.7624489468013401, 0.7749028756506179, 0.7861937645561443, 0.7965273292770004, 0.805982831643418, 0.8146221955378997, 0.8226235053958745, 0.8300623313268752, 0.8367102158412465, 0.8428533119041807, 0.848579745419645, 0.8538723591129745, 0.8587792408487358, 0.8632707285285681, 0.8674542439110055, 0.8712429371669637, 0.8746762905090908, 0.8779592496934758, 0.8810056776653048, 0.8838674628399509, 0.8865348342030146, 0.8890248801944778, 0.89132239217503, 0.8933642706045858, 0.8952937258970685, 0.8971667062485381, 0.8988759179766255, 0.9006259732377865, 0.9021633759140079, 0.9035799794990776, 0.9049819815491699, 0.9062320186883704, 0.9074841109371805, 0.9086674645493449, 0.9098007180944104, 0.9108112345202635, 0.9118877581270606, 0.9128942764320016, 0.9137930840829191, 0.9145996580275684, 0.9154314569306938, 0.9161471347670362, 0.9169324212903327, 0.9177662379848288, 0.9184437318334048, 0.9191146527677113, 0.9197255404321167, 0.9203859275653756, 0.9209590995147203, 0.9215643660338365, 0.9220596941363352, 0.9226513717815252, 0.9232262346033726, 0.9238094934959765, 0.9243156029699083, 0.924846395614094, 0.9253923442879787, 0.9259378157415338, 0.9263393282850275, 0.9267265719271129, 0.927180973557931, 0.9276158173786085, 0.9280448238760418, 0.9284450473707906, 0.928807601457241, 0.9292115471938698, 0.9296621571803652, 0.9299782944035051, 0.9303851708455075, 0.9307748890550744, 0.9311491648554494, 0.9315707189581397, 0.9319595294152669, 0.9322506352972696, 0.9326043952969544, 0.9328968969437295, 0.9331272072493565, 0.9334968394655973, 0.9338718614013904, 0.9341905576141926, 0.9345009136174791, 0.934834351667496, 0.9350920929713346, 0.9354181777918481, 0.9356763600126633, 0.9359040181290441, 0.9361371457279044, 0.9364081370374668, 0.9367343821572496, 0.936988002765054, 0.9372374377826663, 0.9374289881220468, 0.9377237363686657, 0.9379184215553286, 0.9382559911645016, 0.9385150979304044, 0.9388329999020698, 0.9391261705000982, 0.9393335534500884, 0.9396213745756679, 0.9399133547651599, 0.9401949604651145, 0.9404131114774267, 0.9406188591532134, 0.9408769732378921, 0.94110457003175, 0.9413399953815161, 0.9415377605492469, 0.9417486903766751, 0.9419432331037134, 0.9420383215580479, 0.9422650776375372, 0.9424691581090776, 0.9426387128864051, 0.9428054298330588, 0.9429813574379882, 0.9431843981647776, 0.9433883112894763, 0.9435977154546463, 0.9437202968503582, 0.9439035612829695, 0.9440496757429078, 0.9442470611097935, 0.9443494138223436, 0.9445615312636387, 0.9447689075490395, 0.9449461344411944, 0.945032697467663, 0.9452023688973674, 0.94540919083116, 0.9455435658656911, 0.945713915161475, 0.945815464821798, 0.9460033301043241, 0.9462312323880094, 0.9463728150315613, 0.9465872982342876, 0.9467073919402705, 0.9469001821580082, 0.9471254580598545, 0.9472646769597514, 0.9474276210284821, 0.9474919177491633, 0.947698020091894, 0.947850571023881, 0.9479313962744341, 0.9480653154705201, 0.9482493721587621, 0.9483750231781801, 0.9485234032133033, 0.9486451805390318, 0.9488324271910109, 0.9489185962366157, 0.9490690895541306, 0.9491739453045999, 0.9492518448917869, 0.949394895696726, 0.9494789355388181, 0.9496651596319952, 0.9498045260217368, 0.9500052498901513, 0.9502141366658421, 0.9503550759404343, 0.9504560389346262, 0.9505986703352811, 0.9506940974194, 0.9507988053245189, 0.9509283365567729, 0.9510943264305074, 0.9511943055521626, 0.9513195808792992, 0.9513876227913692, 0.9516041546298795, 0.9517096215198327, 0.9518045417207907, 0.9519417346075351, 0.9520487376173697, 0.9522062167968093, 0.9523385362935989, 0.9525211532524742, 0.952614920280168, 0.9527440164874453, 0.9528602030739949, 0.9530447710018894, 0.9531661762546416, 0.9532660292174127, 0.9534288380603773, 0.9535871307249277, 0.9537272411818467, 0.9537686347107208, 0.9539305947690605, 0.9540645941156839, 0.9541922523511743, 0.9543024388807628, 0.9544039596985688, 0.9545400343169472, 0.9546601485323113, 0.954723545443786, 0.9548464850170544, 0.9549453659271137, 0.9550249469814611, 0.955117746400962, 0.955271854113807, 0.9553799628200733, 0.9554678488910071, 0.9556481228254358, 0.955772722307598, 0.9558660383121323, 0.9559476697750368, 0.9560729027975331, 0.956206788988368, 0.9563225806777665, 0.9564691461394016, 0.9565257609372262, 0.9566214201376212, 0.9567569251826826, 0.9568788797232379, 0.9569462858685612, 0.9569740102228816, 0.9570907268476523, 0.9571840071040635, 0.9573009005113041, 0.9573990457542914], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.028013333333333328, 0.06053199999999999, 0.11939879999999999, 0.18092558666666664, 0.23905969466666666, 0.2922070585333333, 0.34134635267999996, 0.386411717412, 0.42735721233746665, 0.46464815777038665, 0.498143341993348, 0.5286623411273466, 0.5561694403479452, 0.581765829646484, 0.6048292466818357, 0.6257463220136521, 0.6449583564789536, 0.6623558541643916, 0.6780136020812857, 0.6925189085398238, 0.7056136843525082, 0.7175723159172573, 0.7281484176588648, 0.737960242559645, 0.7467375516370138, 0.7546637964733125, 0.7619440834926479, 0.7685896751433832, 0.7745840409623782, 0.7799256368661404, 0.7848530731795264, 0.7894744325282403, 0.7934869892754164, 0.7971249570145414, 0.800399127979754, 0.8034258818484452, 0.806256626996934, 0.8086176309639073, 0.8108092012008499, 0.8129016144140983, 0.8147181196393551, 0.8165929743420863, 0.8182136769078777, 0.81967230921709, 0.8209317449620477, 0.8221185704658429, 0.8234000467525919, 0.8244333754106661, 0.8255233712029328, 0.8263443674159728, 0.8270699306743755, 0.8279762709402713, 0.8286186438462441, 0.8294367794616196, 0.830146434848791, 0.8309184580305786, 0.8314666122275207, 0.8319599510047686, 0.8323506225709585, 0.8327022269805293, 0.8331120042824763, 0.8334008038542288, 0.8338873901354725, 0.8342586511219252, 0.8344994526763994, 0.8348228407420928, 0.8349805566678835, 0.8352558343344285, 0.8356102509009856, 0.8357425591442204, 0.8359816365631316, 0.8361834729068185, 0.8364851256161366, 0.836756613054523, 0.8370009517490707, 0.8371808565741636, 0.8375027709167473, 0.8377124938250726, 0.8378479111092321, 0.8379964533316422, 0.838116807998478, 0.8382651271986301, 0.8385186144787671, 0.8386267530308904, 0.8387907443944681, 0.8389516699550214, 0.8390565029595192, 0.8391641859969006, 0.8392211007305439, 0.8393923239908229, 0.8395464249250739, 0.8396317824325665, 0.8399086041893099, 0.8400244104370456, 0.8399953027266743, 0.8401824391206736, 0.8404441952086062, 0.8403864423544123, 0.8404544647856378, 0.8406490183070741, 0.8408241164763667, 0.8410483714953967, 0.841103534345857, 0.841179847577938, 0.8413151961534775, 0.8414236765381298, 0.8414679755509834, 0.8414278446625517, 0.8414583935296299, 0.8415792208433336, 0.8416212987590002, 0.8417258355497668, 0.8419665853281234, 0.841969926795311, 0.8420396007824466, 0.8420889740375352, 0.8422534099671151, 0.8422414023037368, 0.8422972620733632, 0.8424408691993602, 0.8426367822794242, 0.8425997707181484, 0.8427264603130001, 0.8428271476150334, 0.8428910995201968, 0.8428286562348438, 0.8429057906113594, 0.8428685448835568, 0.8428883570618677, 0.8429995213556809, 0.8430862358867794, 0.8430842789647681, 0.843122517734958, 0.8432102659614622, 0.8430759060319827, 0.843154982095451, 0.8430928172192392, 0.8431968688306487, 0.8431971819475838, 0.8433441304194921, 0.8433830507108762, 0.8432847456397885, 0.843329604409143, 0.8435433106348953, 0.8434823129047391, 0.8436274149475985, 0.843638006786172, 0.8436875394408881, 0.8434921188301326, 0.8434362402804527, 0.8433992829190741, 0.8433660212938333, 0.84350941916445, 0.8435584772480049, 0.8436692961898712, 0.843702366570884, 0.843625463247129, 0.8437295835890828, 0.8437566252301745, 0.8437809627071571, 0.8438828664364414, 0.8439079131261306, 0.8438904551468508, 0.8440347429654991, 0.8441112686689491, 0.8441001418020542, 0.8442101276218487, 0.8443357815263305, 0.8445022033736974, 0.8445986497029944, 0.8446587847326948, 0.8448062395927587, 0.8447922823001496, 0.8448730540701346, 0.8449190819964545, 0.8448538404634757, 0.8448084564171281, 0.8446876107754153, 0.8447521830312071, 0.8447836313947531, 0.8447986015886111, 0.8449587414297499, 0.8450095339534416, 0.8449485805580974, 0.844907055835621, 0.8450963502520589, 0.8450533818935196, 0.8450280437041677, 0.8450719060004176, 0.8449913820670425, 0.8448789105270049, 0.8449376861409711, 0.8449639175268739, 0.8451741924408531, 0.8452167731967678, 0.845135095877091, 0.8451949196227152, 0.8452354276604437, 0.8452852182277326, 0.845303363071626, 0.8452130267644634, 0.8451983907546837, 0.8450918850125487, 0.8451426965112938, 0.8451084268601644, 0.845117584174148, 0.8451258257567331, 0.8450932431810598, 0.8450905855296205, 0.8451415269766585, 0.8450273742789927, 0.8449779701844267, 0.845040173165984, 0.8451228225160523, 0.8451172069311137, 0.8451654862380024, 0.8451022709475354, 0.8450587105194486, 0.8450995061341704, 0.8451628888540866, 0.8452199333020113, 0.8452046066384767, 0.8451641459746291, 0.8451277313771662, 0.8451482915727829, 0.8451401290821713, 0.8452261161739542, 0.8452235045565587, 0.8451811541009029, 0.8452097053574793, 0.8452087348217313, 0.8452345280062249, 0.8453644085389358, 0.8453479676850422, 0.8453998375832046, 0.8454331871582175, 0.845343201775729, 0.8453422149314894, 0.8452746601050072, 0.8452138607611732, 0.8452391413517226, 0.845261893883217, 0.8451890378282286, 0.8452034673787391], "moving_var_accuracy_train": [0.007093999258131487, 0.01574454719534948, 0.04664438819498738, 0.07745402682350103, 0.1015949905668343, 0.11887138721808699, 0.13116374897452432, 0.13854406503166336, 0.1418366338152382, 0.1420897114703311, 0.1398478229380277, 0.13582226669273398, 0.13075561244016876, 0.12486672062981348, 0.11832469133299624, 0.11131228229950857, 0.1042835905913168, 0.09737479887857647, 0.09048515418479003, 0.08382133062044331, 0.07743082230990248, 0.07135873477907424, 0.06561876439521268, 0.06020424550618387, 0.05514486399412659, 0.050435036319725894, 0.046063280164264774, 0.04203314078282821, 0.03832785188563096, 0.034892816013715626, 0.03174317307549, 0.028863984135204298, 0.02622969155904525, 0.023823419798459464, 0.021622638972816285, 0.019617891284130464, 0.017785289925007112, 0.016112852169053463, 0.01459856734120516, 0.013222237117572517, 0.011973721735287678, 0.010840383391655342, 0.009812148013846228, 0.008878440264168634, 0.008028119645441539, 0.007258812860428588, 0.00656450407295865, 0.005934346308245668, 0.005368475918175157, 0.004852900789257315, 0.004385671601786677, 0.003964794929344175, 0.0035823787720541864, 0.003238250509844539, 0.0029270283908028856, 0.0026458839240992293, 0.0023904858227115763, 0.0021618673681243436, 0.001954798343195541, 0.0017665892056161173, 0.0015957853388081891, 0.0014424338096645376, 0.0013028001815869692, 0.0011780702377413104, 0.0010665204664873635, 0.0009639994010723513, 0.0008716506750659336, 0.0007878442612060436, 0.0007129848355774045, 0.0006446430867713055, 0.0005834759061266423, 0.0005273364648761029, 0.00047775356031085097, 0.00043295240965524794, 0.00039271888711193704, 0.00035575231959717466, 0.00032271275511755145, 0.00029312401919644475, 0.0002664894692365916, 0.00024129143321617726, 0.00021851190863958028, 0.0001985190453544337, 0.0001803689431544228, 0.0001639884680125401, 0.00014903123082302632, 0.00013531111693114033, 0.00012324855466129187, 0.00011275114343452696, 0.00010237551378576606, 9.36278983586985e-05, 8.563203106864085e-05, 7.832956933449328e-05, 7.209598315449963e-05, 6.624694698319287e-05, 6.038493599570283e-05, 5.547275763252553e-05, 5.069549678956854e-05, 4.610333264251395e-05, 4.272265115581051e-05, 3.9716159111164305e-05, 3.665864868453765e-05, 3.3859671455068354e-05, 3.147433270835255e-05, 2.8924774654857303e-05, 2.6989278980896105e-05, 2.4890273615112037e-05, 2.286770021518699e-05, 2.107006648982159e-05, 1.962398644956502e-05, 1.861951070824695e-05, 1.733647035175015e-05, 1.61627837686761e-05, 1.4876729184459309e-05, 1.4170945025977328e-05, 1.3094971420533115e-05, 1.2811053447814859e-05, 1.2134174948262714e-05, 1.1830312425735406e-05, 1.1420822179096366e-05, 1.0665809152706412e-05, 1.0344797240403837e-05, 1.0077589395865694e-05, 9.78354638850141e-06, 9.233500527206294e-06, 8.691139429310512e-06, 8.421631412764987e-06, 8.045670976657855e-06, 7.739929736804838e-06, 7.317936317232444e-06, 6.98656521439942e-06, 6.628530546751221e-06, 6.0470538194056834e-06, 5.905113313733473e-06, 5.689441532137499e-06, 5.379236781555048e-06, 5.0914639661130546e-06, 4.860872269087566e-06, 4.745814872795372e-06, 4.645458447335351e-06, 4.575563542116766e-06, 4.253242975077052e-06, 4.130191347912217e-06, 3.9093171317486894e-06, 3.8690342661192116e-06, 3.576415539404479e-06, 3.623718265578028e-06, 3.648390752740046e-06, 3.566236019191912e-06, 3.277050835235673e-06, 3.2084412982331663e-06, 3.272574979089503e-06, 3.1078273303275476e-06, 3.0582145404612217e-06, 2.8452040880207314e-06, 2.8783239586260792e-06, 3.0579466209439556e-06, 2.9325627634461172e-06, 3.053333885366792e-06, 2.877802980780706e-06, 2.9245352952006023e-06, 3.088824853254282e-06, 2.954379486725409e-06, 2.8978984638636258e-06, 2.64531523209047e-06, 2.763087289993293e-06, 2.6962246426450824e-06, 2.4853966685232776e-06, 2.3982661613938304e-06, 2.4633313256343246e-06, 2.3590918011977125e-06, 2.321332334486299e-06, 2.2226665545919125e-06, 2.315951677229437e-06, 2.1511824492904578e-06, 2.1398983519111695e-06, 2.0248610723783587e-06, 1.8769900762957648e-06, 1.8734628638093065e-06, 1.7496808329581724e-06, 1.8868274655787553e-06, 1.872951634327492e-06, 2.0482671130564784e-06, 2.2361435672771414e-06, 2.191304122652779e-06, 2.063915446153166e-06, 2.040617349613216e-06, 1.918512570102902e-06, 1.8253350216419491e-06, 1.793806580640907e-06, 1.8623996662182887e-06, 1.766122122498871e-06, 1.7307550785518192e-06, 1.5993468868800047e-06, 1.861386531989204e-06, 1.775357262677943e-06, 1.6789101373591479e-06, 1.6804161171827543e-06, 1.6154213024877006e-06, 1.6770763998513734e-06, 1.6669448029422553e-06, 1.800390905668149e-06, 1.6994821144440277e-06, 1.6795263796001085e-06, 1.6330676476864943e-06, 1.776348762983713e-06, 1.7313670052478373e-06, 1.647965832290616e-06, 1.7217297231888387e-06, 1.7750658597244566e-06, 1.7742377349944624e-06, 1.6122347795889186e-06, 1.6870908461065681e-06, 1.679984185555291e-06, 1.6586553927965452e-06, 1.6020594952416068e-06, 1.5346118337495652e-06, 1.5477973662760866e-06, 1.5228644522412776e-06, 1.4067505224779941e-06, 1.4021027183090732e-06, 1.3498893558454962e-06, 1.271898718160419e-06, 1.22221443668179e-06, 1.3137356774383967e-06, 1.287549541029843e-06, 1.2283102401045548e-06, 1.3979674390037318e-06, 1.3978959736993882e-06, 1.336477266649685e-06, 1.26280280160794e-06, 1.2776723107592399e-06, 1.3112346885499112e-06, 1.3007806576987233e-06, 1.3640355028276658e-06, 1.256479070539329e-06, 1.2131873070673405e-06, 1.2571231314944626e-06, 1.2652670080034332e-06, 1.179632603049161e-06, 1.0685871011465642e-06, 1.084333325512612e-06, 1.0542108490866808e-06, 1.0717663820851138e-06, 1.0512821423658136e-06], "duration": 196202.156408, "accuracy_train": [0.2807529411764706, 0.35056470588235294, 0.6610117647058823, 0.7482117647058824, 0.7784, 0.7948235294117647, 0.8162352941176471, 0.8269647058823529, 0.8339529411764706, 0.8416235294117647, 0.8458117647058824, 0.8502823529411765, 0.8584941176470589, 0.8642352941176471, 0.8669176470588236, 0.867035294117647, 0.8722588235294118, 0.8778588235294118, 0.8777647058823529, 0.8804470588235294, 0.8827058823529412, 0.8850823529411764, 0.8869882352941176, 0.8878117647058823, 0.8895294117647059, 0.8910823529411764, 0.8923764705882353, 0.8946352941176471, 0.8970117647058824, 0.8965411764705883, 0.8981411764705882, 0.9001176470588236, 0.9015058823529412, 0.9029411764705882, 0.9036941176470589, 0.9051058823529412, 0.9053411764705882, 0.9055764705882353, 0.9075058823529412, 0.9084235294117647, 0.9096235294117647, 0.9105411764705882, 0.911435294117647, 0.912, 0.9117411764705883, 0.9126588235294117, 0.9140235294117647, 0.9142588235294118, 0.9163764705882353, 0.916, 0.9163294117647058, 0.9176, 0.9174823529411764, 0.9187529411764706, 0.9193176470588236, 0.92, 0.9199058823529411, 0.9215764705882353, 0.9219529411764706, 0.9218823529411765, 0.9218588235294117, 0.9229176470588235, 0.9225882352941176, 0.924, 0.9252705882352941, 0.9245411764705882, 0.9251529411764706, 0.9252235294117647, 0.9263294117647058, 0.9261176470588235, 0.9270117647058823, 0.9265176470588236, 0.9279764705882353, 0.9284, 0.9290588235294117, 0.9288705882352941, 0.9296235294117647, 0.9303058823529412, 0.9308470588235294, 0.9299529411764705, 0.9302117647058824, 0.9312705882352941, 0.9315294117647058, 0.9319058823529411, 0.9320470588235295, 0.9320705882352941, 0.9328470588235294, 0.9337176470588235, 0.9328235294117647, 0.9340470588235295, 0.9342823529411765, 0.9345176470588236, 0.935364705882353, 0.9354588235294118, 0.9348705882352941, 0.9357882352941177, 0.9355294117647058, 0.9352, 0.9368235294117647, 0.9372470588235294, 0.9370588235294117, 0.9372941176470588, 0.9378352941176471, 0.9374117647058824, 0.9383529411764706, 0.938, 0.9379529411764705, 0.9382352941176471, 0.9388470588235294, 0.9396705882352941, 0.9392705882352941, 0.9394823529411764, 0.9391529411764706, 0.9403764705882353, 0.9396705882352941, 0.9412941176470588, 0.9408470588235294, 0.9416941176470588, 0.941764705882353, 0.9412, 0.9422117647058823, 0.9425411764705882, 0.9427294117647059, 0.9423764705882353, 0.9424705882352942, 0.9432, 0.9431529411764706, 0.9434588235294118, 0.9433176470588235, 0.9436470588235294, 0.9436941176470588, 0.9428941176470588, 0.9443058823529412, 0.9443058823529412, 0.9441647058823529, 0.9443058823529412, 0.944564705882353, 0.9450117647058823, 0.9452235294117647, 0.9454823529411764, 0.9448235294117647, 0.9455529411764706, 0.945364705882353, 0.9460235294117647, 0.9452705882352941, 0.9464705882352941, 0.946635294117647, 0.9465411764705882, 0.9458117647058824, 0.9467294117647059, 0.9472705882352941, 0.9467529411764706, 0.9472470588235294, 0.9467294117647059, 0.9476941176470588, 0.9482823529411765, 0.9476470588235294, 0.9485176470588236, 0.9477882352941176, 0.948635294117647, 0.9491529411764706, 0.9485176470588236, 0.9488941176470588, 0.9480705882352941, 0.9495529411764706, 0.9492235294117647, 0.9486588235294118, 0.9492705882352941, 0.9499058823529412, 0.9495058823529412, 0.9498588235294118, 0.9497411764705882, 0.9505176470588236, 0.9496941176470588, 0.9504235294117647, 0.9501176470588235, 0.9499529411764706, 0.9506823529411764, 0.9502352941176471, 0.9513411764705882, 0.9510588235294117, 0.9518117647058824, 0.9520941176470589, 0.9516235294117648, 0.9513647058823529, 0.9518823529411765, 0.9515529411764706, 0.9517411764705882, 0.9520941176470589, 0.9525882352941176, 0.9520941176470589, 0.9524470588235294, 0.952, 0.9535529411764706, 0.9526588235294118, 0.9526588235294118, 0.9531764705882353, 0.9530117647058823, 0.9536235294117648, 0.9535294117647058, 0.9541647058823529, 0.9534588235294118, 0.9539058823529412, 0.9539058823529412, 0.9547058823529412, 0.9542588235294117, 0.9541647058823529, 0.9548941176470588, 0.9550117647058823, 0.9549882352941177, 0.9541411764705883, 0.9553882352941176, 0.9552705882352941, 0.9553411764705882, 0.9552941176470588, 0.9553176470588235, 0.955764705882353, 0.9557411764705882, 0.9552941176470588, 0.9559529411764706, 0.955835294117647, 0.9557411764705882, 0.9559529411764706, 0.9566588235294118, 0.9563529411764706, 0.9562588235294117, 0.9572705882352941, 0.9568941176470588, 0.9567058823529412, 0.9566823529411764, 0.9572, 0.9574117647058824, 0.9573647058823529, 0.9577882352941176, 0.9570352941176471, 0.9574823529411765, 0.9579764705882353, 0.9579764705882353, 0.9575529411764706, 0.9572235294117647, 0.9581411764705883, 0.9580235294117647, 0.9583529411764706, 0.9582823529411765], "end": "2016-02-06 07:18:52.485000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0, 211.0, 212.0, 213.0, 214.0, 215.0, 216.0, 217.0, 218.0, 219.0, 220.0, 221.0, 222.0, 223.0, 224.0, 225.0, 226.0, 227.0, 228.0, 229.0, 230.0, 231.0, 232.0, 233.0, 234.0, 235.0, 236.0, 237.0, 238.0, 239.0, 240.0, 241.0, 242.0, 243.0], "moving_var_accuracy_valid": [0.007062721599999999, 0.015873622575999998, 0.04547396159855999, 0.07499647473643359, 0.0979129978793912, 0.11354347866523487, 0.12392116286180505, 0.12980703046147823, 0.13191512940755315, 0.131239147968327, 0.12821257946666537, 0.12377400529326785, 0.1182063693317278, 0.11228230870465619, 0.10584136868230948, 0.09919494817801049, 0.09259737377487282, 0.0860616927288189, 0.07966200908439805, 0.07358944341506321, 0.06777375745581704, 0.062283461530348076, 0.05706180072975279, 0.052222067827729646, 0.04769323143671363, 0.04348933650788682, 0.039617426068853275, 0.03605315845746285, 0.032771234405863985, 0.029750904786469388, 0.026994330965424723, 0.02448711052895177, 0.02218330498090036, 0.02008408776624855, 0.018172160749208664, 0.016437395825122537, 0.01486577430547152, 0.01342936593251294, 0.012129656160192721, 0.010956094281668235, 0.00989018207460188, 0.00893279958854887, 0.008063159720954846, 0.007275992222780662, 0.0065626686060636895, 0.005919078738445449, 0.005341950497862406, 0.004817365361116541, 0.004346321642449319, 0.003917755791240822, 0.003530718190494236, 0.0031850394455430683, 0.002870249287541714, 0.0025892484717538566, 0.002334856121495343, 0.0021067346874847656, 0.0018987654759489167, 0.0017110793766962535, 0.0015413450574802909, 0.0013883231826797287, 0.001251002121346475, 0.0011266525559456267, 0.0010161181962329159, 0.0009157468890901811, 0.0008246940686788975, 0.000743165880380304, 0.0006690731611615058, 0.0006028478451886412, 0.0005436935605936285, 0.0004894817537753165, 0.00044104800050788446, 0.00039730984164379186, 0.00035839780669276347, 0.0003232213748863013, 0.00029143654997655064, 0.00026258418669372103, 0.00023725842761999845, 0.0002139284381424863, 0.00019270063489587963, 0.00017362915453283853, 0.00015639660629201702, 0.00014095493292901943, 0.00012743774184683873, 0.00011479921318025274, 0.00010356133036817991, 9.343827065571614e-05, 8.419335321963297e-05, 7.587837862652685e-05, 6.831969434602533e-05, 6.175158155516775e-05, 5.5790147281084375e-05, 5.02767056897441e-05, 4.5938707685827954e-05, 4.1465536700376704e-05, 3.732660835956653e-05, 3.3909127793241196e-05, 3.113486126004528e-05, 2.8051393663548692e-05, 2.528789775754219e-05, 2.3099767636117083e-05, 2.1065725192511943e-05, 1.941176549530204e-05, 1.7497975406410016e-05, 1.58005912502848e-05, 1.4385405257361562e-05, 1.3052776676314113e-05, 1.1765160631541005e-05, 1.0603138962243745e-05, 9.551224165537202e-06, 8.7274949066151e-06, 7.870680374835231e-06, 7.181963802965829e-06, 6.985411524678245e-06, 6.286970860837116e-06, 5.7019639551037094e-06, 5.153707024455805e-06, 4.8816888964412065e-06, 4.3948176626153175e-06, 3.983418721118132e-06, 3.770683908740416e-06, 3.7390529323277825e-06, 3.3774763401076177e-06, 3.1841809870903442e-06, 2.957004283498055e-06, 2.69811247071442e-06, 2.463393698614075e-06, 2.270601737116751e-06, 2.0560267615609864e-06, 1.8539567870897254e-06, 1.7797786103516961e-06, 1.6694754384493411e-06, 1.5025623604982312e-06, 1.3654659563591e-06, 1.2982171220148724e-06, 1.3308687256611486e-06, 1.2540590674179594e-06, 1.163433407185981e-06, 1.144530706999422e-06, 1.0300785186794154e-06, 1.121415347377109e-06, 1.0229069143722272e-06, 1.0075912059488644e-06, 9.24942868045968e-07, 1.2434817395693029e-06, 1.1526200733702464e-06, 1.226849491611022e-06, 1.1051742258492206e-06, 1.016738158213345e-06, 1.258767278364414e-06, 1.1609922613569678e-06, 1.0571856542618912e-06, 9.614241102586028e-07, 1.0503482429092935e-06, 9.6697367867717e-07, 9.80803851696492e-07, 8.925663174298761e-07, 8.565367765280852e-07, 8.684525093524051e-07, 7.881885115935654e-07, 7.147004755070928e-07, 7.366897583347972e-07, 6.686668124807971e-07, 6.045431605974975e-07, 7.314596160301998e-07, 7.110193040239646e-07, 6.410316381236423e-07, 6.858003993142563e-07, 7.593204927865286e-07, 9.326545250371313e-07, 9.231061224470804e-07, 8.633415063761607e-07, 9.72693777546577e-07, 8.771776539447304e-07, 8.481765979889128e-07, 7.82426068201795e-07, 7.424915800104002e-07, 6.867798269752594e-07, 7.495348663665603e-07, 7.121075656922855e-07, 6.497978052505205e-07, 5.868349850627896e-07, 7.589544050364374e-07, 7.062778886995216e-07, 6.69087947465395e-07, 6.176978759095366e-07, 8.784194731696769e-07, 8.071940443727028e-07, 7.322528544921579e-07, 6.763426783337671e-07, 6.670653451161491e-07, 7.142074364703686e-07, 6.738778479972426e-07, 6.126828336549944e-07, 9.493544053304152e-07, 8.707370517657716e-07, 8.437040075354341e-07, 7.915435316465091e-07, 7.271572885673671e-07, 6.76753465029245e-07, 6.120412367655662e-07, 6.242829486149857e-07, 5.637825687939246e-07, 6.094955698841643e-07, 5.717822885382865e-07, 5.251737405812228e-07, 4.734110741176412e-07, 4.2668127985745546e-07, 3.935677700092747e-07, 3.5427456100890336e-07, 3.422023841448987e-07, 4.2525969118992964e-07, 4.0470060310984777e-07, 3.990534410304623e-07, 4.206263325277722e-07, 3.7884751242281514e-07, 3.619407844433526e-07, 3.617122625383465e-07, 3.4261863434051963e-07, 3.2333531053124193e-07, 3.2715810213400757e-07, 3.237289132718817e-07, 2.934701814805807e-07, 2.7885675120347374e-07, 2.629052822585996e-07, 2.404192488269182e-07, 2.1697696022109075e-07, 2.61823283778461e-07, 2.3570234030939542e-07, 2.2827415612679708e-07, 2.127833087829296e-07, 1.9151345536137878e-07, 1.7834970512209934e-07, 3.1233530960516166e-07, 2.8353449373539654e-07, 2.793954213803039e-07, 2.614656266241278e-07, 3.081953855159779e-07, 2.773846117183585e-07, 2.907190417759768e-07, 2.9491617949424426e-07, 2.711765358715325e-07, 2.487179814890151e-07, 2.716182260763237e-07, 2.4633031082010087e-07], "accuracy_test": 0.8407, "start": "2016-02-04 00:48:50.329000", "learning_rate_per_epoch": [0.0005673099658451974, 0.0002836549829225987, 0.00018910331709776074, 0.00014182749146129936, 0.00011346199607942253, 9.455165854888037e-05, 8.104427979560569e-05, 7.091374573064968e-05, 6.303444388322532e-05, 5.673099803971127e-05, 5.157363193575293e-05, 4.7275829274440184e-05, 4.363922926131636e-05, 4.0522139897802845e-05, 3.782066414714791e-05, 3.545687286532484e-05, 3.3371175959473476e-05, 3.151722194161266e-05, 2.9858420020900667e-05, 2.8365499019855633e-05, 2.7014759325538762e-05, 2.5786815967876464e-05, 2.4665650926181115e-05, 2.3637914637220092e-05, 2.2692398488288745e-05, 2.181961463065818e-05, 2.1011479475419037e-05, 2.0261069948901422e-05, 1.956241248990409e-05, 1.8910332073573954e-05, 1.8300321244169027e-05, 1.772843643266242e-05, 1.7191210645250976e-05, 1.6685587979736738e-05, 1.620885632291902e-05, 1.575861097080633e-05, 1.5332701877923682e-05, 1.4929210010450333e-05, 1.4546409147442318e-05, 1.4182749509927817e-05, 1.3836828657076694e-05, 1.3507379662769381e-05, 1.319325474469224e-05, 1.2893407983938232e-05, 1.2606888049049303e-05, 1.2332825463090558e-05, 1.2070424418197945e-05, 1.1818957318610046e-05, 1.1577754776226357e-05, 1.1346199244144373e-05, 1.1123725016659591e-05, 1.090980731532909e-05, 1.0703961379476823e-05, 1.0505739737709519e-05, 1.0314726750948466e-05, 1.0130534974450711e-05, 9.952806067303754e-06, 9.781206244952045e-06, 9.615423550712876e-06, 9.455166036786977e-06, 9.300163583247922e-06, 9.150160622084513e-06, 9.004919775179587e-06, 8.86421821633121e-06, 8.727845852263272e-06, 8.595605322625488e-06, 8.4673129094881e-06, 8.342793989868369e-06, 8.221883945225272e-06, 8.10442816145951e-06, 7.990281119418796e-06, 7.879305485403165e-06, 7.77136938268086e-06, 7.666350938961841e-06, 7.564132829429582e-06, 7.464605005225167e-06, 7.36766196496319e-06, 7.273204573721159e-06, 7.18113869879744e-06, 7.091374754963908e-06, 7.003826794971246e-06, 6.918414328538347e-06, 6.835059593868209e-06, 6.7536898313846905e-06, 6.674235009995755e-06, 6.59662737234612e-06, 6.520804163301364e-06, 6.446703991969116e-06, 6.374269105435815e-06, 6.3034440245246515e-06, 6.234175543795573e-06, 6.166412731545279e-06, 6.100106929807225e-06, 6.035212209098972e-06, 5.971684004180133e-06, 5.909478659305023e-06, 5.8485561567067634e-06, 5.788877388113178e-06, 5.730403699999442e-06, 5.673099622072186e-06, 5.616930138785392e-06, 5.561862508329796e-06, 5.5078635341487825e-06, 5.454903657664545e-06, 5.4029519560572226e-06, 5.3519806897384115e-06, 5.3019621191197075e-06, 5.252869868854759e-06, 5.204678473091917e-06, 5.157363375474233e-06, 5.1109004743921105e-06, 5.065267487225356e-06, 5.020442131353775e-06, 4.976403033651877e-06, 4.933130185236223e-06, 4.890603122476023e-06, 4.84880320072989e-06, 4.807711775356438e-06, 4.767310656461632e-06, 4.727583018393489e-06, 4.688512035500025e-06, 4.650081791623961e-06, 4.612275915860664e-06, 4.575080311042257e-06, 4.538479515758809e-06, 4.502459887589794e-06, 4.467007784114685e-06, 4.432109108165605e-06, 4.39775158156408e-06, 4.363922926131636e-06, 4.330610408942448e-06, 4.297802661312744e-06, 4.265488314558752e-06, 4.23365645474405e-06, 4.202296167932218e-06, 4.1713969949341845e-06, 4.1409484765608795e-06, 4.110941972612636e-06, 4.081366569153033e-06, 4.052214080729755e-06, 4.023474957648432e-06, 3.995140559709398e-06, 3.967202701460337e-06, 3.939652742701583e-06, 3.912482497980818e-06, 3.88568469134043e-06, 3.859251592075452e-06, 3.833175469480921e-06, 3.8074495023465715e-06, 3.782066414714791e-06, 3.7570196127489908e-06, 3.7323025026125833e-06, 3.7079082630953053e-06, 3.683830982481595e-06, 3.6600642943085404e-06, 3.6366022868605796e-06, 3.6134392757958267e-06, 3.59056934939872e-06, 3.5679872780747246e-06, 3.545687377481954e-06, 3.5236644180258736e-06, 3.501913397485623e-06, 3.480429313640343e-06, 3.4592071642691735e-06, 3.4382421745249303e-06, 3.4175297969341045e-06, 3.3970657113968628e-06, 3.3768449156923452e-06, 3.356863771841745e-06, 3.3371175049978774e-06, 3.317602022434585e-06, 3.29831368617306e-06, 3.279248403487145e-06, 3.260402081650682e-06, 3.241771310058539e-06, 3.223351995984558e-06, 3.2051409561972832e-06, 3.1871345527179074e-06, 3.1693293749412987e-06, 3.1517220122623257e-06, 3.134309281449532e-06, 3.1170877718977863e-06, 3.1000545277493075e-06, 3.0832063657726394e-06, 3.0665403301100014e-06, 3.0500534649036126e-06, 3.033743041669368e-06, 3.017606104549486e-06, 3.0016399250598624e-06, 2.9858420020900667e-06, 2.9702091524086427e-06, 2.9547393296525115e-06, 2.939429805337568e-06, 2.9242780783533817e-06, 2.909281874963199e-06, 2.894438694056589e-06, 2.879746034523123e-06, 2.865201849999721e-06, 2.8508038667496294e-06, 2.836549811036093e-06, 2.8224376364960335e-06, 2.808465069392696e-06, 2.794630290736677e-06, 2.780931254164898e-06, 2.7673656859406037e-06, 2.7539317670743912e-06, 2.7406279059505323e-06, 2.7274518288322724e-06, 2.714401716730208e-06, 2.7014759780286113e-06, 2.6886727937380783e-06, 2.6759903448692057e-06, 2.6634270398062654e-06, 2.6509810595598537e-06, 2.638651039887918e-06, 2.6264349344273796e-06, 2.614331606309861e-06, 2.6023392365459586e-06, 2.59045646089362e-06, 2.5786816877371166e-06, 2.567013325460721e-06, 2.5554502371960552e-06, 2.543990831327392e-06, 2.532633743612678e-06, 2.5213776098098606e-06, 2.5102210656768875e-06, 2.499162746971706e-06, 2.4882015168259386e-06, 2.477336010997533e-06, 2.4665650926181115e-06, 2.455887397445622e-06, 2.4453015612380113e-06, 2.434806674500578e-06, 2.424401600364945e-06, 2.414084974589059e-06, 2.403855887678219e-06, 2.393712975390372e-06, 2.383655328230816e-06, 2.373681809331174e-06, 2.3637915091967443e-06, 2.3539832909591496e-06, 2.3442560177500127e-06, 2.3346090074483072e-06, 2.3250408958119806e-06], "accuracy_train_first": 0.2807529411764706, "accuracy_train_last": 0.9582823529411765, "batch_size_eval": 1024, "accuracy_train_std": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "accuracy_test_std": 0, "error_valid": [0.7198666666666667, 0.6468, 0.3508, 0.2653333333333333, 0.23773333333333335, 0.2294666666666667, 0.21640000000000004, 0.20799999999999996, 0.2041333333333334, 0.19973333333333332, 0.20040000000000002, 0.19666666666666666, 0.1962666666666667, 0.18786666666666663, 0.1876, 0.18600000000000005, 0.18213333333333337, 0.1810666666666667, 0.1810666666666667, 0.1769333333333334, 0.17653333333333332, 0.17479999999999996, 0.17666666666666664, 0.1737333333333333, 0.17426666666666668, 0.17400000000000004, 0.17253333333333332, 0.17159999999999997, 0.17146666666666666, 0.17200000000000004, 0.17079999999999995, 0.16893333333333338, 0.1704, 0.17013333333333336, 0.17013333333333336, 0.16933333333333334, 0.16826666666666668, 0.17013333333333336, 0.16946666666666665, 0.16826666666666668, 0.16893333333333338, 0.1665333333333333, 0.16720000000000002, 0.16720000000000002, 0.1677333333333333, 0.16720000000000002, 0.1650666666666667, 0.16626666666666667, 0.16466666666666663, 0.16626666666666667, 0.1664, 0.16386666666666672, 0.16559999999999997, 0.1632, 0.16346666666666665, 0.16213333333333335, 0.16359999999999997, 0.16359999999999997, 0.16413333333333335, 0.16413333333333335, 0.1632, 0.16400000000000003, 0.16173333333333328, 0.1624, 0.16333333333333333, 0.16226666666666667, 0.16359999999999997, 0.16226666666666667, 0.1612, 0.1630666666666667, 0.16186666666666671, 0.16200000000000003, 0.16080000000000005, 0.16080000000000005, 0.16080000000000005, 0.1612, 0.15959999999999996, 0.1604, 0.16093333333333337, 0.16066666666666662, 0.16080000000000005, 0.1604, 0.1592, 0.1604, 0.15973333333333328, 0.15959999999999996, 0.16000000000000003, 0.1598666666666667, 0.16026666666666667, 0.1590666666666667, 0.1590666666666667, 0.15959999999999996, 0.15759999999999996, 0.15893333333333337, 0.16026666666666667, 0.15813333333333335, 0.1572, 0.16013333333333335, 0.15893333333333337, 0.15759999999999996, 0.15759999999999996, 0.15693333333333337, 0.15839999999999999, 0.15813333333333335, 0.15746666666666664, 0.15759999999999996, 0.15813333333333335, 0.15893333333333337, 0.15826666666666667, 0.15733333333333333, 0.15800000000000003, 0.15733333333333333, 0.1558666666666667, 0.15800000000000003, 0.15733333333333333, 0.15746666666666664, 0.15626666666666666, 0.1578666666666667, 0.1572, 0.15626666666666666, 0.15559999999999996, 0.15773333333333328, 0.15613333333333335, 0.15626666666666666, 0.1565333333333333, 0.15773333333333328, 0.15639999999999998, 0.15746666666666664, 0.15693333333333337, 0.15600000000000003, 0.15613333333333335, 0.15693333333333337, 0.1565333333333333, 0.15600000000000003, 0.15813333333333335, 0.15613333333333335, 0.15746666666666664, 0.1558666666666667, 0.15680000000000005, 0.15533333333333332, 0.15626666666666666, 0.15759999999999996, 0.15626666666666666, 0.1545333333333333, 0.1570666666666667, 0.15506666666666669, 0.15626666666666666, 0.1558666666666667, 0.15826666666666667, 0.1570666666666667, 0.15693333333333337, 0.15693333333333337, 0.1552, 0.15600000000000003, 0.15533333333333332, 0.15600000000000003, 0.1570666666666667, 0.15533333333333332, 0.15600000000000003, 0.15600000000000003, 0.1552, 0.1558666666666667, 0.15626666666666666, 0.15466666666666662, 0.1552, 0.15600000000000003, 0.15480000000000005, 0.1545333333333333, 0.15400000000000003, 0.1545333333333333, 0.15480000000000005, 0.1538666666666667, 0.15533333333333332, 0.15439999999999998, 0.15466666666666662, 0.15573333333333328, 0.15559999999999996, 0.15639999999999998, 0.15466666666666662, 0.15493333333333337, 0.15506666666666669, 0.15359999999999996, 0.1545333333333333, 0.15559999999999996, 0.15546666666666664, 0.1532, 0.15533333333333332, 0.1552, 0.1545333333333333, 0.15573333333333328, 0.15613333333333335, 0.1545333333333333, 0.15480000000000005, 0.15293333333333337, 0.15439999999999998, 0.15559999999999996, 0.15426666666666666, 0.15439999999999998, 0.15426666666666666, 0.1545333333333333, 0.15559999999999996, 0.15493333333333337, 0.1558666666666667, 0.15439999999999998, 0.1552, 0.15480000000000005, 0.15480000000000005, 0.1552, 0.15493333333333337, 0.15439999999999998, 0.15600000000000003, 0.15546666666666664, 0.15439999999999998, 0.15413333333333334, 0.15493333333333337, 0.15439999999999998, 0.15546666666666664, 0.15533333333333332, 0.1545333333333333, 0.15426666666666666, 0.15426666666666666, 0.15493333333333337, 0.1552, 0.1552, 0.15466666666666662, 0.15493333333333337, 0.15400000000000003, 0.15480000000000005, 0.1552, 0.1545333333333333, 0.15480000000000005, 0.1545333333333333, 0.15346666666666664, 0.15480000000000005, 0.15413333333333334, 0.15426666666666666, 0.15546666666666664, 0.15466666666666662, 0.15533333333333332, 0.15533333333333332, 0.1545333333333333, 0.1545333333333333, 0.15546666666666664, 0.15466666666666662], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.03820602494498997, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 128, "valid_ratio": 0.15, "learning_rate": 0.0005673099633497458, "optimization": "rmsprop", "nb_data_augmentation": 3, "learning_rate_decay_method": "lin", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 1.0684867195444023e-05, "rotation_range": [0, 0], "momentum": 0.856655704625862}, "accuracy_valid_max": 0.8470666666666666, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        y_pred = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            y_pred.extend((nnet.predict(X[mini_batch]) == y[mini_batch]).tolist())\n        return np.mean(y_pred)\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            acc = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = acc\n            status[\"accuracy_train_std\"] = 0\n            acc = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = acc\n            status[\"accuracy_valid_std\"] = 0\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n\n    # rescaling to [-1, 1]\n    X_min = X_train.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X_train.max(axis=(0, 2, 3))[None, :, None, None]\n    def preprocess(a):\n        return (a / 255.) * 2 - 1\n        # return 2 * ((a - X_min) / (X_max - X_min)) - 1\n    X_train = preprocess(X_train)\n    X_valid = preprocess(X_valid)\n\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = preprocess(X_test)\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    acc = evaluate(X_test, y_test, batch_size_eval)\n    light.set(\"accuracy_test\", acc)\n    light.set(\"accuracy_test_std\", 0)\n    print(\"Test accuracy : {}+-{}\".format(acc, 0))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8453333333333334, "accuracy_valid_std": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "accuracy_valid": [0.28013333333333335, 0.3532, 0.6492, 0.7346666666666667, 0.7622666666666666, 0.7705333333333333, 0.7836, 0.792, 0.7958666666666666, 0.8002666666666667, 0.7996, 0.8033333333333333, 0.8037333333333333, 0.8121333333333334, 0.8124, 0.814, 0.8178666666666666, 0.8189333333333333, 0.8189333333333333, 0.8230666666666666, 0.8234666666666667, 0.8252, 0.8233333333333334, 0.8262666666666667, 0.8257333333333333, 0.826, 0.8274666666666667, 0.8284, 0.8285333333333333, 0.828, 0.8292, 0.8310666666666666, 0.8296, 0.8298666666666666, 0.8298666666666666, 0.8306666666666667, 0.8317333333333333, 0.8298666666666666, 0.8305333333333333, 0.8317333333333333, 0.8310666666666666, 0.8334666666666667, 0.8328, 0.8328, 0.8322666666666667, 0.8328, 0.8349333333333333, 0.8337333333333333, 0.8353333333333334, 0.8337333333333333, 0.8336, 0.8361333333333333, 0.8344, 0.8368, 0.8365333333333334, 0.8378666666666666, 0.8364, 0.8364, 0.8358666666666666, 0.8358666666666666, 0.8368, 0.836, 0.8382666666666667, 0.8376, 0.8366666666666667, 0.8377333333333333, 0.8364, 0.8377333333333333, 0.8388, 0.8369333333333333, 0.8381333333333333, 0.838, 0.8392, 0.8392, 0.8392, 0.8388, 0.8404, 0.8396, 0.8390666666666666, 0.8393333333333334, 0.8392, 0.8396, 0.8408, 0.8396, 0.8402666666666667, 0.8404, 0.84, 0.8401333333333333, 0.8397333333333333, 0.8409333333333333, 0.8409333333333333, 0.8404, 0.8424, 0.8410666666666666, 0.8397333333333333, 0.8418666666666667, 0.8428, 0.8398666666666667, 0.8410666666666666, 0.8424, 0.8424, 0.8430666666666666, 0.8416, 0.8418666666666667, 0.8425333333333334, 0.8424, 0.8418666666666667, 0.8410666666666666, 0.8417333333333333, 0.8426666666666667, 0.842, 0.8426666666666667, 0.8441333333333333, 0.842, 0.8426666666666667, 0.8425333333333334, 0.8437333333333333, 0.8421333333333333, 0.8428, 0.8437333333333333, 0.8444, 0.8422666666666667, 0.8438666666666667, 0.8437333333333333, 0.8434666666666667, 0.8422666666666667, 0.8436, 0.8425333333333334, 0.8430666666666666, 0.844, 0.8438666666666667, 0.8430666666666666, 0.8434666666666667, 0.844, 0.8418666666666667, 0.8438666666666667, 0.8425333333333334, 0.8441333333333333, 0.8432, 0.8446666666666667, 0.8437333333333333, 0.8424, 0.8437333333333333, 0.8454666666666667, 0.8429333333333333, 0.8449333333333333, 0.8437333333333333, 0.8441333333333333, 0.8417333333333333, 0.8429333333333333, 0.8430666666666666, 0.8430666666666666, 0.8448, 0.844, 0.8446666666666667, 0.844, 0.8429333333333333, 0.8446666666666667, 0.844, 0.844, 0.8448, 0.8441333333333333, 0.8437333333333333, 0.8453333333333334, 0.8448, 0.844, 0.8452, 0.8454666666666667, 0.846, 0.8454666666666667, 0.8452, 0.8461333333333333, 0.8446666666666667, 0.8456, 0.8453333333333334, 0.8442666666666667, 0.8444, 0.8436, 0.8453333333333334, 0.8450666666666666, 0.8449333333333333, 0.8464, 0.8454666666666667, 0.8444, 0.8445333333333334, 0.8468, 0.8446666666666667, 0.8448, 0.8454666666666667, 0.8442666666666667, 0.8438666666666667, 0.8454666666666667, 0.8452, 0.8470666666666666, 0.8456, 0.8444, 0.8457333333333333, 0.8456, 0.8457333333333333, 0.8454666666666667, 0.8444, 0.8450666666666666, 0.8441333333333333, 0.8456, 0.8448, 0.8452, 0.8452, 0.8448, 0.8450666666666666, 0.8456, 0.844, 0.8445333333333334, 0.8456, 0.8458666666666667, 0.8450666666666666, 0.8456, 0.8445333333333334, 0.8446666666666667, 0.8454666666666667, 0.8457333333333333, 0.8457333333333333, 0.8450666666666666, 0.8448, 0.8448, 0.8453333333333334, 0.8450666666666666, 0.846, 0.8452, 0.8448, 0.8454666666666667, 0.8452, 0.8454666666666667, 0.8465333333333334, 0.8452, 0.8458666666666667, 0.8457333333333333, 0.8445333333333334, 0.8453333333333334, 0.8446666666666667, 0.8446666666666667, 0.8454666666666667, 0.8454666666666667, 0.8445333333333334, 0.8453333333333334], "seed": 489668217, "model": "residualv3", "loss_std": [0.39024072885513306, 0.14676834642887115, 0.11089233309030533, 0.09340031445026398, 0.0893016904592514, 0.08716873079538345, 0.08436784893274307, 0.08307191729545593, 0.0808653011918068, 0.08098185062408447, 0.07943525165319443, 0.07664244621992111, 0.07584269344806671, 0.07653947174549103, 0.07417849451303482, 0.07582638412714005, 0.07458792626857758, 0.07358817756175995, 0.07175188511610031, 0.07124239206314087, 0.07404923439025879, 0.07230041921138763, 0.07128696888685226, 0.07191061973571777, 0.06963256746530533, 0.06920219957828522, 0.07118911296129227, 0.06743662804365158, 0.06757485121488571, 0.06824004650115967, 0.06698843091726303, 0.0699111595749855, 0.06826349347829819, 0.06902757287025452, 0.06826448440551758, 0.06739947199821472, 0.06701504439115524, 0.06688141822814941, 0.06648743897676468, 0.065937340259552, 0.06632677465677261, 0.0669713094830513, 0.0658373236656189, 0.06424734741449356, 0.06552738696336746, 0.06651071459054947, 0.0657280907034874, 0.06609110534191132, 0.06487392634153366, 0.06451761722564697, 0.06493046134710312, 0.06331189721822739, 0.06424058228731155, 0.06254085153341293, 0.06327402591705322, 0.06267037242650986, 0.06369943916797638, 0.06260625272989273, 0.06265585124492645, 0.06141391396522522, 0.06202557310461998, 0.06316513568162918, 0.061712153255939484, 0.0628623515367508, 0.06204862892627716, 0.062191739678382874, 0.06321921199560165, 0.06305190920829773, 0.063023142516613, 0.060035910457372665, 0.06122962012887001, 0.061354123055934906, 0.0609501376748085, 0.06027062609791756, 0.06208237260580063, 0.05910643935203552, 0.06186789274215698, 0.06029830127954483, 0.06126009672880173, 0.05766317620873451, 0.06050487980246544, 0.059114400297403336, 0.058827463537454605, 0.0592140331864357, 0.06099388375878334, 0.05879063159227371, 0.05930119752883911, 0.060070011764764786, 0.05859587341547012, 0.06008194386959076, 0.05851499363780022, 0.060379862785339355, 0.060721445828676224, 0.0574771873652935, 0.0602269247174263, 0.058939024806022644, 0.058431386947631836, 0.05802857130765915, 0.05824446678161621, 0.05742829665541649, 0.059638965874910355, 0.058096155524253845, 0.05788480490446091, 0.058493662625551224, 0.05911509320139885, 0.05772435665130615, 0.05704330652952194, 0.05888789892196655, 0.058568324893713, 0.05779394134879112, 0.05852508544921875, 0.05675329267978668, 0.05533599480986595, 0.058724015951156616, 0.057751331478357315, 0.05878238007426262, 0.05699947848916054, 0.0564752034842968, 0.0557459257543087, 0.05744391679763794, 0.05707766115665436, 0.05636957660317421, 0.05726335570216179, 0.05719694495201111, 0.05668503791093826, 0.056638363748788834, 0.055541783571243286, 0.05782516300678253, 0.055205173790454865, 0.05781332775950432, 0.05586513131856918, 0.0576593354344368, 0.056408919394016266, 0.05688744783401489, 0.056161221116781235, 0.057807981967926025, 0.056272029876708984, 0.05437665060162544, 0.05545790120959282, 0.05624395236372948, 0.05560972914099693, 0.05749044194817543, 0.05591193586587906, 0.0559457428753376, 0.054809607565402985, 0.054463572800159454, 0.05586547404527664, 0.05400553345680237, 0.056531406939029694, 0.05581783875823021, 0.054584652185440063, 0.05584236606955528, 0.05360395461320877, 0.054799046367406845, 0.05540107563138008, 0.05485447123646736, 0.05396204814314842, 0.05462460219860077, 0.05278940871357918, 0.0549212284386158, 0.05557077378034592, 0.054205019026994705, 0.054193295538425446, 0.055242881178855896, 0.05601758882403374, 0.053681161254644394, 0.05478191003203392, 0.05435943976044655, 0.0548207089304924, 0.05448342114686966, 0.05424016714096069, 0.054676756262779236, 0.053001053631305695, 0.05527020990848541, 0.051418207585811615, 0.05513893812894821, 0.054828014224767685, 0.05382797122001648, 0.05320701748132706, 0.05223960801959038, 0.05449676141142845, 0.053950730711221695, 0.05211549252271652, 0.05286521092057228, 0.05239373445510864, 0.054129742085933685, 0.05379977077245712, 0.053821224719285965, 0.052681632339954376, 0.05516376718878746, 0.05305497720837593, 0.05287794768810272, 0.0534411184489727, 0.054649244993925095, 0.05286776274442673, 0.05334887653589249, 0.05174859240651131, 0.05330535024404526, 0.05281764268875122, 0.05348862335085869, 0.05367886275053024, 0.052959032356739044, 0.0527920201420784, 0.05177544429898262, 0.05281013250350952, 0.05288245156407356, 0.054194945842027664, 0.052224304527044296, 0.05280078947544098, 0.05098598450422287, 0.05224442109465599, 0.05375730246305466, 0.05330922082066536, 0.05305858328938484, 0.053732626140117645, 0.05246496573090553, 0.05265233665704727, 0.05227101221680641, 0.053195785731077194, 0.052199531346559525, 0.052562594413757324, 0.05140283331274986, 0.05276699364185333, 0.0507974699139595, 0.04986849054694176, 0.04972311109304428, 0.05247926339507103, 0.05270975083112717, 0.05231304094195366, 0.05323774367570877, 0.05186276510357857, 0.05117570981383324, 0.05239895358681679, 0.05334240570664406, 0.051759541034698486, 0.05155310779809952, 0.05123796686530113, 0.05097629129886627, 0.053018759936094284, 0.05083537474274635, 0.051185738295316696, 0.05307626351714134, 0.05127487704157829, 0.0519510842859745]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:42 2016", "state": "available"}], "summary": "56ab7689cb2587c077cde6b68bb9fced"}