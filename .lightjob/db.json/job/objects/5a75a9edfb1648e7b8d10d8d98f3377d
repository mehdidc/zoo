{"content": {"hp_model": {"f0": 64, "f1": 32, "f2": 32, "f3": 16, "nonlin": "rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.8196585178375244, 1.4661742448806763, 1.3461754322052002, 1.2641340494155884, 1.1979188919067383, 1.1415789127349854, 1.0926837921142578, 1.0480470657348633, 1.0101261138916016, 0.9756028056144714, 0.9441738724708557, 0.9150952100753784, 0.8886691331863403, 0.8663478493690491, 0.8450673222541809, 0.8246612548828125, 0.8068419694900513, 0.7904480695724487, 0.7726631164550781, 0.7596947550773621, 0.7463064789772034, 0.732795774936676, 0.7205115556716919, 0.7107768058776855, 0.7011418342590332, 0.6912932395935059, 0.6813322305679321, 0.6727339625358582, 0.6648743748664856, 0.656288206577301, 0.6483876705169678, 0.6409202814102173, 0.6335424184799194, 0.626891553401947, 0.6209360361099243, 0.6154518127441406, 0.6100599765777588, 0.6047929525375366, 0.5996407866477966, 0.5937922596931458, 0.5903181433677673, 0.5850205421447754, 0.5809137225151062, 0.5789819955825806, 0.5736346244812012, 0.5693545937538147, 0.5657304525375366, 0.5615102052688599, 0.5580384135246277, 0.5543646216392517, 0.5531653165817261, 0.550771176815033, 0.5456438064575195, 0.5442003011703491, 0.5407861471176147, 0.5372345447540283, 0.5362247824668884, 0.5341189503669739, 0.5310786366462708, 0.529015839099884, 0.5267301201820374, 0.5254144072532654, 0.5242501497268677, 0.5208899974822998, 0.5198214650154114, 0.5171169638633728, 0.5171730518341064, 0.5139049887657166, 0.511662483215332, 0.5116497278213501, 0.5108758807182312, 0.5078536868095398, 0.5065523982048035, 0.5065452456474304, 0.5025703310966492, 0.5022578239440918, 0.5024586319923401, 0.5012909770011902, 0.49921298027038574, 0.5000318884849548, 0.49736207723617554, 0.4961102306842804, 0.49489498138427734, 0.4931311011314392, 0.49452152848243713, 0.4925895035266876, 0.49060407280921936, 0.4915459454059601, 0.4893026053905487, 0.48928597569465637, 0.48965954780578613, 0.48860782384872437, 0.48559096455574036, 0.4872841238975525, 0.4839697778224945, 0.4853076934814453, 0.48426753282546997, 0.4847615361213684, 0.4833652079105377, 0.48226624727249146, 0.482584148645401, 0.48138850927352905, 0.4800868332386017, 0.48057806491851807, 0.4801918864250183, 0.48003196716308594, 0.48016858100891113, 0.47772490978240967, 0.47827479243278503, 0.4766443371772766, 0.4766518175601959, 0.47696515917778015, 0.47605687379837036, 0.4757384657859802, 0.4751821458339691, 0.472715824842453, 0.47572341561317444, 0.47515127062797546, 0.47447285056114197, 0.4742766320705414, 0.47449061274528503, 0.4739516079425812, 0.47263145446777344, 0.4735853672027588, 0.47231486439704895, 0.47084227204322815, 0.47206738591194153, 0.4717852473258972, 0.4724465310573578, 0.47185608744621277, 0.46943479776382446, 0.4716084599494934, 0.4704878628253937, 0.4698861837387085, 0.46920377016067505, 0.4705909788608551, 0.4695739150047302, 0.4701662063598633, 0.4682355523109436, 0.46975597739219666, 0.4670179784297943, 0.4712621569633484, 0.46892815828323364, 0.46880272030830383, 0.4678593575954437, 0.46849292516708374, 0.46848392486572266, 0.4691009819507599, 0.4672554135322571, 0.46945086121559143, 0.4676879048347473, 0.4681672155857086, 0.46700745820999146, 0.46724918484687805, 0.46686065196990967, 0.4666995406150818, 0.46758750081062317, 0.4668351411819458, 0.4677477478981018, 0.46779483556747437, 0.46698492765426636, 0.46757087111473083, 0.46712568402290344, 0.46605151891708374, 0.4649919867515564, 0.46816742420196533, 0.46601590514183044, 0.4653942584991455, 0.46758776903152466, 0.4666867256164551, 0.46631988883018494, 0.4661366045475006, 0.4642682373523712, 0.465473473072052, 0.4659772515296936, 0.4640451669692993, 0.4661473333835602, 0.4646628797054291, 0.4661315381526947, 0.4656332731246948, 0.4649345278739929, 0.46499523520469666, 0.4669520854949951, 0.46512511372566223], "moving_avg_accuracy_train": [0.03487764705882352, 0.0717075294117647, 0.11061677647058821, 0.1507362752941176, 0.19271441247058818, 0.23252297122352936, 0.2694730270423529, 0.3060269008087058, 0.34081009308077637, 0.37307261318446344, 0.4035371165718994, 0.4312257578558859, 0.45811729971735615, 0.48139968739267935, 0.5051279539475291, 0.5270339820821879, 0.5473870544622044, 0.5673777607806898, 0.5863246905849737, 0.6036098685852999, 0.6193288817267699, 0.6347254053187988, 0.6491611000810366, 0.66232028419058, 0.6751494322421102, 0.6867733125473109, 0.6975242165866975, 0.7078706184574396, 0.717775321317578, 0.7271624950681732, 0.7356438926201794, 0.7440724445346321, 0.7516110824341101, 0.7589464447789344, 0.765722388536335, 0.7719572085062308, 0.7780391347144313, 0.7832163977135764, 0.7880265226481011, 0.7926568115597616, 0.797393483344962, 0.8016870761869364, 0.8058077803329485, 0.8095728846525948, 0.8130697138343941, 0.8164309777450723, 0.8195643505588004, 0.822480856679391, 0.8252304180702754, 0.827933258616189, 0.8304999327545701, 0.8328664100673484, 0.8350032984723783, 0.8370888509780817, 0.83898231882145, 0.8409782045863639, 0.8427062664806687, 0.8445744633620136, 0.8462040758493417, 0.8478707270879369, 0.8494436543791432, 0.8508569360000524, 0.8521100659294589, 0.8534849416894542, 0.8547670357558029, 0.8560456262978697, 0.8571187107269063, 0.8582421337718626, 0.8591873321593823, 0.8601980107081498, 0.861119386107923, 0.8619674474971308, 0.8627730556885942, 0.8636110442373819, 0.864431116284232, 0.8652232987734558, 0.8659668512490515, 0.8666760484770875, 0.8672719730411433, 0.8679188933840879, 0.8684540628692086, 0.8690368918764053, 0.8696108497475884, 0.8700944706551825, 0.8706897294720172, 0.8712042859365802, 0.871733269107628, 0.8722070010203947, 0.8726192420948259, 0.8728796708265198, 0.8733211155085737, 0.8737372392518339, 0.8741164565031211, 0.8745118696763383, 0.874863035649881, 0.875207320320187, 0.8755312941705212, 0.8758393412240574, 0.8760977600428281, 0.8764009252150159, 0.8766620091641025, 0.876939337659457, 0.8772077568346878, 0.8774681576218072, 0.8777095771537441, 0.877995090026605, 0.8782144045533562, 0.8784729640980206, 0.8787056676882185, 0.8789315715076318, 0.8791301790627509, 0.8792736317447112, 0.8794215626878871, 0.8795994064190984, 0.8797406422477768, 0.8799430486112344, 0.8801158025736403, 0.8802336340809822, 0.8804126236140604, 0.8805925377232426, 0.8807921074803301, 0.880952896732297, 0.8810552541178909, 0.8811920816472782, 0.8813316970119621, 0.8814738214284129, 0.8815946745796893, 0.8816846188864264, 0.8818126275860191, 0.8819231295332994, 0.8820719930505577, 0.882144793745502, 0.8822291379003636, 0.8822932829338567, 0.8824004252287062, 0.8824803827058355, 0.8825335209058401, 0.8825695805799619, 0.8827055636984363, 0.8828138308580045, 0.8829418595369098, 0.8830406147596894, 0.8831036121072499, 0.8831226626612307, 0.8831609846304017, 0.8831248861673616, 0.8831394563741549, 0.8831596283837982, 0.8832177831924772, 0.8832818872261707, 0.8833160514447301, 0.8833562110061395, 0.8834441193172903, 0.8835114720914435, 0.8835226778234756, 0.8835845276881868, 0.883590780801721, 0.8836222909568431, 0.8836082971552764, 0.8836427615573959, 0.8836596618722444, 0.8837407545085494, 0.8838043261165179, 0.8837885993872191, 0.883788562977909, 0.8837602949154122, 0.8838125007179886, 0.8838265447638367, 0.8838862432286295, 0.8838929130234136, 0.8839459746622487, 0.8839654948430826, 0.8839595335940685, 0.883980050822897, 0.8840149869170779, 0.8840370176371348, 0.8841086099910684, 0.8841448078154911, 0.8840973858574713, 0.8841370590364301, 0.8841045296033754, 0.8840823119371555, 0.8841305513316752, 0.8841739667867429], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.03490666666666666, 0.07184266666666667, 0.1102584, 0.14927256, 0.18961197066666666, 0.22898410693333332, 0.26489902957333333, 0.29976912661599997, 0.3332322139544, 0.3645356592256267, 0.39413542663639733, 0.4210685506394243, 0.44664169557548183, 0.4692975260179337, 0.4919677734161403, 0.5120776627411929, 0.5311365631337404, 0.5496362401536996, 0.5676459494716629, 0.5837480211911633, 0.5987998857387137, 0.6133732304981756, 0.6265692407816914, 0.6384589833701889, 0.65033308503317, 0.6606464431965197, 0.6702617988768677, 0.6793289523225142, 0.6881560570902627, 0.6967404513812365, 0.7045064062431129, 0.711589098952135, 0.7179635223902548, 0.724273836817896, 0.7296864531361065, 0.7350511411558291, 0.7401993603735796, 0.7446060910028883, 0.7487188152359328, 0.7526736003790062, 0.7565262403411056, 0.7600202829736618, 0.7633515880096289, 0.7661630958753327, 0.7690934529544661, 0.7718241076590195, 0.7743616968931176, 0.7767521938704725, 0.7789569744834252, 0.781114610368416, 0.7830164826649078, 0.7850881677317503, 0.7869126842919085, 0.788394749196051, 0.7900219409431126, 0.7916597468488014, 0.7929471054972546, 0.7944257282808624, 0.7958364887861096, 0.7970395065741653, 0.7983488892500821, 0.7995940003250739, 0.8007146002925665, 0.8017498069299765, 0.8025348262369789, 0.8034280102799476, 0.8042052092519528, 0.8049046883267575, 0.8055475528274151, 0.8063527975446736, 0.8071175177902061, 0.8075924326778522, 0.8082998560767337, 0.8087365371357269, 0.8092228834221542, 0.8097139284132722, 0.8102358689052783, 0.8106656153480838, 0.811145720479942, 0.8115911484319478, 0.8120587002554197, 0.8123861635632111, 0.8128408805402233, 0.8133434591528677, 0.8137024465709142, 0.8139855352471561, 0.8142269817224405, 0.8144709502168631, 0.8146638551951767, 0.8148241363423256, 0.8152350560414263, 0.8154315504372838, 0.815715062060222, 0.8161302225208665, 0.8162772002687798, 0.8164494802419018, 0.8167245322177116, 0.8168120789959403, 0.8166642044296797, 0.8167177839867118, 0.816819338921374, 0.8170040716959033, 0.817143664526313, 0.817362631407015, 0.8175063682663135, 0.8175957314396822, 0.817676158295714, 0.8177352091328093, 0.8177750215528616, 0.8177841860642422, 0.8178191007911513, 0.8177571907120362, 0.8179148049741659, 0.8180033244767493, 0.8180163253624076, 0.8180280261595002, 0.8180385568768835, 0.8181280345225285, 0.8181818977369423, 0.8180970412965813, 0.8181006705002565, 0.8181972701168976, 0.8181375431052078, 0.8182571221280204, 0.8181914099152183, 0.8183056022570299, 0.8183817086979935, 0.8184235378281941, 0.8184878507120414, 0.8185323989741706, 0.8185991590767536, 0.8185792431690783, 0.8186946521855037, 0.8188385203002867, 0.8188880016035913, 0.8188125347765655, 0.8187046146322423, 0.8187141531690181, 0.8187360711854496, 0.8186891307335712, 0.8186202176602141, 0.8185981958941927, 0.8185117096381067, 0.8185272053409627, 0.8186744848068664, 0.8186737029928465, 0.8186196660268952, 0.8185976994242057, 0.8186445961484519, 0.8186201365336068, 0.8186647895469128, 0.8186916439255548, 0.818689146199666, 0.8187402315796994, 0.8187062084217296, 0.8187422542462234, 0.8186946954882677, 0.8186785592727742, 0.8186640366788301, 0.8187176330109471, 0.8187392030431857, 0.8188652827388672, 0.8189654211316472, 0.8189222123518158, 0.8188566577833009, 0.8187709920049707, 0.818827226137807, 0.8188645035240263, 0.8189780531716236, 0.8189469145211279, 0.8189855564023484, 0.8190070007621135, 0.8190663006859022, 0.8190930039506453, 0.8191303702222474, 0.8192173332000227, 0.8192289332133537, 0.8191727065586849, 0.8192554359028164, 0.8193032256458681, 0.819279569747948, 0.8191916127731532, 0.8191657848291711, 0.8191025396795873], "moving_var_accuracy_train": [0.010948052379238751, 0.02206120924849827, 0.03348045388380955, 0.04461857616808919, 0.05601619455853951, 0.06467706725256289, 0.07049712015243417, 0.07547307932312895, 0.07881460557253854, 0.080300976846252, 0.0806236528614147, 0.07946123528065277, 0.07802350696577236, 0.07509978245197149, 0.07265707991005635, 0.0697102385367789, 0.06646744268085673, 0.06341735346477846, 0.06030649345937679, 0.05696484051996575, 0.05349214283524453, 0.0502764050001992, 0.04712426804959577, 0.043970318382495925, 0.04105456990179907, 0.03816514425176587, 0.035388867265566164, 0.03281341282404759, 0.03041499979037153, 0.02816657109054914, 0.025997320921410817, 0.02403695321564135, 0.022144737446492226, 0.020414531568411796, 0.018786299135801734, 0.017257526042734676, 0.01586468187607917, 0.014519450157932106, 0.013275740859110523, 0.012141122951848484, 0.011128935193070049, 0.010181956129196928, 0.009316582340207899, 0.008512508201027485, 0.007771307709864888, 0.007095859794573451, 0.006474636041824397, 0.005903726509204937, 0.005381394648864625, 0.004909003307127875, 0.0044773933216087965, 0.004080055923294966, 0.003713146959465429, 0.0033809780268052956, 0.003075147208389594, 0.0028034845274299055, 0.0025500118558818494, 0.0023264221065808655, 0.002117680627652479, 0.0019309121020472308, 0.0017600877942133026, 0.0016020552992519702, 0.001455982780906543, 0.0013273970530146922, 0.0012094512344679217, 0.0011032192549894927, 0.00100326092121711, 0.0009142935431368516, 0.0008309047887490937, 0.0007570075500346363, 0.0006889471886769385, 0.0006265253428880287, 0.0005697138496226021, 0.0005190624879314348, 0.00047320890259651475, 0.00043153599020295943, 0.0003933582237383426, 0.00035854904773879425, 0.0003258902777393225, 0.00029706780333642893, 0.0002699386804030245, 0.0002460020192273921, 0.00022436666604568907, 0.00020403500208147947, 0.00018682049940450644, 0.00017052136466106786, 0.00015598763695222781, 0.00014240867058356658, 0.00012969728785624303, 0.00011733796718924389, 0.00010735803113614248, 9.818065874987278e-05, 8.965684438794988e-05, 8.209832414713895e-05, 7.499834960119243e-05, 6.856530204894269e-05, 6.265340334535195e-05, 5.724209989554725e-05, 5.211891247904627e-05, 4.773420332579047e-05, 4.357426644944754e-05, 3.990903965352289e-05, 3.656657537085471e-05, 3.352019496316118e-05, 3.069272598045087e-05, 2.8357111787529235e-05, 2.5954290363573483e-05, 2.3960538670449616e-05, 2.2051843451423792e-05, 2.0305951926911713e-05, 1.863036138277431e-05, 1.6952533292151036e-05, 1.545423203847626e-05, 1.4193464369209016e-05, 1.2953645966010377e-05, 1.2026996393122544e-05, 1.1092892137552968e-05, 1.0108561300899734e-05, 9.386040447373887e-06, 8.738758182781906e-06, 8.22333515599941e-06, 7.633680292332403e-06, 6.964605572569581e-06, 6.436640970496925e-06, 5.968408923949715e-06, 5.553362179318207e-06, 5.129475318947213e-06, 4.6893375918820085e-06, 4.3678798772364415e-06, 4.040988012687751e-06, 3.836332332353561e-06, 3.5003985697774655e-06, 3.2143841409335143e-06, 2.9299769947365782e-06, 2.74029453737373e-06, 2.5238038669764178e-06, 2.2968364949764225e-06, 2.0788555463587617e-06, 2.0373926683130687e-06, 1.9391494020503235e-06, 1.8927565454457351e-06, 1.791254237137401e-06, 1.6478468056205518e-06, 1.486328437521307e-06, 1.3509127536594942e-06, 1.2275493695983093e-06, 1.1067050509724608e-06, 9.996967356326689e-07, 9.301648980218232e-07, 8.741323524415852e-07, 7.972238616653944e-07, 7.320165888521669e-07, 7.283657704913444e-07, 6.963567591175581e-07, 6.278511990791832e-07, 5.994947310544771e-07, 5.39897170808881e-07, 4.948434626103141e-07, 4.471215546898662e-07, 4.1309955434191944e-07, 3.7436018468558207e-07, 3.961083071830347e-07, 3.928696205221467e-07, 3.5580862859988547e-07, 3.2022777767063774e-07, 2.953967501194725e-07, 2.903860875113739e-07, 2.631225957743157e-07, 2.688854964844393e-07, 2.4239732229815504e-07, 2.4349742771115495e-07, 2.2257702207815186e-07, 2.0063914827861442e-07, 1.843638435599409e-07, 1.7691223529349164e-07, 1.6358918540017561e-07, 1.9335945313591526e-07, 1.8581605025866409e-07, 1.874740241545942e-07, 1.8289227189737285e-07, 1.7412652084142068e-07, 1.6115649098760534e-07, 1.65984194541531e-07, 1.6634989073605885e-07], "duration": 165033.612625, "accuracy_train": [0.3487764705882353, 0.4031764705882353, 0.4608, 0.5118117647058823, 0.5705176470588236, 0.5908, 0.6020235294117647, 0.6350117647058824, 0.6538588235294117, 0.663435294117647, 0.6777176470588235, 0.6804235294117648, 0.7001411764705883, 0.6909411764705883, 0.7186823529411764, 0.7241882352941177, 0.7305647058823529, 0.7472941176470588, 0.7568470588235294, 0.7591764705882353, 0.7608, 0.7732941176470588, 0.7790823529411764, 0.7807529411764705, 0.7906117647058823, 0.7913882352941176, 0.7942823529411764, 0.8009882352941177, 0.8069176470588235, 0.8116470588235294, 0.8119764705882353, 0.8199294117647059, 0.8194588235294118, 0.8249647058823529, 0.8267058823529412, 0.8280705882352941, 0.8327764705882353, 0.8298117647058824, 0.8313176470588235, 0.8343294117647059, 0.8400235294117647, 0.8403294117647059, 0.8428941176470588, 0.8434588235294118, 0.8445411764705882, 0.8466823529411764, 0.847764705882353, 0.8487294117647058, 0.8499764705882353, 0.8522588235294117, 0.8536, 0.8541647058823529, 0.8542352941176471, 0.8558588235294118, 0.8560235294117647, 0.8589411764705882, 0.8582588235294117, 0.8613882352941177, 0.8608705882352942, 0.8628705882352942, 0.8636, 0.8635764705882353, 0.8633882352941177, 0.8658588235294118, 0.8663058823529411, 0.8675529411764706, 0.8667764705882353, 0.8683529411764705, 0.8676941176470588, 0.8692941176470588, 0.8694117647058823, 0.8696, 0.8700235294117648, 0.8711529411764706, 0.8718117647058824, 0.8723529411764706, 0.8726588235294118, 0.8730588235294118, 0.8726352941176471, 0.8737411764705882, 0.8732705882352941, 0.8742823529411765, 0.8747764705882353, 0.8744470588235295, 0.8760470588235294, 0.875835294117647, 0.8764941176470589, 0.8764705882352941, 0.8763294117647059, 0.8752235294117647, 0.8772941176470588, 0.8774823529411765, 0.8775294117647059, 0.8780705882352942, 0.8780235294117648, 0.8783058823529412, 0.8784470588235294, 0.8786117647058823, 0.8784235294117647, 0.8791294117647059, 0.8790117647058824, 0.8794352941176471, 0.8796235294117647, 0.8798117647058824, 0.8798823529411764, 0.8805647058823529, 0.8801882352941176, 0.8808, 0.8808, 0.880964705882353, 0.8809176470588236, 0.8805647058823529, 0.8807529411764706, 0.8812, 0.8810117647058824, 0.8817647058823529, 0.8816705882352941, 0.8812941176470588, 0.8820235294117647, 0.8822117647058824, 0.8825882352941177, 0.8824, 0.8819764705882353, 0.8824235294117647, 0.8825882352941177, 0.8827529411764706, 0.8826823529411765, 0.8824941176470589, 0.882964705882353, 0.8829176470588236, 0.8834117647058823, 0.8828, 0.8829882352941176, 0.8828705882352941, 0.8833647058823529, 0.8832, 0.8830117647058824, 0.8828941176470588, 0.8839294117647059, 0.8837882352941177, 0.8840941176470588, 0.8839294117647059, 0.8836705882352941, 0.8832941176470588, 0.8835058823529411, 0.8828, 0.8832705882352941, 0.8833411764705882, 0.8837411764705883, 0.8838588235294118, 0.8836235294117647, 0.8837176470588235, 0.884235294117647, 0.8841176470588236, 0.8836235294117647, 0.8841411764705882, 0.8836470588235295, 0.8839058823529412, 0.8834823529411765, 0.8839529411764706, 0.8838117647058824, 0.8844705882352941, 0.8843764705882353, 0.8836470588235295, 0.8837882352941177, 0.8835058823529411, 0.8842823529411765, 0.8839529411764706, 0.8844235294117647, 0.8839529411764706, 0.8844235294117647, 0.8841411764705882, 0.8839058823529412, 0.884164705882353, 0.8843294117647059, 0.884235294117647, 0.8847529411764706, 0.8844705882352941, 0.8836705882352941, 0.8844941176470589, 0.8838117647058824, 0.8838823529411765, 0.8845647058823529, 0.8845647058823529], "end": "2016-02-06 06:26:07.065000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0], "moving_var_accuracy_valid": [0.0109662784, 0.022148063423999997, 0.03321517418943999, 0.043592598895046394, 0.053878751481947575, 0.06244236236156154, 0.06780706113954005, 0.07196966803547096, 0.07485070515988035, 0.07618478581653046, 0.0764516233118229, 0.07533499949770243, 0.07368737122521765, 0.07093821398003003, 0.06846985363589007, 0.06526253711029387, 0.062005458556821866, 0.05888505514972497, 0.0559156963022103, 0.05265761709492868, 0.04943088302265603, 0.04639923611769367, 0.043326524692548184, 0.040266166032679955, 0.03750849804213725, 0.03471493644737331, 0.03207553838637239, 0.02960790399219726, 0.027348373600204822, 0.02527676266827046, 0.023291876895693724, 0.021414170030218258, 0.019638452494712497, 0.018032987858822517, 0.016493356811613683, 0.01510304002839292, 0.01383127347557976, 0.012622919601575373, 0.01151285814697148, 0.010502335262025196, 0.009585687247920763, 0.008736993528391765, 0.007963172514736529, 0.007237996451573104, 0.006591479739916839, 0.0059994400419646505, 0.005457450269857278, 0.004963135525060237, 0.004510571490515485, 0.004101412874973737, 0.0037238256515658283, 0.0033900699975548516, 0.0030810227439039936, 0.002792689116934412, 0.002537249982076318, 0.0023076666575310664, 0.002091815622385685, 0.001902310988172955, 0.0017299920961841447, 0.001570018152751136, 0.0014284466844039418, 0.0012995547302651525, 0.0011809009558229372, 0.0010724557352798834, 0.000970756459563193, 0.0008808608132184002, 0.0007982110760753342, 0.0007227934072526078, 0.0006542335394231985, 0.0005946459569729331, 0.0005404445347609873, 0.0004884299786394593, 0.00044409101156307805, 0.0004013981235323218, 0.0003633871055719844, 0.00032921852166450385, 0.00029874846639281374, 0.0002705357577994682, 0.0002455566904582514, 0.00022278667595627898, 0.00020247545072933808, 0.00018319299561795122, 0.00016673460381880426, 0.00015233441079391137, 0.0001382608174113619, 0.0001251559884577733, 0.00011316505721584128, 0.0001023842371306947, 9.248072439354911e-05, 8.34638623693768e-05, 7.663717112442074e-05, 6.932094444040883e-05, 6.311225955943791e-05, 5.835225747623678e-05, 5.2711453854048226e-05, 4.770743197089371e-05, 4.3617571078376346e-05, 3.932479391594297e-05, 3.5589116510469784e-05, 3.205604177980854e-05, 2.8943258244615783e-05, 2.635606820202172e-05, 2.389583680653559e-05, 2.1937771579481442e-05, 1.9929936984022257e-05, 1.8008815276410663e-05, 1.6266150061310003e-05, 1.4670918067433868e-05, 1.321809151980435e-05, 1.1897038262243505e-05, 1.07183057794154e-05, 9.680970922538234e-06, 8.93645413092466e-06, 8.113330038870721e-06, 7.303518242234777e-06, 6.574398595884713e-06, 5.9179568003737075e-06, 5.39821736196785e-06, 4.884506838573967e-06, 4.460861693953062e-06, 4.014894064631601e-06, 3.6973880315851696e-06, 3.359755071755149e-06, 3.1524718488508825e-06, 2.876087518167857e-06, 2.7058377847066244e-06, 2.4873837194413935e-06, 2.254392432697339e-06, 2.0661785126863624e-06, 1.8774215903463014e-06, 1.729791632983631e-06, 1.5603822600920695e-06, 1.5242172037335877e-06, 1.558077793421066e-06, 1.4243056084695166e-06, 1.33313222545463e-06, 1.3046398208658657e-06, 1.174994691933683e-06, 1.0618188177389476e-06, 9.75467590167888e-07, 9.206619362668202e-07, 8.329603662484529e-07, 8.169831820494659e-07, 7.374459151075312e-07, 8.589224932887565e-07, 7.730357450583374e-07, 7.220121137555112e-07, 6.541536870834321e-07, 6.085320430802212e-07, 5.530632935975742e-07, 5.157019886135638e-07, 4.7062220862246305e-07, 4.2361613547175785e-07, 4.0474196640299413e-07, 3.7468594726692665e-07, 3.4891106571111546e-07, 3.3437647826457465e-07, 3.032822274921717e-07, 2.748521563567386e-07, 2.7322004206861666e-07, 2.500854344787303e-07, 3.6814169799893907e-07, 4.2157680757599824e-07, 3.9622211470905316e-07, 3.9527651631673234e-07, 4.217964948771575e-07, 4.0807734465205945e-07, 3.797760418969422e-07, 4.578401399326239e-07, 4.207826659315994e-07, 3.9214315419680733e-07, 3.570675838687679e-07, 3.530091541339407e-07, 3.2412581785201355e-07, 3.0427938034781375e-07, 3.419144778448847e-07, 3.0893407284392603e-07, 3.064935958166302e-07, 3.374415356588321e-07, 3.2425211796147045e-07, 2.968633197229968e-07, 3.368048524861758e-07, 3.091281114505884e-07, 3.142148408184012e-07], "accuracy_test": 0.8093, "start": "2016-02-04 08:35:33.452000", "learning_rate_per_epoch": [0.00013477489119395614, 0.000130550135509111, 0.0001264578168047592, 0.00012249377323314548, 0.00011865398846566677, 0.0001149345698649995, 0.00011133174120914191, 0.00010784184996737167, 0.00010446135274833068, 0.00010118682985194027, 9.801494888961315e-05, 9.494249388808385e-05, 9.196635073749349e-05, 8.908349991543218e-05, 8.629101648693904e-05, 8.358607010450214e-05, 8.096591773210093e-05, 7.842789636924863e-05, 7.596943760290742e-05, 7.358803850365803e-05, 7.128129072953016e-05, 6.904685142217204e-05, 6.688245775876567e-05, 6.478591240011156e-05, 6.275508349062875e-05, 6.078791557229124e-05, 5.8882411394733936e-05, 5.703663919121027e-05, 5.524872540263459e-05, 5.351685467758216e-05, 5.183927351026796e-05, 5.021427932661027e-05, 4.8640224122209474e-05, 4.711551082436927e-05, 4.563859329209663e-05, 4.4207969040144235e-05, 4.282219379092567e-05, 4.1479856008663774e-05, 4.01795950892847e-05, 3.892009408446029e-05, 3.770007606362924e-05, 3.651830047601834e-05, 3.5373570426600054e-05, 3.4264721762156114e-05, 3.3190633985213935e-05, 3.215021570213139e-05, 3.114241189905442e-05, 3.0166198484948836e-05, 2.92205859295791e-05, 2.830461562552955e-05, 2.741735806921497e-05, 2.6557912860880606e-05, 2.572540870460216e-05, 2.4918999770306982e-05, 2.413786933175288e-05, 2.338122430955991e-05, 2.264829890918918e-05, 2.1938347344985232e-05, 2.1250651116133668e-05, 2.0584511730703525e-05, 1.993925434362609e-05, 1.931422229972668e-05, 1.8708784409682266e-05, 1.8122324036085047e-05, 1.7554248188389465e-05, 1.7003978427965194e-05, 1.6470958144054748e-05, 1.5954647096805274e-05, 1.545451959827915e-05, 1.4970069969422184e-05, 1.4500807083095424e-05, 1.4046253454580437e-05, 1.3605948879558127e-05, 1.3179445886635222e-05, 1.2766312465828378e-05, 1.236613024957478e-05, 1.1978491784248035e-05, 1.1603005077631678e-05, 1.1239288141950965e-05, 1.0886972631851677e-05, 1.0545701115916017e-05, 1.0215127076662611e-05, 9.894915820041206e-06, 9.584741746948566e-06, 9.284291081712581e-06, 8.993258234113455e-06, 8.711348527867813e-06, 8.438275472144596e-06, 8.173762580554467e-06, 7.917541552160401e-06, 7.669352271477692e-06, 7.428942808473948e-06, 7.1960694185690954e-06, 6.970496087888023e-06, 6.751993623765884e-06, 6.540340564242797e-06, 6.335321813821793e-06, 6.136730007710867e-06, 5.9443632380862255e-06, 5.75802687308169e-06, 5.57753128305194e-06, 5.402693659561919e-06, 5.233336651144782e-06, 5.069288363301894e-06, 4.9103823585028294e-06, 4.7564576561853755e-06, 4.607358278008178e-06, 4.46293233835604e-06, 4.323033863329329e-06, 4.187520517007215e-06, 4.056255420437083e-06, 3.929104877897771e-06, 3.8059401958889794e-06, 3.6866363188892137e-06, 3.5710722841031384e-06, 3.4591307667142246e-06, 3.3506980798847508e-06, 3.2456646295031533e-06, 3.1439235499419738e-06, 3.045371613552561e-06, 2.9499090032913955e-06, 2.8574388579727383e-06, 2.7678672722686315e-06, 2.681103524082573e-06, 2.5970596198021667e-06, 2.515650294299121e-06, 2.4367927835555747e-06, 2.3604072794114472e-06, 2.2864162474434124e-06, 2.2147444269648986e-06, 2.1453192857734393e-06, 2.0780705654033227e-06, 2.0129298263782403e-06, 1.949830902958638e-06, 1.8887100168285542e-06, 1.8295050949745928e-06, 1.772155997059599e-06, 1.7166046291094972e-06, 1.6627946024527773e-06, 1.610671347407333e-06, 1.560181999593624e-06, 1.5112752862478374e-06, 1.4639016399087268e-06, 1.4180130847307737e-06, 1.3735630091105122e-06, 1.3305062793733669e-06, 1.2887992397736525e-06, 1.2483995988077368e-06, 1.209266315527202e-06, 1.1713597132256837e-06, 1.1346413657520316e-06, 1.099073983823473e-06, 1.0646215287124505e-06, 1.0312490985597833e-06, 9.989228146878304e-07, 9.676098216004902e-07, 9.372784006700385e-07, 9.078977427634527e-07, 8.794380619292497e-07, 8.51870538554067e-07, 8.251671488324064e-07, 7.9930083529689e-07, 7.742453362880042e-07, 7.499752427975181e-07, 7.264659416250652e-07, 7.036935585347237e-07, 6.816350150984363e-07, 6.602679718525906e-07, 6.395707146111818e-07, 6.195222113092314e-07, 6.00102168846206e-07, 5.8129091939918e-07, 5.630693067359971e-07, 5.454189135889465e-07, 5.283217774376681e-07, 5.117606178828282e-07, 4.957185524290253e-07, 4.801793807018839e-07, 4.6512730023096083e-07, 4.5054704855829186e-07, 4.364238463949732e-07, 4.22743369199452e-07, 4.094917187558167e-07, 3.966554800172162e-07], "accuracy_train_first": 0.3487764705882353, "accuracy_train_last": 0.8845647058823529, "batch_size_eval": 1024, "accuracy_train_std": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "accuracy_test_std": 0, "error_valid": [0.6509333333333334, 0.5957333333333333, 0.544, 0.49960000000000004, 0.44733333333333336, 0.41666666666666663, 0.4118666666666667, 0.38639999999999997, 0.36560000000000004, 0.35373333333333334, 0.3394666666666667, 0.33653333333333335, 0.32320000000000004, 0.3268, 0.30400000000000005, 0.3069333333333333, 0.29733333333333334, 0.2838666666666667, 0.27026666666666666, 0.2713333333333333, 0.2657333333333334, 0.2554666666666666, 0.2546666666666667, 0.2545333333333333, 0.24280000000000002, 0.24653333333333338, 0.24319999999999997, 0.23906666666666665, 0.23240000000000005, 0.22599999999999998, 0.22560000000000002, 0.22466666666666668, 0.22466666666666668, 0.2189333333333333, 0.22160000000000002, 0.21666666666666667, 0.2134666666666667, 0.21573333333333333, 0.21426666666666672, 0.21173333333333333, 0.20879999999999999, 0.20853333333333335, 0.20666666666666667, 0.20853333333333335, 0.20453333333333334, 0.2036, 0.20279999999999998, 0.20173333333333332, 0.20120000000000005, 0.19946666666666668, 0.19986666666666664, 0.1962666666666667, 0.19666666666666666, 0.1982666666666667, 0.19533333333333336, 0.1936, 0.19546666666666668, 0.1922666666666667, 0.19146666666666667, 0.19213333333333338, 0.18986666666666663, 0.18920000000000003, 0.18920000000000003, 0.1889333333333333, 0.1904, 0.18853333333333333, 0.18879999999999997, 0.18879999999999997, 0.18866666666666665, 0.1864, 0.18600000000000005, 0.18813333333333337, 0.18533333333333335, 0.18733333333333335, 0.1864, 0.18586666666666662, 0.1850666666666667, 0.18546666666666667, 0.18453333333333333, 0.1844, 0.1837333333333333, 0.18466666666666665, 0.1830666666666667, 0.18213333333333337, 0.1830666666666667, 0.18346666666666667, 0.18359999999999999, 0.18333333333333335, 0.18359999999999999, 0.1837333333333333, 0.1810666666666667, 0.18279999999999996, 0.1817333333333333, 0.18013333333333337, 0.1824, 0.18200000000000005, 0.18079999999999996, 0.1824, 0.18466666666666665, 0.18279999999999996, 0.1822666666666667, 0.18133333333333335, 0.18159999999999998, 0.18066666666666664, 0.18120000000000003, 0.18159999999999998, 0.18159999999999998, 0.1817333333333333, 0.18186666666666662, 0.18213333333333337, 0.18186666666666662, 0.18279999999999996, 0.18066666666666664, 0.18120000000000003, 0.18186666666666662, 0.18186666666666662, 0.18186666666666662, 0.1810666666666667, 0.18133333333333335, 0.18266666666666664, 0.18186666666666662, 0.18093333333333328, 0.1824, 0.18066666666666664, 0.1824, 0.18066666666666664, 0.18093333333333328, 0.18120000000000003, 0.18093333333333328, 0.1810666666666667, 0.18079999999999996, 0.18159999999999998, 0.1802666666666667, 0.17986666666666662, 0.18066666666666664, 0.18186666666666662, 0.1822666666666667, 0.18120000000000003, 0.1810666666666667, 0.1817333333333333, 0.18200000000000005, 0.18159999999999998, 0.1822666666666667, 0.18133333333333335, 0.18000000000000005, 0.18133333333333335, 0.18186666666666662, 0.18159999999999998, 0.18093333333333328, 0.18159999999999998, 0.18093333333333328, 0.1810666666666667, 0.18133333333333335, 0.18079999999999996, 0.18159999999999998, 0.18093333333333328, 0.1817333333333333, 0.18146666666666667, 0.18146666666666667, 0.18079999999999996, 0.1810666666666667, 0.18000000000000005, 0.18013333333333337, 0.18146666666666667, 0.1817333333333333, 0.18200000000000005, 0.18066666666666664, 0.18079999999999996, 0.18000000000000005, 0.18133333333333335, 0.18066666666666664, 0.18079999999999996, 0.1804, 0.18066666666666664, 0.18053333333333332, 0.18000000000000005, 0.18066666666666664, 0.18133333333333335, 0.18000000000000005, 0.1802666666666667, 0.18093333333333328, 0.18159999999999998, 0.1810666666666667, 0.18146666666666667], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.03134676951029593, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 64, "valid_ratio": 0.15, "learning_rate": 0.0001391363572657838, "optimization": "rmsprop", "nb_data_augmentation": 4, "learning_rate_decay_method": "exp", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 7.830235388229065e-06, "rotation_range": [0, 0], "momentum": 0.7280601127909146}, "accuracy_valid_max": 0.8201333333333334, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        y_pred = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            y_pred.extend((nnet.predict(X[mini_batch]) == y[mini_batch]).tolist())\n        return np.mean(y_pred)\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            acc = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = acc\n            status[\"accuracy_train_std\"] = 0\n            acc = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = acc\n            status[\"accuracy_valid_std\"] = 0\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n\n    # rescaling to [-1, 1]\n    X_min = X_train.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X_train.max(axis=(0, 2, 3))[None, :, None, None]\n    def preprocess(a):\n        return (a / 255.) * 2 - 1\n        # return 2 * ((a - X_min) / (X_max - X_min)) - 1\n    X_train = preprocess(X_train)\n    X_valid = preprocess(X_valid)\n\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = preprocess(X_test)\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    acc = evaluate(X_test, y_test, batch_size_eval)\n    light.set(\"accuracy_test\", acc)\n    light.set(\"accuracy_test_std\", 0)\n    print(\"Test accuracy : {}+-{}\".format(acc, 0))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8185333333333333, "accuracy_valid_std": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "accuracy_valid": [0.3490666666666667, 0.40426666666666666, 0.456, 0.5004, 0.5526666666666666, 0.5833333333333334, 0.5881333333333333, 0.6136, 0.6344, 0.6462666666666667, 0.6605333333333333, 0.6634666666666666, 0.6768, 0.6732, 0.696, 0.6930666666666667, 0.7026666666666667, 0.7161333333333333, 0.7297333333333333, 0.7286666666666667, 0.7342666666666666, 0.7445333333333334, 0.7453333333333333, 0.7454666666666667, 0.7572, 0.7534666666666666, 0.7568, 0.7609333333333334, 0.7676, 0.774, 0.7744, 0.7753333333333333, 0.7753333333333333, 0.7810666666666667, 0.7784, 0.7833333333333333, 0.7865333333333333, 0.7842666666666667, 0.7857333333333333, 0.7882666666666667, 0.7912, 0.7914666666666667, 0.7933333333333333, 0.7914666666666667, 0.7954666666666667, 0.7964, 0.7972, 0.7982666666666667, 0.7988, 0.8005333333333333, 0.8001333333333334, 0.8037333333333333, 0.8033333333333333, 0.8017333333333333, 0.8046666666666666, 0.8064, 0.8045333333333333, 0.8077333333333333, 0.8085333333333333, 0.8078666666666666, 0.8101333333333334, 0.8108, 0.8108, 0.8110666666666667, 0.8096, 0.8114666666666667, 0.8112, 0.8112, 0.8113333333333334, 0.8136, 0.814, 0.8118666666666666, 0.8146666666666667, 0.8126666666666666, 0.8136, 0.8141333333333334, 0.8149333333333333, 0.8145333333333333, 0.8154666666666667, 0.8156, 0.8162666666666667, 0.8153333333333334, 0.8169333333333333, 0.8178666666666666, 0.8169333333333333, 0.8165333333333333, 0.8164, 0.8166666666666667, 0.8164, 0.8162666666666667, 0.8189333333333333, 0.8172, 0.8182666666666667, 0.8198666666666666, 0.8176, 0.818, 0.8192, 0.8176, 0.8153333333333334, 0.8172, 0.8177333333333333, 0.8186666666666667, 0.8184, 0.8193333333333334, 0.8188, 0.8184, 0.8184, 0.8182666666666667, 0.8181333333333334, 0.8178666666666666, 0.8181333333333334, 0.8172, 0.8193333333333334, 0.8188, 0.8181333333333334, 0.8181333333333334, 0.8181333333333334, 0.8189333333333333, 0.8186666666666667, 0.8173333333333334, 0.8181333333333334, 0.8190666666666667, 0.8176, 0.8193333333333334, 0.8176, 0.8193333333333334, 0.8190666666666667, 0.8188, 0.8190666666666667, 0.8189333333333333, 0.8192, 0.8184, 0.8197333333333333, 0.8201333333333334, 0.8193333333333334, 0.8181333333333334, 0.8177333333333333, 0.8188, 0.8189333333333333, 0.8182666666666667, 0.818, 0.8184, 0.8177333333333333, 0.8186666666666667, 0.82, 0.8186666666666667, 0.8181333333333334, 0.8184, 0.8190666666666667, 0.8184, 0.8190666666666667, 0.8189333333333333, 0.8186666666666667, 0.8192, 0.8184, 0.8190666666666667, 0.8182666666666667, 0.8185333333333333, 0.8185333333333333, 0.8192, 0.8189333333333333, 0.82, 0.8198666666666666, 0.8185333333333333, 0.8182666666666667, 0.818, 0.8193333333333334, 0.8192, 0.82, 0.8186666666666667, 0.8193333333333334, 0.8192, 0.8196, 0.8193333333333334, 0.8194666666666667, 0.82, 0.8193333333333334, 0.8186666666666667, 0.82, 0.8197333333333333, 0.8190666666666667, 0.8184, 0.8189333333333333, 0.8185333333333333], "seed": 976945963, "model": "residualv3", "loss_std": [0.3409377634525299, 0.1348419040441513, 0.13183744251728058, 0.1296738088130951, 0.13015331327915192, 0.1307583898305893, 0.13040965795516968, 0.12928850948810577, 0.1293724924325943, 0.12626110017299652, 0.1257430613040924, 0.12441197037696838, 0.12248311191797256, 0.12286198884248734, 0.12171760201454163, 0.12115975469350815, 0.11938637495040894, 0.12040317803621292, 0.1191394180059433, 0.11819012463092804, 0.11874323338270187, 0.1178603246808052, 0.11763099581003189, 0.1166539117693901, 0.11690816283226013, 0.1161179393529892, 0.11528052389621735, 0.11516908556222916, 0.11422281712293625, 0.11403276771306992, 0.11292673647403717, 0.1137547641992569, 0.11202971637248993, 0.11116140335798264, 0.11150196939706802, 0.11034401506185532, 0.11037378013134003, 0.10984668880701065, 0.11139889061450958, 0.1097574457526207, 0.11087404191493988, 0.10808659344911575, 0.10864711552858353, 0.10880199819803238, 0.10837716609239578, 0.10745537281036377, 0.10773056745529175, 0.10804464668035507, 0.10586921125650406, 0.10815370082855225, 0.10660338401794434, 0.10699332505464554, 0.1062123253941536, 0.10598604381084442, 0.10673379898071289, 0.10460346937179565, 0.10458358377218246, 0.10539014637470245, 0.10584191977977753, 0.10459879785776138, 0.10371562838554382, 0.10471998155117035, 0.10372120141983032, 0.10391122847795486, 0.10407192260026932, 0.10427781194448471, 0.10326909273862839, 0.10335788130760193, 0.10330401360988617, 0.10411885380744934, 0.10212250053882599, 0.10359403491020203, 0.10331189632415771, 0.10402968525886536, 0.10379240661859512, 0.09967833012342453, 0.10230150073766708, 0.10236260294914246, 0.10121093690395355, 0.10154639929533005, 0.1015864759683609, 0.10079425573348999, 0.10090316087007523, 0.10097672045230865, 0.10153483599424362, 0.10141777992248535, 0.10010989755392075, 0.10100756585597992, 0.10028819739818573, 0.10038363188505173, 0.10194007307291031, 0.10025846213102341, 0.09832911938428879, 0.10020891577005386, 0.10048497468233109, 0.10080445557832718, 0.09972339868545532, 0.10079585015773773, 0.09900315850973129, 0.09936119616031647, 0.10039176791906357, 0.10007232427597046, 0.09966278821229935, 0.0996166244149208, 0.09924344718456268, 0.10170970112085342, 0.09927050769329071, 0.09981706738471985, 0.0997704267501831, 0.09803365916013718, 0.0991273820400238, 0.10054139792919159, 0.09844505041837692, 0.09997135400772095, 0.09792107343673706, 0.09767676144838333, 0.09967093169689178, 0.09797749668359756, 0.09904798120260239, 0.10076886415481567, 0.09823894500732422, 0.09864763915538788, 0.09771939367055893, 0.09707355499267578, 0.09884626418352127, 0.09767230600118637, 0.09839419275522232, 0.09767010062932968, 0.09979452937841415, 0.09897493571043015, 0.09816662967205048, 0.10001211613416672, 0.09827221184968948, 0.09798576682806015, 0.0971941351890564, 0.09709979593753815, 0.0987972840666771, 0.09646527469158173, 0.0968237817287445, 0.096774160861969, 0.09827807545661926, 0.09860432893037796, 0.09803856909275055, 0.09906606376171112, 0.0991147831082344, 0.09745143353939056, 0.09833992272615433, 0.09869362413883209, 0.09814396500587463, 0.09879560768604279, 0.09883485734462738, 0.0972638800740242, 0.09753239899873734, 0.09682620316743851, 0.09848794341087341, 0.09771586954593658, 0.09774089604616165, 0.09812328219413757, 0.0986480712890625, 0.09758257865905762, 0.09729801118373871, 0.09659745544195175, 0.09878931194543839, 0.09836744517087936, 0.09736315906047821, 0.0981394574046135, 0.09872881323099136, 0.09766586869955063, 0.09842509031295776, 0.09865553677082062, 0.09877386689186096, 0.09706038236618042, 0.09718244522809982, 0.09841091185808182, 0.0988556370139122, 0.09628169983625412, 0.09782741218805313, 0.09666506201028824, 0.09968724101781845, 0.09779677540063858, 0.09824364632368088, 0.09741346538066864, 0.09776309132575989, 0.09739205241203308]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:42 2016", "state": "available"}], "summary": "a630ad9897b5e3a35a1bd717c924ab17"}