{"content": {"hp_model": {"f0": 64, "f1": 16, "f2": 64, "f3": 32, "nonlin": "leaky_rectify", "nbg1": 8, "nbg3": 1, "nbg2": 2, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.018780583446167454, 0.01023163711664504, 0.01530940852559351, 0.015601284254289144, 0.011365967339548394, 0.017793609172215955, 0.01362620365897682, 0.018271254802855876, 0.010493379711010787, 0.020609222267230793, 0.015358695243678572, 0.021859549374324654, 0.022645405080108046, 0.025069335451906646, 0.014251460903097065, 0.021334736974533367, 0.015521813626465032, 0.014222611469891263, 0.012134446395441765, 0.014739247748915293, 0.02192887076177562, 0.011910822274120778, 0.013868820738988067, 0.013805617667627945, 0.01110204930716355, 0.016759331777122417, 0.013607998193637323, 0.015516953011371709, 0.012696522486443403, 0.009031018561177545, 0.00857566059054487, 0.016717050885298058, 0.015479187756133706, 0.005832369378603811, 0.010951949505197362, 0.010590220434579804, 0.011870036493589248, 0.019755638721937432, 0.009491393816767436, 0.01160406538220715, 0.01255464642658215, 0.010817329633540143, 0.01227433743107302, 0.006056965211731947, 0.008460191942918126, 0.01570160500706675, 0.008391255266720545, 0.009783049858217313, 0.009201384911671726, 0.013099410784702, 0.008564036648953187, 0.015494415105427074, 0.013736966695409004, 0.01036751919963781, 0.010808926035478861, 0.010792728032178187, 0.015737743676480037, 0.007284266677555643, 0.00897386825623329, 0.012398057880412452, 0.0059966803147581665, 0.010704169274275142, 0.009654335522904359, 0.010445103200396336, 0.00857153223500017, 0.006269821930511047, 0.010026703655427897, 0.012154939306223297, 0.008717709275080879, 0.009705168193089685, 0.012470102167203928, 0.010643906294748776, 0.007940398601288236, 0.008275430483845964, 0.011113783792603502, 0.009531719325229558, 0.008913849807920924, 0.0099779723100465, 0.007381381048724007, 0.006325817442732443, 0.007316043680855321, 0.011196974988389126, 0.014878589630374391, 0.01186722613495477, 0.007609614195389884, 0.013807669918868462, 0.009427109608882966, 0.012635282334568586, 0.007749190367046359, 0.0063958706455314466, 0.005762318301958369, 0.008917272601378758, 0.008042761204750299, 0.00818303641709265, 0.010208382163799835, 0.007142981129404509, 0.010844926450810624, 0.0100931630818015, 0.009304158582661308, 0.0107922142531728, 0.01000347232489324, 0.007505658499764526, 0.009083775529314962, 0.007498968415635662, 0.009417558677311508, 0.00882832596598687, 0.007447519165791899, 0.008368561845276366, 0.010913126268341095, 0.007981734106538957, 0.00959970857467507, 0.008249666476724739, 0.009982945874068724, 0.008985420968409706, 0.00953842775038319, 0.008786625594349565, 0.007300859555566086, 0.008118202122854514, 0.010360719611617313, 0.009784246049167233, 0.00711384250601941, 0.012559696946193435, 0.01066318005605427, 0.011544592247389419, 0.00748415499034515, 0.006978250519807151, 0.005893973546302055, 0.008235354893178653, 0.007515863917687413, 0.008692816217462953, 0.012033699406885015, 0.009110044074138414, 0.009495315185819506, 0.008670693420589516, 0.011550078450289911, 0.006569401126901642, 0.00962160259652813, 0.0085816884030739, 0.009137745679530527, 0.009608040902382122, 0.008791068523365043, 0.005046993690274959, 0.012364148047245409, 0.009413001616953413, 0.008842475282571675, 0.010427006826683865, 0.008871741129308719, 0.003494646774546202, 0.00496480527965508, 0.0059470745074784274, 0.007837753771908068, 0.009459880202473204, 0.00805215403531939, 0.008396884166599372, 0.0056249261429374645, 0.00921463009570911, 0.009266399522935604, 0.00467060922275565, 0.008368105242457997, 0.011484676006073521, 0.010678873132412526, 0.011330808708608637, 0.008638332267203865, 0.008141992583296153, 0.00848685490150008, 0.007891128060683693, 0.006020298983197029, 0.004883598965331085, 0.010273396161374172, 0.007132611104616243, 0.006964675114268481, 0.007235295355983827, 0.00456056471446473, 0.0033931930227003447, 0.006989962455113544, 0.0066862818327297424, 0.005759566137492656, 0.008020460269199024, 0.006077244535380848, 0.008570168294135821, 0.008378330511797897, 0.008442928080119799, 0.010158672593521196, 0.009797445399476412, 0.007908704837991649, 0.007392601797362269, 0.010399606153790706, 0.00853504521166558, 0.00812667870655139, 0.0057210949355670625, 0.00717335916965551, 0.008197626685599606, 0.005350598983183383, 0.007214621094429971, 0.009205581612776919, 0.009810017017194163, 0.005277672022576681, 0.0043845881665385885, 0.008018008818160265, 0.009096396575465392, 0.0060982194199814795, 0.009497092318484592, 0.00601636705992443, 0.008048665543390824, 0.010060858696183596, 0.008085861484278142, 0.004828413084991993, 0.011968929946667503, 0.005790324625818808, 0.008307037949228305, 0.007055545150117717, 0.0060056868657505005, 0.006710457554264065, 0.003298582192127179, 0.00870552912186739, 0.007602745876874584, 0.009129300656414707, 0.010146185375653007, 0.008634850381891525, 0.008003609604902349, 0.0073295185363471, 0.007405379488445151, 0.008447227988694276, 0.005757589625167128, 0.008500478315949326, 0.006952261647571985, 0.008378680067220886, 0.008666944020007472, 0.004669321349350197, 0.007797687324171761, 0.00926302972749801, 0.0066553012217200926, 0.007685670221327095, 0.00953573389979795, 0.008774009115271932, 0.006474938371473711, 0.00971335182370224, 0.005764994675484774, 0.0059264087179110945, 0.009355697218309464, 0.0052350231794677906, 0.01355112537175257, 0.0082521201436297, 0.009178906277532106, 0.008811595463211258, 0.008077566605346887, 0.010843406224616317, 0.005498228351850071, 0.008137316611261781, 0.00840651661482722, 0.00705251755586009, 0.012825191738918103, 0.011460554026476147, 0.007736213322078499, 0.01007972822672342, 0.007687698415513315, 0.002474812594951858, 0.006725590387173443, 0.008211203774408118, 0.00785404154744468, 0.006417566113143484, 0.010362319523163327, 0.00626735837785294, 0.005680234811680583], "moving_avg_accuracy_train": [0.03487249172319121, 0.07708898038079087, 0.11765031582370107, 0.15621273368646754, 0.1926526777008496, 0.22233132373659706, 0.2564109523563851, 0.27415570997812017, 0.30126769037150414, 0.3318360471014726, 0.3602172098620248, 0.3908907783045783, 0.4056250922642183, 0.42937226978798104, 0.4559343175069496, 0.47847744300754663, 0.5038777080910204, 0.5316921178028671, 0.5519408675628166, 0.5731029373679968, 0.5881011847499218, 0.6097270659230931, 0.6239536011520279, 0.6420435013958137, 0.6549999535010643, 0.6551232097832485, 0.6710302822042407, 0.6832177843917162, 0.6909135559594659, 0.7014914416368212, 0.710516498342925, 0.7213914487877945, 0.7300654102501464, 0.7425234087231015, 0.748983940451382, 0.7575112908864486, 0.7685197731338594, 0.7725550906305972, 0.7821474043630802, 0.7890301904010246, 0.7979792421492665, 0.805073534850176, 0.8093240558691266, 0.8143055563303405, 0.8227589271058059, 0.8252223077087174, 0.8295919333654942, 0.8335551478304859, 0.8304407527814258, 0.8333775802810609, 0.8408303764530748, 0.8431723810569829, 0.8494180849098025, 0.8503223205656477, 0.8559133040925547, 0.8627309035524853, 0.8677343595473659, 0.8713611411832993, 0.8747436470008314, 0.879910402611518, 0.885283567892079, 0.8888060320065717, 0.8878403867585724, 0.8909304937257106, 0.8923283066985623, 0.8883674247124916, 0.8927276933615821, 0.8977583454909093, 0.8971207134611983, 0.9004753367556284, 0.9025368608504605, 0.9065868846214499, 0.9095110017379316, 0.9115405296499174, 0.9088549642495252, 0.9108339105675128, 0.9137239861382072, 0.916962072828004, 0.9204390368607259, 0.9229009867818423, 0.9254795009739516, 0.9223346401161282, 0.9174745906774666, 0.9120266879208587, 0.9154832604597529, 0.9186222217257469, 0.9181809574712232, 0.9192507722480489, 0.923040517864929, 0.926814012134407, 0.9284734871091354, 0.9314572547292019, 0.9341542713313093, 0.9367861633196255, 0.9376716374615464, 0.9404214368475439, 0.9426823065556466, 0.9428131529061266, 0.9422754029013666, 0.9437441203112575, 0.9396959897435408, 0.9411622678513665, 0.9428097280817337, 0.9442808525938353, 0.9456351276380693, 0.9474003130504806, 0.9489798595705063, 0.9499432889765972, 0.9510614554646887, 0.9508404872207299, 0.9494929201916248, 0.9484659775749171, 0.9497317670567204, 0.9514847808272388, 0.9530183514421431, 0.9542637784622422, 0.9512441855804883, 0.9509766460022291, 0.9515613603068143, 0.9529851106214171, 0.9546594721021879, 0.9564244168551014, 0.9582010599910198, 0.9583075456193634, 0.9591100837276928, 0.9602973256894566, 0.9616029725847967, 0.9629455736513447, 0.9626636744684269, 0.9634537407263646, 0.9647437263632612, 0.9659862017912577, 0.966804485480026, 0.9655483964166117, 0.9661104443487786, 0.9637848330188287, 0.9645859802598122, 0.9653511906040784, 0.9651170481782682, 0.9659524206664122, 0.9665275806450367, 0.9672172135400661, 0.9676937960170395, 0.9687666783200974, 0.9697183935976299, 0.9684663518165398, 0.9683762782408751, 0.9693228917477678, 0.970290993197991, 0.9702741137067725, 0.9708960850361229, 0.9710884857206336, 0.9715382669473889, 0.97222444910624, 0.9724746755861199, 0.9730160275584695, 0.9738822435895365, 0.974752554869887, 0.9746709157138784, 0.9716425368247534, 0.9720578392803826, 0.9723945172559527, 0.9734368887065756, 0.973600712409746, 0.9735087715616747, 0.9733609927293814, 0.973604557741004, 0.9740654375323797, 0.9748476389053414, 0.9755935088683971, 0.9763461720434807, 0.9767421898462847, 0.9773845991723797, 0.9778929770527608, 0.9783203262593987, 0.9787374926287061, 0.9789966488717878, 0.9791508344310377, 0.9796244228629339, 0.9794648230980968, 0.9797745152299631, 0.9800021569724706, 0.9790747231193081, 0.9790073317586048, 0.9791140541994386, 0.9796262699842752, 0.9798314978215804, 0.9802766195418218, 0.979498450741248, 0.9795769876231033, 0.9795221127810587, 0.980191124307724, 0.9804514378067227, 0.9797627440248968, 0.9802147411248066, 0.9804657174956685, 0.9810450188484917, 0.9814152193445949, 0.9809904733768204, 0.9812662191189188, 0.9812726108594356, 0.9818038110080344, 0.9816773524512971, 0.9819401778085576, 0.9793403635218416, 0.9802856414922858, 0.9808853476918944, 0.9787932671608924, 0.979625952199602, 0.979628923868946, 0.9801734301416121, 0.9809423594488794, 0.9813972666956673, 0.9821856464249101, 0.9825883045871903, 0.9826902802665758, 0.9805758886947908, 0.9811954343479677, 0.9814855612250941, 0.9822419321109365, 0.9825622678427184, 0.9829250108120456, 0.9830841048189732, 0.9836736458989991, 0.9842739152376706, 0.984788617054389, 0.9848100704156261, 0.9849874525109682, 0.9852819910765472, 0.9853750508224823, 0.9856238541104814, 0.9859407830220615, 0.9855936146151119, 0.9858507884583718, 0.9858009740089909, 0.9861327790628721, 0.9849783658495067, 0.9848693092860122, 0.9850246356479242, 0.9854851917629028, 0.9854603112390212, 0.9853169749806137, 0.985490241693285, 0.9857925940120517, 0.9863507404513319, 0.9863415395085888, 0.9849918640899374, 0.9845233487369146, 0.9847689305298898, 0.9853503522090437, 0.9831466647525358, 0.9801800964858001, 0.9809785860931725, 0.9815811495434252, 0.9823512130855298, 0.9828372599317386, 0.9832630763492791, 0.9839439301726846, 0.9844660899078155, 0.984977850299186, 0.98548493762761, 0.9856995007470011, 0.9858368760806621, 0.986223255696433], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 411971891, "moving_var_accuracy_train": [0.010944816110856357, 0.02589042173096589, 0.038108376954079856, 0.04768107990147536, 0.0548637975892695, 0.05730481610497924, 0.06202712427624541, 0.05865829965610799, 0.059408005018158336, 0.06187702441487797, 0.0629387355701588, 0.06511267222214306, 0.06055530507067994, 0.05957513052671786, 0.059967498885268956, 0.058544481562763075, 0.05849659460328342, 0.059609707631521305, 0.05733884366993869, 0.05563545808889867, 0.05209643910077357, 0.05109590381934136, 0.047807862178988296, 0.04597227637856056, 0.0428858756011054, 0.03859742476999474, 0.037014996870055494, 0.034650314069177436, 0.031718306762466564, 0.029553501074848703, 0.02733121580429933, 0.025662475148474716, 0.02377336610068055, 0.02279284502418186, 0.020889206753672743, 0.019454727427287356, 0.018599934817082664, 0.016886495421069698, 0.01602595822364415, 0.014849717094076849, 0.014085515129403573, 0.01312992451679881, 0.011979534425511801, 0.011004919104566295, 0.010547562491317098, 0.009547420438138594, 0.008764521049747996, 0.008029432564832876, 0.007313784417044075, 0.00666003057720319, 0.006493925056517132, 0.00589389742094796, 0.005655587028407296, 0.005097387104658282, 0.004868980265375762, 0.004800399200402596, 0.004545670427396703, 0.004209485289969728, 0.003891508871423496, 0.003742616256146192, 0.0036281927767216108, 0.0033770432799904507, 0.0030477311886962597, 0.0028288969194418456, 0.0025635921574613113, 0.0024484302166833953, 0.002374694679245224, 0.002364992358937441, 0.0021321522944915184, 0.0020202185420701707, 0.0018564456222053148, 0.001818425292894995, 0.0017135369118036132, 0.001579254072533014, 0.0014862390189577688, 0.0013728611738272846, 0.0013107478876834705, 0.0012740399476108798, 0.0012554394628133702, 0.0011844462932587992, 0.001125840282883098, 0.0011022676029304134, 0.001204621567553481, 0.0013512762108072332, 0.0013236796331762472, 0.001279989370323323, 0.0011537428605718743, 0.001048669107425117, 0.0010730617432485722, 0.0010939088999397607, 0.0010093027246715325, 0.0009884982750993968, 0.0009551135345578485, 0.0009219438800455253, 0.0008368060721450662, 0.0008211780348996473, 0.0007850640179428324, 0.0007067117030554546, 0.000638643108358483, 0.0005941929749936879, 0.0006822599273338636, 0.0006333836780058773, 0.0005944724371010646, 0.0005545030593619147, 0.0005155593014846393, 0.0004920462871978801, 0.0004652963633584219, 0.00042712049300726307, 0.0003956611103623544, 0.00035653444200966274, 0.0003372244298900771, 0.00031299348714316344, 0.0002961141455390402, 0.00029416024650178173, 0.000285910771329688, 0.0002712794903582555, 0.000326213011866281, 0.0002942359075130691, 0.00026788933412364054, 0.00025934398533626087, 0.00025864096411723606, 0.0002608121375330401, 0.0002631390712713909, 0.00023692721684564534, 0.00021903110189896982, 0.00020981388299102683, 0.00020417491902972415, 0.00019998062574181467, 0.00018069776751160062, 0.00016824583298782528, 0.00016639781617963807, 0.00016365174126425038, 0.00015331286089556345, 0.0001521814124230654, 0.00013980635208323572, 0.00017450192939683152, 0.0001628282685727676, 0.0001518153635542374, 0.0001371272312788922, 0.00012969513289653442, 0.00011970290061598365, 0.0001120129523235446, 0.0001028558348074136, 0.00010292993925260674, 0.00010078880305274547, 0.00010481840034182725, 9.440957954894171e-05, 9.303331577693031e-05, 9.21649679605561e-05, 8.295103541951464e-05, 7.813756688836873e-05, 7.065697241013331e-05, 6.541200353659433e-05, 6.310841677906363e-05, 5.736109472225499e-05, 5.426254287173096e-05, 5.558926049685585e-05, 5.684730996951872e-05, 5.122256353871107e-05, 0.00012864001544972263, 0.00011732829907161463, 0.00010661563769755891, 0.00010573291809746604, 9.540117013920367e-05, 8.593713120117994e-05, 7.753996533052781e-05, 7.031988403145546e-05, 6.519958726719726e-05, 6.418617943124654e-05, 6.277445950422092e-05, 6.15955302499399e-05, 5.684744812618542e-05, 5.487691099385195e-05, 5.171525251781353e-05, 4.81873733657587e-05, 4.49348860463133e-05, 4.104585506663631e-05, 3.71552282401032e-05, 3.545827944152626e-05, 3.214170026179826e-05, 2.979071318447697e-05, 2.727802873241626e-05, 3.229142782710171e-05, 2.9103159403868664e-05, 2.6295350577879652e-05, 2.602710061221273e-05, 2.3803456737836302e-05, 2.320631117652841e-05, 2.6335600198554225e-05, 2.375755255500274e-05, 2.140889853410719e-05, 2.3296196485995323e-05, 2.1576444897244467e-05, 2.3687492533650087e-05, 2.315745568522633e-05, 2.1408612365282836e-05, 2.2288061645201128e-05, 2.129269114651662e-05, 2.078710426613017e-05, 1.9392715268085337e-05, 1.7453811430398307e-05, 1.824799266820007e-05, 1.656711930052894e-05, 1.553210188624791e-05, 7.481020062633003e-05, 7.537113453635944e-05, 7.107084881536475e-05, 0.00010335497246760395, 9.925975458406051e-05, 8.933385860302267e-05, 8.306885647147532e-05, 8.00832413404998e-05, 7.39373826350716e-05, 7.213752774889288e-05, 6.638297733486084e-05, 5.983827095404986e-05, 9.409030932816135e-05, 8.813580974267849e-05, 8.007979121189098e-05, 7.722068434325184e-05, 7.042215073843317e-05, 6.45641778207572e-05, 5.833555816604398e-05, 5.5630030514782576e-05, 5.3309936973846635e-05, 5.036320491766051e-05, 4.533102664626974e-05, 4.108110365137455e-05, 3.7753769985757496e-05, 3.4056334034003413e-05, 3.1207828315675504e-05, 2.8991040899066317e-05, 2.7176669934214957e-05, 2.5054248411707302e-05, 2.2571156884840694e-05, 2.1304892540387042e-05, 3.116843209108325e-05, 2.815862888834582e-05, 2.5559902507854275e-05, 2.4912919672467482e-05, 2.242719906943836e-05, 2.036938670926281e-05, 1.860264022181606e-05, 1.756512852160705e-05, 1.861236269857736e-05, 1.6751888344845865e-05, 3.1471313131768274e-05, 3.0299741542755053e-05, 2.7812561141847792e-05, 2.8073765548574095e-05, 6.89725346474498e-05, 0.0001412800267135324, 0.0001328902949199144, 0.000122869009832147, 0.00011591908957883912, 0.00010645335445134206, 9.74398955992305e-05, 9.186796339891953e-05, 8.513502415995586e-05, 7.897861002754183e-05, 7.339498705262099e-05, 6.64698243371844e-05, 5.999268974415198e-05, 5.533702363708695e-05], "duration": 167639.041936, "accuracy_train": [0.3487249172319122, 0.4570373782991879, 0.482702334809893, 0.5032744944513657, 0.5206121738302879, 0.4894391380583241, 0.5631276099344776, 0.4338585285737357, 0.54527551391196, 0.6069512576711886, 0.6156476747069952, 0.66695289428756, 0.5382339179009782, 0.6430968675018457, 0.694992746977667, 0.68136557251292, 0.732480093842285, 0.7820218052094868, 0.7341796154023624, 0.763561565614618, 0.7230854111872462, 0.8043599964816353, 0.75199241821244, 0.8048526035898856, 0.7716080224483205, 0.6562325163229051, 0.814193933993171, 0.792905304078996, 0.7601755000692137, 0.7966924127330195, 0.7917420086978589, 0.8192660027916205, 0.8081310634113142, 0.8546453949796974, 0.8071287260059062, 0.8342574448020488, 0.8675961133605574, 0.8088729481012367, 0.8684782279554264, 0.8509752647425249, 0.878520707883444, 0.8689221691583611, 0.8475787450396824, 0.8591390604812662, 0.8988392640849945, 0.8473927331349206, 0.8689185642764857, 0.8692240780154117, 0.8024111973398856, 0.8598090277777777, 0.9079055420011997, 0.8642504224921558, 0.9056294195851791, 0.858460441468254, 0.9062321558347176, 0.9240892986918604, 0.912765463501292, 0.9040021759067, 0.9051861993586194, 0.9264112031076966, 0.9336420554171282, 0.9205082090370063, 0.879149579526578, 0.9187414564299556, 0.9049086234542267, 0.8527194868378553, 0.931970111203396, 0.9430342146548542, 0.8913820251937985, 0.9306669464055003, 0.9210905777039498, 0.9430370985603543, 0.935828055786268, 0.9298062808577889, 0.8846848756459949, 0.9286444274294019, 0.9397346662744556, 0.9461048530361758, 0.9517317131552234, 0.94505853607189, 0.9486861287029347, 0.894030892395718, 0.8737341457295128, 0.8629955631113879, 0.9465924133098007, 0.9468728731196937, 0.9142095791805095, 0.9288791052394795, 0.9571482284168512, 0.9607754605597084, 0.9434087618816908, 0.9583111633098007, 0.9584274207502769, 0.9604731912144703, 0.9456409047388336, 0.9651696313215209, 0.9630301339285714, 0.9439907700604466, 0.9374356528585271, 0.9569625770002769, 0.9032628146340901, 0.9543587708217978, 0.9576368701550388, 0.9575209732027501, 0.9578236030361758, 0.9632869817621816, 0.9631957782507383, 0.9586141536314139, 0.9611249538575121, 0.9488517730251015, 0.9373648169296788, 0.9392234940245479, 0.9611238723929494, 0.9672619047619048, 0.9668204869762828, 0.9654726216431341, 0.9240678496447029, 0.9485687897978959, 0.9568237890480805, 0.9657988634528424, 0.9697287254291252, 0.9723089196313216, 0.9741908482142857, 0.9592659162744556, 0.9663329267026578, 0.9709825033453304, 0.9733537946428571, 0.9750289832502769, 0.9601265818221669, 0.9705643370478036, 0.9763535970953304, 0.9771684806432264, 0.9741690386789406, 0.9542435948458842, 0.9711688757382798, 0.9428543310492802, 0.9717963054286637, 0.9722380837024732, 0.9630097663459765, 0.9734707730597084, 0.9717040204526578, 0.9734239095953304, 0.9719830383098007, 0.9784226190476191, 0.9782838310954227, 0.9571979757867294, 0.967565616059893, 0.9778424133098007, 0.97900390625, 0.9701221982858066, 0.9764938270002769, 0.9728200918812293, 0.9755862979881875, 0.9784000885358989, 0.9747267139050388, 0.9778881953096161, 0.98167818786914, 0.9825853563930418, 0.9739361633098007, 0.9443871268226283, 0.9757955613810447, 0.9754246190360835, 0.9828182317621816, 0.9750751257382798, 0.972681303929033, 0.9720309832387413, 0.9757966428456073, 0.9782133556547619, 0.9818874512619971, 0.9823063385358989, 0.9831201406192323, 0.9803063500715209, 0.9831662831072352, 0.9824683779761905, 0.98216646911914, 0.9824919899524732, 0.9813290550595238, 0.9805385044642857, 0.98388671875, 0.9780284252145626, 0.9825617444167589, 0.9820509326550388, 0.9707278184408453, 0.9784008095122739, 0.9800745561669435, 0.9842362120478036, 0.9816785483573275, 0.9842827150239941, 0.9724949315360835, 0.9802838195598007, 0.9790282392026578, 0.9862122280477114, 0.9827942592977114, 0.9735644999884644, 0.9842827150239941, 0.9827245048334257, 0.9862587310239018, 0.9847470238095238, 0.9771677596668512, 0.9837479307978036, 0.9813301365240864, 0.9865846123454227, 0.9805392254406607, 0.9843056060239018, 0.955942034941399, 0.9887931432262828, 0.9862827034883721, 0.9599645423818751, 0.9871201175479882, 0.9796556688930418, 0.9850739865956073, 0.9878627232142857, 0.9854914319167589, 0.9892810639880952, 0.9862122280477114, 0.9836080613810447, 0.9615463645487264, 0.9867713452265596, 0.9840967031192323, 0.989049270083518, 0.985445289428756, 0.9861896975359912, 0.9845159508813216, 0.9889795156192323, 0.9896763392857143, 0.9894209334048542, 0.9850031506667589, 0.9865838913690477, 0.9879328381667589, 0.9862125885358989, 0.9878630837024732, 0.9887931432262828, 0.9824690989525655, 0.9881653530477114, 0.9853526439645626, 0.9891190245478036, 0.9745886469292175, 0.9838878002145626, 0.9864225729051311, 0.9896301967977114, 0.9852363865240864, 0.9840269486549464, 0.9870496421073275, 0.9885137648809523, 0.9913740584048542, 0.9862587310239018, 0.9728447853220746, 0.9803067105597084, 0.9869791666666666, 0.9905831473214286, 0.9633134776439645, 0.9534809820851791, 0.9881649925595238, 0.9870042205956996, 0.9892817849644703, 0.9872116815476191, 0.9870954241071429, 0.9900716145833334, 0.9891655275239941, 0.9895836938215209, 0.9900487235834257, 0.9876305688215209, 0.9870732540836102, 0.9897006722383721], "end": "2016-01-26 20:01:41.469000", "learning_rate_per_epoch": [0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695, 0.002001108368858695], "accuracy_valid": [0.3503873894013554, 0.4543236422251506, 0.48675610645707834, 0.4991469785391566, 0.5182914274284638, 0.4930434629141566, 0.5624117564006024, 0.43926928416792166, 0.5343547039721386, 0.5992593420557228, 0.5973665168486446, 0.6549763507153614, 0.5407244211219879, 0.6224541721573795, 0.6745076007153614, 0.6693409379706325, 0.7017307511295181, 0.7465526167168675, 0.7059620317206325, 0.7341926298945783, 0.6995849609375, 0.7629409238516567, 0.719482421875, 0.764660203313253, 0.7354942229856928, 0.6297166203878012, 0.7679869870105422, 0.7545386624623494, 0.7218120528990963, 0.7506412368222892, 0.7476100691829819, 0.7733992611069277, 0.7599200512989458, 0.7966838055346386, 0.7573153943900602, 0.7766539791980422, 0.808129000376506, 0.7595950207078314, 0.8055449336408133, 0.7913538921310241, 0.8121985010353916, 0.8070406626506024, 0.7915877376694277, 0.7965720303087349, 0.8256159403237951, 0.7863901896649097, 0.8014651378953314, 0.8023711055158133, 0.7472541533320783, 0.7870608410203314, 0.828880953501506, 0.7963087702371988, 0.8326563088290663, 0.7899493481739458, 0.8271219644201807, 0.8377626717808735, 0.8338873070406627, 0.8211596385542168, 0.8295530755835843, 0.8454736916415663, 0.8435602762612951, 0.8368464090737951, 0.8072642131024097, 0.8325636530496988, 0.8210066829819277, 0.7823809887989458, 0.8399599374058735, 0.8462561182228916, 0.8154223338667168, 0.8418615869728916, 0.8370405449924698, 0.8481783579631024, 0.8446794992469879, 0.8379656320594879, 0.807640719126506, 0.8400717126317772, 0.8444868340549698, 0.8508550804781627, 0.8508139001317772, 0.8488298898719879, 0.8503359139683735, 0.8122999811746988, 0.7917509883283133, 0.7862166439194277, 0.8465620293674698, 0.8477018425263554, 0.8277014307228916, 0.8349447595067772, 0.8542730492281627, 0.8582704842808735, 0.8438558923192772, 0.8522581537085843, 0.8578130882906627, 0.8555746423192772, 0.8477312570594879, 0.8538347726844879, 0.8543848244540663, 0.8421572030308735, 0.8420145425451807, 0.857386577560241, 0.8146296121987951, 0.8540583231362951, 0.8517095726656627, 0.8514139566076807, 0.8518110528049698, 0.8531935358621988, 0.8601530144013554, 0.8552187264683735, 0.8571115516754518, 0.8451574854103916, 0.8439676675451807, 0.8376906061746988, 0.8554628670933735, 0.8601324242281627, 0.8621267295745482, 0.8582910744540663, 0.8214037791792168, 0.8450868905308735, 0.8520551934299698, 0.8565306146460843, 0.8587793557040663, 0.8643239951995482, 0.8612516472138554, 0.8542024543486446, 0.8585661003388554, 0.8616884530308735, 0.8625944206513554, 0.8654226280120482, 0.8539980233433735, 0.8653108527861446, 0.8693700583584337, 0.8641504494540663, 0.8645784309111446, 0.8506006447665663, 0.8602853798004518, 0.8451986657567772, 0.8626959007906627, 0.8622076195406627, 0.8593294074736446, 0.8687185264495482, 0.8581690041415663, 0.8627973809299698, 0.8677728492093373, 0.8643239951995482, 0.863926899002259, 0.8505094597138554, 0.8575277673192772, 0.8633062523531627, 0.8658594338290663, 0.8566629800451807, 0.8636621682040663, 0.8594411826995482, 0.8653711525790663, 0.8652181970067772, 0.8641916298004518, 0.8674169333584337, 0.868321430252259, 0.8706819465361446, 0.8655446983245482, 0.8391863351844879, 0.862706195877259, 0.8607030661709337, 0.8696436135165663, 0.8623502800263554, 0.8600809487951807, 0.859776508377259, 0.8642828148531627, 0.8688303016754518, 0.8718114646084337, 0.8701833701995482, 0.8702848503388554, 0.867955219314759, 0.8706613563629518, 0.8680875847138554, 0.869053852127259, 0.8688200065888554, 0.869420063064759, 0.8701936652861446, 0.874110210372741, 0.8666330360504518, 0.868443500564759, 0.8698274543486446, 0.8627164909638554, 0.8683111351656627, 0.8667139260165663, 0.868077289627259, 0.8738866599209337, 0.8703363257718373, 0.8605898202183735, 0.8672022072665663, 0.8656667686370482, 0.8706201760165663, 0.8712614128388554, 0.858677875564759, 0.8692171027861446, 0.8708246070218373, 0.8743337608245482, 0.8740793251129518, 0.8656770637236446, 0.8685449807040663, 0.8675993034638554, 0.8740999152861446, 0.8632856621799698, 0.871546733810241, 0.8516066217996988, 0.8749029320406627, 0.8707422463290663, 0.8505403449736446, 0.8746087867093373, 0.8660329795745482, 0.8689214867281627, 0.871251117752259, 0.8716585090361446, 0.878260600997741, 0.8716585090361446, 0.8717702842620482, 0.8523508094879518, 0.8738969550075302, 0.8707128317959337, 0.8749235222138554, 0.866246234939759, 0.8742013954254518, 0.869542133377259, 0.8751882530120482, 0.8764398413968373, 0.8733469032379518, 0.8672933923192772, 0.8680361092808735, 0.8730012824736446, 0.8694612434111446, 0.8700612998870482, 0.8716379188629518, 0.8704583960843373, 0.871302593185241, 0.8622076195406627, 0.8702951454254518, 0.8554834572665663, 0.8660123894013554, 0.869664203689759, 0.8697053840361446, 0.8653711525790663, 0.8723615163780121, 0.8738866599209337, 0.8745264260165663, 0.8742219855986446, 0.868861186935241, 0.8578542686370482, 0.8603971550263554, 0.8717805793486446, 0.8773149237575302, 0.8510065653237951, 0.8445986092808735, 0.8715673239834337, 0.8669889519013554, 0.8752397284450302, 0.8715364387236446, 0.8695627235504518, 0.874964702560241, 0.8715761483433735, 0.8717805793486446, 0.8737131141754518, 0.8700098244540663, 0.8703054405120482, 0.872157085372741], "accuracy_test": 0.2551319355867347, "start": "2016-01-24 21:27:42.427000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0, 211.0, 212.0, 213.0, 214.0, 215.0, 216.0, 217.0, 218.0, 219.0, 220.0, 221.0, 222.0, 223.0, 224.0, 225.0, 226.0, 227.0, 228.0, 229.0, 230.0, 231.0, 232.0, 233.0, 234.0, 235.0, 236.0, 237.0, 238.0, 239.0, 240.0, 241.0, 242.0, 243.0, 244.0, 245.0, 246.0, 247.0, 248.0, 249.0, 250.0, 251.0, 252.0, 253.0, 254.0, 255.0, 256.0, 257.0, 258.0, 259.0, 260.0, 261.0, 262.0, 263.0], "accuracy_train_last": 0.9897006722383721, "batch_size_eval": 1024, "accuracy_train_std": [0.014931804157649315, 0.014226419589801888, 0.015024757583305664, 0.014515543354769551, 0.019275398016461026, 0.01614367532321925, 0.01563418536589948, 0.016560490710634417, 0.020011623828357104, 0.015887822615707772, 0.016130529453407726, 0.015544820151721667, 0.016926483922629472, 0.019214587945827987, 0.015938272861298196, 0.017314615456362047, 0.01995775286517219, 0.018443187690167194, 0.018237256935834348, 0.02030192867264921, 0.021288394047348717, 0.01943751254223829, 0.01635383815773465, 0.01856673833795526, 0.01774509505443505, 0.02011136250349804, 0.01983934512152377, 0.01699724159643784, 0.017508530346516043, 0.017379704927899142, 0.017905921028655875, 0.01668742937503493, 0.016691362625520484, 0.019343949890156156, 0.018078283878236002, 0.019737338921961868, 0.017912800996699087, 0.019751827029731463, 0.017392110380144505, 0.017234200844292138, 0.01865916798626029, 0.017990512853761416, 0.019454084699890028, 0.015222550912709642, 0.0185876524746951, 0.018439738051867976, 0.019876032537252123, 0.018707870786385528, 0.017812758648158847, 0.017820263006091925, 0.019129845295911132, 0.0165021343274185, 0.017028858601369105, 0.016510317599266047, 0.01859364652490103, 0.017031199609549096, 0.016530290490567336, 0.017122528226950127, 0.017793799268917537, 0.01620807891161258, 0.01715376246455533, 0.015810886022089802, 0.018060525575850674, 0.017431464808166614, 0.01648323091785338, 0.018442170533108107, 0.013369262846390985, 0.0149970002717187, 0.01397877453595849, 0.016520434341555017, 0.014248651330129676, 0.013956734393164659, 0.015180213969651014, 0.016595392574420866, 0.015866335091389187, 0.016074957731600423, 0.014860001477652651, 0.014470245811334131, 0.012885625433727729, 0.015192716735390391, 0.014374576785314546, 0.015090837969208898, 0.019228525966540455, 0.015052387936272173, 0.013287392826963246, 0.01367594387924975, 0.012583126326515695, 0.013683337006777567, 0.013102722411123754, 0.012039943864713763, 0.01213223604580675, 0.012143520955361404, 0.012067835303400375, 0.011856231024186922, 0.012917994075629873, 0.009982225761293994, 0.011078449607194098, 0.010983873456802453, 0.013185475854580042, 0.011878798940140258, 0.014604825984709151, 0.011812281299202296, 0.01167544563847963, 0.011659411998622766, 0.010884586840861155, 0.010482283781131214, 0.009752096972008125, 0.010419134226432519, 0.010801657993263257, 0.01195141096514385, 0.014589503162613877, 0.014924165871887532, 0.010511042159883755, 0.010634347122514974, 0.01026659585291832, 0.010394527923027462, 0.013676135289303547, 0.013633353244960615, 0.011092008698546867, 0.009808385869960828, 0.00829399907166004, 0.008016444256439059, 0.008758376017108297, 0.0097056828860124, 0.009469476315263983, 0.00973002138532641, 0.008912935644253371, 0.008126072549580731, 0.009222736075657241, 0.00800594799306893, 0.007449115645152153, 0.007406403106403587, 0.00820475408061459, 0.01026694773002403, 0.008120414363446096, 0.010996139638242586, 0.008939222540690484, 0.00809667525302932, 0.00908111566083605, 0.00891127896369375, 0.007814121074293886, 0.00834355431609779, 0.007782052491362932, 0.007128012839387218, 0.006638352514855984, 0.009720539473291694, 0.00809821347045016, 0.008112680373160453, 0.006631078188781296, 0.009678046137375355, 0.007853762536240398, 0.008089325740711244, 0.008450179505533043, 0.007671026182510906, 0.0074559645659653204, 0.008101129898078476, 0.005622334794987079, 0.006181270267028818, 0.008174540914207362, 0.010350796210090036, 0.00743999240842079, 0.008013320480602758, 0.0046425246038210724, 0.007797680470666406, 0.007070681596383982, 0.006853904974359994, 0.007448196022747138, 0.00730944099219594, 0.005591514465467036, 0.0057523737293285315, 0.004959856021834024, 0.006678670468218362, 0.005989858392140588, 0.00652998494143521, 0.006110948835638029, 0.006186021332022514, 0.007192681637487127, 0.006517595757445867, 0.005320121363188712, 0.005433169497300815, 0.005908836811558732, 0.006348655252469271, 0.0074368291110061165, 0.006379163798036308, 0.006003810129390891, 0.005318834052063656, 0.00632263026341586, 0.004674621519530395, 0.007292899274914045, 0.005297234551219248, 0.006396507356526355, 0.004631705346355562, 0.0059790400709980466, 0.006423494771635122, 0.006076941925229549, 0.005854457090091418, 0.005620357993408465, 0.005109828619206546, 0.007120823559554186, 0.00508872878970731, 0.007200920339343376, 0.004656014190121315, 0.006068350093961925, 0.005976823640988583, 0.010986939961661448, 0.004685296065483056, 0.004985562072733153, 0.00851685453713831, 0.00499117964461557, 0.006896913621201916, 0.0046870127528454925, 0.004817941740301121, 0.00556428825425811, 0.0049908440420012015, 0.005251995609106152, 0.0059201533247696895, 0.008472919827292324, 0.004748660749776967, 0.005555696738574771, 0.0043868250593690926, 0.005738571012794336, 0.004965589379042319, 0.005276453175653602, 0.00398401992712309, 0.004404300778479008, 0.00394128249491872, 0.005642327292210735, 0.005524516384580031, 0.004043252485440686, 0.005196038923450574, 0.004935102536011238, 0.00376044577666491, 0.005849234320021503, 0.004532441395622887, 0.005583226582725948, 0.004653240562911142, 0.007150242609271012, 0.005612916544163284, 0.005341111136401001, 0.00392993035568243, 0.0053642305083935376, 0.0052770749530519365, 0.004809471446387781, 0.004858615285196792, 0.0039755223460428805, 0.005803225384010773, 0.005547825249912851, 0.006493252210361416, 0.004463316797136096, 0.004433480198169776, 0.007776623903504675, 0.009897817701282612, 0.004536662664959819, 0.0045393906330682045, 0.003645905249368733, 0.004816370513714014, 0.004622604016013542, 0.004484885990525597, 0.004165231369327389, 0.005086886610204394, 0.0039025582250527373, 0.004239805494912847, 0.004383652805904998, 0.004482345149646261], "accuracy_test_std": 0.010813066059429279, "error_valid": [0.6496126105986446, 0.5456763577748494, 0.5132438935429217, 0.5008530214608433, 0.4817085725715362, 0.5069565370858433, 0.43758824359939763, 0.5607307158320783, 0.4656452960278614, 0.40074065794427716, 0.4026334831513554, 0.3450236492846386, 0.45927557887801207, 0.3775458278426205, 0.3254923992846386, 0.33065906202936746, 0.2982692488704819, 0.25344738328313254, 0.29403796827936746, 0.26580737010542166, 0.3004150390625, 0.23705907614834332, 0.280517578125, 0.23533979668674698, 0.2645057770143072, 0.3702833796121988, 0.23201301298945776, 0.24546133753765065, 0.27818794710090367, 0.24935876317771077, 0.2523899308170181, 0.2266007388930723, 0.2400799487010542, 0.20331619446536142, 0.24268460560993976, 0.22334602080195776, 0.19187099962349397, 0.24040497929216864, 0.19445506635918675, 0.20864610786897586, 0.1878014989646084, 0.19295933734939763, 0.2084122623305723, 0.2034279696912651, 0.17438405967620485, 0.2136098103350903, 0.19853486210466864, 0.19762889448418675, 0.25274584666792166, 0.21293915897966864, 0.17111904649849397, 0.20369122976280118, 0.16734369117093373, 0.2100506518260542, 0.1728780355798193, 0.1622373282191265, 0.16611269295933728, 0.1788403614457832, 0.17044692441641573, 0.15452630835843373, 0.15643972373870485, 0.16315359092620485, 0.1927357868975903, 0.16743634695030118, 0.1789933170180723, 0.2176190112010542, 0.1600400625941265, 0.1537438817771084, 0.1845776661332832, 0.1581384130271084, 0.16295945500753017, 0.15182164203689763, 0.15532050075301207, 0.16203436794051207, 0.19235928087349397, 0.15992828736822284, 0.15551316594503017, 0.14914491952183728, 0.14918609986822284, 0.15117011012801207, 0.1496640860316265, 0.18770001882530118, 0.20824901167168675, 0.2137833560805723, 0.15343797063253017, 0.1522981574736446, 0.1722985692771084, 0.16505524049322284, 0.14572695077183728, 0.1417295157191265, 0.15614410768072284, 0.14774184629141573, 0.14218691170933728, 0.14442535768072284, 0.15226874294051207, 0.14616522731551207, 0.14561517554593373, 0.1578427969691265, 0.1579854574548193, 0.14261342243975905, 0.18537038780120485, 0.14594167686370485, 0.14829042733433728, 0.1485860433923193, 0.14818894719503017, 0.14680646413780118, 0.1398469855986446, 0.1447812735316265, 0.14288844832454817, 0.1548425145896084, 0.1560323324548193, 0.16230939382530118, 0.1445371329066265, 0.13986757577183728, 0.13787327042545183, 0.14170892554593373, 0.1785962208207832, 0.1549131094691265, 0.14794480657003017, 0.14346938535391573, 0.14122064429593373, 0.13567600480045183, 0.1387483527861446, 0.1457975456513554, 0.1414338996611446, 0.1383115469691265, 0.1374055793486446, 0.13457737198795183, 0.1460019766566265, 0.1346891472138554, 0.13062994164156627, 0.13584955054593373, 0.1354215690888554, 0.14939935523343373, 0.13971462019954817, 0.15480133424322284, 0.13730409920933728, 0.13779238045933728, 0.1406705925263554, 0.13128147355045183, 0.14183099585843373, 0.13720261907003017, 0.13222715079066272, 0.13567600480045183, 0.13607310099774095, 0.1494905402861446, 0.14247223268072284, 0.13669374764683728, 0.13414056617093373, 0.1433370199548193, 0.13633783179593373, 0.14055881730045183, 0.13462884742093373, 0.13478180299322284, 0.13580837019954817, 0.13258306664156627, 0.13167856974774095, 0.1293180534638554, 0.13445530167545183, 0.16081366481551207, 0.13729380412274095, 0.13929693382906627, 0.13035638648343373, 0.1376497199736446, 0.1399190512048193, 0.14022349162274095, 0.13571718514683728, 0.13116969832454817, 0.12818853539156627, 0.12981662980045183, 0.1297151496611446, 0.13204478068524095, 0.12933864363704817, 0.1319124152861446, 0.13094614787274095, 0.1311799934111446, 0.13057993693524095, 0.1298063347138554, 0.12588978962725905, 0.13336696394954817, 0.13155649943524095, 0.1301725456513554, 0.1372835090361446, 0.13168886483433728, 0.13328607398343373, 0.13192271037274095, 0.12611334007906627, 0.12966367422816272, 0.1394101797816265, 0.13279779273343373, 0.13433323136295183, 0.12937982398343373, 0.1287385871611446, 0.14132212443524095, 0.1307828972138554, 0.12917539297816272, 0.12566623917545183, 0.12592067488704817, 0.1343229362763554, 0.13145501929593373, 0.1324006965361446, 0.1259000847138554, 0.13671433782003017, 0.12845326618975905, 0.14839337820030118, 0.12509706795933728, 0.12925775367093373, 0.1494596550263554, 0.12539121329066272, 0.13396702042545183, 0.13107851327183728, 0.12874888224774095, 0.1283414909638554, 0.12173939900225905, 0.1283414909638554, 0.12822971573795183, 0.14764919051204817, 0.12610304499246983, 0.12928716820406627, 0.1250764777861446, 0.13375376506024095, 0.12579860457454817, 0.13045786662274095, 0.12481174698795183, 0.12356015860316272, 0.12665309676204817, 0.13270660768072284, 0.1319638907191265, 0.1269987175263554, 0.1305387565888554, 0.12993870011295183, 0.12836208113704817, 0.12954160391566272, 0.12869740681475905, 0.13779238045933728, 0.12970485457454817, 0.14451654273343373, 0.1339876105986446, 0.13033579631024095, 0.1302946159638554, 0.13462884742093373, 0.12763848362198793, 0.12611334007906627, 0.12547357398343373, 0.1257780144013554, 0.13113881306475905, 0.14214573136295183, 0.1396028449736446, 0.1282194206513554, 0.12268507624246983, 0.14899343467620485, 0.1554013907191265, 0.12843267601656627, 0.1330110480986446, 0.12476027155496983, 0.1284635612763554, 0.13043727644954817, 0.12503529743975905, 0.1284238516566265, 0.1282194206513554, 0.12628688582454817, 0.12999017554593373, 0.12969455948795183, 0.12784291462725905], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "momentum": 0.9096245537841923, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.002001108323913533, "patience_threshold": 1, "do_flip": true, "batch_size": 128, "optimization": "rmsprop", "nb_data_augmentation": 3, "learning_rate_decay_method": "none", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 1.547745300025057e-07, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.04178252534389574}, "accuracy_valid_max": 0.878260600997741, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.872157085372741, "loss_train": [1.530814290046692, 1.1599234342575073, 1.0047271251678467, 0.910099446773529, 0.8386550545692444, 0.7762867212295532, 0.724814772605896, 0.6815173625946045, 0.644830048084259, 0.6131008863449097, 0.5850169062614441, 0.5603308081626892, 0.5369476079940796, 0.5178616046905518, 0.49778643250465393, 0.48202550411224365, 0.4646168351173401, 0.44726261496543884, 0.4356766939163208, 0.42160698771476746, 0.4117670953273773, 0.40055787563323975, 0.38643401861190796, 0.37823253870010376, 0.36532509326934814, 0.3590574860572815, 0.3481558561325073, 0.340437114238739, 0.33179381489753723, 0.3247871696949005, 0.3170340657234192, 0.31240424513816833, 0.3033371865749359, 0.29458165168762207, 0.28926512598991394, 0.28523731231689453, 0.27847835421562195, 0.2720464766025543, 0.2674190104007721, 0.26112085580825806, 0.25559908151626587, 0.2514471709728241, 0.24615581333637238, 0.24271610379219055, 0.23617331683635712, 0.23170022666454315, 0.22856080532073975, 0.22417820990085602, 0.2186429351568222, 0.21890147030353546, 0.2117319405078888, 0.21097241342067719, 0.20526348054409027, 0.2023179531097412, 0.19902975857257843, 0.19467805325984955, 0.19332818686962128, 0.19014503061771393, 0.1884322315454483, 0.1835658699274063, 0.18130357563495636, 0.17738209664821625, 0.17622734606266022, 0.17502368986606598, 0.1710990071296692, 0.16826096177101135, 0.16886402666568756, 0.16304144263267517, 0.16042166948318481, 0.15910854935646057, 0.15681181848049164, 0.15552274882793427, 0.15277904272079468, 0.149717777967453, 0.14946790039539337, 0.14856292307376862, 0.14457355439662933, 0.14384928345680237, 0.14149965345859528, 0.1405324786901474, 0.1381710171699524, 0.13599225878715515, 0.13368608057498932, 0.1357588768005371, 0.13266244530677795, 0.12866748869419098, 0.1291772723197937, 0.12678807973861694, 0.1259019821882248, 0.1248699203133583, 0.12112857401371002, 0.11934320628643036, 0.1197839230298996, 0.11813616752624512, 0.11860105395317078, 0.11658105254173279, 0.11599104851484299, 0.11513157188892365, 0.11459904909133911, 0.1127360612154007, 0.11234988272190094, 0.11142602562904358, 0.1072329580783844, 0.10652197152376175, 0.10920935124158859, 0.10639719665050507, 0.10691095888614655, 0.10447869449853897, 0.10257063060998917, 0.10094574838876724, 0.10103508830070496, 0.10030271112918854, 0.09816155582666397, 0.097288116812706, 0.09795117378234863, 0.09744810312986374, 0.09704484790563583, 0.09640540927648544, 0.09409251064062119, 0.09359503537416458, 0.09236003458499908, 0.09103352576494217, 0.09152928739786148, 0.09080531448125839, 0.09034914523363113, 0.0911157876253128, 0.08873718976974487, 0.08884981274604797, 0.08659235388040543, 0.08735939115285873, 0.08655434846878052, 0.08374856412410736, 0.08373784273862839, 0.08359689265489578, 0.08331653475761414, 0.08312719315290451, 0.08235558122396469, 0.08203726261854172, 0.0825086236000061, 0.07834363728761673, 0.07906534522771835, 0.0801590159535408, 0.07884759455919266, 0.0799313485622406, 0.07756374776363373, 0.07628347724676132, 0.07621310651302338, 0.07606019824743271, 0.07499631494283676, 0.07394876331090927, 0.07415101677179337, 0.07464872300624847, 0.07282040268182755, 0.07393454760313034, 0.07345911115407944, 0.07236834615468979, 0.07386147230863571, 0.07177497446537018, 0.07062361389398575, 0.07204558700323105, 0.06965824216604233, 0.07017725706100464, 0.06952700763940811, 0.06980050355195999, 0.0643312931060791, 0.0683838278055191, 0.06756293028593063, 0.06833939999341965, 0.06780456006526947, 0.06499353796243668, 0.06413073092699051, 0.06697063148021698, 0.06473059952259064, 0.06584908813238144, 0.06474216282367706, 0.06605436652898788, 0.06527134031057358, 0.06479784846305847, 0.06220308318734169, 0.062192969024181366, 0.06233901530504227, 0.06365421414375305, 0.06386852264404297, 0.06356305629014969, 0.061322081834077835, 0.05954219028353691, 0.06016603112220764, 0.059418972581624985, 0.06107693538069725, 0.05929247662425041, 0.059626560658216476, 0.05859755724668503, 0.05918577313423157, 0.05896579101681709, 0.05972797051072121, 0.05790138244628906, 0.06044496223330498, 0.05755682289600372, 0.057036545127630234, 0.056247156113386154, 0.05536495894193649, 0.058088645339012146, 0.058129068464040756, 0.056556884199380875, 0.05760521814227104, 0.055866025388240814, 0.056674372404813766, 0.056501440703868866, 0.05446939915418625, 0.056105226278305054, 0.054004404693841934, 0.052780620753765106, 0.052549149841070175, 0.054558005183935165, 0.0552227720618248, 0.05218292772769928, 0.053977809846401215, 0.05515819787979126, 0.0537518672645092, 0.05458036810159683, 0.0532623827457428, 0.0524899885058403, 0.05340663343667984, 0.05216120928525925, 0.051026199012994766, 0.0492805615067482, 0.05139247328042984, 0.05146047845482826, 0.04989456757903099, 0.051281657069921494, 0.05165817216038704, 0.05053377524018288, 0.0496046282351017, 0.05022524297237396, 0.05064821615815163, 0.05063570663332939, 0.0487077459692955, 0.0505036935210228, 0.049271319061517715, 0.04830487072467804, 0.04895111918449402, 0.04912981763482094, 0.04803423583507538, 0.048113010823726654, 0.047857996076345444, 0.04753762483596802, 0.04714227095246315, 0.04813135415315628, 0.04738221690058708, 0.0459473691880703, 0.046826038509607315, 0.04804889112710953, 0.04835812747478485, 0.04727068170905113, 0.046378958970308304, 0.046813320368528366, 0.04615384340286255, 0.04685533046722412, 0.04689624533057213, 0.0463954359292984, 0.04575682803988457, 0.04628743231296539, 0.04519875720143318, 0.04528538137674332], "accuracy_train_first": 0.3487249172319122, "model": "residualv5", "loss_std": [0.2327096313238144, 0.10240159183740616, 0.09045971184968948, 0.08475665003061295, 0.08488431572914124, 0.08471545577049255, 0.0824919193983078, 0.0839410051703453, 0.08148729801177979, 0.07945568114519119, 0.07606090605258942, 0.07854518294334412, 0.07348615676164627, 0.07359492778778076, 0.07352545112371445, 0.07203971594572067, 0.07012457400560379, 0.07156604528427124, 0.06881115585565567, 0.06502765417098999, 0.06615389138460159, 0.06657754629850388, 0.06293482333421707, 0.06570972502231598, 0.06411527097225189, 0.06362807005643845, 0.06269846856594086, 0.061864908784627914, 0.0610039196908474, 0.06303547322750092, 0.05834505707025528, 0.05930451303720474, 0.056342579424381256, 0.05659690126776695, 0.054946672171354294, 0.05481276288628578, 0.053636226803064346, 0.0515345074236393, 0.054573338478803635, 0.0511409230530262, 0.05124831572175026, 0.051330868154764175, 0.04948516935110092, 0.05083800479769707, 0.05050366371870041, 0.04802471026778221, 0.04844902083277702, 0.048823870718479156, 0.04581863805651665, 0.04857536777853966, 0.04630475491285324, 0.045440923422575, 0.043991029262542725, 0.04314255714416504, 0.04495953023433685, 0.044229570776224136, 0.04195927828550339, 0.04187135398387909, 0.04186838120222092, 0.04182063415646553, 0.041901394724845886, 0.0426645390689373, 0.038710638880729675, 0.04046981409192085, 0.039959173649549484, 0.039783891290426254, 0.03920898586511612, 0.03774136304855347, 0.03700374811887741, 0.035300809890031815, 0.03574918210506439, 0.03578237444162369, 0.03565426543354988, 0.03523338958621025, 0.034344278275966644, 0.037233248353004456, 0.03394656628370285, 0.035526931285858154, 0.03427590802311897, 0.03453116863965988, 0.033504847437143326, 0.031480394303798676, 0.03132665902376175, 0.03460720553994179, 0.03377046808600426, 0.03074554167687893, 0.03028104640543461, 0.02906322106719017, 0.03157208859920502, 0.030794501304626465, 0.02865464985370636, 0.028800111263990402, 0.029106393456459045, 0.028333459049463272, 0.03026018850505352, 0.028726059943437576, 0.030561167746782303, 0.028082270175218582, 0.028602950274944305, 0.028939688578248024, 0.029206879436969757, 0.029942860826849937, 0.027294501662254333, 0.0286960881203413, 0.027125949040055275, 0.026391031220555305, 0.027897102758288383, 0.027301955968141556, 0.025550108402967453, 0.02528056502342224, 0.026298455893993378, 0.024766363203525543, 0.026238882914185524, 0.02472427859902382, 0.026755617931485176, 0.025322679430246353, 0.025507155805826187, 0.02571292780339718, 0.02378375083208084, 0.025631431490182877, 0.02272401563823223, 0.023886149749159813, 0.023267429322004318, 0.026313336566090584, 0.022424407303333282, 0.02504579722881317, 0.022596707567572594, 0.022676436230540276, 0.023498250171542168, 0.022805770859122276, 0.024152005091309547, 0.023108411580324173, 0.023455198854207993, 0.02144361287355423, 0.021501092240214348, 0.022244246676564217, 0.022702936083078384, 0.02208852395415306, 0.023766200989484787, 0.021324429661035538, 0.019820062443614006, 0.02304938994348049, 0.020789584144949913, 0.024223968386650085, 0.02254389598965645, 0.020142965018749237, 0.02039651945233345, 0.019398994743824005, 0.02167116105556488, 0.019277039915323257, 0.021888306364417076, 0.021269656717777252, 0.019402777776122093, 0.021342596039175987, 0.020553376525640488, 0.019422437995672226, 0.02005789242684841, 0.019767679274082184, 0.02048690989613533, 0.019398247823119164, 0.01990666054189205, 0.01954892836511135, 0.019282354041934013, 0.01884032040834427, 0.018371233716607094, 0.020081834867596626, 0.019864052534103394, 0.019546641036868095, 0.018634138628840446, 0.018026575446128845, 0.019111596047878265, 0.019453953951597214, 0.018209092319011688, 0.018375232815742493, 0.019261552020907402, 0.019658301025629044, 0.018860958516597748, 0.020569734275341034, 0.017431849613785744, 0.018415847793221474, 0.017902785912156105, 0.019029289484024048, 0.01913718320429325, 0.018398471176624298, 0.017920663580298424, 0.0181183610111475, 0.018317021429538727, 0.01749376766383648, 0.017379557713866234, 0.017171120271086693, 0.018201734870672226, 0.017442651093006134, 0.017675936222076416, 0.017731962725520134, 0.01729918085038662, 0.019018126651644707, 0.017835460603237152, 0.018274428322911263, 0.016800668090581894, 0.016736701130867004, 0.01840941235423088, 0.01717228628695011, 0.017630979418754578, 0.01743382401764393, 0.017270226031541824, 0.017558753490447998, 0.017698535695672035, 0.016778843477368355, 0.01604948192834854, 0.016320962458848953, 0.015923207625746727, 0.015967993065714836, 0.015759997069835663, 0.01757589913904667, 0.016773635521531105, 0.01597229577600956, 0.016138747334480286, 0.018187623471021652, 0.01710548996925354, 0.016063490882515907, 0.016147734597325325, 0.017008543014526367, 0.016985837370157242, 0.014776171185076237, 0.016390379518270493, 0.01657075621187687, 0.015352281741797924, 0.015361231751739979, 0.013862992636859417, 0.016398396342992783, 0.015683462843298912, 0.01575075462460518, 0.015187758021056652, 0.014757643453776836, 0.016088681295514107, 0.015865620225667953, 0.0148684186860919, 0.016168590635061264, 0.015435616485774517, 0.015764467418193817, 0.01647614687681198, 0.014653481543064117, 0.016023967415094376, 0.015846269205212593, 0.01575341261923313, 0.014520976692438126, 0.014040275476872921, 0.015528776682913303, 0.015022178180515766, 0.014762899838387966, 0.016259027644991875, 0.014893416315317154, 0.01589236781001091, 0.01596502959728241, 0.014813332818448544, 0.015868136659264565, 0.01445753313601017, 0.015227232128381729, 0.016021277755498886, 0.015203770250082016, 0.014291463419795036, 0.015054786577820778, 0.01532803475856781, 0.013949497602880001]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:16 2016", "state": "available"}], "summary": "f58085db106525a5bd56cac52a8b5751"}