{"content": {"hp_model": {"f0": 64, "f1": 16, "f2": 16, "f3": 32, "nonlin": "leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.015603101435145027, 0.014886766705804598, 0.016802116618020432, 0.015002204843422587, 0.013393037099725957, 0.011926411467652186, 0.014884017389025485, 0.02053061122589009, 0.01933085067749736, 0.015667922974949247, 0.01658859861406375, 0.01638896111533142, 0.018427933396471557, 0.019300045726539206, 0.016577717466927083, 0.014359890687814203, 0.01693766260308036, 0.019700617980049604, 0.015538288587224466, 0.018129937057773896, 0.016387029388720505, 0.018371278249479327, 0.01718182017189878, 0.015586494103500279, 0.018945492688733235, 0.01629026744718121, 0.0153407943713697, 0.012678905050188255, 0.016365565771819073, 0.01425140466979176, 0.012543604086059004, 0.015984548468551914, 0.01551815311464199, 0.016403593226851417, 0.014643248799488856, 0.015704546712017892, 0.013440627649780261, 0.016873374845491684, 0.012911147955977987, 0.016250623070360726, 0.01722066946432623, 0.017571552507468308, 0.014758723679272277, 0.01611660870068607, 0.01591029452873695, 0.015962119939270877, 0.01513369011050164, 0.01665412915484746, 0.017350911242035562, 0.016731028850647048, 0.016300327210410102, 0.01769480009460581, 0.016593910821276803, 0.01678485223545183, 0.017768559618309854, 0.01749626476253819, 0.01658118802937142, 0.01814840360767443, 0.01787336723076628, 0.018025758158165227, 0.01710783075136728, 0.017005485427557556, 0.016548196435515396, 0.01715689984856897, 0.016698225790872715, 0.016942266280532362, 0.016988405054805197, 0.016351571973044328, 0.016369694572359587, 0.016647465761400315, 0.016071563368795797, 0.01697741086314317, 0.016672746645787086, 0.01646056453090757, 0.01616619737157075, 0.0158614986393958, 0.016264823413841403, 0.015528682778464082, 0.01627060081106291, 0.01632080042274951, 0.01675378882542158, 0.016101450954630718, 0.016803522549172, 0.01663907881824259, 0.016313494655043073, 0.016755331845142402, 0.016766380643595633, 0.016903018339617944, 0.016952584321523457, 0.016733174810125376, 0.016390804065004972, 0.016518068345093128, 0.01647552186292993, 0.016296951064910168, 0.01683234514850634, 0.016681536397250462, 0.016094876655091236, 0.016644122277795194, 0.016270843560298724, 0.01663907881824259, 0.01586197886004235, 0.01627166557058737, 0.01645284070534824, 0.016547626851057342, 0.016453320822772243, 0.01646056453090757, 0.01603150607115336, 0.016624743810401594, 0.01705167361078666, 0.0157234031821123, 0.01773121226136763, 0.017385421425469918, 0.016786756029091195, 0.016518399841055572, 0.016500494969149272, 0.016712589080278187, 0.016380373281773462, 0.016285826315472443, 0.01647566897038089, 0.016847478586545214, 0.016372852862601036, 0.01560387475590924, 0.016748215626201542, 0.01751364367574341, 0.0176083119179501, 0.01641753550393815, 0.01646337279200101, 0.016453320822772243, 0.016694419302948592, 0.016729848455602252, 0.01644498224703979, 0.015586243957106212, 0.016838956706282007, 0.016779653137092197, 0.016760902678135285, 0.016067608422441555, 0.01633698469214701, 0.016737208469074883, 0.01632618413250576, 0.01710585773826905, 0.016787932422844584, 0.016701558443503534, 0.01761956456312109, 0.016330169903398153, 0.016543785712512412, 0.01634269813025148, 0.01646671481634265, 0.01710031004314705, 0.016427849799202313, 0.016917388163746053, 0.01721847019626134, 0.01709659307790449, 0.01715205364070177], "moving_avg_accuracy_train": [0.04624055881436876, 0.10558664585928845, 0.1658239319816468, 0.2217438963762285, 0.2766235323822933, 0.3290122495055904, 0.3780030720414193, 0.4227967549224103, 0.4643921183628935, 0.5018978080700703, 0.5370034076790378, 0.5692659813283544, 0.59863482994132, 0.6257083184715051, 0.6507278249641478, 0.6741381298039272, 0.69526774988232, 0.714967785409961, 0.7331510411586012, 0.7495509567109766, 0.7653639568259163, 0.7800349298102681, 0.7936689940747654, 0.8063209763175749, 0.8180424736182187, 0.8288778865900069, 0.8386575158550557, 0.8478242305566948, 0.8563346183571992, 0.8640707693860049, 0.8713122871202541, 0.8779388990263073, 0.884225901231004, 0.8899679446211926, 0.8951752751533054, 0.9000316084953024, 0.9045465398269276, 0.9088167720741629, 0.9127111704193029, 0.9162510422108906, 0.9195972899935391, 0.9226298753860271, 0.9254103194642571, 0.9280987310394261, 0.9305275299546787, 0.9327111598784152, 0.9347600961181023, 0.9365158191278775, 0.9382493936092849, 0.939821272435418, 0.9413103681408426, 0.9426853954590488, 0.9438717307228062, 0.9450045005780356, 0.9460658461263137, 0.9469885410852493, 0.9478352786387767, 0.9486740002500282, 0.949428921797792, 0.9500990145467227, 0.9507462397993225, 0.9513264534266714, 0.9518137684591428, 0.9522687001276712, 0.952638575050766, 0.953062107236304, 0.9533945301759257, 0.9537517674441859, 0.9540476682998965, 0.9543070036236074, 0.9545590066054236, 0.9548067716771625, 0.9550181705464987, 0.9552177301241394, 0.9554020200904537, 0.9555631586648801, 0.9556849679425874, 0.955845677468696, 0.9560136396279264, 0.9561113271486148, 0.9562154859100822, 0.9563417448299174, 0.9564182475744543, 0.9565265554766618, 0.9565984920005626, 0.9566679212185112, 0.956707228124207, 0.9567541940345622, 0.9567987164050539, 0.9568666522753919, 0.9568813636801432, 0.9569619972110768, 0.9569694992710691, 0.9570133814083771, 0.9570320571391249, 0.957090609828913, 0.9571340066544842, 0.9571312832165644, 0.9571451081641031, 0.9571691042632983, 0.9572000013478119, 0.9571697521012735, 0.957212246194856, 0.9572388290862138, 0.957258103390817, 0.9573010629506832, 0.9572746223878962, 0.9572670658742357, 0.9572324713726833, 0.9572430808534013, 0.9572340281955712, 0.9572468792404474, 0.9572235318998743, 0.9572861886016827, 0.9572845951083472, 0.9573388924381363, 0.9573598222004134, 0.9573832732352632, 0.9573648876856848, 0.957318185854178, 0.9573551011188894, 0.9573487973273678, 0.9573152942269215, 0.9573409089591297, 0.9573222176860019, 0.957272771359216, 0.9572677250972331, 0.9572538828662105, 0.9572554478487847, 0.9571918242640722, 0.9572274608437559, 0.9572106695916524, 0.9572327598457115, 0.9572456656279363, 0.9572596780783856, 0.9572722532349711, 0.9572974857199366, 0.9573178698075959, 0.9573153612448412, 0.957329307482391, 0.9573070900104992, 0.957345186957216, 0.9573143339937757, 0.9572935417731081, 0.9572957911626117, 0.9573000686643368, 0.9573225556551844, 0.9573428660445847, 0.9573494115045411, 0.9573391345232914, 0.9573322103889763, 0.9573352071656933, 0.9573750705968722], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 205107939, "moving_var_accuracy_train": [0.01924370351518587, 0.04901695559155597, 0.07677203578688216, 0.09723821396921553, 0.11462036260571751, 0.1278595254815693, 0.13667447916804612, 0.14106529748562657, 0.14253033607477691, 0.14093739331099853, 0.13793528209504494, 0.13350961681183865, 0.1279214185503164, 0.12172604072603135, 0.11518721799964676, 0.10860087755390467, 0.1017589374004292, 0.09507586625849913, 0.08854395673923336, 0.08211017613643543, 0.07614961727650761, 0.0704717925836251, 0.06509760270057859, 0.06002849632257204, 0.05526218818103583, 0.05079262493135503, 0.046574132775275716, 0.04267297742353937, 0.039057519985820195, 0.0356904002819026, 0.03259331646557136, 0.029729192687195247, 0.027112010988972472, 0.024697549450728506, 0.022471841127091736, 0.020436912776139876, 0.018576682942889503, 0.016883128599608502, 0.015331312785883315, 0.013910957738002883, 0.012620638332208514, 0.011441343666452254, 0.010366787123256511, 0.009395156422108383, 0.008508732357434138, 0.007700773278485267, 0.006968479208065468, 0.006299374356842409, 0.005696484445501454, 0.005149073228347721, 0.004654122559692175, 0.004205726604855279, 0.0037978204665920588, 0.0034295869278371032, 0.003096766324409039, 0.002794751985853342, 0.002521729467628991, 0.002275887606336715, 0.0020534280045925476, 0.0018521264227628178, 0.0016706838852349616, 0.0015066453273917192, 0.0013581180781203999, 0.0012241689357156296, 0.0011029833092726764, 0.0009942993939550883, 0.0008958639996566603, 0.0008074261658835, 0.0007274715651428424, 0.0006553297019196766, 0.0005903682812533066, 0.0005318839409049401, 0.00047909775215205564, 0.00043154639316210307, 0.00038869741897105015, 0.00035006136783545863, 0.00031518876855313286, 0.0002839023396638579, 0.0002557660072798723, 0.0002302752922171692, 0.00020734540442376612, 0.00018675433581493125, 0.0001681315762627332, 0.00015142399405158535, 0.00013632816841766522, 0.0001227387353226432, 0.00011047876708589739, 9.945074254792711e-05, 8.952350846640223e-05, 8.061269516206937e-05, 7.255337347473025e-05, 6.535655202405466e-05, 5.8821403349786356e-05, 5.295659379258019e-05, 4.766407345959283e-05, 4.292852187096638e-05, 3.865261924409668e-05, 3.478742407371394e-05, 3.131040182891261e-05, 2.818454396101057e-05, 2.537468123339251e-05, 2.2845448262298468e-05, 2.0577155167973207e-05, 1.8525799502192396e-05, 1.6676563041334538e-05, 1.5025516451256204e-05, 1.352925673637504e-05, 1.2176844970825839e-05, 1.0969931489582184e-05, 9.873951390353917e-06, 8.887293806842606e-06, 8.000050770348049e-06, 7.204951578119717e-06, 6.51978918084134e-06, 5.867833115746298e-06, 5.3075836043717365e-06, 4.7807677384753774e-06, 4.307640523947563e-06, 3.879918727452496e-06, 3.5115564043020835e-06, 3.1726653947902836e-06, 2.855756495399192e-06, 2.580282965514885e-06, 2.328159699518267e-06, 2.098488002786687e-06, 1.9106436556015434e-06, 1.7198084728813878e-06, 1.549552091830419e-06, 1.3946189251814955e-06, 1.291588677448195e-06, 1.1738595020072736e-06, 1.0590110671313858e-06, 9.57501774337849e-07, 8.632506298375464e-07, 7.786927057621289e-07, 7.022466462542843e-07, 6.377520863066269e-07, 5.777164769433017e-07, 5.200014652328241e-07, 4.697517965857125e-07, 4.272191614424819e-07, 3.975596414405022e-07, 3.663708254739001e-07, 3.3362459088916825e-07, 3.003076695784982e-07, 2.7044157580972784e-07, 2.4794840104517657e-07, 2.2686616819900294e-07, 2.0456513879347553e-07, 1.8505917200656927e-07, 1.6698474753003554e-07, 1.5036709881325541e-07, 1.496322272401318e-07], "duration": 64242.696836, "accuracy_train": [0.4624055881436877, 0.6397014292635659, 0.7079595070828719, 0.725023575927464, 0.770540256436877, 0.800510703615264, 0.8189204748638795, 0.825939900851329, 0.8387503893272426, 0.8394490154346622, 0.8529538041597453, 0.8596291441722037, 0.8629544674580103, 0.869369715243171, 0.8759033833979328, 0.8848308733619417, 0.8854343305878553, 0.8922681051587301, 0.896800342896364, 0.8971501966823551, 0.9076809578603728, 0.9120736866694352, 0.9163755724552418, 0.9201888165028608, 0.9235359493240125, 0.9263966033361019, 0.9266741792404946, 0.930324662871447, 0.9329281085617387, 0.9336961286452565, 0.9364859467284975, 0.9375784061807864, 0.9408089210732743, 0.9416463351328904, 0.942041249942322, 0.9437386085732743, 0.9451809218115541, 0.9472488622992802, 0.9477607555255629, 0.9481098883351791, 0.9497135200373754, 0.9499231439184201, 0.9504343161683279, 0.9522944352159468, 0.9523867201919527, 0.9523638291920451, 0.9532005222752861, 0.9523173262158545, 0.9538515639419527, 0.9539681818706165, 0.954712229489664, 0.9550606413229051, 0.9545487480966224, 0.9551994292751015, 0.9556179560608158, 0.9552927957156699, 0.9554559166205242, 0.956222494751292, 0.956223215727667, 0.9561298492870985, 0.9565712670727206, 0.9565483760728128, 0.9561996037513842, 0.956363085144426, 0.9559674493586194, 0.9568738969061462, 0.9563863366325213, 0.9569669028585271, 0.956710776001292, 0.9566410215370063, 0.9568270334417681, 0.9570366573228128, 0.9569207603705242, 0.9570137663229051, 0.9570606297872831, 0.9570134058347176, 0.9567812514419527, 0.9572920632036729, 0.9575252990610004, 0.9569905148348099, 0.957152914763289, 0.9574780751084349, 0.9571067722752861, 0.9575013265965301, 0.9572459207156699, 0.957292784180048, 0.9570609902754706, 0.9571768872277593, 0.9571994177394795, 0.9574780751084349, 0.9570137663229051, 0.9576876989894795, 0.9570370178110004, 0.9574083206441492, 0.9572001387158545, 0.9576175840370063, 0.9575245780846253, 0.9571067722752861, 0.9572695326919527, 0.9573850691560539, 0.9574780751084349, 0.956897508882429, 0.9575946930370985, 0.9574780751084349, 0.9574315721322444, 0.9576876989894795, 0.9570366573228128, 0.957199057251292, 0.9569211208587117, 0.9573385661798633, 0.9571525542751015, 0.9573625386443337, 0.9570134058347176, 0.9578500989179586, 0.9572702536683279, 0.9578275684062385, 0.9575481900609081, 0.957594332548911, 0.9571994177394795, 0.9568978693706165, 0.957687338501292, 0.9572920632036729, 0.9570137663229051, 0.9575714415490033, 0.9571539962278516, 0.9568277544181433, 0.9572223087393872, 0.9571293027870063, 0.9572695326919527, 0.9566192120016611, 0.9575481900609081, 0.9570595483227206, 0.9574315721322444, 0.9573618176679586, 0.957385790132429, 0.9573854296442414, 0.9575245780846253, 0.9575013265965301, 0.957292784180048, 0.9574548236203396, 0.9571071327634736, 0.957688059477667, 0.9570366573228128, 0.9571064117870985, 0.9573160356681433, 0.9573385661798633, 0.9575249385728128, 0.9575256595491879, 0.9574083206441492, 0.9572466416920451, 0.9572698931801403, 0.9573621781561462, 0.9577338414774824], "end": "2016-01-24 22:17:50.192000", "learning_rate_per_epoch": [0.0027973204851150513, 0.0025574311148375273, 0.0023381139617413282, 0.002137604635208845, 0.0019542905502021313, 0.0017866968410089612, 0.001633475418202579, 0.001493393792770803, 0.0013653250643983483, 0.0012482390739023685, 0.0011411940213292837, 0.0010433288989588618, 0.0009538563317619264, 0.0008720566402189434, 0.0007972718449309468, 0.0007289003697223961, 0.0006663922104053199, 0.0006092445109970868, 0.0005569976638071239, 0.0005092312931083143, 0.000465561228338629, 0.0004256361862644553, 0.0003891349770128727, 0.0003557640011422336, 0.00032525480492040515, 0.00029736198484897614, 0.0002718611794989556, 0.00024854723596945405, 0.00022723260917700827, 0.00020774586300831288, 0.0001899302442325279, 0.00017364243103656918, 0.0001587514125276357, 0.00014513739733956754, 0.00013269088231027126, 0.00012131173571106046, 0.00011090842599514872, 0.0001013972723740153, 9.270176815334707e-05, 8.475196227664128e-05, 7.748390635242686e-05, 7.083913078531623e-05, 6.476419366663322e-05, 5.9210222389083356e-05, 5.4132542572915554e-05, 4.949030699208379e-05, 4.524617543211207e-05, 4.1366009099874645e-05, 3.7818590499227867e-05, 3.457538696238771e-05, 3.1610310543328524e-05, 2.8899510652991012e-05, 2.6421181246405467e-05, 2.4155384380719624e-05, 2.2083895601099357e-05, 2.019005114561878e-05, 1.84586169780232e-05, 1.6875665096449666e-05, 1.5428462575073354e-05, 1.410536697221687e-05, 1.2895735380880069e-05, 1.1789838026743382e-05, 1.077877914212877e-05, 9.854425115918275e-06, 9.009340828924906e-06, 8.23672871774761e-06, 7.5303728408471216e-06, 6.884592039568815e-06, 6.294191280176165e-06, 5.754421636083862e-06, 5.260940724838292e-06, 4.809779056813568e-06, 4.397307748149615e-06, 4.020208507427014e-06, 3.675448169815354e-06, 3.3602534585952526e-06, 3.072088702538167e-06, 2.8086360543966293e-06, 2.5677763915155083e-06, 2.3475720354326768e-06, 2.146251745216432e-06, 1.962195938176592e-06, 1.7939242979991832e-06, 1.640083041820617e-06, 1.4994346884122933e-06, 1.3708479400520446e-06, 1.2532883602034417e-06, 1.1458103017503163e-06, 1.0475492899786332e-06, 9.577147466188762e-07, 8.755841918173246e-07, 8.004968776731403e-07, 7.318487860175082e-07, 6.690877398796147e-07, 6.117089128565567e-07, 5.592506795437657e-07, 5.11291091243038e-07, 4.674443800922745e-07, 4.273578326774441e-07, 3.9070897628334933e-07, 3.5720302093977807e-07, 3.2657041515449237e-07, 2.9856477112844004e-07, 2.729608183926757e-07, 2.4955255639724783e-07, 2.2815171973888937e-07, 2.085861581235804e-07, 1.906984721244953e-07, 1.743447768376427e-07, 1.5939352238092397e-07, 1.4572444229088433e-07, 1.3322757297373755e-07, 1.2180240105408302e-07, 1.1135701782905016e-07, 1.01807394514708e-07, 9.307671433589348e-08, 8.509475435403147e-08, 7.779729571666394e-08, 7.112564759381712e-08, 6.502613558723169e-08, 5.94497002737171e-08, 5.4351481537651125e-08, 4.969047040503938e-08, 4.5429175088429474e-08, 4.153331190082099e-08, 3.7971545907566906e-08, 3.471522802556137e-08, 3.173816054413692e-08, 2.9016396396741584e-08, 2.6528041985329764e-08, 2.4253081321035097e-08, 2.217321437569808e-08, 2.0271709644248403e-08, 1.853327269429883e-08, 1.6943918268452762e-08, 1.5490861926537036e-08, 1.4162415240548398e-08, 1.2947891647741017e-08, 1.183752118549819e-08, 1.0822373219809833e-08, 9.894280950106804e-09, 9.045779236771523e-09, 8.270041540470174e-09, 7.560829295982785e-09, 6.912436401051991e-09, 6.319647916086524e-09, 5.777695211151013e-09, 5.28221821838315e-09, 4.829232125302951e-09, 4.415092735854387e-09, 4.036468492785161e-09, 3.690313832294123e-09, 3.3738443150355124e-09], "accuracy_valid": [0.44673469267695787, 0.6233674934111446, 0.6884853868599398, 0.7047619187688253, 0.7454436888177711, 0.7738463620105422, 0.7922186794051205, 0.7935114481362951, 0.8033888483621988, 0.803490328501506, 0.8137957101844879, 0.8200315912085843, 0.8217714608433735, 0.8261865822665663, 0.8284750329442772, 0.8330019295933735, 0.8327783791415663, 0.8361154579254518, 0.840611469314759, 0.8342329278049698, 0.8431852409638554, 0.848353374435241, 0.8478445030120482, 0.8510183311370482, 0.8533994375941265, 0.8538362434111446, 0.8526155402861446, 0.8533170769013554, 0.8551996070218373, 0.8564203101468373, 0.855433452560241, 0.8554437476468373, 0.8570409567959337, 0.8571836172816265, 0.8555761130459337, 0.8563085349209337, 0.856532085372741, 0.8590249670557228, 0.8578851538968373, 0.8599912344691265, 0.8601235998682228, 0.8600015295557228, 0.8605912909450302, 0.8601133047816265, 0.8574174628200302, 0.8608457266566265, 0.8618222891566265, 0.8614663733057228, 0.8628091467432228, 0.8631753576807228, 0.8630532873682228, 0.8633077230798193, 0.8643960608057228, 0.8626870764307228, 0.8621987951807228, 0.8635518637048193, 0.8650064123682228, 0.8644063558923193, 0.8646504965173193, 0.8648946371423193, 0.8637857092432228, 0.8647725668298193, 0.8637857092432228, 0.8650167074548193, 0.8639077795557228, 0.8643960608057228, 0.8639077795557228, 0.8647622717432228, 0.8643960608057228, 0.8646402014307228, 0.8648843420557228, 0.8643960608057228, 0.8643960608057228, 0.8647622717432228, 0.8650064123682228, 0.8645078360316265, 0.8637857092432228, 0.8646299063441265, 0.8648843420557228, 0.8645181311182228, 0.8640298498682228, 0.8646402014307228, 0.8640298498682228, 0.8645181311182228, 0.8645181311182228, 0.8642739904932228, 0.8634194983057228, 0.8635415686182228, 0.8646402014307228, 0.8646402014307228, 0.8651284826807228, 0.8646402014307228, 0.8642739904932228, 0.8641519201807228, 0.8635415686182228, 0.8650064123682228, 0.8642739904932228, 0.8643960608057228, 0.8646402014307228, 0.8645181311182228, 0.8640298498682228, 0.8642739904932228, 0.8652505529932228, 0.8639077795557228, 0.8647622717432228, 0.8647622717432228, 0.8634092032191265, 0.8645181311182228, 0.8636636389307228, 0.8641416250941265, 0.8634297933923193, 0.8645284262048193, 0.8643960608057228, 0.8647622717432228, 0.8641519201807228, 0.8642739904932228, 0.8646402014307228, 0.8647622717432228, 0.8636636389307228, 0.8639077795557228, 0.8648843420557228, 0.8645078360316265, 0.8642739904932228, 0.8641622152673193, 0.8644063558923193, 0.8642739904932228, 0.8651284826807228, 0.8647622717432228, 0.8641519201807228, 0.8643960608057228, 0.8640298498682228, 0.8643857657191265, 0.8640298498682228, 0.8643960608057228, 0.8640298498682228, 0.8651284826807228, 0.8647622717432228, 0.8641519201807228, 0.8641519201807228, 0.8640298498682228, 0.8631753576807228, 0.8641519201807228, 0.8636739340173193, 0.8642739904932228, 0.8641519201807228, 0.8645181311182228, 0.8640298498682228, 0.8639077795557228, 0.8643960608057228, 0.8646402014307228, 0.8641622152673193, 0.8641519201807228, 0.8643960608057228], "accuracy_test": 0.8548110650510203, "start": "2016-01-24 04:27:07.495000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0], "accuracy_train_last": 0.9577338414774824, "batch_size_eval": 1024, "accuracy_train_std": [0.015055299531887171, 0.01905037854355071, 0.017914435198705076, 0.01676886491124911, 0.019156437199503305, 0.017113438345607027, 0.017629048378209278, 0.016878150966198085, 0.018525624417817202, 0.0164777815025283, 0.015211259149360041, 0.015580374777570417, 0.015443189541857654, 0.014572554272812902, 0.014854303895743753, 0.014706479217426353, 0.014064236809548098, 0.014147090615117775, 0.013311970673435553, 0.013382649604338227, 0.012520013891749367, 0.013306654639535384, 0.01204985758061769, 0.0128811426524337, 0.012296020188957039, 0.010870805503492394, 0.0107869033110841, 0.010564650893137732, 0.01050116525925372, 0.009552406472457758, 0.009300830132305935, 0.009019555744584329, 0.008969078708608727, 0.00909790391946598, 0.009108445962014189, 0.008496020469936198, 0.007488303260908507, 0.007688149621243941, 0.00847299324855782, 0.0074637735893331465, 0.007328444943347785, 0.007233728227382867, 0.007270441018316026, 0.007471386904058052, 0.0072580328930840505, 0.007386628420165244, 0.006968755700458992, 0.007260730957012897, 0.007503768511478238, 0.00708090180172775, 0.006733020600184696, 0.006677985248739032, 0.007236349971473088, 0.007013357540070041, 0.007109525398680992, 0.0068896473941547, 0.006886771504757244, 0.007001974232421411, 0.007093911798372157, 0.006993205216751943, 0.00678538559535298, 0.0071624575200631485, 0.007006705827992083, 0.006971900850385529, 0.006772827267745766, 0.006886365465060641, 0.00665930912831427, 0.0066667373296562926, 0.006942184541485116, 0.006976145193403124, 0.006460507015225977, 0.0068766135819881705, 0.006800098345669819, 0.006597609872436961, 0.006617028585275457, 0.006629454357144914, 0.006698810892395984, 0.0065907152989109164, 0.006568747068133239, 0.0066642821876430605, 0.006542159781442658, 0.006563446784087663, 0.006797236849798065, 0.006608372772834047, 0.006567486422207041, 0.006613080973088497, 0.006574342113269697, 0.006425706836742262, 0.006816511909291828, 0.006884262824930627, 0.0065179717387466, 0.006808625017563339, 0.006819331642186971, 0.006886787242120058, 0.00655632404676951, 0.006794043436189767, 0.006587747586807384, 0.006583419008618088, 0.006525827789957449, 0.006938928971055196, 0.006713941349590061, 0.006535759054945955, 0.006590004298144304, 0.006781243964012462, 0.006723632161196087, 0.006612368485951209, 0.006548491984834449, 0.006790533855354163, 0.0065610359730393355, 0.0066640975250363025, 0.0069206866734373845, 0.006556491855135705, 0.006622600618193405, 0.006613871739139392, 0.006509301602769468, 0.006606357107971642, 0.006733523093386559, 0.0068442853000035275, 0.006661518875298694, 0.006482263274856022, 0.006789334888308337, 0.006824296995048885, 0.006770858307915198, 0.006952389663615448, 0.0066488254187440496, 0.0067879227919231564, 0.006930588794363471, 0.006641461712283676, 0.006848406332815837, 0.0066832063502165525, 0.006803969985646347, 0.006869003491119125, 0.00682086565305055, 0.006806277928402329, 0.00676494953876319, 0.00696434388323599, 0.006714067439851118, 0.006625530621630573, 0.006842581984841242, 0.006720518291429943, 0.006589147727511718, 0.0066729209865319505, 0.006624336544480004, 0.006687028956055497, 0.006600090263608344, 0.006742009809023808, 0.006573181223063167, 0.006560636095561086, 0.006679238626647116, 0.006572258374183044, 0.006607636514578933, 0.006895044209581399, 0.006683524046662584], "accuracy_test_std": 0.00940627548352256, "error_valid": [0.5532653073230421, 0.3766325065888554, 0.31151461314006024, 0.2952380812311747, 0.2545563111822289, 0.22615363798945776, 0.20778132059487953, 0.20648855186370485, 0.19661115163780118, 0.19650967149849397, 0.18620428981551207, 0.17996840879141573, 0.1782285391566265, 0.17381341773343373, 0.17152496705572284, 0.1669980704066265, 0.16722162085843373, 0.16388454207454817, 0.15938853068524095, 0.16576707219503017, 0.1568147590361446, 0.15164662556475905, 0.15215549698795183, 0.14898166886295183, 0.1466005624058735, 0.1461637565888554, 0.1473844597138554, 0.1466829230986446, 0.14480039297816272, 0.14357968985316272, 0.14456654743975905, 0.14455625235316272, 0.14295904320406627, 0.1428163827183735, 0.14442388695406627, 0.14369146507906627, 0.14346791462725905, 0.14097503294427716, 0.14211484610316272, 0.1400087655308735, 0.13987640013177716, 0.13999847044427716, 0.13940870905496983, 0.1398866952183735, 0.14258253717996983, 0.1391542733433735, 0.1381777108433735, 0.13853362669427716, 0.13719085325677716, 0.13682464231927716, 0.13694671263177716, 0.1366922769201807, 0.13560393919427716, 0.13731292356927716, 0.13780120481927716, 0.1364481362951807, 0.13499358763177716, 0.1355936441076807, 0.1353495034826807, 0.1351053628576807, 0.13621429075677716, 0.1352274331701807, 0.13621429075677716, 0.1349832925451807, 0.13609222044427716, 0.13560393919427716, 0.13609222044427716, 0.13523772825677716, 0.13560393919427716, 0.13535979856927716, 0.13511565794427716, 0.13560393919427716, 0.13560393919427716, 0.13523772825677716, 0.13499358763177716, 0.1354921639683735, 0.13621429075677716, 0.1353700936558735, 0.13511565794427716, 0.13548186888177716, 0.13597015013177716, 0.13535979856927716, 0.13597015013177716, 0.13548186888177716, 0.13548186888177716, 0.13572600950677716, 0.13658050169427716, 0.13645843138177716, 0.13535979856927716, 0.13535979856927716, 0.13487151731927716, 0.13535979856927716, 0.13572600950677716, 0.13584807981927716, 0.13645843138177716, 0.13499358763177716, 0.13572600950677716, 0.13560393919427716, 0.13535979856927716, 0.13548186888177716, 0.13597015013177716, 0.13572600950677716, 0.13474944700677716, 0.13609222044427716, 0.13523772825677716, 0.13523772825677716, 0.1365907967808735, 0.13548186888177716, 0.13633636106927716, 0.1358583749058735, 0.1365702066076807, 0.1354715737951807, 0.13560393919427716, 0.13523772825677716, 0.13584807981927716, 0.13572600950677716, 0.13535979856927716, 0.13523772825677716, 0.13633636106927716, 0.13609222044427716, 0.13511565794427716, 0.1354921639683735, 0.13572600950677716, 0.1358377847326807, 0.1355936441076807, 0.13572600950677716, 0.13487151731927716, 0.13523772825677716, 0.13584807981927716, 0.13560393919427716, 0.13597015013177716, 0.1356142342808735, 0.13597015013177716, 0.13560393919427716, 0.13597015013177716, 0.13487151731927716, 0.13523772825677716, 0.13584807981927716, 0.13584807981927716, 0.13597015013177716, 0.13682464231927716, 0.13584807981927716, 0.1363260659826807, 0.13572600950677716, 0.13584807981927716, 0.13548186888177716, 0.13597015013177716, 0.13609222044427716, 0.13560393919427716, 0.13535979856927716, 0.1358377847326807, 0.13584807981927716, 0.13560393919427716], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-3, 3], "momentum": 0.9881589416330527, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.0030597115695609146, "patience_threshold": 1, "do_flip": true, "batch_size": 128, "optimization": "adam", "nb_data_augmentation": 3, "learning_rate_decay_method": "exp", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 3.8446016886867794e-07, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.08575682050521195}, "accuracy_valid_max": 0.8652505529932228, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-3, 3)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8643960608057228, "loss_train": [1.4997889995574951, 1.127835988998413, 0.9437006711959839, 0.8126355409622192, 0.7232547402381897, 0.6655058860778809, 0.6132342219352722, 0.5723491907119751, 0.5408288240432739, 0.5106865763664246, 0.48604655265808105, 0.4644109010696411, 0.44520464539527893, 0.4269084632396698, 0.4108518362045288, 0.394379585981369, 0.3828223943710327, 0.37140199542045593, 0.35890793800354004, 0.3473857343196869, 0.33773788809776306, 0.3298909366130829, 0.3210313320159912, 0.31413379311561584, 0.30646657943725586, 0.3007175028324127, 0.29474830627441406, 0.28920188546180725, 0.2843247354030609, 0.2807038128376007, 0.27477118372917175, 0.27178025245666504, 0.2675156593322754, 0.265433132648468, 0.26236864924430847, 0.25941547751426697, 0.25722697377204895, 0.25377950072288513, 0.25168392062187195, 0.25103384256362915, 0.24856553971767426, 0.2472819685935974, 0.2442871630191803, 0.2433035969734192, 0.2426472306251526, 0.24108268320560455, 0.23947973549365997, 0.23842096328735352, 0.2381470799446106, 0.2362205982208252, 0.23682433366775513, 0.23602281510829926, 0.23425622284412384, 0.23317721486091614, 0.23376384377479553, 0.2331562638282776, 0.23251305520534515, 0.23343180119991302, 0.23250246047973633, 0.2321464866399765, 0.23139871656894684, 0.23144297301769257, 0.23046956956386566, 0.22843894362449646, 0.22850288450717926, 0.2294173389673233, 0.22789789736270905, 0.2290201336145401, 0.22978343069553375, 0.22910930216312408, 0.22858279943466187, 0.22837096452713013, 0.22846391797065735, 0.22729991376399994, 0.22805185616016388, 0.22711387276649475, 0.2279321700334549, 0.22865785658359528, 0.22715575993061066, 0.22758951783180237, 0.22804507613182068, 0.226105198264122, 0.22792892158031464, 0.22583575546741486, 0.2278236597776413, 0.2264201045036316, 0.2269902229309082, 0.226674884557724, 0.22796113789081573, 0.2271108478307724, 0.22721295058727264, 0.22670187056064606, 0.2260739654302597, 0.22525723278522491, 0.2268708348274231, 0.22729483246803284, 0.2270824909210205, 0.22619670629501343, 0.22695957124233246, 0.2260763794183731, 0.22512011229991913, 0.2248164713382721, 0.22664283215999603, 0.22552737593650818, 0.22682373225688934, 0.22665290534496307, 0.22612234950065613, 0.2250642031431198, 0.22604911029338837, 0.22610238194465637, 0.22790223360061646, 0.22521138191223145, 0.2264135479927063, 0.22682519257068634, 0.22726230323314667, 0.22626841068267822, 0.22691349685192108, 0.22581946849822998, 0.22597120702266693, 0.22582465410232544, 0.227754145860672, 0.22663737833499908, 0.22577446699142456, 0.2272229641675949, 0.22824496030807495, 0.22718419134616852, 0.2269502729177475, 0.22694337368011475, 0.22401435673236847, 0.22639749944210052, 0.2280254364013672, 0.22676056623458862, 0.2263839840888977, 0.22656449675559998, 0.22696168720722198, 0.2268039882183075, 0.2269611358642578, 0.22601740062236786, 0.2268974334001541, 0.2269582450389862, 0.22712036967277527, 0.22596782445907593, 0.2258065640926361, 0.22550047934055328, 0.22755081951618195, 0.22634121775627136, 0.2267211526632309, 0.22684426605701447, 0.22649431228637695, 0.22508345544338226, 0.22665491700172424, 0.22665517032146454, 0.22603048384189606], "accuracy_train_first": 0.4624055881436877, "model": "residualv3", "loss_std": [0.2605164051055908, 0.12189025431871414, 0.10196293145418167, 0.09163930267095566, 0.08671330660581589, 0.08317908644676208, 0.07959530502557755, 0.07757005095481873, 0.07682023197412491, 0.07544037699699402, 0.07305363565683365, 0.07134281098842621, 0.0682046040892601, 0.06948058307170868, 0.06624785810709, 0.06357161700725555, 0.062270887196063995, 0.06223146244883537, 0.06222682446241379, 0.0610743872821331, 0.06042499095201492, 0.05981654301285744, 0.057488277554512024, 0.05835069715976715, 0.054511431604623795, 0.055395182222127914, 0.05549651011824608, 0.054537463933229446, 0.05357092618942261, 0.05312595143914223, 0.05187149718403816, 0.05107297748327255, 0.050268955528736115, 0.05015639215707779, 0.05050086975097656, 0.05074508860707283, 0.05100185051560402, 0.05030083656311035, 0.04725763574242592, 0.048176392912864685, 0.04898504167795181, 0.047237150371074677, 0.04627891629934311, 0.048272546380758286, 0.04699775576591492, 0.0468197837471962, 0.0457829087972641, 0.046501316130161285, 0.04631110280752182, 0.046837951987981796, 0.04666795954108238, 0.04674750939011574, 0.045840468257665634, 0.045875225216150284, 0.04674879088997841, 0.04569290578365326, 0.0460348017513752, 0.04729901999235153, 0.0475744791328907, 0.04538353160023689, 0.04658421501517296, 0.045978792011737823, 0.04467334225773811, 0.046203650534152985, 0.04577194154262543, 0.04623575881123543, 0.04420290142297745, 0.045657817274332047, 0.04494881257414818, 0.04643597826361656, 0.04511996731162071, 0.046604350209236145, 0.0465218611061573, 0.04550302028656006, 0.044569820165634155, 0.04612376540899277, 0.04603065550327301, 0.04566850885748863, 0.04491649940609932, 0.04378623515367508, 0.04598073661327362, 0.04454081133008003, 0.04480121657252312, 0.04547267407178879, 0.04544501006603241, 0.045363545417785645, 0.045106157660484314, 0.04590601101517677, 0.04646219685673714, 0.04409785196185112, 0.04471215233206749, 0.044649820774793625, 0.04443441703915596, 0.04575565829873085, 0.04544125869870186, 0.04528968781232834, 0.045372799038887024, 0.04557297006249428, 0.04606197774410248, 0.04513927921652794, 0.046932704746723175, 0.04677439481019974, 0.04443030804395676, 0.04335799440741539, 0.04470610246062279, 0.04274303838610649, 0.04548916220664978, 0.04611779376864433, 0.04524466022849083, 0.04484068602323532, 0.0452762097120285, 0.0455721952021122, 0.043871793895959854, 0.044938813894987106, 0.044986624270677567, 0.04353102296590805, 0.045419927686452866, 0.04362728074193001, 0.04502716660499573, 0.04522767663002014, 0.04512449726462364, 0.04534013196825981, 0.04466884955763817, 0.04607623070478439, 0.045121047645807266, 0.046002794057130814, 0.04535279795527458, 0.044464096426963806, 0.044542159885168076, 0.04505113884806633, 0.04518713802099228, 0.04564228653907776, 0.046959709376096725, 0.04449983686208725, 0.045694973319768906, 0.04417144134640694, 0.04477239400148392, 0.04456685483455658, 0.04550854116678238, 0.04463103786110878, 0.046224869787693024, 0.044841405004262924, 0.04462999850511551, 0.043989527970552444, 0.04594052955508232, 0.045027460902929306, 0.04506878927350044, 0.04428097978234291, 0.04433821141719818, 0.04519225284457207, 0.045292939990758896, 0.04640679433941841, 0.04505959153175354]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:12 2016", "state": "available"}], "summary": "48a6b9d49635f43604c29e46a3348ad5"}