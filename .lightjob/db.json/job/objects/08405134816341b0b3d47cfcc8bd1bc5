{"content": {"hp_model": {"f0": 64, "f1": 64, "f2": 64, "f3": 64, "nonlin": "rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.5166932344436646, 1.0690079927444458, 0.8571213483810425, 0.731758177280426, 0.6446201205253601, 0.5801246762275696, 0.5283918380737305, 0.48513466119766235, 0.44246116280555725, 0.41079196333885193, 0.3820662200450897, 0.3552696108818054, 0.328802227973938, 0.306501567363739, 0.2848624289035797, 0.2669670879840851, 0.24910275638103485, 0.2333010733127594, 0.2199442982673645, 0.20777074992656708, 0.191372349858284, 0.17976754903793335, 0.17240320146083832, 0.16382327675819397, 0.1524686962366104, 0.14371725916862488, 0.13792268931865692, 0.1306915134191513, 0.12246451526880264, 0.11439547687768936, 0.10979244112968445, 0.10536004602909088, 0.10138047486543655, 0.09310176968574524, 0.08900295943021774, 0.08769335597753525, 0.08233403414487839, 0.07899350672960281, 0.07407142966985703, 0.07275725156068802, 0.0687335729598999, 0.06639427691698074, 0.061768267303705215, 0.058045338839292526, 0.056985847651958466, 0.05419618636369705, 0.05266299471259117, 0.0504448227584362, 0.04892571642994881, 0.04747805371880531, 0.04648914933204651, 0.04449820891022682, 0.04118804261088371, 0.04055911302566528, 0.0400216206908226, 0.0388171412050724, 0.037093885242938995, 0.03682660683989525, 0.03491046652197838, 0.035922616720199585, 0.033140331506729126, 0.03285256028175354, 0.03215903043746948, 0.03009536676108837, 0.030973263084888458, 0.029399478808045387, 0.0281034167855978, 0.02786305360496044, 0.0272043626755476, 0.026949530467391014, 0.026153357699513435, 0.026082724332809448, 0.02568749338388443, 0.026084790006279945, 0.024914409965276718, 0.02475556544959545, 0.023152973502874374, 0.02389279566705227, 0.02309860847890377, 0.022878289222717285, 0.0226264800876379, 0.022339746356010437, 0.022659536451101303, 0.02182929217815399, 0.0214496199041605, 0.021491801366209984, 0.021449914202094078, 0.02058275230228901, 0.020608559250831604, 0.020390857011079788, 0.020283330231904984, 0.020628051832318306, 0.02007637917995453, 0.019724464043974876, 0.019654754549264908, 0.018937624990940094, 0.019465330988168716, 0.019009027630090714, 0.019246887415647507, 0.018577415496110916, 0.018692782148718834, 0.018693218007683754, 0.018393216654658318, 0.018273307010531425, 0.018322881311178207, 0.017872154712677002, 0.017963269725441933, 0.018011849373579025, 0.01801573857665062, 0.018513787537813187, 0.018006393685936928, 0.017879368737339973, 0.017618969082832336, 0.017727671191096306, 0.017968328669667244, 0.017783578485250473, 0.0174094308167696, 0.01727060228586197, 0.01746409386396408, 0.017256272956728935, 0.017051996663212776, 0.017275676131248474, 0.01708986610174179, 0.017061525955796242, 0.0171657782047987, 0.017111295834183693, 0.017135322093963623, 0.01728864386677742, 0.0173662006855011, 0.016710344702005386, 0.017075037583708763, 0.016702549532055855, 0.017009008675813675, 0.01664079539477825, 0.017078664153814316, 0.01705515757203102, 0.01695994660258293, 0.01690434105694294, 0.016865937039256096, 0.0167703814804554, 0.016700847074389458, 0.01649497263133526, 0.016868798062205315, 0.016632862389087677, 0.016476303339004517, 0.016840817406773567, 0.016757000237703323, 0.016780875623226166, 0.016504522413015366, 0.016574757173657417, 0.016595590859651566, 0.016747884452342987], "moving_avg_accuracy_train": [0.06080720154461977, 0.12537580352816996, 0.19011081280713588, 0.25238459073415576, 0.3106769043743799, 0.3647928430147843, 0.41505031509827267, 0.4611931017697392, 0.5041188439644891, 0.5434980602682525, 0.5792834409166303, 0.6125411786644376, 0.6428708151815302, 0.6710623458992114, 0.6971182812463057, 0.7206290048301008, 0.7429535556090877, 0.7627828373970623, 0.7815755626205343, 0.7985493249954317, 0.8142000600911729, 0.8293506758809205, 0.8422561333655029, 0.8542779821921124, 0.8656299610181669, 0.876048957810407, 0.8856539195067565, 0.8944332076156047, 0.9028135836171487, 0.9097561778209562, 0.9165229126424504, 0.9230245613698997, 0.928694575471005, 0.9343231438905896, 0.939374832477721, 0.9440120330097108, 0.9483227693659011, 0.9520605259114538, 0.9553687753286603, 0.9583346101089172, 0.9614456036361392, 0.9643221916725345, 0.9670715922219661, 0.9694995136914454, 0.9717194841973008, 0.9737384560894939, 0.9757438317972205, 0.9774463633865553, 0.9789507039824237, 0.9804325297520478, 0.9817684980935188, 0.9829824592960718, 0.9841843063724169, 0.9852915814268511, 0.9863299816544133, 0.9872807818520671, 0.9881365380787744, 0.9889694616518494, 0.9896889019819117, 0.9903503131230063, 0.9909990976214291, 0.9915295612962094, 0.9920906479118358, 0.9925886143706523, 0.9930925877550156, 0.9935461638009426, 0.9939218301589436, 0.9942762419716299, 0.9946091634959047, 0.9949204546606184, 0.995205230957661, 0.9954707941714187, 0.995698175319753, 0.9959423819318346, 0.9961667821315082, 0.9963780789552714, 0.9965682100478395, 0.9967160765430555, 0.9968840336208928, 0.9970630967766607, 0.9971731003430423, 0.9973093059337381, 0.9974411915606024, 0.9975436125831136, 0.9976520675450403, 0.9977473518619648, 0.997842408342435, 0.9979372597700962, 0.9980342517990389, 0.9981261949227064, 0.9981578265090164, 0.9982327618640672, 0.9982909030883746, 0.9983548559342991, 0.9984217140908691, 0.9984609961413152, 0.9985242157236123, 0.9985718127524416, 0.9986076746319593, 0.9986515760675729, 0.9987166639965299, 0.9987450161980674, 0.9987705331794512, 0.9987958236115061, 0.9988139347027365, 0.9988302346848439, 0.9988356040735024, 0.9988474119697235, 0.9988650145227511, 0.9988576053323808, 0.9988602376562855, 0.998885858235895, 0.9989182173527817, 0.9989426902603608, 0.9989670410259913, 0.9989726806733922, 0.9989893821001006, 0.9989881373424715, 0.9989939925070339, 0.9990062376015686, 0.9990056324426022, 0.9990004375019134, 0.9989934369064839, 0.998984811221788, 0.9989886738496091, 0.9990061011075053, 0.9990124850443739, 0.9990089299923175, 0.9990034052966571, 0.99901238396342, 0.9990158144658874, 0.9990235522157273, 0.9990374916370116, 0.9990453868185485, 0.9990501673331222, 0.9990521446474291, 0.9990655499743528, 0.9990752896197747, 0.9990840553006544, 0.9990919444134461, 0.9991083452101966, 0.9991161304808436, 0.9991138366291877, 0.9991187476091261, 0.9991278177886898, 0.9991266803550588, 0.9991210063671719, 0.9991275255221214, 0.9991217670175283, 0.9991305352562517, 0.9991361015222932, 0.99914343631054], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.059139242516942754, 0.12199943818241712, 0.18500907291274465, 0.2452111611975244, 0.3016107493793533, 0.3532910536300475, 0.4008440136379011, 0.44385831568149053, 0.4838835169164288, 0.5196722054168039, 0.5521394022320512, 0.5821951050340719, 0.6091943504135714, 0.6341691760085094, 0.6566515195145862, 0.6769650479095132, 0.6963754178832455, 0.7138875490053126, 0.7300004119041037, 0.7447328925981059, 0.7573389755145001, 0.7698004375263031, 0.7802903909498775, 0.7896591363522543, 0.7989821204956433, 0.8072751499746935, 0.8148344737385194, 0.8216175690981011, 0.8281231279356555, 0.8333555722957044, 0.8390301571971581, 0.8440671299469453, 0.848130361182296, 0.8523284967037502, 0.8561138781610107, 0.8599907657120031, 0.8637495487040557, 0.866663438666105, 0.8694731631353378, 0.8719927966523763, 0.8746510918177108, 0.8770944446088313, 0.8793412607371801, 0.881082633533944, 0.8825958933827334, 0.8839578272466438, 0.8855619856929131, 0.8867137890532152, 0.8877514415861467, 0.8889813861417639, 0.8898919942331598, 0.8907847837029161, 0.8919321501180161, 0.8930491996016965, 0.8943088327759394, 0.8949867244418696, 0.8957230152883452, 0.8967030598626733, 0.8973674324343879, 0.8978788890215215, 0.8986820263336012, 0.8991321771923344, 0.8997204184339443, 0.9002996931850529, 0.9007122066884602, 0.9013805556261654, 0.9018467628176904, 0.9023294434636321, 0.90279944763007, 0.9032834865361142, 0.9037903047217347, 0.9038130179430853, 0.9042201138803281, 0.9045854707151869, 0.9050373916877195, 0.9054298545144294, 0.9057332134248087, 0.9055291327167405, 0.9055153290083194, 0.905775578392879, 0.905886703017823, 0.9061098150014323, 0.9059535233544517, 0.9060367054693077, 0.9063913015827685, 0.9066351368800639, 0.9069054757899491, 0.9069900894025957, 0.9071994894890681, 0.9072180806380529, 0.9073446759533892, 0.9071778500184419, 0.9072586107620797, 0.9072946743376037, 0.9073403680954849, 0.9073814924775779, 0.9075538112738714, 0.9075857983693758, 0.9077132725139894, 0.9077140179282531, 0.9078052855631387, 0.9080583248720356, 0.9082616461875429, 0.9082350568229302, 0.908014784386119, 0.9079874376304891, 0.9079506185191721, 0.9079317473675561, 0.907913733822442, 0.9078120724130894, 0.9078070558720817, 0.907752683351515, 0.9077047775916648, 0.9076240118053899, 0.9075635296289924, 0.9076210179688041, 0.9075374506222249, 0.9074856245641441, 0.9076129680753502, 0.907592270383026, 0.9075715834426149, 0.907443101914995, 0.9074973374689774, 0.9075838000699712, 0.907700296521935, 0.9077288126152234, 0.9076568208491831, 0.9077161575895659, 0.9077552946073413, 0.9077294827670891, 0.9077072816195217, 0.9075886148280514, 0.9074929922383186, 0.9075310612373784, 0.9075897373990321, 0.9075316531546109, 0.9075892406158818, 0.9077641691521852, 0.9077730614425389, 0.9078176855976073, 0.9077113629621689, 0.9076156725902743, 0.9076404440454788, 0.9077125959888225, 0.9076544329166721, 0.9076265002142367, 0.9076003312733854, 0.9076632579540288, 0.9076588568103579, 0.9076548957810541, 0.9077367800734306, 0.9077759138601388], "moving_var_accuracy_train": [0.03327764183719209, 0.067471816912464, 0.09844022805834786, 0.12349841600824718, 0.14173051887319482, 0.15391428032026344, 0.1612551737902406, 0.1642920672674928, 0.16444643462747477, 0.16195829525501454, 0.1572878069428567, 0.15151372032948837, 0.14464132995787016, 0.13733005859993683, 0.1297072586412508, 0.12171131988802826, 0.11402565800657755, 0.1061618959519619, 0.09872420504869003, 0.0914447620262564, 0.0845047954049643, 0.07812018629374483, 0.07180712516034761, 0.06592713628720152, 0.06049422946788609, 0.055421805968507876, 0.05070992297435216, 0.046332613774200414, 0.042331428714125666, 0.03853208237122177, 0.03509097243539941, 0.03196231811743576, 0.029055427844852776, 0.02643501210245301, 0.024021186910439904, 0.021812600878360892, 0.019798582821918025, 0.017944461955670624, 0.016248516387961584, 0.014702830332659459, 0.013319651825931268, 0.012062159471918334, 0.010923976355157437, 0.00988463194359932, 0.008940523170661203, 0.008083157081108277, 0.007311035158559706, 0.006606019167017882, 0.00596578461597149, 0.0053889684228780365, 0.004866134883274951, 0.004392784711159188, 0.003966506167597546, 0.0035808900732533415, 0.0032325055412214185, 0.0029173911762420054, 0.00263224292709374, 0.0023752624894916212, 0.0021423945900391414, 0.001932092313313303, 0.0017426713739105175, 0.0015709367619118186, 0.0014166764494327523, 0.0012772405398364325, 0.001151802388402109, 0.001038473730626847, 0.0009358964844769658, 0.0008434373056260136, 0.0007600911057353414, 0.0006849541148648667, 0.000617188581232596, 0.0005561044374938483, 0.000500959313424024, 0.00045140011390608066, 0.0004067133015619949, 0.0003664437885353871, 0.00033012475817309863, 0.00029730906285945613, 0.00026783204279347137, 0.00024133741103790613, 0.00021731257699566544, 0.00019574828696252995, 0.00017633000263343743, 0.00015879141296276382, 0.0001430181339753862, 0.00012879803248731355, 0.0001159995508488966, 0.00010448056690397139, 9.411717729668017e-05, 8.478154140891985e-05, 7.631239228330027e-05, 6.873169082189947e-05, 6.188894535738539e-05, 5.5736860520163376e-05, 5.02034045860466e-05, 4.519695184282721e-05, 4.071322709881681e-05, 3.666229368331548e-05, 3.300763898460685e-05, 2.972422111058657e-05, 2.6789926945991116e-05, 2.4118168877380205e-05, 2.1712212036692625e-05, 1.9546747286605065e-05, 1.7595024662574544e-05, 1.5837913401067375e-05, 1.4254381533971728e-05, 1.283019821829309e-05, 1.1549967045321603e-05, 1.0395464405706939e-05, 9.355980327298501e-06, 8.426290021464411e-06, 7.593085031329143e-06, 6.839166837044545e-06, 6.160586791421253e-06, 5.544814362884383e-06, 4.992843365482809e-06, 4.493572973728525e-06, 4.044524222924144e-06, 3.641421281693204e-06, 3.2772824494802555e-06, 2.9497970912110703e-06, 2.655258457117262e-06, 2.390402233333811e-06, 2.1514962890435964e-06, 1.939080043999272e-06, 1.7455388314488152e-06, 1.5710986938600496e-06, 1.4142635248332968e-06, 1.2735627204615136e-06, 1.1463123635399801e-06, 1.0322199821392362e-06, 9.30746751117013e-07, 8.382330810288222e-07, 7.546154528022459e-07, 6.79189095468833e-07, 6.128875110313446e-07, 5.52452506164702e-07, 4.978987899997901e-07, 4.486690539055731e-07, 4.0622302372151474e-07, 3.6614621530078687e-07, 3.2957894956947694e-07, 2.968381141281245e-07, 2.6789471613115094e-07, 2.411168883154177e-07, 2.172949467307352e-07, 1.9594794648895865e-07, 1.7665159521640072e-07, 1.5967837378754785e-07, 1.4398938626759526e-07, 1.300746397084736e-07], "duration": 196628.57906, "accuracy_train": [0.6080720154461978, 0.7064932213801218, 0.7727258963178294, 0.8128485920773348, 0.8353077271363971, 0.8518362907784238, 0.8673675638496677, 0.8764781818129384, 0.8904505237172389, 0.8979110070021227, 0.9013518667520304, 0.9118608183947029, 0.9158375438353636, 0.9247861223583426, 0.931621699370155, 0.9322255170842562, 0.9438745126199704, 0.9412463734888336, 0.9507100896317828, 0.9513131863695091, 0.9550566759528424, 0.965706217988649, 0.9584052507267442, 0.9624746216315985, 0.9677977704526578, 0.9698199289405685, 0.9720985747739018, 0.9734468005952381, 0.9782369676310447, 0.9722395256552234, 0.9774235260358989, 0.9815393999169435, 0.9797247023809523, 0.9849802596668512, 0.9848400297619048, 0.9857468377976191, 0.9871193965716132, 0.9857003348214286, 0.985143020083518, 0.9850271231312293, 0.989444545381137, 0.9902114840000923, 0.9918161971668512, 0.9913508069167589, 0.99169921875, 0.9919092031192323, 0.9937922131667589, 0.9927691476905685, 0.9924897693452381, 0.9937689616786637, 0.9937922131667589, 0.9939081101190477, 0.9950009300595238, 0.9952570569167589, 0.9956755837024732, 0.9958379836309523, 0.99583834411914, 0.9964657738095238, 0.9961638649524732, 0.9963030133928571, 0.9968381581072352, 0.9963037343692323, 0.9971404274524732, 0.9970703125, 0.9976283482142857, 0.9976283482142857, 0.9973028273809523, 0.9974659482858066, 0.997605457214378, 0.9977220751430418, 0.9977682176310447, 0.9978608630952381, 0.9977446056547619, 0.9981402414405685, 0.9981863839285714, 0.99827975036914, 0.9982793898809523, 0.998046875, 0.9983956473214286, 0.9986746651785714, 0.9981631324404762, 0.99853515625, 0.9986281622023809, 0.9984654017857143, 0.9986281622023809, 0.9986049107142857, 0.9986979166666666, 0.9987909226190477, 0.9989071800595238, 0.9989536830357143, 0.9984425107858066, 0.9989071800595238, 0.9988141741071429, 0.9989304315476191, 0.9990234375, 0.9988145345953304, 0.9990931919642857, 0.9990001860119048, 0.9989304315476191, 0.9990466889880952, 0.9993024553571429, 0.9990001860119048, 0.9990001860119048, 0.9990234375, 0.9989769345238095, 0.9989769345238095, 0.9988839285714286, 0.9989536830357143, 0.9990234375, 0.9987909226190477, 0.9988839285714286, 0.9991164434523809, 0.9992094494047619, 0.9991629464285714, 0.9991861979166666, 0.9990234375, 0.9991396949404762, 0.9989769345238095, 0.9990466889880952, 0.9991164434523809, 0.9990001860119048, 0.9989536830357143, 0.9989304315476191, 0.9989071800595238, 0.9990234375, 0.9991629464285714, 0.9990699404761905, 0.9989769345238095, 0.9989536830357143, 0.9990931919642857, 0.9990466889880952, 0.9990931919642857, 0.9991629464285714, 0.9991164434523809, 0.9990931919642857, 0.9990699404761905, 0.9991861979166666, 0.9991629464285714, 0.9991629464285714, 0.9991629464285714, 0.9992559523809523, 0.9991861979166666, 0.9990931919642857, 0.9991629464285714, 0.9992094494047619, 0.9991164434523809, 0.9990699404761905, 0.9991861979166666, 0.9990699404761905, 0.9992094494047619, 0.9991861979166666, 0.9992094494047619], "end": "2016-02-05 18:15:37.478000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0], "moving_var_accuracy_valid": [0.03147705004929993, 0.06389198283628547, 0.09323471117230064, 0.11652986295970633, 0.13350509858745474, 0.14419227335570814, 0.15012460206971412, 0.15176421348541697, 0.15100594274195103, 0.1474328204889479, 0.14217660826141318, 0.13608905487358247, 0.12904078264578603, 0.12175038160268559, 0.11412444536814412, 0.10642575575399323, 0.09917404234124845, 0.09201671073505181, 0.08515165881870375, 0.07858990682342573, 0.07216113607953822, 0.06634261479082888, 0.06069870541720486, 0.05541879538921537, 0.05065917815033478, 0.046212229376764856, 0.04210529682098545, 0.03830886058280152, 0.034858875186621306, 0.03161939393378825, 0.028747262764643677, 0.0261008763385182, 0.023639377337313746, 0.021434058680290826, 0.01941961482725449, 0.017612925658276393, 0.015978789138680855, 0.014457327017211152, 0.01308264527982709, 0.011831517729386036, 0.010711964755121806, 0.009694498035366513, 0.008770481876261336, 0.007920725101590987, 0.0071492621897615094, 0.006451029745432352, 0.005829086689775752, 0.005258117879625405, 0.004741996596674757, 0.004281411809496314, 0.0038607334924117237, 0.003481833800506321, 0.003145498467670183, 0.002842178816844079, 0.0025722410167625526, 0.0023191527490829355, 0.0020921165920700768, 0.001891549319172099, 0.0017063669054813092, 0.0015380845054978785, 0.0013900813208265832, 0.0012528969109044895, 0.0011307214696390177, 0.0010206693558105632, 0.0009201339267439477, 0.0008321407467923378, 0.000750882814421971, 0.0006778913584334751, 0.0006120903578383494, 0.0005529899650175947, 0.0005000027505753162, 0.00045000711853160173, 0.00040649795059751696, 0.0003670495260887674, 0.00033218266656862325, 0.00030035064354490363, 0.00027114381884697216, 0.0002444042773809256, 0.00021996556452412861, 0.00019857857575118876, 0.00017883185631649044, 0.00016139668129991198, 0.0001454768568801639, 0.00013099144457023483, 0.00011902394574634491, 0.00010765665204157481, 9.754873497319852e-05, 8.785829664688476e-05, 7.946710254812794e-05, 7.152350297070031e-05, 6.451539003841613e-05, 5.831432906771403e-05, 5.2541596840358905e-05, 4.7299142389639165e-05, 4.258801942625901e-05, 3.834443841685595e-05, 3.477723848317445e-05, 3.13087232033663e-05, 2.832409780093446e-05, 2.5491693021622836e-05, 2.3017491750058933e-05, 2.1292002601676563e-05, 1.9534858357565394e-05, 1.7587735470603425e-05, 1.6265641441311322e-05, 1.464580790257156e-05, 1.3193427934937954e-05, 1.1877290224713987e-05, 1.0692481592510767e-05, 9.71624881262375e-06, 8.744850422514515e-06, 7.896972719197943e-06, 7.12793010371966e-06, 6.473845103441127e-06, 5.859383436052924e-06, 5.3031892753762695e-06, 4.835721860567243e-06, 4.376323137176381e-06, 4.084638152075372e-06, 3.6800298870757523e-06, 3.3158784439003004e-06, 3.132858125966199e-06, 2.8460457712116084e-06, 2.6287232264260187e-06, 2.487993713664632e-06, 2.2465128504860794e-06, 2.068506894836036e-06, 1.8933438441858434e-06, 1.7177948152103816e-06, 1.552011593564189e-06, 1.4012464527875295e-06, 1.3878580740893519e-06, 1.3313653836853847e-06, 1.211272083521521e-06, 1.1211309026871053e-06, 1.0393818274683239e-06, 9.652904859820724e-07, 1.1441613727031066e-06, 1.030456890882418e-06, 9.453330387342877e-07, 9.525402601200594e-07, 9.396960595680086e-07, 8.512490785477431e-07, 8.129772970474116e-07, 7.621260540003381e-07, 6.929355713883448e-07, 6.298053354370623e-07, 6.024627061245135e-07, 5.423907661025734e-07, 4.882928972706305e-07, 4.99808943585599e-07, 4.636111285861058e-07], "accuracy_test": 0.9089783960459183, "start": "2016-02-03 11:38:28.899000", "learning_rate_per_epoch": [0.0009708176949061453, 0.0009337663068436086, 0.0008981290156953037, 0.0008638518047519028, 0.0008308828109875321, 0.0007991720922291279, 0.0007686715689487755, 0.0007393351406790316, 0.0007111183367669582, 0.0006839784327894449, 0.0006578743341378868, 0.0006327664596028626, 0.0006086168577894568, 0.0005853889160789549, 0.0005630474770441651, 0.0005415587220340967, 0.0005208900547586381, 0.0005010102177038789, 0.0004818890884052962, 0.00046349773765541613, 0.0004458082839846611, 0.0004287939518690109, 0.0004124289844185114, 0.00039668858516961336, 0.00038154891808517277, 0.0003669870493467897, 0.00035298094735480845, 0.000339509395416826, 0.00032655199174769223, 0.0003140890912618488, 0.0003021018346771598, 0.0002905720903072506, 0.0002794823667500168, 0.0002688159001991153, 0.0002585565089248121, 0.0002486886805854738, 0.00023919744126033038, 0.00023006844276096672, 0.00022128784621600062, 0.00021284236572682858, 0.00020471921015996486, 0.0001969060831470415, 0.0001893911394290626, 0.0001821629994083196, 0.00017521072004456073, 0.0001685237803030759, 0.0001620920520508662, 0.00015590578550472856, 0.00014995562378317118, 0.00014423254469875246, 0.00013872788986191154, 0.00013343332102522254, 0.00012834082008339465, 0.00012344267452135682, 0.00011873147013830021, 0.0001142000692198053, 0.00010984161053784192, 0.000105649494798854, 0.00010161737009184435, 9.773913188837469e-05, 9.400890849065036e-05, 9.042104647960514e-05, 8.697011799085885e-05, 8.36508916108869e-05, 8.04583469289355e-05, 7.738764543319121e-05, 7.443413778673857e-05, 7.159334927564487e-05, 6.886097980896011e-05, 6.623288936680183e-05, 6.370510527631268e-05, 6.127379310782999e-05, 5.893527122680098e-05, 5.668599987984635e-05, 5.4522570280823857e-05, 5.244170824880712e-05, 5.044026329414919e-05, 4.851520498050377e-05, 4.666361564886756e-05, 4.4882694055559114e-05, 4.316974082030356e-05, 4.152216206421144e-05, 3.99374621338211e-05, 3.841324360109866e-05, 3.694719634950161e-05, 3.5537101211957633e-05, 3.4180822694906965e-05, 3.2876308978302404e-05, 3.1621581001672894e-05, 3.041473974008113e-05, 2.9253957109176554e-05, 2.8137475965195335e-05, 2.7063606466981582e-05, 2.603072061901912e-05, 2.5037254090420902e-05, 2.4081704395939596e-05, 2.316262362000998e-05, 2.227861841674894e-05, 2.142835182894487e-05, 2.0610536012100056e-05, 1.9823932234430686e-05, 1.9067349057877436e-05, 1.8339640519116074e-05, 1.7639706129557453e-05, 1.6966485418379307e-05, 1.6318957932526246e-05, 1.569614323670976e-05, 1.5097098184924107e-05, 1.4520916010951623e-05, 1.3966723599878605e-05, 1.3433681488095317e-05, 1.2920982953801285e-05, 1.2427852198015898e-05, 1.1953541616094299e-05, 1.1497333616716787e-05, 1.1058536074415315e-05, 1.063648596755229e-05, 1.0230543011857662e-05, 9.840093298407737e-06, 9.46454474615166e-06, 9.103328920900822e-06, 8.755899216339458e-06, 8.421729035035241e-06, 8.100312697933987e-06, 7.791163625370245e-06, 7.493812972825253e-06, 7.207810540421633e-06, 6.9327234086813405e-06, 6.668135029030964e-06, 6.413644769054372e-06, 6.168867002998013e-06, 5.933431566518266e-06, 5.706981482944684e-06, 5.4891738727747e-06, 5.27967904417892e-06, 5.078179583506426e-06, 4.884370355284773e-06, 4.697957592725288e-06, 4.518659352470422e-06, 4.346204150351696e-06, 4.180330506642349e-06, 4.020787855552044e-06, 3.867333816742757e-06, 3.7197364690655377e-06, 3.577772304197424e-06, 3.4412262266414473e-06, 3.309891326352954e-06, 3.1835688787396066e-06, 3.0620676625403576e-06, 2.945203505078098e-06, 2.8327995096333325e-06, 2.7246853733231546e-06], "accuracy_train_first": 0.6080720154461978, "accuracy_train_last": 0.9992094494047619, "batch_size_eval": 1024, "accuracy_train_std": [0.015822553909345487, 0.017663797287854135, 0.016464746533261763, 0.01629832972261365, 0.014723304037265053, 0.015358917028770331, 0.013762637174491839, 0.015619952005830641, 0.01509468272629187, 0.015199430698974498, 0.015166309015176542, 0.01519726385591988, 0.013947590291976196, 0.014429464197598696, 0.015825805154124576, 0.01531638539595021, 0.012500763079069029, 0.014937625066744979, 0.011994104950527357, 0.01198631195968561, 0.01166102857990352, 0.00994614863640186, 0.011243925959891278, 0.010277241395190949, 0.009910440070086525, 0.009343212130980955, 0.008293405704917966, 0.008797424137265246, 0.007906683089630518, 0.007256708839077762, 0.008364941793451002, 0.0064122911249979795, 0.007047924550626144, 0.005691519889096326, 0.005553942941530363, 0.00511030470744652, 0.004894533646304378, 0.005673601322696441, 0.005741481660740047, 0.005416776066771309, 0.004467298662918825, 0.0038689427810536295, 0.003794729071414817, 0.003762212927480689, 0.003747531138716419, 0.003619115014049651, 0.0031166262268544514, 0.0030892162480448478, 0.0031209872339455205, 0.002530645999403476, 0.0025023921103176673, 0.0030198303748240735, 0.002336398566426191, 0.0026326323006948577, 0.0020237041421742436, 0.0022152457941687874, 0.002142666650763392, 0.0021727392831517238, 0.0022291765009781733, 0.0018521091025881355, 0.0018310635095034058, 0.0018273482335551833, 0.0017365949725032387, 0.002077075096179994, 0.0018472864644755586, 0.001624944680957603, 0.001780523597384361, 0.0016378334101272382, 0.0015977002500510195, 0.0017523198791507466, 0.001449890526084043, 0.0017986495169097579, 0.0016573924281246964, 0.0014104832235559903, 0.00130622878582479, 0.001163434340084185, 0.0014423421148875393, 0.0014453376365253026, 0.001519196077373478, 0.001220869195761261, 0.001479532949212522, 0.0012652313858270824, 0.0011288352021892073, 0.0014318084373456707, 0.0015522871773134784, 0.001169991222601295, 0.0010367139513396556, 0.0011438219750975487, 0.0010698211560944328, 0.0010084285919922545, 0.0011084115638540502, 0.0010698211560944328, 0.0011967177485544803, 0.0010398381591795898, 0.0010655170421679315, 0.0011579116407677676, 0.0009139343381246999, 0.0010967704999135747, 0.000948478332141515, 0.0009162974560657567, 0.0008599907928249131, 0.0010758682385843819, 0.0009878462512206102, 0.001022008043891179, 0.0011266779612709931, 0.0011662887094479588, 0.0011390856318746177, 0.0009623419869128943, 0.0009995435853889824, 0.0009721235565646797, 0.0010346259050077655, 0.0008200889178082402, 0.0008847794638411667, 0.0007555839290026005, 0.0009268569302452915, 0.0008524136337343452, 0.0009338302468388767, 0.0010645017803440845, 0.0008653178044051523, 0.0009242283721331748, 0.0008386667037235601, 0.0008887423202016552, 0.0009721235565646797, 0.001048381725966843, 0.0009765625, 0.0010124414554072377, 0.0009029244716772506, 0.0009518921824060046, 0.0009623419869128942, 0.0009856546872315218, 0.0009645865193773968, 0.0008887423202016552, 0.0008932930018740023, 0.0008737115991469465, 0.0008084699893375052, 0.0008774163999308597, 0.0008764916678278815, 0.0010124414554072377, 0.0007850605941282614, 0.000867501772050671, 0.000948478332141515, 0.0008501909086115244, 0.0009384503381050302, 0.0008134698514014092, 0.0009346982534524639, 0.0009952071502526366, 0.0009754546578963464, 0.0009268569302452915, 0.001020949516373705, 0.0009346982534524639, 0.0009268569302452915, 0.0010915824585477588], "accuracy_test_std": 0.008754378722486569, "error_valid": [0.4086075748305723, 0.31225880082831325, 0.24790421451430722, 0.21297004423945776, 0.19079295698418675, 0.18158620811370485, 0.17117934629141573, 0.16901296592620485, 0.1558896719691265, 0.1582295980798193, 0.15565582643072284, 0.14730356974774095, 0.14781244117093373, 0.14105739363704817, 0.14100738893072284, 0.1402131965361446, 0.12893125235316272, 0.12850327089608427, 0.12498382200677716, 0.12267478115587349, 0.12920627823795183, 0.11804640436746983, 0.12530002823795183, 0.1260221550263554, 0.11711102221385539, 0.11808758471385539, 0.11713161238704817, 0.11733457266566272, 0.11332684252635539, 0.11955242846385539, 0.10989857868975905, 0.11060011530496983, 0.11530055769954817, 0.10988828360316272, 0.10981768872364461, 0.10511724632906627, 0.10242140436746983, 0.10711155167545183, 0.10523931664156627, 0.10533050169427716, 0.10142425169427716, 0.10091538027108427, 0.10043739410768071, 0.10324501129518071, 0.10378476797816272, 0.10378476797816272, 0.10000058829066272, 0.10291998070406627, 0.10290968561746983, 0.09994911285768071, 0.10191253294427716, 0.10118011106927716, 0.09774155214608427, 0.09689735504518071, 0.09435446865587349, 0.09891225056475905, 0.09765036709337349, 0.09447653896837349, 0.09665321442018071, 0.09751800169427716, 0.09408973785768071, 0.09681646507906627, 0.09498541039156627, 0.09448683405496983, 0.09557517178087349, 0.09260430393448793, 0.09395737245858427, 0.0933264307228916, 0.09297051487198793, 0.09236016330948793, 0.09164833160768071, 0.09598256306475905, 0.09211602268448793, 0.09212631777108427, 0.09089531955948793, 0.09103798004518071, 0.09153655638177716, 0.09630759365587349, 0.09460890436746983, 0.09188217714608427, 0.09311317535768071, 0.09188217714608427, 0.09545310146837349, 0.09321465549698793, 0.09041733339608427, 0.09117034544427716, 0.09066147402108427, 0.09224838808358427, 0.09091590973268071, 0.09261459902108427, 0.09151596620858427, 0.09432358339608427, 0.09201454254518071, 0.09238075348268071, 0.09224838808358427, 0.09224838808358427, 0.09089531955948793, 0.09212631777108427, 0.09113946018448793, 0.09227927334337349, 0.0913733057228916, 0.0896643213478916, 0.0899084619728916, 0.09200424745858427, 0.09396766754518071, 0.09225868317018071, 0.09238075348268071, 0.09223809299698793, 0.09224838808358427, 0.09310288027108427, 0.09223809299698793, 0.09273666933358427, 0.09272637424698793, 0.09310288027108427, 0.09298080995858427, 0.0918615869728916, 0.09321465549698793, 0.09298080995858427, 0.09124094032379515, 0.0925940088478916, 0.09261459902108427, 0.09371323183358427, 0.09201454254518071, 0.09163803652108427, 0.0912512354103916, 0.09201454254518071, 0.09299110504518071, 0.09174981174698793, 0.09189247223268071, 0.09250282379518071, 0.09249252870858427, 0.09347938629518071, 0.09336761106927716, 0.09212631777108427, 0.09188217714608427, 0.09299110504518071, 0.09189247223268071, 0.09066147402108427, 0.09214690794427716, 0.09178069700677716, 0.09324554075677716, 0.09324554075677716, 0.09213661285768071, 0.09163803652108427, 0.09286903473268071, 0.09262489410768071, 0.09263518919427716, 0.09177040192018071, 0.09238075348268071, 0.09238075348268071, 0.09152626129518071, 0.09187188205948793], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.038165123671927426, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 32, "valid_ratio": 0.15, "learning_rate": 0.0010093391911023776, "optimization": "adam", "nb_data_augmentation": 2, "learning_rate_decay_method": "exp", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 5.887233248831469e-07, "rotation_range": [0, 0], "momentum": 0.7193399328056745}, "accuracy_valid_max": 0.9103356786521084, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.9081281179405121, "accuracy_valid_std": [0.017685537824304372, 0.015761709991641483, 0.009355442779284684, 0.009782431310104799, 0.014892658516566987, 0.015421935019766613, 0.014626388874686674, 0.014329660171790475, 0.011170635823157095, 0.01234575121649595, 0.010504690446364737, 0.01163478305207056, 0.012275912775495999, 0.009835325703918898, 0.010275080402552279, 0.015897037557012016, 0.012918209865021242, 0.0179724913251742, 0.012497099321673852, 0.012311931245544597, 0.01221660033517046, 0.013029711221755176, 0.013512353963743274, 0.013181966645923165, 0.013773216232282401, 0.012537444345653423, 0.015444771964440768, 0.011068117787456717, 0.012776515840312159, 0.010800092115722217, 0.011050757849432408, 0.009919073310180557, 0.013444081466132863, 0.00916018800750769, 0.013214341348402298, 0.011406483300247046, 0.011602814548175813, 0.013676462261590229, 0.011875471434543287, 0.008934695790243533, 0.010441641802017242, 0.012287769741777126, 0.010376956931134326, 0.009374285725316251, 0.012600780112885129, 0.013138033986908848, 0.011909972797429303, 0.010730895691317846, 0.01262130419595654, 0.008372840397685221, 0.010111812539735204, 0.008482092610941452, 0.01122851595753296, 0.01233501454296866, 0.011732884576185269, 0.012546151476930029, 0.0125690031342358, 0.00880544327773517, 0.010339232664091728, 0.01027441642343536, 0.009737975956359798, 0.010678031289308993, 0.012273009420787679, 0.012258345581330947, 0.009250972030516625, 0.009030811500719955, 0.012467546090796769, 0.007892466837280416, 0.009698571979255553, 0.007428817083951075, 0.00846708448985161, 0.01144158454755226, 0.009230660005091209, 0.009323284172470692, 0.0069121397540954695, 0.008814429889138052, 0.011464389569739003, 0.00911513609156979, 0.008828965054026312, 0.006836091279069609, 0.007774586490739846, 0.007827878211940122, 0.008115713831416177, 0.008814420073242388, 0.0076436996542088844, 0.009951892072183029, 0.010109100201063762, 0.006899886953654195, 0.007986546670646685, 0.008375620833496324, 0.00866899826308901, 0.008503132053112221, 0.008401983226090457, 0.010246447699982184, 0.010250234999095578, 0.010003025910970695, 0.010612611813110474, 0.010025641464237168, 0.008141136036483351, 0.010510535060651836, 0.01189371109470446, 0.00969217347349408, 0.008557091635610099, 0.009547852224336125, 0.008659576607627971, 0.00888878525746578, 0.008629722611038561, 0.00842177894489611, 0.010250234999095578, 0.007435440105680718, 0.008407612147191013, 0.009119688299051425, 0.0068128706987036, 0.007338614180521817, 0.008550642251379257, 0.007798184954330138, 0.008284569706491638, 0.00856457246394551, 0.007798359699862087, 0.009721765747051666, 0.007922123982515624, 0.008172493226603388, 0.009380718437620833, 0.007703919486981723, 0.008291175709067582, 0.008749502077778167, 0.007998081466869644, 0.008347952293417114, 0.008354465718044376, 0.007212664249678791, 0.007712828112292387, 0.009359412326569942, 0.00953965506838201, 0.007589344011912381, 0.008286534453745747, 0.008697805328165274, 0.007670061459682499, 0.007252781702355476, 0.008782781118873971, 0.008408507274859834, 0.008108242774779138, 0.008137594119453318, 0.008390832508770396, 0.007355621436810241, 0.00939185994672094, 0.008747090749177065, 0.009569606420287115, 0.007678378089922292, 0.007281101580415532, 0.00798393864479806, 0.008329188232894403, 0.008239948849399333], "accuracy_valid": [0.5913924251694277, 0.6877411991716867, 0.7520957854856928, 0.7870299557605422, 0.8092070430158133, 0.8184137918862951, 0.8288206537085843, 0.8309870340737951, 0.8441103280308735, 0.8417704019201807, 0.8443441735692772, 0.852696430252259, 0.8521875588290663, 0.8589426063629518, 0.8589926110692772, 0.8597868034638554, 0.8710687476468373, 0.8714967291039157, 0.8750161779932228, 0.8773252188441265, 0.8707937217620482, 0.8819535956325302, 0.8746999717620482, 0.8739778449736446, 0.8828889777861446, 0.8819124152861446, 0.8828683876129518, 0.8826654273343373, 0.8866731574736446, 0.8804475715361446, 0.890101421310241, 0.8893998846950302, 0.8846994423004518, 0.8901117163968373, 0.8901823112763554, 0.8948827536709337, 0.8975785956325302, 0.8928884483245482, 0.8947606833584337, 0.8946694983057228, 0.8985757483057228, 0.8990846197289157, 0.8995626058923193, 0.8967549887048193, 0.8962152320218373, 0.8962152320218373, 0.8999994117093373, 0.8970800192959337, 0.8970903143825302, 0.9000508871423193, 0.8980874670557228, 0.8988198889307228, 0.9022584478539157, 0.9031026449548193, 0.9056455313441265, 0.901087749435241, 0.9023496329066265, 0.9055234610316265, 0.9033467855798193, 0.9024819983057228, 0.9059102621423193, 0.9031835349209337, 0.9050145896084337, 0.9055131659450302, 0.9044248282191265, 0.9073956960655121, 0.9060426275414157, 0.9066735692771084, 0.9070294851280121, 0.9076398366905121, 0.9083516683923193, 0.904017436935241, 0.9078839773155121, 0.9078736822289157, 0.9091046804405121, 0.9089620199548193, 0.9084634436182228, 0.9036924063441265, 0.9053910956325302, 0.9081178228539157, 0.9068868246423193, 0.9081178228539157, 0.9045468985316265, 0.9067853445030121, 0.9095826666039157, 0.9088296545557228, 0.9093385259789157, 0.9077516119164157, 0.9090840902673193, 0.9073854009789157, 0.9084840337914157, 0.9056764166039157, 0.9079854574548193, 0.9076192465173193, 0.9077516119164157, 0.9077516119164157, 0.9091046804405121, 0.9078736822289157, 0.9088605398155121, 0.9077207266566265, 0.9086266942771084, 0.9103356786521084, 0.9100915380271084, 0.9079957525414157, 0.9060323324548193, 0.9077413168298193, 0.9076192465173193, 0.9077619070030121, 0.9077516119164157, 0.9068971197289157, 0.9077619070030121, 0.9072633306664157, 0.9072736257530121, 0.9068971197289157, 0.9070191900414157, 0.9081384130271084, 0.9067853445030121, 0.9070191900414157, 0.9087590596762049, 0.9074059911521084, 0.9073854009789157, 0.9062867681664157, 0.9079854574548193, 0.9083619634789157, 0.9087487645896084, 0.9079854574548193, 0.9070088949548193, 0.9082501882530121, 0.9081075277673193, 0.9074971762048193, 0.9075074712914157, 0.9065206137048193, 0.9066323889307228, 0.9078736822289157, 0.9081178228539157, 0.9070088949548193, 0.9081075277673193, 0.9093385259789157, 0.9078530920557228, 0.9082193029932228, 0.9067544592432228, 0.9067544592432228, 0.9078633871423193, 0.9083619634789157, 0.9071309652673193, 0.9073751058923193, 0.9073648108057228, 0.9082295980798193, 0.9076192465173193, 0.9076192465173193, 0.9084737387048193, 0.9081281179405121], "seed": 244828988, "model": "residualv3", "loss_std": [0.32805365324020386, 0.20060701668262482, 0.18674397468566895, 0.17489255964756012, 0.16952338814735413, 0.15950451791286469, 0.15477405488491058, 0.15024566650390625, 0.14355529844760895, 0.13827109336853027, 0.13183465600013733, 0.12668918073177338, 0.12079843878746033, 0.11665377765893936, 0.11035355180501938, 0.10460133105516434, 0.10112882405519485, 0.0955064445734024, 0.09058284759521484, 0.08645129948854446, 0.08225523680448532, 0.07919857650995255, 0.07604604959487915, 0.07532896101474762, 0.07042054831981659, 0.06489308178424835, 0.06472177058458328, 0.0624542199075222, 0.05968746542930603, 0.054588496685028076, 0.056160818785429, 0.05346360802650452, 0.051768913865089417, 0.04599830508232117, 0.04307500645518303, 0.04581373929977417, 0.04380639269948006, 0.042952537536621094, 0.040282223373651505, 0.03981764614582062, 0.03733561933040619, 0.03632037341594696, 0.03404811769723892, 0.031180931255221367, 0.031209131702780724, 0.03128829970955849, 0.029210299253463745, 0.02894998900592327, 0.02788976952433586, 0.027606744319200516, 0.027405990287661552, 0.024799147620797157, 0.022570520639419556, 0.023288819938898087, 0.023051196709275246, 0.022852791473269463, 0.020599599927663803, 0.020852727815508842, 0.020239759236574173, 0.020996976643800735, 0.01888197287917137, 0.0197753943502903, 0.018214726820588112, 0.016994336619973183, 0.018038857728242874, 0.016599200665950775, 0.01580292358994484, 0.014593717642128468, 0.01390706468373537, 0.014330459758639336, 0.013892189599573612, 0.014198563061654568, 0.013167794793844223, 0.014992568641901016, 0.013567701913416386, 0.01335789542645216, 0.01113914418965578, 0.012547043152153492, 0.011437604203820229, 0.011355020105838776, 0.01110973209142685, 0.011759351938962936, 0.011826029978692532, 0.010422316379845142, 0.00979691557586193, 0.010511635802686214, 0.010659978725016117, 0.008810149505734444, 0.01009528711438179, 0.009173735976219177, 0.009423317387700081, 0.010031496174633503, 0.009028974920511246, 0.008586143143475056, 0.008682760410010815, 0.0077497223392128944, 0.008925730362534523, 0.00816487055271864, 0.008031251840293407, 0.007327585946768522, 0.007520390208810568, 0.007544554769992828, 0.0074759479612112045, 0.00772124296054244, 0.0069529470056295395, 0.006239924114197493, 0.006829087622463703, 0.007572515401989222, 0.007384801749140024, 0.0076771080493927, 0.006846306845545769, 0.006544631440192461, 0.006679318845272064, 0.006444315891712904, 0.006828035693615675, 0.006744501646608114, 0.005937459412962198, 0.0055463784374296665, 0.006565933581441641, 0.0061654639430344105, 0.005893546622246504, 0.00640951469540596, 0.005812039598822594, 0.005762898363173008, 0.006295503582805395, 0.005648461636155844, 0.005964355543255806, 0.0066293952986598015, 0.006421877071261406, 0.0050083911046385765, 0.006233856547623873, 0.004855903331190348, 0.005662968847900629, 0.006080807652324438, 0.00631737383082509, 0.0064923265017569065, 0.006369352340698242, 0.005556553602218628, 0.006002651061862707, 0.0055684433318674564, 0.0056074014864861965, 0.005222843028604984, 0.005752239376306534, 0.005462053697556257, 0.005213110242038965, 0.005425704177469015, 0.005898725241422653, 0.006095499265938997, 0.005144154652953148, 0.005997692700475454, 0.005647464655339718, 0.0061058420687913895]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:40 2016", "state": "available"}], "summary": "19acf3156d4fdf89aca314b92ee6a134"}