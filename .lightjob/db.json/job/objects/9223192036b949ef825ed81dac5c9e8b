{"content": {"hp_model": {"f0": 32, "f1": 64, "f2": 32, "f3": 16, "nonlin": "rectify", "nbg1": 7, "nbg3": 6, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.016864699828836906, 0.027037106322183004, 0.02225012964632918, 0.018952637112698924, 0.025839008027661366, 0.014853913734418677, 0.014101145489226377, 0.009008218900776268, 0.007842099888840161, 0.010456569483118476, 0.008534799507699604, 0.006049177942162569, 0.0051717474999766104, 0.0045792805070082624, 0.007065023067716839, 0.0042120376407562094, 0.005657421693284974, 0.0055959493001593585, 0.005823898544800741, 0.005361916016773374, 0.004759089434486649, 0.00831569492037216, 0.008068034324568308, 0.009729275475868708, 0.008218395158993117, 0.006721621799066039, 0.006545816912554626, 0.01250860612842889, 0.012390975948985502, 0.010830100260734147, 0.012127015839710124, 0.007970998603563465, 0.007571291686261691, 0.007191842974603768, 0.009913592717352732, 0.00959984917531204, 0.009414808531516041, 0.008859774568118062, 0.010252476663943381, 0.010430677138404, 0.009260796491221891, 0.008199239247607203, 0.011019359286213181, 0.009829778833443444, 0.010849877402383922, 0.007649669819951453, 0.008276948573756935, 0.011978874957438412, 0.007975984295702272, 0.006020283354035174, 0.006910046540985989, 0.0061082546637185025, 0.007388370396944331, 0.009687689772180737, 0.007362452354800354, 0.006164517052516467, 0.008631671567915033, 0.006904627312787999, 0.00886639211020553, 0.005178349611814087, 0.0037044471974621864, 0.006325093176742614, 0.009764037970271627, 0.006068283059409817, 0.007851705771562547, 0.007708019897997392, 0.007284366450868823, 0.0077369593958241875, 0.008686790318240213, 0.008416319743071754, 0.008766005515462829, 0.009373701931102856, 0.010438576599462394, 0.01039261942899179, 0.010870341099060592, 0.008077597132577741, 0.009279275917618457, 0.01002764688007355, 0.011050455530240021, 0.009714564946566872, 0.0067550177542468354, 0.010628137877985335, 0.007579729103283142, 0.008096621008860128, 0.010002070092204324, 0.008542561678260252, 0.011456855211652238, 0.01033903632172568, 0.00906505899332371, 0.010645177247935149, 0.009533668571536758, 0.009137011835113727, 0.009576377750420534, 0.010078445738683814, 0.01226328459888937, 0.010875656783147353, 0.011400341449812102, 0.01210130114213459, 0.011342004446315154, 0.009913674210609924, 0.01245710643927386, 0.012280619366391252, 0.01141473510136069, 0.010382716784655473, 0.012656145952169476, 0.013588725360445123, 0.012089048193443022, 0.012026408045241441, 0.01038802369032313, 0.011691327754219156, 0.011725025395160234, 0.012799341183053644, 0.010728815277390067, 0.010424007561437295, 0.008754630724904575, 0.009783915759878139, 0.011099681453533227, 0.009442755572937658, 0.011278935799804799, 0.009593516499143482, 0.009138210574982193, 0.00895500993056688, 0.009578421223482758, 0.009028047645260998, 0.008964861475564492, 0.00929770070238214, 0.01170779379747194, 0.008758499040652158, 0.011199150375529092, 0.009812249031122275, 0.010325340687559007, 0.010614582138607321, 0.013353618485542248, 0.012187656149551846, 0.011209769265509248, 0.011050466491779664, 0.011886251971352044, 0.011698566238806036, 0.012749418407538788, 0.01108511170174171, 0.012709387688965314, 0.010389519464579318, 0.010056559915116793, 0.010819775171985502, 0.009708066803492574, 0.011885799201352151], "moving_avg_accuracy_train": [0.05776987227182538, 0.11702522443994554, 0.18236245230741277, 0.24499933470102547, 0.30420674105881146, 0.3592787605783456, 0.4100757988685177, 0.4567577816950749, 0.49967132673062464, 0.5391978920030679, 0.5753182107185049, 0.6083889672813904, 0.6390153144451395, 0.6670555742520098, 0.6925848128770118, 0.7160493728406949, 0.7376626974556195, 0.7573239890507276, 0.7754050540910683, 0.7918291472999939, 0.806878259349941, 0.8206457105294264, 0.8334385952373733, 0.8450055257018696, 0.8555670419878103, 0.8652885012891677, 0.874207478425847, 0.8825367911452778, 0.8901680672725367, 0.8971942538084798, 0.903659727865847, 0.9096554018758202, 0.9151004087074335, 0.9201356653403815, 0.9246580957147966, 0.9288583832386473, 0.9326759885863403, 0.9361629506242548, 0.9394917705143027, 0.9423320315915638, 0.9449510455789558, 0.9474545704449713, 0.9497473064029658, 0.9518828483782561, 0.9537978967584075, 0.955746943686249, 0.9573754918367733, 0.9589644541079686, 0.9604386979794253, 0.9617795044054122, 0.9630839224876191, 0.9642369724223198, 0.9652839819099697, 0.9662703961785981, 0.9672791488560963, 0.968089333967026, 0.9689091453216154, 0.9697399093954894, 0.9704899943084229, 0.9710326093455577, 0.9716883014956272, 0.9721598778902228, 0.9727702004036646, 0.9733287912610002, 0.9738362093790401, 0.9740277826721716, 0.9744142204729224, 0.9748665740923891, 0.97530631653088, 0.975532420960064, 0.975721891955835, 0.9760319247806003, 0.9763365309597938, 0.9766035929281831, 0.976864911087838, 0.9770954471339084, 0.9773121941217912, 0.9774956767156567, 0.9776956882822786, 0.9779641264446377, 0.9781335330312094, 0.9782254729924388, 0.9783826958170875, 0.9785545314402514, 0.9787090393058239, 0.9789666429253061, 0.9791403935114207, 0.9793363326175045, 0.9794475375974946, 0.9795987753532952, 0.9797023372501823, 0.9797607017740566, 0.9797737383646004, 0.979859803960357, 0.979990741419157, 0.9801388481154196, 0.9801350324599316, 0.98021054528306, 0.9802529662357894, 0.9803702001527697, 0.9804059562137662, 0.9805357568698444, 0.9807083449829245, 0.9808079428109056, 0.9808882802608505, 0.9809163700896012, 0.980939361835486, 0.9810042322341633, 0.9810371110537057, 0.9810946037770081, 0.9811485642303335, 0.9812343310192787, 0.981346434410291, 0.9815078534288872, 0.9815135855682336, 0.9816069199043136, 0.9816467790282234, 0.9817549120969311, 0.9816755205492443, 0.981659691483661, 0.981729258828235, 0.981961605301447, 0.982042833942814, 0.9821228430688353, 0.9821135431715586, 0.9820935835687807, 0.9821197617048428, 0.9821712959106506, 0.9820944438089727, 0.9821785925436162, 0.9822730357417279, 0.9821882627081143, 0.9822863531385764, 0.9823537081867065, 0.9824631198062049, 0.9825545787685062, 0.9824602886715097, 0.9824242917580317, 0.9824710216930628, 0.9824245427357351, 0.982478006726512, 0.9824377686634492, 0.9823503650840645, 0.9824089577400176, 0.9823500479386995, 0.9824481277413135], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 142812973, "moving_var_accuracy_train": [0.03003622328072718, 0.05863337179776594, 0.09119061472663738, 0.11738196457789508, 0.13719342082864952, 0.1507704247514241, 0.15891643416776058, 0.1626376584365554, 0.16294804372056323, 0.16071438360863735, 0.15638504206471637, 0.1505896123150195, 0.1439724093488679, 0.1366514739443122, 0.12885200477283168, 0.12092207446415203, 0.11303408922592777, 0.10520977778802577, 0.09763112422616047, 0.09029576934316372, 0.0833044743702741, 0.07667991134106233, 0.07048484129931352, 0.06464050209271661, 0.05918036251976868, 0.054112887206323346, 0.0494175318641725, 0.04510017573115789, 0.041114285536016336, 0.037447162657555516, 0.03407866758487836, 0.030994333787895324, 0.028161733303672627, 0.025573744257542188, 0.023200441220210866, 0.021039178835736923, 0.01906642794747984, 0.01726921529101657, 0.015642023138658312, 0.014150424571675521, 0.01279711522290337, 0.011573812431405853, 0.010463740931823, 0.009458411694394743, 0.008545577217640156, 0.0077252085512184935, 0.0069765572178038275, 0.006301624705916984, 0.005691022790258036, 0.005138100368079944, 0.004639603890070647, 0.0041876092184308005, 0.0037787143563927806, 0.0034096000387376854, 0.003077798272543156, 0.0027759260445145897, 0.002504382255977154, 0.0022601555508973943, 0.0020392036421971495, 0.001837933157684158, 0.0016580092316767078, 0.0014942097671724953, 0.00134814123258897, 0.0012161353230431635, 0.0010968390490574834, 0.000987485447091506, 0.0008900809099469968, 0.0008029144331256988, 0.0007243633505230181, 0.0006523871243867861, 0.0005874715052722535, 0.000529589437916916, 0.0004774655584448505, 0.00043036090145500555, 0.00038793939593459394, 0.00034962377815797444, 0.0003150842136529829, 0.0002838787850479493, 0.0002558509481841971, 0.00023091438478887376, 0.00020808123363415113, 0.00018734918687897376, 0.00016883673934039103, 0.00015221881273884505, 0.0001372117855896745, 0.00012408784365364008, 0.00011195076268385274, 0.00010110121561510389, 9.110239298176479e-05, 8.219800941260465e-05, 7.407473406972601e-05, 6.669791842157711e-05, 6.002965615365646e-05, 5.409335611924735e-05, 4.883832207037573e-05, 4.4151910204638505e-05, 3.973685021721589e-05, 3.5814484873605554e-05, 3.224923222131926e-05, 2.9148003120802227e-05, 2.6244709271803912e-05, 2.3771872237488413e-05, 2.166276492472873e-05, 1.9585765978302795e-05, 1.7685276333245322e-05, 1.5923850046233977e-05, 1.4336222625020076e-05, 1.2940473880139046e-05, 1.1656155643095603e-05, 1.0520288797880546e-05, 9.494465492800243e-06, 8.611222422294034e-06, 7.863204712552781e-06, 7.311389137378604e-06, 6.580545940434124e-06, 6.000893031014045e-06, 5.415102475742303e-06, 4.978827073101448e-06, 4.537671526388309e-06, 4.0861594076046525e-06, 3.721100005724006e-06, 3.8348539576777865e-06, 3.510751391514839e-06, 3.217289394583593e-06, 2.896338847929443e-06, 2.6102904348239535e-06, 2.355429044610739e-06, 2.143788109463739e-06, 1.982565508308071e-06, 1.8480380433562345e-06, 1.7435098980465525e-06, 1.6338371132942948e-06, 1.5570489948990206e-06, 1.4421744179866756e-06, 1.4056950985194198e-06, 1.3404082647344917e-06, 1.286383039785405e-06, 1.1694067358263697e-06, 1.0721192436958197e-06, 9.843499605946508e-07, 9.116405493232316e-07, 8.350484098623087e-07, 8.202980400795271e-07, 7.691661300563251e-07, 7.234827992727456e-07, 7.377113484726681e-07], "duration": 214759.860237, "accuracy_train": [0.577698722718254, 0.650323393953027, 0.770397503114618, 0.80873127624354, 0.8370733982788853, 0.8549269362541528, 0.8672491434800664, 0.8768956271340901, 0.8858932320505721, 0.8949369794550572, 0.9004010791574382, 0.9060257763473607, 0.9146524389188816, 0.9194179125138427, 0.9223479605020304, 0.9272304125138427, 0.9321826189899409, 0.9342756134067, 0.9381346394541344, 0.9396459861803249, 0.9423202677994648, 0.9445527711447952, 0.9485745576088963, 0.9491078998823367, 0.9506206885612772, 0.9527816350013842, 0.9544782726559615, 0.957500605620155, 0.9588495524178663, 0.9604299326319674, 0.9618489943821521, 0.9636164679655776, 0.9641054701919527, 0.965452975036914, 0.965359969084533, 0.9666609709533037, 0.9670344367155776, 0.9675456089654854, 0.9694511495247323, 0.967894381286914, 0.9685221714654854, 0.9699862942391103, 0.9703819300249169, 0.9711027261558692, 0.9710333321797711, 0.9732883660368217, 0.9720324251914912, 0.9732651145487264, 0.973706892822536, 0.9738467622392949, 0.9748236852274824, 0.9746144218346253, 0.9747070672988187, 0.9751481245962532, 0.9763579229535806, 0.9753809999653931, 0.9762874475129198, 0.9772167860603543, 0.9772407585248246, 0.9759161446797711, 0.9775895308462532, 0.9764040654415835, 0.97826310302464, 0.978356108977021, 0.978402972441399, 0.9757519423103543, 0.9778921606796788, 0.9789377566675894, 0.9792639984772978, 0.9775673608227206, 0.977427130917774, 0.9788222202034883, 0.979077986572536, 0.9790071506436876, 0.9792167745247323, 0.9791702715485419, 0.9792629170127353, 0.9791470200604466, 0.9794957923818751, 0.9803800699058692, 0.9796581923103543, 0.9790529326435032, 0.9797977012389257, 0.9801010520487264, 0.9800996100959765, 0.981285075500646, 0.9807041487864526, 0.9810997845722591, 0.9804483824174051, 0.9809599151555003, 0.9806343943221669, 0.9802859824889257, 0.9798910676794942, 0.9806343943221669, 0.9811691785483574, 0.9814718083817828, 0.9801006915605389, 0.9808901606912146, 0.9806347548103543, 0.9814253054055924, 0.9807277607627353, 0.9817039627745479, 0.982261638000646, 0.9817043232627353, 0.9816113173103543, 0.9811691785483574, 0.9811462875484496, 0.9815880658222591, 0.9813330204295865, 0.9816120382867294, 0.9816342083102622, 0.982006232119786, 0.9823553649294019, 0.9829606245962532, 0.9815651748223514, 0.982446928929033, 0.982005511143411, 0.9827281097153008, 0.9809609966200628, 0.981517229893411, 0.9823553649294019, 0.9840527235603543, 0.9827738917151162, 0.9828429252030271, 0.9820298440960686, 0.9819139471437799, 0.9823553649294019, 0.9826351037629198, 0.9814027748938722, 0.982935931155408, 0.9831230245247323, 0.9814253054055924, 0.9831691670127353, 0.9829599036198781, 0.9834478243816908, 0.9833777094292175, 0.9816116777985419, 0.9821003195367294, 0.9828915911083426, 0.982006232119786, 0.9829591826435032, 0.9820756260958842, 0.9815637328696014, 0.9829362916435955, 0.9818198597268365, 0.9833308459648394], "end": "2016-01-28 23:49:32.934000", "learning_rate_per_epoch": [0.0011577786644920707, 0.0005788893322460353, 0.0003859262214973569, 0.00028944466612301767, 0.00023155573580879718, 0.00019296311074867845, 0.0001653969520702958, 0.00014472233306150883, 0.0001286420738324523, 0.00011577786790439859, 0.00010525260586291552, 9.648155537433922e-05, 8.905989670893177e-05, 8.26984760351479e-05, 7.718524284427986e-05, 7.236116653075442e-05, 6.810462946305051e-05, 6.432103691622615e-05, 6.093571937526576e-05, 5.7888933952199295e-05, 5.513231735676527e-05, 5.262630293145776e-05, 5.033820343669504e-05, 4.824077768716961e-05, 4.6311146434163675e-05, 4.452994835446589e-05, 4.28806924901437e-05, 4.134923801757395e-05, 3.992340134573169e-05, 3.859262142213993e-05, 3.734769779839553e-05, 3.618058326537721e-05, 3.508420195430517e-05, 3.405231473152526e-05, 3.307938823127188e-05, 3.2160518458113074e-05, 3.129131437162869e-05, 3.046785968763288e-05, 2.9686632842640392e-05, 2.8944466976099648e-05, 2.8238504455657676e-05, 2.7566158678382635e-05, 2.6925084966933355e-05, 2.631315146572888e-05, 2.5728413675096817e-05, 2.516910171834752e-05, 2.4633587599964812e-05, 2.4120388843584806e-05, 2.362813575018663e-05, 2.3155573217081837e-05, 2.270154254802037e-05, 2.2264974177232943e-05, 2.184488039347343e-05, 2.144034624507185e-05, 2.1050520444987342e-05, 2.0674619008786976e-05, 2.031190524576232e-05, 1.9961700672865845e-05, 1.9623366824816912e-05, 1.9296310711069964e-05, 1.8979977539856918e-05, 1.8673848899197765e-05, 1.8377439118921757e-05, 1.8090291632688604e-05, 1.7811978977988474e-05, 1.7542100977152586e-05, 1.72802774613956e-05, 1.702615736576263e-05, 1.6779400539235212e-05, 1.653969411563594e-05, 1.6306741599692032e-05, 1.6080259229056537e-05, 1.5859981431276537e-05, 1.5645657185814343e-05, 1.543704820505809e-05, 1.523392984381644e-05, 1.5036086551845074e-05, 1.4843316421320196e-05, 1.465542572987033e-05, 1.4472233488049824e-05, 1.4293563253886532e-05, 1.4119252227828838e-05, 1.394914033880923e-05, 1.3783079339191318e-05, 1.3620925528812222e-05, 1.3462542483466677e-05, 1.330780014541233e-05, 1.315657573286444e-05, 1.3008749192522373e-05, 1.2864206837548409e-05, 1.2722842257062439e-05, 1.258455085917376e-05, 1.2449232599465176e-05, 1.2316793799982406e-05, 1.2187143511255272e-05, 1.2060194421792403e-05, 1.1935861948586535e-05, 1.1814067875093315e-05, 1.169473398476839e-05, 1.1577786608540919e-05, 1.146315480582416e-05, 1.1350771274010185e-05, 1.1240569619985763e-05, 1.1132487088616472e-05, 1.1026463653251994e-05, 1.0922440196736716e-05, 1.0820361239893828e-05, 1.0720173122535925e-05, 1.0621822184475604e-05, 1.0525260222493671e-05, 1.0430438123876229e-05, 1.0337309504393488e-05, 1.024582888931036e-05, 1.015595262288116e-05, 1.0067640687339008e-05, 9.980850336432923e-06, 9.89554428088013e-06, 9.811683412408456e-06, 9.72923226072453e-06, 9.648155355534982e-06, 9.568418136041146e-06, 9.489988769928459e-06, 9.412834515387658e-06, 9.336924449598882e-06, 9.262229468731675e-06, 9.188719559460878e-06, 9.116367436945438e-06, 9.045145816344302e-06, 8.975028322311118e-06, 8.905989488994237e-06, 8.83800476003671e-06, 8.771050488576293e-06, 8.705103027750738e-06, 8.6401387306978e-06, 8.57613849802874e-06, 8.513078682881314e-06, 8.450939276372083e-06, 8.389700269617606e-06, 8.329342563229147e-06, 8.26984705781797e-06, 8.211196472984739e-06, 8.153370799846016e-06, 8.096354576991871e-06, 8.040129614528269e-06, 7.984680451045278e-06, 7.929990715638269e-06], "accuracy_valid": [0.5735083890248494, 0.6365084360881024, 0.7421242587537651, 0.7748302781438253, 0.7909435593938253, 0.8083201948418675, 0.8099071089043675, 0.8159709149096386, 0.8234480892319277, 0.8286868175828314, 0.8275984798569277, 0.8338755412274097, 0.837425875376506, 0.838280367564006, 0.8394907756024097, 0.8428278543862951, 0.8441603327371988, 0.8494505365210843, 0.8461649331701807, 0.8476297769201807, 0.8474871164344879, 0.8499491128576807, 0.8495829019201807, 0.8487990046121988, 0.8496637918862951, 0.8483621987951807, 0.8493387612951807, 0.8522169733621988, 0.848290133189006, 0.8525934793862951, 0.849022555064006, 0.8565614999058735, 0.8540686182228916, 0.8532758965549698, 0.8549539956701807, 0.8524919992469879, 0.8551672510353916, 0.8561746987951807, 0.8566526849585843, 0.8527155496987951, 0.8531023508094879, 0.8531126458960843, 0.8539671380835843, 0.8544760095067772, 0.8572630365210843, 0.8539465479103916, 0.8566629800451807, 0.8544039439006024, 0.8553305016942772, 0.8551981362951807, 0.8544760095067772, 0.8565614999058735, 0.8563070641942772, 0.8555334619728916, 0.8592161615210843, 0.8562158791415663, 0.8548216302710843, 0.8563070641942772, 0.8561644037085843, 0.8559408532567772, 0.8570394860692772, 0.8542215737951807, 0.8560320383094879, 0.8571512612951807, 0.8556761224585843, 0.8538450677710843, 0.8570394860692772, 0.8569277108433735, 0.8578733880835843, 0.8587587655308735, 0.8543848244540663, 0.8556658273719879, 0.8571409662085843, 0.8556555322853916, 0.8555231668862951, 0.8581278237951807, 0.8553099115210843, 0.8572630365210843, 0.8555437570594879, 0.8575071771460843, 0.8571615563817772, 0.8551672510353916, 0.8583719644201807, 0.8563070641942772, 0.8569174157567772, 0.8552996164344879, 0.8546789697853916, 0.8573851068335843, 0.8581278237951807, 0.8598368081701807, 0.8582601891942772, 0.8599794686558735, 0.8569071206701807, 0.8574262871799698, 0.8557878976844879, 0.8600809487951807, 0.8575277673192772, 0.8575174722326807, 0.8576395425451807, 0.8575174722326807, 0.8560320383094879, 0.8570394860692772, 0.8581072336219879, 0.8597353280308735, 0.8571718514683735, 0.8565614999058735, 0.8549745858433735, 0.8555334619728916, 0.8570291909826807, 0.8541906885353916, 0.8555231668862951, 0.8542112787085843, 0.8571821465549698, 0.8572630365210843, 0.8562761789344879, 0.8548216302710843, 0.8554319818335843, 0.8570600762424698, 0.8556761224585843, 0.8559511483433735, 0.8572027367281627, 0.8580469338290663, 0.8566835702183735, 0.8576601327183735, 0.8559614434299698, 0.8586264001317772, 0.8566423898719879, 0.8554319818335843, 0.8543127588478916, 0.8580057534826807, 0.8572733316076807, 0.8545877847326807, 0.8560011530496988, 0.8551981362951807, 0.8564085443335843, 0.8576292474585843, 0.8533361963478916, 0.8551981362951807, 0.8577616128576807, 0.8560526284826807, 0.8548216302710843, 0.8548731057040663, 0.8556967126317772, 0.8570497811558735, 0.8539671380835843, 0.8564188394201807], "accuracy_test": 0.7225306919642857, "start": "2016-01-26 12:10:13.074000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0], "accuracy_train_last": 0.9833308459648394, "batch_size_eval": 1024, "accuracy_train_std": [0.023505915402104276, 0.022815958740579827, 0.023239212495538175, 0.022042404103182752, 0.0204949749529335, 0.021792096904570978, 0.02138690858007477, 0.021429494573268846, 0.019940735414671753, 0.019761772996218493, 0.019195734276165118, 0.018673970857489744, 0.01663679807886798, 0.016749282118464055, 0.015097908807835646, 0.015691530521106018, 0.013951979315289777, 0.01327547031164153, 0.012843495674299179, 0.01233111066295203, 0.011192197113144507, 0.012528598237021238, 0.011386642641801543, 0.011512264952349917, 0.01076728758598942, 0.011222109113198231, 0.011382850261003061, 0.010874304501202574, 0.009997354919116628, 0.00994170543295797, 0.008789184534111026, 0.008360318194306693, 0.008708660032673971, 0.008007310431376353, 0.007314657240971478, 0.00823080252941991, 0.00801360389745132, 0.007290155762506205, 0.007160550949255222, 0.0065600175283931006, 0.007003573543518696, 0.00647274701628975, 0.007051314068406265, 0.00685953818526473, 0.005911173236193586, 0.005704101589880039, 0.005886317779647394, 0.006233405070665305, 0.006410675477598223, 0.00540255557725174, 0.005404086008979207, 0.005338607820508027, 0.005572603833881153, 0.005988946493965659, 0.005097563735662045, 0.005552063723700646, 0.005584324793459252, 0.005562857659413803, 0.0054026048270849805, 0.005269378813926349, 0.005285346892687031, 0.005424578839934155, 0.005170398633262855, 0.00566433335420528, 0.00525265383876551, 0.005916756395463425, 0.005497844221810457, 0.0047813026472010006, 0.004752313875643763, 0.005120785119204873, 0.004774921185402915, 0.00495700383450916, 0.005218559818229278, 0.0056048541126319, 0.005389017299747743, 0.004648806370059213, 0.005479994047156442, 0.0048550343162144585, 0.004971144706021917, 0.004864642303757407, 0.004611714918161846, 0.004942974398300898, 0.004849556643217141, 0.0044512398249226546, 0.004632258663880126, 0.004221831521526987, 0.0040617511064840505, 0.005002351392103201, 0.004919849678990202, 0.004820527304074425, 0.004502485343653838, 0.005004769939589007, 0.004722842436799274, 0.004621935500878599, 0.0041797654756696775, 0.004184831231474486, 0.005081926360096289, 0.004396304896570376, 0.004471019125967467, 0.004628027395645592, 0.004425740451622401, 0.003937552691645509, 0.004188848824033163, 0.004363013303494892, 0.004032344749409778, 0.004163436110178963, 0.004271010849245671, 0.004163434908493065, 0.0042712497054772465, 0.004466999266814929, 0.004603201694612388, 0.004079422600818779, 0.004178198308969724, 0.003656701824522088, 0.0044753776613736486, 0.004605742679811239, 0.00404192110346733, 0.0042661542463961625, 0.00416418368276036, 0.004649570623119646, 0.004178198308969724, 0.003949281740869305, 0.003627650364042001, 0.004286167045373547, 0.004331430996204783, 0.004126845090829217, 0.004353845676837415, 0.0044232752647170165, 0.0038797877761957912, 0.004391280549281127, 0.004065064830808601, 0.004412014945503653, 0.004281802286523722, 0.003860263821095625, 0.0036482283701966406, 0.003870171519597826, 0.004918708828828547, 0.004273005199267754, 0.004317905991559235, 0.004096086906320359, 0.0044983641401502685, 0.004106321011324658, 0.004687532422117611, 0.004248347248680849, 0.004507826834677175, 0.00439772764038784], "accuracy_test_std": 0.008509003979842408, "error_valid": [0.42649161097515065, 0.36349156391189763, 0.2578757412462349, 0.22516972185617468, 0.20905644060617468, 0.19167980515813254, 0.19009289109563254, 0.18402908509036142, 0.1765519107680723, 0.17131318241716864, 0.1724015201430723, 0.1661244587725903, 0.16257412462349397, 0.16171963243599397, 0.1605092243975903, 0.15717214561370485, 0.15583966726280118, 0.15054946347891573, 0.1538350668298193, 0.1523702230798193, 0.15251288356551207, 0.1500508871423193, 0.1504170980798193, 0.15120099538780118, 0.15033620811370485, 0.1516378012048193, 0.1506612387048193, 0.14778302663780118, 0.15170986681099397, 0.14740652061370485, 0.15097744493599397, 0.1434385000941265, 0.1459313817771084, 0.14672410344503017, 0.1450460043298193, 0.14750800075301207, 0.1448327489646084, 0.1438253012048193, 0.14334731504141573, 0.14728445030120485, 0.14689764919051207, 0.14688735410391573, 0.14603286191641573, 0.14552399049322284, 0.14273696347891573, 0.1460534520896084, 0.1433370199548193, 0.14559605609939763, 0.14466949830572284, 0.1448018637048193, 0.14552399049322284, 0.1434385000941265, 0.14369293580572284, 0.1444665380271084, 0.14078383847891573, 0.14378412085843373, 0.14517836972891573, 0.14369293580572284, 0.14383559629141573, 0.14405914674322284, 0.14296051393072284, 0.1457784262048193, 0.14396796169051207, 0.1428487387048193, 0.14432387754141573, 0.14615493222891573, 0.14296051393072284, 0.1430722891566265, 0.14212661191641573, 0.1412412344691265, 0.14561517554593373, 0.14433417262801207, 0.14285903379141573, 0.1443444677146084, 0.14447683311370485, 0.1418721762048193, 0.14469008847891573, 0.14273696347891573, 0.14445624294051207, 0.14249282285391573, 0.14283844361822284, 0.1448327489646084, 0.1416280355798193, 0.14369293580572284, 0.14308258424322284, 0.14470038356551207, 0.1453210302146084, 0.14261489316641573, 0.1418721762048193, 0.1401631918298193, 0.14173981080572284, 0.1400205313441265, 0.1430928793298193, 0.14257371282003017, 0.14421210231551207, 0.1399190512048193, 0.14247223268072284, 0.1424825277673193, 0.1423604574548193, 0.1424825277673193, 0.14396796169051207, 0.14296051393072284, 0.14189276637801207, 0.1402646719691265, 0.1428281485316265, 0.1434385000941265, 0.1450254141566265, 0.1444665380271084, 0.1429708090173193, 0.1458093114646084, 0.14447683311370485, 0.14578872129141573, 0.14281785344503017, 0.14273696347891573, 0.14372382106551207, 0.14517836972891573, 0.14456801816641573, 0.14293992375753017, 0.14432387754141573, 0.1440488516566265, 0.14279726327183728, 0.14195306617093373, 0.1433164297816265, 0.1423398672816265, 0.14403855657003017, 0.14137359986822284, 0.14335761012801207, 0.14456801816641573, 0.1456872411521084, 0.1419942465173193, 0.1427266683923193, 0.1454122152673193, 0.14399884695030118, 0.1448018637048193, 0.14359145566641573, 0.14237075254141573, 0.1466638036521084, 0.1448018637048193, 0.1422383871423193, 0.1439473715173193, 0.14517836972891573, 0.14512689429593373, 0.14430328736822284, 0.1429502188441265, 0.14603286191641573, 0.1435811605798193], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "momentum": 0.5005929127619781, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.0011577786485289093, "patience_threshold": 1, "do_flip": true, "batch_size": 16, "l2_decay": 2.7564622991932855e-05, "optimization": "rmsprop", "nb_data_augmentation": 1, "learning_rate_decay_method": "lin", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.04233228298412746}, "accuracy_valid_max": 0.8600809487951807, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        #nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        nb_data_augmentation=make_constant_param(1),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8564188394201807, "loss_train": [1.7396676540374756, 1.2785342931747437, 1.0101970434188843, 0.8870752453804016, 0.8046676516532898, 0.7477058172225952, 0.7032008767127991, 0.6650051474571228, 0.6336585283279419, 0.6069977283477783, 0.584492027759552, 0.5636790990829468, 0.5443001985549927, 0.52644282579422, 0.5114641189575195, 0.4962107241153717, 0.4846169352531433, 0.47288647294044495, 0.458565890789032, 0.45037907361984253, 0.43924441933631897, 0.43054452538490295, 0.4218080937862396, 0.4168558418750763, 0.40969544649124146, 0.4005628228187561, 0.3957257568836212, 0.3868566155433655, 0.3859636187553406, 0.37802836298942566, 0.3724469542503357, 0.3693382441997528, 0.36509430408477783, 0.35721683502197266, 0.35576385259628296, 0.3491620421409607, 0.3440445065498352, 0.3405350148677826, 0.3399188220500946, 0.3356074094772339, 0.33367958664894104, 0.3283035457134247, 0.3259979784488678, 0.3249809443950653, 0.3193885385990143, 0.31944960355758667, 0.3158673346042633, 0.3141750693321228, 0.3061680495738983, 0.30727118253707886, 0.3061191141605377, 0.30180853605270386, 0.2990174889564514, 0.29721125960350037, 0.2969544231891632, 0.2954005002975464, 0.29518401622772217, 0.29106229543685913, 0.29156869649887085, 0.28678545355796814, 0.28559210896492004, 0.2849506735801697, 0.2839927673339844, 0.2822662889957428, 0.2790890634059906, 0.2768588662147522, 0.2778065800666809, 0.27518463134765625, 0.27596408128738403, 0.27261677384376526, 0.27132171392440796, 0.26978540420532227, 0.2688327431678772, 0.2679806053638458, 0.2660103142261505, 0.2658348083496094, 0.26232823729515076, 0.2636474072933197, 0.2604149580001831, 0.2602386474609375, 0.25882992148399353, 0.2569502890110016, 0.25759634375572205, 0.2567436397075653, 0.25452566146850586, 0.2547530233860016, 0.2527448832988739, 0.2526276707649231, 0.2506088316440582, 0.24936755001544952, 0.24971462786197662, 0.2476496547460556, 0.24925430119037628, 0.24456867575645447, 0.24689853191375732, 0.24453623592853546, 0.24657900631427765, 0.24255532026290894, 0.24347364902496338, 0.24011684954166412, 0.2426709532737732, 0.24094538390636444, 0.24050630629062653, 0.23801803588867188, 0.23850108683109283, 0.2375340759754181, 0.23634949326515198, 0.23569273948669434, 0.23590856790542603, 0.23652705550193787, 0.23471051454544067, 0.23388594388961792, 0.23113928735256195, 0.23304033279418945, 0.23182769119739532, 0.23177681863307953, 0.2295321524143219, 0.2299019992351532, 0.22810478508472443, 0.2295905351638794, 0.22839292883872986, 0.22729529440402985, 0.22662217915058136, 0.22499345242977142, 0.22471919655799866, 0.22634737193584442, 0.22319869697093964, 0.22347350418567657, 0.22616592049598694, 0.22336675226688385, 0.2219032645225525, 0.22147701680660248, 0.2216099202632904, 0.2215602844953537, 0.22262617945671082, 0.22056519985198975, 0.22022834420204163, 0.21777081489562988, 0.2185308039188385, 0.21811817586421967, 0.21862131357192993, 0.21609576046466827, 0.21773508191108704, 0.21693387627601624, 0.21583707630634308, 0.2163887917995453], "accuracy_train_first": 0.577698722718254, "model": "residualv5", "loss_std": [0.32074224948883057, 0.2644210457801819, 0.24882079660892487, 0.24034202098846436, 0.23130002617835999, 0.22690927982330322, 0.22136174142360687, 0.21387453377246857, 0.2072226107120514, 0.20295493304729462, 0.20000401139259338, 0.1937420666217804, 0.19083811342716217, 0.18574434518814087, 0.18162600696086884, 0.1751173883676529, 0.17486803233623505, 0.17131473124027252, 0.16515128314495087, 0.16297858953475952, 0.15854765474796295, 0.1549711674451828, 0.15360575914382935, 0.15091198682785034, 0.15393605828285217, 0.14475485682487488, 0.1459798365831375, 0.14243775606155396, 0.1422545462846756, 0.13744641840457916, 0.13618579506874084, 0.13273610174655914, 0.13589631021022797, 0.13041426241397858, 0.12994343042373657, 0.12497569620609283, 0.12742242217063904, 0.12185882031917572, 0.12394626438617706, 0.12468376755714417, 0.12013064324855804, 0.11763492226600647, 0.11884476244449615, 0.11969072371721268, 0.11585512012243271, 0.11390475928783417, 0.11439301818609238, 0.11369630694389343, 0.10895706713199615, 0.11066757142543793, 0.1099858209490776, 0.10929226130247116, 0.10821293294429779, 0.10637453943490982, 0.10847707092761993, 0.10732059180736542, 0.11063626408576965, 0.10454380512237549, 0.10566500574350357, 0.10484876483678818, 0.10361811518669128, 0.10170520097017288, 0.10206588357686996, 0.10088527202606201, 0.1027212142944336, 0.09921664744615555, 0.10007108747959137, 0.10010848939418793, 0.09846244752407074, 0.09783463180065155, 0.09892447292804718, 0.0954282358288765, 0.09708155691623688, 0.0959814041852951, 0.09593100100755692, 0.09485854208469391, 0.09566973894834518, 0.09356359392404556, 0.09321706742048264, 0.0938192829489708, 0.09183047711849213, 0.09093349426984787, 0.09220491349697113, 0.09087184071540833, 0.08954731374979019, 0.08892614394426346, 0.08980835229158401, 0.09148186445236206, 0.08996403962373734, 0.08838164806365967, 0.08867274224758148, 0.08728044480085373, 0.08610667288303375, 0.0850880965590477, 0.08697044849395752, 0.08622440695762634, 0.08487718552350998, 0.08513357490301132, 0.08767790347337723, 0.0823865532875061, 0.0853375494480133, 0.0839150920510292, 0.08576002717018127, 0.08283048123121262, 0.08406378328800201, 0.08137943595647812, 0.08426162600517273, 0.08185426890850067, 0.08286212384700775, 0.08481425791978836, 0.07852872461080551, 0.08193066716194153, 0.07735643535852432, 0.07983773946762085, 0.07976195216178894, 0.0792669877409935, 0.0795457512140274, 0.07815056294202805, 0.0762774795293808, 0.07822948694229126, 0.08005516976118088, 0.0785377249121666, 0.07796268910169601, 0.0782952830195427, 0.07899485528469086, 0.07719286531209946, 0.0771506279706955, 0.07420279085636139, 0.07770179212093353, 0.0780348852276802, 0.07468480616807938, 0.07471085339784622, 0.07326330244541168, 0.07463672757148743, 0.07618644833564758, 0.0740167573094368, 0.073862224817276, 0.07192087918519974, 0.07396281510591507, 0.07446189969778061, 0.07351158559322357, 0.07082128524780273, 0.0726848617196083, 0.07515514642000198, 0.07119520753622055, 0.07301634550094604]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:20 2016", "state": "available"}], "summary": "54107c847ce77bc0cf0b2facd30679ef"}