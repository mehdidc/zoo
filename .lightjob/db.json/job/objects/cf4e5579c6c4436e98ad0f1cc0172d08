{"content": {"hp_model": {"f0": 16, "f1": 16, "f2": 32, "f3": 32, "nonlin": "rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.015518302607038587, 0.015134571382491394, 0.01434706985236291, 0.006963202068181584, 0.010960019938213189, 0.009555298298182179, 0.0127835404479596, 0.011438805924495157, 0.014270489093636523, 0.013076012599323391, 0.014117650885243345, 0.011444645526831257, 0.010594946201649113, 0.012659948682591798, 0.012388129942664262, 0.009698381512900915, 0.008757723538305048, 0.012141821899089007, 0.014383569478445695, 0.01570600386128582, 0.011136869669182538, 0.011547017977036687, 0.010909134481102508, 0.008464236734802556, 0.013582059864695266, 0.015804001087226124, 0.013650375297369033, 0.014569920548496916, 0.016070023812119638, 0.015212021430778556, 0.01901980190160362, 0.017951031912817147, 0.01867007128909483, 0.01594966653295231, 0.017190542709231045, 0.01871947706020805, 0.01680562919819819, 0.014209866868837868, 0.01599884599113717, 0.01442093497857959, 0.01257673934340284, 0.017353897439851315, 0.020694450701548635, 0.01385059372487083, 0.01562907719060102, 0.016798254802956564, 0.013243272164706199, 0.018601538131327915, 0.018239379147822123, 0.01645137982378891, 0.01008802354529783, 0.020778441143182762, 0.02059862099452172, 0.009599288112406294, 0.015520020864721488, 0.016777168309691823, 0.01473107042739826, 0.018107680248653005, 0.0193370880740103, 0.015712700602249028, 0.021688169216259094, 0.017248823682945863, 0.011379299079620243, 0.014981288087362422, 0.012910306329031666, 0.016617704437667018, 0.008101492815295444, 0.014421208262932706, 0.01373674569707839, 0.015320655446557101, 0.017069411407376077, 0.011076217722164366, 0.009323993148976795, 0.017963664600728863, 0.016437109504089838, 0.014469824690994415, 0.014958093191423723, 0.016112069224166244, 0.014984900392565047, 0.014787941662846495, 0.013564466507053436, 0.01296063878527363, 0.014095252071589142, 0.013825654076459871, 0.014073550521761204, 0.014547104120361934, 0.014161532736198997, 0.014625335889070149, 0.014620965188371419, 0.014273004168822465, 0.013988589582139775, 0.014259698165346868, 0.0135978239190029, 0.013882208778145555, 0.013682779279222835, 0.013429580308083732, 0.013675867338714356, 0.014009739088071885, 0.014182816759889952, 0.014427693871669159, 0.01442638101498072, 0.014905943960594657, 0.01436012258153055, 0.014592285666729767, 0.014078371728572623, 0.014256498006280694, 0.013984302309366326, 0.014088646358424438, 0.014264857306758697, 0.014765536555383913, 0.014821943186441162, 0.014696458563443498, 0.015128135958487948, 0.015310876508982205, 0.015536668412919808, 0.015447557793471964, 0.015447557793471964, 0.015619208771169095, 0.015582637031248256, 0.015401186003293611, 0.015636096280764594, 0.015670028657140532, 0.015543905962090671, 0.015768752072510907, 0.015854098757767006, 0.016188924716626477, 0.01635883277022364, 0.01653330432745977, 0.016453293017887425, 0.016640604982665258, 0.016496707629314177, 0.016496707629314177, 0.016064702500827095, 0.016278476747964645, 0.016247726982978637, 0.01622334969263476, 0.016640885683925247, 0.016640885683925247, 0.016164458748028028, 0.01633553351019277, 0.01633553351019277, 0.01648130677134414, 0.016686121812777392, 0.01673362671928023, 0.016391993765729078, 0.016391993765729078, 0.01645344762033928, 0.016370161999149438, 0.016370161999149438, 0.01632939614051193, 0.01636627372513455, 0.016317699397178077, 0.016317699397178077, 0.01641396603039453, 0.01602509967314196, 0.01627539025166573, 0.01627539025166573, 0.01634184256549737, 0.01634184256549737, 0.016288451101939053, 0.01621994278855503, 0.01597064934180487, 0.01597064934180487, 0.016002088626715514, 0.016039293416879867, 0.016039293416879867, 0.016039293416879867, 0.01600792720168355, 0.016031859366076796, 0.015933284266069707, 0.015964797133748685, 0.01579049063520367, 0.01573497176427072, 0.01573497176427072, 0.01573497176427072, 0.015708690280808176, 0.015592094874336245, 0.015746588304577758, 0.015746588304577758, 0.015640420346665337, 0.015640420346665337, 0.015564014997081165, 0.015564014997081165, 0.015564014997081165, 0.015564014997081165, 0.01567157127783813, 0.01567157127783813, 0.015845305130949075, 0.015845305130949075, 0.01555311456548938, 0.01555311456548938, 0.015696443501137317, 0.01563713535800809, 0.01558975556180962, 0.015336938339176624, 0.015336938339176624, 0.015336938339176624, 0.015163359015283786, 0.01528243264555728, 0.01528243264555728, 0.01528243264555728, 0.015329790245508903, 0.015329790245508903, 0.015367997655391286, 0.015367997655391286, 0.015367997655391286, 0.015367997655391286, 0.015367997655391286, 0.0151495296712562, 0.0151495296712562, 0.015118371848038324, 0.015118371848038324, 0.015118371848038324, 0.015290512865306386, 0.015290512865306386, 0.015290512865306386, 0.01536827802569682, 0.01536827802569682, 0.01536827802569682, 0.01536827802569682, 0.01536827802569682, 0.015572754392216902, 0.015818036253847322, 0.015943437199559176, 0.015943437199559176, 0.01599043145907465, 0.015924795701710643], "moving_avg_accuracy_train": [0.04245833477528607, 0.08772193094776669, 0.12904750224944628, 0.16721897148394238, 0.20469070551864155, 0.2411535704711148, 0.2761691629210502, 0.3096127091497093, 0.34060450555662836, 0.3703409653288079, 0.3982892445284391, 0.4247236004604235, 0.44940708955151665, 0.47253837441327157, 0.49421643931136855, 0.5142543623041427, 0.5330324324702308, 0.5510575629600571, 0.5676613606103877, 0.5832093532349802, 0.5980231799316004, 0.612018255320454, 0.6254738758882148, 0.6379744152039245, 0.6499989227999033, 0.6619623753600292, 0.6735803068843346, 0.6848267795585349, 0.6952205031807545, 0.7048142366216767, 0.7142204019279937, 0.7226300029369754, 0.7299521780712495, 0.7370743424277457, 0.7444788772579685, 0.7516611063015053, 0.7582250217418605, 0.7649185540822554, 0.771372993864829, 0.7785559002738427, 0.785936372331176, 0.7910840869864396, 0.7977559333403981, 0.7964771294985602, 0.8010777229751992, 0.806533822695721, 0.811384146965693, 0.8155820280943822, 0.8202946506875373, 0.826514352170094, 0.8333863731984703, 0.8387251263478923, 0.8437809039608882, 0.8472874364364772, 0.8533281043607512, 0.8522232507929854, 0.8584771464708851, 0.863580385242955, 0.863619162742792, 0.8666830825127174, 0.871300621206813, 0.8733480007447234, 0.8756878997347121, 0.8793213955447403, 0.8831656732856243, 0.8838774136524754, 0.886122440807669, 0.8903909876091465, 0.8932633089698172, 0.8988916214490813, 0.9006952072912071, 0.9038761040074076, 0.907576036721054, 0.910971008232365, 0.9147774335603835, 0.9211955386781824, 0.927506581461573, 0.9335376174368628, 0.9391236238848524, 0.944248685938043, 0.9489286711013908, 0.9531848355757847, 0.9570688620253583, 0.9605807618716412, 0.9637530974773434, 0.9666430767546182, 0.9692603341458322, 0.9716228412443535, 0.9737746742699274, 0.9757276000346106, 0.9775061595621111, 0.9791115134344807, 0.9805609822172323, 0.9818655041217088, 0.9830465492821663, 0.9841118150753875, 0.9850868303309532, 0.9859643440609623, 0.9867610818643991, 0.9874851213339209, 0.9881460574517285, 0.9887502005529933, 0.9893032299393698, 0.9898032815359182, 0.990267278865669, 0.9906872016112542, 0.9910651320822809, 0.9914075946550144, 0.9917251115657126, 0.9920201773805792, 0.9922834114651495, 0.9925179969924534, 0.9927337742646458, 0.9929302989584285, 0.9931094963316425, 0.9932730991163445, 0.993427317069005, 0.9935707635240185, 0.9937045156311497, 0.9938318679739964, 0.9939488102313678, 0.9940540582630022, 0.9941534317890921, 0.9942475182601921, 0.9943368463818012, 0.9944242171376779, 0.994502850817967, 0.9945782714278462, 0.994653125423166, 0.9947181688701443, 0.9947790331212343, 0.9948407503448252, 0.9948986209948665, 0.9949507045799036, 0.9950022301040562, 0.9950486030757935, 0.995090338750357, 0.9951302260062737, 0.9951684496854082, 0.9952028509966292, 0.9952338121767281, 0.9952686526852458, 0.9953023342917211, 0.9953303225887394, 0.995355512056056, 0.9953805077254504, 0.9954053289767149, 0.9954276681028529, 0.9954500984651866, 0.9954726109400965, 0.995495197316325, 0.9955155250549306, 0.9955338200196756, 0.9955502854879462, 0.9955651044093897, 0.9955784414386889, 0.9955927699138676, 0.9956056655415284, 0.9956172716064232, 0.995630042213638, 0.9956392106113219, 0.9956521124668563, 0.9956683744344563, 0.9956923108005344, 0.9957138535300047, 0.9957355671353376, 0.9957551093801372, 0.9957726974004568, 0.9957885266187445, 0.9958027729152035, 0.995817919730826, 0.9958338770136957, 0.9958482385682785, 0.995861163967403, 0.995872796826615, 0.9958855915487155, 0.9958971067986059, 0.9959097956723167, 0.9959212156586565, 0.9959314936463624, 0.9959407438352975, 0.9959513941541487, 0.9959609794411147, 0.9959696061993841, 0.9959796954306361, 0.9959864505899535, 0.9959925302333391, 0.9960003270611957, 0.9960073442062667, 0.99601598478564, 0.996023761307076, 0.9960307601763685, 0.9960370591587316, 0.9960427282428584, 0.9960478304185726, 0.9960524223767153, 0.9960588802878533, 0.9960646924078774, 0.9960699233158992, 0.9960746311331188, 0.9960788681686165, 0.9960826815005643, 0.9960861134993174, 0.9960892022981952, 0.9960943073659947, 0.9960989019270142, 0.9961007118831223, 0.9961023408436196, 0.9961014817592576, 0.9961030337321414, 0.9961044305077368, 0.996108012754582, 0.9961112367767428, 0.9961164635454972, 0.996116517339757, 0.9961188909034003, 0.9961210271106793, 0.99612527484604, 0.996131422956674, 0.9961392814050541, 0.9961463540085963, 0.9961527193517843, 0.9961584481606535, 0.9961659292374453, 0.996172662206558, 0.9961833721763783, 0.9961930111492167], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 722832890, "moving_var_accuracy_train": [0.016224391727012403, 0.03304109080049978, 0.04510720731114163, 0.05371003615170822, 0.06097621020064247, 0.06684445386545892, 0.07119483391049279, 0.07414158757857964, 0.07537185182147296, 0.07579297999736773, 0.0752436387896158, 0.0740082514725038, 0.07209089802864443, 0.06969731527998087, 0.06695703023151772, 0.06387499242914509, 0.06066103645869334, 0.0575190807754014, 0.05424834756557936, 0.050999173480910855, 0.04787430128539722, 0.04484963037311477, 0.041994150859575224, 0.03920110712227014, 0.03658229545637492, 0.03421218368516288, 0.032005752312777615, 0.029943525410003672, 0.027921438285619073, 0.025957651949076413, 0.024158170266096627, 0.022378845741659364, 0.020623489405766256, 0.01901766749127815, 0.01760934496661818, 0.016312670196260783, 0.015069168049807905, 0.013965481621554336, 0.012943871595560727, 0.012113831736330488, 0.011392690872799144, 0.010491912481067383, 0.009843343036898107, 0.00887372678660139, 0.00817684325097689, 0.007627080143321703, 0.007076102938704448, 0.006527092498569439, 0.006074262554062129, 0.00581499847744498, 0.005658520686830503, 0.005349189184861631, 0.005044318251824106, 0.004650548356662932, 0.0045139005417388165, 0.004073496800220778, 0.004018148020549172, 0.0038507206321770665, 0.0034656621022098027, 0.0032035843311976745, 0.0030751208704011337, 0.0028053346501113094, 0.0025740773306503306, 0.002435490223798733, 0.002324947443560367, 0.0020970118683525775, 0.0019326720038653296, 0.0019033892296464283, 0.0017873023766724777, 0.0018936732512833932, 0.0017335822231643045, 0.0016512869361200863, 0.0016093637612776789, 0.0015521598692134298, 0.001527343746292119, 0.00174533803139095, 0.0019292675773759454, 0.0020637013740555004, 0.002138162448982784, 0.0021607425535259987, 0.0021417886483357977, 0.0020906442078000576, 0.0020173507401689385, 0.0019266166309249429, 0.001824528386589305, 0.0017172433699380791, 0.0016071693592110534, 0.0014966853814050176, 0.0013886903115940697, 0.0012841465518159323, 0.0011842013625701063, 0.001088975675812883, 0.0009989867460011384, 0.0009144040679943549, 0.0008355174702342779, 0.0007621788441027142, 0.0006945168524297158, 0.0006319954403039355, 0.0005745090164203697, 0.0005217762131591601, 0.00047353012080964755, 0.0004294620087099372, 0.00038926838135870786, 0.00035259200761573285, 0.00031927044855230193, 0.00028893041970741, 0.00026132286070504297, 0.00023624610015804808, 0.00021352884303945763, 0.00019295953325143753, 0.00017428720957581012, 0.00015735376194481271, 0.00014203742423108483, 0.00012818127940537394, 0.00011565215675193771, 0.00010432783391720454, 9.410909911778922e-05, 8.488338117511375e-05, 7.655604969306069e-05, 6.904641229681139e-05, 6.226485049116252e-05, 5.613805977551242e-05, 5.061312967714916e-05, 4.563148708583074e-05, 4.1140153997039495e-05, 3.709484143817778e-05, 3.344100659544222e-05, 3.0148100351448946e-05, 2.718371840184216e-05, 2.450342241161136e-05, 2.2086420283996995e-05, 1.991205939678717e-05, 1.795099456633427e-05, 1.618030940817376e-05, 1.4586172384109085e-05, 1.3146909218267864e-05, 1.184789509522252e-05, 1.0677424524361338e-05, 9.622831518744427e-06, 8.67119941879356e-06, 7.812706828972303e-06, 7.042360895378997e-06, 6.348334861373987e-06, 5.720551478166484e-06, 5.154206913723052e-06, 4.6444092737470145e-06, 4.185513197001326e-06, 3.771453206310694e-06, 3.398835976069438e-06, 3.0635136822016174e-06, 2.7617536135016495e-06, 2.4892972047628413e-06, 2.243379835901756e-06, 2.0214818571198905e-06, 1.821310075302631e-06, 1.6407799549270976e-06, 1.478549706242915e-06, 1.3321914105335315e-06, 1.2001845761612538e-06, 1.081633914222845e-06, 9.74227058445357e-07, 8.783024734869149e-07, 7.928522904502593e-07, 7.187236079944932e-07, 6.510280499323472e-07, 5.901685708480742e-07, 5.345888077495248e-07, 4.839139731034413e-07, 4.377776531574808e-07, 3.958265005068819e-07, 3.583086846677151e-07, 3.247695300902207e-07, 2.94148865331514e-07, 2.6623757228111934e-07, 2.408317257740419e-07, 2.1822189741927657e-07, 1.9759311649768515e-07, 1.7928287249236367e-07, 1.6252833003512867e-07, 1.4722623031313615e-07, 1.3327370123985404e-07, 1.209671947405594e-07, 1.096973748025037e-07, 9.939742594641454e-08, 9.037381663709226e-08, 8.174712457000669e-08, 7.390507068627098e-08, 6.706167833926811e-08, 6.079867342985623e-08, 5.5390742594032153e-08, 5.039593690542961e-08, 4.579720075723467e-08, 4.157457529081279e-08, 3.770636439526598e-08, 3.417001772890251e-08, 3.09427906722745e-08, 2.8223853151443007e-08, 2.5705494488879868e-08, 2.338120662858306e-08, 2.1242557852483748e-08, 1.9279874295509974e-08, 1.748276037086129e-08, 1.584049187274607e-08, 1.4342308792037933e-08, 1.3142633367974198e-08, 1.2018359949840568e-08, 1.0846007424875775e-08, 9.785288293103839e-09, 8.813401697262103e-09, 7.953739106023289e-09, 7.1759240339957184e-09, 6.573824062746867e-09, 6.009990526514697e-09, 5.654863478359758e-09, 5.08940317492527e-09, 4.6311670967538485e-09, 4.209120820928763e-09, 3.950598040082083e-09, 3.895731615396041e-09, 4.06195535235769e-09, 4.105955304908744e-09, 4.0600181195254516e-09, 3.9493895671101115e-09, 4.058149200081215e-09, 4.060330137714881e-09, 4.686628205926641e-09, 5.054153561740997e-09], "duration": 40646.977515, "accuracy_train": [0.42458334775286083, 0.4950942965000923, 0.5009776439645626, 0.5107621945944075, 0.5419363118309339, 0.5693193550433739, 0.5913094949704688, 0.6106046252076412, 0.6195306732189, 0.6379691032784238, 0.64982375732512, 0.6626328038482835, 0.6715584913713547, 0.6807199381690661, 0.6893190233942414, 0.6945956692391104, 0.702035063965024, 0.7132837373684939, 0.7170955394633628, 0.7231412868563123, 0.7313476202011813, 0.7379739338201367, 0.746574460998062, 0.7504792690453119, 0.7582194911637136, 0.7696334484011628, 0.7781416906030824, 0.7860450336263382, 0.7887640157807309, 0.7911578375899779, 0.7988758896848468, 0.798316412017811, 0.7958517542797158, 0.8011738216362125, 0.811119690729974, 0.816301167693337, 0.8173002607050572, 0.8251603451458103, 0.8294629519079919, 0.8432020579549648, 0.8523606208471761, 0.8374135188838132, 0.8578025505260245, 0.7849678949220191, 0.8424830642649501, 0.8556387201804172, 0.8550370653954411, 0.8533629582525839, 0.8627082540259321, 0.8824916655131044, 0.8952345624538575, 0.8867739046926911, 0.8892829024778516, 0.8788462287167773, 0.9076941156792175, 0.8422795686830934, 0.9147622075719823, 0.9095095341915835, 0.8639681602413253, 0.8942583604420451, 0.9128584694536729, 0.8917744165859173, 0.8967469906446106, 0.9120228578349945, 0.9177641729535806, 0.8902830769541344, 0.9063276852044113, 0.9288079088224437, 0.9191142012158545, 0.9495464337624585, 0.9169274798703396, 0.9325041744532114, 0.9408754311438722, 0.9415257518341639, 0.9490352615125508, 0.9789584847383721, 0.9843059665120893, 0.9878169412144703, 0.9893976819167589, 0.9903742444167589, 0.9910485375715209, 0.9914903158453304, 0.9920251000715209, 0.9921878604881875, 0.9923041179286637, 0.9926528902500923, 0.9928156506667589, 0.9928854051310447, 0.9931411715000923, 0.9933039319167589, 0.9935131953096161, 0.9935596982858066, 0.9936062012619971, 0.9936062012619971, 0.9936759557262828, 0.993699207214378, 0.9938619676310447, 0.9938619676310447, 0.9939317220953304, 0.9940014765596161, 0.9940944825119971, 0.994187488464378, 0.9942804944167589, 0.9943037459048542, 0.9944432548334257, 0.9944665063215209, 0.9944665063215209, 0.9944897578096161, 0.9945827637619971, 0.994675769714378, 0.9946525182262828, 0.9946292667381875, 0.994675769714378, 0.9946990212024732, 0.9947222726905685, 0.9947455241786637, 0.9948152786429494, 0.99486178161914, 0.9949082845953304, 0.9949780390596161, 0.9950012905477114, 0.9950012905477114, 0.9950477935239018, 0.9950942965000923, 0.9951407994762828, 0.9952105539405685, 0.9952105539405685, 0.9952570569167589, 0.9953268113810447, 0.9953035598929494, 0.9953268113810447, 0.9953962053571429, 0.9954194568452381, 0.9954194568452381, 0.9954659598214286, 0.9954659598214286, 0.9954659598214286, 0.9954892113095238, 0.9955124627976191, 0.9955124627976191, 0.9955124627976191, 0.9955822172619048, 0.99560546875, 0.9955822172619048, 0.9955822172619048, 0.99560546875, 0.9956287202380952, 0.9956287202380952, 0.9956519717261905, 0.9956752232142857, 0.9956984747023809, 0.9956984747023809, 0.9956984747023809, 0.9956984747023809, 0.9956984747023809, 0.9956984747023809, 0.9957217261904762, 0.9957217261904762, 0.9957217261904762, 0.9957449776785714, 0.9957217261904762, 0.9957682291666666, 0.9958147321428571, 0.9959077380952381, 0.9959077380952381, 0.9959309895833334, 0.9959309895833334, 0.9959309895833334, 0.9959309895833334, 0.9959309895833334, 0.9959542410714286, 0.9959774925595238, 0.9959774925595238, 0.9959774925595238, 0.9959774925595238, 0.9960007440476191, 0.9960007440476191, 0.9960239955357143, 0.9960239955357143, 0.9960239955357143, 0.9960239955357143, 0.9960472470238095, 0.9960472470238095, 0.9960472470238095, 0.9960704985119048, 0.9960472470238095, 0.9960472470238095, 0.9960704985119048, 0.9960704985119048, 0.99609375, 0.99609375, 0.99609375, 0.99609375, 0.99609375, 0.99609375, 0.99609375, 0.9961170014880952, 0.9961170014880952, 0.9961170014880952, 0.9961170014880952, 0.9961170014880952, 0.9961170014880952, 0.9961170014880952, 0.9961170014880952, 0.9961402529761905, 0.9961402529761905, 0.9961170014880952, 0.9961170014880952, 0.99609375, 0.9961170014880952, 0.9961170014880952, 0.9961402529761905, 0.9961402529761905, 0.9961635044642857, 0.9961170014880952, 0.9961402529761905, 0.9961402529761905, 0.9961635044642857, 0.9961867559523809, 0.9962100074404762, 0.9962100074404762, 0.9962100074404762, 0.9962100074404762, 0.9962332589285714, 0.9962332589285714, 0.9962797619047619, 0.9962797619047619], "end": "2016-01-23 21:59:48.524000", "learning_rate_per_epoch": [0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964, 0.002064852276816964], "accuracy_valid": [0.4081384130271084, 0.46394660673945787, 0.4772434464420181, 0.48297045604292166, 0.5181796522025602, 0.5485457454819277, 0.5662665309676205, 0.5819635965737951, 0.5888098291603916, 0.6020654885165663, 0.6145475456513554, 0.6233571983245482, 0.627232563064759, 0.633214008377259, 0.6367746376129518, 0.6432546592620482, 0.6452180793486446, 0.6493375847138554, 0.6489507836031627, 0.6494390648531627, 0.6528982139495482, 0.6544954230986446, 0.655360210372741, 0.6566117987575302, 0.655604350997741, 0.6575471809111446, 0.6560823371611446, 0.656092632247741, 0.655360210372741, 0.6519628317959337, 0.6525628882718373, 0.6476491905120482, 0.6410368034638554, 0.6431840643825302, 0.6426957831325302, 0.6442929922816265, 0.638880718185241, 0.6426751929593373, 0.6427678487387049, 0.6419427710843373, 0.6536424016378012, 0.6461858174887049, 0.6512524708207832, 0.6115163780120482, 0.6369070030120482, 0.6416589208396084, 0.634730327560241, 0.6375085302146084, 0.6351788991905121, 0.6546086690512049, 0.6576810170368976, 0.6503156179405121, 0.6520554875753012, 0.6545880788780121, 0.6589326054216867, 0.6303666815700302, 0.6568059346762049, 0.6579560429216867, 0.6351583090173193, 0.6478536215173193, 0.6625550051769578, 0.6471314947289157, 0.6476403661521084, 0.6517495764307228, 0.6579854574548193, 0.6354318641754518, 0.6494096503200302, 0.6534997411521084, 0.6582913685993976, 0.6608548451618976, 0.6467035132718373, 0.6543645284262049, 0.6518613516566265, 0.6551072453878012, 0.6548425145896084, 0.6696644978350903, 0.6700615940323795, 0.6692173969314759, 0.6696953830948795, 0.6690644413591867, 0.6693085819841867, 0.6696747929216867, 0.6710381565323795, 0.6715264377823795, 0.6718926487198795, 0.6725030002823795, 0.6720147190323795, 0.6720147190323795, 0.6722588596573795, 0.6721367893448795, 0.6718926487198795, 0.6720250141189759, 0.6716485080948795, 0.6722691547439759, 0.6725132953689759, 0.6721367893448795, 0.6721367893448795, 0.6728795063064759, 0.6727574359939759, 0.6732457172439759, 0.6731236469314759, 0.6731236469314759, 0.6731236469314759, 0.6733677875564759, 0.6731133518448795, 0.6732354221573795, 0.6736016330948795, 0.6733574924698795, 0.6732354221573795, 0.6734795627823795, 0.6734795627823795, 0.6737237034073795, 0.6737237034073795, 0.6736016330948795, 0.6738457737198795, 0.6739678440323795, 0.6739678440323795, 0.6736016330948795, 0.6738457737198795, 0.6739678440323795, 0.6738457737198795, 0.6737237034073795, 0.6732251270707832, 0.6727368458207832, 0.6726147755082832, 0.6726147755082832, 0.6724927051957832, 0.6723706348832832, 0.6724927051957832, 0.6724927051957832, 0.6724927051957832, 0.6724927051957832, 0.6724927051957832, 0.6724927051957832, 0.6726147755082832, 0.6727368458207832, 0.6732354221573795, 0.6732354221573795, 0.6727368458207832, 0.6726147755082832, 0.6726147755082832, 0.6731133518448795, 0.6736119281814759, 0.6737339984939759, 0.6738560688064759, 0.6738560688064759, 0.6737339984939759, 0.6738560688064759, 0.6738560688064759, 0.6739781391189759, 0.6737339984939759, 0.6736119281814759, 0.6736119281814759, 0.6734898578689759, 0.6733677875564759, 0.6734898578689759, 0.6734898578689759, 0.6733677875564759, 0.6733677875564759, 0.6736119281814759, 0.6737339984939759, 0.6736119281814759, 0.6736119281814759, 0.6734898578689759, 0.6734898578689759, 0.6734898578689759, 0.6734898578689759, 0.6736119281814759, 0.6734898578689759, 0.6736119281814759, 0.6734898578689759, 0.6736119281814759, 0.6737339984939759, 0.6737339984939759, 0.6737339984939759, 0.6738560688064759, 0.6741002094314759, 0.6738560688064759, 0.6738560688064759, 0.6734898578689759, 0.6734898578689759, 0.6734898578689759, 0.6734898578689759, 0.6734898578689759, 0.6734898578689759, 0.6733677875564759, 0.6733677875564759, 0.6732457172439759, 0.6732457172439759, 0.6728692112198795, 0.6728692112198795, 0.6726250705948795, 0.6725030002823795, 0.6726250705948795, 0.6725030002823795, 0.6725030002823795, 0.6725030002823795, 0.6726250705948795, 0.6725030002823795, 0.6725030002823795, 0.6725030002823795, 0.6723809299698795, 0.6723809299698795, 0.6725030002823795, 0.6725030002823795, 0.6725030002823795, 0.6725030002823795, 0.6725030002823795, 0.6722588596573795, 0.6722588596573795, 0.6723809299698795, 0.6723809299698795, 0.6723809299698795, 0.6722588596573795, 0.6722588596573795, 0.6722588596573795, 0.6722588596573795, 0.6722588596573795, 0.6722588596573795, 0.6722588596573795, 0.6722588596573795, 0.6720147190323795, 0.6721367893448795, 0.6720147190323795, 0.6720147190323795, 0.6721367893448795, 0.6723809299698795], "accuracy_test": 0.6760549363057324, "start": "2016-01-23 10:42:21.546000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0, 211.0, 212.0, 213.0, 214.0, 215.0, 216.0, 217.0, 218.0, 219.0, 220.0, 221.0, 222.0, 223.0, 224.0, 225.0, 226.0], "accuracy_train_last": 0.9962797619047619, "batch_size_eval": 1024, "accuracy_train_std": [0.017856547400575653, 0.016116501038631645, 0.015431505533463665, 0.014940850295843223, 0.015142919760887717, 0.015017776117290136, 0.015154925182758556, 0.016082755126471632, 0.016054067118814738, 0.015556224358939013, 0.014858152498786881, 0.015281723924138799, 0.014678628825871363, 0.014618445566228442, 0.013602063388435824, 0.013935069691997988, 0.014332400681077424, 0.016750592464518793, 0.016733945279893758, 0.016261494614271226, 0.01718588868719816, 0.017222871572313156, 0.017451709092156358, 0.01874478861813644, 0.018810758301917207, 0.020210454745986116, 0.020955534312026056, 0.021760345372286737, 0.02277462197348307, 0.022960660805108366, 0.022808839137061176, 0.023348803738299673, 0.02335045178351461, 0.024961798220724397, 0.026342348931975984, 0.028144119856276364, 0.02748121190603964, 0.027663375604668975, 0.02660700649196917, 0.027199400910784072, 0.02672459505545269, 0.02718349914187171, 0.026909605645707164, 0.0285619330783953, 0.02533635425310265, 0.027462572901265216, 0.025778844353566293, 0.02653277549039429, 0.027013623206074283, 0.024003901533713085, 0.02249931140142159, 0.021682478091961308, 0.022575472523376532, 0.021384595870745565, 0.02262209152858632, 0.02222493666193768, 0.01974089179222044, 0.0187063754230411, 0.01881262796445105, 0.01851781414203702, 0.020125723848508147, 0.020784752578539428, 0.02088745614781548, 0.018021977578693043, 0.01720905838299031, 0.01979656964161969, 0.0179743114419755, 0.014635263719409737, 0.01482994776267918, 0.012631408267885932, 0.017733486209383986, 0.014472757248533075, 0.014095128211051937, 0.013162574420662751, 0.013351551076517424, 0.007421772759859868, 0.005803959000105536, 0.004421581923683694, 0.0037015569213271877, 0.0035639489592643844, 0.003328971927993907, 0.003156448732544326, 0.0029297835365687955, 0.002976537757654171, 0.002985682000052226, 0.0029010556275917286, 0.002874168594647135, 0.0028059155174751885, 0.0026749877056912205, 0.0027010119379840786, 0.0026607522559252937, 0.002611111506182975, 0.0025685194251022473, 0.002577344583057064, 0.002689729822585694, 0.002672486058759767, 0.0026646642841180943, 0.00259559843688628, 0.002544789940997317, 0.0025181914660164775, 0.0024577693097559633, 0.0024853173062894672, 0.0024263051059944794, 0.00236320261172415, 0.0023590598254955484, 0.002370373184681928, 0.0023799332127908876, 0.0024003998981473773, 0.00227789003583798, 0.0022878218949388433, 0.0023469652869071503, 0.002337389653167267, 0.0022578507113853925, 0.0022063781632364497, 0.0021951971097959595, 0.0022146859883891376, 0.0022099092966905427, 0.0021534013832704016, 0.0021793474673033257, 0.002158772999100304, 0.0021862670704355914, 0.0021862670704355914, 0.0021778354437872703, 0.0021683739722594035, 0.0021788128002907534, 0.002161237112831761, 0.002161237112831761, 0.0021375859213115344, 0.002083395322818617, 0.0021018713490178315, 0.002094265774401258, 0.002248071422111574, 0.0022702074757968067, 0.0022702074757968067, 0.0022934266493824774, 0.0022934266493824774, 0.0022934266493824774, 0.0022848062679572635, 0.0022659168656445805, 0.0022659168656445805, 0.0022659168656445805, 0.002186134537489427, 0.0021862581841362544, 0.0021964965920051233, 0.0021964965920051233, 0.002196619655377044, 0.0021964965920051233, 0.0021964965920051233, 0.0022167096131352473, 0.0022058298410772737, 0.0022457856346144297, 0.0022457856346144297, 0.0022457856346144297, 0.0022457856346144297, 0.0022457856346144297, 0.002235652042236118, 0.002244702080756542, 0.002244702080756542, 0.002244702080756542, 0.0022735390189582657, 0.0023242185639136535, 0.002331186357792835, 0.002346903437675882, 0.0023464426732780624, 0.0023464426732780624, 0.0023238696258062576, 0.0023238696258062576, 0.0023238696258062576, 0.0023238696258062576, 0.0023238696258062576, 0.002310687918304473, 0.002326659665381327, 0.002326659665381327, 0.002326659665381327, 0.002326659665381327, 0.002351965888467595, 0.002351965888467595, 0.002299077231767693, 0.002299077231767693, 0.002299077231767693, 0.002299077231767693, 0.002314428411110362, 0.002314428411110362, 0.002314428411110362, 0.0023196782743720667, 0.002314428411110362, 0.002314428411110362, 0.00230001764585008, 0.00230001764585008, 0.0022552744890219755, 0.0022552744890219755, 0.0022552744890219755, 0.0022552744890219755, 0.0022552744890219755, 0.0022552744890219755, 0.0022552744890219755, 0.0022501146321447013, 0.0022501146321447013, 0.0022501146321447013, 0.0022501146321447013, 0.0022501146321447013, 0.0022501146321447013, 0.0022501146321447013, 0.0022501146321447013, 0.0022547950004465508, 0.0022547950004465508, 0.002260183382046177, 0.002260183382046177, 0.0022350474034036304, 0.0022298407414924636, 0.0022298407414924636, 0.002234563574541288, 0.002234563574541288, 0.0022390350022982062, 0.0022400006239074294, 0.002244702080756542, 0.002244702080756542, 0.0022288707182854988, 0.0022432565281235743, 0.002237102508679082, 0.002237102508679082, 0.002237102508679082, 0.002237102508679082, 0.0022509554148588077, 0.0022509554148588077, 0.002267705606442822, 0.002267705606442822], "accuracy_test_std": 0.05660885414976008, "error_valid": [0.5918615869728916, 0.5360533932605421, 0.5227565535579819, 0.5170295439570783, 0.48182034779743976, 0.4514542545180723, 0.4337334690323795, 0.41803640342620485, 0.4111901708396084, 0.39793451148343373, 0.3854524543486446, 0.37664280167545183, 0.37276743693524095, 0.36678599162274095, 0.36322536238704817, 0.35674534073795183, 0.3547819206513554, 0.3506624152861446, 0.3510492163968373, 0.3505609351468373, 0.34710178605045183, 0.3455045769013554, 0.34463978962725905, 0.3433882012424698, 0.34439564900225905, 0.3424528190888554, 0.3439176628388554, 0.34390736775225905, 0.34463978962725905, 0.34803716820406627, 0.3474371117281627, 0.35235080948795183, 0.3589631965361446, 0.3568159356174698, 0.3573042168674698, 0.3557070077183735, 0.36111928181475905, 0.3573248070406627, 0.35723215126129515, 0.3580572289156627, 0.3463575983621988, 0.35381418251129515, 0.3487475291792168, 0.38848362198795183, 0.36309299698795183, 0.3583410791603916, 0.36526967243975905, 0.3624914697853916, 0.36482110080948793, 0.34539133094879515, 0.34231898296310237, 0.34968438205948793, 0.3479445124246988, 0.34541192112198793, 0.34106739457831325, 0.3696333184299698, 0.34319406532379515, 0.34204395707831325, 0.3648416909826807, 0.3521463784826807, 0.33744499482304224, 0.35286850527108427, 0.3523596338478916, 0.34825042356927716, 0.3420145425451807, 0.36456813582454817, 0.3505903496799698, 0.3465002588478916, 0.34170863140060237, 0.33914515483810237, 0.3532964867281627, 0.34563547157379515, 0.3481386483433735, 0.3448927546121988, 0.3451574854103916, 0.3303355021649097, 0.3299384059676205, 0.33078260306852414, 0.3303046169051205, 0.33093555864081325, 0.33069141801581325, 0.33032520707831325, 0.3289618434676205, 0.3284735622176205, 0.3281073512801205, 0.3274969997176205, 0.3279852809676205, 0.3279852809676205, 0.3277411403426205, 0.3278632106551205, 0.3281073512801205, 0.32797498588102414, 0.3283514919051205, 0.32773084525602414, 0.32748670463102414, 0.3278632106551205, 0.3278632106551205, 0.32712049369352414, 0.32724256400602414, 0.32675428275602414, 0.32687635306852414, 0.32687635306852414, 0.32687635306852414, 0.32663221244352414, 0.3268866481551205, 0.3267645778426205, 0.3263983669051205, 0.3266425075301205, 0.3267645778426205, 0.3265204372176205, 0.3265204372176205, 0.3262762965926205, 0.3262762965926205, 0.3263983669051205, 0.3261542262801205, 0.3260321559676205, 0.3260321559676205, 0.3263983669051205, 0.3261542262801205, 0.3260321559676205, 0.3261542262801205, 0.3262762965926205, 0.3267748729292168, 0.3272631541792168, 0.3273852244917168, 0.3273852244917168, 0.3275072948042168, 0.3276293651167168, 0.3275072948042168, 0.3275072948042168, 0.3275072948042168, 0.3275072948042168, 0.3275072948042168, 0.3275072948042168, 0.3273852244917168, 0.3272631541792168, 0.3267645778426205, 0.3267645778426205, 0.3272631541792168, 0.3273852244917168, 0.3273852244917168, 0.3268866481551205, 0.32638807181852414, 0.32626600150602414, 0.32614393119352414, 0.32614393119352414, 0.32626600150602414, 0.32614393119352414, 0.32614393119352414, 0.32602186088102414, 0.32626600150602414, 0.32638807181852414, 0.32638807181852414, 0.32651014213102414, 0.32663221244352414, 0.32651014213102414, 0.32651014213102414, 0.32663221244352414, 0.32663221244352414, 0.32638807181852414, 0.32626600150602414, 0.32638807181852414, 0.32638807181852414, 0.32651014213102414, 0.32651014213102414, 0.32651014213102414, 0.32651014213102414, 0.32638807181852414, 0.32651014213102414, 0.32638807181852414, 0.32651014213102414, 0.32638807181852414, 0.32626600150602414, 0.32626600150602414, 0.32626600150602414, 0.32614393119352414, 0.32589979056852414, 0.32614393119352414, 0.32614393119352414, 0.32651014213102414, 0.32651014213102414, 0.32651014213102414, 0.32651014213102414, 0.32651014213102414, 0.32651014213102414, 0.32663221244352414, 0.32663221244352414, 0.32675428275602414, 0.32675428275602414, 0.3271307887801205, 0.3271307887801205, 0.3273749294051205, 0.3274969997176205, 0.3273749294051205, 0.3274969997176205, 0.3274969997176205, 0.3274969997176205, 0.3273749294051205, 0.3274969997176205, 0.3274969997176205, 0.3274969997176205, 0.3276190700301205, 0.3276190700301205, 0.3274969997176205, 0.3274969997176205, 0.3274969997176205, 0.3274969997176205, 0.3274969997176205, 0.3277411403426205, 0.3277411403426205, 0.3276190700301205, 0.3276190700301205, 0.3276190700301205, 0.3277411403426205, 0.3277411403426205, 0.3277411403426205, 0.3277411403426205, 0.3277411403426205, 0.3277411403426205, 0.3277411403426205, 0.3277411403426205, 0.3279852809676205, 0.3278632106551205, 0.3279852809676205, 0.3279852809676205, 0.3278632106551205, 0.3276190700301205], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-3, 3], "momentum": 0.7474103248732826, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.002064852322426285, "patience_threshold": 1, "do_flip": true, "batch_size": 64, "optimization": "nesterov_momentum", "nb_data_augmentation": 0, "learning_rate_decay_method": "none", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 3.203386564182184e-07, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.03162999816382122}, "accuracy_valid_max": 0.6741002094314759, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import os\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-3, 3)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.6723809299698795, "loss_train": [1.7573117017745972, 1.44873046875, 1.3299016952514648, 1.2473430633544922, 1.1786904335021973, 1.1187477111816406, 1.065962553024292, 1.0195884704589844, 0.9780604839324951, 0.9409096240997314, 0.9073890447616577, 0.8768016695976257, 0.8483317494392395, 0.8214373588562012, 0.7957453727722168, 0.7712969779968262, 0.7469739317893982, 0.7233604192733765, 0.699706494808197, 0.6762185096740723, 0.6533244252204895, 0.6301254034042358, 0.606346607208252, 0.5830557942390442, 0.5597935914993286, 0.5357014536857605, 0.5114912986755371, 0.48622825741767883, 0.4611693322658539, 0.43619635701179504, 0.41023269295692444, 0.3849200904369354, 0.3592084050178528, 0.33360713720321655, 0.3090674877166748, 0.2839425206184387, 0.2612825632095337, 0.23984847962856293, 0.22242963314056396, 0.2054757922887802, 0.19200770556926727, 0.18324726819992065, 0.17575186491012573, 0.17162325978279114, 0.16260699927806854, 0.160059854388237, 0.14496470987796783, 0.14172308146953583, 0.129685178399086, 0.12146667391061783, 0.11706577241420746, 0.11289997398853302, 0.10858622193336487, 0.10128214955329895, 0.09232795238494873, 0.08413147181272507, 0.0798223465681076, 0.06966739892959595, 0.066132552921772, 0.07047627866268158, 0.06650146096944809, 0.060764145106077194, 0.0522124320268631, 0.054811857640743256, 0.055451780557632446, 0.06100698560476303, 0.0630069375038147, 0.06407954543828964, 0.04818585515022278, 0.04366588220000267, 0.046077191829681396, 0.031821925193071365, 0.030216388404369354, 0.02091868966817856, 0.016809945926070213, 0.013800326734781265, 0.010465839877724648, 0.008829212747514248, 0.008128945715725422, 0.00775837991386652, 0.007513897493481636, 0.007324902806431055, 0.00717176403850317, 0.007044953294098377, 0.006937972269952297, 0.006842972245067358, 0.006760201416909695, 0.00668652169406414, 0.006620024796575308, 0.006560375913977623, 0.006505999248474836, 0.006456305738538504, 0.0064103249460458755, 0.006368348840624094, 0.006329480558633804, 0.006293646525591612, 0.006259906571358442, 0.0062284814193844795, 0.006198989227414131, 0.006171372719109058, 0.0061453222297132015, 0.006120600737631321, 0.00609733397141099, 0.006075278390198946, 0.00605438370257616, 0.006034567020833492, 0.0060155438259243965, 0.005997456144541502, 0.005980303045362234, 0.0059638903476297855, 0.005948103964328766, 0.005932963918894529, 0.0059183514676988125, 0.005904388148337603, 0.005891019944101572, 0.005878155119717121, 0.005865783430635929, 0.0058539207093417645, 0.0058424146845936775, 0.005831326823681593, 0.005820595659315586, 0.005810261704027653, 0.00580028910189867, 0.005790670868009329, 0.005781357176601887, 0.005772297736257315, 0.005763575900346041, 0.0057551199570298195, 0.0057468656450510025, 0.0057388246059417725, 0.005731092765927315, 0.005723546724766493, 0.00571620836853981, 0.005709083750844002, 0.005702158436179161, 0.005695397034287453, 0.0056888023391366005, 0.005682388320565224, 0.005676190834492445, 0.005670096259564161, 0.005664178170263767, 0.005658410955220461, 0.005652767606079578, 0.005647261627018452, 0.005641903728246689, 0.005636705085635185, 0.005631568841636181, 0.005626576021313667, 0.005621719174087048, 0.005616936832666397, 0.005612279288470745, 0.005607718136161566, 0.0056032524444162846, 0.005598859861493111, 0.005594561342149973, 0.005590375512838364, 0.005586241837590933, 0.005582193844020367, 0.005578235257416964, 0.005574376322329044, 0.0055705830454826355, 0.005566868465393782, 0.0055632260628044605, 0.0055596609599888325, 0.005556123796850443, 0.005552670918405056, 0.005549310706555843, 0.00554595747962594, 0.0055426862090826035, 0.005539456848055124, 0.005536331329494715, 0.0055332183837890625, 0.005530183203518391, 0.005527173168957233, 0.005524236708879471, 0.0055213370360434055, 0.005518475081771612, 0.005515668075531721, 0.005512905772775412, 0.005510194227099419, 0.005507518537342548, 0.005504885222762823, 0.005502304062247276, 0.00549975223839283, 0.005497240461409092, 0.0054947673343122005, 0.005492330528795719, 0.005489945877343416, 0.005487583111971617, 0.005485267844051123, 0.005482964683324099, 0.005480698309838772, 0.005478459410369396, 0.00547626381739974, 0.00547410361468792, 0.005471967626363039, 0.005469860974699259, 0.00546778179705143, 0.005465738475322723, 0.005463713314384222, 0.005461710039526224, 0.005459742154926062, 0.0054578036069869995, 0.005455881357192993, 0.005453994031995535, 0.0054521216079592705, 0.005450285505503416, 0.005448466166853905, 0.005446666851639748, 0.005444889422506094, 0.005443126428872347, 0.005441403482109308, 0.005439685191959143, 0.005438006017357111, 0.005436333827674389, 0.005434687249362469, 0.005433054640889168, 0.005431442521512508, 0.005429857410490513, 0.005428282544016838, 0.005426726769655943, 0.005425192415714264, 0.00542367622256279, 0.005422176793217659, 0.0054206885397434235, 0.0054192207753658295, 0.0054177651181817055], "accuracy_train_first": 0.42458334775286083, "model": "residualv3", "loss_std": [0.3102776110172272, 0.12868483364582062, 0.12675946950912476, 0.12726183235645294, 0.12881958484649658, 0.12946072220802307, 0.12979727983474731, 0.13005368411540985, 0.1298331916332245, 0.1290111392736435, 0.1279330998659134, 0.12663871049880981, 0.12526780366897583, 0.12393073737621307, 0.12265349179506302, 0.121439129114151, 0.11989349871873856, 0.11879642307758331, 0.11740846931934357, 0.1157773956656456, 0.1143103837966919, 0.11313029378652573, 0.1115124449133873, 0.1100429967045784, 0.10858489573001862, 0.10614899545907974, 0.10430791229009628, 0.10169567912817001, 0.09963419288396835, 0.09704168140888214, 0.09391272813081741, 0.09079402685165405, 0.08690590411424637, 0.08405404537916183, 0.08098605275154114, 0.07704193890094757, 0.07294018566608429, 0.06978446245193481, 0.06593650579452515, 0.06321465969085693, 0.06010165810585022, 0.05691524222493172, 0.056173235177993774, 0.05873125419020653, 0.05467633157968521, 0.0557364746928215, 0.05091379955410957, 0.050821200013160706, 0.04816344007849693, 0.04547515884041786, 0.04530109092593193, 0.045178674161434174, 0.04234636574983597, 0.04264287278056145, 0.039113372564315796, 0.037375930696725845, 0.03388885781168938, 0.032048679888248444, 0.02979111671447754, 0.031539157032966614, 0.03548712655901909, 0.031399596482515335, 0.025044817477464676, 0.026512539014220238, 0.029668595641851425, 0.030881371349096298, 0.03416774421930313, 0.03323308750987053, 0.025300906971096992, 0.023982971906661987, 0.027258137241005898, 0.017691409215331078, 0.017591558396816254, 0.010687388479709625, 0.007591061294078827, 0.006504969205707312, 0.0027681945357471704, 0.0012728823348879814, 0.0009110033279284835, 0.0007304559694603086, 0.0006370569462887943, 0.0005697490414604545, 0.0005182554596103728, 0.0004761615127790719, 0.00044151910697109997, 0.00041125030838884413, 0.0003854807873722166, 0.0003628838749136776, 0.00034307187888771296, 0.00032569427276030183, 0.0003099705500062555, 0.00029578106477856636, 0.00028253698837943375, 0.00027055107057094574, 0.000259844062384218, 0.0002501093258615583, 0.00024100541486404836, 0.00023254772531799972, 0.00022474926663562655, 0.0002174178371205926, 0.00021057001140434295, 0.00020419730572029948, 0.00019831910321954638, 0.00019261293346062303, 0.00018733028264250606, 0.00018232993897981942, 0.00017754602595232427, 0.00017302235937677324, 0.00016883770877029747, 0.00016483156650792807, 0.00016097190382424742, 0.00015728871221654117, 0.00015376646479126066, 0.00015044886094983667, 0.00014725977962370962, 0.0001441949134459719, 0.00014125870075076818, 0.0001384640927426517, 0.00013576223864220083, 0.0001331800303887576, 0.0001306704943999648, 0.00012827735918108374, 0.00012593690189532936, 0.00012368532770778984, 0.00012152908311691135, 0.00011943771824007854, 0.00011746875679818913, 0.00011553920194273815, 0.00011365228419890627, 0.00011184379400219768, 0.00011012875620508566, 0.00010844203643500805, 0.00010677471436792985, 0.00010518438648432493, 0.00010361344902776182, 0.00010208578896708786, 0.00010060525528388098, 9.917663555825129e-05, 9.78009047685191e-05, 9.645257523516193e-05, 9.513577970210463e-05, 9.387177851749584e-05, 9.263007814297453e-05, 9.142840281128883e-05, 9.026809857459739e-05, 8.912938938010484e-05, 8.800733485259116e-05, 8.692424307810143e-05, 8.585860632592812e-05, 8.48178897285834e-05, 8.381575753446668e-05, 8.28186166472733e-05, 8.186027844203636e-05, 8.090879418887198e-05, 7.999462104635313e-05, 7.911250577308238e-05, 7.823747000657022e-05, 7.738920976407826e-05, 7.654941873624921e-05, 7.572905451525003e-05, 7.492741860914975e-05, 7.414245919790119e-05, 7.338133582379669e-05, 7.263390580192208e-05, 7.18975716154091e-05, 7.117062341421843e-05, 7.048033876344562e-05, 6.978680175961927e-05, 6.911881791893393e-05, 6.845462485216558e-05, 6.78048309055157e-05, 6.71613379381597e-05, 6.654038588749245e-05, 6.592555291717872e-05, 6.53327297186479e-05, 6.473952089436352e-05, 6.414768722606823e-05, 6.357411621138453e-05, 6.301924440776929e-05, 6.248203862924129e-05, 6.1946535424795e-05, 6.141951598692685e-05, 6.0911159380339086e-05, 6.0401493101380765e-05, 5.990153658785857e-05, 5.9405527281342074e-05, 5.891838372917846e-05, 5.8443722082301974e-05, 5.797882840852253e-05, 5.752148717874661e-05, 5.706505908165127e-05, 5.6614764616824687e-05, 5.616667840513401e-05, 5.573392991209403e-05, 5.5306474678218365e-05, 5.4883694247109815e-05, 5.446398790809326e-05, 5.4054224165156484e-05, 5.36586667294614e-05, 5.326109385350719e-05, 5.2874875109409913e-05, 5.2494197007035837e-05, 5.211745883570984e-05, 5.174481339054182e-05, 5.138061169418506e-05, 5.1020364480791613e-05, 5.066286030341871e-05, 5.030915781389922e-05, 4.996226925868541e-05, 4.9614674935583025e-05, 4.92706494696904e-05, 4.8937687097350135e-05, 4.860795888816938e-05, 4.828337114304304e-05, 4.795823770109564e-05, 4.7636589442845434e-05, 4.7319583245553076e-05, 4.700932913692668e-05, 4.670702401199378e-05, 4.6404526074184105e-05, 4.6107310481602326e-05, 4.581471876008436e-05, 4.552201062324457e-05, 4.5234308345243335e-05, 4.4949323637411e-05, 4.466894097276963e-05, 4.439059193828143e-05]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:09 2016", "state": "available"}], "summary": "69dd9b9043f3a4e1af977fdae1d99fa0"}