{"content": {"hp_model": {"f0": 16, "f1": 32, "f2": 16, "f3": 16, "nonlin": "leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.8691736459732056, 1.5563139915466309, 1.395424723625183, 1.2894433736801147, 1.2038172483444214, 1.13616943359375, 1.0807198286056519, 1.0300242900848389, 0.9849849939346313, 0.9452674388885498, 0.9071775078773499, 0.8750883340835571, 0.8396507501602173, 0.812800407409668, 0.7891572713851929, 0.7670083045959473, 0.743742823600769, 0.7247143983840942, 0.7059009075164795, 0.6874247789382935, 0.673172116279602, 0.6576258540153503, 0.6437302827835083, 0.628026008605957, 0.6160203814506531, 0.6041778922080994, 0.5915815830230713, 0.5792938470840454, 0.5703238844871521, 0.5591318607330322, 0.5480169057846069, 0.537699282169342, 0.531150221824646, 0.5184335112571716, 0.5092481970787048, 0.501885712146759, 0.49244987964630127, 0.48375457525253296, 0.47624897956848145, 0.4701978862285614, 0.46202993392944336, 0.4565054774284363, 0.44588494300842285, 0.4404403269290924, 0.43411239981651306, 0.4286889433860779, 0.42195016145706177, 0.41494429111480713, 0.4117199182510376, 0.404835969209671, 0.3995553255081177, 0.39439716935157776, 0.38862696290016174, 0.3834103047847748, 0.3796914219856262, 0.3744233250617981, 0.3692673444747925, 0.3641202747821808, 0.3632722795009613, 0.3573947846889496, 0.3546411991119385, 0.3480599820613861, 0.3455120325088501, 0.34264084696769714, 0.33924901485443115, 0.33518704771995544, 0.3343493342399597, 0.33086666464805603, 0.3272256851196289, 0.32603245973587036, 0.32302412390708923, 0.31804704666137695, 0.31686991453170776, 0.31580987572669983, 0.3134763836860657, 0.31142300367355347, 0.3085893988609314, 0.30727091431617737, 0.30346086621284485, 0.3024123013019562, 0.30040428042411804, 0.30034247040748596, 0.2966763973236084, 0.296018123626709, 0.2947905361652374, 0.2915929853916168, 0.29090675711631775, 0.2897241711616516, 0.28917038440704346, 0.2868872284889221, 0.28693971037864685, 0.2848091423511505, 0.28475332260131836, 0.2828630805015564, 0.2827783226966858, 0.2821236848831177, 0.28060615062713623, 0.2796400785446167, 0.27742546796798706, 0.27917999029159546, 0.2780213952064514, 0.27783384919166565, 0.274903267621994, 0.27469128370285034, 0.27633368968963623, 0.2729012370109558, 0.27363255620002747, 0.27145951986312866, 0.2717055678367615, 0.2707841694355011, 0.27057573199272156, 0.26995396614074707, 0.2708556056022644, 0.2690223455429077, 0.2695397734642029, 0.2700921297073364, 0.2693680226802826, 0.2685900330543518, 0.26729172468185425, 0.2678320109844208, 0.26638156175613403, 0.26621127128601074, 0.26674291491508484, 0.26432570815086365, 0.2650914490222931, 0.2656242549419403, 0.26553019881248474, 0.2648096978664398, 0.26594072580337524, 0.264588326215744, 0.26513829827308655, 0.2634921669960022, 0.2643820643424988, 0.2643445134162903, 0.2648804783821106, 0.264396607875824, 0.2651498317718506, 0.2643667757511139, 0.2636183500289917, 0.2629109025001526, 0.2630997896194458, 0.26246023178100586, 0.2621169090270996, 0.2628454864025116, 0.26322290301322937, 0.26187387108802795, 0.26383236050605774, 0.26310721039772034, 0.26063790917396545, 0.26005876064300537, 0.261520117521286, 0.2619391679763794, 0.26285699009895325, 0.2616410255432129, 0.26143863797187805, 0.26039373874664307, 0.26252296566963196, 0.2615119218826294, 0.26123905181884766, 0.2607501149177551, 0.2605852782726288, 0.26106807589530945, 0.2614310681819916, 0.2605561912059784, 0.2615853250026703, 0.26165199279785156, 0.2600884437561035, 0.25929978489875793, 0.2599846124649048, 0.2599683403968811, 0.26126351952552795, 0.26120346784591675, 0.2618044316768646, 0.26073551177978516, 0.26102760434150696, 0.25910693407058716, 0.26169130206108093, 0.2609703242778778, 0.26131704449653625, 0.2615699768066406, 0.25982600450515747, 0.26079654693603516, 0.26097023487091064, 0.2592324912548065], "moving_avg_accuracy_train": [0.015833596489710222, 0.03701841842296511, 0.07055853913603959, 0.10977078533433116, 0.13502372494066361, 0.1587327996768944, 0.1822147180930643, 0.21296506187530456, 0.2353893741689941, 0.25863515248552954, 0.2750899584002287, 0.2987106964910844, 0.3237306584726515, 0.3461039242975901, 0.37567794897285145, 0.40110416708774793, 0.4267568714280226, 0.4457251149618999, 0.47047934851692813, 0.48800899721197766, 0.510029444591878, 0.5279605113280114, 0.55072642523074, 0.5716364194336258, 0.5901550013851506, 0.6089656204903602, 0.622093739625571, 0.635883026088909, 0.6537144413187039, 0.6730336586682382, 0.6903068056982397, 0.6993252063161233, 0.7156710633146328, 0.728859542387147, 0.7422448821856693, 0.7534806436556406, 0.7657706282414884, 0.7763970999319206, 0.786102686433053, 0.7956677563602534, 0.8031675117031409, 0.8116955977652004, 0.8194171258555131, 0.8265898596177839, 0.8341961244692668, 0.8414300266379733, 0.8485728709195433, 0.8548178161146414, 0.8604799392247073, 0.8661083811987851, 0.8712134344075795, 0.87591501123837, 0.8802580375289387, 0.8839624004369437, 0.8880307135897701, 0.8915761182309315, 0.895496970987711, 0.898384141592659, 0.9021542898930185, 0.9058287663692943, 0.9093356498491054, 0.9127917531285452, 0.916014033466992, 0.9192370651632052, 0.9219029897088541, 0.9248021206475294, 0.9273439452256799, 0.9296247560948616, 0.9319168320092311, 0.934205311864325, 0.9362928815684426, 0.9384808309473588, 0.9404384317419732, 0.9422327884916407, 0.9439034770889514, 0.9457162695205602, 0.9475849839363982, 0.949299342945167, 0.9509259714102019, 0.9523271940596948, 0.9535812108513536, 0.9549028493638558, 0.9561575002894119, 0.9574494104902603, 0.9586377423567475, 0.9599025174877671, 0.9610872820330565, 0.9620442520809506, 0.962666070845493, 0.963655896312162, 0.9644630338750211, 0.9653057511708892, 0.9661618169383518, 0.966943865824297, 0.9676406983264003, 0.9684957121616267, 0.9692442982740447, 0.9698180804252301, 0.9703484713029729, 0.9708932163595988, 0.9713997629522289, 0.9720021753094146, 0.9725117222499109, 0.9729773620404237, 0.9734219784399712, 0.9738570104317068, 0.9742532255707066, 0.9746632615696068, 0.9750253185221883, 0.9753232679937975, 0.9757100411563502, 0.9759697813478857, 0.9762384247524105, 0.9765173701486165, 0.9766777762504492, 0.9768895710575748, 0.9771127384673213, 0.9773228897313312, 0.9774980749760829, 0.977672017738026, 0.9778448062166228, 0.9780422045747502, 0.9782477288339603, 0.9784466515601066, 0.9785211584636658, 0.9786787873339844, 0.9787950766803664, 0.9789299640266339, 0.9789909087692271, 0.9790690465744749, 0.9791230945575313, 0.9791996395279964, 0.9792290024716529, 0.9793205332876105, 0.9794005858731629, 0.9795260755739603, 0.9795344206570682, 0.9795954096544843, 0.9796433243057302, 0.9796724965989944, 0.97971499165578, 0.9797928007854677, 0.9798535284069486, 0.9799546862424718, 0.9799922498718238, 0.9799981553525262, 0.9800429978149203, 0.9800554542453607, 0.9801317331506049, 0.9801957699165245, 0.9802487527082331, 0.9803057017671901, 0.9803779183083559, 0.9804010244680149, 0.9804241812093363, 0.9804612983181922, 0.9804644767816388, 0.9805138403749313, 0.9805443167160373, 0.9805415184885089, 0.9805552761253999, 0.9805467316593162, 0.9805646182767456, 0.9805342132562416, 0.9805394008211213, 0.9805789468616559, 0.9806098880005181, 0.9806005326445416, 0.98060606371702, 0.9806063913846313, 0.9806415635176245, 0.9806755435861279, 0.9807084507965903, 0.9807287666907685, 0.9807144989121955, 0.9807295596971941, 0.9806919611298833, 0.9807092756931131, 0.9806783558238295, 0.9806714542807599, 0.980690819528902, 0.9807268494427062, 0.980719748835368, 0.9806878177006777], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.015364240163780117, 0.035974312288215356, 0.06999027408461972, 0.11002890904438063, 0.13570976484551484, 0.1594165615631471, 0.18266712349794684, 0.2133495769519925, 0.23492512848119085, 0.25833688309353864, 0.2750983802134016, 0.29838508774929035, 0.3232627581441655, 0.34514511373035134, 0.3734576919657499, 0.39842837608242787, 0.4233139835043056, 0.44191964393587807, 0.46571966702346496, 0.48221790104589257, 0.5035882490248876, 0.5198326044217212, 0.5408370957679527, 0.5599893966391996, 0.5773261826906411, 0.5948397640638963, 0.6068319265882296, 0.6195587313409278, 0.635629319520841, 0.6526279404207901, 0.6682126084927924, 0.6765063584680614, 0.6904691391781077, 0.7021302624873301, 0.7135866874810669, 0.7237123054893608, 0.7342618204223526, 0.7427206981504486, 0.7512350964735814, 0.7595317910807413, 0.7656376587064775, 0.7726680840538267, 0.7785641027380524, 0.7843782143957231, 0.7900646340611959, 0.7954325823644136, 0.801076606460427, 0.8060362168516584, 0.8101529217854685, 0.8143838880783072, 0.8178387833442716, 0.8215138305557782, 0.824337062758107, 0.8269726865368897, 0.8297159593459266, 0.8319804738687887, 0.834655361581684, 0.8362672444747203, 0.8389915883630917, 0.8409215369721892, 0.8428976308747443, 0.8450718879303423, 0.8471212279870821, 0.8486329556684191, 0.8500189541527821, 0.8514413692608472, 0.8525852064970366, 0.8535536248533571, 0.854743613695205, 0.8557047403716183, 0.8566918246928902, 0.8578640508267638, 0.8588406646164519, 0.8597247645704694, 0.8603708816281062, 0.8610500432299792, 0.8618464531577343, 0.8624910094138736, 0.8632287719419892, 0.8637320077937239, 0.8641493284751949, 0.8645238875798591, 0.8652160241889666, 0.8657524684097537, 0.866125404927212, 0.8665882656487227, 0.8667809957009438, 0.8672840425916928, 0.8675679453731862, 0.8677980143053706, 0.8680427269467461, 0.8682263472302342, 0.868429256087783, 0.8685843714710981, 0.8687494188872413, 0.8690200318742702, 0.8692890271337559, 0.8694812652336332, 0.869581037336023, 0.8698061390805834, 0.8699354884631877, 0.8701139675724412, 0.8703335749097001, 0.8703237019819832, 0.8704002655657879, 0.8705556515186218, 0.8708928703934915, 0.8709389902159647, 0.8711391894624404, 0.8712339195655187, 0.8714544835106988, 0.8716153404589512, 0.8715759767349687, 0.871590407017044, 0.8714447028646619, 0.8714366689486777, 0.8715148876430419, 0.87160866902181, 0.8717530779102917, 0.8718851049272445, 0.8719530421001827, 0.8720152150644868, 0.87204778617852, 0.8721869634623999, 0.8721647091342322, 0.8722067449037909, 0.8721580983689841, 0.8721854996578386, 0.8721766282500367, 0.8720567216844457, 0.872121763230233, 0.8721670640815321, 0.8721966573251109, 0.8723830121592414, 0.8723035023589799, 0.8722818011724042, 0.872335512291986, 0.8723472312058597, 0.872308950103346, 0.8722500830485836, 0.8721716591281379, 0.872225206929556, 0.8723100210445823, 0.8722998750206964, 0.8722795660766086, 0.8723345302144297, 0.8723829684298091, 0.8724265628236505, 0.8724413837156078, 0.8724181014246193, 0.8723727333002297, 0.872356316050779, 0.8724025756825234, 0.8723709671635933, 0.8723791405903063, 0.8724973894642576, 0.8724807136296541, 0.8724911489496705, 0.8724018549790258, 0.8723703185304454, 0.8723673792978828, 0.8724257691448264, 0.872513911592166, 0.8724955835447716, 0.8724668812708668, 0.8724156056531928, 0.8724559363246958, 0.8724545833266388, 0.8723811529495472, 0.8723761007664147, 0.8723959678640956, 0.8723283990332582, 0.8723286222417547, 0.8722799950044015, 0.8723583008032836, 0.8723555338347775, 0.8723530435631219, 0.872448458568632, 0.872448882854841, 0.8725102998686792, 0.8724801259623836, 0.8725506256967175, 0.872565247332618, 0.8725417857111785], "moving_var_accuracy_train": [0.002256325000190675, 0.006069862623265066, 0.015587333637967028, 0.02786700254140921, 0.03081970091611797, 0.03279681284813977, 0.034479735995858726, 0.039542015180806515, 0.04011346169933011, 0.04096541141527068, 0.039305716012957376, 0.040396597823272845, 0.041990924518977124, 0.042296899280139716, 0.04593881577156242, 0.04716336730304278, 0.04836958173246443, 0.0467707719240623, 0.047608643441728014, 0.045613376347901885, 0.04541613963841037, 0.04376823406323034, 0.04405599217934536, 0.04358544367949327, 0.042313340209002015, 0.04126656070819329, 0.03869103224562859, 0.03653322881157771, 0.03574154025229621, 0.035526475658053475, 0.0346590825671286, 0.03192515825775755, 0.03113732580112329, 0.029589017043226265, 0.02824262123260078, 0.026554540131631523, 0.025258479608551786, 0.023748928752984618, 0.02222182156166483, 0.02082305446990845, 0.01924696599474614, 0.017976823662208565, 0.016715739260433106, 0.015507198321009568, 0.014477175873826758, 0.013500422351721818, 0.012609562136426446, 0.01169959998719181, 0.010818176731494515, 0.010021473289845111, 0.0092538800752422, 0.008527435489980422, 0.007844448837227517, 0.007183504694492594, 0.0066141147722284825, 0.006065832341631754, 0.005597606884531687, 0.005112867982997199, 0.004729507348557807, 0.004378072610074368, 0.004050949434735678, 0.0037533563401654997, 0.003471468521364811, 0.0032178130690614917, 0.0029599961453031795, 0.0027396411725691214, 0.0025238249049870124, 0.0023182612984771088, 0.002133717676604493, 0.0019674801693685784, 0.0018099536778576645, 0.0016720424124341952, 0.0015393279790304516, 0.001414372626433105, 0.0012980561672924487, 0.0011978264981640852, 0.0011094726904593247, 0.0010249766627119135, 0.0009462922779100752, 0.0008693338743401356, 0.000796553509929982, 0.0007326187141565436, 0.0006735241832458783, 0.0006211930526247959, 0.0005717829409864943, 0.0005290015520762558, 0.0004887344001186026, 0.00044810308515983956, 0.00040677270382728943, 0.00037491322353475645, 0.0003432851405896841, 0.00031534817849751355, 0.00029040899803175384, 0.0002668725023686514, 0.0002445554319556747, 0.00022667932668596328, 0.0002090548245267127, 0.00019111237568721214, 0.00017453296846722563, 0.00015975039621096866, 0.00014608466164441742, 0.00013474230131278658, 0.00012360481394263056, 0.00011319571627894668, 0.00010365529833577137, 9.499304400669544e-05, 8.690661753338007e-05, 7.97291214635887e-05, 7.293597644944388e-05, 6.644134379318901e-05, 6.114355072730929e-05, 5.5636380358468585e-05, 5.072226583177399e-05, 4.635033405517717e-05, 4.1946871707205925e-05, 3.815589789941379e-05, 3.4788541344428675e-05, 3.170715919387024e-05, 2.8812652104291658e-05, 2.6203691653754404e-05, 2.3852025213401162e-05, 2.181751769818325e-05, 2.0015927918479675e-05, 1.837046738542878e-05, 1.6583382154987858e-05, 1.5148665686310155e-05, 1.3755508026416555e-05, 1.25437085894228e-05, 1.1322766085328235e-05, 1.0245439126275953e-05, 9.247185873900535e-06, 8.375199479041912e-06, 7.54543917327936e-06, 6.866296268380274e-06, 6.237342389624801e-06, 5.755337135718333e-06, 5.1804301858551925e-06, 4.695864087522074e-06, 4.2469400030060295e-06, 3.8299052069540556e-06, 3.4631671549196184e-06, 3.1713387853925883e-06, 2.887395502949895e-06, 2.690752121844749e-06, 2.4343761459110793e-06, 2.1912524036409146e-06, 1.99022478117893e-06, 1.7925987669948914e-06, 1.6657051327628273e-06, 1.5360409859915e-06, 1.4077014733474587e-06, 1.296120083857563e-06, 1.2134451348333967e-06, 1.0969056728777663e-06, 9.92041217607644e-07, 9.052362137753129e-07, 8.148035160667114e-07, 7.552540435446504e-07, 6.880879054950632e-07, 6.193495856412645e-07, 5.591180802325818e-07, 5.038633433152286e-07, 4.5635638873129973e-07, 4.190409373048133e-07, 3.7737904103876417e-07, 3.537161408325662e-07, 3.269607134160589e-07, 2.9505234624347737e-07, 2.6582244648397045e-07, 2.392411681301455e-07, 2.2645076177070104e-07, 2.1419749109304784e-07, 2.0252370248756553e-07, 1.859859522451403e-07, 1.6921948256929045e-07, 1.543389795153313e-07, 1.5162795193820053e-07, 1.3916330364294057e-07, 1.3385131812729152e-07, 1.2089486798522809e-07, 1.1218049670717865e-07, 1.1264583923506991e-07, 1.0183502293270014e-07, 1.008278969029592e-07], "duration": 91780.257734, "accuracy_train": [0.15833596489710225, 0.22768181582225916, 0.3724196255537099, 0.4626810011189553, 0.36230018139765596, 0.37211447230297157, 0.3935519838385936, 0.48971815591546697, 0.43720818481220003, 0.4678471573343485, 0.42318321163252126, 0.5112973393087855, 0.5489103163067552, 0.5474633167220377, 0.6418441710502031, 0.6299401301218162, 0.6576312104904947, 0.6164393067667958, 0.6932674505121816, 0.6457758354674235, 0.7082134710109819, 0.6893401119532115, 0.7556196503552971, 0.7598263672595976, 0.7568222389488741, 0.7782611924372462, 0.7402468118424695, 0.7599866042589516, 0.8141971783868586, 0.8469066148140458, 0.845765128968254, 0.7804908118770765, 0.8627837763012183, 0.8475558540397747, 0.8627129403723699, 0.854602496885382, 0.8763804895141197, 0.8720353451458103, 0.8734529649432448, 0.8817533857050572, 0.8706653097891289, 0.8884483723237356, 0.8889108786683279, 0.8911444634782208, 0.9026525081326136, 0.9065351461563308, 0.9128584694536729, 0.9110223228705242, 0.9114390472153008, 0.9167643589654854, 0.9171589132867294, 0.9182292027154854, 0.9193452741440569, 0.9173016666089886, 0.9246455319652085, 0.9234847600013842, 0.9307846457987264, 0.9243686770371908, 0.9360856245962532, 0.9388990546557769, 0.9408976011674051, 0.9438966826435032, 0.9450145565130121, 0.9482443504291252, 0.9458963106196937, 0.9508942990956073, 0.950220366429033, 0.9501520539174971, 0.9525455152385567, 0.9548016305601699, 0.9550810089055003, 0.9581723753576044, 0.9580568388935032, 0.958381999238649, 0.9589396744647471, 0.9620314014050388, 0.9644034136789406, 0.9647285740240864, 0.965565627595515, 0.9649381979051311, 0.9648673619762828, 0.966797595976375, 0.9674493586194168, 0.9690766022978959, 0.9693327291551311, 0.9712854936669435, 0.9717501629406607, 0.9706569825119971, 0.968262439726375, 0.9725643255121816, 0.971727271940753, 0.9728902068337025, 0.973866408845515, 0.9739823057978036, 0.9739121908453304, 0.9761908366786637, 0.9759815732858066, 0.9749821197858989, 0.9751219892026578, 0.9757959218692323, 0.9759586822858989, 0.9774238865240864, 0.977097644714378, 0.9771681201550388, 0.9774235260358989, 0.9777722983573275, 0.9778191618217055, 0.9783535855597084, 0.9782838310954227, 0.9780048132382798, 0.9791909996193245, 0.9783074430717055, 0.9786562153931341, 0.9790278787144703, 0.9781214311669435, 0.9787957243217055, 0.9791212451550388, 0.9792142511074198, 0.9790747421788483, 0.979237502595515, 0.9793999025239941, 0.9798187897978959, 0.9800974471668512, 0.9802369560954227, 0.9791917205956996, 0.9800974471668512, 0.9798416807978036, 0.9801439501430418, 0.9795394114525655, 0.9797722868217055, 0.9796095264050388, 0.9798885442621816, 0.9794932689645626, 0.9801443106312293, 0.9801210591431341, 0.980655482881137, 0.9796095264050388, 0.9801443106312293, 0.9800745561669435, 0.9799350472383721, 0.9800974471668512, 0.9804930829526578, 0.9804000770002769, 0.9808651067621816, 0.9803303225359912, 0.9800513046788483, 0.9804465799764673, 0.9801675621193245, 0.9808182432978036, 0.9807721008098007, 0.9807255978336102, 0.9808182432978036, 0.9810278671788483, 0.9806089799049464, 0.9806325918812293, 0.9807953522978959, 0.9804930829526578, 0.9809581127145626, 0.9808186037859912, 0.980516334440753, 0.9806790948574198, 0.9804698314645626, 0.9807255978336102, 0.9802605680717055, 0.9805860889050388, 0.9809348612264673, 0.9808883582502769, 0.980516334440753, 0.9806558433693245, 0.9806093403931341, 0.9809581127145626, 0.9809813642026578, 0.981004615690753, 0.9809116097383721, 0.9805860889050388, 0.9808651067621816, 0.9803535740240864, 0.9808651067621816, 0.9804000770002769, 0.9806093403931341, 0.9808651067621816, 0.9810511186669435, 0.9806558433693245, 0.9804004374884644], "end": "2016-01-30 13:35:58.477000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0], "moving_var_accuracy_valid": [0.0021245388822928274, 0.0057350606508333474, 0.015575325498159986, 0.028445623553312885, 0.03153661839008944, 0.03344106644658075, 0.03496225747447831, 0.03993874827666759, 0.040134413263102665, 0.04105396422305149, 0.03947709787203893, 0.0404098248155932, 0.04193892869251833, 0.04205457319726901, 0.045063534654562205, 0.04616899677640023, 0.04712573820956254, 0.04552869978946112, 0.04607379970124203, 0.04391614526384691, 0.04363475669215228, 0.04164619276326476, 0.04145227139736351, 0.04060833991559204, 0.03925258327937394, 0.03808785474409522, 0.03557337692777591, 0.03347378326791801, 0.032450779181161515, 0.03180627927554702, 0.030811588258222715, 0.028349506030270904, 0.027269188633655378, 0.02576610594178584, 0.024370742410341328, 0.02285642142975617, 0.021572409674673276, 0.020059142218975867, 0.018705682806322992, 0.017454630798330983, 0.016044702293664996, 0.014885073989380352, 0.01370943391736497, 0.01264272557493923, 0.011669471334952199, 0.010761858022331145, 0.009972367292065456, 0.009196510179954204, 0.00842938449756729, 0.007747555729750795, 0.007080226868464769, 0.006493757929679512, 0.005916117897325958, 0.005387024721922923, 0.004916052161073843, 0.004470599179184741, 0.004087934479755647, 0.0037025245299278538, 0.0033990705235340244, 0.003092685785884437, 0.002818561731301435, 0.0025792521018656446, 0.002359125043692507, 0.0021437804245659406, 0.001946691308297253, 0.001770231560124396, 0.0016049836767179965, 0.0014529258160619221, 0.0013203778954492351, 0.0011966539862973298, 0.0010857576067833043, 0.0009895488730854009, 0.000899177956224741, 0.00081629485516051, 0.000738422574913982, 0.0006687316617557135, 0.000607566914537384, 0.0005505492979895995, 0.000500393010121664, 0.000452632926011739, 0.0004089370423712157, 0.00036930598884007607, 0.00033668686772706945, 0.0003056081325725051, 0.0002762990541297403, 0.00025059730914442314, 0.0002258718820872432, 0.00020556219944714894, 0.0001857313866064911, 0.00016763463336785025, 0.0001514101285227064, 0.00013657256334700984, 0.00012328585505255493, 0.00011117381658656828, 0.00010030160077409139, 9.09305231954205e-05, 8.24886969225101e-05, 7.457242661365947e-05, 6.720477420403086e-05, 6.0940333942264794e-05, 5.499688191305937e-05, 4.9783886853712796e-05, 4.5239544611543e-05, 4.0716467422704046e-05, 3.6697578521718816e-05, 3.324512381858972e-05, 3.0944060562846534e-05, 2.786879784878646e-05, 2.54426357085132e-05, 2.297913626952481e-05, 2.1119058727792868e-05, 1.9240027475223284e-05, 1.7329970252592865e-05, 1.5598847324700565e-05, 1.4230029892423036e-05, 1.2807607797435113e-05, 1.1581910495023907e-05, 1.0502873968554323e-05, 9.640271915351442e-06, 8.833124922665298e-06, 7.991351565600288e-06, 7.227005706453394e-06, 6.513853033032361e-06, 6.036800576862884e-06, 5.437577815276359e-06, 4.90972308705026e-06, 4.44004914648362e-06, 4.002801707513304e-06, 3.6032298536494683e-06, 3.372305128531027e-06, 3.0731482397836067e-06, 2.784302919960953e-06, 2.5137544685544406e-06, 2.574932139533197e-06, 2.3743352006184747e-06, 2.1411401540457693e-06, 1.952990097941832e-06, 1.7589270846290585e-06, 1.5962233614531196e-06, 1.4677889965353092e-06, 1.3763628985644549e-06, 1.2645329120384626e-06, 1.2028205278039134e-06, 1.0834649512297703e-06, 9.788305349964115e-07, 9.081369895144864e-07, 8.384396369452265e-07, 7.716999138202741e-07, 6.965068519839185e-07, 6.317347524485822e-07, 5.870856775994341e-07, 5.308028445552248e-07, 4.969821418618337e-07, 4.562758138962153e-07, 4.112494766446817e-07, 4.959696946969662e-07, 4.488754763647708e-07, 4.0496799186291966e-07, 4.362319114182066e-07, 4.015596485779022e-07, 3.614814355126261e-07, 3.5601765999629303e-07, 3.903377132038698e-07, 3.543271977750758e-07, 3.263088627433659e-07, 3.173406771797077e-07, 3.0024567703668944e-07, 2.702375847666989e-07, 2.9174200880835713e-07, 2.627975289171542e-07, 2.4007009015776895e-07, 2.5715300324842945e-07, 2.3143815132188255e-07, 2.2957581010311752e-07, 2.6180441233993655e-07, 2.3569287613836529e-07, 2.1217940160079063e-07, 2.728976709291343e-07, 2.4560952400530593e-07, 2.5499701790396947e-07, 2.376914977038133e-07, 2.586542608038226e-07, 2.3471296485111156e-07, 2.1619569749113972e-07], "accuracy_test": 0.8669025031887756, "start": "2016-01-29 12:06:18.219000", "learning_rate_per_epoch": [0.008816681802272797, 0.008400621823966503, 0.008004195988178253, 0.007626477163285017, 0.0072665829211473465, 0.006923672277480364, 0.006596943363547325, 0.006285632960498333, 0.005989013239741325, 0.005706391297280788, 0.005437106359750032, 0.005180528853088617, 0.004936059005558491, 0.004703125916421413, 0.004481184761971235, 0.004269717261195183, 0.004068228881806135, 0.0038762488402426243, 0.0036933282390236855, 0.003519039833918214, 0.0033529759384691715, 0.003194748656824231, 0.0030439882539212704, 0.0029003422241657972, 0.002763474825769663, 0.002633066149428487, 0.0025088114198297262, 0.0023904202971607447, 0.0022776161786168814, 0.0021701352670788765, 0.0020677263382822275, 0.0019701500423252583, 0.0018771784380078316, 0.0017885941779240966, 0.001704190275631845, 0.0016237694071605802, 0.0015471435617655516, 0.0014741336926817894, 0.0014045691350474954, 0.001338287373073399, 0.0012751334579661489, 0.0012149597750976682, 0.00115762569475919, 0.0011029972229152918, 0.0010509466519579291, 0.001001352327875793, 0.0009540984174236655, 0.000909074442461133, 0.0008661751635372639, 0.0008253002888523042, 0.000786354299634695, 0.0007492461591027677, 0.0007138891378417611, 0.0006802006391808391, 0.0006481019081547856, 0.0006175179150886834, 0.0005883771809749305, 0.0005606116028502584, 0.0005341562791727483, 0.0005089493934065104, 0.0004849320393987, 0.00046204804675653577, 0.00044024395174346864, 0.0004194687935523689, 0.00039967402699403465, 0.00038081337697803974, 0.0003628427512012422, 0.00034572018194012344, 0.00032940562232397497, 0.00031386094633489847, 0.00029904983239248395, 0.0002849376469384879, 0.0002714914153330028, 0.00025867970543913543, 0.00024647259851917624, 0.00023484152916353196, 0.00022375932894647121, 0.00021320011001080275, 0.00020313917775638402, 0.00019355301628820598, 0.0001844192302087322, 0.00017571647185832262, 0.00016742439765948802, 0.0001595236244611442, 0.00015199568588286638, 0.00014482298865914345, 0.00013798878353554755, 0.00013147707795724273, 0.00012527266517281532, 0.00011936103692278266, 0.0001137283761636354, 0.0001083615206880495, 0.0001032479340210557, 9.83756544883363e-05, 9.373329521622509e-05, 8.931001502787694e-05, 8.509546751156449e-05, 8.107980102067813e-05, 7.725363684585318e-05, 7.36080328351818e-05, 7.013446156634018e-05, 6.682481034658849e-05, 6.367134483298287e-05, 6.066668720450252e-05, 5.780381980002858e-05, 5.507605237653479e-05, 5.247700755717233e-05, 5.000061355531216e-05, 4.7641078708693385e-05, 4.539289147942327e-05, 4.325079498812556e-05, 4.120978701394051e-05, 3.926509452867322e-05, 3.741217005881481e-05, 3.564668804756366e-05, 3.396451938897371e-05, 3.236173142795451e-05, 3.0834577046334743e-05, 2.9379491024883464e-05, 2.7993070034426637e-05, 2.6672074454836547e-05, 2.5413417461095378e-05, 2.421415592834819e-05, 2.3071486793924123e-05, 2.1982741600368172e-05, 2.0945373762515374e-05, 1.9956960386480205e-05, 1.901518953673076e-05, 1.8117860236088745e-05, 1.726287700876128e-05, 1.6448240785393864e-05, 1.5672047084080987e-05, 1.493248146289261e-05, 1.4227815881895367e-05, 1.3556404155679047e-05, 1.291667649638839e-05, 1.2307137694733683e-05, 1.1726362572517246e-05, 1.1172994163644034e-05, 1.0645739166648127e-05, 1.0143366125703324e-05, 9.664699973654933e-06, 9.208622032019775e-06, 8.774066373007372e-06, 8.360017091035843e-06, 7.965507393237203e-06, 7.589614597236505e-06, 7.2314601311518345e-06, 6.8902068051102106e-06, 6.565057447005529e-06, 6.2552517192671075e-06, 5.960065664112335e-06, 5.678809429809917e-06, 5.410825906437822e-06, 5.155488452146528e-06, 4.9122004384116735e-06, 4.680393431044649e-06, 4.459525371203199e-06, 4.2490801206440665e-06, 4.0485656427335925e-06, 3.857513547700364e-06, 3.6754770462721353e-06, 3.5020309496758273e-06, 3.3367698506481247e-06, 3.1793074413144495e-06, 3.029275603694259e-06, 2.88632372758002e-06, 2.7501178010425065e-06, 2.6203395009360975e-06, 2.496685510777752e-06, 2.378866611252306e-06, 2.2666076802124735e-06, 2.159646328436793e-06, 2.0577324448822765e-06, 1.9606279693107354e-06, 1.8681057554204017e-06, 1.7799496845327667e-06, 1.695953642411041e-06, 1.6159214055733173e-06, 1.5396659591715434e-06, 1.467008928557334e-06, 1.3977806929688086e-06, 1.3318192486622138e-06, 1.2689705499724369e-06], "accuracy_train_first": 0.15833596489710225, "accuracy_train_last": 0.9804004374884644, "batch_size_eval": 1024, "accuracy_train_std": [0.009910550582545547, 0.010499011870449439, 0.01727428031026028, 0.016972067533397955, 0.017809955910123427, 0.01723155495927688, 0.018349764816204456, 0.017097344409148348, 0.01782495508349935, 0.014891752240751982, 0.019317883548143514, 0.01591670068839248, 0.015308344194931368, 0.01687870412767974, 0.019043474451561904, 0.017219676032762664, 0.017219637605865475, 0.015906084749952712, 0.016790917255597452, 0.014815845284245533, 0.016660016723273642, 0.01759151279814507, 0.015574404209086453, 0.018044700192853903, 0.018571223262511507, 0.017034711507466643, 0.01900453116359307, 0.01689072334292127, 0.017365730134810752, 0.018514843462266468, 0.019065476680074588, 0.017812513121506758, 0.018077678622270506, 0.020886197903861793, 0.019826059643358197, 0.01689501619494383, 0.020104555314307045, 0.018094144035249716, 0.019132526778443515, 0.01831236237840725, 0.016741702450513686, 0.017994032533579382, 0.018593255329383267, 0.01725257347629649, 0.01742802903351592, 0.016522847959158447, 0.017692876968558546, 0.017530132491394814, 0.01793770892069536, 0.016699434691758594, 0.017189728068264052, 0.016945026413914398, 0.016120658236000236, 0.015964834295581022, 0.01599382102971379, 0.016682191167300527, 0.016360376717048507, 0.013966823116228834, 0.013440097732082905, 0.014393674061131869, 0.014212204759700796, 0.01287455717724612, 0.013187496088061053, 0.01280185479142301, 0.013734111792607409, 0.012507754075853162, 0.011445592381804044, 0.01187001896814005, 0.012085151101836292, 0.0111890356844285, 0.011589629286811932, 0.01055983796118125, 0.010539682344789112, 0.010618463825826277, 0.011464962632211912, 0.010497874452415389, 0.009883424433730848, 0.010033084822795028, 0.009406582386478953, 0.009045083556641322, 0.00924734973425732, 0.009760812388020096, 0.00896869082617209, 0.009202666632839663, 0.008652399540502757, 0.008005286120202695, 0.00862257131338304, 0.008506503358333894, 0.009684217492171783, 0.008038373176845321, 0.008677315894294262, 0.007985087214175554, 0.008049489379271543, 0.008312464428761091, 0.0076232334649540875, 0.00805096523414123, 0.008262835501493287, 0.008138829596100109, 0.007636554086283512, 0.007731222200877218, 0.007439086233720489, 0.007045314940004428, 0.00729715079294095, 0.0073227052418539966, 0.007578802405821383, 0.006763781859057525, 0.0064780077554253065, 0.006607856812156785, 0.00717109756359713, 0.006605562697935144, 0.006942074446322479, 0.0067567255164652056, 0.006210456460769853, 0.006785195330028013, 0.007038298100994995, 0.0063575489076020675, 0.006208111691679887, 0.006361048335884879, 0.005970172983615102, 0.006497978384284087, 0.006344400485751514, 0.006634843439071369, 0.006401836320974464, 0.006032420758169454, 0.005872411308693493, 0.00600278972210193, 0.006492970796539772, 0.006088098444809265, 0.006405445027815534, 0.006179892863591075, 0.006227199469885093, 0.006277476008644556, 0.005904536439422425, 0.006066281102571924, 0.006222073091151928, 0.005992446558739518, 0.005989288540872982, 0.006173876029960075, 0.0062046789969200915, 0.006027022795228278, 0.006261980424417933, 0.006058058801691968, 0.005967079608316559, 0.00575652587529706, 0.006213820204890559, 0.006182992624281267, 0.00578975497230913, 0.005852219676174514, 0.005983262724608886, 0.00603923899473086, 0.0057997789259967075, 0.005782555972234211, 0.005669385783003056, 0.005678570456060094, 0.005688548445792361, 0.005492545048179486, 0.005722686431307806, 0.00552377420420775, 0.005751648897398667, 0.0056966905214517905, 0.005582243544784857, 0.005502246722722117, 0.005444362779432179, 0.00565506705224747, 0.005737387551470432, 0.005721610891200807, 0.005657384352895247, 0.0056204479097456675, 0.005420043177631749, 0.005537478855291856, 0.005531989477976525, 0.005486599399616979, 0.005601130826955304, 0.005701595765494298, 0.005669708444894294, 0.005596522817386176, 0.005823848058463423, 0.005510661786856262, 0.00561012035054153, 0.005504576965700118, 0.005823196048445113, 0.005681050926665233, 0.005685834777935711, 0.005746460348536343], "accuracy_test_std": 0.00871719772202811, "error_valid": [0.8463575983621988, 0.7785350385918675, 0.623866069747741, 0.5296233763177711, 0.6331625329442772, 0.6272222679781627, 0.6080778190888554, 0.5105083419615963, 0.5708949077560241, 0.5309573253953314, 0.5740481457078314, 0.4920345444277108, 0.45283820830195776, 0.45791368599397586, 0.3717291039156627, 0.3768354668674698, 0.35271554969879515, 0.3906294121799698, 0.320080125188253, 0.36929799275225905, 0.3040786191641567, 0.33396819700677716, 0.2701224821159638, 0.26763989551957834, 0.26664274284638556, 0.24753800357680722, 0.2852386106927711, 0.2659000258847892, 0.21973538685993976, 0.19438447147966864, 0.19152537885918675, 0.2488498917545181, 0.18386583443147586, 0.19291962772966864, 0.18330548757530118, 0.18515713243599397, 0.17079254518072284, 0.18114940229668675, 0.17213531861822284, 0.1657979574548193, 0.17940953266189763, 0.16405808782003017, 0.16837172910391573, 0.16329478068524095, 0.15875758894954817, 0.1562558829066265, 0.14812717667545183, 0.14932728962725905, 0.15279673381024095, 0.1475374152861446, 0.15106715926204817, 0.14541074454066272, 0.15025384742093373, 0.14930669945406627, 0.14559458537274095, 0.14763889542545183, 0.14127064900225905, 0.14922580948795183, 0.13648931664156627, 0.14170892554593373, 0.13931752400225905, 0.13535979856927716, 0.13443471150225905, 0.13776149519954817, 0.13750705948795183, 0.13575689476656627, 0.13712025837725905, 0.13773060993975905, 0.13454648672816272, 0.13564511954066272, 0.13442441641566272, 0.1315859139683735, 0.1323698112763554, 0.1323183358433735, 0.13381406485316272, 0.13283750235316272, 0.13098585749246983, 0.1317079842808735, 0.13013136530496983, 0.13173886954066272, 0.13209478539156627, 0.13210508047816272, 0.12855474632906627, 0.12941953360316272, 0.13051816641566272, 0.1292459878576807, 0.13148443382906627, 0.12818853539156627, 0.1298769295933735, 0.13013136530496983, 0.1297548592808735, 0.1301210702183735, 0.12974456419427716, 0.13001959007906627, 0.12976515436746983, 0.12854445124246983, 0.1282900155308735, 0.12878859186746983, 0.12952101374246983, 0.1281679452183735, 0.1289003670933735, 0.12827972044427716, 0.12768995905496983, 0.12976515436746983, 0.12891066217996983, 0.1280458749058735, 0.1260721597326807, 0.12864593138177716, 0.12705901731927716, 0.12791350950677716, 0.1265604409826807, 0.12693694700677716, 0.1287782967808735, 0.12827972044427716, 0.12986663450677716, 0.1286356362951807, 0.1277811441076807, 0.12754729856927716, 0.1269472420933735, 0.1269266519201807, 0.1274355233433735, 0.12742522825677716, 0.1276590737951807, 0.1265604409826807, 0.12803557981927716, 0.1274149331701807, 0.12827972044427716, 0.12756788874246983, 0.1279032144201807, 0.1290224374058735, 0.1272928628576807, 0.12742522825677716, 0.1275370034826807, 0.12593979433358427, 0.1284120858433735, 0.12791350950677716, 0.12718108763177716, 0.12754729856927716, 0.12803557981927716, 0.12827972044427716, 0.1285341561558735, 0.1272928628576807, 0.1269266519201807, 0.12779143919427716, 0.1279032144201807, 0.1271707925451807, 0.12718108763177716, 0.12718108763177716, 0.12742522825677716, 0.12779143919427716, 0.12803557981927716, 0.12779143919427716, 0.12718108763177716, 0.12791350950677716, 0.12754729856927716, 0.1264383706701807, 0.12766936888177716, 0.1274149331701807, 0.12840179075677716, 0.12791350950677716, 0.1276590737951807, 0.1270487222326807, 0.12669280638177716, 0.12766936888177716, 0.12779143919427716, 0.1280458749058735, 0.12718108763177716, 0.1275575936558735, 0.12827972044427716, 0.12766936888177716, 0.12742522825677716, 0.12827972044427716, 0.12766936888177716, 0.12815765013177716, 0.12693694700677716, 0.12766936888177716, 0.12766936888177716, 0.12669280638177716, 0.12754729856927716, 0.12693694700677716, 0.12779143919427716, 0.12681487669427716, 0.12730315794427716, 0.12766936888177716], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.04719009150962536, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 128, "valid_ratio": 0.15, "learning_rate": 0.009253348413863495, "optimization": "rmsprop", "nb_data_augmentation": 3, "learning_rate_decay_method": "exp", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 2.888998595214301e-05, "rotation_range": [0, 0], "momentum": 0.5312120616459168}, "accuracy_valid_max": 0.8740602056664157, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8723306311182228, "accuracy_valid_std": [0.009498688197217118, 0.008749321606568855, 0.011848600313745383, 0.01470312294556214, 0.01720873347775312, 0.018106312148731388, 0.024635811877938733, 0.011545549261227871, 0.01574437058956384, 0.017847229558644564, 0.014069076666106298, 0.01630221043198746, 0.016630729330640772, 0.016475081845610112, 0.014011154589235582, 0.018229023221735315, 0.01473231348016099, 0.01731029549123784, 0.02087477339825285, 0.014301387072560826, 0.014432954295238122, 0.013952286305805842, 0.008914620099557838, 0.011017339331359371, 0.013770596445698094, 0.010603719704920562, 0.012602034619997957, 0.010807944923234147, 0.012340920220841508, 0.008806324986014654, 0.011918245330211338, 0.014512439979825965, 0.014305911051831747, 0.01334420364632616, 0.014698991695087227, 0.011936257260993692, 0.015543005592897771, 0.011284311649200233, 0.013105702508103842, 0.014957314103833464, 0.015353805812063437, 0.01691337388019264, 0.011063058524431709, 0.015008773817670108, 0.017973565500621656, 0.01410467564979353, 0.015695772311102363, 0.016449847149057454, 0.014310592081916448, 0.014035585787715299, 0.013337972613913028, 0.014574416160641437, 0.012332118612861096, 0.016728604813510312, 0.011650080183453676, 0.012314238141116155, 0.01471221142105385, 0.0149592252002581, 0.015189031492852181, 0.01186855298490073, 0.013124069102567065, 0.01699472349445475, 0.01384410985571941, 0.011876607862942598, 0.011851147157112692, 0.014418725414869023, 0.01264905290516805, 0.011930103855809478, 0.013986665791977652, 0.013557834155328189, 0.014022528369256137, 0.0152547078349771, 0.011864253306747612, 0.01598945017528873, 0.013281985704509407, 0.012724639234595915, 0.013841737658541207, 0.015384607808643341, 0.015041490668487676, 0.011817966921369182, 0.01385545785538521, 0.01306579316281127, 0.013397251948435131, 0.011676392920385206, 0.011436113253179316, 0.015780970698830055, 0.013994679929809755, 0.013868202171929364, 0.014690741610554166, 0.01322824429103749, 0.014999355551015628, 0.016135223308906686, 0.014657707527908702, 0.012070917244847611, 0.014224391353980764, 0.013049014615126966, 0.014689598924829971, 0.013035221523717727, 0.01396652058006417, 0.014873526957370252, 0.014469592163579474, 0.01468453126715479, 0.013785077488149044, 0.013098752333222472, 0.013194815242188487, 0.013314535047626043, 0.013891070419065796, 0.014112112606218368, 0.014662875401316278, 0.014003149184907743, 0.014117569987453478, 0.013847926019008795, 0.013519211825872167, 0.014545870296799712, 0.014694901307808738, 0.015504159057969936, 0.014140949767993967, 0.013985156404533176, 0.01328323580157693, 0.014369851437911465, 0.012392010119127064, 0.012829697417783063, 0.014109600585283726, 0.01364524680978916, 0.013353465898256481, 0.01437904383273654, 0.0128765669145087, 0.011618926381459243, 0.013516748043501883, 0.012416188280370735, 0.012845710797209265, 0.012132459748849585, 0.013760229186405423, 0.014786748048897284, 0.013193839571833775, 0.013797323696874872, 0.013130173071467573, 0.013916797441453362, 0.013308754960566032, 0.014611286436318077, 0.013292349006579341, 0.014436220671700164, 0.014616606573370342, 0.014091583704778475, 0.015427753822999461, 0.014639459181326607, 0.013664060876356852, 0.013399776201852742, 0.014339077077599455, 0.014318171424610072, 0.014293448730728672, 0.01413381845802068, 0.013940443112183913, 0.014088022319892809, 0.01413775562396958, 0.014033176210914359, 0.01400073866048694, 0.014026522064342718, 0.014088583026951458, 0.014062614164816346, 0.015010225593821392, 0.013598062763938608, 0.013247078455021915, 0.013941009754875817, 0.013254604774003193, 0.013043170096315365, 0.013157381982142075, 0.012835658265380384, 0.014214274498800762, 0.014228773126134095, 0.014163415593570828, 0.014773559588202293, 0.013751601515652943, 0.014153727999526782, 0.014221655055175874, 0.01366463898002039, 0.01397517181654483, 0.013982569443347564, 0.013330536606852635, 0.014246779574546458, 0.013696914859562587, 0.013817081426065157, 0.013141100283262127, 0.014262245930827377], "accuracy_valid": [0.1536424016378012, 0.22146496140813254, 0.37613393025225905, 0.4703766236822289, 0.3668374670557229, 0.37277773202183734, 0.3919221809111446, 0.4894916580384036, 0.4291050922439759, 0.46904267460466864, 0.42595185429216864, 0.5079654555722892, 0.5471617916980422, 0.5420863140060241, 0.6282708960843373, 0.6231645331325302, 0.6472844503012049, 0.6093705878200302, 0.679919874811747, 0.630702007247741, 0.6959213808358433, 0.6660318029932228, 0.7298775178840362, 0.7323601044804217, 0.7333572571536144, 0.7524619964231928, 0.7147613893072289, 0.7340999741152108, 0.7802646131400602, 0.8056155285203314, 0.8084746211408133, 0.7511501082454819, 0.8161341655685241, 0.8070803722703314, 0.8166945124246988, 0.814842867564006, 0.8292074548192772, 0.8188505977033133, 0.8278646813817772, 0.8342020425451807, 0.8205904673381024, 0.8359419121799698, 0.8316282708960843, 0.836705219314759, 0.8412424110504518, 0.8437441170933735, 0.8518728233245482, 0.850672710372741, 0.847203266189759, 0.8524625847138554, 0.8489328407379518, 0.8545892554593373, 0.8497461525790663, 0.8506933005459337, 0.854405414627259, 0.8523611045745482, 0.858729350997741, 0.8507741905120482, 0.8635106833584337, 0.8582910744540663, 0.860682475997741, 0.8646402014307228, 0.865565288497741, 0.8622385048004518, 0.8624929405120482, 0.8642431052334337, 0.862879741622741, 0.862269390060241, 0.8654535132718373, 0.8643548804593373, 0.8655755835843373, 0.8684140860316265, 0.8676301887236446, 0.8676816641566265, 0.8661859351468373, 0.8671624976468373, 0.8690141425075302, 0.8682920157191265, 0.8698686346950302, 0.8682611304593373, 0.8679052146084337, 0.8678949195218373, 0.8714452536709337, 0.8705804663968373, 0.8694818335843373, 0.8707540121423193, 0.8685155661709337, 0.8718114646084337, 0.8701230704066265, 0.8698686346950302, 0.8702451407191265, 0.8698789297816265, 0.8702554358057228, 0.8699804099209337, 0.8702348456325302, 0.8714555487575302, 0.8717099844691265, 0.8712114081325302, 0.8704789862575302, 0.8718320547816265, 0.8710996329066265, 0.8717202795557228, 0.8723100409450302, 0.8702348456325302, 0.8710893378200302, 0.8719541250941265, 0.8739278402673193, 0.8713540686182228, 0.8729409826807228, 0.8720864904932228, 0.8734395590173193, 0.8730630529932228, 0.8712217032191265, 0.8717202795557228, 0.8701333654932228, 0.8713643637048193, 0.8722188558923193, 0.8724527014307228, 0.8730527579066265, 0.8730733480798193, 0.8725644766566265, 0.8725747717432228, 0.8723409262048193, 0.8734395590173193, 0.8719644201807228, 0.8725850668298193, 0.8717202795557228, 0.8724321112575302, 0.8720967855798193, 0.8709775625941265, 0.8727071371423193, 0.8725747717432228, 0.8724629965173193, 0.8740602056664157, 0.8715879141566265, 0.8720864904932228, 0.8728189123682228, 0.8724527014307228, 0.8719644201807228, 0.8717202795557228, 0.8714658438441265, 0.8727071371423193, 0.8730733480798193, 0.8722085608057228, 0.8720967855798193, 0.8728292074548193, 0.8728189123682228, 0.8728189123682228, 0.8725747717432228, 0.8722085608057228, 0.8719644201807228, 0.8722085608057228, 0.8728189123682228, 0.8720864904932228, 0.8724527014307228, 0.8735616293298193, 0.8723306311182228, 0.8725850668298193, 0.8715982092432228, 0.8720864904932228, 0.8723409262048193, 0.8729512777673193, 0.8733071936182228, 0.8723306311182228, 0.8722085608057228, 0.8719541250941265, 0.8728189123682228, 0.8724424063441265, 0.8717202795557228, 0.8723306311182228, 0.8725747717432228, 0.8717202795557228, 0.8723306311182228, 0.8718423498682228, 0.8730630529932228, 0.8723306311182228, 0.8723306311182228, 0.8733071936182228, 0.8724527014307228, 0.8730630529932228, 0.8722085608057228, 0.8731851233057228, 0.8726968420557228, 0.8723306311182228], "seed": 407254986, "model": "residualv3", "loss_std": [0.24155382812023163, 0.12284540385007858, 0.10887864977121353, 0.11345464736223221, 0.1084737554192543, 0.1133628636598587, 0.10775133967399597, 0.10842674225568771, 0.10208936035633087, 0.10120025277137756, 0.1016436368227005, 0.10977456718683243, 0.09826535731554031, 0.09926733374595642, 0.09766163676977158, 0.0972193032503128, 0.09597304463386536, 0.09145159274339676, 0.09643356502056122, 0.08984772115945816, 0.09083044528961182, 0.08897008001804352, 0.089230015873909, 0.08344948291778564, 0.08619486540555954, 0.0818120688199997, 0.08189531415700912, 0.08083231747150421, 0.08001724630594254, 0.07828792929649353, 0.07620079070329666, 0.07367387413978577, 0.07795356959104538, 0.07273093611001968, 0.06947352737188339, 0.07180481404066086, 0.07022412121295929, 0.06952665001153946, 0.06852967292070389, 0.06967545300722122, 0.06542067229747772, 0.06460975110530853, 0.06693356484174728, 0.06472539901733398, 0.06313332915306091, 0.0629199743270874, 0.06182542070746422, 0.06250583380460739, 0.0598580464720726, 0.06099841743707657, 0.060168590396642685, 0.06057198345661163, 0.0571909099817276, 0.058354154229164124, 0.058354027569293976, 0.055817726999521255, 0.05594760924577713, 0.057371679693460464, 0.05678434297442436, 0.05370393767952919, 0.053090374916791916, 0.05463956668972969, 0.053706809878349304, 0.05005056783556938, 0.05310843884944916, 0.05299251526594162, 0.050289336591959, 0.05152466148138046, 0.0512513667345047, 0.05113432556390762, 0.05098697170615196, 0.047943614423274994, 0.04833212122321129, 0.050791215151548386, 0.04829317703843117, 0.04816100001335144, 0.04818248376250267, 0.047845643013715744, 0.04756831377744675, 0.04739575460553169, 0.045946430414915085, 0.04636896401643753, 0.04565739631652832, 0.04545820504426956, 0.04654880240559578, 0.04475035145878792, 0.04371093958616257, 0.04379728436470032, 0.04315236583352089, 0.042710646986961365, 0.04419931396842003, 0.04351290687918663, 0.042244959622621536, 0.04346922039985657, 0.04397840052843094, 0.042680706828832626, 0.042570050805807114, 0.0436539463698864, 0.04287360608577728, 0.04297778755426407, 0.04173138365149498, 0.0422840490937233, 0.04218989238142967, 0.03881416469812393, 0.04264960065484047, 0.040856245905160904, 0.042288217693567276, 0.040845196694135666, 0.04281746223568916, 0.04213551804423332, 0.04099326580762863, 0.040878843516111374, 0.04125380888581276, 0.04041552543640137, 0.0408092699944973, 0.040424756705760956, 0.040239907801151276, 0.04175089672207832, 0.04100305587053299, 0.04001639038324356, 0.04012086242437363, 0.04003390669822693, 0.04103228077292442, 0.0393080860376358, 0.04009920731186867, 0.039933718740940094, 0.04057649150490761, 0.040805358439683914, 0.04062154144048691, 0.03930053487420082, 0.03989526256918907, 0.04113244265317917, 0.04112115129828453, 0.03929252550005913, 0.04216529428958893, 0.04000832512974739, 0.0390668585896492, 0.04011162370443344, 0.03902757540345192, 0.03853151202201843, 0.03926737979054451, 0.0399162583053112, 0.03873104229569435, 0.03904574364423752, 0.04024472460150719, 0.03993583843111992, 0.03832307085394859, 0.03937628120183945, 0.03836018964648247, 0.039514608681201935, 0.040088310837745667, 0.03928029537200928, 0.04006673023104668, 0.03942085802555084, 0.03877441957592964, 0.03933599963784218, 0.03958941996097565, 0.038941640406847, 0.03941673785448074, 0.03973076865077019, 0.03858314827084541, 0.03757079690694809, 0.03927692025899887, 0.03855243697762489, 0.03918583691120148, 0.039744336158037186, 0.04052826762199402, 0.037097204476594925, 0.039678867906332016, 0.03839019685983658, 0.040293172001838684, 0.03952372074127197, 0.03959808498620987, 0.03873775154352188, 0.03971351310610771, 0.03903967887163162, 0.039492376148700714, 0.03882347047328949, 0.040268007665872574, 0.03971888870000839, 0.03937865048646927, 0.03910362720489502, 0.040330156683921814, 0.03924904018640518]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:24 2016", "state": "available"}], "summary": "47da65fa77412479170e94aad3d3a5a3"}