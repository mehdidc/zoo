{"content": {"hp_model": {"f0": 64, "f1": 16, "f2": 64, "f3": 16, "nonlin": "rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.6197322607040405, 1.2467082738876343, 1.0617362260818481, 0.9375545978546143, 0.8521558046340942, 0.7881054878234863, 0.7395074367523193, 0.699187695980072, 0.6654258370399475, 0.632552444934845, 0.6085668802261353, 0.5843712091445923, 0.5628589391708374, 0.5447802543640137, 0.525688111782074, 0.5122713446617126, 0.4969145953655243, 0.4847702085971832, 0.4689481556415558, 0.457952618598938, 0.4476584196090698, 0.4392108619213104, 0.42992740869522095, 0.4206213355064392, 0.4137994945049286, 0.40400785207748413, 0.398447185754776, 0.39041078090667725, 0.38519391417503357, 0.37697523832321167, 0.37195369601249695, 0.3676958680152893, 0.36208170652389526, 0.3554252088069916, 0.3499889075756073, 0.3484894931316376, 0.3423674404621124, 0.33774784207344055, 0.3340853452682495, 0.33280640840530396, 0.3279181718826294, 0.32468733191490173, 0.32275208830833435, 0.3172230124473572, 0.31475624442100525, 0.3126446008682251, 0.31101471185684204, 0.3074452877044678, 0.306364506483078, 0.30455639958381653, 0.30278363823890686, 0.2994632124900818, 0.29680705070495605, 0.2966744601726532, 0.29494741559028625, 0.29109251499176025, 0.29169589281082153, 0.28875765204429626, 0.28779298067092896, 0.28392425179481506, 0.28166595101356506, 0.2834642827510834, 0.2804875075817108, 0.27788352966308594, 0.27930599451065063, 0.2789272367954254, 0.2757452130317688, 0.2730831205844879, 0.27208831906318665, 0.2734784483909607, 0.27063432335853577, 0.2676207721233368, 0.26959457993507385, 0.2679652273654938, 0.26727160811424255, 0.2652358412742615, 0.26531192660331726, 0.2635771334171295, 0.26511844992637634, 0.26428794860839844, 0.2625159025192261, 0.2608679234981537, 0.2609574496746063, 0.2586652934551239, 0.25971123576164246, 0.25858116149902344, 0.25654029846191406, 0.25617486238479614, 0.2543098032474518, 0.255409836769104, 0.2577696740627289, 0.25584349036216736, 0.2532837688922882, 0.25119221210479736, 0.2512933611869812, 0.25081712007522583, 0.2515215575695038, 0.2514292597770691, 0.2489267885684967, 0.24870771169662476, 0.24630825221538544, 0.2479662001132965, 0.24612219631671906, 0.24799533188343048, 0.24715791642665863, 0.2455308437347412, 0.24664880335330963, 0.2454153150320053, 0.2460937201976776, 0.2446805089712143, 0.242811918258667, 0.24113930761814117, 0.24263735115528107, 0.2417912781238556, 0.24053330719470978, 0.24193711578845978, 0.2383587658405304, 0.2040577530860901, 0.1841699331998825, 0.1800413876771927, 0.17563942074775696, 0.17413002252578735, 0.1715141385793686, 0.16951587796211243, 0.16867975890636444, 0.1666385978460312, 0.16484524309635162, 0.16389499604701996, 0.16251757740974426, 0.16129279136657715, 0.16084039211273193, 0.16030599176883698, 0.15915018320083618, 0.15803353488445282, 0.15694428980350494, 0.15662041306495667, 0.15523362159729004, 0.15461912751197815, 0.1536831110715866, 0.15306538343429565, 0.1526721864938736, 0.1518651694059372, 0.15133021771907806, 0.1507773995399475, 0.14925163984298706, 0.14909204840660095, 0.14895008504390717, 0.1486031860113144, 0.1491038203239441, 0.1485540121793747, 0.14865005016326904, 0.14873197674751282, 0.14868292212486267, 0.14931009709835052, 0.14885485172271729, 0.149079367518425, 0.14901649951934814, 0.1485619992017746, 0.14927007257938385, 0.14885224401950836, 0.14861202239990234, 0.14928743243217468, 0.14897161722183228, 0.1492714285850525, 0.14908379316329956, 0.14900504052639008, 0.1488170027732849, 0.14861129224300385, 0.14879851043224335, 0.1488242894411087, 0.14873188734054565, 0.14914429187774658], "moving_avg_accuracy_train": [0.041736470588235285, 0.08761929411764705, 0.12547148235294114, 0.16469139294117643, 0.20021519482352937, 0.24131132239999997, 0.27779901368941173, 0.3145838182028235, 0.34208778932371764, 0.3705378339207577, 0.39870993288162304, 0.43073541018169603, 0.4594830456341147, 0.48918415283540906, 0.5177433846106917, 0.5367102226202107, 0.5633192003581896, 0.5891590450282529, 0.6092125522901335, 0.6271148264728847, 0.640386873237361, 0.6458964212077425, 0.6619467790869682, 0.6787968070606243, 0.6921759498839736, 0.7076148254838115, 0.7200980488177833, 0.7351682439360049, 0.7441078901306396, 0.7548076893528697, 0.7703904498293473, 0.7815161107287656, 0.7895833231853009, 0.7990626379255943, 0.8029210800153879, 0.8125207367197315, 0.8207016042242289, 0.8268596790959236, 0.8345713582451547, 0.8392601047735805, 0.8470964472373989, 0.8546220966313061, 0.8591810634387638, 0.8653194276831228, 0.8706510143265752, 0.8761129717174471, 0.8798546157221729, 0.8845044482676027, 0.8894798857937836, 0.8892966030967582, 0.8939151780812, 0.8960342485083742, 0.9000637648340074, 0.9030220942329596, 0.9056069436331932, 0.9079874257404621, 0.9111722125781806, 0.9146291089674213, 0.9142885510118557, 0.9155538135577289, 0.9178454910254854, 0.9165574125111722, 0.9196687300835844, 0.9221936217811084, 0.9241930831324094, 0.9251220101132861, 0.926522750278428, 0.9274163576035264, 0.9233758983137619, 0.9219559555412092, 0.9237697717517942, 0.9242869122236735, 0.9272158680601298, 0.9263225165482345, 0.9287443825404699, 0.9304534736981876, 0.9278951851518983, 0.9308444901661203, 0.9333223940906846, 0.9354183899757338, 0.9379424333311015, 0.9387858370568148, 0.9388601945276039, 0.9381482927219023, 0.9393452281555944, 0.9357330582812113, 0.936418575982502, 0.9381743654430753, 0.9392910465458266, 0.9392513536559499, 0.941596806525649, 0.9409100670495547, 0.9396496485798934, 0.942369389604257, 0.9428477447614783, 0.9441088526382716, 0.9435638497273856, 0.9451015824017058, 0.9459867182791822, 0.9480868699806757, 0.949172300629667, 0.9502762470372884, 0.9511121517453243, 0.952116230688439, 0.9531399017372422, 0.9521341468576355, 0.9529889674659897, 0.9540241883664495, 0.9531700048239222, 0.9505965337532947, 0.9509651156720829, 0.9493768393989923, 0.9501756260473284, 0.9514310046190662, 0.951574962980689, 0.9529657019767377, 0.9532432494261227, 0.9575518656599811, 0.9615566790939829, 0.9651845405963494, 0.9684660865367145, 0.9714688896477489, 0.9741737653888564, 0.9765846241440884, 0.9787873382002679, 0.9807415455567117, 0.9825591557069229, 0.9841714754303482, 0.9855943278873134, 0.9869337186279937, 0.9881297585299003, 0.989215606206322, 0.990159927938631, 0.9910333469094738, 0.9918123651597028, 0.9924993639378502, 0.9931317804852416, 0.9937197789073058, 0.9942560363106928, 0.9947316091502119, 0.9951572717646024, 0.9955238975293187, 0.9958538607175632, 0.9961720040575717, 0.9964818624753439, 0.9967677938748684, 0.9970274850756168, 0.997258854215114, 0.9974623805583084, 0.9976502601495364, 0.9978217047228181, 0.9979689460152421, 0.9981061690607768, 0.9982320227429344, 0.9983405851745233, 0.9984288795982474, 0.9985154034031285, 0.998590921886345, 0.9986683002859458, 0.9987308820220571, 0.9987919114669103, 0.9988444850261016, 0.9988941541705503, 0.9989388564005541, 0.998976735466381, 0.9990108266256252, 0.9990391557277686, 0.9990670048608741, 0.9990991279041984, 0.999120979819661, 0.9991406465435773, 0.9991536407127489], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.04203999999999999, 0.08716933333333332, 0.12395906666666665, 0.16242982666666664, 0.19653351066666663, 0.23610682626666663, 0.27177614363999997, 0.307398529276, 0.33413867634839994, 0.3612314753802266, 0.38824166117553727, 0.41797749505798354, 0.4447664122188518, 0.47234310433029997, 0.49854879389726997, 0.5163072478408763, 0.5403831897234553, 0.5634115374177765, 0.5817237170093322, 0.598031345308399, 0.609868210777559, 0.6154947230331365, 0.6291452507298227, 0.6438440589901738, 0.6551129864244898, 0.6681883544487075, 0.6787828523371701, 0.6913312337701197, 0.6985447770597745, 0.7078769660204637, 0.720155936085084, 0.7291936758099089, 0.7357543082289181, 0.7430588774060263, 0.7464196563320903, 0.7541776906988813, 0.7609465882956599, 0.7655985961327605, 0.7716254031861511, 0.7748495295342026, 0.780791243247449, 0.7863121189227041, 0.789814240363767, 0.7951661496607236, 0.7994895346946513, 0.8036472478918529, 0.8063225231026676, 0.8099036041257341, 0.8138599103798274, 0.812887252675178, 0.8162518607409937, 0.8179466746668943, 0.8208853405335382, 0.8229034731468511, 0.8249064591654993, 0.8265624799156162, 0.8289062319240545, 0.831575608731649, 0.8310047145251509, 0.8314909097393024, 0.8333418187653722, 0.832367636888835, 0.8343575398666182, 0.8362684525466231, 0.8376949406252941, 0.8379654465627647, 0.8386089019064882, 0.8391880117158393, 0.8357892105442555, 0.83493028948983, 0.8357839272075136, 0.836232201153429, 0.8384356477047528, 0.8377654162676108, 0.8398955413075164, 0.8411726538434314, 0.8389753884590883, 0.8413711829465128, 0.8434607313185282, 0.8444879915200086, 0.8462125257013411, 0.8466712731312069, 0.8467241458180863, 0.8463183979029443, 0.8469532247793164, 0.8442579023013848, 0.8450054454045797, 0.8465982341974551, 0.8472317441110429, 0.847361903033272, 0.8490923793966115, 0.8482631414569504, 0.846943493977922, 0.8491158112467965, 0.8488042301221168, 0.8497371404432384, 0.8488300930655812, 0.8505604170923564, 0.8508777087164541, 0.8527099378448086, 0.8534656107269945, 0.8545057163209616, 0.8552951446888655, 0.8562722968866455, 0.8573117338646477, 0.8564205604781829, 0.857191837763698, 0.8578726539873283, 0.8571787219219288, 0.8547008497297359, 0.8549507647567623, 0.8536156882810861, 0.8544141194529775, 0.8548260408410131, 0.8550234367569118, 0.8557610930812206, 0.8559849837730985, 0.860119818729122, 0.8637345035228765, 0.8670543865039222, 0.8701356145201967, 0.8730287197348438, 0.8753791810946927, 0.8776679296518901, 0.8797811366867011, 0.8813096896846976, 0.8828320540495611, 0.8840421819779384, 0.8853846304468111, 0.8866728340687967, 0.8878722173285837, 0.8889649955957253, 0.8898018293694862, 0.8904216464325375, 0.8908728151226171, 0.8915722002770221, 0.8921483135826532, 0.8926268155577212, 0.8932174673352824, 0.8936157206017542, 0.8938541485415789, 0.8941620670207543, 0.8945058603186788, 0.894535274286811, 0.8946417468581299, 0.8947775721723169, 0.8948731482884185, 0.8949858334595766, 0.8951405834469524, 0.8951598584355905, 0.8953905392586982, 0.8954248186661616, 0.8954423367995454, 0.8955647697862575, 0.895581626140965, 0.8955301301935352, 0.8955904505075151, 0.8956980721234302, 0.8957815982444206, 0.8958034384199786, 0.8958097612446474, 0.8957754517868494, 0.8957579066081645, 0.8957021159473479, 0.8956519043526131, 0.8957133805840184, 0.8958620425256166, 0.8958491716063882, 0.8959175877790828, 0.8958591623345079, 0.8958999127677237, 0.895856588157618], "moving_var_accuracy_train": [0.015677396794463665, 0.03305675857029757, 0.04264617610106906, 0.05222537096990469, 0.05836029837450418, 0.06772429385308769, 0.07293402900846166, 0.07781872269542477, 0.07684506627265308, 0.07644520498354987, 0.07594368892394203, 0.07758000079822523, 0.07725983961534912, 0.07747325757465925, 0.07706659929354215, 0.07259760786090197, 0.07171018634115409, 0.07054844586019572, 0.06711288965569683, 0.0632860234783568, 0.05854274615838711, 0.05296166761208981, 0.04998402674334185, 0.047540935053424604, 0.04439785471227037, 0.04210329915912877, 0.03929544702646825, 0.037409899351932875, 0.03438816488350683, 0.03197971972572048, 0.030967149569753924, 0.02898445758681812, 0.0266717310795062, 0.024813274643065483, 0.022465935357001546, 0.021048722500872627, 0.019546189588920647, 0.01793286760515717, 0.01667481080234763, 0.015205188818183285, 0.01423734430525715, 0.013323328463931573, 0.012178053222701919, 0.011299363540799542, 0.010425259531949352, 0.009651230385611716, 0.008812106445773455, 0.008125484285500952, 0.00753573066414322, 0.006782459930652158, 0.006296195051569148, 0.005706989681690148, 0.005282423729888032, 0.004832946772393572, 0.0044097851129512, 0.004019806857223328, 0.00370911197631634, 0.003445751972498218, 0.0031022205927382883, 0.002806406537254367, 0.002573031954074937, 0.0023306610749987623, 0.0021847176408264944, 0.0020236215795018716, 0.0018572400328097992, 0.0016792821775510256, 0.0015290126168881009, 0.001383298161662516, 0.001391896146946455, 0.0012708526695477302, 0.0011733767658049844, 0.001058445997633387, 0.001029810438497244, 0.0009340120869617709, 0.0008933997922247098, 0.000830348746270739, 0.0008062174342183428, 0.0008038812913987403, 0.0007787532329932136, 0.0007404166984451802, 0.0007237121823386494, 0.0006577429327057094, 0.0005920184007362979, 0.0005373777982913186, 0.0004965339083540365, 0.000564310458331231, 0.0005121088231671529, 0.0004886431105191811, 0.0004510015896344396, 0.0004059156104005566, 0.00041483439183631984, 0.00037759545262492446, 0.00035413379983040326, 0.0003852933410038202, 0.00034882341981140076, 0.00032825461552245293, 0.0002981024075260754, 0.0002895737627725171, 0.0002676675761896299, 0.00028059655309424275, 0.0002631403350287445, 0.00024779458056397463, 0.00022930375263582585, 0.00021544694808829887, 0.00020333337502488892, 0.0001921039234230718, 0.00017946999553296626, 0.00017116813679440986, 0.00016061798883388941, 0.00020416097011271116, 0.00018496754677915824, 0.00018917438577820676, 0.00017599948818642562, 0.0001725833175931877, 0.000155511501922799, 0.00015736774632669524, 0.00014232426497396705, 0.0002951694031325772, 0.00040999923858977833, 0.00048745172645397084, 0.0005356234476371112, 0.0005632125415861429, 0.0005727384624010132, 0.0005677747756000217, 0.0005546648409596318, 0.0005335686943914803, 0.000509945184875689, 0.000482346840403038, 0.00045233273839136065, 0.0004232451725582088, 0.0003937952583249607, 0.000365027319079979, 0.00033655027897898024, 0.00030976099736873487, 0.00028424672253957135, 0.0002600697561761979, 0.00023766233676330878, 0.00021700778238612642, 0.0001978951521717007, 0.0001801411626857237, 0.00016375774436875972, 0.00014859169999406754, 0.00013471241134502967, 0.00012215210687365187, 0.00011080100633786559, 0.00010045671659118507, 9.101800060978216e-05, 8.239798565720882e-05, 7.453099384285502e-05, 6.73955831257695e-05, 6.092056398856211e-05, 5.50236275734583e-05, 4.9690736294144626e-05, 4.486421500854383e-05, 4.0483865721661925e-05, 3.6505642296842744e-05, 3.292245538645845e-05, 2.9681537219578582e-05, 2.6767270248143745e-05, 2.4125791486581713e-05, 2.1746733676175258e-05, 1.959693612069213e-05, 1.7659445723815283e-05, 1.5911485755739567e-05, 1.4333250592816908e-05, 1.2910385397782772e-05, 1.1626569700258733e-05, 1.0470892898165407e-05, 9.433090617560636e-06, 8.494079111688998e-06, 7.648152220786483e-06, 6.884856634599997e-06], "duration": 119075.777594, "accuracy_train": [0.4173647058823529, 0.5005647058823529, 0.4661411764705882, 0.5176705882352941, 0.5199294117647059, 0.6111764705882353, 0.6061882352941177, 0.6456470588235295, 0.5896235294117647, 0.6265882352941177, 0.6522588235294118, 0.718964705882353, 0.7182117647058823, 0.7564941176470589, 0.7747764705882353, 0.7074117647058824, 0.8028, 0.8217176470588236, 0.7896941176470588, 0.788235294117647, 0.759835294117647, 0.6954823529411764, 0.8064, 0.8304470588235294, 0.8125882352941176, 0.8465647058823529, 0.8324470588235294, 0.8708, 0.824564705882353, 0.8511058823529412, 0.9106352941176471, 0.8816470588235295, 0.8621882352941177, 0.8843764705882353, 0.8376470588235294, 0.8989176470588235, 0.8943294117647059, 0.8822823529411765, 0.9039764705882353, 0.8814588235294117, 0.9176235294117647, 0.9223529411764706, 0.9002117647058824, 0.920564705882353, 0.9186352941176471, 0.9252705882352941, 0.9135294117647059, 0.9263529411764706, 0.9342588235294118, 0.8876470588235295, 0.9354823529411764, 0.9151058823529412, 0.9363294117647059, 0.9296470588235294, 0.9288705882352941, 0.9294117647058824, 0.9398352941176471, 0.9457411764705882, 0.9112235294117647, 0.9269411764705883, 0.9384705882352942, 0.9049647058823529, 0.9476705882352942, 0.9449176470588235, 0.9421882352941177, 0.9334823529411764, 0.9391294117647059, 0.9354588235294118, 0.8870117647058824, 0.9091764705882353, 0.9400941176470589, 0.9289411764705883, 0.9535764705882352, 0.9182823529411764, 0.9505411764705882, 0.9458352941176471, 0.9048705882352941, 0.9573882352941177, 0.9556235294117648, 0.9542823529411765, 0.9606588235294118, 0.9463764705882353, 0.9395294117647058, 0.9317411764705882, 0.9501176470588235, 0.9032235294117648, 0.9425882352941176, 0.9539764705882353, 0.9493411764705882, 0.9388941176470589, 0.9627058823529412, 0.9347294117647059, 0.9283058823529412, 0.9668470588235294, 0.9471529411764706, 0.9554588235294118, 0.9386588235294118, 0.9589411764705882, 0.9539529411764706, 0.9669882352941176, 0.9589411764705882, 0.9602117647058823, 0.958635294117647, 0.9611529411764705, 0.9623529411764706, 0.9430823529411765, 0.9606823529411764, 0.9633411764705883, 0.9454823529411764, 0.927435294117647, 0.9542823529411765, 0.9350823529411765, 0.9573647058823529, 0.9627294117647058, 0.9528705882352941, 0.9654823529411765, 0.9557411764705882, 0.9963294117647059, 0.9976, 0.997835294117647, 0.998, 0.9984941176470589, 0.9985176470588235, 0.9982823529411765, 0.9986117647058823, 0.9983294117647059, 0.9989176470588236, 0.9986823529411765, 0.9984, 0.9989882352941176, 0.9988941176470588, 0.9989882352941176, 0.9986588235294117, 0.9988941176470588, 0.9988235294117647, 0.9986823529411765, 0.9988235294117647, 0.9990117647058824, 0.9990823529411764, 0.9990117647058824, 0.9989882352941176, 0.9988235294117647, 0.9988235294117647, 0.999035294117647, 0.9992705882352941, 0.9993411764705883, 0.9993647058823529, 0.9993411764705883, 0.9992941176470588, 0.9993411764705883, 0.9993647058823529, 0.9992941176470588, 0.9993411764705883, 0.9993647058823529, 0.9993176470588235, 0.9992235294117647, 0.9992941176470588, 0.9992705882352941, 0.9993647058823529, 0.9992941176470588, 0.9993411764705883, 0.9993176470588235, 0.9993411764705883, 0.9993411764705883, 0.9993176470588235, 0.9993176470588235, 0.9992941176470588, 0.9993176470588235, 0.9993882352941177, 0.9993176470588235, 0.9993176470588235, 0.9992705882352941], "end": "2016-02-05 09:54:10.279000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0], "moving_var_accuracy_valid": [0.015906254399999997, 0.032645539503999996, 0.041562345862239995, 0.050726105650814396, 0.05612104644707966, 0.06460336757056652, 0.06959373263042608, 0.07405494859298256, 0.07308477292276655, 0.07238247346490043, 0.0717101773486852, 0.07249713796397665, 0.07170623891144577, 0.07137988055058765, 0.07042253598665249, 0.06621854656619193, 0.06481355070737264, 0.06310493881441029, 0.05981246822550976, 0.0562246700696235, 0.05186320551987611, 0.04696180372934797, 0.043942655513995184, 0.04149288464106675, 0.038486494706638986, 0.036176532476693696, 0.03356906969860207, 0.031629319618023025, 0.028934704517346223, 0.02682504182279369, 0.0254994955931449, 0.023684672687833516, 0.02170358249868634, 0.02001343482658614, 0.018113744858836413, 0.01684405424807956, 0.015572010595352672, 0.01420958012806542, 0.013115523744588067, 0.011897526286503063, 0.011025509314504382, 0.010197278997048554, 0.009287934788635273, 0.008616927707877402, 0.007923459860453968, 0.0072866930856802245, 0.0066224376541946, 0.006075611160419048, 0.005608921276962739, 0.00505654371636019, 0.004652774631653129, 0.004213348716678657, 0.003869735658692793, 0.003519417826027766, 0.003203583620343092, 0.0029079069008321376, 0.0026665547720424545, 0.002464029447706522, 0.0022205597846909893, 0.0020006312782982656, 0.0018314009284735176, 0.001656802108583328, 0.0015267593224739077, 0.001406947675661947, 0.0012845667222430672, 0.0011567686111786221, 0.0010448180630750572, 0.0009433545703091325, 0.0009529857579138623, 0.0008643268905220954, 0.0007844524776473556, 0.0007078157756578985, 0.0006807307884329739, 0.0006167006012036768, 0.0005958674352540039, 0.0005509598395931253, 0.0005393156321569062, 0.0005370425499749784, 0.0005226342065684097, 0.000479868157605481, 0.00045864750512818945, 0.0004146767974550476, 0.00037323427739870336, 0.00033739253199461145, 0.00030728032526183114, 0.00034193516207604033, 0.00031277103208864407, 0.00030432671412816394, 0.00027750605601087406, 0.0002499079225151091, 0.0002518680662602875, 0.00023286997967942002, 0.00022525620693163154, 0.00024520124708832047, 0.00022155486755479816, 0.00020723227580461605, 0.00019391366273198725, 0.0002014684875975087, 0.00018222770461026082, 0.00019421850635835394, 0.00017993602926635746, 0.00017167880315913897, 0.00016011969717568702, 0.000152701165216757, 0.00014715491177622487, 0.00013958713064129007, 0.00013098223543752402, 0.00012205560846699434, 0.0001141839230228014, 0.0001580241861281058, 0.0001427838852018977, 0.00014454735944484463, 0.0001358300545265909, 0.0001237741621432223, 0.00011174743225742141, 0.00010546992070681452, 9.537407201331931e-05, 0.00023970840583397053, 0.0003333310806743711, 0.0003991925796774651, 0.0004447190165041896, 0.0004755776349009293, 0.0004777418888481233, 0.0004771130295859694, 0.0004695925223751445, 0.0004436615385467885, 0.00042015372402676794, 0.00039131803805143807, 0.0003684057452705053, 0.00034650038788872496, 0.0003247970309345679, 0.00030306480691134484, 0.00027906094310437105, 0.0002546124075187808, 0.0002309831454490759, 0.00021228708725198675, 0.000194045537395115, 0.00017670166091689947, 0.0001621713205262355, 0.00014738163945191078, 0.00013315510644912066, 0.0001206929199125681, 0.00010968737240659194, 9.872642179962424e-05, 8.895580729565114e-05, 8.022626320985204e-05, 7.228585003458852e-05, 6.517154656132016e-05, 5.8869919932523057e-05, 5.298627166595374e-05, 4.8166567278704804e-05, 4.3360486250818776e-05, 3.902719959071219e-05, 3.5259388157758216e-05, 3.1736006572228644e-05, 2.8586272408421047e-05, 2.576039203008655e-05, 2.3288594536987595e-05, 2.1022524599278016e-05, 1.892456507876583e-05, 1.703246837389538e-05, 1.5339815786555424e-05, 1.3808604707555662e-05, 1.2455757617309112e-05, 1.12328726937905e-05, 1.0143599367661698e-05, 9.328142786813131e-06, 8.396819453187856e-06, 7.599264462044546e-06, 6.870059809004118e-06, 6.1979992083692414e-06, 5.595092484099625e-06], "accuracy_test": 0.7782, "start": "2016-02-04 00:49:34.501000", "learning_rate_per_epoch": [0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 0.0008454652270302176, 8.454652561340481e-05, 8.454652561340481e-05, 8.454652561340481e-05, 8.454652561340481e-05, 8.454652561340481e-05, 8.454652561340481e-05, 8.454652561340481e-05, 8.454652561340481e-05, 8.454652561340481e-05, 8.454652561340481e-05, 8.454652561340481e-05, 8.454652561340481e-05, 8.454652561340481e-05, 8.454652561340481e-05, 8.454652561340481e-05, 8.454652561340481e-05, 8.454652561340481e-05, 8.454652561340481e-05, 8.454652561340481e-05, 8.454652561340481e-05, 8.454652561340481e-05, 8.454652561340481e-05, 8.454652561340481e-05, 8.454652561340481e-05, 8.454652561340481e-05, 8.454652561340481e-05, 8.454652561340481e-05, 8.454652743239421e-06, 8.454652515865746e-07, 8.45465280008284e-08, 8.454652977718524e-09, 8.454653088740827e-10, 8.454652949962949e-11, 8.454652776490601e-12, 8.454652993331035e-13, 8.454653264381579e-14, 8.454653433788168e-15, 8.454653539667286e-16, 8.454653407318388e-17, 8.454653572754511e-18, 8.454653365959358e-19, 8.454653107465416e-20, 8.454653430582843e-21, 8.454653228634451e-22, 8.454653354852196e-23, 8.454653039307834e-24, 8.45465323652306e-25, 8.454653113263544e-26, 8.454652959189149e-27, 8.454653344375137e-28, 8.454653103633894e-29, 8.454653404560448e-30, 8.45465378071864e-31, 8.45465378071864e-32, 8.454653486845053e-33, 8.454653119503068e-34], "accuracy_train_first": 0.4173647058823529, "accuracy_train_last": 0.9992705882352941, "batch_size_eval": 1024, "accuracy_train_std": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "accuracy_test_std": 0, "error_valid": [0.5796, 0.5066666666666666, 0.5449333333333333, 0.4913333333333333, 0.4965333333333334, 0.4077333333333333, 0.4072, 0.372, 0.4252, 0.39493333333333336, 0.3686666666666667, 0.3144, 0.3141333333333334, 0.27946666666666664, 0.26559999999999995, 0.32386666666666664, 0.24293333333333333, 0.22933333333333328, 0.2534666666666666, 0.2552, 0.28359999999999996, 0.33386666666666664, 0.248, 0.22386666666666666, 0.24346666666666672, 0.2141333333333333, 0.22586666666666666, 0.19573333333333331, 0.23653333333333337, 0.20813333333333328, 0.16933333333333334, 0.18946666666666667, 0.20520000000000005, 0.19120000000000004, 0.22333333333333338, 0.17600000000000005, 0.17813333333333337, 0.19253333333333333, 0.17413333333333336, 0.19613333333333338, 0.1657333333333333, 0.16400000000000003, 0.17866666666666664, 0.15666666666666662, 0.16159999999999997, 0.15893333333333337, 0.16959999999999997, 0.1578666666666667, 0.1505333333333333, 0.19586666666666663, 0.15346666666666664, 0.16679999999999995, 0.15266666666666662, 0.15893333333333337, 0.1570666666666667, 0.1585333333333333, 0.15000000000000002, 0.14439999999999997, 0.17413333333333336, 0.16413333333333335, 0.15000000000000002, 0.1764, 0.14773333333333338, 0.1465333333333333, 0.14946666666666664, 0.15959999999999996, 0.15559999999999996, 0.15559999999999996, 0.19479999999999997, 0.17279999999999995, 0.1565333333333333, 0.15973333333333328, 0.14173333333333338, 0.16826666666666668, 0.14093333333333335, 0.14733333333333332, 0.18079999999999996, 0.13706666666666667, 0.13773333333333337, 0.14626666666666666, 0.13826666666666665, 0.1492, 0.15280000000000005, 0.15733333333333333, 0.14733333333333332, 0.18000000000000005, 0.14826666666666666, 0.13906666666666667, 0.14706666666666668, 0.15146666666666664, 0.1353333333333333, 0.1592, 0.16493333333333338, 0.1313333333333333, 0.15400000000000003, 0.1418666666666667, 0.15933333333333333, 0.1338666666666667, 0.14626666666666666, 0.13080000000000003, 0.13973333333333338, 0.13613333333333333, 0.13759999999999994, 0.13493333333333335, 0.1333333333333333, 0.15159999999999996, 0.1358666666666667, 0.136, 0.14906666666666668, 0.16759999999999997, 0.14280000000000004, 0.15839999999999999, 0.13839999999999997, 0.14146666666666663, 0.1432, 0.13759999999999994, 0.14200000000000002, 0.10266666666666668, 0.10373333333333334, 0.10306666666666664, 0.1021333333333333, 0.10093333333333332, 0.1034666666666667, 0.10173333333333334, 0.10119999999999996, 0.10493333333333332, 0.1034666666666667, 0.10506666666666664, 0.10253333333333337, 0.10173333333333334, 0.10133333333333339, 0.10119999999999996, 0.10266666666666668, 0.10399999999999998, 0.10506666666666664, 0.1021333333333333, 0.10266666666666668, 0.10306666666666664, 0.1014666666666667, 0.1028, 0.10399999999999998, 0.10306666666666664, 0.10240000000000005, 0.10519999999999996, 0.10440000000000005, 0.10399999999999998, 0.10426666666666662, 0.10399999999999998, 0.1034666666666667, 0.10466666666666669, 0.10253333333333337, 0.10426666666666662, 0.10440000000000005, 0.10333333333333339, 0.10426666666666662, 0.10493333333333332, 0.10386666666666666, 0.10333333333333339, 0.1034666666666667, 0.10399999999999998, 0.1041333333333333, 0.10453333333333337, 0.10440000000000005, 0.1048, 0.1048, 0.10373333333333334, 0.1028, 0.10426666666666662, 0.1034666666666667, 0.10466666666666669, 0.10373333333333334, 0.10453333333333337], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.028192930796501117, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 64, "valid_ratio": 0.15, "learning_rate": 0.0008454652082365744, "optimization": "rmsprop", "nb_data_augmentation": 3, "learning_rate_decay_method": "discrete", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 2.3904808462983076e-05, "rotation_range": [0, 0], "momentum": 0.6635154338103032}, "accuracy_valid_max": 0.8990666666666667, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        y_pred = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            y_pred.extend((nnet.predict(X[mini_batch]) == y[mini_batch]).tolist())\n        return np.mean(y_pred)\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            acc = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = acc\n            status[\"accuracy_train_std\"] = 0\n            acc = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = acc\n            status[\"accuracy_valid_std\"] = 0\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n\n    # rescaling to [-1, 1]\n    X_min = X_train.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X_train.max(axis=(0, 2, 3))[None, :, None, None]\n    def preprocess(a):\n        return (a / 255.) * 2 - 1\n        # return 2 * ((a - X_min) / (X_max - X_min)) - 1\n    X_train = preprocess(X_train)\n    X_valid = preprocess(X_valid)\n\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = preprocess(X_test)\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    acc = evaluate(X_test, y_test, batch_size_eval)\n    light.set(\"accuracy_test\", acc)\n    light.set(\"accuracy_test_std\", 0)\n    print(\"Test accuracy : {}+-{}\".format(acc, 0))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8954666666666666, "accuracy_valid_std": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "accuracy_valid": [0.4204, 0.49333333333333335, 0.4550666666666667, 0.5086666666666667, 0.5034666666666666, 0.5922666666666667, 0.5928, 0.628, 0.5748, 0.6050666666666666, 0.6313333333333333, 0.6856, 0.6858666666666666, 0.7205333333333334, 0.7344, 0.6761333333333334, 0.7570666666666667, 0.7706666666666667, 0.7465333333333334, 0.7448, 0.7164, 0.6661333333333334, 0.752, 0.7761333333333333, 0.7565333333333333, 0.7858666666666667, 0.7741333333333333, 0.8042666666666667, 0.7634666666666666, 0.7918666666666667, 0.8306666666666667, 0.8105333333333333, 0.7948, 0.8088, 0.7766666666666666, 0.824, 0.8218666666666666, 0.8074666666666667, 0.8258666666666666, 0.8038666666666666, 0.8342666666666667, 0.836, 0.8213333333333334, 0.8433333333333334, 0.8384, 0.8410666666666666, 0.8304, 0.8421333333333333, 0.8494666666666667, 0.8041333333333334, 0.8465333333333334, 0.8332, 0.8473333333333334, 0.8410666666666666, 0.8429333333333333, 0.8414666666666667, 0.85, 0.8556, 0.8258666666666666, 0.8358666666666666, 0.85, 0.8236, 0.8522666666666666, 0.8534666666666667, 0.8505333333333334, 0.8404, 0.8444, 0.8444, 0.8052, 0.8272, 0.8434666666666667, 0.8402666666666667, 0.8582666666666666, 0.8317333333333333, 0.8590666666666666, 0.8526666666666667, 0.8192, 0.8629333333333333, 0.8622666666666666, 0.8537333333333333, 0.8617333333333334, 0.8508, 0.8472, 0.8426666666666667, 0.8526666666666667, 0.82, 0.8517333333333333, 0.8609333333333333, 0.8529333333333333, 0.8485333333333334, 0.8646666666666667, 0.8408, 0.8350666666666666, 0.8686666666666667, 0.846, 0.8581333333333333, 0.8406666666666667, 0.8661333333333333, 0.8537333333333333, 0.8692, 0.8602666666666666, 0.8638666666666667, 0.8624, 0.8650666666666667, 0.8666666666666667, 0.8484, 0.8641333333333333, 0.864, 0.8509333333333333, 0.8324, 0.8572, 0.8416, 0.8616, 0.8585333333333334, 0.8568, 0.8624, 0.858, 0.8973333333333333, 0.8962666666666667, 0.8969333333333334, 0.8978666666666667, 0.8990666666666667, 0.8965333333333333, 0.8982666666666667, 0.8988, 0.8950666666666667, 0.8965333333333333, 0.8949333333333334, 0.8974666666666666, 0.8982666666666667, 0.8986666666666666, 0.8988, 0.8973333333333333, 0.896, 0.8949333333333334, 0.8978666666666667, 0.8973333333333333, 0.8969333333333334, 0.8985333333333333, 0.8972, 0.896, 0.8969333333333334, 0.8976, 0.8948, 0.8956, 0.896, 0.8957333333333334, 0.896, 0.8965333333333333, 0.8953333333333333, 0.8974666666666666, 0.8957333333333334, 0.8956, 0.8966666666666666, 0.8957333333333334, 0.8950666666666667, 0.8961333333333333, 0.8966666666666666, 0.8965333333333333, 0.896, 0.8958666666666667, 0.8954666666666666, 0.8956, 0.8952, 0.8952, 0.8962666666666667, 0.8972, 0.8957333333333334, 0.8965333333333333, 0.8953333333333333, 0.8962666666666667, 0.8954666666666666], "seed": 261753474, "model": "residualv3", "loss_std": [0.257914662361145, 0.14670012891292572, 0.1371423304080963, 0.12971973419189453, 0.12765739858150482, 0.12255673110485077, 0.12178535759449005, 0.11677234619855881, 0.11500676721334457, 0.11259971559047699, 0.10875646770000458, 0.10881739854812622, 0.10759921371936798, 0.10342969000339508, 0.1003161147236824, 0.09896350651979446, 0.0975019782781601, 0.09661117941141129, 0.09260909259319305, 0.09334567189216614, 0.09193523228168488, 0.09039869159460068, 0.08700314164161682, 0.08562295138835907, 0.08505991846323013, 0.0823732540011406, 0.08115106076002121, 0.08016373217105865, 0.07771313935518265, 0.07841203361749649, 0.07791277766227722, 0.07576043158769608, 0.07399486750364304, 0.07291928678750992, 0.06853529810905457, 0.071474090218544, 0.06627930700778961, 0.0675182193517685, 0.06632591038942337, 0.06672602146863937, 0.06410747766494751, 0.061318539083004, 0.06450610607862473, 0.061269160360097885, 0.05942349135875702, 0.057494983077049255, 0.059675104916095734, 0.05721205100417137, 0.058160219341516495, 0.056778520345687866, 0.05711503326892853, 0.05352795869112015, 0.05379726365208626, 0.055490657687187195, 0.05666076019406319, 0.05316290259361267, 0.05472611263394356, 0.05184785649180412, 0.05288446322083473, 0.05213148891925812, 0.049615100026130676, 0.04833177104592323, 0.04882960394024849, 0.04840435832738876, 0.048884231597185135, 0.048260483890771866, 0.048268213868141174, 0.04852815344929695, 0.0482381172478199, 0.04756099730730057, 0.04535510390996933, 0.04478076100349426, 0.04657112807035446, 0.044048186391592026, 0.04559100791811943, 0.04445626214146614, 0.0451710969209671, 0.045748818665742874, 0.04515496641397476, 0.04293779656291008, 0.04465735703706741, 0.044691551476716995, 0.044076621532440186, 0.042693182826042175, 0.04402017593383789, 0.042457837611436844, 0.04237142205238342, 0.03945889696478844, 0.04173625633120537, 0.040794163942337036, 0.042165692895650864, 0.04141489788889885, 0.04196042940020561, 0.041682168841362, 0.04114589840173721, 0.0409914031624794, 0.04260513186454773, 0.04208146780729294, 0.04092708230018616, 0.041009366512298584, 0.0394638366997242, 0.040846046060323715, 0.0401323027908802, 0.03951595723628998, 0.03795916959643364, 0.03921831399202347, 0.04017568379640579, 0.03956932947039604, 0.04048788920044899, 0.04175911471247673, 0.03677220642566681, 0.03778611123561859, 0.04047510772943497, 0.038088392466306686, 0.03755468502640724, 0.038089897483587265, 0.03669968619942665, 0.03292681649327278, 0.018093906342983246, 0.016373146325349808, 0.01388512458652258, 0.014139763079583645, 0.012696600519120693, 0.01179561298340559, 0.012040135450661182, 0.0107297757640481, 0.010178782977163792, 0.009869905188679695, 0.009284177795052528, 0.008739636279642582, 0.009247460402548313, 0.009249660186469555, 0.009388964623212814, 0.008632592856884003, 0.008374711498618126, 0.009036044590175152, 0.008242405951023102, 0.007393679581582546, 0.00752998935058713, 0.007749601732939482, 0.007801654748618603, 0.00743720680475235, 0.007533753756433725, 0.008129776455461979, 0.007059705443680286, 0.00663071870803833, 0.006748504471033812, 0.006880796514451504, 0.006702232640236616, 0.005858773831278086, 0.006315285339951515, 0.006631091702729464, 0.00656027952209115, 0.006890377029776573, 0.006485235411673784, 0.0068111298605799675, 0.006933844182640314, 0.006024859379976988, 0.007203905377537012, 0.006061083637177944, 0.006576023995876312, 0.006729771848767996, 0.006881887558847666, 0.007055490743368864, 0.006876776460558176, 0.0068481434136629105, 0.006641089450567961, 0.006374526768922806, 0.006744734011590481, 0.006873463746160269, 0.0062558469362556934, 0.006593566387891769]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:39 2016", "state": "available"}], "summary": "6a90a7b7010f41b5e7445c5030aac34a"}