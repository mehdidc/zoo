{"content": {"hp_model": {"f0": 32, "f1": 32, "f2": 16, "f3": 64, "nonlin": "rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.7227164506912231, 1.2969976663589478, 1.0516425371170044, 0.9026637077331543, 0.8071889281272888, 0.7409431338310242, 0.6862480044364929, 0.6452783346176147, 0.6080986857414246, 0.5774709582328796, 0.547817587852478, 0.5247987508773804, 0.5020673871040344, 0.4824534058570862, 0.46445927023887634, 0.44843319058418274, 0.4325184226036072, 0.4196372926235199, 0.40514227747917175, 0.39230313897132874, 0.38094186782836914, 0.3699686527252197, 0.35919180512428284, 0.35009491443634033, 0.34016042947769165, 0.3308376669883728, 0.32184210419654846, 0.3138869106769562, 0.30476298928260803, 0.297948956489563, 0.29083773493766785, 0.28357577323913574, 0.2763644754886627, 0.26900213956832886, 0.26327183842658997, 0.25625085830688477, 0.25161492824554443, 0.24602845311164856, 0.24038903415203094, 0.23249328136444092, 0.2295270413160324, 0.22396868467330933, 0.22079922258853912, 0.2141178399324417, 0.21086125075817108, 0.2081373929977417, 0.2034643143415451, 0.1990032196044922, 0.19462046027183533, 0.19274699687957764, 0.1878761649131775, 0.18470610678195953, 0.1810278296470642, 0.17893141508102417, 0.1755247861146927, 0.17236530780792236, 0.16799676418304443, 0.16636082530021667, 0.16197653114795685, 0.16028107702732086, 0.15915553271770477, 0.15408951044082642, 0.15222908556461334, 0.1505945473909378, 0.1470239907503128, 0.14577902853488922, 0.1443798542022705, 0.14206799864768982, 0.13859492540359497, 0.1371636688709259, 0.1358145773410797, 0.13345308601856232, 0.13262607157230377, 0.12872466444969177, 0.12870894372463226, 0.1275947540998459, 0.1244160607457161, 0.12198682874441147, 0.12278809398412704, 0.12063088268041611, 0.12147493660449982, 0.11924663186073303, 0.11696259677410126, 0.1148843839764595, 0.11391592025756836, 0.11326061189174652, 0.11196151375770569, 0.11087419092655182, 0.10821136087179184, 0.10920823365449905, 0.10723060369491577, 0.10558471828699112, 0.10586938261985779, 0.10351207107305527, 0.10288096219301224, 0.10196422040462494, 0.10158572345972061, 0.09988784044981003, 0.10023903846740723, 0.09980238974094391, 0.09804970771074295, 0.09890221804380417, 0.09702028334140778, 0.09791583567857742, 0.09503636509180069, 0.09411937743425369, 0.09367869794368744, 0.09414929151535034, 0.09187572449445724, 0.09170421212911606, 0.09137284010648727, 0.08949724584817886, 0.09055298566818237, 0.08982979506254196, 0.08871274441480637, 0.08690425008535385, 0.0856977105140686, 0.0863506942987442, 0.08626961708068848, 0.08632080256938934, 0.08492064476013184, 0.08608103543519974, 0.08377493917942047, 0.08375968039035797, 0.08264100551605225, 0.08288774639368057, 0.08268839865922928, 0.08192643523216248, 0.08156745880842209, 0.08241657167673111, 0.08004644513130188, 0.08031561225652695, 0.07895788550376892, 0.07987053692340851, 0.07847137749195099, 0.07821205258369446, 0.079020194709301, 0.0784255638718605, 0.07695108652114868, 0.07741383463144302, 0.07716748118400574, 0.07590630650520325, 0.07591143250465393, 0.07596070319414139, 0.07614170014858246, 0.07518203556537628, 0.07406716048717499, 0.07383439689874649, 0.0735728070139885, 0.07219208776950836, 0.07251134514808655, 0.07367654889822006, 0.07121641933917999, 0.07304182648658752, 0.0734734982252121, 0.07182101160287857, 0.07081348448991776, 0.07143723219633102, 0.07061372697353363, 0.07149955630302429, 0.0706987977027893, 0.06941375881433487, 0.06946995109319687, 0.07008016854524612, 0.0696963220834732, 0.06885368376970291, 0.06904406100511551, 0.06986629962921143, 0.06845350563526154, 0.06828576326370239, 0.06802646070718765, 0.0682850107550621, 0.06712402403354645, 0.06685013324022293, 0.06752312183380127, 0.06716907024383545, 0.06662968546152115, 0.06629002839326859, 0.06719499826431274, 0.0659237951040268, 0.06554166972637177, 0.06623184680938721, 0.06471511721611023, 0.0658554956316948, 0.06622854620218277, 0.06467273831367493, 0.06510057300329208, 0.06480082124471664, 0.06486397981643677, 0.06403122842311859, 0.06455042213201523, 0.06441514194011688, 0.06365299969911575, 0.06419091671705246, 0.06308761239051819, 0.06290165334939957, 0.06319603323936462, 0.06345220655202866, 0.06280362606048584, 0.06277085095643997, 0.06258249282836914, 0.06169506907463074, 0.06267838180065155, 0.06201648712158203, 0.061919935047626495, 0.061494261026382446, 0.06123470887541771, 0.0609959177672863, 0.06227138638496399, 0.06071125715970993, 0.060822341591119766, 0.06034953147172928, 0.059375423938035965, 0.06006330996751785, 0.06019914522767067, 0.059035543352365494, 0.06039934605360031, 0.060571227222681046, 0.05962858721613884, 0.059786610305309296, 0.059252362698316574, 0.05913722515106201, 0.05914491415023804, 0.05846447870135307, 0.05793273448944092, 0.0588107593357563, 0.057610224932432175, 0.058821432292461395, 0.0578174963593483, 0.05786348506808281, 0.057912427932024, 0.058297257870435715, 0.05725930631160736, 0.058157119899988174, 0.057443976402282715, 0.056833911687135696, 0.05785267800092697, 0.05682765319943428, 0.05727142095565796, 0.05651777610182762, 0.05782679095864296, 0.05714678764343262, 0.056847915053367615, 0.05617586895823479, 0.05622866004705429, 0.05580809339880943, 0.05565387383103371, 0.05535805970430374, 0.055325478315353394, 0.05613318458199501, 0.055735617876052856, 0.05532051995396614, 0.05518984794616699, 0.05535716563463211, 0.05561374872922897, 0.05536305159330368, 0.05527845025062561, 0.05537809804081917, 0.054377585649490356, 0.054447829723358154, 0.054769035428762436, 0.05438004806637764, 0.054676588624715805, 0.0539662167429924, 0.054103098809719086, 0.05335095152258873, 0.05496329441666603, 0.05358029156923294, 0.05370368808507919, 0.05371228978037834, 0.053271740674972534, 0.0536528155207634, 0.053865429013967514], "moving_avg_accuracy_train": [0.020927058823529406, 0.0529990588235294, 0.09734856470588232, 0.14061841411764703, 0.18657069035294116, 0.23453009190588237, 0.2855006121270588, 0.3336540803261176, 0.380514554646447, 0.42515721682886115, 0.46514502455773976, 0.5028140515137305, 0.536365587538828, 0.5671384405496511, 0.595469302377039, 0.6225459015510998, 0.6469689584548134, 0.6692391214328615, 0.6903457975248695, 0.709833570713559, 0.7281090371716149, 0.7447051922779828, 0.7605146730501846, 0.7749737939804603, 0.7882740616412378, 0.8003078319477023, 0.8119358722823438, 0.8227069909364624, 0.8327374683134043, 0.8419060744232404, 0.850143702275034, 0.8580046261651777, 0.8650300459016012, 0.8719082177820292, 0.8781103371802969, 0.8838028328740318, 0.8892672554689816, 0.8944181769809069, 0.8989998886945809, 0.9035304880604169, 0.9074150863131988, 0.9113229894465847, 0.9147201022666321, 0.9175680920399689, 0.9207289298947956, 0.9236183898464926, 0.9264636096853728, 0.9291607781286002, 0.9319058767863284, 0.9342917596959308, 0.9360978778439848, 0.938471619471351, 0.9404526928183337, 0.9425650705953238, 0.944546210594615, 0.9462680601233888, 0.9480389011698734, 0.9495714816411214, 0.9511343334770092, 0.9517456060116611, 0.9530886924693185, 0.9535845291047396, 0.9547460761942655, 0.9562102921042508, 0.9574716158350022, 0.9584538660162079, 0.9595614205910576, 0.9600711608848931, 0.9610828683258155, 0.9624334050226456, 0.9630418292262634, 0.9636952933624605, 0.964652822849744, 0.965269893505946, 0.9655499629788808, 0.9660773196221693, 0.9666742935423052, 0.9671880406586629, 0.9679398248280907, 0.9684917246982229, 0.968764905169577, 0.9693519440643841, 0.9698991025991222, 0.9704880158686217, 0.9714580378111712, 0.9724228222653482, 0.9730205400388134, 0.9738949566231674, 0.9741831080196742, 0.9748095031000598, 0.9749473763194656, 0.9747679328051662, 0.9752111395246496, 0.9755253196898316, 0.9760174936032013, 0.9762651560075871, 0.9767986404068284, 0.9774152469543809, 0.9778925457883545, 0.9783315265036366, 0.978698373853273, 0.9783767717620633, 0.9778496828211511, 0.9777282439508007, 0.9781695372027794, 0.9787808187766192, 0.9794321486636631, 0.9800418749737674, 0.9804870992410966, 0.9808572128463987, 0.9813409033264647, 0.9814279894644066, 0.9817275434591424, 0.9815736126426399, 0.9817433102019053, 0.9821830968287737, 0.9823930224400139, 0.9826643084313067, 0.9828614069999407, 0.9830599721822996, 0.9833398573170108, 0.9836929304088391, 0.9840365785444257, 0.9842493912782184, 0.9845938639151025, 0.9846756539941806, 0.9850127944771154, 0.9853515150294039, 0.9851716576441105, 0.9853580212914641, 0.9856245721034942, 0.9858668207754977, 0.9862730798744186, 0.9862810660046238, 0.9864576652865144, 0.9868613105225688, 0.9862975324114883, 0.9867760144644571, 0.9870701777238937, 0.9871678658338573, 0.9873075498387069, 0.987485030148954, 0.9878047624281763, 0.987951345008888, 0.9879891516844698, 0.9882208247513169, 0.9884058010997145, 0.9887181621662137, 0.9888251694790041, 0.9887473584134566, 0.989032622572111, 0.9890281838443116, 0.9892665419304686, 0.9895540053844806, 0.9896880166107384, 0.9897309796555469, 0.9899108228664628, 0.9901197405798166, 0.9901430606394819, 0.9904275781049455, 0.9904977614709216, 0.9905068088532413, 0.9907925985561524, 0.9907956916417135, 0.9909584754187186, 0.9912320396415526, 0.9912006003832797, 0.9912099521096576, 0.9913101333692801, 0.9914191200323521, 0.9915219139114698, 0.9914144284026758, 0.9914471032094669, 0.9912953340649908, 0.991422271246727, 0.9914141617691131, 0.9915033338274959, 0.9915624122094522, 0.9916108768708598, 0.9916615538896562, 0.9917118690889258, 0.9917289174741508, 0.991821908079677, 0.9918914819775917, 0.9919399808386561, 0.9920118651077316, 0.9920718550675467, 0.9922976107372626, 0.9921925555458893, 0.9922227117560063, 0.9923039699921704, 0.9924123965223651, 0.9925523333407168, 0.9927323941242922, 0.9927203311824512, 0.9929165333583237, 0.9929801741401385, 0.9931456861378894, 0.9932428822299828, 0.9933138881246316, 0.9933119110768743, 0.9933689552633046, 0.9933850009134447, 0.9934488537632767, 0.9933816154457725, 0.9934881597835482, 0.9936146379228404, 0.9935284682482034, 0.9935567978939712, 0.9935399416339858, 0.9936753592352932, 0.9937266468411756, 0.9937022174511757, 0.993786113353117, 0.9939110314295699, 0.9939105165219071, 0.9938229942814811, 0.9936854007356859, 0.9936415665444702, 0.993736233419435, 0.9938355512539621, 0.9939743490697424, 0.9940733847510035, 0.9941460462759032, 0.9941785004718422, 0.9942618268952462, 0.9942709383233687, 0.9944838444910318, 0.9945672247478109, 0.9945481493318533, 0.9945921579280798, 0.994638824488213, 0.9946925890982151, 0.9945833301883936, 0.9945814677577896, 0.9946927327467165, 0.9945599300602802, 0.9945321723483698, 0.994624837466474, 0.9942847066610031, 0.9943527065831381, 0.9945644947483537, 0.9946774570382243, 0.9946732407461666, 0.9948294460833146, 0.994875913239689, 0.9950094983863084, 0.9950214897241482, 0.9951146348693803, 0.9952643478530305, 0.9952932071853746, 0.9953262394080136, 0.9952877331142711, 0.9953683715675499, 0.9954597697049126, 0.9953608515579506, 0.9952835899315673, 0.9953364074089988, 0.9953651196092753, 0.995414490001289, 0.9953648057070426, 0.9952283251363383, 0.9952396102697633, 0.9953132963016104, 0.9953325549067434, 0.9952345935337161, 0.9953017224156386, 0.9954091972328983, 0.9954376892743144, 0.9955268615233536, 0.995548293018077, 0.9956428754809752, 0.9955938820505247], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.02046666666666666, 0.052486666666666654, 0.09702466666666665, 0.14056219999999997, 0.1867193133333333, 0.23407404866666662, 0.28421331046666665, 0.33083197942, 0.375668781478, 0.4176352366635333, 0.4554450463305133, 0.490940541697462, 0.5222731541943824, 0.5513258387749441, 0.5776999215641163, 0.602623262741038, 0.6249342698002675, 0.6453075094869074, 0.66439009187155, 0.681551082684395, 0.6975159744159555, 0.7120043769743599, 0.7260572726102572, 0.7384648786825648, 0.7499517241476417, 0.7602232183995441, 0.769774229892923, 0.7788101402369639, 0.7869557928799342, 0.7945535469252741, 0.8010848588994134, 0.807496373009472, 0.8132400690418581, 0.8187027288043389, 0.8231391225905716, 0.8274252103315145, 0.8311626892983631, 0.8350597537018601, 0.8385271116650075, 0.8417544004985067, 0.8445122937819893, 0.8473943977371237, 0.8497482912967447, 0.8516667955004035, 0.8536734492836965, 0.8555994376886602, 0.8569061605864609, 0.8588022111944815, 0.8604953234083667, 0.8619791244008633, 0.8627678786274436, 0.8643710907646992, 0.8655739816882292, 0.8667365835194063, 0.8676495918341323, 0.8691512993173857, 0.8698895027189805, 0.8706738857804157, 0.8715798305357075, 0.8711418474821369, 0.8719343294005899, 0.8717808964605309, 0.8722028068144778, 0.87282252613303, 0.8735936068530603, 0.874020912834421, 0.8745121548843121, 0.8744342727292143, 0.8749775121229595, 0.8758264275773302, 0.8761104514862639, 0.8758327396709708, 0.8762894657038737, 0.8769005191334863, 0.876703800553471, 0.8766200871647905, 0.8766514117816447, 0.8768129372701469, 0.8771983102097988, 0.8771718125221522, 0.8768279646032703, 0.8770785014762765, 0.8775173179953155, 0.8778722528624506, 0.8788450275762055, 0.8792271914852516, 0.8793978056700599, 0.8800446917697204, 0.8801468892594151, 0.8804655336668069, 0.8805789803001263, 0.8802410822701137, 0.8806569740431023, 0.880591276638792, 0.8807721489749128, 0.8808016007440882, 0.8810014406696794, 0.8814479632693781, 0.8816098336091069, 0.8819555169148628, 0.8819466318900432, 0.8818719687010389, 0.8811781051642683, 0.8809269613145081, 0.8813142651830573, 0.8815961719980849, 0.8819432214649431, 0.8823222326517821, 0.8825033427199372, 0.8828396751146101, 0.8829690409364825, 0.882778803509501, 0.8831142564918842, 0.8824961641760292, 0.8825532144250929, 0.882871226315917, 0.883490770350992, 0.8839416933158928, 0.8842275239843035, 0.8843514382525398, 0.8845296277606192, 0.884743331651224, 0.8849223318194348, 0.885256765304158, 0.8857710887737422, 0.8856339798963679, 0.8859505819067311, 0.885915523716058, 0.8858973046777855, 0.8860142408766736, 0.8858128167890063, 0.886084868443439, 0.8865430482657618, 0.8863020767725189, 0.8865918690952671, 0.8868260155190737, 0.8859834139671663, 0.8861317392371163, 0.8863585653134047, 0.8858427087820643, 0.8861651045705246, 0.8861752607801388, 0.8865444013687915, 0.8863832945652457, 0.8863716317753878, 0.8865744685978489, 0.8869036884047308, 0.8870933195642576, 0.8875306542744985, 0.8875242555137153, 0.8876784966290104, 0.8874706469661093, 0.8874035822694984, 0.8876765573758819, 0.8878689016382937, 0.8880420114744643, 0.8878111436603512, 0.8876566959609827, 0.8879843596982179, 0.8879459237283961, 0.8881513313555565, 0.8878428648866674, 0.888031911731334, 0.8881753872248672, 0.8884645151690472, 0.8884713969854758, 0.8885709239535949, 0.8886871648915688, 0.8887651150690786, 0.888941936895504, 0.8892077432059536, 0.889033635552025, 0.8891169386634892, 0.8886852447971403, 0.8886433869840928, 0.8885257149523502, 0.8885664767904485, 0.8882564957780703, 0.8882841795335966, 0.888455761580237, 0.8885568520888799, 0.8884345002133253, 0.8883910501919927, 0.8884986118394601, 0.8888354173221807, 0.8893652089232961, 0.8897220213642998, 0.8898564858945365, 0.8894175039717495, 0.8894224202412412, 0.889280178217117, 0.889178827062072, 0.8891409443558648, 0.8893201832536116, 0.8893348315949171, 0.8893080151020921, 0.8894705469252163, 0.8896701588993613, 0.8900364763427584, 0.8902194953751492, 0.8902375458376344, 0.8902271245872042, 0.8902044121284838, 0.890130637582302, 0.8898375738240719, 0.8896671497749981, 0.8898871014641649, 0.8897650579844151, 0.8902152188526402, 0.8904203636340429, 0.8906316606039719, 0.8906084945435747, 0.8906943117558839, 0.8907182139136288, 0.8909797258555993, 0.8909084199367059, 0.8910309112763686, 0.8911011534820651, 0.8910177048005252, 0.8907559343204727, 0.8910136742217587, 0.8912589734662495, 0.8918397427862912, 0.8914424351743288, 0.8912581916568958, 0.8916123724912063, 0.8917178019087523, 0.8920526883845437, 0.8921007528794226, 0.8918506775914803, 0.8918122764989989, 0.8917777155157656, 0.8916932772975223, 0.8916172829011035, 0.8913888879443265, 0.8914099991498938, 0.8915623325682378, 0.891446099311414, 0.8916081560469393, 0.8908606737755788, 0.8907612730646876, 0.8910318124248855, 0.8908752978490636, 0.8909477680641573, 0.8909863245910749, 0.8909143587986341, 0.8911962562521041, 0.8913032972935604, 0.891799634230871, 0.8919663374744506, 0.8920763703936723, 0.891828733354305, 0.8916458600188745, 0.891841274016987, 0.8918171466152883, 0.8918487652870928, 0.8921705554250502, 0.8921001665492118, 0.8921301498942906, 0.8919171349048616, 0.891792088081042, 0.8916262126062712, 0.891823591345644, 0.8920812322110796, 0.8918864423233049, 0.8920177980909744, 0.8922826849485437, 0.8924144164536894, 0.8924129748083204, 0.8926516773274884, 0.8926931762614062, 0.8927571919685988, 0.8926148061050723], "moving_var_accuracy_train": [0.003941476119031141, 0.012804847163128026, 0.029226270494894943, 0.0431541622584566, 0.05784325125345394, 0.07275986390395485, 0.0888658228981156, 0.10084804910468398, 0.11052638067595186, 0.117410448188955, 0.12006062627271537, 0.12082516397174423, 0.11887399770136056, 0.11550931427305601, 0.11118212243269328, 0.10666219019491871, 0.1013643425521251, 0.09569154972853201, 0.09013182073655528, 0.08453659839758401, 0.07908887262616134, 0.0736588766423768, 0.06854244611871867, 0.06356979710953382, 0.058804891477215356, 0.05422770697959253, 0.05002183817984973, 0.04606380733541467, 0.04236292088955731, 0.03888319884257749, 0.035605605571941555, 0.03260119213440708, 0.029785281623222717, 0.027232536696650853, 0.024855479592259082, 0.0226615721980419, 0.020664154206903488, 0.018836526718011278, 0.01714180278625511, 0.015612359483153024, 0.014186934467107363, 0.01290568638249598, 0.011718981123855558, 0.010620082423211281, 0.009647992244390698, 0.00875833382926377, 0.007955357929721407, 0.007225294595249543, 0.006570585235490541, 0.005964758647266484, 0.005397641347422406, 0.004908589056501586, 0.004453052015306549, 0.004047906072630392, 0.0036784397066384742, 0.0033372786281722737, 0.00303177366746228, 0.002749735526823707, 0.0024967445268897787, 0.00225043296120538, 0.002041624596179527, 0.0018396748222828058, 0.0016678500648252032, 0.0015203604124221673, 0.0013826428091637602, 0.0012530618670136913, 0.0011387957745387586, 0.0010272547135893174, 0.0009337412097445467, 0.0008567826330954578, 0.0007744359898898436, 0.0007008355292965228, 0.0006390037408380247, 0.0005785303525069332, 0.0005213832674432697, 0.00047174788596192634, 0.0004277804981176368, 0.000387377873201966, 0.00035372670081839004, 0.00032109537193641746, 0.0002896574828721394, 0.00026379326656107146, 0.00024010838206419468, 0.00021921891340870905, 0.00020576550518908754, 0.00019356623605737324, 0.00017742501128208151, 0.0001665639494208125, 0.00015065483552451054, 0.00013912068914264015, 0.00012537970145004, 0.00011313153107845342, 0.00010358626773636505, 9.411602354847323e-05, 8.68845376426415e-05, 7.874811387729252e-05, 7.343475292766785e-05, 6.951311034526209e-05, 6.461212690294964e-05, 5.988525082816175e-05, 5.510791854676241e-05, 5.0527977837719934e-05, 4.79755848186356e-05, 4.3310752929859895e-05, 4.073233524505172e-05, 4.002208818319007e-05, 3.983795496068149e-05, 3.9200055023713725e-05, 3.7064071355311325e-05, 3.4590520947247734e-05, 3.323707717708136e-05, 2.9981625418167724e-05, 2.7791056238210607e-05, 2.522520288081159e-05, 2.296185794731621e-05, 2.240638264713423e-05, 2.0562363242712267e-05, 1.916849172008611e-05, 1.760127315989577e-05, 1.6195999028713095e-05, 1.5281420323532654e-05, 1.4875223764738396e-05, 1.4450547758094403e-05, 1.3413096319263956e-05, 1.3139739265394144e-05, 1.1885971892174967e-05, 1.172034805005957e-05, 1.1580897757936958e-05, 1.0713946093544221e-05, 9.95513416568441e-06, 9.599064767660943e-06, 9.167318062682072e-06, 9.736004355517396e-06, 8.762977924446545e-06, 8.167365889280235e-06, 8.816994589657308e-06, 1.0795906957492055e-05, 1.17768219368622e-05, 1.1377927951997479e-05, 1.0326021858252015e-05, 9.469024263324111e-06, 8.805615181719901e-06, 8.845112236937786e-06, 8.153979089957153e-06, 7.351445283428358e-06, 7.099352444206751e-06, 6.6973634449851e-06, 6.905752023266881e-06, 6.3182319058557836e-06, 5.7408997725649015e-06, 5.899190557223325e-06, 5.309448822241282e-06, 5.289835135145342e-06, 5.504568758163179e-06, 5.115742961214999e-06, 4.620781074066468e-06, 4.4497951912726796e-06, 4.397635170721923e-06, 3.962766080294913e-06, 4.295041165649744e-06, 3.909868392822307e-06, 3.51961824968161e-06, 3.902738213323923e-06, 3.512550496596132e-06, 3.3997824694409864e-06, 3.733340678629941e-06, 3.3689024534137006e-06, 3.032799301148556e-06, 2.8198459340496133e-06, 2.6447641751927477e-06, 2.4753869919301192e-06, 2.3318265041435245e-06, 2.1082526407187815e-06, 2.104732235581996e-06, 2.039276444987982e-06, 1.8359406731337127e-06, 1.723911509786378e-06, 1.5829326557389e-06, 1.4457788008133005e-06, 1.3243143628387644e-06, 1.2146675000528157e-06, 1.095816576996574e-06, 1.064060193741909e-06, 1.0012189198070614e-06, 9.222662835472155e-07, 8.76545788457259e-07, 8.212803671190529e-07, 1.1978429320871e-06, 1.1773879779887609e-06, 1.067833753267447e-06, 1.0204764864411642e-06, 1.0242356498476622e-06, 1.0980529030365415e-06, 1.280044584768895e-06, 1.1533497573847363e-06, 1.3844724260002856e-06, 1.2824765253901164e-06, 1.4007768654464031e-06, 1.345722901765844e-06, 1.2565271452631188e-06, 1.130909609197318e-06, 1.047105001126945e-06, 9.447116670100252e-07, 8.869351781940361e-07, 8.389305824417257e-07, 8.572027874061041e-07, 9.154529861349426e-07, 8.90734602964834e-07, 8.088842621323649e-07, 7.305530374253726e-07, 8.225390743772327e-07, 7.639589335938846e-07, 6.929341960964071e-07, 6.869874777495982e-07, 7.58729462397114e-07, 6.828589023265142e-07, 6.83514295216558e-07, 7.855507202952739e-07, 7.24288575141519e-07, 7.325160725676824e-07, 7.480407556072681e-07, 8.466201830348935e-07, 8.502307601969808e-07, 8.127249589839649e-07, 7.409319365920476e-07, 7.293283784685926e-07, 6.571427037236056e-07, 9.993897594122526e-07, 9.620211884560828e-07, 8.690939130560593e-07, 7.996153306268416e-07, 7.392537080761496e-07, 6.913440368667732e-07, 7.296472175585568e-07, 6.567137136324959e-07, 7.024614221174257e-07, 7.90944261628118e-07, 7.187842505997961e-07, 7.241872425591938e-07, 1.692969201776005e-06, 1.5652881862916809e-06, 1.8124474099910168e-06, 1.7460469793870125e-06, 1.5716022755167555e-06, 1.6340430141469182e-06, 1.4900714823259468e-06, 1.5016692566691258e-06, 1.3527964606508972e-06, 1.2956009773088343e-06, 1.3677666768389485e-06, 1.2384857587251302e-06, 1.124457332444875e-06, 1.0253562111204676e-06, 9.813436313331516e-07, 9.583918438201127e-07, 9.506158576233799e-07, 9.092785020635675e-07, 8.43457825157266e-07, 7.665315566440489e-07, 7.118153214478732e-07, 6.628505511560845e-07, 7.642080116583108e-07, 6.889333986202603e-07, 6.689067403627177e-07, 6.053541111714918e-07, 6.311865755028651e-07, 6.086244990460715e-07, 6.517195762464508e-07, 5.938537864382903e-07, 6.060336177827612e-07, 5.495640366992237e-07, 5.751202136202813e-07, 5.392113983039888e-07], "duration": 209475.448995, "accuracy_train": [0.20927058823529412, 0.3416470588235294, 0.4964941176470588, 0.5300470588235294, 0.6001411764705883, 0.666164705882353, 0.7442352941176471, 0.767035294117647, 0.8022588235294118, 0.8269411764705883, 0.8250352941176471, 0.841835294117647, 0.8383294117647059, 0.8440941176470588, 0.8504470588235294, 0.8662352941176471, 0.8667764705882353, 0.8696705882352941, 0.8803058823529412, 0.8852235294117647, 0.8925882352941177, 0.8940705882352942, 0.9028, 0.9051058823529412, 0.9079764705882353, 0.9086117647058823, 0.9165882352941176, 0.9196470588235294, 0.9230117647058823, 0.9244235294117648, 0.9242823529411764, 0.9287529411764706, 0.9282588235294118, 0.9338117647058823, 0.9339294117647059, 0.9350352941176471, 0.9384470588235294, 0.9407764705882353, 0.9402352941176471, 0.9443058823529412, 0.9423764705882353, 0.9464941176470588, 0.9452941176470588, 0.9432, 0.9491764705882353, 0.9496235294117648, 0.9520705882352941, 0.953435294117647, 0.9566117647058824, 0.955764705882353, 0.9523529411764706, 0.959835294117647, 0.9582823529411765, 0.9615764705882353, 0.9623764705882353, 0.961764705882353, 0.9639764705882353, 0.9633647058823529, 0.9652, 0.9572470588235295, 0.9651764705882353, 0.9580470588235294, 0.9652, 0.9693882352941177, 0.9688235294117648, 0.9672941176470589, 0.9695294117647059, 0.9646588235294118, 0.9701882352941177, 0.9745882352941176, 0.9685176470588235, 0.9695764705882353, 0.9732705882352941, 0.9708235294117648, 0.9680705882352941, 0.9708235294117648, 0.9720470588235294, 0.9718117647058824, 0.9747058823529412, 0.9734588235294117, 0.9712235294117647, 0.974635294117647, 0.9748235294117648, 0.9757882352941176, 0.9801882352941177, 0.9811058823529412, 0.9784, 0.981764705882353, 0.9767764705882352, 0.9804470588235294, 0.9761882352941177, 0.9731529411764706, 0.9792, 0.9783529411764705, 0.9804470588235294, 0.9784941176470588, 0.9816, 0.982964705882353, 0.9821882352941177, 0.9822823529411765, 0.982, 0.9754823529411765, 0.9731058823529412, 0.976635294117647, 0.9821411764705882, 0.9842823529411765, 0.9852941176470589, 0.9855294117647059, 0.9844941176470589, 0.9841882352941177, 0.9856941176470588, 0.9822117647058823, 0.9844235294117647, 0.9801882352941177, 0.9832705882352941, 0.9861411764705882, 0.9842823529411765, 0.9851058823529412, 0.984635294117647, 0.9848470588235294, 0.9858588235294118, 0.9868705882352942, 0.9871294117647059, 0.9861647058823529, 0.9876941176470588, 0.9854117647058823, 0.9880470588235294, 0.9884, 0.9835529411764706, 0.987035294117647, 0.9880235294117647, 0.9880470588235294, 0.9899294117647058, 0.9863529411764705, 0.9880470588235294, 0.9904941176470589, 0.9812235294117647, 0.9910823529411765, 0.9897176470588235, 0.9880470588235294, 0.9885647058823529, 0.9890823529411765, 0.9906823529411765, 0.9892705882352941, 0.9883294117647059, 0.9903058823529411, 0.9900705882352941, 0.9915294117647059, 0.9897882352941176, 0.9880470588235294, 0.9916, 0.9889882352941176, 0.9914117647058823, 0.9921411764705882, 0.9908941176470588, 0.9901176470588235, 0.9915294117647059, 0.992, 0.9903529411764705, 0.9929882352941176, 0.9911294117647059, 0.9905882352941177, 0.9933647058823529, 0.9908235294117647, 0.9924235294117647, 0.9936941176470588, 0.9909176470588236, 0.9912941176470588, 0.9922117647058823, 0.9924, 0.9924470588235295, 0.9904470588235295, 0.9917411764705882, 0.9899294117647058, 0.9925647058823529, 0.9913411764705883, 0.9923058823529411, 0.9920941176470588, 0.9920470588235294, 0.9921176470588235, 0.992164705882353, 0.9918823529411764, 0.9926588235294118, 0.9925176470588235, 0.9923764705882353, 0.9926588235294118, 0.9926117647058823, 0.9943294117647059, 0.9912470588235294, 0.9924941176470589, 0.993035294117647, 0.9933882352941177, 0.9938117647058824, 0.9943529411764706, 0.9926117647058823, 0.9946823529411765, 0.9935529411764706, 0.9946352941176471, 0.9941176470588236, 0.9939529411764706, 0.9932941176470588, 0.9938823529411764, 0.9935294117647059, 0.9940235294117648, 0.9927764705882353, 0.9944470588235295, 0.9947529411764706, 0.9927529411764706, 0.9938117647058824, 0.9933882352941177, 0.9948941176470588, 0.9941882352941176, 0.9934823529411765, 0.9945411764705883, 0.995035294117647, 0.9939058823529412, 0.993035294117647, 0.9924470588235295, 0.9932470588235294, 0.9945882352941177, 0.9947294117647059, 0.9952235294117647, 0.994964705882353, 0.9948, 0.9944705882352941, 0.9950117647058824, 0.9943529411764706, 0.9964, 0.9953176470588235, 0.9943764705882353, 0.9949882352941176, 0.9950588235294118, 0.9951764705882353, 0.9936, 0.9945647058823529, 0.9956941176470588, 0.9933647058823529, 0.9942823529411765, 0.9954588235294117, 0.9912235294117647, 0.994964705882353, 0.9964705882352941, 0.9956941176470588, 0.9946352941176471, 0.9962352941176471, 0.9952941176470588, 0.9962117647058824, 0.9951294117647059, 0.9959529411764706, 0.9966117647058823, 0.9955529411764706, 0.9956235294117647, 0.9949411764705882, 0.9960941176470588, 0.9962823529411765, 0.9944705882352941, 0.9945882352941177, 0.9958117647058824, 0.9956235294117647, 0.9958588235294118, 0.9949176470588236, 0.994, 0.9953411764705883, 0.9959764705882352, 0.9955058823529411, 0.9943529411764706, 0.9959058823529412, 0.9963764705882353, 0.9956941176470588, 0.9963294117647059, 0.9957411764705882, 0.9964941176470589, 0.9951529411764706], "end": "2016-02-08 11:37:13.493000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0, 211.0, 212.0, 213.0, 214.0, 215.0, 216.0, 217.0, 218.0, 219.0, 220.0, 221.0, 222.0, 223.0, 224.0, 225.0, 226.0, 227.0, 228.0, 229.0, 230.0, 231.0, 232.0, 233.0, 234.0, 235.0, 236.0, 237.0, 238.0, 239.0, 240.0, 241.0, 242.0, 243.0, 244.0, 245.0, 246.0, 247.0, 248.0, 249.0, 250.0, 251.0, 252.0, 253.0, 254.0, 255.0, 256.0, 257.0, 258.0, 259.0, 260.0, 261.0, 262.0, 263.0, 264.0, 265.0, 266.0, 267.0, 268.0, 269.0, 270.0, 271.0, 272.0], "moving_var_accuracy_valid": [0.003769959999999999, 0.012620487599999997, 0.029211139835999995, 0.04334967713115999, 0.05818902141943959, 0.07255235790390607, 0.08792263227815593, 0.0986900717051647, 0.1069141139037427, 0.11207335276092283, 0.11373225284830982, 0.1136983992855845, 0.11116415280996565, 0.10764426386100759, 0.10314016766163785, 0.09841670731426523, 0.09305506590681162, 0.08748517937409397, 0.08201396599088462, 0.07646306584290312, 0.07111065917061685, 0.0658888175318046, 0.061077290660404945, 0.056355099790374535, 0.051907118379984095, 0.04766593888948748, 0.04372034138545864, 0.040083136328622704, 0.036671987608579794, 0.03352432164652313, 0.03055581180680263, 0.027870198244773697, 0.025380088817308362, 0.023110645800703173, 0.020976715529071578, 0.019044378909271964, 0.017265659759593488, 0.01567577778231917, 0.014216403145288662, 0.012888501369693257, 0.01166810501099164, 0.010576053218766288, 0.009568315230899885, 0.008644609633225008, 0.007816388604556544, 0.007068134626125381, 0.006376688886097571, 0.005771375068661391, 0.0052200372225145175, 0.004717848488731073, 0.004251662838927501, 0.0038496291574481455, 0.00347768876086853, 0.003142084671942384, 0.0028353784623929754, 0.0025721367444410128, 0.0023198275683560463, 0.002093382122604041, 0.001891430533440403, 0.0017040139424932992, 0.0015392647965636434, 0.0013855501919111354, 0.0012485972478409302, 0.0011271939913609188, 0.0010198256815160494, 0.0009194864269798037, 0.0008297096530460555, 0.0007467932784121942, 0.0006747699319212251, 0.0006137788557671275, 0.0005531269964180283, 0.0004985084114474058, 0.00045053495832484645, 0.000408841939136934, 0.00036830602902074966, 0.00033153849750167395, 0.0002983934788360962, 0.00026878894530340945, 0.00024324666149661264, 0.00021892831449400692, 0.00019809956556648083, 0.00017885452753245475, 0.0001627021142156429, 0.00014756571163325263, 0.0001413257562634163, 0.00012850762391747135, 0.00011591884432624402, 0.00010809311452702757, 9.737780201642376e-05, 8.855383014004014e-05, 7.98142783735395e-05, 7.28604262443633e-05, 6.713107732148361e-05, 6.0456814929733163e-05, 5.470556665452399e-05, 4.9242816649439635e-05, 4.4677958947238266e-05, 4.2004604940889766e-05, 3.803996250875621e-05, 3.531143878878601e-05, 3.178100540290181e-05, 2.8653076188742288e-05, 3.0120788038805736e-05, 2.7676368334376333e-05, 2.625877008027722e-05, 2.4348136143480544e-05, 2.299731252115138e-05, 2.1990426586778405e-05, 2.0086591639184937e-05, 1.9096007792624393e-05, 1.7337026656180247e-05, 1.5929036498183405e-05, 1.534889117887331e-05, 1.725234505925796e-05, 1.555640313159629e-05, 1.4910946882786016e-05, 1.6874365497080163e-05, 1.701691262984652e-05, 1.605051390589897e-05, 1.4583655228161942e-05, 1.34110532124518e-05, 1.2480972066943087e-05, 1.1521244402224669e-05, 1.1375731763339038e-05, 1.2618916269291268e-05, 1.152621424065552e-05, 1.12757243132842e-05, 1.0159213572555227e-05, 9.146279615499864e-06, 8.354718325443495e-06, 7.884391460732795e-06, 7.762061238775633e-06, 8.875213861151582e-06, 8.510297820037608e-06, 8.41508435094751e-06, 8.066996845885651e-06, 1.3650093538787489e-05, 1.2483087656260497e-05, 1.1697829510593816e-05, 1.2923018207874083e-05, 1.2566167786838757e-05, 1.1310479345498431e-05, 1.1405814378666975e-05, 1.0498831560139029e-05, 9.450172590130553e-06, 8.87544032003298e-06, 8.963367419219124e-06, 8.390670467268963e-06, 9.272958259575502e-06, 8.346030930873996e-06, 7.72554073261399e-06, 7.341800000665255e-06, 6.648099062382378e-06, 6.653927834489837e-06, 6.3215018885855255e-06, 5.959054838138171e-06, 5.84284888266456e-06, 5.473250820959979e-06, 5.892197461153587e-06, 5.3162736290235045e-06, 5.164376905782123e-06, 5.504303277063411e-06, 5.2755213346626375e-06, 4.9332361563979e-06, 5.192267253709586e-06, 4.67346676291484e-06, 4.29527064307014e-06, 3.987351179712469e-06, 3.6433021333054756e-06, 3.560365544678875e-06, 3.840205942284402e-06, 3.729006624464975e-06, 3.418560637434986e-06, 4.753940921880967e-06, 4.294315518310839e-06, 3.989504329969685e-06, 3.605507643979094e-06, 4.109750931896248e-06, 3.7056733515869815e-06, 3.6000696049918683e-06, 3.3320362629319248e-06, 3.133562469704422e-06, 2.837197361918151e-06, 2.65760319777943e-06, 3.4127842767177595e-06, 5.597618114556378e-06, 6.1836923655960985e-06, 5.728049518062449e-06, 6.889590723060016e-06, 6.200849178105452e-06, 5.7628594011372345e-06, 5.279021970684178e-06, 4.764035668481985e-06, 4.5767713438233676e-06, 4.121025374568053e-06, 3.71539495569817e-06, 3.5816048018808026e-06, 3.5820487836914214e-06, 4.431540129355874e-06, 4.289849812375769e-06, 3.863797203901517e-06, 3.478394905656107e-06, 3.1351981171206373e-06, 2.870662458387434e-06, 3.3565735100408236e-06, 3.2823153675613015e-06, 3.3894925409113773e-06, 3.1845947853652423e-06, 4.689938572359779e-06, 4.599704147154447e-06, 4.541551417949751e-06, 4.092226273343714e-06, 3.749284991366034e-06, 3.3794983105332102e-06, 3.6570449416182927e-06, 3.337101254079366e-06, 3.1384282833028012e-06, 2.8689911621224787e-06, 2.644765187966874e-06, 2.997002727212562e-06, 3.2951711649258517e-06, 3.5071995225630514e-06, 6.19211659822224e-06, 6.9935849851097605e-06, 6.5997375500431315e-06, 7.068760365574251e-06, 6.461922587773714e-06, 6.825070894008461e-06, 6.163355565619345e-06, 6.109858855812059e-06, 5.512144765364712e-06, 4.971680442886662e-06, 4.538680712898805e-06, 4.136788976192587e-06, 4.192588385103921e-06, 3.77734069359809e-06, 3.6084558573374614e-06, 3.3692018015304916e-06, 3.2686430911393836e-06, 7.970346496011084e-06, 7.262236358341071e-06, 7.194736631253595e-06, 6.695734280130582e-06, 6.073428240798977e-06, 5.479464868630609e-06, 4.978130059302271e-06, 5.195512621827496e-06, 4.779081420649102e-06, 6.518326476634209e-06, 6.116603571750291e-06, 5.61390840438711e-06, 5.604434493347508e-06, 5.344974955316034e-06, 5.154157135709415e-06, 4.64398060575305e-06, 4.188580208837873e-06, 4.7016622239338006e-06, 4.2760873461165035e-06, 3.856569620343893e-06, 3.879291129802674e-06, 3.6320923901484743e-06, 3.5165152093077844e-06, 3.5154889891848453e-06, 3.7613494301479824e-06, 3.7267023905464432e-06, 3.509321190792215e-06, 3.7898744975291123e-06, 3.567065752807586e-06, 3.2103778825991558e-06, 3.402150128253425e-06, 3.077434569074929e-06, 2.806573209073888e-06, 2.708379495356313e-06], "accuracy_test": 0.8458, "start": "2016-02-06 01:25:58.044000", "learning_rate_per_epoch": [0.0045228987000882626, 0.0031981724314391613, 0.0026112967170774937, 0.0022614493500441313, 0.002022701781243086, 0.0018464657478034496, 0.0017094950890168548, 0.0015990862157195807, 0.0015076329000294209, 0.0014302661875262856, 0.0013637052616104484, 0.0013056483585387468, 0.0012544264318421483, 0.001208795583806932, 0.0011678074952214956, 0.0011307246750220656, 0.0010969641152769327, 0.001066057477146387, 0.0010376240825280547, 0.001011350890621543, 0.000986977480351925, 0.0009642852819524705, 0.0009430896025151014, 0.0009232328739017248, 0.0009045797633007169, 0.0008870134479366243, 0.0008704322972334921, 0.0008547475445084274, 0.000839881191495806, 0.00082576455315575, 0.0008123366278596222, 0.0007995431078597903, 0.0007873356225900352, 0.0007756707491353154, 0.000764509430155158, 0.0007538164500147104, 0.0007435599691234529, 0.0007337110582739115, 0.0007242434076033533, 0.0007151330937631428, 0.0007063581142574549, 0.0006978984456509352, 0.0006897355779074132, 0.0006818526308052242, 0.0006742339464835823, 0.00066686503123492, 0.0006597326137125492, 0.0006528241792693734, 0.0006461283774115145, 0.0006396344979293644, 0.0006333325291052461, 0.0006272132159210742, 0.0006212679436430335, 0.0006154885631985962, 0.0006098675657995045, 0.000604397791903466, 0.0005990725476294756, 0.0005938857211731374, 0.0005888312589377165, 0.0005839037476107478, 0.0005790978320874274, 0.0005744087393395603, 0.0005698316963389516, 0.0005653623375110328, 0.0005609965301118791, 0.0005567303742282093, 0.000552560028154403, 0.0005484820576384664, 0.0005444930284284055, 0.0005405897973105311, 0.0005367693374864757, 0.0005330287385731936, 0.0005293652648106217, 0.0005257762968540192, 0.0005222593899816275, 0.0005188120412640274, 0.000515432155225426, 0.0005121174617670476, 0.0005088658654130995, 0.0005056754453107715, 0.0005025442806072533, 0.0004994706250727177, 0.0004964526160620153, 0.0004934887401759624, 0.0004905772511847317, 0.00048771672300063074, 0.00048490564222447574, 0.00048214264097623527, 0.0004794263222720474, 0.00047675540554337204, 0.0004741286102216691, 0.0004715448012575507, 0.00046900275629013777, 0.00046650139847770333, 0.00046403962187469006, 0.0004616164369508624, 0.00045923079596832395, 0.00045688176760450006, 0.0004545684205368161, 0.00045228988165035844, 0.00045004524872638285, 0.00044783370685763657, 0.00044565447024069726, 0.00044350672396831214, 0.00044138971134088933, 0.00043930276297032833, 0.00043724512215703726, 0.00043521614861674607, 0.0004332151438575238, 0.0004312414675951004, 0.0004292945668566972, 0.0004273737722542137, 0.0004254785308148712, 0.00042360828956589103, 0.0004217624955344945, 0.000419940595747903, 0.00041814212454482913, 0.0004163665871601552, 0.0004146134597249329, 0.000412882276577875, 0.00041117260116152465, 0.0004094840260222554, 0.00040781605639494956, 0.0004061683139298111, 0.00040454036206938326, 0.0004029318515677005, 0.00040134237497113645, 0.00039977155392989516, 0.00039821903919801116, 0.0003966844524256885, 0.00039516750257462263, 0.0003936678112950176, 0.0003921850584447384, 0.00039071895298548043, 0.00038926914567127824, 0.0003878353745676577, 0.0003864173195324838, 0.00038501471863128245, 0.00038362728082574904, 0.000382254715077579, 0.00038089678855612874, 0.0003795532393269241, 0.00037822380545549095, 0.0003769082250073552, 0.0003756062942557037, 0.00037431775126606226, 0.00037304239231161773, 0.00037177998456172645, 0.00037053029518574476, 0.0003692931495606899, 0.00036806828575208783, 0.00036685552913695574, 0.0003656547050923109, 0.0003644655807875097, 0.0003632879815995693, 0.00036212170380167663, 0.0003609666309785098, 0.00035982250119559467, 0.00035868919803760946, 0.0003575665468815714, 0.0003564543731044978, 0.00035535250208340585, 0.00035426075919531286, 0.00035317905712872744, 0.0003521071921568364, 0.00035104501876048744, 0.00034999242052435875, 0.0003489492228254676, 0.00034791528014466166, 0.0003468905051704496, 0.0003458747232798487, 0.0003448677889537066, 0.000343869614880532, 0.0003428800846450031, 0.00034189902362413704, 0.0003409263154026121, 0.0003399619017727673, 0.00033900560811161995, 0.0003380573180038482, 0.00033711697324179113, 0.00033618442830629647, 0.0003352595667820424, 0.0003343423013575375, 0.00033343251561746, 0.00033253012225031853, 0.0003316350339446217, 0.0003307471051812172, 0.0003298663068562746, 0.0003289924643468112, 0.0003281255776528269, 0.00032726547215133905, 0.0003264120896346867, 0.00032556537189520895, 0.00032472520251758397, 0.00032389149419032037, 0.00032306418870575726, 0.00032224319875240326, 0.00032142840791493654, 0.00032061978708952665, 0.0003198172489646822, 0.0003190206771250814, 0.0003182300424668938, 0.00031744525767862797, 0.00031666626455262303, 0.00031589294667355716, 0.0003151252749375999, 0.0003143631911370903, 0.0003136066079605371, 0.0003128554380964488, 0.000312109652440995, 0.00031136919278651476, 0.00031063397182151675, 0.00030990393133834004, 0.00030917898402549326, 0.0003084591298829764, 0.0003077442815992981, 0.00030703438096679747, 0.00030632936977781355, 0.00030562918982468545, 0.00030493378289975226, 0.00030424309079535306, 0.0003035570844076574, 0.00030287570552900434, 0.000302198895951733, 0.00030152659746818244, 0.0003008587518706918, 0.00030019533005543053, 0.0002995362738147378, 0.0002988815540447831, 0.0002982311125379056, 0.00029758489108644426, 0.0002969428605865687, 0.000296304962830618, 0.0002956711396109313, 0.0002950413909275085, 0.00029441562946885824, 0.0002937938552349806, 0.0002931759809143841, 0.00029256200650706887, 0.0002919518738053739, 0.0002913455246016383, 0.0002907429588958621, 0.0002901440893765539, 0.0002895489160437137, 0.00028895740979351103, 0.00028836948331445456, 0.0002877851657103747, 0.00028720436966978014, 0.00028662706608884037, 0.00028605322586372495, 0.0002854828489944339, 0.0002849158481694758, 0.0002843522233888507, 0.00028379191644489765, 0.0002832348982337862, 0.0002826811687555164, 0.0002821306698024273, 0.0002815833722706884, 0.0002810392470564693, 0.00028049826505593956, 0.00027996039716526866, 0.0002794256142806262, 0.00027889388729818165, 0.00027836518711410463, 0.0002778394555207342, 0.0002773167216219008, 0.0002767968981061131, 0.0002762800140772015, 0.0002757660113275051, 0.0002752548607531935, 0.0002747465332504362, 0.0002742410288192332, 0.00027373828925192356], "accuracy_train_first": 0.20927058823529412, "accuracy_train_last": 0.9951529411764706, "batch_size_eval": 1024, "accuracy_train_std": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "accuracy_test_std": 0, "error_valid": [0.7953333333333333, 0.6593333333333333, 0.5021333333333333, 0.4676, 0.3978666666666667, 0.33973333333333333, 0.2645333333333333, 0.24960000000000004, 0.2208, 0.20466666666666666, 0.2042666666666667, 0.1896, 0.19573333333333331, 0.18720000000000003, 0.18493333333333328, 0.1730666666666667, 0.17426666666666668, 0.17133333333333334, 0.16386666666666672, 0.16400000000000003, 0.15880000000000005, 0.15759999999999996, 0.14746666666666663, 0.1498666666666667, 0.1466666666666666, 0.14733333333333332, 0.14426666666666665, 0.1398666666666667, 0.13973333333333338, 0.13706666666666667, 0.14013333333333333, 0.13480000000000003, 0.13506666666666667, 0.13213333333333332, 0.13693333333333335, 0.134, 0.1352, 0.12986666666666669, 0.13026666666666664, 0.12919999999999998, 0.1306666666666667, 0.1266666666666667, 0.12906666666666666, 0.13106666666666666, 0.12826666666666664, 0.12706666666666666, 0.1313333333333333, 0.12413333333333332, 0.12426666666666664, 0.1246666666666667, 0.13013333333333332, 0.12119999999999997, 0.12360000000000004, 0.12280000000000002, 0.12413333333333332, 0.11733333333333329, 0.12346666666666661, 0.12226666666666663, 0.12026666666666663, 0.13280000000000003, 0.12093333333333334, 0.12960000000000005, 0.124, 0.12160000000000004, 0.11946666666666672, 0.12213333333333332, 0.12106666666666666, 0.12626666666666664, 0.12013333333333331, 0.11653333333333338, 0.1213333333333333, 0.1266666666666667, 0.11960000000000004, 0.11760000000000004, 0.12506666666666666, 0.12413333333333332, 0.12306666666666666, 0.12173333333333336, 0.11933333333333329, 0.12306666666666666, 0.12626666666666664, 0.1206666666666667, 0.11853333333333338, 0.11893333333333334, 0.11240000000000006, 0.11733333333333329, 0.11906666666666665, 0.11413333333333331, 0.11893333333333334, 0.1166666666666667, 0.11839999999999995, 0.12280000000000002, 0.11560000000000004, 0.12, 0.11760000000000004, 0.11893333333333334, 0.11719999999999997, 0.11453333333333338, 0.11693333333333333, 0.11493333333333333, 0.11813333333333331, 0.11880000000000002, 0.12506666666666666, 0.1213333333333333, 0.11519999999999997, 0.11586666666666667, 0.11493333333333333, 0.11426666666666663, 0.11586666666666667, 0.11413333333333331, 0.11586666666666667, 0.11893333333333334, 0.11386666666666667, 0.12306666666666666, 0.11693333333333333, 0.11426666666666663, 0.11093333333333333, 0.11199999999999999, 0.11319999999999997, 0.11453333333333338, 0.11386666666666667, 0.11333333333333329, 0.11346666666666672, 0.11173333333333335, 0.10960000000000003, 0.11560000000000004, 0.11119999999999997, 0.11439999999999995, 0.11426666666666663, 0.11293333333333333, 0.11599999999999999, 0.11146666666666671, 0.10933333333333328, 0.11586666666666667, 0.11080000000000001, 0.11106666666666665, 0.12160000000000004, 0.11253333333333337, 0.11160000000000003, 0.11880000000000002, 0.11093333333333333, 0.11373333333333335, 0.1101333333333333, 0.11506666666666665, 0.11373333333333335, 0.11160000000000003, 0.1101333333333333, 0.11119999999999997, 0.10853333333333337, 0.11253333333333337, 0.11093333333333333, 0.11439999999999995, 0.11319999999999997, 0.10986666666666667, 0.11040000000000005, 0.11040000000000005, 0.11426666666666663, 0.11373333333333335, 0.10906666666666665, 0.11240000000000006, 0.10999999999999999, 0.11493333333333333, 0.11026666666666662, 0.11053333333333337, 0.10893333333333333, 0.11146666666666671, 0.11053333333333337, 0.11026666666666662, 0.11053333333333337, 0.10946666666666671, 0.10840000000000005, 0.11253333333333337, 0.1101333333333333, 0.11519999999999997, 0.11173333333333335, 0.11253333333333337, 0.11106666666666665, 0.11453333333333338, 0.11146666666666671, 0.10999999999999999, 0.11053333333333337, 0.11266666666666669, 0.11199999999999999, 0.11053333333333337, 0.1081333333333333, 0.10586666666666666, 0.10706666666666664, 0.10893333333333333, 0.11453333333333338, 0.11053333333333337, 0.11199999999999999, 0.11173333333333335, 0.11119999999999997, 0.10906666666666665, 0.11053333333333337, 0.11093333333333333, 0.10906666666666665, 0.10853333333333337, 0.10666666666666669, 0.1081333333333333, 0.10960000000000003, 0.10986666666666667, 0.10999999999999999, 0.11053333333333337, 0.11280000000000001, 0.11186666666666667, 0.1081333333333333, 0.11133333333333328, 0.10573333333333335, 0.10773333333333335, 0.10746666666666671, 0.10960000000000003, 0.10853333333333337, 0.10906666666666665, 0.10666666666666669, 0.10973333333333335, 0.10786666666666667, 0.10826666666666662, 0.10973333333333335, 0.11160000000000003, 0.10666666666666669, 0.10653333333333337, 0.10293333333333332, 0.11213333333333331, 0.11040000000000005, 0.10519999999999996, 0.10733333333333328, 0.10493333333333332, 0.10746666666666671, 0.11040000000000005, 0.10853333333333337, 0.10853333333333337, 0.10906666666666665, 0.10906666666666665, 0.11066666666666669, 0.10840000000000005, 0.10706666666666664, 0.10960000000000003, 0.10693333333333332, 0.11586666666666667, 0.1101333333333333, 0.10653333333333337, 0.11053333333333337, 0.10840000000000005, 0.10866666666666669, 0.10973333333333335, 0.10626666666666662, 0.10773333333333335, 0.10373333333333334, 0.10653333333333337, 0.10693333333333332, 0.11040000000000005, 0.10999999999999999, 0.10640000000000005, 0.10840000000000005, 0.10786666666666667, 0.10493333333333332, 0.10853333333333337, 0.10760000000000003, 0.10999999999999999, 0.10933333333333328, 0.10986666666666667, 0.10640000000000005, 0.10560000000000003, 0.10986666666666667, 0.1068, 0.10533333333333328, 0.10640000000000005, 0.10760000000000003, 0.10519999999999996, 0.10693333333333332, 0.10666666666666669, 0.10866666666666669], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.09505793382167371, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 64, "valid_ratio": 0.15, "learning_rate": 0.004522898768379161, "optimization": "rmsprop", "nb_data_augmentation": 4, "learning_rate_decay_method": "sqrt", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 3.951654832952621e-06, "rotation_range": [0, 0], "momentum": 0.9083186705949815}, "accuracy_valid_max": 0.8970666666666667, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        y_pred = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            y_pred.extend((nnet.predict(X[mini_batch]) == y[mini_batch]).tolist())\n        return np.mean(y_pred)\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            acc = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = acc\n            status[\"accuracy_train_std\"] = 0\n            acc = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = acc\n            status[\"accuracy_valid_std\"] = 0\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n\n    # rescaling to [-1, 1]\n    X_min = X_train.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X_train.max(axis=(0, 2, 3))[None, :, None, None]\n    def preprocess(a):\n        return (a / 255.) * 2 - 1\n        # return 2 * ((a - X_min) / (X_max - X_min)) - 1\n    X_train = preprocess(X_train)\n    X_valid = preprocess(X_valid)\n\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = preprocess(X_test)\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    acc = evaluate(X_test, y_test, batch_size_eval)\n    light.set(\"accuracy_test\", acc)\n    light.set(\"accuracy_test_std\", 0)\n    print(\"Test accuracy : {}+-{}\".format(acc, 0))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8913333333333333, "accuracy_valid_std": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "accuracy_valid": [0.20466666666666666, 0.3406666666666667, 0.4978666666666667, 0.5324, 0.6021333333333333, 0.6602666666666667, 0.7354666666666667, 0.7504, 0.7792, 0.7953333333333333, 0.7957333333333333, 0.8104, 0.8042666666666667, 0.8128, 0.8150666666666667, 0.8269333333333333, 0.8257333333333333, 0.8286666666666667, 0.8361333333333333, 0.836, 0.8412, 0.8424, 0.8525333333333334, 0.8501333333333333, 0.8533333333333334, 0.8526666666666667, 0.8557333333333333, 0.8601333333333333, 0.8602666666666666, 0.8629333333333333, 0.8598666666666667, 0.8652, 0.8649333333333333, 0.8678666666666667, 0.8630666666666666, 0.866, 0.8648, 0.8701333333333333, 0.8697333333333334, 0.8708, 0.8693333333333333, 0.8733333333333333, 0.8709333333333333, 0.8689333333333333, 0.8717333333333334, 0.8729333333333333, 0.8686666666666667, 0.8758666666666667, 0.8757333333333334, 0.8753333333333333, 0.8698666666666667, 0.8788, 0.8764, 0.8772, 0.8758666666666667, 0.8826666666666667, 0.8765333333333334, 0.8777333333333334, 0.8797333333333334, 0.8672, 0.8790666666666667, 0.8704, 0.876, 0.8784, 0.8805333333333333, 0.8778666666666667, 0.8789333333333333, 0.8737333333333334, 0.8798666666666667, 0.8834666666666666, 0.8786666666666667, 0.8733333333333333, 0.8804, 0.8824, 0.8749333333333333, 0.8758666666666667, 0.8769333333333333, 0.8782666666666666, 0.8806666666666667, 0.8769333333333333, 0.8737333333333334, 0.8793333333333333, 0.8814666666666666, 0.8810666666666667, 0.8876, 0.8826666666666667, 0.8809333333333333, 0.8858666666666667, 0.8810666666666667, 0.8833333333333333, 0.8816, 0.8772, 0.8844, 0.88, 0.8824, 0.8810666666666667, 0.8828, 0.8854666666666666, 0.8830666666666667, 0.8850666666666667, 0.8818666666666667, 0.8812, 0.8749333333333333, 0.8786666666666667, 0.8848, 0.8841333333333333, 0.8850666666666667, 0.8857333333333334, 0.8841333333333333, 0.8858666666666667, 0.8841333333333333, 0.8810666666666667, 0.8861333333333333, 0.8769333333333333, 0.8830666666666667, 0.8857333333333334, 0.8890666666666667, 0.888, 0.8868, 0.8854666666666666, 0.8861333333333333, 0.8866666666666667, 0.8865333333333333, 0.8882666666666666, 0.8904, 0.8844, 0.8888, 0.8856, 0.8857333333333334, 0.8870666666666667, 0.884, 0.8885333333333333, 0.8906666666666667, 0.8841333333333333, 0.8892, 0.8889333333333334, 0.8784, 0.8874666666666666, 0.8884, 0.8812, 0.8890666666666667, 0.8862666666666666, 0.8898666666666667, 0.8849333333333333, 0.8862666666666666, 0.8884, 0.8898666666666667, 0.8888, 0.8914666666666666, 0.8874666666666666, 0.8890666666666667, 0.8856, 0.8868, 0.8901333333333333, 0.8896, 0.8896, 0.8857333333333334, 0.8862666666666666, 0.8909333333333334, 0.8876, 0.89, 0.8850666666666667, 0.8897333333333334, 0.8894666666666666, 0.8910666666666667, 0.8885333333333333, 0.8894666666666666, 0.8897333333333334, 0.8894666666666666, 0.8905333333333333, 0.8916, 0.8874666666666666, 0.8898666666666667, 0.8848, 0.8882666666666666, 0.8874666666666666, 0.8889333333333334, 0.8854666666666666, 0.8885333333333333, 0.89, 0.8894666666666666, 0.8873333333333333, 0.888, 0.8894666666666666, 0.8918666666666667, 0.8941333333333333, 0.8929333333333334, 0.8910666666666667, 0.8854666666666666, 0.8894666666666666, 0.888, 0.8882666666666666, 0.8888, 0.8909333333333334, 0.8894666666666666, 0.8890666666666667, 0.8909333333333334, 0.8914666666666666, 0.8933333333333333, 0.8918666666666667, 0.8904, 0.8901333333333333, 0.89, 0.8894666666666666, 0.8872, 0.8881333333333333, 0.8918666666666667, 0.8886666666666667, 0.8942666666666667, 0.8922666666666667, 0.8925333333333333, 0.8904, 0.8914666666666666, 0.8909333333333334, 0.8933333333333333, 0.8902666666666667, 0.8921333333333333, 0.8917333333333334, 0.8902666666666667, 0.8884, 0.8933333333333333, 0.8934666666666666, 0.8970666666666667, 0.8878666666666667, 0.8896, 0.8948, 0.8926666666666667, 0.8950666666666667, 0.8925333333333333, 0.8896, 0.8914666666666666, 0.8914666666666666, 0.8909333333333334, 0.8909333333333334, 0.8893333333333333, 0.8916, 0.8929333333333334, 0.8904, 0.8930666666666667, 0.8841333333333333, 0.8898666666666667, 0.8934666666666666, 0.8894666666666666, 0.8916, 0.8913333333333333, 0.8902666666666667, 0.8937333333333334, 0.8922666666666667, 0.8962666666666667, 0.8934666666666666, 0.8930666666666667, 0.8896, 0.89, 0.8936, 0.8916, 0.8921333333333333, 0.8950666666666667, 0.8914666666666666, 0.8924, 0.89, 0.8906666666666667, 0.8901333333333333, 0.8936, 0.8944, 0.8901333333333333, 0.8932, 0.8946666666666667, 0.8936, 0.8924, 0.8948, 0.8930666666666667, 0.8933333333333333, 0.8913333333333333], "seed": 536909909, "model": "residualv3", "loss_std": [0.34348323941230774, 0.15432773530483246, 0.13469447195529938, 0.12681597471237183, 0.12361722439527512, 0.12100131809711456, 0.11570513993501663, 0.11398406326770782, 0.1121126264333725, 0.11031855642795563, 0.10562923550605774, 0.10342299938201904, 0.10416267067193985, 0.09950524568557739, 0.100572369992733, 0.09892142564058304, 0.09620993584394455, 0.09319251775741577, 0.09097044914960861, 0.09021027386188507, 0.08748827874660492, 0.0852358490228653, 0.08480992913246155, 0.08342592418193817, 0.08029965311288834, 0.0798167958855629, 0.0766289234161377, 0.07564891874790192, 0.0737263485789299, 0.07325346022844315, 0.07087207585573196, 0.06834215670824051, 0.06597388535737991, 0.06645841896533966, 0.0636008232831955, 0.06439457833766937, 0.06244562566280365, 0.062126465141773224, 0.05874351039528847, 0.05954990163445473, 0.058017440140247345, 0.056259412318468094, 0.05675959214568138, 0.05379011482000351, 0.0546317920088768, 0.05362696200609207, 0.050239693373441696, 0.05103481560945511, 0.04928917810320854, 0.04911647364497185, 0.04816458374261856, 0.049127306789159775, 0.04735001549124718, 0.045643314719200134, 0.044447049498558044, 0.04248310625553131, 0.041790857911109924, 0.04287869483232498, 0.04195170849561691, 0.04112676531076431, 0.040570180863142014, 0.03866812586784363, 0.03784509375691414, 0.036585450172424316, 0.03707236051559448, 0.03517010435461998, 0.03497987985610962, 0.034881748259067535, 0.03523922711610794, 0.03676914796233177, 0.033607617020606995, 0.03388277068734169, 0.03276725485920906, 0.03194940462708473, 0.0302065871655941, 0.03093080408871174, 0.030532652512192726, 0.029558341950178146, 0.02989288419485092, 0.029995186254382133, 0.031048797070980072, 0.028798367828130722, 0.028513377532362938, 0.026992473751306534, 0.027389906346797943, 0.026983339339494705, 0.028174711391329765, 0.027694106101989746, 0.027209356427192688, 0.025607116520404816, 0.024618200957775116, 0.025202136486768723, 0.025727499276399612, 0.02516917511820793, 0.023838451132178307, 0.02286265231668949, 0.023399772122502327, 0.02292240597307682, 0.023501964285969734, 0.02263381890952587, 0.022540509700775146, 0.02203201688826084, 0.022447790950536728, 0.02233322151005268, 0.02137281559407711, 0.02249709889292717, 0.02085433527827263, 0.02101164497435093, 0.0203079991042614, 0.020895423367619514, 0.01968982256948948, 0.019953439012169838, 0.02129400707781315, 0.01977972872555256, 0.019536050036549568, 0.019338438287377357, 0.01903446391224861, 0.019420262426137924, 0.01855308748781681, 0.0202084518969059, 0.019501324743032455, 0.01956753432750702, 0.018819119781255722, 0.018581630662083626, 0.01787613146007061, 0.0182294100522995, 0.01847805827856064, 0.017984112724661827, 0.017771415412425995, 0.018770070746541023, 0.01758144423365593, 0.016288965940475464, 0.016203423961997032, 0.01725606806576252, 0.01643460802733898, 0.016932014375925064, 0.016582613810896873, 0.017098836600780487, 0.016202159225940704, 0.016484417021274567, 0.01680452562868595, 0.016377761960029602, 0.01544952392578125, 0.015518277883529663, 0.017396626994013786, 0.015348979271948338, 0.014766531996428967, 0.014933304861187935, 0.014573846012353897, 0.01463726069778204, 0.014077280648052692, 0.01544881146401167, 0.01428087055683136, 0.01558021828532219, 0.01637168601155281, 0.014648363925516605, 0.014545746147632599, 0.013882849365472794, 0.014456633478403091, 0.014603771269321442, 0.01402189675718546, 0.01412178948521614, 0.013598949648439884, 0.014514023438096046, 0.013646158389747143, 0.013582848943769932, 0.013680889271199703, 0.013753321953117847, 0.013566693291068077, 0.013484508730471134, 0.01364844385534525, 0.013428207486867905, 0.01277163065969944, 0.012648173607885838, 0.013919868506491184, 0.012691928073763847, 0.01327610109001398, 0.01300466526299715, 0.01338755339384079, 0.01240844652056694, 0.01318726222962141, 0.013474849052727222, 0.01238025352358818, 0.012656805105507374, 0.0135792912915349, 0.012160909362137318, 0.012371611781418324, 0.012656651437282562, 0.012470267713069916, 0.012055552564561367, 0.01206128392368555, 0.012674769386649132, 0.01213940978050232, 0.011783100664615631, 0.011402139440178871, 0.011893986724317074, 0.01209722738713026, 0.012571630999445915, 0.01141077559441328, 0.011190264485776424, 0.01146873738616705, 0.011264042928814888, 0.011672744527459145, 0.012016836553812027, 0.011451887898147106, 0.011246542446315289, 0.010996179655194283, 0.01052702497690916, 0.012876354157924652, 0.011190556921064854, 0.0106116384267807, 0.01094992645084858, 0.010067311115562916, 0.011161097325384617, 0.011061654426157475, 0.01031575445085764, 0.010294013656675816, 0.011276965960860252, 0.010729929432272911, 0.011051254346966743, 0.010253836400806904, 0.01072133518755436, 0.010392717085778713, 0.009813846088945866, 0.009358965791761875, 0.010895714163780212, 0.009709359146654606, 0.010381704196333885, 0.010176626034080982, 0.01096373237669468, 0.009597478434443474, 0.010534027591347694, 0.01018369197845459, 0.011358557268977165, 0.009862503036856651, 0.00971865002065897, 0.01027039997279644, 0.009667996317148209, 0.009961705654859543, 0.009817326441407204, 0.010071608237922192, 0.010252218693494797, 0.009863253682851791, 0.009422876872122288, 0.009518172591924667, 0.009953528642654419, 0.009424538351595402, 0.008789220824837685, 0.00912389438599348, 0.009587685577571392, 0.009676855057477951, 0.009384321980178356, 0.008556313812732697, 0.009436817839741707, 0.010105215944349766, 0.009461158886551857, 0.00980145949870348, 0.009146270342171192, 0.008735845796763897, 0.008749200962483883, 0.009230013936758041, 0.00893392413854599, 0.009155983105301857, 0.008823070675134659, 0.00936146080493927, 0.008521354757249355, 0.010014346800744534, 0.008981006219983101, 0.00864881556481123, 0.009053896181285381, 0.008764559403061867, 0.009004605934023857, 0.009441366419196129]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:46 2016", "state": "available"}], "summary": "e117dbf9c1af3c90b15ed4c9ae438a1a"}