{"content": {"hp_model": {"f0": 16, "f1": 32, "f2": 32, "f3": 16, "nonlin": "very_leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.653328776359558, 1.3073593378067017, 1.147170066833496, 1.039827585220337, 0.96072918176651, 0.8968274593353271, 0.8420996069908142, 0.7923624515533447, 0.7456533312797546, 0.7016865015029907, 0.659535825252533, 0.617516040802002, 0.5757297277450562, 0.535794734954834, 0.4952771067619324, 0.4561292827129364, 0.4152596890926361, 0.3764229118824005, 0.33753955364227295, 0.3014737069606781, 0.2674499452114105, 0.2367178052663803, 0.21066373586654663, 0.18995685875415802, 0.16963733732700348, 0.1515692174434662, 0.13559050858020782, 0.11495700478553772, 0.09932713955640793, 0.08255985379219055, 0.0683237612247467, 0.05684956908226013, 0.04494720697402954, 0.035559844225645065, 0.02811453677713871, 0.02228507027029991, 0.018442854285240173, 0.015702111646533012, 0.013741541653871536, 0.012311750091612339, 0.011210689321160316, 0.010305516421794891, 0.009540929459035397, 0.008893143385648727, 0.008338378742337227, 0.007852687500417233, 0.007432969287037849, 0.007061087992042303, 0.0067359949462115765, 0.006440682802349329, 0.006172299385070801, 0.0059341550804674625, 0.005715923383831978, 0.005519548896700144, 0.005338291637599468, 0.005171564873307943, 0.00502307852730155, 0.004885588772594929, 0.0047587561421096325, 0.0046423436142504215, 0.004531482700258493, 0.004430125933140516, 0.004336897749453783, 0.00424986844882369, 0.004168451298028231, 0.004092961549758911, 0.00402231328189373, 0.003956199157983065, 0.003895140951499343, 0.0038378341123461723, 0.0037840104196220636, 0.003733276156708598, 0.003684921422973275, 0.003639788832515478, 0.003596747061237693, 0.0035561518743634224, 0.0035177916288375854, 0.0034817680716514587, 0.0034470015671104193, 0.003414432518184185, 0.0033832299523055553, 0.0033537938725203276, 0.0033260700292885303, 0.003299345728009939, 0.0032739033922553062, 0.003249810542911291, 0.0032268245704472065, 0.003204766195267439, 0.0031841821037232876], "moving_avg_accuracy_train": [0.04411519255837024, 0.09548451595088131, 0.1460639980086632, 0.19394945117475887, 0.23949877818837378, 0.28265280315884983, 0.3232676590786606, 0.36131101720757014, 0.39669383249378065, 0.43011910695365874, 0.46193812729834455, 0.4915489782762897, 0.5194262064349564, 0.5449154931777195, 0.5694994430639896, 0.5920759686842241, 0.6116391323305211, 0.629548032664514, 0.6465331071781475, 0.6634468458212592, 0.6779416913644101, 0.6926400168639049, 0.7045363076920492, 0.7166543347647601, 0.7287580468159235, 0.7436500307844326, 0.7574851498393872, 0.7696394984294148, 0.7831195114186993, 0.7956397508626082, 0.8075079629014397, 0.8190146734173127, 0.8296450804411223, 0.8399168587053802, 0.8499032537110881, 0.8598023233542834, 0.8691487221558061, 0.8780394617319383, 0.8863457218445049, 0.8941213361910622, 0.901396081811297, 0.9081549774599939, 0.9144123697045352, 0.9201835316531939, 0.9255054245426918, 0.9303928564908773, 0.9349171032799587, 0.9390633301520367, 0.9428484488083446, 0.9463155094680694, 0.949514847023708, 0.9524454401464111, 0.9551433917770726, 0.9576273518160967, 0.9599163942738373, 0.962009120617956, 0.9639204400645582, 0.9656708545010241, 0.967257853237891, 0.9687047532915475, 0.9700325399767431, 0.9712554137303147, 0.9723815767454338, 0.973411363451889, 0.9743451469341271, 0.9751925275145701, 0.9759667957810163, 0.9766729378160561, 0.977322416540449, 0.9779162479876408, 0.978471622629399, 0.9789807604022196, 0.9794366592489485, 0.9798632442526712, 0.9802657719464979, 0.9806326971685608, 0.9809745556124652, 0.9812822282119791, 0.9815754095932082, 0.981846248282743, 0.9820923282521339, 0.9823161253733953, 0.9825221930801495, 0.9827123043138474, 0.9828880547217944, 0.9830532055353753, 0.9832041664164078, 0.9833423563581465, 0.9834644021569018], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.043909426769578305, 0.09456837114081323, 0.1451523319841867, 0.19261690276731924, 0.23731255553463854, 0.27926910000941263, 0.3183067278473268, 0.3540610924778802, 0.3867527159578783, 0.416857741331217, 0.44457791128695073, 0.46983432855434, 0.4932559044056078, 0.5135255405746102, 0.5324294518315317, 0.5489964593498092, 0.5625923777032319, 0.5740342176814027, 0.5841355316530967, 0.5937791191598504, 0.6018794699038803, 0.6102247378043055, 0.6161250332256821, 0.6216264935702827, 0.6269684328804231, 0.6336856226778478, 0.6395632835840088, 0.644684338979373, 0.6502139637222489, 0.654834563067268, 0.6591710604032369, 0.6626874010402476, 0.6658399005823072, 0.6686537656163205, 0.6716501113344324, 0.6746265546908236, 0.6776227365240755, 0.6806143279413216, 0.6836617936317527, 0.6863668621507311, 0.6887637732154019, 0.6907389172135154, 0.6925786114767272, 0.6942831644386177, 0.6958416761668192, 0.6971965381058601, 0.6982928140298374, 0.6993171129638266, 0.7001779468481668, 0.7008295975229134, 0.7014404971926853, 0.7019892773868205, 0.702458765499042, 0.7028813048000413, 0.7032982112646907, 0.7037121071939446, 0.7041344711639327, 0.7045522493393315, 0.7049150131572809, 0.7052913582270949, 0.7056412463125179, 0.7059561455893987, 0.7062273479073413, 0.7064714299934898, 0.7066422757460233, 0.7067594158295535, 0.7068648419047306, 0.7069475183411401, 0.7069975130714086, 0.7070303012974003, 0.7071086388257928, 0.707154728538846, 0.7071962092805939, 0.707245748979417, 0.7072903347083579, 0.7073548759269046, 0.7073885489610967, 0.7074188546918696, 0.7074217157870653, 0.7074242907727413, 0.7074144012285997, 0.7074421217326222, 0.7074670701862426, 0.7074529027007508, 0.7074767730575583, 0.707510463409935, 0.7075407847270739, 0.707555866881249, 0.7075572337887567], "moving_var_accuracy_train": [0.01751535193015877, 0.039513083209382356, 0.05858633093554547, 0.07336484746629199, 0.08470103344220187, 0.09299135893835389, 0.09853832173700144, 0.10171016344282148, 0.10280663965674215, 0.10258121644553156, 0.10143514530223817, 0.09918285323275704, 0.09625882655777458, 0.09248027754989031, 0.08867158512299705, 0.08439172219242728, 0.07939700631985074, 0.0743438640884217, 0.06950591248568266, 0.06512999223110183, 0.06050789793386987, 0.05640147509288476, 0.05203502320280642, 0.04815314010374037, 0.04465632470212362, 0.04218663291057627, 0.03969066429290157, 0.037051151570442586, 0.03498143316511983, 0.032894097410203174, 0.03087237778217083, 0.02897677948601867, 0.027096151518843536, 0.025336121225349936, 0.023700061869705177, 0.02221197990094215, 0.020776978445861856, 0.019410687853171103, 0.018090564681372528, 0.016825649819432763, 0.015619382152041625, 0.014468587970347216, 0.013374123792630917, 0.012336468205506643, 0.011357724280301573, 0.010436934771704725, 0.00957746057561087, 0.008774435293522455, 0.008025935873351184, 0.007331526872579967, 0.006690496032476247, 0.006098741813686131, 0.005554378119330025, 0.005054470824676235, 0.004596181180568666, 0.004175978594474115, 0.0037912590132693465, 0.003439708668236907, 0.0031184048863305723, 0.0028254060755849545, 0.002558732625358903, 0.0023163181447775796, 0.002096100518529422, 0.0018960346126236057, 0.0017142787156865533, 0.0015493133287509044, 0.0013997774180116457, 0.0012642874053733321, 0.0011416550683569504, 0.0010306632836103197, 0.0009303729241836609, 0.0008396686232107087, 0.000757572354715677, 0.0006834528921327193, 0.0006165658598181239, 0.0005561209809035857, 0.0005015606875742454, 0.00045225658067324576, 0.0004078045205066163, 0.0003676842508176953, 0.00033146082389794455, 0.0002987655078715138, 0.000269271132182265, 0.00024266929949464164, 0.00021868036339821973, 0.00019705780017943594, 0.0001775571228499111, 0.0001599732787048997, 0.00014411000742735412], "duration": 28391.401663, "accuracy_train": [0.44115192558370253, 0.557808426483481, 0.6012793365287006, 0.6249185296696198, 0.6494427213109081, 0.6710390278931341, 0.6888013623569582, 0.7037012403677556, 0.7151391700696751, 0.7309465770925618, 0.7483093104005168, 0.7580466370777962, 0.7703212598629567, 0.7743190738625876, 0.7907549920404209, 0.7952646992663345, 0.7877076051471945, 0.7907281356704503, 0.799398777800849, 0.8156704936092655, 0.8083953012527685, 0.8249249463593578, 0.8116029251453488, 0.8257165784191584, 0.8376914552763934, 0.8776778865010151, 0.8820012213339794, 0.879028635739664, 0.9044396283222591, 0.9083219058577889, 0.9143218712509228, 0.9225750680601699, 0.925318743655408, 0.9323628630837025, 0.9397808087624585, 0.9488939501430418, 0.9532663113695091, 0.9580561179171282, 0.9611020628576044, 0.9641018653100776, 0.966868792393411, 0.9689850382982651, 0.970728899905408, 0.9721239891911223, 0.9734024605481728, 0.9743797440245479, 0.9756353243816908, 0.9763793720007383, 0.9769145167151162, 0.9775190554055924, 0.9783088850244556, 0.9788207782507383, 0.9794249564530271, 0.9799829921673128, 0.9805177763935032, 0.9808436577150241, 0.9811223150839794, 0.9814245844292175, 0.9815408418696937, 0.9817268537744556, 0.9819826201435032, 0.9822612775124585, 0.9825170438815062, 0.9826794438099853, 0.982749198274271, 0.9828189527385567, 0.982935210179033, 0.9830282161314139, 0.9831677250599853, 0.9832607310123662, 0.9834699944052234, 0.9835630003576044, 0.9835397488695091, 0.9837025092861758, 0.9838885211909376, 0.9839350241671282, 0.9840512816076044, 0.9840512816076044, 0.984214042024271, 0.9842837964885567, 0.9843070479766519, 0.9843302994647471, 0.9843768024409376, 0.9844233054171282, 0.9844698083933187, 0.9845395628576044, 0.9845628143456996, 0.9845860658337948, 0.9845628143456996], "end": "2016-01-29 23:36:45.172000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0], "moving_var_accuracy_valid": [0.01735233983309664, 0.03871406365305786, 0.057871291139187556, 0.0723601313419118, 0.0831034305943916, 0.09063625215332571, 0.09528805442289656, 0.09726462029181807, 0.09715683847445794, 0.09559796760157643, 0.09295384124279166, 0.0893994366371728, 0.08539662491166575, 0.08055468577431278, 0.07571543794416757, 0.0706140857927468, 0.06521631817632856, 0.05987292767747031, 0.05480396380531595, 0.05016055644478808, 0.04573504193989605, 0.04178832921287525, 0.03792281766612339, 0.03440293049281996, 0.03121946428387697, 0.02850360360446089, 0.02596416532356512, 0.023603775666470105, 0.021518588847996328, 0.019558879407961412, 0.017772238349469064, 0.01610629637780159, 0.014585111020285604, 0.013197860446123826, 0.011958877190473472, 0.010842722406910366, 0.009839244116420513, 0.008935866277648126, 0.008125863074092505, 0.007379133327914564, 0.006692926638990582, 0.006058744719411075, 0.005483330522308814, 0.004961146977276941, 0.004486892908811721, 0.004054724475795306, 0.003660068416329205, 0.0033035042694518264, 0.0029798231572944985, 0.0026856626789821276, 0.002420455196742662, 0.0021811201143816703, 0.0019649918747311596, 0.0017700995424060454, 0.0015946538871678398, 0.0014367302870133317, 0.0012946627802202953, 0.0011667673496328227, 0.0010512749929580594, 0.0009474222141664137, 0.0008537817878006614, 0.0007692960630118153, 0.000693028412985951, 0.0006242617562703629, 0.0005620982750837554, 0.0005060119437679051, 0.00045551078130705996, 0.0004100212217145903, 0.0003690415948006229, 0.00033214711093043376, 0.0002989876307525822, 0.00026910798603216795, 0.0002422126732963748, 0.0002180134936025727, 0.0001962300352273421, 0.00017664452182463138, 0.00015899027450125356, 0.0001430995129869873, 0.00012878963536108005, 0.00011591073149993313, 0.00010432053857768977, 9.389540055701018e-05, 8.451146232935156e-05, 7.606212255522284e-05, 6.846103844510759e-05, 6.162514995918623e-05, 5.547090940372502e-05, 4.9925865705723586e-05, 4.493329595107644e-05], "accuracy_test": 0.6757453762755101, "start": "2016-01-29 15:43:33.771000", "learning_rate_per_epoch": [0.003473693737760186, 0.003336552530527115, 0.0032048255670815706, 0.003078299108892679, 0.0029567680321633816, 0.0028400348965078592, 0.0027279104106128216, 0.0026202125009149313, 0.002516766544431448, 0.002417404670268297, 0.002321965526789427, 0.0022302945144474506, 0.0021422426216304302, 0.002057666890323162, 0.001976430183276534, 0.00189840083476156, 0.0018234520684927702, 0.0017514622304588556, 0.0016823145560920238, 0.0016158968210220337, 0.0015521012246608734, 0.0014908242737874389, 0.0014319665497168899, 0.0013754325918853283, 0.0013211305486038327, 0.001268972409889102, 0.001218873425386846, 0.0011707523372024298, 0.0011245310306549072, 0.0010801345342770219, 0.0010374909033998847, 0.0009965307544916868, 0.0009571877308189869, 0.0009193979785777628, 0.0008831001468934119, 0.0008482353878207505, 0.0008147470653057098, 0.0007825808716006577, 0.0007516845944337547, 0.0007220081170089543, 0.0006935032433830202, 0.000666123756673187, 0.000639825186226517, 0.0006145649240352213, 0.0005903019336983562, 0.000566996808629483, 0.000544611772056669, 0.0005231105023995042, 0.0005024581332691014, 0.00048262110794894397, 0.0004635672376025468, 0.00044526561396196485, 0.0004276865511201322, 0.00041080149821937084, 0.00039458306855522096, 0.0003790049522649497, 0.0003640418581198901, 0.00034966948442161083, 0.0003358645481057465, 0.00032260463922284544, 0.00030986822093836963, 0.0002976346295326948, 0.0002858840161934495, 0.0002745973179116845, 0.0002637562283780426, 0.00025334313977509737, 0.00024334115732926875, 0.0002337340556550771, 0.00022450624965131283, 0.00021564275084529072, 0.00020712918194476515, 0.0001989517331821844, 0.00019109711865894496, 0.00018355260544922203, 0.00017630595539230853, 0.00016934539598878473, 0.00016265964950434864, 0.00015623784565832466, 0.00015006957983132452, 0.0001441448403056711, 0.00013845400826539844, 0.00013298784324433655, 0.00012773748312611133, 0.0001226944150403142, 0.00011785044625867158, 0.00011319771147100255, 0.0001087286727852188, 0.0001044360687956214, 0.00010031293641077355], "accuracy_train_first": 0.44115192558370253, "accuracy_train_last": 0.9845628143456996, "batch_size_eval": 1024, "accuracy_train_std": [0.015836182219940185, 0.013420798889266687, 0.013613406086992906, 0.01627380311661588, 0.018275385635410674, 0.019530150420193956, 0.018628699838266486, 0.019900613154985265, 0.01938283357836015, 0.02055371460397327, 0.02168289112608696, 0.02413017562887546, 0.023384810828741102, 0.02384481831560374, 0.024896074109680363, 0.024985432810691183, 0.026735979050825403, 0.028952895260782603, 0.02981831876792195, 0.031487987524332045, 0.03115831041950532, 0.030352990914961204, 0.031267168347828125, 0.029498030540904187, 0.027173377615267315, 0.026316920053309056, 0.026967911527970317, 0.02492938818064815, 0.024891831666168828, 0.023667795835326938, 0.02179869054728839, 0.021559680082432427, 0.02031091727868154, 0.0204555525103309, 0.01826893031735433, 0.016756983561198275, 0.015093542949459209, 0.013451720767349904, 0.012629379392834404, 0.011641215597116132, 0.010479915491889204, 0.009834538565048869, 0.00956648637715144, 0.008911664882841192, 0.008828324705102301, 0.00870100687864654, 0.008192905314037635, 0.007754919049601083, 0.007387094659554507, 0.007126066439186138, 0.007120438112056146, 0.006665067251900166, 0.006158526453993569, 0.005897850698759255, 0.005547258655270196, 0.0054764207825933895, 0.005238406135918478, 0.0050611008050137195, 0.0049765352898641846, 0.004886545685263508, 0.004820525969653668, 0.004772253419165451, 0.004912313353499675, 0.0049229300119711635, 0.004780664777991321, 0.004820337084734437, 0.004909805310054557, 0.0048619694692110475, 0.004879898449424194, 0.004925077826198067, 0.004831073016949334, 0.004762512264978856, 0.004698463446570935, 0.004570606959142338, 0.004529421628481429, 0.004468598591481745, 0.004465828685811142, 0.00444033338135591, 0.004461947929091409, 0.0044460795080580005, 0.004371822753274495, 0.004441680973678205, 0.00437494627922165, 0.004348650304188038, 0.004337427593012126, 0.004269370274219441, 0.00431339080966147, 0.00437763966113044, 0.004417420234774168], "accuracy_test_std": 0.014297858323101608, "error_valid": [0.5609057323042168, 0.4495011295180723, 0.39959202042545183, 0.38020196018448793, 0.36042656955948793, 0.3431219997176205, 0.3303546216114458, 0.3241496258471386, 0.3190226727221386, 0.3121970303087349, 0.3059405591114458, 0.3028579160391567, 0.2959499129329819, 0.30404773390436746, 0.2974353468561747, 0.3019004729856928, 0.3150443571159638, 0.32298922251506024, 0.3249526426016567, 0.31942859327936746, 0.32521737339984935, 0.31466785109186746, 0.3307723079819277, 0.32886036332831325, 0.32495411332831325, 0.30585966914533136, 0.30753776826054224, 0.30922616246234935, 0.30001941359186746, 0.30358004282756024, 0.30180046357304224, 0.3056655332266567, 0.3057876035391567, 0.30602144907756024, 0.30138277720256024, 0.2985854551016567, 0.2954116269766567, 0.2924613493034638, 0.28891101515436746, 0.2892875211784638, 0.28966402720256024, 0.2914847868034638, 0.29086414015436746, 0.29037585890436746, 0.29013171827936746, 0.2906097044427711, 0.29184070265436746, 0.2914641966302711, 0.2920745481927711, 0.29330554640436746, 0.29306140577936746, 0.2930717008659638, 0.2933158414909638, 0.2933158414909638, 0.2929496305534638, 0.2925628294427711, 0.2920642531061747, 0.29168774708207834, 0.2918201124811747, 0.29132153614457834, 0.2912097609186747, 0.2912097609186747, 0.2913318312311747, 0.2913318312311747, 0.2918201124811747, 0.2921863234186747, 0.2921863234186747, 0.2923083937311747, 0.2925525343561747, 0.2926746046686747, 0.2921863234186747, 0.2924304640436747, 0.2924304640436747, 0.2923083937311747, 0.2923083937311747, 0.2920642531061747, 0.2923083937311747, 0.2923083937311747, 0.2925525343561747, 0.2925525343561747, 0.2926746046686747, 0.2923083937311747, 0.2923083937311747, 0.2926746046686747, 0.2923083937311747, 0.2921863234186747, 0.2921863234186747, 0.2923083937311747, 0.2924304640436747], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.03947996842010329, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 32, "valid_ratio": 0.15, "learning_rate": 0.003616472089260999, "optimization": "nesterov_momentum", "nb_data_augmentation": 0, "learning_rate_decay_method": "exp", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 1.982983426239294e-08, "rotation_range": [0, 0], "momentum": 0.7580661556566679}, "accuracy_valid_max": 0.7110889848456325, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.7075695359563253, "accuracy_valid_std": [0.01014191433699578, 0.017548138535683083, 0.02296017923867515, 0.022946165073614935, 0.01986473680321214, 0.01979740814923671, 0.02229137499672088, 0.023746560417053236, 0.018771486634977824, 0.01912159842754412, 0.01753602005971578, 0.01697247109407339, 0.021691912090796402, 0.019375350015974985, 0.018039448456958488, 0.0239929559234874, 0.01857092250752504, 0.021460491815339103, 0.022382629009483415, 0.02171536903181711, 0.02396495970542752, 0.019888051182105668, 0.018212909807761437, 0.01663447165772497, 0.017521637679782206, 0.01842654258886913, 0.017152790104367443, 0.01589553505177425, 0.022158382880497367, 0.01297348508129403, 0.01522823753213094, 0.019222158112154202, 0.017994969974006278, 0.0192337544821584, 0.018356298127438376, 0.018832701124874316, 0.017617994104266836, 0.017986842883562608, 0.017485135090709533, 0.01677450084732821, 0.016358651291223745, 0.01811129833148029, 0.017886071279836803, 0.016915868046142934, 0.01626515747789803, 0.017064657285659847, 0.016709261475632613, 0.017254639787419892, 0.01709946632674629, 0.017026309677117834, 0.01736901201340012, 0.016271594119089034, 0.01579982693166851, 0.015890108684961733, 0.01550280955652973, 0.01625355118001945, 0.01796824929640253, 0.019021601863994105, 0.018413202594770573, 0.018494137828054112, 0.0179413867079846, 0.018283629921837945, 0.018646701456618325, 0.018278665602375437, 0.018237563657471454, 0.01797066211256264, 0.0179441082934619, 0.017978877245446466, 0.018275483014873053, 0.018763810810581785, 0.018416220028626955, 0.01822329892678709, 0.018308142049464217, 0.018339930448543654, 0.01842423649397008, 0.018702921756136033, 0.01871313245946037, 0.018700387410080736, 0.018624389458406356, 0.01843783457742401, 0.018346209104163843, 0.017772142556672345, 0.01740616120180437, 0.01705306486547812, 0.017150888034047743, 0.016967532691465916, 0.016967532691465916, 0.01708821796281075, 0.017214117372372165], "accuracy_valid": [0.43909426769578314, 0.5504988704819277, 0.6004079795745482, 0.6197980398155121, 0.6395734304405121, 0.6568780002823795, 0.6696453783885542, 0.6758503741528614, 0.6809773272778614, 0.6878029696912651, 0.6940594408885542, 0.6971420839608433, 0.7040500870670181, 0.6959522660956325, 0.7025646531438253, 0.6980995270143072, 0.6849556428840362, 0.6770107774849398, 0.6750473573983433, 0.6805714067206325, 0.6747826266001506, 0.6853321489081325, 0.6692276920180723, 0.6711396366716867, 0.6750458866716867, 0.6941403308546686, 0.6924622317394578, 0.6907738375376506, 0.6999805864081325, 0.6964199571724398, 0.6981995364269578, 0.6943344667733433, 0.6942123964608433, 0.6939785509224398, 0.6986172227974398, 0.7014145448983433, 0.7045883730233433, 0.7075386506965362, 0.7110889848456325, 0.7107124788215362, 0.7103359727974398, 0.7085152131965362, 0.7091358598456325, 0.7096241410956325, 0.7098682817206325, 0.7093902955572289, 0.7081592973456325, 0.7085358033697289, 0.7079254518072289, 0.7066944535956325, 0.7069385942206325, 0.7069282991340362, 0.7066841585090362, 0.7066841585090362, 0.7070503694465362, 0.7074371705572289, 0.7079357468938253, 0.7083122529179217, 0.7081798875188253, 0.7086784638554217, 0.7087902390813253, 0.7087902390813253, 0.7086681687688253, 0.7086681687688253, 0.7081798875188253, 0.7078136765813253, 0.7078136765813253, 0.7076916062688253, 0.7074474656438253, 0.7073253953313253, 0.7078136765813253, 0.7075695359563253, 0.7075695359563253, 0.7076916062688253, 0.7076916062688253, 0.7079357468938253, 0.7076916062688253, 0.7076916062688253, 0.7074474656438253, 0.7074474656438253, 0.7073253953313253, 0.7076916062688253, 0.7076916062688253, 0.7073253953313253, 0.7076916062688253, 0.7078136765813253, 0.7078136765813253, 0.7076916062688253, 0.7075695359563253], "seed": 481383988, "model": "residualv3", "loss_std": [0.3272711932659149, 0.18399913609027863, 0.18297801911830902, 0.18321925401687622, 0.18272240459918976, 0.18094289302825928, 0.1803136020898819, 0.17752280831336975, 0.17530706524848938, 0.17211736738681793, 0.16864195466041565, 0.16545040905475616, 0.1615285575389862, 0.15614299476146698, 0.15154610574245453, 0.14596296846866608, 0.13843530416488647, 0.13153573870658875, 0.12227699905633926, 0.11204209178686142, 0.10291395336389542, 0.09368611872196198, 0.08584482222795486, 0.08150166273117065, 0.07506898045539856, 0.06773443520069122, 0.06534566730260849, 0.05618985742330551, 0.050055261701345444, 0.04292924329638481, 0.035689182579517365, 0.030345937237143517, 0.023693770170211792, 0.01793680153787136, 0.014038942754268646, 0.010128641501069069, 0.007630918640643358, 0.0059657650999724865, 0.004904971923679113, 0.004261065274477005, 0.0038117726799100637, 0.0034573632292449474, 0.003180503146722913, 0.002947893925011158, 0.002738187788054347, 0.002560501219704747, 0.0024102290626615286, 0.0022805389016866684, 0.0021676989272236824, 0.002070013200864196, 0.001980585278943181, 0.0019040904007852077, 0.0018334289779886603, 0.0017709233798086643, 0.001709786127321422, 0.001654226565733552, 0.0016039916081354022, 0.0015589038375765085, 0.0015155751025304198, 0.0014759142650291324, 0.0014383692760020494, 0.001404174487106502, 0.001372973551042378, 0.0013434315333142877, 0.0013162036193534732, 0.0012908090138807893, 0.0012666200054809451, 0.001243619597516954, 0.0012227935949340463, 0.0012034177780151367, 0.0011851724702864885, 0.00116811937186867, 0.0011517759412527084, 0.0011360988719388843, 0.0011212341487407684, 0.0011072532506659627, 0.0010940477950498462, 0.0010816884459927678, 0.0010697758989408612, 0.0010586563730612397, 0.0010480318451300263, 0.0010380825260654092, 0.0010287617333233356, 0.0010196276707574725, 0.0010108922142535448, 0.0010026631643995643, 0.000994857051409781, 0.0009873233502730727, 0.0009803641587495804]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:22 2016", "state": "available"}], "summary": "b50b22cb4b1532e43bc5b7778fc63369"}