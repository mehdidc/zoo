{"content": {"hp_model": {"f0": 64, "f1": 16, "f2": 32, "f3": 16, "nonlin": "rectify", "nbg1": 3, "nbg3": 2, "nbg2": 1, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.023275497403931544, 0.018295250776280348, 0.019262123809921594, 0.015541185699837843, 0.017037435646399826, 0.011224279052383535, 0.01639451342129217, 0.01741248041088786, 0.023167849035328907, 0.015738045703763748, 0.015373126623584268, 0.01466549597052907, 0.014712833022154549, 0.012981340654017565, 0.011838129824729104, 0.018250463618597067, 0.01524647379164318, 0.019458145431262206, 0.01513370547531662, 0.01930372409906486, 0.014864534639970856, 0.013193965397443108, 0.01716262813057549, 0.013073790486099367, 0.011762993440190545, 0.01727396645340169, 0.016228181535752, 0.0065630665797366886, 0.012306447911085266, 0.014821816003741769, 0.017713786220233074, 0.011716206592482252, 0.014343209747451123, 0.0115979947801686, 0.019160805698937362, 0.016591926952298455, 0.02013438071315795, 0.012406287142900402, 0.012147529743911228, 0.014140910380061322, 0.012527404048421186, 0.009272583884196831, 0.011885261969557703, 0.014892544065569288, 0.017262427381891148, 0.020441575517138138, 0.013723548241218105, 0.014600835528600416, 0.006154043183371962, 0.01903721622448129, 0.01283895809382009, 0.01103766069766599, 0.012907386638532168, 0.016994973719134072, 0.008916908379449353, 0.016291722926739673, 0.010542082909318889, 0.015830777599597164, 0.016930653108107322, 0.013583088705001214, 0.015207381618470716, 0.015709326427294695, 0.014391364472437608, 0.0093608839078737, 0.01663011542355354, 0.01584336521021419, 0.010778005140563566, 0.00935278133231517, 0.017015655746936968, 0.01457660301466151, 0.011224449310855986, 0.011340702583195372, 0.015630555214000467, 0.014394297974407102, 0.010898731943666687, 0.008632320579513193, 0.01099526107738297, 0.01300820372954345, 0.01208635936245956, 0.011208380639455116, 0.013958659492393959, 0.01461131745040787, 0.00837804071051737, 0.013335395786740149, 0.010985708709136639, 0.011577483920610771, 0.015565022476737996, 0.01926557791375313, 0.013978044803759831, 0.007324822963378728, 0.012096567070804962, 0.01535469818749363, 0.009567145067767001, 0.01461109694546146, 0.010521709558593573, 0.012513581197073527, 0.009113095187488209, 0.011379208883812162, 0.013541189106198401, 0.015305468977757294, 0.014986946743294767, 0.011810938968717377, 0.01133969957368504, 0.011552019950683041, 0.014665191839776074, 0.010745029408049014, 0.013924552919997118, 0.011474744344763269, 0.020151501217030805, 0.014665061748944172, 0.018695827220479647, 0.014771843165498294, 0.01448012539680123, 0.009915561895709244, 0.015278872351089566, 0.01104219213342976], "moving_avg_accuracy_train": [0.04287224731220007, 0.08738764304171279, 0.12850704466016055, 0.17192324709013931, 0.2065271088486872, 0.24408062316223114, 0.2810148350073683, 0.31566354833318183, 0.34933443438095446, 0.3825537160893614, 0.408595053655859, 0.43630501580596154, 0.4597699094934532, 0.4822058080155457, 0.5050301490889911, 0.5250187066872442, 0.546083823375478, 0.5611068887296522, 0.5798763195099261, 0.5891114973769015, 0.6033205459365535, 0.6166365705176398, 0.6202753666600895, 0.6356834684330802, 0.6392469982624374, 0.6495773495966772, 0.6580195878166883, 0.6648709771051099, 0.6709068028384435, 0.6759645528448354, 0.6771483244948756, 0.6870735272867133, 0.6910495332697437, 0.6931311196022378, 0.6965081976048694, 0.7046413199701669, 0.7086740906586431, 0.7122386243068799, 0.7130171403772532, 0.7191302826345167, 0.7226345915457605, 0.72809465669674, 0.7326435588230701, 0.7286450975859273, 0.7300755991034051, 0.7369204264900377, 0.7423327581488967, 0.7343411514519933, 0.7425472827941971, 0.7414150779609291, 0.7414797211026822, 0.7469474397454594, 0.7439804904195384, 0.735829193379015, 0.7397908339859732, 0.7418355550643508, 0.748718150582279, 0.7501558809628661, 0.7477072138937113, 0.7485560094790079, 0.7414486659312086, 0.7442086910185972, 0.7497989682114956, 0.755250348643253, 0.7596875238022297, 0.7667563602462961, 0.7636612659344757, 0.7668621461643486, 0.7624643033525463, 0.7560649827671995, 0.7585701284031318, 0.7556125328697122, 0.7611129605535789, 0.763318281994299, 0.7613599433493634, 0.7667479741101727, 0.7667812598536866, 0.7665686084726462, 0.7588010042122217, 0.760881376982214, 0.7568960678185072, 0.758093148063696, 0.7617696761652261, 0.7584900169997001, 0.761255251843427, 0.7628465639087614, 0.7661548514662979, 0.7615538031506021, 0.7663440987076349, 0.7626185148854595, 0.7673863690655183, 0.7662391310550074, 0.7648161360408225, 0.7611410058888906, 0.7582748065377738, 0.7566839200492789, 0.7604663134760139, 0.7592747658770282, 0.7590741596462376, 0.7657508924392273, 0.7690749097613806, 0.7674201390291056, 0.770873030274543, 0.7644326521386539, 0.7586411423580665, 0.7629928235610749, 0.7571895116680072, 0.7576460403830817, 0.7615528946204287, 0.752157638265621, 0.7491093745544152, 0.753779575059245, 0.750554950482907, 0.7509079606487175, 0.7540614861739159, 0.7596314566560536], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 258852835, "moving_var_accuracy_train": [0.016542266306386014, 0.032722623788343494, 0.04466760811464199, 0.05716554700414585, 0.06222583754117351, 0.06869565172273413, 0.07410331059205395, 0.07749777954905825, 0.07995155869933122, 0.08188808892440061, 0.07980264139223106, 0.07873295527424899, 0.07581507086871168, 0.0727638896642832, 0.07017605560678758, 0.06675433195983692, 0.0640725510336534, 0.05969652836401018, 0.05689749931394863, 0.05197534597466579, 0.04859488492593416, 0.04533124502913759, 0.040917288062520596, 0.03896224565849008, 0.035180309796243536, 0.032622724244818664, 0.030001894295807546, 0.02742417868286028, 0.0250096415451228, 0.022738904906754936, 0.020477626253954394, 0.019316450482690872, 0.01752708304661563, 0.015813371756890703, 0.014334676483724352, 0.013496537950032035, 0.012293253309861321, 0.011178281080039902, 0.010065907757482376, 0.009395651556052017, 0.008566608028955617, 0.007978258029136523, 0.007366664821217243, 0.006773887569479916, 0.00611491582385548, 0.005925089199045084, 0.005596220285009954, 0.00561139025489087, 0.005656316553851275, 0.005102221888526427, 0.004592037308295766, 0.004401897101875348, 0.00404093248641106, 0.004234832028755567, 0.003952600192568314, 0.003594968131906737, 0.003661802408286705, 0.003314225785283404, 0.0030367669404951334, 0.0027395743319561915, 0.0029202458895185674, 0.0026967809469138383, 0.0027083636440634238, 0.0027049852171628103, 0.0026116834059694842, 0.002800231103429193, 0.002606424472277844, 0.0024379927332639764, 0.0023682626525134568, 0.0024999981228485155, 0.002306480102478741, 0.0021545584342845927, 0.002211394933205161, 0.0020340264237967386, 0.0018651395936512994, 0.0019399035136010172, 0.001745923133707407, 0.0015717378058253922, 0.0019575851087619304, 0.0018007781556448664, 0.0017636445422533093, 0.0016001770980487691, 0.0015618111181759497, 0.0015024354845365186, 0.0014210106497515196, 0.001301700051579874, 0.0012700329454920485, 0.0013335564613731537, 0.0014067231989493938, 0.0013909706523989535, 0.0014564654884997874, 0.0013226643351246568, 0.0012086221349057432, 0.0012093191561179275, 0.0011623231289892171, 0.0010688690944637787, 0.0010907406853288802, 0.0009944446879218288, 0.0008953624048681343, 0.0012070350114821843, 0.001185773330755743, 0.0010918403932677146, 0.0010899584755163162, 0.0013542688627638353, 0.0015207162463352097, 0.0015390787853352397, 0.001688276767155696, 0.0015213248566493147, 0.0015065639612712733, 0.0021503451428971417, 0.0020189378334849197, 0.0020133410049342406, 0.0019055907373657203, 0.001716153209223638, 0.0016340403974439747, 0.0017498574982465504], "duration": 20379.704729, "accuracy_train": [0.42872247312200074, 0.48802620460732743, 0.49858165922619047, 0.5626690689599483, 0.5179618646756183, 0.582062251984127, 0.6134227416136028, 0.6275019682655039, 0.6523724088109081, 0.681527251465024, 0.6429670917543374, 0.6856946751568844, 0.6709539526808785, 0.684128894714378, 0.71044921875, 0.7049157250715209, 0.7356698735695828, 0.6963144769172204, 0.748801196532392, 0.6722280981796788, 0.731201982973422, 0.736480791747416, 0.6530245319421374, 0.7743563843899963, 0.6713187667266519, 0.7425505116048358, 0.7339997317967885, 0.7265334807009044, 0.7252292344384459, 0.7214843029023624, 0.6878022693452381, 0.7764003524132521, 0.7268335871170174, 0.7118653965946844, 0.726901899628553, 0.7778394212578442, 0.744969026854928, 0.7443194271410114, 0.7200237850106128, 0.7741485629498893, 0.7541733717469545, 0.7772352430555556, 0.7735836779600407, 0.6926589464516427, 0.7429501127607051, 0.7985238729697305, 0.7910437430786268, 0.6624166911798633, 0.8164024648740311, 0.7312252344615172, 0.7420615093784606, 0.7961569075304541, 0.7172779464862495, 0.6624675200143042, 0.7754455994485974, 0.760238044769749, 0.8106615102436323, 0.7630954543881506, 0.7256692102713178, 0.7561951697466777, 0.6774825740010152, 0.7690489168050941, 0.8001114629475821, 0.8043127725290697, 0.7996221002330195, 0.8303758882428941, 0.7358054171280916, 0.7956700682332041, 0.7228837180463271, 0.6984710974990771, 0.7811164391265227, 0.7289941730689369, 0.8106168097083795, 0.7831661749607788, 0.7437348955449428, 0.8152402509574567, 0.767080831545312, 0.7646547460432817, 0.6888925658684016, 0.7796047319121447, 0.7210282853451458, 0.7688668702703949, 0.794858429078996, 0.7289730845099668, 0.7861423654369692, 0.77716837249677, 0.7959294394841271, 0.7201443683093393, 0.8094567587209303, 0.7290882604858804, 0.8102970566860466, 0.7559139889604097, 0.7520091809131598, 0.7280648345215024, 0.7324790123777224, 0.742365941652824, 0.7945078543166297, 0.7485508374861573, 0.7572687035691215, 0.8258414875761352, 0.7989910656607604, 0.7525272024386305, 0.801949051483481, 0.7064692489156515, 0.7065175543327796, 0.8021579543881506, 0.7049597046303987, 0.7617547988187523, 0.7967145827565523, 0.6676003310723514, 0.7216750011535622, 0.7958113796027132, 0.7215333292958656, 0.7540850521410114, 0.7824432159007014, 0.8097611909952934], "end": "2016-01-25 22:38:31.751000", "learning_rate_per_epoch": [0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484, 0.005973279010504484], "accuracy_valid": [0.4236604621611446, 0.48904455713478917, 0.4859604433358434, 0.5578539744917168, 0.5099803510918675, 0.5709360881024097, 0.593895601939006, 0.6172639777861446, 0.6404058617281627, 0.6598165121423193, 0.6299798804593373, 0.6749958819653614, 0.6500920674887049, 0.6620240728539157, 0.6833672580948795, 0.6775475927146084, 0.7080357563064759, 0.6693482916039157, 0.7164894931287651, 0.6475580054593373, 0.7048825183546686, 0.7039059558546686, 0.6260927499058735, 0.7396843232304217, 0.6467946983245482, 0.7106212937688253, 0.7057384812688253, 0.6957272449171686, 0.6899693500564759, 0.6875176487198795, 0.6552293157003012, 0.740478515625, 0.6946080219314759, 0.6800095891378012, 0.6928063817771084, 0.7389416062688253, 0.7061752870858433, 0.706531202936747, 0.6819421239646084, 0.7379959290286144, 0.7097859210278614, 0.7437435288027108, 0.7373958725527108, 0.6585752188441265, 0.7088917192206325, 0.7586581678275602, 0.7445068359375, 0.6367555181664157, 0.7699592314570783, 0.6973141589796686, 0.7033882600715362, 0.7502750258847892, 0.6776799581137049, 0.634974468185241, 0.7353309723268072, 0.7229915756777108, 0.7637336455195783, 0.7251167756965362, 0.6882397755082832, 0.7203560335090362, 0.6437341161521084, 0.7275993622929217, 0.7597362104668675, 0.7605598173945783, 0.7541812758847892, 0.7807528943900602, 0.703113234186747, 0.7513736586972892, 0.684180569935994, 0.6655949971762049, 0.7351883118411144, 0.6840084949171686, 0.7669780685240963, 0.7375473573983433, 0.7012718844126506, 0.7624923522213856, 0.7143848832831325, 0.7203766236822289, 0.6505700536521084, 0.7372532120670181, 0.6826554263930723, 0.7295113069465362, 0.7525428863893072, 0.6836525790662651, 0.7439876694277108, 0.7350765366152108, 0.7516986892884037, 0.6784638554216867, 0.7584434417356928, 0.6906208819653614, 0.7660941618034638, 0.7108345491340362, 0.7123288074171686, 0.6902326101280121, 0.6894810688064759, 0.7053913897778614, 0.7484233810240963, 0.7070797839796686, 0.7140686770519578, 0.7752906155873494, 0.7531135283320783, 0.712512648249247, 0.7490337325865963, 0.6713528920368976, 0.6709866810993976, 0.7500102950865963, 0.6725633000753012, 0.7240181428840362, 0.7573565747364458, 0.6379865163780121, 0.6830716420368976, 0.7479542192206325, 0.6792477527296686, 0.7072430346385542, 0.7298569277108433, 0.7664294874811747], "accuracy_test": 0.26828164859693876, "start": "2016-01-25 16:58:52.047000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0], "accuracy_train_last": 0.8097611909952934, "batch_size_eval": 1024, "accuracy_train_std": [0.014975856629650178, 0.015302421695226352, 0.015920411225515168, 0.015325344017906193, 0.017552993838928423, 0.013596711778069015, 0.014788283455306521, 0.015449736928162079, 0.01682543926269193, 0.01504444007531264, 0.012744568890906091, 0.015750510737817205, 0.01424931004102805, 0.01734433607406132, 0.015976005932680996, 0.015000121007934037, 0.017928270435107398, 0.015180518344068016, 0.017407853791184952, 0.01866473290095433, 0.018518012031988836, 0.018185967448178148, 0.01758095871740862, 0.014661234638981108, 0.019007601328402193, 0.016687089229497423, 0.01700848851792877, 0.018554223383425918, 0.01896407400868005, 0.01864452603516897, 0.01728305179028127, 0.015808738602767915, 0.01684493972207251, 0.01767115345530122, 0.020333693680107977, 0.01693990122223382, 0.015665441467881965, 0.016852802459832693, 0.018773396832304008, 0.016738992458071176, 0.01809477433256237, 0.01601865349828895, 0.018156094820523147, 0.01998862980439967, 0.01484378241407442, 0.01904609981638405, 0.018388556334958762, 0.017563782373505508, 0.018537334862851234, 0.01824219627736863, 0.0185097020207119, 0.017721838949062197, 0.01966161181381281, 0.016361949533541784, 0.016107345507466297, 0.019312165175328296, 0.017547070400782048, 0.02028450603191566, 0.018700798696833658, 0.020235103510454537, 0.01818508493458337, 0.019918724282289146, 0.01603597691474958, 0.021363335997153947, 0.01839098388082031, 0.018654530871425958, 0.017532301121687764, 0.01713490362825092, 0.01618145295626716, 0.01803028677621011, 0.01800030917177676, 0.018779393577840548, 0.01787389667970512, 0.015269122129352207, 0.01687047102114179, 0.01877299238223386, 0.016932975345650397, 0.01957096817289412, 0.019006672006188136, 0.020677118191194686, 0.014894656629322591, 0.017121991061861283, 0.015728128936573098, 0.017073038166042928, 0.01412072582747754, 0.015037279655997278, 0.016287014146788348, 0.01491125732013669, 0.01443650844155727, 0.016997988790904722, 0.014207143032671107, 0.01715430237325303, 0.01825365660584264, 0.01791614661402565, 0.020270406292038166, 0.016456044421095696, 0.016668842841698107, 0.01588032929764918, 0.01587124654489018, 0.0185895344600916, 0.017991169028777072, 0.016304162942445893, 0.014559129147551702, 0.01922637302258888, 0.018742180407873735, 0.01500986819146224, 0.01679844329584674, 0.015501556016044048, 0.01724254214401572, 0.018879742082696452, 0.01663836104121398, 0.01443159331974499, 0.016321321780259114, 0.013583869364628182, 0.017267530916980845, 0.01639455324300946], "accuracy_test_std": 0.013515725277768445, "error_valid": [0.5763395378388554, 0.5109554428652108, 0.5140395566641567, 0.4421460255082832, 0.49001964890813254, 0.4290639118975903, 0.40610439806099397, 0.3827360222138554, 0.3595941382718373, 0.3401834878576807, 0.3700201195406627, 0.3250041180346386, 0.34990793251129515, 0.33797592714608427, 0.3166327419051205, 0.3224524072853916, 0.29196424369352414, 0.33065170839608427, 0.2835105068712349, 0.3524419945406627, 0.29511748164533136, 0.29609404414533136, 0.3739072500941265, 0.26031567676957834, 0.35320530167545183, 0.2893787062311747, 0.2942615187311747, 0.30427275508283136, 0.31003064994352414, 0.3124823512801205, 0.3447706842996988, 0.259521484375, 0.30539197806852414, 0.3199904108621988, 0.3071936182228916, 0.2610583937311747, 0.2938247129141567, 0.293468797063253, 0.3180578760353916, 0.26200407097138556, 0.2902140789721386, 0.2562564711972892, 0.2626041274472892, 0.3414247811558735, 0.29110828077936746, 0.24134183217243976, 0.2554931640625, 0.36324448183358427, 0.23004076854292166, 0.30268584102033136, 0.2966117399284638, 0.24972497411521077, 0.32232004188629515, 0.36502553181475905, 0.2646690276731928, 0.2770084243222892, 0.23626635448042166, 0.2748832243034638, 0.3117602244917168, 0.2796439664909638, 0.3562658838478916, 0.27240063770707834, 0.24026378953313254, 0.23944018260542166, 0.24581872411521077, 0.21924710560993976, 0.296886765813253, 0.24862634130271077, 0.31581943006400603, 0.33440500282379515, 0.26481168815888556, 0.31599150508283136, 0.23302193147590367, 0.2624526426016567, 0.29872811558734935, 0.23750764777861444, 0.28561511671686746, 0.2796233763177711, 0.3494299463478916, 0.2627467879329819, 0.3173445736069277, 0.2704886930534638, 0.24745711361069278, 0.3163474209337349, 0.2560123305722892, 0.2649234633847892, 0.24830131071159633, 0.32153614457831325, 0.24155655826430722, 0.3093791180346386, 0.2339058381965362, 0.2891654508659638, 0.28767119258283136, 0.30976738987198793, 0.31051893119352414, 0.2946086102221386, 0.25157661897590367, 0.29292021602033136, 0.28593132294804224, 0.22470938441265065, 0.24688647166792166, 0.287487351750753, 0.25096626741340367, 0.32864710796310237, 0.32901331890060237, 0.24998970491340367, 0.3274366999246988, 0.2759818571159638, 0.2426434252635542, 0.36201348362198793, 0.31692835796310237, 0.25204578077936746, 0.32075224727033136, 0.2927569653614458, 0.2701430722891567, 0.23357051251882532], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "momentum": 0.6823138531505847, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.005973278996813416, "patience_threshold": 1, "do_flip": true, "batch_size": 128, "l2_decay": 3.665890891667997e-05, "optimization": "rmsprop", "nb_data_augmentation": 1, "learning_rate_decay_method": "none", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.017333913623954256}, "accuracy_valid_max": 0.7807528943900602, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        #nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        nb_data_augmentation=make_constant_param(1),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.7664294874811747, "loss_train": [1.6419861316680908, 1.2655513286590576, 1.1165496110916138, 1.030595302581787, 0.9655117392539978, 0.9157854318618774, 0.8772298693656921, 0.847438395023346, 0.8239848613739014, 0.8066684007644653, 0.7886016964912415, 0.7756505012512207, 0.7620035409927368, 0.749191164970398, 0.739804208278656, 0.7307429313659668, 0.7271135449409485, 0.7176939845085144, 0.7095435857772827, 0.7060543894767761, 0.7002473473548889, 0.6965083479881287, 0.6903569102287292, 0.6874745488166809, 0.6829816699028015, 0.6787140369415283, 0.67523193359375, 0.6687931418418884, 0.6689103841781616, 0.667415976524353, 0.6630172729492188, 0.6629728674888611, 0.6593799591064453, 0.6565624475479126, 0.6549049019813538, 0.6497504115104675, 0.6506304740905762, 0.6447488069534302, 0.6452426910400391, 0.6456387042999268, 0.640917956829071, 0.6375027894973755, 0.639771580696106, 0.6375952363014221, 0.6372706294059753, 0.6300233006477356, 0.6293889284133911, 0.6308349967002869, 0.6276715993881226, 0.6267325282096863, 0.627170979976654, 0.6253913044929504, 0.6260198950767517, 0.6250643134117126, 0.6225005388259888, 0.6216123700141907, 0.6186302304267883, 0.6187265515327454, 0.6169435381889343, 0.6154075860977173, 0.619957447052002, 0.6149739027023315, 0.6122274398803711, 0.612446665763855, 0.6137710213661194, 0.6135922074317932, 0.6102660298347473, 0.6082558035850525, 0.6093751788139343, 0.6105737090110779, 0.6089801788330078, 0.6066958904266357, 0.6084524393081665, 0.6058383584022522, 0.6049836277961731, 0.6035979390144348, 0.6038383841514587, 0.6009632349014282, 0.6033703088760376, 0.5996677279472351, 0.6047253608703613, 0.6025732159614563, 0.6006883382797241, 0.6026128530502319, 0.6014179587364197, 0.5990918278694153, 0.5963819622993469, 0.5980383157730103, 0.5992479920387268, 0.5973769426345825, 0.5985607504844666, 0.5954526662826538, 0.594768226146698, 0.5959192514419556, 0.5958008170127869, 0.595816969871521, 0.5924932956695557, 0.5931528210639954, 0.5930989384651184, 0.592327892780304, 0.5917732119560242, 0.5926839113235474, 0.5914222598075867, 0.5880549550056458, 0.588887095451355, 0.5885778069496155, 0.5904642939567566, 0.589105486869812, 0.5885219573974609, 0.5880566835403442, 0.5884107351303101, 0.5860934853553772, 0.5845078825950623, 0.5855648517608643, 0.5841070413589478, 0.5846431851387024], "accuracy_train_first": 0.42872247312200074, "model": "residualv5", "loss_std": [0.21656954288482666, 0.10779628902673721, 0.0965237021446228, 0.09375891089439392, 0.09217098355293274, 0.08920198678970337, 0.09409517049789429, 0.08684509247541428, 0.08402258902788162, 0.08649478107690811, 0.08072235435247421, 0.0825280174612999, 0.08139745146036148, 0.08191443979740143, 0.07953527569770813, 0.08005402982234955, 0.07864151149988174, 0.07889864593744278, 0.07835569232702255, 0.07749924808740616, 0.08099408447742462, 0.08131909370422363, 0.07548973709344864, 0.07839646190404892, 0.07915046811103821, 0.07755952328443527, 0.07788614183664322, 0.07703106850385666, 0.07813350856304169, 0.08088906854391098, 0.07443435490131378, 0.07501440495252609, 0.07273875921964645, 0.07507000863552094, 0.07538513839244843, 0.08001797646284103, 0.0760355293750763, 0.0735635906457901, 0.07613173872232437, 0.07532446086406708, 0.0775800570845604, 0.07453298568725586, 0.07699194550514221, 0.0729614645242691, 0.07745716720819473, 0.07429419457912445, 0.07366317510604858, 0.07077216356992722, 0.07411826401948929, 0.07242325693368912, 0.07271352410316467, 0.0722188875079155, 0.07299330085515976, 0.07142552733421326, 0.07469473034143448, 0.07143525034189224, 0.07278136909008026, 0.07322144508361816, 0.06823015213012695, 0.06849092990159988, 0.07271541655063629, 0.0731748640537262, 0.07313881814479828, 0.06882684677839279, 0.07143598049879074, 0.06998381018638611, 0.07083845138549805, 0.06889556348323822, 0.06926968693733215, 0.07063882797956467, 0.06678038835525513, 0.06700485199689865, 0.06863794475793839, 0.07141382992267609, 0.0686364471912384, 0.07132822275161743, 0.07066792249679565, 0.06828353554010391, 0.07131373882293701, 0.06958167999982834, 0.07033274322748184, 0.0721329003572464, 0.0712771862745285, 0.0669681578874588, 0.0688714012503624, 0.06863123178482056, 0.06837936490774155, 0.06851138919591904, 0.06977689266204834, 0.06579176336526871, 0.07213912159204483, 0.06717652082443237, 0.0684986263513565, 0.07064365595579147, 0.06791938096284866, 0.06998726725578308, 0.06870242208242416, 0.0687807947397232, 0.07131312042474747, 0.06914109736680984, 0.07023327052593231, 0.06618518382310867, 0.06837993860244751, 0.06886962801218033, 0.06852544099092484, 0.06700450927019119, 0.06744313985109329, 0.06557712703943253, 0.06628026068210602, 0.06943177431821823, 0.06735449284315109, 0.0707138329744339, 0.0702010914683342, 0.06980976462364197, 0.06497599929571152, 0.07038258016109467]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:14 2016", "state": "available"}], "summary": "da6044db4be353344ded913a2a73302f"}